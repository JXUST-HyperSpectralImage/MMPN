creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbe2f95db70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.059, val_acc:0.590]
Epoch [2/120    avg_loss:1.442, val_acc:0.712]
Epoch [3/120    avg_loss:0.989, val_acc:0.676]
Epoch [4/120    avg_loss:0.738, val_acc:0.788]
Epoch [5/120    avg_loss:0.563, val_acc:0.801]
Epoch [6/120    avg_loss:0.455, val_acc:0.837]
Epoch [7/120    avg_loss:0.409, val_acc:0.816]
Epoch [8/120    avg_loss:0.323, val_acc:0.873]
Epoch [9/120    avg_loss:0.323, val_acc:0.887]
Epoch [10/120    avg_loss:0.263, val_acc:0.853]
Epoch [11/120    avg_loss:0.244, val_acc:0.887]
Epoch [12/120    avg_loss:0.246, val_acc:0.898]
Epoch [13/120    avg_loss:0.200, val_acc:0.920]
Epoch [14/120    avg_loss:0.176, val_acc:0.918]
Epoch [15/120    avg_loss:0.178, val_acc:0.932]
Epoch [16/120    avg_loss:0.137, val_acc:0.910]
Epoch [17/120    avg_loss:0.134, val_acc:0.936]
Epoch [18/120    avg_loss:0.171, val_acc:0.931]
Epoch [19/120    avg_loss:0.103, val_acc:0.955]
Epoch [20/120    avg_loss:0.094, val_acc:0.958]
Epoch [21/120    avg_loss:0.073, val_acc:0.964]
Epoch [22/120    avg_loss:0.124, val_acc:0.942]
Epoch [23/120    avg_loss:0.107, val_acc:0.960]
Epoch [24/120    avg_loss:0.076, val_acc:0.961]
Epoch [25/120    avg_loss:0.071, val_acc:0.947]
Epoch [26/120    avg_loss:0.072, val_acc:0.967]
Epoch [27/120    avg_loss:0.085, val_acc:0.928]
Epoch [28/120    avg_loss:0.077, val_acc:0.962]
Epoch [29/120    avg_loss:0.092, val_acc:0.943]
Epoch [30/120    avg_loss:0.093, val_acc:0.958]
Epoch [31/120    avg_loss:0.058, val_acc:0.958]
Epoch [32/120    avg_loss:0.057, val_acc:0.958]
Epoch [33/120    avg_loss:0.052, val_acc:0.966]
Epoch [34/120    avg_loss:0.067, val_acc:0.958]
Epoch [35/120    avg_loss:0.039, val_acc:0.970]
Epoch [36/120    avg_loss:0.032, val_acc:0.962]
Epoch [37/120    avg_loss:0.026, val_acc:0.952]
Epoch [38/120    avg_loss:0.040, val_acc:0.967]
Epoch [39/120    avg_loss:0.027, val_acc:0.977]
Epoch [40/120    avg_loss:0.028, val_acc:0.962]
Epoch [41/120    avg_loss:0.023, val_acc:0.974]
Epoch [42/120    avg_loss:0.053, val_acc:0.970]
Epoch [43/120    avg_loss:0.077, val_acc:0.971]
Epoch [44/120    avg_loss:0.042, val_acc:0.971]
Epoch [45/120    avg_loss:0.026, val_acc:0.967]
Epoch [46/120    avg_loss:0.032, val_acc:0.971]
Epoch [47/120    avg_loss:0.030, val_acc:0.968]
Epoch [48/120    avg_loss:0.030, val_acc:0.974]
Epoch [49/120    avg_loss:0.021, val_acc:0.975]
Epoch [50/120    avg_loss:0.033, val_acc:0.975]
Epoch [51/120    avg_loss:0.019, val_acc:0.976]
Epoch [52/120    avg_loss:0.018, val_acc:0.980]
Epoch [53/120    avg_loss:0.015, val_acc:0.980]
Epoch [54/120    avg_loss:0.022, val_acc:0.980]
Epoch [55/120    avg_loss:0.010, val_acc:0.969]
Epoch [56/120    avg_loss:0.018, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.984]
Epoch [58/120    avg_loss:0.012, val_acc:0.982]
Epoch [59/120    avg_loss:0.011, val_acc:0.981]
Epoch [60/120    avg_loss:0.010, val_acc:0.980]
Epoch [61/120    avg_loss:0.007, val_acc:0.978]
Epoch [62/120    avg_loss:0.027, val_acc:0.977]
Epoch [63/120    avg_loss:0.025, val_acc:0.983]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.015, val_acc:0.982]
Epoch [66/120    avg_loss:0.008, val_acc:0.982]
Epoch [67/120    avg_loss:0.006, val_acc:0.978]
Epoch [68/120    avg_loss:0.026, val_acc:0.963]
Epoch [69/120    avg_loss:0.017, val_acc:0.974]
Epoch [70/120    avg_loss:0.014, val_acc:0.978]
Epoch [71/120    avg_loss:0.008, val_acc:0.982]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.005, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.005, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.004, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     0     0     0     0     0    44     1]
 [    0     0 18075     0    14     0     0     0     1     0]
 [    0    11     0  1941     0     0     0     0    84     0]
 [    0    11    18     3  2931     0     8     0     0     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    58     0     0     0  4818     0     2     0]
 [    0    33     0     0     0     0     0  1253     4     0]
 [    0    35     6    34    35     0     1     0  3460     0]
 [    0     9     0     0    14    65     2     0     0   829]]

Accuracy:
98.80943773648568

F1 scores:
[       nan 0.98885276 0.99732392 0.9671151  0.98256788 0.97570093
 0.99268569 0.98545026 0.96567123 0.94742857]

Kappa:
0.9842084147540138
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ac1ea5b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:1.979, val_acc:0.254]
Epoch [2/120    avg_loss:1.363, val_acc:0.533]
Epoch [3/120    avg_loss:0.877, val_acc:0.685]
Epoch [4/120    avg_loss:0.708, val_acc:0.693]
Epoch [5/120    avg_loss:0.519, val_acc:0.766]
Epoch [6/120    avg_loss:0.430, val_acc:0.722]
Epoch [7/120    avg_loss:0.424, val_acc:0.725]
Epoch [8/120    avg_loss:0.324, val_acc:0.852]
Epoch [9/120    avg_loss:0.303, val_acc:0.866]
Epoch [10/120    avg_loss:0.287, val_acc:0.752]
Epoch [11/120    avg_loss:0.275, val_acc:0.871]
Epoch [12/120    avg_loss:0.221, val_acc:0.833]
Epoch [13/120    avg_loss:0.218, val_acc:0.882]
Epoch [14/120    avg_loss:0.202, val_acc:0.917]
Epoch [15/120    avg_loss:0.167, val_acc:0.873]
Epoch [16/120    avg_loss:0.188, val_acc:0.933]
Epoch [17/120    avg_loss:0.245, val_acc:0.920]
Epoch [18/120    avg_loss:0.141, val_acc:0.931]
Epoch [19/120    avg_loss:0.166, val_acc:0.921]
Epoch [20/120    avg_loss:0.136, val_acc:0.948]
Epoch [21/120    avg_loss:0.128, val_acc:0.912]
Epoch [22/120    avg_loss:0.119, val_acc:0.937]
Epoch [23/120    avg_loss:0.102, val_acc:0.946]
Epoch [24/120    avg_loss:0.084, val_acc:0.949]
Epoch [25/120    avg_loss:0.078, val_acc:0.921]
Epoch [26/120    avg_loss:0.123, val_acc:0.947]
Epoch [27/120    avg_loss:0.077, val_acc:0.962]
Epoch [28/120    avg_loss:0.065, val_acc:0.960]
Epoch [29/120    avg_loss:0.074, val_acc:0.952]
Epoch [30/120    avg_loss:0.086, val_acc:0.950]
Epoch [31/120    avg_loss:0.071, val_acc:0.960]
Epoch [32/120    avg_loss:0.055, val_acc:0.960]
Epoch [33/120    avg_loss:0.050, val_acc:0.961]
Epoch [34/120    avg_loss:0.064, val_acc:0.960]
Epoch [35/120    avg_loss:0.072, val_acc:0.938]
Epoch [36/120    avg_loss:0.129, val_acc:0.949]
Epoch [37/120    avg_loss:0.087, val_acc:0.956]
Epoch [38/120    avg_loss:0.047, val_acc:0.971]
Epoch [39/120    avg_loss:0.038, val_acc:0.970]
Epoch [40/120    avg_loss:0.039, val_acc:0.966]
Epoch [41/120    avg_loss:0.115, val_acc:0.953]
Epoch [42/120    avg_loss:0.101, val_acc:0.917]
Epoch [43/120    avg_loss:0.074, val_acc:0.955]
Epoch [44/120    avg_loss:0.065, val_acc:0.957]
Epoch [45/120    avg_loss:0.078, val_acc:0.962]
Epoch [46/120    avg_loss:0.044, val_acc:0.950]
Epoch [47/120    avg_loss:0.035, val_acc:0.958]
Epoch [48/120    avg_loss:0.050, val_acc:0.969]
Epoch [49/120    avg_loss:0.066, val_acc:0.909]
Epoch [50/120    avg_loss:0.053, val_acc:0.961]
Epoch [51/120    avg_loss:0.286, val_acc:0.496]
Epoch [52/120    avg_loss:1.019, val_acc:0.715]
Epoch [53/120    avg_loss:0.487, val_acc:0.786]
Epoch [54/120    avg_loss:0.374, val_acc:0.792]
Epoch [55/120    avg_loss:0.314, val_acc:0.846]
Epoch [56/120    avg_loss:0.274, val_acc:0.850]
Epoch [57/120    avg_loss:0.235, val_acc:0.876]
Epoch [58/120    avg_loss:0.231, val_acc:0.877]
Epoch [59/120    avg_loss:0.199, val_acc:0.877]
Epoch [60/120    avg_loss:0.190, val_acc:0.891]
Epoch [61/120    avg_loss:0.159, val_acc:0.882]
Epoch [62/120    avg_loss:0.157, val_acc:0.907]
Epoch [63/120    avg_loss:0.152, val_acc:0.921]
Epoch [64/120    avg_loss:0.141, val_acc:0.913]
Epoch [65/120    avg_loss:0.135, val_acc:0.913]
Epoch [66/120    avg_loss:0.134, val_acc:0.919]
Epoch [67/120    avg_loss:0.139, val_acc:0.914]
Epoch [68/120    avg_loss:0.131, val_acc:0.916]
Epoch [69/120    avg_loss:0.132, val_acc:0.919]
Epoch [70/120    avg_loss:0.128, val_acc:0.920]
Epoch [71/120    avg_loss:0.130, val_acc:0.921]
Epoch [72/120    avg_loss:0.123, val_acc:0.919]
Epoch [73/120    avg_loss:0.127, val_acc:0.921]
Epoch [74/120    avg_loss:0.136, val_acc:0.922]
Epoch [75/120    avg_loss:0.133, val_acc:0.925]
Epoch [76/120    avg_loss:0.129, val_acc:0.922]
Epoch [77/120    avg_loss:0.144, val_acc:0.924]
Epoch [78/120    avg_loss:0.132, val_acc:0.923]
Epoch [79/120    avg_loss:0.128, val_acc:0.923]
Epoch [80/120    avg_loss:0.131, val_acc:0.923]
Epoch [81/120    avg_loss:0.131, val_acc:0.923]
Epoch [82/120    avg_loss:0.131, val_acc:0.922]
Epoch [83/120    avg_loss:0.129, val_acc:0.922]
Epoch [84/120    avg_loss:0.125, val_acc:0.922]
Epoch [85/120    avg_loss:0.120, val_acc:0.922]
Epoch [86/120    avg_loss:0.135, val_acc:0.922]
Epoch [87/120    avg_loss:0.127, val_acc:0.922]
Epoch [88/120    avg_loss:0.131, val_acc:0.922]
Epoch [89/120    avg_loss:0.127, val_acc:0.922]
Epoch [90/120    avg_loss:0.122, val_acc:0.922]
Epoch [91/120    avg_loss:0.123, val_acc:0.922]
Epoch [92/120    avg_loss:0.118, val_acc:0.922]
Epoch [93/120    avg_loss:0.124, val_acc:0.922]
Epoch [94/120    avg_loss:0.129, val_acc:0.922]
Epoch [95/120    avg_loss:0.133, val_acc:0.922]
Epoch [96/120    avg_loss:0.123, val_acc:0.922]
Epoch [97/120    avg_loss:0.121, val_acc:0.922]
Epoch [98/120    avg_loss:0.129, val_acc:0.922]
Epoch [99/120    avg_loss:0.132, val_acc:0.922]
Epoch [100/120    avg_loss:0.113, val_acc:0.922]
Epoch [101/120    avg_loss:0.122, val_acc:0.922]
Epoch [102/120    avg_loss:0.125, val_acc:0.922]
Epoch [103/120    avg_loss:0.129, val_acc:0.922]
Epoch [104/120    avg_loss:0.134, val_acc:0.922]
Epoch [105/120    avg_loss:0.142, val_acc:0.922]
Epoch [106/120    avg_loss:0.134, val_acc:0.922]
Epoch [107/120    avg_loss:0.134, val_acc:0.922]
Epoch [108/120    avg_loss:0.123, val_acc:0.922]
Epoch [109/120    avg_loss:0.130, val_acc:0.922]
Epoch [110/120    avg_loss:0.128, val_acc:0.922]
Epoch [111/120    avg_loss:0.145, val_acc:0.922]
Epoch [112/120    avg_loss:0.116, val_acc:0.922]
Epoch [113/120    avg_loss:0.130, val_acc:0.922]
Epoch [114/120    avg_loss:0.140, val_acc:0.922]
Epoch [115/120    avg_loss:0.123, val_acc:0.922]
Epoch [116/120    avg_loss:0.117, val_acc:0.922]
Epoch [117/120    avg_loss:0.127, val_acc:0.922]
Epoch [118/120    avg_loss:0.135, val_acc:0.922]
Epoch [119/120    avg_loss:0.130, val_acc:0.922]
Epoch [120/120    avg_loss:0.126, val_acc:0.922]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6150     0    14     4     0    63   107    94     0]
 [    0     0 16009     0   827     0  1251     0     3     0]
 [    0     6     0  1748     0     0     0     0   280     2]
 [    0     9    37     0  2902     2     9     0     2    11]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   100     0     6     0  4690     1    81     0]
 [    0    76     0     0     0     0    12  1193     9     0]
 [    0     5     1   196    90     0    18     0  3261     0]
 [    0     0     0     0     0     8     0     0     0   911]]

Accuracy:
91.98901019449063

F1 scores:
[       nan 0.97018457 0.93518708 0.87531297 0.85340391 0.99618321
 0.85889571 0.92087997 0.89330229 0.98860553]

Kappa:
0.8960633293243702
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99e4624b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.350]
Epoch [2/120    avg_loss:1.476, val_acc:0.671]
Epoch [3/120    avg_loss:1.001, val_acc:0.618]
Epoch [4/120    avg_loss:0.726, val_acc:0.623]
Epoch [5/120    avg_loss:0.593, val_acc:0.712]
Epoch [6/120    avg_loss:0.542, val_acc:0.696]
Epoch [7/120    avg_loss:0.434, val_acc:0.762]
Epoch [8/120    avg_loss:0.421, val_acc:0.798]
Epoch [9/120    avg_loss:0.371, val_acc:0.797]
Epoch [10/120    avg_loss:0.338, val_acc:0.836]
Epoch [11/120    avg_loss:0.316, val_acc:0.846]
Epoch [12/120    avg_loss:0.254, val_acc:0.864]
Epoch [13/120    avg_loss:0.213, val_acc:0.877]
Epoch [14/120    avg_loss:0.271, val_acc:0.886]
Epoch [15/120    avg_loss:0.233, val_acc:0.884]
Epoch [16/120    avg_loss:0.197, val_acc:0.919]
Epoch [17/120    avg_loss:0.189, val_acc:0.860]
Epoch [18/120    avg_loss:0.225, val_acc:0.847]
Epoch [19/120    avg_loss:0.164, val_acc:0.923]
Epoch [20/120    avg_loss:0.141, val_acc:0.920]
Epoch [21/120    avg_loss:0.160, val_acc:0.933]
Epoch [22/120    avg_loss:0.127, val_acc:0.918]
Epoch [23/120    avg_loss:0.127, val_acc:0.927]
Epoch [24/120    avg_loss:0.129, val_acc:0.943]
Epoch [25/120    avg_loss:0.097, val_acc:0.947]
Epoch [26/120    avg_loss:0.090, val_acc:0.944]
Epoch [27/120    avg_loss:0.115, val_acc:0.934]
Epoch [28/120    avg_loss:0.082, val_acc:0.949]
Epoch [29/120    avg_loss:0.155, val_acc:0.878]
Epoch [30/120    avg_loss:0.094, val_acc:0.923]
Epoch [31/120    avg_loss:0.076, val_acc:0.931]
Epoch [32/120    avg_loss:0.082, val_acc:0.932]
Epoch [33/120    avg_loss:0.083, val_acc:0.941]
Epoch [34/120    avg_loss:0.111, val_acc:0.908]
Epoch [35/120    avg_loss:0.118, val_acc:0.943]
Epoch [36/120    avg_loss:0.061, val_acc:0.958]
Epoch [37/120    avg_loss:0.078, val_acc:0.922]
Epoch [38/120    avg_loss:0.076, val_acc:0.962]
Epoch [39/120    avg_loss:0.055, val_acc:0.967]
Epoch [40/120    avg_loss:0.046, val_acc:0.946]
Epoch [41/120    avg_loss:0.060, val_acc:0.964]
Epoch [42/120    avg_loss:0.030, val_acc:0.962]
Epoch [43/120    avg_loss:0.041, val_acc:0.958]
Epoch [44/120    avg_loss:0.046, val_acc:0.970]
Epoch [45/120    avg_loss:0.073, val_acc:0.950]
Epoch [46/120    avg_loss:0.054, val_acc:0.959]
Epoch [47/120    avg_loss:0.110, val_acc:0.944]
Epoch [48/120    avg_loss:0.116, val_acc:0.935]
Epoch [49/120    avg_loss:0.072, val_acc:0.962]
Epoch [50/120    avg_loss:0.040, val_acc:0.967]
Epoch [51/120    avg_loss:0.061, val_acc:0.961]
Epoch [52/120    avg_loss:0.055, val_acc:0.962]
Epoch [53/120    avg_loss:0.047, val_acc:0.950]
Epoch [54/120    avg_loss:0.071, val_acc:0.936]
Epoch [55/120    avg_loss:0.038, val_acc:0.951]
Epoch [56/120    avg_loss:0.037, val_acc:0.969]
Epoch [57/120    avg_loss:0.027, val_acc:0.936]
Epoch [58/120    avg_loss:0.032, val_acc:0.959]
Epoch [59/120    avg_loss:0.018, val_acc:0.966]
Epoch [60/120    avg_loss:0.020, val_acc:0.966]
Epoch [61/120    avg_loss:0.017, val_acc:0.971]
Epoch [62/120    avg_loss:0.017, val_acc:0.971]
Epoch [63/120    avg_loss:0.016, val_acc:0.971]
Epoch [64/120    avg_loss:0.018, val_acc:0.973]
Epoch [65/120    avg_loss:0.020, val_acc:0.967]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.013, val_acc:0.974]
Epoch [68/120    avg_loss:0.014, val_acc:0.971]
Epoch [69/120    avg_loss:0.019, val_acc:0.971]
Epoch [70/120    avg_loss:0.012, val_acc:0.976]
Epoch [71/120    avg_loss:0.022, val_acc:0.968]
Epoch [72/120    avg_loss:0.014, val_acc:0.975]
Epoch [73/120    avg_loss:0.015, val_acc:0.973]
Epoch [74/120    avg_loss:0.016, val_acc:0.977]
Epoch [75/120    avg_loss:0.012, val_acc:0.977]
Epoch [76/120    avg_loss:0.017, val_acc:0.976]
Epoch [77/120    avg_loss:0.012, val_acc:0.974]
Epoch [78/120    avg_loss:0.016, val_acc:0.975]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.017, val_acc:0.975]
Epoch [81/120    avg_loss:0.011, val_acc:0.976]
Epoch [82/120    avg_loss:0.014, val_acc:0.975]
Epoch [83/120    avg_loss:0.012, val_acc:0.975]
Epoch [84/120    avg_loss:0.013, val_acc:0.973]
Epoch [85/120    avg_loss:0.018, val_acc:0.973]
Epoch [86/120    avg_loss:0.013, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.971]
Epoch [88/120    avg_loss:0.015, val_acc:0.975]
Epoch [89/120    avg_loss:0.011, val_acc:0.975]
Epoch [90/120    avg_loss:0.012, val_acc:0.974]
Epoch [91/120    avg_loss:0.015, val_acc:0.973]
Epoch [92/120    avg_loss:0.013, val_acc:0.973]
Epoch [93/120    avg_loss:0.013, val_acc:0.974]
Epoch [94/120    avg_loss:0.010, val_acc:0.974]
Epoch [95/120    avg_loss:0.011, val_acc:0.974]
Epoch [96/120    avg_loss:0.013, val_acc:0.974]
Epoch [97/120    avg_loss:0.009, val_acc:0.973]
Epoch [98/120    avg_loss:0.010, val_acc:0.973]
Epoch [99/120    avg_loss:0.013, val_acc:0.973]
Epoch [100/120    avg_loss:0.014, val_acc:0.973]
Epoch [101/120    avg_loss:0.013, val_acc:0.974]
Epoch [102/120    avg_loss:0.013, val_acc:0.974]
Epoch [103/120    avg_loss:0.011, val_acc:0.974]
Epoch [104/120    avg_loss:0.016, val_acc:0.974]
Epoch [105/120    avg_loss:0.012, val_acc:0.974]
Epoch [106/120    avg_loss:0.011, val_acc:0.974]
Epoch [107/120    avg_loss:0.014, val_acc:0.973]
Epoch [108/120    avg_loss:0.013, val_acc:0.973]
Epoch [109/120    avg_loss:0.013, val_acc:0.973]
Epoch [110/120    avg_loss:0.015, val_acc:0.973]
Epoch [111/120    avg_loss:0.011, val_acc:0.973]
Epoch [112/120    avg_loss:0.011, val_acc:0.973]
Epoch [113/120    avg_loss:0.017, val_acc:0.973]
Epoch [114/120    avg_loss:0.010, val_acc:0.973]
Epoch [115/120    avg_loss:0.011, val_acc:0.973]
Epoch [116/120    avg_loss:0.010, val_acc:0.973]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.015, val_acc:0.973]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.014, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6313     0     3     0     0     1    69    35    11]
 [    0     0 17917     0    59     0   113     0     1     0]
 [    0     7     0  1895     0     0     0     0   133     1]
 [    0    21    21     0  2904     0    19     0     0     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    86     0     0     0  4789     0     3     0]
 [    0    16     0     0     0     0     0  1266     0     8]
 [    0    11     2    95    49     0     6     0  3408     0]
 [    0     3     0     0     0    25     0     0     0   891]]

Accuracy:
98.05991372038658

F1 scores:
[       nan 0.98617512 0.99219183 0.94068007 0.97058824 0.99051233
 0.97674893 0.96457143 0.95315341 0.97005988]

Kappa:
0.9743154768357802
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61d4961a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.024, val_acc:0.612]
Epoch [2/120    avg_loss:1.474, val_acc:0.673]
Epoch [3/120    avg_loss:1.105, val_acc:0.690]
Epoch [4/120    avg_loss:0.808, val_acc:0.699]
Epoch [5/120    avg_loss:0.696, val_acc:0.778]
Epoch [6/120    avg_loss:0.503, val_acc:0.789]
Epoch [7/120    avg_loss:0.437, val_acc:0.770]
Epoch [8/120    avg_loss:0.401, val_acc:0.855]
Epoch [9/120    avg_loss:0.352, val_acc:0.822]
Epoch [10/120    avg_loss:0.309, val_acc:0.860]
Epoch [11/120    avg_loss:0.277, val_acc:0.855]
Epoch [12/120    avg_loss:0.267, val_acc:0.870]
Epoch [13/120    avg_loss:0.216, val_acc:0.882]
Epoch [14/120    avg_loss:0.236, val_acc:0.787]
Epoch [15/120    avg_loss:0.216, val_acc:0.921]
Epoch [16/120    avg_loss:0.209, val_acc:0.859]
Epoch [17/120    avg_loss:0.212, val_acc:0.859]
Epoch [18/120    avg_loss:0.229, val_acc:0.934]
Epoch [19/120    avg_loss:0.167, val_acc:0.945]
Epoch [20/120    avg_loss:0.147, val_acc:0.878]
Epoch [21/120    avg_loss:0.178, val_acc:0.904]
Epoch [22/120    avg_loss:0.133, val_acc:0.910]
Epoch [23/120    avg_loss:0.155, val_acc:0.910]
Epoch [24/120    avg_loss:0.112, val_acc:0.922]
Epoch [25/120    avg_loss:0.110, val_acc:0.854]
Epoch [26/120    avg_loss:0.115, val_acc:0.949]
Epoch [27/120    avg_loss:0.120, val_acc:0.890]
Epoch [28/120    avg_loss:0.159, val_acc:0.937]
Epoch [29/120    avg_loss:0.124, val_acc:0.922]
Epoch [30/120    avg_loss:0.139, val_acc:0.945]
Epoch [31/120    avg_loss:0.097, val_acc:0.934]
Epoch [32/120    avg_loss:0.141, val_acc:0.912]
Epoch [33/120    avg_loss:0.115, val_acc:0.940]
Epoch [34/120    avg_loss:0.085, val_acc:0.962]
Epoch [35/120    avg_loss:0.074, val_acc:0.968]
Epoch [36/120    avg_loss:0.080, val_acc:0.905]
Epoch [37/120    avg_loss:0.096, val_acc:0.967]
Epoch [38/120    avg_loss:0.060, val_acc:0.971]
Epoch [39/120    avg_loss:0.086, val_acc:0.944]
Epoch [40/120    avg_loss:0.083, val_acc:0.931]
Epoch [41/120    avg_loss:0.113, val_acc:0.967]
Epoch [42/120    avg_loss:0.078, val_acc:0.963]
Epoch [43/120    avg_loss:0.054, val_acc:0.972]
Epoch [44/120    avg_loss:0.069, val_acc:0.958]
Epoch [45/120    avg_loss:0.057, val_acc:0.972]
Epoch [46/120    avg_loss:0.044, val_acc:0.959]
Epoch [47/120    avg_loss:0.044, val_acc:0.977]
Epoch [48/120    avg_loss:0.032, val_acc:0.975]
Epoch [49/120    avg_loss:0.085, val_acc:0.967]
Epoch [50/120    avg_loss:0.043, val_acc:0.973]
Epoch [51/120    avg_loss:0.032, val_acc:0.948]
Epoch [52/120    avg_loss:0.042, val_acc:0.958]
Epoch [53/120    avg_loss:0.031, val_acc:0.973]
Epoch [54/120    avg_loss:0.026, val_acc:0.971]
Epoch [55/120    avg_loss:0.042, val_acc:0.969]
Epoch [56/120    avg_loss:0.023, val_acc:0.976]
Epoch [57/120    avg_loss:0.029, val_acc:0.981]
Epoch [58/120    avg_loss:0.051, val_acc:0.957]
Epoch [59/120    avg_loss:0.055, val_acc:0.962]
Epoch [60/120    avg_loss:0.036, val_acc:0.982]
Epoch [61/120    avg_loss:0.036, val_acc:0.979]
Epoch [62/120    avg_loss:0.027, val_acc:0.973]
Epoch [63/120    avg_loss:0.017, val_acc:0.981]
Epoch [64/120    avg_loss:0.042, val_acc:0.982]
Epoch [65/120    avg_loss:0.018, val_acc:0.987]
Epoch [66/120    avg_loss:0.020, val_acc:0.984]
Epoch [67/120    avg_loss:0.015, val_acc:0.984]
Epoch [68/120    avg_loss:0.025, val_acc:0.981]
Epoch [69/120    avg_loss:0.020, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.025, val_acc:0.980]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.014, val_acc:0.984]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.980]
Epoch [76/120    avg_loss:0.021, val_acc:0.981]
Epoch [77/120    avg_loss:0.021, val_acc:0.987]
Epoch [78/120    avg_loss:0.046, val_acc:0.982]
Epoch [79/120    avg_loss:0.020, val_acc:0.979]
Epoch [80/120    avg_loss:0.095, val_acc:0.967]
Epoch [81/120    avg_loss:0.088, val_acc:0.963]
Epoch [82/120    avg_loss:0.074, val_acc:0.977]
Epoch [83/120    avg_loss:0.028, val_acc:0.979]
Epoch [84/120    avg_loss:0.015, val_acc:0.981]
Epoch [85/120    avg_loss:0.016, val_acc:0.981]
Epoch [86/120    avg_loss:0.028, val_acc:0.981]
Epoch [87/120    avg_loss:0.024, val_acc:0.980]
Epoch [88/120    avg_loss:0.017, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.007, val_acc:0.980]
Epoch [93/120    avg_loss:0.013, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6300     0     7     0     0    21     5    45    54]
 [    0     1 17881     0    84     0   124     0     0     0]
 [    0     1     0  1906     0     0     0     0   128     1]
 [    0    16     7     3  2922     0    19     0     3     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4856     0     3     0]
 [    0    17     0     0     0     0     0  1272     0     1]
 [    0    56     1    71    44     0     1     0  3397     1]
 [    0     0     3     0    14    22     0     0     0   880]]

Accuracy:
98.1346251174897

F1 scores:
[       nan 0.98260937 0.9933613  0.94755158 0.96819085 0.99164134
 0.9811092  0.99104012 0.95060865 0.94725511]

Kappa:
0.9753316852135478
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff6243f7b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.036, val_acc:0.335]
Epoch [2/120    avg_loss:1.430, val_acc:0.553]
Epoch [3/120    avg_loss:1.035, val_acc:0.639]
Epoch [4/120    avg_loss:0.768, val_acc:0.660]
Epoch [5/120    avg_loss:0.599, val_acc:0.743]
Epoch [6/120    avg_loss:0.566, val_acc:0.706]
Epoch [7/120    avg_loss:0.485, val_acc:0.757]
Epoch [8/120    avg_loss:0.408, val_acc:0.797]
Epoch [9/120    avg_loss:0.378, val_acc:0.803]
Epoch [10/120    avg_loss:0.326, val_acc:0.783]
Epoch [11/120    avg_loss:0.292, val_acc:0.867]
Epoch [12/120    avg_loss:0.290, val_acc:0.876]
Epoch [13/120    avg_loss:0.447, val_acc:0.258]
Epoch [14/120    avg_loss:1.009, val_acc:0.714]
Epoch [15/120    avg_loss:0.545, val_acc:0.712]
Epoch [16/120    avg_loss:0.491, val_acc:0.785]
Epoch [17/120    avg_loss:0.422, val_acc:0.782]
Epoch [18/120    avg_loss:0.396, val_acc:0.834]
Epoch [19/120    avg_loss:0.347, val_acc:0.806]
Epoch [20/120    avg_loss:0.298, val_acc:0.830]
Epoch [21/120    avg_loss:0.271, val_acc:0.855]
Epoch [22/120    avg_loss:0.277, val_acc:0.860]
Epoch [23/120    avg_loss:0.244, val_acc:0.830]
Epoch [24/120    avg_loss:0.257, val_acc:0.845]
Epoch [25/120    avg_loss:0.239, val_acc:0.876]
Epoch [26/120    avg_loss:0.220, val_acc:0.896]
Epoch [27/120    avg_loss:0.214, val_acc:0.877]
Epoch [28/120    avg_loss:0.186, val_acc:0.912]
Epoch [29/120    avg_loss:0.203, val_acc:0.871]
Epoch [30/120    avg_loss:0.194, val_acc:0.854]
Epoch [31/120    avg_loss:0.171, val_acc:0.943]
Epoch [32/120    avg_loss:0.158, val_acc:0.919]
Epoch [33/120    avg_loss:0.130, val_acc:0.915]
Epoch [34/120    avg_loss:0.142, val_acc:0.891]
Epoch [35/120    avg_loss:0.157, val_acc:0.926]
Epoch [36/120    avg_loss:0.125, val_acc:0.887]
Epoch [37/120    avg_loss:0.115, val_acc:0.905]
Epoch [38/120    avg_loss:0.100, val_acc:0.943]
Epoch [39/120    avg_loss:0.097, val_acc:0.937]
Epoch [40/120    avg_loss:0.099, val_acc:0.923]
Epoch [41/120    avg_loss:0.148, val_acc:0.930]
Epoch [42/120    avg_loss:0.149, val_acc:0.922]
Epoch [43/120    avg_loss:0.125, val_acc:0.924]
Epoch [44/120    avg_loss:0.087, val_acc:0.935]
Epoch [45/120    avg_loss:0.102, val_acc:0.915]
Epoch [46/120    avg_loss:0.118, val_acc:0.950]
Epoch [47/120    avg_loss:0.079, val_acc:0.916]
Epoch [48/120    avg_loss:0.072, val_acc:0.949]
Epoch [49/120    avg_loss:0.066, val_acc:0.947]
Epoch [50/120    avg_loss:0.065, val_acc:0.956]
Epoch [51/120    avg_loss:0.074, val_acc:0.948]
Epoch [52/120    avg_loss:0.074, val_acc:0.947]
Epoch [53/120    avg_loss:0.081, val_acc:0.912]
Epoch [54/120    avg_loss:0.111, val_acc:0.952]
Epoch [55/120    avg_loss:0.088, val_acc:0.929]
Epoch [56/120    avg_loss:0.071, val_acc:0.933]
Epoch [57/120    avg_loss:0.056, val_acc:0.947]
Epoch [58/120    avg_loss:0.056, val_acc:0.945]
Epoch [59/120    avg_loss:0.052, val_acc:0.961]
Epoch [60/120    avg_loss:0.049, val_acc:0.949]
Epoch [61/120    avg_loss:0.054, val_acc:0.957]
Epoch [62/120    avg_loss:0.051, val_acc:0.947]
Epoch [63/120    avg_loss:0.062, val_acc:0.934]
Epoch [64/120    avg_loss:0.077, val_acc:0.896]
Epoch [65/120    avg_loss:0.087, val_acc:0.954]
Epoch [66/120    avg_loss:0.052, val_acc:0.961]
Epoch [67/120    avg_loss:0.048, val_acc:0.958]
Epoch [68/120    avg_loss:0.046, val_acc:0.970]
Epoch [69/120    avg_loss:0.050, val_acc:0.964]
Epoch [70/120    avg_loss:0.037, val_acc:0.962]
Epoch [71/120    avg_loss:0.030, val_acc:0.970]
Epoch [72/120    avg_loss:0.030, val_acc:0.973]
Epoch [73/120    avg_loss:0.022, val_acc:0.956]
Epoch [74/120    avg_loss:0.029, val_acc:0.965]
Epoch [75/120    avg_loss:0.020, val_acc:0.962]
Epoch [76/120    avg_loss:0.032, val_acc:0.975]
Epoch [77/120    avg_loss:0.060, val_acc:0.965]
Epoch [78/120    avg_loss:0.048, val_acc:0.959]
Epoch [79/120    avg_loss:0.078, val_acc:0.951]
Epoch [80/120    avg_loss:0.040, val_acc:0.968]
Epoch [81/120    avg_loss:0.044, val_acc:0.964]
Epoch [82/120    avg_loss:0.025, val_acc:0.972]
Epoch [83/120    avg_loss:0.022, val_acc:0.965]
Epoch [84/120    avg_loss:0.025, val_acc:0.968]
Epoch [85/120    avg_loss:0.021, val_acc:0.970]
Epoch [86/120    avg_loss:0.057, val_acc:0.950]
Epoch [87/120    avg_loss:0.039, val_acc:0.961]
Epoch [88/120    avg_loss:0.033, val_acc:0.945]
Epoch [89/120    avg_loss:0.024, val_acc:0.974]
Epoch [90/120    avg_loss:0.018, val_acc:0.976]
Epoch [91/120    avg_loss:0.015, val_acc:0.978]
Epoch [92/120    avg_loss:0.013, val_acc:0.974]
Epoch [93/120    avg_loss:0.015, val_acc:0.972]
Epoch [94/120    avg_loss:0.012, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.975]
Epoch [96/120    avg_loss:0.013, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.020, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.977]
Epoch [100/120    avg_loss:0.011, val_acc:0.977]
Epoch [101/120    avg_loss:0.011, val_acc:0.976]
Epoch [102/120    avg_loss:0.011, val_acc:0.974]
Epoch [103/120    avg_loss:0.014, val_acc:0.975]
Epoch [104/120    avg_loss:0.011, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.975]
Epoch [106/120    avg_loss:0.008, val_acc:0.975]
Epoch [107/120    avg_loss:0.012, val_acc:0.976]
Epoch [108/120    avg_loss:0.013, val_acc:0.975]
Epoch [109/120    avg_loss:0.017, val_acc:0.978]
Epoch [110/120    avg_loss:0.012, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.980]
Epoch [119/120    avg_loss:0.010, val_acc:0.975]
Epoch [120/120    avg_loss:0.013, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0     5     0     0     0    28    15    13]
 [    0     4 17962     0    36     0    84     0     4     0]
 [    0     6     0  1961     0     0     0     0    68     1]
 [    0    26    11     0  2911     0    19     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    71     0     0     0  4792     1    14     0]
 [    0    23     0     0     0     0     0  1265     2     0]
 [    0    89     0   154    31     0     1     2  3294     0]
 [    0     0     0     0    14    16     0     0     0   889]]

Accuracy:
98.20933651459282

F1 scores:
[       nan 0.98386225 0.9941883  0.94369586 0.97619048 0.99390708
 0.98056067 0.97834493 0.94519369 0.97424658]

Kappa:
0.9762827000755273
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6383bb9b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:1.943, val_acc:0.669]
Epoch [2/120    avg_loss:1.408, val_acc:0.709]
Epoch [3/120    avg_loss:1.048, val_acc:0.656]
Epoch [4/120    avg_loss:0.792, val_acc:0.767]
Epoch [5/120    avg_loss:0.680, val_acc:0.722]
Epoch [6/120    avg_loss:0.600, val_acc:0.717]
Epoch [7/120    avg_loss:0.523, val_acc:0.791]
Epoch [8/120    avg_loss:0.457, val_acc:0.780]
Epoch [9/120    avg_loss:0.395, val_acc:0.864]
Epoch [10/120    avg_loss:0.372, val_acc:0.334]
Epoch [11/120    avg_loss:0.412, val_acc:0.790]
Epoch [12/120    avg_loss:0.334, val_acc:0.790]
Epoch [13/120    avg_loss:0.271, val_acc:0.894]
Epoch [14/120    avg_loss:0.262, val_acc:0.830]
Epoch [15/120    avg_loss:0.282, val_acc:0.906]
Epoch [16/120    avg_loss:0.231, val_acc:0.911]
Epoch [17/120    avg_loss:0.214, val_acc:0.924]
Epoch [18/120    avg_loss:0.199, val_acc:0.916]
Epoch [19/120    avg_loss:0.191, val_acc:0.868]
Epoch [20/120    avg_loss:0.179, val_acc:0.904]
Epoch [21/120    avg_loss:0.189, val_acc:0.937]
Epoch [22/120    avg_loss:0.146, val_acc:0.932]
Epoch [23/120    avg_loss:0.172, val_acc:0.945]
Epoch [24/120    avg_loss:0.154, val_acc:0.901]
Epoch [25/120    avg_loss:0.201, val_acc:0.910]
Epoch [26/120    avg_loss:0.164, val_acc:0.892]
Epoch [27/120    avg_loss:0.228, val_acc:0.889]
Epoch [28/120    avg_loss:0.174, val_acc:0.933]
Epoch [29/120    avg_loss:0.136, val_acc:0.903]
Epoch [30/120    avg_loss:0.110, val_acc:0.939]
Epoch [31/120    avg_loss:0.109, val_acc:0.945]
Epoch [32/120    avg_loss:0.083, val_acc:0.959]
Epoch [33/120    avg_loss:0.077, val_acc:0.937]
Epoch [34/120    avg_loss:0.109, val_acc:0.954]
Epoch [35/120    avg_loss:0.101, val_acc:0.959]
Epoch [36/120    avg_loss:0.084, val_acc:0.957]
Epoch [37/120    avg_loss:0.079, val_acc:0.963]
Epoch [38/120    avg_loss:0.035, val_acc:0.964]
Epoch [39/120    avg_loss:0.047, val_acc:0.976]
Epoch [40/120    avg_loss:0.089, val_acc:0.957]
Epoch [41/120    avg_loss:0.049, val_acc:0.959]
Epoch [42/120    avg_loss:0.039, val_acc:0.965]
Epoch [43/120    avg_loss:0.028, val_acc:0.964]
Epoch [44/120    avg_loss:0.029, val_acc:0.971]
Epoch [45/120    avg_loss:0.047, val_acc:0.967]
Epoch [46/120    avg_loss:0.158, val_acc:0.926]
Epoch [47/120    avg_loss:0.054, val_acc:0.974]
Epoch [48/120    avg_loss:0.039, val_acc:0.972]
Epoch [49/120    avg_loss:0.028, val_acc:0.970]
Epoch [50/120    avg_loss:0.054, val_acc:0.961]
Epoch [51/120    avg_loss:0.079, val_acc:0.965]
Epoch [52/120    avg_loss:0.038, val_acc:0.975]
Epoch [53/120    avg_loss:0.022, val_acc:0.975]
Epoch [54/120    avg_loss:0.021, val_acc:0.974]
Epoch [55/120    avg_loss:0.020, val_acc:0.974]
Epoch [56/120    avg_loss:0.015, val_acc:0.973]
Epoch [57/120    avg_loss:0.016, val_acc:0.973]
Epoch [58/120    avg_loss:0.016, val_acc:0.973]
Epoch [59/120    avg_loss:0.016, val_acc:0.973]
Epoch [60/120    avg_loss:0.013, val_acc:0.974]
Epoch [61/120    avg_loss:0.018, val_acc:0.976]
Epoch [62/120    avg_loss:0.012, val_acc:0.976]
Epoch [63/120    avg_loss:0.011, val_acc:0.977]
Epoch [64/120    avg_loss:0.014, val_acc:0.977]
Epoch [65/120    avg_loss:0.012, val_acc:0.976]
Epoch [66/120    avg_loss:0.014, val_acc:0.978]
Epoch [67/120    avg_loss:0.010, val_acc:0.978]
Epoch [68/120    avg_loss:0.012, val_acc:0.978]
Epoch [69/120    avg_loss:0.011, val_acc:0.978]
Epoch [70/120    avg_loss:0.009, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.978]
Epoch [72/120    avg_loss:0.016, val_acc:0.976]
Epoch [73/120    avg_loss:0.009, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.976]
Epoch [75/120    avg_loss:0.012, val_acc:0.977]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.012, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.011, val_acc:0.977]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.011, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.975]
Epoch [85/120    avg_loss:0.010, val_acc:0.977]
Epoch [86/120    avg_loss:0.011, val_acc:0.977]
Epoch [87/120    avg_loss:0.010, val_acc:0.975]
Epoch [88/120    avg_loss:0.009, val_acc:0.974]
Epoch [89/120    avg_loss:0.010, val_acc:0.977]
Epoch [90/120    avg_loss:0.010, val_acc:0.975]
Epoch [91/120    avg_loss:0.016, val_acc:0.977]
Epoch [92/120    avg_loss:0.011, val_acc:0.977]
Epoch [93/120    avg_loss:0.011, val_acc:0.977]
Epoch [94/120    avg_loss:0.011, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.977]
Epoch [97/120    avg_loss:0.009, val_acc:0.977]
Epoch [98/120    avg_loss:0.009, val_acc:0.978]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.009, val_acc:0.978]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.977]
Epoch [105/120    avg_loss:0.007, val_acc:0.977]
Epoch [106/120    avg_loss:0.010, val_acc:0.978]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.009, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.012, val_acc:0.978]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.977]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6331     0     1     0     0    11     4    57    28]
 [    0     0 17896     0    74     0   119     0     1     0]
 [    0     6     0  1830     0     0     0     0   200     0]
 [    0    10     2     0  2938     0    13     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    44     0     0     0  4834     0     0     0]
 [    0    48     0     0     0     0     0  1240     0     2]
 [    0    26     1    88    32     0    24     0  3400     0]
 [    0     0     0     0    15     5     0     0     0   899]]

Accuracy:
98.02376304436893

F1 scores:
[       nan 0.98513966 0.99331169 0.92541087 0.97429945 0.99808795
 0.97864156 0.97868982 0.9398756  0.97136683]

Kappa:
0.9738502633977496
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75c2ff5b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.024, val_acc:0.280]
Epoch [2/120    avg_loss:1.453, val_acc:0.602]
Epoch [3/120    avg_loss:1.079, val_acc:0.699]
Epoch [4/120    avg_loss:0.770, val_acc:0.767]
Epoch [5/120    avg_loss:0.615, val_acc:0.676]
Epoch [6/120    avg_loss:0.533, val_acc:0.732]
Epoch [7/120    avg_loss:0.467, val_acc:0.757]
Epoch [8/120    avg_loss:0.405, val_acc:0.759]
Epoch [9/120    avg_loss:0.405, val_acc:0.701]
Epoch [10/120    avg_loss:0.413, val_acc:0.753]
Epoch [11/120    avg_loss:0.349, val_acc:0.822]
Epoch [12/120    avg_loss:0.327, val_acc:0.822]
Epoch [13/120    avg_loss:0.298, val_acc:0.830]
Epoch [14/120    avg_loss:0.239, val_acc:0.874]
Epoch [15/120    avg_loss:0.232, val_acc:0.867]
Epoch [16/120    avg_loss:0.256, val_acc:0.822]
Epoch [17/120    avg_loss:0.210, val_acc:0.882]
Epoch [18/120    avg_loss:0.211, val_acc:0.805]
Epoch [19/120    avg_loss:0.187, val_acc:0.883]
Epoch [20/120    avg_loss:0.161, val_acc:0.934]
Epoch [21/120    avg_loss:0.178, val_acc:0.876]
Epoch [22/120    avg_loss:0.178, val_acc:0.909]
Epoch [23/120    avg_loss:0.133, val_acc:0.836]
Epoch [24/120    avg_loss:0.123, val_acc:0.929]
Epoch [25/120    avg_loss:0.148, val_acc:0.914]
Epoch [26/120    avg_loss:0.142, val_acc:0.938]
Epoch [27/120    avg_loss:0.108, val_acc:0.956]
Epoch [28/120    avg_loss:0.105, val_acc:0.954]
Epoch [29/120    avg_loss:0.095, val_acc:0.956]
Epoch [30/120    avg_loss:0.115, val_acc:0.924]
Epoch [31/120    avg_loss:0.092, val_acc:0.951]
Epoch [32/120    avg_loss:0.086, val_acc:0.967]
Epoch [33/120    avg_loss:0.076, val_acc:0.951]
Epoch [34/120    avg_loss:0.092, val_acc:0.959]
Epoch [35/120    avg_loss:0.078, val_acc:0.967]
Epoch [36/120    avg_loss:0.059, val_acc:0.970]
Epoch [37/120    avg_loss:0.079, val_acc:0.956]
Epoch [38/120    avg_loss:0.130, val_acc:0.943]
Epoch [39/120    avg_loss:0.085, val_acc:0.962]
Epoch [40/120    avg_loss:0.048, val_acc:0.972]
Epoch [41/120    avg_loss:0.052, val_acc:0.966]
Epoch [42/120    avg_loss:0.056, val_acc:0.971]
Epoch [43/120    avg_loss:0.055, val_acc:0.972]
Epoch [44/120    avg_loss:0.044, val_acc:0.975]
Epoch [45/120    avg_loss:0.038, val_acc:0.972]
Epoch [46/120    avg_loss:0.032, val_acc:0.980]
Epoch [47/120    avg_loss:0.064, val_acc:0.970]
Epoch [48/120    avg_loss:0.091, val_acc:0.959]
Epoch [49/120    avg_loss:1.099, val_acc:0.438]
Epoch [50/120    avg_loss:0.946, val_acc:0.526]
Epoch [51/120    avg_loss:0.868, val_acc:0.599]
Epoch [52/120    avg_loss:0.801, val_acc:0.549]
Epoch [53/120    avg_loss:0.740, val_acc:0.624]
Epoch [54/120    avg_loss:0.689, val_acc:0.664]
Epoch [55/120    avg_loss:0.664, val_acc:0.660]
Epoch [56/120    avg_loss:0.701, val_acc:0.638]
Epoch [57/120    avg_loss:0.635, val_acc:0.683]
Epoch [58/120    avg_loss:0.604, val_acc:0.649]
Epoch [59/120    avg_loss:0.571, val_acc:0.720]
Epoch [60/120    avg_loss:0.571, val_acc:0.723]
Epoch [61/120    avg_loss:0.537, val_acc:0.720]
Epoch [62/120    avg_loss:0.521, val_acc:0.715]
Epoch [63/120    avg_loss:0.525, val_acc:0.717]
Epoch [64/120    avg_loss:0.530, val_acc:0.717]
Epoch [65/120    avg_loss:0.526, val_acc:0.725]
Epoch [66/120    avg_loss:0.511, val_acc:0.730]
Epoch [67/120    avg_loss:0.487, val_acc:0.729]
Epoch [68/120    avg_loss:0.522, val_acc:0.719]
Epoch [69/120    avg_loss:0.517, val_acc:0.726]
Epoch [70/120    avg_loss:0.519, val_acc:0.731]
Epoch [71/120    avg_loss:0.528, val_acc:0.727]
Epoch [72/120    avg_loss:0.511, val_acc:0.727]
Epoch [73/120    avg_loss:0.526, val_acc:0.727]
Epoch [74/120    avg_loss:0.497, val_acc:0.726]
Epoch [75/120    avg_loss:0.511, val_acc:0.725]
Epoch [76/120    avg_loss:0.508, val_acc:0.726]
Epoch [77/120    avg_loss:0.500, val_acc:0.727]
Epoch [78/120    avg_loss:0.494, val_acc:0.725]
Epoch [79/120    avg_loss:0.485, val_acc:0.725]
Epoch [80/120    avg_loss:0.519, val_acc:0.724]
Epoch [81/120    avg_loss:0.498, val_acc:0.725]
Epoch [82/120    avg_loss:0.506, val_acc:0.727]
Epoch [83/120    avg_loss:0.496, val_acc:0.726]
Epoch [84/120    avg_loss:0.509, val_acc:0.727]
Epoch [85/120    avg_loss:0.521, val_acc:0.729]
Epoch [86/120    avg_loss:0.509, val_acc:0.729]
Epoch [87/120    avg_loss:0.491, val_acc:0.729]
Epoch [88/120    avg_loss:0.493, val_acc:0.729]
Epoch [89/120    avg_loss:0.522, val_acc:0.729]
Epoch [90/120    avg_loss:0.511, val_acc:0.729]
Epoch [91/120    avg_loss:0.487, val_acc:0.729]
Epoch [92/120    avg_loss:0.515, val_acc:0.729]
Epoch [93/120    avg_loss:0.508, val_acc:0.729]
Epoch [94/120    avg_loss:0.504, val_acc:0.728]
Epoch [95/120    avg_loss:0.521, val_acc:0.729]
Epoch [96/120    avg_loss:0.535, val_acc:0.728]
Epoch [97/120    avg_loss:0.496, val_acc:0.729]
Epoch [98/120    avg_loss:0.535, val_acc:0.729]
Epoch [99/120    avg_loss:0.483, val_acc:0.729]
Epoch [100/120    avg_loss:0.496, val_acc:0.729]
Epoch [101/120    avg_loss:0.498, val_acc:0.729]
Epoch [102/120    avg_loss:0.517, val_acc:0.729]
Epoch [103/120    avg_loss:0.507, val_acc:0.729]
Epoch [104/120    avg_loss:0.515, val_acc:0.729]
Epoch [105/120    avg_loss:0.493, val_acc:0.729]
Epoch [106/120    avg_loss:0.505, val_acc:0.729]
Epoch [107/120    avg_loss:0.526, val_acc:0.729]
Epoch [108/120    avg_loss:0.503, val_acc:0.729]
Epoch [109/120    avg_loss:0.495, val_acc:0.729]
Epoch [110/120    avg_loss:0.490, val_acc:0.729]
Epoch [111/120    avg_loss:0.509, val_acc:0.729]
Epoch [112/120    avg_loss:0.504, val_acc:0.729]
Epoch [113/120    avg_loss:0.503, val_acc:0.729]
Epoch [114/120    avg_loss:0.527, val_acc:0.729]
Epoch [115/120    avg_loss:0.491, val_acc:0.729]
Epoch [116/120    avg_loss:0.521, val_acc:0.729]
Epoch [117/120    avg_loss:0.504, val_acc:0.729]
Epoch [118/120    avg_loss:0.533, val_acc:0.729]
Epoch [119/120    avg_loss:0.519, val_acc:0.729]
Epoch [120/120    avg_loss:0.503, val_acc:0.729]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4759   852    83    32     0   313     9   274   110]
 [    0     7 13173     0   801     0  4109     0     0     0]
 [    0    31    14  1570     2     0     5     0   400    14]
 [    0     9   321     0  2427     0   211     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   304   125    52     0  4290     0   107     0]
 [    0    66    19     0     4     0     0  1173    28     0]
 [    0   191   188   120    87     0   123     2  2860     0]
 [    0    18     0     0    15    40     0     0     0   846]]

Accuracy:
78.09269033330924

F1 scores:
[       nan 0.82671762 0.79930827 0.7981698  0.75938673 0.98490566
 0.61598105 0.94826192 0.789619   0.89571202]

Kappa:
0.7194713257787341
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f25f3eceac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.000, val_acc:0.584]
Epoch [2/120    avg_loss:1.520, val_acc:0.654]
Epoch [3/120    avg_loss:1.213, val_acc:0.672]
Epoch [4/120    avg_loss:0.880, val_acc:0.727]
Epoch [5/120    avg_loss:0.671, val_acc:0.752]
Epoch [6/120    avg_loss:0.576, val_acc:0.785]
Epoch [7/120    avg_loss:0.472, val_acc:0.792]
Epoch [8/120    avg_loss:0.386, val_acc:0.863]
Epoch [9/120    avg_loss:0.400, val_acc:0.868]
Epoch [10/120    avg_loss:0.287, val_acc:0.873]
Epoch [11/120    avg_loss:0.259, val_acc:0.893]
Epoch [12/120    avg_loss:0.226, val_acc:0.911]
Epoch [13/120    avg_loss:0.228, val_acc:0.889]
Epoch [14/120    avg_loss:0.233, val_acc:0.927]
Epoch [15/120    avg_loss:0.217, val_acc:0.830]
Epoch [16/120    avg_loss:0.211, val_acc:0.907]
Epoch [17/120    avg_loss:0.170, val_acc:0.932]
Epoch [18/120    avg_loss:0.168, val_acc:0.881]
Epoch [19/120    avg_loss:0.128, val_acc:0.951]
Epoch [20/120    avg_loss:0.102, val_acc:0.902]
Epoch [21/120    avg_loss:0.103, val_acc:0.945]
Epoch [22/120    avg_loss:0.107, val_acc:0.953]
Epoch [23/120    avg_loss:0.109, val_acc:0.946]
Epoch [24/120    avg_loss:0.090, val_acc:0.906]
Epoch [25/120    avg_loss:0.087, val_acc:0.924]
Epoch [26/120    avg_loss:0.143, val_acc:0.963]
Epoch [27/120    avg_loss:0.079, val_acc:0.961]
Epoch [28/120    avg_loss:0.060, val_acc:0.968]
Epoch [29/120    avg_loss:0.041, val_acc:0.946]
Epoch [30/120    avg_loss:0.093, val_acc:0.916]
Epoch [31/120    avg_loss:0.149, val_acc:0.949]
Epoch [32/120    avg_loss:0.072, val_acc:0.927]
Epoch [33/120    avg_loss:0.043, val_acc:0.967]
Epoch [34/120    avg_loss:0.042, val_acc:0.973]
Epoch [35/120    avg_loss:0.041, val_acc:0.979]
Epoch [36/120    avg_loss:0.051, val_acc:0.968]
Epoch [37/120    avg_loss:0.049, val_acc:0.950]
Epoch [38/120    avg_loss:0.052, val_acc:0.973]
Epoch [39/120    avg_loss:0.034, val_acc:0.968]
Epoch [40/120    avg_loss:0.029, val_acc:0.971]
Epoch [41/120    avg_loss:0.037, val_acc:0.973]
Epoch [42/120    avg_loss:0.075, val_acc:0.954]
Epoch [43/120    avg_loss:0.040, val_acc:0.935]
Epoch [44/120    avg_loss:0.081, val_acc:0.901]
Epoch [45/120    avg_loss:0.044, val_acc:0.957]
Epoch [46/120    avg_loss:0.031, val_acc:0.974]
Epoch [47/120    avg_loss:0.021, val_acc:0.977]
Epoch [48/120    avg_loss:0.036, val_acc:0.972]
Epoch [49/120    avg_loss:0.017, val_acc:0.980]
Epoch [50/120    avg_loss:0.017, val_acc:0.975]
Epoch [51/120    avg_loss:0.014, val_acc:0.974]
Epoch [52/120    avg_loss:0.011, val_acc:0.977]
Epoch [53/120    avg_loss:0.013, val_acc:0.980]
Epoch [54/120    avg_loss:0.014, val_acc:0.979]
Epoch [55/120    avg_loss:0.015, val_acc:0.981]
Epoch [56/120    avg_loss:0.012, val_acc:0.978]
Epoch [57/120    avg_loss:0.014, val_acc:0.983]
Epoch [58/120    avg_loss:0.009, val_acc:0.981]
Epoch [59/120    avg_loss:0.011, val_acc:0.979]
Epoch [60/120    avg_loss:0.010, val_acc:0.979]
Epoch [61/120    avg_loss:0.012, val_acc:0.981]
Epoch [62/120    avg_loss:0.010, val_acc:0.982]
Epoch [63/120    avg_loss:0.012, val_acc:0.981]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.012, val_acc:0.980]
Epoch [67/120    avg_loss:0.016, val_acc:0.979]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.983]
Epoch [74/120    avg_loss:0.014, val_acc:0.982]
Epoch [75/120    avg_loss:0.010, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.009, val_acc:0.983]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     2     1     0     0    11    50     3]
 [    0     1 17987     0    74     0    17     0    11     0]
 [    0     5     0  1944     0     0     0     0    87     0]
 [    0     6    13     0  2930     0    15     0     4     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     1     0  4854     0     0     0]
 [    0    36     0     0     0     0     0  1251     3     0]
 [    0    19     0    38    42     0     1     0  3469     2]
 [    0     0     0     1     0    41     0     0     0   877]]

Accuracy:
98.76846697033234

F1 scores:
[       nan 0.98958333 0.99615097 0.96692365 0.97342193 0.98453414
 0.99416283 0.98040752 0.96428075 0.97174515]

Kappa:
0.9836923599251753
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7943c95b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.093, val_acc:0.262]
Epoch [2/120    avg_loss:1.602, val_acc:0.699]
Epoch [3/120    avg_loss:1.126, val_acc:0.714]
Epoch [4/120    avg_loss:0.761, val_acc:0.746]
Epoch [5/120    avg_loss:0.632, val_acc:0.755]
Epoch [6/120    avg_loss:0.646, val_acc:0.695]
Epoch [7/120    avg_loss:0.537, val_acc:0.799]
Epoch [8/120    avg_loss:0.457, val_acc:0.847]
Epoch [9/120    avg_loss:0.425, val_acc:0.844]
Epoch [10/120    avg_loss:0.385, val_acc:0.876]
Epoch [11/120    avg_loss:0.338, val_acc:0.856]
Epoch [12/120    avg_loss:0.342, val_acc:0.875]
Epoch [13/120    avg_loss:0.277, val_acc:0.891]
Epoch [14/120    avg_loss:0.249, val_acc:0.882]
Epoch [15/120    avg_loss:0.256, val_acc:0.896]
Epoch [16/120    avg_loss:0.227, val_acc:0.934]
Epoch [17/120    avg_loss:0.237, val_acc:0.927]
Epoch [18/120    avg_loss:0.199, val_acc:0.932]
Epoch [19/120    avg_loss:0.166, val_acc:0.906]
Epoch [20/120    avg_loss:0.196, val_acc:0.916]
Epoch [21/120    avg_loss:0.180, val_acc:0.923]
Epoch [22/120    avg_loss:0.142, val_acc:0.936]
Epoch [23/120    avg_loss:0.142, val_acc:0.927]
Epoch [24/120    avg_loss:0.139, val_acc:0.936]
Epoch [25/120    avg_loss:0.267, val_acc:0.882]
Epoch [26/120    avg_loss:0.143, val_acc:0.935]
Epoch [27/120    avg_loss:0.123, val_acc:0.942]
Epoch [28/120    avg_loss:0.132, val_acc:0.917]
Epoch [29/120    avg_loss:0.113, val_acc:0.842]
Epoch [30/120    avg_loss:0.142, val_acc:0.933]
Epoch [31/120    avg_loss:0.104, val_acc:0.955]
Epoch [32/120    avg_loss:0.088, val_acc:0.959]
Epoch [33/120    avg_loss:0.072, val_acc:0.953]
Epoch [34/120    avg_loss:0.070, val_acc:0.959]
Epoch [35/120    avg_loss:0.086, val_acc:0.928]
Epoch [36/120    avg_loss:0.068, val_acc:0.937]
Epoch [37/120    avg_loss:0.086, val_acc:0.957]
Epoch [38/120    avg_loss:0.071, val_acc:0.962]
Epoch [39/120    avg_loss:0.060, val_acc:0.952]
Epoch [40/120    avg_loss:0.099, val_acc:0.921]
Epoch [41/120    avg_loss:0.205, val_acc:0.954]
Epoch [42/120    avg_loss:0.085, val_acc:0.889]
Epoch [43/120    avg_loss:0.079, val_acc:0.957]
Epoch [44/120    avg_loss:0.067, val_acc:0.965]
Epoch [45/120    avg_loss:0.055, val_acc:0.944]
Epoch [46/120    avg_loss:0.076, val_acc:0.955]
Epoch [47/120    avg_loss:0.056, val_acc:0.948]
Epoch [48/120    avg_loss:0.041, val_acc:0.972]
Epoch [49/120    avg_loss:0.033, val_acc:0.969]
Epoch [50/120    avg_loss:0.035, val_acc:0.971]
Epoch [51/120    avg_loss:0.049, val_acc:0.952]
Epoch [52/120    avg_loss:0.039, val_acc:0.938]
Epoch [53/120    avg_loss:0.065, val_acc:0.964]
Epoch [54/120    avg_loss:0.036, val_acc:0.975]
Epoch [55/120    avg_loss:0.042, val_acc:0.968]
Epoch [56/120    avg_loss:0.025, val_acc:0.981]
Epoch [57/120    avg_loss:0.017, val_acc:0.981]
Epoch [58/120    avg_loss:0.025, val_acc:0.916]
Epoch [59/120    avg_loss:0.086, val_acc:0.958]
Epoch [60/120    avg_loss:0.057, val_acc:0.971]
Epoch [61/120    avg_loss:0.054, val_acc:0.980]
Epoch [62/120    avg_loss:0.022, val_acc:0.976]
Epoch [63/120    avg_loss:0.092, val_acc:0.953]
Epoch [64/120    avg_loss:0.043, val_acc:0.977]
Epoch [65/120    avg_loss:0.025, val_acc:0.970]
Epoch [66/120    avg_loss:0.026, val_acc:0.962]
Epoch [67/120    avg_loss:0.024, val_acc:0.969]
Epoch [68/120    avg_loss:0.029, val_acc:0.972]
Epoch [69/120    avg_loss:0.019, val_acc:0.958]
Epoch [70/120    avg_loss:0.031, val_acc:0.961]
Epoch [71/120    avg_loss:0.026, val_acc:0.974]
Epoch [72/120    avg_loss:0.022, val_acc:0.974]
Epoch [73/120    avg_loss:0.017, val_acc:0.974]
Epoch [74/120    avg_loss:0.015, val_acc:0.976]
Epoch [75/120    avg_loss:0.015, val_acc:0.977]
Epoch [76/120    avg_loss:0.016, val_acc:0.976]
Epoch [77/120    avg_loss:0.011, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.978]
Epoch [80/120    avg_loss:0.011, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.010, val_acc:0.980]
Epoch [84/120    avg_loss:0.011, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.980]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.010, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.978]
Epoch [90/120    avg_loss:0.011, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.012, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.011, val_acc:0.978]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.008, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.011, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.009, val_acc:0.978]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.009, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.012, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.978]
Epoch [119/120    avg_loss:0.020, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6303     0     6     3     0     6     1   104     9]
 [    0     0 17800     0    94     0   190     0     6     0]
 [    0    12     0  1872     0     0     0     0   144     8]
 [    0    12     1     3  2946     0     1     0     7     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4866     0     1     0]
 [    0    20     0     0     0     0     0  1269     0     1]
 [    0     6     0    39    46     0     0     0  3480     0]
 [    0     0     0     0    14    17     0     0     0   888]]

Accuracy:
98.15872556816812

F1 scores:
[       nan 0.98599922 0.99158821 0.94641052 0.96987654 0.99352874
 0.97897596 0.99140625 0.9517298  0.97208539]

Kappa:
0.9756747284654755
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f213309bba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.066, val_acc:0.238]
Epoch [2/120    avg_loss:1.526, val_acc:0.381]
Epoch [3/120    avg_loss:1.096, val_acc:0.548]
Epoch [4/120    avg_loss:0.781, val_acc:0.647]
Epoch [5/120    avg_loss:0.656, val_acc:0.686]
Epoch [6/120    avg_loss:0.532, val_acc:0.697]
Epoch [7/120    avg_loss:0.440, val_acc:0.708]
Epoch [8/120    avg_loss:0.417, val_acc:0.726]
Epoch [9/120    avg_loss:0.398, val_acc:0.739]
Epoch [10/120    avg_loss:0.396, val_acc:0.815]
Epoch [11/120    avg_loss:0.304, val_acc:0.794]
Epoch [12/120    avg_loss:0.306, val_acc:0.859]
Epoch [13/120    avg_loss:0.264, val_acc:0.861]
Epoch [14/120    avg_loss:0.217, val_acc:0.910]
Epoch [15/120    avg_loss:0.226, val_acc:0.885]
Epoch [16/120    avg_loss:0.195, val_acc:0.932]
Epoch [17/120    avg_loss:0.193, val_acc:0.896]
Epoch [18/120    avg_loss:0.179, val_acc:0.901]
Epoch [19/120    avg_loss:0.127, val_acc:0.931]
Epoch [20/120    avg_loss:0.091, val_acc:0.935]
Epoch [21/120    avg_loss:0.167, val_acc:0.926]
Epoch [22/120    avg_loss:0.130, val_acc:0.931]
Epoch [23/120    avg_loss:0.158, val_acc:0.948]
Epoch [24/120    avg_loss:0.107, val_acc:0.952]
Epoch [25/120    avg_loss:0.089, val_acc:0.950]
Epoch [26/120    avg_loss:0.082, val_acc:0.944]
Epoch [27/120    avg_loss:0.060, val_acc:0.939]
Epoch [28/120    avg_loss:0.056, val_acc:0.952]
Epoch [29/120    avg_loss:0.094, val_acc:0.965]
Epoch [30/120    avg_loss:0.087, val_acc:0.937]
Epoch [31/120    avg_loss:0.086, val_acc:0.876]
Epoch [32/120    avg_loss:0.078, val_acc:0.950]
Epoch [33/120    avg_loss:0.053, val_acc:0.940]
Epoch [34/120    avg_loss:0.091, val_acc:0.932]
Epoch [35/120    avg_loss:0.054, val_acc:0.968]
Epoch [36/120    avg_loss:0.056, val_acc:0.961]
Epoch [37/120    avg_loss:0.034, val_acc:0.970]
Epoch [38/120    avg_loss:0.039, val_acc:0.972]
Epoch [39/120    avg_loss:0.036, val_acc:0.970]
Epoch [40/120    avg_loss:0.031, val_acc:0.971]
Epoch [41/120    avg_loss:0.044, val_acc:0.975]
Epoch [42/120    avg_loss:0.031, val_acc:0.919]
Epoch [43/120    avg_loss:0.035, val_acc:0.954]
Epoch [44/120    avg_loss:0.028, val_acc:0.975]
Epoch [45/120    avg_loss:0.049, val_acc:0.969]
Epoch [46/120    avg_loss:0.075, val_acc:0.967]
Epoch [47/120    avg_loss:0.060, val_acc:0.960]
Epoch [48/120    avg_loss:0.094, val_acc:0.916]
Epoch [49/120    avg_loss:0.108, val_acc:0.967]
Epoch [50/120    avg_loss:0.041, val_acc:0.961]
Epoch [51/120    avg_loss:0.036, val_acc:0.961]
Epoch [52/120    avg_loss:0.032, val_acc:0.959]
Epoch [53/120    avg_loss:0.023, val_acc:0.975]
Epoch [54/120    avg_loss:0.020, val_acc:0.974]
Epoch [55/120    avg_loss:0.012, val_acc:0.983]
Epoch [56/120    avg_loss:0.015, val_acc:0.981]
Epoch [57/120    avg_loss:0.015, val_acc:0.981]
Epoch [58/120    avg_loss:0.014, val_acc:0.980]
Epoch [59/120    avg_loss:0.016, val_acc:0.981]
Epoch [60/120    avg_loss:0.019, val_acc:0.981]
Epoch [61/120    avg_loss:0.013, val_acc:0.984]
Epoch [62/120    avg_loss:0.012, val_acc:0.982]
Epoch [63/120    avg_loss:0.022, val_acc:0.977]
Epoch [64/120    avg_loss:0.017, val_acc:0.981]
Epoch [65/120    avg_loss:0.010, val_acc:0.982]
Epoch [66/120    avg_loss:0.012, val_acc:0.983]
Epoch [67/120    avg_loss:0.010, val_acc:0.981]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.980]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.009, val_acc:0.987]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.019, val_acc:0.986]
Epoch [76/120    avg_loss:0.042, val_acc:0.978]
Epoch [77/120    avg_loss:0.014, val_acc:0.988]
Epoch [78/120    avg_loss:0.011, val_acc:0.983]
Epoch [79/120    avg_loss:0.039, val_acc:0.960]
Epoch [80/120    avg_loss:0.029, val_acc:0.977]
Epoch [81/120    avg_loss:0.013, val_acc:0.977]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.013, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.976]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.973]
Epoch [91/120    avg_loss:0.017, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.012, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.983]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.020, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.031, val_acc:0.981]
Epoch [113/120    avg_loss:0.015, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6376     0     0     0     0     0     6    47     3]
 [    0     2 17961     0    26     0    97     0     4     0]
 [    0     0     0  1908     0     0     0     0   128     0]
 [    0    13     1     1  2950     0     7     0     0     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4860     0     3     0]
 [    0    16     0     0     0     0     0  1272     2     0]
 [    0     7     0    61    47     0     0     0  3456     0]
 [    0     0     0     0     1    32     0     0     0   886]]

Accuracy:
98.7491866097896

F1 scores:
[       nan 0.99268255 0.9959797  0.95257114 0.98398933 0.98788796
 0.98760415 0.99065421 0.95853557 0.9800885 ]

Kappa:
0.9834447628371523
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a361c1b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.016, val_acc:0.243]
Epoch [2/120    avg_loss:1.554, val_acc:0.688]
Epoch [3/120    avg_loss:1.164, val_acc:0.718]
Epoch [4/120    avg_loss:0.845, val_acc:0.700]
Epoch [5/120    avg_loss:0.623, val_acc:0.761]
Epoch [6/120    avg_loss:0.498, val_acc:0.800]
Epoch [7/120    avg_loss:0.421, val_acc:0.802]
Epoch [8/120    avg_loss:0.417, val_acc:0.839]
Epoch [9/120    avg_loss:0.392, val_acc:0.857]
Epoch [10/120    avg_loss:0.362, val_acc:0.788]
Epoch [11/120    avg_loss:0.296, val_acc:0.831]
Epoch [12/120    avg_loss:0.255, val_acc:0.869]
Epoch [13/120    avg_loss:0.243, val_acc:0.883]
Epoch [14/120    avg_loss:0.213, val_acc:0.899]
Epoch [15/120    avg_loss:0.239, val_acc:0.914]
Epoch [16/120    avg_loss:0.204, val_acc:0.931]
Epoch [17/120    avg_loss:0.179, val_acc:0.935]
Epoch [18/120    avg_loss:0.209, val_acc:0.911]
Epoch [19/120    avg_loss:0.166, val_acc:0.908]
Epoch [20/120    avg_loss:0.150, val_acc:0.926]
Epoch [21/120    avg_loss:0.173, val_acc:0.947]
Epoch [22/120    avg_loss:0.148, val_acc:0.949]
Epoch [23/120    avg_loss:0.129, val_acc:0.957]
Epoch [24/120    avg_loss:0.111, val_acc:0.924]
Epoch [25/120    avg_loss:0.107, val_acc:0.921]
Epoch [26/120    avg_loss:0.124, val_acc:0.967]
Epoch [27/120    avg_loss:0.125, val_acc:0.930]
Epoch [28/120    avg_loss:0.100, val_acc:0.959]
Epoch [29/120    avg_loss:0.505, val_acc:0.340]
Epoch [30/120    avg_loss:1.560, val_acc:0.650]
Epoch [31/120    avg_loss:1.052, val_acc:0.682]
Epoch [32/120    avg_loss:0.838, val_acc:0.748]
Epoch [33/120    avg_loss:0.701, val_acc:0.780]
Epoch [34/120    avg_loss:0.558, val_acc:0.786]
Epoch [35/120    avg_loss:0.519, val_acc:0.831]
Epoch [36/120    avg_loss:0.458, val_acc:0.810]
Epoch [37/120    avg_loss:0.428, val_acc:0.831]
Epoch [38/120    avg_loss:0.401, val_acc:0.791]
Epoch [39/120    avg_loss:0.311, val_acc:0.842]
Epoch [40/120    avg_loss:0.298, val_acc:0.849]
Epoch [41/120    avg_loss:0.301, val_acc:0.853]
Epoch [42/120    avg_loss:0.307, val_acc:0.853]
Epoch [43/120    avg_loss:0.269, val_acc:0.855]
Epoch [44/120    avg_loss:0.289, val_acc:0.853]
Epoch [45/120    avg_loss:0.277, val_acc:0.870]
Epoch [46/120    avg_loss:0.259, val_acc:0.868]
Epoch [47/120    avg_loss:0.264, val_acc:0.886]
Epoch [48/120    avg_loss:0.246, val_acc:0.897]
Epoch [49/120    avg_loss:0.261, val_acc:0.894]
Epoch [50/120    avg_loss:0.240, val_acc:0.892]
Epoch [51/120    avg_loss:0.244, val_acc:0.891]
Epoch [52/120    avg_loss:0.245, val_acc:0.887]
Epoch [53/120    avg_loss:0.253, val_acc:0.885]
Epoch [54/120    avg_loss:0.231, val_acc:0.890]
Epoch [55/120    avg_loss:0.234, val_acc:0.888]
Epoch [56/120    avg_loss:0.226, val_acc:0.891]
Epoch [57/120    avg_loss:0.249, val_acc:0.892]
Epoch [58/120    avg_loss:0.224, val_acc:0.890]
Epoch [59/120    avg_loss:0.232, val_acc:0.893]
Epoch [60/120    avg_loss:0.239, val_acc:0.892]
Epoch [61/120    avg_loss:0.240, val_acc:0.891]
Epoch [62/120    avg_loss:0.237, val_acc:0.895]
Epoch [63/120    avg_loss:0.225, val_acc:0.894]
Epoch [64/120    avg_loss:0.233, val_acc:0.894]
Epoch [65/120    avg_loss:0.234, val_acc:0.897]
Epoch [66/120    avg_loss:0.234, val_acc:0.897]
Epoch [67/120    avg_loss:0.243, val_acc:0.897]
Epoch [68/120    avg_loss:0.253, val_acc:0.897]
Epoch [69/120    avg_loss:0.251, val_acc:0.897]
Epoch [70/120    avg_loss:0.227, val_acc:0.897]
Epoch [71/120    avg_loss:0.252, val_acc:0.897]
Epoch [72/120    avg_loss:0.237, val_acc:0.897]
Epoch [73/120    avg_loss:0.248, val_acc:0.897]
Epoch [74/120    avg_loss:0.225, val_acc:0.897]
Epoch [75/120    avg_loss:0.236, val_acc:0.897]
Epoch [76/120    avg_loss:0.235, val_acc:0.897]
Epoch [77/120    avg_loss:0.230, val_acc:0.897]
Epoch [78/120    avg_loss:0.234, val_acc:0.897]
Epoch [79/120    avg_loss:0.226, val_acc:0.897]
Epoch [80/120    avg_loss:0.232, val_acc:0.897]
Epoch [81/120    avg_loss:0.239, val_acc:0.897]
Epoch [82/120    avg_loss:0.228, val_acc:0.897]
Epoch [83/120    avg_loss:0.237, val_acc:0.897]
Epoch [84/120    avg_loss:0.231, val_acc:0.897]
Epoch [85/120    avg_loss:0.237, val_acc:0.897]
Epoch [86/120    avg_loss:0.217, val_acc:0.897]
Epoch [87/120    avg_loss:0.256, val_acc:0.897]
Epoch [88/120    avg_loss:0.221, val_acc:0.897]
Epoch [89/120    avg_loss:0.225, val_acc:0.897]
Epoch [90/120    avg_loss:0.237, val_acc:0.897]
Epoch [91/120    avg_loss:0.213, val_acc:0.897]
Epoch [92/120    avg_loss:0.237, val_acc:0.897]
Epoch [93/120    avg_loss:0.253, val_acc:0.897]
Epoch [94/120    avg_loss:0.228, val_acc:0.897]
Epoch [95/120    avg_loss:0.211, val_acc:0.897]
Epoch [96/120    avg_loss:0.237, val_acc:0.897]
Epoch [97/120    avg_loss:0.223, val_acc:0.897]
Epoch [98/120    avg_loss:0.239, val_acc:0.897]
Epoch [99/120    avg_loss:0.237, val_acc:0.897]
Epoch [100/120    avg_loss:0.235, val_acc:0.897]
Epoch [101/120    avg_loss:0.230, val_acc:0.897]
Epoch [102/120    avg_loss:0.234, val_acc:0.897]
Epoch [103/120    avg_loss:0.223, val_acc:0.897]
Epoch [104/120    avg_loss:0.225, val_acc:0.897]
Epoch [105/120    avg_loss:0.230, val_acc:0.897]
Epoch [106/120    avg_loss:0.219, val_acc:0.897]
Epoch [107/120    avg_loss:0.241, val_acc:0.897]
Epoch [108/120    avg_loss:0.227, val_acc:0.897]
Epoch [109/120    avg_loss:0.221, val_acc:0.897]
Epoch [110/120    avg_loss:0.233, val_acc:0.897]
Epoch [111/120    avg_loss:0.232, val_acc:0.897]
Epoch [112/120    avg_loss:0.232, val_acc:0.897]
Epoch [113/120    avg_loss:0.218, val_acc:0.897]
Epoch [114/120    avg_loss:0.240, val_acc:0.897]
Epoch [115/120    avg_loss:0.228, val_acc:0.897]
Epoch [116/120    avg_loss:0.237, val_acc:0.897]
Epoch [117/120    avg_loss:0.234, val_acc:0.897]
Epoch [118/120    avg_loss:0.242, val_acc:0.897]
Epoch [119/120    avg_loss:0.229, val_acc:0.897]
Epoch [120/120    avg_loss:0.228, val_acc:0.897]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5562     7     0   142     0    57   209   401    54]
 [    0     0 15404     0   353     0  2333     0     0     0]
 [    0    14     0  1749     5     0     0     0   257    11]
 [    0    30   113     0  2629     0   190     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   284     2     0     0  4487     0   105     0]
 [    0    87     0     0     0     0     1  1186    13     3]
 [    0   114    18    63    38     0   138     0  3200     0]
 [    0     5     0     0    14    65     0     0     0   835]]

Accuracy:
87.62200853155954

F1 scores:
[       nan 0.90852663 0.90836184 0.90857143 0.8545425  0.97570093
 0.74263489 0.88342644 0.84689692 0.91657519]

Kappa:
0.8398941292241847
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f06bd1efb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.045, val_acc:0.284]
Epoch [2/120    avg_loss:1.514, val_acc:0.464]
Epoch [3/120    avg_loss:1.122, val_acc:0.683]
Epoch [4/120    avg_loss:0.809, val_acc:0.660]
Epoch [5/120    avg_loss:0.649, val_acc:0.624]
Epoch [6/120    avg_loss:0.519, val_acc:0.739]
Epoch [7/120    avg_loss:0.448, val_acc:0.780]
Epoch [8/120    avg_loss:0.440, val_acc:0.743]
Epoch [9/120    avg_loss:0.398, val_acc:0.819]
Epoch [10/120    avg_loss:0.296, val_acc:0.896]
Epoch [11/120    avg_loss:0.314, val_acc:0.843]
Epoch [12/120    avg_loss:0.270, val_acc:0.910]
Epoch [13/120    avg_loss:0.240, val_acc:0.923]
Epoch [14/120    avg_loss:0.302, val_acc:0.849]
Epoch [15/120    avg_loss:0.206, val_acc:0.892]
Epoch [16/120    avg_loss:0.249, val_acc:0.918]
Epoch [17/120    avg_loss:0.166, val_acc:0.890]
Epoch [18/120    avg_loss:0.165, val_acc:0.903]
Epoch [19/120    avg_loss:0.121, val_acc:0.932]
Epoch [20/120    avg_loss:0.171, val_acc:0.896]
Epoch [21/120    avg_loss:0.128, val_acc:0.942]
Epoch [22/120    avg_loss:0.204, val_acc:0.896]
Epoch [23/120    avg_loss:0.181, val_acc:0.946]
Epoch [24/120    avg_loss:0.127, val_acc:0.938]
Epoch [25/120    avg_loss:0.171, val_acc:0.912]
Epoch [26/120    avg_loss:0.119, val_acc:0.950]
Epoch [27/120    avg_loss:0.109, val_acc:0.908]
Epoch [28/120    avg_loss:0.124, val_acc:0.954]
Epoch [29/120    avg_loss:0.121, val_acc:0.964]
Epoch [30/120    avg_loss:0.075, val_acc:0.949]
Epoch [31/120    avg_loss:0.074, val_acc:0.965]
Epoch [32/120    avg_loss:0.107, val_acc:0.956]
Epoch [33/120    avg_loss:0.165, val_acc:0.949]
Epoch [34/120    avg_loss:0.087, val_acc:0.964]
Epoch [35/120    avg_loss:0.075, val_acc:0.959]
Epoch [36/120    avg_loss:0.076, val_acc:0.941]
Epoch [37/120    avg_loss:0.091, val_acc:0.964]
Epoch [38/120    avg_loss:0.093, val_acc:0.957]
Epoch [39/120    avg_loss:0.084, val_acc:0.969]
Epoch [40/120    avg_loss:0.060, val_acc:0.962]
Epoch [41/120    avg_loss:0.058, val_acc:0.959]
Epoch [42/120    avg_loss:0.041, val_acc:0.968]
Epoch [43/120    avg_loss:0.038, val_acc:0.969]
Epoch [44/120    avg_loss:0.042, val_acc:0.972]
Epoch [45/120    avg_loss:0.051, val_acc:0.978]
Epoch [46/120    avg_loss:0.033, val_acc:0.983]
Epoch [47/120    avg_loss:0.027, val_acc:0.978]
Epoch [48/120    avg_loss:0.047, val_acc:0.958]
Epoch [49/120    avg_loss:0.036, val_acc:0.976]
Epoch [50/120    avg_loss:0.040, val_acc:0.973]
Epoch [51/120    avg_loss:0.036, val_acc:0.966]
Epoch [52/120    avg_loss:0.028, val_acc:0.984]
Epoch [53/120    avg_loss:0.029, val_acc:0.970]
Epoch [54/120    avg_loss:0.033, val_acc:0.975]
Epoch [55/120    avg_loss:0.020, val_acc:0.975]
Epoch [56/120    avg_loss:0.018, val_acc:0.982]
Epoch [57/120    avg_loss:0.017, val_acc:0.977]
Epoch [58/120    avg_loss:0.019, val_acc:0.974]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.017, val_acc:0.986]
Epoch [61/120    avg_loss:0.019, val_acc:0.982]
Epoch [62/120    avg_loss:0.024, val_acc:0.983]
Epoch [63/120    avg_loss:0.040, val_acc:0.972]
Epoch [64/120    avg_loss:0.025, val_acc:0.981]
Epoch [65/120    avg_loss:0.017, val_acc:0.984]
Epoch [66/120    avg_loss:0.012, val_acc:0.986]
Epoch [67/120    avg_loss:0.024, val_acc:0.977]
Epoch [68/120    avg_loss:0.072, val_acc:0.951]
Epoch [69/120    avg_loss:0.059, val_acc:0.971]
Epoch [70/120    avg_loss:0.020, val_acc:0.976]
Epoch [71/120    avg_loss:0.023, val_acc:0.972]
Epoch [72/120    avg_loss:0.034, val_acc:0.978]
Epoch [73/120    avg_loss:0.042, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.984]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.018, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.039, val_acc:0.977]
Epoch [81/120    avg_loss:0.018, val_acc:0.979]
Epoch [82/120    avg_loss:0.009, val_acc:0.989]
Epoch [83/120    avg_loss:0.022, val_acc:0.987]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.968]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.017, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.985]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.012, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.005, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.966]
Epoch [108/120    avg_loss:0.013, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.051, val_acc:0.966]
Epoch [113/120    avg_loss:0.021, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     6     0     0     0    10    34     2]
 [    0     0 18035     0    34     0    19     0     2     0]
 [    0     1     0  1911     0     0     0     0   123     1]
 [    0    12     2     2  2947     0     2     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     3     0  4860     0     1     0]
 [    0    23     0     0     0     0     0  1255     0    12]
 [    0     1     0    58    43     0     0     0  3469     0]
 [    0     1     0     0     7    15     0     0     0   896]]

Accuracy:
98.9516303954884

F1 scores:
[       nan 0.99299611 0.99803547 0.95240468 0.98135198 0.99428571
 0.99600369 0.98238748 0.9629424  0.97816594]

Kappa:
0.9861139199750849
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f391386fb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.071, val_acc:0.591]
Epoch [2/120    avg_loss:1.509, val_acc:0.690]
Epoch [3/120    avg_loss:1.007, val_acc:0.758]
Epoch [4/120    avg_loss:0.722, val_acc:0.680]
Epoch [5/120    avg_loss:0.529, val_acc:0.792]
Epoch [6/120    avg_loss:0.493, val_acc:0.812]
Epoch [7/120    avg_loss:0.508, val_acc:0.761]
Epoch [8/120    avg_loss:0.435, val_acc:0.835]
Epoch [9/120    avg_loss:0.422, val_acc:0.845]
Epoch [10/120    avg_loss:0.321, val_acc:0.867]
Epoch [11/120    avg_loss:0.309, val_acc:0.858]
Epoch [12/120    avg_loss:0.253, val_acc:0.885]
Epoch [13/120    avg_loss:0.211, val_acc:0.893]
Epoch [14/120    avg_loss:0.196, val_acc:0.925]
Epoch [15/120    avg_loss:0.197, val_acc:0.916]
Epoch [16/120    avg_loss:0.163, val_acc:0.936]
Epoch [17/120    avg_loss:0.152, val_acc:0.944]
Epoch [18/120    avg_loss:0.107, val_acc:0.945]
Epoch [19/120    avg_loss:0.190, val_acc:0.890]
Epoch [20/120    avg_loss:0.129, val_acc:0.952]
Epoch [21/120    avg_loss:0.113, val_acc:0.945]
Epoch [22/120    avg_loss:0.098, val_acc:0.961]
Epoch [23/120    avg_loss:0.096, val_acc:0.935]
Epoch [24/120    avg_loss:0.078, val_acc:0.918]
Epoch [25/120    avg_loss:0.083, val_acc:0.959]
Epoch [26/120    avg_loss:0.064, val_acc:0.965]
Epoch [27/120    avg_loss:0.051, val_acc:0.964]
Epoch [28/120    avg_loss:0.069, val_acc:0.973]
Epoch [29/120    avg_loss:0.069, val_acc:0.961]
Epoch [30/120    avg_loss:0.049, val_acc:0.971]
Epoch [31/120    avg_loss:0.081, val_acc:0.970]
Epoch [32/120    avg_loss:0.053, val_acc:0.968]
Epoch [33/120    avg_loss:0.053, val_acc:0.977]
Epoch [34/120    avg_loss:0.035, val_acc:0.975]
Epoch [35/120    avg_loss:0.028, val_acc:0.982]
Epoch [36/120    avg_loss:0.035, val_acc:0.966]
Epoch [37/120    avg_loss:0.030, val_acc:0.981]
Epoch [38/120    avg_loss:0.021, val_acc:0.982]
Epoch [39/120    avg_loss:0.023, val_acc:0.984]
Epoch [40/120    avg_loss:0.033, val_acc:0.986]
Epoch [41/120    avg_loss:0.037, val_acc:0.971]
Epoch [42/120    avg_loss:0.029, val_acc:0.978]
Epoch [43/120    avg_loss:0.017, val_acc:0.984]
Epoch [44/120    avg_loss:0.025, val_acc:0.980]
Epoch [45/120    avg_loss:0.026, val_acc:0.970]
Epoch [46/120    avg_loss:0.019, val_acc:0.979]
Epoch [47/120    avg_loss:0.043, val_acc:0.969]
Epoch [48/120    avg_loss:0.032, val_acc:0.978]
Epoch [49/120    avg_loss:0.029, val_acc:0.982]
Epoch [50/120    avg_loss:0.015, val_acc:0.986]
Epoch [51/120    avg_loss:0.011, val_acc:0.984]
Epoch [52/120    avg_loss:0.015, val_acc:0.977]
Epoch [53/120    avg_loss:0.010, val_acc:0.981]
Epoch [54/120    avg_loss:0.012, val_acc:0.981]
Epoch [55/120    avg_loss:0.008, val_acc:0.981]
Epoch [56/120    avg_loss:0.010, val_acc:0.987]
Epoch [57/120    avg_loss:0.011, val_acc:0.984]
Epoch [58/120    avg_loss:0.010, val_acc:0.982]
Epoch [59/120    avg_loss:0.010, val_acc:0.982]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.006, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.985]
Epoch [63/120    avg_loss:0.018, val_acc:0.984]
Epoch [64/120    avg_loss:0.044, val_acc:0.944]
Epoch [65/120    avg_loss:0.030, val_acc:0.982]
Epoch [66/120    avg_loss:0.040, val_acc:0.977]
Epoch [67/120    avg_loss:0.024, val_acc:0.968]
Epoch [68/120    avg_loss:0.200, val_acc:0.946]
Epoch [69/120    avg_loss:0.094, val_acc:0.955]
Epoch [70/120    avg_loss:0.045, val_acc:0.970]
Epoch [71/120    avg_loss:0.030, val_acc:0.972]
Epoch [72/120    avg_loss:0.025, val_acc:0.977]
Epoch [73/120    avg_loss:0.028, val_acc:0.977]
Epoch [74/120    avg_loss:0.021, val_acc:0.979]
Epoch [75/120    avg_loss:0.025, val_acc:0.982]
Epoch [76/120    avg_loss:0.022, val_acc:0.982]
Epoch [77/120    avg_loss:0.022, val_acc:0.983]
Epoch [78/120    avg_loss:0.020, val_acc:0.982]
Epoch [79/120    avg_loss:0.018, val_acc:0.982]
Epoch [80/120    avg_loss:0.016, val_acc:0.985]
Epoch [81/120    avg_loss:0.018, val_acc:0.983]
Epoch [82/120    avg_loss:0.012, val_acc:0.985]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.018, val_acc:0.986]
Epoch [85/120    avg_loss:0.017, val_acc:0.986]
Epoch [86/120    avg_loss:0.015, val_acc:0.986]
Epoch [87/120    avg_loss:0.020, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.986]
Epoch [89/120    avg_loss:0.015, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.015, val_acc:0.986]
Epoch [93/120    avg_loss:0.017, val_acc:0.985]
Epoch [94/120    avg_loss:0.017, val_acc:0.985]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.016, val_acc:0.985]
Epoch [97/120    avg_loss:0.013, val_acc:0.985]
Epoch [98/120    avg_loss:0.018, val_acc:0.985]
Epoch [99/120    avg_loss:0.016, val_acc:0.985]
Epoch [100/120    avg_loss:0.019, val_acc:0.985]
Epoch [101/120    avg_loss:0.014, val_acc:0.985]
Epoch [102/120    avg_loss:0.017, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.016, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.013, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.985]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.014, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.985]
Epoch [114/120    avg_loss:0.016, val_acc:0.985]
Epoch [115/120    avg_loss:0.013, val_acc:0.985]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.016, val_acc:0.985]
Epoch [118/120    avg_loss:0.015, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6309     0     0     0     0     0    45    76     2]
 [    0     0 18007     0    50     0    27     0     6     0]
 [    0     3     0  1956     0     0     0     0    72     5]
 [    0    61    19     4  2864     0    11     0     5     8]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     3     0     0  4842     0    16     0]
 [    0    26     0     0     0     0     0  1253     0    11]
 [    0    14     0    64    46     0     2     0  3444     1]
 [    0     0     0     0    14    21     0     0     0   884]]

Accuracy:
98.4840816523269

F1 scores:
[       nan 0.98232775 0.99670661 0.96283534 0.9633367  0.99201824
 0.99221311 0.9683153  0.95799722 0.96612022]

Kappa:
0.9799249630536736
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbf6770cac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.003, val_acc:0.525]
Epoch [2/120    avg_loss:1.540, val_acc:0.642]
Epoch [3/120    avg_loss:1.187, val_acc:0.627]
Epoch [4/120    avg_loss:0.895, val_acc:0.709]
Epoch [5/120    avg_loss:0.696, val_acc:0.666]
Epoch [6/120    avg_loss:0.640, val_acc:0.704]
Epoch [7/120    avg_loss:0.472, val_acc:0.828]
Epoch [8/120    avg_loss:0.429, val_acc:0.810]
Epoch [9/120    avg_loss:0.363, val_acc:0.868]
Epoch [10/120    avg_loss:0.361, val_acc:0.832]
Epoch [11/120    avg_loss:0.331, val_acc:0.802]
Epoch [12/120    avg_loss:0.310, val_acc:0.892]
Epoch [13/120    avg_loss:0.221, val_acc:0.916]
Epoch [14/120    avg_loss:0.245, val_acc:0.920]
Epoch [15/120    avg_loss:0.209, val_acc:0.855]
Epoch [16/120    avg_loss:0.167, val_acc:0.908]
Epoch [17/120    avg_loss:0.148, val_acc:0.914]
Epoch [18/120    avg_loss:0.178, val_acc:0.928]
Epoch [19/120    avg_loss:0.149, val_acc:0.901]
Epoch [20/120    avg_loss:0.129, val_acc:0.903]
Epoch [21/120    avg_loss:0.156, val_acc:0.936]
Epoch [22/120    avg_loss:0.113, val_acc:0.938]
Epoch [23/120    avg_loss:0.103, val_acc:0.953]
Epoch [24/120    avg_loss:0.095, val_acc:0.949]
Epoch [25/120    avg_loss:0.085, val_acc:0.947]
Epoch [26/120    avg_loss:0.093, val_acc:0.954]
Epoch [27/120    avg_loss:0.075, val_acc:0.945]
Epoch [28/120    avg_loss:0.063, val_acc:0.961]
Epoch [29/120    avg_loss:0.067, val_acc:0.966]
Epoch [30/120    avg_loss:0.066, val_acc:0.948]
Epoch [31/120    avg_loss:0.092, val_acc:0.940]
Epoch [32/120    avg_loss:0.048, val_acc:0.966]
Epoch [33/120    avg_loss:0.131, val_acc:0.945]
Epoch [34/120    avg_loss:0.056, val_acc:0.965]
Epoch [35/120    avg_loss:0.061, val_acc:0.956]
Epoch [36/120    avg_loss:0.040, val_acc:0.968]
Epoch [37/120    avg_loss:0.065, val_acc:0.930]
Epoch [38/120    avg_loss:0.066, val_acc:0.969]
Epoch [39/120    avg_loss:0.070, val_acc:0.960]
Epoch [40/120    avg_loss:0.041, val_acc:0.968]
Epoch [41/120    avg_loss:0.035, val_acc:0.959]
Epoch [42/120    avg_loss:0.043, val_acc:0.969]
Epoch [43/120    avg_loss:0.023, val_acc:0.973]
Epoch [44/120    avg_loss:0.038, val_acc:0.963]
Epoch [45/120    avg_loss:0.081, val_acc:0.945]
Epoch [46/120    avg_loss:0.056, val_acc:0.970]
Epoch [47/120    avg_loss:0.050, val_acc:0.969]
Epoch [48/120    avg_loss:0.035, val_acc:0.964]
Epoch [49/120    avg_loss:0.064, val_acc:0.940]
Epoch [50/120    avg_loss:0.042, val_acc:0.958]
Epoch [51/120    avg_loss:0.054, val_acc:0.930]
Epoch [52/120    avg_loss:0.053, val_acc:0.964]
Epoch [53/120    avg_loss:0.028, val_acc:0.976]
Epoch [54/120    avg_loss:0.034, val_acc:0.973]
Epoch [55/120    avg_loss:0.036, val_acc:0.965]
Epoch [56/120    avg_loss:0.024, val_acc:0.971]
Epoch [57/120    avg_loss:0.032, val_acc:0.960]
Epoch [58/120    avg_loss:0.017, val_acc:0.968]
Epoch [59/120    avg_loss:0.047, val_acc:0.963]
Epoch [60/120    avg_loss:0.025, val_acc:0.960]
Epoch [61/120    avg_loss:0.060, val_acc:0.946]
Epoch [62/120    avg_loss:0.019, val_acc:0.973]
Epoch [63/120    avg_loss:0.040, val_acc:0.975]
Epoch [64/120    avg_loss:0.017, val_acc:0.974]
Epoch [65/120    avg_loss:0.014, val_acc:0.977]
Epoch [66/120    avg_loss:0.016, val_acc:0.973]
Epoch [67/120    avg_loss:0.015, val_acc:0.975]
Epoch [68/120    avg_loss:0.024, val_acc:0.969]
Epoch [69/120    avg_loss:0.264, val_acc:0.517]
Epoch [70/120    avg_loss:0.999, val_acc:0.632]
Epoch [71/120    avg_loss:0.775, val_acc:0.632]
Epoch [72/120    avg_loss:0.734, val_acc:0.648]
Epoch [73/120    avg_loss:0.599, val_acc:0.743]
Epoch [74/120    avg_loss:0.524, val_acc:0.760]
Epoch [75/120    avg_loss:0.495, val_acc:0.767]
Epoch [76/120    avg_loss:0.480, val_acc:0.796]
Epoch [77/120    avg_loss:0.471, val_acc:0.823]
Epoch [78/120    avg_loss:0.399, val_acc:0.815]
Epoch [79/120    avg_loss:0.358, val_acc:0.825]
Epoch [80/120    avg_loss:0.352, val_acc:0.829]
Epoch [81/120    avg_loss:0.340, val_acc:0.831]
Epoch [82/120    avg_loss:0.342, val_acc:0.846]
Epoch [83/120    avg_loss:0.321, val_acc:0.834]
Epoch [84/120    avg_loss:0.334, val_acc:0.834]
Epoch [85/120    avg_loss:0.334, val_acc:0.834]
Epoch [86/120    avg_loss:0.328, val_acc:0.839]
Epoch [87/120    avg_loss:0.304, val_acc:0.839]
Epoch [88/120    avg_loss:0.333, val_acc:0.840]
Epoch [89/120    avg_loss:0.288, val_acc:0.847]
Epoch [90/120    avg_loss:0.300, val_acc:0.834]
Epoch [91/120    avg_loss:0.313, val_acc:0.827]
Epoch [92/120    avg_loss:0.329, val_acc:0.837]
Epoch [93/120    avg_loss:0.310, val_acc:0.846]
Epoch [94/120    avg_loss:0.303, val_acc:0.847]
Epoch [95/120    avg_loss:0.312, val_acc:0.845]
Epoch [96/120    avg_loss:0.291, val_acc:0.847]
Epoch [97/120    avg_loss:0.288, val_acc:0.848]
Epoch [98/120    avg_loss:0.289, val_acc:0.848]
Epoch [99/120    avg_loss:0.309, val_acc:0.846]
Epoch [100/120    avg_loss:0.286, val_acc:0.846]
Epoch [101/120    avg_loss:0.294, val_acc:0.847]
Epoch [102/120    avg_loss:0.299, val_acc:0.848]
Epoch [103/120    avg_loss:0.298, val_acc:0.844]
Epoch [104/120    avg_loss:0.312, val_acc:0.843]
Epoch [105/120    avg_loss:0.300, val_acc:0.843]
Epoch [106/120    avg_loss:0.289, val_acc:0.844]
Epoch [107/120    avg_loss:0.285, val_acc:0.844]
Epoch [108/120    avg_loss:0.276, val_acc:0.844]
Epoch [109/120    avg_loss:0.305, val_acc:0.844]
Epoch [110/120    avg_loss:0.295, val_acc:0.844]
Epoch [111/120    avg_loss:0.280, val_acc:0.844]
Epoch [112/120    avg_loss:0.279, val_acc:0.845]
Epoch [113/120    avg_loss:0.284, val_acc:0.845]
Epoch [114/120    avg_loss:0.280, val_acc:0.845]
Epoch [115/120    avg_loss:0.292, val_acc:0.845]
Epoch [116/120    avg_loss:0.294, val_acc:0.844]
Epoch [117/120    avg_loss:0.285, val_acc:0.844]
Epoch [118/120    avg_loss:0.296, val_acc:0.844]
Epoch [119/120    avg_loss:0.278, val_acc:0.844]
Epoch [120/120    avg_loss:0.286, val_acc:0.844]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5737    21    25    94    15     0    28   429    83]
 [    0     6 16405     0   374     0  1305     0     0     0]
 [    0    11     0  1708     2     0     0     0   254    61]
 [    0    59   297     0  2538     0    51     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     3   750     0     0     0  4034     0    91     0]
 [    0    83     0     0     0     0     0  1190    17     0]
 [    0   120    32   121    38     0    16     1  3243     0]
 [    0    10     0     0    23    48     0     0     0   838]]

Accuracy:
89.16684742004675

F1 scores:
[       nan 0.92079287 0.92175867 0.8781491  0.84025824 0.97643098
 0.78451964 0.94858509 0.84984277 0.88164124]

Kappa:
0.8574289114272315
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f209f75bb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.015, val_acc:0.491]
Epoch [2/120    avg_loss:1.514, val_acc:0.657]
Epoch [3/120    avg_loss:1.044, val_acc:0.753]
Epoch [4/120    avg_loss:0.744, val_acc:0.768]
Epoch [5/120    avg_loss:0.558, val_acc:0.762]
Epoch [6/120    avg_loss:0.556, val_acc:0.821]
Epoch [7/120    avg_loss:0.440, val_acc:0.848]
Epoch [8/120    avg_loss:0.386, val_acc:0.839]
Epoch [9/120    avg_loss:0.341, val_acc:0.897]
Epoch [10/120    avg_loss:0.331, val_acc:0.832]
Epoch [11/120    avg_loss:0.245, val_acc:0.897]
Epoch [12/120    avg_loss:0.208, val_acc:0.911]
Epoch [13/120    avg_loss:0.218, val_acc:0.930]
Epoch [14/120    avg_loss:0.183, val_acc:0.913]
Epoch [15/120    avg_loss:0.176, val_acc:0.954]
Epoch [16/120    avg_loss:0.191, val_acc:0.941]
Epoch [17/120    avg_loss:0.157, val_acc:0.922]
Epoch [18/120    avg_loss:0.125, val_acc:0.953]
Epoch [19/120    avg_loss:0.129, val_acc:0.935]
Epoch [20/120    avg_loss:0.149, val_acc:0.953]
Epoch [21/120    avg_loss:0.105, val_acc:0.898]
Epoch [22/120    avg_loss:0.098, val_acc:0.963]
Epoch [23/120    avg_loss:0.114, val_acc:0.970]
Epoch [24/120    avg_loss:0.074, val_acc:0.960]
Epoch [25/120    avg_loss:0.102, val_acc:0.895]
Epoch [26/120    avg_loss:0.095, val_acc:0.954]
Epoch [27/120    avg_loss:0.068, val_acc:0.961]
Epoch [28/120    avg_loss:0.103, val_acc:0.947]
Epoch [29/120    avg_loss:0.080, val_acc:0.977]
Epoch [30/120    avg_loss:0.057, val_acc:0.960]
Epoch [31/120    avg_loss:0.051, val_acc:0.965]
Epoch [32/120    avg_loss:0.040, val_acc:0.966]
Epoch [33/120    avg_loss:0.029, val_acc:0.963]
Epoch [34/120    avg_loss:0.048, val_acc:0.982]
Epoch [35/120    avg_loss:0.072, val_acc:0.953]
Epoch [36/120    avg_loss:0.175, val_acc:0.953]
Epoch [37/120    avg_loss:0.140, val_acc:0.932]
Epoch [38/120    avg_loss:0.049, val_acc:0.976]
Epoch [39/120    avg_loss:0.038, val_acc:0.979]
Epoch [40/120    avg_loss:0.030, val_acc:0.978]
Epoch [41/120    avg_loss:0.026, val_acc:0.978]
Epoch [42/120    avg_loss:0.054, val_acc:0.960]
Epoch [43/120    avg_loss:0.046, val_acc:0.968]
Epoch [44/120    avg_loss:0.035, val_acc:0.983]
Epoch [45/120    avg_loss:0.040, val_acc:0.978]
Epoch [46/120    avg_loss:0.016, val_acc:0.982]
Epoch [47/120    avg_loss:0.022, val_acc:0.981]
Epoch [48/120    avg_loss:0.016, val_acc:0.983]
Epoch [49/120    avg_loss:0.019, val_acc:0.980]
Epoch [50/120    avg_loss:0.027, val_acc:0.981]
Epoch [51/120    avg_loss:0.020, val_acc:0.978]
Epoch [52/120    avg_loss:0.019, val_acc:0.988]
Epoch [53/120    avg_loss:0.027, val_acc:0.982]
Epoch [54/120    avg_loss:0.014, val_acc:0.962]
Epoch [55/120    avg_loss:0.016, val_acc:0.973]
Epoch [56/120    avg_loss:0.042, val_acc:0.950]
Epoch [57/120    avg_loss:0.024, val_acc:0.978]
Epoch [58/120    avg_loss:0.023, val_acc:0.982]
Epoch [59/120    avg_loss:0.015, val_acc:0.983]
Epoch [60/120    avg_loss:0.026, val_acc:0.982]
Epoch [61/120    avg_loss:0.012, val_acc:0.983]
Epoch [62/120    avg_loss:0.012, val_acc:0.988]
Epoch [63/120    avg_loss:0.019, val_acc:0.984]
Epoch [64/120    avg_loss:0.030, val_acc:0.985]
Epoch [65/120    avg_loss:0.053, val_acc:0.954]
Epoch [66/120    avg_loss:0.038, val_acc:0.968]
Epoch [67/120    avg_loss:0.031, val_acc:0.987]
Epoch [68/120    avg_loss:0.024, val_acc:0.980]
Epoch [69/120    avg_loss:0.025, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.012, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.006, val_acc:0.988]
Epoch [74/120    avg_loss:0.005, val_acc:0.992]
Epoch [75/120    avg_loss:0.006, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.982]
Epoch [77/120    avg_loss:0.006, val_acc:0.978]
Epoch [78/120    avg_loss:0.005, val_acc:0.989]
Epoch [79/120    avg_loss:0.027, val_acc:0.979]
Epoch [80/120    avg_loss:0.011, val_acc:0.988]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.018, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.990]
Epoch [84/120    avg_loss:0.014, val_acc:0.983]
Epoch [85/120    avg_loss:0.040, val_acc:0.978]
Epoch [86/120    avg_loss:0.048, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.991]
Epoch [89/120    avg_loss:0.009, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.011, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6376     0     0     0     0    19     8    29     0]
 [    0     0 18058     0    16     0    15     0     1     0]
 [    0    10     0  1957     0     0     0     0    68     1]
 [    0    38     5     0  2906     0     8     0    13     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4872     0     2     0]
 [    0    28     0     0     0     0     4  1254     0     4]
 [    0    17     0     9    41     0     5     0  3499     0]
 [    0     0     0     0    14    12     0     0     0   893]]

Accuracy:
99.10105318969465

F1 scores:
[       nan 0.98845051 0.99886606 0.97801099 0.97697092 0.99542334
 0.99418427 0.98275862 0.97424474 0.98185816]

Kappa:
0.9880882077429005
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40c6066b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.010, val_acc:0.321]
Epoch [2/120    avg_loss:1.499, val_acc:0.590]
Epoch [3/120    avg_loss:1.068, val_acc:0.639]
Epoch [4/120    avg_loss:0.703, val_acc:0.681]
Epoch [5/120    avg_loss:0.530, val_acc:0.771]
Epoch [6/120    avg_loss:0.476, val_acc:0.821]
Epoch [7/120    avg_loss:0.408, val_acc:0.794]
Epoch [8/120    avg_loss:0.355, val_acc:0.825]
Epoch [9/120    avg_loss:0.323, val_acc:0.858]
Epoch [10/120    avg_loss:0.301, val_acc:0.887]
Epoch [11/120    avg_loss:0.245, val_acc:0.887]
Epoch [12/120    avg_loss:0.228, val_acc:0.896]
Epoch [13/120    avg_loss:0.220, val_acc:0.905]
Epoch [14/120    avg_loss:0.202, val_acc:0.903]
Epoch [15/120    avg_loss:0.200, val_acc:0.874]
Epoch [16/120    avg_loss:0.175, val_acc:0.871]
Epoch [17/120    avg_loss:0.171, val_acc:0.931]
Epoch [18/120    avg_loss:0.141, val_acc:0.898]
Epoch [19/120    avg_loss:0.183, val_acc:0.900]
Epoch [20/120    avg_loss:0.134, val_acc:0.953]
Epoch [21/120    avg_loss:0.098, val_acc:0.936]
Epoch [22/120    avg_loss:0.111, val_acc:0.947]
Epoch [23/120    avg_loss:0.088, val_acc:0.909]
Epoch [24/120    avg_loss:0.107, val_acc:0.954]
Epoch [25/120    avg_loss:0.110, val_acc:0.928]
Epoch [26/120    avg_loss:0.123, val_acc:0.922]
Epoch [27/120    avg_loss:0.089, val_acc:0.913]
Epoch [28/120    avg_loss:0.064, val_acc:0.963]
Epoch [29/120    avg_loss:0.070, val_acc:0.965]
Epoch [30/120    avg_loss:0.052, val_acc:0.950]
Epoch [31/120    avg_loss:0.073, val_acc:0.947]
Epoch [32/120    avg_loss:0.050, val_acc:0.970]
Epoch [33/120    avg_loss:0.066, val_acc:0.947]
Epoch [34/120    avg_loss:0.066, val_acc:0.941]
Epoch [35/120    avg_loss:0.067, val_acc:0.938]
Epoch [36/120    avg_loss:0.074, val_acc:0.961]
Epoch [37/120    avg_loss:0.051, val_acc:0.901]
Epoch [38/120    avg_loss:0.053, val_acc:0.914]
Epoch [39/120    avg_loss:0.153, val_acc:0.933]
Epoch [40/120    avg_loss:0.085, val_acc:0.951]
Epoch [41/120    avg_loss:0.055, val_acc:0.974]
Epoch [42/120    avg_loss:0.057, val_acc:0.961]
Epoch [43/120    avg_loss:0.055, val_acc:0.958]
Epoch [44/120    avg_loss:0.088, val_acc:0.918]
Epoch [45/120    avg_loss:0.045, val_acc:0.956]
Epoch [46/120    avg_loss:0.064, val_acc:0.947]
Epoch [47/120    avg_loss:0.047, val_acc:0.959]
Epoch [48/120    avg_loss:0.027, val_acc:0.974]
Epoch [49/120    avg_loss:0.031, val_acc:0.982]
Epoch [50/120    avg_loss:0.077, val_acc:0.961]
Epoch [51/120    avg_loss:0.063, val_acc:0.941]
Epoch [52/120    avg_loss:0.070, val_acc:0.972]
Epoch [53/120    avg_loss:0.042, val_acc:0.968]
Epoch [54/120    avg_loss:0.024, val_acc:0.981]
Epoch [55/120    avg_loss:0.020, val_acc:0.982]
Epoch [56/120    avg_loss:0.023, val_acc:0.967]
Epoch [57/120    avg_loss:0.018, val_acc:0.983]
Epoch [58/120    avg_loss:0.015, val_acc:0.985]
Epoch [59/120    avg_loss:0.019, val_acc:0.979]
Epoch [60/120    avg_loss:0.024, val_acc:0.971]
Epoch [61/120    avg_loss:0.023, val_acc:0.978]
Epoch [62/120    avg_loss:0.028, val_acc:0.980]
Epoch [63/120    avg_loss:0.024, val_acc:0.984]
Epoch [64/120    avg_loss:0.016, val_acc:0.985]
Epoch [65/120    avg_loss:0.022, val_acc:0.985]
Epoch [66/120    avg_loss:0.043, val_acc:0.960]
Epoch [67/120    avg_loss:0.021, val_acc:0.986]
Epoch [68/120    avg_loss:0.016, val_acc:0.982]
Epoch [69/120    avg_loss:0.014, val_acc:0.978]
Epoch [70/120    avg_loss:0.019, val_acc:0.984]
Epoch [71/120    avg_loss:0.014, val_acc:0.979]
Epoch [72/120    avg_loss:0.024, val_acc:0.928]
Epoch [73/120    avg_loss:0.015, val_acc:0.976]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.013, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.982]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.005, val_acc:0.984]
Epoch [82/120    avg_loss:0.088, val_acc:0.906]
Epoch [83/120    avg_loss:0.077, val_acc:0.970]
Epoch [84/120    avg_loss:0.058, val_acc:0.923]
Epoch [85/120    avg_loss:0.051, val_acc:0.974]
Epoch [86/120    avg_loss:0.021, val_acc:0.981]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.017, val_acc:0.951]
Epoch [90/120    avg_loss:0.015, val_acc:0.984]
Epoch [91/120    avg_loss:0.011, val_acc:0.982]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.009, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     2     0     0     0     0    57    16]
 [    0     0 18000     0    50     0    37     0     3     0]
 [    0    12     0  1907     0     0     0     0   117     0]
 [    0     3     8     0  2952     0     3     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   117     1     8     0  4748     0     4     0]
 [    0    40     0     0     0     0     1  1246     2     1]
 [    0     8     0    22    28     0    17     0  3496     0]
 [    0     0     0     0     0    36     0     0     0   883]]

Accuracy:
98.55638300436219

F1 scores:
[       nan 0.98926237 0.99406323 0.96118952 0.98236273 0.98639456
 0.98058653 0.98264984 0.96361632 0.97086311]

Kappa:
0.9808628220207076
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4917cbac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.450]
Epoch [2/120    avg_loss:1.553, val_acc:0.661]
Epoch [3/120    avg_loss:1.086, val_acc:0.698]
Epoch [4/120    avg_loss:0.784, val_acc:0.711]
Epoch [5/120    avg_loss:0.622, val_acc:0.690]
Epoch [6/120    avg_loss:0.545, val_acc:0.716]
Epoch [7/120    avg_loss:0.476, val_acc:0.820]
Epoch [8/120    avg_loss:0.465, val_acc:0.781]
Epoch [9/120    avg_loss:0.391, val_acc:0.706]
Epoch [10/120    avg_loss:0.345, val_acc:0.849]
Epoch [11/120    avg_loss:0.342, val_acc:0.830]
Epoch [12/120    avg_loss:0.262, val_acc:0.865]
Epoch [13/120    avg_loss:0.272, val_acc:0.871]
Epoch [14/120    avg_loss:0.213, val_acc:0.891]
Epoch [15/120    avg_loss:0.217, val_acc:0.911]
Epoch [16/120    avg_loss:0.205, val_acc:0.911]
Epoch [17/120    avg_loss:0.204, val_acc:0.901]
Epoch [18/120    avg_loss:0.203, val_acc:0.916]
Epoch [19/120    avg_loss:0.136, val_acc:0.922]
Epoch [20/120    avg_loss:0.133, val_acc:0.952]
Epoch [21/120    avg_loss:0.111, val_acc:0.944]
Epoch [22/120    avg_loss:0.105, val_acc:0.941]
Epoch [23/120    avg_loss:0.110, val_acc:0.945]
Epoch [24/120    avg_loss:0.172, val_acc:0.897]
Epoch [25/120    avg_loss:0.103, val_acc:0.956]
Epoch [26/120    avg_loss:0.107, val_acc:0.947]
Epoch [27/120    avg_loss:0.096, val_acc:0.949]
Epoch [28/120    avg_loss:0.078, val_acc:0.955]
Epoch [29/120    avg_loss:0.139, val_acc:0.935]
Epoch [30/120    avg_loss:0.110, val_acc:0.955]
Epoch [31/120    avg_loss:0.082, val_acc:0.947]
Epoch [32/120    avg_loss:0.062, val_acc:0.970]
Epoch [33/120    avg_loss:0.050, val_acc:0.965]
Epoch [34/120    avg_loss:0.055, val_acc:0.957]
Epoch [35/120    avg_loss:0.061, val_acc:0.948]
Epoch [36/120    avg_loss:0.035, val_acc:0.962]
Epoch [37/120    avg_loss:0.061, val_acc:0.917]
Epoch [38/120    avg_loss:0.037, val_acc:0.974]
Epoch [39/120    avg_loss:0.032, val_acc:0.968]
Epoch [40/120    avg_loss:0.029, val_acc:0.970]
Epoch [41/120    avg_loss:0.026, val_acc:0.983]
Epoch [42/120    avg_loss:0.039, val_acc:0.969]
Epoch [43/120    avg_loss:0.108, val_acc:0.956]
Epoch [44/120    avg_loss:0.036, val_acc:0.975]
Epoch [45/120    avg_loss:0.029, val_acc:0.972]
Epoch [46/120    avg_loss:0.029, val_acc:0.930]
Epoch [47/120    avg_loss:0.041, val_acc:0.975]
Epoch [48/120    avg_loss:0.030, val_acc:0.954]
Epoch [49/120    avg_loss:0.032, val_acc:0.972]
Epoch [50/120    avg_loss:0.109, val_acc:0.963]
Epoch [51/120    avg_loss:0.035, val_acc:0.973]
Epoch [52/120    avg_loss:0.017, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.981]
Epoch [54/120    avg_loss:0.016, val_acc:0.979]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.010, val_acc:0.984]
Epoch [57/120    avg_loss:0.012, val_acc:0.983]
Epoch [58/120    avg_loss:0.009, val_acc:0.984]
Epoch [59/120    avg_loss:0.013, val_acc:0.983]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.009, val_acc:0.984]
Epoch [62/120    avg_loss:0.009, val_acc:0.983]
Epoch [63/120    avg_loss:0.010, val_acc:0.983]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.008, val_acc:0.985]
Epoch [66/120    avg_loss:0.009, val_acc:0.989]
Epoch [67/120    avg_loss:0.007, val_acc:0.989]
Epoch [68/120    avg_loss:0.011, val_acc:0.988]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.988]
Epoch [74/120    avg_loss:0.006, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.989]
Epoch [81/120    avg_loss:0.011, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6301     0     0     1     0     7    84    38     1]
 [    0     4 18013     0    31     0    41     0     1     0]
 [    0     8     0  2001     0     0     0     0    24     3]
 [    0    45    12     1  2905     0     4     0     4     1]
 [    0     0     0     7     0  1298     0     0     0     0]
 [    0     0    18     0     2     0  4858     0     0     0]
 [    0     7     0     0     0     0     0  1282     0     1]
 [    0    11     2    31    45     0     0     0  3478     4]
 [    0     0     0     4    14    13     0     0     0   888]]

Accuracy:
98.86968886318175

F1 scores:
[       nan 0.9839163  0.99698353 0.98088235 0.97319933 0.99235474
 0.99264405 0.96536145 0.97751546 0.97743533]

Kappa:
0.9850334171189857
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f57e1097b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.104, val_acc:0.337]
Epoch [2/120    avg_loss:1.596, val_acc:0.490]
Epoch [3/120    avg_loss:1.147, val_acc:0.526]
Epoch [4/120    avg_loss:0.870, val_acc:0.711]
Epoch [5/120    avg_loss:0.711, val_acc:0.780]
Epoch [6/120    avg_loss:0.573, val_acc:0.771]
Epoch [7/120    avg_loss:0.530, val_acc:0.798]
Epoch [8/120    avg_loss:0.443, val_acc:0.807]
Epoch [9/120    avg_loss:0.368, val_acc:0.851]
Epoch [10/120    avg_loss:0.302, val_acc:0.799]
Epoch [11/120    avg_loss:0.301, val_acc:0.891]
Epoch [12/120    avg_loss:0.268, val_acc:0.894]
Epoch [13/120    avg_loss:0.206, val_acc:0.918]
Epoch [14/120    avg_loss:0.339, val_acc:0.792]
Epoch [15/120    avg_loss:0.318, val_acc:0.908]
Epoch [16/120    avg_loss:0.222, val_acc:0.844]
Epoch [17/120    avg_loss:0.205, val_acc:0.922]
Epoch [18/120    avg_loss:0.168, val_acc:0.850]
Epoch [19/120    avg_loss:0.139, val_acc:0.940]
Epoch [20/120    avg_loss:0.168, val_acc:0.920]
Epoch [21/120    avg_loss:0.159, val_acc:0.941]
Epoch [22/120    avg_loss:0.140, val_acc:0.950]
Epoch [23/120    avg_loss:0.103, val_acc:0.963]
Epoch [24/120    avg_loss:0.120, val_acc:0.960]
Epoch [25/120    avg_loss:0.114, val_acc:0.967]
Epoch [26/120    avg_loss:0.097, val_acc:0.963]
Epoch [27/120    avg_loss:0.100, val_acc:0.946]
Epoch [28/120    avg_loss:0.126, val_acc:0.963]
Epoch [29/120    avg_loss:0.087, val_acc:0.955]
Epoch [30/120    avg_loss:0.068, val_acc:0.961]
Epoch [31/120    avg_loss:0.081, val_acc:0.966]
Epoch [32/120    avg_loss:0.064, val_acc:0.967]
Epoch [33/120    avg_loss:0.046, val_acc:0.974]
Epoch [34/120    avg_loss:0.061, val_acc:0.916]
Epoch [35/120    avg_loss:0.059, val_acc:0.945]
Epoch [36/120    avg_loss:0.083, val_acc:0.971]
Epoch [37/120    avg_loss:0.082, val_acc:0.973]
Epoch [38/120    avg_loss:0.092, val_acc:0.976]
Epoch [39/120    avg_loss:0.066, val_acc:0.973]
Epoch [40/120    avg_loss:0.062, val_acc:0.952]
Epoch [41/120    avg_loss:0.038, val_acc:0.977]
Epoch [42/120    avg_loss:0.039, val_acc:0.975]
Epoch [43/120    avg_loss:0.029, val_acc:0.983]
Epoch [44/120    avg_loss:0.022, val_acc:0.979]
Epoch [45/120    avg_loss:0.029, val_acc:0.946]
Epoch [46/120    avg_loss:0.076, val_acc:0.970]
Epoch [47/120    avg_loss:0.049, val_acc:0.974]
Epoch [48/120    avg_loss:0.039, val_acc:0.978]
Epoch [49/120    avg_loss:0.034, val_acc:0.976]
Epoch [50/120    avg_loss:0.024, val_acc:0.979]
Epoch [51/120    avg_loss:0.027, val_acc:0.970]
Epoch [52/120    avg_loss:0.018, val_acc:0.983]
Epoch [53/120    avg_loss:0.019, val_acc:0.978]
Epoch [54/120    avg_loss:0.036, val_acc:0.973]
Epoch [55/120    avg_loss:0.033, val_acc:0.979]
Epoch [56/120    avg_loss:0.020, val_acc:0.988]
Epoch [57/120    avg_loss:0.018, val_acc:0.979]
Epoch [58/120    avg_loss:0.009, val_acc:0.986]
Epoch [59/120    avg_loss:0.010, val_acc:0.987]
Epoch [60/120    avg_loss:0.016, val_acc:0.981]
Epoch [61/120    avg_loss:0.028, val_acc:0.976]
Epoch [62/120    avg_loss:0.028, val_acc:0.975]
Epoch [63/120    avg_loss:0.023, val_acc:0.974]
Epoch [64/120    avg_loss:0.016, val_acc:0.983]
Epoch [65/120    avg_loss:0.016, val_acc:0.979]
Epoch [66/120    avg_loss:0.074, val_acc:0.950]
Epoch [67/120    avg_loss:0.034, val_acc:0.980]
Epoch [68/120    avg_loss:0.015, val_acc:0.983]
Epoch [69/120    avg_loss:0.016, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.987]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.008, val_acc:0.989]
Epoch [74/120    avg_loss:0.010, val_acc:0.990]
Epoch [75/120    avg_loss:0.013, val_acc:0.990]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.989]
Epoch [79/120    avg_loss:0.010, val_acc:0.991]
Epoch [80/120    avg_loss:0.007, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.006, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.006, val_acc:0.991]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0     0     0     0     0    20    10    19]
 [    0     0 18028     0    29     0    33     0     0     0]
 [    0     7     0  1962     0     0     1     0    60     6]
 [    0    45     1     0  2923     0     0     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     6     0     0     0     0     0  1282     0     2]
 [    0    65     0    27    65     0     0     0  3414     0]
 [    0     0     0     0    14    12     0     0     0   893]]

Accuracy:
98.97091075603115

F1 scores:
[       nan 0.98670583 0.99820049 0.97490683 0.97384641 0.99542334
 0.99632203 0.98919753 0.96741286 0.97117999]

Kappa:
0.9863704862298768
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f088894fc50>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.060, val_acc:0.343]
Epoch [2/120    avg_loss:1.572, val_acc:0.423]
Epoch [3/120    avg_loss:1.084, val_acc:0.709]
Epoch [4/120    avg_loss:0.810, val_acc:0.742]
Epoch [5/120    avg_loss:0.608, val_acc:0.754]
Epoch [6/120    avg_loss:0.520, val_acc:0.782]
Epoch [7/120    avg_loss:0.464, val_acc:0.797]
Epoch [8/120    avg_loss:0.366, val_acc:0.878]
Epoch [9/120    avg_loss:0.365, val_acc:0.909]
Epoch [10/120    avg_loss:0.360, val_acc:0.841]
Epoch [11/120    avg_loss:0.254, val_acc:0.910]
Epoch [12/120    avg_loss:0.292, val_acc:0.905]
Epoch [13/120    avg_loss:0.209, val_acc:0.909]
Epoch [14/120    avg_loss:0.215, val_acc:0.909]
Epoch [15/120    avg_loss:0.159, val_acc:0.878]
Epoch [16/120    avg_loss:0.139, val_acc:0.910]
Epoch [17/120    avg_loss:0.169, val_acc:0.929]
Epoch [18/120    avg_loss:0.189, val_acc:0.940]
Epoch [19/120    avg_loss:0.132, val_acc:0.939]
Epoch [20/120    avg_loss:0.122, val_acc:0.947]
Epoch [21/120    avg_loss:0.133, val_acc:0.940]
Epoch [22/120    avg_loss:0.112, val_acc:0.940]
Epoch [23/120    avg_loss:0.107, val_acc:0.942]
Epoch [24/120    avg_loss:0.083, val_acc:0.957]
Epoch [25/120    avg_loss:0.085, val_acc:0.965]
Epoch [26/120    avg_loss:0.114, val_acc:0.952]
Epoch [27/120    avg_loss:0.116, val_acc:0.925]
Epoch [28/120    avg_loss:0.088, val_acc:0.952]
Epoch [29/120    avg_loss:0.066, val_acc:0.973]
Epoch [30/120    avg_loss:0.049, val_acc:0.957]
Epoch [31/120    avg_loss:0.058, val_acc:0.971]
Epoch [32/120    avg_loss:0.053, val_acc:0.974]
Epoch [33/120    avg_loss:0.049, val_acc:0.974]
Epoch [34/120    avg_loss:0.082, val_acc:0.957]
Epoch [35/120    avg_loss:0.253, val_acc:0.952]
Epoch [36/120    avg_loss:0.121, val_acc:0.959]
Epoch [37/120    avg_loss:0.090, val_acc:0.966]
Epoch [38/120    avg_loss:0.085, val_acc:0.969]
Epoch [39/120    avg_loss:0.083, val_acc:0.930]
Epoch [40/120    avg_loss:0.043, val_acc:0.978]
Epoch [41/120    avg_loss:0.029, val_acc:0.974]
Epoch [42/120    avg_loss:0.021, val_acc:0.979]
Epoch [43/120    avg_loss:0.023, val_acc:0.979]
Epoch [44/120    avg_loss:0.023, val_acc:0.971]
Epoch [45/120    avg_loss:0.062, val_acc:0.942]
Epoch [46/120    avg_loss:0.094, val_acc:0.974]
Epoch [47/120    avg_loss:0.036, val_acc:0.972]
Epoch [48/120    avg_loss:0.023, val_acc:0.980]
Epoch [49/120    avg_loss:0.025, val_acc:0.980]
Epoch [50/120    avg_loss:0.044, val_acc:0.965]
Epoch [51/120    avg_loss:0.026, val_acc:0.983]
Epoch [52/120    avg_loss:0.021, val_acc:0.895]
Epoch [53/120    avg_loss:0.025, val_acc:0.954]
Epoch [54/120    avg_loss:0.028, val_acc:0.983]
Epoch [55/120    avg_loss:0.028, val_acc:0.980]
Epoch [56/120    avg_loss:0.025, val_acc:0.980]
Epoch [57/120    avg_loss:0.012, val_acc:0.985]
Epoch [58/120    avg_loss:0.023, val_acc:0.968]
Epoch [59/120    avg_loss:0.016, val_acc:0.978]
Epoch [60/120    avg_loss:0.010, val_acc:0.980]
Epoch [61/120    avg_loss:0.007, val_acc:0.986]
Epoch [62/120    avg_loss:0.006, val_acc:0.989]
Epoch [63/120    avg_loss:0.007, val_acc:0.981]
Epoch [64/120    avg_loss:0.008, val_acc:0.983]
Epoch [65/120    avg_loss:0.005, val_acc:0.978]
Epoch [66/120    avg_loss:0.006, val_acc:0.989]
Epoch [67/120    avg_loss:0.004, val_acc:0.986]
Epoch [68/120    avg_loss:0.005, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.987]
Epoch [70/120    avg_loss:0.007, val_acc:0.989]
Epoch [71/120    avg_loss:0.007, val_acc:0.982]
Epoch [72/120    avg_loss:0.009, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.987]
Epoch [74/120    avg_loss:0.005, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.982]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.030, val_acc:0.973]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.983]
Epoch [86/120    avg_loss:0.003, val_acc:0.983]
Epoch [87/120    avg_loss:0.003, val_acc:0.984]
Epoch [88/120    avg_loss:0.004, val_acc:0.985]
Epoch [89/120    avg_loss:0.004, val_acc:0.985]
Epoch [90/120    avg_loss:0.003, val_acc:0.985]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.003, val_acc:0.985]
Epoch [93/120    avg_loss:0.003, val_acc:0.985]
Epoch [94/120    avg_loss:0.003, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.985]
Epoch [99/120    avg_loss:0.003, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.003, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.003, val_acc:0.985]
Epoch [109/120    avg_loss:0.003, val_acc:0.985]
Epoch [110/120    avg_loss:0.003, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.002, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6312     0     0     1     0     1     0   113     5]
 [    0     0 17973     0    92     0    10     0    15     0]
 [    0     3     0  1954     0     0     0     0    75     4]
 [    0     8     0     0  2950     0     3     0     8     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     4     0  4867     0     1     0]
 [    0     0     0     0     0     1     0  1288     0     1]
 [    0     5     0    35    60     0     1     0  3465     5]
 [    0     0     0     0     0    53     0     0     0   866]]

Accuracy:
98.76364688019666

F1 scores:
[       nan 0.98934169 0.99658987 0.97093168 0.97055437 0.97972973
 0.99733607 0.9992242  0.95612583 0.96062119]

Kappa:
0.9836419685898004
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2c3f3db38>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:1.969, val_acc:0.335]
Epoch [2/120    avg_loss:1.505, val_acc:0.634]
Epoch [3/120    avg_loss:1.028, val_acc:0.739]
Epoch [4/120    avg_loss:0.755, val_acc:0.751]
Epoch [5/120    avg_loss:0.603, val_acc:0.770]
Epoch [6/120    avg_loss:0.465, val_acc:0.767]
Epoch [7/120    avg_loss:0.441, val_acc:0.905]
Epoch [8/120    avg_loss:0.398, val_acc:0.892]
Epoch [9/120    avg_loss:0.325, val_acc:0.884]
Epoch [10/120    avg_loss:0.288, val_acc:0.931]
Epoch [11/120    avg_loss:0.348, val_acc:0.929]
Epoch [12/120    avg_loss:0.269, val_acc:0.845]
Epoch [13/120    avg_loss:0.237, val_acc:0.938]
Epoch [14/120    avg_loss:0.180, val_acc:0.929]
Epoch [15/120    avg_loss:0.142, val_acc:0.942]
Epoch [16/120    avg_loss:0.207, val_acc:0.941]
Epoch [17/120    avg_loss:0.128, val_acc:0.963]
Epoch [18/120    avg_loss:0.134, val_acc:0.961]
Epoch [19/120    avg_loss:0.098, val_acc:0.916]
Epoch [20/120    avg_loss:0.151, val_acc:0.962]
Epoch [21/120    avg_loss:0.093, val_acc:0.880]
Epoch [22/120    avg_loss:0.139, val_acc:0.917]
Epoch [23/120    avg_loss:0.161, val_acc:0.966]
Epoch [24/120    avg_loss:0.273, val_acc:0.251]
Epoch [25/120    avg_loss:0.965, val_acc:0.726]
Epoch [26/120    avg_loss:0.499, val_acc:0.779]
Epoch [27/120    avg_loss:0.433, val_acc:0.753]
Epoch [28/120    avg_loss:0.390, val_acc:0.858]
Epoch [29/120    avg_loss:0.388, val_acc:0.837]
Epoch [30/120    avg_loss:0.382, val_acc:0.870]
Epoch [31/120    avg_loss:0.282, val_acc:0.877]
Epoch [32/120    avg_loss:0.242, val_acc:0.876]
Epoch [33/120    avg_loss:0.251, val_acc:0.887]
Epoch [34/120    avg_loss:0.212, val_acc:0.890]
Epoch [35/120    avg_loss:0.196, val_acc:0.934]
Epoch [36/120    avg_loss:0.177, val_acc:0.951]
Epoch [37/120    avg_loss:0.121, val_acc:0.941]
Epoch [38/120    avg_loss:0.119, val_acc:0.943]
Epoch [39/120    avg_loss:0.110, val_acc:0.950]
Epoch [40/120    avg_loss:0.108, val_acc:0.945]
Epoch [41/120    avg_loss:0.098, val_acc:0.954]
Epoch [42/120    avg_loss:0.099, val_acc:0.950]
Epoch [43/120    avg_loss:0.087, val_acc:0.959]
Epoch [44/120    avg_loss:0.089, val_acc:0.953]
Epoch [45/120    avg_loss:0.085, val_acc:0.959]
Epoch [46/120    avg_loss:0.087, val_acc:0.954]
Epoch [47/120    avg_loss:0.076, val_acc:0.955]
Epoch [48/120    avg_loss:0.083, val_acc:0.957]
Epoch [49/120    avg_loss:0.081, val_acc:0.959]
Epoch [50/120    avg_loss:0.068, val_acc:0.960]
Epoch [51/120    avg_loss:0.076, val_acc:0.960]
Epoch [52/120    avg_loss:0.092, val_acc:0.959]
Epoch [53/120    avg_loss:0.079, val_acc:0.960]
Epoch [54/120    avg_loss:0.086, val_acc:0.959]
Epoch [55/120    avg_loss:0.086, val_acc:0.961]
Epoch [56/120    avg_loss:0.083, val_acc:0.961]
Epoch [57/120    avg_loss:0.074, val_acc:0.960]
Epoch [58/120    avg_loss:0.084, val_acc:0.962]
Epoch [59/120    avg_loss:0.074, val_acc:0.962]
Epoch [60/120    avg_loss:0.077, val_acc:0.963]
Epoch [61/120    avg_loss:0.082, val_acc:0.961]
Epoch [62/120    avg_loss:0.079, val_acc:0.962]
Epoch [63/120    avg_loss:0.075, val_acc:0.962]
Epoch [64/120    avg_loss:0.070, val_acc:0.962]
Epoch [65/120    avg_loss:0.078, val_acc:0.962]
Epoch [66/120    avg_loss:0.071, val_acc:0.962]
Epoch [67/120    avg_loss:0.072, val_acc:0.963]
Epoch [68/120    avg_loss:0.072, val_acc:0.963]
Epoch [69/120    avg_loss:0.076, val_acc:0.962]
Epoch [70/120    avg_loss:0.086, val_acc:0.962]
Epoch [71/120    avg_loss:0.075, val_acc:0.963]
Epoch [72/120    avg_loss:0.084, val_acc:0.963]
Epoch [73/120    avg_loss:0.077, val_acc:0.963]
Epoch [74/120    avg_loss:0.073, val_acc:0.963]
Epoch [75/120    avg_loss:0.072, val_acc:0.962]
Epoch [76/120    avg_loss:0.070, val_acc:0.962]
Epoch [77/120    avg_loss:0.076, val_acc:0.962]
Epoch [78/120    avg_loss:0.084, val_acc:0.962]
Epoch [79/120    avg_loss:0.082, val_acc:0.962]
Epoch [80/120    avg_loss:0.085, val_acc:0.962]
Epoch [81/120    avg_loss:0.075, val_acc:0.962]
Epoch [82/120    avg_loss:0.068, val_acc:0.962]
Epoch [83/120    avg_loss:0.096, val_acc:0.963]
Epoch [84/120    avg_loss:0.076, val_acc:0.963]
Epoch [85/120    avg_loss:0.083, val_acc:0.963]
Epoch [86/120    avg_loss:0.081, val_acc:0.963]
Epoch [87/120    avg_loss:0.072, val_acc:0.963]
Epoch [88/120    avg_loss:0.084, val_acc:0.963]
Epoch [89/120    avg_loss:0.074, val_acc:0.963]
Epoch [90/120    avg_loss:0.068, val_acc:0.963]
Epoch [91/120    avg_loss:0.078, val_acc:0.963]
Epoch [92/120    avg_loss:0.086, val_acc:0.963]
Epoch [93/120    avg_loss:0.076, val_acc:0.963]
Epoch [94/120    avg_loss:0.074, val_acc:0.963]
Epoch [95/120    avg_loss:0.070, val_acc:0.963]
Epoch [96/120    avg_loss:0.091, val_acc:0.963]
Epoch [97/120    avg_loss:0.077, val_acc:0.963]
Epoch [98/120    avg_loss:0.071, val_acc:0.963]
Epoch [99/120    avg_loss:0.077, val_acc:0.963]
Epoch [100/120    avg_loss:0.079, val_acc:0.963]
Epoch [101/120    avg_loss:0.078, val_acc:0.963]
Epoch [102/120    avg_loss:0.081, val_acc:0.963]
Epoch [103/120    avg_loss:0.070, val_acc:0.963]
Epoch [104/120    avg_loss:0.074, val_acc:0.963]
Epoch [105/120    avg_loss:0.079, val_acc:0.963]
Epoch [106/120    avg_loss:0.068, val_acc:0.963]
Epoch [107/120    avg_loss:0.076, val_acc:0.963]
Epoch [108/120    avg_loss:0.077, val_acc:0.963]
Epoch [109/120    avg_loss:0.075, val_acc:0.963]
Epoch [110/120    avg_loss:0.077, val_acc:0.963]
Epoch [111/120    avg_loss:0.084, val_acc:0.963]
Epoch [112/120    avg_loss:0.075, val_acc:0.963]
Epoch [113/120    avg_loss:0.073, val_acc:0.963]
Epoch [114/120    avg_loss:0.076, val_acc:0.963]
Epoch [115/120    avg_loss:0.072, val_acc:0.963]
Epoch [116/120    avg_loss:0.071, val_acc:0.963]
Epoch [117/120    avg_loss:0.074, val_acc:0.963]
Epoch [118/120    avg_loss:0.081, val_acc:0.963]
Epoch [119/120    avg_loss:0.072, val_acc:0.963]
Epoch [120/120    avg_loss:0.075, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6345     0     1     3     0     6     7    54    16]
 [    0     0 17420     0   250     0   420     0     0     0]
 [    0     8     0  1845     0     0     0     0   174     9]
 [    0    49    14     0  2894     0    11     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2     9     0     0     0  4731     0   136     0]
 [    0    25     0     0     0     0     0  1262     1     2]
 [    0    72     3    87    77     0     0     0  3332     0]
 [    0     2     0     0    14    45     0     0     0   858]]

Accuracy:
96.382522353168

F1 scores:
[       nan 0.98105914 0.98041423 0.92970522 0.93204509 0.98305085
 0.94186741 0.98632278 0.91639164 0.95121951]

Kappa:
0.9523862669569128
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91ce669b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.106, val_acc:0.539]
Epoch [2/120    avg_loss:1.646, val_acc:0.639]
Epoch [3/120    avg_loss:1.223, val_acc:0.702]
Epoch [4/120    avg_loss:0.985, val_acc:0.739]
Epoch [5/120    avg_loss:0.716, val_acc:0.798]
Epoch [6/120    avg_loss:0.597, val_acc:0.833]
Epoch [7/120    avg_loss:0.540, val_acc:0.751]
Epoch [8/120    avg_loss:0.396, val_acc:0.720]
Epoch [9/120    avg_loss:0.341, val_acc:0.846]
Epoch [10/120    avg_loss:0.294, val_acc:0.904]
Epoch [11/120    avg_loss:0.268, val_acc:0.929]
Epoch [12/120    avg_loss:0.241, val_acc:0.888]
Epoch [13/120    avg_loss:0.243, val_acc:0.884]
Epoch [14/120    avg_loss:0.257, val_acc:0.895]
Epoch [15/120    avg_loss:0.164, val_acc:0.911]
Epoch [16/120    avg_loss:0.136, val_acc:0.940]
Epoch [17/120    avg_loss:0.142, val_acc:0.916]
Epoch [18/120    avg_loss:0.164, val_acc:0.953]
Epoch [19/120    avg_loss:0.088, val_acc:0.967]
Epoch [20/120    avg_loss:0.094, val_acc:0.941]
Epoch [21/120    avg_loss:0.171, val_acc:0.934]
Epoch [22/120    avg_loss:0.155, val_acc:0.879]
Epoch [23/120    avg_loss:0.165, val_acc:0.934]
Epoch [24/120    avg_loss:0.116, val_acc:0.911]
Epoch [25/120    avg_loss:0.066, val_acc:0.931]
Epoch [26/120    avg_loss:0.305, val_acc:0.921]
Epoch [27/120    avg_loss:0.137, val_acc:0.952]
Epoch [28/120    avg_loss:0.073, val_acc:0.968]
Epoch [29/120    avg_loss:0.069, val_acc:0.956]
Epoch [30/120    avg_loss:0.069, val_acc:0.963]
Epoch [31/120    avg_loss:0.068, val_acc:0.932]
Epoch [32/120    avg_loss:0.059, val_acc:0.975]
Epoch [33/120    avg_loss:0.041, val_acc:0.979]
Epoch [34/120    avg_loss:0.032, val_acc:0.982]
Epoch [35/120    avg_loss:0.047, val_acc:0.971]
Epoch [36/120    avg_loss:0.030, val_acc:0.981]
Epoch [37/120    avg_loss:0.041, val_acc:0.970]
Epoch [38/120    avg_loss:0.036, val_acc:0.977]
Epoch [39/120    avg_loss:0.023, val_acc:0.972]
Epoch [40/120    avg_loss:0.029, val_acc:0.971]
Epoch [41/120    avg_loss:0.030, val_acc:0.982]
Epoch [42/120    avg_loss:0.014, val_acc:0.984]
Epoch [43/120    avg_loss:0.021, val_acc:0.967]
Epoch [44/120    avg_loss:0.018, val_acc:0.930]
Epoch [45/120    avg_loss:0.018, val_acc:0.971]
Epoch [46/120    avg_loss:0.029, val_acc:0.980]
Epoch [47/120    avg_loss:0.017, val_acc:0.986]
Epoch [48/120    avg_loss:0.013, val_acc:0.979]
Epoch [49/120    avg_loss:0.018, val_acc:0.984]
Epoch [50/120    avg_loss:0.015, val_acc:0.985]
Epoch [51/120    avg_loss:0.015, val_acc:0.983]
Epoch [52/120    avg_loss:0.008, val_acc:0.986]
Epoch [53/120    avg_loss:0.006, val_acc:0.985]
Epoch [54/120    avg_loss:0.014, val_acc:0.979]
Epoch [55/120    avg_loss:0.017, val_acc:0.943]
Epoch [56/120    avg_loss:0.039, val_acc:0.986]
Epoch [57/120    avg_loss:0.009, val_acc:0.986]
Epoch [58/120    avg_loss:0.014, val_acc:0.989]
Epoch [59/120    avg_loss:0.082, val_acc:0.901]
Epoch [60/120    avg_loss:0.090, val_acc:0.985]
Epoch [61/120    avg_loss:0.056, val_acc:0.963]
Epoch [62/120    avg_loss:0.088, val_acc:0.944]
Epoch [63/120    avg_loss:0.044, val_acc:0.979]
Epoch [64/120    avg_loss:0.036, val_acc:0.975]
Epoch [65/120    avg_loss:0.036, val_acc:0.976]
Epoch [66/120    avg_loss:0.026, val_acc:0.981]
Epoch [67/120    avg_loss:0.025, val_acc:0.976]
Epoch [68/120    avg_loss:0.015, val_acc:0.989]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.991]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.989]
Epoch [77/120    avg_loss:0.013, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.027, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.489, val_acc:0.920]
Epoch [86/120    avg_loss:0.115, val_acc:0.978]
Epoch [87/120    avg_loss:0.028, val_acc:0.983]
Epoch [88/120    avg_loss:0.025, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.012, val_acc:0.985]
Epoch [91/120    avg_loss:0.020, val_acc:0.986]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.022, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     0    50    14     0]
 [    0     0 18016     0    41     0    33     0     0     0]
 [    0     1     0  1969     0     0     0     0    64     2]
 [    0     5     4     0  2956     0     4     0     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     1     0     0  4865     0     1     0]
 [    0     5     0     0     0     0     0  1282     1     2]
 [    0    13     1    31    48     0     0     0  3478     0]
 [    0     0     0     0    15    25     0     0     0   879]]

Accuracy:
99.09623309955896

F1 scores:
[       nan 0.99313787 0.99750844 0.97547684 0.9801061  0.99051233
 0.99488753 0.97787948 0.97545926 0.9750416 ]

Kappa:
0.9880336031765447
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71f2b1eb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.069, val_acc:0.568]
Epoch [2/120    avg_loss:1.585, val_acc:0.600]
Epoch [3/120    avg_loss:1.187, val_acc:0.675]
Epoch [4/120    avg_loss:0.880, val_acc:0.710]
Epoch [5/120    avg_loss:0.679, val_acc:0.810]
Epoch [6/120    avg_loss:0.553, val_acc:0.774]
Epoch [7/120    avg_loss:0.451, val_acc:0.775]
Epoch [8/120    avg_loss:0.435, val_acc:0.812]
Epoch [9/120    avg_loss:0.412, val_acc:0.883]
Epoch [10/120    avg_loss:0.252, val_acc:0.767]
Epoch [11/120    avg_loss:0.274, val_acc:0.865]
Epoch [12/120    avg_loss:0.256, val_acc:0.823]
Epoch [13/120    avg_loss:0.228, val_acc:0.927]
Epoch [14/120    avg_loss:0.190, val_acc:0.895]
Epoch [15/120    avg_loss:0.139, val_acc:0.860]
Epoch [16/120    avg_loss:0.139, val_acc:0.923]
Epoch [17/120    avg_loss:0.110, val_acc:0.961]
Epoch [18/120    avg_loss:0.126, val_acc:0.891]
Epoch [19/120    avg_loss:0.145, val_acc:0.897]
Epoch [20/120    avg_loss:0.140, val_acc:0.908]
Epoch [21/120    avg_loss:0.085, val_acc:0.966]
Epoch [22/120    avg_loss:0.058, val_acc:0.980]
Epoch [23/120    avg_loss:0.053, val_acc:0.972]
Epoch [24/120    avg_loss:0.059, val_acc:0.965]
Epoch [25/120    avg_loss:0.084, val_acc:0.964]
Epoch [26/120    avg_loss:0.134, val_acc:0.920]
Epoch [27/120    avg_loss:0.098, val_acc:0.956]
Epoch [28/120    avg_loss:0.057, val_acc:0.978]
Epoch [29/120    avg_loss:0.127, val_acc:0.937]
Epoch [30/120    avg_loss:0.112, val_acc:0.971]
Epoch [31/120    avg_loss:0.055, val_acc:0.951]
Epoch [32/120    avg_loss:0.079, val_acc:0.945]
Epoch [33/120    avg_loss:0.046, val_acc:0.974]
Epoch [34/120    avg_loss:0.065, val_acc:0.966]
Epoch [35/120    avg_loss:0.037, val_acc:0.968]
Epoch [36/120    avg_loss:0.023, val_acc:0.988]
Epoch [37/120    avg_loss:0.019, val_acc:0.987]
Epoch [38/120    avg_loss:0.016, val_acc:0.987]
Epoch [39/120    avg_loss:0.023, val_acc:0.987]
Epoch [40/120    avg_loss:0.016, val_acc:0.987]
Epoch [41/120    avg_loss:0.013, val_acc:0.987]
Epoch [42/120    avg_loss:0.017, val_acc:0.990]
Epoch [43/120    avg_loss:0.013, val_acc:0.991]
Epoch [44/120    avg_loss:0.013, val_acc:0.992]
Epoch [45/120    avg_loss:0.013, val_acc:0.990]
Epoch [46/120    avg_loss:0.011, val_acc:0.990]
Epoch [47/120    avg_loss:0.012, val_acc:0.990]
Epoch [48/120    avg_loss:0.013, val_acc:0.989]
Epoch [49/120    avg_loss:0.010, val_acc:0.989]
Epoch [50/120    avg_loss:0.012, val_acc:0.990]
Epoch [51/120    avg_loss:0.013, val_acc:0.989]
Epoch [52/120    avg_loss:0.010, val_acc:0.991]
Epoch [53/120    avg_loss:0.013, val_acc:0.991]
Epoch [54/120    avg_loss:0.011, val_acc:0.990]
Epoch [55/120    avg_loss:0.012, val_acc:0.990]
Epoch [56/120    avg_loss:0.010, val_acc:0.990]
Epoch [57/120    avg_loss:0.010, val_acc:0.991]
Epoch [58/120    avg_loss:0.013, val_acc:0.991]
Epoch [59/120    avg_loss:0.010, val_acc:0.991]
Epoch [60/120    avg_loss:0.009, val_acc:0.991]
Epoch [61/120    avg_loss:0.012, val_acc:0.989]
Epoch [62/120    avg_loss:0.010, val_acc:0.990]
Epoch [63/120    avg_loss:0.011, val_acc:0.989]
Epoch [64/120    avg_loss:0.014, val_acc:0.989]
Epoch [65/120    avg_loss:0.013, val_acc:0.990]
Epoch [66/120    avg_loss:0.010, val_acc:0.990]
Epoch [67/120    avg_loss:0.012, val_acc:0.989]
Epoch [68/120    avg_loss:0.010, val_acc:0.989]
Epoch [69/120    avg_loss:0.012, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.989]
Epoch [71/120    avg_loss:0.009, val_acc:0.989]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.011, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.989]
Epoch [76/120    avg_loss:0.012, val_acc:0.989]
Epoch [77/120    avg_loss:0.011, val_acc:0.989]
Epoch [78/120    avg_loss:0.011, val_acc:0.989]
Epoch [79/120    avg_loss:0.009, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.989]
Epoch [81/120    avg_loss:0.012, val_acc:0.989]
Epoch [82/120    avg_loss:0.009, val_acc:0.989]
Epoch [83/120    avg_loss:0.010, val_acc:0.989]
Epoch [84/120    avg_loss:0.011, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.011, val_acc:0.989]
Epoch [88/120    avg_loss:0.012, val_acc:0.989]
Epoch [89/120    avg_loss:0.009, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.989]
Epoch [92/120    avg_loss:0.010, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.989]
Epoch [96/120    avg_loss:0.009, val_acc:0.989]
Epoch [97/120    avg_loss:0.010, val_acc:0.989]
Epoch [98/120    avg_loss:0.011, val_acc:0.989]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.013, val_acc:0.989]
Epoch [101/120    avg_loss:0.010, val_acc:0.989]
Epoch [102/120    avg_loss:0.012, val_acc:0.989]
Epoch [103/120    avg_loss:0.009, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.989]
Epoch [106/120    avg_loss:0.014, val_acc:0.989]
Epoch [107/120    avg_loss:0.010, val_acc:0.989]
Epoch [108/120    avg_loss:0.010, val_acc:0.989]
Epoch [109/120    avg_loss:0.010, val_acc:0.989]
Epoch [110/120    avg_loss:0.010, val_acc:0.989]
Epoch [111/120    avg_loss:0.011, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.989]
Epoch [113/120    avg_loss:0.008, val_acc:0.989]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.011, val_acc:0.989]
Epoch [116/120    avg_loss:0.011, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.014, val_acc:0.989]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.012, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6342     0    15     0     0     0     0    75     0]
 [    0     0 17975     0    81     0    25     0     9     0]
 [    0     7     0  1982     0     0     0     0    40     7]
 [    0    25     4     0  2920     0    20     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    46     1     0     0  4817     0    14     0]
 [    0     1     0     0     0     0     2  1280     0     7]
 [    0     5     0    19    58     0     0     0  3489     0]
 [    0     0     0     0    15    17     0     0     0   887]]

Accuracy:
98.80461764634998

F1 scores:
[       nan 0.99000937 0.99543126 0.97804096 0.96592789 0.99352874
 0.98891398 0.99610895 0.96903208 0.97472527]

Kappa:
0.9841739905657317
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa951e92be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:1.956, val_acc:0.268]
Epoch [2/120    avg_loss:1.435, val_acc:0.398]
Epoch [3/120    avg_loss:1.049, val_acc:0.664]
Epoch [4/120    avg_loss:0.839, val_acc:0.701]
Epoch [5/120    avg_loss:0.705, val_acc:0.700]
Epoch [6/120    avg_loss:0.512, val_acc:0.748]
Epoch [7/120    avg_loss:0.499, val_acc:0.835]
Epoch [8/120    avg_loss:0.377, val_acc:0.765]
Epoch [9/120    avg_loss:0.345, val_acc:0.833]
Epoch [10/120    avg_loss:0.319, val_acc:0.832]
Epoch [11/120    avg_loss:0.299, val_acc:0.850]
Epoch [12/120    avg_loss:0.245, val_acc:0.844]
Epoch [13/120    avg_loss:0.236, val_acc:0.892]
Epoch [14/120    avg_loss:0.202, val_acc:0.873]
Epoch [15/120    avg_loss:0.261, val_acc:0.825]
Epoch [16/120    avg_loss:0.217, val_acc:0.930]
Epoch [17/120    avg_loss:0.228, val_acc:0.795]
Epoch [18/120    avg_loss:0.201, val_acc:0.907]
Epoch [19/120    avg_loss:0.153, val_acc:0.912]
Epoch [20/120    avg_loss:0.244, val_acc:0.902]
Epoch [21/120    avg_loss:0.171, val_acc:0.910]
Epoch [22/120    avg_loss:0.170, val_acc:0.938]
Epoch [23/120    avg_loss:0.155, val_acc:0.943]
Epoch [24/120    avg_loss:0.121, val_acc:0.948]
Epoch [25/120    avg_loss:0.308, val_acc:0.866]
Epoch [26/120    avg_loss:0.233, val_acc:0.927]
Epoch [27/120    avg_loss:0.128, val_acc:0.949]
Epoch [28/120    avg_loss:0.104, val_acc:0.936]
Epoch [29/120    avg_loss:0.089, val_acc:0.949]
Epoch [30/120    avg_loss:0.090, val_acc:0.963]
Epoch [31/120    avg_loss:0.082, val_acc:0.956]
Epoch [32/120    avg_loss:0.064, val_acc:0.955]
Epoch [33/120    avg_loss:0.073, val_acc:0.963]
Epoch [34/120    avg_loss:0.054, val_acc:0.950]
Epoch [35/120    avg_loss:0.098, val_acc:0.958]
Epoch [36/120    avg_loss:0.213, val_acc:0.721]
Epoch [37/120    avg_loss:0.145, val_acc:0.947]
Epoch [38/120    avg_loss:0.077, val_acc:0.949]
Epoch [39/120    avg_loss:0.073, val_acc:0.953]
Epoch [40/120    avg_loss:0.057, val_acc:0.958]
Epoch [41/120    avg_loss:0.041, val_acc:0.966]
Epoch [42/120    avg_loss:0.074, val_acc:0.942]
Epoch [43/120    avg_loss:0.069, val_acc:0.960]
Epoch [44/120    avg_loss:0.051, val_acc:0.966]
Epoch [45/120    avg_loss:0.077, val_acc:0.953]
Epoch [46/120    avg_loss:0.041, val_acc:0.958]
Epoch [47/120    avg_loss:0.036, val_acc:0.964]
Epoch [48/120    avg_loss:0.058, val_acc:0.956]
Epoch [49/120    avg_loss:0.037, val_acc:0.970]
Epoch [50/120    avg_loss:0.057, val_acc:0.952]
Epoch [51/120    avg_loss:0.048, val_acc:0.967]
Epoch [52/120    avg_loss:0.033, val_acc:0.968]
Epoch [53/120    avg_loss:0.020, val_acc:0.972]
Epoch [54/120    avg_loss:0.025, val_acc:0.965]
Epoch [55/120    avg_loss:0.022, val_acc:0.968]
Epoch [56/120    avg_loss:0.032, val_acc:0.962]
Epoch [57/120    avg_loss:0.030, val_acc:0.953]
Epoch [58/120    avg_loss:0.020, val_acc:0.970]
Epoch [59/120    avg_loss:0.023, val_acc:0.948]
Epoch [60/120    avg_loss:0.050, val_acc:0.970]
Epoch [61/120    avg_loss:0.015, val_acc:0.974]
Epoch [62/120    avg_loss:0.026, val_acc:0.970]
Epoch [63/120    avg_loss:0.037, val_acc:0.969]
Epoch [64/120    avg_loss:0.015, val_acc:0.973]
Epoch [65/120    avg_loss:0.040, val_acc:0.926]
Epoch [66/120    avg_loss:0.026, val_acc:0.965]
Epoch [67/120    avg_loss:0.026, val_acc:0.974]
Epoch [68/120    avg_loss:0.011, val_acc:0.969]
Epoch [69/120    avg_loss:0.020, val_acc:0.955]
Epoch [70/120    avg_loss:0.480, val_acc:0.895]
Epoch [71/120    avg_loss:0.166, val_acc:0.902]
Epoch [72/120    avg_loss:0.074, val_acc:0.962]
Epoch [73/120    avg_loss:0.036, val_acc:0.967]
Epoch [74/120    avg_loss:0.035, val_acc:0.968]
Epoch [75/120    avg_loss:0.024, val_acc:0.970]
Epoch [76/120    avg_loss:0.019, val_acc:0.970]
Epoch [77/120    avg_loss:0.015, val_acc:0.963]
Epoch [78/120    avg_loss:0.050, val_acc:0.973]
Epoch [79/120    avg_loss:0.016, val_acc:0.968]
Epoch [80/120    avg_loss:0.017, val_acc:0.974]
Epoch [81/120    avg_loss:0.013, val_acc:0.966]
Epoch [82/120    avg_loss:0.016, val_acc:0.973]
Epoch [83/120    avg_loss:0.021, val_acc:0.975]
Epoch [84/120    avg_loss:0.020, val_acc:0.978]
Epoch [85/120    avg_loss:0.009, val_acc:0.978]
Epoch [86/120    avg_loss:0.007, val_acc:0.974]
Epoch [87/120    avg_loss:0.007, val_acc:0.973]
Epoch [88/120    avg_loss:0.005, val_acc:0.974]
Epoch [89/120    avg_loss:0.009, val_acc:0.968]
Epoch [90/120    avg_loss:0.009, val_acc:0.975]
Epoch [91/120    avg_loss:0.016, val_acc:0.972]
Epoch [92/120    avg_loss:0.006, val_acc:0.973]
Epoch [93/120    avg_loss:0.009, val_acc:0.973]
Epoch [94/120    avg_loss:0.006, val_acc:0.979]
Epoch [95/120    avg_loss:0.005, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.981]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.199, val_acc:0.946]
Epoch [99/120    avg_loss:0.056, val_acc:0.967]
Epoch [100/120    avg_loss:0.019, val_acc:0.968]
Epoch [101/120    avg_loss:0.028, val_acc:0.971]
Epoch [102/120    avg_loss:0.010, val_acc:0.970]
Epoch [103/120    avg_loss:0.028, val_acc:0.967]
Epoch [104/120    avg_loss:0.010, val_acc:0.970]
Epoch [105/120    avg_loss:0.012, val_acc:0.969]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.012, val_acc:0.970]
Epoch [108/120    avg_loss:0.014, val_acc:0.973]
Epoch [109/120    avg_loss:0.006, val_acc:0.973]
Epoch [110/120    avg_loss:0.007, val_acc:0.970]
Epoch [111/120    avg_loss:0.005, val_acc:0.973]
Epoch [112/120    avg_loss:0.006, val_acc:0.973]
Epoch [113/120    avg_loss:0.006, val_acc:0.973]
Epoch [114/120    avg_loss:0.004, val_acc:0.973]
Epoch [115/120    avg_loss:0.005, val_acc:0.973]
Epoch [116/120    avg_loss:0.006, val_acc:0.975]
Epoch [117/120    avg_loss:0.004, val_acc:0.974]
Epoch [118/120    avg_loss:0.007, val_acc:0.975]
Epoch [119/120    avg_loss:0.004, val_acc:0.975]
Epoch [120/120    avg_loss:0.004, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     0     0     5     8    40     0]
 [    0     0 18029     0    10     0    44     0     7     0]
 [    0     9     0  1938     0     0     0     0    86     3]
 [    0    27    14     0  2925     0     1     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     1     0     0  4848     0     0     0]
 [    0     1     0     0     0     0     0  1288     0     1]
 [    0     8     0    63    36     0     0     0  3461     3]
 [    0     0     0     0    14    62     0     0     0   843]]

Accuracy:
98.850408502639

F1 scores:
[       nan 0.9923771  0.99712405 0.95988113 0.98203794 0.97679641
 0.99181669 0.99613302 0.96541144 0.95308084]

Kappa:
0.9847693853956251
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89359a0be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.111, val_acc:0.520]
Epoch [2/120    avg_loss:1.617, val_acc:0.624]
Epoch [3/120    avg_loss:1.160, val_acc:0.647]
Epoch [4/120    avg_loss:0.837, val_acc:0.736]
Epoch [5/120    avg_loss:0.643, val_acc:0.759]
Epoch [6/120    avg_loss:0.529, val_acc:0.796]
Epoch [7/120    avg_loss:0.480, val_acc:0.742]
Epoch [8/120    avg_loss:0.394, val_acc:0.817]
Epoch [9/120    avg_loss:0.339, val_acc:0.878]
Epoch [10/120    avg_loss:0.302, val_acc:0.879]
Epoch [11/120    avg_loss:0.296, val_acc:0.869]
Epoch [12/120    avg_loss:0.267, val_acc:0.908]
Epoch [13/120    avg_loss:0.209, val_acc:0.934]
Epoch [14/120    avg_loss:0.265, val_acc:0.915]
Epoch [15/120    avg_loss:0.209, val_acc:0.931]
Epoch [16/120    avg_loss:0.199, val_acc:0.879]
Epoch [17/120    avg_loss:0.186, val_acc:0.930]
Epoch [18/120    avg_loss:0.171, val_acc:0.949]
Epoch [19/120    avg_loss:0.148, val_acc:0.945]
Epoch [20/120    avg_loss:0.125, val_acc:0.948]
Epoch [21/120    avg_loss:0.097, val_acc:0.966]
Epoch [22/120    avg_loss:0.128, val_acc:0.931]
Epoch [23/120    avg_loss:0.109, val_acc:0.952]
Epoch [24/120    avg_loss:0.115, val_acc:0.933]
Epoch [25/120    avg_loss:0.110, val_acc:0.964]
Epoch [26/120    avg_loss:0.066, val_acc:0.966]
Epoch [27/120    avg_loss:0.079, val_acc:0.963]
Epoch [28/120    avg_loss:0.045, val_acc:0.971]
Epoch [29/120    avg_loss:0.083, val_acc:0.853]
Epoch [30/120    avg_loss:0.080, val_acc:0.935]
Epoch [31/120    avg_loss:0.059, val_acc:0.965]
Epoch [32/120    avg_loss:0.040, val_acc:0.976]
Epoch [33/120    avg_loss:0.067, val_acc:0.963]
Epoch [34/120    avg_loss:0.048, val_acc:0.971]
Epoch [35/120    avg_loss:0.062, val_acc:0.965]
Epoch [36/120    avg_loss:0.049, val_acc:0.967]
Epoch [37/120    avg_loss:0.042, val_acc:0.960]
Epoch [38/120    avg_loss:0.037, val_acc:0.970]
Epoch [39/120    avg_loss:0.026, val_acc:0.978]
Epoch [40/120    avg_loss:0.032, val_acc:0.979]
Epoch [41/120    avg_loss:0.041, val_acc:0.978]
Epoch [42/120    avg_loss:0.026, val_acc:0.973]
Epoch [43/120    avg_loss:0.026, val_acc:0.977]
Epoch [44/120    avg_loss:0.021, val_acc:0.978]
Epoch [45/120    avg_loss:0.018, val_acc:0.982]
Epoch [46/120    avg_loss:0.018, val_acc:0.978]
Epoch [47/120    avg_loss:0.020, val_acc:0.981]
Epoch [48/120    avg_loss:0.012, val_acc:0.983]
Epoch [49/120    avg_loss:0.030, val_acc:0.976]
Epoch [50/120    avg_loss:0.652, val_acc:0.684]
Epoch [51/120    avg_loss:0.636, val_acc:0.766]
Epoch [52/120    avg_loss:0.326, val_acc:0.907]
Epoch [53/120    avg_loss:0.185, val_acc:0.910]
Epoch [54/120    avg_loss:0.188, val_acc:0.861]
Epoch [55/120    avg_loss:0.146, val_acc:0.931]
Epoch [56/120    avg_loss:0.082, val_acc:0.959]
Epoch [57/120    avg_loss:0.052, val_acc:0.973]
Epoch [58/120    avg_loss:0.054, val_acc:0.981]
Epoch [59/120    avg_loss:0.037, val_acc:0.975]
Epoch [60/120    avg_loss:0.035, val_acc:0.971]
Epoch [61/120    avg_loss:0.044, val_acc:0.976]
Epoch [62/120    avg_loss:0.026, val_acc:0.981]
Epoch [63/120    avg_loss:0.026, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.983]
Epoch [65/120    avg_loss:0.019, val_acc:0.984]
Epoch [66/120    avg_loss:0.018, val_acc:0.986]
Epoch [67/120    avg_loss:0.018, val_acc:0.986]
Epoch [68/120    avg_loss:0.017, val_acc:0.986]
Epoch [69/120    avg_loss:0.018, val_acc:0.986]
Epoch [70/120    avg_loss:0.018, val_acc:0.985]
Epoch [71/120    avg_loss:0.017, val_acc:0.984]
Epoch [72/120    avg_loss:0.015, val_acc:0.984]
Epoch [73/120    avg_loss:0.017, val_acc:0.985]
Epoch [74/120    avg_loss:0.014, val_acc:0.985]
Epoch [75/120    avg_loss:0.017, val_acc:0.985]
Epoch [76/120    avg_loss:0.014, val_acc:0.984]
Epoch [77/120    avg_loss:0.014, val_acc:0.986]
Epoch [78/120    avg_loss:0.013, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.012, val_acc:0.986]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.014, val_acc:0.986]
Epoch [85/120    avg_loss:0.013, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.986]
Epoch [92/120    avg_loss:0.024, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.987]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.014, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.987]
Epoch [105/120    avg_loss:0.017, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.012, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.987]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.013, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6319     0     0     7     0     8    28    70     0]
 [    0     0 18042     0    11     0    14     0    21     2]
 [    0     0     0  1969     0     0     0     0    62     5]
 [    0    25     9     1  2919     0     8     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4868     0     3     0]
 [    0     0     0     0     0     2     0  1284     4     0]
 [    0    22     0     4    51     0     0     0  3493     1]
 [    0     0     0     0    18    21     0     0     0   880]]

Accuracy:
99.00224134191309

F1 scores:
[       nan 0.98749805 0.9982295  0.98204489 0.9765808  0.99126472
 0.99590835 0.98693313 0.96571745 0.97399004]

Kappa:
0.9867849186787128
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94631f7ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.060, val_acc:0.422]
Epoch [2/120    avg_loss:1.623, val_acc:0.657]
Epoch [3/120    avg_loss:1.151, val_acc:0.733]
Epoch [4/120    avg_loss:0.845, val_acc:0.710]
Epoch [5/120    avg_loss:0.620, val_acc:0.772]
Epoch [6/120    avg_loss:0.459, val_acc:0.762]
Epoch [7/120    avg_loss:0.414, val_acc:0.878]
Epoch [8/120    avg_loss:0.380, val_acc:0.822]
Epoch [9/120    avg_loss:0.328, val_acc:0.881]
Epoch [10/120    avg_loss:0.287, val_acc:0.898]
Epoch [11/120    avg_loss:0.281, val_acc:0.895]
Epoch [12/120    avg_loss:0.227, val_acc:0.942]
Epoch [13/120    avg_loss:0.290, val_acc:0.927]
Epoch [14/120    avg_loss:0.188, val_acc:0.946]
Epoch [15/120    avg_loss:0.233, val_acc:0.942]
Epoch [16/120    avg_loss:0.177, val_acc:0.937]
Epoch [17/120    avg_loss:0.209, val_acc:0.955]
Epoch [18/120    avg_loss:0.119, val_acc:0.966]
Epoch [19/120    avg_loss:0.111, val_acc:0.951]
Epoch [20/120    avg_loss:0.130, val_acc:0.965]
Epoch [21/120    avg_loss:0.109, val_acc:0.960]
Epoch [22/120    avg_loss:0.135, val_acc:0.943]
Epoch [23/120    avg_loss:0.118, val_acc:0.977]
Epoch [24/120    avg_loss:0.104, val_acc:0.941]
Epoch [25/120    avg_loss:0.247, val_acc:0.796]
Epoch [26/120    avg_loss:0.282, val_acc:0.930]
Epoch [27/120    avg_loss:0.139, val_acc:0.971]
Epoch [28/120    avg_loss:0.093, val_acc:0.973]
Epoch [29/120    avg_loss:0.111, val_acc:0.971]
Epoch [30/120    avg_loss:0.092, val_acc:0.982]
Epoch [31/120    avg_loss:0.052, val_acc:0.975]
Epoch [32/120    avg_loss:0.074, val_acc:0.966]
Epoch [33/120    avg_loss:0.500, val_acc:0.881]
Epoch [34/120    avg_loss:0.245, val_acc:0.927]
Epoch [35/120    avg_loss:0.150, val_acc:0.960]
Epoch [36/120    avg_loss:0.106, val_acc:0.979]
Epoch [37/120    avg_loss:0.123, val_acc:0.927]
Epoch [38/120    avg_loss:0.081, val_acc:0.978]
Epoch [39/120    avg_loss:0.044, val_acc:0.978]
Epoch [40/120    avg_loss:0.053, val_acc:0.974]
Epoch [41/120    avg_loss:0.059, val_acc:0.974]
Epoch [42/120    avg_loss:0.043, val_acc:0.978]
Epoch [43/120    avg_loss:0.036, val_acc:0.980]
Epoch [44/120    avg_loss:0.034, val_acc:0.986]
Epoch [45/120    avg_loss:0.024, val_acc:0.985]
Epoch [46/120    avg_loss:0.029, val_acc:0.988]
Epoch [47/120    avg_loss:0.022, val_acc:0.988]
Epoch [48/120    avg_loss:0.019, val_acc:0.988]
Epoch [49/120    avg_loss:0.021, val_acc:0.988]
Epoch [50/120    avg_loss:0.020, val_acc:0.988]
Epoch [51/120    avg_loss:0.019, val_acc:0.988]
Epoch [52/120    avg_loss:0.023, val_acc:0.988]
Epoch [53/120    avg_loss:0.021, val_acc:0.987]
Epoch [54/120    avg_loss:0.018, val_acc:0.988]
Epoch [55/120    avg_loss:0.015, val_acc:0.989]
Epoch [56/120    avg_loss:0.018, val_acc:0.988]
Epoch [57/120    avg_loss:0.020, val_acc:0.988]
Epoch [58/120    avg_loss:0.026, val_acc:0.988]
Epoch [59/120    avg_loss:0.017, val_acc:0.988]
Epoch [60/120    avg_loss:0.019, val_acc:0.989]
Epoch [61/120    avg_loss:0.018, val_acc:0.988]
Epoch [62/120    avg_loss:0.015, val_acc:0.989]
Epoch [63/120    avg_loss:0.013, val_acc:0.989]
Epoch [64/120    avg_loss:0.020, val_acc:0.988]
Epoch [65/120    avg_loss:0.017, val_acc:0.989]
Epoch [66/120    avg_loss:0.014, val_acc:0.988]
Epoch [67/120    avg_loss:0.019, val_acc:0.988]
Epoch [68/120    avg_loss:0.019, val_acc:0.989]
Epoch [69/120    avg_loss:0.016, val_acc:0.989]
Epoch [70/120    avg_loss:0.019, val_acc:0.989]
Epoch [71/120    avg_loss:0.013, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.989]
Epoch [73/120    avg_loss:0.016, val_acc:0.989]
Epoch [74/120    avg_loss:0.015, val_acc:0.989]
Epoch [75/120    avg_loss:0.015, val_acc:0.990]
Epoch [76/120    avg_loss:0.019, val_acc:0.989]
Epoch [77/120    avg_loss:0.015, val_acc:0.990]
Epoch [78/120    avg_loss:0.016, val_acc:0.990]
Epoch [79/120    avg_loss:0.015, val_acc:0.989]
Epoch [80/120    avg_loss:0.014, val_acc:0.990]
Epoch [81/120    avg_loss:0.020, val_acc:0.988]
Epoch [82/120    avg_loss:0.013, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.014, val_acc:0.989]
Epoch [85/120    avg_loss:0.013, val_acc:0.990]
Epoch [86/120    avg_loss:0.016, val_acc:0.988]
Epoch [87/120    avg_loss:0.013, val_acc:0.989]
Epoch [88/120    avg_loss:0.012, val_acc:0.991]
Epoch [89/120    avg_loss:0.011, val_acc:0.989]
Epoch [90/120    avg_loss:0.014, val_acc:0.991]
Epoch [91/120    avg_loss:0.012, val_acc:0.991]
Epoch [92/120    avg_loss:0.011, val_acc:0.991]
Epoch [93/120    avg_loss:0.013, val_acc:0.989]
Epoch [94/120    avg_loss:0.014, val_acc:0.991]
Epoch [95/120    avg_loss:0.013, val_acc:0.989]
Epoch [96/120    avg_loss:0.013, val_acc:0.991]
Epoch [97/120    avg_loss:0.012, val_acc:0.991]
Epoch [98/120    avg_loss:0.012, val_acc:0.992]
Epoch [99/120    avg_loss:0.011, val_acc:0.991]
Epoch [100/120    avg_loss:0.014, val_acc:0.991]
Epoch [101/120    avg_loss:0.012, val_acc:0.992]
Epoch [102/120    avg_loss:0.010, val_acc:0.993]
Epoch [103/120    avg_loss:0.010, val_acc:0.991]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.010, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.993]
Epoch [107/120    avg_loss:0.012, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.991]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.011, val_acc:0.992]
Epoch [113/120    avg_loss:0.013, val_acc:0.991]
Epoch [114/120    avg_loss:0.014, val_acc:0.991]
Epoch [115/120    avg_loss:0.011, val_acc:0.993]
Epoch [116/120    avg_loss:0.012, val_acc:0.992]
Epoch [117/120    avg_loss:0.013, val_acc:0.993]
Epoch [118/120    avg_loss:0.011, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.011, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     0    48    14     2]
 [    0     0 18053     0    23     0    14     0     0     0]
 [    0    21     0  1961     0     0     0     0    49     5]
 [    0    13     2     0  2952     0     2     0     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     1     0  4862     0     0     0]
 [    0     8     0     0     0     0     3  1278     1     0]
 [    0    10     1    36    35     0     0     0  3488     1]
 [    0     0     0     0     2     4     0     0     0   913]]

Accuracy:
99.24565589376522

F1 scores:
[       nan 0.99097417 0.99847902 0.97247706 0.98646617 0.99846978
 0.99641357 0.97706422 0.97908772 0.99185225]

Kappa:
0.9900073312912488
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f549dcd8b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:1.963, val_acc:0.347]
Epoch [2/120    avg_loss:1.485, val_acc:0.663]
Epoch [3/120    avg_loss:1.051, val_acc:0.721]
Epoch [4/120    avg_loss:0.834, val_acc:0.744]
Epoch [5/120    avg_loss:0.636, val_acc:0.767]
Epoch [6/120    avg_loss:0.544, val_acc:0.763]
Epoch [7/120    avg_loss:0.458, val_acc:0.781]
Epoch [8/120    avg_loss:0.404, val_acc:0.795]
Epoch [9/120    avg_loss:0.371, val_acc:0.819]
Epoch [10/120    avg_loss:0.314, val_acc:0.828]
Epoch [11/120    avg_loss:0.301, val_acc:0.836]
Epoch [12/120    avg_loss:0.303, val_acc:0.799]
Epoch [13/120    avg_loss:0.259, val_acc:0.849]
Epoch [14/120    avg_loss:0.226, val_acc:0.878]
Epoch [15/120    avg_loss:0.228, val_acc:0.907]
Epoch [16/120    avg_loss:0.179, val_acc:0.913]
Epoch [17/120    avg_loss:0.214, val_acc:0.914]
Epoch [18/120    avg_loss:0.124, val_acc:0.947]
Epoch [19/120    avg_loss:0.135, val_acc:0.825]
Epoch [20/120    avg_loss:0.158, val_acc:0.933]
Epoch [21/120    avg_loss:0.108, val_acc:0.908]
Epoch [22/120    avg_loss:0.142, val_acc:0.947]
Epoch [23/120    avg_loss:0.132, val_acc:0.913]
Epoch [24/120    avg_loss:0.102, val_acc:0.954]
Epoch [25/120    avg_loss:0.093, val_acc:0.945]
Epoch [26/120    avg_loss:0.097, val_acc:0.958]
Epoch [27/120    avg_loss:0.095, val_acc:0.947]
Epoch [28/120    avg_loss:0.079, val_acc:0.946]
Epoch [29/120    avg_loss:0.067, val_acc:0.959]
Epoch [30/120    avg_loss:0.040, val_acc:0.966]
Epoch [31/120    avg_loss:0.041, val_acc:0.953]
Epoch [32/120    avg_loss:0.082, val_acc:0.942]
Epoch [33/120    avg_loss:0.065, val_acc:0.966]
Epoch [34/120    avg_loss:0.056, val_acc:0.966]
Epoch [35/120    avg_loss:0.048, val_acc:0.956]
Epoch [36/120    avg_loss:0.038, val_acc:0.920]
Epoch [37/120    avg_loss:0.049, val_acc:0.966]
Epoch [38/120    avg_loss:0.074, val_acc:0.908]
Epoch [39/120    avg_loss:0.126, val_acc:0.954]
Epoch [40/120    avg_loss:0.052, val_acc:0.969]
Epoch [41/120    avg_loss:0.053, val_acc:0.967]
Epoch [42/120    avg_loss:0.029, val_acc:0.978]
Epoch [43/120    avg_loss:0.023, val_acc:0.977]
Epoch [44/120    avg_loss:0.036, val_acc:0.975]
Epoch [45/120    avg_loss:0.032, val_acc:0.971]
Epoch [46/120    avg_loss:0.042, val_acc:0.981]
Epoch [47/120    avg_loss:0.021, val_acc:0.981]
Epoch [48/120    avg_loss:0.018, val_acc:0.981]
Epoch [49/120    avg_loss:0.048, val_acc:0.963]
Epoch [50/120    avg_loss:0.034, val_acc:0.972]
Epoch [51/120    avg_loss:0.024, val_acc:0.968]
Epoch [52/120    avg_loss:0.017, val_acc:0.977]
Epoch [53/120    avg_loss:0.012, val_acc:0.984]
Epoch [54/120    avg_loss:0.018, val_acc:0.976]
Epoch [55/120    avg_loss:0.024, val_acc:0.983]
Epoch [56/120    avg_loss:0.018, val_acc:0.980]
Epoch [57/120    avg_loss:0.017, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.016, val_acc:0.986]
Epoch [60/120    avg_loss:0.042, val_acc:0.978]
Epoch [61/120    avg_loss:0.028, val_acc:0.977]
Epoch [62/120    avg_loss:0.119, val_acc:0.950]
Epoch [63/120    avg_loss:0.070, val_acc:0.961]
Epoch [64/120    avg_loss:0.035, val_acc:0.971]
Epoch [65/120    avg_loss:0.030, val_acc:0.974]
Epoch [66/120    avg_loss:0.016, val_acc:0.979]
Epoch [67/120    avg_loss:0.021, val_acc:0.969]
Epoch [68/120    avg_loss:0.016, val_acc:0.975]
Epoch [69/120    avg_loss:0.018, val_acc:0.982]
Epoch [70/120    avg_loss:0.021, val_acc:0.977]
Epoch [71/120    avg_loss:0.014, val_acc:0.981]
Epoch [72/120    avg_loss:0.010, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.007, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.983]
Epoch [82/120    avg_loss:0.005, val_acc:0.983]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     2     0     4     6    15     3]
 [    0     0 17979     0    69     0    41     0     1     0]
 [    0    14     0  1970     0     0     0     0    48     4]
 [    0    12     9     1  2912     0    27     0     9     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     3     0     0  4871     0     0     0]
 [    0     3     0     0     0     0     1  1285     0     1]
 [    0    14     0    15    21     0     0     0  3521     0]
 [    0     0     0     0    17    29     0     0     0   873]]

Accuracy:
99.09623309955896

F1 scores:
[       nan 0.99433098 0.99656338 0.97888199 0.97180043 0.98901099
 0.99185502 0.99573809 0.98283322 0.96892342]

Kappa:
0.9880358605968799
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f52f3307ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.067, val_acc:0.439]
Epoch [2/120    avg_loss:1.538, val_acc:0.634]
Epoch [3/120    avg_loss:1.094, val_acc:0.733]
Epoch [4/120    avg_loss:0.760, val_acc:0.774]
Epoch [5/120    avg_loss:0.574, val_acc:0.836]
Epoch [6/120    avg_loss:0.455, val_acc:0.855]
Epoch [7/120    avg_loss:0.414, val_acc:0.816]
Epoch [8/120    avg_loss:0.368, val_acc:0.873]
Epoch [9/120    avg_loss:0.364, val_acc:0.861]
Epoch [10/120    avg_loss:0.349, val_acc:0.873]
Epoch [11/120    avg_loss:0.307, val_acc:0.881]
Epoch [12/120    avg_loss:0.255, val_acc:0.908]
Epoch [13/120    avg_loss:0.230, val_acc:0.923]
Epoch [14/120    avg_loss:0.304, val_acc:0.902]
Epoch [15/120    avg_loss:0.224, val_acc:0.877]
Epoch [16/120    avg_loss:0.205, val_acc:0.916]
Epoch [17/120    avg_loss:0.170, val_acc:0.947]
Epoch [18/120    avg_loss:0.154, val_acc:0.927]
Epoch [19/120    avg_loss:0.163, val_acc:0.849]
Epoch [20/120    avg_loss:0.127, val_acc:0.916]
Epoch [21/120    avg_loss:0.122, val_acc:0.965]
Epoch [22/120    avg_loss:0.114, val_acc:0.972]
Epoch [23/120    avg_loss:0.114, val_acc:0.942]
Epoch [24/120    avg_loss:0.078, val_acc:0.932]
Epoch [25/120    avg_loss:0.103, val_acc:0.967]
Epoch [26/120    avg_loss:0.208, val_acc:0.953]
Epoch [27/120    avg_loss:0.081, val_acc:0.975]
Epoch [28/120    avg_loss:0.085, val_acc:0.966]
Epoch [29/120    avg_loss:0.131, val_acc:0.963]
Epoch [30/120    avg_loss:0.082, val_acc:0.978]
Epoch [31/120    avg_loss:0.091, val_acc:0.965]
Epoch [32/120    avg_loss:0.083, val_acc:0.976]
Epoch [33/120    avg_loss:0.066, val_acc:0.968]
Epoch [34/120    avg_loss:0.045, val_acc:0.963]
Epoch [35/120    avg_loss:0.044, val_acc:0.987]
Epoch [36/120    avg_loss:0.079, val_acc:0.948]
Epoch [37/120    avg_loss:0.153, val_acc:0.967]
Epoch [38/120    avg_loss:0.067, val_acc:0.978]
Epoch [39/120    avg_loss:0.046, val_acc:0.985]
Epoch [40/120    avg_loss:0.053, val_acc:0.936]
Epoch [41/120    avg_loss:1.678, val_acc:0.398]
Epoch [42/120    avg_loss:1.417, val_acc:0.435]
Epoch [43/120    avg_loss:1.044, val_acc:0.691]
Epoch [44/120    avg_loss:0.852, val_acc:0.719]
Epoch [45/120    avg_loss:0.735, val_acc:0.745]
Epoch [46/120    avg_loss:0.673, val_acc:0.765]
Epoch [47/120    avg_loss:0.584, val_acc:0.776]
Epoch [48/120    avg_loss:0.527, val_acc:0.762]
Epoch [49/120    avg_loss:0.487, val_acc:0.794]
Epoch [50/120    avg_loss:0.474, val_acc:0.806]
Epoch [51/120    avg_loss:0.450, val_acc:0.817]
Epoch [52/120    avg_loss:0.454, val_acc:0.822]
Epoch [53/120    avg_loss:0.448, val_acc:0.826]
Epoch [54/120    avg_loss:0.417, val_acc:0.818]
Epoch [55/120    avg_loss:0.427, val_acc:0.824]
Epoch [56/120    avg_loss:0.420, val_acc:0.844]
Epoch [57/120    avg_loss:0.397, val_acc:0.837]
Epoch [58/120    avg_loss:0.436, val_acc:0.842]
Epoch [59/120    avg_loss:0.435, val_acc:0.837]
Epoch [60/120    avg_loss:0.413, val_acc:0.834]
Epoch [61/120    avg_loss:0.386, val_acc:0.843]
Epoch [62/120    avg_loss:0.385, val_acc:0.840]
Epoch [63/120    avg_loss:0.397, val_acc:0.838]
Epoch [64/120    avg_loss:0.398, val_acc:0.838]
Epoch [65/120    avg_loss:0.407, val_acc:0.834]
Epoch [66/120    avg_loss:0.376, val_acc:0.840]
Epoch [67/120    avg_loss:0.383, val_acc:0.850]
Epoch [68/120    avg_loss:0.373, val_acc:0.847]
Epoch [69/120    avg_loss:0.379, val_acc:0.844]
Epoch [70/120    avg_loss:0.377, val_acc:0.846]
Epoch [71/120    avg_loss:0.411, val_acc:0.837]
Epoch [72/120    avg_loss:0.383, val_acc:0.839]
Epoch [73/120    avg_loss:0.383, val_acc:0.843]
Epoch [74/120    avg_loss:0.371, val_acc:0.844]
Epoch [75/120    avg_loss:0.392, val_acc:0.844]
Epoch [76/120    avg_loss:0.360, val_acc:0.843]
Epoch [77/120    avg_loss:0.393, val_acc:0.844]
Epoch [78/120    avg_loss:0.392, val_acc:0.843]
Epoch [79/120    avg_loss:0.406, val_acc:0.843]
Epoch [80/120    avg_loss:0.380, val_acc:0.843]
Epoch [81/120    avg_loss:0.398, val_acc:0.843]
Epoch [82/120    avg_loss:0.385, val_acc:0.842]
Epoch [83/120    avg_loss:0.342, val_acc:0.843]
Epoch [84/120    avg_loss:0.372, val_acc:0.843]
Epoch [85/120    avg_loss:0.393, val_acc:0.843]
Epoch [86/120    avg_loss:0.370, val_acc:0.843]
Epoch [87/120    avg_loss:0.396, val_acc:0.843]
Epoch [88/120    avg_loss:0.389, val_acc:0.843]
Epoch [89/120    avg_loss:0.386, val_acc:0.843]
Epoch [90/120    avg_loss:0.374, val_acc:0.843]
Epoch [91/120    avg_loss:0.374, val_acc:0.843]
Epoch [92/120    avg_loss:0.379, val_acc:0.843]
Epoch [93/120    avg_loss:0.381, val_acc:0.843]
Epoch [94/120    avg_loss:0.380, val_acc:0.843]
Epoch [95/120    avg_loss:0.379, val_acc:0.843]
Epoch [96/120    avg_loss:0.369, val_acc:0.843]
Epoch [97/120    avg_loss:0.411, val_acc:0.843]
Epoch [98/120    avg_loss:0.379, val_acc:0.843]
Epoch [99/120    avg_loss:0.373, val_acc:0.843]
Epoch [100/120    avg_loss:0.379, val_acc:0.843]
Epoch [101/120    avg_loss:0.385, val_acc:0.843]
Epoch [102/120    avg_loss:0.390, val_acc:0.843]
Epoch [103/120    avg_loss:0.392, val_acc:0.843]
Epoch [104/120    avg_loss:0.389, val_acc:0.843]
Epoch [105/120    avg_loss:0.367, val_acc:0.843]
Epoch [106/120    avg_loss:0.404, val_acc:0.843]
Epoch [107/120    avg_loss:0.388, val_acc:0.843]
Epoch [108/120    avg_loss:0.387, val_acc:0.843]
Epoch [109/120    avg_loss:0.379, val_acc:0.843]
Epoch [110/120    avg_loss:0.371, val_acc:0.843]
Epoch [111/120    avg_loss:0.388, val_acc:0.843]
Epoch [112/120    avg_loss:0.387, val_acc:0.843]
Epoch [113/120    avg_loss:0.364, val_acc:0.843]
Epoch [114/120    avg_loss:0.408, val_acc:0.843]
Epoch [115/120    avg_loss:0.402, val_acc:0.843]
Epoch [116/120    avg_loss:0.386, val_acc:0.843]
Epoch [117/120    avg_loss:0.378, val_acc:0.843]
Epoch [118/120    avg_loss:0.389, val_acc:0.843]
Epoch [119/120    avg_loss:0.386, val_acc:0.843]
Epoch [120/120    avg_loss:0.371, val_acc:0.843]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5328     0    10   320     0    38    90   494   152]
 [    0     9 14311     0   141     0  3624     0     5     0]
 [    0    20     0  1493     0     0     0     0   477    46]
 [    0    94    69     9  2696     0    68     1    32     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2   274   151     2     0  4331     0   118     0]
 [    0    60     0     1    23     0     0  1203     3     0]
 [    0   195     4     4    38     0    77     0  3253     0]
 [    0    21     0     6    14    40     0     0     3   835]]

Accuracy:
83.76111633287543

F1 scores:
[       nan 0.87624373 0.87400757 0.80485175 0.86883661 0.98490566
 0.66548863 0.93111455 0.81774761 0.85421995]

Kappa:
0.7924278901111588
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ff9b02b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.024, val_acc:0.315]
Epoch [2/120    avg_loss:1.555, val_acc:0.478]
Epoch [3/120    avg_loss:1.201, val_acc:0.679]
Epoch [4/120    avg_loss:0.926, val_acc:0.719]
Epoch [5/120    avg_loss:0.715, val_acc:0.736]
Epoch [6/120    avg_loss:0.667, val_acc:0.765]
Epoch [7/120    avg_loss:0.572, val_acc:0.695]
Epoch [8/120    avg_loss:0.530, val_acc:0.791]
Epoch [9/120    avg_loss:0.423, val_acc:0.816]
Epoch [10/120    avg_loss:0.464, val_acc:0.828]
Epoch [11/120    avg_loss:0.391, val_acc:0.839]
Epoch [12/120    avg_loss:0.333, val_acc:0.837]
Epoch [13/120    avg_loss:0.309, val_acc:0.877]
Epoch [14/120    avg_loss:0.270, val_acc:0.891]
Epoch [15/120    avg_loss:0.416, val_acc:0.886]
Epoch [16/120    avg_loss:0.245, val_acc:0.894]
Epoch [17/120    avg_loss:0.194, val_acc:0.855]
Epoch [18/120    avg_loss:0.181, val_acc:0.917]
Epoch [19/120    avg_loss:0.169, val_acc:0.909]
Epoch [20/120    avg_loss:0.181, val_acc:0.941]
Epoch [21/120    avg_loss:0.184, val_acc:0.907]
Epoch [22/120    avg_loss:0.152, val_acc:0.957]
Epoch [23/120    avg_loss:0.126, val_acc:0.947]
Epoch [24/120    avg_loss:0.747, val_acc:0.627]
Epoch [25/120    avg_loss:0.765, val_acc:0.717]
Epoch [26/120    avg_loss:0.657, val_acc:0.737]
Epoch [27/120    avg_loss:0.621, val_acc:0.757]
Epoch [28/120    avg_loss:0.587, val_acc:0.788]
Epoch [29/120    avg_loss:0.495, val_acc:0.793]
Epoch [30/120    avg_loss:0.495, val_acc:0.751]
Epoch [31/120    avg_loss:0.420, val_acc:0.832]
Epoch [32/120    avg_loss:0.401, val_acc:0.788]
Epoch [33/120    avg_loss:0.351, val_acc:0.864]
Epoch [34/120    avg_loss:0.316, val_acc:0.852]
Epoch [35/120    avg_loss:0.396, val_acc:0.795]
Epoch [36/120    avg_loss:0.343, val_acc:0.858]
Epoch [37/120    avg_loss:0.273, val_acc:0.863]
Epoch [38/120    avg_loss:0.267, val_acc:0.872]
Epoch [39/120    avg_loss:0.272, val_acc:0.860]
Epoch [40/120    avg_loss:0.239, val_acc:0.881]
Epoch [41/120    avg_loss:0.260, val_acc:0.881]
Epoch [42/120    avg_loss:0.248, val_acc:0.888]
Epoch [43/120    avg_loss:0.237, val_acc:0.884]
Epoch [44/120    avg_loss:0.243, val_acc:0.882]
Epoch [45/120    avg_loss:0.231, val_acc:0.883]
Epoch [46/120    avg_loss:0.234, val_acc:0.892]
Epoch [47/120    avg_loss:0.215, val_acc:0.883]
Epoch [48/120    avg_loss:0.228, val_acc:0.888]
Epoch [49/120    avg_loss:0.213, val_acc:0.889]
Epoch [50/120    avg_loss:0.220, val_acc:0.890]
Epoch [51/120    avg_loss:0.226, val_acc:0.890]
Epoch [52/120    avg_loss:0.223, val_acc:0.891]
Epoch [53/120    avg_loss:0.207, val_acc:0.890]
Epoch [54/120    avg_loss:0.218, val_acc:0.889]
Epoch [55/120    avg_loss:0.203, val_acc:0.891]
Epoch [56/120    avg_loss:0.212, val_acc:0.893]
Epoch [57/120    avg_loss:0.213, val_acc:0.891]
Epoch [58/120    avg_loss:0.205, val_acc:0.892]
Epoch [59/120    avg_loss:0.212, val_acc:0.892]
Epoch [60/120    avg_loss:0.230, val_acc:0.894]
Epoch [61/120    avg_loss:0.211, val_acc:0.896]
Epoch [62/120    avg_loss:0.207, val_acc:0.894]
Epoch [63/120    avg_loss:0.200, val_acc:0.895]
Epoch [64/120    avg_loss:0.215, val_acc:0.894]
Epoch [65/120    avg_loss:0.210, val_acc:0.894]
Epoch [66/120    avg_loss:0.203, val_acc:0.894]
Epoch [67/120    avg_loss:0.199, val_acc:0.896]
Epoch [68/120    avg_loss:0.218, val_acc:0.895]
Epoch [69/120    avg_loss:0.213, val_acc:0.894]
Epoch [70/120    avg_loss:0.195, val_acc:0.894]
Epoch [71/120    avg_loss:0.207, val_acc:0.894]
Epoch [72/120    avg_loss:0.203, val_acc:0.894]
Epoch [73/120    avg_loss:0.212, val_acc:0.894]
Epoch [74/120    avg_loss:0.219, val_acc:0.894]
Epoch [75/120    avg_loss:0.217, val_acc:0.894]
Epoch [76/120    avg_loss:0.201, val_acc:0.894]
Epoch [77/120    avg_loss:0.199, val_acc:0.894]
Epoch [78/120    avg_loss:0.221, val_acc:0.894]
Epoch [79/120    avg_loss:0.213, val_acc:0.894]
Epoch [80/120    avg_loss:0.194, val_acc:0.894]
Epoch [81/120    avg_loss:0.206, val_acc:0.894]
Epoch [82/120    avg_loss:0.211, val_acc:0.894]
Epoch [83/120    avg_loss:0.203, val_acc:0.894]
Epoch [84/120    avg_loss:0.180, val_acc:0.894]
Epoch [85/120    avg_loss:0.218, val_acc:0.894]
Epoch [86/120    avg_loss:0.214, val_acc:0.894]
Epoch [87/120    avg_loss:0.233, val_acc:0.894]
Epoch [88/120    avg_loss:0.229, val_acc:0.894]
Epoch [89/120    avg_loss:0.223, val_acc:0.894]
Epoch [90/120    avg_loss:0.200, val_acc:0.894]
Epoch [91/120    avg_loss:0.219, val_acc:0.894]
Epoch [92/120    avg_loss:0.215, val_acc:0.894]
Epoch [93/120    avg_loss:0.200, val_acc:0.894]
Epoch [94/120    avg_loss:0.206, val_acc:0.894]
Epoch [95/120    avg_loss:0.215, val_acc:0.894]
Epoch [96/120    avg_loss:0.235, val_acc:0.894]
Epoch [97/120    avg_loss:0.202, val_acc:0.894]
Epoch [98/120    avg_loss:0.215, val_acc:0.894]
Epoch [99/120    avg_loss:0.210, val_acc:0.894]
Epoch [100/120    avg_loss:0.203, val_acc:0.894]
Epoch [101/120    avg_loss:0.212, val_acc:0.894]
Epoch [102/120    avg_loss:0.206, val_acc:0.894]
Epoch [103/120    avg_loss:0.212, val_acc:0.894]
Epoch [104/120    avg_loss:0.236, val_acc:0.894]
Epoch [105/120    avg_loss:0.210, val_acc:0.894]
Epoch [106/120    avg_loss:0.211, val_acc:0.894]
Epoch [107/120    avg_loss:0.193, val_acc:0.894]
Epoch [108/120    avg_loss:0.202, val_acc:0.894]
Epoch [109/120    avg_loss:0.205, val_acc:0.894]
Epoch [110/120    avg_loss:0.197, val_acc:0.894]
Epoch [111/120    avg_loss:0.193, val_acc:0.894]
Epoch [112/120    avg_loss:0.220, val_acc:0.894]
Epoch [113/120    avg_loss:0.226, val_acc:0.894]
Epoch [114/120    avg_loss:0.211, val_acc:0.894]
Epoch [115/120    avg_loss:0.203, val_acc:0.894]
Epoch [116/120    avg_loss:0.217, val_acc:0.894]
Epoch [117/120    avg_loss:0.201, val_acc:0.894]
Epoch [118/120    avg_loss:0.220, val_acc:0.894]
Epoch [119/120    avg_loss:0.205, val_acc:0.894]
Epoch [120/120    avg_loss:0.204, val_acc:0.894]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5566     0    33   144     0   157   103   270   159]
 [    0     0 17030     0     4     0  1056     0     0     0]
 [    0    14     0  2005     1     0     0     0     0    16]
 [    0   124   270     2  2363     0   182     0    28     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   180     3     4     0  4691     0     0     0]
 [    0    35     0     0     0     0    18  1233     0     4]
 [    0    75    30    28    45     0    20     0  3373     0]
 [    0    17     0    10    19    91     0     0     0   782]]

Accuracy:
92.42040826163449

F1 scores:
[       nan 0.90777134 0.95674157 0.9740102  0.85122478 0.96630877
 0.85275404 0.93907083 0.93151063 0.83058948]

Kappa:
0.9002396991953304
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a9f906b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.017, val_acc:0.135]
Epoch [2/120    avg_loss:1.654, val_acc:0.361]
Epoch [3/120    avg_loss:1.317, val_acc:0.647]
Epoch [4/120    avg_loss:1.023, val_acc:0.678]
Epoch [5/120    avg_loss:0.760, val_acc:0.757]
Epoch [6/120    avg_loss:0.617, val_acc:0.745]
Epoch [7/120    avg_loss:0.546, val_acc:0.828]
Epoch [8/120    avg_loss:0.436, val_acc:0.832]
Epoch [9/120    avg_loss:0.432, val_acc:0.800]
Epoch [10/120    avg_loss:0.362, val_acc:0.893]
Epoch [11/120    avg_loss:0.315, val_acc:0.881]
Epoch [12/120    avg_loss:0.268, val_acc:0.906]
Epoch [13/120    avg_loss:0.247, val_acc:0.920]
Epoch [14/120    avg_loss:0.207, val_acc:0.932]
Epoch [15/120    avg_loss:0.212, val_acc:0.900]
Epoch [16/120    avg_loss:0.182, val_acc:0.929]
Epoch [17/120    avg_loss:0.141, val_acc:0.955]
Epoch [18/120    avg_loss:0.147, val_acc:0.938]
Epoch [19/120    avg_loss:0.158, val_acc:0.895]
Epoch [20/120    avg_loss:0.106, val_acc:0.942]
Epoch [21/120    avg_loss:0.081, val_acc:0.966]
Epoch [22/120    avg_loss:0.083, val_acc:0.975]
Epoch [23/120    avg_loss:0.099, val_acc:0.961]
Epoch [24/120    avg_loss:0.053, val_acc:0.964]
Epoch [25/120    avg_loss:0.052, val_acc:0.948]
Epoch [26/120    avg_loss:0.061, val_acc:0.963]
Epoch [27/120    avg_loss:0.045, val_acc:0.957]
Epoch [28/120    avg_loss:0.094, val_acc:0.938]
Epoch [29/120    avg_loss:0.086, val_acc:0.959]
Epoch [30/120    avg_loss:0.078, val_acc:0.941]
Epoch [31/120    avg_loss:0.066, val_acc:0.959]
Epoch [32/120    avg_loss:0.078, val_acc:0.962]
Epoch [33/120    avg_loss:0.082, val_acc:0.955]
Epoch [34/120    avg_loss:0.046, val_acc:0.970]
Epoch [35/120    avg_loss:0.050, val_acc:0.941]
Epoch [36/120    avg_loss:0.039, val_acc:0.969]
Epoch [37/120    avg_loss:0.025, val_acc:0.972]
Epoch [38/120    avg_loss:0.025, val_acc:0.975]
Epoch [39/120    avg_loss:0.020, val_acc:0.975]
Epoch [40/120    avg_loss:0.021, val_acc:0.978]
Epoch [41/120    avg_loss:0.019, val_acc:0.974]
Epoch [42/120    avg_loss:0.019, val_acc:0.974]
Epoch [43/120    avg_loss:0.022, val_acc:0.974]
Epoch [44/120    avg_loss:0.032, val_acc:0.978]
Epoch [45/120    avg_loss:0.018, val_acc:0.975]
Epoch [46/120    avg_loss:0.019, val_acc:0.977]
Epoch [47/120    avg_loss:0.016, val_acc:0.977]
Epoch [48/120    avg_loss:0.014, val_acc:0.975]
Epoch [49/120    avg_loss:0.014, val_acc:0.978]
Epoch [50/120    avg_loss:0.014, val_acc:0.975]
Epoch [51/120    avg_loss:0.014, val_acc:0.976]
Epoch [52/120    avg_loss:0.015, val_acc:0.977]
Epoch [53/120    avg_loss:0.016, val_acc:0.977]
Epoch [54/120    avg_loss:0.016, val_acc:0.978]
Epoch [55/120    avg_loss:0.020, val_acc:0.978]
Epoch [56/120    avg_loss:0.013, val_acc:0.978]
Epoch [57/120    avg_loss:0.016, val_acc:0.979]
Epoch [58/120    avg_loss:0.016, val_acc:0.980]
Epoch [59/120    avg_loss:0.015, val_acc:0.979]
Epoch [60/120    avg_loss:0.013, val_acc:0.979]
Epoch [61/120    avg_loss:0.014, val_acc:0.980]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.015, val_acc:0.980]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.011, val_acc:0.979]
Epoch [67/120    avg_loss:0.011, val_acc:0.975]
Epoch [68/120    avg_loss:0.013, val_acc:0.979]
Epoch [69/120    avg_loss:0.012, val_acc:0.980]
Epoch [70/120    avg_loss:0.010, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.978]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.014, val_acc:0.979]
Epoch [75/120    avg_loss:0.013, val_acc:0.979]
Epoch [76/120    avg_loss:0.009, val_acc:0.979]
Epoch [77/120    avg_loss:0.010, val_acc:0.980]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.980]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.980]
Epoch [83/120    avg_loss:0.012, val_acc:0.974]
Epoch [84/120    avg_loss:0.013, val_acc:0.978]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.011, val_acc:0.980]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.011, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.980]
Epoch [90/120    avg_loss:0.011, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.012, val_acc:0.980]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.012, val_acc:0.980]
Epoch [95/120    avg_loss:0.011, val_acc:0.980]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.008, val_acc:0.981]
Epoch [98/120    avg_loss:0.009, val_acc:0.981]
Epoch [99/120    avg_loss:0.012, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.015, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.980]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     0     6     1     0     1    18    43     0]
 [    0     1 18081     0     5     0     3     0     0     0]
 [    0     2     0  2018     0     0     0     0    16     0]
 [    0     9     7     0  2949     0     1     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    27     7     0     0  4844     0     0     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0    12     0    19    54     0     0     0  3486     0]
 [    0    18     0     3     1    26     0     0     0   871]]

Accuracy:
99.30831706552912

F1 scores:
[       nan 0.9912759  0.99881232 0.9870384  0.98595787 0.99013657
 0.99599054 0.99268387 0.9789385  0.97318436]

Kappa:
0.9908331505260667
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f512d93fb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.049, val_acc:0.268]
Epoch [2/120    avg_loss:1.520, val_acc:0.507]
Epoch [3/120    avg_loss:1.070, val_acc:0.593]
Epoch [4/120    avg_loss:0.740, val_acc:0.699]
Epoch [5/120    avg_loss:0.942, val_acc:0.485]
Epoch [6/120    avg_loss:1.043, val_acc:0.731]
Epoch [7/120    avg_loss:0.803, val_acc:0.736]
Epoch [8/120    avg_loss:0.718, val_acc:0.755]
Epoch [9/120    avg_loss:0.621, val_acc:0.777]
Epoch [10/120    avg_loss:0.529, val_acc:0.763]
Epoch [11/120    avg_loss:0.503, val_acc:0.797]
Epoch [12/120    avg_loss:0.441, val_acc:0.831]
Epoch [13/120    avg_loss:0.370, val_acc:0.812]
Epoch [14/120    avg_loss:0.379, val_acc:0.839]
Epoch [15/120    avg_loss:0.361, val_acc:0.856]
Epoch [16/120    avg_loss:0.316, val_acc:0.872]
Epoch [17/120    avg_loss:0.297, val_acc:0.851]
Epoch [18/120    avg_loss:0.300, val_acc:0.889]
Epoch [19/120    avg_loss:0.325, val_acc:0.878]
Epoch [20/120    avg_loss:0.279, val_acc:0.862]
Epoch [21/120    avg_loss:0.253, val_acc:0.820]
Epoch [22/120    avg_loss:0.272, val_acc:0.889]
Epoch [23/120    avg_loss:0.215, val_acc:0.888]
Epoch [24/120    avg_loss:0.178, val_acc:0.918]
Epoch [25/120    avg_loss:0.189, val_acc:0.912]
Epoch [26/120    avg_loss:0.227, val_acc:0.885]
Epoch [27/120    avg_loss:0.167, val_acc:0.909]
Epoch [28/120    avg_loss:0.164, val_acc:0.894]
Epoch [29/120    avg_loss:0.165, val_acc:0.921]
Epoch [30/120    avg_loss:0.175, val_acc:0.849]
Epoch [31/120    avg_loss:0.187, val_acc:0.905]
Epoch [32/120    avg_loss:0.135, val_acc:0.873]
Epoch [33/120    avg_loss:0.139, val_acc:0.943]
Epoch [34/120    avg_loss:0.154, val_acc:0.943]
Epoch [35/120    avg_loss:0.139, val_acc:0.930]
Epoch [36/120    avg_loss:0.121, val_acc:0.947]
Epoch [37/120    avg_loss:0.138, val_acc:0.934]
Epoch [38/120    avg_loss:0.118, val_acc:0.945]
Epoch [39/120    avg_loss:0.100, val_acc:0.946]
Epoch [40/120    avg_loss:0.082, val_acc:0.929]
Epoch [41/120    avg_loss:0.092, val_acc:0.959]
Epoch [42/120    avg_loss:0.115, val_acc:0.949]
Epoch [43/120    avg_loss:0.113, val_acc:0.955]
Epoch [44/120    avg_loss:0.120, val_acc:0.938]
Epoch [45/120    avg_loss:0.095, val_acc:0.950]
Epoch [46/120    avg_loss:0.098, val_acc:0.948]
Epoch [47/120    avg_loss:0.067, val_acc:0.965]
Epoch [48/120    avg_loss:0.084, val_acc:0.962]
Epoch [49/120    avg_loss:0.083, val_acc:0.948]
Epoch [50/120    avg_loss:0.076, val_acc:0.962]
Epoch [51/120    avg_loss:0.079, val_acc:0.965]
Epoch [52/120    avg_loss:0.079, val_acc:0.957]
Epoch [53/120    avg_loss:0.064, val_acc:0.965]
Epoch [54/120    avg_loss:0.060, val_acc:0.959]
Epoch [55/120    avg_loss:0.053, val_acc:0.971]
Epoch [56/120    avg_loss:0.059, val_acc:0.961]
Epoch [57/120    avg_loss:0.066, val_acc:0.968]
Epoch [58/120    avg_loss:0.073, val_acc:0.963]
Epoch [59/120    avg_loss:0.070, val_acc:0.936]
Epoch [60/120    avg_loss:0.052, val_acc:0.965]
Epoch [61/120    avg_loss:0.041, val_acc:0.970]
Epoch [62/120    avg_loss:0.037, val_acc:0.965]
Epoch [63/120    avg_loss:0.040, val_acc:0.974]
Epoch [64/120    avg_loss:0.049, val_acc:0.976]
Epoch [65/120    avg_loss:0.038, val_acc:0.975]
Epoch [66/120    avg_loss:0.047, val_acc:0.977]
Epoch [67/120    avg_loss:0.027, val_acc:0.981]
Epoch [68/120    avg_loss:0.041, val_acc:0.972]
Epoch [69/120    avg_loss:0.051, val_acc:0.977]
Epoch [70/120    avg_loss:0.060, val_acc:0.981]
Epoch [71/120    avg_loss:0.026, val_acc:0.970]
Epoch [72/120    avg_loss:0.036, val_acc:0.970]
Epoch [73/120    avg_loss:0.023, val_acc:0.977]
Epoch [74/120    avg_loss:0.021, val_acc:0.964]
Epoch [75/120    avg_loss:0.043, val_acc:0.980]
Epoch [76/120    avg_loss:0.029, val_acc:0.978]
Epoch [77/120    avg_loss:0.020, val_acc:0.984]
Epoch [78/120    avg_loss:0.029, val_acc:0.983]
Epoch [79/120    avg_loss:0.024, val_acc:0.982]
Epoch [80/120    avg_loss:0.017, val_acc:0.985]
Epoch [81/120    avg_loss:0.029, val_acc:0.985]
Epoch [82/120    avg_loss:0.042, val_acc:0.977]
Epoch [83/120    avg_loss:0.027, val_acc:0.969]
Epoch [84/120    avg_loss:0.026, val_acc:0.978]
Epoch [85/120    avg_loss:0.022, val_acc:0.963]
Epoch [86/120    avg_loss:0.042, val_acc:0.981]
Epoch [87/120    avg_loss:0.028, val_acc:0.974]
Epoch [88/120    avg_loss:0.019, val_acc:0.982]
Epoch [89/120    avg_loss:0.021, val_acc:0.976]
Epoch [90/120    avg_loss:0.016, val_acc:0.984]
Epoch [91/120    avg_loss:0.031, val_acc:0.970]
Epoch [92/120    avg_loss:0.026, val_acc:0.978]
Epoch [93/120    avg_loss:0.019, val_acc:0.981]
Epoch [94/120    avg_loss:0.012, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.971]
Epoch [96/120    avg_loss:0.011, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.982]
Epoch [98/120    avg_loss:0.019, val_acc:0.981]
Epoch [99/120    avg_loss:0.024, val_acc:0.982]
Epoch [100/120    avg_loss:0.020, val_acc:0.980]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.014, val_acc:0.978]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.982]
Epoch [107/120    avg_loss:0.011, val_acc:0.987]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.973]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.012, val_acc:0.989]
Epoch [120/120    avg_loss:0.034, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6333     0     0     1     0     0     3    66    29]
 [    0     9 18014     0    59     0     6     0     2     0]
 [    0     6     0  1965     1     0     0     0    61     3]
 [    0    29     8     0  2906     0     3     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     2     0     0  4874     0     0     0]
 [    0    44     0     0     0     0     1  1242     0     3]
 [    0    43     0    24    52     0     5     0  3447     0]
 [    0     0     0     0    15   123     0     0     0   781]]

Accuracy:
98.49131178753044

F1 scores:
[       nan 0.98216501 0.99761865 0.97591259 0.96769897 0.95499451
 0.99805467 0.97988166 0.96110414 0.90028818]

Kappa:
0.9800200923367762
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7158e4b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.069, val_acc:0.266]
Epoch [2/120    avg_loss:1.689, val_acc:0.574]
Epoch [3/120    avg_loss:1.295, val_acc:0.696]
Epoch [4/120    avg_loss:0.962, val_acc:0.652]
Epoch [5/120    avg_loss:0.731, val_acc:0.689]
Epoch [6/120    avg_loss:0.575, val_acc:0.818]
Epoch [7/120    avg_loss:0.524, val_acc:0.769]
Epoch [8/120    avg_loss:0.434, val_acc:0.845]
Epoch [9/120    avg_loss:0.477, val_acc:0.791]
Epoch [10/120    avg_loss:0.369, val_acc:0.773]
Epoch [11/120    avg_loss:0.250, val_acc:0.905]
Epoch [12/120    avg_loss:0.222, val_acc:0.888]
Epoch [13/120    avg_loss:0.243, val_acc:0.797]
Epoch [14/120    avg_loss:0.359, val_acc:0.903]
Epoch [15/120    avg_loss:0.198, val_acc:0.889]
Epoch [16/120    avg_loss:0.202, val_acc:0.887]
Epoch [17/120    avg_loss:0.800, val_acc:0.688]
Epoch [18/120    avg_loss:0.535, val_acc:0.727]
Epoch [19/120    avg_loss:0.456, val_acc:0.699]
Epoch [20/120    avg_loss:0.346, val_acc:0.854]
Epoch [21/120    avg_loss:0.331, val_acc:0.803]
Epoch [22/120    avg_loss:0.244, val_acc:0.882]
Epoch [23/120    avg_loss:0.183, val_acc:0.892]
Epoch [24/120    avg_loss:0.134, val_acc:0.914]
Epoch [25/120    avg_loss:0.176, val_acc:0.851]
Epoch [26/120    avg_loss:0.208, val_acc:0.926]
Epoch [27/120    avg_loss:0.140, val_acc:0.935]
Epoch [28/120    avg_loss:0.145, val_acc:0.943]
Epoch [29/120    avg_loss:0.103, val_acc:0.941]
Epoch [30/120    avg_loss:0.092, val_acc:0.951]
Epoch [31/120    avg_loss:0.075, val_acc:0.952]
Epoch [32/120    avg_loss:0.103, val_acc:0.729]
Epoch [33/120    avg_loss:0.141, val_acc:0.949]
Epoch [34/120    avg_loss:0.090, val_acc:0.943]
Epoch [35/120    avg_loss:0.064, val_acc:0.954]
Epoch [36/120    avg_loss:0.132, val_acc:0.936]
Epoch [37/120    avg_loss:0.113, val_acc:0.934]
Epoch [38/120    avg_loss:0.089, val_acc:0.945]
Epoch [39/120    avg_loss:0.105, val_acc:0.947]
Epoch [40/120    avg_loss:0.071, val_acc:0.959]
Epoch [41/120    avg_loss:0.052, val_acc:0.967]
Epoch [42/120    avg_loss:0.061, val_acc:0.961]
Epoch [43/120    avg_loss:0.058, val_acc:0.969]
Epoch [44/120    avg_loss:0.029, val_acc:0.976]
Epoch [45/120    avg_loss:0.033, val_acc:0.972]
Epoch [46/120    avg_loss:0.024, val_acc:0.967]
Epoch [47/120    avg_loss:0.030, val_acc:0.966]
Epoch [48/120    avg_loss:0.016, val_acc:0.973]
Epoch [49/120    avg_loss:0.023, val_acc:0.976]
Epoch [50/120    avg_loss:0.018, val_acc:0.980]
Epoch [51/120    avg_loss:0.024, val_acc:0.983]
Epoch [52/120    avg_loss:0.013, val_acc:0.980]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.018, val_acc:0.976]
Epoch [55/120    avg_loss:0.022, val_acc:0.977]
Epoch [56/120    avg_loss:0.010, val_acc:0.981]
Epoch [57/120    avg_loss:0.068, val_acc:0.938]
Epoch [58/120    avg_loss:0.037, val_acc:0.974]
Epoch [59/120    avg_loss:0.019, val_acc:0.977]
Epoch [60/120    avg_loss:0.014, val_acc:0.985]
Epoch [61/120    avg_loss:0.022, val_acc:0.973]
Epoch [62/120    avg_loss:0.013, val_acc:0.987]
Epoch [63/120    avg_loss:0.021, val_acc:0.982]
Epoch [64/120    avg_loss:0.011, val_acc:0.986]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.008, val_acc:0.983]
Epoch [68/120    avg_loss:0.013, val_acc:0.983]
Epoch [69/120    avg_loss:0.015, val_acc:0.981]
Epoch [70/120    avg_loss:0.026, val_acc:0.978]
Epoch [71/120    avg_loss:0.019, val_acc:0.982]
Epoch [72/120    avg_loss:0.012, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.886]
Epoch [74/120    avg_loss:0.072, val_acc:0.965]
Epoch [75/120    avg_loss:0.058, val_acc:0.966]
Epoch [76/120    avg_loss:0.017, val_acc:0.968]
Epoch [77/120    avg_loss:0.032, val_acc:0.970]
Epoch [78/120    avg_loss:0.013, val_acc:0.982]
Epoch [79/120    avg_loss:0.028, val_acc:0.962]
Epoch [80/120    avg_loss:0.015, val_acc:0.974]
Epoch [81/120    avg_loss:0.010, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.007, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     1     0     0    16    17     6]
 [    0     0 18042     0    33     0     6     0     9     0]
 [    0    10     0  1984     0     0     0     0    40     2]
 [    0    23     9     0  2923     0     5     0     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     7     0     0  4855     0     1     0]
 [    0     3     0     0     0     0     1  1285     0     1]
 [    0     7     0     7    49     0     0     0  3508     0]
 [    0     0     0     0    14    52     0     0     0   853]]

Accuracy:
99.16612440652641

F1 scores:
[       nan 0.99354939 0.99800863 0.98363907 0.97563418 0.98046582
 0.99640841 0.99189502 0.98057303 0.95627803]

Kappa:
0.9889534025874732
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e811b6b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.044, val_acc:0.246]
Epoch [2/120    avg_loss:1.551, val_acc:0.345]
Epoch [3/120    avg_loss:1.175, val_acc:0.684]
Epoch [4/120    avg_loss:0.853, val_acc:0.715]
Epoch [5/120    avg_loss:0.689, val_acc:0.803]
Epoch [6/120    avg_loss:0.527, val_acc:0.818]
Epoch [7/120    avg_loss:0.413, val_acc:0.858]
Epoch [8/120    avg_loss:0.336, val_acc:0.842]
Epoch [9/120    avg_loss:0.326, val_acc:0.845]
Epoch [10/120    avg_loss:0.311, val_acc:0.826]
Epoch [11/120    avg_loss:0.237, val_acc:0.892]
Epoch [12/120    avg_loss:0.249, val_acc:0.895]
Epoch [13/120    avg_loss:0.255, val_acc:0.843]
Epoch [14/120    avg_loss:0.178, val_acc:0.934]
Epoch [15/120    avg_loss:0.157, val_acc:0.933]
Epoch [16/120    avg_loss:0.127, val_acc:0.939]
Epoch [17/120    avg_loss:0.141, val_acc:0.911]
Epoch [18/120    avg_loss:0.125, val_acc:0.934]
Epoch [19/120    avg_loss:0.152, val_acc:0.922]
Epoch [20/120    avg_loss:0.123, val_acc:0.900]
Epoch [21/120    avg_loss:0.128, val_acc:0.935]
Epoch [22/120    avg_loss:0.083, val_acc:0.961]
Epoch [23/120    avg_loss:0.087, val_acc:0.949]
Epoch [24/120    avg_loss:0.115, val_acc:0.955]
Epoch [25/120    avg_loss:0.105, val_acc:0.891]
Epoch [26/120    avg_loss:0.128, val_acc:0.963]
Epoch [27/120    avg_loss:0.063, val_acc:0.966]
Epoch [28/120    avg_loss:0.051, val_acc:0.974]
Epoch [29/120    avg_loss:0.039, val_acc:0.970]
Epoch [30/120    avg_loss:0.060, val_acc:0.952]
Epoch [31/120    avg_loss:0.071, val_acc:0.974]
Epoch [32/120    avg_loss:0.042, val_acc:0.968]
Epoch [33/120    avg_loss:0.035, val_acc:0.979]
Epoch [34/120    avg_loss:0.026, val_acc:0.969]
Epoch [35/120    avg_loss:0.041, val_acc:0.969]
Epoch [36/120    avg_loss:0.027, val_acc:0.939]
Epoch [37/120    avg_loss:0.046, val_acc:0.533]
Epoch [38/120    avg_loss:1.756, val_acc:0.586]
Epoch [39/120    avg_loss:1.010, val_acc:0.683]
Epoch [40/120    avg_loss:0.732, val_acc:0.723]
Epoch [41/120    avg_loss:0.579, val_acc:0.765]
Epoch [42/120    avg_loss:0.516, val_acc:0.802]
Epoch [43/120    avg_loss:0.469, val_acc:0.781]
Epoch [44/120    avg_loss:0.390, val_acc:0.821]
Epoch [45/120    avg_loss:0.345, val_acc:0.866]
Epoch [46/120    avg_loss:0.289, val_acc:0.858]
Epoch [47/120    avg_loss:0.275, val_acc:0.889]
Epoch [48/120    avg_loss:0.219, val_acc:0.890]
Epoch [49/120    avg_loss:0.210, val_acc:0.889]
Epoch [50/120    avg_loss:0.240, val_acc:0.905]
Epoch [51/120    avg_loss:0.208, val_acc:0.895]
Epoch [52/120    avg_loss:0.211, val_acc:0.895]
Epoch [53/120    avg_loss:0.218, val_acc:0.903]
Epoch [54/120    avg_loss:0.197, val_acc:0.905]
Epoch [55/120    avg_loss:0.200, val_acc:0.908]
Epoch [56/120    avg_loss:0.193, val_acc:0.905]
Epoch [57/120    avg_loss:0.203, val_acc:0.907]
Epoch [58/120    avg_loss:0.189, val_acc:0.907]
Epoch [59/120    avg_loss:0.185, val_acc:0.908]
Epoch [60/120    avg_loss:0.190, val_acc:0.907]
Epoch [61/120    avg_loss:0.194, val_acc:0.910]
Epoch [62/120    avg_loss:0.163, val_acc:0.914]
Epoch [63/120    avg_loss:0.164, val_acc:0.914]
Epoch [64/120    avg_loss:0.191, val_acc:0.915]
Epoch [65/120    avg_loss:0.184, val_acc:0.913]
Epoch [66/120    avg_loss:0.164, val_acc:0.914]
Epoch [67/120    avg_loss:0.155, val_acc:0.916]
Epoch [68/120    avg_loss:0.161, val_acc:0.914]
Epoch [69/120    avg_loss:0.174, val_acc:0.915]
Epoch [70/120    avg_loss:0.160, val_acc:0.913]
Epoch [71/120    avg_loss:0.173, val_acc:0.915]
Epoch [72/120    avg_loss:0.168, val_acc:0.915]
Epoch [73/120    avg_loss:0.160, val_acc:0.915]
Epoch [74/120    avg_loss:0.165, val_acc:0.916]
Epoch [75/120    avg_loss:0.156, val_acc:0.916]
Epoch [76/120    avg_loss:0.164, val_acc:0.916]
Epoch [77/120    avg_loss:0.153, val_acc:0.917]
Epoch [78/120    avg_loss:0.168, val_acc:0.917]
Epoch [79/120    avg_loss:0.165, val_acc:0.917]
Epoch [80/120    avg_loss:0.165, val_acc:0.917]
Epoch [81/120    avg_loss:0.170, val_acc:0.917]
Epoch [82/120    avg_loss:0.166, val_acc:0.917]
Epoch [83/120    avg_loss:0.165, val_acc:0.917]
Epoch [84/120    avg_loss:0.174, val_acc:0.917]
Epoch [85/120    avg_loss:0.173, val_acc:0.917]
Epoch [86/120    avg_loss:0.173, val_acc:0.917]
Epoch [87/120    avg_loss:0.167, val_acc:0.917]
Epoch [88/120    avg_loss:0.173, val_acc:0.917]
Epoch [89/120    avg_loss:0.160, val_acc:0.917]
Epoch [90/120    avg_loss:0.181, val_acc:0.917]
Epoch [91/120    avg_loss:0.164, val_acc:0.917]
Epoch [92/120    avg_loss:0.152, val_acc:0.917]
Epoch [93/120    avg_loss:0.172, val_acc:0.917]
Epoch [94/120    avg_loss:0.182, val_acc:0.917]
Epoch [95/120    avg_loss:0.170, val_acc:0.917]
Epoch [96/120    avg_loss:0.187, val_acc:0.917]
Epoch [97/120    avg_loss:0.165, val_acc:0.917]
Epoch [98/120    avg_loss:0.166, val_acc:0.917]
Epoch [99/120    avg_loss:0.148, val_acc:0.917]
Epoch [100/120    avg_loss:0.161, val_acc:0.917]
Epoch [101/120    avg_loss:0.168, val_acc:0.917]
Epoch [102/120    avg_loss:0.170, val_acc:0.917]
Epoch [103/120    avg_loss:0.155, val_acc:0.917]
Epoch [104/120    avg_loss:0.169, val_acc:0.917]
Epoch [105/120    avg_loss:0.177, val_acc:0.917]
Epoch [106/120    avg_loss:0.176, val_acc:0.917]
Epoch [107/120    avg_loss:0.163, val_acc:0.917]
Epoch [108/120    avg_loss:0.168, val_acc:0.917]
Epoch [109/120    avg_loss:0.168, val_acc:0.917]
Epoch [110/120    avg_loss:0.170, val_acc:0.917]
Epoch [111/120    avg_loss:0.154, val_acc:0.917]
Epoch [112/120    avg_loss:0.172, val_acc:0.917]
Epoch [113/120    avg_loss:0.157, val_acc:0.917]
Epoch [114/120    avg_loss:0.164, val_acc:0.917]
Epoch [115/120    avg_loss:0.176, val_acc:0.917]
Epoch [116/120    avg_loss:0.177, val_acc:0.917]
Epoch [117/120    avg_loss:0.173, val_acc:0.917]
Epoch [118/120    avg_loss:0.167, val_acc:0.917]
Epoch [119/120    avg_loss:0.168, val_acc:0.917]
Epoch [120/120    avg_loss:0.168, val_acc:0.917]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5842    45     6   148     0     0    37   206   148]
 [    0    14 12091     0   105     0  5880     0     0     0]
 [    0    69     0  1788     0     0     0     0   163    16]
 [    0   114    34     1  2779     0    22     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    14     0     0    15     0  4819     0    30     0]
 [    0    56     0     0    12     0     0  1221     0     1]
 [    0   438     0    26    66     0   110     0  2931     0]
 [    0    19     0     0    15    35     0     0     1   849]]

Accuracy:
81.0377654062131

F1 scores:
[       nan 0.89890752 0.79914078 0.92714545 0.90935864 0.98676749
 0.61353364 0.95839874 0.84662045 0.87842732]

Kappa:
0.7629369203511828
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4b4906ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.025, val_acc:0.313]
Epoch [2/120    avg_loss:1.596, val_acc:0.625]
Epoch [3/120    avg_loss:1.199, val_acc:0.662]
Epoch [4/120    avg_loss:0.896, val_acc:0.666]
Epoch [5/120    avg_loss:0.708, val_acc:0.797]
Epoch [6/120    avg_loss:0.626, val_acc:0.716]
Epoch [7/120    avg_loss:0.513, val_acc:0.818]
Epoch [8/120    avg_loss:0.433, val_acc:0.841]
Epoch [9/120    avg_loss:0.340, val_acc:0.847]
Epoch [10/120    avg_loss:0.328, val_acc:0.878]
Epoch [11/120    avg_loss:0.278, val_acc:0.891]
Epoch [12/120    avg_loss:0.207, val_acc:0.908]
Epoch [13/120    avg_loss:0.212, val_acc:0.925]
Epoch [14/120    avg_loss:0.197, val_acc:0.917]
Epoch [15/120    avg_loss:0.180, val_acc:0.938]
Epoch [16/120    avg_loss:0.131, val_acc:0.913]
Epoch [17/120    avg_loss:0.167, val_acc:0.934]
Epoch [18/120    avg_loss:0.152, val_acc:0.832]
Epoch [19/120    avg_loss:0.138, val_acc:0.949]
Epoch [20/120    avg_loss:0.097, val_acc:0.949]
Epoch [21/120    avg_loss:0.092, val_acc:0.951]
Epoch [22/120    avg_loss:0.099, val_acc:0.947]
Epoch [23/120    avg_loss:0.101, val_acc:0.959]
Epoch [24/120    avg_loss:0.079, val_acc:0.966]
Epoch [25/120    avg_loss:0.100, val_acc:0.944]
Epoch [26/120    avg_loss:0.073, val_acc:0.961]
Epoch [27/120    avg_loss:0.132, val_acc:0.894]
Epoch [28/120    avg_loss:0.075, val_acc:0.955]
Epoch [29/120    avg_loss:0.053, val_acc:0.973]
Epoch [30/120    avg_loss:0.047, val_acc:0.956]
Epoch [31/120    avg_loss:0.061, val_acc:0.962]
Epoch [32/120    avg_loss:0.071, val_acc:0.972]
Epoch [33/120    avg_loss:0.067, val_acc:0.967]
Epoch [34/120    avg_loss:0.055, val_acc:0.967]
Epoch [35/120    avg_loss:0.038, val_acc:0.974]
Epoch [36/120    avg_loss:0.098, val_acc:0.962]
Epoch [37/120    avg_loss:0.058, val_acc:0.962]
Epoch [38/120    avg_loss:0.042, val_acc:0.948]
Epoch [39/120    avg_loss:0.053, val_acc:0.938]
Epoch [40/120    avg_loss:0.044, val_acc:0.971]
Epoch [41/120    avg_loss:0.031, val_acc:0.967]
Epoch [42/120    avg_loss:0.031, val_acc:0.967]
Epoch [43/120    avg_loss:0.030, val_acc:0.979]
Epoch [44/120    avg_loss:0.041, val_acc:0.971]
Epoch [45/120    avg_loss:0.025, val_acc:0.967]
Epoch [46/120    avg_loss:0.022, val_acc:0.973]
Epoch [47/120    avg_loss:0.056, val_acc:0.950]
Epoch [48/120    avg_loss:0.044, val_acc:0.970]
Epoch [49/120    avg_loss:0.034, val_acc:0.971]
Epoch [50/120    avg_loss:0.026, val_acc:0.972]
Epoch [51/120    avg_loss:0.022, val_acc:0.967]
Epoch [52/120    avg_loss:0.034, val_acc:0.961]
Epoch [53/120    avg_loss:0.057, val_acc:0.921]
Epoch [54/120    avg_loss:0.052, val_acc:0.979]
Epoch [55/120    avg_loss:0.026, val_acc:0.976]
Epoch [56/120    avg_loss:0.020, val_acc:0.980]
Epoch [57/120    avg_loss:0.015, val_acc:0.980]
Epoch [58/120    avg_loss:0.019, val_acc:0.983]
Epoch [59/120    avg_loss:0.012, val_acc:0.986]
Epoch [60/120    avg_loss:0.007, val_acc:0.987]
Epoch [61/120    avg_loss:0.011, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.983]
Epoch [63/120    avg_loss:0.040, val_acc:0.967]
Epoch [64/120    avg_loss:0.020, val_acc:0.980]
Epoch [65/120    avg_loss:0.015, val_acc:0.984]
Epoch [66/120    avg_loss:0.021, val_acc:0.975]
Epoch [67/120    avg_loss:0.012, val_acc:0.975]
Epoch [68/120    avg_loss:0.027, val_acc:0.963]
Epoch [69/120    avg_loss:0.027, val_acc:0.973]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.008, val_acc:0.981]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.975]
Epoch [74/120    avg_loss:0.010, val_acc:0.985]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.004, val_acc:0.985]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.004, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.989]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     0     0     1     1    27     1]
 [    0     4 18073     0     4     0     0     0     9     0]
 [    0     3     0  1948     0     0     0     0    84     1]
 [    0    40    17     0  2899     0     1     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4858     0     5     0]
 [    0     0     0     0     0     0     3  1286     0     1]
 [    0     9     3    37    52     0     0     0  3462     8]
 [    0     0     0     0    10    43     0     0     0   866]]

Accuracy:
99.05044224326996

F1 scores:
[       nan 0.99332816 0.99856346 0.96891321 0.9765875  0.98379193
 0.99743353 0.99805976 0.96528649 0.96436526]

Kappa:
0.9874137235764054
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7311d6898>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.092, val_acc:0.325]
Epoch [2/120    avg_loss:1.605, val_acc:0.544]
Epoch [3/120    avg_loss:1.152, val_acc:0.695]
Epoch [4/120    avg_loss:0.873, val_acc:0.723]
Epoch [5/120    avg_loss:0.635, val_acc:0.800]
Epoch [6/120    avg_loss:0.483, val_acc:0.798]
Epoch [7/120    avg_loss:0.419, val_acc:0.815]
Epoch [8/120    avg_loss:0.409, val_acc:0.779]
Epoch [9/120    avg_loss:0.376, val_acc:0.859]
Epoch [10/120    avg_loss:0.392, val_acc:0.856]
Epoch [11/120    avg_loss:0.328, val_acc:0.876]
Epoch [12/120    avg_loss:0.279, val_acc:0.886]
Epoch [13/120    avg_loss:0.219, val_acc:0.928]
Epoch [14/120    avg_loss:0.200, val_acc:0.922]
Epoch [15/120    avg_loss:0.255, val_acc:0.916]
Epoch [16/120    avg_loss:0.154, val_acc:0.928]
Epoch [17/120    avg_loss:0.123, val_acc:0.961]
Epoch [18/120    avg_loss:0.117, val_acc:0.924]
Epoch [19/120    avg_loss:0.154, val_acc:0.940]
Epoch [20/120    avg_loss:0.129, val_acc:0.929]
Epoch [21/120    avg_loss:0.121, val_acc:0.952]
Epoch [22/120    avg_loss:0.088, val_acc:0.960]
Epoch [23/120    avg_loss:0.085, val_acc:0.944]
Epoch [24/120    avg_loss:0.089, val_acc:0.966]
Epoch [25/120    avg_loss:0.057, val_acc:0.971]
Epoch [26/120    avg_loss:0.064, val_acc:0.962]
Epoch [27/120    avg_loss:0.057, val_acc:0.954]
Epoch [28/120    avg_loss:0.060, val_acc:0.962]
Epoch [29/120    avg_loss:0.065, val_acc:0.968]
Epoch [30/120    avg_loss:0.036, val_acc:0.980]
Epoch [31/120    avg_loss:0.055, val_acc:0.974]
Epoch [32/120    avg_loss:0.050, val_acc:0.936]
Epoch [33/120    avg_loss:0.074, val_acc:0.959]
Epoch [34/120    avg_loss:0.050, val_acc:0.961]
Epoch [35/120    avg_loss:0.097, val_acc:0.953]
Epoch [36/120    avg_loss:0.058, val_acc:0.958]
Epoch [37/120    avg_loss:0.049, val_acc:0.969]
Epoch [38/120    avg_loss:0.039, val_acc:0.977]
Epoch [39/120    avg_loss:0.030, val_acc:0.978]
Epoch [40/120    avg_loss:0.024, val_acc:0.981]
Epoch [41/120    avg_loss:0.018, val_acc:0.982]
Epoch [42/120    avg_loss:0.045, val_acc:0.971]
Epoch [43/120    avg_loss:0.039, val_acc:0.981]
Epoch [44/120    avg_loss:0.029, val_acc:0.980]
Epoch [45/120    avg_loss:0.028, val_acc:0.969]
Epoch [46/120    avg_loss:0.019, val_acc:0.978]
Epoch [47/120    avg_loss:0.015, val_acc:0.983]
Epoch [48/120    avg_loss:0.013, val_acc:0.981]
Epoch [49/120    avg_loss:0.027, val_acc:0.964]
Epoch [50/120    avg_loss:0.029, val_acc:0.974]
Epoch [51/120    avg_loss:0.021, val_acc:0.982]
Epoch [52/120    avg_loss:0.020, val_acc:0.978]
Epoch [53/120    avg_loss:0.028, val_acc:0.980]
Epoch [54/120    avg_loss:0.017, val_acc:0.981]
Epoch [55/120    avg_loss:0.022, val_acc:0.969]
Epoch [56/120    avg_loss:0.017, val_acc:0.983]
Epoch [57/120    avg_loss:0.025, val_acc:0.980]
Epoch [58/120    avg_loss:0.065, val_acc:0.951]
Epoch [59/120    avg_loss:0.074, val_acc:0.973]
Epoch [60/120    avg_loss:0.026, val_acc:0.977]
Epoch [61/120    avg_loss:0.027, val_acc:0.971]
Epoch [62/120    avg_loss:0.011, val_acc:0.982]
Epoch [63/120    avg_loss:0.012, val_acc:0.982]
Epoch [64/120    avg_loss:0.010, val_acc:0.982]
Epoch [65/120    avg_loss:0.013, val_acc:0.982]
Epoch [66/120    avg_loss:0.011, val_acc:0.982]
Epoch [67/120    avg_loss:0.026, val_acc:0.981]
Epoch [68/120    avg_loss:0.043, val_acc:0.983]
Epoch [69/120    avg_loss:0.015, val_acc:0.984]
Epoch [70/120    avg_loss:0.012, val_acc:0.985]
Epoch [71/120    avg_loss:0.028, val_acc:0.973]
Epoch [72/120    avg_loss:0.022, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.963]
Epoch [74/120    avg_loss:0.014, val_acc:0.987]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.985]
Epoch [77/120    avg_loss:0.012, val_acc:0.982]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.982]
Epoch [80/120    avg_loss:0.029, val_acc:0.978]
Epoch [81/120    avg_loss:0.020, val_acc:0.987]
Epoch [82/120    avg_loss:0.018, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.012, val_acc:0.987]
Epoch [86/120    avg_loss:0.013, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.990]
Epoch [88/120    avg_loss:0.444, val_acc:0.651]
Epoch [89/120    avg_loss:0.535, val_acc:0.794]
Epoch [90/120    avg_loss:0.307, val_acc:0.878]
Epoch [91/120    avg_loss:0.186, val_acc:0.907]
Epoch [92/120    avg_loss:0.203, val_acc:0.952]
Epoch [93/120    avg_loss:0.105, val_acc:0.925]
Epoch [94/120    avg_loss:0.119, val_acc:0.952]
Epoch [95/120    avg_loss:0.035, val_acc:0.977]
Epoch [96/120    avg_loss:0.043, val_acc:0.979]
Epoch [97/120    avg_loss:0.035, val_acc:0.973]
Epoch [98/120    avg_loss:0.035, val_acc:0.977]
Epoch [99/120    avg_loss:0.023, val_acc:0.979]
Epoch [100/120    avg_loss:0.056, val_acc:0.969]
Epoch [101/120    avg_loss:0.041, val_acc:0.980]
Epoch [102/120    avg_loss:0.026, val_acc:0.984]
Epoch [103/120    avg_loss:0.024, val_acc:0.987]
Epoch [104/120    avg_loss:0.021, val_acc:0.987]
Epoch [105/120    avg_loss:0.020, val_acc:0.987]
Epoch [106/120    avg_loss:0.018, val_acc:0.987]
Epoch [107/120    avg_loss:0.015, val_acc:0.987]
Epoch [108/120    avg_loss:0.015, val_acc:0.987]
Epoch [109/120    avg_loss:0.016, val_acc:0.987]
Epoch [110/120    avg_loss:0.012, val_acc:0.987]
Epoch [111/120    avg_loss:0.015, val_acc:0.987]
Epoch [112/120    avg_loss:0.016, val_acc:0.987]
Epoch [113/120    avg_loss:0.014, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.987]
Epoch [115/120    avg_loss:0.015, val_acc:0.987]
Epoch [116/120    avg_loss:0.013, val_acc:0.987]
Epoch [117/120    avg_loss:0.014, val_acc:0.987]
Epoch [118/120    avg_loss:0.015, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.987]
Epoch [120/120    avg_loss:0.016, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     1     0     0     2    11     4]
 [    0     0 17903     0   158     0    26     0     3     0]
 [    0    10     0  2014     0     0     0     0    11     1]
 [    0    40    12     0  2892     0     5     0    22     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4873     0     3     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    21     0    32    67     0     0     0  3436    15]
 [    0     0     0     0     6    45     0     0     0   868]]

Accuracy:
98.79738751114645

F1 scores:
[       nan 0.99310986 0.99447299 0.98628795 0.9488189  0.98305085
 0.99631977 0.99883766 0.97378489 0.95964621]

Kappa:
0.9840951628067709
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1b17caac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.054, val_acc:0.492]
Epoch [2/120    avg_loss:1.676, val_acc:0.614]
Epoch [3/120    avg_loss:1.262, val_acc:0.698]
Epoch [4/120    avg_loss:0.882, val_acc:0.761]
Epoch [5/120    avg_loss:0.726, val_acc:0.764]
Epoch [6/120    avg_loss:0.589, val_acc:0.747]
Epoch [7/120    avg_loss:0.459, val_acc:0.804]
Epoch [8/120    avg_loss:0.405, val_acc:0.881]
Epoch [9/120    avg_loss:0.357, val_acc:0.841]
Epoch [10/120    avg_loss:0.291, val_acc:0.906]
Epoch [11/120    avg_loss:0.286, val_acc:0.871]
Epoch [12/120    avg_loss:0.279, val_acc:0.923]
Epoch [13/120    avg_loss:0.484, val_acc:0.826]
Epoch [14/120    avg_loss:0.308, val_acc:0.889]
Epoch [15/120    avg_loss:0.275, val_acc:0.911]
Epoch [16/120    avg_loss:0.220, val_acc:0.927]
Epoch [17/120    avg_loss:0.174, val_acc:0.949]
Epoch [18/120    avg_loss:0.121, val_acc:0.944]
Epoch [19/120    avg_loss:0.139, val_acc:0.948]
Epoch [20/120    avg_loss:0.133, val_acc:0.938]
Epoch [21/120    avg_loss:0.117, val_acc:0.950]
Epoch [22/120    avg_loss:0.100, val_acc:0.957]
Epoch [23/120    avg_loss:0.093, val_acc:0.934]
Epoch [24/120    avg_loss:0.103, val_acc:0.926]
Epoch [25/120    avg_loss:0.101, val_acc:0.908]
Epoch [26/120    avg_loss:0.069, val_acc:0.958]
Epoch [27/120    avg_loss:0.078, val_acc:0.968]
Epoch [28/120    avg_loss:0.059, val_acc:0.959]
Epoch [29/120    avg_loss:0.053, val_acc:0.964]
Epoch [30/120    avg_loss:0.060, val_acc:0.968]
Epoch [31/120    avg_loss:0.069, val_acc:0.972]
Epoch [32/120    avg_loss:0.050, val_acc:0.968]
Epoch [33/120    avg_loss:0.068, val_acc:0.971]
Epoch [34/120    avg_loss:0.031, val_acc:0.973]
Epoch [35/120    avg_loss:0.033, val_acc:0.975]
Epoch [36/120    avg_loss:0.091, val_acc:0.950]
Epoch [37/120    avg_loss:0.071, val_acc:0.973]
Epoch [38/120    avg_loss:0.057, val_acc:0.968]
Epoch [39/120    avg_loss:0.030, val_acc:0.975]
Epoch [40/120    avg_loss:0.314, val_acc:0.717]
Epoch [41/120    avg_loss:0.276, val_acc:0.954]
Epoch [42/120    avg_loss:0.170, val_acc:0.835]
Epoch [43/120    avg_loss:0.226, val_acc:0.939]
Epoch [44/120    avg_loss:0.086, val_acc:0.958]
Epoch [45/120    avg_loss:0.066, val_acc:0.961]
Epoch [46/120    avg_loss:0.082, val_acc:0.978]
Epoch [47/120    avg_loss:0.051, val_acc:0.961]
Epoch [48/120    avg_loss:0.039, val_acc:0.973]
Epoch [49/120    avg_loss:0.026, val_acc:0.967]
Epoch [50/120    avg_loss:0.045, val_acc:0.972]
Epoch [51/120    avg_loss:0.049, val_acc:0.850]
Epoch [52/120    avg_loss:0.112, val_acc:0.973]
Epoch [53/120    avg_loss:0.039, val_acc:0.981]
Epoch [54/120    avg_loss:0.026, val_acc:0.968]
Epoch [55/120    avg_loss:0.034, val_acc:0.963]
Epoch [56/120    avg_loss:0.022, val_acc:0.974]
Epoch [57/120    avg_loss:0.027, val_acc:0.971]
Epoch [58/120    avg_loss:0.499, val_acc:0.512]
Epoch [59/120    avg_loss:1.443, val_acc:0.624]
Epoch [60/120    avg_loss:1.142, val_acc:0.700]
Epoch [61/120    avg_loss:1.038, val_acc:0.701]
Epoch [62/120    avg_loss:0.902, val_acc:0.686]
Epoch [63/120    avg_loss:0.825, val_acc:0.728]
Epoch [64/120    avg_loss:0.758, val_acc:0.723]
Epoch [65/120    avg_loss:0.702, val_acc:0.754]
Epoch [66/120    avg_loss:0.628, val_acc:0.771]
Epoch [67/120    avg_loss:0.556, val_acc:0.770]
Epoch [68/120    avg_loss:0.511, val_acc:0.767]
Epoch [69/120    avg_loss:0.578, val_acc:0.761]
Epoch [70/120    avg_loss:0.528, val_acc:0.762]
Epoch [71/120    avg_loss:0.477, val_acc:0.767]
Epoch [72/120    avg_loss:0.542, val_acc:0.773]
Epoch [73/120    avg_loss:0.519, val_acc:0.777]
Epoch [74/120    avg_loss:0.491, val_acc:0.778]
Epoch [75/120    avg_loss:0.479, val_acc:0.787]
Epoch [76/120    avg_loss:0.505, val_acc:0.786]
Epoch [77/120    avg_loss:0.470, val_acc:0.784]
Epoch [78/120    avg_loss:0.479, val_acc:0.787]
Epoch [79/120    avg_loss:0.478, val_acc:0.789]
Epoch [80/120    avg_loss:0.485, val_acc:0.788]
Epoch [81/120    avg_loss:0.465, val_acc:0.791]
Epoch [82/120    avg_loss:0.441, val_acc:0.789]
Epoch [83/120    avg_loss:0.482, val_acc:0.791]
Epoch [84/120    avg_loss:0.462, val_acc:0.787]
Epoch [85/120    avg_loss:0.448, val_acc:0.789]
Epoch [86/120    avg_loss:0.459, val_acc:0.786]
Epoch [87/120    avg_loss:0.495, val_acc:0.786]
Epoch [88/120    avg_loss:0.440, val_acc:0.786]
Epoch [89/120    avg_loss:0.463, val_acc:0.784]
Epoch [90/120    avg_loss:0.468, val_acc:0.787]
Epoch [91/120    avg_loss:0.459, val_acc:0.786]
Epoch [92/120    avg_loss:0.460, val_acc:0.788]
Epoch [93/120    avg_loss:0.508, val_acc:0.788]
Epoch [94/120    avg_loss:0.460, val_acc:0.787]
Epoch [95/120    avg_loss:0.465, val_acc:0.787]
Epoch [96/120    avg_loss:0.479, val_acc:0.787]
Epoch [97/120    avg_loss:0.453, val_acc:0.787]
Epoch [98/120    avg_loss:0.479, val_acc:0.788]
Epoch [99/120    avg_loss:0.508, val_acc:0.788]
Epoch [100/120    avg_loss:0.436, val_acc:0.786]
Epoch [101/120    avg_loss:0.455, val_acc:0.787]
Epoch [102/120    avg_loss:0.447, val_acc:0.787]
Epoch [103/120    avg_loss:0.483, val_acc:0.787]
Epoch [104/120    avg_loss:0.471, val_acc:0.787]
Epoch [105/120    avg_loss:0.460, val_acc:0.787]
Epoch [106/120    avg_loss:0.460, val_acc:0.787]
Epoch [107/120    avg_loss:0.457, val_acc:0.786]
Epoch [108/120    avg_loss:0.436, val_acc:0.787]
Epoch [109/120    avg_loss:0.450, val_acc:0.787]
Epoch [110/120    avg_loss:0.488, val_acc:0.786]
Epoch [111/120    avg_loss:0.476, val_acc:0.786]
Epoch [112/120    avg_loss:0.468, val_acc:0.786]
Epoch [113/120    avg_loss:0.452, val_acc:0.786]
Epoch [114/120    avg_loss:0.483, val_acc:0.787]
Epoch [115/120    avg_loss:0.429, val_acc:0.787]
Epoch [116/120    avg_loss:0.491, val_acc:0.787]
Epoch [117/120    avg_loss:0.437, val_acc:0.787]
Epoch [118/120    avg_loss:0.486, val_acc:0.787]
Epoch [119/120    avg_loss:0.467, val_acc:0.787]
Epoch [120/120    avg_loss:0.478, val_acc:0.787]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5090    10    33   155     0   253   348   433   110]
 [    0     0 16849     0   126     0  1115     0     0     0]
 [    0    24     0  1779     7     0     4     0   161    61]
 [    0   127    67     0  2500     0   190     0    60    28]
 [    0     0     0     0     0  1302     0     3     0     0]
 [    0     0  1486    42    10     0  3194     0   146     0]
 [    0   119     0     0     0     0     1  1170     0     0]
 [    0   111     1   137    20     0    80     0  3209    13]
 [    0    46     0     1    14    99     0     3     1   755]]

Accuracy:
86.39529559202758

F1 scores:
[       nan 0.85195414 0.923157   0.88331678 0.86147484 0.96230599
 0.65753989 0.8315565  0.84659016 0.80063627]

Kappa:
0.8195501467333867
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b6cb39be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.968, val_acc:0.267]
Epoch [2/120    avg_loss:1.533, val_acc:0.594]
Epoch [3/120    avg_loss:1.184, val_acc:0.682]
Epoch [4/120    avg_loss:0.873, val_acc:0.700]
Epoch [5/120    avg_loss:0.732, val_acc:0.751]
Epoch [6/120    avg_loss:0.566, val_acc:0.757]
Epoch [7/120    avg_loss:0.530, val_acc:0.747]
Epoch [8/120    avg_loss:0.453, val_acc:0.829]
Epoch [9/120    avg_loss:0.583, val_acc:0.795]
Epoch [10/120    avg_loss:0.349, val_acc:0.884]
Epoch [11/120    avg_loss:0.265, val_acc:0.874]
Epoch [12/120    avg_loss:0.307, val_acc:0.886]
Epoch [13/120    avg_loss:0.266, val_acc:0.875]
Epoch [14/120    avg_loss:0.243, val_acc:0.907]
Epoch [15/120    avg_loss:0.241, val_acc:0.873]
Epoch [16/120    avg_loss:1.117, val_acc:0.325]
Epoch [17/120    avg_loss:1.344, val_acc:0.685]
Epoch [18/120    avg_loss:1.028, val_acc:0.697]
Epoch [19/120    avg_loss:0.854, val_acc:0.704]
Epoch [20/120    avg_loss:0.769, val_acc:0.753]
Epoch [21/120    avg_loss:0.701, val_acc:0.753]
Epoch [22/120    avg_loss:0.629, val_acc:0.761]
Epoch [23/120    avg_loss:0.629, val_acc:0.781]
Epoch [24/120    avg_loss:0.604, val_acc:0.791]
Epoch [25/120    avg_loss:0.488, val_acc:0.803]
Epoch [26/120    avg_loss:0.460, val_acc:0.840]
Epoch [27/120    avg_loss:0.423, val_acc:0.834]
Epoch [28/120    avg_loss:0.413, val_acc:0.845]
Epoch [29/120    avg_loss:0.396, val_acc:0.848]
Epoch [30/120    avg_loss:0.354, val_acc:0.843]
Epoch [31/120    avg_loss:0.336, val_acc:0.857]
Epoch [32/120    avg_loss:0.354, val_acc:0.855]
Epoch [33/120    avg_loss:0.323, val_acc:0.866]
Epoch [34/120    avg_loss:0.339, val_acc:0.860]
Epoch [35/120    avg_loss:0.326, val_acc:0.856]
Epoch [36/120    avg_loss:0.335, val_acc:0.856]
Epoch [37/120    avg_loss:0.303, val_acc:0.853]
Epoch [38/120    avg_loss:0.325, val_acc:0.849]
Epoch [39/120    avg_loss:0.318, val_acc:0.861]
Epoch [40/120    avg_loss:0.325, val_acc:0.859]
Epoch [41/120    avg_loss:0.296, val_acc:0.858]
Epoch [42/120    avg_loss:0.310, val_acc:0.861]
Epoch [43/120    avg_loss:0.285, val_acc:0.865]
Epoch [44/120    avg_loss:0.311, val_acc:0.866]
Epoch [45/120    avg_loss:0.300, val_acc:0.862]
Epoch [46/120    avg_loss:0.311, val_acc:0.858]
Epoch [47/120    avg_loss:0.320, val_acc:0.864]
Epoch [48/120    avg_loss:0.285, val_acc:0.862]
Epoch [49/120    avg_loss:0.302, val_acc:0.863]
Epoch [50/120    avg_loss:0.322, val_acc:0.867]
Epoch [51/120    avg_loss:0.292, val_acc:0.867]
Epoch [52/120    avg_loss:0.288, val_acc:0.872]
Epoch [53/120    avg_loss:0.277, val_acc:0.870]
Epoch [54/120    avg_loss:0.299, val_acc:0.870]
Epoch [55/120    avg_loss:0.300, val_acc:0.870]
Epoch [56/120    avg_loss:0.300, val_acc:0.870]
Epoch [57/120    avg_loss:0.298, val_acc:0.870]
Epoch [58/120    avg_loss:0.282, val_acc:0.869]
Epoch [59/120    avg_loss:0.290, val_acc:0.867]
Epoch [60/120    avg_loss:0.310, val_acc:0.870]
Epoch [61/120    avg_loss:0.297, val_acc:0.867]
Epoch [62/120    avg_loss:0.292, val_acc:0.869]
Epoch [63/120    avg_loss:0.290, val_acc:0.867]
Epoch [64/120    avg_loss:0.298, val_acc:0.867]
Epoch [65/120    avg_loss:0.313, val_acc:0.867]
Epoch [66/120    avg_loss:0.299, val_acc:0.865]
Epoch [67/120    avg_loss:0.306, val_acc:0.866]
Epoch [68/120    avg_loss:0.299, val_acc:0.866]
Epoch [69/120    avg_loss:0.289, val_acc:0.865]
Epoch [70/120    avg_loss:0.288, val_acc:0.865]
Epoch [71/120    avg_loss:0.312, val_acc:0.865]
Epoch [72/120    avg_loss:0.305, val_acc:0.865]
Epoch [73/120    avg_loss:0.272, val_acc:0.865]
Epoch [74/120    avg_loss:0.296, val_acc:0.866]
Epoch [75/120    avg_loss:0.311, val_acc:0.867]
Epoch [76/120    avg_loss:0.290, val_acc:0.867]
Epoch [77/120    avg_loss:0.292, val_acc:0.867]
Epoch [78/120    avg_loss:0.313, val_acc:0.865]
Epoch [79/120    avg_loss:0.289, val_acc:0.867]
Epoch [80/120    avg_loss:0.306, val_acc:0.867]
Epoch [81/120    avg_loss:0.309, val_acc:0.867]
Epoch [82/120    avg_loss:0.302, val_acc:0.867]
Epoch [83/120    avg_loss:0.313, val_acc:0.867]
Epoch [84/120    avg_loss:0.303, val_acc:0.867]
Epoch [85/120    avg_loss:0.304, val_acc:0.867]
Epoch [86/120    avg_loss:0.312, val_acc:0.867]
Epoch [87/120    avg_loss:0.326, val_acc:0.867]
Epoch [88/120    avg_loss:0.310, val_acc:0.867]
Epoch [89/120    avg_loss:0.290, val_acc:0.867]
Epoch [90/120    avg_loss:0.281, val_acc:0.867]
Epoch [91/120    avg_loss:0.285, val_acc:0.867]
Epoch [92/120    avg_loss:0.296, val_acc:0.867]
Epoch [93/120    avg_loss:0.302, val_acc:0.867]
Epoch [94/120    avg_loss:0.290, val_acc:0.867]
Epoch [95/120    avg_loss:0.307, val_acc:0.867]
Epoch [96/120    avg_loss:0.285, val_acc:0.867]
Epoch [97/120    avg_loss:0.303, val_acc:0.867]
Epoch [98/120    avg_loss:0.308, val_acc:0.867]
Epoch [99/120    avg_loss:0.297, val_acc:0.867]
Epoch [100/120    avg_loss:0.299, val_acc:0.867]
Epoch [101/120    avg_loss:0.285, val_acc:0.867]
Epoch [102/120    avg_loss:0.293, val_acc:0.867]
Epoch [103/120    avg_loss:0.297, val_acc:0.867]
Epoch [104/120    avg_loss:0.303, val_acc:0.867]
Epoch [105/120    avg_loss:0.316, val_acc:0.867]
Epoch [106/120    avg_loss:0.301, val_acc:0.867]
Epoch [107/120    avg_loss:0.293, val_acc:0.867]
Epoch [108/120    avg_loss:0.300, val_acc:0.867]
Epoch [109/120    avg_loss:0.310, val_acc:0.867]
Epoch [110/120    avg_loss:0.301, val_acc:0.867]
Epoch [111/120    avg_loss:0.292, val_acc:0.867]
Epoch [112/120    avg_loss:0.307, val_acc:0.867]
Epoch [113/120    avg_loss:0.278, val_acc:0.867]
Epoch [114/120    avg_loss:0.297, val_acc:0.867]
Epoch [115/120    avg_loss:0.328, val_acc:0.867]
Epoch [116/120    avg_loss:0.311, val_acc:0.867]
Epoch [117/120    avg_loss:0.315, val_acc:0.867]
Epoch [118/120    avg_loss:0.280, val_acc:0.867]
Epoch [119/120    avg_loss:0.326, val_acc:0.867]
Epoch [120/120    avg_loss:0.293, val_acc:0.867]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5733     2    78   163     0     0    53   298   105]
 [    0     0 14530     0   111     0  3449     0     0     0]
 [    0    13     0  1895     2     0     0     0   106    20]
 [    0   134    73     1  2564     0   160     0    40     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   120    11     8     0  4669     0    70     0]
 [    0    65     0     0     0     0    10  1215     0     0]
 [    0    78     0    59    47     0    30     0  3357     0]
 [    0    37     0     1    22    30     0     0     0   829]]

Accuracy:
86.99539681392042

F1 scores:
[       nan 0.91786744 0.88557062 0.92869395 0.87077602 0.98863636
 0.70763868 0.94996091 0.90217683 0.88521089]

Kappa:
0.8335133314557078
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd5d57d5b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.053, val_acc:0.545]
Epoch [2/120    avg_loss:1.565, val_acc:0.564]
Epoch [3/120    avg_loss:1.181, val_acc:0.668]
Epoch [4/120    avg_loss:0.839, val_acc:0.708]
Epoch [5/120    avg_loss:0.634, val_acc:0.800]
Epoch [6/120    avg_loss:0.505, val_acc:0.793]
Epoch [7/120    avg_loss:0.416, val_acc:0.849]
Epoch [8/120    avg_loss:0.390, val_acc:0.876]
Epoch [9/120    avg_loss:0.480, val_acc:0.895]
Epoch [10/120    avg_loss:0.298, val_acc:0.878]
Epoch [11/120    avg_loss:0.273, val_acc:0.914]
Epoch [12/120    avg_loss:0.242, val_acc:0.908]
Epoch [13/120    avg_loss:0.204, val_acc:0.922]
Epoch [14/120    avg_loss:0.190, val_acc:0.933]
Epoch [15/120    avg_loss:0.149, val_acc:0.940]
Epoch [16/120    avg_loss:0.123, val_acc:0.940]
Epoch [17/120    avg_loss:0.136, val_acc:0.899]
Epoch [18/120    avg_loss:0.132, val_acc:0.946]
Epoch [19/120    avg_loss:0.113, val_acc:0.923]
Epoch [20/120    avg_loss:0.087, val_acc:0.961]
Epoch [21/120    avg_loss:0.085, val_acc:0.939]
Epoch [22/120    avg_loss:0.069, val_acc:0.927]
Epoch [23/120    avg_loss:0.134, val_acc:0.906]
Epoch [24/120    avg_loss:0.779, val_acc:0.648]
Epoch [25/120    avg_loss:0.704, val_acc:0.811]
Epoch [26/120    avg_loss:0.493, val_acc:0.810]
Epoch [27/120    avg_loss:0.385, val_acc:0.861]
Epoch [28/120    avg_loss:0.350, val_acc:0.832]
Epoch [29/120    avg_loss:0.333, val_acc:0.825]
Epoch [30/120    avg_loss:0.269, val_acc:0.906]
Epoch [31/120    avg_loss:0.230, val_acc:0.910]
Epoch [32/120    avg_loss:0.231, val_acc:0.827]
Epoch [33/120    avg_loss:0.325, val_acc:0.890]
Epoch [34/120    avg_loss:0.223, val_acc:0.884]
Epoch [35/120    avg_loss:0.195, val_acc:0.899]
Epoch [36/120    avg_loss:0.190, val_acc:0.904]
Epoch [37/120    avg_loss:0.175, val_acc:0.908]
Epoch [38/120    avg_loss:0.164, val_acc:0.911]
Epoch [39/120    avg_loss:0.163, val_acc:0.911]
Epoch [40/120    avg_loss:0.170, val_acc:0.918]
Epoch [41/120    avg_loss:0.159, val_acc:0.919]
Epoch [42/120    avg_loss:0.161, val_acc:0.920]
Epoch [43/120    avg_loss:0.144, val_acc:0.928]
Epoch [44/120    avg_loss:0.137, val_acc:0.922]
Epoch [45/120    avg_loss:0.139, val_acc:0.929]
Epoch [46/120    avg_loss:0.127, val_acc:0.932]
Epoch [47/120    avg_loss:0.133, val_acc:0.930]
Epoch [48/120    avg_loss:0.132, val_acc:0.933]
Epoch [49/120    avg_loss:0.126, val_acc:0.934]
Epoch [50/120    avg_loss:0.109, val_acc:0.935]
Epoch [51/120    avg_loss:0.137, val_acc:0.933]
Epoch [52/120    avg_loss:0.120, val_acc:0.933]
Epoch [53/120    avg_loss:0.121, val_acc:0.929]
Epoch [54/120    avg_loss:0.118, val_acc:0.929]
Epoch [55/120    avg_loss:0.127, val_acc:0.928]
Epoch [56/120    avg_loss:0.127, val_acc:0.931]
Epoch [57/120    avg_loss:0.123, val_acc:0.936]
Epoch [58/120    avg_loss:0.118, val_acc:0.936]
Epoch [59/120    avg_loss:0.144, val_acc:0.936]
Epoch [60/120    avg_loss:0.127, val_acc:0.936]
Epoch [61/120    avg_loss:0.124, val_acc:0.936]
Epoch [62/120    avg_loss:0.115, val_acc:0.937]
Epoch [63/120    avg_loss:0.111, val_acc:0.936]
Epoch [64/120    avg_loss:0.116, val_acc:0.937]
Epoch [65/120    avg_loss:0.111, val_acc:0.937]
Epoch [66/120    avg_loss:0.118, val_acc:0.937]
Epoch [67/120    avg_loss:0.117, val_acc:0.936]
Epoch [68/120    avg_loss:0.114, val_acc:0.936]
Epoch [69/120    avg_loss:0.120, val_acc:0.936]
Epoch [70/120    avg_loss:0.124, val_acc:0.936]
Epoch [71/120    avg_loss:0.113, val_acc:0.936]
Epoch [72/120    avg_loss:0.113, val_acc:0.936]
Epoch [73/120    avg_loss:0.111, val_acc:0.936]
Epoch [74/120    avg_loss:0.102, val_acc:0.936]
Epoch [75/120    avg_loss:0.123, val_acc:0.936]
Epoch [76/120    avg_loss:0.139, val_acc:0.936]
Epoch [77/120    avg_loss:0.133, val_acc:0.936]
Epoch [78/120    avg_loss:0.118, val_acc:0.936]
Epoch [79/120    avg_loss:0.120, val_acc:0.936]
Epoch [80/120    avg_loss:0.129, val_acc:0.936]
Epoch [81/120    avg_loss:0.129, val_acc:0.936]
Epoch [82/120    avg_loss:0.130, val_acc:0.936]
Epoch [83/120    avg_loss:0.111, val_acc:0.936]
Epoch [84/120    avg_loss:0.123, val_acc:0.936]
Epoch [85/120    avg_loss:0.114, val_acc:0.936]
Epoch [86/120    avg_loss:0.127, val_acc:0.936]
Epoch [87/120    avg_loss:0.122, val_acc:0.936]
Epoch [88/120    avg_loss:0.125, val_acc:0.936]
Epoch [89/120    avg_loss:0.140, val_acc:0.936]
Epoch [90/120    avg_loss:0.116, val_acc:0.936]
Epoch [91/120    avg_loss:0.131, val_acc:0.936]
Epoch [92/120    avg_loss:0.124, val_acc:0.936]
Epoch [93/120    avg_loss:0.119, val_acc:0.936]
Epoch [94/120    avg_loss:0.122, val_acc:0.936]
Epoch [95/120    avg_loss:0.127, val_acc:0.936]
Epoch [96/120    avg_loss:0.131, val_acc:0.936]
Epoch [97/120    avg_loss:0.121, val_acc:0.936]
Epoch [98/120    avg_loss:0.124, val_acc:0.936]
Epoch [99/120    avg_loss:0.129, val_acc:0.936]
Epoch [100/120    avg_loss:0.120, val_acc:0.936]
Epoch [101/120    avg_loss:0.119, val_acc:0.936]
Epoch [102/120    avg_loss:0.121, val_acc:0.936]
Epoch [103/120    avg_loss:0.128, val_acc:0.936]
Epoch [104/120    avg_loss:0.133, val_acc:0.936]
Epoch [105/120    avg_loss:0.124, val_acc:0.936]
Epoch [106/120    avg_loss:0.113, val_acc:0.936]
Epoch [107/120    avg_loss:0.110, val_acc:0.936]
Epoch [108/120    avg_loss:0.119, val_acc:0.936]
Epoch [109/120    avg_loss:0.110, val_acc:0.936]
Epoch [110/120    avg_loss:0.119, val_acc:0.936]
Epoch [111/120    avg_loss:0.133, val_acc:0.936]
Epoch [112/120    avg_loss:0.110, val_acc:0.936]
Epoch [113/120    avg_loss:0.124, val_acc:0.936]
Epoch [114/120    avg_loss:0.118, val_acc:0.936]
Epoch [115/120    avg_loss:0.145, val_acc:0.936]
Epoch [116/120    avg_loss:0.123, val_acc:0.936]
Epoch [117/120    avg_loss:0.116, val_acc:0.936]
Epoch [118/120    avg_loss:0.109, val_acc:0.936]
Epoch [119/120    avg_loss:0.113, val_acc:0.936]
Epoch [120/120    avg_loss:0.120, val_acc:0.936]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5946     0     0   145     0     0    85   230    26]
 [    0    19 16896     0   100     0  1074     0     1     0]
 [    0    20     0  1932    11     0     0     0    60    13]
 [    0    62    18     0  2840     0    14     0    31     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    38    14     0     0  4803     0    23     0]
 [    0    25     0     0     0     0     4  1260     0     1]
 [    0    59     0    83    57     0     0     0  3372     0]
 [    0     0     0     4     7   111     3     0     3   791]]

Accuracy:
94.34121418070518

F1 scores:
[       nan 0.94658919 0.96432852 0.94961907 0.92628832 0.95920617
 0.89142539 0.95635674 0.924976   0.90039841]

Kappa:
0.9259478468522594
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d823e9b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.117, val_acc:0.241]
Epoch [2/120    avg_loss:1.706, val_acc:0.346]
Epoch [3/120    avg_loss:1.331, val_acc:0.669]
Epoch [4/120    avg_loss:1.041, val_acc:0.659]
Epoch [5/120    avg_loss:0.785, val_acc:0.722]
Epoch [6/120    avg_loss:0.680, val_acc:0.784]
Epoch [7/120    avg_loss:0.586, val_acc:0.765]
Epoch [8/120    avg_loss:0.488, val_acc:0.853]
Epoch [9/120    avg_loss:0.454, val_acc:0.838]
Epoch [10/120    avg_loss:0.390, val_acc:0.783]
Epoch [11/120    avg_loss:0.362, val_acc:0.873]
Epoch [12/120    avg_loss:0.378, val_acc:0.897]
Epoch [13/120    avg_loss:0.260, val_acc:0.909]
Epoch [14/120    avg_loss:0.259, val_acc:0.889]
Epoch [15/120    avg_loss:0.215, val_acc:0.949]
Epoch [16/120    avg_loss:0.235, val_acc:0.937]
Epoch [17/120    avg_loss:0.211, val_acc:0.909]
Epoch [18/120    avg_loss:0.160, val_acc:0.920]
Epoch [19/120    avg_loss:0.147, val_acc:0.928]
Epoch [20/120    avg_loss:0.122, val_acc:0.944]
Epoch [21/120    avg_loss:0.128, val_acc:0.919]
Epoch [22/120    avg_loss:0.137, val_acc:0.930]
Epoch [23/120    avg_loss:0.123, val_acc:0.959]
Epoch [24/120    avg_loss:0.102, val_acc:0.941]
Epoch [25/120    avg_loss:0.108, val_acc:0.963]
Epoch [26/120    avg_loss:0.090, val_acc:0.966]
Epoch [27/120    avg_loss:0.149, val_acc:0.954]
Epoch [28/120    avg_loss:0.103, val_acc:0.890]
Epoch [29/120    avg_loss:0.079, val_acc:0.973]
Epoch [30/120    avg_loss:0.088, val_acc:0.978]
Epoch [31/120    avg_loss:0.064, val_acc:0.969]
Epoch [32/120    avg_loss:0.050, val_acc:0.984]
Epoch [33/120    avg_loss:0.052, val_acc:0.961]
Epoch [34/120    avg_loss:0.090, val_acc:0.939]
Epoch [35/120    avg_loss:0.066, val_acc:0.982]
Epoch [36/120    avg_loss:0.047, val_acc:0.975]
Epoch [37/120    avg_loss:0.070, val_acc:0.967]
Epoch [38/120    avg_loss:0.053, val_acc:0.962]
Epoch [39/120    avg_loss:0.041, val_acc:0.984]
Epoch [40/120    avg_loss:0.029, val_acc:0.989]
Epoch [41/120    avg_loss:0.041, val_acc:0.985]
Epoch [42/120    avg_loss:0.030, val_acc:0.981]
Epoch [43/120    avg_loss:0.042, val_acc:0.970]
Epoch [44/120    avg_loss:0.041, val_acc:0.956]
Epoch [45/120    avg_loss:0.043, val_acc:0.977]
Epoch [46/120    avg_loss:0.027, val_acc:0.973]
Epoch [47/120    avg_loss:0.026, val_acc:0.983]
Epoch [48/120    avg_loss:0.036, val_acc:0.988]
Epoch [49/120    avg_loss:0.025, val_acc:0.984]
Epoch [50/120    avg_loss:0.015, val_acc:0.986]
Epoch [51/120    avg_loss:0.014, val_acc:0.988]
Epoch [52/120    avg_loss:0.017, val_acc:0.986]
Epoch [53/120    avg_loss:0.018, val_acc:0.988]
Epoch [54/120    avg_loss:0.014, val_acc:0.987]
Epoch [55/120    avg_loss:0.016, val_acc:0.986]
Epoch [56/120    avg_loss:0.010, val_acc:0.985]
Epoch [57/120    avg_loss:0.009, val_acc:0.987]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.988]
Epoch [60/120    avg_loss:0.010, val_acc:0.990]
Epoch [61/120    avg_loss:0.008, val_acc:0.990]
Epoch [62/120    avg_loss:0.010, val_acc:0.990]
Epoch [63/120    avg_loss:0.012, val_acc:0.990]
Epoch [64/120    avg_loss:0.009, val_acc:0.990]
Epoch [65/120    avg_loss:0.012, val_acc:0.987]
Epoch [66/120    avg_loss:0.010, val_acc:0.990]
Epoch [67/120    avg_loss:0.011, val_acc:0.991]
Epoch [68/120    avg_loss:0.010, val_acc:0.991]
Epoch [69/120    avg_loss:0.010, val_acc:0.991]
Epoch [70/120    avg_loss:0.010, val_acc:0.990]
Epoch [71/120    avg_loss:0.008, val_acc:0.990]
Epoch [72/120    avg_loss:0.010, val_acc:0.991]
Epoch [73/120    avg_loss:0.013, val_acc:0.991]
Epoch [74/120    avg_loss:0.009, val_acc:0.991]
Epoch [75/120    avg_loss:0.008, val_acc:0.991]
Epoch [76/120    avg_loss:0.009, val_acc:0.991]
Epoch [77/120    avg_loss:0.011, val_acc:0.992]
Epoch [78/120    avg_loss:0.008, val_acc:0.993]
Epoch [79/120    avg_loss:0.009, val_acc:0.993]
Epoch [80/120    avg_loss:0.011, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.992]
Epoch [84/120    avg_loss:0.009, val_acc:0.992]
Epoch [85/120    avg_loss:0.007, val_acc:0.991]
Epoch [86/120    avg_loss:0.009, val_acc:0.991]
Epoch [87/120    avg_loss:0.010, val_acc:0.991]
Epoch [88/120    avg_loss:0.010, val_acc:0.992]
Epoch [89/120    avg_loss:0.009, val_acc:0.993]
Epoch [90/120    avg_loss:0.010, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.993]
Epoch [92/120    avg_loss:0.007, val_acc:0.993]
Epoch [93/120    avg_loss:0.007, val_acc:0.993]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.013, val_acc:0.990]
Epoch [96/120    avg_loss:0.012, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.007, val_acc:0.991]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.009, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.006, val_acc:0.991]
Epoch [105/120    avg_loss:0.008, val_acc:0.991]
Epoch [106/120    avg_loss:0.009, val_acc:0.992]
Epoch [107/120    avg_loss:0.010, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.007, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.009, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.008, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     2     3     0     0     3    39     3]
 [    0     3 18052     0    33     0     0     0     2     0]
 [    0     2     0  2024     3     0     0     0     5     2]
 [    0    36    18     0  2909     0     0     0     9     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    12     0     0  4858     0     8     0]
 [    0     2     0     0     0     0     2  1284     0     2]
 [    0    20     0    15    66     0     0     0  3470     0]
 [    0     0     1     0    16    88     0     0     0   814]]

Accuracy:
99.04803219820211

F1 scores:
[       nan 0.99122466 0.99842372 0.9899731  0.96934355 0.96738325
 0.99774081 0.99650757 0.97691441 0.93563218]

Kappa:
0.9873883335261895
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a74494b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.032, val_acc:0.213]
Epoch [2/120    avg_loss:1.625, val_acc:0.430]
Epoch [3/120    avg_loss:1.310, val_acc:0.562]
Epoch [4/120    avg_loss:1.039, val_acc:0.688]
Epoch [5/120    avg_loss:0.754, val_acc:0.742]
Epoch [6/120    avg_loss:0.578, val_acc:0.788]
Epoch [7/120    avg_loss:0.495, val_acc:0.815]
Epoch [8/120    avg_loss:0.412, val_acc:0.805]
Epoch [9/120    avg_loss:0.405, val_acc:0.822]
Epoch [10/120    avg_loss:0.338, val_acc:0.834]
Epoch [11/120    avg_loss:0.338, val_acc:0.871]
Epoch [12/120    avg_loss:0.319, val_acc:0.797]
Epoch [13/120    avg_loss:0.368, val_acc:0.865]
Epoch [14/120    avg_loss:0.299, val_acc:0.923]
Epoch [15/120    avg_loss:0.222, val_acc:0.903]
Epoch [16/120    avg_loss:0.186, val_acc:0.942]
Epoch [17/120    avg_loss:0.178, val_acc:0.947]
Epoch [18/120    avg_loss:0.148, val_acc:0.933]
Epoch [19/120    avg_loss:0.136, val_acc:0.958]
Epoch [20/120    avg_loss:0.130, val_acc:0.941]
Epoch [21/120    avg_loss:0.109, val_acc:0.936]
Epoch [22/120    avg_loss:0.113, val_acc:0.957]
Epoch [23/120    avg_loss:0.098, val_acc:0.963]
Epoch [24/120    avg_loss:0.119, val_acc:0.956]
Epoch [25/120    avg_loss:0.080, val_acc:0.969]
Epoch [26/120    avg_loss:0.082, val_acc:0.964]
Epoch [27/120    avg_loss:0.068, val_acc:0.939]
Epoch [28/120    avg_loss:0.178, val_acc:0.938]
Epoch [29/120    avg_loss:0.087, val_acc:0.951]
Epoch [30/120    avg_loss:0.072, val_acc:0.974]
Epoch [31/120    avg_loss:0.085, val_acc:0.935]
Epoch [32/120    avg_loss:0.061, val_acc:0.957]
Epoch [33/120    avg_loss:0.049, val_acc:0.964]
Epoch [34/120    avg_loss:0.055, val_acc:0.972]
Epoch [35/120    avg_loss:0.059, val_acc:0.959]
Epoch [36/120    avg_loss:0.072, val_acc:0.973]
Epoch [37/120    avg_loss:0.035, val_acc:0.977]
Epoch [38/120    avg_loss:0.077, val_acc:0.959]
Epoch [39/120    avg_loss:0.054, val_acc:0.974]
Epoch [40/120    avg_loss:0.032, val_acc:0.981]
Epoch [41/120    avg_loss:0.319, val_acc:0.947]
Epoch [42/120    avg_loss:0.058, val_acc:0.964]
Epoch [43/120    avg_loss:0.037, val_acc:0.977]
Epoch [44/120    avg_loss:0.036, val_acc:0.978]
Epoch [45/120    avg_loss:0.029, val_acc:0.967]
Epoch [46/120    avg_loss:0.026, val_acc:0.960]
Epoch [47/120    avg_loss:0.029, val_acc:0.982]
Epoch [48/120    avg_loss:0.021, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.988]
Epoch [50/120    avg_loss:0.021, val_acc:0.983]
Epoch [51/120    avg_loss:0.038, val_acc:0.945]
Epoch [52/120    avg_loss:0.149, val_acc:0.932]
Epoch [53/120    avg_loss:0.052, val_acc:0.976]
Epoch [54/120    avg_loss:0.050, val_acc:0.958]
Epoch [55/120    avg_loss:0.024, val_acc:0.978]
Epoch [56/120    avg_loss:0.024, val_acc:0.981]
Epoch [57/120    avg_loss:0.018, val_acc:0.981]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.016, val_acc:0.982]
Epoch [60/120    avg_loss:0.061, val_acc:0.974]
Epoch [61/120    avg_loss:0.032, val_acc:0.982]
Epoch [62/120    avg_loss:0.025, val_acc:0.976]
Epoch [63/120    avg_loss:0.025, val_acc:0.981]
Epoch [64/120    avg_loss:0.015, val_acc:0.982]
Epoch [65/120    avg_loss:0.014, val_acc:0.984]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.013, val_acc:0.984]
Epoch [68/120    avg_loss:0.013, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.988]
Epoch [71/120    avg_loss:0.011, val_acc:0.987]
Epoch [72/120    avg_loss:0.011, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.011, val_acc:0.987]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.010, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.011, val_acc:0.987]
Epoch [110/120    avg_loss:0.009, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     1     0     0     0    22     0]
 [    0     0 18042     0    41     0     3     0     4     0]
 [    0     8     0  1996     0     0     0     0    26     6]
 [    0    32    12     0  2911     0     8     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     4     3     0  4852     0     0     0]
 [    0     0     0     0     0     0     1  1281     0     8]
 [    0    16     0     9    51     0     0     0  3495     0]
 [    0     1     0     0    14    38     0     0     0   866]]

Accuracy:
99.19022485720483

F1 scores:
[       nan 0.99379749 0.99781545 0.9868974  0.97146671 0.98564955
 0.99609936 0.99649942 0.98091496 0.96222222]

Kappa:
0.9892709857049399
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68b9da4b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.018, val_acc:0.368]
Epoch [2/120    avg_loss:1.592, val_acc:0.505]
Epoch [3/120    avg_loss:1.220, val_acc:0.522]
Epoch [4/120    avg_loss:0.941, val_acc:0.743]
Epoch [5/120    avg_loss:0.727, val_acc:0.734]
Epoch [6/120    avg_loss:0.596, val_acc:0.659]
Epoch [7/120    avg_loss:0.457, val_acc:0.798]
Epoch [8/120    avg_loss:0.378, val_acc:0.818]
Epoch [9/120    avg_loss:0.332, val_acc:0.846]
Epoch [10/120    avg_loss:0.297, val_acc:0.900]
Epoch [11/120    avg_loss:0.532, val_acc:0.841]
Epoch [12/120    avg_loss:0.373, val_acc:0.766]
Epoch [13/120    avg_loss:0.302, val_acc:0.909]
Epoch [14/120    avg_loss:0.221, val_acc:0.927]
Epoch [15/120    avg_loss:0.197, val_acc:0.906]
Epoch [16/120    avg_loss:0.178, val_acc:0.841]
Epoch [17/120    avg_loss:0.169, val_acc:0.927]
Epoch [18/120    avg_loss:0.133, val_acc:0.894]
Epoch [19/120    avg_loss:0.403, val_acc:0.799]
Epoch [20/120    avg_loss:0.260, val_acc:0.910]
Epoch [21/120    avg_loss:0.217, val_acc:0.937]
Epoch [22/120    avg_loss:0.165, val_acc:0.946]
Epoch [23/120    avg_loss:0.170, val_acc:0.937]
Epoch [24/120    avg_loss:0.140, val_acc:0.929]
Epoch [25/120    avg_loss:0.213, val_acc:0.811]
Epoch [26/120    avg_loss:0.145, val_acc:0.942]
Epoch [27/120    avg_loss:0.173, val_acc:0.932]
Epoch [28/120    avg_loss:0.139, val_acc:0.912]
Epoch [29/120    avg_loss:0.141, val_acc:0.952]
Epoch [30/120    avg_loss:0.101, val_acc:0.961]
Epoch [31/120    avg_loss:0.096, val_acc:0.963]
Epoch [32/120    avg_loss:0.065, val_acc:0.969]
Epoch [33/120    avg_loss:0.060, val_acc:0.974]
Epoch [34/120    avg_loss:0.078, val_acc:0.959]
Epoch [35/120    avg_loss:0.077, val_acc:0.966]
Epoch [36/120    avg_loss:0.056, val_acc:0.942]
Epoch [37/120    avg_loss:0.046, val_acc:0.976]
Epoch [38/120    avg_loss:0.046, val_acc:0.969]
Epoch [39/120    avg_loss:0.041, val_acc:0.973]
Epoch [40/120    avg_loss:0.037, val_acc:0.973]
Epoch [41/120    avg_loss:0.081, val_acc:0.963]
Epoch [42/120    avg_loss:0.145, val_acc:0.584]
Epoch [43/120    avg_loss:1.575, val_acc:0.573]
Epoch [44/120    avg_loss:0.985, val_acc:0.672]
Epoch [45/120    avg_loss:0.748, val_acc:0.716]
Epoch [46/120    avg_loss:0.646, val_acc:0.733]
Epoch [47/120    avg_loss:0.584, val_acc:0.722]
Epoch [48/120    avg_loss:0.501, val_acc:0.784]
Epoch [49/120    avg_loss:0.515, val_acc:0.777]
Epoch [50/120    avg_loss:0.439, val_acc:0.833]
Epoch [51/120    avg_loss:0.396, val_acc:0.838]
Epoch [52/120    avg_loss:0.358, val_acc:0.833]
Epoch [53/120    avg_loss:0.355, val_acc:0.840]
Epoch [54/120    avg_loss:0.341, val_acc:0.843]
Epoch [55/120    avg_loss:0.333, val_acc:0.833]
Epoch [56/120    avg_loss:0.340, val_acc:0.838]
Epoch [57/120    avg_loss:0.336, val_acc:0.840]
Epoch [58/120    avg_loss:0.327, val_acc:0.832]
Epoch [59/120    avg_loss:0.341, val_acc:0.843]
Epoch [60/120    avg_loss:0.338, val_acc:0.846]
Epoch [61/120    avg_loss:0.326, val_acc:0.843]
Epoch [62/120    avg_loss:0.334, val_acc:0.841]
Epoch [63/120    avg_loss:0.329, val_acc:0.847]
Epoch [64/120    avg_loss:0.316, val_acc:0.849]
Epoch [65/120    avg_loss:0.316, val_acc:0.849]
Epoch [66/120    avg_loss:0.310, val_acc:0.849]
Epoch [67/120    avg_loss:0.314, val_acc:0.849]
Epoch [68/120    avg_loss:0.315, val_acc:0.848]
Epoch [69/120    avg_loss:0.303, val_acc:0.847]
Epoch [70/120    avg_loss:0.316, val_acc:0.847]
Epoch [71/120    avg_loss:0.308, val_acc:0.847]
Epoch [72/120    avg_loss:0.320, val_acc:0.848]
Epoch [73/120    avg_loss:0.315, val_acc:0.847]
Epoch [74/120    avg_loss:0.304, val_acc:0.848]
Epoch [75/120    avg_loss:0.304, val_acc:0.847]
Epoch [76/120    avg_loss:0.330, val_acc:0.847]
Epoch [77/120    avg_loss:0.299, val_acc:0.847]
Epoch [78/120    avg_loss:0.313, val_acc:0.847]
Epoch [79/120    avg_loss:0.304, val_acc:0.847]
Epoch [80/120    avg_loss:0.303, val_acc:0.847]
Epoch [81/120    avg_loss:0.306, val_acc:0.848]
Epoch [82/120    avg_loss:0.330, val_acc:0.848]
Epoch [83/120    avg_loss:0.316, val_acc:0.847]
Epoch [84/120    avg_loss:0.296, val_acc:0.847]
Epoch [85/120    avg_loss:0.316, val_acc:0.847]
Epoch [86/120    avg_loss:0.322, val_acc:0.848]
Epoch [87/120    avg_loss:0.312, val_acc:0.848]
Epoch [88/120    avg_loss:0.324, val_acc:0.848]
Epoch [89/120    avg_loss:0.302, val_acc:0.848]
Epoch [90/120    avg_loss:0.298, val_acc:0.848]
Epoch [91/120    avg_loss:0.331, val_acc:0.848]
Epoch [92/120    avg_loss:0.296, val_acc:0.848]
Epoch [93/120    avg_loss:0.312, val_acc:0.848]
Epoch [94/120    avg_loss:0.296, val_acc:0.848]
Epoch [95/120    avg_loss:0.299, val_acc:0.848]
Epoch [96/120    avg_loss:0.330, val_acc:0.848]
Epoch [97/120    avg_loss:0.316, val_acc:0.848]
Epoch [98/120    avg_loss:0.305, val_acc:0.848]
Epoch [99/120    avg_loss:0.315, val_acc:0.848]
Epoch [100/120    avg_loss:0.319, val_acc:0.848]
Epoch [101/120    avg_loss:0.311, val_acc:0.848]
Epoch [102/120    avg_loss:0.325, val_acc:0.848]
Epoch [103/120    avg_loss:0.306, val_acc:0.848]
Epoch [104/120    avg_loss:0.306, val_acc:0.848]
Epoch [105/120    avg_loss:0.308, val_acc:0.848]
Epoch [106/120    avg_loss:0.316, val_acc:0.848]
Epoch [107/120    avg_loss:0.306, val_acc:0.848]
Epoch [108/120    avg_loss:0.302, val_acc:0.848]
Epoch [109/120    avg_loss:0.334, val_acc:0.848]
Epoch [110/120    avg_loss:0.320, val_acc:0.848]
Epoch [111/120    avg_loss:0.321, val_acc:0.848]
Epoch [112/120    avg_loss:0.311, val_acc:0.848]
Epoch [113/120    avg_loss:0.321, val_acc:0.848]
Epoch [114/120    avg_loss:0.312, val_acc:0.848]
Epoch [115/120    avg_loss:0.328, val_acc:0.848]
Epoch [116/120    avg_loss:0.323, val_acc:0.848]
Epoch [117/120    avg_loss:0.322, val_acc:0.848]
Epoch [118/120    avg_loss:0.308, val_acc:0.848]
Epoch [119/120    avg_loss:0.341, val_acc:0.848]
Epoch [120/120    avg_loss:0.302, val_acc:0.848]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5310     4   106   153     0   116   150   573    20]
 [    0    19 14171     0    17     0  3840     0    43     0]
 [    0    24     0  1818     0     0     0     3   165    26]
 [    0   295   105     0  2433     0    55     0    84     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    91   123    96     8     0  4437     0   123     0]
 [    0    65     0     0     2     0     7  1211     0     5]
 [    0   142     0    68    16     0     0     0  3345     0]
 [    0    15     0     5    14    74     0     0    12   799]]

Accuracy:
83.93945966789579

F1 scores:
[       nan 0.85693537 0.87224941 0.88060063 0.8666073  0.97242921
 0.66556664 0.91258478 0.8451238  0.90333522]

Kappa:
0.7951117059621083
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e6ee0dac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.130, val_acc:0.334]
Epoch [2/120    avg_loss:1.748, val_acc:0.411]
Epoch [3/120    avg_loss:1.358, val_acc:0.701]
Epoch [4/120    avg_loss:1.090, val_acc:0.683]
Epoch [5/120    avg_loss:0.802, val_acc:0.730]
Epoch [6/120    avg_loss:0.646, val_acc:0.746]
Epoch [7/120    avg_loss:0.552, val_acc:0.850]
Epoch [8/120    avg_loss:0.451, val_acc:0.821]
Epoch [9/120    avg_loss:0.435, val_acc:0.842]
Epoch [10/120    avg_loss:0.294, val_acc:0.893]
Epoch [11/120    avg_loss:0.323, val_acc:0.903]
Epoch [12/120    avg_loss:0.304, val_acc:0.900]
Epoch [13/120    avg_loss:0.216, val_acc:0.912]
Epoch [14/120    avg_loss:0.206, val_acc:0.910]
Epoch [15/120    avg_loss:0.200, val_acc:0.929]
Epoch [16/120    avg_loss:0.184, val_acc:0.944]
Epoch [17/120    avg_loss:0.131, val_acc:0.948]
Epoch [18/120    avg_loss:0.146, val_acc:0.967]
Epoch [19/120    avg_loss:0.113, val_acc:0.940]
Epoch [20/120    avg_loss:0.143, val_acc:0.952]
Epoch [21/120    avg_loss:0.110, val_acc:0.935]
Epoch [22/120    avg_loss:0.091, val_acc:0.933]
Epoch [23/120    avg_loss:0.105, val_acc:0.925]
Epoch [24/120    avg_loss:0.134, val_acc:0.910]
Epoch [25/120    avg_loss:0.094, val_acc:0.958]
Epoch [26/120    avg_loss:0.103, val_acc:0.956]
Epoch [27/120    avg_loss:0.089, val_acc:0.948]
Epoch [28/120    avg_loss:0.073, val_acc:0.941]
Epoch [29/120    avg_loss:0.069, val_acc:0.964]
Epoch [30/120    avg_loss:0.047, val_acc:0.973]
Epoch [31/120    avg_loss:0.032, val_acc:0.962]
Epoch [32/120    avg_loss:0.036, val_acc:0.959]
Epoch [33/120    avg_loss:0.075, val_acc:0.938]
Epoch [34/120    avg_loss:0.101, val_acc:0.963]
Epoch [35/120    avg_loss:0.067, val_acc:0.966]
Epoch [36/120    avg_loss:0.043, val_acc:0.982]
Epoch [37/120    avg_loss:0.039, val_acc:0.980]
Epoch [38/120    avg_loss:0.047, val_acc:0.982]
Epoch [39/120    avg_loss:0.030, val_acc:0.971]
Epoch [40/120    avg_loss:0.056, val_acc:0.977]
Epoch [41/120    avg_loss:0.054, val_acc:0.959]
Epoch [42/120    avg_loss:0.066, val_acc:0.971]
Epoch [43/120    avg_loss:0.035, val_acc:0.983]
Epoch [44/120    avg_loss:0.055, val_acc:0.975]
Epoch [45/120    avg_loss:0.036, val_acc:0.981]
Epoch [46/120    avg_loss:0.030, val_acc:0.984]
Epoch [47/120    avg_loss:0.023, val_acc:0.988]
Epoch [48/120    avg_loss:0.020, val_acc:0.983]
Epoch [49/120    avg_loss:0.014, val_acc:0.989]
Epoch [50/120    avg_loss:0.015, val_acc:0.981]
Epoch [51/120    avg_loss:0.011, val_acc:0.987]
Epoch [52/120    avg_loss:0.010, val_acc:0.990]
Epoch [53/120    avg_loss:0.014, val_acc:0.987]
Epoch [54/120    avg_loss:0.014, val_acc:0.987]
Epoch [55/120    avg_loss:0.008, val_acc:0.984]
Epoch [56/120    avg_loss:0.009, val_acc:0.988]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.025, val_acc:0.985]
Epoch [59/120    avg_loss:0.029, val_acc:0.988]
Epoch [60/120    avg_loss:0.011, val_acc:0.987]
Epoch [61/120    avg_loss:0.007, val_acc:0.991]
Epoch [62/120    avg_loss:0.008, val_acc:0.989]
Epoch [63/120    avg_loss:0.017, val_acc:0.976]
Epoch [64/120    avg_loss:1.158, val_acc:0.684]
Epoch [65/120    avg_loss:0.837, val_acc:0.688]
Epoch [66/120    avg_loss:0.713, val_acc:0.761]
Epoch [67/120    avg_loss:0.641, val_acc:0.698]
Epoch [68/120    avg_loss:0.595, val_acc:0.786]
Epoch [69/120    avg_loss:0.566, val_acc:0.797]
Epoch [70/120    avg_loss:0.480, val_acc:0.777]
Epoch [71/120    avg_loss:0.438, val_acc:0.839]
Epoch [72/120    avg_loss:0.406, val_acc:0.788]
Epoch [73/120    avg_loss:0.381, val_acc:0.858]
Epoch [74/120    avg_loss:0.367, val_acc:0.791]
Epoch [75/120    avg_loss:0.300, val_acc:0.861]
Epoch [76/120    avg_loss:0.298, val_acc:0.864]
Epoch [77/120    avg_loss:0.293, val_acc:0.880]
Epoch [78/120    avg_loss:0.264, val_acc:0.876]
Epoch [79/120    avg_loss:0.265, val_acc:0.875]
Epoch [80/120    avg_loss:0.274, val_acc:0.880]
Epoch [81/120    avg_loss:0.280, val_acc:0.884]
Epoch [82/120    avg_loss:0.264, val_acc:0.884]
Epoch [83/120    avg_loss:0.273, val_acc:0.890]
Epoch [84/120    avg_loss:0.260, val_acc:0.892]
Epoch [85/120    avg_loss:0.264, val_acc:0.885]
Epoch [86/120    avg_loss:0.244, val_acc:0.892]
Epoch [87/120    avg_loss:0.264, val_acc:0.884]
Epoch [88/120    avg_loss:0.273, val_acc:0.885]
Epoch [89/120    avg_loss:0.241, val_acc:0.885]
Epoch [90/120    avg_loss:0.264, val_acc:0.886]
Epoch [91/120    avg_loss:0.254, val_acc:0.888]
Epoch [92/120    avg_loss:0.267, val_acc:0.889]
Epoch [93/120    avg_loss:0.247, val_acc:0.889]
Epoch [94/120    avg_loss:0.285, val_acc:0.888]
Epoch [95/120    avg_loss:0.244, val_acc:0.888]
Epoch [96/120    avg_loss:0.238, val_acc:0.890]
Epoch [97/120    avg_loss:0.260, val_acc:0.891]
Epoch [98/120    avg_loss:0.255, val_acc:0.892]
Epoch [99/120    avg_loss:0.252, val_acc:0.893]
Epoch [100/120    avg_loss:0.237, val_acc:0.892]
Epoch [101/120    avg_loss:0.238, val_acc:0.893]
Epoch [102/120    avg_loss:0.244, val_acc:0.894]
Epoch [103/120    avg_loss:0.254, val_acc:0.893]
Epoch [104/120    avg_loss:0.253, val_acc:0.893]
Epoch [105/120    avg_loss:0.231, val_acc:0.893]
Epoch [106/120    avg_loss:0.251, val_acc:0.893]
Epoch [107/120    avg_loss:0.253, val_acc:0.893]
Epoch [108/120    avg_loss:0.246, val_acc:0.894]
Epoch [109/120    avg_loss:0.250, val_acc:0.894]
Epoch [110/120    avg_loss:0.250, val_acc:0.894]
Epoch [111/120    avg_loss:0.248, val_acc:0.895]
Epoch [112/120    avg_loss:0.254, val_acc:0.895]
Epoch [113/120    avg_loss:0.246, val_acc:0.894]
Epoch [114/120    avg_loss:0.239, val_acc:0.894]
Epoch [115/120    avg_loss:0.252, val_acc:0.894]
Epoch [116/120    avg_loss:0.248, val_acc:0.894]
Epoch [117/120    avg_loss:0.249, val_acc:0.894]
Epoch [118/120    avg_loss:0.251, val_acc:0.894]
Epoch [119/120    avg_loss:0.244, val_acc:0.894]
Epoch [120/120    avg_loss:0.243, val_acc:0.894]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5469     0    26   185     0     0    65   621    66]
 [    0     0 17615     0   308     0   167     0     0     0]
 [    0     3     0  1899    12     0     0     0    99    23]
 [    0    56    58     0  2768     0    29     0    55     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   314    73    11     0  4405     0    75     0]
 [    0     7     0     0     0     0     6  1257     5    15]
 [    0    52     0    74    52     0     0     4  3389     0]
 [    0    19     0    16    23   107     0     3     0   751]]

Accuracy:
93.6495312462343

F1 scores:
[       nan 0.90862269 0.97652244 0.92095053 0.87442742 0.96061833
 0.928835   0.95990836 0.86730646 0.84382022]

Kappa:
0.9161558760420826
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b2aa22ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.150, val_acc:0.317]
Epoch [2/120    avg_loss:1.640, val_acc:0.569]
Epoch [3/120    avg_loss:1.180, val_acc:0.720]
Epoch [4/120    avg_loss:0.892, val_acc:0.754]
Epoch [5/120    avg_loss:0.703, val_acc:0.783]
Epoch [6/120    avg_loss:0.627, val_acc:0.769]
Epoch [7/120    avg_loss:0.497, val_acc:0.833]
Epoch [8/120    avg_loss:0.400, val_acc:0.834]
Epoch [9/120    avg_loss:0.377, val_acc:0.826]
Epoch [10/120    avg_loss:0.381, val_acc:0.902]
Epoch [11/120    avg_loss:0.311, val_acc:0.900]
Epoch [12/120    avg_loss:0.315, val_acc:0.455]
Epoch [13/120    avg_loss:0.489, val_acc:0.885]
Epoch [14/120    avg_loss:0.271, val_acc:0.807]
Epoch [15/120    avg_loss:0.234, val_acc:0.878]
Epoch [16/120    avg_loss:0.243, val_acc:0.799]
Epoch [17/120    avg_loss:0.602, val_acc:0.212]
Epoch [18/120    avg_loss:1.435, val_acc:0.621]
Epoch [19/120    avg_loss:1.201, val_acc:0.500]
Epoch [20/120    avg_loss:1.023, val_acc:0.597]
Epoch [21/120    avg_loss:0.967, val_acc:0.637]
Epoch [22/120    avg_loss:0.886, val_acc:0.756]
Epoch [23/120    avg_loss:0.811, val_acc:0.726]
Epoch [24/120    avg_loss:0.671, val_acc:0.782]
Epoch [25/120    avg_loss:0.648, val_acc:0.794]
Epoch [26/120    avg_loss:0.687, val_acc:0.789]
Epoch [27/120    avg_loss:0.650, val_acc:0.775]
Epoch [28/120    avg_loss:0.649, val_acc:0.784]
Epoch [29/120    avg_loss:0.633, val_acc:0.780]
Epoch [30/120    avg_loss:0.607, val_acc:0.785]
Epoch [31/120    avg_loss:0.621, val_acc:0.786]
Epoch [32/120    avg_loss:0.603, val_acc:0.800]
Epoch [33/120    avg_loss:0.624, val_acc:0.796]
Epoch [34/120    avg_loss:0.608, val_acc:0.800]
Epoch [35/120    avg_loss:0.646, val_acc:0.802]
Epoch [36/120    avg_loss:0.581, val_acc:0.808]
Epoch [37/120    avg_loss:0.599, val_acc:0.816]
Epoch [38/120    avg_loss:0.602, val_acc:0.816]
Epoch [39/120    avg_loss:0.578, val_acc:0.816]
Epoch [40/120    avg_loss:0.563, val_acc:0.816]
Epoch [41/120    avg_loss:0.606, val_acc:0.815]
Epoch [42/120    avg_loss:0.581, val_acc:0.814]
Epoch [43/120    avg_loss:0.621, val_acc:0.816]
Epoch [44/120    avg_loss:0.595, val_acc:0.814]
Epoch [45/120    avg_loss:0.596, val_acc:0.816]
Epoch [46/120    avg_loss:0.585, val_acc:0.816]
Epoch [47/120    avg_loss:0.570, val_acc:0.815]
Epoch [48/120    avg_loss:0.561, val_acc:0.816]
Epoch [49/120    avg_loss:0.588, val_acc:0.815]
Epoch [50/120    avg_loss:0.582, val_acc:0.815]
Epoch [51/120    avg_loss:0.581, val_acc:0.815]
Epoch [52/120    avg_loss:0.584, val_acc:0.815]
Epoch [53/120    avg_loss:0.586, val_acc:0.815]
Epoch [54/120    avg_loss:0.568, val_acc:0.815]
Epoch [55/120    avg_loss:0.583, val_acc:0.814]
Epoch [56/120    avg_loss:0.603, val_acc:0.815]
Epoch [57/120    avg_loss:0.596, val_acc:0.815]
Epoch [58/120    avg_loss:0.578, val_acc:0.815]
Epoch [59/120    avg_loss:0.572, val_acc:0.815]
Epoch [60/120    avg_loss:0.595, val_acc:0.815]
Epoch [61/120    avg_loss:0.585, val_acc:0.815]
Epoch [62/120    avg_loss:0.560, val_acc:0.815]
Epoch [63/120    avg_loss:0.561, val_acc:0.815]
Epoch [64/120    avg_loss:0.571, val_acc:0.815]
Epoch [65/120    avg_loss:0.578, val_acc:0.815]
Epoch [66/120    avg_loss:0.585, val_acc:0.815]
Epoch [67/120    avg_loss:0.586, val_acc:0.815]
Epoch [68/120    avg_loss:0.588, val_acc:0.815]
Epoch [69/120    avg_loss:0.608, val_acc:0.815]
Epoch [70/120    avg_loss:0.587, val_acc:0.815]
Epoch [71/120    avg_loss:0.561, val_acc:0.815]
Epoch [72/120    avg_loss:0.596, val_acc:0.815]
Epoch [73/120    avg_loss:0.615, val_acc:0.815]
Epoch [74/120    avg_loss:0.557, val_acc:0.815]
Epoch [75/120    avg_loss:0.584, val_acc:0.815]
Epoch [76/120    avg_loss:0.590, val_acc:0.815]
Epoch [77/120    avg_loss:0.587, val_acc:0.815]
Epoch [78/120    avg_loss:0.581, val_acc:0.815]
Epoch [79/120    avg_loss:0.564, val_acc:0.815]
Epoch [80/120    avg_loss:0.572, val_acc:0.815]
Epoch [81/120    avg_loss:0.600, val_acc:0.815]
Epoch [82/120    avg_loss:0.579, val_acc:0.815]
Epoch [83/120    avg_loss:0.606, val_acc:0.815]
Epoch [84/120    avg_loss:0.653, val_acc:0.815]
Epoch [85/120    avg_loss:0.582, val_acc:0.815]
Epoch [86/120    avg_loss:0.576, val_acc:0.815]
Epoch [87/120    avg_loss:0.590, val_acc:0.815]
Epoch [88/120    avg_loss:0.598, val_acc:0.815]
Epoch [89/120    avg_loss:0.578, val_acc:0.815]
Epoch [90/120    avg_loss:0.594, val_acc:0.815]
Epoch [91/120    avg_loss:0.586, val_acc:0.815]
Epoch [92/120    avg_loss:0.544, val_acc:0.815]
Epoch [93/120    avg_loss:0.573, val_acc:0.815]
Epoch [94/120    avg_loss:0.580, val_acc:0.815]
Epoch [95/120    avg_loss:0.588, val_acc:0.815]
Epoch [96/120    avg_loss:0.562, val_acc:0.815]
Epoch [97/120    avg_loss:0.561, val_acc:0.815]
Epoch [98/120    avg_loss:0.569, val_acc:0.815]
Epoch [99/120    avg_loss:0.617, val_acc:0.815]
Epoch [100/120    avg_loss:0.567, val_acc:0.815]
Epoch [101/120    avg_loss:0.578, val_acc:0.815]
Epoch [102/120    avg_loss:0.587, val_acc:0.815]
Epoch [103/120    avg_loss:0.570, val_acc:0.815]
Epoch [104/120    avg_loss:0.583, val_acc:0.815]
Epoch [105/120    avg_loss:0.581, val_acc:0.815]
Epoch [106/120    avg_loss:0.569, val_acc:0.815]
Epoch [107/120    avg_loss:0.601, val_acc:0.815]
Epoch [108/120    avg_loss:0.613, val_acc:0.815]
Epoch [109/120    avg_loss:0.562, val_acc:0.815]
Epoch [110/120    avg_loss:0.590, val_acc:0.815]
Epoch [111/120    avg_loss:0.551, val_acc:0.815]
Epoch [112/120    avg_loss:0.575, val_acc:0.815]
Epoch [113/120    avg_loss:0.567, val_acc:0.815]
Epoch [114/120    avg_loss:0.574, val_acc:0.815]
Epoch [115/120    avg_loss:0.581, val_acc:0.815]
Epoch [116/120    avg_loss:0.576, val_acc:0.815]
Epoch [117/120    avg_loss:0.584, val_acc:0.815]
Epoch [118/120    avg_loss:0.586, val_acc:0.815]
Epoch [119/120    avg_loss:0.610, val_acc:0.815]
Epoch [120/120    avg_loss:0.578, val_acc:0.815]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5236    11     0   464     0    64   168   333   156]
 [    0     0 17194     0   169     0   727     0     0     0]
 [    0    14    10  1749    11     0     0     5   172    75]
 [    0   135   225    18  2359     0   145     0    90     0]
 [    0     0     0     0     0  1301     0     0     4     0]
 [    0     0  1648     4   171     0  2946     0   109     0]
 [    0    45     0     0     0     0    36  1206     0     3]
 [    0   122     0    31    70     0   153     0  3195     0]
 [    0    20     0    12    20   132     0    18     2   715]]

Accuracy:
86.52302798062324

F1 scores:
[       nan 0.87237587 0.92495562 0.90857143 0.75657473 0.95032871
 0.65839759 0.89765538 0.85473515 0.76552463]

Kappa:
0.8199633367723146
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f092fb87b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.046, val_acc:0.362]
Epoch [2/120    avg_loss:1.602, val_acc:0.705]
Epoch [3/120    avg_loss:1.222, val_acc:0.655]
Epoch [4/120    avg_loss:0.892, val_acc:0.676]
Epoch [5/120    avg_loss:0.694, val_acc:0.683]
Epoch [6/120    avg_loss:0.552, val_acc:0.795]
Epoch [7/120    avg_loss:0.479, val_acc:0.821]
Epoch [8/120    avg_loss:0.432, val_acc:0.755]
Epoch [9/120    avg_loss:0.380, val_acc:0.757]
Epoch [10/120    avg_loss:0.444, val_acc:0.507]
Epoch [11/120    avg_loss:0.620, val_acc:0.801]
Epoch [12/120    avg_loss:0.382, val_acc:0.837]
Epoch [13/120    avg_loss:0.311, val_acc:0.822]
Epoch [14/120    avg_loss:0.266, val_acc:0.911]
Epoch [15/120    avg_loss:0.248, val_acc:0.900]
Epoch [16/120    avg_loss:0.210, val_acc:0.903]
Epoch [17/120    avg_loss:0.126, val_acc:0.931]
Epoch [18/120    avg_loss:0.150, val_acc:0.919]
Epoch [19/120    avg_loss:0.139, val_acc:0.941]
Epoch [20/120    avg_loss:0.108, val_acc:0.886]
Epoch [21/120    avg_loss:0.114, val_acc:0.943]
Epoch [22/120    avg_loss:0.120, val_acc:0.950]
Epoch [23/120    avg_loss:0.090, val_acc:0.934]
Epoch [24/120    avg_loss:0.085, val_acc:0.955]
Epoch [25/120    avg_loss:0.076, val_acc:0.956]
Epoch [26/120    avg_loss:0.077, val_acc:0.959]
Epoch [27/120    avg_loss:0.049, val_acc:0.951]
Epoch [28/120    avg_loss:0.054, val_acc:0.971]
Epoch [29/120    avg_loss:0.058, val_acc:0.970]
Epoch [30/120    avg_loss:0.047, val_acc:0.970]
Epoch [31/120    avg_loss:0.047, val_acc:0.967]
Epoch [32/120    avg_loss:0.051, val_acc:0.961]
Epoch [33/120    avg_loss:0.032, val_acc:0.952]
Epoch [34/120    avg_loss:0.028, val_acc:0.972]
Epoch [35/120    avg_loss:0.049, val_acc:0.962]
Epoch [36/120    avg_loss:0.073, val_acc:0.967]
Epoch [37/120    avg_loss:0.044, val_acc:0.970]
Epoch [38/120    avg_loss:0.024, val_acc:0.972]
Epoch [39/120    avg_loss:0.023, val_acc:0.963]
Epoch [40/120    avg_loss:0.028, val_acc:0.969]
Epoch [41/120    avg_loss:0.022, val_acc:0.970]
Epoch [42/120    avg_loss:0.018, val_acc:0.983]
Epoch [43/120    avg_loss:0.025, val_acc:0.974]
Epoch [44/120    avg_loss:0.015, val_acc:0.980]
Epoch [45/120    avg_loss:0.017, val_acc:0.975]
Epoch [46/120    avg_loss:0.019, val_acc:0.972]
Epoch [47/120    avg_loss:0.024, val_acc:0.979]
Epoch [48/120    avg_loss:0.013, val_acc:0.978]
Epoch [49/120    avg_loss:0.022, val_acc:0.967]
Epoch [50/120    avg_loss:0.144, val_acc:0.971]
Epoch [51/120    avg_loss:0.025, val_acc:0.975]
Epoch [52/120    avg_loss:0.119, val_acc:0.961]
Epoch [53/120    avg_loss:0.026, val_acc:0.977]
Epoch [54/120    avg_loss:0.050, val_acc:0.963]
Epoch [55/120    avg_loss:0.041, val_acc:0.971]
Epoch [56/120    avg_loss:0.017, val_acc:0.973]
Epoch [57/120    avg_loss:0.019, val_acc:0.976]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.014, val_acc:0.978]
Epoch [60/120    avg_loss:0.017, val_acc:0.978]
Epoch [61/120    avg_loss:0.012, val_acc:0.978]
Epoch [62/120    avg_loss:0.013, val_acc:0.978]
Epoch [63/120    avg_loss:0.012, val_acc:0.980]
Epoch [64/120    avg_loss:0.012, val_acc:0.980]
Epoch [65/120    avg_loss:0.011, val_acc:0.979]
Epoch [66/120    avg_loss:0.012, val_acc:0.979]
Epoch [67/120    avg_loss:0.012, val_acc:0.981]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.014, val_acc:0.980]
Epoch [70/120    avg_loss:0.011, val_acc:0.980]
Epoch [71/120    avg_loss:0.011, val_acc:0.981]
Epoch [72/120    avg_loss:0.014, val_acc:0.981]
Epoch [73/120    avg_loss:0.011, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.981]
Epoch [75/120    avg_loss:0.013, val_acc:0.980]
Epoch [76/120    avg_loss:0.010, val_acc:0.981]
Epoch [77/120    avg_loss:0.017, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.981]
Epoch [79/120    avg_loss:0.012, val_acc:0.981]
Epoch [80/120    avg_loss:0.010, val_acc:0.981]
Epoch [81/120    avg_loss:0.014, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.012, val_acc:0.981]
Epoch [86/120    avg_loss:0.010, val_acc:0.981]
Epoch [87/120    avg_loss:0.011, val_acc:0.981]
Epoch [88/120    avg_loss:0.012, val_acc:0.981]
Epoch [89/120    avg_loss:0.012, val_acc:0.981]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.013, val_acc:0.981]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.013, val_acc:0.981]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.014, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.012, val_acc:0.981]
Epoch [110/120    avg_loss:0.010, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.012, val_acc:0.981]
Epoch [114/120    avg_loss:0.012, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.011, val_acc:0.981]
Epoch [117/120    avg_loss:0.013, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.012, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     0     2     0     0     4    50     2]
 [    0     2 17927     0   156     0     5     0     0     0]
 [    0     1     0  1969     0     0     0     0    64     2]
 [    0    39     7     0  2910     0     6     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     5     0     0  4859     0    14     0]
 [    0     5     0     0     0     0     3  1282     0     0]
 [    0    17     0    21    58     0     0     0  3475     0]
 [    0     0     0     0     0     6     0     0     0   913]]

Accuracy:
98.84558841250332

F1 scores:
[       nan 0.99052059 0.99528092 0.9769288  0.95441128 0.99770642
 0.99661573 0.99534161 0.96742762 0.99455338]

Kappa:
0.984730177267248
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a248dbb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.112, val_acc:0.428]
Epoch [2/120    avg_loss:1.681, val_acc:0.528]
Epoch [3/120    avg_loss:1.304, val_acc:0.586]
Epoch [4/120    avg_loss:0.989, val_acc:0.619]
Epoch [5/120    avg_loss:0.825, val_acc:0.669]
Epoch [6/120    avg_loss:0.649, val_acc:0.735]
Epoch [7/120    avg_loss:0.559, val_acc:0.761]
Epoch [8/120    avg_loss:0.465, val_acc:0.800]
Epoch [9/120    avg_loss:0.399, val_acc:0.827]
Epoch [10/120    avg_loss:0.425, val_acc:0.786]
Epoch [11/120    avg_loss:0.334, val_acc:0.847]
Epoch [12/120    avg_loss:0.258, val_acc:0.878]
Epoch [13/120    avg_loss:0.241, val_acc:0.887]
Epoch [14/120    avg_loss:0.206, val_acc:0.866]
Epoch [15/120    avg_loss:0.252, val_acc:0.887]
Epoch [16/120    avg_loss:0.781, val_acc:0.548]
Epoch [17/120    avg_loss:0.560, val_acc:0.828]
Epoch [18/120    avg_loss:0.302, val_acc:0.906]
Epoch [19/120    avg_loss:0.200, val_acc:0.907]
Epoch [20/120    avg_loss:0.179, val_acc:0.885]
Epoch [21/120    avg_loss:0.154, val_acc:0.948]
Epoch [22/120    avg_loss:0.128, val_acc:0.922]
Epoch [23/120    avg_loss:0.098, val_acc:0.953]
Epoch [24/120    avg_loss:0.088, val_acc:0.947]
Epoch [25/120    avg_loss:0.105, val_acc:0.923]
Epoch [26/120    avg_loss:0.098, val_acc:0.942]
Epoch [27/120    avg_loss:0.062, val_acc:0.923]
Epoch [28/120    avg_loss:0.055, val_acc:0.959]
Epoch [29/120    avg_loss:0.040, val_acc:0.949]
Epoch [30/120    avg_loss:0.037, val_acc:0.966]
Epoch [31/120    avg_loss:0.052, val_acc:0.954]
Epoch [32/120    avg_loss:0.043, val_acc:0.971]
Epoch [33/120    avg_loss:0.062, val_acc:0.955]
Epoch [34/120    avg_loss:0.061, val_acc:0.953]
Epoch [35/120    avg_loss:0.071, val_acc:0.972]
Epoch [36/120    avg_loss:0.041, val_acc:0.973]
Epoch [37/120    avg_loss:0.035, val_acc:0.964]
Epoch [38/120    avg_loss:0.035, val_acc:0.970]
Epoch [39/120    avg_loss:0.028, val_acc:0.973]
Epoch [40/120    avg_loss:0.035, val_acc:0.979]
Epoch [41/120    avg_loss:0.038, val_acc:0.947]
Epoch [42/120    avg_loss:0.057, val_acc:0.959]
Epoch [43/120    avg_loss:0.054, val_acc:0.972]
Epoch [44/120    avg_loss:0.031, val_acc:0.967]
Epoch [45/120    avg_loss:0.023, val_acc:0.975]
Epoch [46/120    avg_loss:0.013, val_acc:0.983]
Epoch [47/120    avg_loss:0.015, val_acc:0.980]
Epoch [48/120    avg_loss:0.017, val_acc:0.976]
Epoch [49/120    avg_loss:0.017, val_acc:0.974]
Epoch [50/120    avg_loss:0.018, val_acc:0.975]
Epoch [51/120    avg_loss:0.021, val_acc:0.981]
Epoch [52/120    avg_loss:0.012, val_acc:0.975]
Epoch [53/120    avg_loss:0.011, val_acc:0.979]
Epoch [54/120    avg_loss:0.009, val_acc:0.978]
Epoch [55/120    avg_loss:0.010, val_acc:0.978]
Epoch [56/120    avg_loss:0.059, val_acc:0.975]
Epoch [57/120    avg_loss:0.058, val_acc:0.966]
Epoch [58/120    avg_loss:0.040, val_acc:0.974]
Epoch [59/120    avg_loss:0.016, val_acc:0.978]
Epoch [60/120    avg_loss:0.011, val_acc:0.978]
Epoch [61/120    avg_loss:0.013, val_acc:0.979]
Epoch [62/120    avg_loss:0.011, val_acc:0.980]
Epoch [63/120    avg_loss:0.009, val_acc:0.980]
Epoch [64/120    avg_loss:0.008, val_acc:0.981]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.008, val_acc:0.980]
Epoch [67/120    avg_loss:0.008, val_acc:0.982]
Epoch [68/120    avg_loss:0.007, val_acc:0.982]
Epoch [69/120    avg_loss:0.008, val_acc:0.982]
Epoch [70/120    avg_loss:0.008, val_acc:0.982]
Epoch [71/120    avg_loss:0.008, val_acc:0.982]
Epoch [72/120    avg_loss:0.007, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.007, val_acc:0.982]
Epoch [77/120    avg_loss:0.011, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.983]
Epoch [80/120    avg_loss:0.008, val_acc:0.983]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.011, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.007, val_acc:0.982]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     0     0     0    13    12    41     0]
 [    0     0 18075     0     7     0     4     0     4     0]
 [    0     1     0  1945     0     0     0     0    90     0]
 [    0    25     6     0  2935     0     1     0     3     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    21     0    30    40     0     0     0  3480     0]
 [    0     0     0     0     2    67     0     0     0   850]]

Accuracy:
99.11069336996601

F1 scores:
[       nan 0.9912028  0.99941942 0.96983296 0.98556078 0.97497198
 0.99815838 0.99537037 0.96814578 0.95990966]

Kappa:
0.9882168443411249
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7bf3a7be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.071, val_acc:0.285]
Epoch [2/120    avg_loss:1.674, val_acc:0.301]
Epoch [3/120    avg_loss:1.341, val_acc:0.580]
Epoch [4/120    avg_loss:0.927, val_acc:0.771]
Epoch [5/120    avg_loss:0.878, val_acc:0.722]
Epoch [6/120    avg_loss:0.591, val_acc:0.790]
Epoch [7/120    avg_loss:0.665, val_acc:0.767]
Epoch [8/120    avg_loss:0.547, val_acc:0.790]
Epoch [9/120    avg_loss:0.416, val_acc:0.827]
Epoch [10/120    avg_loss:0.334, val_acc:0.888]
Epoch [11/120    avg_loss:0.243, val_acc:0.920]
Epoch [12/120    avg_loss:0.242, val_acc:0.896]
Epoch [13/120    avg_loss:0.187, val_acc:0.928]
Epoch [14/120    avg_loss:0.189, val_acc:0.946]
Epoch [15/120    avg_loss:0.167, val_acc:0.943]
Epoch [16/120    avg_loss:0.129, val_acc:0.902]
Epoch [17/120    avg_loss:0.148, val_acc:0.877]
Epoch [18/120    avg_loss:0.240, val_acc:0.919]
Epoch [19/120    avg_loss:0.246, val_acc:0.862]
Epoch [20/120    avg_loss:0.230, val_acc:0.903]
Epoch [21/120    avg_loss:0.126, val_acc:0.954]
Epoch [22/120    avg_loss:0.107, val_acc:0.960]
Epoch [23/120    avg_loss:0.076, val_acc:0.943]
Epoch [24/120    avg_loss:0.141, val_acc:0.946]
Epoch [25/120    avg_loss:0.100, val_acc:0.905]
Epoch [26/120    avg_loss:0.111, val_acc:0.944]
Epoch [27/120    avg_loss:0.889, val_acc:0.404]
Epoch [28/120    avg_loss:1.180, val_acc:0.519]
Epoch [29/120    avg_loss:0.988, val_acc:0.522]
Epoch [30/120    avg_loss:0.922, val_acc:0.628]
Epoch [31/120    avg_loss:0.858, val_acc:0.651]
Epoch [32/120    avg_loss:0.833, val_acc:0.690]
Epoch [33/120    avg_loss:0.778, val_acc:0.699]
Epoch [34/120    avg_loss:0.693, val_acc:0.706]
Epoch [35/120    avg_loss:0.714, val_acc:0.721]
Epoch [36/120    avg_loss:0.656, val_acc:0.723]
Epoch [37/120    avg_loss:0.677, val_acc:0.719]
Epoch [38/120    avg_loss:0.660, val_acc:0.722]
Epoch [39/120    avg_loss:0.676, val_acc:0.717]
Epoch [40/120    avg_loss:0.681, val_acc:0.712]
Epoch [41/120    avg_loss:0.655, val_acc:0.723]
Epoch [42/120    avg_loss:0.693, val_acc:0.721]
Epoch [43/120    avg_loss:0.642, val_acc:0.723]
Epoch [44/120    avg_loss:0.609, val_acc:0.723]
Epoch [45/120    avg_loss:0.637, val_acc:0.721]
Epoch [46/120    avg_loss:0.638, val_acc:0.726]
Epoch [47/120    avg_loss:0.667, val_acc:0.729]
Epoch [48/120    avg_loss:0.644, val_acc:0.727]
Epoch [49/120    avg_loss:0.616, val_acc:0.727]
Epoch [50/120    avg_loss:0.615, val_acc:0.725]
Epoch [51/120    avg_loss:0.625, val_acc:0.726]
Epoch [52/120    avg_loss:0.655, val_acc:0.725]
Epoch [53/120    avg_loss:0.627, val_acc:0.726]
Epoch [54/120    avg_loss:0.619, val_acc:0.727]
Epoch [55/120    avg_loss:0.620, val_acc:0.728]
Epoch [56/120    avg_loss:0.647, val_acc:0.727]
Epoch [57/120    avg_loss:0.639, val_acc:0.728]
Epoch [58/120    avg_loss:0.625, val_acc:0.727]
Epoch [59/120    avg_loss:0.661, val_acc:0.729]
Epoch [60/120    avg_loss:0.645, val_acc:0.729]
Epoch [61/120    avg_loss:0.629, val_acc:0.726]
Epoch [62/120    avg_loss:0.627, val_acc:0.726]
Epoch [63/120    avg_loss:0.605, val_acc:0.726]
Epoch [64/120    avg_loss:0.614, val_acc:0.725]
Epoch [65/120    avg_loss:0.631, val_acc:0.725]
Epoch [66/120    avg_loss:0.664, val_acc:0.725]
Epoch [67/120    avg_loss:0.647, val_acc:0.725]
Epoch [68/120    avg_loss:0.632, val_acc:0.725]
Epoch [69/120    avg_loss:0.639, val_acc:0.725]
Epoch [70/120    avg_loss:0.636, val_acc:0.725]
Epoch [71/120    avg_loss:0.604, val_acc:0.725]
Epoch [72/120    avg_loss:0.606, val_acc:0.726]
Epoch [73/120    avg_loss:0.624, val_acc:0.725]
Epoch [74/120    avg_loss:0.604, val_acc:0.725]
Epoch [75/120    avg_loss:0.640, val_acc:0.725]
Epoch [76/120    avg_loss:0.637, val_acc:0.725]
Epoch [77/120    avg_loss:0.611, val_acc:0.725]
Epoch [78/120    avg_loss:0.644, val_acc:0.725]
Epoch [79/120    avg_loss:0.622, val_acc:0.725]
Epoch [80/120    avg_loss:0.634, val_acc:0.725]
Epoch [81/120    avg_loss:0.626, val_acc:0.725]
Epoch [82/120    avg_loss:0.634, val_acc:0.725]
Epoch [83/120    avg_loss:0.652, val_acc:0.725]
Epoch [84/120    avg_loss:0.661, val_acc:0.725]
Epoch [85/120    avg_loss:0.661, val_acc:0.725]
Epoch [86/120    avg_loss:0.634, val_acc:0.725]
Epoch [87/120    avg_loss:0.632, val_acc:0.725]
Epoch [88/120    avg_loss:0.598, val_acc:0.725]
Epoch [89/120    avg_loss:0.642, val_acc:0.725]
Epoch [90/120    avg_loss:0.619, val_acc:0.725]
Epoch [91/120    avg_loss:0.623, val_acc:0.725]
Epoch [92/120    avg_loss:0.659, val_acc:0.725]
Epoch [93/120    avg_loss:0.654, val_acc:0.725]
Epoch [94/120    avg_loss:0.638, val_acc:0.725]
Epoch [95/120    avg_loss:0.647, val_acc:0.725]
Epoch [96/120    avg_loss:0.630, val_acc:0.725]
Epoch [97/120    avg_loss:0.627, val_acc:0.725]
Epoch [98/120    avg_loss:0.651, val_acc:0.725]
Epoch [99/120    avg_loss:0.651, val_acc:0.725]
Epoch [100/120    avg_loss:0.635, val_acc:0.725]
Epoch [101/120    avg_loss:0.630, val_acc:0.725]
Epoch [102/120    avg_loss:0.643, val_acc:0.725]
Epoch [103/120    avg_loss:0.646, val_acc:0.725]
Epoch [104/120    avg_loss:0.633, val_acc:0.725]
Epoch [105/120    avg_loss:0.628, val_acc:0.725]
Epoch [106/120    avg_loss:0.632, val_acc:0.725]
Epoch [107/120    avg_loss:0.630, val_acc:0.725]
Epoch [108/120    avg_loss:0.625, val_acc:0.725]
Epoch [109/120    avg_loss:0.624, val_acc:0.725]
Epoch [110/120    avg_loss:0.630, val_acc:0.725]
Epoch [111/120    avg_loss:0.627, val_acc:0.725]
Epoch [112/120    avg_loss:0.623, val_acc:0.725]
Epoch [113/120    avg_loss:0.647, val_acc:0.725]
Epoch [114/120    avg_loss:0.610, val_acc:0.725]
Epoch [115/120    avg_loss:0.623, val_acc:0.725]
Epoch [116/120    avg_loss:0.631, val_acc:0.725]
Epoch [117/120    avg_loss:0.618, val_acc:0.725]
Epoch [118/120    avg_loss:0.634, val_acc:0.725]
Epoch [119/120    avg_loss:0.667, val_acc:0.725]
Epoch [120/120    avg_loss:0.635, val_acc:0.725]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4208   274   206   659     0   357    82   496   150]
 [    0     0 13963     0   106     0  3067     0   954     0]
 [    0     3     0  1768    28     0     0     0   200    37]
 [    0    84   211     0  2386     0   182     0   106     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   599   197    25     0  3976     0    81     0]
 [    0    23     0     0     0     0    29  1215     8    15]
 [    0   276    27    68   122     0    66     0  3012     0]
 [    0    40     0    10    40   163     4     0     9   653]]

Accuracy:
78.29272407394018

F1 scores:
[       nan 0.76052774 0.84205765 0.8252042  0.7529189  0.9412189
 0.63317143 0.93931194 0.71399787 0.73494654]

Kappa:
0.7222121434996251
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f9d735ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.143, val_acc:0.242]
Epoch [2/120    avg_loss:1.744, val_acc:0.568]
Epoch [3/120    avg_loss:1.280, val_acc:0.705]
Epoch [4/120    avg_loss:0.967, val_acc:0.669]
Epoch [5/120    avg_loss:0.706, val_acc:0.765]
Epoch [6/120    avg_loss:0.664, val_acc:0.790]
Epoch [7/120    avg_loss:0.519, val_acc:0.734]
Epoch [8/120    avg_loss:0.397, val_acc:0.834]
Epoch [9/120    avg_loss:0.448, val_acc:0.846]
Epoch [10/120    avg_loss:0.347, val_acc:0.852]
Epoch [11/120    avg_loss:0.280, val_acc:0.896]
Epoch [12/120    avg_loss:0.313, val_acc:0.842]
Epoch [13/120    avg_loss:0.232, val_acc:0.808]
Epoch [14/120    avg_loss:0.217, val_acc:0.943]
Epoch [15/120    avg_loss:0.223, val_acc:0.912]
Epoch [16/120    avg_loss:0.140, val_acc:0.922]
Epoch [17/120    avg_loss:0.142, val_acc:0.940]
Epoch [18/120    avg_loss:0.197, val_acc:0.859]
Epoch [19/120    avg_loss:0.153, val_acc:0.947]
Epoch [20/120    avg_loss:0.133, val_acc:0.926]
Epoch [21/120    avg_loss:0.158, val_acc:0.873]
Epoch [22/120    avg_loss:0.129, val_acc:0.847]
Epoch [23/120    avg_loss:0.141, val_acc:0.938]
Epoch [24/120    avg_loss:0.091, val_acc:0.885]
Epoch [25/120    avg_loss:0.095, val_acc:0.949]
Epoch [26/120    avg_loss:0.084, val_acc:0.955]
Epoch [27/120    avg_loss:0.084, val_acc:0.917]
Epoch [28/120    avg_loss:0.071, val_acc:0.970]
Epoch [29/120    avg_loss:0.065, val_acc:0.949]
Epoch [30/120    avg_loss:0.072, val_acc:0.938]
Epoch [31/120    avg_loss:0.070, val_acc:0.951]
Epoch [32/120    avg_loss:0.056, val_acc:0.963]
Epoch [33/120    avg_loss:0.039, val_acc:0.973]
Epoch [34/120    avg_loss:0.068, val_acc:0.961]
Epoch [35/120    avg_loss:0.050, val_acc:0.973]
Epoch [36/120    avg_loss:0.045, val_acc:0.980]
Epoch [37/120    avg_loss:0.047, val_acc:0.979]
Epoch [38/120    avg_loss:0.063, val_acc:0.972]
Epoch [39/120    avg_loss:0.050, val_acc:0.973]
Epoch [40/120    avg_loss:0.041, val_acc:0.966]
Epoch [41/120    avg_loss:0.034, val_acc:0.981]
Epoch [42/120    avg_loss:0.025, val_acc:0.977]
Epoch [43/120    avg_loss:0.066, val_acc:0.948]
Epoch [44/120    avg_loss:0.056, val_acc:0.982]
Epoch [45/120    avg_loss:0.037, val_acc:0.973]
Epoch [46/120    avg_loss:0.057, val_acc:0.969]
Epoch [47/120    avg_loss:0.033, val_acc:0.978]
Epoch [48/120    avg_loss:0.044, val_acc:0.981]
Epoch [49/120    avg_loss:0.025, val_acc:0.976]
Epoch [50/120    avg_loss:0.027, val_acc:0.978]
Epoch [51/120    avg_loss:0.027, val_acc:0.943]
Epoch [52/120    avg_loss:0.035, val_acc:0.985]
Epoch [53/120    avg_loss:0.023, val_acc:0.975]
Epoch [54/120    avg_loss:0.012, val_acc:0.984]
Epoch [55/120    avg_loss:0.016, val_acc:0.970]
Epoch [56/120    avg_loss:0.015, val_acc:0.984]
Epoch [57/120    avg_loss:0.019, val_acc:0.965]
Epoch [58/120    avg_loss:0.014, val_acc:0.978]
Epoch [59/120    avg_loss:0.015, val_acc:0.980]
Epoch [60/120    avg_loss:0.015, val_acc:0.987]
Epoch [61/120    avg_loss:0.029, val_acc:0.979]
Epoch [62/120    avg_loss:0.051, val_acc:0.970]
Epoch [63/120    avg_loss:0.054, val_acc:0.961]
Epoch [64/120    avg_loss:0.049, val_acc:0.919]
Epoch [65/120    avg_loss:0.050, val_acc:0.960]
Epoch [66/120    avg_loss:0.025, val_acc:0.985]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.014, val_acc:0.987]
Epoch [69/120    avg_loss:0.011, val_acc:0.988]
Epoch [70/120    avg_loss:0.015, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.988]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.009, val_acc:0.988]
Epoch [74/120    avg_loss:0.010, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.992]
Epoch [76/120    avg_loss:0.007, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.979]
Epoch [78/120    avg_loss:0.006, val_acc:0.987]
Epoch [79/120    avg_loss:0.005, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.989]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.004, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.012, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.004, val_acc:0.991]
Epoch [90/120    avg_loss:0.005, val_acc:0.991]
Epoch [91/120    avg_loss:0.003, val_acc:0.991]
Epoch [92/120    avg_loss:0.003, val_acc:0.990]
Epoch [93/120    avg_loss:0.003, val_acc:0.989]
Epoch [94/120    avg_loss:0.003, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.003, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.003, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.004, val_acc:0.989]
Epoch [113/120    avg_loss:0.002, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6425     0     0     0     0     0     0     7     0]
 [    0     1 18079     0     7     0     3     0     0     0]
 [    0    11     0  1981     0     0     0     0    44     0]
 [    0    14     7     0  2930     0     9     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8    23     0     0  4846     0     1     0]
 [    0     0     0     0     0     0     0  1281     0     9]
 [    0     4     0    16    55     0     0     0  3496     0]
 [    0     0     0     0    14    50     0     0     0   855]]

Accuracy:
99.28903670498639

F1 scores:
[       nan 0.99712889 0.99928145 0.97682446 0.98026096 0.98120301
 0.99548069 0.99649942 0.98050764 0.95905777]

Kappa:
0.9905781827661857
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77dd838b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.122, val_acc:0.554]
Epoch [2/120    avg_loss:1.652, val_acc:0.557]
Epoch [3/120    avg_loss:1.243, val_acc:0.402]
Epoch [4/120    avg_loss:0.960, val_acc:0.606]
Epoch [5/120    avg_loss:0.775, val_acc:0.764]
Epoch [6/120    avg_loss:0.678, val_acc:0.778]
Epoch [7/120    avg_loss:0.554, val_acc:0.777]
Epoch [8/120    avg_loss:0.497, val_acc:0.773]
Epoch [9/120    avg_loss:0.443, val_acc:0.815]
Epoch [10/120    avg_loss:0.337, val_acc:0.868]
Epoch [11/120    avg_loss:0.279, val_acc:0.884]
Epoch [12/120    avg_loss:0.309, val_acc:0.899]
Epoch [13/120    avg_loss:0.265, val_acc:0.873]
Epoch [14/120    avg_loss:0.221, val_acc:0.944]
Epoch [15/120    avg_loss:0.177, val_acc:0.922]
Epoch [16/120    avg_loss:0.179, val_acc:0.928]
Epoch [17/120    avg_loss:0.196, val_acc:0.942]
Epoch [18/120    avg_loss:0.163, val_acc:0.934]
Epoch [19/120    avg_loss:0.128, val_acc:0.932]
Epoch [20/120    avg_loss:0.113, val_acc:0.960]
Epoch [21/120    avg_loss:0.088, val_acc:0.955]
Epoch [22/120    avg_loss:0.102, val_acc:0.845]
Epoch [23/120    avg_loss:0.139, val_acc:0.948]
Epoch [24/120    avg_loss:0.117, val_acc:0.945]
Epoch [25/120    avg_loss:0.109, val_acc:0.949]
Epoch [26/120    avg_loss:0.103, val_acc:0.934]
Epoch [27/120    avg_loss:0.100, val_acc:0.962]
Epoch [28/120    avg_loss:0.103, val_acc:0.923]
Epoch [29/120    avg_loss:0.093, val_acc:0.968]
Epoch [30/120    avg_loss:0.062, val_acc:0.971]
Epoch [31/120    avg_loss:0.077, val_acc:0.962]
Epoch [32/120    avg_loss:0.063, val_acc:0.947]
Epoch [33/120    avg_loss:0.101, val_acc:0.982]
Epoch [34/120    avg_loss:0.042, val_acc:0.975]
Epoch [35/120    avg_loss:0.061, val_acc:0.973]
Epoch [36/120    avg_loss:0.040, val_acc:0.968]
Epoch [37/120    avg_loss:0.067, val_acc:0.979]
Epoch [38/120    avg_loss:0.045, val_acc:0.946]
Epoch [39/120    avg_loss:0.031, val_acc:0.985]
Epoch [40/120    avg_loss:0.018, val_acc:0.979]
Epoch [41/120    avg_loss:0.029, val_acc:0.978]
Epoch [42/120    avg_loss:0.023, val_acc:0.974]
Epoch [43/120    avg_loss:0.027, val_acc:0.980]
Epoch [44/120    avg_loss:0.014, val_acc:0.988]
Epoch [45/120    avg_loss:0.022, val_acc:0.985]
Epoch [46/120    avg_loss:0.023, val_acc:0.985]
Epoch [47/120    avg_loss:0.013, val_acc:0.985]
Epoch [48/120    avg_loss:0.029, val_acc:0.985]
Epoch [49/120    avg_loss:0.014, val_acc:0.981]
Epoch [50/120    avg_loss:0.039, val_acc:0.974]
Epoch [51/120    avg_loss:0.037, val_acc:0.979]
Epoch [52/120    avg_loss:0.051, val_acc:0.939]
Epoch [53/120    avg_loss:0.020, val_acc:0.980]
Epoch [54/120    avg_loss:0.011, val_acc:0.986]
Epoch [55/120    avg_loss:0.026, val_acc:0.979]
Epoch [56/120    avg_loss:0.018, val_acc:0.986]
Epoch [57/120    avg_loss:0.017, val_acc:0.980]
Epoch [58/120    avg_loss:0.010, val_acc:0.982]
Epoch [59/120    avg_loss:0.008, val_acc:0.988]
Epoch [60/120    avg_loss:0.010, val_acc:0.986]
Epoch [61/120    avg_loss:0.008, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.009, val_acc:0.987]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.008, val_acc:0.988]
Epoch [66/120    avg_loss:0.007, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.987]
Epoch [68/120    avg_loss:0.010, val_acc:0.988]
Epoch [69/120    avg_loss:0.011, val_acc:0.987]
Epoch [70/120    avg_loss:0.015, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.987]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.006, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.989]
Epoch [81/120    avg_loss:0.006, val_acc:0.989]
Epoch [82/120    avg_loss:0.006, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.006, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     1     0    20    20     8     6]
 [    0     0 18059     0    30     0     1     0     0     0]
 [    0     2     0  2012     0     0     0     0    21     1]
 [    0    24    10     0  2912     0     5     0    21     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     5     8     0     0  4865     0     0     0]
 [    0     0     0     0     0     0     1  1288     0     1]
 [    0    25     4    29    28     0     0     0  3485     0]
 [    0     0     0     0    14    81     0     0     0   824]]

Accuracy:
99.11551346010171

F1 scores:
[       nan 0.99175739 0.99861756 0.98506732 0.97767333 0.96951673
 0.99590583 0.99153195 0.98086124 0.94063927]

Kappa:
0.9882817270814245
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59ef17eb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.150, val_acc:0.164]
Epoch [2/120    avg_loss:1.751, val_acc:0.307]
Epoch [3/120    avg_loss:1.482, val_acc:0.345]
Epoch [4/120    avg_loss:1.232, val_acc:0.428]
Epoch [5/120    avg_loss:0.981, val_acc:0.487]
Epoch [6/120    avg_loss:0.763, val_acc:0.492]
Epoch [7/120    avg_loss:0.654, val_acc:0.697]
Epoch [8/120    avg_loss:0.548, val_acc:0.774]
Epoch [9/120    avg_loss:0.465, val_acc:0.725]
Epoch [10/120    avg_loss:0.402, val_acc:0.752]
Epoch [11/120    avg_loss:0.395, val_acc:0.808]
Epoch [12/120    avg_loss:0.339, val_acc:0.832]
Epoch [13/120    avg_loss:0.328, val_acc:0.707]
Epoch [14/120    avg_loss:0.319, val_acc:0.789]
Epoch [15/120    avg_loss:0.297, val_acc:0.836]
Epoch [16/120    avg_loss:0.287, val_acc:0.868]
Epoch [17/120    avg_loss:0.249, val_acc:0.922]
Epoch [18/120    avg_loss:0.222, val_acc:0.901]
Epoch [19/120    avg_loss:0.199, val_acc:0.906]
Epoch [20/120    avg_loss:0.181, val_acc:0.917]
Epoch [21/120    avg_loss:0.162, val_acc:0.924]
Epoch [22/120    avg_loss:0.155, val_acc:0.899]
Epoch [23/120    avg_loss:0.146, val_acc:0.924]
Epoch [24/120    avg_loss:0.174, val_acc:0.919]
Epoch [25/120    avg_loss:0.169, val_acc:0.951]
Epoch [26/120    avg_loss:0.141, val_acc:0.932]
Epoch [27/120    avg_loss:0.117, val_acc:0.963]
Epoch [28/120    avg_loss:0.092, val_acc:0.952]
Epoch [29/120    avg_loss:0.104, val_acc:0.951]
Epoch [30/120    avg_loss:0.095, val_acc:0.956]
Epoch [31/120    avg_loss:0.071, val_acc:0.953]
Epoch [32/120    avg_loss:0.067, val_acc:0.950]
Epoch [33/120    avg_loss:0.084, val_acc:0.958]
Epoch [34/120    avg_loss:0.061, val_acc:0.920]
Epoch [35/120    avg_loss:0.090, val_acc:0.952]
Epoch [36/120    avg_loss:0.065, val_acc:0.963]
Epoch [37/120    avg_loss:0.055, val_acc:0.961]
Epoch [38/120    avg_loss:0.068, val_acc:0.947]
Epoch [39/120    avg_loss:0.059, val_acc:0.950]
Epoch [40/120    avg_loss:0.067, val_acc:0.974]
Epoch [41/120    avg_loss:0.044, val_acc:0.974]
Epoch [42/120    avg_loss:0.045, val_acc:0.973]
Epoch [43/120    avg_loss:0.062, val_acc:0.970]
Epoch [44/120    avg_loss:0.030, val_acc:0.968]
Epoch [45/120    avg_loss:0.047, val_acc:0.975]
Epoch [46/120    avg_loss:0.037, val_acc:0.974]
Epoch [47/120    avg_loss:0.024, val_acc:0.980]
Epoch [48/120    avg_loss:0.044, val_acc:0.975]
Epoch [49/120    avg_loss:0.030, val_acc:0.972]
Epoch [50/120    avg_loss:0.035, val_acc:0.978]
Epoch [51/120    avg_loss:0.021, val_acc:0.982]
Epoch [52/120    avg_loss:0.018, val_acc:0.971]
Epoch [53/120    avg_loss:0.020, val_acc:0.981]
Epoch [54/120    avg_loss:0.020, val_acc:0.981]
Epoch [55/120    avg_loss:0.026, val_acc:0.973]
Epoch [56/120    avg_loss:0.023, val_acc:0.979]
Epoch [57/120    avg_loss:0.031, val_acc:0.965]
Epoch [58/120    avg_loss:0.026, val_acc:0.983]
Epoch [59/120    avg_loss:0.018, val_acc:0.969]
Epoch [60/120    avg_loss:0.036, val_acc:0.967]
Epoch [61/120    avg_loss:0.028, val_acc:0.966]
Epoch [62/120    avg_loss:0.094, val_acc:0.951]
Epoch [63/120    avg_loss:0.050, val_acc:0.965]
Epoch [64/120    avg_loss:0.024, val_acc:0.967]
Epoch [65/120    avg_loss:0.026, val_acc:0.973]
Epoch [66/120    avg_loss:0.018, val_acc:0.962]
Epoch [67/120    avg_loss:0.053, val_acc:0.962]
Epoch [68/120    avg_loss:0.021, val_acc:0.974]
Epoch [69/120    avg_loss:0.014, val_acc:0.981]
Epoch [70/120    avg_loss:0.016, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.981]
Epoch [72/120    avg_loss:0.010, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.010, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.008, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6258     0     8     0     1     0    34    97    34]
 [    0     0 17999     0     6     0    81     0     4     0]
 [    0     3     0  1898     0     0     0     0   134     1]
 [    0    19     7     1  2926     0     7     0     7     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    10     0     0     0  4866     0     1     0]
 [    0    26     0     0     0     0     0  1260     1     3]
 [    0    13     0    54    36     0     3     0  3465     0]
 [    0     2     0     0    15     4     0     0     0   898]]

Accuracy:
98.51059214807317

F1 scores:
[       nan 0.98133919 0.99700881 0.94971228 0.98270361 0.99808795
 0.9895272  0.9752322  0.95192308 0.9655914 ]

Kappa:
0.9802835074223805
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f14cf2b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.065, val_acc:0.204]
Epoch [2/120    avg_loss:1.736, val_acc:0.295]
Epoch [3/120    avg_loss:1.463, val_acc:0.376]
Epoch [4/120    avg_loss:1.225, val_acc:0.416]
Epoch [5/120    avg_loss:0.971, val_acc:0.461]
Epoch [6/120    avg_loss:0.804, val_acc:0.608]
Epoch [7/120    avg_loss:0.677, val_acc:0.690]
Epoch [8/120    avg_loss:0.557, val_acc:0.737]
Epoch [9/120    avg_loss:0.452, val_acc:0.768]
Epoch [10/120    avg_loss:0.381, val_acc:0.679]
Epoch [11/120    avg_loss:0.396, val_acc:0.840]
Epoch [12/120    avg_loss:0.339, val_acc:0.854]
Epoch [13/120    avg_loss:0.294, val_acc:0.868]
Epoch [14/120    avg_loss:0.270, val_acc:0.900]
Epoch [15/120    avg_loss:0.239, val_acc:0.904]
Epoch [16/120    avg_loss:0.219, val_acc:0.926]
Epoch [17/120    avg_loss:0.218, val_acc:0.929]
Epoch [18/120    avg_loss:0.188, val_acc:0.897]
Epoch [19/120    avg_loss:0.182, val_acc:0.910]
Epoch [20/120    avg_loss:0.163, val_acc:0.928]
Epoch [21/120    avg_loss:0.159, val_acc:0.871]
Epoch [22/120    avg_loss:0.174, val_acc:0.925]
Epoch [23/120    avg_loss:0.130, val_acc:0.939]
Epoch [24/120    avg_loss:0.121, val_acc:0.950]
Epoch [25/120    avg_loss:0.125, val_acc:0.939]
Epoch [26/120    avg_loss:0.102, val_acc:0.940]
Epoch [27/120    avg_loss:0.084, val_acc:0.963]
Epoch [28/120    avg_loss:0.098, val_acc:0.924]
Epoch [29/120    avg_loss:0.104, val_acc:0.887]
Epoch [30/120    avg_loss:0.095, val_acc:0.919]
Epoch [31/120    avg_loss:0.064, val_acc:0.950]
Epoch [32/120    avg_loss:0.079, val_acc:0.964]
Epoch [33/120    avg_loss:0.112, val_acc:0.945]
Epoch [34/120    avg_loss:0.103, val_acc:0.965]
Epoch [35/120    avg_loss:0.056, val_acc:0.901]
Epoch [36/120    avg_loss:0.066, val_acc:0.954]
Epoch [37/120    avg_loss:0.051, val_acc:0.966]
Epoch [38/120    avg_loss:0.053, val_acc:0.954]
Epoch [39/120    avg_loss:0.071, val_acc:0.961]
Epoch [40/120    avg_loss:0.040, val_acc:0.961]
Epoch [41/120    avg_loss:0.062, val_acc:0.962]
Epoch [42/120    avg_loss:0.042, val_acc:0.965]
Epoch [43/120    avg_loss:0.049, val_acc:0.961]
Epoch [44/120    avg_loss:0.046, val_acc:0.968]
Epoch [45/120    avg_loss:0.033, val_acc:0.971]
Epoch [46/120    avg_loss:0.023, val_acc:0.978]
Epoch [47/120    avg_loss:0.022, val_acc:0.978]
Epoch [48/120    avg_loss:0.031, val_acc:0.964]
Epoch [49/120    avg_loss:0.042, val_acc:0.975]
Epoch [50/120    avg_loss:0.050, val_acc:0.952]
Epoch [51/120    avg_loss:0.050, val_acc:0.962]
Epoch [52/120    avg_loss:0.035, val_acc:0.932]
Epoch [53/120    avg_loss:0.089, val_acc:0.957]
Epoch [54/120    avg_loss:0.037, val_acc:0.979]
Epoch [55/120    avg_loss:0.029, val_acc:0.969]
Epoch [56/120    avg_loss:0.039, val_acc:0.970]
Epoch [57/120    avg_loss:0.024, val_acc:0.948]
Epoch [58/120    avg_loss:0.018, val_acc:0.978]
Epoch [59/120    avg_loss:0.012, val_acc:0.979]
Epoch [60/120    avg_loss:0.013, val_acc:0.983]
Epoch [61/120    avg_loss:0.016, val_acc:0.979]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.012, val_acc:0.981]
Epoch [64/120    avg_loss:0.023, val_acc:0.973]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.011, val_acc:0.980]
Epoch [67/120    avg_loss:0.009, val_acc:0.979]
Epoch [68/120    avg_loss:0.016, val_acc:0.977]
Epoch [69/120    avg_loss:0.013, val_acc:0.977]
Epoch [70/120    avg_loss:0.007, val_acc:0.983]
Epoch [71/120    avg_loss:0.035, val_acc:0.975]
Epoch [72/120    avg_loss:0.027, val_acc:0.978]
Epoch [73/120    avg_loss:0.022, val_acc:0.976]
Epoch [74/120    avg_loss:0.024, val_acc:0.975]
Epoch [75/120    avg_loss:0.014, val_acc:0.983]
Epoch [76/120    avg_loss:0.011, val_acc:0.970]
Epoch [77/120    avg_loss:0.107, val_acc:0.940]
Epoch [78/120    avg_loss:0.075, val_acc:0.945]
Epoch [79/120    avg_loss:0.047, val_acc:0.963]
Epoch [80/120    avg_loss:0.046, val_acc:0.962]
Epoch [81/120    avg_loss:0.091, val_acc:0.961]
Epoch [82/120    avg_loss:0.064, val_acc:0.954]
Epoch [83/120    avg_loss:0.030, val_acc:0.965]
Epoch [84/120    avg_loss:0.019, val_acc:0.961]
Epoch [85/120    avg_loss:0.031, val_acc:0.974]
Epoch [86/120    avg_loss:0.012, val_acc:0.970]
Epoch [87/120    avg_loss:0.011, val_acc:0.969]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.011, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.979]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.011, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.980]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.980]
Epoch [109/120    avg_loss:0.010, val_acc:0.980]
Epoch [110/120    avg_loss:0.009, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.005, val_acc:0.980]
Epoch [120/120    avg_loss:0.007, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6318     0     6     0     0     2     2   101     3]
 [    0     0 18053     0    15     0    21     0     1     0]
 [    0    12     0  1912     0     0     0     0   112     0]
 [    0     6    12     0  2941     0     5     0     1     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     3     0  4846     0     0     0]
 [    0    23     0     0     0     0     0  1260     7     0]
 [    0    10     0    42    34     0     2     0  3483     0]
 [    0     3     0     0     0    13     0     0     0   903]]

Accuracy:
98.86245872797822

F1 scores:
[       nan 0.9868791  0.99784435 0.95695696 0.9860855  0.99504384
 0.99364363 0.98746082 0.95739417 0.98580786]

Kappa:
0.9849264478009304
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f578bbbaa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.077, val_acc:0.247]
Epoch [2/120    avg_loss:1.678, val_acc:0.372]
Epoch [3/120    avg_loss:1.389, val_acc:0.488]
Epoch [4/120    avg_loss:1.150, val_acc:0.695]
Epoch [5/120    avg_loss:0.918, val_acc:0.624]
Epoch [6/120    avg_loss:0.768, val_acc:0.705]
Epoch [7/120    avg_loss:0.618, val_acc:0.733]
Epoch [8/120    avg_loss:0.526, val_acc:0.764]
Epoch [9/120    avg_loss:0.453, val_acc:0.751]
Epoch [10/120    avg_loss:0.408, val_acc:0.851]
Epoch [11/120    avg_loss:0.363, val_acc:0.848]
Epoch [12/120    avg_loss:0.284, val_acc:0.862]
Epoch [13/120    avg_loss:0.285, val_acc:0.892]
Epoch [14/120    avg_loss:0.272, val_acc:0.928]
Epoch [15/120    avg_loss:0.196, val_acc:0.926]
Epoch [16/120    avg_loss:0.171, val_acc:0.939]
Epoch [17/120    avg_loss:0.207, val_acc:0.905]
Epoch [18/120    avg_loss:0.182, val_acc:0.940]
Epoch [19/120    avg_loss:0.149, val_acc:0.938]
Epoch [20/120    avg_loss:0.161, val_acc:0.923]
Epoch [21/120    avg_loss:0.153, val_acc:0.943]
Epoch [22/120    avg_loss:0.118, val_acc:0.937]
Epoch [23/120    avg_loss:0.128, val_acc:0.916]
Epoch [24/120    avg_loss:0.125, val_acc:0.943]
Epoch [25/120    avg_loss:0.117, val_acc:0.936]
Epoch [26/120    avg_loss:0.114, val_acc:0.828]
Epoch [27/120    avg_loss:0.123, val_acc:0.954]
Epoch [28/120    avg_loss:0.092, val_acc:0.958]
Epoch [29/120    avg_loss:0.071, val_acc:0.955]
Epoch [30/120    avg_loss:0.059, val_acc:0.957]
Epoch [31/120    avg_loss:0.061, val_acc:0.968]
Epoch [32/120    avg_loss:0.054, val_acc:0.925]
Epoch [33/120    avg_loss:0.093, val_acc:0.962]
Epoch [34/120    avg_loss:0.096, val_acc:0.924]
Epoch [35/120    avg_loss:0.089, val_acc:0.965]
Epoch [36/120    avg_loss:0.054, val_acc:0.978]
Epoch [37/120    avg_loss:0.049, val_acc:0.975]
Epoch [38/120    avg_loss:0.035, val_acc:0.970]
Epoch [39/120    avg_loss:0.037, val_acc:0.966]
Epoch [40/120    avg_loss:0.046, val_acc:0.973]
Epoch [41/120    avg_loss:0.046, val_acc:0.905]
Epoch [42/120    avg_loss:0.048, val_acc:0.975]
Epoch [43/120    avg_loss:0.029, val_acc:0.973]
Epoch [44/120    avg_loss:0.036, val_acc:0.967]
Epoch [45/120    avg_loss:0.052, val_acc:0.975]
Epoch [46/120    avg_loss:0.036, val_acc:0.975]
Epoch [47/120    avg_loss:0.048, val_acc:0.970]
Epoch [48/120    avg_loss:0.022, val_acc:0.975]
Epoch [49/120    avg_loss:0.030, val_acc:0.976]
Epoch [50/120    avg_loss:0.027, val_acc:0.979]
Epoch [51/120    avg_loss:0.024, val_acc:0.981]
Epoch [52/120    avg_loss:0.018, val_acc:0.982]
Epoch [53/120    avg_loss:0.015, val_acc:0.982]
Epoch [54/120    avg_loss:0.021, val_acc:0.980]
Epoch [55/120    avg_loss:0.018, val_acc:0.979]
Epoch [56/120    avg_loss:0.012, val_acc:0.983]
Epoch [57/120    avg_loss:0.014, val_acc:0.981]
Epoch [58/120    avg_loss:0.017, val_acc:0.982]
Epoch [59/120    avg_loss:0.012, val_acc:0.980]
Epoch [60/120    avg_loss:0.012, val_acc:0.982]
Epoch [61/120    avg_loss:0.021, val_acc:0.980]
Epoch [62/120    avg_loss:0.016, val_acc:0.980]
Epoch [63/120    avg_loss:0.011, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.980]
Epoch [66/120    avg_loss:0.016, val_acc:0.981]
Epoch [67/120    avg_loss:0.013, val_acc:0.982]
Epoch [68/120    avg_loss:0.012, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.982]
Epoch [70/120    avg_loss:0.010, val_acc:0.982]
Epoch [71/120    avg_loss:0.013, val_acc:0.982]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.983]
Epoch [74/120    avg_loss:0.011, val_acc:0.982]
Epoch [75/120    avg_loss:0.013, val_acc:0.982]
Epoch [76/120    avg_loss:0.011, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.982]
Epoch [78/120    avg_loss:0.011, val_acc:0.982]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.013, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.016, val_acc:0.983]
Epoch [87/120    avg_loss:0.012, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.014, val_acc:0.982]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.012, val_acc:0.982]
Epoch [93/120    avg_loss:0.011, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.013, val_acc:0.982]
Epoch [97/120    avg_loss:0.011, val_acc:0.982]
Epoch [98/120    avg_loss:0.010, val_acc:0.982]
Epoch [99/120    avg_loss:0.012, val_acc:0.982]
Epoch [100/120    avg_loss:0.011, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.014, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.015, val_acc:0.982]
Epoch [114/120    avg_loss:0.013, val_acc:0.982]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.982]
Epoch [120/120    avg_loss:0.015, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     0     0     0     0     1    67    10]
 [    0     0 17880     0   113     0    93     0     4     0]
 [    0    11     0  1825     0     0     0     0   200     0]
 [    0    12     2     0  2941     0    15     0     0     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    35     0     6     0  4819     0    18     0]
 [    0     6     0     0     0     0     0  1282     1     1]
 [    0    36     1    38    33     0     5     0  3458     0]
 [    0     0     0     0     8    17     0     0     0   894]]

Accuracy:
98.22861687513557

F1 scores:
[       nan 0.98887246 0.99311264 0.93613747 0.96854932 0.99352874
 0.98246687 0.99650214 0.94493783 0.97918949]

Kappa:
0.9765689732144648
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4be5c16b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.060, val_acc:0.160]
Epoch [2/120    avg_loss:1.671, val_acc:0.254]
Epoch [3/120    avg_loss:1.400, val_acc:0.331]
Epoch [4/120    avg_loss:1.183, val_acc:0.384]
Epoch [5/120    avg_loss:0.909, val_acc:0.631]
Epoch [6/120    avg_loss:0.713, val_acc:0.705]
Epoch [7/120    avg_loss:0.621, val_acc:0.727]
Epoch [8/120    avg_loss:0.561, val_acc:0.582]
Epoch [9/120    avg_loss:0.488, val_acc:0.684]
Epoch [10/120    avg_loss:0.407, val_acc:0.646]
Epoch [11/120    avg_loss:0.358, val_acc:0.790]
Epoch [12/120    avg_loss:0.366, val_acc:0.836]
Epoch [13/120    avg_loss:0.346, val_acc:0.859]
Epoch [14/120    avg_loss:0.269, val_acc:0.863]
Epoch [15/120    avg_loss:0.253, val_acc:0.853]
Epoch [16/120    avg_loss:0.239, val_acc:0.881]
Epoch [17/120    avg_loss:0.199, val_acc:0.876]
Epoch [18/120    avg_loss:0.170, val_acc:0.911]
Epoch [19/120    avg_loss:0.201, val_acc:0.900]
Epoch [20/120    avg_loss:0.159, val_acc:0.921]
Epoch [21/120    avg_loss:0.285, val_acc:0.811]
Epoch [22/120    avg_loss:0.155, val_acc:0.926]
Epoch [23/120    avg_loss:0.156, val_acc:0.910]
Epoch [24/120    avg_loss:0.202, val_acc:0.871]
Epoch [25/120    avg_loss:0.143, val_acc:0.938]
Epoch [26/120    avg_loss:0.134, val_acc:0.887]
Epoch [27/120    avg_loss:0.148, val_acc:0.940]
Epoch [28/120    avg_loss:0.116, val_acc:0.947]
Epoch [29/120    avg_loss:0.110, val_acc:0.952]
Epoch [30/120    avg_loss:0.132, val_acc:0.911]
Epoch [31/120    avg_loss:0.134, val_acc:0.949]
Epoch [32/120    avg_loss:0.105, val_acc:0.959]
Epoch [33/120    avg_loss:0.074, val_acc:0.958]
Epoch [34/120    avg_loss:0.080, val_acc:0.963]
Epoch [35/120    avg_loss:0.087, val_acc:0.965]
Epoch [36/120    avg_loss:0.072, val_acc:0.951]
Epoch [37/120    avg_loss:0.077, val_acc:0.951]
Epoch [38/120    avg_loss:0.070, val_acc:0.970]
Epoch [39/120    avg_loss:0.062, val_acc:0.970]
Epoch [40/120    avg_loss:0.058, val_acc:0.965]
Epoch [41/120    avg_loss:0.098, val_acc:0.961]
Epoch [42/120    avg_loss:0.073, val_acc:0.956]
Epoch [43/120    avg_loss:0.051, val_acc:0.971]
Epoch [44/120    avg_loss:0.048, val_acc:0.970]
Epoch [45/120    avg_loss:0.047, val_acc:0.950]
Epoch [46/120    avg_loss:0.075, val_acc:0.961]
Epoch [47/120    avg_loss:0.086, val_acc:0.914]
Epoch [48/120    avg_loss:0.072, val_acc:0.970]
Epoch [49/120    avg_loss:0.049, val_acc:0.970]
Epoch [50/120    avg_loss:0.040, val_acc:0.960]
Epoch [51/120    avg_loss:0.050, val_acc:0.975]
Epoch [52/120    avg_loss:0.034, val_acc:0.971]
Epoch [53/120    avg_loss:0.057, val_acc:0.940]
Epoch [54/120    avg_loss:0.045, val_acc:0.951]
Epoch [55/120    avg_loss:0.034, val_acc:0.956]
Epoch [56/120    avg_loss:0.041, val_acc:0.975]
Epoch [57/120    avg_loss:0.027, val_acc:0.982]
Epoch [58/120    avg_loss:0.025, val_acc:0.983]
Epoch [59/120    avg_loss:0.023, val_acc:0.968]
Epoch [60/120    avg_loss:0.035, val_acc:0.982]
Epoch [61/120    avg_loss:0.024, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.984]
Epoch [63/120    avg_loss:0.019, val_acc:0.984]
Epoch [64/120    avg_loss:0.032, val_acc:0.969]
Epoch [65/120    avg_loss:0.035, val_acc:0.977]
Epoch [66/120    avg_loss:0.025, val_acc:0.974]
Epoch [67/120    avg_loss:0.019, val_acc:0.981]
Epoch [68/120    avg_loss:0.017, val_acc:0.984]
Epoch [69/120    avg_loss:0.014, val_acc:0.984]
Epoch [70/120    avg_loss:0.018, val_acc:0.982]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.018, val_acc:0.954]
Epoch [73/120    avg_loss:0.018, val_acc:0.979]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.021, val_acc:0.975]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.979]
Epoch [78/120    avg_loss:0.016, val_acc:0.974]
Epoch [79/120    avg_loss:0.042, val_acc:0.975]
Epoch [80/120    avg_loss:0.024, val_acc:0.977]
Epoch [81/120    avg_loss:0.017, val_acc:0.975]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.008, val_acc:0.979]
Epoch [85/120    avg_loss:0.006, val_acc:0.980]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.981]
Epoch [88/120    avg_loss:0.009, val_acc:0.979]
Epoch [89/120    avg_loss:0.007, val_acc:0.979]
Epoch [90/120    avg_loss:0.007, val_acc:0.979]
Epoch [91/120    avg_loss:0.006, val_acc:0.980]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.007, val_acc:0.981]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.007, val_acc:0.982]
Epoch [98/120    avg_loss:0.006, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.008, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.982]
Epoch [105/120    avg_loss:0.006, val_acc:0.982]
Epoch [106/120    avg_loss:0.006, val_acc:0.982]
Epoch [107/120    avg_loss:0.006, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.006, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     2     0     0     5     9    36     5]
 [    0     0 18044     0     5     0    38     0     3     0]
 [    0     5     0  1959     0     0     0     0    72     0]
 [    0    12    22     0  2897     0    37     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    49     0     0     0  4829     0     0     0]
 [    0    24     0     0     0     0     0  1266     0     0]
 [    0    39     1    63    21     0    26     1  3419     1]
 [    0     4     0     0     2     4     0     0     0   909]]

Accuracy:
98.81907791675705

F1 scores:
[       nan 0.98906214 0.99674087 0.96502463 0.98253349 0.99846978
 0.98420463 0.98674981 0.96282737 0.98965705]

Kappa:
0.9843448237029438
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa05b4f5a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.048, val_acc:0.085]
Epoch [2/120    avg_loss:1.702, val_acc:0.174]
Epoch [3/120    avg_loss:1.509, val_acc:0.314]
Epoch [4/120    avg_loss:1.314, val_acc:0.410]
Epoch [5/120    avg_loss:1.121, val_acc:0.471]
Epoch [6/120    avg_loss:0.877, val_acc:0.535]
Epoch [7/120    avg_loss:0.730, val_acc:0.574]
Epoch [8/120    avg_loss:0.607, val_acc:0.669]
Epoch [9/120    avg_loss:0.564, val_acc:0.677]
Epoch [10/120    avg_loss:0.479, val_acc:0.766]
Epoch [11/120    avg_loss:0.423, val_acc:0.798]
Epoch [12/120    avg_loss:0.367, val_acc:0.809]
Epoch [13/120    avg_loss:0.370, val_acc:0.845]
Epoch [14/120    avg_loss:0.323, val_acc:0.826]
Epoch [15/120    avg_loss:0.288, val_acc:0.895]
Epoch [16/120    avg_loss:0.271, val_acc:0.876]
Epoch [17/120    avg_loss:0.231, val_acc:0.808]
Epoch [18/120    avg_loss:0.214, val_acc:0.836]
Epoch [19/120    avg_loss:0.243, val_acc:0.905]
Epoch [20/120    avg_loss:0.881, val_acc:0.603]
Epoch [21/120    avg_loss:0.613, val_acc:0.727]
Epoch [22/120    avg_loss:0.462, val_acc:0.742]
Epoch [23/120    avg_loss:0.408, val_acc:0.839]
Epoch [24/120    avg_loss:0.383, val_acc:0.802]
Epoch [25/120    avg_loss:0.303, val_acc:0.891]
Epoch [26/120    avg_loss:0.236, val_acc:0.892]
Epoch [27/120    avg_loss:0.273, val_acc:0.904]
Epoch [28/120    avg_loss:0.233, val_acc:0.832]
Epoch [29/120    avg_loss:0.210, val_acc:0.930]
Epoch [30/120    avg_loss:0.176, val_acc:0.938]
Epoch [31/120    avg_loss:0.169, val_acc:0.924]
Epoch [32/120    avg_loss:0.155, val_acc:0.930]
Epoch [33/120    avg_loss:0.159, val_acc:0.887]
Epoch [34/120    avg_loss:0.153, val_acc:0.928]
Epoch [35/120    avg_loss:0.128, val_acc:0.940]
Epoch [36/120    avg_loss:0.172, val_acc:0.936]
Epoch [37/120    avg_loss:0.133, val_acc:0.910]
Epoch [38/120    avg_loss:0.152, val_acc:0.934]
Epoch [39/120    avg_loss:0.123, val_acc:0.960]
Epoch [40/120    avg_loss:0.095, val_acc:0.956]
Epoch [41/120    avg_loss:0.082, val_acc:0.944]
Epoch [42/120    avg_loss:0.199, val_acc:0.873]
Epoch [43/120    avg_loss:0.171, val_acc:0.941]
Epoch [44/120    avg_loss:0.127, val_acc:0.945]
Epoch [45/120    avg_loss:0.092, val_acc:0.961]
Epoch [46/120    avg_loss:0.087, val_acc:0.961]
Epoch [47/120    avg_loss:0.112, val_acc:0.938]
Epoch [48/120    avg_loss:0.073, val_acc:0.955]
Epoch [49/120    avg_loss:0.094, val_acc:0.929]
Epoch [50/120    avg_loss:0.114, val_acc:0.964]
Epoch [51/120    avg_loss:0.070, val_acc:0.961]
Epoch [52/120    avg_loss:0.079, val_acc:0.960]
Epoch [53/120    avg_loss:0.100, val_acc:0.962]
Epoch [54/120    avg_loss:0.070, val_acc:0.967]
Epoch [55/120    avg_loss:0.047, val_acc:0.975]
Epoch [56/120    avg_loss:0.065, val_acc:0.977]
Epoch [57/120    avg_loss:0.073, val_acc:0.963]
Epoch [58/120    avg_loss:0.087, val_acc:0.963]
Epoch [59/120    avg_loss:0.054, val_acc:0.977]
Epoch [60/120    avg_loss:0.045, val_acc:0.978]
Epoch [61/120    avg_loss:0.036, val_acc:0.971]
Epoch [62/120    avg_loss:0.033, val_acc:0.982]
Epoch [63/120    avg_loss:0.061, val_acc:0.970]
Epoch [64/120    avg_loss:0.032, val_acc:0.978]
Epoch [65/120    avg_loss:0.057, val_acc:0.956]
Epoch [66/120    avg_loss:0.039, val_acc:0.980]
Epoch [67/120    avg_loss:0.021, val_acc:0.983]
Epoch [68/120    avg_loss:0.025, val_acc:0.983]
Epoch [69/120    avg_loss:0.023, val_acc:0.980]
Epoch [70/120    avg_loss:0.022, val_acc:0.968]
Epoch [71/120    avg_loss:0.031, val_acc:0.954]
Epoch [72/120    avg_loss:0.032, val_acc:0.978]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.013, val_acc:0.985]
Epoch [75/120    avg_loss:0.022, val_acc:0.976]
Epoch [76/120    avg_loss:0.018, val_acc:0.984]
Epoch [77/120    avg_loss:0.026, val_acc:0.983]
Epoch [78/120    avg_loss:0.025, val_acc:0.979]
Epoch [79/120    avg_loss:0.015, val_acc:0.979]
Epoch [80/120    avg_loss:0.021, val_acc:0.979]
Epoch [81/120    avg_loss:0.022, val_acc:0.979]
Epoch [82/120    avg_loss:0.031, val_acc:0.982]
Epoch [83/120    avg_loss:0.016, val_acc:0.979]
Epoch [84/120    avg_loss:0.019, val_acc:0.979]
Epoch [85/120    avg_loss:0.013, val_acc:0.976]
Epoch [86/120    avg_loss:0.034, val_acc:0.973]
Epoch [87/120    avg_loss:0.024, val_acc:0.976]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.017, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.011, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.013, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6333     0    23     1     0     4    10    52     9]
 [    0     0 18049     0     6     0    32     0     3     0]
 [    0     2     0  1878     0     0     0     0   153     3]
 [    0    17    20     0  2924     0     4     0     2     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     2     0  4876     0     0     0]
 [    0    14     0     0     0     0     0  1273     0     3]
 [    0    13     0    51    29     0     0     0  3478     0]
 [    0     0     0     0    16    25     0     0     0   878]]

Accuracy:
98.79738751114645

F1 scores:
[       nan 0.9886816  0.99831301 0.94182548 0.98285714 0.99051233
 0.99571166 0.98950641 0.95825871 0.96642818]

Kappa:
0.9840679260300642
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c02326b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.076, val_acc:0.220]
Epoch [2/120    avg_loss:1.703, val_acc:0.230]
Epoch [3/120    avg_loss:1.441, val_acc:0.354]
Epoch [4/120    avg_loss:1.245, val_acc:0.452]
Epoch [5/120    avg_loss:1.046, val_acc:0.544]
Epoch [6/120    avg_loss:0.806, val_acc:0.579]
Epoch [7/120    avg_loss:0.633, val_acc:0.697]
Epoch [8/120    avg_loss:0.515, val_acc:0.760]
Epoch [9/120    avg_loss:0.428, val_acc:0.761]
Epoch [10/120    avg_loss:0.500, val_acc:0.731]
Epoch [11/120    avg_loss:0.383, val_acc:0.849]
Epoch [12/120    avg_loss:0.306, val_acc:0.817]
Epoch [13/120    avg_loss:0.398, val_acc:0.828]
Epoch [14/120    avg_loss:0.344, val_acc:0.789]
Epoch [15/120    avg_loss:0.250, val_acc:0.874]
Epoch [16/120    avg_loss:0.264, val_acc:0.917]
Epoch [17/120    avg_loss:0.222, val_acc:0.877]
Epoch [18/120    avg_loss:0.204, val_acc:0.903]
Epoch [19/120    avg_loss:0.187, val_acc:0.900]
Epoch [20/120    avg_loss:0.180, val_acc:0.950]
Epoch [21/120    avg_loss:0.221, val_acc:0.876]
Epoch [22/120    avg_loss:0.194, val_acc:0.946]
Epoch [23/120    avg_loss:0.141, val_acc:0.938]
Epoch [24/120    avg_loss:0.148, val_acc:0.931]
Epoch [25/120    avg_loss:0.145, val_acc:0.937]
Epoch [26/120    avg_loss:0.169, val_acc:0.918]
Epoch [27/120    avg_loss:0.174, val_acc:0.902]
Epoch [28/120    avg_loss:0.149, val_acc:0.957]
Epoch [29/120    avg_loss:0.115, val_acc:0.942]
Epoch [30/120    avg_loss:0.110, val_acc:0.951]
Epoch [31/120    avg_loss:0.093, val_acc:0.938]
Epoch [32/120    avg_loss:0.100, val_acc:0.947]
Epoch [33/120    avg_loss:0.146, val_acc:0.943]
Epoch [34/120    avg_loss:0.133, val_acc:0.907]
Epoch [35/120    avg_loss:0.104, val_acc:0.920]
Epoch [36/120    avg_loss:0.146, val_acc:0.920]
Epoch [37/120    avg_loss:0.082, val_acc:0.961]
Epoch [38/120    avg_loss:0.088, val_acc:0.928]
Epoch [39/120    avg_loss:0.094, val_acc:0.972]
Epoch [40/120    avg_loss:0.052, val_acc:0.944]
Epoch [41/120    avg_loss:0.065, val_acc:0.970]
Epoch [42/120    avg_loss:0.067, val_acc:0.956]
Epoch [43/120    avg_loss:0.058, val_acc:0.976]
Epoch [44/120    avg_loss:0.060, val_acc:0.952]
Epoch [45/120    avg_loss:0.045, val_acc:0.947]
Epoch [46/120    avg_loss:0.052, val_acc:0.959]
Epoch [47/120    avg_loss:0.053, val_acc:0.962]
Epoch [48/120    avg_loss:0.041, val_acc:0.955]
Epoch [49/120    avg_loss:0.061, val_acc:0.970]
Epoch [50/120    avg_loss:0.033, val_acc:0.975]
Epoch [51/120    avg_loss:0.060, val_acc:0.978]
Epoch [52/120    avg_loss:0.055, val_acc:0.968]
Epoch [53/120    avg_loss:0.046, val_acc:0.960]
Epoch [54/120    avg_loss:0.053, val_acc:0.961]
Epoch [55/120    avg_loss:0.040, val_acc:0.965]
Epoch [56/120    avg_loss:0.023, val_acc:0.976]
Epoch [57/120    avg_loss:0.018, val_acc:0.972]
Epoch [58/120    avg_loss:0.041, val_acc:0.969]
Epoch [59/120    avg_loss:0.062, val_acc:0.940]
Epoch [60/120    avg_loss:0.051, val_acc:0.977]
Epoch [61/120    avg_loss:0.043, val_acc:0.965]
Epoch [62/120    avg_loss:0.024, val_acc:0.973]
Epoch [63/120    avg_loss:0.019, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.981]
Epoch [65/120    avg_loss:0.021, val_acc:0.970]
Epoch [66/120    avg_loss:0.024, val_acc:0.980]
Epoch [67/120    avg_loss:0.016, val_acc:0.983]
Epoch [68/120    avg_loss:0.026, val_acc:0.959]
Epoch [69/120    avg_loss:0.038, val_acc:0.983]
Epoch [70/120    avg_loss:0.033, val_acc:0.980]
Epoch [71/120    avg_loss:0.014, val_acc:0.979]
Epoch [72/120    avg_loss:0.021, val_acc:0.960]
Epoch [73/120    avg_loss:0.032, val_acc:0.964]
Epoch [74/120    avg_loss:0.023, val_acc:0.983]
Epoch [75/120    avg_loss:0.012, val_acc:0.983]
Epoch [76/120    avg_loss:0.010, val_acc:0.979]
Epoch [77/120    avg_loss:0.019, val_acc:0.940]
Epoch [78/120    avg_loss:0.021, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.017, val_acc:0.984]
Epoch [82/120    avg_loss:0.017, val_acc:0.984]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.022, val_acc:0.979]
Epoch [85/120    avg_loss:0.039, val_acc:0.955]
Epoch [86/120    avg_loss:0.085, val_acc:0.970]
Epoch [87/120    avg_loss:0.037, val_acc:0.972]
Epoch [88/120    avg_loss:0.027, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.972]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0     2     0     0     5    24    26]
 [    0     0 18055     0    13     0    19     0     3     0]
 [    0     5     0  1892     0     0     0     0   136     3]
 [    0     0    12     1  2932     0    24     0     0     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    43     0     0     0  4835     0     0     0]
 [    0    14     0     0     0     0     0  1274     0     2]
 [    0    17     1    84    54     0    12     0  3403     0]
 [    0     0     0     0    19    33     0     0     0   867]]

Accuracy:
98.66242498734726

F1 scores:
[       nan 0.9927587  0.99748626 0.94293546 0.97863818 0.98751419
 0.98996724 0.99182561 0.95362197 0.95274725]

Kappa:
0.9822719520325153
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d75d4aba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.111, val_acc:0.134]
Epoch [2/120    avg_loss:1.753, val_acc:0.190]
Epoch [3/120    avg_loss:1.530, val_acc:0.564]
Epoch [4/120    avg_loss:1.265, val_acc:0.430]
Epoch [5/120    avg_loss:1.087, val_acc:0.521]
Epoch [6/120    avg_loss:0.869, val_acc:0.659]
Epoch [7/120    avg_loss:0.721, val_acc:0.653]
Epoch [8/120    avg_loss:0.693, val_acc:0.679]
Epoch [9/120    avg_loss:0.524, val_acc:0.699]
Epoch [10/120    avg_loss:0.454, val_acc:0.714]
Epoch [11/120    avg_loss:0.389, val_acc:0.693]
Epoch [12/120    avg_loss:0.386, val_acc:0.765]
Epoch [13/120    avg_loss:0.311, val_acc:0.839]
Epoch [14/120    avg_loss:0.334, val_acc:0.726]
Epoch [15/120    avg_loss:0.299, val_acc:0.843]
Epoch [16/120    avg_loss:0.250, val_acc:0.889]
Epoch [17/120    avg_loss:0.265, val_acc:0.873]
Epoch [18/120    avg_loss:0.208, val_acc:0.904]
Epoch [19/120    avg_loss:0.328, val_acc:0.759]
Epoch [20/120    avg_loss:0.426, val_acc:0.867]
Epoch [21/120    avg_loss:0.259, val_acc:0.877]
Epoch [22/120    avg_loss:0.202, val_acc:0.911]
Epoch [23/120    avg_loss:0.197, val_acc:0.871]
Epoch [24/120    avg_loss:0.164, val_acc:0.929]
Epoch [25/120    avg_loss:0.149, val_acc:0.914]
Epoch [26/120    avg_loss:0.141, val_acc:0.825]
Epoch [27/120    avg_loss:0.154, val_acc:0.938]
Epoch [28/120    avg_loss:0.123, val_acc:0.921]
Epoch [29/120    avg_loss:0.136, val_acc:0.922]
Epoch [30/120    avg_loss:0.123, val_acc:0.920]
Epoch [31/120    avg_loss:0.097, val_acc:0.959]
Epoch [32/120    avg_loss:0.134, val_acc:0.851]
Epoch [33/120    avg_loss:0.107, val_acc:0.960]
Epoch [34/120    avg_loss:0.098, val_acc:0.892]
Epoch [35/120    avg_loss:0.095, val_acc:0.951]
Epoch [36/120    avg_loss:0.084, val_acc:0.946]
Epoch [37/120    avg_loss:0.095, val_acc:0.942]
Epoch [38/120    avg_loss:0.073, val_acc:0.968]
Epoch [39/120    avg_loss:0.071, val_acc:0.966]
Epoch [40/120    avg_loss:0.067, val_acc:0.961]
Epoch [41/120    avg_loss:0.048, val_acc:0.948]
Epoch [42/120    avg_loss:0.059, val_acc:0.961]
Epoch [43/120    avg_loss:0.052, val_acc:0.946]
Epoch [44/120    avg_loss:0.062, val_acc:0.973]
Epoch [45/120    avg_loss:0.041, val_acc:0.975]
Epoch [46/120    avg_loss:0.037, val_acc:0.975]
Epoch [47/120    avg_loss:0.033, val_acc:0.965]
Epoch [48/120    avg_loss:0.030, val_acc:0.960]
Epoch [49/120    avg_loss:0.035, val_acc:0.967]
Epoch [50/120    avg_loss:0.042, val_acc:0.973]
Epoch [51/120    avg_loss:0.050, val_acc:0.946]
Epoch [52/120    avg_loss:0.054, val_acc:0.965]
Epoch [53/120    avg_loss:0.054, val_acc:0.946]
Epoch [54/120    avg_loss:0.052, val_acc:0.976]
Epoch [55/120    avg_loss:0.041, val_acc:0.933]
Epoch [56/120    avg_loss:0.066, val_acc:0.945]
Epoch [57/120    avg_loss:0.035, val_acc:0.969]
Epoch [58/120    avg_loss:0.039, val_acc:0.975]
Epoch [59/120    avg_loss:0.049, val_acc:0.965]
Epoch [60/120    avg_loss:0.042, val_acc:0.955]
Epoch [61/120    avg_loss:0.019, val_acc:0.978]
Epoch [62/120    avg_loss:0.019, val_acc:0.974]
Epoch [63/120    avg_loss:0.019, val_acc:0.984]
Epoch [64/120    avg_loss:0.019, val_acc:0.977]
Epoch [65/120    avg_loss:0.037, val_acc:0.976]
Epoch [66/120    avg_loss:0.021, val_acc:0.982]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.026, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.016, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.028, val_acc:0.979]
Epoch [73/120    avg_loss:0.016, val_acc:0.980]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.017, val_acc:0.972]
Epoch [76/120    avg_loss:0.023, val_acc:0.973]
Epoch [77/120    avg_loss:0.057, val_acc:0.982]
Epoch [78/120    avg_loss:0.041, val_acc:0.976]
Epoch [79/120    avg_loss:0.029, val_acc:0.977]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.021, val_acc:0.927]
Epoch [83/120    avg_loss:0.036, val_acc:0.969]
Epoch [84/120    avg_loss:0.022, val_acc:0.979]
Epoch [85/120    avg_loss:0.020, val_acc:0.975]
Epoch [86/120    avg_loss:0.015, val_acc:0.981]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.014, val_acc:0.983]
Epoch [89/120    avg_loss:0.015, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.016, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.982]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6329     0     5     0     0     3     6    63    26]
 [    0     0 18039     0     5     0    45     0     1     0]
 [    0     5     0  1876     0     0     0     0   154     1]
 [    0    27     5     0  2926     0     8     0     4     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     9     0  4840     0     0     0]
 [    0     5     0     0     0     0     0  1284     0     1]
 [    0    10     0    67    22     0     1     0  3465     6]
 [    0     4     0     0     0    13     0     0     0   902]]

Accuracy:
98.72990624924687

F1 scores:
[       nan 0.98798002 0.99764953 0.94176707 0.98618133 0.99504384
 0.99028133 0.99534884 0.95480849 0.97145934]

Kappa:
0.9831744989846949
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67ddf9cb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.115, val_acc:0.396]
Epoch [2/120    avg_loss:1.726, val_acc:0.586]
Epoch [3/120    avg_loss:1.460, val_acc:0.616]
Epoch [4/120    avg_loss:1.189, val_acc:0.686]
Epoch [5/120    avg_loss:0.966, val_acc:0.640]
Epoch [6/120    avg_loss:0.835, val_acc:0.682]
Epoch [7/120    avg_loss:0.703, val_acc:0.699]
Epoch [8/120    avg_loss:0.593, val_acc:0.667]
Epoch [9/120    avg_loss:0.549, val_acc:0.629]
Epoch [10/120    avg_loss:0.528, val_acc:0.675]
Epoch [11/120    avg_loss:0.462, val_acc:0.733]
Epoch [12/120    avg_loss:0.350, val_acc:0.719]
Epoch [13/120    avg_loss:0.375, val_acc:0.787]
Epoch [14/120    avg_loss:0.313, val_acc:0.757]
Epoch [15/120    avg_loss:0.301, val_acc:0.778]
Epoch [16/120    avg_loss:0.290, val_acc:0.838]
Epoch [17/120    avg_loss:0.265, val_acc:0.878]
Epoch [18/120    avg_loss:0.227, val_acc:0.914]
Epoch [19/120    avg_loss:0.242, val_acc:0.877]
Epoch [20/120    avg_loss:0.200, val_acc:0.912]
Epoch [21/120    avg_loss:0.217, val_acc:0.918]
Epoch [22/120    avg_loss:0.201, val_acc:0.924]
Epoch [23/120    avg_loss:0.154, val_acc:0.938]
Epoch [24/120    avg_loss:0.181, val_acc:0.922]
Epoch [25/120    avg_loss:0.175, val_acc:0.948]
Epoch [26/120    avg_loss:0.261, val_acc:0.858]
Epoch [27/120    avg_loss:0.152, val_acc:0.942]
Epoch [28/120    avg_loss:0.143, val_acc:0.943]
Epoch [29/120    avg_loss:0.112, val_acc:0.957]
Epoch [30/120    avg_loss:0.110, val_acc:0.937]
Epoch [31/120    avg_loss:0.152, val_acc:0.930]
Epoch [32/120    avg_loss:0.102, val_acc:0.932]
Epoch [33/120    avg_loss:0.101, val_acc:0.946]
Epoch [34/120    avg_loss:0.080, val_acc:0.968]
Epoch [35/120    avg_loss:0.064, val_acc:0.966]
Epoch [36/120    avg_loss:0.069, val_acc:0.942]
Epoch [37/120    avg_loss:0.052, val_acc:0.969]
Epoch [38/120    avg_loss:0.108, val_acc:0.970]
Epoch [39/120    avg_loss:0.073, val_acc:0.951]
Epoch [40/120    avg_loss:0.059, val_acc:0.965]
Epoch [41/120    avg_loss:0.068, val_acc:0.973]
Epoch [42/120    avg_loss:0.045, val_acc:0.974]
Epoch [43/120    avg_loss:0.030, val_acc:0.970]
Epoch [44/120    avg_loss:0.072, val_acc:0.963]
Epoch [45/120    avg_loss:0.050, val_acc:0.973]
Epoch [46/120    avg_loss:0.086, val_acc:0.954]
Epoch [47/120    avg_loss:0.052, val_acc:0.965]
Epoch [48/120    avg_loss:0.037, val_acc:0.974]
Epoch [49/120    avg_loss:0.037, val_acc:0.976]
Epoch [50/120    avg_loss:0.033, val_acc:0.963]
Epoch [51/120    avg_loss:0.040, val_acc:0.978]
Epoch [52/120    avg_loss:0.042, val_acc:0.969]
Epoch [53/120    avg_loss:0.046, val_acc:0.982]
Epoch [54/120    avg_loss:0.040, val_acc:0.966]
Epoch [55/120    avg_loss:0.047, val_acc:0.975]
Epoch [56/120    avg_loss:0.027, val_acc:0.978]
Epoch [57/120    avg_loss:0.027, val_acc:0.969]
Epoch [58/120    avg_loss:0.036, val_acc:0.982]
Epoch [59/120    avg_loss:0.022, val_acc:0.984]
Epoch [60/120    avg_loss:0.030, val_acc:0.978]
Epoch [61/120    avg_loss:0.023, val_acc:0.978]
Epoch [62/120    avg_loss:0.017, val_acc:0.984]
Epoch [63/120    avg_loss:0.031, val_acc:0.965]
Epoch [64/120    avg_loss:0.042, val_acc:0.977]
Epoch [65/120    avg_loss:0.021, val_acc:0.967]
Epoch [66/120    avg_loss:0.042, val_acc:0.981]
Epoch [67/120    avg_loss:0.014, val_acc:0.983]
Epoch [68/120    avg_loss:0.015, val_acc:0.983]
Epoch [69/120    avg_loss:0.033, val_acc:0.969]
Epoch [70/120    avg_loss:0.032, val_acc:0.966]
Epoch [71/120    avg_loss:0.045, val_acc:0.972]
Epoch [72/120    avg_loss:0.030, val_acc:0.977]
Epoch [73/120    avg_loss:0.018, val_acc:0.978]
Epoch [74/120    avg_loss:0.021, val_acc:0.966]
Epoch [75/120    avg_loss:0.017, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.983]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.011, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.984]
Epoch [81/120    avg_loss:0.013, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.984]
Epoch [83/120    avg_loss:0.013, val_acc:0.985]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.985]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.009, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     2     0     0     1    13    13     1]
 [    0     0 17966     0    92     0    28     0     4     0]
 [    0     3     0  1914     0     0     0     0   119     0]
 [    0    23    14     0  2898     0    32     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     1     0  4870     0     0     0]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    21     4    53    19     0    27     0  3447     0]
 [    0     0     0     0     1     9     0     0     0   909]]

Accuracy:
98.80220760128215

F1 scores:
[       nan 0.9936365  0.9958704  0.95580524 0.96874478 0.99656357
 0.99023993 0.99304482 0.96338737 0.99235808]

Kappa:
0.9841433320327398
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb15c9bbe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.115, val_acc:0.206]
Epoch [2/120    avg_loss:1.734, val_acc:0.322]
Epoch [3/120    avg_loss:1.427, val_acc:0.430]
Epoch [4/120    avg_loss:1.180, val_acc:0.600]
Epoch [5/120    avg_loss:1.000, val_acc:0.697]
Epoch [6/120    avg_loss:0.870, val_acc:0.751]
Epoch [7/120    avg_loss:0.712, val_acc:0.630]
Epoch [8/120    avg_loss:0.627, val_acc:0.657]
Epoch [9/120    avg_loss:0.532, val_acc:0.718]
Epoch [10/120    avg_loss:0.452, val_acc:0.762]
Epoch [11/120    avg_loss:0.423, val_acc:0.756]
Epoch [12/120    avg_loss:0.410, val_acc:0.748]
Epoch [13/120    avg_loss:0.330, val_acc:0.803]
Epoch [14/120    avg_loss:0.297, val_acc:0.843]
Epoch [15/120    avg_loss:0.314, val_acc:0.868]
Epoch [16/120    avg_loss:0.283, val_acc:0.884]
Epoch [17/120    avg_loss:0.273, val_acc:0.937]
Epoch [18/120    avg_loss:0.227, val_acc:0.907]
Epoch [19/120    avg_loss:0.184, val_acc:0.948]
Epoch [20/120    avg_loss:0.153, val_acc:0.943]
Epoch [21/120    avg_loss:0.181, val_acc:0.941]
Epoch [22/120    avg_loss:0.168, val_acc:0.906]
Epoch [23/120    avg_loss:0.210, val_acc:0.892]
Epoch [24/120    avg_loss:0.128, val_acc:0.963]
Epoch [25/120    avg_loss:0.130, val_acc:0.961]
Epoch [26/120    avg_loss:0.103, val_acc:0.948]
Epoch [27/120    avg_loss:0.102, val_acc:0.949]
Epoch [28/120    avg_loss:0.142, val_acc:0.954]
Epoch [29/120    avg_loss:0.084, val_acc:0.975]
Epoch [30/120    avg_loss:0.096, val_acc:0.943]
Epoch [31/120    avg_loss:0.083, val_acc:0.933]
Epoch [32/120    avg_loss:0.080, val_acc:0.938]
Epoch [33/120    avg_loss:0.076, val_acc:0.903]
Epoch [34/120    avg_loss:0.091, val_acc:0.968]
Epoch [35/120    avg_loss:0.052, val_acc:0.957]
Epoch [36/120    avg_loss:0.063, val_acc:0.975]
Epoch [37/120    avg_loss:0.059, val_acc:0.973]
Epoch [38/120    avg_loss:0.061, val_acc:0.951]
Epoch [39/120    avg_loss:0.061, val_acc:0.968]
Epoch [40/120    avg_loss:0.059, val_acc:0.951]
Epoch [41/120    avg_loss:0.114, val_acc:0.969]
Epoch [42/120    avg_loss:0.060, val_acc:0.958]
Epoch [43/120    avg_loss:0.073, val_acc:0.945]
Epoch [44/120    avg_loss:0.042, val_acc:0.975]
Epoch [45/120    avg_loss:0.051, val_acc:0.971]
Epoch [46/120    avg_loss:0.053, val_acc:0.969]
Epoch [47/120    avg_loss:0.032, val_acc:0.979]
Epoch [48/120    avg_loss:0.029, val_acc:0.968]
Epoch [49/120    avg_loss:0.035, val_acc:0.977]
Epoch [50/120    avg_loss:0.030, val_acc:0.974]
Epoch [51/120    avg_loss:0.031, val_acc:0.980]
Epoch [52/120    avg_loss:0.017, val_acc:0.984]
Epoch [53/120    avg_loss:0.027, val_acc:0.956]
Epoch [54/120    avg_loss:0.032, val_acc:0.970]
Epoch [55/120    avg_loss:0.029, val_acc:0.975]
Epoch [56/120    avg_loss:0.035, val_acc:0.963]
Epoch [57/120    avg_loss:0.018, val_acc:0.973]
Epoch [58/120    avg_loss:0.031, val_acc:0.974]
Epoch [59/120    avg_loss:0.032, val_acc:0.980]
Epoch [60/120    avg_loss:0.021, val_acc:0.953]
Epoch [61/120    avg_loss:0.024, val_acc:0.976]
Epoch [62/120    avg_loss:0.019, val_acc:0.978]
Epoch [63/120    avg_loss:0.018, val_acc:0.981]
Epoch [64/120    avg_loss:0.028, val_acc:0.977]
Epoch [65/120    avg_loss:0.013, val_acc:0.981]
Epoch [66/120    avg_loss:0.010, val_acc:0.979]
Epoch [67/120    avg_loss:0.010, val_acc:0.978]
Epoch [68/120    avg_loss:0.009, val_acc:0.979]
Epoch [69/120    avg_loss:0.012, val_acc:0.978]
Epoch [70/120    avg_loss:0.010, val_acc:0.979]
Epoch [71/120    avg_loss:0.011, val_acc:0.981]
Epoch [72/120    avg_loss:0.012, val_acc:0.980]
Epoch [73/120    avg_loss:0.008, val_acc:0.980]
Epoch [74/120    avg_loss:0.008, val_acc:0.980]
Epoch [75/120    avg_loss:0.008, val_acc:0.981]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.010, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.009, val_acc:0.980]
Epoch [81/120    avg_loss:0.008, val_acc:0.980]
Epoch [82/120    avg_loss:0.009, val_acc:0.980]
Epoch [83/120    avg_loss:0.011, val_acc:0.980]
Epoch [84/120    avg_loss:0.008, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.980]
Epoch [86/120    avg_loss:0.010, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.009, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.980]
Epoch [90/120    avg_loss:0.008, val_acc:0.980]
Epoch [91/120    avg_loss:0.008, val_acc:0.980]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.980]
Epoch [95/120    avg_loss:0.010, val_acc:0.980]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.980]
Epoch [101/120    avg_loss:0.008, val_acc:0.980]
Epoch [102/120    avg_loss:0.008, val_acc:0.980]
Epoch [103/120    avg_loss:0.008, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.980]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.010, val_acc:0.980]
Epoch [116/120    avg_loss:0.009, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.010, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0     5     1     0     0    25    29     1]
 [    0     0 18011     0    46     0    26     0     7     0]
 [    0     9     0  1875     0     0     0     0   146     6]
 [    0     7     4     0  2939     0    16     0     3     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     0     0     0  4834     0    24     0]
 [    0    12     0     0     0     0     0  1277     0     1]
 [    0    34     0    50    47     0     3     0  3437     0]
 [    0     0     0     0     0     7     0     0     0   912]]

Accuracy:
98.71785602390764

F1 scores:
[       nan 0.99043918 0.99714879 0.94553707 0.97885096 0.99732518
 0.99087834 0.98533951 0.95247333 0.99022801]

Kappa:
0.9830202537471584
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd33bbd6b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.065, val_acc:0.181]
Epoch [2/120    avg_loss:1.694, val_acc:0.215]
Epoch [3/120    avg_loss:1.399, val_acc:0.590]
Epoch [4/120    avg_loss:1.228, val_acc:0.678]
Epoch [5/120    avg_loss:1.070, val_acc:0.723]
Epoch [6/120    avg_loss:0.926, val_acc:0.738]
Epoch [7/120    avg_loss:0.788, val_acc:0.711]
Epoch [8/120    avg_loss:0.684, val_acc:0.698]
Epoch [9/120    avg_loss:0.598, val_acc:0.608]
Epoch [10/120    avg_loss:0.611, val_acc:0.615]
Epoch [11/120    avg_loss:0.512, val_acc:0.686]
Epoch [12/120    avg_loss:0.395, val_acc:0.785]
Epoch [13/120    avg_loss:0.367, val_acc:0.797]
Epoch [14/120    avg_loss:0.348, val_acc:0.835]
Epoch [15/120    avg_loss:0.385, val_acc:0.784]
Epoch [16/120    avg_loss:0.343, val_acc:0.831]
Epoch [17/120    avg_loss:0.248, val_acc:0.879]
Epoch [18/120    avg_loss:0.226, val_acc:0.870]
Epoch [19/120    avg_loss:0.201, val_acc:0.902]
Epoch [20/120    avg_loss:0.214, val_acc:0.891]
Epoch [21/120    avg_loss:0.202, val_acc:0.922]
Epoch [22/120    avg_loss:0.162, val_acc:0.908]
Epoch [23/120    avg_loss:0.148, val_acc:0.933]
Epoch [24/120    avg_loss:0.161, val_acc:0.920]
Epoch [25/120    avg_loss:0.126, val_acc:0.938]
Epoch [26/120    avg_loss:0.162, val_acc:0.910]
Epoch [27/120    avg_loss:0.136, val_acc:0.929]
Epoch [28/120    avg_loss:0.130, val_acc:0.942]
Epoch [29/120    avg_loss:0.101, val_acc:0.942]
Epoch [30/120    avg_loss:0.116, val_acc:0.933]
Epoch [31/120    avg_loss:0.105, val_acc:0.934]
Epoch [32/120    avg_loss:0.092, val_acc:0.942]
Epoch [33/120    avg_loss:0.096, val_acc:0.936]
Epoch [34/120    avg_loss:0.104, val_acc:0.943]
Epoch [35/120    avg_loss:0.079, val_acc:0.954]
Epoch [36/120    avg_loss:0.083, val_acc:0.948]
Epoch [37/120    avg_loss:0.070, val_acc:0.954]
Epoch [38/120    avg_loss:0.073, val_acc:0.877]
Epoch [39/120    avg_loss:0.078, val_acc:0.951]
Epoch [40/120    avg_loss:0.079, val_acc:0.900]
Epoch [41/120    avg_loss:0.122, val_acc:0.954]
Epoch [42/120    avg_loss:0.048, val_acc:0.961]
Epoch [43/120    avg_loss:0.050, val_acc:0.952]
Epoch [44/120    avg_loss:0.039, val_acc:0.961]
Epoch [45/120    avg_loss:0.032, val_acc:0.972]
Epoch [46/120    avg_loss:0.039, val_acc:0.970]
Epoch [47/120    avg_loss:0.029, val_acc:0.974]
Epoch [48/120    avg_loss:0.054, val_acc:0.961]
Epoch [49/120    avg_loss:0.052, val_acc:0.961]
Epoch [50/120    avg_loss:0.041, val_acc:0.960]
Epoch [51/120    avg_loss:0.038, val_acc:0.968]
Epoch [52/120    avg_loss:0.026, val_acc:0.974]
Epoch [53/120    avg_loss:0.036, val_acc:0.942]
Epoch [54/120    avg_loss:0.042, val_acc:0.961]
Epoch [55/120    avg_loss:0.021, val_acc:0.974]
Epoch [56/120    avg_loss:0.016, val_acc:0.922]
Epoch [57/120    avg_loss:0.105, val_acc:0.947]
Epoch [58/120    avg_loss:0.085, val_acc:0.962]
Epoch [59/120    avg_loss:0.031, val_acc:0.972]
Epoch [60/120    avg_loss:0.025, val_acc:0.975]
Epoch [61/120    avg_loss:0.016, val_acc:0.974]
Epoch [62/120    avg_loss:0.021, val_acc:0.947]
Epoch [63/120    avg_loss:0.023, val_acc:0.962]
Epoch [64/120    avg_loss:0.047, val_acc:0.958]
Epoch [65/120    avg_loss:0.070, val_acc:0.956]
Epoch [66/120    avg_loss:0.041, val_acc:0.963]
Epoch [67/120    avg_loss:0.019, val_acc:0.968]
Epoch [68/120    avg_loss:0.031, val_acc:0.970]
Epoch [69/120    avg_loss:0.024, val_acc:0.980]
Epoch [70/120    avg_loss:0.015, val_acc:0.972]
Epoch [71/120    avg_loss:0.015, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.016, val_acc:0.975]
Epoch [74/120    avg_loss:0.024, val_acc:0.968]
Epoch [75/120    avg_loss:0.014, val_acc:0.975]
Epoch [76/120    avg_loss:0.014, val_acc:0.974]
Epoch [77/120    avg_loss:0.039, val_acc:0.965]
Epoch [78/120    avg_loss:0.046, val_acc:0.979]
Epoch [79/120    avg_loss:0.046, val_acc:0.973]
Epoch [80/120    avg_loss:0.027, val_acc:0.975]
Epoch [81/120    avg_loss:0.024, val_acc:0.970]
Epoch [82/120    avg_loss:0.013, val_acc:0.978]
Epoch [83/120    avg_loss:0.009, val_acc:0.978]
Epoch [84/120    avg_loss:0.009, val_acc:0.977]
Epoch [85/120    avg_loss:0.008, val_acc:0.978]
Epoch [86/120    avg_loss:0.009, val_acc:0.978]
Epoch [87/120    avg_loss:0.008, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.979]
Epoch [90/120    avg_loss:0.006, val_acc:0.980]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.012, val_acc:0.977]
Epoch [93/120    avg_loss:0.007, val_acc:0.979]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.006, val_acc:0.981]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.006, val_acc:0.979]
Epoch [102/120    avg_loss:0.008, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.005, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.979]
Epoch [108/120    avg_loss:0.006, val_acc:0.979]
Epoch [109/120    avg_loss:0.005, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.979]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.006, val_acc:0.979]
Epoch [114/120    avg_loss:0.009, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     1     0     0     0     0     0    20     3]
 [    0     0 18013     0    36     0    41     0     0     0]
 [    0     2     1  1884     0     0     0     0   149     0]
 [    0    10     7     4  2936     3    10     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     4     0  4865     0     0     0]
 [    0    22     0     0     0     0     0  1268     0     0]
 [    0    34     0    51    33     0     0     1  3452     0]
 [    0     1     0     0     0     8     0     0     0   910]]

Accuracy:
98.91065962933507

F1 scores:
[       nan 0.99279572 0.99736995 0.94792453 0.98177562 0.99580313
 0.99346539 0.99101211 0.95968863 0.99344978]

Kappa:
0.9855712743447688
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ccd48fb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.131, val_acc:0.142]
Epoch [2/120    avg_loss:1.766, val_acc:0.119]
Epoch [3/120    avg_loss:1.527, val_acc:0.615]
Epoch [4/120    avg_loss:1.328, val_acc:0.693]
Epoch [5/120    avg_loss:1.048, val_acc:0.746]
Epoch [6/120    avg_loss:0.830, val_acc:0.693]
Epoch [7/120    avg_loss:0.648, val_acc:0.729]
Epoch [8/120    avg_loss:0.518, val_acc:0.698]
Epoch [9/120    avg_loss:0.446, val_acc:0.770]
Epoch [10/120    avg_loss:0.419, val_acc:0.774]
Epoch [11/120    avg_loss:0.394, val_acc:0.801]
Epoch [12/120    avg_loss:0.348, val_acc:0.876]
Epoch [13/120    avg_loss:0.315, val_acc:0.869]
Epoch [14/120    avg_loss:0.232, val_acc:0.836]
Epoch [15/120    avg_loss:0.241, val_acc:0.946]
Epoch [16/120    avg_loss:0.221, val_acc:0.914]
Epoch [17/120    avg_loss:0.187, val_acc:0.940]
Epoch [18/120    avg_loss:0.166, val_acc:0.938]
Epoch [19/120    avg_loss:0.160, val_acc:0.938]
Epoch [20/120    avg_loss:0.152, val_acc:0.943]
Epoch [21/120    avg_loss:0.114, val_acc:0.955]
Epoch [22/120    avg_loss:0.101, val_acc:0.977]
Epoch [23/120    avg_loss:0.115, val_acc:0.942]
Epoch [24/120    avg_loss:0.094, val_acc:0.958]
Epoch [25/120    avg_loss:0.095, val_acc:0.965]
Epoch [26/120    avg_loss:0.078, val_acc:0.945]
Epoch [27/120    avg_loss:0.091, val_acc:0.953]
Epoch [28/120    avg_loss:0.084, val_acc:0.972]
Epoch [29/120    avg_loss:0.073, val_acc:0.953]
Epoch [30/120    avg_loss:0.059, val_acc:0.973]
Epoch [31/120    avg_loss:0.058, val_acc:0.979]
Epoch [32/120    avg_loss:0.079, val_acc:0.978]
Epoch [33/120    avg_loss:0.054, val_acc:0.978]
Epoch [34/120    avg_loss:0.053, val_acc:0.977]
Epoch [35/120    avg_loss:0.048, val_acc:0.983]
Epoch [36/120    avg_loss:0.058, val_acc:0.967]
Epoch [37/120    avg_loss:0.059, val_acc:0.973]
Epoch [38/120    avg_loss:0.034, val_acc:0.988]
Epoch [39/120    avg_loss:0.023, val_acc:0.971]
Epoch [40/120    avg_loss:0.034, val_acc:0.959]
Epoch [41/120    avg_loss:0.055, val_acc:0.983]
Epoch [42/120    avg_loss:0.032, val_acc:0.981]
Epoch [43/120    avg_loss:0.050, val_acc:0.948]
Epoch [44/120    avg_loss:0.059, val_acc:0.981]
Epoch [45/120    avg_loss:0.027, val_acc:0.965]
Epoch [46/120    avg_loss:0.042, val_acc:0.974]
Epoch [47/120    avg_loss:0.022, val_acc:0.985]
Epoch [48/120    avg_loss:0.020, val_acc:0.980]
Epoch [49/120    avg_loss:0.031, val_acc:0.976]
Epoch [50/120    avg_loss:0.060, val_acc:0.973]
Epoch [51/120    avg_loss:0.027, val_acc:0.986]
Epoch [52/120    avg_loss:0.013, val_acc:0.986]
Epoch [53/120    avg_loss:0.014, val_acc:0.986]
Epoch [54/120    avg_loss:0.014, val_acc:0.986]
Epoch [55/120    avg_loss:0.012, val_acc:0.987]
Epoch [56/120    avg_loss:0.014, val_acc:0.988]
Epoch [57/120    avg_loss:0.012, val_acc:0.988]
Epoch [58/120    avg_loss:0.012, val_acc:0.988]
Epoch [59/120    avg_loss:0.009, val_acc:0.989]
Epoch [60/120    avg_loss:0.011, val_acc:0.988]
Epoch [61/120    avg_loss:0.011, val_acc:0.988]
Epoch [62/120    avg_loss:0.010, val_acc:0.988]
Epoch [63/120    avg_loss:0.009, val_acc:0.988]
Epoch [64/120    avg_loss:0.012, val_acc:0.988]
Epoch [65/120    avg_loss:0.011, val_acc:0.988]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.987]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.008, val_acc:0.988]
Epoch [70/120    avg_loss:0.011, val_acc:0.988]
Epoch [71/120    avg_loss:0.011, val_acc:0.987]
Epoch [72/120    avg_loss:0.010, val_acc:0.987]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.009, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     0     0     0     1    37     1]
 [    0     0 17954     0    58     0    77     0     1     0]
 [    0    16     0  1924     0     0     0     0    91     5]
 [    0    19     3     0  2918     0    18     0    12     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4871     0     0     0]
 [    0    17     0     0     0     0     1  1272     0     0]
 [    0    48     0    46    53     0     0     0  3424     0]
 [    0     0     0     0     2    25     0     0     0   892]]

Accuracy:
98.6985756633649

F1 scores:
[       nan 0.98924565 0.99595052 0.96055916 0.97218058 0.99051233
 0.98953784 0.99258681 0.95964126 0.98075866]

Kappa:
0.982774433007571
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1304ddbe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.099, val_acc:0.508]
Epoch [2/120    avg_loss:1.759, val_acc:0.302]
Epoch [3/120    avg_loss:1.491, val_acc:0.346]
Epoch [4/120    avg_loss:1.255, val_acc:0.423]
Epoch [5/120    avg_loss:1.056, val_acc:0.438]
Epoch [6/120    avg_loss:0.885, val_acc:0.693]
Epoch [7/120    avg_loss:0.721, val_acc:0.677]
Epoch [8/120    avg_loss:0.597, val_acc:0.736]
Epoch [9/120    avg_loss:0.522, val_acc:0.758]
Epoch [10/120    avg_loss:0.448, val_acc:0.860]
Epoch [11/120    avg_loss:0.387, val_acc:0.806]
Epoch [12/120    avg_loss:0.351, val_acc:0.857]
Epoch [13/120    avg_loss:0.323, val_acc:0.875]
Epoch [14/120    avg_loss:0.253, val_acc:0.863]
Epoch [15/120    avg_loss:0.265, val_acc:0.891]
Epoch [16/120    avg_loss:0.306, val_acc:0.863]
Epoch [17/120    avg_loss:0.287, val_acc:0.927]
Epoch [18/120    avg_loss:0.209, val_acc:0.926]
Epoch [19/120    avg_loss:0.168, val_acc:0.879]
Epoch [20/120    avg_loss:0.151, val_acc:0.950]
Epoch [21/120    avg_loss:0.139, val_acc:0.903]
Epoch [22/120    avg_loss:0.154, val_acc:0.937]
Epoch [23/120    avg_loss:0.136, val_acc:0.906]
Epoch [24/120    avg_loss:0.136, val_acc:0.938]
Epoch [25/120    avg_loss:0.106, val_acc:0.955]
Epoch [26/120    avg_loss:0.134, val_acc:0.953]
Epoch [27/120    avg_loss:0.106, val_acc:0.966]
Epoch [28/120    avg_loss:0.071, val_acc:0.949]
Epoch [29/120    avg_loss:0.087, val_acc:0.963]
Epoch [30/120    avg_loss:0.090, val_acc:0.968]
Epoch [31/120    avg_loss:0.058, val_acc:0.958]
Epoch [32/120    avg_loss:0.055, val_acc:0.971]
Epoch [33/120    avg_loss:0.047, val_acc:0.959]
Epoch [34/120    avg_loss:0.069, val_acc:0.963]
Epoch [35/120    avg_loss:0.049, val_acc:0.973]
Epoch [36/120    avg_loss:0.052, val_acc:0.976]
Epoch [37/120    avg_loss:0.048, val_acc:0.947]
Epoch [38/120    avg_loss:0.052, val_acc:0.973]
Epoch [39/120    avg_loss:0.037, val_acc:0.983]
Epoch [40/120    avg_loss:0.063, val_acc:0.968]
Epoch [41/120    avg_loss:0.043, val_acc:0.976]
Epoch [42/120    avg_loss:0.037, val_acc:0.922]
Epoch [43/120    avg_loss:0.040, val_acc:0.983]
Epoch [44/120    avg_loss:0.022, val_acc:0.983]
Epoch [45/120    avg_loss:0.030, val_acc:0.984]
Epoch [46/120    avg_loss:0.031, val_acc:0.982]
Epoch [47/120    avg_loss:0.036, val_acc:0.979]
Epoch [48/120    avg_loss:0.035, val_acc:0.970]
Epoch [49/120    avg_loss:0.032, val_acc:0.975]
Epoch [50/120    avg_loss:0.022, val_acc:0.983]
Epoch [51/120    avg_loss:0.026, val_acc:0.981]
Epoch [52/120    avg_loss:0.023, val_acc:0.991]
Epoch [53/120    avg_loss:0.040, val_acc:0.986]
Epoch [54/120    avg_loss:0.048, val_acc:0.973]
Epoch [55/120    avg_loss:0.030, val_acc:0.988]
Epoch [56/120    avg_loss:0.030, val_acc:0.983]
Epoch [57/120    avg_loss:0.024, val_acc:0.981]
Epoch [58/120    avg_loss:0.024, val_acc:0.988]
Epoch [59/120    avg_loss:0.020, val_acc:0.976]
Epoch [60/120    avg_loss:0.033, val_acc:0.983]
Epoch [61/120    avg_loss:0.041, val_acc:0.952]
Epoch [62/120    avg_loss:0.023, val_acc:0.970]
Epoch [63/120    avg_loss:0.016, val_acc:0.984]
Epoch [64/120    avg_loss:0.017, val_acc:0.979]
Epoch [65/120    avg_loss:0.019, val_acc:0.985]
Epoch [66/120    avg_loss:0.018, val_acc:0.989]
Epoch [67/120    avg_loss:0.009, val_acc:0.991]
Epoch [68/120    avg_loss:0.010, val_acc:0.993]
Epoch [69/120    avg_loss:0.016, val_acc:0.992]
Epoch [70/120    avg_loss:0.008, val_acc:0.993]
Epoch [71/120    avg_loss:0.008, val_acc:0.993]
Epoch [72/120    avg_loss:0.008, val_acc:0.993]
Epoch [73/120    avg_loss:0.007, val_acc:0.994]
Epoch [74/120    avg_loss:0.007, val_acc:0.994]
Epoch [75/120    avg_loss:0.007, val_acc:0.993]
Epoch [76/120    avg_loss:0.006, val_acc:0.993]
Epoch [77/120    avg_loss:0.008, val_acc:0.993]
Epoch [78/120    avg_loss:0.008, val_acc:0.994]
Epoch [79/120    avg_loss:0.005, val_acc:0.994]
Epoch [80/120    avg_loss:0.008, val_acc:0.994]
Epoch [81/120    avg_loss:0.007, val_acc:0.994]
Epoch [82/120    avg_loss:0.007, val_acc:0.994]
Epoch [83/120    avg_loss:0.009, val_acc:0.994]
Epoch [84/120    avg_loss:0.007, val_acc:0.993]
Epoch [85/120    avg_loss:0.005, val_acc:0.994]
Epoch [86/120    avg_loss:0.009, val_acc:0.994]
Epoch [87/120    avg_loss:0.006, val_acc:0.994]
Epoch [88/120    avg_loss:0.007, val_acc:0.995]
Epoch [89/120    avg_loss:0.005, val_acc:0.994]
Epoch [90/120    avg_loss:0.009, val_acc:0.993]
Epoch [91/120    avg_loss:0.006, val_acc:0.993]
Epoch [92/120    avg_loss:0.006, val_acc:0.994]
Epoch [93/120    avg_loss:0.006, val_acc:0.994]
Epoch [94/120    avg_loss:0.007, val_acc:0.994]
Epoch [95/120    avg_loss:0.006, val_acc:0.994]
Epoch [96/120    avg_loss:0.005, val_acc:0.994]
Epoch [97/120    avg_loss:0.007, val_acc:0.993]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.007, val_acc:0.995]
Epoch [101/120    avg_loss:0.007, val_acc:0.994]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.007, val_acc:0.993]
Epoch [104/120    avg_loss:0.006, val_acc:0.994]
Epoch [105/120    avg_loss:0.007, val_acc:0.993]
Epoch [106/120    avg_loss:0.005, val_acc:0.994]
Epoch [107/120    avg_loss:0.006, val_acc:0.994]
Epoch [108/120    avg_loss:0.006, val_acc:0.995]
Epoch [109/120    avg_loss:0.007, val_acc:0.995]
Epoch [110/120    avg_loss:0.005, val_acc:0.995]
Epoch [111/120    avg_loss:0.007, val_acc:0.995]
Epoch [112/120    avg_loss:0.006, val_acc:0.996]
Epoch [113/120    avg_loss:0.005, val_acc:0.995]
Epoch [114/120    avg_loss:0.007, val_acc:0.995]
Epoch [115/120    avg_loss:0.005, val_acc:0.995]
Epoch [116/120    avg_loss:0.006, val_acc:0.996]
Epoch [117/120    avg_loss:0.007, val_acc:0.995]
Epoch [118/120    avg_loss:0.005, val_acc:0.995]
Epoch [119/120    avg_loss:0.009, val_acc:0.993]
Epoch [120/120    avg_loss:0.007, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0     0     0     2     6    23    10]
 [    0     0 18033     0    30     0    21     0     6     0]
 [    0    14     0  1952     0     0     0     0    66     4]
 [    0    12     3     0  2951     0     4     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    16     2     0     0  4860     0     0     0]
 [    0     9     0     0     0     0     0  1278     1     2]
 [    0    23     0    29    47     0     0     0  3471     1]
 [    0     0     0     0     3    18     0     0     0   898]]

Accuracy:
99.14684404598366

F1 scores:
[       nan 0.99231426 0.99789718 0.97138592 0.98317508 0.99315068
 0.99539171 0.99300699 0.97226891 0.97928026]

Kappa:
0.9886991848028739
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d7cd1ab70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.031, val_acc:0.122]
Epoch [2/120    avg_loss:1.672, val_acc:0.268]
Epoch [3/120    avg_loss:1.423, val_acc:0.624]
Epoch [4/120    avg_loss:1.249, val_acc:0.515]
Epoch [5/120    avg_loss:1.038, val_acc:0.624]
Epoch [6/120    avg_loss:0.904, val_acc:0.663]
Epoch [7/120    avg_loss:0.702, val_acc:0.634]
Epoch [8/120    avg_loss:0.633, val_acc:0.650]
Epoch [9/120    avg_loss:0.551, val_acc:0.751]
Epoch [10/120    avg_loss:0.462, val_acc:0.708]
Epoch [11/120    avg_loss:0.380, val_acc:0.775]
Epoch [12/120    avg_loss:0.352, val_acc:0.802]
Epoch [13/120    avg_loss:0.314, val_acc:0.791]
Epoch [14/120    avg_loss:0.280, val_acc:0.823]
Epoch [15/120    avg_loss:0.270, val_acc:0.844]
Epoch [16/120    avg_loss:0.267, val_acc:0.747]
Epoch [17/120    avg_loss:0.261, val_acc:0.847]
Epoch [18/120    avg_loss:0.230, val_acc:0.822]
Epoch [19/120    avg_loss:0.205, val_acc:0.867]
Epoch [20/120    avg_loss:0.201, val_acc:0.855]
Epoch [21/120    avg_loss:0.225, val_acc:0.875]
Epoch [22/120    avg_loss:0.224, val_acc:0.851]
Epoch [23/120    avg_loss:0.164, val_acc:0.917]
Epoch [24/120    avg_loss:0.154, val_acc:0.895]
Epoch [25/120    avg_loss:0.104, val_acc:0.958]
Epoch [26/120    avg_loss:0.117, val_acc:0.941]
Epoch [27/120    avg_loss:0.140, val_acc:0.888]
Epoch [28/120    avg_loss:0.108, val_acc:0.947]
Epoch [29/120    avg_loss:0.075, val_acc:0.963]
Epoch [30/120    avg_loss:0.072, val_acc:0.960]
Epoch [31/120    avg_loss:0.067, val_acc:0.921]
Epoch [32/120    avg_loss:0.141, val_acc:0.928]
Epoch [33/120    avg_loss:0.087, val_acc:0.963]
Epoch [34/120    avg_loss:0.070, val_acc:0.921]
Epoch [35/120    avg_loss:0.055, val_acc:0.952]
Epoch [36/120    avg_loss:0.049, val_acc:0.971]
Epoch [37/120    avg_loss:0.058, val_acc:0.879]
Epoch [38/120    avg_loss:0.128, val_acc:0.918]
Epoch [39/120    avg_loss:0.076, val_acc:0.949]
Epoch [40/120    avg_loss:0.086, val_acc:0.970]
Epoch [41/120    avg_loss:0.059, val_acc:0.978]
Epoch [42/120    avg_loss:0.036, val_acc:0.970]
Epoch [43/120    avg_loss:0.045, val_acc:0.966]
Epoch [44/120    avg_loss:0.056, val_acc:0.945]
Epoch [45/120    avg_loss:0.045, val_acc:0.969]
Epoch [46/120    avg_loss:0.028, val_acc:0.976]
Epoch [47/120    avg_loss:0.049, val_acc:0.959]
Epoch [48/120    avg_loss:0.033, val_acc:0.961]
Epoch [49/120    avg_loss:0.035, val_acc:0.969]
Epoch [50/120    avg_loss:0.049, val_acc:0.975]
Epoch [51/120    avg_loss:0.079, val_acc:0.951]
Epoch [52/120    avg_loss:0.051, val_acc:0.976]
Epoch [53/120    avg_loss:0.037, val_acc:0.973]
Epoch [54/120    avg_loss:0.027, val_acc:0.972]
Epoch [55/120    avg_loss:0.020, val_acc:0.975]
Epoch [56/120    avg_loss:0.016, val_acc:0.976]
Epoch [57/120    avg_loss:0.014, val_acc:0.975]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.981]
Epoch [60/120    avg_loss:0.016, val_acc:0.980]
Epoch [61/120    avg_loss:0.014, val_acc:0.980]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.012, val_acc:0.979]
Epoch [64/120    avg_loss:0.013, val_acc:0.983]
Epoch [65/120    avg_loss:0.013, val_acc:0.981]
Epoch [66/120    avg_loss:0.012, val_acc:0.982]
Epoch [67/120    avg_loss:0.017, val_acc:0.981]
Epoch [68/120    avg_loss:0.013, val_acc:0.982]
Epoch [69/120    avg_loss:0.013, val_acc:0.983]
Epoch [70/120    avg_loss:0.011, val_acc:0.984]
Epoch [71/120    avg_loss:0.010, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.013, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.010, val_acc:0.983]
Epoch [77/120    avg_loss:0.013, val_acc:0.983]
Epoch [78/120    avg_loss:0.013, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.015, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.011, val_acc:0.983]
Epoch [99/120    avg_loss:0.014, val_acc:0.983]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     0     0     0    29     6     0]
 [    0     6 18066     0    17     0     1     0     0     0]
 [    0     0     1  1958     0     0     0     0    74     3]
 [    0    18    17     0  2913     0    15     0     5     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4858     0     1     0]
 [    0     1     0     0     0     0     0  1287     0     2]
 [    0     2     0    29    51     0     0     0  3486     3]
 [    0     0     0     0    14    20     0     0     0   885]]

Accuracy:
99.18540476706914

F1 scores:
[       nan 0.99517735 0.99831459 0.97340293 0.97637004 0.99239544
 0.99630845 0.98772064 0.97606048 0.9746696 ]

Kappa:
0.9892046605890313
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f100ffbab70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.084, val_acc:0.177]
Epoch [2/120    avg_loss:1.706, val_acc:0.499]
Epoch [3/120    avg_loss:1.477, val_acc:0.496]
Epoch [4/120    avg_loss:1.261, val_acc:0.561]
Epoch [5/120    avg_loss:1.057, val_acc:0.609]
Epoch [6/120    avg_loss:0.828, val_acc:0.677]
Epoch [7/120    avg_loss:0.678, val_acc:0.692]
Epoch [8/120    avg_loss:0.528, val_acc:0.744]
Epoch [9/120    avg_loss:0.464, val_acc:0.806]
Epoch [10/120    avg_loss:0.384, val_acc:0.778]
Epoch [11/120    avg_loss:0.480, val_acc:0.763]
Epoch [12/120    avg_loss:0.431, val_acc:0.786]
Epoch [13/120    avg_loss:0.388, val_acc:0.819]
Epoch [14/120    avg_loss:0.281, val_acc:0.879]
Epoch [15/120    avg_loss:0.296, val_acc:0.880]
Epoch [16/120    avg_loss:0.263, val_acc:0.868]
Epoch [17/120    avg_loss:0.232, val_acc:0.893]
Epoch [18/120    avg_loss:0.229, val_acc:0.839]
Epoch [19/120    avg_loss:0.188, val_acc:0.935]
Epoch [20/120    avg_loss:0.172, val_acc:0.951]
Epoch [21/120    avg_loss:0.151, val_acc:0.920]
Epoch [22/120    avg_loss:0.375, val_acc:0.637]
Epoch [23/120    avg_loss:0.719, val_acc:0.699]
Epoch [24/120    avg_loss:0.586, val_acc:0.718]
Epoch [25/120    avg_loss:0.414, val_acc:0.771]
Epoch [26/120    avg_loss:0.317, val_acc:0.838]
Epoch [27/120    avg_loss:0.285, val_acc:0.846]
Epoch [28/120    avg_loss:0.226, val_acc:0.930]
Epoch [29/120    avg_loss:0.193, val_acc:0.940]
Epoch [30/120    avg_loss:0.158, val_acc:0.948]
Epoch [31/120    avg_loss:0.138, val_acc:0.911]
Epoch [32/120    avg_loss:0.132, val_acc:0.953]
Epoch [33/120    avg_loss:0.123, val_acc:0.944]
Epoch [34/120    avg_loss:0.119, val_acc:0.934]
Epoch [35/120    avg_loss:0.111, val_acc:0.938]
Epoch [36/120    avg_loss:0.109, val_acc:0.960]
Epoch [37/120    avg_loss:0.064, val_acc:0.963]
Epoch [38/120    avg_loss:0.080, val_acc:0.961]
Epoch [39/120    avg_loss:0.090, val_acc:0.952]
Epoch [40/120    avg_loss:0.071, val_acc:0.957]
Epoch [41/120    avg_loss:0.061, val_acc:0.958]
Epoch [42/120    avg_loss:0.095, val_acc:0.919]
Epoch [43/120    avg_loss:0.132, val_acc:0.952]
Epoch [44/120    avg_loss:0.075, val_acc:0.968]
Epoch [45/120    avg_loss:0.098, val_acc:0.965]
Epoch [46/120    avg_loss:0.059, val_acc:0.961]
Epoch [47/120    avg_loss:0.059, val_acc:0.962]
Epoch [48/120    avg_loss:0.039, val_acc:0.970]
Epoch [49/120    avg_loss:0.067, val_acc:0.943]
Epoch [50/120    avg_loss:0.063, val_acc:0.955]
Epoch [51/120    avg_loss:0.050, val_acc:0.967]
Epoch [52/120    avg_loss:0.032, val_acc:0.966]
Epoch [53/120    avg_loss:0.049, val_acc:0.971]
Epoch [54/120    avg_loss:0.023, val_acc:0.975]
Epoch [55/120    avg_loss:0.052, val_acc:0.957]
Epoch [56/120    avg_loss:0.050, val_acc:0.968]
Epoch [57/120    avg_loss:0.037, val_acc:0.967]
Epoch [58/120    avg_loss:0.036, val_acc:0.958]
Epoch [59/120    avg_loss:0.060, val_acc:0.973]
Epoch [60/120    avg_loss:0.034, val_acc:0.963]
Epoch [61/120    avg_loss:0.045, val_acc:0.968]
Epoch [62/120    avg_loss:0.034, val_acc:0.976]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.037, val_acc:0.965]
Epoch [65/120    avg_loss:0.028, val_acc:0.955]
Epoch [66/120    avg_loss:0.023, val_acc:0.975]
Epoch [67/120    avg_loss:0.025, val_acc:0.973]
Epoch [68/120    avg_loss:0.031, val_acc:0.972]
Epoch [69/120    avg_loss:0.027, val_acc:0.968]
Epoch [70/120    avg_loss:0.018, val_acc:0.982]
Epoch [71/120    avg_loss:0.013, val_acc:0.982]
Epoch [72/120    avg_loss:0.014, val_acc:0.980]
Epoch [73/120    avg_loss:0.023, val_acc:0.957]
Epoch [74/120    avg_loss:0.115, val_acc:0.978]
Epoch [75/120    avg_loss:0.037, val_acc:0.961]
Epoch [76/120    avg_loss:0.036, val_acc:0.974]
Epoch [77/120    avg_loss:0.028, val_acc:0.966]
Epoch [78/120    avg_loss:0.023, val_acc:0.978]
Epoch [79/120    avg_loss:0.024, val_acc:0.971]
Epoch [80/120    avg_loss:0.023, val_acc:0.974]
Epoch [81/120    avg_loss:0.013, val_acc:0.976]
Epoch [82/120    avg_loss:0.014, val_acc:0.974]
Epoch [83/120    avg_loss:0.012, val_acc:0.980]
Epoch [84/120    avg_loss:0.028, val_acc:0.975]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.008, val_acc:0.981]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.979]
Epoch [93/120    avg_loss:0.007, val_acc:0.978]
Epoch [94/120    avg_loss:0.007, val_acc:0.978]
Epoch [95/120    avg_loss:0.011, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.978]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.010, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.008, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.006, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     3     0     0     0    15    18]
 [    0     0 17895     0    69     0   125     0     1     0]
 [    0     0     0  1904     0     0     0     0   131     1]
 [    0    11     3    10  2924     0    20     1     0     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4871     0     0     0]
 [    0     5     0     0     0     3     0  1281     0     1]
 [    0     7     0    35    41     0     1     0  3487     0]
 [    0     0     0     0    17     9     0     0     0   893]]

Accuracy:
98.70580579856843

F1 scores:
[       nan 0.99540892 0.99430476 0.95558344 0.97046133 0.99542334
 0.98453765 0.99611198 0.96793893 0.973297  ]

Kappa:
0.9828835267455069
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ee2e4abe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.083, val_acc:0.396]
Epoch [2/120    avg_loss:1.712, val_acc:0.503]
Epoch [3/120    avg_loss:1.454, val_acc:0.532]
Epoch [4/120    avg_loss:1.211, val_acc:0.698]
Epoch [5/120    avg_loss:1.012, val_acc:0.712]
Epoch [6/120    avg_loss:0.844, val_acc:0.751]
Epoch [7/120    avg_loss:0.665, val_acc:0.681]
Epoch [8/120    avg_loss:0.545, val_acc:0.807]
Epoch [9/120    avg_loss:0.444, val_acc:0.724]
Epoch [10/120    avg_loss:0.421, val_acc:0.876]
Epoch [11/120    avg_loss:0.355, val_acc:0.839]
Epoch [12/120    avg_loss:0.290, val_acc:0.853]
Epoch [13/120    avg_loss:0.288, val_acc:0.887]
Epoch [14/120    avg_loss:0.293, val_acc:0.866]
Epoch [15/120    avg_loss:0.230, val_acc:0.882]
Epoch [16/120    avg_loss:0.207, val_acc:0.908]
Epoch [17/120    avg_loss:0.184, val_acc:0.912]
Epoch [18/120    avg_loss:0.181, val_acc:0.895]
Epoch [19/120    avg_loss:0.208, val_acc:0.878]
Epoch [20/120    avg_loss:0.147, val_acc:0.912]
Epoch [21/120    avg_loss:0.154, val_acc:0.865]
Epoch [22/120    avg_loss:0.197, val_acc:0.928]
Epoch [23/120    avg_loss:0.129, val_acc:0.925]
Epoch [24/120    avg_loss:0.108, val_acc:0.926]
Epoch [25/120    avg_loss:0.102, val_acc:0.893]
Epoch [26/120    avg_loss:0.180, val_acc:0.895]
Epoch [27/120    avg_loss:0.181, val_acc:0.895]
Epoch [28/120    avg_loss:0.142, val_acc:0.922]
Epoch [29/120    avg_loss:0.133, val_acc:0.935]
Epoch [30/120    avg_loss:0.095, val_acc:0.940]
Epoch [31/120    avg_loss:0.119, val_acc:0.950]
Epoch [32/120    avg_loss:0.091, val_acc:0.956]
Epoch [33/120    avg_loss:0.094, val_acc:0.951]
Epoch [34/120    avg_loss:0.076, val_acc:0.930]
Epoch [35/120    avg_loss:0.079, val_acc:0.961]
Epoch [36/120    avg_loss:0.082, val_acc:0.961]
Epoch [37/120    avg_loss:0.045, val_acc:0.958]
Epoch [38/120    avg_loss:0.051, val_acc:0.965]
Epoch [39/120    avg_loss:0.098, val_acc:0.953]
Epoch [40/120    avg_loss:0.053, val_acc:0.917]
Epoch [41/120    avg_loss:0.045, val_acc:0.963]
Epoch [42/120    avg_loss:0.043, val_acc:0.964]
Epoch [43/120    avg_loss:0.062, val_acc:0.948]
Epoch [44/120    avg_loss:0.048, val_acc:0.962]
Epoch [45/120    avg_loss:0.034, val_acc:0.972]
Epoch [46/120    avg_loss:0.034, val_acc:0.973]
Epoch [47/120    avg_loss:0.052, val_acc:0.964]
Epoch [48/120    avg_loss:0.039, val_acc:0.949]
Epoch [49/120    avg_loss:0.034, val_acc:0.975]
Epoch [50/120    avg_loss:0.045, val_acc:0.977]
Epoch [51/120    avg_loss:0.067, val_acc:0.953]
Epoch [52/120    avg_loss:0.058, val_acc:0.969]
Epoch [53/120    avg_loss:0.056, val_acc:0.953]
Epoch [54/120    avg_loss:0.042, val_acc:0.977]
Epoch [55/120    avg_loss:0.049, val_acc:0.967]
Epoch [56/120    avg_loss:0.037, val_acc:0.973]
Epoch [57/120    avg_loss:0.022, val_acc:0.977]
Epoch [58/120    avg_loss:0.019, val_acc:0.970]
Epoch [59/120    avg_loss:0.024, val_acc:0.973]
Epoch [60/120    avg_loss:0.028, val_acc:0.969]
Epoch [61/120    avg_loss:0.018, val_acc:0.975]
Epoch [62/120    avg_loss:0.019, val_acc:0.975]
Epoch [63/120    avg_loss:0.019, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.976]
Epoch [65/120    avg_loss:0.023, val_acc:0.976]
Epoch [66/120    avg_loss:0.020, val_acc:0.968]
Epoch [67/120    avg_loss:0.016, val_acc:0.979]
Epoch [68/120    avg_loss:0.014, val_acc:0.979]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.008, val_acc:0.980]
Epoch [71/120    avg_loss:0.031, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.981]
Epoch [73/120    avg_loss:0.008, val_acc:0.981]
Epoch [74/120    avg_loss:0.010, val_acc:0.977]
Epoch [75/120    avg_loss:0.011, val_acc:0.979]
Epoch [76/120    avg_loss:0.010, val_acc:0.976]
Epoch [77/120    avg_loss:0.030, val_acc:0.973]
Epoch [78/120    avg_loss:0.023, val_acc:0.980]
Epoch [79/120    avg_loss:0.016, val_acc:0.975]
Epoch [80/120    avg_loss:0.008, val_acc:0.977]
Epoch [81/120    avg_loss:0.032, val_acc:0.962]
Epoch [82/120    avg_loss:0.023, val_acc:0.968]
Epoch [83/120    avg_loss:0.024, val_acc:0.961]
Epoch [84/120    avg_loss:0.080, val_acc:0.918]
Epoch [85/120    avg_loss:0.056, val_acc:0.962]
Epoch [86/120    avg_loss:0.041, val_acc:0.959]
Epoch [87/120    avg_loss:0.043, val_acc:0.967]
Epoch [88/120    avg_loss:0.036, val_acc:0.969]
Epoch [89/120    avg_loss:0.036, val_acc:0.971]
Epoch [90/120    avg_loss:0.029, val_acc:0.973]
Epoch [91/120    avg_loss:0.026, val_acc:0.971]
Epoch [92/120    avg_loss:0.023, val_acc:0.975]
Epoch [93/120    avg_loss:0.021, val_acc:0.977]
Epoch [94/120    avg_loss:0.022, val_acc:0.979]
Epoch [95/120    avg_loss:0.019, val_acc:0.976]
Epoch [96/120    avg_loss:0.016, val_acc:0.977]
Epoch [97/120    avg_loss:0.019, val_acc:0.978]
Epoch [98/120    avg_loss:0.019, val_acc:0.978]
Epoch [99/120    avg_loss:0.015, val_acc:0.976]
Epoch [100/120    avg_loss:0.017, val_acc:0.977]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.017, val_acc:0.975]
Epoch [103/120    avg_loss:0.015, val_acc:0.976]
Epoch [104/120    avg_loss:0.017, val_acc:0.976]
Epoch [105/120    avg_loss:0.019, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.978]
Epoch [107/120    avg_loss:0.013, val_acc:0.978]
Epoch [108/120    avg_loss:0.016, val_acc:0.977]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.016, val_acc:0.978]
Epoch [112/120    avg_loss:0.018, val_acc:0.978]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.015, val_acc:0.978]
Epoch [115/120    avg_loss:0.016, val_acc:0.978]
Epoch [116/120    avg_loss:0.017, val_acc:0.978]
Epoch [117/120    avg_loss:0.020, val_acc:0.978]
Epoch [118/120    avg_loss:0.015, val_acc:0.978]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6261     0     1     0     0     4    19   117    30]
 [    0     0 17896     0    70     0   120     0     4     0]
 [    0    10     0  1941     0     0     0     0    82     3]
 [    0     8     5     0  2948     0     6     0     4     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    32     0     0     0  4842     0     4     0]
 [    0     3     0     0     0     0     0  1281     0     6]
 [    0    25     0    44    53     0     0     0  3449     0]
 [    0     0     0     0     4    19     0     0     0   896]]

Accuracy:
98.37562962427397

F1 scores:
[       nan 0.9829657  0.99358743 0.96519145 0.97502894 0.99277292
 0.98314721 0.98918919 0.95394828 0.96603774]

Kappa:
0.9785192632233196
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a30229a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.048, val_acc:0.147]
Epoch [2/120    avg_loss:1.716, val_acc:0.172]
Epoch [3/120    avg_loss:1.548, val_acc:0.323]
Epoch [4/120    avg_loss:1.395, val_acc:0.374]
Epoch [5/120    avg_loss:1.197, val_acc:0.417]
Epoch [6/120    avg_loss:1.019, val_acc:0.642]
Epoch [7/120    avg_loss:0.879, val_acc:0.648]
Epoch [8/120    avg_loss:0.739, val_acc:0.737]
Epoch [9/120    avg_loss:0.650, val_acc:0.733]
Epoch [10/120    avg_loss:0.641, val_acc:0.752]
Epoch [11/120    avg_loss:0.498, val_acc:0.781]
Epoch [12/120    avg_loss:0.439, val_acc:0.823]
Epoch [13/120    avg_loss:0.458, val_acc:0.816]
Epoch [14/120    avg_loss:0.358, val_acc:0.864]
Epoch [15/120    avg_loss:0.311, val_acc:0.873]
Epoch [16/120    avg_loss:0.260, val_acc:0.901]
Epoch [17/120    avg_loss:0.246, val_acc:0.880]
Epoch [18/120    avg_loss:0.223, val_acc:0.892]
Epoch [19/120    avg_loss:0.221, val_acc:0.921]
Epoch [20/120    avg_loss:0.283, val_acc:0.861]
Epoch [21/120    avg_loss:0.214, val_acc:0.924]
Epoch [22/120    avg_loss:0.185, val_acc:0.918]
Epoch [23/120    avg_loss:0.165, val_acc:0.922]
Epoch [24/120    avg_loss:0.225, val_acc:0.907]
Epoch [25/120    avg_loss:0.184, val_acc:0.944]
Epoch [26/120    avg_loss:0.122, val_acc:0.927]
Epoch [27/120    avg_loss:0.110, val_acc:0.954]
Epoch [28/120    avg_loss:0.105, val_acc:0.959]
Epoch [29/120    avg_loss:0.096, val_acc:0.951]
Epoch [30/120    avg_loss:0.129, val_acc:0.937]
Epoch [31/120    avg_loss:0.127, val_acc:0.951]
Epoch [32/120    avg_loss:0.079, val_acc:0.938]
Epoch [33/120    avg_loss:0.077, val_acc:0.956]
Epoch [34/120    avg_loss:0.210, val_acc:0.947]
Epoch [35/120    avg_loss:0.151, val_acc:0.957]
Epoch [36/120    avg_loss:0.082, val_acc:0.882]
Epoch [37/120    avg_loss:0.111, val_acc:0.941]
Epoch [38/120    avg_loss:0.097, val_acc:0.955]
Epoch [39/120    avg_loss:0.066, val_acc:0.971]
Epoch [40/120    avg_loss:0.054, val_acc:0.974]
Epoch [41/120    avg_loss:0.068, val_acc:0.949]
Epoch [42/120    avg_loss:0.060, val_acc:0.968]
Epoch [43/120    avg_loss:0.069, val_acc:0.961]
Epoch [44/120    avg_loss:0.080, val_acc:0.967]
Epoch [45/120    avg_loss:0.062, val_acc:0.969]
Epoch [46/120    avg_loss:0.095, val_acc:0.968]
Epoch [47/120    avg_loss:0.071, val_acc:0.965]
Epoch [48/120    avg_loss:0.057, val_acc:0.951]
Epoch [49/120    avg_loss:0.054, val_acc:0.944]
Epoch [50/120    avg_loss:0.057, val_acc:0.977]
Epoch [51/120    avg_loss:0.049, val_acc:0.944]
Epoch [52/120    avg_loss:0.037, val_acc:0.967]
Epoch [53/120    avg_loss:0.026, val_acc:0.977]
Epoch [54/120    avg_loss:0.022, val_acc:0.982]
Epoch [55/120    avg_loss:0.037, val_acc:0.976]
Epoch [56/120    avg_loss:0.035, val_acc:0.967]
Epoch [57/120    avg_loss:0.028, val_acc:0.983]
Epoch [58/120    avg_loss:0.026, val_acc:0.974]
Epoch [59/120    avg_loss:0.023, val_acc:0.978]
Epoch [60/120    avg_loss:0.028, val_acc:0.978]
Epoch [61/120    avg_loss:0.030, val_acc:0.980]
Epoch [62/120    avg_loss:0.061, val_acc:0.974]
Epoch [63/120    avg_loss:0.024, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.972]
Epoch [65/120    avg_loss:0.028, val_acc:0.976]
Epoch [66/120    avg_loss:0.015, val_acc:0.979]
Epoch [67/120    avg_loss:0.016, val_acc:0.980]
Epoch [68/120    avg_loss:0.012, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.013, val_acc:0.988]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.015, val_acc:0.978]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.020, val_acc:0.984]
Epoch [75/120    avg_loss:0.014, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.980]
Epoch [77/120    avg_loss:0.030, val_acc:0.976]
Epoch [78/120    avg_loss:0.036, val_acc:0.978]
Epoch [79/120    avg_loss:0.018, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.977]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.019, val_acc:0.979]
Epoch [84/120    avg_loss:0.012, val_acc:0.981]
Epoch [85/120    avg_loss:0.012, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     0     2     0     0     1    37     7]
 [    0     3 18001     0    35     0    47     0     4     0]
 [    0    10     0  1930     0     0     0     0    94     2]
 [    0    10     0     0  2948     0     4     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4874     0     3     0]
 [    0     4     0     0     0     0     0  1283     0     3]
 [    0     5     0    28    47     0     0     0  3491     0]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
99.01670161232015

F1 scores:
[       nan 0.99385166 0.99753401 0.96620776 0.97972748 0.98602191
 0.99438947 0.996892   0.96878035 0.96337403]

Kappa:
0.9869824693808853
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f573d6b7b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.053, val_acc:0.226]
Epoch [2/120    avg_loss:1.710, val_acc:0.269]
Epoch [3/120    avg_loss:1.422, val_acc:0.361]
Epoch [4/120    avg_loss:1.190, val_acc:0.458]
Epoch [5/120    avg_loss:0.993, val_acc:0.498]
Epoch [6/120    avg_loss:0.846, val_acc:0.743]
Epoch [7/120    avg_loss:0.725, val_acc:0.615]
Epoch [8/120    avg_loss:0.580, val_acc:0.777]
Epoch [9/120    avg_loss:0.477, val_acc:0.744]
Epoch [10/120    avg_loss:0.450, val_acc:0.808]
Epoch [11/120    avg_loss:0.398, val_acc:0.842]
Epoch [12/120    avg_loss:0.375, val_acc:0.873]
Epoch [13/120    avg_loss:0.272, val_acc:0.903]
Epoch [14/120    avg_loss:0.287, val_acc:0.863]
Epoch [15/120    avg_loss:0.241, val_acc:0.871]
Epoch [16/120    avg_loss:0.967, val_acc:0.320]
Epoch [17/120    avg_loss:1.097, val_acc:0.425]
Epoch [18/120    avg_loss:1.001, val_acc:0.388]
Epoch [19/120    avg_loss:0.936, val_acc:0.513]
Epoch [20/120    avg_loss:0.910, val_acc:0.446]
Epoch [21/120    avg_loss:0.887, val_acc:0.517]
Epoch [22/120    avg_loss:0.819, val_acc:0.557]
Epoch [23/120    avg_loss:0.849, val_acc:0.556]
Epoch [24/120    avg_loss:0.825, val_acc:0.532]
Epoch [25/120    avg_loss:0.770, val_acc:0.583]
Epoch [26/120    avg_loss:0.771, val_acc:0.627]
Epoch [27/120    avg_loss:0.745, val_acc:0.613]
Epoch [28/120    avg_loss:0.727, val_acc:0.608]
Epoch [29/120    avg_loss:0.759, val_acc:0.597]
Epoch [30/120    avg_loss:0.756, val_acc:0.615]
Epoch [31/120    avg_loss:0.700, val_acc:0.608]
Epoch [32/120    avg_loss:0.735, val_acc:0.608]
Epoch [33/120    avg_loss:0.756, val_acc:0.613]
Epoch [34/120    avg_loss:0.723, val_acc:0.600]
Epoch [35/120    avg_loss:0.720, val_acc:0.612]
Epoch [36/120    avg_loss:0.703, val_acc:0.603]
Epoch [37/120    avg_loss:0.723, val_acc:0.605]
Epoch [38/120    avg_loss:0.727, val_acc:0.605]
Epoch [39/120    avg_loss:0.716, val_acc:0.610]
Epoch [40/120    avg_loss:0.735, val_acc:0.609]
Epoch [41/120    avg_loss:0.688, val_acc:0.608]
Epoch [42/120    avg_loss:0.723, val_acc:0.610]
Epoch [43/120    avg_loss:0.749, val_acc:0.610]
Epoch [44/120    avg_loss:0.702, val_acc:0.608]
Epoch [45/120    avg_loss:0.716, val_acc:0.608]
Epoch [46/120    avg_loss:0.722, val_acc:0.609]
Epoch [47/120    avg_loss:0.723, val_acc:0.609]
Epoch [48/120    avg_loss:0.718, val_acc:0.608]
Epoch [49/120    avg_loss:0.708, val_acc:0.608]
Epoch [50/120    avg_loss:0.737, val_acc:0.609]
Epoch [51/120    avg_loss:0.719, val_acc:0.611]
Epoch [52/120    avg_loss:0.708, val_acc:0.611]
Epoch [53/120    avg_loss:0.710, val_acc:0.611]
Epoch [54/120    avg_loss:0.706, val_acc:0.612]
Epoch [55/120    avg_loss:0.756, val_acc:0.612]
Epoch [56/120    avg_loss:0.710, val_acc:0.612]
Epoch [57/120    avg_loss:0.708, val_acc:0.611]
Epoch [58/120    avg_loss:0.683, val_acc:0.611]
Epoch [59/120    avg_loss:0.703, val_acc:0.612]
Epoch [60/120    avg_loss:0.687, val_acc:0.611]
Epoch [61/120    avg_loss:0.703, val_acc:0.611]
Epoch [62/120    avg_loss:0.739, val_acc:0.612]
Epoch [63/120    avg_loss:0.722, val_acc:0.612]
Epoch [64/120    avg_loss:0.697, val_acc:0.612]
Epoch [65/120    avg_loss:0.719, val_acc:0.612]
Epoch [66/120    avg_loss:0.713, val_acc:0.612]
Epoch [67/120    avg_loss:0.710, val_acc:0.612]
Epoch [68/120    avg_loss:0.710, val_acc:0.612]
Epoch [69/120    avg_loss:0.704, val_acc:0.612]
Epoch [70/120    avg_loss:0.714, val_acc:0.612]
Epoch [71/120    avg_loss:0.722, val_acc:0.612]
Epoch [72/120    avg_loss:0.710, val_acc:0.612]
Epoch [73/120    avg_loss:0.708, val_acc:0.612]
Epoch [74/120    avg_loss:0.712, val_acc:0.612]
Epoch [75/120    avg_loss:0.722, val_acc:0.612]
Epoch [76/120    avg_loss:0.713, val_acc:0.612]
Epoch [77/120    avg_loss:0.732, val_acc:0.612]
Epoch [78/120    avg_loss:0.709, val_acc:0.612]
Epoch [79/120    avg_loss:0.708, val_acc:0.612]
Epoch [80/120    avg_loss:0.723, val_acc:0.612]
Epoch [81/120    avg_loss:0.699, val_acc:0.612]
Epoch [82/120    avg_loss:0.693, val_acc:0.612]
Epoch [83/120    avg_loss:0.690, val_acc:0.612]
Epoch [84/120    avg_loss:0.719, val_acc:0.612]
Epoch [85/120    avg_loss:0.753, val_acc:0.612]
Epoch [86/120    avg_loss:0.723, val_acc:0.612]
Epoch [87/120    avg_loss:0.726, val_acc:0.612]
Epoch [88/120    avg_loss:0.705, val_acc:0.612]
Epoch [89/120    avg_loss:0.699, val_acc:0.612]
Epoch [90/120    avg_loss:0.701, val_acc:0.612]
Epoch [91/120    avg_loss:0.715, val_acc:0.612]
Epoch [92/120    avg_loss:0.708, val_acc:0.612]
Epoch [93/120    avg_loss:0.722, val_acc:0.612]
Epoch [94/120    avg_loss:0.726, val_acc:0.612]
Epoch [95/120    avg_loss:0.709, val_acc:0.612]
Epoch [96/120    avg_loss:0.722, val_acc:0.612]
Epoch [97/120    avg_loss:0.703, val_acc:0.612]
Epoch [98/120    avg_loss:0.713, val_acc:0.612]
Epoch [99/120    avg_loss:0.720, val_acc:0.612]
Epoch [100/120    avg_loss:0.694, val_acc:0.612]
Epoch [101/120    avg_loss:0.711, val_acc:0.612]
Epoch [102/120    avg_loss:0.714, val_acc:0.612]
Epoch [103/120    avg_loss:0.678, val_acc:0.612]
Epoch [104/120    avg_loss:0.703, val_acc:0.612]
Epoch [105/120    avg_loss:0.693, val_acc:0.612]
Epoch [106/120    avg_loss:0.718, val_acc:0.612]
Epoch [107/120    avg_loss:0.707, val_acc:0.612]
Epoch [108/120    avg_loss:0.727, val_acc:0.612]
Epoch [109/120    avg_loss:0.708, val_acc:0.612]
Epoch [110/120    avg_loss:0.730, val_acc:0.612]
Epoch [111/120    avg_loss:0.713, val_acc:0.612]
Epoch [112/120    avg_loss:0.685, val_acc:0.612]
Epoch [113/120    avg_loss:0.714, val_acc:0.612]
Epoch [114/120    avg_loss:0.714, val_acc:0.612]
Epoch [115/120    avg_loss:0.722, val_acc:0.612]
Epoch [116/120    avg_loss:0.709, val_acc:0.612]
Epoch [117/120    avg_loss:0.716, val_acc:0.612]
Epoch [118/120    avg_loss:0.713, val_acc:0.612]
Epoch [119/120    avg_loss:0.697, val_acc:0.612]
Epoch [120/120    avg_loss:0.709, val_acc:0.612]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0]
 [   0 3610 1027   14  139    0 1020   71  327  224]
 [   0  782 9392    0  818    0 7098    0    0    0]
 [   0   35    1 1505    0    0   32    0  430   33]
 [   0    0  302    0 2197    0  449    0   22    2]
 [   0    0    0    0    0 1305    0    0    0    0]
 [   0    0  270  153  333    0 4014    0  108    0]
 [   0  122    2    0   37    0    0 1103   26    0]
 [   0  126  212   24   13    0  394    0 2802    0]
 [   0   44    0    0   14   30   10    0    0  821]]

Accuracy:
64.46629551972622

F1 scores:
[       nan 0.64747556 0.64117968 0.80653805 0.67361643 0.98863636
 0.44861693 0.89529221 0.76914631 0.82141071]

Kappa:
0.5610066267791279
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53f7273be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.152, val_acc:0.142]
Epoch [2/120    avg_loss:1.835, val_acc:0.280]
Epoch [3/120    avg_loss:1.537, val_acc:0.358]
Epoch [4/120    avg_loss:1.290, val_acc:0.411]
Epoch [5/120    avg_loss:1.083, val_acc:0.462]
Epoch [6/120    avg_loss:0.900, val_acc:0.470]
Epoch [7/120    avg_loss:0.764, val_acc:0.561]
Epoch [8/120    avg_loss:0.663, val_acc:0.679]
Epoch [9/120    avg_loss:0.539, val_acc:0.731]
Epoch [10/120    avg_loss:0.484, val_acc:0.753]
Epoch [11/120    avg_loss:0.436, val_acc:0.787]
Epoch [12/120    avg_loss:0.451, val_acc:0.796]
Epoch [13/120    avg_loss:0.417, val_acc:0.782]
Epoch [14/120    avg_loss:0.339, val_acc:0.847]
Epoch [15/120    avg_loss:0.442, val_acc:0.764]
Epoch [16/120    avg_loss:0.332, val_acc:0.847]
Epoch [17/120    avg_loss:0.278, val_acc:0.822]
Epoch [18/120    avg_loss:0.304, val_acc:0.841]
Epoch [19/120    avg_loss:0.273, val_acc:0.880]
Epoch [20/120    avg_loss:0.238, val_acc:0.851]
Epoch [21/120    avg_loss:0.229, val_acc:0.891]
Epoch [22/120    avg_loss:0.182, val_acc:0.922]
Epoch [23/120    avg_loss:0.200, val_acc:0.900]
Epoch [24/120    avg_loss:0.170, val_acc:0.882]
Epoch [25/120    avg_loss:0.137, val_acc:0.947]
Epoch [26/120    avg_loss:0.153, val_acc:0.915]
Epoch [27/120    avg_loss:0.151, val_acc:0.899]
Epoch [28/120    avg_loss:0.148, val_acc:0.944]
Epoch [29/120    avg_loss:0.138, val_acc:0.931]
Epoch [30/120    avg_loss:0.110, val_acc:0.908]
Epoch [31/120    avg_loss:0.427, val_acc:0.499]
Epoch [32/120    avg_loss:1.078, val_acc:0.474]
Epoch [33/120    avg_loss:0.963, val_acc:0.463]
Epoch [34/120    avg_loss:0.962, val_acc:0.413]
Epoch [35/120    avg_loss:0.882, val_acc:0.510]
Epoch [36/120    avg_loss:0.870, val_acc:0.595]
Epoch [37/120    avg_loss:0.801, val_acc:0.637]
Epoch [38/120    avg_loss:0.799, val_acc:0.571]
Epoch [39/120    avg_loss:0.760, val_acc:0.658]
Epoch [40/120    avg_loss:0.733, val_acc:0.668]
Epoch [41/120    avg_loss:0.755, val_acc:0.679]
Epoch [42/120    avg_loss:0.730, val_acc:0.676]
Epoch [43/120    avg_loss:0.708, val_acc:0.688]
Epoch [44/120    avg_loss:0.698, val_acc:0.693]
Epoch [45/120    avg_loss:0.722, val_acc:0.687]
Epoch [46/120    avg_loss:0.706, val_acc:0.702]
Epoch [47/120    avg_loss:0.696, val_acc:0.709]
Epoch [48/120    avg_loss:0.692, val_acc:0.703]
Epoch [49/120    avg_loss:0.646, val_acc:0.709]
Epoch [50/120    avg_loss:0.693, val_acc:0.709]
Epoch [51/120    avg_loss:0.705, val_acc:0.709]
Epoch [52/120    avg_loss:0.688, val_acc:0.711]
Epoch [53/120    avg_loss:0.672, val_acc:0.710]
Epoch [54/120    avg_loss:0.650, val_acc:0.711]
Epoch [55/120    avg_loss:0.656, val_acc:0.715]
Epoch [56/120    avg_loss:0.671, val_acc:0.711]
Epoch [57/120    avg_loss:0.644, val_acc:0.715]
Epoch [58/120    avg_loss:0.678, val_acc:0.713]
Epoch [59/120    avg_loss:0.673, val_acc:0.715]
Epoch [60/120    avg_loss:0.681, val_acc:0.711]
Epoch [61/120    avg_loss:0.656, val_acc:0.710]
Epoch [62/120    avg_loss:0.656, val_acc:0.714]
Epoch [63/120    avg_loss:0.673, val_acc:0.715]
Epoch [64/120    avg_loss:0.655, val_acc:0.711]
Epoch [65/120    avg_loss:0.662, val_acc:0.711]
Epoch [66/120    avg_loss:0.656, val_acc:0.711]
Epoch [67/120    avg_loss:0.673, val_acc:0.711]
Epoch [68/120    avg_loss:0.685, val_acc:0.710]
Epoch [69/120    avg_loss:0.661, val_acc:0.712]
Epoch [70/120    avg_loss:0.641, val_acc:0.710]
Epoch [71/120    avg_loss:0.650, val_acc:0.710]
Epoch [72/120    avg_loss:0.658, val_acc:0.710]
Epoch [73/120    avg_loss:0.683, val_acc:0.712]
Epoch [74/120    avg_loss:0.664, val_acc:0.713]
Epoch [75/120    avg_loss:0.664, val_acc:0.714]
Epoch [76/120    avg_loss:0.658, val_acc:0.713]
Epoch [77/120    avg_loss:0.688, val_acc:0.712]
Epoch [78/120    avg_loss:0.663, val_acc:0.712]
Epoch [79/120    avg_loss:0.668, val_acc:0.712]
Epoch [80/120    avg_loss:0.661, val_acc:0.712]
Epoch [81/120    avg_loss:0.642, val_acc:0.711]
Epoch [82/120    avg_loss:0.673, val_acc:0.711]
Epoch [83/120    avg_loss:0.660, val_acc:0.711]
Epoch [84/120    avg_loss:0.651, val_acc:0.711]
Epoch [85/120    avg_loss:0.634, val_acc:0.711]
Epoch [86/120    avg_loss:0.649, val_acc:0.711]
Epoch [87/120    avg_loss:0.634, val_acc:0.711]
Epoch [88/120    avg_loss:0.647, val_acc:0.711]
Epoch [89/120    avg_loss:0.663, val_acc:0.711]
Epoch [90/120    avg_loss:0.672, val_acc:0.711]
Epoch [91/120    avg_loss:0.633, val_acc:0.711]
Epoch [92/120    avg_loss:0.670, val_acc:0.711]
Epoch [93/120    avg_loss:0.664, val_acc:0.711]
Epoch [94/120    avg_loss:0.647, val_acc:0.711]
Epoch [95/120    avg_loss:0.657, val_acc:0.711]
Epoch [96/120    avg_loss:0.659, val_acc:0.711]
Epoch [97/120    avg_loss:0.651, val_acc:0.711]
Epoch [98/120    avg_loss:0.643, val_acc:0.711]
Epoch [99/120    avg_loss:0.657, val_acc:0.711]
Epoch [100/120    avg_loss:0.668, val_acc:0.711]
Epoch [101/120    avg_loss:0.657, val_acc:0.711]
Epoch [102/120    avg_loss:0.690, val_acc:0.711]
Epoch [103/120    avg_loss:0.644, val_acc:0.711]
Epoch [104/120    avg_loss:0.670, val_acc:0.711]
Epoch [105/120    avg_loss:0.675, val_acc:0.711]
Epoch [106/120    avg_loss:0.660, val_acc:0.711]
Epoch [107/120    avg_loss:0.662, val_acc:0.711]
Epoch [108/120    avg_loss:0.648, val_acc:0.711]
Epoch [109/120    avg_loss:0.665, val_acc:0.711]
Epoch [110/120    avg_loss:0.658, val_acc:0.711]
Epoch [111/120    avg_loss:0.661, val_acc:0.711]
Epoch [112/120    avg_loss:0.645, val_acc:0.711]
Epoch [113/120    avg_loss:0.650, val_acc:0.711]
Epoch [114/120    avg_loss:0.656, val_acc:0.711]
Epoch [115/120    avg_loss:0.663, val_acc:0.711]
Epoch [116/120    avg_loss:0.681, val_acc:0.711]
Epoch [117/120    avg_loss:0.666, val_acc:0.711]
Epoch [118/120    avg_loss:0.662, val_acc:0.711]
Epoch [119/120    avg_loss:0.671, val_acc:0.711]
Epoch [120/120    avg_loss:0.667, val_acc:0.711]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4699   827    20    67     0   266    37   364   152]
 [    0     0 11140     0  2706     0  4244     0     0     0]
 [    0   131    24  1389     1     0     4     0   474    13]
 [    0     3   292     0  2575     0    87     0     3    12]
 [    0     0     0     0     0  1304     0     1     0     0]
 [    0    41   605   222    45     0  3883     0    82     0]
 [    0   128     3     0    23     0     0  1092    44     0]
 [    0   424   290   108    20     0   112     0  2617     0]
 [    0    41     0     0     0    42     1     0     0   835]]

Accuracy:
71.17827103366834

F1 scores:
[       nan 0.78981427 0.71248121 0.73589404 0.61243905 0.98377971
 0.57632653 0.90247934 0.73151642 0.86483687]

Kappa:
0.6376626257329903
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe83477fbe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.131, val_acc:0.107]
Epoch [2/120    avg_loss:1.758, val_acc:0.235]
Epoch [3/120    avg_loss:1.496, val_acc:0.340]
Epoch [4/120    avg_loss:1.318, val_acc:0.367]
Epoch [5/120    avg_loss:1.084, val_acc:0.405]
Epoch [6/120    avg_loss:0.913, val_acc:0.696]
Epoch [7/120    avg_loss:0.789, val_acc:0.792]
Epoch [8/120    avg_loss:0.685, val_acc:0.691]
Epoch [9/120    avg_loss:0.620, val_acc:0.749]
Epoch [10/120    avg_loss:0.481, val_acc:0.807]
Epoch [11/120    avg_loss:0.577, val_acc:0.503]
Epoch [12/120    avg_loss:0.833, val_acc:0.668]
Epoch [13/120    avg_loss:0.591, val_acc:0.771]
Epoch [14/120    avg_loss:0.442, val_acc:0.795]
Epoch [15/120    avg_loss:0.430, val_acc:0.785]
Epoch [16/120    avg_loss:0.378, val_acc:0.868]
Epoch [17/120    avg_loss:0.323, val_acc:0.878]
Epoch [18/120    avg_loss:0.304, val_acc:0.905]
Epoch [19/120    avg_loss:0.246, val_acc:0.917]
Epoch [20/120    avg_loss:0.218, val_acc:0.900]
Epoch [21/120    avg_loss:0.223, val_acc:0.922]
Epoch [22/120    avg_loss:0.205, val_acc:0.923]
Epoch [23/120    avg_loss:0.166, val_acc:0.899]
Epoch [24/120    avg_loss:0.164, val_acc:0.905]
Epoch [25/120    avg_loss:0.159, val_acc:0.935]
Epoch [26/120    avg_loss:0.180, val_acc:0.932]
Epoch [27/120    avg_loss:0.135, val_acc:0.951]
Epoch [28/120    avg_loss:0.124, val_acc:0.960]
Epoch [29/120    avg_loss:0.106, val_acc:0.948]
Epoch [30/120    avg_loss:0.095, val_acc:0.955]
Epoch [31/120    avg_loss:0.097, val_acc:0.922]
Epoch [32/120    avg_loss:0.095, val_acc:0.972]
Epoch [33/120    avg_loss:0.093, val_acc:0.963]
Epoch [34/120    avg_loss:0.074, val_acc:0.974]
Epoch [35/120    avg_loss:0.119, val_acc:0.957]
Epoch [36/120    avg_loss:0.120, val_acc:0.958]
Epoch [37/120    avg_loss:0.085, val_acc:0.975]
Epoch [38/120    avg_loss:0.058, val_acc:0.974]
Epoch [39/120    avg_loss:0.061, val_acc:0.970]
Epoch [40/120    avg_loss:0.062, val_acc:0.964]
Epoch [41/120    avg_loss:0.072, val_acc:0.978]
Epoch [42/120    avg_loss:0.065, val_acc:0.961]
Epoch [43/120    avg_loss:0.055, val_acc:0.972]
Epoch [44/120    avg_loss:0.051, val_acc:0.977]
Epoch [45/120    avg_loss:0.056, val_acc:0.922]
Epoch [46/120    avg_loss:0.055, val_acc:0.972]
Epoch [47/120    avg_loss:0.039, val_acc:0.976]
Epoch [48/120    avg_loss:0.040, val_acc:0.986]
Epoch [49/120    avg_loss:0.046, val_acc:0.965]
Epoch [50/120    avg_loss:0.048, val_acc:0.981]
Epoch [51/120    avg_loss:0.040, val_acc:0.977]
Epoch [52/120    avg_loss:0.033, val_acc:0.960]
Epoch [53/120    avg_loss:0.043, val_acc:0.979]
Epoch [54/120    avg_loss:0.050, val_acc:0.948]
Epoch [55/120    avg_loss:0.038, val_acc:0.953]
Epoch [56/120    avg_loss:0.054, val_acc:0.976]
Epoch [57/120    avg_loss:0.040, val_acc:0.950]
Epoch [58/120    avg_loss:0.058, val_acc:0.978]
Epoch [59/120    avg_loss:0.027, val_acc:0.981]
Epoch [60/120    avg_loss:0.028, val_acc:0.980]
Epoch [61/120    avg_loss:0.034, val_acc:0.967]
Epoch [62/120    avg_loss:0.025, val_acc:0.981]
Epoch [63/120    avg_loss:0.020, val_acc:0.984]
Epoch [64/120    avg_loss:0.013, val_acc:0.984]
Epoch [65/120    avg_loss:0.014, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.012, val_acc:0.985]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.015, val_acc:0.986]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.014, val_acc:0.985]
Epoch [73/120    avg_loss:0.015, val_acc:0.985]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.986]
Epoch [78/120    avg_loss:0.015, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.014, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.987]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.011, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.013, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.987]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.013, val_acc:0.988]
Epoch [102/120    avg_loss:0.016, val_acc:0.989]
Epoch [103/120    avg_loss:0.013, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.989]
Epoch [105/120    avg_loss:0.013, val_acc:0.987]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.009, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     2     1     0     1     7    55     8]
 [    0     4 17965     0    83     0    31     0     7     0]
 [    0    12     0  1915     0     0     0     0   109     0]
 [    0    12     3     0  2923     0    26     0     4     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     0     1     0  4843     0     4     0]
 [    0    14     0     0     0     0     0  1276     0     0]
 [    0    21     0    57    40     0     0     0  3453     0]
 [    0     1     0     0     0    24     0     0     0   894]]

Accuracy:
98.64796471694021

F1 scores:
[       nan 0.98926404 0.99562181 0.95511222 0.97109635 0.99088838
 0.99048983 0.99183832 0.95876718 0.97972603]

Kappa:
0.982102465661934
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a5d666b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.127, val_acc:0.095]
Epoch [2/120    avg_loss:1.784, val_acc:0.509]
Epoch [3/120    avg_loss:1.480, val_acc:0.566]
Epoch [4/120    avg_loss:1.281, val_acc:0.461]
Epoch [5/120    avg_loss:1.076, val_acc:0.573]
Epoch [6/120    avg_loss:0.873, val_acc:0.702]
Epoch [7/120    avg_loss:0.728, val_acc:0.705]
Epoch [8/120    avg_loss:0.594, val_acc:0.660]
Epoch [9/120    avg_loss:0.556, val_acc:0.713]
Epoch [10/120    avg_loss:0.472, val_acc:0.702]
Epoch [11/120    avg_loss:0.477, val_acc:0.718]
Epoch [12/120    avg_loss:0.421, val_acc:0.775]
Epoch [13/120    avg_loss:0.357, val_acc:0.729]
Epoch [14/120    avg_loss:0.341, val_acc:0.768]
Epoch [15/120    avg_loss:0.298, val_acc:0.796]
Epoch [16/120    avg_loss:0.295, val_acc:0.805]
Epoch [17/120    avg_loss:0.311, val_acc:0.774]
Epoch [18/120    avg_loss:0.255, val_acc:0.898]
Epoch [19/120    avg_loss:0.244, val_acc:0.793]
Epoch [20/120    avg_loss:0.358, val_acc:0.817]
Epoch [21/120    avg_loss:0.258, val_acc:0.762]
Epoch [22/120    avg_loss:0.245, val_acc:0.862]
Epoch [23/120    avg_loss:0.265, val_acc:0.863]
Epoch [24/120    avg_loss:0.208, val_acc:0.935]
Epoch [25/120    avg_loss:0.161, val_acc:0.933]
Epoch [26/120    avg_loss:0.164, val_acc:0.885]
Epoch [27/120    avg_loss:0.163, val_acc:0.933]
Epoch [28/120    avg_loss:0.136, val_acc:0.926]
Epoch [29/120    avg_loss:0.123, val_acc:0.942]
Epoch [30/120    avg_loss:0.150, val_acc:0.928]
Epoch [31/120    avg_loss:0.110, val_acc:0.954]
Epoch [32/120    avg_loss:0.116, val_acc:0.942]
Epoch [33/120    avg_loss:0.132, val_acc:0.954]
Epoch [34/120    avg_loss:0.163, val_acc:0.956]
Epoch [35/120    avg_loss:0.143, val_acc:0.953]
Epoch [36/120    avg_loss:0.143, val_acc:0.962]
Epoch [37/120    avg_loss:0.114, val_acc:0.956]
Epoch [38/120    avg_loss:0.091, val_acc:0.970]
Epoch [39/120    avg_loss:0.113, val_acc:0.961]
Epoch [40/120    avg_loss:0.078, val_acc:0.976]
Epoch [41/120    avg_loss:0.116, val_acc:0.939]
Epoch [42/120    avg_loss:0.070, val_acc:0.951]
Epoch [43/120    avg_loss:0.079, val_acc:0.929]
Epoch [44/120    avg_loss:0.084, val_acc:0.951]
Epoch [45/120    avg_loss:0.093, val_acc:0.970]
Epoch [46/120    avg_loss:0.089, val_acc:0.968]
Epoch [47/120    avg_loss:0.054, val_acc:0.964]
Epoch [48/120    avg_loss:0.082, val_acc:0.957]
Epoch [49/120    avg_loss:0.069, val_acc:0.975]
Epoch [50/120    avg_loss:0.105, val_acc:0.967]
Epoch [51/120    avg_loss:0.098, val_acc:0.975]
Epoch [52/120    avg_loss:0.055, val_acc:0.965]
Epoch [53/120    avg_loss:0.055, val_acc:0.978]
Epoch [54/120    avg_loss:0.076, val_acc:0.961]
Epoch [55/120    avg_loss:0.068, val_acc:0.966]
Epoch [56/120    avg_loss:0.043, val_acc:0.979]
Epoch [57/120    avg_loss:0.044, val_acc:0.968]
Epoch [58/120    avg_loss:0.037, val_acc:0.936]
Epoch [59/120    avg_loss:0.043, val_acc:0.972]
Epoch [60/120    avg_loss:0.049, val_acc:0.982]
Epoch [61/120    avg_loss:0.049, val_acc:0.979]
Epoch [62/120    avg_loss:0.040, val_acc:0.973]
Epoch [63/120    avg_loss:0.024, val_acc:0.979]
Epoch [64/120    avg_loss:0.018, val_acc:0.980]
Epoch [65/120    avg_loss:0.033, val_acc:0.975]
Epoch [66/120    avg_loss:0.029, val_acc:0.978]
Epoch [67/120    avg_loss:0.049, val_acc:0.986]
Epoch [68/120    avg_loss:0.031, val_acc:0.984]
Epoch [69/120    avg_loss:0.028, val_acc:0.979]
Epoch [70/120    avg_loss:0.038, val_acc:0.973]
Epoch [71/120    avg_loss:0.025, val_acc:0.978]
Epoch [72/120    avg_loss:0.023, val_acc:0.980]
Epoch [73/120    avg_loss:0.027, val_acc:0.979]
Epoch [74/120    avg_loss:0.020, val_acc:0.983]
Epoch [75/120    avg_loss:0.020, val_acc:0.980]
Epoch [76/120    avg_loss:0.038, val_acc:0.975]
Epoch [77/120    avg_loss:0.033, val_acc:0.982]
Epoch [78/120    avg_loss:0.019, val_acc:0.991]
Epoch [79/120    avg_loss:0.016, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.980]
Epoch [81/120    avg_loss:0.014, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.975]
Epoch [90/120    avg_loss:0.021, val_acc:0.976]
Epoch [91/120    avg_loss:0.017, val_acc:0.974]
Epoch [92/120    avg_loss:0.013, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     0     0     0     3     2    50     3]
 [    0     0 18039     0    24     0    24     0     3     0]
 [    0     5     0  1960     0     0     0     0    71     0]
 [    0    10     1     5  2954     0     0     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     2     0  4876     0     0     0]
 [    0    19     0     0     0     0     0  1271     0     0]
 [    0     9     0    64    45     0     0     0  3453     0]
 [    0     0     0     1     4     7     0     0     0   907]]

Accuracy:
99.14684404598366

F1 scores:
[       nan 0.99213947 0.99856075 0.96409247 0.98450258 0.99732518
 0.99703507 0.99180648 0.96587413 0.9917988 ]

Kappa:
0.9887013800519957
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05b73bbb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.137, val_acc:0.228]
Epoch [2/120    avg_loss:1.795, val_acc:0.263]
Epoch [3/120    avg_loss:1.573, val_acc:0.302]
Epoch [4/120    avg_loss:1.380, val_acc:0.357]
Epoch [5/120    avg_loss:1.143, val_acc:0.396]
Epoch [6/120    avg_loss:1.013, val_acc:0.568]
Epoch [7/120    avg_loss:0.837, val_acc:0.644]
Epoch [8/120    avg_loss:0.701, val_acc:0.668]
Epoch [9/120    avg_loss:0.617, val_acc:0.733]
Epoch [10/120    avg_loss:0.502, val_acc:0.725]
Epoch [11/120    avg_loss:0.590, val_acc:0.690]
Epoch [12/120    avg_loss:0.424, val_acc:0.825]
Epoch [13/120    avg_loss:0.363, val_acc:0.824]
Epoch [14/120    avg_loss:1.433, val_acc:0.440]
Epoch [15/120    avg_loss:1.298, val_acc:0.423]
Epoch [16/120    avg_loss:1.207, val_acc:0.376]
Epoch [17/120    avg_loss:1.136, val_acc:0.514]
Epoch [18/120    avg_loss:1.067, val_acc:0.482]
Epoch [19/120    avg_loss:1.048, val_acc:0.470]
Epoch [20/120    avg_loss:1.042, val_acc:0.542]
Epoch [21/120    avg_loss:0.979, val_acc:0.507]
Epoch [22/120    avg_loss:0.952, val_acc:0.535]
Epoch [23/120    avg_loss:0.908, val_acc:0.542]
Epoch [24/120    avg_loss:0.877, val_acc:0.548]
Epoch [25/120    avg_loss:0.862, val_acc:0.558]
Epoch [26/120    avg_loss:0.876, val_acc:0.569]
Epoch [27/120    avg_loss:0.821, val_acc:0.573]
Epoch [28/120    avg_loss:0.822, val_acc:0.582]
Epoch [29/120    avg_loss:0.812, val_acc:0.573]
Epoch [30/120    avg_loss:0.833, val_acc:0.585]
Epoch [31/120    avg_loss:0.826, val_acc:0.592]
Epoch [32/120    avg_loss:0.844, val_acc:0.592]
Epoch [33/120    avg_loss:0.829, val_acc:0.598]
Epoch [34/120    avg_loss:0.808, val_acc:0.600]
Epoch [35/120    avg_loss:0.819, val_acc:0.606]
Epoch [36/120    avg_loss:0.827, val_acc:0.607]
Epoch [37/120    avg_loss:0.839, val_acc:0.604]
Epoch [38/120    avg_loss:0.850, val_acc:0.611]
Epoch [39/120    avg_loss:0.802, val_acc:0.610]
Epoch [40/120    avg_loss:0.795, val_acc:0.609]
Epoch [41/120    avg_loss:0.799, val_acc:0.608]
Epoch [42/120    avg_loss:0.814, val_acc:0.608]
Epoch [43/120    avg_loss:0.810, val_acc:0.610]
Epoch [44/120    avg_loss:0.796, val_acc:0.609]
Epoch [45/120    avg_loss:0.791, val_acc:0.610]
Epoch [46/120    avg_loss:0.825, val_acc:0.610]
Epoch [47/120    avg_loss:0.782, val_acc:0.609]
Epoch [48/120    avg_loss:0.791, val_acc:0.610]
Epoch [49/120    avg_loss:0.798, val_acc:0.607]
Epoch [50/120    avg_loss:0.793, val_acc:0.611]
Epoch [51/120    avg_loss:0.795, val_acc:0.609]
Epoch [52/120    avg_loss:0.809, val_acc:0.610]
Epoch [53/120    avg_loss:0.830, val_acc:0.610]
Epoch [54/120    avg_loss:0.773, val_acc:0.610]
Epoch [55/120    avg_loss:0.810, val_acc:0.609]
Epoch [56/120    avg_loss:0.808, val_acc:0.610]
Epoch [57/120    avg_loss:0.796, val_acc:0.610]
Epoch [58/120    avg_loss:0.814, val_acc:0.610]
Epoch [59/120    avg_loss:0.792, val_acc:0.610]
Epoch [60/120    avg_loss:0.806, val_acc:0.610]
Epoch [61/120    avg_loss:0.791, val_acc:0.610]
Epoch [62/120    avg_loss:0.822, val_acc:0.610]
Epoch [63/120    avg_loss:0.778, val_acc:0.610]
Epoch [64/120    avg_loss:0.781, val_acc:0.611]
Epoch [65/120    avg_loss:0.816, val_acc:0.611]
Epoch [66/120    avg_loss:0.794, val_acc:0.611]
Epoch [67/120    avg_loss:0.792, val_acc:0.611]
Epoch [68/120    avg_loss:0.851, val_acc:0.611]
Epoch [69/120    avg_loss:0.814, val_acc:0.611]
Epoch [70/120    avg_loss:0.804, val_acc:0.611]
Epoch [71/120    avg_loss:0.795, val_acc:0.611]
Epoch [72/120    avg_loss:0.799, val_acc:0.611]
Epoch [73/120    avg_loss:0.780, val_acc:0.611]
Epoch [74/120    avg_loss:0.804, val_acc:0.611]
Epoch [75/120    avg_loss:0.791, val_acc:0.611]
Epoch [76/120    avg_loss:0.803, val_acc:0.611]
Epoch [77/120    avg_loss:0.791, val_acc:0.611]
Epoch [78/120    avg_loss:0.803, val_acc:0.611]
Epoch [79/120    avg_loss:0.810, val_acc:0.611]
Epoch [80/120    avg_loss:0.809, val_acc:0.611]
Epoch [81/120    avg_loss:0.808, val_acc:0.611]
Epoch [82/120    avg_loss:0.820, val_acc:0.611]
Epoch [83/120    avg_loss:0.826, val_acc:0.611]
Epoch [84/120    avg_loss:0.804, val_acc:0.611]
Epoch [85/120    avg_loss:0.836, val_acc:0.611]
Epoch [86/120    avg_loss:0.817, val_acc:0.611]
Epoch [87/120    avg_loss:0.786, val_acc:0.611]
Epoch [88/120    avg_loss:0.793, val_acc:0.611]
Epoch [89/120    avg_loss:0.770, val_acc:0.611]
Epoch [90/120    avg_loss:0.821, val_acc:0.611]
Epoch [91/120    avg_loss:0.807, val_acc:0.611]
Epoch [92/120    avg_loss:0.798, val_acc:0.611]
Epoch [93/120    avg_loss:0.759, val_acc:0.611]
Epoch [94/120    avg_loss:0.819, val_acc:0.611]
Epoch [95/120    avg_loss:0.793, val_acc:0.611]
Epoch [96/120    avg_loss:0.830, val_acc:0.611]
Epoch [97/120    avg_loss:0.800, val_acc:0.611]
Epoch [98/120    avg_loss:0.811, val_acc:0.611]
Epoch [99/120    avg_loss:0.810, val_acc:0.611]
Epoch [100/120    avg_loss:0.816, val_acc:0.611]
Epoch [101/120    avg_loss:0.776, val_acc:0.611]
Epoch [102/120    avg_loss:0.803, val_acc:0.611]
Epoch [103/120    avg_loss:0.799, val_acc:0.611]
Epoch [104/120    avg_loss:0.814, val_acc:0.611]
Epoch [105/120    avg_loss:0.817, val_acc:0.611]
Epoch [106/120    avg_loss:0.786, val_acc:0.611]
Epoch [107/120    avg_loss:0.786, val_acc:0.611]
Epoch [108/120    avg_loss:0.809, val_acc:0.611]
Epoch [109/120    avg_loss:0.803, val_acc:0.611]
Epoch [110/120    avg_loss:0.770, val_acc:0.611]
Epoch [111/120    avg_loss:0.813, val_acc:0.611]
Epoch [112/120    avg_loss:0.789, val_acc:0.611]
Epoch [113/120    avg_loss:0.819, val_acc:0.611]
Epoch [114/120    avg_loss:0.772, val_acc:0.611]
Epoch [115/120    avg_loss:0.825, val_acc:0.611]
Epoch [116/120    avg_loss:0.824, val_acc:0.611]
Epoch [117/120    avg_loss:0.833, val_acc:0.611]
Epoch [118/120    avg_loss:0.818, val_acc:0.611]
Epoch [119/120    avg_loss:0.791, val_acc:0.611]
Epoch [120/120    avg_loss:0.814, val_acc:0.611]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  3656   698    42    92     0  1496    17   251   180]
 [    0     0 10775     0   515     0  6799     0     1     0]
 [    0    26    16  1528     0     0    27     0   385    54]
 [    0     0   280     0  1893     0   781     0    18     0]
 [    0     0     0     0     0  1304     0     1     0     0]
 [    0     0   310   143   309     0  4034     0    82     0]
 [    0   123     3     0    45     0     3  1087    29     0]
 [    0   255   401    57     1     0   562     0  2295     0]
 [    0    31     0     1    14    55    12     0     0   806]]

Accuracy:
65.98221386739932

F1 scores:
[       nan 0.69485888 0.70487031 0.80273181 0.64817668 0.97897898
 0.43395009 0.90772443 0.69209891 0.82286881]

Kappa:
0.5741866187438724
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e6bb8ab70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.133, val_acc:0.151]
Epoch [2/120    avg_loss:1.784, val_acc:0.203]
Epoch [3/120    avg_loss:1.481, val_acc:0.442]
Epoch [4/120    avg_loss:1.218, val_acc:0.502]
Epoch [5/120    avg_loss:1.024, val_acc:0.478]
Epoch [6/120    avg_loss:0.840, val_acc:0.584]
Epoch [7/120    avg_loss:0.738, val_acc:0.674]
Epoch [8/120    avg_loss:0.601, val_acc:0.660]
Epoch [9/120    avg_loss:0.590, val_acc:0.774]
Epoch [10/120    avg_loss:0.503, val_acc:0.802]
Epoch [11/120    avg_loss:0.374, val_acc:0.821]
Epoch [12/120    avg_loss:0.345, val_acc:0.836]
Epoch [13/120    avg_loss:0.280, val_acc:0.809]
Epoch [14/120    avg_loss:0.333, val_acc:0.889]
Epoch [15/120    avg_loss:0.222, val_acc:0.878]
Epoch [16/120    avg_loss:0.229, val_acc:0.884]
Epoch [17/120    avg_loss:0.221, val_acc:0.883]
Epoch [18/120    avg_loss:0.215, val_acc:0.921]
Epoch [19/120    avg_loss:0.201, val_acc:0.906]
Epoch [20/120    avg_loss:0.180, val_acc:0.866]
Epoch [21/120    avg_loss:0.146, val_acc:0.899]
Epoch [22/120    avg_loss:0.131, val_acc:0.954]
Epoch [23/120    avg_loss:0.139, val_acc:0.926]
Epoch [24/120    avg_loss:0.124, val_acc:0.962]
Epoch [25/120    avg_loss:0.106, val_acc:0.941]
Epoch [26/120    avg_loss:0.108, val_acc:0.939]
Epoch [27/120    avg_loss:0.090, val_acc:0.945]
Epoch [28/120    avg_loss:0.089, val_acc:0.971]
Epoch [29/120    avg_loss:0.069, val_acc:0.941]
Epoch [30/120    avg_loss:0.058, val_acc:0.974]
Epoch [31/120    avg_loss:0.085, val_acc:0.958]
Epoch [32/120    avg_loss:0.058, val_acc:0.956]
Epoch [33/120    avg_loss:0.042, val_acc:0.974]
Epoch [34/120    avg_loss:0.070, val_acc:0.959]
Epoch [35/120    avg_loss:0.042, val_acc:0.976]
Epoch [36/120    avg_loss:0.033, val_acc:0.976]
Epoch [37/120    avg_loss:0.035, val_acc:0.972]
Epoch [38/120    avg_loss:0.031, val_acc:0.980]
Epoch [39/120    avg_loss:0.030, val_acc:0.975]
Epoch [40/120    avg_loss:0.045, val_acc:0.973]
Epoch [41/120    avg_loss:0.038, val_acc:0.974]
Epoch [42/120    avg_loss:0.054, val_acc:0.967]
Epoch [43/120    avg_loss:0.059, val_acc:0.970]
Epoch [44/120    avg_loss:0.049, val_acc:0.956]
Epoch [45/120    avg_loss:0.034, val_acc:0.982]
Epoch [46/120    avg_loss:0.028, val_acc:0.976]
Epoch [47/120    avg_loss:0.034, val_acc:0.938]
Epoch [48/120    avg_loss:0.031, val_acc:0.983]
Epoch [49/120    avg_loss:0.035, val_acc:0.972]
Epoch [50/120    avg_loss:0.053, val_acc:0.965]
Epoch [51/120    avg_loss:0.062, val_acc:0.964]
Epoch [52/120    avg_loss:0.065, val_acc:0.976]
Epoch [53/120    avg_loss:0.062, val_acc:0.970]
Epoch [54/120    avg_loss:0.042, val_acc:0.976]
Epoch [55/120    avg_loss:0.021, val_acc:0.967]
Epoch [56/120    avg_loss:0.023, val_acc:0.979]
Epoch [57/120    avg_loss:0.026, val_acc:0.980]
Epoch [58/120    avg_loss:0.034, val_acc:0.980]
Epoch [59/120    avg_loss:0.013, val_acc:0.987]
Epoch [60/120    avg_loss:0.016, val_acc:0.979]
Epoch [61/120    avg_loss:0.016, val_acc:0.986]
Epoch [62/120    avg_loss:0.019, val_acc:0.963]
Epoch [63/120    avg_loss:0.018, val_acc:0.981]
Epoch [64/120    avg_loss:0.025, val_acc:0.986]
Epoch [65/120    avg_loss:0.028, val_acc:0.980]
Epoch [66/120    avg_loss:0.017, val_acc:0.974]
Epoch [67/120    avg_loss:0.017, val_acc:0.988]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.974]
Epoch [71/120    avg_loss:0.012, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.021, val_acc:0.983]
Epoch [75/120    avg_loss:0.015, val_acc:0.980]
Epoch [76/120    avg_loss:0.015, val_acc:0.985]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.041, val_acc:0.962]
Epoch [79/120    avg_loss:0.040, val_acc:0.976]
Epoch [80/120    avg_loss:0.020, val_acc:0.986]
Epoch [81/120    avg_loss:0.016, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.991]
Epoch [105/120    avg_loss:0.010, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.009, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     0     1     2     1     0    33    46     0]
 [    0     0 18081     0     8     0     1     0     0     0]
 [    0     6     0  1969     2     0     0     0    58     1]
 [    0    15     3     5  2937     0     1     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     0     0     0  4861     0     0     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    22     0    23    34     0     0     0  3492     0]
 [    0     0     0     0    15    16     0     0     0   888]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.99017467 0.9991987  0.97620228 0.9839196  0.99352874
 0.99804948 0.98659517 0.97297297 0.98121547]

Kappa:
0.9896848739072325
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb72b56be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.084, val_acc:0.120]
Epoch [2/120    avg_loss:1.797, val_acc:0.127]
Epoch [3/120    avg_loss:1.572, val_acc:0.470]
Epoch [4/120    avg_loss:1.416, val_acc:0.626]
Epoch [5/120    avg_loss:1.251, val_acc:0.593]
Epoch [6/120    avg_loss:1.122, val_acc:0.568]
Epoch [7/120    avg_loss:0.961, val_acc:0.660]
Epoch [8/120    avg_loss:0.765, val_acc:0.751]
Epoch [9/120    avg_loss:0.648, val_acc:0.777]
Epoch [10/120    avg_loss:0.535, val_acc:0.797]
Epoch [11/120    avg_loss:0.503, val_acc:0.795]
Epoch [12/120    avg_loss:0.385, val_acc:0.820]
Epoch [13/120    avg_loss:0.409, val_acc:0.886]
Epoch [14/120    avg_loss:0.308, val_acc:0.890]
Epoch [15/120    avg_loss:0.267, val_acc:0.889]
Epoch [16/120    avg_loss:0.285, val_acc:0.880]
Epoch [17/120    avg_loss:0.244, val_acc:0.916]
Epoch [18/120    avg_loss:0.219, val_acc:0.851]
Epoch [19/120    avg_loss:0.228, val_acc:0.919]
Epoch [20/120    avg_loss:0.186, val_acc:0.907]
Epoch [21/120    avg_loss:0.200, val_acc:0.874]
Epoch [22/120    avg_loss:1.008, val_acc:0.680]
Epoch [23/120    avg_loss:0.561, val_acc:0.816]
Epoch [24/120    avg_loss:0.353, val_acc:0.897]
Epoch [25/120    avg_loss:0.290, val_acc:0.918]
Epoch [26/120    avg_loss:0.210, val_acc:0.942]
Epoch [27/120    avg_loss:0.208, val_acc:0.927]
Epoch [28/120    avg_loss:0.172, val_acc:0.908]
Epoch [29/120    avg_loss:0.134, val_acc:0.950]
Epoch [30/120    avg_loss:0.114, val_acc:0.932]
Epoch [31/120    avg_loss:0.130, val_acc:0.909]
Epoch [32/120    avg_loss:0.148, val_acc:0.915]
Epoch [33/120    avg_loss:0.135, val_acc:0.942]
Epoch [34/120    avg_loss:0.104, val_acc:0.958]
Epoch [35/120    avg_loss:0.088, val_acc:0.900]
Epoch [36/120    avg_loss:0.105, val_acc:0.954]
Epoch [37/120    avg_loss:0.071, val_acc:0.877]
Epoch [38/120    avg_loss:0.079, val_acc:0.956]
Epoch [39/120    avg_loss:0.122, val_acc:0.956]
Epoch [40/120    avg_loss:0.058, val_acc:0.971]
Epoch [41/120    avg_loss:0.053, val_acc:0.949]
Epoch [42/120    avg_loss:0.112, val_acc:0.951]
Epoch [43/120    avg_loss:0.119, val_acc:0.944]
Epoch [44/120    avg_loss:0.064, val_acc:0.947]
Epoch [45/120    avg_loss:0.062, val_acc:0.942]
Epoch [46/120    avg_loss:0.055, val_acc:0.966]
Epoch [47/120    avg_loss:0.045, val_acc:0.957]
Epoch [48/120    avg_loss:0.059, val_acc:0.964]
Epoch [49/120    avg_loss:0.034, val_acc:0.964]
Epoch [50/120    avg_loss:0.053, val_acc:0.967]
Epoch [51/120    avg_loss:0.054, val_acc:0.969]
Epoch [52/120    avg_loss:0.028, val_acc:0.963]
Epoch [53/120    avg_loss:0.036, val_acc:0.969]
Epoch [54/120    avg_loss:0.028, val_acc:0.976]
Epoch [55/120    avg_loss:0.019, val_acc:0.979]
Epoch [56/120    avg_loss:0.024, val_acc:0.981]
Epoch [57/120    avg_loss:0.016, val_acc:0.981]
Epoch [58/120    avg_loss:0.021, val_acc:0.982]
Epoch [59/120    avg_loss:0.019, val_acc:0.985]
Epoch [60/120    avg_loss:0.018, val_acc:0.982]
Epoch [61/120    avg_loss:0.018, val_acc:0.983]
Epoch [62/120    avg_loss:0.020, val_acc:0.984]
Epoch [63/120    avg_loss:0.019, val_acc:0.986]
Epoch [64/120    avg_loss:0.019, val_acc:0.986]
Epoch [65/120    avg_loss:0.015, val_acc:0.985]
Epoch [66/120    avg_loss:0.015, val_acc:0.986]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.019, val_acc:0.984]
Epoch [69/120    avg_loss:0.018, val_acc:0.985]
Epoch [70/120    avg_loss:0.016, val_acc:0.984]
Epoch [71/120    avg_loss:0.019, val_acc:0.986]
Epoch [72/120    avg_loss:0.017, val_acc:0.986]
Epoch [73/120    avg_loss:0.015, val_acc:0.986]
Epoch [74/120    avg_loss:0.016, val_acc:0.985]
Epoch [75/120    avg_loss:0.016, val_acc:0.986]
Epoch [76/120    avg_loss:0.014, val_acc:0.985]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.017, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.984]
Epoch [80/120    avg_loss:0.018, val_acc:0.986]
Epoch [81/120    avg_loss:0.014, val_acc:0.985]
Epoch [82/120    avg_loss:0.018, val_acc:0.985]
Epoch [83/120    avg_loss:0.012, val_acc:0.985]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.016, val_acc:0.985]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.986]
Epoch [89/120    avg_loss:0.017, val_acc:0.985]
Epoch [90/120    avg_loss:0.014, val_acc:0.985]
Epoch [91/120    avg_loss:0.016, val_acc:0.985]
Epoch [92/120    avg_loss:0.012, val_acc:0.986]
Epoch [93/120    avg_loss:0.014, val_acc:0.986]
Epoch [94/120    avg_loss:0.013, val_acc:0.986]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.013, val_acc:0.986]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.013, val_acc:0.985]
Epoch [99/120    avg_loss:0.013, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.986]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.013, val_acc:0.986]
Epoch [104/120    avg_loss:0.013, val_acc:0.986]
Epoch [105/120    avg_loss:0.015, val_acc:0.984]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.987]
Epoch [109/120    avg_loss:0.014, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.015, val_acc:0.985]
Epoch [114/120    avg_loss:0.012, val_acc:0.982]
Epoch [115/120    avg_loss:0.011, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.014, val_acc:0.987]
Epoch [119/120    avg_loss:0.013, val_acc:0.986]
Epoch [120/120    avg_loss:0.014, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0     0    11    15     6]
 [    0     0 18043     0    25     0    18     0     4     0]
 [    0     1     0  1906     0     0     0     0   125     4]
 [    0    27     6     0  2932     0     0     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     7     0     0     0     0     0  1281     0     2]
 [    0    10     0    21    53     0     0     0  3487     0]
 [    0     0     0     0     8    43     0     0     0   868]]

Accuracy:
99.04803219820211

F1 scores:
[       nan 0.99402035 0.99847818 0.96189755 0.97896494 0.98379193
 0.99795334 0.99225407 0.96740186 0.96498054]

Kappa:
0.987389413029939
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa2a30cfb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.180, val_acc:0.133]
Epoch [2/120    avg_loss:1.886, val_acc:0.193]
Epoch [3/120    avg_loss:1.593, val_acc:0.256]
Epoch [4/120    avg_loss:1.377, val_acc:0.413]
Epoch [5/120    avg_loss:1.145, val_acc:0.438]
Epoch [6/120    avg_loss:0.952, val_acc:0.552]
Epoch [7/120    avg_loss:0.829, val_acc:0.703]
Epoch [8/120    avg_loss:0.677, val_acc:0.726]
Epoch [9/120    avg_loss:0.531, val_acc:0.761]
Epoch [10/120    avg_loss:0.495, val_acc:0.780]
Epoch [11/120    avg_loss:0.413, val_acc:0.840]
Epoch [12/120    avg_loss:0.391, val_acc:0.856]
Epoch [13/120    avg_loss:0.302, val_acc:0.879]
Epoch [14/120    avg_loss:0.304, val_acc:0.904]
Epoch [15/120    avg_loss:0.288, val_acc:0.864]
Epoch [16/120    avg_loss:0.243, val_acc:0.899]
Epoch [17/120    avg_loss:0.239, val_acc:0.931]
Epoch [18/120    avg_loss:0.171, val_acc:0.952]
Epoch [19/120    avg_loss:0.147, val_acc:0.959]
Epoch [20/120    avg_loss:0.132, val_acc:0.916]
Epoch [21/120    avg_loss:0.156, val_acc:0.949]
Epoch [22/120    avg_loss:0.145, val_acc:0.919]
Epoch [23/120    avg_loss:0.189, val_acc:0.921]
Epoch [24/120    avg_loss:0.172, val_acc:0.940]
Epoch [25/120    avg_loss:0.149, val_acc:0.958]
Epoch [26/120    avg_loss:0.106, val_acc:0.927]
Epoch [27/120    avg_loss:0.079, val_acc:0.954]
Epoch [28/120    avg_loss:0.093, val_acc:0.970]
Epoch [29/120    avg_loss:0.063, val_acc:0.967]
Epoch [30/120    avg_loss:0.053, val_acc:0.976]
Epoch [31/120    avg_loss:0.046, val_acc:0.979]
Epoch [32/120    avg_loss:0.045, val_acc:0.969]
Epoch [33/120    avg_loss:0.062, val_acc:0.970]
Epoch [34/120    avg_loss:0.054, val_acc:0.972]
Epoch [35/120    avg_loss:0.074, val_acc:0.890]
Epoch [36/120    avg_loss:0.087, val_acc:0.958]
Epoch [37/120    avg_loss:0.049, val_acc:0.978]
Epoch [38/120    avg_loss:0.044, val_acc:0.981]
Epoch [39/120    avg_loss:0.032, val_acc:0.981]
Epoch [40/120    avg_loss:0.028, val_acc:0.985]
Epoch [41/120    avg_loss:0.026, val_acc:0.971]
Epoch [42/120    avg_loss:0.043, val_acc:0.984]
Epoch [43/120    avg_loss:0.026, val_acc:0.985]
Epoch [44/120    avg_loss:0.023, val_acc:0.987]
Epoch [45/120    avg_loss:0.045, val_acc:0.986]
Epoch [46/120    avg_loss:0.019, val_acc:0.986]
Epoch [47/120    avg_loss:0.027, val_acc:0.985]
Epoch [48/120    avg_loss:0.030, val_acc:0.981]
Epoch [49/120    avg_loss:0.020, val_acc:0.982]
Epoch [50/120    avg_loss:0.025, val_acc:0.981]
Epoch [51/120    avg_loss:0.014, val_acc:0.988]
Epoch [52/120    avg_loss:0.073, val_acc:0.953]
Epoch [53/120    avg_loss:0.069, val_acc:0.979]
Epoch [54/120    avg_loss:0.023, val_acc:0.986]
Epoch [55/120    avg_loss:0.031, val_acc:0.988]
Epoch [56/120    avg_loss:0.013, val_acc:0.987]
Epoch [57/120    avg_loss:0.014, val_acc:0.981]
Epoch [58/120    avg_loss:0.017, val_acc:0.981]
Epoch [59/120    avg_loss:0.016, val_acc:0.977]
Epoch [60/120    avg_loss:0.026, val_acc:0.978]
Epoch [61/120    avg_loss:0.023, val_acc:0.976]
Epoch [62/120    avg_loss:0.013, val_acc:0.988]
Epoch [63/120    avg_loss:0.024, val_acc:0.986]
Epoch [64/120    avg_loss:0.034, val_acc:0.968]
Epoch [65/120    avg_loss:0.014, val_acc:0.986]
Epoch [66/120    avg_loss:0.019, val_acc:0.978]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.037, val_acc:0.984]
Epoch [69/120    avg_loss:0.024, val_acc:0.977]
Epoch [70/120    avg_loss:0.012, val_acc:0.983]
Epoch [71/120    avg_loss:0.012, val_acc:0.981]
Epoch [72/120    avg_loss:0.009, val_acc:0.987]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.009, val_acc:0.989]
Epoch [75/120    avg_loss:0.015, val_acc:0.965]
Epoch [76/120    avg_loss:0.015, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.989]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.989]
Epoch [85/120    avg_loss:0.008, val_acc:0.989]
Epoch [86/120    avg_loss:0.023, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.016, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.003, val_acc:0.988]
Epoch [95/120    avg_loss:0.025, val_acc:0.967]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.963]
Epoch [104/120    avg_loss:0.014, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.991]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.036, val_acc:0.948]
Epoch [116/120    avg_loss:0.096, val_acc:0.970]
Epoch [117/120    avg_loss:0.022, val_acc:0.984]
Epoch [118/120    avg_loss:0.020, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.043, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6125    11     8     1     0     3     2   147   135]
 [    0     0 17977     0   102     0    11     0     0     0]
 [    0     0     3  1864     0     0     0     0   164     5]
 [    0    34     0     0  2906     0    22     0     8     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     1     0     0     0     0     5  1265     0    19]
 [    0     1     0     0    52     0     0     0  3517     1]
 [    0     0     0     0     9    15     0     0     0   895]]

Accuracy:
98.16354565830382

F1 scores:
[       nan 0.97276265 0.99645252 0.95394063 0.96193313 0.99428571
 0.99571254 0.98944075 0.94964223 0.90587045]

Kappa:
0.9757088770491816
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35fca46b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.088, val_acc:0.190]
Epoch [2/120    avg_loss:1.792, val_acc:0.238]
Epoch [3/120    avg_loss:1.508, val_acc:0.329]
Epoch [4/120    avg_loss:1.243, val_acc:0.644]
Epoch [5/120    avg_loss:0.990, val_acc:0.508]
Epoch [6/120    avg_loss:0.803, val_acc:0.651]
Epoch [7/120    avg_loss:0.635, val_acc:0.732]
Epoch [8/120    avg_loss:0.549, val_acc:0.736]
Epoch [9/120    avg_loss:0.468, val_acc:0.822]
Epoch [10/120    avg_loss:0.403, val_acc:0.845]
Epoch [11/120    avg_loss:0.350, val_acc:0.857]
Epoch [12/120    avg_loss:0.311, val_acc:0.854]
Epoch [13/120    avg_loss:0.330, val_acc:0.819]
Epoch [14/120    avg_loss:0.332, val_acc:0.888]
Epoch [15/120    avg_loss:0.228, val_acc:0.876]
Epoch [16/120    avg_loss:0.188, val_acc:0.937]
Epoch [17/120    avg_loss:0.189, val_acc:0.891]
Epoch [18/120    avg_loss:0.181, val_acc:0.939]
Epoch [19/120    avg_loss:0.146, val_acc:0.883]
Epoch [20/120    avg_loss:0.151, val_acc:0.939]
Epoch [21/120    avg_loss:0.495, val_acc:0.489]
Epoch [22/120    avg_loss:1.037, val_acc:0.619]
Epoch [23/120    avg_loss:0.701, val_acc:0.754]
Epoch [24/120    avg_loss:0.570, val_acc:0.801]
Epoch [25/120    avg_loss:0.470, val_acc:0.838]
Epoch [26/120    avg_loss:0.414, val_acc:0.862]
Epoch [27/120    avg_loss:0.374, val_acc:0.894]
Epoch [28/120    avg_loss:0.344, val_acc:0.862]
Epoch [29/120    avg_loss:0.302, val_acc:0.890]
Epoch [30/120    avg_loss:0.272, val_acc:0.911]
Epoch [31/120    avg_loss:0.256, val_acc:0.906]
Epoch [32/120    avg_loss:0.293, val_acc:0.878]
Epoch [33/120    avg_loss:0.307, val_acc:0.903]
Epoch [34/120    avg_loss:0.227, val_acc:0.926]
Epoch [35/120    avg_loss:0.203, val_acc:0.920]
Epoch [36/120    avg_loss:0.180, val_acc:0.931]
Epoch [37/120    avg_loss:0.177, val_acc:0.927]
Epoch [38/120    avg_loss:0.173, val_acc:0.929]
Epoch [39/120    avg_loss:0.162, val_acc:0.935]
Epoch [40/120    avg_loss:0.176, val_acc:0.932]
Epoch [41/120    avg_loss:0.162, val_acc:0.932]
Epoch [42/120    avg_loss:0.163, val_acc:0.917]
Epoch [43/120    avg_loss:0.148, val_acc:0.938]
Epoch [44/120    avg_loss:0.160, val_acc:0.940]
Epoch [45/120    avg_loss:0.142, val_acc:0.928]
Epoch [46/120    avg_loss:0.142, val_acc:0.937]
Epoch [47/120    avg_loss:0.147, val_acc:0.938]
Epoch [48/120    avg_loss:0.152, val_acc:0.939]
Epoch [49/120    avg_loss:0.134, val_acc:0.929]
Epoch [50/120    avg_loss:0.156, val_acc:0.938]
Epoch [51/120    avg_loss:0.123, val_acc:0.933]
Epoch [52/120    avg_loss:0.141, val_acc:0.933]
Epoch [53/120    avg_loss:0.137, val_acc:0.932]
Epoch [54/120    avg_loss:0.120, val_acc:0.934]
Epoch [55/120    avg_loss:0.142, val_acc:0.940]
Epoch [56/120    avg_loss:0.123, val_acc:0.938]
Epoch [57/120    avg_loss:0.125, val_acc:0.930]
Epoch [58/120    avg_loss:0.111, val_acc:0.939]
Epoch [59/120    avg_loss:0.120, val_acc:0.938]
Epoch [60/120    avg_loss:0.106, val_acc:0.932]
Epoch [61/120    avg_loss:0.112, val_acc:0.934]
Epoch [62/120    avg_loss:0.116, val_acc:0.939]
Epoch [63/120    avg_loss:0.114, val_acc:0.946]
Epoch [64/120    avg_loss:0.115, val_acc:0.948]
Epoch [65/120    avg_loss:0.115, val_acc:0.946]
Epoch [66/120    avg_loss:0.102, val_acc:0.928]
Epoch [67/120    avg_loss:0.108, val_acc:0.944]
Epoch [68/120    avg_loss:0.108, val_acc:0.942]
Epoch [69/120    avg_loss:0.103, val_acc:0.949]
Epoch [70/120    avg_loss:0.106, val_acc:0.948]
Epoch [71/120    avg_loss:0.094, val_acc:0.951]
Epoch [72/120    avg_loss:0.087, val_acc:0.948]
Epoch [73/120    avg_loss:0.094, val_acc:0.946]
Epoch [74/120    avg_loss:0.100, val_acc:0.950]
Epoch [75/120    avg_loss:0.098, val_acc:0.943]
Epoch [76/120    avg_loss:0.100, val_acc:0.948]
Epoch [77/120    avg_loss:0.100, val_acc:0.957]
Epoch [78/120    avg_loss:0.086, val_acc:0.949]
Epoch [79/120    avg_loss:0.094, val_acc:0.951]
Epoch [80/120    avg_loss:0.078, val_acc:0.954]
Epoch [81/120    avg_loss:0.080, val_acc:0.956]
Epoch [82/120    avg_loss:0.091, val_acc:0.953]
Epoch [83/120    avg_loss:0.081, val_acc:0.956]
Epoch [84/120    avg_loss:0.087, val_acc:0.957]
Epoch [85/120    avg_loss:0.084, val_acc:0.953]
Epoch [86/120    avg_loss:0.084, val_acc:0.959]
Epoch [87/120    avg_loss:0.068, val_acc:0.958]
Epoch [88/120    avg_loss:0.085, val_acc:0.954]
Epoch [89/120    avg_loss:0.080, val_acc:0.961]
Epoch [90/120    avg_loss:0.079, val_acc:0.940]
Epoch [91/120    avg_loss:0.074, val_acc:0.956]
Epoch [92/120    avg_loss:0.089, val_acc:0.962]
Epoch [93/120    avg_loss:0.070, val_acc:0.959]
Epoch [94/120    avg_loss:0.063, val_acc:0.959]
Epoch [95/120    avg_loss:0.076, val_acc:0.957]
Epoch [96/120    avg_loss:0.073, val_acc:0.954]
Epoch [97/120    avg_loss:0.073, val_acc:0.956]
Epoch [98/120    avg_loss:0.067, val_acc:0.959]
Epoch [99/120    avg_loss:0.073, val_acc:0.961]
Epoch [100/120    avg_loss:0.075, val_acc:0.943]
Epoch [101/120    avg_loss:0.079, val_acc:0.957]
Epoch [102/120    avg_loss:0.074, val_acc:0.956]
Epoch [103/120    avg_loss:0.061, val_acc:0.953]
Epoch [104/120    avg_loss:0.071, val_acc:0.961]
Epoch [105/120    avg_loss:0.054, val_acc:0.959]
Epoch [106/120    avg_loss:0.061, val_acc:0.960]
Epoch [107/120    avg_loss:0.057, val_acc:0.960]
Epoch [108/120    avg_loss:0.059, val_acc:0.961]
Epoch [109/120    avg_loss:0.062, val_acc:0.962]
Epoch [110/120    avg_loss:0.056, val_acc:0.964]
Epoch [111/120    avg_loss:0.067, val_acc:0.959]
Epoch [112/120    avg_loss:0.060, val_acc:0.964]
Epoch [113/120    avg_loss:0.057, val_acc:0.965]
Epoch [114/120    avg_loss:0.053, val_acc:0.964]
Epoch [115/120    avg_loss:0.058, val_acc:0.965]
Epoch [116/120    avg_loss:0.063, val_acc:0.962]
Epoch [117/120    avg_loss:0.051, val_acc:0.964]
Epoch [118/120    avg_loss:0.063, val_acc:0.964]
Epoch [119/120    avg_loss:0.063, val_acc:0.960]
Epoch [120/120    avg_loss:0.059, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6191     0    10    25     0     0     0   171    35]
 [    0    12 17698     0   132     0   248     0     0     0]
 [    0    12     0  1847     0     0     0     0   168     9]
 [    0    44     5     8  2902     0     5     0     6     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0    52     0     0     0     0     0  1237     1     0]
 [    0    80     0    32    63     0     0     0  3396     0]
 [    0     4     0     0    21    56     0     0     0   838]]

Accuracy:
97.09348564818163

F1 scores:
[       nan 0.96530755 0.98890845 0.93923214 0.94914146 0.97899475
 0.97421032 0.97902651 0.92812244 0.92956184]

Kappa:
0.9616405313952134
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f5b7dbc88>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.132, val_acc:0.101]
Epoch [2/120    avg_loss:1.877, val_acc:0.257]
Epoch [3/120    avg_loss:1.622, val_acc:0.290]
Epoch [4/120    avg_loss:1.409, val_acc:0.342]
Epoch [5/120    avg_loss:1.167, val_acc:0.424]
Epoch [6/120    avg_loss:1.017, val_acc:0.537]
Epoch [7/120    avg_loss:0.931, val_acc:0.592]
Epoch [8/120    avg_loss:0.780, val_acc:0.717]
Epoch [9/120    avg_loss:0.682, val_acc:0.741]
Epoch [10/120    avg_loss:0.558, val_acc:0.762]
Epoch [11/120    avg_loss:0.504, val_acc:0.797]
Epoch [12/120    avg_loss:0.475, val_acc:0.820]
Epoch [13/120    avg_loss:0.468, val_acc:0.835]
Epoch [14/120    avg_loss:0.376, val_acc:0.812]
Epoch [15/120    avg_loss:0.312, val_acc:0.831]
Epoch [16/120    avg_loss:0.293, val_acc:0.811]
Epoch [17/120    avg_loss:0.330, val_acc:0.718]
Epoch [18/120    avg_loss:0.340, val_acc:0.839]
Epoch [19/120    avg_loss:0.242, val_acc:0.873]
Epoch [20/120    avg_loss:0.215, val_acc:0.846]
Epoch [21/120    avg_loss:0.219, val_acc:0.882]
Epoch [22/120    avg_loss:0.164, val_acc:0.878]
Epoch [23/120    avg_loss:0.164, val_acc:0.917]
Epoch [24/120    avg_loss:0.169, val_acc:0.936]
Epoch [25/120    avg_loss:0.158, val_acc:0.872]
Epoch [26/120    avg_loss:0.143, val_acc:0.916]
Epoch [27/120    avg_loss:0.120, val_acc:0.902]
Epoch [28/120    avg_loss:0.140, val_acc:0.932]
Epoch [29/120    avg_loss:0.091, val_acc:0.933]
Epoch [30/120    avg_loss:0.098, val_acc:0.905]
Epoch [31/120    avg_loss:0.096, val_acc:0.941]
Epoch [32/120    avg_loss:0.114, val_acc:0.941]
Epoch [33/120    avg_loss:0.069, val_acc:0.953]
Epoch [34/120    avg_loss:0.084, val_acc:0.872]
Epoch [35/120    avg_loss:0.084, val_acc:0.955]
Epoch [36/120    avg_loss:0.079, val_acc:0.936]
Epoch [37/120    avg_loss:0.092, val_acc:0.949]
Epoch [38/120    avg_loss:0.211, val_acc:0.889]
Epoch [39/120    avg_loss:0.120, val_acc:0.949]
Epoch [40/120    avg_loss:0.080, val_acc:0.964]
Epoch [41/120    avg_loss:0.059, val_acc:0.948]
Epoch [42/120    avg_loss:0.108, val_acc:0.961]
Epoch [43/120    avg_loss:0.091, val_acc:0.933]
Epoch [44/120    avg_loss:0.060, val_acc:0.967]
Epoch [45/120    avg_loss:0.090, val_acc:0.915]
Epoch [46/120    avg_loss:0.055, val_acc:0.953]
Epoch [47/120    avg_loss:0.051, val_acc:0.954]
Epoch [48/120    avg_loss:0.039, val_acc:0.969]
Epoch [49/120    avg_loss:0.121, val_acc:0.950]
Epoch [50/120    avg_loss:0.061, val_acc:0.971]
Epoch [51/120    avg_loss:0.033, val_acc:0.975]
Epoch [52/120    avg_loss:0.035, val_acc:0.963]
Epoch [53/120    avg_loss:0.040, val_acc:0.952]
Epoch [54/120    avg_loss:0.035, val_acc:0.948]
Epoch [55/120    avg_loss:0.047, val_acc:0.961]
Epoch [56/120    avg_loss:0.030, val_acc:0.973]
Epoch [57/120    avg_loss:0.024, val_acc:0.935]
Epoch [58/120    avg_loss:0.104, val_acc:0.961]
Epoch [59/120    avg_loss:0.049, val_acc:0.977]
Epoch [60/120    avg_loss:0.032, val_acc:0.963]
Epoch [61/120    avg_loss:0.030, val_acc:0.977]
Epoch [62/120    avg_loss:0.027, val_acc:0.977]
Epoch [63/120    avg_loss:0.024, val_acc:0.949]
Epoch [64/120    avg_loss:0.017, val_acc:0.982]
Epoch [65/120    avg_loss:0.021, val_acc:0.977]
Epoch [66/120    avg_loss:0.025, val_acc:0.964]
Epoch [67/120    avg_loss:0.030, val_acc:0.978]
Epoch [68/120    avg_loss:0.014, val_acc:0.979]
Epoch [69/120    avg_loss:0.013, val_acc:0.979]
Epoch [70/120    avg_loss:0.022, val_acc:0.975]
Epoch [71/120    avg_loss:0.013, val_acc:0.968]
Epoch [72/120    avg_loss:0.016, val_acc:0.987]
Epoch [73/120    avg_loss:0.012, val_acc:0.983]
Epoch [74/120    avg_loss:0.016, val_acc:0.963]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.015, val_acc:0.980]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.016, val_acc:0.980]
Epoch [80/120    avg_loss:0.017, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.977]
Epoch [82/120    avg_loss:0.023, val_acc:0.975]
Epoch [83/120    avg_loss:0.017, val_acc:0.981]
Epoch [84/120    avg_loss:0.017, val_acc:0.961]
Epoch [85/120    avg_loss:0.013, val_acc:0.973]
Epoch [86/120    avg_loss:0.010, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.979]
Epoch [88/120    avg_loss:0.007, val_acc:0.979]
Epoch [89/120    avg_loss:0.009, val_acc:0.981]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0     1     2     0     0    20    26     0]
 [    0     8 18061     0    15     0     6     0     0     0]
 [    0    12     0  1922     0     0     0     0   101     1]
 [    0    30    11     3  2893     0    13     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     2     0     0  4874     0     0     0]
 [    0    20     0     0     0     0     0  1269     0     1]
 [    0     3     0    38    42     0     0     0  3488     0]
 [    0     3     0     0    16    20     0     0     1   879]]

Accuracy:
98.99019111657388

F1 scores:
[       nan 0.99030331 0.99883862 0.96051974 0.97407407 0.99239544
 0.9976461  0.98410237 0.96767929 0.97666667]

Kappa:
0.9866190914287186
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9884cf5c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.165, val_acc:0.164]
Epoch [2/120    avg_loss:1.894, val_acc:0.237]
Epoch [3/120    avg_loss:1.632, val_acc:0.306]
Epoch [4/120    avg_loss:1.372, val_acc:0.372]
Epoch [5/120    avg_loss:1.140, val_acc:0.439]
Epoch [6/120    avg_loss:0.960, val_acc:0.510]
Epoch [7/120    avg_loss:0.760, val_acc:0.559]
Epoch [8/120    avg_loss:0.634, val_acc:0.674]
Epoch [9/120    avg_loss:0.556, val_acc:0.731]
Epoch [10/120    avg_loss:0.566, val_acc:0.717]
Epoch [11/120    avg_loss:0.476, val_acc:0.769]
Epoch [12/120    avg_loss:0.401, val_acc:0.750]
Epoch [13/120    avg_loss:0.354, val_acc:0.743]
Epoch [14/120    avg_loss:0.322, val_acc:0.877]
Epoch [15/120    avg_loss:0.283, val_acc:0.809]
Epoch [16/120    avg_loss:0.236, val_acc:0.928]
Epoch [17/120    avg_loss:0.248, val_acc:0.907]
Epoch [18/120    avg_loss:0.194, val_acc:0.887]
Epoch [19/120    avg_loss:0.398, val_acc:0.627]
Epoch [20/120    avg_loss:1.264, val_acc:0.524]
Epoch [21/120    avg_loss:1.128, val_acc:0.532]
Epoch [22/120    avg_loss:1.008, val_acc:0.543]
Epoch [23/120    avg_loss:0.962, val_acc:0.516]
Epoch [24/120    avg_loss:0.958, val_acc:0.554]
Epoch [25/120    avg_loss:0.845, val_acc:0.620]
Epoch [26/120    avg_loss:0.797, val_acc:0.647]
Epoch [27/120    avg_loss:0.800, val_acc:0.651]
Epoch [28/120    avg_loss:0.705, val_acc:0.688]
Epoch [29/120    avg_loss:0.688, val_acc:0.700]
Epoch [30/120    avg_loss:0.626, val_acc:0.718]
Epoch [31/120    avg_loss:0.606, val_acc:0.716]
Epoch [32/120    avg_loss:0.622, val_acc:0.722]
Epoch [33/120    avg_loss:0.605, val_acc:0.718]
Epoch [34/120    avg_loss:0.626, val_acc:0.720]
Epoch [35/120    avg_loss:0.588, val_acc:0.718]
Epoch [36/120    avg_loss:0.599, val_acc:0.729]
Epoch [37/120    avg_loss:0.595, val_acc:0.713]
Epoch [38/120    avg_loss:0.584, val_acc:0.719]
Epoch [39/120    avg_loss:0.589, val_acc:0.735]
Epoch [40/120    avg_loss:0.577, val_acc:0.727]
Epoch [41/120    avg_loss:0.600, val_acc:0.734]
Epoch [42/120    avg_loss:0.580, val_acc:0.729]
Epoch [43/120    avg_loss:0.569, val_acc:0.735]
Epoch [44/120    avg_loss:0.562, val_acc:0.736]
Epoch [45/120    avg_loss:0.573, val_acc:0.733]
Epoch [46/120    avg_loss:0.570, val_acc:0.738]
Epoch [47/120    avg_loss:0.570, val_acc:0.736]
Epoch [48/120    avg_loss:0.566, val_acc:0.739]
Epoch [49/120    avg_loss:0.584, val_acc:0.735]
Epoch [50/120    avg_loss:0.563, val_acc:0.736]
Epoch [51/120    avg_loss:0.554, val_acc:0.737]
Epoch [52/120    avg_loss:0.552, val_acc:0.738]
Epoch [53/120    avg_loss:0.565, val_acc:0.738]
Epoch [54/120    avg_loss:0.584, val_acc:0.739]
Epoch [55/120    avg_loss:0.568, val_acc:0.736]
Epoch [56/120    avg_loss:0.559, val_acc:0.738]
Epoch [57/120    avg_loss:0.580, val_acc:0.736]
Epoch [58/120    avg_loss:0.550, val_acc:0.737]
Epoch [59/120    avg_loss:0.557, val_acc:0.737]
Epoch [60/120    avg_loss:0.571, val_acc:0.738]
Epoch [61/120    avg_loss:0.583, val_acc:0.738]
Epoch [62/120    avg_loss:0.556, val_acc:0.738]
Epoch [63/120    avg_loss:0.558, val_acc:0.737]
Epoch [64/120    avg_loss:0.558, val_acc:0.737]
Epoch [65/120    avg_loss:0.565, val_acc:0.737]
Epoch [66/120    avg_loss:0.556, val_acc:0.737]
Epoch [67/120    avg_loss:0.576, val_acc:0.737]
Epoch [68/120    avg_loss:0.562, val_acc:0.737]
Epoch [69/120    avg_loss:0.559, val_acc:0.737]
Epoch [70/120    avg_loss:0.560, val_acc:0.737]
Epoch [71/120    avg_loss:0.570, val_acc:0.737]
Epoch [72/120    avg_loss:0.551, val_acc:0.737]
Epoch [73/120    avg_loss:0.568, val_acc:0.737]
Epoch [74/120    avg_loss:0.557, val_acc:0.737]
Epoch [75/120    avg_loss:0.566, val_acc:0.737]
Epoch [76/120    avg_loss:0.553, val_acc:0.737]
Epoch [77/120    avg_loss:0.570, val_acc:0.737]
Epoch [78/120    avg_loss:0.565, val_acc:0.737]
Epoch [79/120    avg_loss:0.546, val_acc:0.737]
Epoch [80/120    avg_loss:0.552, val_acc:0.737]
Epoch [81/120    avg_loss:0.554, val_acc:0.737]
Epoch [82/120    avg_loss:0.565, val_acc:0.737]
Epoch [83/120    avg_loss:0.572, val_acc:0.737]
Epoch [84/120    avg_loss:0.554, val_acc:0.737]
Epoch [85/120    avg_loss:0.558, val_acc:0.737]
Epoch [86/120    avg_loss:0.556, val_acc:0.737]
Epoch [87/120    avg_loss:0.584, val_acc:0.737]
Epoch [88/120    avg_loss:0.567, val_acc:0.737]
Epoch [89/120    avg_loss:0.567, val_acc:0.737]
Epoch [90/120    avg_loss:0.558, val_acc:0.737]
Epoch [91/120    avg_loss:0.568, val_acc:0.737]
Epoch [92/120    avg_loss:0.552, val_acc:0.737]
Epoch [93/120    avg_loss:0.577, val_acc:0.737]
Epoch [94/120    avg_loss:0.567, val_acc:0.737]
Epoch [95/120    avg_loss:0.544, val_acc:0.737]
Epoch [96/120    avg_loss:0.563, val_acc:0.737]
Epoch [97/120    avg_loss:0.561, val_acc:0.737]
Epoch [98/120    avg_loss:0.564, val_acc:0.737]
Epoch [99/120    avg_loss:0.541, val_acc:0.737]
Epoch [100/120    avg_loss:0.552, val_acc:0.737]
Epoch [101/120    avg_loss:0.589, val_acc:0.737]
Epoch [102/120    avg_loss:0.570, val_acc:0.737]
Epoch [103/120    avg_loss:0.537, val_acc:0.737]
Epoch [104/120    avg_loss:0.564, val_acc:0.737]
Epoch [105/120    avg_loss:0.568, val_acc:0.737]
Epoch [106/120    avg_loss:0.555, val_acc:0.737]
Epoch [107/120    avg_loss:0.583, val_acc:0.737]
Epoch [108/120    avg_loss:0.554, val_acc:0.737]
Epoch [109/120    avg_loss:0.549, val_acc:0.737]
Epoch [110/120    avg_loss:0.554, val_acc:0.737]
Epoch [111/120    avg_loss:0.541, val_acc:0.737]
Epoch [112/120    avg_loss:0.587, val_acc:0.737]
Epoch [113/120    avg_loss:0.582, val_acc:0.737]
Epoch [114/120    avg_loss:0.558, val_acc:0.737]
Epoch [115/120    avg_loss:0.554, val_acc:0.737]
Epoch [116/120    avg_loss:0.552, val_acc:0.737]
Epoch [117/120    avg_loss:0.579, val_acc:0.737]
Epoch [118/120    avg_loss:0.572, val_acc:0.737]
Epoch [119/120    avg_loss:0.571, val_acc:0.737]
Epoch [120/120    avg_loss:0.540, val_acc:0.737]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4758   183    51   327     0   336    72   537   168]
 [    0     0 12260     0   234     0  5596     0     0     0]
 [    0     8     0  1809    14     0     1     0   143    61]
 [    0    31   585     0  2146     0   185     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   335    47   296     0  4092     0   108     0]
 [    0    61     0     0    20     0     2  1179    28     0]
 [    0   116    64   107    16     0   157     0  3111     0]
 [    0    32     0     3    14    68    13     0     0   789]]

Accuracy:
75.79350733858723

F1 scores:
[       nan 0.83196363 0.77799283 0.89267209 0.71071369 0.97460792
 0.53630406 0.92798111 0.82706367 0.81466185]

Kappa:
0.6945182402796852
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89a5a56ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.121, val_acc:0.442]
Epoch [2/120    avg_loss:1.798, val_acc:0.428]
Epoch [3/120    avg_loss:1.555, val_acc:0.632]
Epoch [4/120    avg_loss:1.310, val_acc:0.692]
Epoch [5/120    avg_loss:1.154, val_acc:0.709]
Epoch [6/120    avg_loss:0.976, val_acc:0.733]
Epoch [7/120    avg_loss:0.828, val_acc:0.751]
Epoch [8/120    avg_loss:0.751, val_acc:0.760]
Epoch [9/120    avg_loss:0.603, val_acc:0.757]
Epoch [10/120    avg_loss:0.489, val_acc:0.800]
Epoch [11/120    avg_loss:0.437, val_acc:0.818]
Epoch [12/120    avg_loss:0.435, val_acc:0.869]
Epoch [13/120    avg_loss:0.411, val_acc:0.805]
Epoch [14/120    avg_loss:0.317, val_acc:0.868]
Epoch [15/120    avg_loss:0.303, val_acc:0.856]
Epoch [16/120    avg_loss:0.314, val_acc:0.894]
Epoch [17/120    avg_loss:0.230, val_acc:0.921]
Epoch [18/120    avg_loss:0.205, val_acc:0.915]
Epoch [19/120    avg_loss:0.141, val_acc:0.937]
Epoch [20/120    avg_loss:0.189, val_acc:0.928]
Epoch [21/120    avg_loss:0.149, val_acc:0.940]
Epoch [22/120    avg_loss:0.138, val_acc:0.938]
Epoch [23/120    avg_loss:0.121, val_acc:0.897]
Epoch [24/120    avg_loss:0.121, val_acc:0.953]
Epoch [25/120    avg_loss:0.123, val_acc:0.936]
Epoch [26/120    avg_loss:0.106, val_acc:0.951]
Epoch [27/120    avg_loss:0.084, val_acc:0.940]
Epoch [28/120    avg_loss:0.087, val_acc:0.950]
Epoch [29/120    avg_loss:0.075, val_acc:0.953]
Epoch [30/120    avg_loss:0.061, val_acc:0.967]
Epoch [31/120    avg_loss:0.107, val_acc:0.968]
Epoch [32/120    avg_loss:0.097, val_acc:0.942]
Epoch [33/120    avg_loss:0.066, val_acc:0.956]
Epoch [34/120    avg_loss:0.059, val_acc:0.950]
Epoch [35/120    avg_loss:0.071, val_acc:0.953]
Epoch [36/120    avg_loss:0.045, val_acc:0.969]
Epoch [37/120    avg_loss:0.177, val_acc:0.909]
Epoch [38/120    avg_loss:0.157, val_acc:0.956]
Epoch [39/120    avg_loss:0.085, val_acc:0.967]
Epoch [40/120    avg_loss:0.054, val_acc:0.961]
Epoch [41/120    avg_loss:0.094, val_acc:0.942]
Epoch [42/120    avg_loss:0.053, val_acc:0.966]
Epoch [43/120    avg_loss:0.057, val_acc:0.964]
Epoch [44/120    avg_loss:0.038, val_acc:0.973]
Epoch [45/120    avg_loss:0.029, val_acc:0.971]
Epoch [46/120    avg_loss:0.038, val_acc:0.951]
Epoch [47/120    avg_loss:0.092, val_acc:0.966]
Epoch [48/120    avg_loss:0.048, val_acc:0.966]
Epoch [49/120    avg_loss:0.024, val_acc:0.975]
Epoch [50/120    avg_loss:0.030, val_acc:0.971]
Epoch [51/120    avg_loss:0.196, val_acc:0.894]
Epoch [52/120    avg_loss:0.063, val_acc:0.951]
Epoch [53/120    avg_loss:0.047, val_acc:0.966]
Epoch [54/120    avg_loss:0.035, val_acc:0.951]
Epoch [55/120    avg_loss:0.031, val_acc:0.966]
Epoch [56/120    avg_loss:0.047, val_acc:0.973]
Epoch [57/120    avg_loss:0.048, val_acc:0.963]
Epoch [58/120    avg_loss:0.032, val_acc:0.971]
Epoch [59/120    avg_loss:0.018, val_acc:0.967]
Epoch [60/120    avg_loss:0.017, val_acc:0.978]
Epoch [61/120    avg_loss:0.021, val_acc:0.978]
Epoch [62/120    avg_loss:0.018, val_acc:0.973]
Epoch [63/120    avg_loss:0.020, val_acc:0.981]
Epoch [64/120    avg_loss:0.021, val_acc:0.974]
Epoch [65/120    avg_loss:0.011, val_acc:0.981]
Epoch [66/120    avg_loss:0.011, val_acc:0.975]
Epoch [67/120    avg_loss:0.014, val_acc:0.986]
Epoch [68/120    avg_loss:0.017, val_acc:0.949]
Epoch [69/120    avg_loss:0.023, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.979]
Epoch [71/120    avg_loss:0.013, val_acc:0.985]
Epoch [72/120    avg_loss:0.009, val_acc:0.978]
Epoch [73/120    avg_loss:0.013, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.977]
Epoch [75/120    avg_loss:0.008, val_acc:0.984]
Epoch [76/120    avg_loss:0.016, val_acc:0.938]
Epoch [77/120    avg_loss:0.055, val_acc:0.963]
Epoch [78/120    avg_loss:0.025, val_acc:0.983]
Epoch [79/120    avg_loss:0.020, val_acc:0.976]
Epoch [80/120    avg_loss:0.029, val_acc:0.966]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.010, val_acc:0.979]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.009, val_acc:0.973]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.005, val_acc:0.983]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     0     0     0     3     8    10]
 [    0     0 18059     0     8     0    10     0    13     0]
 [    0     8     0  2012     0     0     0     0    14     2]
 [    0    38    10     0  2903     0     5     0    12     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     2     0     0  4854     0     0     0]
 [    0     4     0     0     0     0     1  1279     0     6]
 [    0     0     0    28    50     0     0     0  3493     0]
 [    0     0     0     0    13    35     0     0     0   871]]

Accuracy:
99.26252620924012

F1 scores:
[       nan 0.99449314 0.99825875 0.98675821 0.97645476 0.98676749
 0.99589659 0.99455677 0.9824216  0.96136865]

Kappa:
0.9902273911471442
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb734dc0be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.206, val_acc:0.139]
Epoch [2/120    avg_loss:1.898, val_acc:0.161]
Epoch [3/120    avg_loss:1.593, val_acc:0.200]
Epoch [4/120    avg_loss:1.371, val_acc:0.420]
Epoch [5/120    avg_loss:1.108, val_acc:0.482]
Epoch [6/120    avg_loss:0.930, val_acc:0.570]
Epoch [7/120    avg_loss:0.776, val_acc:0.701]
Epoch [8/120    avg_loss:0.597, val_acc:0.697]
Epoch [9/120    avg_loss:0.518, val_acc:0.721]
Epoch [10/120    avg_loss:0.530, val_acc:0.693]
Epoch [11/120    avg_loss:0.449, val_acc:0.743]
Epoch [12/120    avg_loss:0.370, val_acc:0.784]
Epoch [13/120    avg_loss:0.367, val_acc:0.809]
Epoch [14/120    avg_loss:0.320, val_acc:0.767]
Epoch [15/120    avg_loss:0.311, val_acc:0.808]
Epoch [16/120    avg_loss:0.304, val_acc:0.821]
Epoch [17/120    avg_loss:0.251, val_acc:0.846]
Epoch [18/120    avg_loss:0.223, val_acc:0.869]
Epoch [19/120    avg_loss:0.204, val_acc:0.886]
Epoch [20/120    avg_loss:0.248, val_acc:0.898]
Epoch [21/120    avg_loss:0.197, val_acc:0.929]
Epoch [22/120    avg_loss:0.145, val_acc:0.930]
Epoch [23/120    avg_loss:0.153, val_acc:0.864]
Epoch [24/120    avg_loss:0.153, val_acc:0.918]
Epoch [25/120    avg_loss:0.127, val_acc:0.943]
Epoch [26/120    avg_loss:0.242, val_acc:0.834]
Epoch [27/120    avg_loss:0.187, val_acc:0.879]
Epoch [28/120    avg_loss:0.165, val_acc:0.867]
Epoch [29/120    avg_loss:0.097, val_acc:0.919]
Epoch [30/120    avg_loss:0.138, val_acc:0.851]
Epoch [31/120    avg_loss:0.103, val_acc:0.959]
Epoch [32/120    avg_loss:0.096, val_acc:0.885]
Epoch [33/120    avg_loss:0.089, val_acc:0.944]
Epoch [34/120    avg_loss:0.080, val_acc:0.963]
Epoch [35/120    avg_loss:0.072, val_acc:0.962]
Epoch [36/120    avg_loss:0.092, val_acc:0.961]
Epoch [37/120    avg_loss:0.077, val_acc:0.949]
Epoch [38/120    avg_loss:0.054, val_acc:0.952]
Epoch [39/120    avg_loss:0.101, val_acc:0.916]
Epoch [40/120    avg_loss:0.068, val_acc:0.965]
Epoch [41/120    avg_loss:0.091, val_acc:0.951]
Epoch [42/120    avg_loss:0.058, val_acc:0.965]
Epoch [43/120    avg_loss:0.050, val_acc:0.953]
Epoch [44/120    avg_loss:0.045, val_acc:0.968]
Epoch [45/120    avg_loss:0.048, val_acc:0.963]
Epoch [46/120    avg_loss:0.055, val_acc:0.959]
Epoch [47/120    avg_loss:0.067, val_acc:0.963]
Epoch [48/120    avg_loss:0.043, val_acc:0.962]
Epoch [49/120    avg_loss:0.037, val_acc:0.968]
Epoch [50/120    avg_loss:0.025, val_acc:0.965]
Epoch [51/120    avg_loss:0.031, val_acc:0.975]
Epoch [52/120    avg_loss:0.038, val_acc:0.972]
Epoch [53/120    avg_loss:0.061, val_acc:0.970]
Epoch [54/120    avg_loss:0.044, val_acc:0.965]
Epoch [55/120    avg_loss:0.028, val_acc:0.975]
Epoch [56/120    avg_loss:0.026, val_acc:0.950]
Epoch [57/120    avg_loss:0.038, val_acc:0.977]
Epoch [58/120    avg_loss:0.032, val_acc:0.976]
Epoch [59/120    avg_loss:0.043, val_acc:0.979]
Epoch [60/120    avg_loss:0.019, val_acc:0.973]
Epoch [61/120    avg_loss:0.024, val_acc:0.976]
Epoch [62/120    avg_loss:0.028, val_acc:0.967]
Epoch [63/120    avg_loss:0.034, val_acc:0.959]
Epoch [64/120    avg_loss:0.038, val_acc:0.970]
Epoch [65/120    avg_loss:0.035, val_acc:0.969]
Epoch [66/120    avg_loss:0.047, val_acc:0.905]
Epoch [67/120    avg_loss:0.032, val_acc:0.976]
Epoch [68/120    avg_loss:0.032, val_acc:0.975]
Epoch [69/120    avg_loss:0.024, val_acc:0.978]
Epoch [70/120    avg_loss:0.031, val_acc:0.955]
Epoch [71/120    avg_loss:0.037, val_acc:0.976]
Epoch [72/120    avg_loss:0.031, val_acc:0.967]
Epoch [73/120    avg_loss:0.016, val_acc:0.979]
Epoch [74/120    avg_loss:0.014, val_acc:0.981]
Epoch [75/120    avg_loss:0.011, val_acc:0.982]
Epoch [76/120    avg_loss:0.010, val_acc:0.981]
Epoch [77/120    avg_loss:0.011, val_acc:0.982]
Epoch [78/120    avg_loss:0.012, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6352     0    14     0     0     2    41    22     1]
 [    0     0 18031     0    37     0    21     0     1     0]
 [    0     4     0  1972     0     0     0     0    59     1]
 [    0     8     4     2  2955     0     1     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     5     0     0  4851     0     0     0]
 [    0     0     0     0     0     1     1  1287     0     1]
 [    0     5     0    34    58     0     0     0  3474     0]
 [    0     0     0     0     5    49     0     0     0   865]]

Accuracy:
99.03357192779505

F1 scores:
[       nan 0.99242247 0.99764849 0.9707113  0.98058736 0.98120301
 0.99466885 0.98319328 0.97461074 0.96810297]

Kappa:
0.9872017560114701
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d8d54bc18>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.155, val_acc:0.115]
Epoch [2/120    avg_loss:1.801, val_acc:0.162]
Epoch [3/120    avg_loss:1.548, val_acc:0.293]
Epoch [4/120    avg_loss:1.360, val_acc:0.371]
Epoch [5/120    avg_loss:1.184, val_acc:0.456]
Epoch [6/120    avg_loss:0.987, val_acc:0.483]
Epoch [7/120    avg_loss:0.851, val_acc:0.710]
Epoch [8/120    avg_loss:0.648, val_acc:0.775]
Epoch [9/120    avg_loss:0.560, val_acc:0.773]
Epoch [10/120    avg_loss:0.487, val_acc:0.789]
Epoch [11/120    avg_loss:0.566, val_acc:0.780]
Epoch [12/120    avg_loss:0.383, val_acc:0.831]
Epoch [13/120    avg_loss:0.372, val_acc:0.761]
Epoch [14/120    avg_loss:0.801, val_acc:0.677]
Epoch [15/120    avg_loss:0.598, val_acc:0.718]
Epoch [16/120    avg_loss:0.427, val_acc:0.817]
Epoch [17/120    avg_loss:0.356, val_acc:0.786]
Epoch [18/120    avg_loss:0.314, val_acc:0.811]
Epoch [19/120    avg_loss:0.298, val_acc:0.828]
Epoch [20/120    avg_loss:0.264, val_acc:0.832]
Epoch [21/120    avg_loss:0.261, val_acc:0.852]
Epoch [22/120    avg_loss:0.274, val_acc:0.779]
Epoch [23/120    avg_loss:0.264, val_acc:0.863]
Epoch [24/120    avg_loss:0.211, val_acc:0.858]
Epoch [25/120    avg_loss:0.170, val_acc:0.896]
Epoch [26/120    avg_loss:0.157, val_acc:0.912]
Epoch [27/120    avg_loss:0.128, val_acc:0.935]
Epoch [28/120    avg_loss:0.124, val_acc:0.921]
Epoch [29/120    avg_loss:0.147, val_acc:0.923]
Epoch [30/120    avg_loss:0.104, val_acc:0.885]
Epoch [31/120    avg_loss:0.100, val_acc:0.948]
Epoch [32/120    avg_loss:0.121, val_acc:0.946]
Epoch [33/120    avg_loss:0.110, val_acc:0.935]
Epoch [34/120    avg_loss:0.080, val_acc:0.944]
Epoch [35/120    avg_loss:0.092, val_acc:0.923]
Epoch [36/120    avg_loss:0.092, val_acc:0.955]
Epoch [37/120    avg_loss:0.084, val_acc:0.959]
Epoch [38/120    avg_loss:0.100, val_acc:0.935]
Epoch [39/120    avg_loss:0.090, val_acc:0.948]
Epoch [40/120    avg_loss:0.055, val_acc:0.963]
Epoch [41/120    avg_loss:0.053, val_acc:0.954]
Epoch [42/120    avg_loss:0.074, val_acc:0.961]
Epoch [43/120    avg_loss:0.067, val_acc:0.957]
Epoch [44/120    avg_loss:0.038, val_acc:0.965]
Epoch [45/120    avg_loss:0.050, val_acc:0.965]
Epoch [46/120    avg_loss:0.066, val_acc:0.905]
Epoch [47/120    avg_loss:0.073, val_acc:0.968]
Epoch [48/120    avg_loss:0.063, val_acc:0.971]
Epoch [49/120    avg_loss:0.041, val_acc:0.975]
Epoch [50/120    avg_loss:0.058, val_acc:0.954]
Epoch [51/120    avg_loss:0.056, val_acc:0.961]
Epoch [52/120    avg_loss:0.042, val_acc:0.964]
Epoch [53/120    avg_loss:0.024, val_acc:0.970]
Epoch [54/120    avg_loss:0.077, val_acc:0.614]
Epoch [55/120    avg_loss:1.557, val_acc:0.610]
Epoch [56/120    avg_loss:1.262, val_acc:0.641]
Epoch [57/120    avg_loss:1.174, val_acc:0.698]
Epoch [58/120    avg_loss:1.056, val_acc:0.687]
Epoch [59/120    avg_loss:1.017, val_acc:0.658]
Epoch [60/120    avg_loss:0.912, val_acc:0.727]
Epoch [61/120    avg_loss:0.886, val_acc:0.714]
Epoch [62/120    avg_loss:0.826, val_acc:0.679]
Epoch [63/120    avg_loss:0.774, val_acc:0.728]
Epoch [64/120    avg_loss:0.789, val_acc:0.720]
Epoch [65/120    avg_loss:0.780, val_acc:0.742]
Epoch [66/120    avg_loss:0.744, val_acc:0.713]
Epoch [67/120    avg_loss:0.759, val_acc:0.727]
Epoch [68/120    avg_loss:0.739, val_acc:0.739]
Epoch [69/120    avg_loss:0.746, val_acc:0.739]
Epoch [70/120    avg_loss:0.833, val_acc:0.744]
Epoch [71/120    avg_loss:0.734, val_acc:0.754]
Epoch [72/120    avg_loss:0.748, val_acc:0.745]
Epoch [73/120    avg_loss:0.716, val_acc:0.750]
Epoch [74/120    avg_loss:0.757, val_acc:0.748]
Epoch [75/120    avg_loss:0.716, val_acc:0.762]
Epoch [76/120    avg_loss:0.721, val_acc:0.758]
Epoch [77/120    avg_loss:0.722, val_acc:0.759]
Epoch [78/120    avg_loss:0.758, val_acc:0.758]
Epoch [79/120    avg_loss:0.708, val_acc:0.756]
Epoch [80/120    avg_loss:0.727, val_acc:0.751]
Epoch [81/120    avg_loss:0.715, val_acc:0.749]
Epoch [82/120    avg_loss:0.717, val_acc:0.748]
Epoch [83/120    avg_loss:0.700, val_acc:0.749]
Epoch [84/120    avg_loss:0.709, val_acc:0.750]
Epoch [85/120    avg_loss:0.718, val_acc:0.748]
Epoch [86/120    avg_loss:0.730, val_acc:0.749]
Epoch [87/120    avg_loss:0.736, val_acc:0.750]
Epoch [88/120    avg_loss:0.709, val_acc:0.750]
Epoch [89/120    avg_loss:0.703, val_acc:0.750]
Epoch [90/120    avg_loss:0.713, val_acc:0.750]
Epoch [91/120    avg_loss:0.715, val_acc:0.750]
Epoch [92/120    avg_loss:0.716, val_acc:0.750]
Epoch [93/120    avg_loss:0.734, val_acc:0.750]
Epoch [94/120    avg_loss:0.712, val_acc:0.750]
Epoch [95/120    avg_loss:0.725, val_acc:0.750]
Epoch [96/120    avg_loss:0.728, val_acc:0.750]
Epoch [97/120    avg_loss:0.730, val_acc:0.750]
Epoch [98/120    avg_loss:0.720, val_acc:0.750]
Epoch [99/120    avg_loss:0.719, val_acc:0.750]
Epoch [100/120    avg_loss:0.726, val_acc:0.750]
Epoch [101/120    avg_loss:0.735, val_acc:0.750]
Epoch [102/120    avg_loss:0.724, val_acc:0.750]
Epoch [103/120    avg_loss:0.702, val_acc:0.750]
Epoch [104/120    avg_loss:0.751, val_acc:0.750]
Epoch [105/120    avg_loss:0.713, val_acc:0.750]
Epoch [106/120    avg_loss:0.716, val_acc:0.750]
Epoch [107/120    avg_loss:0.729, val_acc:0.750]
Epoch [108/120    avg_loss:0.710, val_acc:0.750]
Epoch [109/120    avg_loss:0.707, val_acc:0.750]
Epoch [110/120    avg_loss:0.720, val_acc:0.750]
Epoch [111/120    avg_loss:0.715, val_acc:0.750]
Epoch [112/120    avg_loss:0.696, val_acc:0.750]
Epoch [113/120    avg_loss:0.753, val_acc:0.750]
Epoch [114/120    avg_loss:0.746, val_acc:0.750]
Epoch [115/120    avg_loss:0.721, val_acc:0.750]
Epoch [116/120    avg_loss:0.729, val_acc:0.750]
Epoch [117/120    avg_loss:0.719, val_acc:0.750]
Epoch [118/120    avg_loss:0.706, val_acc:0.750]
Epoch [119/120    avg_loss:0.714, val_acc:0.750]
Epoch [120/120    avg_loss:0.712, val_acc:0.750]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4920    56    36   296    45   327    14   504   234]
 [    0     0 16730     0     4     0  1356     0     0     0]
 [    0    13     0  1589    33     0     4     0   293   104]
 [    0    60   409     1  1995     0   487     0    19     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1  2028    58   352     0  2340     0    99     0]
 [    0   100     0     0    24     0     0  1152     0    14]
 [    0   124     0   284   150     0   217     0  2794     2]
 [    0    20     0     4    18   133     0     0     0   744]]

Accuracy:
80.90280288241391

F1 scores:
[       nan 0.84318766 0.8967384  0.79291417 0.68275154 0.93615495
 0.4870434  0.93811075 0.76758242 0.73736373]

Kappa:
0.7445119314475562
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9196b95ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.152, val_acc:0.239]
Epoch [2/120    avg_loss:1.810, val_acc:0.272]
Epoch [3/120    avg_loss:1.567, val_acc:0.318]
Epoch [4/120    avg_loss:1.361, val_acc:0.402]
Epoch [5/120    avg_loss:1.150, val_acc:0.488]
Epoch [6/120    avg_loss:0.935, val_acc:0.562]
Epoch [7/120    avg_loss:0.742, val_acc:0.747]
Epoch [8/120    avg_loss:0.619, val_acc:0.730]
Epoch [9/120    avg_loss:0.584, val_acc:0.791]
Epoch [10/120    avg_loss:0.489, val_acc:0.776]
Epoch [11/120    avg_loss:0.443, val_acc:0.796]
Epoch [12/120    avg_loss:0.382, val_acc:0.737]
Epoch [13/120    avg_loss:0.372, val_acc:0.783]
Epoch [14/120    avg_loss:0.340, val_acc:0.774]
Epoch [15/120    avg_loss:0.318, val_acc:0.799]
Epoch [16/120    avg_loss:0.260, val_acc:0.872]
Epoch [17/120    avg_loss:0.251, val_acc:0.785]
Epoch [18/120    avg_loss:0.253, val_acc:0.911]
Epoch [19/120    avg_loss:0.197, val_acc:0.878]
Epoch [20/120    avg_loss:0.182, val_acc:0.891]
Epoch [21/120    avg_loss:0.165, val_acc:0.922]
Epoch [22/120    avg_loss:0.158, val_acc:0.936]
Epoch [23/120    avg_loss:0.157, val_acc:0.938]
Epoch [24/120    avg_loss:0.133, val_acc:0.954]
Epoch [25/120    avg_loss:0.165, val_acc:0.941]
Epoch [26/120    avg_loss:0.149, val_acc:0.926]
Epoch [27/120    avg_loss:0.106, val_acc:0.957]
Epoch [28/120    avg_loss:0.105, val_acc:0.952]
Epoch [29/120    avg_loss:0.088, val_acc:0.967]
Epoch [30/120    avg_loss:0.091, val_acc:0.930]
Epoch [31/120    avg_loss:0.110, val_acc:0.964]
Epoch [32/120    avg_loss:0.075, val_acc:0.967]
Epoch [33/120    avg_loss:0.066, val_acc:0.955]
Epoch [34/120    avg_loss:0.091, val_acc:0.908]
Epoch [35/120    avg_loss:0.061, val_acc:0.980]
Epoch [36/120    avg_loss:0.046, val_acc:0.979]
Epoch [37/120    avg_loss:0.055, val_acc:0.980]
Epoch [38/120    avg_loss:0.050, val_acc:0.970]
Epoch [39/120    avg_loss:0.066, val_acc:0.973]
Epoch [40/120    avg_loss:0.055, val_acc:0.984]
Epoch [41/120    avg_loss:0.083, val_acc:0.975]
Epoch [42/120    avg_loss:0.069, val_acc:0.976]
Epoch [43/120    avg_loss:0.036, val_acc:0.982]
Epoch [44/120    avg_loss:0.065, val_acc:0.972]
Epoch [45/120    avg_loss:0.070, val_acc:0.945]
Epoch [46/120    avg_loss:0.086, val_acc:0.979]
Epoch [47/120    avg_loss:0.216, val_acc:0.979]
Epoch [48/120    avg_loss:0.068, val_acc:0.989]
Epoch [49/120    avg_loss:0.065, val_acc:0.949]
Epoch [50/120    avg_loss:0.049, val_acc:0.991]
Epoch [51/120    avg_loss:0.043, val_acc:0.988]
Epoch [52/120    avg_loss:0.026, val_acc:0.989]
Epoch [53/120    avg_loss:0.027, val_acc:0.978]
Epoch [54/120    avg_loss:0.019, val_acc:0.992]
Epoch [55/120    avg_loss:0.023, val_acc:0.992]
Epoch [56/120    avg_loss:0.024, val_acc:0.988]
Epoch [57/120    avg_loss:0.032, val_acc:0.981]
Epoch [58/120    avg_loss:0.027, val_acc:0.986]
Epoch [59/120    avg_loss:0.014, val_acc:0.991]
Epoch [60/120    avg_loss:0.022, val_acc:0.990]
Epoch [61/120    avg_loss:0.021, val_acc:0.985]
Epoch [62/120    avg_loss:0.041, val_acc:0.986]
Epoch [63/120    avg_loss:0.028, val_acc:0.985]
Epoch [64/120    avg_loss:0.020, val_acc:0.991]
Epoch [65/120    avg_loss:0.016, val_acc:0.993]
Epoch [66/120    avg_loss:0.025, val_acc:0.991]
Epoch [67/120    avg_loss:0.016, val_acc:0.985]
Epoch [68/120    avg_loss:0.012, val_acc:0.992]
Epoch [69/120    avg_loss:0.019, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.993]
Epoch [71/120    avg_loss:0.010, val_acc:0.991]
Epoch [72/120    avg_loss:0.009, val_acc:0.990]
Epoch [73/120    avg_loss:0.010, val_acc:0.993]
Epoch [74/120    avg_loss:0.009, val_acc:0.991]
Epoch [75/120    avg_loss:0.008, val_acc:0.990]
Epoch [76/120    avg_loss:0.013, val_acc:0.992]
Epoch [77/120    avg_loss:0.009, val_acc:0.993]
Epoch [78/120    avg_loss:0.027, val_acc:0.985]
Epoch [79/120    avg_loss:0.020, val_acc:0.991]
Epoch [80/120    avg_loss:0.009, val_acc:0.991]
Epoch [81/120    avg_loss:0.015, val_acc:0.991]
Epoch [82/120    avg_loss:0.026, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.015, val_acc:0.990]
Epoch [85/120    avg_loss:0.021, val_acc:0.979]
Epoch [86/120    avg_loss:0.151, val_acc:0.977]
Epoch [87/120    avg_loss:0.074, val_acc:0.982]
Epoch [88/120    avg_loss:0.059, val_acc:0.986]
Epoch [89/120    avg_loss:0.051, val_acc:0.979]
Epoch [90/120    avg_loss:0.018, val_acc:0.988]
Epoch [91/120    avg_loss:0.017, val_acc:0.989]
Epoch [92/120    avg_loss:0.013, val_acc:0.991]
Epoch [93/120    avg_loss:0.009, val_acc:0.991]
Epoch [94/120    avg_loss:0.011, val_acc:0.991]
Epoch [95/120    avg_loss:0.010, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.992]
Epoch [97/120    avg_loss:0.010, val_acc:0.991]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.012, val_acc:0.992]
Epoch [100/120    avg_loss:0.009, val_acc:0.992]
Epoch [101/120    avg_loss:0.012, val_acc:0.991]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.008, val_acc:0.992]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.008, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.013, val_acc:0.992]
Epoch [110/120    avg_loss:0.010, val_acc:0.992]
Epoch [111/120    avg_loss:0.008, val_acc:0.992]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.010, val_acc:0.992]
Epoch [118/120    avg_loss:0.009, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     1     0     0     0     9     0]
 [    0     2 18013     0    10     0    57     0     8     0]
 [    0    23     0  1967     0     0     0     0    46     0]
 [    0    21    12    13  2910     0     8     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    33    11     0     0  4834     0     0     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0    17     0    22    55     0     0     0  3477     0]
 [    0     0     0     0    11     3     0     0     0   905]]

Accuracy:
99.10587327983033

F1 scores:
[       nan 0.99427156 0.99662499 0.97159793 0.97667394 0.99885189
 0.98885139 0.99961225 0.97682259 0.99232456]

Kappa:
0.9881543206410448
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb41a88b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.110, val_acc:0.083]
Epoch [2/120    avg_loss:1.812, val_acc:0.152]
Epoch [3/120    avg_loss:1.628, val_acc:0.229]
Epoch [4/120    avg_loss:1.447, val_acc:0.342]
Epoch [5/120    avg_loss:1.263, val_acc:0.380]
Epoch [6/120    avg_loss:1.105, val_acc:0.451]
Epoch [7/120    avg_loss:0.931, val_acc:0.499]
Epoch [8/120    avg_loss:0.817, val_acc:0.650]
Epoch [9/120    avg_loss:0.675, val_acc:0.743]
Epoch [10/120    avg_loss:0.583, val_acc:0.712]
Epoch [11/120    avg_loss:0.518, val_acc:0.767]
Epoch [12/120    avg_loss:0.654, val_acc:0.754]
Epoch [13/120    avg_loss:0.408, val_acc:0.795]
Epoch [14/120    avg_loss:0.424, val_acc:0.820]
Epoch [15/120    avg_loss:0.329, val_acc:0.785]
Epoch [16/120    avg_loss:0.278, val_acc:0.885]
Epoch [17/120    avg_loss:0.313, val_acc:0.873]
Epoch [18/120    avg_loss:0.253, val_acc:0.897]
Epoch [19/120    avg_loss:0.263, val_acc:0.905]
Epoch [20/120    avg_loss:0.203, val_acc:0.896]
Epoch [21/120    avg_loss:0.181, val_acc:0.932]
Epoch [22/120    avg_loss:0.162, val_acc:0.939]
Epoch [23/120    avg_loss:0.165, val_acc:0.910]
Epoch [24/120    avg_loss:0.198, val_acc:0.954]
Epoch [25/120    avg_loss:0.134, val_acc:0.959]
Epoch [26/120    avg_loss:0.107, val_acc:0.955]
Epoch [27/120    avg_loss:0.117, val_acc:0.904]
Epoch [28/120    avg_loss:0.111, val_acc:0.949]
Epoch [29/120    avg_loss:0.089, val_acc:0.954]
Epoch [30/120    avg_loss:0.126, val_acc:0.938]
Epoch [31/120    avg_loss:0.070, val_acc:0.967]
Epoch [32/120    avg_loss:0.078, val_acc:0.967]
Epoch [33/120    avg_loss:0.063, val_acc:0.965]
Epoch [34/120    avg_loss:0.065, val_acc:0.972]
Epoch [35/120    avg_loss:0.076, val_acc:0.979]
Epoch [36/120    avg_loss:0.064, val_acc:0.976]
Epoch [37/120    avg_loss:0.060, val_acc:0.961]
Epoch [38/120    avg_loss:0.062, val_acc:0.973]
Epoch [39/120    avg_loss:0.070, val_acc:0.960]
Epoch [40/120    avg_loss:0.110, val_acc:0.957]
Epoch [41/120    avg_loss:0.081, val_acc:0.966]
Epoch [42/120    avg_loss:0.082, val_acc:0.967]
Epoch [43/120    avg_loss:0.064, val_acc:0.970]
Epoch [44/120    avg_loss:0.063, val_acc:0.973]
Epoch [45/120    avg_loss:0.051, val_acc:0.975]
Epoch [46/120    avg_loss:0.038, val_acc:0.973]
Epoch [47/120    avg_loss:0.059, val_acc:0.978]
Epoch [48/120    avg_loss:0.037, val_acc:0.967]
Epoch [49/120    avg_loss:0.032, val_acc:0.979]
Epoch [50/120    avg_loss:0.025, val_acc:0.983]
Epoch [51/120    avg_loss:0.033, val_acc:0.983]
Epoch [52/120    avg_loss:0.020, val_acc:0.981]
Epoch [53/120    avg_loss:0.022, val_acc:0.983]
Epoch [54/120    avg_loss:0.020, val_acc:0.985]
Epoch [55/120    avg_loss:0.019, val_acc:0.986]
Epoch [56/120    avg_loss:0.023, val_acc:0.985]
Epoch [57/120    avg_loss:0.026, val_acc:0.985]
Epoch [58/120    avg_loss:0.021, val_acc:0.984]
Epoch [59/120    avg_loss:0.020, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.984]
Epoch [61/120    avg_loss:0.016, val_acc:0.985]
Epoch [62/120    avg_loss:0.016, val_acc:0.985]
Epoch [63/120    avg_loss:0.021, val_acc:0.983]
Epoch [64/120    avg_loss:0.017, val_acc:0.985]
Epoch [65/120    avg_loss:0.020, val_acc:0.985]
Epoch [66/120    avg_loss:0.020, val_acc:0.985]
Epoch [67/120    avg_loss:0.021, val_acc:0.984]
Epoch [68/120    avg_loss:0.023, val_acc:0.985]
Epoch [69/120    avg_loss:0.018, val_acc:0.985]
Epoch [70/120    avg_loss:0.022, val_acc:0.985]
Epoch [71/120    avg_loss:0.019, val_acc:0.985]
Epoch [72/120    avg_loss:0.017, val_acc:0.985]
Epoch [73/120    avg_loss:0.014, val_acc:0.985]
Epoch [74/120    avg_loss:0.022, val_acc:0.985]
Epoch [75/120    avg_loss:0.023, val_acc:0.985]
Epoch [76/120    avg_loss:0.019, val_acc:0.985]
Epoch [77/120    avg_loss:0.016, val_acc:0.985]
Epoch [78/120    avg_loss:0.023, val_acc:0.985]
Epoch [79/120    avg_loss:0.018, val_acc:0.985]
Epoch [80/120    avg_loss:0.019, val_acc:0.985]
Epoch [81/120    avg_loss:0.016, val_acc:0.985]
Epoch [82/120    avg_loss:0.017, val_acc:0.985]
Epoch [83/120    avg_loss:0.018, val_acc:0.985]
Epoch [84/120    avg_loss:0.024, val_acc:0.985]
Epoch [85/120    avg_loss:0.019, val_acc:0.985]
Epoch [86/120    avg_loss:0.018, val_acc:0.985]
Epoch [87/120    avg_loss:0.018, val_acc:0.985]
Epoch [88/120    avg_loss:0.016, val_acc:0.985]
Epoch [89/120    avg_loss:0.019, val_acc:0.985]
Epoch [90/120    avg_loss:0.017, val_acc:0.985]
Epoch [91/120    avg_loss:0.017, val_acc:0.985]
Epoch [92/120    avg_loss:0.021, val_acc:0.985]
Epoch [93/120    avg_loss:0.017, val_acc:0.985]
Epoch [94/120    avg_loss:0.019, val_acc:0.985]
Epoch [95/120    avg_loss:0.022, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.985]
Epoch [97/120    avg_loss:0.016, val_acc:0.985]
Epoch [98/120    avg_loss:0.021, val_acc:0.985]
Epoch [99/120    avg_loss:0.017, val_acc:0.985]
Epoch [100/120    avg_loss:0.019, val_acc:0.985]
Epoch [101/120    avg_loss:0.016, val_acc:0.985]
Epoch [102/120    avg_loss:0.017, val_acc:0.985]
Epoch [103/120    avg_loss:0.019, val_acc:0.985]
Epoch [104/120    avg_loss:0.017, val_acc:0.985]
Epoch [105/120    avg_loss:0.019, val_acc:0.985]
Epoch [106/120    avg_loss:0.019, val_acc:0.985]
Epoch [107/120    avg_loss:0.016, val_acc:0.985]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.017, val_acc:0.985]
Epoch [110/120    avg_loss:0.019, val_acc:0.985]
Epoch [111/120    avg_loss:0.015, val_acc:0.985]
Epoch [112/120    avg_loss:0.022, val_acc:0.985]
Epoch [113/120    avg_loss:0.018, val_acc:0.985]
Epoch [114/120    avg_loss:0.019, val_acc:0.985]
Epoch [115/120    avg_loss:0.018, val_acc:0.985]
Epoch [116/120    avg_loss:0.019, val_acc:0.985]
Epoch [117/120    avg_loss:0.020, val_acc:0.985]
Epoch [118/120    avg_loss:0.014, val_acc:0.985]
Epoch [119/120    avg_loss:0.018, val_acc:0.985]
Epoch [120/120    avg_loss:0.027, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     0     0     0     8    11     2]
 [    0     0 18020     0    35     0    33     0     2     0]
 [    0     6     0  1947     0     0     0     1    78     4]
 [    0    29    12     0  2921     0     0     0     9     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    18    12     0     0  4848     0     0     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0    38     0    24    56     0     0     0  3453     0]
 [    0     0     0     0    14    41     0     0     0   864]]

Accuracy:
98.94681030535271

F1 scores:
[       nan 0.99272221 0.99723298 0.96889774 0.97399133 0.98453414
 0.99354442 0.99535963 0.96939921 0.96374791]

Kappa:
0.9860485336534515
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb61c0d5be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.156, val_acc:0.128]
Epoch [2/120    avg_loss:1.831, val_acc:0.155]
Epoch [3/120    avg_loss:1.635, val_acc:0.310]
Epoch [4/120    avg_loss:1.420, val_acc:0.388]
Epoch [5/120    avg_loss:1.217, val_acc:0.405]
Epoch [6/120    avg_loss:1.049, val_acc:0.521]
Epoch [7/120    avg_loss:0.876, val_acc:0.639]
Epoch [8/120    avg_loss:0.694, val_acc:0.694]
Epoch [9/120    avg_loss:0.610, val_acc:0.761]
Epoch [10/120    avg_loss:0.492, val_acc:0.798]
Epoch [11/120    avg_loss:0.479, val_acc:0.806]
Epoch [12/120    avg_loss:0.430, val_acc:0.815]
Epoch [13/120    avg_loss:0.338, val_acc:0.846]
Epoch [14/120    avg_loss:0.339, val_acc:0.841]
Epoch [15/120    avg_loss:0.269, val_acc:0.863]
Epoch [16/120    avg_loss:0.270, val_acc:0.876]
Epoch [17/120    avg_loss:0.217, val_acc:0.912]
Epoch [18/120    avg_loss:0.199, val_acc:0.931]
Epoch [19/120    avg_loss:0.191, val_acc:0.932]
Epoch [20/120    avg_loss:0.163, val_acc:0.937]
Epoch [21/120    avg_loss:0.166, val_acc:0.958]
Epoch [22/120    avg_loss:0.130, val_acc:0.938]
Epoch [23/120    avg_loss:0.135, val_acc:0.961]
Epoch [24/120    avg_loss:0.141, val_acc:0.960]
Epoch [25/120    avg_loss:0.092, val_acc:0.968]
Epoch [26/120    avg_loss:0.093, val_acc:0.953]
Epoch [27/120    avg_loss:0.113, val_acc:0.954]
Epoch [28/120    avg_loss:0.090, val_acc:0.961]
Epoch [29/120    avg_loss:0.102, val_acc:0.977]
Epoch [30/120    avg_loss:0.068, val_acc:0.976]
Epoch [31/120    avg_loss:0.115, val_acc:0.972]
Epoch [32/120    avg_loss:0.072, val_acc:0.970]
Epoch [33/120    avg_loss:0.114, val_acc:0.958]
Epoch [34/120    avg_loss:0.074, val_acc:0.980]
Epoch [35/120    avg_loss:0.064, val_acc:0.973]
Epoch [36/120    avg_loss:0.076, val_acc:0.955]
Epoch [37/120    avg_loss:0.078, val_acc:0.975]
Epoch [38/120    avg_loss:0.050, val_acc:0.972]
Epoch [39/120    avg_loss:0.055, val_acc:0.975]
Epoch [40/120    avg_loss:0.059, val_acc:0.982]
Epoch [41/120    avg_loss:0.049, val_acc:0.987]
Epoch [42/120    avg_loss:0.041, val_acc:0.986]
Epoch [43/120    avg_loss:0.038, val_acc:0.984]
Epoch [44/120    avg_loss:0.056, val_acc:0.965]
Epoch [45/120    avg_loss:0.040, val_acc:0.977]
Epoch [46/120    avg_loss:0.068, val_acc:0.963]
Epoch [47/120    avg_loss:0.040, val_acc:0.976]
Epoch [48/120    avg_loss:0.047, val_acc:0.969]
Epoch [49/120    avg_loss:0.039, val_acc:0.984]
Epoch [50/120    avg_loss:0.430, val_acc:0.573]
Epoch [51/120    avg_loss:0.964, val_acc:0.656]
Epoch [52/120    avg_loss:0.599, val_acc:0.689]
Epoch [53/120    avg_loss:0.509, val_acc:0.766]
Epoch [54/120    avg_loss:0.448, val_acc:0.819]
Epoch [55/120    avg_loss:0.379, val_acc:0.843]
Epoch [56/120    avg_loss:0.346, val_acc:0.837]
Epoch [57/120    avg_loss:0.343, val_acc:0.863]
Epoch [58/120    avg_loss:0.308, val_acc:0.874]
Epoch [59/120    avg_loss:0.292, val_acc:0.869]
Epoch [60/120    avg_loss:0.315, val_acc:0.888]
Epoch [61/120    avg_loss:0.288, val_acc:0.903]
Epoch [62/120    avg_loss:0.288, val_acc:0.905]
Epoch [63/120    avg_loss:0.260, val_acc:0.908]
Epoch [64/120    avg_loss:0.267, val_acc:0.914]
Epoch [65/120    avg_loss:0.242, val_acc:0.918]
Epoch [66/120    avg_loss:0.242, val_acc:0.914]
Epoch [67/120    avg_loss:0.241, val_acc:0.928]
Epoch [68/120    avg_loss:0.227, val_acc:0.928]
Epoch [69/120    avg_loss:0.244, val_acc:0.926]
Epoch [70/120    avg_loss:0.232, val_acc:0.928]
Epoch [71/120    avg_loss:0.225, val_acc:0.929]
Epoch [72/120    avg_loss:0.238, val_acc:0.929]
Epoch [73/120    avg_loss:0.228, val_acc:0.928]
Epoch [74/120    avg_loss:0.223, val_acc:0.928]
Epoch [75/120    avg_loss:0.226, val_acc:0.930]
Epoch [76/120    avg_loss:0.227, val_acc:0.929]
Epoch [77/120    avg_loss:0.232, val_acc:0.930]
Epoch [78/120    avg_loss:0.237, val_acc:0.931]
Epoch [79/120    avg_loss:0.238, val_acc:0.931]
Epoch [80/120    avg_loss:0.219, val_acc:0.930]
Epoch [81/120    avg_loss:0.214, val_acc:0.930]
Epoch [82/120    avg_loss:0.231, val_acc:0.930]
Epoch [83/120    avg_loss:0.235, val_acc:0.930]
Epoch [84/120    avg_loss:0.226, val_acc:0.930]
Epoch [85/120    avg_loss:0.232, val_acc:0.931]
Epoch [86/120    avg_loss:0.223, val_acc:0.930]
Epoch [87/120    avg_loss:0.218, val_acc:0.930]
Epoch [88/120    avg_loss:0.217, val_acc:0.930]
Epoch [89/120    avg_loss:0.226, val_acc:0.930]
Epoch [90/120    avg_loss:0.219, val_acc:0.930]
Epoch [91/120    avg_loss:0.211, val_acc:0.931]
Epoch [92/120    avg_loss:0.215, val_acc:0.931]
Epoch [93/120    avg_loss:0.225, val_acc:0.930]
Epoch [94/120    avg_loss:0.219, val_acc:0.930]
Epoch [95/120    avg_loss:0.224, val_acc:0.930]
Epoch [96/120    avg_loss:0.218, val_acc:0.930]
Epoch [97/120    avg_loss:0.229, val_acc:0.930]
Epoch [98/120    avg_loss:0.225, val_acc:0.930]
Epoch [99/120    avg_loss:0.228, val_acc:0.930]
Epoch [100/120    avg_loss:0.212, val_acc:0.930]
Epoch [101/120    avg_loss:0.209, val_acc:0.931]
Epoch [102/120    avg_loss:0.219, val_acc:0.931]
Epoch [103/120    avg_loss:0.219, val_acc:0.930]
Epoch [104/120    avg_loss:0.231, val_acc:0.931]
Epoch [105/120    avg_loss:0.228, val_acc:0.931]
Epoch [106/120    avg_loss:0.214, val_acc:0.931]
Epoch [107/120    avg_loss:0.217, val_acc:0.931]
Epoch [108/120    avg_loss:0.245, val_acc:0.931]
Epoch [109/120    avg_loss:0.226, val_acc:0.931]
Epoch [110/120    avg_loss:0.207, val_acc:0.931]
Epoch [111/120    avg_loss:0.216, val_acc:0.931]
Epoch [112/120    avg_loss:0.220, val_acc:0.931]
Epoch [113/120    avg_loss:0.218, val_acc:0.931]
Epoch [114/120    avg_loss:0.215, val_acc:0.931]
Epoch [115/120    avg_loss:0.212, val_acc:0.931]
Epoch [116/120    avg_loss:0.215, val_acc:0.931]
Epoch [117/120    avg_loss:0.210, val_acc:0.931]
Epoch [118/120    avg_loss:0.220, val_acc:0.931]
Epoch [119/120    avg_loss:0.220, val_acc:0.931]
Epoch [120/120    avg_loss:0.231, val_acc:0.931]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5898     1     0   111     0    19    46   349     8]
 [    0     0 17199     0   150     0   741     0     0     0]
 [    0     6     0  1835     2     0     0     0   173    20]
 [    0    76    43     0  2808     0    25     0    20     0]
 [    0     0     0     0     0  1300     0     0     0     5]
 [    0     0    69     0     0     0  4729     0    80     0]
 [    0    31     0     0     0     0     4  1243    12     0]
 [    0    14     1    80    83     0     0     0  3393     0]
 [    0     1     0     0    20     2     0     0     0   896]]

Accuracy:
94.71718121128865

F1 scores:
[       nan 0.94686145 0.97161258 0.92887876 0.91376505 0.99731492
 0.90977299 0.96393951 0.89312977 0.96969697]

Kappa:
0.9306167128951707
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3152d5db38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.188, val_acc:0.476]
Epoch [2/120    avg_loss:1.898, val_acc:0.548]
Epoch [3/120    avg_loss:1.605, val_acc:0.584]
Epoch [4/120    avg_loss:1.361, val_acc:0.683]
Epoch [5/120    avg_loss:1.159, val_acc:0.667]
Epoch [6/120    avg_loss:1.046, val_acc:0.689]
Epoch [7/120    avg_loss:0.782, val_acc:0.615]
Epoch [8/120    avg_loss:0.639, val_acc:0.758]
Epoch [9/120    avg_loss:0.538, val_acc:0.725]
Epoch [10/120    avg_loss:0.466, val_acc:0.820]
Epoch [11/120    avg_loss:0.463, val_acc:0.787]
Epoch [12/120    avg_loss:0.430, val_acc:0.782]
Epoch [13/120    avg_loss:0.418, val_acc:0.787]
Epoch [14/120    avg_loss:0.354, val_acc:0.784]
Epoch [15/120    avg_loss:0.310, val_acc:0.822]
Epoch [16/120    avg_loss:0.267, val_acc:0.850]
Epoch [17/120    avg_loss:0.253, val_acc:0.889]
Epoch [18/120    avg_loss:0.215, val_acc:0.903]
Epoch [19/120    avg_loss:0.199, val_acc:0.931]
Epoch [20/120    avg_loss:0.167, val_acc:0.923]
Epoch [21/120    avg_loss:0.155, val_acc:0.920]
Epoch [22/120    avg_loss:0.195, val_acc:0.928]
Epoch [23/120    avg_loss:0.172, val_acc:0.953]
Epoch [24/120    avg_loss:0.276, val_acc:0.952]
Epoch [25/120    avg_loss:0.149, val_acc:0.951]
Epoch [26/120    avg_loss:0.113, val_acc:0.936]
Epoch [27/120    avg_loss:0.105, val_acc:0.961]
Epoch [28/120    avg_loss:0.096, val_acc:0.961]
Epoch [29/120    avg_loss:0.216, val_acc:0.405]
Epoch [30/120    avg_loss:1.561, val_acc:0.663]
Epoch [31/120    avg_loss:1.220, val_acc:0.720]
Epoch [32/120    avg_loss:1.148, val_acc:0.699]
Epoch [33/120    avg_loss:1.034, val_acc:0.710]
Epoch [34/120    avg_loss:0.898, val_acc:0.745]
Epoch [35/120    avg_loss:0.832, val_acc:0.749]
Epoch [36/120    avg_loss:0.791, val_acc:0.720]
Epoch [37/120    avg_loss:0.723, val_acc:0.785]
Epoch [38/120    avg_loss:0.678, val_acc:0.776]
Epoch [39/120    avg_loss:0.625, val_acc:0.812]
Epoch [40/120    avg_loss:0.586, val_acc:0.767]
Epoch [41/120    avg_loss:0.514, val_acc:0.806]
Epoch [42/120    avg_loss:0.456, val_acc:0.813]
Epoch [43/120    avg_loss:0.426, val_acc:0.808]
Epoch [44/120    avg_loss:0.455, val_acc:0.806]
Epoch [45/120    avg_loss:0.419, val_acc:0.812]
Epoch [46/120    avg_loss:0.442, val_acc:0.814]
Epoch [47/120    avg_loss:0.427, val_acc:0.813]
Epoch [48/120    avg_loss:0.444, val_acc:0.816]
Epoch [49/120    avg_loss:0.429, val_acc:0.819]
Epoch [50/120    avg_loss:0.409, val_acc:0.822]
Epoch [51/120    avg_loss:0.414, val_acc:0.823]
Epoch [52/120    avg_loss:0.403, val_acc:0.819]
Epoch [53/120    avg_loss:0.413, val_acc:0.824]
Epoch [54/120    avg_loss:0.391, val_acc:0.812]
Epoch [55/120    avg_loss:0.395, val_acc:0.818]
Epoch [56/120    avg_loss:0.408, val_acc:0.819]
Epoch [57/120    avg_loss:0.363, val_acc:0.820]
Epoch [58/120    avg_loss:0.389, val_acc:0.825]
Epoch [59/120    avg_loss:0.383, val_acc:0.821]
Epoch [60/120    avg_loss:0.383, val_acc:0.823]
Epoch [61/120    avg_loss:0.381, val_acc:0.824]
Epoch [62/120    avg_loss:0.386, val_acc:0.825]
Epoch [63/120    avg_loss:0.381, val_acc:0.823]
Epoch [64/120    avg_loss:0.384, val_acc:0.823]
Epoch [65/120    avg_loss:0.374, val_acc:0.824]
Epoch [66/120    avg_loss:0.382, val_acc:0.823]
Epoch [67/120    avg_loss:0.370, val_acc:0.823]
Epoch [68/120    avg_loss:0.356, val_acc:0.823]
Epoch [69/120    avg_loss:0.393, val_acc:0.823]
Epoch [70/120    avg_loss:0.381, val_acc:0.823]
Epoch [71/120    avg_loss:0.402, val_acc:0.823]
Epoch [72/120    avg_loss:0.353, val_acc:0.823]
Epoch [73/120    avg_loss:0.387, val_acc:0.823]
Epoch [74/120    avg_loss:0.361, val_acc:0.823]
Epoch [75/120    avg_loss:0.358, val_acc:0.823]
Epoch [76/120    avg_loss:0.379, val_acc:0.823]
Epoch [77/120    avg_loss:0.375, val_acc:0.823]
Epoch [78/120    avg_loss:0.396, val_acc:0.823]
Epoch [79/120    avg_loss:0.376, val_acc:0.823]
Epoch [80/120    avg_loss:0.387, val_acc:0.823]
Epoch [81/120    avg_loss:0.380, val_acc:0.823]
Epoch [82/120    avg_loss:0.365, val_acc:0.823]
Epoch [83/120    avg_loss:0.373, val_acc:0.823]
Epoch [84/120    avg_loss:0.371, val_acc:0.823]
Epoch [85/120    avg_loss:0.377, val_acc:0.823]
Epoch [86/120    avg_loss:0.380, val_acc:0.823]
Epoch [87/120    avg_loss:0.359, val_acc:0.823]
Epoch [88/120    avg_loss:0.378, val_acc:0.823]
Epoch [89/120    avg_loss:0.381, val_acc:0.823]
Epoch [90/120    avg_loss:0.377, val_acc:0.823]
Epoch [91/120    avg_loss:0.368, val_acc:0.823]
Epoch [92/120    avg_loss:0.358, val_acc:0.823]
Epoch [93/120    avg_loss:0.365, val_acc:0.823]
Epoch [94/120    avg_loss:0.388, val_acc:0.823]
Epoch [95/120    avg_loss:0.388, val_acc:0.823]
Epoch [96/120    avg_loss:0.383, val_acc:0.823]
Epoch [97/120    avg_loss:0.376, val_acc:0.823]
Epoch [98/120    avg_loss:0.378, val_acc:0.823]
Epoch [99/120    avg_loss:0.370, val_acc:0.823]
Epoch [100/120    avg_loss:0.388, val_acc:0.823]
Epoch [101/120    avg_loss:0.395, val_acc:0.823]
Epoch [102/120    avg_loss:0.382, val_acc:0.823]
Epoch [103/120    avg_loss:0.379, val_acc:0.823]
Epoch [104/120    avg_loss:0.364, val_acc:0.823]
Epoch [105/120    avg_loss:0.383, val_acc:0.823]
Epoch [106/120    avg_loss:0.371, val_acc:0.823]
Epoch [107/120    avg_loss:0.387, val_acc:0.823]
Epoch [108/120    avg_loss:0.379, val_acc:0.823]
Epoch [109/120    avg_loss:0.387, val_acc:0.823]
Epoch [110/120    avg_loss:0.377, val_acc:0.823]
Epoch [111/120    avg_loss:0.393, val_acc:0.823]
Epoch [112/120    avg_loss:0.381, val_acc:0.823]
Epoch [113/120    avg_loss:0.385, val_acc:0.823]
Epoch [114/120    avg_loss:0.381, val_acc:0.823]
Epoch [115/120    avg_loss:0.377, val_acc:0.823]
Epoch [116/120    avg_loss:0.375, val_acc:0.823]
Epoch [117/120    avg_loss:0.364, val_acc:0.823]
Epoch [118/120    avg_loss:0.384, val_acc:0.823]
Epoch [119/120    avg_loss:0.376, val_acc:0.823]
Epoch [120/120    avg_loss:0.357, val_acc:0.823]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5637     0     5   160     0    77    29   454    70]
 [    0     0 13817     0    61     0  4212     0     0     0]
 [    0     5     0  1864     4     0     0     3   142    18]
 [    0   137    30     0  2636     0   143     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2   825    39    25     0  3854     0   133     0]
 [    0    36     6     0     0     0     3  1225     0    20]
 [    0   101     0    83    37     0    82     0  3268     0]
 [    0    16     0     3    19    74     1     1     0   805]]

Accuracy:
82.93206082953752

F1 scores:
[       nan 0.91169335 0.84332275 0.92506203 0.89144403 0.97242921
 0.58173585 0.96153846 0.86067948 0.87882096]

Kappa:
0.7816363866485337
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f6a727be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.090, val_acc:0.122]
Epoch [2/120    avg_loss:1.735, val_acc:0.240]
Epoch [3/120    avg_loss:1.475, val_acc:0.309]
Epoch [4/120    avg_loss:1.283, val_acc:0.414]
Epoch [5/120    avg_loss:1.084, val_acc:0.388]
Epoch [6/120    avg_loss:0.934, val_acc:0.489]
Epoch [7/120    avg_loss:0.769, val_acc:0.583]
Epoch [8/120    avg_loss:0.664, val_acc:0.680]
Epoch [9/120    avg_loss:0.603, val_acc:0.610]
Epoch [10/120    avg_loss:0.954, val_acc:0.625]
Epoch [11/120    avg_loss:0.617, val_acc:0.690]
Epoch [12/120    avg_loss:0.477, val_acc:0.775]
Epoch [13/120    avg_loss:0.369, val_acc:0.845]
Epoch [14/120    avg_loss:0.323, val_acc:0.861]
Epoch [15/120    avg_loss:0.260, val_acc:0.870]
Epoch [16/120    avg_loss:0.241, val_acc:0.912]
Epoch [17/120    avg_loss:0.258, val_acc:0.859]
Epoch [18/120    avg_loss:0.199, val_acc:0.913]
Epoch [19/120    avg_loss:0.185, val_acc:0.851]
Epoch [20/120    avg_loss:0.164, val_acc:0.902]
Epoch [21/120    avg_loss:0.160, val_acc:0.916]
Epoch [22/120    avg_loss:0.122, val_acc:0.961]
Epoch [23/120    avg_loss:0.116, val_acc:0.955]
Epoch [24/120    avg_loss:0.141, val_acc:0.951]
Epoch [25/120    avg_loss:0.095, val_acc:0.930]
Epoch [26/120    avg_loss:0.087, val_acc:0.947]
Epoch [27/120    avg_loss:0.080, val_acc:0.967]
Epoch [28/120    avg_loss:0.065, val_acc:0.968]
Epoch [29/120    avg_loss:0.058, val_acc:0.961]
Epoch [30/120    avg_loss:0.088, val_acc:0.948]
Epoch [31/120    avg_loss:0.071, val_acc:0.908]
Epoch [32/120    avg_loss:0.052, val_acc:0.961]
Epoch [33/120    avg_loss:0.043, val_acc:0.967]
Epoch [34/120    avg_loss:0.117, val_acc:0.955]
Epoch [35/120    avg_loss:0.052, val_acc:0.960]
Epoch [36/120    avg_loss:0.038, val_acc:0.973]
Epoch [37/120    avg_loss:0.035, val_acc:0.980]
Epoch [38/120    avg_loss:0.056, val_acc:0.968]
Epoch [39/120    avg_loss:0.046, val_acc:0.975]
Epoch [40/120    avg_loss:0.026, val_acc:0.972]
Epoch [41/120    avg_loss:0.024, val_acc:0.980]
Epoch [42/120    avg_loss:0.016, val_acc:0.979]
Epoch [43/120    avg_loss:0.022, val_acc:0.979]
Epoch [44/120    avg_loss:0.024, val_acc:0.978]
Epoch [45/120    avg_loss:0.052, val_acc:0.927]
Epoch [46/120    avg_loss:0.031, val_acc:0.981]
Epoch [47/120    avg_loss:0.043, val_acc:0.972]
Epoch [48/120    avg_loss:0.026, val_acc:0.985]
Epoch [49/120    avg_loss:0.021, val_acc:0.985]
Epoch [50/120    avg_loss:0.014, val_acc:0.982]
Epoch [51/120    avg_loss:0.025, val_acc:0.967]
Epoch [52/120    avg_loss:0.029, val_acc:0.964]
Epoch [53/120    avg_loss:0.027, val_acc:0.964]
Epoch [54/120    avg_loss:0.046, val_acc:0.978]
Epoch [55/120    avg_loss:0.027, val_acc:0.960]
Epoch [56/120    avg_loss:0.018, val_acc:0.982]
Epoch [57/120    avg_loss:0.040, val_acc:0.952]
Epoch [58/120    avg_loss:0.028, val_acc:0.979]
Epoch [59/120    avg_loss:0.021, val_acc:0.980]
Epoch [60/120    avg_loss:0.012, val_acc:0.983]
Epoch [61/120    avg_loss:0.018, val_acc:0.986]
Epoch [62/120    avg_loss:0.011, val_acc:0.985]
Epoch [63/120    avg_loss:0.013, val_acc:0.981]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.022, val_acc:0.973]
Epoch [67/120    avg_loss:0.029, val_acc:0.986]
Epoch [68/120    avg_loss:0.017, val_acc:0.969]
Epoch [69/120    avg_loss:0.046, val_acc:0.981]
Epoch [70/120    avg_loss:0.022, val_acc:0.985]
Epoch [71/120    avg_loss:0.026, val_acc:0.973]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.010, val_acc:0.989]
Epoch [76/120    avg_loss:0.010, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.006, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.004, val_acc:0.990]
Epoch [89/120    avg_loss:0.011, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.003, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.011, val_acc:0.989]
Epoch [105/120    avg_loss:0.025, val_acc:0.984]
Epoch [106/120    avg_loss:0.015, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.008, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.991]
Epoch [115/120    avg_loss:0.003, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     0     0     0    13     2    11     1]
 [    0     0 18072     0    12     0     0     0     6     0]
 [    0     2     0  1997     0     0     0     0    36     1]
 [    0    11     4     2  2949     0     2     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    28     6     0     0  4844     0     0     0]
 [    0     0     0     0     0     0     7  1281     0     2]
 [    0     5     0    28    52     0     0     0  3486     0]
 [    0     0     0     0    14    16     0     0     0   889]]

Accuracy:
99.36133805702167

F1 scores:
[       nan 0.99649942 0.99861856 0.98156795 0.98316386 0.99390708
 0.99425287 0.99572483 0.98045282 0.97961433]

Kappa:
0.9915362087300015
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc92f2e8c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.180, val_acc:0.077]
Epoch [2/120    avg_loss:1.889, val_acc:0.263]
Epoch [3/120    avg_loss:1.637, val_acc:0.552]
Epoch [4/120    avg_loss:1.459, val_acc:0.574]
Epoch [5/120    avg_loss:1.196, val_acc:0.518]
Epoch [6/120    avg_loss:1.011, val_acc:0.617]
Epoch [7/120    avg_loss:0.871, val_acc:0.647]
Epoch [8/120    avg_loss:0.746, val_acc:0.742]
Epoch [9/120    avg_loss:0.602, val_acc:0.676]
Epoch [10/120    avg_loss:0.532, val_acc:0.763]
Epoch [11/120    avg_loss:0.410, val_acc:0.778]
Epoch [12/120    avg_loss:0.335, val_acc:0.847]
Epoch [13/120    avg_loss:0.331, val_acc:0.806]
Epoch [14/120    avg_loss:0.297, val_acc:0.884]
Epoch [15/120    avg_loss:0.428, val_acc:0.898]
Epoch [16/120    avg_loss:0.237, val_acc:0.875]
Epoch [17/120    avg_loss:0.228, val_acc:0.911]
Epoch [18/120    avg_loss:0.213, val_acc:0.890]
Epoch [19/120    avg_loss:0.190, val_acc:0.928]
Epoch [20/120    avg_loss:0.239, val_acc:0.902]
Epoch [21/120    avg_loss:0.160, val_acc:0.945]
Epoch [22/120    avg_loss:0.135, val_acc:0.937]
Epoch [23/120    avg_loss:0.151, val_acc:0.950]
Epoch [24/120    avg_loss:0.114, val_acc:0.950]
Epoch [25/120    avg_loss:0.101, val_acc:0.933]
Epoch [26/120    avg_loss:0.102, val_acc:0.923]
Epoch [27/120    avg_loss:0.106, val_acc:0.949]
Epoch [28/120    avg_loss:0.084, val_acc:0.932]
Epoch [29/120    avg_loss:0.067, val_acc:0.943]
Epoch [30/120    avg_loss:0.093, val_acc:0.944]
Epoch [31/120    avg_loss:0.083, val_acc:0.961]
Epoch [32/120    avg_loss:0.083, val_acc:0.964]
Epoch [33/120    avg_loss:0.112, val_acc:0.956]
Epoch [34/120    avg_loss:0.078, val_acc:0.931]
Epoch [35/120    avg_loss:0.084, val_acc:0.937]
Epoch [36/120    avg_loss:0.074, val_acc:0.946]
Epoch [37/120    avg_loss:0.083, val_acc:0.949]
Epoch [38/120    avg_loss:0.108, val_acc:0.956]
Epoch [39/120    avg_loss:0.046, val_acc:0.961]
Epoch [40/120    avg_loss:0.048, val_acc:0.967]
Epoch [41/120    avg_loss:0.046, val_acc:0.968]
Epoch [42/120    avg_loss:0.047, val_acc:0.967]
Epoch [43/120    avg_loss:0.036, val_acc:0.961]
Epoch [44/120    avg_loss:0.041, val_acc:0.964]
Epoch [45/120    avg_loss:0.058, val_acc:0.971]
Epoch [46/120    avg_loss:0.038, val_acc:0.972]
Epoch [47/120    avg_loss:0.036, val_acc:0.969]
Epoch [48/120    avg_loss:0.021, val_acc:0.965]
Epoch [49/120    avg_loss:0.030, val_acc:0.971]
Epoch [50/120    avg_loss:0.048, val_acc:0.961]
Epoch [51/120    avg_loss:0.042, val_acc:0.965]
Epoch [52/120    avg_loss:0.047, val_acc:0.963]
Epoch [53/120    avg_loss:0.023, val_acc:0.972]
Epoch [54/120    avg_loss:0.029, val_acc:0.896]
Epoch [55/120    avg_loss:0.039, val_acc:0.968]
Epoch [56/120    avg_loss:0.028, val_acc:0.974]
Epoch [57/120    avg_loss:0.023, val_acc:0.938]
Epoch [58/120    avg_loss:0.190, val_acc:0.948]
Epoch [59/120    avg_loss:0.041, val_acc:0.967]
Epoch [60/120    avg_loss:0.035, val_acc:0.971]
Epoch [61/120    avg_loss:0.036, val_acc:0.977]
Epoch [62/120    avg_loss:0.083, val_acc:0.945]
Epoch [63/120    avg_loss:0.050, val_acc:0.965]
Epoch [64/120    avg_loss:0.042, val_acc:0.961]
Epoch [65/120    avg_loss:0.033, val_acc:0.964]
Epoch [66/120    avg_loss:0.067, val_acc:0.967]
Epoch [67/120    avg_loss:0.031, val_acc:0.965]
Epoch [68/120    avg_loss:0.024, val_acc:0.971]
Epoch [69/120    avg_loss:0.016, val_acc:0.971]
Epoch [70/120    avg_loss:0.017, val_acc:0.967]
Epoch [71/120    avg_loss:0.012, val_acc:0.971]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.093, val_acc:0.921]
Epoch [74/120    avg_loss:0.037, val_acc:0.975]
Epoch [75/120    avg_loss:0.020, val_acc:0.977]
Epoch [76/120    avg_loss:0.026, val_acc:0.962]
Epoch [77/120    avg_loss:0.016, val_acc:0.976]
Epoch [78/120    avg_loss:0.017, val_acc:0.970]
Epoch [79/120    avg_loss:0.013, val_acc:0.975]
Epoch [80/120    avg_loss:0.019, val_acc:0.974]
Epoch [81/120    avg_loss:0.024, val_acc:0.963]
Epoch [82/120    avg_loss:0.032, val_acc:0.971]
Epoch [83/120    avg_loss:0.032, val_acc:0.975]
Epoch [84/120    avg_loss:0.014, val_acc:0.976]
Epoch [85/120    avg_loss:0.018, val_acc:0.976]
Epoch [86/120    avg_loss:0.010, val_acc:0.977]
Epoch [87/120    avg_loss:0.007, val_acc:0.977]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.013, val_acc:0.971]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.013, val_acc:0.979]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.979]
Epoch [95/120    avg_loss:0.018, val_acc:0.959]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.981]
Epoch [98/120    avg_loss:0.005, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.005, val_acc:0.980]
Epoch [102/120    avg_loss:0.006, val_acc:0.975]
Epoch [103/120    avg_loss:0.006, val_acc:0.980]
Epoch [104/120    avg_loss:0.446, val_acc:0.532]
Epoch [105/120    avg_loss:1.493, val_acc:0.592]
Epoch [106/120    avg_loss:1.240, val_acc:0.646]
Epoch [107/120    avg_loss:1.163, val_acc:0.688]
Epoch [108/120    avg_loss:1.041, val_acc:0.695]
Epoch [109/120    avg_loss:0.970, val_acc:0.732]
Epoch [110/120    avg_loss:0.883, val_acc:0.760]
Epoch [111/120    avg_loss:0.850, val_acc:0.772]
Epoch [112/120    avg_loss:0.784, val_acc:0.699]
Epoch [113/120    avg_loss:0.753, val_acc:0.713]
Epoch [114/120    avg_loss:0.767, val_acc:0.748]
Epoch [115/120    avg_loss:0.725, val_acc:0.726]
Epoch [116/120    avg_loss:0.730, val_acc:0.728]
Epoch [117/120    avg_loss:0.752, val_acc:0.767]
Epoch [118/120    avg_loss:0.736, val_acc:0.765]
Epoch [119/120    avg_loss:0.744, val_acc:0.752]
Epoch [120/120    avg_loss:0.728, val_acc:0.752]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4579    76   102   362    21   286   257   426   323]
 [    0     0 13398     0    26     0  4666     0     0     0]
 [    0    20     0  1562   154     0     4     0   176   120]
 [    0    39   273     5  2332     0   198     5    81    39]
 [    0     0     0     0     0  1302     2     1     0     0]
 [    0     0   143    10   172     0  4367     0   186     0]
 [    0    87     0     0     0     0    10  1193     0     0]
 [    0   124     6   126    57     0   119     0  3115    24]
 [    0    18     0     7    13   150     0     0     1   730]]

Accuracy:
78.51444822018172

F1 scores:
[       nan 0.8105142  0.83774151 0.81185031 0.76609724 0.93736501
 0.60110117 0.86890022 0.82451032 0.6774942 ]

Kappa:
0.7279645977332292
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a6ef36b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.160, val_acc:0.246]
Epoch [2/120    avg_loss:1.870, val_acc:0.266]
Epoch [3/120    avg_loss:1.624, val_acc:0.313]
Epoch [4/120    avg_loss:1.412, val_acc:0.334]
Epoch [5/120    avg_loss:1.251, val_acc:0.380]
Epoch [6/120    avg_loss:1.118, val_acc:0.482]
Epoch [7/120    avg_loss:0.915, val_acc:0.671]
Epoch [8/120    avg_loss:0.746, val_acc:0.677]
Epoch [9/120    avg_loss:0.603, val_acc:0.786]
Epoch [10/120    avg_loss:0.538, val_acc:0.756]
Epoch [11/120    avg_loss:0.401, val_acc:0.783]
Epoch [12/120    avg_loss:0.374, val_acc:0.887]
Epoch [13/120    avg_loss:0.401, val_acc:0.880]
Epoch [14/120    avg_loss:0.322, val_acc:0.858]
Epoch [15/120    avg_loss:0.266, val_acc:0.880]
Epoch [16/120    avg_loss:0.253, val_acc:0.931]
Epoch [17/120    avg_loss:0.223, val_acc:0.879]
Epoch [18/120    avg_loss:0.185, val_acc:0.916]
Epoch [19/120    avg_loss:0.161, val_acc:0.952]
Epoch [20/120    avg_loss:0.167, val_acc:0.937]
Epoch [21/120    avg_loss:0.142, val_acc:0.950]
Epoch [22/120    avg_loss:0.124, val_acc:0.899]
Epoch [23/120    avg_loss:0.117, val_acc:0.946]
Epoch [24/120    avg_loss:0.113, val_acc:0.943]
Epoch [25/120    avg_loss:0.127, val_acc:0.928]
Epoch [26/120    avg_loss:0.084, val_acc:0.963]
Epoch [27/120    avg_loss:0.120, val_acc:0.932]
Epoch [28/120    avg_loss:0.525, val_acc:0.912]
Epoch [29/120    avg_loss:0.154, val_acc:0.927]
Epoch [30/120    avg_loss:0.123, val_acc:0.881]
Epoch [31/120    avg_loss:0.134, val_acc:0.901]
Epoch [32/120    avg_loss:0.099, val_acc:0.963]
Epoch [33/120    avg_loss:0.078, val_acc:0.947]
Epoch [34/120    avg_loss:0.077, val_acc:0.957]
Epoch [35/120    avg_loss:0.073, val_acc:0.954]
Epoch [36/120    avg_loss:0.059, val_acc:0.946]
Epoch [37/120    avg_loss:0.059, val_acc:0.970]
Epoch [38/120    avg_loss:0.055, val_acc:0.955]
Epoch [39/120    avg_loss:0.066, val_acc:0.973]
Epoch [40/120    avg_loss:0.083, val_acc:0.967]
Epoch [41/120    avg_loss:0.055, val_acc:0.968]
Epoch [42/120    avg_loss:0.059, val_acc:0.972]
Epoch [43/120    avg_loss:0.044, val_acc:0.960]
Epoch [44/120    avg_loss:0.040, val_acc:0.978]
Epoch [45/120    avg_loss:0.036, val_acc:0.947]
Epoch [46/120    avg_loss:0.028, val_acc:0.969]
Epoch [47/120    avg_loss:0.030, val_acc:0.984]
Epoch [48/120    avg_loss:0.047, val_acc:0.978]
Epoch [49/120    avg_loss:0.030, val_acc:0.983]
Epoch [50/120    avg_loss:0.028, val_acc:0.967]
Epoch [51/120    avg_loss:0.022, val_acc:0.946]
Epoch [52/120    avg_loss:0.030, val_acc:0.981]
Epoch [53/120    avg_loss:0.026, val_acc:0.984]
Epoch [54/120    avg_loss:0.029, val_acc:0.973]
Epoch [55/120    avg_loss:0.026, val_acc:0.965]
Epoch [56/120    avg_loss:0.026, val_acc:0.965]
Epoch [57/120    avg_loss:0.041, val_acc:0.988]
Epoch [58/120    avg_loss:0.022, val_acc:0.938]
Epoch [59/120    avg_loss:0.046, val_acc:0.981]
Epoch [60/120    avg_loss:0.029, val_acc:0.983]
Epoch [61/120    avg_loss:0.026, val_acc:0.988]
Epoch [62/120    avg_loss:0.035, val_acc:0.979]
Epoch [63/120    avg_loss:0.020, val_acc:0.987]
Epoch [64/120    avg_loss:0.017, val_acc:0.981]
Epoch [65/120    avg_loss:0.027, val_acc:0.981]
Epoch [66/120    avg_loss:0.020, val_acc:0.992]
Epoch [67/120    avg_loss:0.007, val_acc:0.992]
Epoch [68/120    avg_loss:0.009, val_acc:0.991]
Epoch [69/120    avg_loss:0.009, val_acc:0.993]
Epoch [70/120    avg_loss:0.075, val_acc:0.974]
Epoch [71/120    avg_loss:0.023, val_acc:0.986]
Epoch [72/120    avg_loss:0.043, val_acc:0.982]
Epoch [73/120    avg_loss:0.022, val_acc:0.980]
Epoch [74/120    avg_loss:0.016, val_acc:0.979]
Epoch [75/120    avg_loss:0.016, val_acc:0.984]
Epoch [76/120    avg_loss:0.017, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.989]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.979]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.989]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0    11     5    13     9]
 [    0     1 18056     0    14     0    19     0     0     0]
 [    0     1     0  1955     0     0     0     0    76     4]
 [    0     7     3     0  2945     0     6     0     6     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    31    17     0     0  4830     0     0     0]
 [    0     0     0     0     0     0     6  1283     0     1]
 [    0    12     0     6    35     0     0     0  3518     0]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
99.2167353529511

F1 scores:
[       nan 0.99540749 0.99812051 0.97409068 0.98494983 0.99126472
 0.99076923 0.99534523 0.97939866 0.96923077]

Kappa:
0.989621181101013
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8fb8c9fba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.103, val_acc:0.175]
Epoch [2/120    avg_loss:1.813, val_acc:0.248]
Epoch [3/120    avg_loss:1.576, val_acc:0.322]
Epoch [4/120    avg_loss:1.387, val_acc:0.417]
Epoch [5/120    avg_loss:1.235, val_acc:0.690]
Epoch [6/120    avg_loss:1.065, val_acc:0.716]
Epoch [7/120    avg_loss:0.946, val_acc:0.775]
Epoch [8/120    avg_loss:0.843, val_acc:0.786]
Epoch [9/120    avg_loss:0.601, val_acc:0.741]
Epoch [10/120    avg_loss:0.516, val_acc:0.757]
Epoch [11/120    avg_loss:0.389, val_acc:0.834]
Epoch [12/120    avg_loss:0.373, val_acc:0.827]
Epoch [13/120    avg_loss:0.304, val_acc:0.902]
Epoch [14/120    avg_loss:0.259, val_acc:0.884]
Epoch [15/120    avg_loss:0.237, val_acc:0.871]
Epoch [16/120    avg_loss:0.272, val_acc:0.851]
Epoch [17/120    avg_loss:0.194, val_acc:0.927]
Epoch [18/120    avg_loss:0.178, val_acc:0.909]
Epoch [19/120    avg_loss:0.602, val_acc:0.509]
Epoch [20/120    avg_loss:0.768, val_acc:0.772]
Epoch [21/120    avg_loss:0.391, val_acc:0.894]
Epoch [22/120    avg_loss:0.254, val_acc:0.864]
Epoch [23/120    avg_loss:0.222, val_acc:0.895]
Epoch [24/120    avg_loss:0.196, val_acc:0.891]
Epoch [25/120    avg_loss:0.158, val_acc:0.944]
Epoch [26/120    avg_loss:0.131, val_acc:0.937]
Epoch [27/120    avg_loss:0.141, val_acc:0.933]
Epoch [28/120    avg_loss:0.100, val_acc:0.953]
Epoch [29/120    avg_loss:0.124, val_acc:0.935]
Epoch [30/120    avg_loss:0.116, val_acc:0.934]
Epoch [31/120    avg_loss:0.118, val_acc:0.937]
Epoch [32/120    avg_loss:0.099, val_acc:0.932]
Epoch [33/120    avg_loss:0.089, val_acc:0.967]
Epoch [34/120    avg_loss:0.066, val_acc:0.965]
Epoch [35/120    avg_loss:0.076, val_acc:0.965]
Epoch [36/120    avg_loss:0.091, val_acc:0.975]
Epoch [37/120    avg_loss:0.092, val_acc:0.975]
Epoch [38/120    avg_loss:0.073, val_acc:0.959]
Epoch [39/120    avg_loss:0.060, val_acc:0.976]
Epoch [40/120    avg_loss:0.416, val_acc:0.753]
Epoch [41/120    avg_loss:0.291, val_acc:0.958]
Epoch [42/120    avg_loss:0.114, val_acc:0.960]
Epoch [43/120    avg_loss:0.095, val_acc:0.962]
Epoch [44/120    avg_loss:0.069, val_acc:0.970]
Epoch [45/120    avg_loss:0.241, val_acc:0.884]
Epoch [46/120    avg_loss:0.182, val_acc:0.948]
Epoch [47/120    avg_loss:0.131, val_acc:0.965]
Epoch [48/120    avg_loss:0.178, val_acc:0.963]
Epoch [49/120    avg_loss:0.105, val_acc:0.954]
Epoch [50/120    avg_loss:0.088, val_acc:0.966]
Epoch [51/120    avg_loss:0.060, val_acc:0.949]
Epoch [52/120    avg_loss:0.043, val_acc:0.979]
Epoch [53/120    avg_loss:0.043, val_acc:0.950]
Epoch [54/120    avg_loss:0.047, val_acc:0.967]
Epoch [55/120    avg_loss:0.045, val_acc:0.959]
Epoch [56/120    avg_loss:0.055, val_acc:0.977]
Epoch [57/120    avg_loss:0.047, val_acc:0.975]
Epoch [58/120    avg_loss:0.116, val_acc:0.794]
Epoch [59/120    avg_loss:0.137, val_acc:0.965]
Epoch [60/120    avg_loss:0.077, val_acc:0.970]
Epoch [61/120    avg_loss:0.079, val_acc:0.970]
Epoch [62/120    avg_loss:0.047, val_acc:0.958]
Epoch [63/120    avg_loss:0.042, val_acc:0.978]
Epoch [64/120    avg_loss:0.040, val_acc:0.984]
Epoch [65/120    avg_loss:0.037, val_acc:0.983]
Epoch [66/120    avg_loss:0.137, val_acc:0.939]
Epoch [67/120    avg_loss:0.090, val_acc:0.950]
Epoch [68/120    avg_loss:0.056, val_acc:0.974]
Epoch [69/120    avg_loss:0.034, val_acc:0.960]
Epoch [70/120    avg_loss:0.033, val_acc:0.973]
Epoch [71/120    avg_loss:0.032, val_acc:0.973]
Epoch [72/120    avg_loss:0.032, val_acc:0.986]
Epoch [73/120    avg_loss:0.019, val_acc:0.986]
Epoch [74/120    avg_loss:0.025, val_acc:0.959]
Epoch [75/120    avg_loss:0.028, val_acc:0.970]
Epoch [76/120    avg_loss:0.024, val_acc:0.981]
Epoch [77/120    avg_loss:0.023, val_acc:0.984]
Epoch [78/120    avg_loss:0.022, val_acc:0.987]
Epoch [79/120    avg_loss:0.016, val_acc:0.966]
Epoch [80/120    avg_loss:0.023, val_acc:0.988]
Epoch [81/120    avg_loss:0.032, val_acc:0.975]
Epoch [82/120    avg_loss:0.035, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.981]
Epoch [84/120    avg_loss:0.029, val_acc:0.985]
Epoch [85/120    avg_loss:0.031, val_acc:0.986]
Epoch [86/120    avg_loss:0.015, val_acc:0.988]
Epoch [87/120    avg_loss:0.136, val_acc:0.922]
Epoch [88/120    avg_loss:0.076, val_acc:0.980]
Epoch [89/120    avg_loss:0.044, val_acc:0.976]
Epoch [90/120    avg_loss:0.031, val_acc:0.960]
Epoch [91/120    avg_loss:0.027, val_acc:0.983]
Epoch [92/120    avg_loss:0.024, val_acc:0.988]
Epoch [93/120    avg_loss:0.015, val_acc:0.986]
Epoch [94/120    avg_loss:0.017, val_acc:0.982]
Epoch [95/120    avg_loss:0.022, val_acc:0.974]
Epoch [96/120    avg_loss:0.020, val_acc:0.985]
Epoch [97/120    avg_loss:0.014, val_acc:0.988]
Epoch [98/120    avg_loss:0.034, val_acc:0.986]
Epoch [99/120    avg_loss:0.019, val_acc:0.975]
Epoch [100/120    avg_loss:0.013, val_acc:0.988]
Epoch [101/120    avg_loss:0.012, val_acc:0.990]
Epoch [102/120    avg_loss:0.014, val_acc:0.982]
Epoch [103/120    avg_loss:0.016, val_acc:0.991]
Epoch [104/120    avg_loss:0.028, val_acc:0.970]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.014, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.993]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.013, val_acc:0.992]
Epoch [115/120    avg_loss:0.021, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.993]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6231     0     8     1     0     7    47   131     7]
 [    0     0 18010     0    15     0    65     0     0     0]
 [    0     0     0  1882     0     0     0     0   154     0]
 [    0     1    11     0  2951     0     1     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4861     0    15     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     8     0     4    60     0     0     0  3499     0]
 [    0     0     0     0     4    10     0     0     0   905]]

Accuracy:
98.65278480707589

F1 scores:
[       nan 0.98342803 0.99747999 0.95727365 0.98317508 0.99618321
 0.99082756 0.98210887 0.94849553 0.98853086]

Kappa:
0.9821679316119644
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe71b0b3b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.153, val_acc:0.189]
Epoch [2/120    avg_loss:1.840, val_acc:0.318]
Epoch [3/120    avg_loss:1.596, val_acc:0.424]
Epoch [4/120    avg_loss:1.386, val_acc:0.654]
Epoch [5/120    avg_loss:1.207, val_acc:0.686]
Epoch [6/120    avg_loss:0.992, val_acc:0.730]
Epoch [7/120    avg_loss:0.793, val_acc:0.663]
Epoch [8/120    avg_loss:0.690, val_acc:0.729]
Epoch [9/120    avg_loss:0.581, val_acc:0.783]
Epoch [10/120    avg_loss:0.500, val_acc:0.754]
Epoch [11/120    avg_loss:0.464, val_acc:0.818]
Epoch [12/120    avg_loss:0.403, val_acc:0.842]
Epoch [13/120    avg_loss:0.423, val_acc:0.732]
Epoch [14/120    avg_loss:0.391, val_acc:0.853]
Epoch [15/120    avg_loss:0.322, val_acc:0.801]
Epoch [16/120    avg_loss:0.298, val_acc:0.839]
Epoch [17/120    avg_loss:0.263, val_acc:0.822]
Epoch [18/120    avg_loss:0.261, val_acc:0.824]
Epoch [19/120    avg_loss:0.223, val_acc:0.931]
Epoch [20/120    avg_loss:0.174, val_acc:0.933]
Epoch [21/120    avg_loss:0.208, val_acc:0.886]
Epoch [22/120    avg_loss:0.162, val_acc:0.900]
Epoch [23/120    avg_loss:0.151, val_acc:0.920]
Epoch [24/120    avg_loss:0.155, val_acc:0.914]
Epoch [25/120    avg_loss:0.153, val_acc:0.896]
Epoch [26/120    avg_loss:0.206, val_acc:0.920]
Epoch [27/120    avg_loss:0.152, val_acc:0.952]
Epoch [28/120    avg_loss:0.141, val_acc:0.937]
Epoch [29/120    avg_loss:0.111, val_acc:0.953]
Epoch [30/120    avg_loss:0.100, val_acc:0.958]
Epoch [31/120    avg_loss:0.107, val_acc:0.972]
Epoch [32/120    avg_loss:0.089, val_acc:0.966]
Epoch [33/120    avg_loss:0.072, val_acc:0.981]
Epoch [34/120    avg_loss:0.070, val_acc:0.964]
Epoch [35/120    avg_loss:0.063, val_acc:0.976]
Epoch [36/120    avg_loss:0.077, val_acc:0.975]
Epoch [37/120    avg_loss:0.058, val_acc:0.934]
Epoch [38/120    avg_loss:0.082, val_acc:0.969]
Epoch [39/120    avg_loss:0.053, val_acc:0.979]
Epoch [40/120    avg_loss:0.041, val_acc:0.978]
Epoch [41/120    avg_loss:0.071, val_acc:0.973]
Epoch [42/120    avg_loss:0.053, val_acc:0.982]
Epoch [43/120    avg_loss:0.044, val_acc:0.986]
Epoch [44/120    avg_loss:0.053, val_acc:0.981]
Epoch [45/120    avg_loss:0.033, val_acc:0.977]
Epoch [46/120    avg_loss:0.035, val_acc:0.971]
Epoch [47/120    avg_loss:0.073, val_acc:0.971]
Epoch [48/120    avg_loss:0.033, val_acc:0.983]
Epoch [49/120    avg_loss:0.023, val_acc:0.985]
Epoch [50/120    avg_loss:0.051, val_acc:0.985]
Epoch [51/120    avg_loss:0.039, val_acc:0.984]
Epoch [52/120    avg_loss:0.028, val_acc:0.987]
Epoch [53/120    avg_loss:0.025, val_acc:0.990]
Epoch [54/120    avg_loss:0.016, val_acc:0.991]
Epoch [55/120    avg_loss:0.017, val_acc:0.988]
Epoch [56/120    avg_loss:0.014, val_acc:0.992]
Epoch [57/120    avg_loss:0.046, val_acc:0.965]
Epoch [58/120    avg_loss:0.170, val_acc:0.934]
Epoch [59/120    avg_loss:0.051, val_acc:0.978]
Epoch [60/120    avg_loss:0.054, val_acc:0.982]
Epoch [61/120    avg_loss:0.022, val_acc:0.985]
Epoch [62/120    avg_loss:0.019, val_acc:0.985]
Epoch [63/120    avg_loss:0.031, val_acc:0.988]
Epoch [64/120    avg_loss:0.026, val_acc:0.980]
Epoch [65/120    avg_loss:0.029, val_acc:0.980]
Epoch [66/120    avg_loss:0.019, val_acc:0.991]
Epoch [67/120    avg_loss:0.011, val_acc:0.991]
Epoch [68/120    avg_loss:0.013, val_acc:0.991]
Epoch [69/120    avg_loss:0.013, val_acc:0.995]
Epoch [70/120    avg_loss:0.009, val_acc:0.994]
Epoch [71/120    avg_loss:0.011, val_acc:0.995]
Epoch [72/120    avg_loss:0.008, val_acc:0.997]
Epoch [73/120    avg_loss:0.036, val_acc:0.986]
Epoch [74/120    avg_loss:0.021, val_acc:0.980]
Epoch [75/120    avg_loss:0.026, val_acc:0.951]
Epoch [76/120    avg_loss:0.014, val_acc:0.983]
Epoch [77/120    avg_loss:0.080, val_acc:0.961]
Epoch [78/120    avg_loss:0.023, val_acc:0.979]
Epoch [79/120    avg_loss:0.025, val_acc:0.939]
Epoch [80/120    avg_loss:0.016, val_acc:0.991]
Epoch [81/120    avg_loss:0.014, val_acc:0.989]
Epoch [82/120    avg_loss:0.017, val_acc:0.990]
Epoch [83/120    avg_loss:0.032, val_acc:0.955]
Epoch [84/120    avg_loss:0.028, val_acc:0.990]
Epoch [85/120    avg_loss:0.013, val_acc:0.995]
Epoch [86/120    avg_loss:0.010, val_acc:0.994]
Epoch [87/120    avg_loss:0.010, val_acc:0.993]
Epoch [88/120    avg_loss:0.009, val_acc:0.994]
Epoch [89/120    avg_loss:0.009, val_acc:0.995]
Epoch [90/120    avg_loss:0.008, val_acc:0.995]
Epoch [91/120    avg_loss:0.009, val_acc:0.995]
Epoch [92/120    avg_loss:0.007, val_acc:0.995]
Epoch [93/120    avg_loss:0.006, val_acc:0.995]
Epoch [94/120    avg_loss:0.012, val_acc:0.993]
Epoch [95/120    avg_loss:0.007, val_acc:0.994]
Epoch [96/120    avg_loss:0.006, val_acc:0.994]
Epoch [97/120    avg_loss:0.008, val_acc:0.996]
Epoch [98/120    avg_loss:0.007, val_acc:0.996]
Epoch [99/120    avg_loss:0.007, val_acc:0.996]
Epoch [100/120    avg_loss:0.007, val_acc:0.996]
Epoch [101/120    avg_loss:0.006, val_acc:0.996]
Epoch [102/120    avg_loss:0.008, val_acc:0.996]
Epoch [103/120    avg_loss:0.006, val_acc:0.996]
Epoch [104/120    avg_loss:0.008, val_acc:0.996]
Epoch [105/120    avg_loss:0.006, val_acc:0.996]
Epoch [106/120    avg_loss:0.007, val_acc:0.996]
Epoch [107/120    avg_loss:0.007, val_acc:0.996]
Epoch [108/120    avg_loss:0.008, val_acc:0.996]
Epoch [109/120    avg_loss:0.007, val_acc:0.996]
Epoch [110/120    avg_loss:0.007, val_acc:0.996]
Epoch [111/120    avg_loss:0.007, val_acc:0.996]
Epoch [112/120    avg_loss:0.007, val_acc:0.996]
Epoch [113/120    avg_loss:0.007, val_acc:0.996]
Epoch [114/120    avg_loss:0.005, val_acc:0.996]
Epoch [115/120    avg_loss:0.007, val_acc:0.996]
Epoch [116/120    avg_loss:0.008, val_acc:0.996]
Epoch [117/120    avg_loss:0.006, val_acc:0.996]
Epoch [118/120    avg_loss:0.007, val_acc:0.996]
Epoch [119/120    avg_loss:0.008, val_acc:0.996]
Epoch [120/120    avg_loss:0.008, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6404     0     1     0     0     0    26     1     0]
 [    0     0 18072     0    17     0     1     0     0     0]
 [    0     1     0  2015     0     0     0     0    17     3]
 [    0    22     2     0  2924     0    15     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    12     0     0  4862     0     3     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     1     0     3    55     0     0     0  3510     2]
 [    0     0     0     0    11    22     0     0     0   886]]

Accuracy:
99.45773985973538

F1 scores:
[       nan 0.99595645 0.99941933 0.99090239 0.97808998 0.99164134
 0.99671997 0.99002302 0.98734177 0.97846494]

Kappa:
0.9928162740788032
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0381809c50>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.147, val_acc:0.147]
Epoch [2/120    avg_loss:1.837, val_acc:0.264]
Epoch [3/120    avg_loss:1.586, val_acc:0.308]
Epoch [4/120    avg_loss:1.339, val_acc:0.390]
Epoch [5/120    avg_loss:1.185, val_acc:0.425]
Epoch [6/120    avg_loss:0.987, val_acc:0.456]
Epoch [7/120    avg_loss:0.875, val_acc:0.616]
Epoch [8/120    avg_loss:0.787, val_acc:0.710]
Epoch [9/120    avg_loss:0.650, val_acc:0.707]
Epoch [10/120    avg_loss:0.561, val_acc:0.766]
Epoch [11/120    avg_loss:0.486, val_acc:0.771]
Epoch [12/120    avg_loss:0.423, val_acc:0.830]
Epoch [13/120    avg_loss:0.381, val_acc:0.804]
Epoch [14/120    avg_loss:0.352, val_acc:0.821]
Epoch [15/120    avg_loss:0.326, val_acc:0.818]
Epoch [16/120    avg_loss:0.302, val_acc:0.864]
Epoch [17/120    avg_loss:0.265, val_acc:0.840]
Epoch [18/120    avg_loss:0.211, val_acc:0.880]
Epoch [19/120    avg_loss:0.214, val_acc:0.875]
Epoch [20/120    avg_loss:0.214, val_acc:0.867]
Epoch [21/120    avg_loss:0.205, val_acc:0.892]
Epoch [22/120    avg_loss:0.172, val_acc:0.916]
Epoch [23/120    avg_loss:0.438, val_acc:0.806]
Epoch [24/120    avg_loss:0.284, val_acc:0.912]
Epoch [25/120    avg_loss:0.216, val_acc:0.840]
Epoch [26/120    avg_loss:0.133, val_acc:0.942]
Epoch [27/120    avg_loss:0.138, val_acc:0.927]
Epoch [28/120    avg_loss:0.131, val_acc:0.938]
Epoch [29/120    avg_loss:0.122, val_acc:0.947]
Epoch [30/120    avg_loss:0.082, val_acc:0.961]
Epoch [31/120    avg_loss:0.080, val_acc:0.955]
Epoch [32/120    avg_loss:0.077, val_acc:0.970]
Epoch [33/120    avg_loss:0.053, val_acc:0.968]
Epoch [34/120    avg_loss:0.057, val_acc:0.967]
Epoch [35/120    avg_loss:0.040, val_acc:0.963]
Epoch [36/120    avg_loss:0.057, val_acc:0.963]
Epoch [37/120    avg_loss:0.056, val_acc:0.979]
Epoch [38/120    avg_loss:0.055, val_acc:0.985]
Epoch [39/120    avg_loss:0.052, val_acc:0.970]
Epoch [40/120    avg_loss:0.053, val_acc:0.976]
Epoch [41/120    avg_loss:0.066, val_acc:0.976]
Epoch [42/120    avg_loss:0.045, val_acc:0.973]
Epoch [43/120    avg_loss:0.045, val_acc:0.960]
Epoch [44/120    avg_loss:0.037, val_acc:0.977]
Epoch [45/120    avg_loss:0.024, val_acc:0.984]
Epoch [46/120    avg_loss:0.028, val_acc:0.979]
Epoch [47/120    avg_loss:0.023, val_acc:0.980]
Epoch [48/120    avg_loss:0.029, val_acc:0.970]
Epoch [49/120    avg_loss:0.029, val_acc:0.968]
Epoch [50/120    avg_loss:0.019, val_acc:0.982]
Epoch [51/120    avg_loss:0.016, val_acc:0.980]
Epoch [52/120    avg_loss:0.015, val_acc:0.985]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.013, val_acc:0.983]
Epoch [55/120    avg_loss:0.014, val_acc:0.986]
Epoch [56/120    avg_loss:0.015, val_acc:0.985]
Epoch [57/120    avg_loss:0.012, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.011, val_acc:0.985]
Epoch [61/120    avg_loss:0.012, val_acc:0.984]
Epoch [62/120    avg_loss:0.010, val_acc:0.986]
Epoch [63/120    avg_loss:0.012, val_acc:0.985]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.012, val_acc:0.985]
Epoch [66/120    avg_loss:0.010, val_acc:0.986]
Epoch [67/120    avg_loss:0.012, val_acc:0.985]
Epoch [68/120    avg_loss:0.012, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.983]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.012, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.982]
Epoch [81/120    avg_loss:0.010, val_acc:0.982]
Epoch [82/120    avg_loss:0.011, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.012, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.013, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.984]
Epoch [100/120    avg_loss:0.011, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.984]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.011, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0     0     2    48     0]
 [    0     0 18055     0    30     0     4     0     1     0]
 [    0     2     0  2002     0     0     0     0    31     1]
 [    0    27    14     0  2905     0    10     0    14     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     3     0     0  4870     0     1     0]
 [    0     0     0     0     0     0     2  1251     0    37]
 [    0     5     0    25    63     0     0     0  3478     0]
 [    0     0     0     0     5    43     0     0     0   871]]

Accuracy:
99.0986431446268

F1 scores:
[       nan 0.99346202 0.99853441 0.9847516  0.97238494 0.98379193
 0.99754199 0.98387731 0.97368421 0.95191257]

Kappa:
0.988059081033816
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe99559cac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.138, val_acc:0.079]
Epoch [2/120    avg_loss:1.892, val_acc:0.299]
Epoch [3/120    avg_loss:1.679, val_acc:0.431]
Epoch [4/120    avg_loss:1.536, val_acc:0.613]
Epoch [5/120    avg_loss:1.353, val_acc:0.645]
Epoch [6/120    avg_loss:1.207, val_acc:0.663]
Epoch [7/120    avg_loss:1.084, val_acc:0.715]
Epoch [8/120    avg_loss:0.876, val_acc:0.716]
Epoch [9/120    avg_loss:0.747, val_acc:0.689]
Epoch [10/120    avg_loss:0.648, val_acc:0.774]
Epoch [11/120    avg_loss:0.497, val_acc:0.824]
Epoch [12/120    avg_loss:0.702, val_acc:0.490]
Epoch [13/120    avg_loss:1.246, val_acc:0.505]
Epoch [14/120    avg_loss:1.061, val_acc:0.561]
Epoch [15/120    avg_loss:0.988, val_acc:0.594]
Epoch [16/120    avg_loss:0.895, val_acc:0.637]
Epoch [17/120    avg_loss:0.797, val_acc:0.729]
Epoch [18/120    avg_loss:0.735, val_acc:0.737]
Epoch [19/120    avg_loss:0.696, val_acc:0.728]
Epoch [20/120    avg_loss:0.718, val_acc:0.754]
Epoch [21/120    avg_loss:0.620, val_acc:0.787]
Epoch [22/120    avg_loss:0.569, val_acc:0.764]
Epoch [23/120    avg_loss:0.532, val_acc:0.809]
Epoch [24/120    avg_loss:0.511, val_acc:0.803]
Epoch [25/120    avg_loss:0.465, val_acc:0.819]
Epoch [26/120    avg_loss:0.408, val_acc:0.812]
Epoch [27/120    avg_loss:0.445, val_acc:0.817]
Epoch [28/120    avg_loss:0.423, val_acc:0.826]
Epoch [29/120    avg_loss:0.417, val_acc:0.832]
Epoch [30/120    avg_loss:0.401, val_acc:0.815]
Epoch [31/120    avg_loss:0.391, val_acc:0.826]
Epoch [32/120    avg_loss:0.392, val_acc:0.834]
Epoch [33/120    avg_loss:0.406, val_acc:0.845]
Epoch [34/120    avg_loss:0.383, val_acc:0.839]
Epoch [35/120    avg_loss:0.389, val_acc:0.822]
Epoch [36/120    avg_loss:0.405, val_acc:0.841]
Epoch [37/120    avg_loss:0.381, val_acc:0.850]
Epoch [38/120    avg_loss:0.382, val_acc:0.848]
Epoch [39/120    avg_loss:0.382, val_acc:0.856]
Epoch [40/120    avg_loss:0.380, val_acc:0.845]
Epoch [41/120    avg_loss:0.365, val_acc:0.851]
Epoch [42/120    avg_loss:0.374, val_acc:0.845]
Epoch [43/120    avg_loss:0.378, val_acc:0.793]
Epoch [44/120    avg_loss:0.381, val_acc:0.846]
Epoch [45/120    avg_loss:0.357, val_acc:0.839]
Epoch [46/120    avg_loss:0.366, val_acc:0.853]
Epoch [47/120    avg_loss:0.371, val_acc:0.833]
Epoch [48/120    avg_loss:0.378, val_acc:0.832]
Epoch [49/120    avg_loss:0.360, val_acc:0.863]
Epoch [50/120    avg_loss:0.355, val_acc:0.865]
Epoch [51/120    avg_loss:0.358, val_acc:0.878]
Epoch [52/120    avg_loss:0.338, val_acc:0.857]
Epoch [53/120    avg_loss:0.351, val_acc:0.883]
Epoch [54/120    avg_loss:0.348, val_acc:0.865]
Epoch [55/120    avg_loss:0.349, val_acc:0.866]
Epoch [56/120    avg_loss:0.333, val_acc:0.876]
Epoch [57/120    avg_loss:0.341, val_acc:0.876]
Epoch [58/120    avg_loss:0.335, val_acc:0.881]
Epoch [59/120    avg_loss:0.319, val_acc:0.857]
Epoch [60/120    avg_loss:0.312, val_acc:0.872]
Epoch [61/120    avg_loss:0.334, val_acc:0.866]
Epoch [62/120    avg_loss:0.324, val_acc:0.883]
Epoch [63/120    avg_loss:0.354, val_acc:0.872]
Epoch [64/120    avg_loss:0.312, val_acc:0.885]
Epoch [65/120    avg_loss:0.311, val_acc:0.884]
Epoch [66/120    avg_loss:0.331, val_acc:0.885]
Epoch [67/120    avg_loss:0.313, val_acc:0.864]
Epoch [68/120    avg_loss:0.303, val_acc:0.885]
Epoch [69/120    avg_loss:0.307, val_acc:0.898]
Epoch [70/120    avg_loss:0.309, val_acc:0.902]
Epoch [71/120    avg_loss:0.293, val_acc:0.899]
Epoch [72/120    avg_loss:0.289, val_acc:0.891]
Epoch [73/120    avg_loss:0.288, val_acc:0.893]
Epoch [74/120    avg_loss:0.291, val_acc:0.878]
Epoch [75/120    avg_loss:0.292, val_acc:0.893]
Epoch [76/120    avg_loss:0.309, val_acc:0.887]
Epoch [77/120    avg_loss:0.281, val_acc:0.887]
Epoch [78/120    avg_loss:0.293, val_acc:0.901]
Epoch [79/120    avg_loss:0.282, val_acc:0.907]
Epoch [80/120    avg_loss:0.280, val_acc:0.889]
Epoch [81/120    avg_loss:0.279, val_acc:0.887]
Epoch [82/120    avg_loss:0.279, val_acc:0.896]
Epoch [83/120    avg_loss:0.290, val_acc:0.885]
Epoch [84/120    avg_loss:0.270, val_acc:0.906]
Epoch [85/120    avg_loss:0.263, val_acc:0.914]
Epoch [86/120    avg_loss:0.259, val_acc:0.896]
Epoch [87/120    avg_loss:0.262, val_acc:0.895]
Epoch [88/120    avg_loss:0.267, val_acc:0.889]
Epoch [89/120    avg_loss:0.262, val_acc:0.907]
Epoch [90/120    avg_loss:0.267, val_acc:0.905]
Epoch [91/120    avg_loss:0.252, val_acc:0.911]
Epoch [92/120    avg_loss:0.283, val_acc:0.911]
Epoch [93/120    avg_loss:0.246, val_acc:0.900]
Epoch [94/120    avg_loss:0.261, val_acc:0.908]
Epoch [95/120    avg_loss:0.260, val_acc:0.914]
Epoch [96/120    avg_loss:0.268, val_acc:0.884]
Epoch [97/120    avg_loss:0.249, val_acc:0.903]
Epoch [98/120    avg_loss:0.248, val_acc:0.923]
Epoch [99/120    avg_loss:0.230, val_acc:0.907]
Epoch [100/120    avg_loss:0.265, val_acc:0.902]
Epoch [101/120    avg_loss:0.253, val_acc:0.918]
Epoch [102/120    avg_loss:0.235, val_acc:0.926]
Epoch [103/120    avg_loss:0.230, val_acc:0.934]
Epoch [104/120    avg_loss:0.262, val_acc:0.924]
Epoch [105/120    avg_loss:0.261, val_acc:0.900]
Epoch [106/120    avg_loss:0.247, val_acc:0.914]
Epoch [107/120    avg_loss:0.237, val_acc:0.924]
Epoch [108/120    avg_loss:0.230, val_acc:0.903]
Epoch [109/120    avg_loss:0.223, val_acc:0.933]
Epoch [110/120    avg_loss:0.219, val_acc:0.911]
Epoch [111/120    avg_loss:0.234, val_acc:0.897]
Epoch [112/120    avg_loss:0.209, val_acc:0.924]
Epoch [113/120    avg_loss:0.232, val_acc:0.922]
Epoch [114/120    avg_loss:0.210, val_acc:0.921]
Epoch [115/120    avg_loss:0.217, val_acc:0.931]
Epoch [116/120    avg_loss:0.212, val_acc:0.919]
Epoch [117/120    avg_loss:0.215, val_acc:0.933]
Epoch [118/120    avg_loss:0.205, val_acc:0.939]
Epoch [119/120    avg_loss:0.207, val_acc:0.940]
Epoch [120/120    avg_loss:0.199, val_acc:0.940]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5661     0     1   270     0     0    37   397    66]
 [    0     0 17687     0   280     0   123     0     0     0]
 [    0    20     0  1840     0     0     0     0   110    66]
 [    0   174    56     0  2656     0    15     0    69     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     1     3     0  4866     0     6     0]
 [    0     0     0     0     0     0     2  1259     3    26]
 [    0    48     0     0    63     0     0     0  3460     0]
 [    0    24     0    14    16   129     0     0     1   735]]

Accuracy:
95.12206878268624

F1 scores:
[       nan 0.91609354 0.98713548 0.94552929 0.8485623  0.95290252
 0.98462161 0.97370456 0.90849416 0.81036384]

Kappa:
0.9356833420856863
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89ff530470>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.181, val_acc:0.082]
Epoch [2/120    avg_loss:1.885, val_acc:0.149]
Epoch [3/120    avg_loss:1.669, val_acc:0.231]
Epoch [4/120    avg_loss:1.491, val_acc:0.345]
Epoch [5/120    avg_loss:1.372, val_acc:0.409]
Epoch [6/120    avg_loss:1.164, val_acc:0.455]
Epoch [7/120    avg_loss:1.013, val_acc:0.517]
Epoch [8/120    avg_loss:0.816, val_acc:0.602]
Epoch [9/120    avg_loss:0.660, val_acc:0.714]
Epoch [10/120    avg_loss:0.555, val_acc:0.780]
Epoch [11/120    avg_loss:0.451, val_acc:0.841]
Epoch [12/120    avg_loss:0.421, val_acc:0.797]
Epoch [13/120    avg_loss:0.432, val_acc:0.883]
Epoch [14/120    avg_loss:0.314, val_acc:0.902]
Epoch [15/120    avg_loss:0.313, val_acc:0.907]
Epoch [16/120    avg_loss:0.255, val_acc:0.919]
Epoch [17/120    avg_loss:0.679, val_acc:0.681]
Epoch [18/120    avg_loss:0.502, val_acc:0.897]
Epoch [19/120    avg_loss:0.287, val_acc:0.921]
Epoch [20/120    avg_loss:0.228, val_acc:0.910]
Epoch [21/120    avg_loss:0.167, val_acc:0.931]
Epoch [22/120    avg_loss:0.183, val_acc:0.940]
Epoch [23/120    avg_loss:0.165, val_acc:0.942]
Epoch [24/120    avg_loss:0.158, val_acc:0.946]
Epoch [25/120    avg_loss:0.117, val_acc:0.944]
Epoch [26/120    avg_loss:0.101, val_acc:0.965]
Epoch [27/120    avg_loss:0.108, val_acc:0.949]
Epoch [28/120    avg_loss:0.518, val_acc:0.795]
Epoch [29/120    avg_loss:0.279, val_acc:0.911]
Epoch [30/120    avg_loss:0.175, val_acc:0.919]
Epoch [31/120    avg_loss:0.125, val_acc:0.951]
Epoch [32/120    avg_loss:0.406, val_acc:0.887]
Epoch [33/120    avg_loss:0.206, val_acc:0.946]
Epoch [34/120    avg_loss:0.124, val_acc:0.967]
Epoch [35/120    avg_loss:0.086, val_acc:0.943]
Epoch [36/120    avg_loss:0.065, val_acc:0.975]
Epoch [37/120    avg_loss:0.066, val_acc:0.973]
Epoch [38/120    avg_loss:0.072, val_acc:0.964]
Epoch [39/120    avg_loss:0.057, val_acc:0.971]
Epoch [40/120    avg_loss:0.060, val_acc:0.960]
Epoch [41/120    avg_loss:0.066, val_acc:0.957]
Epoch [42/120    avg_loss:0.092, val_acc:0.968]
Epoch [43/120    avg_loss:0.041, val_acc:0.978]
Epoch [44/120    avg_loss:0.041, val_acc:0.981]
Epoch [45/120    avg_loss:0.036, val_acc:0.977]
Epoch [46/120    avg_loss:0.028, val_acc:0.970]
Epoch [47/120    avg_loss:0.025, val_acc:0.983]
Epoch [48/120    avg_loss:0.024, val_acc:0.984]
Epoch [49/120    avg_loss:0.054, val_acc:0.977]
Epoch [50/120    avg_loss:0.059, val_acc:0.964]
Epoch [51/120    avg_loss:0.034, val_acc:0.984]
Epoch [52/120    avg_loss:0.046, val_acc:0.973]
Epoch [53/120    avg_loss:0.052, val_acc:0.968]
Epoch [54/120    avg_loss:0.057, val_acc:0.973]
Epoch [55/120    avg_loss:0.050, val_acc:0.965]
Epoch [56/120    avg_loss:0.027, val_acc:0.978]
Epoch [57/120    avg_loss:0.017, val_acc:0.981]
Epoch [58/120    avg_loss:0.025, val_acc:0.981]
Epoch [59/120    avg_loss:0.017, val_acc:0.987]
Epoch [60/120    avg_loss:0.015, val_acc:0.984]
Epoch [61/120    avg_loss:0.020, val_acc:0.984]
Epoch [62/120    avg_loss:0.019, val_acc:0.975]
Epoch [63/120    avg_loss:0.034, val_acc:0.976]
Epoch [64/120    avg_loss:0.037, val_acc:0.974]
Epoch [65/120    avg_loss:0.019, val_acc:0.983]
Epoch [66/120    avg_loss:0.024, val_acc:0.986]
Epoch [67/120    avg_loss:0.022, val_acc:0.986]
Epoch [68/120    avg_loss:0.015, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.977]
Epoch [70/120    avg_loss:0.013, val_acc:0.979]
Epoch [71/120    avg_loss:0.018, val_acc:0.982]
Epoch [72/120    avg_loss:0.045, val_acc:0.985]
Epoch [73/120    avg_loss:0.021, val_acc:0.988]
Epoch [74/120    avg_loss:0.019, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.988]
Epoch [76/120    avg_loss:0.020, val_acc:0.985]
Epoch [77/120    avg_loss:0.013, val_acc:0.987]
Epoch [78/120    avg_loss:0.015, val_acc:0.987]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.987]
Epoch [86/120    avg_loss:0.011, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.011, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     0     0    16     7    11     1]
 [    0     0 18059     0    21     0    10     0     0     0]
 [    0     1     0  1994     0     0     0     0    38     3]
 [    0    32    11     0  2918     0     3     0     8     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0    22     5     0     0  4850     0     1     0]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     3     0    59    55     0     0     0  3454     0]
 [    0     0     0     0    14    22     0     0     0   883]]

Accuracy:
99.16130431639071

F1 scores:
[       nan 0.99448115 0.99823116 0.97410845 0.97591973 0.99125808
 0.99375064 0.99574139 0.97529295 0.97731046]

Kappa:
0.9888866825003744
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cb2a0eb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.141, val_acc:0.077]
Epoch [2/120    avg_loss:1.874, val_acc:0.161]
Epoch [3/120    avg_loss:1.651, val_acc:0.189]
Epoch [4/120    avg_loss:1.457, val_acc:0.289]
Epoch [5/120    avg_loss:1.263, val_acc:0.369]
Epoch [6/120    avg_loss:1.150, val_acc:0.443]
Epoch [7/120    avg_loss:0.963, val_acc:0.470]
Epoch [8/120    avg_loss:0.860, val_acc:0.532]
Epoch [9/120    avg_loss:0.815, val_acc:0.556]
Epoch [10/120    avg_loss:0.655, val_acc:0.611]
Epoch [11/120    avg_loss:0.569, val_acc:0.686]
Epoch [12/120    avg_loss:0.555, val_acc:0.744]
Epoch [13/120    avg_loss:0.490, val_acc:0.720]
Epoch [14/120    avg_loss:0.438, val_acc:0.790]
Epoch [15/120    avg_loss:0.356, val_acc:0.812]
Epoch [16/120    avg_loss:0.310, val_acc:0.831]
Epoch [17/120    avg_loss:0.315, val_acc:0.869]
Epoch [18/120    avg_loss:0.269, val_acc:0.904]
Epoch [19/120    avg_loss:0.263, val_acc:0.850]
Epoch [20/120    avg_loss:0.237, val_acc:0.902]
Epoch [21/120    avg_loss:0.222, val_acc:0.894]
Epoch [22/120    avg_loss:0.219, val_acc:0.931]
Epoch [23/120    avg_loss:0.246, val_acc:0.907]
Epoch [24/120    avg_loss:0.194, val_acc:0.918]
Epoch [25/120    avg_loss:0.151, val_acc:0.930]
Epoch [26/120    avg_loss:0.122, val_acc:0.946]
Epoch [27/120    avg_loss:0.119, val_acc:0.934]
Epoch [28/120    avg_loss:0.126, val_acc:0.951]
Epoch [29/120    avg_loss:0.095, val_acc:0.945]
Epoch [30/120    avg_loss:0.087, val_acc:0.949]
Epoch [31/120    avg_loss:0.097, val_acc:0.958]
Epoch [32/120    avg_loss:0.100, val_acc:0.961]
Epoch [33/120    avg_loss:0.070, val_acc:0.964]
Epoch [34/120    avg_loss:0.081, val_acc:0.943]
Epoch [35/120    avg_loss:0.068, val_acc:0.953]
Epoch [36/120    avg_loss:0.053, val_acc:0.975]
Epoch [37/120    avg_loss:0.049, val_acc:0.974]
Epoch [38/120    avg_loss:0.042, val_acc:0.977]
Epoch [39/120    avg_loss:0.053, val_acc:0.968]
Epoch [40/120    avg_loss:0.073, val_acc:0.970]
Epoch [41/120    avg_loss:0.049, val_acc:0.965]
Epoch [42/120    avg_loss:0.041, val_acc:0.974]
Epoch [43/120    avg_loss:0.037, val_acc:0.975]
Epoch [44/120    avg_loss:0.067, val_acc:0.952]
Epoch [45/120    avg_loss:0.110, val_acc:0.959]
Epoch [46/120    avg_loss:0.041, val_acc:0.964]
Epoch [47/120    avg_loss:0.097, val_acc:0.970]
Epoch [48/120    avg_loss:0.043, val_acc:0.967]
Epoch [49/120    avg_loss:0.033, val_acc:0.973]
Epoch [50/120    avg_loss:0.023, val_acc:0.974]
Epoch [51/120    avg_loss:0.023, val_acc:0.980]
Epoch [52/120    avg_loss:0.033, val_acc:0.977]
Epoch [53/120    avg_loss:0.027, val_acc:0.977]
Epoch [54/120    avg_loss:0.056, val_acc:0.963]
Epoch [55/120    avg_loss:0.050, val_acc:0.978]
Epoch [56/120    avg_loss:0.054, val_acc:0.978]
Epoch [57/120    avg_loss:0.038, val_acc:0.962]
Epoch [58/120    avg_loss:0.030, val_acc:0.979]
Epoch [59/120    avg_loss:0.031, val_acc:0.957]
Epoch [60/120    avg_loss:0.025, val_acc:0.987]
Epoch [61/120    avg_loss:0.025, val_acc:0.960]
Epoch [62/120    avg_loss:0.037, val_acc:0.977]
Epoch [63/120    avg_loss:0.028, val_acc:0.978]
Epoch [64/120    avg_loss:0.036, val_acc:0.967]
Epoch [65/120    avg_loss:0.041, val_acc:0.977]
Epoch [66/120    avg_loss:0.067, val_acc:0.972]
Epoch [67/120    avg_loss:0.053, val_acc:0.972]
Epoch [68/120    avg_loss:0.038, val_acc:0.977]
Epoch [69/120    avg_loss:0.016, val_acc:0.983]
Epoch [70/120    avg_loss:0.022, val_acc:0.972]
Epoch [71/120    avg_loss:0.030, val_acc:0.972]
Epoch [72/120    avg_loss:0.016, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.980]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     3    25     8]
 [    0     0 18009     0    78     0     3     0     0     0]
 [    0     0     0  2025     3     0     0     0     8     0]
 [    0    45    15     0  2891     0     0     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4868     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    26     0    12    51     0     0     0  3482     0]
 [    0     0     0     1    14    53     0     1     0   850]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99170478 0.99706566 0.99410898 0.96222333 0.98009763
 0.99866653 0.99845201 0.97987899 0.95666854]

Kappa:
0.9879674480571335
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb2e596ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.155, val_acc:0.092]
Epoch [2/120    avg_loss:1.900, val_acc:0.174]
Epoch [3/120    avg_loss:1.680, val_acc:0.303]
Epoch [4/120    avg_loss:1.474, val_acc:0.343]
Epoch [5/120    avg_loss:1.287, val_acc:0.376]
Epoch [6/120    avg_loss:1.174, val_acc:0.535]
Epoch [7/120    avg_loss:0.994, val_acc:0.664]
Epoch [8/120    avg_loss:0.907, val_acc:0.638]
Epoch [9/120    avg_loss:0.734, val_acc:0.661]
Epoch [10/120    avg_loss:0.649, val_acc:0.693]
Epoch [11/120    avg_loss:0.571, val_acc:0.729]
Epoch [12/120    avg_loss:0.530, val_acc:0.750]
Epoch [13/120    avg_loss:0.811, val_acc:0.715]
Epoch [14/120    avg_loss:0.681, val_acc:0.615]
Epoch [15/120    avg_loss:0.548, val_acc:0.696]
Epoch [16/120    avg_loss:0.506, val_acc:0.666]
Epoch [17/120    avg_loss:0.448, val_acc:0.749]
Epoch [18/120    avg_loss:0.392, val_acc:0.803]
Epoch [19/120    avg_loss:0.344, val_acc:0.812]
Epoch [20/120    avg_loss:0.299, val_acc:0.823]
Epoch [21/120    avg_loss:0.428, val_acc:0.766]
Epoch [22/120    avg_loss:0.307, val_acc:0.799]
Epoch [23/120    avg_loss:0.317, val_acc:0.838]
Epoch [24/120    avg_loss:0.264, val_acc:0.822]
Epoch [25/120    avg_loss:0.213, val_acc:0.898]
Epoch [26/120    avg_loss:0.176, val_acc:0.920]
Epoch [27/120    avg_loss:0.183, val_acc:0.939]
Epoch [28/120    avg_loss:0.150, val_acc:0.947]
Epoch [29/120    avg_loss:0.132, val_acc:0.955]
Epoch [30/120    avg_loss:0.155, val_acc:0.957]
Epoch [31/120    avg_loss:0.172, val_acc:0.938]
Epoch [32/120    avg_loss:0.160, val_acc:0.954]
Epoch [33/120    avg_loss:0.100, val_acc:0.950]
Epoch [34/120    avg_loss:0.122, val_acc:0.938]
Epoch [35/120    avg_loss:0.092, val_acc:0.957]
Epoch [36/120    avg_loss:0.089, val_acc:0.958]
Epoch [37/120    avg_loss:0.110, val_acc:0.708]
Epoch [38/120    avg_loss:0.231, val_acc:0.951]
Epoch [39/120    avg_loss:0.190, val_acc:0.931]
Epoch [40/120    avg_loss:0.111, val_acc:0.958]
Epoch [41/120    avg_loss:0.068, val_acc:0.972]
Epoch [42/120    avg_loss:0.056, val_acc:0.971]
Epoch [43/120    avg_loss:0.044, val_acc:0.972]
Epoch [44/120    avg_loss:0.051, val_acc:0.977]
Epoch [45/120    avg_loss:0.043, val_acc:0.978]
Epoch [46/120    avg_loss:0.043, val_acc:0.970]
Epoch [47/120    avg_loss:0.054, val_acc:0.978]
Epoch [48/120    avg_loss:0.056, val_acc:0.968]
Epoch [49/120    avg_loss:0.062, val_acc:0.970]
Epoch [50/120    avg_loss:0.038, val_acc:0.979]
Epoch [51/120    avg_loss:0.036, val_acc:0.977]
Epoch [52/120    avg_loss:0.039, val_acc:0.956]
Epoch [53/120    avg_loss:0.040, val_acc:0.977]
Epoch [54/120    avg_loss:0.034, val_acc:0.972]
Epoch [55/120    avg_loss:0.021, val_acc:0.982]
Epoch [56/120    avg_loss:0.036, val_acc:0.980]
Epoch [57/120    avg_loss:0.027, val_acc:0.977]
Epoch [58/120    avg_loss:0.038, val_acc:0.981]
Epoch [59/120    avg_loss:0.034, val_acc:0.983]
Epoch [60/120    avg_loss:0.021, val_acc:0.984]
Epoch [61/120    avg_loss:0.018, val_acc:0.985]
Epoch [62/120    avg_loss:0.020, val_acc:0.978]
Epoch [63/120    avg_loss:0.028, val_acc:0.977]
Epoch [64/120    avg_loss:0.021, val_acc:0.978]
Epoch [65/120    avg_loss:0.015, val_acc:0.984]
Epoch [66/120    avg_loss:0.021, val_acc:0.971]
Epoch [67/120    avg_loss:0.023, val_acc:0.981]
Epoch [68/120    avg_loss:0.028, val_acc:0.979]
Epoch [69/120    avg_loss:0.021, val_acc:0.983]
Epoch [70/120    avg_loss:0.021, val_acc:0.989]
Epoch [71/120    avg_loss:0.023, val_acc:0.977]
Epoch [72/120    avg_loss:0.023, val_acc:0.984]
Epoch [73/120    avg_loss:0.021, val_acc:0.982]
Epoch [74/120    avg_loss:0.016, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.981]
Epoch [76/120    avg_loss:0.014, val_acc:0.981]
Epoch [77/120    avg_loss:0.037, val_acc:0.975]
Epoch [78/120    avg_loss:0.021, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.026, val_acc:0.974]
Epoch [83/120    avg_loss:0.023, val_acc:0.977]
Epoch [84/120    avg_loss:0.020, val_acc:0.984]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.010, val_acc:0.985]
Epoch [94/120    avg_loss:0.014, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.987]
Epoch [96/120    avg_loss:0.012, val_acc:0.987]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.011, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.014, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.012, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.012, val_acc:0.987]
Epoch [106/120    avg_loss:0.011, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.010, val_acc:0.987]
Epoch [110/120    avg_loss:0.013, val_acc:0.987]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.013, val_acc:0.987]
Epoch [115/120    avg_loss:0.012, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.987]
Epoch [117/120    avg_loss:0.011, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.987]
Epoch [120/120    avg_loss:0.011, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     0     0     0    13    10     0]
 [    0     0 18055     0    22     0     7     0     6     0]
 [    0     1     0  2022     0     0     0     0     7     6]
 [    0    34    10     3  2890     0    10     0     9    16]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4874     0     0     0]
 [    0     2     0     0     0     0     2  1278     0     8]
 [    0    24     0    68    98     0     0     0  3380     1]
 [    0     0     0     0    13    54     0     0     0   852]]

Accuracy:
98.9685007109633

F1 scores:
[       nan 0.99348938 0.99875536 0.97846601 0.96413678 0.97972973
 0.9976461  0.99031383 0.9680653  0.94561598]

Kappa:
0.9863360917517898
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f784afeeb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.083, val_acc:0.196]
Epoch [2/120    avg_loss:1.828, val_acc:0.290]
Epoch [3/120    avg_loss:1.592, val_acc:0.330]
Epoch [4/120    avg_loss:1.361, val_acc:0.419]
Epoch [5/120    avg_loss:1.151, val_acc:0.491]
Epoch [6/120    avg_loss:0.926, val_acc:0.549]
Epoch [7/120    avg_loss:0.778, val_acc:0.680]
Epoch [8/120    avg_loss:0.647, val_acc:0.770]
Epoch [9/120    avg_loss:0.556, val_acc:0.796]
Epoch [10/120    avg_loss:0.476, val_acc:0.859]
Epoch [11/120    avg_loss:0.407, val_acc:0.840]
Epoch [12/120    avg_loss:0.380, val_acc:0.857]
Epoch [13/120    avg_loss:0.315, val_acc:0.869]
Epoch [14/120    avg_loss:0.343, val_acc:0.881]
Epoch [15/120    avg_loss:0.331, val_acc:0.902]
Epoch [16/120    avg_loss:0.278, val_acc:0.902]
Epoch [17/120    avg_loss:0.211, val_acc:0.904]
Epoch [18/120    avg_loss:0.192, val_acc:0.929]
Epoch [19/120    avg_loss:0.203, val_acc:0.926]
Epoch [20/120    avg_loss:0.235, val_acc:0.937]
Epoch [21/120    avg_loss:0.163, val_acc:0.929]
Epoch [22/120    avg_loss:0.150, val_acc:0.913]
Epoch [23/120    avg_loss:0.124, val_acc:0.960]
Epoch [24/120    avg_loss:0.130, val_acc:0.951]
Epoch [25/120    avg_loss:0.119, val_acc:0.931]
Epoch [26/120    avg_loss:0.113, val_acc:0.931]
Epoch [27/120    avg_loss:0.158, val_acc:0.957]
Epoch [28/120    avg_loss:0.125, val_acc:0.933]
Epoch [29/120    avg_loss:0.089, val_acc:0.953]
Epoch [30/120    avg_loss:0.081, val_acc:0.944]
Epoch [31/120    avg_loss:0.087, val_acc:0.962]
Epoch [32/120    avg_loss:0.076, val_acc:0.966]
Epoch [33/120    avg_loss:0.075, val_acc:0.959]
Epoch [34/120    avg_loss:0.054, val_acc:0.963]
Epoch [35/120    avg_loss:0.055, val_acc:0.975]
Epoch [36/120    avg_loss:0.194, val_acc:0.897]
Epoch [37/120    avg_loss:0.167, val_acc:0.956]
Epoch [38/120    avg_loss:0.129, val_acc:0.921]
Epoch [39/120    avg_loss:0.126, val_acc:0.950]
Epoch [40/120    avg_loss:0.086, val_acc:0.964]
Epoch [41/120    avg_loss:0.080, val_acc:0.938]
Epoch [42/120    avg_loss:0.064, val_acc:0.968]
Epoch [43/120    avg_loss:0.092, val_acc:0.957]
Epoch [44/120    avg_loss:0.081, val_acc:0.951]
Epoch [45/120    avg_loss:0.065, val_acc:0.959]
Epoch [46/120    avg_loss:0.067, val_acc:0.967]
Epoch [47/120    avg_loss:0.048, val_acc:0.968]
Epoch [48/120    avg_loss:0.035, val_acc:0.941]
Epoch [49/120    avg_loss:0.038, val_acc:0.961]
Epoch [50/120    avg_loss:0.028, val_acc:0.970]
Epoch [51/120    avg_loss:0.028, val_acc:0.976]
Epoch [52/120    avg_loss:0.022, val_acc:0.978]
Epoch [53/120    avg_loss:0.026, val_acc:0.976]
Epoch [54/120    avg_loss:0.025, val_acc:0.977]
Epoch [55/120    avg_loss:0.031, val_acc:0.979]
Epoch [56/120    avg_loss:0.022, val_acc:0.980]
Epoch [57/120    avg_loss:0.020, val_acc:0.978]
Epoch [58/120    avg_loss:0.026, val_acc:0.978]
Epoch [59/120    avg_loss:0.023, val_acc:0.977]
Epoch [60/120    avg_loss:0.021, val_acc:0.982]
Epoch [61/120    avg_loss:0.025, val_acc:0.978]
Epoch [62/120    avg_loss:0.022, val_acc:0.977]
Epoch [63/120    avg_loss:0.024, val_acc:0.977]
Epoch [64/120    avg_loss:0.032, val_acc:0.977]
Epoch [65/120    avg_loss:0.023, val_acc:0.978]
Epoch [66/120    avg_loss:0.027, val_acc:0.981]
Epoch [67/120    avg_loss:0.024, val_acc:0.982]
Epoch [68/120    avg_loss:0.021, val_acc:0.981]
Epoch [69/120    avg_loss:0.019, val_acc:0.982]
Epoch [70/120    avg_loss:0.021, val_acc:0.977]
Epoch [71/120    avg_loss:0.024, val_acc:0.980]
Epoch [72/120    avg_loss:0.024, val_acc:0.981]
Epoch [73/120    avg_loss:0.026, val_acc:0.980]
Epoch [74/120    avg_loss:0.019, val_acc:0.982]
Epoch [75/120    avg_loss:0.022, val_acc:0.982]
Epoch [76/120    avg_loss:0.023, val_acc:0.978]
Epoch [77/120    avg_loss:0.019, val_acc:0.980]
Epoch [78/120    avg_loss:0.020, val_acc:0.982]
Epoch [79/120    avg_loss:0.021, val_acc:0.982]
Epoch [80/120    avg_loss:0.024, val_acc:0.984]
Epoch [81/120    avg_loss:0.018, val_acc:0.983]
Epoch [82/120    avg_loss:0.018, val_acc:0.982]
Epoch [83/120    avg_loss:0.019, val_acc:0.982]
Epoch [84/120    avg_loss:0.021, val_acc:0.983]
Epoch [85/120    avg_loss:0.017, val_acc:0.984]
Epoch [86/120    avg_loss:0.018, val_acc:0.983]
Epoch [87/120    avg_loss:0.017, val_acc:0.983]
Epoch [88/120    avg_loss:0.016, val_acc:0.985]
Epoch [89/120    avg_loss:0.015, val_acc:0.983]
Epoch [90/120    avg_loss:0.017, val_acc:0.984]
Epoch [91/120    avg_loss:0.015, val_acc:0.984]
Epoch [92/120    avg_loss:0.020, val_acc:0.984]
Epoch [93/120    avg_loss:0.020, val_acc:0.984]
Epoch [94/120    avg_loss:0.016, val_acc:0.984]
Epoch [95/120    avg_loss:0.015, val_acc:0.984]
Epoch [96/120    avg_loss:0.019, val_acc:0.984]
Epoch [97/120    avg_loss:0.018, val_acc:0.983]
Epoch [98/120    avg_loss:0.018, val_acc:0.984]
Epoch [99/120    avg_loss:0.017, val_acc:0.984]
Epoch [100/120    avg_loss:0.015, val_acc:0.983]
Epoch [101/120    avg_loss:0.018, val_acc:0.984]
Epoch [102/120    avg_loss:0.018, val_acc:0.984]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.018, val_acc:0.984]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.017, val_acc:0.983]
Epoch [108/120    avg_loss:0.015, val_acc:0.983]
Epoch [109/120    avg_loss:0.016, val_acc:0.983]
Epoch [110/120    avg_loss:0.022, val_acc:0.983]
Epoch [111/120    avg_loss:0.017, val_acc:0.983]
Epoch [112/120    avg_loss:0.019, val_acc:0.983]
Epoch [113/120    avg_loss:0.017, val_acc:0.983]
Epoch [114/120    avg_loss:0.018, val_acc:0.983]
Epoch [115/120    avg_loss:0.015, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.016, val_acc:0.983]
Epoch [118/120    avg_loss:0.017, val_acc:0.983]
Epoch [119/120    avg_loss:0.014, val_acc:0.983]
Epoch [120/120    avg_loss:0.022, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6428     0     0     3     0     0     0     1     0]
 [    0    18 17963     0    79     0    28     0     0     2]
 [    0    13     0  1980     0     0     0     0    39     4]
 [    0    35    19     0  2886     0     7     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4868     0     0     8]
 [    0     0     0     0     0     0     0  1277     0    13]
 [    0     9     0     3    56     0     0     0  3503     0]
 [    0     0     0     0    16    41     0     0     0   862]]

Accuracy:
98.9853710264382

F1 scores:
[       nan 0.99389254 0.99589732 0.98531973 0.96007984 0.98453414
 0.99539924 0.99493572 0.98136994 0.95353982]

Kappa:
0.9865684365335106
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a32da0ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.107, val_acc:0.110]
Epoch [2/120    avg_loss:1.817, val_acc:0.240]
Epoch [3/120    avg_loss:1.601, val_acc:0.360]
Epoch [4/120    avg_loss:1.411, val_acc:0.412]
Epoch [5/120    avg_loss:1.301, val_acc:0.437]
Epoch [6/120    avg_loss:1.070, val_acc:0.479]
Epoch [7/120    avg_loss:0.893, val_acc:0.538]
Epoch [8/120    avg_loss:0.718, val_acc:0.610]
Epoch [9/120    avg_loss:0.564, val_acc:0.737]
Epoch [10/120    avg_loss:0.508, val_acc:0.810]
Epoch [11/120    avg_loss:0.473, val_acc:0.866]
Epoch [12/120    avg_loss:0.417, val_acc:0.801]
Epoch [13/120    avg_loss:0.371, val_acc:0.789]
Epoch [14/120    avg_loss:0.342, val_acc:0.919]
Epoch [15/120    avg_loss:0.258, val_acc:0.860]
Epoch [16/120    avg_loss:0.251, val_acc:0.920]
Epoch [17/120    avg_loss:0.257, val_acc:0.914]
Epoch [18/120    avg_loss:0.188, val_acc:0.919]
Epoch [19/120    avg_loss:0.167, val_acc:0.930]
Epoch [20/120    avg_loss:0.174, val_acc:0.939]
Epoch [21/120    avg_loss:0.157, val_acc:0.937]
Epoch [22/120    avg_loss:0.126, val_acc:0.942]
Epoch [23/120    avg_loss:0.166, val_acc:0.937]
Epoch [24/120    avg_loss:0.149, val_acc:0.955]
Epoch [25/120    avg_loss:0.137, val_acc:0.923]
Epoch [26/120    avg_loss:0.126, val_acc:0.928]
Epoch [27/120    avg_loss:0.120, val_acc:0.934]
Epoch [28/120    avg_loss:0.096, val_acc:0.934]
Epoch [29/120    avg_loss:0.154, val_acc:0.947]
Epoch [30/120    avg_loss:0.147, val_acc:0.905]
Epoch [31/120    avg_loss:0.151, val_acc:0.954]
Epoch [32/120    avg_loss:0.090, val_acc:0.961]
Epoch [33/120    avg_loss:0.071, val_acc:0.956]
Epoch [34/120    avg_loss:0.082, val_acc:0.964]
Epoch [35/120    avg_loss:0.052, val_acc:0.968]
Epoch [36/120    avg_loss:0.053, val_acc:0.936]
Epoch [37/120    avg_loss:0.069, val_acc:0.968]
Epoch [38/120    avg_loss:0.079, val_acc:0.967]
Epoch [39/120    avg_loss:0.044, val_acc:0.974]
Epoch [40/120    avg_loss:0.042, val_acc:0.974]
Epoch [41/120    avg_loss:0.035, val_acc:0.972]
Epoch [42/120    avg_loss:0.052, val_acc:0.972]
Epoch [43/120    avg_loss:0.037, val_acc:0.966]
Epoch [44/120    avg_loss:0.051, val_acc:0.957]
Epoch [45/120    avg_loss:0.037, val_acc:0.975]
Epoch [46/120    avg_loss:0.030, val_acc:0.942]
Epoch [47/120    avg_loss:0.037, val_acc:0.972]
Epoch [48/120    avg_loss:0.020, val_acc:0.979]
Epoch [49/120    avg_loss:0.020, val_acc:0.979]
Epoch [50/120    avg_loss:0.014, val_acc:0.980]
Epoch [51/120    avg_loss:0.021, val_acc:0.965]
Epoch [52/120    avg_loss:0.025, val_acc:0.979]
Epoch [53/120    avg_loss:0.026, val_acc:0.979]
Epoch [54/120    avg_loss:0.019, val_acc:0.968]
Epoch [55/120    avg_loss:0.023, val_acc:0.973]
Epoch [56/120    avg_loss:0.018, val_acc:0.961]
Epoch [57/120    avg_loss:0.023, val_acc:0.973]
Epoch [58/120    avg_loss:0.027, val_acc:0.979]
Epoch [59/120    avg_loss:0.028, val_acc:0.982]
Epoch [60/120    avg_loss:0.013, val_acc:0.981]
Epoch [61/120    avg_loss:0.012, val_acc:0.976]
Epoch [62/120    avg_loss:0.041, val_acc:0.975]
Epoch [63/120    avg_loss:0.018, val_acc:0.985]
Epoch [64/120    avg_loss:0.016, val_acc:0.980]
Epoch [65/120    avg_loss:0.026, val_acc:0.979]
Epoch [66/120    avg_loss:0.014, val_acc:0.978]
Epoch [67/120    avg_loss:0.012, val_acc:0.980]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.015, val_acc:0.974]
Epoch [70/120    avg_loss:0.012, val_acc:0.983]
Epoch [71/120    avg_loss:0.013, val_acc:0.976]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.021, val_acc:0.964]
Epoch [76/120    avg_loss:0.016, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.984]
Epoch [78/120    avg_loss:0.041, val_acc:0.979]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.015, val_acc:0.981]
Epoch [81/120    avg_loss:0.018, val_acc:0.978]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.980]
Epoch [85/120    avg_loss:0.006, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.985]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.005, val_acc:0.985]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.003, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.004, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.003, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     0     0     0     4     3     9     0]
 [    0     1 18051     0    18     0    20     0     0     0]
 [    0     8     0  2015     0     0     0     0    13     0]
 [    0    27    15     2  2913     0     7     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     6     0     0  4864     0     0     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    23     0    21    63     0     0     0  3464     0]
 [    0     0     0     0    14    38     0     0     0   867]]

Accuracy:
99.2552960740366

F1 scores:
[       nan 0.9941892  0.99828559 0.9877451  0.97424749 0.98564955
 0.99539548 0.99845081 0.98060863 0.97034135]

Kappa:
0.9901328027655882
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f707003cc18>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.125, val_acc:0.189]
Epoch [2/120    avg_loss:1.841, val_acc:0.236]
Epoch [3/120    avg_loss:1.696, val_acc:0.261]
Epoch [4/120    avg_loss:1.552, val_acc:0.321]
Epoch [5/120    avg_loss:1.388, val_acc:0.344]
Epoch [6/120    avg_loss:1.259, val_acc:0.451]
Epoch [7/120    avg_loss:1.098, val_acc:0.530]
Epoch [8/120    avg_loss:0.898, val_acc:0.594]
Epoch [9/120    avg_loss:0.775, val_acc:0.714]
Epoch [10/120    avg_loss:0.601, val_acc:0.695]
Epoch [11/120    avg_loss:0.539, val_acc:0.810]
Epoch [12/120    avg_loss:0.451, val_acc:0.812]
Epoch [13/120    avg_loss:0.347, val_acc:0.839]
Epoch [14/120    avg_loss:0.347, val_acc:0.858]
Epoch [15/120    avg_loss:0.307, val_acc:0.847]
Epoch [16/120    avg_loss:0.853, val_acc:0.253]
Epoch [17/120    avg_loss:1.653, val_acc:0.270]
Epoch [18/120    avg_loss:1.454, val_acc:0.331]
Epoch [19/120    avg_loss:1.329, val_acc:0.340]
Epoch [20/120    avg_loss:1.183, val_acc:0.503]
Epoch [21/120    avg_loss:1.101, val_acc:0.614]
Epoch [22/120    avg_loss:1.005, val_acc:0.615]
Epoch [23/120    avg_loss:0.924, val_acc:0.739]
Epoch [24/120    avg_loss:0.841, val_acc:0.661]
Epoch [25/120    avg_loss:0.809, val_acc:0.714]
Epoch [26/120    avg_loss:0.772, val_acc:0.756]
Epoch [27/120    avg_loss:0.707, val_acc:0.741]
Epoch [28/120    avg_loss:0.645, val_acc:0.737]
Epoch [29/120    avg_loss:0.645, val_acc:0.763]
Epoch [30/120    avg_loss:0.642, val_acc:0.758]
Epoch [31/120    avg_loss:0.614, val_acc:0.763]
Epoch [32/120    avg_loss:0.616, val_acc:0.761]
Epoch [33/120    avg_loss:0.611, val_acc:0.763]
Epoch [34/120    avg_loss:0.602, val_acc:0.755]
Epoch [35/120    avg_loss:0.622, val_acc:0.775]
Epoch [36/120    avg_loss:0.584, val_acc:0.767]
Epoch [37/120    avg_loss:0.575, val_acc:0.760]
Epoch [38/120    avg_loss:0.584, val_acc:0.777]
Epoch [39/120    avg_loss:0.578, val_acc:0.776]
Epoch [40/120    avg_loss:0.590, val_acc:0.777]
Epoch [41/120    avg_loss:0.562, val_acc:0.782]
Epoch [42/120    avg_loss:0.590, val_acc:0.781]
Epoch [43/120    avg_loss:0.560, val_acc:0.777]
Epoch [44/120    avg_loss:0.581, val_acc:0.779]
Epoch [45/120    avg_loss:0.591, val_acc:0.780]
Epoch [46/120    avg_loss:0.560, val_acc:0.780]
Epoch [47/120    avg_loss:0.550, val_acc:0.780]
Epoch [48/120    avg_loss:0.544, val_acc:0.775]
Epoch [49/120    avg_loss:0.577, val_acc:0.777]
Epoch [50/120    avg_loss:0.562, val_acc:0.779]
Epoch [51/120    avg_loss:0.568, val_acc:0.778]
Epoch [52/120    avg_loss:0.557, val_acc:0.777]
Epoch [53/120    avg_loss:0.566, val_acc:0.779]
Epoch [54/120    avg_loss:0.581, val_acc:0.780]
Epoch [55/120    avg_loss:0.562, val_acc:0.780]
Epoch [56/120    avg_loss:0.587, val_acc:0.780]
Epoch [57/120    avg_loss:0.546, val_acc:0.780]
Epoch [58/120    avg_loss:0.578, val_acc:0.780]
Epoch [59/120    avg_loss:0.571, val_acc:0.780]
Epoch [60/120    avg_loss:0.535, val_acc:0.779]
Epoch [61/120    avg_loss:0.553, val_acc:0.779]
Epoch [62/120    avg_loss:0.570, val_acc:0.779]
Epoch [63/120    avg_loss:0.567, val_acc:0.780]
Epoch [64/120    avg_loss:0.546, val_acc:0.780]
Epoch [65/120    avg_loss:0.559, val_acc:0.780]
Epoch [66/120    avg_loss:0.562, val_acc:0.780]
Epoch [67/120    avg_loss:0.576, val_acc:0.780]
Epoch [68/120    avg_loss:0.560, val_acc:0.780]
Epoch [69/120    avg_loss:0.590, val_acc:0.780]
Epoch [70/120    avg_loss:0.578, val_acc:0.780]
Epoch [71/120    avg_loss:0.564, val_acc:0.780]
Epoch [72/120    avg_loss:0.566, val_acc:0.780]
Epoch [73/120    avg_loss:0.547, val_acc:0.780]
Epoch [74/120    avg_loss:0.565, val_acc:0.780]
Epoch [75/120    avg_loss:0.554, val_acc:0.780]
Epoch [76/120    avg_loss:0.572, val_acc:0.780]
Epoch [77/120    avg_loss:0.566, val_acc:0.780]
Epoch [78/120    avg_loss:0.549, val_acc:0.780]
Epoch [79/120    avg_loss:0.585, val_acc:0.780]
Epoch [80/120    avg_loss:0.554, val_acc:0.780]
Epoch [81/120    avg_loss:0.570, val_acc:0.780]
Epoch [82/120    avg_loss:0.563, val_acc:0.780]
Epoch [83/120    avg_loss:0.578, val_acc:0.780]
Epoch [84/120    avg_loss:0.565, val_acc:0.780]
Epoch [85/120    avg_loss:0.558, val_acc:0.780]
Epoch [86/120    avg_loss:0.561, val_acc:0.780]
Epoch [87/120    avg_loss:0.544, val_acc:0.780]
Epoch [88/120    avg_loss:0.561, val_acc:0.780]
Epoch [89/120    avg_loss:0.567, val_acc:0.780]
Epoch [90/120    avg_loss:0.584, val_acc:0.780]
Epoch [91/120    avg_loss:0.558, val_acc:0.780]
Epoch [92/120    avg_loss:0.594, val_acc:0.780]
Epoch [93/120    avg_loss:0.549, val_acc:0.780]
Epoch [94/120    avg_loss:0.571, val_acc:0.780]
Epoch [95/120    avg_loss:0.557, val_acc:0.780]
Epoch [96/120    avg_loss:0.594, val_acc:0.780]
Epoch [97/120    avg_loss:0.549, val_acc:0.780]
Epoch [98/120    avg_loss:0.557, val_acc:0.780]
Epoch [99/120    avg_loss:0.556, val_acc:0.780]
Epoch [100/120    avg_loss:0.563, val_acc:0.780]
Epoch [101/120    avg_loss:0.536, val_acc:0.780]
Epoch [102/120    avg_loss:0.565, val_acc:0.780]
Epoch [103/120    avg_loss:0.572, val_acc:0.780]
Epoch [104/120    avg_loss:0.577, val_acc:0.780]
Epoch [105/120    avg_loss:0.556, val_acc:0.780]
Epoch [106/120    avg_loss:0.565, val_acc:0.780]
Epoch [107/120    avg_loss:0.560, val_acc:0.780]
Epoch [108/120    avg_loss:0.563, val_acc:0.780]
Epoch [109/120    avg_loss:0.582, val_acc:0.780]
Epoch [110/120    avg_loss:0.576, val_acc:0.780]
Epoch [111/120    avg_loss:0.568, val_acc:0.780]
Epoch [112/120    avg_loss:0.559, val_acc:0.780]
Epoch [113/120    avg_loss:0.555, val_acc:0.780]
Epoch [114/120    avg_loss:0.555, val_acc:0.780]
Epoch [115/120    avg_loss:0.557, val_acc:0.780]
Epoch [116/120    avg_loss:0.550, val_acc:0.780]
Epoch [117/120    avg_loss:0.558, val_acc:0.780]
Epoch [118/120    avg_loss:0.544, val_acc:0.780]
Epoch [119/120    avg_loss:0.568, val_acc:0.780]
Epoch [120/120    avg_loss:0.571, val_acc:0.780]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4824     0     5   441     0     4    35   760   363]
 [    0     0 14177     0    79     0  3834     0     0     0]
 [    0     1     0  1816    18     0     0     0    97   104]
 [    0   107   257     0  2446     0   101     0    56     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   688   113    22     0  3899     0   156     0]
 [    0    21     0     0     0     0     4  1244     3    18]
 [    0    43     4   225    72     0    20     0  3207     0]
 [    0    19     0    12    31   111     0     0     2   744]]

Accuracy:
81.12693707372328

F1 scores:
[       nan 0.84284092 0.85362476 0.86332303 0.80447295 0.95920617
 0.61208791 0.96847022 0.81686195 0.69112866]

Kappa:
0.7582089093699849
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f76a3623c50>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.100, val_acc:0.168]
Epoch [2/120    avg_loss:1.814, val_acc:0.205]
Epoch [3/120    avg_loss:1.592, val_acc:0.274]
Epoch [4/120    avg_loss:1.413, val_acc:0.420]
Epoch [5/120    avg_loss:1.200, val_acc:0.527]
Epoch [6/120    avg_loss:1.071, val_acc:0.522]
Epoch [7/120    avg_loss:0.855, val_acc:0.604]
Epoch [8/120    avg_loss:0.709, val_acc:0.712]
Epoch [9/120    avg_loss:0.609, val_acc:0.704]
Epoch [10/120    avg_loss:0.578, val_acc:0.730]
Epoch [11/120    avg_loss:0.503, val_acc:0.727]
Epoch [12/120    avg_loss:0.443, val_acc:0.740]
Epoch [13/120    avg_loss:0.389, val_acc:0.853]
Epoch [14/120    avg_loss:0.324, val_acc:0.843]
Epoch [15/120    avg_loss:0.275, val_acc:0.848]
Epoch [16/120    avg_loss:0.241, val_acc:0.926]
Epoch [17/120    avg_loss:0.239, val_acc:0.894]
Epoch [18/120    avg_loss:0.274, val_acc:0.901]
Epoch [19/120    avg_loss:0.219, val_acc:0.920]
Epoch [20/120    avg_loss:0.207, val_acc:0.927]
Epoch [21/120    avg_loss:0.158, val_acc:0.953]
Epoch [22/120    avg_loss:0.143, val_acc:0.949]
Epoch [23/120    avg_loss:0.142, val_acc:0.933]
Epoch [24/120    avg_loss:0.160, val_acc:0.942]
Epoch [25/120    avg_loss:0.149, val_acc:0.949]
Epoch [26/120    avg_loss:0.142, val_acc:0.929]
Epoch [27/120    avg_loss:0.129, val_acc:0.945]
Epoch [28/120    avg_loss:0.106, val_acc:0.936]
Epoch [29/120    avg_loss:0.099, val_acc:0.961]
Epoch [30/120    avg_loss:0.087, val_acc:0.955]
Epoch [31/120    avg_loss:0.087, val_acc:0.968]
Epoch [32/120    avg_loss:0.065, val_acc:0.969]
Epoch [33/120    avg_loss:0.056, val_acc:0.955]
Epoch [34/120    avg_loss:0.073, val_acc:0.958]
Epoch [35/120    avg_loss:0.105, val_acc:0.961]
Epoch [36/120    avg_loss:0.317, val_acc:0.787]
Epoch [37/120    avg_loss:0.311, val_acc:0.907]
Epoch [38/120    avg_loss:0.121, val_acc:0.952]
Epoch [39/120    avg_loss:0.059, val_acc:0.973]
Epoch [40/120    avg_loss:0.066, val_acc:0.953]
Epoch [41/120    avg_loss:0.055, val_acc:0.965]
Epoch [42/120    avg_loss:0.047, val_acc:0.953]
Epoch [43/120    avg_loss:0.052, val_acc:0.977]
Epoch [44/120    avg_loss:0.094, val_acc:0.948]
Epoch [45/120    avg_loss:0.080, val_acc:0.859]
Epoch [46/120    avg_loss:0.060, val_acc:0.951]
Epoch [47/120    avg_loss:0.055, val_acc:0.968]
Epoch [48/120    avg_loss:0.043, val_acc:0.957]
Epoch [49/120    avg_loss:0.033, val_acc:0.967]
Epoch [50/120    avg_loss:0.037, val_acc:0.977]
Epoch [51/120    avg_loss:0.029, val_acc:0.968]
Epoch [52/120    avg_loss:0.029, val_acc:0.963]
Epoch [53/120    avg_loss:0.027, val_acc:0.975]
Epoch [54/120    avg_loss:0.067, val_acc:0.963]
Epoch [55/120    avg_loss:0.039, val_acc:0.965]
Epoch [56/120    avg_loss:0.025, val_acc:0.983]
Epoch [57/120    avg_loss:0.021, val_acc:0.979]
Epoch [58/120    avg_loss:0.023, val_acc:0.981]
Epoch [59/120    avg_loss:0.024, val_acc:0.973]
Epoch [60/120    avg_loss:0.035, val_acc:0.987]
Epoch [61/120    avg_loss:0.034, val_acc:0.978]
Epoch [62/120    avg_loss:0.028, val_acc:0.985]
Epoch [63/120    avg_loss:0.021, val_acc:0.986]
Epoch [64/120    avg_loss:0.028, val_acc:0.983]
Epoch [65/120    avg_loss:0.029, val_acc:0.946]
Epoch [66/120    avg_loss:0.023, val_acc:0.984]
Epoch [67/120    avg_loss:0.014, val_acc:0.988]
Epoch [68/120    avg_loss:0.027, val_acc:0.985]
Epoch [69/120    avg_loss:0.019, val_acc:0.970]
Epoch [70/120    avg_loss:0.020, val_acc:0.980]
Epoch [71/120    avg_loss:0.654, val_acc:0.657]
Epoch [72/120    avg_loss:0.461, val_acc:0.816]
Epoch [73/120    avg_loss:0.303, val_acc:0.851]
Epoch [74/120    avg_loss:0.189, val_acc:0.940]
Epoch [75/120    avg_loss:0.114, val_acc:0.953]
Epoch [76/120    avg_loss:0.065, val_acc:0.956]
Epoch [77/120    avg_loss:0.048, val_acc:0.970]
Epoch [78/120    avg_loss:0.047, val_acc:0.967]
Epoch [79/120    avg_loss:0.039, val_acc:0.974]
Epoch [80/120    avg_loss:0.024, val_acc:0.977]
Epoch [81/120    avg_loss:0.024, val_acc:0.982]
Epoch [82/120    avg_loss:0.020, val_acc:0.984]
Epoch [83/120    avg_loss:0.019, val_acc:0.985]
Epoch [84/120    avg_loss:0.020, val_acc:0.984]
Epoch [85/120    avg_loss:0.019, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.018, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.014, val_acc:0.985]
Epoch [90/120    avg_loss:0.018, val_acc:0.984]
Epoch [91/120    avg_loss:0.017, val_acc:0.984]
Epoch [92/120    avg_loss:0.020, val_acc:0.984]
Epoch [93/120    avg_loss:0.016, val_acc:0.985]
Epoch [94/120    avg_loss:0.023, val_acc:0.985]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.013, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.985]
Epoch [99/120    avg_loss:0.019, val_acc:0.984]
Epoch [100/120    avg_loss:0.020, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.985]
Epoch [102/120    avg_loss:0.014, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.015, val_acc:0.985]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.015, val_acc:0.985]
Epoch [108/120    avg_loss:0.014, val_acc:0.985]
Epoch [109/120    avg_loss:0.018, val_acc:0.985]
Epoch [110/120    avg_loss:0.016, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.015, val_acc:0.985]
Epoch [114/120    avg_loss:0.017, val_acc:0.985]
Epoch [115/120    avg_loss:0.016, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.020, val_acc:0.985]
Epoch [120/120    avg_loss:0.014, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     3     0     2     0     6     3]
 [    0     0 18000     0    80     0    10     0     0     0]
 [    0     8     0  1958     3     0     0     0    61     6]
 [    0    42    18     0  2882     0     5     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     2     0     0  4872     0     0     0]
 [    0     2     0     0     0     0     2  1281     0     5]
 [    0    28     0     5    56     0     0     0  3482     0]
 [    0     0     0     0    14    60     0     0     0   845]]

Accuracy:
98.91547971947075

F1 scores:
[       nan 0.99273009 0.99689854 0.97875531 0.95906822 0.97752809
 0.99744088 0.99649942 0.9746676  0.95050619]

Kappa:
0.9856368392444989
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f0a7b2b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.120, val_acc:0.078]
Epoch [2/120    avg_loss:1.853, val_acc:0.229]
Epoch [3/120    avg_loss:1.662, val_acc:0.235]
Epoch [4/120    avg_loss:1.477, val_acc:0.268]
Epoch [5/120    avg_loss:1.264, val_acc:0.332]
Epoch [6/120    avg_loss:1.142, val_acc:0.405]
Epoch [7/120    avg_loss:1.013, val_acc:0.535]
Epoch [8/120    avg_loss:0.798, val_acc:0.817]
Epoch [9/120    avg_loss:0.670, val_acc:0.798]
Epoch [10/120    avg_loss:0.539, val_acc:0.850]
Epoch [11/120    avg_loss:0.645, val_acc:0.684]
Epoch [12/120    avg_loss:0.704, val_acc:0.831]
Epoch [13/120    avg_loss:0.428, val_acc:0.885]
Epoch [14/120    avg_loss:0.384, val_acc:0.907]
Epoch [15/120    avg_loss:0.287, val_acc:0.913]
Epoch [16/120    avg_loss:0.284, val_acc:0.913]
Epoch [17/120    avg_loss:0.257, val_acc:0.921]
Epoch [18/120    avg_loss:0.214, val_acc:0.924]
Epoch [19/120    avg_loss:0.227, val_acc:0.876]
Epoch [20/120    avg_loss:0.251, val_acc:0.916]
Epoch [21/120    avg_loss:0.186, val_acc:0.954]
Epoch [22/120    avg_loss:0.248, val_acc:0.932]
Epoch [23/120    avg_loss:0.207, val_acc:0.860]
Epoch [24/120    avg_loss:0.180, val_acc:0.948]
Epoch [25/120    avg_loss:0.136, val_acc:0.952]
Epoch [26/120    avg_loss:0.156, val_acc:0.950]
Epoch [27/120    avg_loss:0.390, val_acc:0.279]
Epoch [28/120    avg_loss:0.979, val_acc:0.668]
Epoch [29/120    avg_loss:0.387, val_acc:0.802]
Epoch [30/120    avg_loss:0.305, val_acc:0.805]
Epoch [31/120    avg_loss:0.254, val_acc:0.922]
Epoch [32/120    avg_loss:0.174, val_acc:0.945]
Epoch [33/120    avg_loss:0.181, val_acc:0.947]
Epoch [34/120    avg_loss:0.186, val_acc:0.924]
Epoch [35/120    avg_loss:0.125, val_acc:0.946]
Epoch [36/120    avg_loss:0.125, val_acc:0.954]
Epoch [37/120    avg_loss:0.129, val_acc:0.952]
Epoch [38/120    avg_loss:0.105, val_acc:0.957]
Epoch [39/120    avg_loss:0.106, val_acc:0.954]
Epoch [40/120    avg_loss:0.101, val_acc:0.957]
Epoch [41/120    avg_loss:0.110, val_acc:0.957]
Epoch [42/120    avg_loss:0.100, val_acc:0.954]
Epoch [43/120    avg_loss:0.094, val_acc:0.962]
Epoch [44/120    avg_loss:0.099, val_acc:0.956]
Epoch [45/120    avg_loss:0.097, val_acc:0.948]
Epoch [46/120    avg_loss:0.094, val_acc:0.957]
Epoch [47/120    avg_loss:0.090, val_acc:0.961]
Epoch [48/120    avg_loss:0.099, val_acc:0.943]
Epoch [49/120    avg_loss:0.085, val_acc:0.960]
Epoch [50/120    avg_loss:0.093, val_acc:0.964]
Epoch [51/120    avg_loss:0.087, val_acc:0.944]
Epoch [52/120    avg_loss:0.087, val_acc:0.952]
Epoch [53/120    avg_loss:0.080, val_acc:0.962]
Epoch [54/120    avg_loss:0.082, val_acc:0.958]
Epoch [55/120    avg_loss:0.083, val_acc:0.955]
Epoch [56/120    avg_loss:0.072, val_acc:0.963]
Epoch [57/120    avg_loss:0.088, val_acc:0.958]
Epoch [58/120    avg_loss:0.070, val_acc:0.960]
Epoch [59/120    avg_loss:0.081, val_acc:0.959]
Epoch [60/120    avg_loss:0.079, val_acc:0.963]
Epoch [61/120    avg_loss:0.082, val_acc:0.962]
Epoch [62/120    avg_loss:0.066, val_acc:0.967]
Epoch [63/120    avg_loss:0.069, val_acc:0.961]
Epoch [64/120    avg_loss:0.076, val_acc:0.967]
Epoch [65/120    avg_loss:0.068, val_acc:0.965]
Epoch [66/120    avg_loss:0.076, val_acc:0.967]
Epoch [67/120    avg_loss:0.068, val_acc:0.970]
Epoch [68/120    avg_loss:0.076, val_acc:0.966]
Epoch [69/120    avg_loss:0.069, val_acc:0.964]
Epoch [70/120    avg_loss:0.057, val_acc:0.960]
Epoch [71/120    avg_loss:0.065, val_acc:0.962]
Epoch [72/120    avg_loss:0.065, val_acc:0.958]
Epoch [73/120    avg_loss:0.061, val_acc:0.964]
Epoch [74/120    avg_loss:0.061, val_acc:0.963]
Epoch [75/120    avg_loss:0.062, val_acc:0.965]
Epoch [76/120    avg_loss:0.065, val_acc:0.970]
Epoch [77/120    avg_loss:0.062, val_acc:0.966]
Epoch [78/120    avg_loss:0.061, val_acc:0.972]
Epoch [79/120    avg_loss:0.054, val_acc:0.964]
Epoch [80/120    avg_loss:0.051, val_acc:0.968]
Epoch [81/120    avg_loss:0.053, val_acc:0.969]
Epoch [82/120    avg_loss:0.055, val_acc:0.964]
Epoch [83/120    avg_loss:0.067, val_acc:0.972]
Epoch [84/120    avg_loss:0.058, val_acc:0.968]
Epoch [85/120    avg_loss:0.062, val_acc:0.966]
Epoch [86/120    avg_loss:0.052, val_acc:0.967]
Epoch [87/120    avg_loss:0.057, val_acc:0.972]
Epoch [88/120    avg_loss:0.048, val_acc:0.970]
Epoch [89/120    avg_loss:0.053, val_acc:0.970]
Epoch [90/120    avg_loss:0.059, val_acc:0.956]
Epoch [91/120    avg_loss:0.062, val_acc:0.962]
Epoch [92/120    avg_loss:0.068, val_acc:0.975]
Epoch [93/120    avg_loss:0.053, val_acc:0.972]
Epoch [94/120    avg_loss:0.052, val_acc:0.966]
Epoch [95/120    avg_loss:0.049, val_acc:0.964]
Epoch [96/120    avg_loss:0.043, val_acc:0.970]
Epoch [97/120    avg_loss:0.041, val_acc:0.971]
Epoch [98/120    avg_loss:0.048, val_acc:0.966]
Epoch [99/120    avg_loss:0.047, val_acc:0.962]
Epoch [100/120    avg_loss:0.043, val_acc:0.970]
Epoch [101/120    avg_loss:0.045, val_acc:0.970]
Epoch [102/120    avg_loss:0.041, val_acc:0.972]
Epoch [103/120    avg_loss:0.042, val_acc:0.970]
Epoch [104/120    avg_loss:0.045, val_acc:0.970]
Epoch [105/120    avg_loss:0.042, val_acc:0.970]
Epoch [106/120    avg_loss:0.038, val_acc:0.973]
Epoch [107/120    avg_loss:0.039, val_acc:0.973]
Epoch [108/120    avg_loss:0.038, val_acc:0.971]
Epoch [109/120    avg_loss:0.043, val_acc:0.974]
Epoch [110/120    avg_loss:0.042, val_acc:0.970]
Epoch [111/120    avg_loss:0.042, val_acc:0.973]
Epoch [112/120    avg_loss:0.044, val_acc:0.972]
Epoch [113/120    avg_loss:0.035, val_acc:0.974]
Epoch [114/120    avg_loss:0.035, val_acc:0.973]
Epoch [115/120    avg_loss:0.047, val_acc:0.970]
Epoch [116/120    avg_loss:0.037, val_acc:0.973]
Epoch [117/120    avg_loss:0.037, val_acc:0.974]
Epoch [118/120    avg_loss:0.045, val_acc:0.974]
Epoch [119/120    avg_loss:0.032, val_acc:0.974]
Epoch [120/120    avg_loss:0.036, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     0    42     0     0     6     2    27]
 [    0     0 17947     0    31     0   101     0     8     3]
 [    0    11     0  1901     0     0     0     0   109    15]
 [    0    72    23     0  2871     0     1     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9    22     0     0  4827     0    19     1]
 [    0    11     0     0     0     0     2  1274     0     3]
 [    0    95     0    30    61     0     0     0  3354    31]
 [    0     0     0     0     0    86     0     1     0   832]]

Accuracy:
98.00689272889403

F1 scores:
[       nan 0.97950062 0.99514819 0.95312108 0.96068262 0.96810089
 0.98419819 0.99105406 0.94906621 0.90879301]

Kappa:
0.9736159146114463
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81d3cccb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.129, val_acc:0.082]
Epoch [2/120    avg_loss:1.895, val_acc:0.214]
Epoch [3/120    avg_loss:1.639, val_acc:0.314]
Epoch [4/120    avg_loss:1.482, val_acc:0.474]
Epoch [5/120    avg_loss:1.366, val_acc:0.492]
Epoch [6/120    avg_loss:1.138, val_acc:0.601]
Epoch [7/120    avg_loss:1.012, val_acc:0.632]
Epoch [8/120    avg_loss:0.803, val_acc:0.689]
Epoch [9/120    avg_loss:0.699, val_acc:0.773]
Epoch [10/120    avg_loss:1.230, val_acc:0.393]
Epoch [11/120    avg_loss:1.121, val_acc:0.545]
Epoch [12/120    avg_loss:0.986, val_acc:0.632]
Epoch [13/120    avg_loss:0.926, val_acc:0.678]
Epoch [14/120    avg_loss:0.800, val_acc:0.711]
Epoch [15/120    avg_loss:0.704, val_acc:0.718]
Epoch [16/120    avg_loss:0.693, val_acc:0.735]
Epoch [17/120    avg_loss:0.580, val_acc:0.754]
Epoch [18/120    avg_loss:0.519, val_acc:0.768]
Epoch [19/120    avg_loss:0.537, val_acc:0.788]
Epoch [20/120    avg_loss:0.497, val_acc:0.720]
Epoch [21/120    avg_loss:0.497, val_acc:0.814]
Epoch [22/120    avg_loss:0.433, val_acc:0.800]
Epoch [23/120    avg_loss:0.419, val_acc:0.787]
Epoch [24/120    avg_loss:0.382, val_acc:0.815]
Epoch [25/120    avg_loss:0.370, val_acc:0.859]
Epoch [26/120    avg_loss:0.379, val_acc:0.843]
Epoch [27/120    avg_loss:0.357, val_acc:0.842]
Epoch [28/120    avg_loss:0.379, val_acc:0.902]
Epoch [29/120    avg_loss:0.370, val_acc:0.843]
Epoch [30/120    avg_loss:0.318, val_acc:0.891]
Epoch [31/120    avg_loss:0.290, val_acc:0.924]
Epoch [32/120    avg_loss:0.257, val_acc:0.895]
Epoch [33/120    avg_loss:0.230, val_acc:0.913]
Epoch [34/120    avg_loss:0.219, val_acc:0.900]
Epoch [35/120    avg_loss:0.246, val_acc:0.915]
Epoch [36/120    avg_loss:0.232, val_acc:0.926]
Epoch [37/120    avg_loss:0.219, val_acc:0.930]
Epoch [38/120    avg_loss:0.195, val_acc:0.918]
Epoch [39/120    avg_loss:0.205, val_acc:0.916]
Epoch [40/120    avg_loss:0.176, val_acc:0.935]
Epoch [41/120    avg_loss:0.169, val_acc:0.918]
Epoch [42/120    avg_loss:0.174, val_acc:0.941]
Epoch [43/120    avg_loss:0.186, val_acc:0.908]
Epoch [44/120    avg_loss:0.195, val_acc:0.925]
Epoch [45/120    avg_loss:0.147, val_acc:0.902]
Epoch [46/120    avg_loss:0.199, val_acc:0.932]
Epoch [47/120    avg_loss:0.160, val_acc:0.951]
Epoch [48/120    avg_loss:0.156, val_acc:0.942]
Epoch [49/120    avg_loss:0.136, val_acc:0.947]
Epoch [50/120    avg_loss:0.129, val_acc:0.950]
Epoch [51/120    avg_loss:0.125, val_acc:0.950]
Epoch [52/120    avg_loss:0.116, val_acc:0.945]
Epoch [53/120    avg_loss:0.104, val_acc:0.952]
Epoch [54/120    avg_loss:0.101, val_acc:0.953]
Epoch [55/120    avg_loss:0.092, val_acc:0.944]
Epoch [56/120    avg_loss:0.117, val_acc:0.958]
Epoch [57/120    avg_loss:0.083, val_acc:0.956]
Epoch [58/120    avg_loss:0.080, val_acc:0.951]
Epoch [59/120    avg_loss:0.103, val_acc:0.961]
Epoch [60/120    avg_loss:0.078, val_acc:0.968]
Epoch [61/120    avg_loss:0.055, val_acc:0.972]
Epoch [62/120    avg_loss:0.076, val_acc:0.969]
Epoch [63/120    avg_loss:0.068, val_acc:0.970]
Epoch [64/120    avg_loss:0.058, val_acc:0.965]
Epoch [65/120    avg_loss:0.062, val_acc:0.970]
Epoch [66/120    avg_loss:0.056, val_acc:0.974]
Epoch [67/120    avg_loss:0.079, val_acc:0.950]
Epoch [68/120    avg_loss:0.075, val_acc:0.963]
Epoch [69/120    avg_loss:0.049, val_acc:0.977]
Epoch [70/120    avg_loss:0.072, val_acc:0.959]
Epoch [71/120    avg_loss:0.095, val_acc:0.931]
Epoch [72/120    avg_loss:0.099, val_acc:0.954]
Epoch [73/120    avg_loss:0.154, val_acc:0.959]
Epoch [74/120    avg_loss:0.068, val_acc:0.961]
Epoch [75/120    avg_loss:0.080, val_acc:0.944]
Epoch [76/120    avg_loss:0.063, val_acc:0.973]
Epoch [77/120    avg_loss:0.052, val_acc:0.961]
Epoch [78/120    avg_loss:0.071, val_acc:0.965]
Epoch [79/120    avg_loss:0.049, val_acc:0.977]
Epoch [80/120    avg_loss:0.034, val_acc:0.971]
Epoch [81/120    avg_loss:0.037, val_acc:0.964]
Epoch [82/120    avg_loss:0.029, val_acc:0.980]
Epoch [83/120    avg_loss:0.037, val_acc:0.962]
Epoch [84/120    avg_loss:0.038, val_acc:0.966]
Epoch [85/120    avg_loss:0.029, val_acc:0.977]
Epoch [86/120    avg_loss:0.023, val_acc:0.978]
Epoch [87/120    avg_loss:0.029, val_acc:0.977]
Epoch [88/120    avg_loss:0.032, val_acc:0.978]
Epoch [89/120    avg_loss:0.023, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.971]
Epoch [91/120    avg_loss:0.023, val_acc:0.983]
Epoch [92/120    avg_loss:0.026, val_acc:0.979]
Epoch [93/120    avg_loss:0.035, val_acc:0.977]
Epoch [94/120    avg_loss:0.028, val_acc:0.973]
Epoch [95/120    avg_loss:0.021, val_acc:0.985]
Epoch [96/120    avg_loss:0.022, val_acc:0.983]
Epoch [97/120    avg_loss:0.023, val_acc:0.981]
Epoch [98/120    avg_loss:0.022, val_acc:0.983]
Epoch [99/120    avg_loss:0.019, val_acc:0.976]
Epoch [100/120    avg_loss:0.023, val_acc:0.979]
Epoch [101/120    avg_loss:0.011, val_acc:0.984]
Epoch [102/120    avg_loss:0.017, val_acc:0.977]
Epoch [103/120    avg_loss:0.044, val_acc:0.970]
Epoch [104/120    avg_loss:0.034, val_acc:0.976]
Epoch [105/120    avg_loss:0.019, val_acc:0.984]
Epoch [106/120    avg_loss:0.018, val_acc:0.973]
Epoch [107/120    avg_loss:0.032, val_acc:0.979]
Epoch [108/120    avg_loss:0.032, val_acc:0.984]
Epoch [109/120    avg_loss:0.017, val_acc:0.985]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.014, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.015, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.013, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.017, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6155     0     2    20     0     0     1   147   107]
 [    0     8 18015     0    65     0     2     0     0     0]
 [    0    11     0  1992     0     0     0     0    25     8]
 [    0    22    21     0  2875     0    12     0    39     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     2     0     0     0     0     0  1279     0     9]
 [    0    23     0     3    65     0     0     0  3480     0]
 [    0     0     0     0    17    52     0     0     0   850]]

Accuracy:
98.3997300749524

F1 scores:
[       nan 0.9728918  0.99734263 0.98785024 0.95610243 0.98046582
 0.99856704 0.99533074 0.95841366 0.89662447]

Kappa:
0.9788215413506032
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e2cc2aac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.174, val_acc:0.047]
Epoch [2/120    avg_loss:1.881, val_acc:0.091]
Epoch [3/120    avg_loss:1.666, val_acc:0.152]
Epoch [4/120    avg_loss:1.484, val_acc:0.177]
Epoch [5/120    avg_loss:1.359, val_acc:0.291]
Epoch [6/120    avg_loss:1.210, val_acc:0.618]
Epoch [7/120    avg_loss:1.126, val_acc:0.676]
Epoch [8/120    avg_loss:0.975, val_acc:0.769]
Epoch [9/120    avg_loss:0.823, val_acc:0.722]
Epoch [10/120    avg_loss:0.659, val_acc:0.692]
Epoch [11/120    avg_loss:0.560, val_acc:0.692]
Epoch [12/120    avg_loss:0.544, val_acc:0.697]
Epoch [13/120    avg_loss:0.478, val_acc:0.778]
Epoch [14/120    avg_loss:0.401, val_acc:0.812]
Epoch [15/120    avg_loss:0.359, val_acc:0.847]
Epoch [16/120    avg_loss:0.312, val_acc:0.829]
Epoch [17/120    avg_loss:0.303, val_acc:0.866]
Epoch [18/120    avg_loss:0.267, val_acc:0.844]
Epoch [19/120    avg_loss:0.243, val_acc:0.908]
Epoch [20/120    avg_loss:0.208, val_acc:0.868]
Epoch [21/120    avg_loss:0.191, val_acc:0.902]
Epoch [22/120    avg_loss:0.222, val_acc:0.902]
Epoch [23/120    avg_loss:0.198, val_acc:0.931]
Epoch [24/120    avg_loss:0.152, val_acc:0.908]
Epoch [25/120    avg_loss:0.142, val_acc:0.945]
Epoch [26/120    avg_loss:0.117, val_acc:0.931]
Epoch [27/120    avg_loss:0.130, val_acc:0.940]
Epoch [28/120    avg_loss:0.097, val_acc:0.938]
Epoch [29/120    avg_loss:0.107, val_acc:0.949]
Epoch [30/120    avg_loss:0.131, val_acc:0.931]
Epoch [31/120    avg_loss:0.084, val_acc:0.954]
Epoch [32/120    avg_loss:0.097, val_acc:0.948]
Epoch [33/120    avg_loss:0.090, val_acc:0.942]
Epoch [34/120    avg_loss:0.075, val_acc:0.953]
Epoch [35/120    avg_loss:0.064, val_acc:0.962]
Epoch [36/120    avg_loss:0.069, val_acc:0.935]
Epoch [37/120    avg_loss:0.080, val_acc:0.954]
Epoch [38/120    avg_loss:0.065, val_acc:0.955]
Epoch [39/120    avg_loss:0.056, val_acc:0.962]
Epoch [40/120    avg_loss:0.079, val_acc:0.940]
Epoch [41/120    avg_loss:0.081, val_acc:0.947]
Epoch [42/120    avg_loss:0.067, val_acc:0.949]
Epoch [43/120    avg_loss:0.061, val_acc:0.966]
Epoch [44/120    avg_loss:0.070, val_acc:0.944]
Epoch [45/120    avg_loss:0.080, val_acc:0.948]
Epoch [46/120    avg_loss:0.075, val_acc:0.960]
Epoch [47/120    avg_loss:0.057, val_acc:0.968]
Epoch [48/120    avg_loss:0.039, val_acc:0.971]
Epoch [49/120    avg_loss:0.032, val_acc:0.954]
Epoch [50/120    avg_loss:0.058, val_acc:0.953]
Epoch [51/120    avg_loss:0.042, val_acc:0.969]
Epoch [52/120    avg_loss:0.052, val_acc:0.965]
Epoch [53/120    avg_loss:0.080, val_acc:0.957]
Epoch [54/120    avg_loss:0.043, val_acc:0.967]
Epoch [55/120    avg_loss:0.038, val_acc:0.957]
Epoch [56/120    avg_loss:0.038, val_acc:0.965]
Epoch [57/120    avg_loss:0.034, val_acc:0.979]
Epoch [58/120    avg_loss:0.134, val_acc:0.919]
Epoch [59/120    avg_loss:0.074, val_acc:0.961]
Epoch [60/120    avg_loss:0.037, val_acc:0.966]
Epoch [61/120    avg_loss:0.031, val_acc:0.952]
Epoch [62/120    avg_loss:0.028, val_acc:0.969]
Epoch [63/120    avg_loss:0.022, val_acc:0.974]
Epoch [64/120    avg_loss:0.026, val_acc:0.960]
Epoch [65/120    avg_loss:0.049, val_acc:0.963]
Epoch [66/120    avg_loss:0.035, val_acc:0.968]
Epoch [67/120    avg_loss:0.027, val_acc:0.956]
Epoch [68/120    avg_loss:0.027, val_acc:0.967]
Epoch [69/120    avg_loss:0.016, val_acc:0.967]
Epoch [70/120    avg_loss:0.014, val_acc:0.975]
Epoch [71/120    avg_loss:0.018, val_acc:0.979]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.015, val_acc:0.982]
Epoch [75/120    avg_loss:0.014, val_acc:0.981]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.978]
Epoch [78/120    avg_loss:0.009, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.012, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.980]
Epoch [83/120    avg_loss:0.014, val_acc:0.980]
Epoch [84/120    avg_loss:0.012, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.010, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.980]
Epoch [88/120    avg_loss:0.009, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.980]
Epoch [90/120    avg_loss:0.011, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.011, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.979]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.010, val_acc:0.979]
Epoch [104/120    avg_loss:0.008, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.010, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.979]
Epoch [108/120    avg_loss:0.014, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.013, val_acc:0.979]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.010, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.979]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.979]
Epoch [119/120    avg_loss:0.009, val_acc:0.979]
Epoch [120/120    avg_loss:0.009, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6335     0     0     0     0     2     6    68    21]
 [    0     0 18049     0    20     0    17     0     4     0]
 [    0     7     0  1876     0     0     0     0   153     0]
 [    0    12    10     5  2928     0    13     0     0     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    49     0     0     0  4827     0     2     0]
 [    0    29     0     0     0     0     0  1260     1     0]
 [    0    12     0    37    42     0     4     0  3476     0]
 [    0     0     0     0    10    12     0     0     0   897]]

Accuracy:
98.6985756633649

F1 scores:
[       nan 0.98776019 0.99723742 0.94891249 0.98057602 0.99542334
 0.99106868 0.98591549 0.95560137 0.9744704 ]

Kappa:
0.9827510955017645
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0e9ae5fa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.233, val_acc:0.127]
Epoch [2/120    avg_loss:1.894, val_acc:0.158]
Epoch [3/120    avg_loss:1.637, val_acc:0.252]
Epoch [4/120    avg_loss:1.424, val_acc:0.292]
Epoch [5/120    avg_loss:1.282, val_acc:0.359]
Epoch [6/120    avg_loss:1.181, val_acc:0.423]
Epoch [7/120    avg_loss:1.022, val_acc:0.457]
Epoch [8/120    avg_loss:0.851, val_acc:0.498]
Epoch [9/120    avg_loss:0.744, val_acc:0.497]
Epoch [10/120    avg_loss:0.671, val_acc:0.542]
Epoch [11/120    avg_loss:0.564, val_acc:0.573]
Epoch [12/120    avg_loss:0.489, val_acc:0.614]
Epoch [13/120    avg_loss:0.499, val_acc:0.668]
Epoch [14/120    avg_loss:0.434, val_acc:0.717]
Epoch [15/120    avg_loss:0.389, val_acc:0.756]
Epoch [16/120    avg_loss:0.321, val_acc:0.765]
Epoch [17/120    avg_loss:0.282, val_acc:0.775]
Epoch [18/120    avg_loss:0.297, val_acc:0.851]
Epoch [19/120    avg_loss:0.248, val_acc:0.823]
Epoch [20/120    avg_loss:0.263, val_acc:0.772]
Epoch [21/120    avg_loss:0.243, val_acc:0.784]
Epoch [22/120    avg_loss:0.176, val_acc:0.926]
Epoch [23/120    avg_loss:0.167, val_acc:0.940]
Epoch [24/120    avg_loss:0.174, val_acc:0.904]
Epoch [25/120    avg_loss:0.173, val_acc:0.895]
Epoch [26/120    avg_loss:0.145, val_acc:0.944]
Epoch [27/120    avg_loss:0.125, val_acc:0.904]
Epoch [28/120    avg_loss:0.145, val_acc:0.879]
Epoch [29/120    avg_loss:0.210, val_acc:0.902]
Epoch [30/120    avg_loss:0.158, val_acc:0.926]
Epoch [31/120    avg_loss:0.104, val_acc:0.952]
Epoch [32/120    avg_loss:0.101, val_acc:0.949]
Epoch [33/120    avg_loss:0.125, val_acc:0.935]
Epoch [34/120    avg_loss:0.069, val_acc:0.934]
Epoch [35/120    avg_loss:0.070, val_acc:0.956]
Epoch [36/120    avg_loss:0.075, val_acc:0.951]
Epoch [37/120    avg_loss:0.071, val_acc:0.960]
Epoch [38/120    avg_loss:0.051, val_acc:0.946]
Epoch [39/120    avg_loss:0.054, val_acc:0.968]
Epoch [40/120    avg_loss:0.033, val_acc:0.967]
Epoch [41/120    avg_loss:0.053, val_acc:0.954]
Epoch [42/120    avg_loss:0.057, val_acc:0.956]
Epoch [43/120    avg_loss:0.050, val_acc:0.913]
Epoch [44/120    avg_loss:0.052, val_acc:0.962]
Epoch [45/120    avg_loss:0.061, val_acc:0.950]
Epoch [46/120    avg_loss:0.061, val_acc:0.963]
Epoch [47/120    avg_loss:0.035, val_acc:0.966]
Epoch [48/120    avg_loss:0.044, val_acc:0.954]
Epoch [49/120    avg_loss:0.050, val_acc:0.944]
Epoch [50/120    avg_loss:0.037, val_acc:0.967]
Epoch [51/120    avg_loss:0.076, val_acc:0.949]
Epoch [52/120    avg_loss:0.108, val_acc:0.967]
Epoch [53/120    avg_loss:0.043, val_acc:0.966]
Epoch [54/120    avg_loss:0.040, val_acc:0.967]
Epoch [55/120    avg_loss:0.032, val_acc:0.967]
Epoch [56/120    avg_loss:0.036, val_acc:0.965]
Epoch [57/120    avg_loss:0.032, val_acc:0.966]
Epoch [58/120    avg_loss:0.029, val_acc:0.967]
Epoch [59/120    avg_loss:0.025, val_acc:0.968]
Epoch [60/120    avg_loss:0.023, val_acc:0.968]
Epoch [61/120    avg_loss:0.020, val_acc:0.968]
Epoch [62/120    avg_loss:0.022, val_acc:0.967]
Epoch [63/120    avg_loss:0.022, val_acc:0.968]
Epoch [64/120    avg_loss:0.027, val_acc:0.967]
Epoch [65/120    avg_loss:0.024, val_acc:0.968]
Epoch [66/120    avg_loss:0.021, val_acc:0.968]
Epoch [67/120    avg_loss:0.020, val_acc:0.968]
Epoch [68/120    avg_loss:0.018, val_acc:0.971]
Epoch [69/120    avg_loss:0.016, val_acc:0.969]
Epoch [70/120    avg_loss:0.017, val_acc:0.970]
Epoch [71/120    avg_loss:0.019, val_acc:0.969]
Epoch [72/120    avg_loss:0.019, val_acc:0.970]
Epoch [73/120    avg_loss:0.018, val_acc:0.968]
Epoch [74/120    avg_loss:0.015, val_acc:0.970]
Epoch [75/120    avg_loss:0.016, val_acc:0.967]
Epoch [76/120    avg_loss:0.015, val_acc:0.971]
Epoch [77/120    avg_loss:0.015, val_acc:0.971]
Epoch [78/120    avg_loss:0.016, val_acc:0.972]
Epoch [79/120    avg_loss:0.017, val_acc:0.968]
Epoch [80/120    avg_loss:0.015, val_acc:0.969]
Epoch [81/120    avg_loss:0.015, val_acc:0.971]
Epoch [82/120    avg_loss:0.015, val_acc:0.971]
Epoch [83/120    avg_loss:0.013, val_acc:0.971]
Epoch [84/120    avg_loss:0.013, val_acc:0.970]
Epoch [85/120    avg_loss:0.017, val_acc:0.971]
Epoch [86/120    avg_loss:0.013, val_acc:0.971]
Epoch [87/120    avg_loss:0.012, val_acc:0.972]
Epoch [88/120    avg_loss:0.012, val_acc:0.971]
Epoch [89/120    avg_loss:0.016, val_acc:0.971]
Epoch [90/120    avg_loss:0.014, val_acc:0.971]
Epoch [91/120    avg_loss:0.014, val_acc:0.972]
Epoch [92/120    avg_loss:0.014, val_acc:0.972]
Epoch [93/120    avg_loss:0.011, val_acc:0.972]
Epoch [94/120    avg_loss:0.013, val_acc:0.974]
Epoch [95/120    avg_loss:0.013, val_acc:0.974]
Epoch [96/120    avg_loss:0.015, val_acc:0.969]
Epoch [97/120    avg_loss:0.012, val_acc:0.972]
Epoch [98/120    avg_loss:0.016, val_acc:0.974]
Epoch [99/120    avg_loss:0.012, val_acc:0.975]
Epoch [100/120    avg_loss:0.013, val_acc:0.970]
Epoch [101/120    avg_loss:0.013, val_acc:0.973]
Epoch [102/120    avg_loss:0.012, val_acc:0.974]
Epoch [103/120    avg_loss:0.015, val_acc:0.971]
Epoch [104/120    avg_loss:0.013, val_acc:0.971]
Epoch [105/120    avg_loss:0.011, val_acc:0.973]
Epoch [106/120    avg_loss:0.014, val_acc:0.971]
Epoch [107/120    avg_loss:0.011, val_acc:0.972]
Epoch [108/120    avg_loss:0.015, val_acc:0.974]
Epoch [109/120    avg_loss:0.011, val_acc:0.975]
Epoch [110/120    avg_loss:0.012, val_acc:0.971]
Epoch [111/120    avg_loss:0.013, val_acc:0.970]
Epoch [112/120    avg_loss:0.012, val_acc:0.971]
Epoch [113/120    avg_loss:0.010, val_acc:0.971]
Epoch [114/120    avg_loss:0.010, val_acc:0.972]
Epoch [115/120    avg_loss:0.014, val_acc:0.972]
Epoch [116/120    avg_loss:0.010, val_acc:0.974]
Epoch [117/120    avg_loss:0.010, val_acc:0.974]
Epoch [118/120    avg_loss:0.014, val_acc:0.976]
Epoch [119/120    avg_loss:0.015, val_acc:0.971]
Epoch [120/120    avg_loss:0.011, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6348     0     0     0     0     1     7    71     5]
 [    0     0 17838     0   127     0   125     0     0     0]
 [    0     8     0  1852     0     0     0     0   176     0]
 [    0    27     4     0  2911     0    20     0     6     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    78     0     4     0  4796     0     0     0]
 [    0     8     0     0     0     0     0  1274     6     2]
 [    0    43     0    69    44     0     0     0  3415     0]
 [    0     1     0     0     1    17     0     0     0   900]]

Accuracy:
97.94182151206228

F1 scores:
[       nan 0.98671019 0.9907248  0.93606267 0.96088463 0.99352874
 0.97678208 0.99105406 0.94271912 0.98360656]

Kappa:
0.9727753534271307
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f39dba5bb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.089, val_acc:0.083]
Epoch [2/120    avg_loss:1.810, val_acc:0.137]
Epoch [3/120    avg_loss:1.590, val_acc:0.225]
Epoch [4/120    avg_loss:1.444, val_acc:0.296]
Epoch [5/120    avg_loss:1.339, val_acc:0.340]
Epoch [6/120    avg_loss:1.253, val_acc:0.387]
Epoch [7/120    avg_loss:1.118, val_acc:0.395]
Epoch [8/120    avg_loss:1.003, val_acc:0.469]
Epoch [9/120    avg_loss:0.857, val_acc:0.605]
Epoch [10/120    avg_loss:0.703, val_acc:0.642]
Epoch [11/120    avg_loss:0.590, val_acc:0.689]
Epoch [12/120    avg_loss:0.549, val_acc:0.734]
Epoch [13/120    avg_loss:0.473, val_acc:0.735]
Epoch [14/120    avg_loss:0.426, val_acc:0.768]
Epoch [15/120    avg_loss:0.369, val_acc:0.810]
Epoch [16/120    avg_loss:0.327, val_acc:0.795]
Epoch [17/120    avg_loss:0.305, val_acc:0.886]
Epoch [18/120    avg_loss:0.262, val_acc:0.923]
Epoch [19/120    avg_loss:0.252, val_acc:0.918]
Epoch [20/120    avg_loss:0.204, val_acc:0.909]
Epoch [21/120    avg_loss:0.209, val_acc:0.904]
Epoch [22/120    avg_loss:0.217, val_acc:0.873]
Epoch [23/120    avg_loss:0.201, val_acc:0.931]
Epoch [24/120    avg_loss:0.170, val_acc:0.923]
Epoch [25/120    avg_loss:0.189, val_acc:0.915]
Epoch [26/120    avg_loss:0.182, val_acc:0.855]
Epoch [27/120    avg_loss:0.140, val_acc:0.935]
Epoch [28/120    avg_loss:0.121, val_acc:0.954]
Epoch [29/120    avg_loss:0.146, val_acc:0.917]
Epoch [30/120    avg_loss:0.120, val_acc:0.901]
Epoch [31/120    avg_loss:0.118, val_acc:0.931]
Epoch [32/120    avg_loss:0.141, val_acc:0.946]
Epoch [33/120    avg_loss:0.091, val_acc:0.974]
Epoch [34/120    avg_loss:0.075, val_acc:0.928]
Epoch [35/120    avg_loss:0.076, val_acc:0.921]
Epoch [36/120    avg_loss:0.116, val_acc:0.939]
Epoch [37/120    avg_loss:0.068, val_acc:0.949]
Epoch [38/120    avg_loss:0.062, val_acc:0.953]
Epoch [39/120    avg_loss:0.072, val_acc:0.953]
Epoch [40/120    avg_loss:0.070, val_acc:0.975]
Epoch [41/120    avg_loss:0.058, val_acc:0.980]
Epoch [42/120    avg_loss:0.063, val_acc:0.958]
Epoch [43/120    avg_loss:0.064, val_acc:0.965]
Epoch [44/120    avg_loss:0.048, val_acc:0.972]
Epoch [45/120    avg_loss:0.052, val_acc:0.961]
Epoch [46/120    avg_loss:0.079, val_acc:0.950]
Epoch [47/120    avg_loss:0.053, val_acc:0.980]
Epoch [48/120    avg_loss:0.060, val_acc:0.950]
Epoch [49/120    avg_loss:0.077, val_acc:0.956]
Epoch [50/120    avg_loss:0.046, val_acc:0.971]
Epoch [51/120    avg_loss:0.032, val_acc:0.977]
Epoch [52/120    avg_loss:0.031, val_acc:0.975]
Epoch [53/120    avg_loss:0.061, val_acc:0.970]
Epoch [54/120    avg_loss:0.049, val_acc:0.971]
Epoch [55/120    avg_loss:0.045, val_acc:0.983]
Epoch [56/120    avg_loss:0.024, val_acc:0.981]
Epoch [57/120    avg_loss:0.041, val_acc:0.983]
Epoch [58/120    avg_loss:0.028, val_acc:0.973]
Epoch [59/120    avg_loss:0.023, val_acc:0.976]
Epoch [60/120    avg_loss:0.018, val_acc:0.985]
Epoch [61/120    avg_loss:0.014, val_acc:0.971]
Epoch [62/120    avg_loss:0.026, val_acc:0.981]
Epoch [63/120    avg_loss:0.013, val_acc:0.984]
Epoch [64/120    avg_loss:0.033, val_acc:0.970]
Epoch [65/120    avg_loss:0.023, val_acc:0.984]
Epoch [66/120    avg_loss:0.021, val_acc:0.980]
Epoch [67/120    avg_loss:0.024, val_acc:0.958]
Epoch [68/120    avg_loss:0.015, val_acc:0.984]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.017, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.987]
Epoch [72/120    avg_loss:0.019, val_acc:0.981]
Epoch [73/120    avg_loss:0.012, val_acc:0.987]
Epoch [74/120    avg_loss:0.011, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.011, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.012, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.989]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.991]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     4     0     0     0     2    16    19]
 [    0     0 18062     0    24     0     0     0     4     0]
 [    0    19     0  1916     0     0     0     0   101     0]
 [    0    11    18     0  2903     0    28     0     8     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    67     0     0     0  4811     0     0     0]
 [    0    20     0     0     0     0     0  1268     0     2]
 [    0    49     2    64    15     0     3     0  3438     0]
 [    0     1     3     0    14    15     0     0     0   886]]

Accuracy:
98.76364688019666

F1 scores:
[       nan 0.98908922 0.99674411 0.95323383 0.9794197  0.99428571
 0.9899177  0.990625   0.96329504 0.96830601]

Kappa:
0.9836031352822021
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99aac50b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.094, val_acc:0.158]
Epoch [2/120    avg_loss:1.839, val_acc:0.175]
Epoch [3/120    avg_loss:1.586, val_acc:0.228]
Epoch [4/120    avg_loss:1.354, val_acc:0.362]
Epoch [5/120    avg_loss:1.145, val_acc:0.415]
Epoch [6/120    avg_loss:0.997, val_acc:0.443]
Epoch [7/120    avg_loss:0.864, val_acc:0.473]
Epoch [8/120    avg_loss:0.774, val_acc:0.498]
Epoch [9/120    avg_loss:0.711, val_acc:0.528]
Epoch [10/120    avg_loss:0.595, val_acc:0.560]
Epoch [11/120    avg_loss:0.525, val_acc:0.589]
Epoch [12/120    avg_loss:0.467, val_acc:0.687]
Epoch [13/120    avg_loss:0.472, val_acc:0.693]
Epoch [14/120    avg_loss:0.410, val_acc:0.757]
Epoch [15/120    avg_loss:0.378, val_acc:0.822]
Epoch [16/120    avg_loss:0.351, val_acc:0.828]
Epoch [17/120    avg_loss:0.362, val_acc:0.812]
Epoch [18/120    avg_loss:0.303, val_acc:0.880]
Epoch [19/120    avg_loss:0.283, val_acc:0.816]
Epoch [20/120    avg_loss:0.274, val_acc:0.875]
Epoch [21/120    avg_loss:0.273, val_acc:0.867]
Epoch [22/120    avg_loss:0.225, val_acc:0.892]
Epoch [23/120    avg_loss:0.201, val_acc:0.900]
Epoch [24/120    avg_loss:0.200, val_acc:0.931]
Epoch [25/120    avg_loss:0.174, val_acc:0.904]
Epoch [26/120    avg_loss:0.172, val_acc:0.932]
Epoch [27/120    avg_loss:0.138, val_acc:0.938]
Epoch [28/120    avg_loss:0.170, val_acc:0.853]
Epoch [29/120    avg_loss:0.155, val_acc:0.952]
Epoch [30/120    avg_loss:0.150, val_acc:0.932]
Epoch [31/120    avg_loss:0.135, val_acc:0.949]
Epoch [32/120    avg_loss:0.131, val_acc:0.949]
Epoch [33/120    avg_loss:0.124, val_acc:0.955]
Epoch [34/120    avg_loss:0.113, val_acc:0.958]
Epoch [35/120    avg_loss:0.116, val_acc:0.919]
Epoch [36/120    avg_loss:0.111, val_acc:0.959]
Epoch [37/120    avg_loss:0.085, val_acc:0.940]
Epoch [38/120    avg_loss:0.079, val_acc:0.952]
Epoch [39/120    avg_loss:0.085, val_acc:0.968]
Epoch [40/120    avg_loss:0.067, val_acc:0.960]
Epoch [41/120    avg_loss:0.077, val_acc:0.951]
Epoch [42/120    avg_loss:0.097, val_acc:0.960]
Epoch [43/120    avg_loss:0.096, val_acc:0.939]
Epoch [44/120    avg_loss:0.060, val_acc:0.963]
Epoch [45/120    avg_loss:0.114, val_acc:0.945]
Epoch [46/120    avg_loss:0.110, val_acc:0.945]
Epoch [47/120    avg_loss:0.097, val_acc:0.942]
Epoch [48/120    avg_loss:0.069, val_acc:0.953]
Epoch [49/120    avg_loss:0.068, val_acc:0.953]
Epoch [50/120    avg_loss:0.076, val_acc:0.950]
Epoch [51/120    avg_loss:0.052, val_acc:0.974]
Epoch [52/120    avg_loss:0.047, val_acc:0.971]
Epoch [53/120    avg_loss:0.041, val_acc:0.967]
Epoch [54/120    avg_loss:0.062, val_acc:0.962]
Epoch [55/120    avg_loss:0.038, val_acc:0.972]
Epoch [56/120    avg_loss:0.049, val_acc:0.973]
Epoch [57/120    avg_loss:0.054, val_acc:0.973]
Epoch [58/120    avg_loss:0.058, val_acc:0.963]
Epoch [59/120    avg_loss:0.067, val_acc:0.965]
Epoch [60/120    avg_loss:0.059, val_acc:0.967]
Epoch [61/120    avg_loss:0.034, val_acc:0.965]
Epoch [62/120    avg_loss:0.026, val_acc:0.973]
Epoch [63/120    avg_loss:0.071, val_acc:0.947]
Epoch [64/120    avg_loss:0.048, val_acc:0.950]
Epoch [65/120    avg_loss:0.035, val_acc:0.972]
Epoch [66/120    avg_loss:0.026, val_acc:0.974]
Epoch [67/120    avg_loss:0.023, val_acc:0.976]
Epoch [68/120    avg_loss:0.024, val_acc:0.975]
Epoch [69/120    avg_loss:0.020, val_acc:0.976]
Epoch [70/120    avg_loss:0.019, val_acc:0.976]
Epoch [71/120    avg_loss:0.020, val_acc:0.978]
Epoch [72/120    avg_loss:0.020, val_acc:0.978]
Epoch [73/120    avg_loss:0.019, val_acc:0.976]
Epoch [74/120    avg_loss:0.019, val_acc:0.978]
Epoch [75/120    avg_loss:0.017, val_acc:0.978]
Epoch [76/120    avg_loss:0.019, val_acc:0.977]
Epoch [77/120    avg_loss:0.016, val_acc:0.978]
Epoch [78/120    avg_loss:0.018, val_acc:0.973]
Epoch [79/120    avg_loss:0.018, val_acc:0.977]
Epoch [80/120    avg_loss:0.016, val_acc:0.977]
Epoch [81/120    avg_loss:0.020, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.977]
Epoch [83/120    avg_loss:0.014, val_acc:0.978]
Epoch [84/120    avg_loss:0.015, val_acc:0.976]
Epoch [85/120    avg_loss:0.017, val_acc:0.977]
Epoch [86/120    avg_loss:0.019, val_acc:0.976]
Epoch [87/120    avg_loss:0.018, val_acc:0.977]
Epoch [88/120    avg_loss:0.020, val_acc:0.973]
Epoch [89/120    avg_loss:0.016, val_acc:0.976]
Epoch [90/120    avg_loss:0.015, val_acc:0.977]
Epoch [91/120    avg_loss:0.016, val_acc:0.977]
Epoch [92/120    avg_loss:0.017, val_acc:0.976]
Epoch [93/120    avg_loss:0.014, val_acc:0.977]
Epoch [94/120    avg_loss:0.015, val_acc:0.977]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.014, val_acc:0.975]
Epoch [97/120    avg_loss:0.016, val_acc:0.975]
Epoch [98/120    avg_loss:0.016, val_acc:0.976]
Epoch [99/120    avg_loss:0.014, val_acc:0.976]
Epoch [100/120    avg_loss:0.016, val_acc:0.976]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.020, val_acc:0.977]
Epoch [103/120    avg_loss:0.014, val_acc:0.977]
Epoch [104/120    avg_loss:0.017, val_acc:0.977]
Epoch [105/120    avg_loss:0.013, val_acc:0.977]
Epoch [106/120    avg_loss:0.015, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.976]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.013, val_acc:0.977]
Epoch [110/120    avg_loss:0.015, val_acc:0.977]
Epoch [111/120    avg_loss:0.016, val_acc:0.977]
Epoch [112/120    avg_loss:0.016, val_acc:0.977]
Epoch [113/120    avg_loss:0.017, val_acc:0.977]
Epoch [114/120    avg_loss:0.013, val_acc:0.977]
Epoch [115/120    avg_loss:0.018, val_acc:0.977]
Epoch [116/120    avg_loss:0.013, val_acc:0.977]
Epoch [117/120    avg_loss:0.015, val_acc:0.977]
Epoch [118/120    avg_loss:0.019, val_acc:0.977]
Epoch [119/120    avg_loss:0.014, val_acc:0.977]
Epoch [120/120    avg_loss:0.015, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     1     0     0     1    19     0]
 [    0     2 17985     0    25     0    77     0     1     0]
 [    0     6     0  1879     0     0     0     0   151     0]
 [    0    16     3     5  2940     0     5     0     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4869     0     7     0]
 [    0    21     0     0     0    10     0  1257     2     0]
 [    0    17     1    53    50     0     2     0  3448     0]
 [    0     0     0     0     0     4     0     0     0   915]]

Accuracy:
98.83353818716411

F1 scores:
[       nan 0.99356838 0.99692359 0.94588472 0.98196393 0.99466463
 0.99054013 0.9866562  0.95764477 0.9972752 ]

Kappa:
0.9845561238579763
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbfe6125be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.197, val_acc:0.195]
Epoch [2/120    avg_loss:1.903, val_acc:0.199]
Epoch [3/120    avg_loss:1.638, val_acc:0.343]
Epoch [4/120    avg_loss:1.436, val_acc:0.393]
Epoch [5/120    avg_loss:1.278, val_acc:0.426]
Epoch [6/120    avg_loss:1.077, val_acc:0.529]
Epoch [7/120    avg_loss:0.959, val_acc:0.713]
Epoch [8/120    avg_loss:0.773, val_acc:0.759]
Epoch [9/120    avg_loss:0.662, val_acc:0.783]
Epoch [10/120    avg_loss:0.519, val_acc:0.776]
Epoch [11/120    avg_loss:0.467, val_acc:0.778]
Epoch [12/120    avg_loss:0.526, val_acc:0.756]
Epoch [13/120    avg_loss:0.437, val_acc:0.772]
Epoch [14/120    avg_loss:0.383, val_acc:0.790]
Epoch [15/120    avg_loss:0.325, val_acc:0.824]
Epoch [16/120    avg_loss:0.327, val_acc:0.779]
Epoch [17/120    avg_loss:0.293, val_acc:0.848]
Epoch [18/120    avg_loss:0.318, val_acc:0.826]
Epoch [19/120    avg_loss:0.262, val_acc:0.834]
Epoch [20/120    avg_loss:0.219, val_acc:0.902]
Epoch [21/120    avg_loss:0.220, val_acc:0.908]
Epoch [22/120    avg_loss:0.247, val_acc:0.915]
Epoch [23/120    avg_loss:0.200, val_acc:0.898]
Epoch [24/120    avg_loss:0.175, val_acc:0.921]
Epoch [25/120    avg_loss:0.160, val_acc:0.938]
Epoch [26/120    avg_loss:0.154, val_acc:0.946]
Epoch [27/120    avg_loss:0.157, val_acc:0.891]
Epoch [28/120    avg_loss:0.150, val_acc:0.918]
Epoch [29/120    avg_loss:0.140, val_acc:0.948]
Epoch [30/120    avg_loss:0.148, val_acc:0.937]
Epoch [31/120    avg_loss:0.120, val_acc:0.927]
Epoch [32/120    avg_loss:0.158, val_acc:0.904]
Epoch [33/120    avg_loss:0.121, val_acc:0.960]
Epoch [34/120    avg_loss:0.110, val_acc:0.949]
Epoch [35/120    avg_loss:0.112, val_acc:0.926]
Epoch [36/120    avg_loss:0.108, val_acc:0.935]
Epoch [37/120    avg_loss:0.104, val_acc:0.932]
Epoch [38/120    avg_loss:0.075, val_acc:0.954]
Epoch [39/120    avg_loss:0.078, val_acc:0.963]
Epoch [40/120    avg_loss:0.073, val_acc:0.955]
Epoch [41/120    avg_loss:0.065, val_acc:0.910]
Epoch [42/120    avg_loss:0.196, val_acc:0.947]
Epoch [43/120    avg_loss:0.127, val_acc:0.946]
Epoch [44/120    avg_loss:0.093, val_acc:0.958]
Epoch [45/120    avg_loss:0.144, val_acc:0.922]
Epoch [46/120    avg_loss:0.097, val_acc:0.954]
Epoch [47/120    avg_loss:0.089, val_acc:0.958]
Epoch [48/120    avg_loss:0.073, val_acc:0.942]
Epoch [49/120    avg_loss:0.099, val_acc:0.944]
Epoch [50/120    avg_loss:0.076, val_acc:0.936]
Epoch [51/120    avg_loss:0.098, val_acc:0.957]
Epoch [52/120    avg_loss:0.067, val_acc:0.953]
Epoch [53/120    avg_loss:0.060, val_acc:0.962]
Epoch [54/120    avg_loss:0.043, val_acc:0.963]
Epoch [55/120    avg_loss:0.043, val_acc:0.967]
Epoch [56/120    avg_loss:0.041, val_acc:0.967]
Epoch [57/120    avg_loss:0.037, val_acc:0.969]
Epoch [58/120    avg_loss:0.040, val_acc:0.966]
Epoch [59/120    avg_loss:0.033, val_acc:0.971]
Epoch [60/120    avg_loss:0.035, val_acc:0.973]
Epoch [61/120    avg_loss:0.032, val_acc:0.971]
Epoch [62/120    avg_loss:0.033, val_acc:0.969]
Epoch [63/120    avg_loss:0.031, val_acc:0.971]
Epoch [64/120    avg_loss:0.030, val_acc:0.971]
Epoch [65/120    avg_loss:0.033, val_acc:0.971]
Epoch [66/120    avg_loss:0.034, val_acc:0.969]
Epoch [67/120    avg_loss:0.032, val_acc:0.970]
Epoch [68/120    avg_loss:0.028, val_acc:0.972]
Epoch [69/120    avg_loss:0.028, val_acc:0.976]
Epoch [70/120    avg_loss:0.024, val_acc:0.973]
Epoch [71/120    avg_loss:0.031, val_acc:0.972]
Epoch [72/120    avg_loss:0.030, val_acc:0.973]
Epoch [73/120    avg_loss:0.027, val_acc:0.969]
Epoch [74/120    avg_loss:0.031, val_acc:0.974]
Epoch [75/120    avg_loss:0.024, val_acc:0.974]
Epoch [76/120    avg_loss:0.026, val_acc:0.971]
Epoch [77/120    avg_loss:0.026, val_acc:0.973]
Epoch [78/120    avg_loss:0.027, val_acc:0.976]
Epoch [79/120    avg_loss:0.024, val_acc:0.977]
Epoch [80/120    avg_loss:0.030, val_acc:0.978]
Epoch [81/120    avg_loss:0.027, val_acc:0.975]
Epoch [82/120    avg_loss:0.026, val_acc:0.973]
Epoch [83/120    avg_loss:0.026, val_acc:0.975]
Epoch [84/120    avg_loss:0.020, val_acc:0.980]
Epoch [85/120    avg_loss:0.026, val_acc:0.978]
Epoch [86/120    avg_loss:0.024, val_acc:0.973]
Epoch [87/120    avg_loss:0.026, val_acc:0.973]
Epoch [88/120    avg_loss:0.027, val_acc:0.976]
Epoch [89/120    avg_loss:0.024, val_acc:0.978]
Epoch [90/120    avg_loss:0.022, val_acc:0.978]
Epoch [91/120    avg_loss:0.024, val_acc:0.977]
Epoch [92/120    avg_loss:0.023, val_acc:0.980]
Epoch [93/120    avg_loss:0.023, val_acc:0.979]
Epoch [94/120    avg_loss:0.025, val_acc:0.978]
Epoch [95/120    avg_loss:0.024, val_acc:0.979]
Epoch [96/120    avg_loss:0.019, val_acc:0.978]
Epoch [97/120    avg_loss:0.025, val_acc:0.978]
Epoch [98/120    avg_loss:0.023, val_acc:0.982]
Epoch [99/120    avg_loss:0.023, val_acc:0.979]
Epoch [100/120    avg_loss:0.022, val_acc:0.981]
Epoch [101/120    avg_loss:0.024, val_acc:0.978]
Epoch [102/120    avg_loss:0.020, val_acc:0.981]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.022, val_acc:0.977]
Epoch [105/120    avg_loss:0.024, val_acc:0.977]
Epoch [106/120    avg_loss:0.021, val_acc:0.979]
Epoch [107/120    avg_loss:0.022, val_acc:0.978]
Epoch [108/120    avg_loss:0.025, val_acc:0.979]
Epoch [109/120    avg_loss:0.021, val_acc:0.978]
Epoch [110/120    avg_loss:0.021, val_acc:0.975]
Epoch [111/120    avg_loss:0.023, val_acc:0.979]
Epoch [112/120    avg_loss:0.022, val_acc:0.979]
Epoch [113/120    avg_loss:0.017, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.018, val_acc:0.979]
Epoch [116/120    avg_loss:0.019, val_acc:0.979]
Epoch [117/120    avg_loss:0.017, val_acc:0.979]
Epoch [118/120    avg_loss:0.015, val_acc:0.979]
Epoch [119/120    avg_loss:0.017, val_acc:0.980]
Epoch [120/120    avg_loss:0.022, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6298     0     6     0     0     1    59    48    20]
 [    0     0 18032     0    34     0    21     0     3     0]
 [    0     5     1  1968     0     0     0     0    62     0]
 [    0    18    11     3  2924     0    10     0     0     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   180     0     0     0  4698     0     0     0]
 [    0     5     0     0     0     0     0  1280     1     4]
 [    0    29    33   120    24     0     1     0  3363     1]
 [    0     1     0     0     0    13     0     0     0   905]]

Accuracy:
98.26476755115321

F1 scores:
[       nan 0.98498592 0.99221394 0.95233487 0.98219684 0.99504384
 0.97783328 0.97375428 0.95431328 0.97574124]

Kappa:
0.9769764813968619
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc553555b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.096, val_acc:0.093]
Epoch [2/120    avg_loss:1.814, val_acc:0.121]
Epoch [3/120    avg_loss:1.618, val_acc:0.147]
Epoch [4/120    avg_loss:1.423, val_acc:0.458]
Epoch [5/120    avg_loss:1.293, val_acc:0.498]
Epoch [6/120    avg_loss:1.136, val_acc:0.591]
Epoch [7/120    avg_loss:0.963, val_acc:0.625]
Epoch [8/120    avg_loss:0.836, val_acc:0.703]
Epoch [9/120    avg_loss:0.707, val_acc:0.753]
Epoch [10/120    avg_loss:0.707, val_acc:0.699]
Epoch [11/120    avg_loss:0.584, val_acc:0.770]
Epoch [12/120    avg_loss:0.464, val_acc:0.775]
Epoch [13/120    avg_loss:0.404, val_acc:0.830]
Epoch [14/120    avg_loss:0.371, val_acc:0.870]
Epoch [15/120    avg_loss:0.346, val_acc:0.837]
Epoch [16/120    avg_loss:0.334, val_acc:0.820]
Epoch [17/120    avg_loss:0.307, val_acc:0.914]
Epoch [18/120    avg_loss:0.243, val_acc:0.898]
Epoch [19/120    avg_loss:0.210, val_acc:0.921]
Epoch [20/120    avg_loss:0.220, val_acc:0.931]
Epoch [21/120    avg_loss:0.191, val_acc:0.930]
Epoch [22/120    avg_loss:0.172, val_acc:0.929]
Epoch [23/120    avg_loss:0.153, val_acc:0.946]
Epoch [24/120    avg_loss:0.155, val_acc:0.936]
Epoch [25/120    avg_loss:0.116, val_acc:0.914]
Epoch [26/120    avg_loss:0.158, val_acc:0.937]
Epoch [27/120    avg_loss:0.134, val_acc:0.950]
Epoch [28/120    avg_loss:0.131, val_acc:0.954]
Epoch [29/120    avg_loss:0.125, val_acc:0.931]
Epoch [30/120    avg_loss:0.100, val_acc:0.956]
Epoch [31/120    avg_loss:0.101, val_acc:0.951]
Epoch [32/120    avg_loss:0.081, val_acc:0.938]
Epoch [33/120    avg_loss:0.109, val_acc:0.955]
Epoch [34/120    avg_loss:0.078, val_acc:0.948]
Epoch [35/120    avg_loss:0.086, val_acc:0.960]
Epoch [36/120    avg_loss:0.056, val_acc:0.967]
Epoch [37/120    avg_loss:0.067, val_acc:0.944]
Epoch [38/120    avg_loss:0.088, val_acc:0.930]
Epoch [39/120    avg_loss:0.061, val_acc:0.967]
Epoch [40/120    avg_loss:0.047, val_acc:0.960]
Epoch [41/120    avg_loss:0.074, val_acc:0.960]
Epoch [42/120    avg_loss:0.047, val_acc:0.967]
Epoch [43/120    avg_loss:0.058, val_acc:0.966]
Epoch [44/120    avg_loss:0.073, val_acc:0.945]
Epoch [45/120    avg_loss:0.072, val_acc:0.960]
Epoch [46/120    avg_loss:0.076, val_acc:0.951]
Epoch [47/120    avg_loss:0.062, val_acc:0.967]
Epoch [48/120    avg_loss:0.055, val_acc:0.972]
Epoch [49/120    avg_loss:0.032, val_acc:0.976]
Epoch [50/120    avg_loss:0.040, val_acc:0.969]
Epoch [51/120    avg_loss:0.033, val_acc:0.929]
Epoch [52/120    avg_loss:0.073, val_acc:0.964]
Epoch [53/120    avg_loss:0.040, val_acc:0.932]
Epoch [54/120    avg_loss:0.064, val_acc:0.965]
Epoch [55/120    avg_loss:0.032, val_acc:0.979]
Epoch [56/120    avg_loss:0.032, val_acc:0.973]
Epoch [57/120    avg_loss:0.030, val_acc:0.963]
Epoch [58/120    avg_loss:0.024, val_acc:0.977]
Epoch [59/120    avg_loss:0.037, val_acc:0.971]
Epoch [60/120    avg_loss:0.023, val_acc:0.970]
Epoch [61/120    avg_loss:0.024, val_acc:0.970]
Epoch [62/120    avg_loss:0.028, val_acc:0.956]
Epoch [63/120    avg_loss:0.051, val_acc:0.965]
Epoch [64/120    avg_loss:0.058, val_acc:0.961]
Epoch [65/120    avg_loss:0.057, val_acc:0.967]
Epoch [66/120    avg_loss:0.044, val_acc:0.972]
Epoch [67/120    avg_loss:0.026, val_acc:0.973]
Epoch [68/120    avg_loss:0.018, val_acc:0.980]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.044, val_acc:0.962]
Epoch [72/120    avg_loss:0.059, val_acc:0.962]
Epoch [73/120    avg_loss:0.155, val_acc:0.962]
Epoch [74/120    avg_loss:0.078, val_acc:0.952]
Epoch [75/120    avg_loss:0.046, val_acc:0.953]
Epoch [76/120    avg_loss:0.036, val_acc:0.969]
Epoch [77/120    avg_loss:0.021, val_acc:0.977]
Epoch [78/120    avg_loss:0.020, val_acc:0.975]
Epoch [79/120    avg_loss:0.021, val_acc:0.969]
Epoch [80/120    avg_loss:0.018, val_acc:0.973]
Epoch [81/120    avg_loss:0.022, val_acc:0.975]
Epoch [82/120    avg_loss:0.019, val_acc:0.964]
Epoch [83/120    avg_loss:0.032, val_acc:0.973]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.016, val_acc:0.980]
Epoch [87/120    avg_loss:0.015, val_acc:0.979]
Epoch [88/120    avg_loss:0.009, val_acc:0.980]
Epoch [89/120    avg_loss:0.015, val_acc:0.980]
Epoch [90/120    avg_loss:0.013, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.980]
Epoch [98/120    avg_loss:0.008, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.008, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.011, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     2     1     0     0     0    58     4]
 [    0     0 18080     0     8     0     1     0     1     0]
 [    0     8     0  1933     0     0     0     0    95     0]
 [    0    13    16     0  2932     0     7     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    36     0     2     0  4836     0     4     0]
 [    0     7     0     0     0     0     0  1282     1     0]
 [    0    26     8    65    37     0     0     0  3435     0]
 [    0     0     0     0     0     6     0     0     0   913]]

Accuracy:
99.01188152218447

F1 scores:
[       nan 0.99074146 0.9980679  0.95787909 0.98521505 0.99770642
 0.99485703 0.99688958 0.95869383 0.99293094]

Kappa:
0.9868997503005376
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8290d83b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.075, val_acc:0.174]
Epoch [2/120    avg_loss:1.828, val_acc:0.525]
Epoch [3/120    avg_loss:1.617, val_acc:0.591]
Epoch [4/120    avg_loss:1.403, val_acc:0.595]
Epoch [5/120    avg_loss:1.230, val_acc:0.640]
Epoch [6/120    avg_loss:1.108, val_acc:0.706]
Epoch [7/120    avg_loss:0.995, val_acc:0.723]
Epoch [8/120    avg_loss:0.842, val_acc:0.758]
Epoch [9/120    avg_loss:0.731, val_acc:0.710]
Epoch [10/120    avg_loss:0.663, val_acc:0.659]
Epoch [11/120    avg_loss:0.576, val_acc:0.698]
Epoch [12/120    avg_loss:0.513, val_acc:0.714]
Epoch [13/120    avg_loss:0.418, val_acc:0.697]
Epoch [14/120    avg_loss:0.368, val_acc:0.804]
Epoch [15/120    avg_loss:0.338, val_acc:0.780]
Epoch [16/120    avg_loss:0.352, val_acc:0.838]
Epoch [17/120    avg_loss:0.315, val_acc:0.741]
Epoch [18/120    avg_loss:0.285, val_acc:0.867]
Epoch [19/120    avg_loss:0.244, val_acc:0.915]
Epoch [20/120    avg_loss:0.215, val_acc:0.917]
Epoch [21/120    avg_loss:0.214, val_acc:0.925]
Epoch [22/120    avg_loss:0.287, val_acc:0.909]
Epoch [23/120    avg_loss:0.211, val_acc:0.918]
Epoch [24/120    avg_loss:0.163, val_acc:0.931]
Epoch [25/120    avg_loss:0.142, val_acc:0.940]
Epoch [26/120    avg_loss:0.136, val_acc:0.951]
Epoch [27/120    avg_loss:0.125, val_acc:0.948]
Epoch [28/120    avg_loss:0.099, val_acc:0.932]
Epoch [29/120    avg_loss:0.092, val_acc:0.932]
Epoch [30/120    avg_loss:0.110, val_acc:0.892]
Epoch [31/120    avg_loss:0.170, val_acc:0.947]
Epoch [32/120    avg_loss:0.083, val_acc:0.962]
Epoch [33/120    avg_loss:0.089, val_acc:0.955]
Epoch [34/120    avg_loss:0.069, val_acc:0.958]
Epoch [35/120    avg_loss:0.111, val_acc:0.908]
Epoch [36/120    avg_loss:0.097, val_acc:0.907]
Epoch [37/120    avg_loss:0.136, val_acc:0.945]
Epoch [38/120    avg_loss:0.068, val_acc:0.959]
Epoch [39/120    avg_loss:0.069, val_acc:0.952]
Epoch [40/120    avg_loss:0.084, val_acc:0.947]
Epoch [41/120    avg_loss:1.574, val_acc:0.580]
Epoch [42/120    avg_loss:0.653, val_acc:0.655]
Epoch [43/120    avg_loss:0.477, val_acc:0.721]
Epoch [44/120    avg_loss:0.400, val_acc:0.761]
Epoch [45/120    avg_loss:0.339, val_acc:0.703]
Epoch [46/120    avg_loss:0.319, val_acc:0.811]
Epoch [47/120    avg_loss:0.278, val_acc:0.806]
Epoch [48/120    avg_loss:0.269, val_acc:0.808]
Epoch [49/120    avg_loss:0.258, val_acc:0.809]
Epoch [50/120    avg_loss:0.261, val_acc:0.818]
Epoch [51/120    avg_loss:0.266, val_acc:0.828]
Epoch [52/120    avg_loss:0.261, val_acc:0.835]
Epoch [53/120    avg_loss:0.267, val_acc:0.814]
Epoch [54/120    avg_loss:0.235, val_acc:0.840]
Epoch [55/120    avg_loss:0.240, val_acc:0.835]
Epoch [56/120    avg_loss:0.234, val_acc:0.828]
Epoch [57/120    avg_loss:0.226, val_acc:0.854]
Epoch [58/120    avg_loss:0.229, val_acc:0.859]
Epoch [59/120    avg_loss:0.218, val_acc:0.859]
Epoch [60/120    avg_loss:0.212, val_acc:0.857]
Epoch [61/120    avg_loss:0.223, val_acc:0.858]
Epoch [62/120    avg_loss:0.216, val_acc:0.858]
Epoch [63/120    avg_loss:0.209, val_acc:0.858]
Epoch [64/120    avg_loss:0.220, val_acc:0.859]
Epoch [65/120    avg_loss:0.225, val_acc:0.857]
Epoch [66/120    avg_loss:0.219, val_acc:0.858]
Epoch [67/120    avg_loss:0.230, val_acc:0.858]
Epoch [68/120    avg_loss:0.204, val_acc:0.857]
Epoch [69/120    avg_loss:0.210, val_acc:0.857]
Epoch [70/120    avg_loss:0.205, val_acc:0.858]
Epoch [71/120    avg_loss:0.217, val_acc:0.856]
Epoch [72/120    avg_loss:0.212, val_acc:0.856]
Epoch [73/120    avg_loss:0.245, val_acc:0.856]
Epoch [74/120    avg_loss:0.211, val_acc:0.856]
Epoch [75/120    avg_loss:0.213, val_acc:0.856]
Epoch [76/120    avg_loss:0.232, val_acc:0.855]
Epoch [77/120    avg_loss:0.200, val_acc:0.855]
Epoch [78/120    avg_loss:0.207, val_acc:0.856]
Epoch [79/120    avg_loss:0.210, val_acc:0.856]
Epoch [80/120    avg_loss:0.215, val_acc:0.856]
Epoch [81/120    avg_loss:0.208, val_acc:0.856]
Epoch [82/120    avg_loss:0.218, val_acc:0.856]
Epoch [83/120    avg_loss:0.212, val_acc:0.856]
Epoch [84/120    avg_loss:0.234, val_acc:0.858]
Epoch [85/120    avg_loss:0.211, val_acc:0.858]
Epoch [86/120    avg_loss:0.217, val_acc:0.858]
Epoch [87/120    avg_loss:0.210, val_acc:0.858]
Epoch [88/120    avg_loss:0.203, val_acc:0.858]
Epoch [89/120    avg_loss:0.211, val_acc:0.858]
Epoch [90/120    avg_loss:0.212, val_acc:0.858]
Epoch [91/120    avg_loss:0.195, val_acc:0.858]
Epoch [92/120    avg_loss:0.215, val_acc:0.858]
Epoch [93/120    avg_loss:0.203, val_acc:0.858]
Epoch [94/120    avg_loss:0.196, val_acc:0.858]
Epoch [95/120    avg_loss:0.221, val_acc:0.857]
Epoch [96/120    avg_loss:0.208, val_acc:0.857]
Epoch [97/120    avg_loss:0.225, val_acc:0.858]
Epoch [98/120    avg_loss:0.197, val_acc:0.858]
Epoch [99/120    avg_loss:0.216, val_acc:0.858]
Epoch [100/120    avg_loss:0.219, val_acc:0.858]
Epoch [101/120    avg_loss:0.205, val_acc:0.858]
Epoch [102/120    avg_loss:0.212, val_acc:0.858]
Epoch [103/120    avg_loss:0.228, val_acc:0.858]
Epoch [104/120    avg_loss:0.217, val_acc:0.858]
Epoch [105/120    avg_loss:0.200, val_acc:0.858]
Epoch [106/120    avg_loss:0.201, val_acc:0.858]
Epoch [107/120    avg_loss:0.220, val_acc:0.858]
Epoch [108/120    avg_loss:0.195, val_acc:0.858]
Epoch [109/120    avg_loss:0.215, val_acc:0.857]
Epoch [110/120    avg_loss:0.197, val_acc:0.857]
Epoch [111/120    avg_loss:0.195, val_acc:0.857]
Epoch [112/120    avg_loss:0.207, val_acc:0.857]
Epoch [113/120    avg_loss:0.201, val_acc:0.857]
Epoch [114/120    avg_loss:0.203, val_acc:0.857]
Epoch [115/120    avg_loss:0.199, val_acc:0.857]
Epoch [116/120    avg_loss:0.213, val_acc:0.857]
Epoch [117/120    avg_loss:0.215, val_acc:0.857]
Epoch [118/120    avg_loss:0.220, val_acc:0.857]
Epoch [119/120    avg_loss:0.216, val_acc:0.857]
Epoch [120/120    avg_loss:0.212, val_acc:0.857]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5967     0    39     8     0    25    16   319    58]
 [    0     0 14686     0    15     0  3389     0     0     0]
 [    0    18     0  1801     0     0     0     0   217     0]
 [    0    19   111     0  2811     0    27     0     0     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   483     0     4     0  4325     0    66     0]
 [    0    76     0     0     0     0     0  1197    17     0]
 [    0   209     6   221    47     0     7     0  3080     1]
 [    0     1     5     0    18     8     0     0     0   887]]

Accuracy:
86.9038151013424

F1 scores:
[       nan 0.93806005 0.87990174 0.87917989 0.95693617 0.99694423
 0.68374042 0.95645226 0.84731774 0.94917068]

Kappa:
0.8312955162647748
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0499ab1b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.166, val_acc:0.140]
Epoch [2/120    avg_loss:1.873, val_acc:0.141]
Epoch [3/120    avg_loss:1.642, val_acc:0.204]
Epoch [4/120    avg_loss:1.445, val_acc:0.315]
Epoch [5/120    avg_loss:1.300, val_acc:0.434]
Epoch [6/120    avg_loss:1.174, val_acc:0.416]
Epoch [7/120    avg_loss:1.005, val_acc:0.467]
Epoch [8/120    avg_loss:0.850, val_acc:0.478]
Epoch [9/120    avg_loss:0.735, val_acc:0.514]
Epoch [10/120    avg_loss:0.604, val_acc:0.573]
Epoch [11/120    avg_loss:0.541, val_acc:0.642]
Epoch [12/120    avg_loss:0.489, val_acc:0.651]
Epoch [13/120    avg_loss:0.503, val_acc:0.688]
Epoch [14/120    avg_loss:0.437, val_acc:0.706]
Epoch [15/120    avg_loss:0.395, val_acc:0.757]
Epoch [16/120    avg_loss:0.345, val_acc:0.821]
Epoch [17/120    avg_loss:0.330, val_acc:0.900]
Epoch [18/120    avg_loss:0.323, val_acc:0.870]
Epoch [19/120    avg_loss:0.265, val_acc:0.921]
Epoch [20/120    avg_loss:0.250, val_acc:0.924]
Epoch [21/120    avg_loss:0.240, val_acc:0.913]
Epoch [22/120    avg_loss:0.222, val_acc:0.892]
Epoch [23/120    avg_loss:0.191, val_acc:0.927]
Epoch [24/120    avg_loss:0.173, val_acc:0.909]
Epoch [25/120    avg_loss:0.161, val_acc:0.917]
Epoch [26/120    avg_loss:0.186, val_acc:0.919]
Epoch [27/120    avg_loss:0.181, val_acc:0.912]
Epoch [28/120    avg_loss:0.165, val_acc:0.906]
Epoch [29/120    avg_loss:0.128, val_acc:0.940]
Epoch [30/120    avg_loss:0.124, val_acc:0.940]
Epoch [31/120    avg_loss:0.138, val_acc:0.914]
Epoch [32/120    avg_loss:0.108, val_acc:0.932]
Epoch [33/120    avg_loss:0.103, val_acc:0.955]
Epoch [34/120    avg_loss:0.094, val_acc:0.953]
Epoch [35/120    avg_loss:0.107, val_acc:0.953]
Epoch [36/120    avg_loss:0.076, val_acc:0.959]
Epoch [37/120    avg_loss:0.084, val_acc:0.929]
Epoch [38/120    avg_loss:0.078, val_acc:0.959]
Epoch [39/120    avg_loss:0.092, val_acc:0.945]
Epoch [40/120    avg_loss:0.063, val_acc:0.943]
Epoch [41/120    avg_loss:0.050, val_acc:0.970]
Epoch [42/120    avg_loss:0.046, val_acc:0.963]
Epoch [43/120    avg_loss:0.057, val_acc:0.961]
Epoch [44/120    avg_loss:0.060, val_acc:0.965]
Epoch [45/120    avg_loss:0.074, val_acc:0.949]
Epoch [46/120    avg_loss:0.066, val_acc:0.942]
Epoch [47/120    avg_loss:0.066, val_acc:0.967]
Epoch [48/120    avg_loss:0.060, val_acc:0.961]
Epoch [49/120    avg_loss:0.049, val_acc:0.963]
Epoch [50/120    avg_loss:0.044, val_acc:0.973]
Epoch [51/120    avg_loss:0.056, val_acc:0.945]
Epoch [52/120    avg_loss:0.062, val_acc:0.966]
Epoch [53/120    avg_loss:0.042, val_acc:0.974]
Epoch [54/120    avg_loss:0.061, val_acc:0.968]
Epoch [55/120    avg_loss:0.057, val_acc:0.971]
Epoch [56/120    avg_loss:0.047, val_acc:0.969]
Epoch [57/120    avg_loss:0.032, val_acc:0.977]
Epoch [58/120    avg_loss:0.018, val_acc:0.970]
Epoch [59/120    avg_loss:0.061, val_acc:0.938]
Epoch [60/120    avg_loss:0.044, val_acc:0.954]
Epoch [61/120    avg_loss:0.027, val_acc:0.972]
Epoch [62/120    avg_loss:0.017, val_acc:0.981]
Epoch [63/120    avg_loss:0.016, val_acc:0.978]
Epoch [64/120    avg_loss:0.023, val_acc:0.968]
Epoch [65/120    avg_loss:0.021, val_acc:0.969]
Epoch [66/120    avg_loss:0.012, val_acc:0.974]
Epoch [67/120    avg_loss:0.023, val_acc:0.974]
Epoch [68/120    avg_loss:0.015, val_acc:0.972]
Epoch [69/120    avg_loss:0.020, val_acc:0.978]
Epoch [70/120    avg_loss:0.043, val_acc:0.968]
Epoch [71/120    avg_loss:0.077, val_acc:0.968]
Epoch [72/120    avg_loss:0.063, val_acc:0.966]
Epoch [73/120    avg_loss:0.023, val_acc:0.971]
Epoch [74/120    avg_loss:0.020, val_acc:0.977]
Epoch [75/120    avg_loss:0.015, val_acc:0.978]
Epoch [76/120    avg_loss:0.014, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.978]
Epoch [78/120    avg_loss:0.011, val_acc:0.978]
Epoch [79/120    avg_loss:0.011, val_acc:0.977]
Epoch [80/120    avg_loss:0.013, val_acc:0.978]
Epoch [81/120    avg_loss:0.011, val_acc:0.977]
Epoch [82/120    avg_loss:0.009, val_acc:0.978]
Epoch [83/120    avg_loss:0.013, val_acc:0.978]
Epoch [84/120    avg_loss:0.011, val_acc:0.978]
Epoch [85/120    avg_loss:0.010, val_acc:0.975]
Epoch [86/120    avg_loss:0.010, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.978]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.010, val_acc:0.978]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.009, val_acc:0.978]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.009, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.978]
Epoch [101/120    avg_loss:0.008, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.008, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.010, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.978]
Epoch [110/120    avg_loss:0.010, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     0     0     1     0     4     7    54    17]
 [    0     0 18059     0    21     0     6     0     4     0]
 [    0     0     0  1871     0     0     0     0   163     2]
 [    0    10     6     1  2941     0     8     0     3     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    57     0     1     0  4819     0     1     0]
 [    0     8     0     0     0     0     2  1274     5     1]
 [    0    19     0    68    38     0     2     0  3444     0]
 [    0     0     0     0    10     7     0     0     0   902]]

Accuracy:
98.72508615911117

F1 scores:
[       nan 0.99063817 0.99740418 0.94114688 0.98295455 0.99732518
 0.99166581 0.99105406 0.95072464 0.97830803]

Kappa:
0.9831014608544192
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd753d46b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.144, val_acc:0.246]
Epoch [2/120    avg_loss:1.841, val_acc:0.258]
Epoch [3/120    avg_loss:1.631, val_acc:0.273]
Epoch [4/120    avg_loss:1.458, val_acc:0.316]
Epoch [5/120    avg_loss:1.297, val_acc:0.357]
Epoch [6/120    avg_loss:1.132, val_acc:0.437]
Epoch [7/120    avg_loss:1.009, val_acc:0.482]
Epoch [8/120    avg_loss:0.881, val_acc:0.507]
Epoch [9/120    avg_loss:0.751, val_acc:0.662]
Epoch [10/120    avg_loss:0.669, val_acc:0.677]
Epoch [11/120    avg_loss:0.550, val_acc:0.736]
Epoch [12/120    avg_loss:0.512, val_acc:0.743]
Epoch [13/120    avg_loss:0.462, val_acc:0.765]
Epoch [14/120    avg_loss:0.368, val_acc:0.784]
Epoch [15/120    avg_loss:0.355, val_acc:0.787]
Epoch [16/120    avg_loss:0.315, val_acc:0.822]
Epoch [17/120    avg_loss:0.296, val_acc:0.815]
Epoch [18/120    avg_loss:0.330, val_acc:0.842]
Epoch [19/120    avg_loss:0.282, val_acc:0.850]
Epoch [20/120    avg_loss:0.238, val_acc:0.848]
Epoch [21/120    avg_loss:0.233, val_acc:0.834]
Epoch [22/120    avg_loss:0.226, val_acc:0.901]
Epoch [23/120    avg_loss:0.180, val_acc:0.921]
Epoch [24/120    avg_loss:0.167, val_acc:0.945]
Epoch [25/120    avg_loss:0.165, val_acc:0.920]
Epoch [26/120    avg_loss:0.170, val_acc:0.934]
Epoch [27/120    avg_loss:0.165, val_acc:0.846]
Epoch [28/120    avg_loss:0.176, val_acc:0.951]
Epoch [29/120    avg_loss:0.134, val_acc:0.925]
Epoch [30/120    avg_loss:0.132, val_acc:0.935]
Epoch [31/120    avg_loss:0.101, val_acc:0.955]
Epoch [32/120    avg_loss:0.097, val_acc:0.953]
Epoch [33/120    avg_loss:0.075, val_acc:0.951]
Epoch [34/120    avg_loss:0.087, val_acc:0.965]
Epoch [35/120    avg_loss:0.090, val_acc:0.966]
Epoch [36/120    avg_loss:0.085, val_acc:0.972]
Epoch [37/120    avg_loss:0.125, val_acc:0.928]
Epoch [38/120    avg_loss:0.141, val_acc:0.458]
Epoch [39/120    avg_loss:0.681, val_acc:0.884]
Epoch [40/120    avg_loss:0.204, val_acc:0.933]
Epoch [41/120    avg_loss:0.132, val_acc:0.951]
Epoch [42/120    avg_loss:0.113, val_acc:0.949]
Epoch [43/120    avg_loss:0.124, val_acc:0.946]
Epoch [44/120    avg_loss:0.138, val_acc:0.906]
Epoch [45/120    avg_loss:0.087, val_acc:0.958]
Epoch [46/120    avg_loss:0.104, val_acc:0.962]
Epoch [47/120    avg_loss:0.127, val_acc:0.954]
Epoch [48/120    avg_loss:0.085, val_acc:0.956]
Epoch [49/120    avg_loss:0.071, val_acc:0.954]
Epoch [50/120    avg_loss:0.052, val_acc:0.971]
Epoch [51/120    avg_loss:0.050, val_acc:0.973]
Epoch [52/120    avg_loss:0.045, val_acc:0.970]
Epoch [53/120    avg_loss:0.038, val_acc:0.972]
Epoch [54/120    avg_loss:0.034, val_acc:0.969]
Epoch [55/120    avg_loss:0.040, val_acc:0.972]
Epoch [56/120    avg_loss:0.041, val_acc:0.973]
Epoch [57/120    avg_loss:0.038, val_acc:0.970]
Epoch [58/120    avg_loss:0.044, val_acc:0.973]
Epoch [59/120    avg_loss:0.039, val_acc:0.971]
Epoch [60/120    avg_loss:0.036, val_acc:0.972]
Epoch [61/120    avg_loss:0.037, val_acc:0.971]
Epoch [62/120    avg_loss:0.031, val_acc:0.971]
Epoch [63/120    avg_loss:0.032, val_acc:0.971]
Epoch [64/120    avg_loss:0.036, val_acc:0.971]
Epoch [65/120    avg_loss:0.033, val_acc:0.973]
Epoch [66/120    avg_loss:0.032, val_acc:0.973]
Epoch [67/120    avg_loss:0.030, val_acc:0.972]
Epoch [68/120    avg_loss:0.027, val_acc:0.974]
Epoch [69/120    avg_loss:0.028, val_acc:0.972]
Epoch [70/120    avg_loss:0.032, val_acc:0.973]
Epoch [71/120    avg_loss:0.033, val_acc:0.970]
Epoch [72/120    avg_loss:0.029, val_acc:0.973]
Epoch [73/120    avg_loss:0.027, val_acc:0.975]
Epoch [74/120    avg_loss:0.027, val_acc:0.973]
Epoch [75/120    avg_loss:0.032, val_acc:0.974]
Epoch [76/120    avg_loss:0.025, val_acc:0.974]
Epoch [77/120    avg_loss:0.026, val_acc:0.973]
Epoch [78/120    avg_loss:0.028, val_acc:0.973]
Epoch [79/120    avg_loss:0.028, val_acc:0.976]
Epoch [80/120    avg_loss:0.027, val_acc:0.974]
Epoch [81/120    avg_loss:0.031, val_acc:0.973]
Epoch [82/120    avg_loss:0.026, val_acc:0.974]
Epoch [83/120    avg_loss:0.031, val_acc:0.975]
Epoch [84/120    avg_loss:0.026, val_acc:0.973]
Epoch [85/120    avg_loss:0.029, val_acc:0.972]
Epoch [86/120    avg_loss:0.023, val_acc:0.976]
Epoch [87/120    avg_loss:0.023, val_acc:0.973]
Epoch [88/120    avg_loss:0.024, val_acc:0.975]
Epoch [89/120    avg_loss:0.029, val_acc:0.974]
Epoch [90/120    avg_loss:0.024, val_acc:0.977]
Epoch [91/120    avg_loss:0.027, val_acc:0.976]
Epoch [92/120    avg_loss:0.023, val_acc:0.978]
Epoch [93/120    avg_loss:0.026, val_acc:0.974]
Epoch [94/120    avg_loss:0.022, val_acc:0.980]
Epoch [95/120    avg_loss:0.026, val_acc:0.977]
Epoch [96/120    avg_loss:0.020, val_acc:0.977]
Epoch [97/120    avg_loss:0.025, val_acc:0.978]
Epoch [98/120    avg_loss:0.024, val_acc:0.976]
Epoch [99/120    avg_loss:0.025, val_acc:0.978]
Epoch [100/120    avg_loss:0.020, val_acc:0.977]
Epoch [101/120    avg_loss:0.022, val_acc:0.976]
Epoch [102/120    avg_loss:0.020, val_acc:0.975]
Epoch [103/120    avg_loss:0.023, val_acc:0.980]
Epoch [104/120    avg_loss:0.022, val_acc:0.973]
Epoch [105/120    avg_loss:0.020, val_acc:0.975]
Epoch [106/120    avg_loss:0.025, val_acc:0.974]
Epoch [107/120    avg_loss:0.029, val_acc:0.974]
Epoch [108/120    avg_loss:0.024, val_acc:0.976]
Epoch [109/120    avg_loss:0.019, val_acc:0.972]
Epoch [110/120    avg_loss:0.021, val_acc:0.978]
Epoch [111/120    avg_loss:0.022, val_acc:0.980]
Epoch [112/120    avg_loss:0.021, val_acc:0.983]
Epoch [113/120    avg_loss:0.020, val_acc:0.978]
Epoch [114/120    avg_loss:0.019, val_acc:0.982]
Epoch [115/120    avg_loss:0.018, val_acc:0.978]
Epoch [116/120    avg_loss:0.019, val_acc:0.978]
Epoch [117/120    avg_loss:0.021, val_acc:0.978]
Epoch [118/120    avg_loss:0.019, val_acc:0.978]
Epoch [119/120    avg_loss:0.021, val_acc:0.982]
Epoch [120/120    avg_loss:0.015, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     3     0     0     0    16    28    11]
 [    0     0 18059     0     4     0    27     0     0     0]
 [    0    14     0  1912     0     0     0     0   110     0]
 [    0    10    10     0  2933     0    17     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     2     0  4852     0     0     0]
 [    0     3     0     0     0     0     0  1287     0     0]
 [    0    28     3    89    21     0     3     0  3427     0]
 [    0     1     0     0     0     5     0     0     0   913]]

Accuracy:
98.96127057575977

F1 scores:
[       nan 0.99113668 0.99812082 0.94653465 0.9888739  0.99808795
 0.9925335  0.99267258 0.96021294 0.99077591]

Kappa:
0.9862350327727528
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb6ad43ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.096, val_acc:0.141]
Epoch [2/120    avg_loss:1.798, val_acc:0.228]
Epoch [3/120    avg_loss:1.551, val_acc:0.250]
Epoch [4/120    avg_loss:1.374, val_acc:0.280]
Epoch [5/120    avg_loss:1.211, val_acc:0.368]
Epoch [6/120    avg_loss:1.064, val_acc:0.414]
Epoch [7/120    avg_loss:0.940, val_acc:0.499]
Epoch [8/120    avg_loss:0.795, val_acc:0.593]
Epoch [9/120    avg_loss:0.711, val_acc:0.637]
Epoch [10/120    avg_loss:0.603, val_acc:0.647]
Epoch [11/120    avg_loss:0.506, val_acc:0.744]
Epoch [12/120    avg_loss:0.457, val_acc:0.714]
Epoch [13/120    avg_loss:0.430, val_acc:0.751]
Epoch [14/120    avg_loss:0.397, val_acc:0.745]
Epoch [15/120    avg_loss:0.376, val_acc:0.782]
Epoch [16/120    avg_loss:0.363, val_acc:0.768]
Epoch [17/120    avg_loss:0.317, val_acc:0.790]
Epoch [18/120    avg_loss:0.279, val_acc:0.814]
Epoch [19/120    avg_loss:0.256, val_acc:0.793]
Epoch [20/120    avg_loss:0.234, val_acc:0.815]
Epoch [21/120    avg_loss:0.216, val_acc:0.863]
Epoch [22/120    avg_loss:0.210, val_acc:0.902]
Epoch [23/120    avg_loss:0.178, val_acc:0.858]
Epoch [24/120    avg_loss:0.177, val_acc:0.884]
Epoch [25/120    avg_loss:0.165, val_acc:0.902]
Epoch [26/120    avg_loss:0.138, val_acc:0.953]
Epoch [27/120    avg_loss:0.111, val_acc:0.927]
Epoch [28/120    avg_loss:0.174, val_acc:0.925]
Epoch [29/120    avg_loss:0.187, val_acc:0.933]
Epoch [30/120    avg_loss:0.124, val_acc:0.927]
Epoch [31/120    avg_loss:0.104, val_acc:0.938]
Epoch [32/120    avg_loss:0.088, val_acc:0.960]
Epoch [33/120    avg_loss:0.069, val_acc:0.963]
Epoch [34/120    avg_loss:0.085, val_acc:0.956]
Epoch [35/120    avg_loss:0.075, val_acc:0.966]
Epoch [36/120    avg_loss:0.063, val_acc:0.962]
Epoch [37/120    avg_loss:0.054, val_acc:0.956]
Epoch [38/120    avg_loss:0.092, val_acc:0.970]
Epoch [39/120    avg_loss:0.062, val_acc:0.957]
Epoch [40/120    avg_loss:0.062, val_acc:0.964]
Epoch [41/120    avg_loss:0.045, val_acc:0.953]
Epoch [42/120    avg_loss:0.048, val_acc:0.955]
Epoch [43/120    avg_loss:0.043, val_acc:0.967]
Epoch [44/120    avg_loss:0.044, val_acc:0.975]
Epoch [45/120    avg_loss:0.037, val_acc:0.977]
Epoch [46/120    avg_loss:0.046, val_acc:0.975]
Epoch [47/120    avg_loss:0.031, val_acc:0.976]
Epoch [48/120    avg_loss:0.044, val_acc:0.959]
Epoch [49/120    avg_loss:0.049, val_acc:0.978]
Epoch [50/120    avg_loss:0.038, val_acc:0.957]
Epoch [51/120    avg_loss:0.045, val_acc:0.955]
Epoch [52/120    avg_loss:0.027, val_acc:0.974]
Epoch [53/120    avg_loss:0.044, val_acc:0.971]
Epoch [54/120    avg_loss:0.021, val_acc:0.974]
Epoch [55/120    avg_loss:0.020, val_acc:0.978]
Epoch [56/120    avg_loss:0.021, val_acc:0.984]
Epoch [57/120    avg_loss:0.018, val_acc:0.983]
Epoch [58/120    avg_loss:0.017, val_acc:0.976]
Epoch [59/120    avg_loss:0.026, val_acc:0.960]
Epoch [60/120    avg_loss:0.028, val_acc:0.975]
Epoch [61/120    avg_loss:0.188, val_acc:0.900]
Epoch [62/120    avg_loss:0.094, val_acc:0.933]
Epoch [63/120    avg_loss:0.110, val_acc:0.955]
Epoch [64/120    avg_loss:0.065, val_acc:0.971]
Epoch [65/120    avg_loss:0.054, val_acc:0.963]
Epoch [66/120    avg_loss:0.042, val_acc:0.964]
Epoch [67/120    avg_loss:0.041, val_acc:0.942]
Epoch [68/120    avg_loss:0.034, val_acc:0.978]
Epoch [69/120    avg_loss:0.044, val_acc:0.974]
Epoch [70/120    avg_loss:0.028, val_acc:0.977]
Epoch [71/120    avg_loss:0.023, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.977]
Epoch [73/120    avg_loss:0.020, val_acc:0.978]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.016, val_acc:0.979]
Epoch [76/120    avg_loss:0.019, val_acc:0.979]
Epoch [77/120    avg_loss:0.016, val_acc:0.978]
Epoch [78/120    avg_loss:0.017, val_acc:0.980]
Epoch [79/120    avg_loss:0.016, val_acc:0.979]
Epoch [80/120    avg_loss:0.016, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.014, val_acc:0.979]
Epoch [84/120    avg_loss:0.015, val_acc:0.979]
Epoch [85/120    avg_loss:0.014, val_acc:0.979]
Epoch [86/120    avg_loss:0.014, val_acc:0.979]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.017, val_acc:0.979]
Epoch [89/120    avg_loss:0.014, val_acc:0.979]
Epoch [90/120    avg_loss:0.015, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.014, val_acc:0.979]
Epoch [93/120    avg_loss:0.015, val_acc:0.979]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.016, val_acc:0.979]
Epoch [96/120    avg_loss:0.016, val_acc:0.979]
Epoch [97/120    avg_loss:0.013, val_acc:0.979]
Epoch [98/120    avg_loss:0.016, val_acc:0.979]
Epoch [99/120    avg_loss:0.018, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.979]
Epoch [101/120    avg_loss:0.016, val_acc:0.979]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.015, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.979]
Epoch [105/120    avg_loss:0.015, val_acc:0.979]
Epoch [106/120    avg_loss:0.015, val_acc:0.979]
Epoch [107/120    avg_loss:0.017, val_acc:0.979]
Epoch [108/120    avg_loss:0.015, val_acc:0.979]
Epoch [109/120    avg_loss:0.012, val_acc:0.979]
Epoch [110/120    avg_loss:0.017, val_acc:0.979]
Epoch [111/120    avg_loss:0.013, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.014, val_acc:0.979]
Epoch [114/120    avg_loss:0.014, val_acc:0.979]
Epoch [115/120    avg_loss:0.013, val_acc:0.979]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.015, val_acc:0.979]
Epoch [118/120    avg_loss:0.014, val_acc:0.979]
Epoch [119/120    avg_loss:0.014, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6346     0     0     0     0     0     7    73     6]
 [    0     0 17988     0    21     0    78     0     3     0]
 [    0     9     0  1921     0     0     0     0   106     0]
 [    0    16     5     0  2934     0    13     0     0     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    49     0     1     0  4828     0     0     0]
 [    0    16     0     0     0     1     0  1270     2     1]
 [    0     7     0    62    29     0     4     0  3469     0]
 [    0     9     0     0     2     3     0     0     0   905]]

Accuracy:
98.72990624924687

F1 scores:
[       nan 0.98885859 0.9956825  0.95595919 0.98472898 0.99846978
 0.98520559 0.98948189 0.96040975 0.98637602]

Kappa:
0.9831785193581994
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83e459ac18>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.090, val_acc:0.127]
Epoch [2/120    avg_loss:1.821, val_acc:0.185]
Epoch [3/120    avg_loss:1.596, val_acc:0.231]
Epoch [4/120    avg_loss:1.422, val_acc:0.317]
Epoch [5/120    avg_loss:1.286, val_acc:0.389]
Epoch [6/120    avg_loss:1.112, val_acc:0.428]
Epoch [7/120    avg_loss:0.966, val_acc:0.495]
Epoch [8/120    avg_loss:0.846, val_acc:0.516]
Epoch [9/120    avg_loss:0.761, val_acc:0.593]
Epoch [10/120    avg_loss:0.699, val_acc:0.619]
Epoch [11/120    avg_loss:0.599, val_acc:0.674]
Epoch [12/120    avg_loss:0.582, val_acc:0.713]
Epoch [13/120    avg_loss:0.487, val_acc:0.743]
Epoch [14/120    avg_loss:0.480, val_acc:0.748]
Epoch [15/120    avg_loss:0.367, val_acc:0.771]
Epoch [16/120    avg_loss:0.357, val_acc:0.813]
Epoch [17/120    avg_loss:0.311, val_acc:0.807]
Epoch [18/120    avg_loss:0.314, val_acc:0.818]
Epoch [19/120    avg_loss:0.292, val_acc:0.840]
Epoch [20/120    avg_loss:0.272, val_acc:0.831]
Epoch [21/120    avg_loss:0.249, val_acc:0.835]
Epoch [22/120    avg_loss:0.247, val_acc:0.877]
Epoch [23/120    avg_loss:0.185, val_acc:0.927]
Epoch [24/120    avg_loss:0.195, val_acc:0.942]
Epoch [25/120    avg_loss:0.143, val_acc:0.927]
Epoch [26/120    avg_loss:0.150, val_acc:0.938]
Epoch [27/120    avg_loss:0.143, val_acc:0.932]
Epoch [28/120    avg_loss:0.166, val_acc:0.953]
Epoch [29/120    avg_loss:0.146, val_acc:0.932]
Epoch [30/120    avg_loss:0.173, val_acc:0.928]
Epoch [31/120    avg_loss:0.128, val_acc:0.945]
Epoch [32/120    avg_loss:0.114, val_acc:0.941]
Epoch [33/120    avg_loss:0.101, val_acc:0.948]
Epoch [34/120    avg_loss:0.079, val_acc:0.966]
Epoch [35/120    avg_loss:0.086, val_acc:0.943]
Epoch [36/120    avg_loss:0.115, val_acc:0.882]
Epoch [37/120    avg_loss:0.094, val_acc:0.972]
Epoch [38/120    avg_loss:0.081, val_acc:0.923]
Epoch [39/120    avg_loss:0.086, val_acc:0.963]
Epoch [40/120    avg_loss:0.114, val_acc:0.949]
Epoch [41/120    avg_loss:0.110, val_acc:0.974]
Epoch [42/120    avg_loss:0.074, val_acc:0.965]
Epoch [43/120    avg_loss:0.070, val_acc:0.957]
Epoch [44/120    avg_loss:0.066, val_acc:0.966]
Epoch [45/120    avg_loss:0.121, val_acc:0.922]
Epoch [46/120    avg_loss:0.088, val_acc:0.968]
Epoch [47/120    avg_loss:0.067, val_acc:0.957]
Epoch [48/120    avg_loss:0.060, val_acc:0.940]
Epoch [49/120    avg_loss:0.057, val_acc:0.963]
Epoch [50/120    avg_loss:0.053, val_acc:0.968]
Epoch [51/120    avg_loss:0.036, val_acc:0.973]
Epoch [52/120    avg_loss:0.035, val_acc:0.973]
Epoch [53/120    avg_loss:0.034, val_acc:0.966]
Epoch [54/120    avg_loss:0.038, val_acc:0.978]
Epoch [55/120    avg_loss:0.023, val_acc:0.978]
Epoch [56/120    avg_loss:0.031, val_acc:0.963]
Epoch [57/120    avg_loss:0.038, val_acc:0.975]
Epoch [58/120    avg_loss:0.027, val_acc:0.978]
Epoch [59/120    avg_loss:0.040, val_acc:0.977]
Epoch [60/120    avg_loss:0.046, val_acc:0.950]
Epoch [61/120    avg_loss:0.038, val_acc:0.978]
Epoch [62/120    avg_loss:0.032, val_acc:0.964]
Epoch [63/120    avg_loss:0.052, val_acc:0.967]
Epoch [64/120    avg_loss:0.050, val_acc:0.963]
Epoch [65/120    avg_loss:0.065, val_acc:0.958]
Epoch [66/120    avg_loss:0.049, val_acc:0.978]
Epoch [67/120    avg_loss:0.608, val_acc:0.381]
Epoch [68/120    avg_loss:0.894, val_acc:0.408]
Epoch [69/120    avg_loss:0.842, val_acc:0.415]
Epoch [70/120    avg_loss:0.794, val_acc:0.429]
Epoch [71/120    avg_loss:0.738, val_acc:0.444]
Epoch [72/120    avg_loss:0.705, val_acc:0.476]
Epoch [73/120    avg_loss:0.655, val_acc:0.498]
Epoch [74/120    avg_loss:0.625, val_acc:0.537]
Epoch [75/120    avg_loss:0.594, val_acc:0.618]
Epoch [76/120    avg_loss:0.569, val_acc:0.623]
Epoch [77/120    avg_loss:0.559, val_acc:0.648]
Epoch [78/120    avg_loss:0.494, val_acc:0.682]
Epoch [79/120    avg_loss:0.496, val_acc:0.693]
Epoch [80/120    avg_loss:0.479, val_acc:0.724]
Epoch [81/120    avg_loss:0.460, val_acc:0.719]
Epoch [82/120    avg_loss:0.451, val_acc:0.722]
Epoch [83/120    avg_loss:0.442, val_acc:0.723]
Epoch [84/120    avg_loss:0.453, val_acc:0.726]
Epoch [85/120    avg_loss:0.454, val_acc:0.723]
Epoch [86/120    avg_loss:0.446, val_acc:0.733]
Epoch [87/120    avg_loss:0.438, val_acc:0.736]
Epoch [88/120    avg_loss:0.432, val_acc:0.733]
Epoch [89/120    avg_loss:0.427, val_acc:0.737]
Epoch [90/120    avg_loss:0.428, val_acc:0.735]
Epoch [91/120    avg_loss:0.424, val_acc:0.734]
Epoch [92/120    avg_loss:0.422, val_acc:0.735]
Epoch [93/120    avg_loss:0.424, val_acc:0.738]
Epoch [94/120    avg_loss:0.422, val_acc:0.738]
Epoch [95/120    avg_loss:0.427, val_acc:0.738]
Epoch [96/120    avg_loss:0.412, val_acc:0.738]
Epoch [97/120    avg_loss:0.414, val_acc:0.738]
Epoch [98/120    avg_loss:0.419, val_acc:0.738]
Epoch [99/120    avg_loss:0.410, val_acc:0.740]
Epoch [100/120    avg_loss:0.412, val_acc:0.742]
Epoch [101/120    avg_loss:0.405, val_acc:0.742]
Epoch [102/120    avg_loss:0.421, val_acc:0.742]
Epoch [103/120    avg_loss:0.420, val_acc:0.743]
Epoch [104/120    avg_loss:0.414, val_acc:0.742]
Epoch [105/120    avg_loss:0.419, val_acc:0.745]
Epoch [106/120    avg_loss:0.403, val_acc:0.744]
Epoch [107/120    avg_loss:0.419, val_acc:0.744]
Epoch [108/120    avg_loss:0.427, val_acc:0.744]
Epoch [109/120    avg_loss:0.423, val_acc:0.743]
Epoch [110/120    avg_loss:0.419, val_acc:0.743]
Epoch [111/120    avg_loss:0.424, val_acc:0.743]
Epoch [112/120    avg_loss:0.431, val_acc:0.743]
Epoch [113/120    avg_loss:0.415, val_acc:0.743]
Epoch [114/120    avg_loss:0.434, val_acc:0.743]
Epoch [115/120    avg_loss:0.426, val_acc:0.743]
Epoch [116/120    avg_loss:0.401, val_acc:0.743]
Epoch [117/120    avg_loss:0.420, val_acc:0.743]
Epoch [118/120    avg_loss:0.411, val_acc:0.743]
Epoch [119/120    avg_loss:0.404, val_acc:0.743]
Epoch [120/120    avg_loss:0.414, val_acc:0.743]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5234   212    50     0     0    93   168   356   319]
 [    0   358 11470     0   865     0  5397     0     0     0]
 [    0    24     0  1800     0     0     0     0   205     7]
 [    0     9   100     0  2688     0   162     0     3    10]
 [    0     0     0     0     0  1302     3     0     0     0]
 [    0     0     0   305    60     0  4466     0    47     0]
 [    0    99     0     0     0     0     0  1184     3     4]
 [    0   302    26   245    24     0   138     1  2835     0]
 [    0     7     7     1    18    24    79     0     0   783]]

Accuracy:
76.54785144482202

F1 scores:
[       nan 0.83979142 0.7670958  0.81135903 0.8112268  0.98973774
 0.58701367 0.89595157 0.80769231 0.7668952 ]

Kappa:
0.7087075887738388
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f016018aba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.135, val_acc:0.133]
Epoch [2/120    avg_loss:1.866, val_acc:0.214]
Epoch [3/120    avg_loss:1.649, val_acc:0.253]
Epoch [4/120    avg_loss:1.471, val_acc:0.253]
Epoch [5/120    avg_loss:1.343, val_acc:0.275]
Epoch [6/120    avg_loss:1.258, val_acc:0.364]
Epoch [7/120    avg_loss:1.107, val_acc:0.415]
Epoch [8/120    avg_loss:1.023, val_acc:0.457]
Epoch [9/120    avg_loss:0.909, val_acc:0.634]
Epoch [10/120    avg_loss:0.788, val_acc:0.731]
Epoch [11/120    avg_loss:0.689, val_acc:0.763]
Epoch [12/120    avg_loss:0.586, val_acc:0.751]
Epoch [13/120    avg_loss:0.506, val_acc:0.792]
Epoch [14/120    avg_loss:0.456, val_acc:0.809]
Epoch [15/120    avg_loss:0.434, val_acc:0.842]
Epoch [16/120    avg_loss:0.332, val_acc:0.830]
Epoch [17/120    avg_loss:0.331, val_acc:0.857]
Epoch [18/120    avg_loss:0.319, val_acc:0.873]
Epoch [19/120    avg_loss:0.276, val_acc:0.855]
Epoch [20/120    avg_loss:0.228, val_acc:0.899]
Epoch [21/120    avg_loss:0.173, val_acc:0.906]
Epoch [22/120    avg_loss:0.195, val_acc:0.892]
Epoch [23/120    avg_loss:0.207, val_acc:0.922]
Epoch [24/120    avg_loss:0.187, val_acc:0.915]
Epoch [25/120    avg_loss:0.138, val_acc:0.924]
Epoch [26/120    avg_loss:0.164, val_acc:0.908]
Epoch [27/120    avg_loss:0.271, val_acc:0.892]
Epoch [28/120    avg_loss:0.227, val_acc:0.923]
Epoch [29/120    avg_loss:0.177, val_acc:0.926]
Epoch [30/120    avg_loss:0.142, val_acc:0.943]
Epoch [31/120    avg_loss:0.136, val_acc:0.897]
Epoch [32/120    avg_loss:0.129, val_acc:0.924]
Epoch [33/120    avg_loss:0.100, val_acc:0.951]
Epoch [34/120    avg_loss:0.123, val_acc:0.932]
Epoch [35/120    avg_loss:0.086, val_acc:0.944]
Epoch [36/120    avg_loss:0.111, val_acc:0.921]
Epoch [37/120    avg_loss:0.098, val_acc:0.957]
Epoch [38/120    avg_loss:0.084, val_acc:0.958]
Epoch [39/120    avg_loss:0.778, val_acc:0.570]
Epoch [40/120    avg_loss:0.609, val_acc:0.732]
Epoch [41/120    avg_loss:0.493, val_acc:0.749]
Epoch [42/120    avg_loss:0.366, val_acc:0.803]
Epoch [43/120    avg_loss:0.310, val_acc:0.829]
Epoch [44/120    avg_loss:0.293, val_acc:0.867]
Epoch [45/120    avg_loss:0.270, val_acc:0.868]
Epoch [46/120    avg_loss:0.240, val_acc:0.884]
Epoch [47/120    avg_loss:0.255, val_acc:0.898]
Epoch [48/120    avg_loss:0.184, val_acc:0.924]
Epoch [49/120    avg_loss:0.215, val_acc:0.915]
Epoch [50/120    avg_loss:0.196, val_acc:0.949]
Epoch [51/120    avg_loss:0.156, val_acc:0.942]
Epoch [52/120    avg_loss:0.115, val_acc:0.960]
Epoch [53/120    avg_loss:0.103, val_acc:0.958]
Epoch [54/120    avg_loss:0.118, val_acc:0.957]
Epoch [55/120    avg_loss:0.099, val_acc:0.958]
Epoch [56/120    avg_loss:0.101, val_acc:0.959]
Epoch [57/120    avg_loss:0.093, val_acc:0.960]
Epoch [58/120    avg_loss:0.101, val_acc:0.960]
Epoch [59/120    avg_loss:0.096, val_acc:0.955]
Epoch [60/120    avg_loss:0.097, val_acc:0.961]
Epoch [61/120    avg_loss:0.095, val_acc:0.966]
Epoch [62/120    avg_loss:0.082, val_acc:0.962]
Epoch [63/120    avg_loss:0.087, val_acc:0.962]
Epoch [64/120    avg_loss:0.083, val_acc:0.963]
Epoch [65/120    avg_loss:0.083, val_acc:0.964]
Epoch [66/120    avg_loss:0.092, val_acc:0.959]
Epoch [67/120    avg_loss:0.076, val_acc:0.961]
Epoch [68/120    avg_loss:0.080, val_acc:0.959]
Epoch [69/120    avg_loss:0.078, val_acc:0.961]
Epoch [70/120    avg_loss:0.074, val_acc:0.959]
Epoch [71/120    avg_loss:0.077, val_acc:0.965]
Epoch [72/120    avg_loss:0.082, val_acc:0.951]
Epoch [73/120    avg_loss:0.074, val_acc:0.961]
Epoch [74/120    avg_loss:0.085, val_acc:0.957]
Epoch [75/120    avg_loss:0.074, val_acc:0.959]
Epoch [76/120    avg_loss:0.069, val_acc:0.960]
Epoch [77/120    avg_loss:0.069, val_acc:0.961]
Epoch [78/120    avg_loss:0.068, val_acc:0.961]
Epoch [79/120    avg_loss:0.074, val_acc:0.963]
Epoch [80/120    avg_loss:0.076, val_acc:0.964]
Epoch [81/120    avg_loss:0.072, val_acc:0.963]
Epoch [82/120    avg_loss:0.082, val_acc:0.964]
Epoch [83/120    avg_loss:0.063, val_acc:0.963]
Epoch [84/120    avg_loss:0.068, val_acc:0.963]
Epoch [85/120    avg_loss:0.068, val_acc:0.964]
Epoch [86/120    avg_loss:0.068, val_acc:0.963]
Epoch [87/120    avg_loss:0.068, val_acc:0.965]
Epoch [88/120    avg_loss:0.072, val_acc:0.965]
Epoch [89/120    avg_loss:0.069, val_acc:0.964]
Epoch [90/120    avg_loss:0.066, val_acc:0.965]
Epoch [91/120    avg_loss:0.068, val_acc:0.964]
Epoch [92/120    avg_loss:0.064, val_acc:0.964]
Epoch [93/120    avg_loss:0.076, val_acc:0.964]
Epoch [94/120    avg_loss:0.062, val_acc:0.964]
Epoch [95/120    avg_loss:0.079, val_acc:0.963]
Epoch [96/120    avg_loss:0.066, val_acc:0.964]
Epoch [97/120    avg_loss:0.071, val_acc:0.963]
Epoch [98/120    avg_loss:0.066, val_acc:0.963]
Epoch [99/120    avg_loss:0.069, val_acc:0.964]
Epoch [100/120    avg_loss:0.070, val_acc:0.964]
Epoch [101/120    avg_loss:0.071, val_acc:0.963]
Epoch [102/120    avg_loss:0.066, val_acc:0.963]
Epoch [103/120    avg_loss:0.075, val_acc:0.963]
Epoch [104/120    avg_loss:0.072, val_acc:0.963]
Epoch [105/120    avg_loss:0.073, val_acc:0.963]
Epoch [106/120    avg_loss:0.064, val_acc:0.964]
Epoch [107/120    avg_loss:0.066, val_acc:0.964]
Epoch [108/120    avg_loss:0.070, val_acc:0.964]
Epoch [109/120    avg_loss:0.074, val_acc:0.964]
Epoch [110/120    avg_loss:0.064, val_acc:0.964]
Epoch [111/120    avg_loss:0.076, val_acc:0.964]
Epoch [112/120    avg_loss:0.068, val_acc:0.964]
Epoch [113/120    avg_loss:0.068, val_acc:0.964]
Epoch [114/120    avg_loss:0.068, val_acc:0.964]
Epoch [115/120    avg_loss:0.066, val_acc:0.964]
Epoch [116/120    avg_loss:0.075, val_acc:0.964]
Epoch [117/120    avg_loss:0.070, val_acc:0.964]
Epoch [118/120    avg_loss:0.067, val_acc:0.964]
Epoch [119/120    avg_loss:0.074, val_acc:0.964]
Epoch [120/120    avg_loss:0.076, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6244     0     0     8     0     0    26   107    47]
 [    0     0 17928     0    24     0   138     0     0     0]
 [    0     5     0  1875     0     0     0     0   155     1]
 [    0    15     8     3  2934     0     8     0     2     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4837     0    33     0]
 [    0    15     0     0     0    19     0  1254     0     2]
 [    0     9     4    90    59     0     0     0  3409     0]
 [    0     0     0     0     4    10     0     0     0   905]]

Accuracy:
98.0671438555901

F1 scores:
[       nan 0.98176101 0.99494978 0.93656344 0.97783703 0.98901099
 0.98103641 0.97587549 0.93692456 0.96481876]

Kappa:
0.9744359033339556
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0b2c297b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.207, val_acc:0.077]
Epoch [2/120    avg_loss:1.927, val_acc:0.083]
Epoch [3/120    avg_loss:1.712, val_acc:0.141]
Epoch [4/120    avg_loss:1.560, val_acc:0.168]
Epoch [5/120    avg_loss:1.406, val_acc:0.189]
Epoch [6/120    avg_loss:1.328, val_acc:0.314]
Epoch [7/120    avg_loss:1.215, val_acc:0.527]
Epoch [8/120    avg_loss:1.061, val_acc:0.611]
Epoch [9/120    avg_loss:0.917, val_acc:0.731]
Epoch [10/120    avg_loss:0.801, val_acc:0.751]
Epoch [11/120    avg_loss:0.751, val_acc:0.761]
Epoch [12/120    avg_loss:0.591, val_acc:0.781]
Epoch [13/120    avg_loss:0.559, val_acc:0.711]
Epoch [14/120    avg_loss:0.469, val_acc:0.739]
Epoch [15/120    avg_loss:0.440, val_acc:0.725]
Epoch [16/120    avg_loss:0.385, val_acc:0.792]
Epoch [17/120    avg_loss:0.368, val_acc:0.787]
Epoch [18/120    avg_loss:0.324, val_acc:0.816]
Epoch [19/120    avg_loss:0.308, val_acc:0.873]
Epoch [20/120    avg_loss:0.281, val_acc:0.862]
Epoch [21/120    avg_loss:0.291, val_acc:0.901]
Epoch [22/120    avg_loss:0.262, val_acc:0.904]
Epoch [23/120    avg_loss:0.244, val_acc:0.927]
Epoch [24/120    avg_loss:0.228, val_acc:0.924]
Epoch [25/120    avg_loss:0.187, val_acc:0.911]
Epoch [26/120    avg_loss:0.190, val_acc:0.941]
Epoch [27/120    avg_loss:0.168, val_acc:0.920]
Epoch [28/120    avg_loss:0.156, val_acc:0.954]
Epoch [29/120    avg_loss:0.126, val_acc:0.947]
Epoch [30/120    avg_loss:0.152, val_acc:0.963]
Epoch [31/120    avg_loss:0.140, val_acc:0.913]
Epoch [32/120    avg_loss:0.138, val_acc:0.936]
Epoch [33/120    avg_loss:0.127, val_acc:0.957]
Epoch [34/120    avg_loss:0.093, val_acc:0.935]
Epoch [35/120    avg_loss:0.098, val_acc:0.966]
Epoch [36/120    avg_loss:0.090, val_acc:0.967]
Epoch [37/120    avg_loss:0.085, val_acc:0.968]
Epoch [38/120    avg_loss:0.067, val_acc:0.973]
Epoch [39/120    avg_loss:0.058, val_acc:0.908]
Epoch [40/120    avg_loss:0.090, val_acc:0.952]
Epoch [41/120    avg_loss:0.073, val_acc:0.964]
Epoch [42/120    avg_loss:0.069, val_acc:0.965]
Epoch [43/120    avg_loss:0.057, val_acc:0.968]
Epoch [44/120    avg_loss:0.069, val_acc:0.946]
Epoch [45/120    avg_loss:0.100, val_acc:0.944]
Epoch [46/120    avg_loss:0.065, val_acc:0.969]
Epoch [47/120    avg_loss:0.050, val_acc:0.968]
Epoch [48/120    avg_loss:0.043, val_acc:0.968]
Epoch [49/120    avg_loss:0.067, val_acc:0.964]
Epoch [50/120    avg_loss:0.050, val_acc:0.974]
Epoch [51/120    avg_loss:0.036, val_acc:0.979]
Epoch [52/120    avg_loss:0.041, val_acc:0.974]
Epoch [53/120    avg_loss:0.030, val_acc:0.980]
Epoch [54/120    avg_loss:0.031, val_acc:0.983]
Epoch [55/120    avg_loss:0.031, val_acc:0.982]
Epoch [56/120    avg_loss:0.022, val_acc:0.984]
Epoch [57/120    avg_loss:0.025, val_acc:0.979]
Epoch [58/120    avg_loss:0.027, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.980]
Epoch [60/120    avg_loss:0.023, val_acc:0.982]
Epoch [61/120    avg_loss:0.020, val_acc:0.982]
Epoch [62/120    avg_loss:0.022, val_acc:0.987]
Epoch [63/120    avg_loss:0.024, val_acc:0.984]
Epoch [64/120    avg_loss:0.016, val_acc:0.983]
Epoch [65/120    avg_loss:0.011, val_acc:0.981]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.014, val_acc:0.976]
Epoch [69/120    avg_loss:0.031, val_acc:0.984]
Epoch [70/120    avg_loss:0.021, val_acc:0.974]
Epoch [71/120    avg_loss:0.034, val_acc:0.966]
Epoch [72/120    avg_loss:0.056, val_acc:0.954]
Epoch [73/120    avg_loss:0.035, val_acc:0.978]
Epoch [74/120    avg_loss:0.022, val_acc:0.984]
Epoch [75/120    avg_loss:0.028, val_acc:0.967]
Epoch [76/120    avg_loss:0.020, val_acc:0.983]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.010, val_acc:0.983]
Epoch [79/120    avg_loss:0.016, val_acc:0.975]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6342     0     0     0     0     0     0    90     0]
 [    0     1 18064     0    17     0     0     0     8     0]
 [    0     0     0  1963     0     0     0     0    70     3]
 [    0    27     1     0  2934     0     5     0     4     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     1     0     0  4864     0     4     0]
 [    0    33     0     0     0     0     0  1255     0     2]
 [    0    11     0    26    41     0     0     0  3493     0]
 [    0     0     0     0    15    37     0     0     0   867]]

Accuracy:
99.02152170245584

F1 scores:
[       nan 0.98738907 0.99900453 0.97516145 0.98143502 0.98602191
 0.99805068 0.98624754 0.96491713 0.96763393]

Kappa:
0.9870358480120317
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d174dcac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.107, val_acc:0.110]
Epoch [2/120    avg_loss:1.801, val_acc:0.138]
Epoch [3/120    avg_loss:1.561, val_acc:0.149]
Epoch [4/120    avg_loss:1.378, val_acc:0.228]
Epoch [5/120    avg_loss:1.255, val_acc:0.393]
Epoch [6/120    avg_loss:1.142, val_acc:0.453]
Epoch [7/120    avg_loss:0.994, val_acc:0.482]
Epoch [8/120    avg_loss:0.812, val_acc:0.551]
Epoch [9/120    avg_loss:0.765, val_acc:0.621]
Epoch [10/120    avg_loss:0.652, val_acc:0.637]
Epoch [11/120    avg_loss:0.577, val_acc:0.661]
Epoch [12/120    avg_loss:0.513, val_acc:0.605]
Epoch [13/120    avg_loss:0.438, val_acc:0.738]
Epoch [14/120    avg_loss:0.379, val_acc:0.754]
Epoch [15/120    avg_loss:0.347, val_acc:0.718]
Epoch [16/120    avg_loss:0.340, val_acc:0.791]
Epoch [17/120    avg_loss:0.306, val_acc:0.767]
Epoch [18/120    avg_loss:0.276, val_acc:0.820]
Epoch [19/120    avg_loss:0.277, val_acc:0.828]
Epoch [20/120    avg_loss:0.256, val_acc:0.845]
Epoch [21/120    avg_loss:0.225, val_acc:0.895]
Epoch [22/120    avg_loss:0.205, val_acc:0.892]
Epoch [23/120    avg_loss:0.198, val_acc:0.937]
Epoch [24/120    avg_loss:0.168, val_acc:0.942]
Epoch [25/120    avg_loss:0.156, val_acc:0.926]
Epoch [26/120    avg_loss:0.172, val_acc:0.959]
Epoch [27/120    avg_loss:0.128, val_acc:0.953]
Epoch [28/120    avg_loss:0.126, val_acc:0.974]
Epoch [29/120    avg_loss:0.299, val_acc:0.841]
Epoch [30/120    avg_loss:0.182, val_acc:0.957]
Epoch [31/120    avg_loss:0.160, val_acc:0.912]
Epoch [32/120    avg_loss:0.147, val_acc:0.953]
Epoch [33/120    avg_loss:0.133, val_acc:0.966]
Epoch [34/120    avg_loss:0.116, val_acc:0.963]
Epoch [35/120    avg_loss:0.096, val_acc:0.954]
Epoch [36/120    avg_loss:0.074, val_acc:0.973]
Epoch [37/120    avg_loss:0.076, val_acc:0.975]
Epoch [38/120    avg_loss:0.055, val_acc:0.968]
Epoch [39/120    avg_loss:0.073, val_acc:0.975]
Epoch [40/120    avg_loss:0.063, val_acc:0.981]
Epoch [41/120    avg_loss:0.045, val_acc:0.979]
Epoch [42/120    avg_loss:0.045, val_acc:0.970]
Epoch [43/120    avg_loss:0.048, val_acc:0.979]
Epoch [44/120    avg_loss:0.058, val_acc:0.978]
Epoch [45/120    avg_loss:0.086, val_acc:0.953]
Epoch [46/120    avg_loss:0.083, val_acc:0.980]
Epoch [47/120    avg_loss:0.040, val_acc:0.983]
Epoch [48/120    avg_loss:0.040, val_acc:0.982]
Epoch [49/120    avg_loss:0.046, val_acc:0.983]
Epoch [50/120    avg_loss:0.030, val_acc:0.983]
Epoch [51/120    avg_loss:0.023, val_acc:0.988]
Epoch [52/120    avg_loss:0.038, val_acc:0.980]
Epoch [53/120    avg_loss:0.022, val_acc:0.985]
Epoch [54/120    avg_loss:0.026, val_acc:0.987]
Epoch [55/120    avg_loss:0.027, val_acc:0.975]
Epoch [56/120    avg_loss:0.026, val_acc:0.983]
Epoch [57/120    avg_loss:0.022, val_acc:0.985]
Epoch [58/120    avg_loss:0.020, val_acc:0.986]
Epoch [59/120    avg_loss:0.023, val_acc:0.986]
Epoch [60/120    avg_loss:0.015, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.989]
Epoch [62/120    avg_loss:0.022, val_acc:0.963]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.988]
Epoch [68/120    avg_loss:0.014, val_acc:0.982]
Epoch [69/120    avg_loss:0.015, val_acc:0.988]
Epoch [70/120    avg_loss:0.030, val_acc:0.978]
Epoch [71/120    avg_loss:0.031, val_acc:0.975]
Epoch [72/120    avg_loss:0.031, val_acc:0.979]
Epoch [73/120    avg_loss:0.019, val_acc:0.988]
Epoch [74/120    avg_loss:0.033, val_acc:0.977]
Epoch [75/120    avg_loss:0.020, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.987]
Epoch [77/120    avg_loss:0.015, val_acc:0.988]
Epoch [78/120    avg_loss:0.011, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.987]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.989]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.989]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.006, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.008, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.991]
Epoch [108/120    avg_loss:0.008, val_acc:0.990]
Epoch [109/120    avg_loss:0.009, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     0     0     0     1    13    11]
 [    0     0 18071     0     6     0    13     0     0     0]
 [    0     1     2  1932     0     0     0     0   101     0]
 [    0    16     5     0  2947     0     3     0     1     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0    20     0     0     0     0     0  1270     0     0]
 [    0     3     0    12    30     0     4     0  3522     0]
 [    0     0     0     0     4     3     0     0     0   912]]

Accuracy:
99.39748873303931

F1 scores:
[       nan 0.99495302 0.9992535  0.97085427 0.98909213 0.99885189
 0.99785166 0.99180008 0.9772475  0.99022801]

Kappa:
0.9920159549409675
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbaf0aefb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.114, val_acc:0.103]
Epoch [2/120    avg_loss:1.902, val_acc:0.173]
Epoch [3/120    avg_loss:1.703, val_acc:0.206]
Epoch [4/120    avg_loss:1.509, val_acc:0.234]
Epoch [5/120    avg_loss:1.428, val_acc:0.259]
Epoch [6/120    avg_loss:1.301, val_acc:0.285]
Epoch [7/120    avg_loss:1.188, val_acc:0.381]
Epoch [8/120    avg_loss:1.072, val_acc:0.552]
Epoch [9/120    avg_loss:0.964, val_acc:0.520]
Epoch [10/120    avg_loss:0.882, val_acc:0.561]
Epoch [11/120    avg_loss:0.781, val_acc:0.582]
Epoch [12/120    avg_loss:0.654, val_acc:0.588]
Epoch [13/120    avg_loss:0.583, val_acc:0.664]
Epoch [14/120    avg_loss:0.510, val_acc:0.694]
Epoch [15/120    avg_loss:0.497, val_acc:0.775]
Epoch [16/120    avg_loss:0.409, val_acc:0.778]
Epoch [17/120    avg_loss:0.404, val_acc:0.831]
Epoch [18/120    avg_loss:0.378, val_acc:0.821]
Epoch [19/120    avg_loss:0.304, val_acc:0.849]
Epoch [20/120    avg_loss:0.294, val_acc:0.895]
Epoch [21/120    avg_loss:0.236, val_acc:0.911]
Epoch [22/120    avg_loss:0.220, val_acc:0.897]
Epoch [23/120    avg_loss:0.182, val_acc:0.940]
Epoch [24/120    avg_loss:0.182, val_acc:0.908]
Epoch [25/120    avg_loss:0.208, val_acc:0.911]
Epoch [26/120    avg_loss:0.164, val_acc:0.939]
Epoch [27/120    avg_loss:0.160, val_acc:0.924]
Epoch [28/120    avg_loss:0.132, val_acc:0.923]
Epoch [29/120    avg_loss:0.153, val_acc:0.943]
Epoch [30/120    avg_loss:0.151, val_acc:0.914]
Epoch [31/120    avg_loss:0.157, val_acc:0.959]
Epoch [32/120    avg_loss:0.125, val_acc:0.931]
Epoch [33/120    avg_loss:0.119, val_acc:0.959]
Epoch [34/120    avg_loss:0.148, val_acc:0.932]
Epoch [35/120    avg_loss:0.147, val_acc:0.951]
Epoch [36/120    avg_loss:0.110, val_acc:0.967]
Epoch [37/120    avg_loss:0.122, val_acc:0.959]
Epoch [38/120    avg_loss:0.089, val_acc:0.965]
Epoch [39/120    avg_loss:0.141, val_acc:0.944]
Epoch [40/120    avg_loss:0.101, val_acc:0.959]
Epoch [41/120    avg_loss:0.101, val_acc:0.961]
Epoch [42/120    avg_loss:0.090, val_acc:0.940]
Epoch [43/120    avg_loss:0.066, val_acc:0.974]
Epoch [44/120    avg_loss:0.054, val_acc:0.974]
Epoch [45/120    avg_loss:0.066, val_acc:0.971]
Epoch [46/120    avg_loss:0.047, val_acc:0.979]
Epoch [47/120    avg_loss:0.059, val_acc:0.963]
Epoch [48/120    avg_loss:0.094, val_acc:0.969]
Epoch [49/120    avg_loss:0.055, val_acc:0.979]
Epoch [50/120    avg_loss:0.050, val_acc:0.966]
Epoch [51/120    avg_loss:0.046, val_acc:0.977]
Epoch [52/120    avg_loss:0.061, val_acc:0.978]
Epoch [53/120    avg_loss:0.074, val_acc:0.976]
Epoch [54/120    avg_loss:0.044, val_acc:0.983]
Epoch [55/120    avg_loss:0.039, val_acc:0.970]
Epoch [56/120    avg_loss:0.034, val_acc:0.978]
Epoch [57/120    avg_loss:0.036, val_acc:0.980]
Epoch [58/120    avg_loss:0.064, val_acc:0.954]
Epoch [59/120    avg_loss:0.038, val_acc:0.951]
Epoch [60/120    avg_loss:0.040, val_acc:0.980]
Epoch [61/120    avg_loss:0.032, val_acc:0.982]
Epoch [62/120    avg_loss:0.017, val_acc:0.986]
Epoch [63/120    avg_loss:0.021, val_acc:0.971]
Epoch [64/120    avg_loss:0.027, val_acc:0.985]
Epoch [65/120    avg_loss:0.029, val_acc:0.981]
Epoch [66/120    avg_loss:0.022, val_acc:0.979]
Epoch [67/120    avg_loss:0.056, val_acc:0.982]
Epoch [68/120    avg_loss:0.032, val_acc:0.957]
Epoch [69/120    avg_loss:0.040, val_acc:0.959]
Epoch [70/120    avg_loss:0.049, val_acc:0.980]
Epoch [71/120    avg_loss:0.041, val_acc:0.986]
Epoch [72/120    avg_loss:0.028, val_acc:0.981]
Epoch [73/120    avg_loss:0.021, val_acc:0.986]
Epoch [74/120    avg_loss:0.017, val_acc:0.985]
Epoch [75/120    avg_loss:0.021, val_acc:0.983]
Epoch [76/120    avg_loss:0.016, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.987]
Epoch [81/120    avg_loss:0.013, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.987]
Epoch [84/120    avg_loss:0.010, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.014, val_acc:0.987]
Epoch [87/120    avg_loss:0.012, val_acc:0.986]
Epoch [88/120    avg_loss:0.011, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.011, val_acc:0.987]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.010, val_acc:0.987]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.012, val_acc:0.987]
Epoch [107/120    avg_loss:0.016, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.010, val_acc:0.987]
Epoch [111/120    avg_loss:0.010, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.011, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6352     0     0     3     0     0    11    66     0]
 [    0     0 18055     0    29     0     1     0     5     0]
 [    0     0     0  1955     0     0     0     0    77     4]
 [    0    33    11     0  2905     0     4     0    18     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     6     0     0  4855     0     4     0]
 [    0     3     0     0     0     0     0  1287     0     0]
 [    0     8     0    47    48     0     0     0  3468     0]
 [    0     0     0     0     2    17     0     0     0   900]]

Accuracy:
99.00947147711662

F1 scores:
[       nan 0.99033365 0.99836877 0.96686449 0.9749958  0.99352874
 0.99712467 0.99459042 0.96213067 0.98684211]

Kappa:
0.9868776445701096
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fed0ef66be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.080, val_acc:0.112]
Epoch [2/120    avg_loss:1.877, val_acc:0.122]
Epoch [3/120    avg_loss:1.679, val_acc:0.136]
Epoch [4/120    avg_loss:1.570, val_acc:0.196]
Epoch [5/120    avg_loss:1.431, val_acc:0.268]
Epoch [6/120    avg_loss:1.313, val_acc:0.361]
Epoch [7/120    avg_loss:1.231, val_acc:0.427]
Epoch [8/120    avg_loss:1.093, val_acc:0.482]
Epoch [9/120    avg_loss:1.014, val_acc:0.514]
Epoch [10/120    avg_loss:0.837, val_acc:0.550]
Epoch [11/120    avg_loss:0.709, val_acc:0.583]
Epoch [12/120    avg_loss:0.646, val_acc:0.645]
Epoch [13/120    avg_loss:0.541, val_acc:0.736]
Epoch [14/120    avg_loss:0.480, val_acc:0.744]
Epoch [15/120    avg_loss:0.448, val_acc:0.723]
Epoch [16/120    avg_loss:0.422, val_acc:0.771]
Epoch [17/120    avg_loss:0.370, val_acc:0.800]
Epoch [18/120    avg_loss:0.347, val_acc:0.797]
Epoch [19/120    avg_loss:0.307, val_acc:0.837]
Epoch [20/120    avg_loss:0.274, val_acc:0.817]
Epoch [21/120    avg_loss:0.258, val_acc:0.882]
Epoch [22/120    avg_loss:0.220, val_acc:0.890]
Epoch [23/120    avg_loss:0.241, val_acc:0.905]
Epoch [24/120    avg_loss:0.198, val_acc:0.927]
Epoch [25/120    avg_loss:0.194, val_acc:0.884]
Epoch [26/120    avg_loss:0.977, val_acc:0.432]
Epoch [27/120    avg_loss:1.278, val_acc:0.528]
Epoch [28/120    avg_loss:1.081, val_acc:0.562]
Epoch [29/120    avg_loss:0.944, val_acc:0.663]
Epoch [30/120    avg_loss:0.865, val_acc:0.661]
Epoch [31/120    avg_loss:0.765, val_acc:0.665]
Epoch [32/120    avg_loss:0.654, val_acc:0.766]
Epoch [33/120    avg_loss:0.637, val_acc:0.787]
Epoch [34/120    avg_loss:0.509, val_acc:0.797]
Epoch [35/120    avg_loss:0.530, val_acc:0.828]
Epoch [36/120    avg_loss:0.439, val_acc:0.863]
Epoch [37/120    avg_loss:0.416, val_acc:0.856]
Epoch [38/120    avg_loss:0.343, val_acc:0.863]
Epoch [39/120    avg_loss:0.314, val_acc:0.858]
Epoch [40/120    avg_loss:0.332, val_acc:0.877]
Epoch [41/120    avg_loss:0.313, val_acc:0.866]
Epoch [42/120    avg_loss:0.303, val_acc:0.874]
Epoch [43/120    avg_loss:0.299, val_acc:0.887]
Epoch [44/120    avg_loss:0.308, val_acc:0.898]
Epoch [45/120    avg_loss:0.286, val_acc:0.898]
Epoch [46/120    avg_loss:0.298, val_acc:0.891]
Epoch [47/120    avg_loss:0.278, val_acc:0.898]
Epoch [48/120    avg_loss:0.292, val_acc:0.903]
Epoch [49/120    avg_loss:0.285, val_acc:0.911]
Epoch [50/120    avg_loss:0.279, val_acc:0.902]
Epoch [51/120    avg_loss:0.276, val_acc:0.906]
Epoch [52/120    avg_loss:0.262, val_acc:0.910]
Epoch [53/120    avg_loss:0.276, val_acc:0.912]
Epoch [54/120    avg_loss:0.272, val_acc:0.912]
Epoch [55/120    avg_loss:0.263, val_acc:0.911]
Epoch [56/120    avg_loss:0.265, val_acc:0.914]
Epoch [57/120    avg_loss:0.266, val_acc:0.913]
Epoch [58/120    avg_loss:0.271, val_acc:0.912]
Epoch [59/120    avg_loss:0.279, val_acc:0.912]
Epoch [60/120    avg_loss:0.285, val_acc:0.917]
Epoch [61/120    avg_loss:0.272, val_acc:0.912]
Epoch [62/120    avg_loss:0.268, val_acc:0.912]
Epoch [63/120    avg_loss:0.261, val_acc:0.913]
Epoch [64/120    avg_loss:0.264, val_acc:0.913]
Epoch [65/120    avg_loss:0.267, val_acc:0.913]
Epoch [66/120    avg_loss:0.266, val_acc:0.913]
Epoch [67/120    avg_loss:0.259, val_acc:0.913]
Epoch [68/120    avg_loss:0.269, val_acc:0.915]
Epoch [69/120    avg_loss:0.268, val_acc:0.915]
Epoch [70/120    avg_loss:0.266, val_acc:0.914]
Epoch [71/120    avg_loss:0.264, val_acc:0.914]
Epoch [72/120    avg_loss:0.270, val_acc:0.914]
Epoch [73/120    avg_loss:0.262, val_acc:0.914]
Epoch [74/120    avg_loss:0.277, val_acc:0.914]
Epoch [75/120    avg_loss:0.271, val_acc:0.914]
Epoch [76/120    avg_loss:0.270, val_acc:0.913]
Epoch [77/120    avg_loss:0.252, val_acc:0.914]
Epoch [78/120    avg_loss:0.258, val_acc:0.913]
Epoch [79/120    avg_loss:0.271, val_acc:0.913]
Epoch [80/120    avg_loss:0.266, val_acc:0.914]
Epoch [81/120    avg_loss:0.293, val_acc:0.914]
Epoch [82/120    avg_loss:0.262, val_acc:0.913]
Epoch [83/120    avg_loss:0.260, val_acc:0.914]
Epoch [84/120    avg_loss:0.256, val_acc:0.914]
Epoch [85/120    avg_loss:0.280, val_acc:0.914]
Epoch [86/120    avg_loss:0.260, val_acc:0.914]
Epoch [87/120    avg_loss:0.256, val_acc:0.914]
Epoch [88/120    avg_loss:0.270, val_acc:0.914]
Epoch [89/120    avg_loss:0.273, val_acc:0.914]
Epoch [90/120    avg_loss:0.267, val_acc:0.914]
Epoch [91/120    avg_loss:0.263, val_acc:0.914]
Epoch [92/120    avg_loss:0.266, val_acc:0.914]
Epoch [93/120    avg_loss:0.262, val_acc:0.914]
Epoch [94/120    avg_loss:0.262, val_acc:0.914]
Epoch [95/120    avg_loss:0.262, val_acc:0.914]
Epoch [96/120    avg_loss:0.259, val_acc:0.914]
Epoch [97/120    avg_loss:0.273, val_acc:0.914]
Epoch [98/120    avg_loss:0.267, val_acc:0.914]
Epoch [99/120    avg_loss:0.262, val_acc:0.914]
Epoch [100/120    avg_loss:0.268, val_acc:0.914]
Epoch [101/120    avg_loss:0.263, val_acc:0.914]
Epoch [102/120    avg_loss:0.267, val_acc:0.914]
Epoch [103/120    avg_loss:0.258, val_acc:0.914]
Epoch [104/120    avg_loss:0.277, val_acc:0.914]
Epoch [105/120    avg_loss:0.284, val_acc:0.914]
Epoch [106/120    avg_loss:0.270, val_acc:0.914]
Epoch [107/120    avg_loss:0.271, val_acc:0.914]
Epoch [108/120    avg_loss:0.267, val_acc:0.914]
Epoch [109/120    avg_loss:0.268, val_acc:0.914]
Epoch [110/120    avg_loss:0.269, val_acc:0.914]
Epoch [111/120    avg_loss:0.269, val_acc:0.914]
Epoch [112/120    avg_loss:0.254, val_acc:0.914]
Epoch [113/120    avg_loss:0.267, val_acc:0.914]
Epoch [114/120    avg_loss:0.265, val_acc:0.914]
Epoch [115/120    avg_loss:0.278, val_acc:0.914]
Epoch [116/120    avg_loss:0.267, val_acc:0.914]
Epoch [117/120    avg_loss:0.270, val_acc:0.914]
Epoch [118/120    avg_loss:0.263, val_acc:0.914]
Epoch [119/120    avg_loss:0.261, val_acc:0.914]
Epoch [120/120    avg_loss:0.270, val_acc:0.914]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5699    78    41    11     0    20    38   342   203]
 [    0     0 17337     0   242     0   511     0     0     0]
 [    0    20     2  1779     0     0     0     0   202    33]
 [    0    80   104     0  2690     0    98     0     0     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    10   278     7     0     0  4503    15    65     0]
 [    0    40     0     0     0     1     2  1232    14     1]
 [    0   201    76   119     2     0    47     0  3126     0]
 [    0    13     0     0    14    13     0     0     0   879]]

Accuracy:
92.90723736533873

F1 scores:
[       nan 0.91220488 0.96410399 0.89352084 0.9070983  0.99466463
 0.89531763 0.9568932  0.85409836 0.86388206]

Kappa:
0.9063178569464698
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ebb826be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.148, val_acc:0.082]
Epoch [2/120    avg_loss:1.892, val_acc:0.102]
Epoch [3/120    avg_loss:1.684, val_acc:0.142]
Epoch [4/120    avg_loss:1.549, val_acc:0.203]
Epoch [5/120    avg_loss:1.404, val_acc:0.287]
Epoch [6/120    avg_loss:1.271, val_acc:0.377]
Epoch [7/120    avg_loss:1.164, val_acc:0.588]
Epoch [8/120    avg_loss:1.020, val_acc:0.687]
Epoch [9/120    avg_loss:0.882, val_acc:0.709]
Epoch [10/120    avg_loss:0.772, val_acc:0.694]
Epoch [11/120    avg_loss:0.714, val_acc:0.735]
Epoch [12/120    avg_loss:0.585, val_acc:0.823]
Epoch [13/120    avg_loss:0.516, val_acc:0.818]
Epoch [14/120    avg_loss:0.483, val_acc:0.782]
Epoch [15/120    avg_loss:0.413, val_acc:0.811]
Epoch [16/120    avg_loss:0.372, val_acc:0.865]
Epoch [17/120    avg_loss:0.345, val_acc:0.846]
Epoch [18/120    avg_loss:0.332, val_acc:0.833]
Epoch [19/120    avg_loss:0.306, val_acc:0.858]
Epoch [20/120    avg_loss:0.268, val_acc:0.853]
Epoch [21/120    avg_loss:0.237, val_acc:0.862]
Epoch [22/120    avg_loss:0.231, val_acc:0.896]
Epoch [23/120    avg_loss:0.210, val_acc:0.922]
Epoch [24/120    avg_loss:0.224, val_acc:0.897]
Epoch [25/120    avg_loss:0.206, val_acc:0.911]
Epoch [26/120    avg_loss:0.176, val_acc:0.948]
Epoch [27/120    avg_loss:0.205, val_acc:0.906]
Epoch [28/120    avg_loss:0.147, val_acc:0.963]
Epoch [29/120    avg_loss:0.125, val_acc:0.961]
Epoch [30/120    avg_loss:0.106, val_acc:0.966]
Epoch [31/120    avg_loss:0.094, val_acc:0.973]
Epoch [32/120    avg_loss:0.111, val_acc:0.930]
Epoch [33/120    avg_loss:0.173, val_acc:0.955]
Epoch [34/120    avg_loss:0.105, val_acc:0.965]
Epoch [35/120    avg_loss:0.113, val_acc:0.921]
Epoch [36/120    avg_loss:0.122, val_acc:0.970]
Epoch [37/120    avg_loss:0.078, val_acc:0.961]
Epoch [38/120    avg_loss:0.081, val_acc:0.961]
Epoch [39/120    avg_loss:0.070, val_acc:0.959]
Epoch [40/120    avg_loss:0.074, val_acc:0.973]
Epoch [41/120    avg_loss:0.066, val_acc:0.972]
Epoch [42/120    avg_loss:0.048, val_acc:0.960]
Epoch [43/120    avg_loss:0.067, val_acc:0.963]
Epoch [44/120    avg_loss:0.062, val_acc:0.973]
Epoch [45/120    avg_loss:0.055, val_acc:0.976]
Epoch [46/120    avg_loss:0.041, val_acc:0.958]
Epoch [47/120    avg_loss:0.052, val_acc:0.980]
Epoch [48/120    avg_loss:0.045, val_acc:0.976]
Epoch [49/120    avg_loss:0.037, val_acc:0.977]
Epoch [50/120    avg_loss:0.045, val_acc:0.978]
Epoch [51/120    avg_loss:0.035, val_acc:0.984]
Epoch [52/120    avg_loss:0.043, val_acc:0.976]
Epoch [53/120    avg_loss:0.054, val_acc:0.924]
Epoch [54/120    avg_loss:0.055, val_acc:0.945]
Epoch [55/120    avg_loss:0.043, val_acc:0.983]
Epoch [56/120    avg_loss:0.048, val_acc:0.974]
Epoch [57/120    avg_loss:0.057, val_acc:0.969]
Epoch [58/120    avg_loss:0.045, val_acc:0.979]
Epoch [59/120    avg_loss:0.044, val_acc:0.975]
Epoch [60/120    avg_loss:0.036, val_acc:0.982]
Epoch [61/120    avg_loss:0.025, val_acc:0.975]
Epoch [62/120    avg_loss:0.036, val_acc:0.974]
Epoch [63/120    avg_loss:0.031, val_acc:0.980]
Epoch [64/120    avg_loss:0.024, val_acc:0.980]
Epoch [65/120    avg_loss:0.018, val_acc:0.985]
Epoch [66/120    avg_loss:0.013, val_acc:0.987]
Epoch [67/120    avg_loss:0.014, val_acc:0.988]
Epoch [68/120    avg_loss:0.014, val_acc:0.986]
Epoch [69/120    avg_loss:0.014, val_acc:0.986]
Epoch [70/120    avg_loss:0.015, val_acc:0.987]
Epoch [71/120    avg_loss:0.016, val_acc:0.988]
Epoch [72/120    avg_loss:0.015, val_acc:0.986]
Epoch [73/120    avg_loss:0.014, val_acc:0.988]
Epoch [74/120    avg_loss:0.014, val_acc:0.987]
Epoch [75/120    avg_loss:0.016, val_acc:0.991]
Epoch [76/120    avg_loss:0.014, val_acc:0.990]
Epoch [77/120    avg_loss:0.013, val_acc:0.989]
Epoch [78/120    avg_loss:0.012, val_acc:0.989]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.012, val_acc:0.991]
Epoch [81/120    avg_loss:0.011, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.990]
Epoch [83/120    avg_loss:0.013, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.015, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.991]
Epoch [87/120    avg_loss:0.013, val_acc:0.991]
Epoch [88/120    avg_loss:0.014, val_acc:0.991]
Epoch [89/120    avg_loss:0.019, val_acc:0.990]
Epoch [90/120    avg_loss:0.012, val_acc:0.989]
Epoch [91/120    avg_loss:0.014, val_acc:0.990]
Epoch [92/120    avg_loss:0.012, val_acc:0.990]
Epoch [93/120    avg_loss:0.011, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.991]
Epoch [95/120    avg_loss:0.015, val_acc:0.991]
Epoch [96/120    avg_loss:0.011, val_acc:0.992]
Epoch [97/120    avg_loss:0.012, val_acc:0.991]
Epoch [98/120    avg_loss:0.013, val_acc:0.990]
Epoch [99/120    avg_loss:0.011, val_acc:0.991]
Epoch [100/120    avg_loss:0.010, val_acc:0.991]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.012, val_acc:0.991]
Epoch [103/120    avg_loss:0.009, val_acc:0.991]
Epoch [104/120    avg_loss:0.011, val_acc:0.991]
Epoch [105/120    avg_loss:0.011, val_acc:0.991]
Epoch [106/120    avg_loss:0.010, val_acc:0.989]
Epoch [107/120    avg_loss:0.011, val_acc:0.991]
Epoch [108/120    avg_loss:0.011, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.991]
Epoch [110/120    avg_loss:0.011, val_acc:0.991]
Epoch [111/120    avg_loss:0.013, val_acc:0.991]
Epoch [112/120    avg_loss:0.010, val_acc:0.991]
Epoch [113/120    avg_loss:0.011, val_acc:0.991]
Epoch [114/120    avg_loss:0.011, val_acc:0.991]
Epoch [115/120    avg_loss:0.013, val_acc:0.991]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     2     0     0     5     0    19     3]
 [    0     0 17994     0     4     0    85     0     7     0]
 [    0     6     0  1966     0     0     0     0    64     0]
 [    0    11     9     0  2944     0     4     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0    25     0     0     0     0     2  1261     0     2]
 [    0     8     4    33    36     0    10     0  3480     0]
 [    0     0     0     0     2    16     0     0     0   901]]

Accuracy:
99.10346323476249

F1 scores:
[       nan 0.99386884 0.99667664 0.97399059 0.98825109 0.99390708
 0.98812303 0.98863191 0.97451694 0.98577681]

Kappa:
0.9881272851966704
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f64efda6b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.208, val_acc:0.458]
Epoch [2/120    avg_loss:1.908, val_acc:0.546]
Epoch [3/120    avg_loss:1.695, val_acc:0.568]
Epoch [4/120    avg_loss:1.490, val_acc:0.613]
Epoch [5/120    avg_loss:1.320, val_acc:0.637]
Epoch [6/120    avg_loss:1.185, val_acc:0.676]
Epoch [7/120    avg_loss:1.059, val_acc:0.737]
Epoch [8/120    avg_loss:0.914, val_acc:0.682]
Epoch [9/120    avg_loss:0.755, val_acc:0.616]
Epoch [10/120    avg_loss:0.687, val_acc:0.557]
Epoch [11/120    avg_loss:0.581, val_acc:0.622]
Epoch [12/120    avg_loss:0.544, val_acc:0.662]
Epoch [13/120    avg_loss:0.475, val_acc:0.703]
Epoch [14/120    avg_loss:0.467, val_acc:0.693]
Epoch [15/120    avg_loss:0.391, val_acc:0.757]
Epoch [16/120    avg_loss:0.351, val_acc:0.820]
Epoch [17/120    avg_loss:0.294, val_acc:0.882]
Epoch [18/120    avg_loss:0.291, val_acc:0.789]
Epoch [19/120    avg_loss:0.361, val_acc:0.812]
Epoch [20/120    avg_loss:0.274, val_acc:0.782]
Epoch [21/120    avg_loss:0.252, val_acc:0.907]
Epoch [22/120    avg_loss:0.228, val_acc:0.912]
Epoch [23/120    avg_loss:0.220, val_acc:0.920]
Epoch [24/120    avg_loss:0.158, val_acc:0.925]
Epoch [25/120    avg_loss:0.154, val_acc:0.916]
Epoch [26/120    avg_loss:0.146, val_acc:0.927]
Epoch [27/120    avg_loss:0.135, val_acc:0.938]
Epoch [28/120    avg_loss:0.171, val_acc:0.937]
Epoch [29/120    avg_loss:0.174, val_acc:0.934]
Epoch [30/120    avg_loss:0.123, val_acc:0.948]
Epoch [31/120    avg_loss:0.135, val_acc:0.943]
Epoch [32/120    avg_loss:0.108, val_acc:0.951]
Epoch [33/120    avg_loss:0.100, val_acc:0.927]
Epoch [34/120    avg_loss:0.121, val_acc:0.943]
Epoch [35/120    avg_loss:0.098, val_acc:0.955]
Epoch [36/120    avg_loss:0.271, val_acc:0.616]
Epoch [37/120    avg_loss:1.065, val_acc:0.694]
Epoch [38/120    avg_loss:0.744, val_acc:0.660]
Epoch [39/120    avg_loss:0.596, val_acc:0.788]
Epoch [40/120    avg_loss:0.443, val_acc:0.796]
Epoch [41/120    avg_loss:0.395, val_acc:0.831]
Epoch [42/120    avg_loss:0.336, val_acc:0.852]
Epoch [43/120    avg_loss:0.315, val_acc:0.834]
Epoch [44/120    avg_loss:0.317, val_acc:0.902]
Epoch [45/120    avg_loss:0.252, val_acc:0.915]
Epoch [46/120    avg_loss:0.230, val_acc:0.925]
Epoch [47/120    avg_loss:0.167, val_acc:0.910]
Epoch [48/120    avg_loss:0.245, val_acc:0.906]
Epoch [49/120    avg_loss:0.153, val_acc:0.930]
Epoch [50/120    avg_loss:0.136, val_acc:0.936]
Epoch [51/120    avg_loss:0.141, val_acc:0.937]
Epoch [52/120    avg_loss:0.126, val_acc:0.938]
Epoch [53/120    avg_loss:0.129, val_acc:0.949]
Epoch [54/120    avg_loss:0.118, val_acc:0.943]
Epoch [55/120    avg_loss:0.107, val_acc:0.944]
Epoch [56/120    avg_loss:0.117, val_acc:0.940]
Epoch [57/120    avg_loss:0.115, val_acc:0.952]
Epoch [58/120    avg_loss:0.095, val_acc:0.948]
Epoch [59/120    avg_loss:0.114, val_acc:0.948]
Epoch [60/120    avg_loss:0.107, val_acc:0.946]
Epoch [61/120    avg_loss:0.111, val_acc:0.953]
Epoch [62/120    avg_loss:0.100, val_acc:0.952]
Epoch [63/120    avg_loss:0.106, val_acc:0.952]
Epoch [64/120    avg_loss:0.102, val_acc:0.950]
Epoch [65/120    avg_loss:0.101, val_acc:0.947]
Epoch [66/120    avg_loss:0.106, val_acc:0.947]
Epoch [67/120    avg_loss:0.098, val_acc:0.947]
Epoch [68/120    avg_loss:0.100, val_acc:0.948]
Epoch [69/120    avg_loss:0.099, val_acc:0.946]
Epoch [70/120    avg_loss:0.111, val_acc:0.948]
Epoch [71/120    avg_loss:0.105, val_acc:0.949]
Epoch [72/120    avg_loss:0.101, val_acc:0.948]
Epoch [73/120    avg_loss:0.096, val_acc:0.952]
Epoch [74/120    avg_loss:0.103, val_acc:0.952]
Epoch [75/120    avg_loss:0.101, val_acc:0.952]
Epoch [76/120    avg_loss:0.104, val_acc:0.952]
Epoch [77/120    avg_loss:0.097, val_acc:0.952]
Epoch [78/120    avg_loss:0.108, val_acc:0.951]
Epoch [79/120    avg_loss:0.103, val_acc:0.952]
Epoch [80/120    avg_loss:0.098, val_acc:0.952]
Epoch [81/120    avg_loss:0.102, val_acc:0.952]
Epoch [82/120    avg_loss:0.098, val_acc:0.952]
Epoch [83/120    avg_loss:0.105, val_acc:0.952]
Epoch [84/120    avg_loss:0.095, val_acc:0.951]
Epoch [85/120    avg_loss:0.104, val_acc:0.952]
Epoch [86/120    avg_loss:0.104, val_acc:0.952]
Epoch [87/120    avg_loss:0.100, val_acc:0.953]
Epoch [88/120    avg_loss:0.106, val_acc:0.953]
Epoch [89/120    avg_loss:0.094, val_acc:0.953]
Epoch [90/120    avg_loss:0.108, val_acc:0.953]
Epoch [91/120    avg_loss:0.092, val_acc:0.953]
Epoch [92/120    avg_loss:0.100, val_acc:0.953]
Epoch [93/120    avg_loss:0.094, val_acc:0.953]
Epoch [94/120    avg_loss:0.103, val_acc:0.953]
Epoch [95/120    avg_loss:0.098, val_acc:0.953]
Epoch [96/120    avg_loss:0.096, val_acc:0.953]
Epoch [97/120    avg_loss:0.103, val_acc:0.953]
Epoch [98/120    avg_loss:0.103, val_acc:0.953]
Epoch [99/120    avg_loss:0.093, val_acc:0.953]
Epoch [100/120    avg_loss:0.099, val_acc:0.953]
Epoch [101/120    avg_loss:0.096, val_acc:0.953]
Epoch [102/120    avg_loss:0.088, val_acc:0.953]
Epoch [103/120    avg_loss:0.095, val_acc:0.953]
Epoch [104/120    avg_loss:0.097, val_acc:0.953]
Epoch [105/120    avg_loss:0.101, val_acc:0.953]
Epoch [106/120    avg_loss:0.101, val_acc:0.953]
Epoch [107/120    avg_loss:0.092, val_acc:0.953]
Epoch [108/120    avg_loss:0.091, val_acc:0.953]
Epoch [109/120    avg_loss:0.095, val_acc:0.953]
Epoch [110/120    avg_loss:0.096, val_acc:0.953]
Epoch [111/120    avg_loss:0.098, val_acc:0.953]
Epoch [112/120    avg_loss:0.097, val_acc:0.953]
Epoch [113/120    avg_loss:0.104, val_acc:0.953]
Epoch [114/120    avg_loss:0.088, val_acc:0.953]
Epoch [115/120    avg_loss:0.096, val_acc:0.953]
Epoch [116/120    avg_loss:0.102, val_acc:0.953]
Epoch [117/120    avg_loss:0.102, val_acc:0.953]
Epoch [118/120    avg_loss:0.087, val_acc:0.953]
Epoch [119/120    avg_loss:0.098, val_acc:0.953]
Epoch [120/120    avg_loss:0.103, val_acc:0.953]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6145    33     1     2     0     0    72   117    62]
 [    0     0 17843     0    88     0   159     0     0     0]
 [    0     4     0  1882     0     0     0     0   141     9]
 [    0    42    28     0  2875     0    12     0     2    13]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    39     2     0     0  4782     0    55     0]
 [    0    22     0     0     0     0     0  1264     0     4]
 [    0    80     3    74    56     0    14     0  3344     0]
 [    0     1     2     0     6    12     0     0     0   898]]

Accuracy:
97.2163979466416

F1 scores:
[       nan 0.96573943 0.99023253 0.94217772 0.95849308 0.99542334
 0.97145759 0.96268088 0.92503458 0.94278215]

Kappa:
0.9631882549299177
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86fd905be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.205, val_acc:0.074]
Epoch [2/120    avg_loss:1.900, val_acc:0.123]
Epoch [3/120    avg_loss:1.736, val_acc:0.227]
Epoch [4/120    avg_loss:1.529, val_acc:0.349]
Epoch [5/120    avg_loss:1.352, val_acc:0.327]
Epoch [6/120    avg_loss:1.184, val_acc:0.425]
Epoch [7/120    avg_loss:1.016, val_acc:0.468]
Epoch [8/120    avg_loss:0.842, val_acc:0.501]
Epoch [9/120    avg_loss:0.688, val_acc:0.602]
Epoch [10/120    avg_loss:0.632, val_acc:0.697]
Epoch [11/120    avg_loss:0.560, val_acc:0.744]
Epoch [12/120    avg_loss:0.461, val_acc:0.833]
Epoch [13/120    avg_loss:0.434, val_acc:0.818]
Epoch [14/120    avg_loss:0.375, val_acc:0.903]
Epoch [15/120    avg_loss:0.334, val_acc:0.873]
Epoch [16/120    avg_loss:0.295, val_acc:0.830]
Epoch [17/120    avg_loss:0.305, val_acc:0.846]
Epoch [18/120    avg_loss:0.267, val_acc:0.919]
Epoch [19/120    avg_loss:0.277, val_acc:0.874]
Epoch [20/120    avg_loss:0.226, val_acc:0.943]
Epoch [21/120    avg_loss:0.197, val_acc:0.936]
Epoch [22/120    avg_loss:0.192, val_acc:0.892]
Epoch [23/120    avg_loss:0.212, val_acc:0.933]
Epoch [24/120    avg_loss:0.177, val_acc:0.931]
Epoch [25/120    avg_loss:0.152, val_acc:0.901]
Epoch [26/120    avg_loss:0.133, val_acc:0.946]
Epoch [27/120    avg_loss:0.109, val_acc:0.960]
Epoch [28/120    avg_loss:0.123, val_acc:0.872]
Epoch [29/120    avg_loss:0.185, val_acc:0.935]
Epoch [30/120    avg_loss:0.114, val_acc:0.957]
Epoch [31/120    avg_loss:0.109, val_acc:0.930]
Epoch [32/120    avg_loss:0.097, val_acc:0.958]
Epoch [33/120    avg_loss:0.094, val_acc:0.956]
Epoch [34/120    avg_loss:0.117, val_acc:0.959]
Epoch [35/120    avg_loss:0.088, val_acc:0.959]
Epoch [36/120    avg_loss:0.071, val_acc:0.953]
Epoch [37/120    avg_loss:0.448, val_acc:0.307]
Epoch [38/120    avg_loss:1.306, val_acc:0.624]
Epoch [39/120    avg_loss:0.722, val_acc:0.677]
Epoch [40/120    avg_loss:0.556, val_acc:0.719]
Epoch [41/120    avg_loss:0.446, val_acc:0.760]
Epoch [42/120    avg_loss:0.399, val_acc:0.761]
Epoch [43/120    avg_loss:0.414, val_acc:0.773]
Epoch [44/120    avg_loss:0.399, val_acc:0.774]
Epoch [45/120    avg_loss:0.374, val_acc:0.795]
Epoch [46/120    avg_loss:0.362, val_acc:0.803]
Epoch [47/120    avg_loss:0.362, val_acc:0.810]
Epoch [48/120    avg_loss:0.344, val_acc:0.816]
Epoch [49/120    avg_loss:0.342, val_acc:0.828]
Epoch [50/120    avg_loss:0.321, val_acc:0.825]
Epoch [51/120    avg_loss:0.299, val_acc:0.846]
Epoch [52/120    avg_loss:0.309, val_acc:0.831]
Epoch [53/120    avg_loss:0.326, val_acc:0.848]
Epoch [54/120    avg_loss:0.286, val_acc:0.848]
Epoch [55/120    avg_loss:0.276, val_acc:0.857]
Epoch [56/120    avg_loss:0.270, val_acc:0.858]
Epoch [57/120    avg_loss:0.289, val_acc:0.863]
Epoch [58/120    avg_loss:0.296, val_acc:0.858]
Epoch [59/120    avg_loss:0.282, val_acc:0.858]
Epoch [60/120    avg_loss:0.282, val_acc:0.861]
Epoch [61/120    avg_loss:0.270, val_acc:0.865]
Epoch [62/120    avg_loss:0.267, val_acc:0.862]
Epoch [63/120    avg_loss:0.268, val_acc:0.865]
Epoch [64/120    avg_loss:0.285, val_acc:0.865]
Epoch [65/120    avg_loss:0.281, val_acc:0.867]
Epoch [66/120    avg_loss:0.276, val_acc:0.867]
Epoch [67/120    avg_loss:0.282, val_acc:0.867]
Epoch [68/120    avg_loss:0.270, val_acc:0.868]
Epoch [69/120    avg_loss:0.274, val_acc:0.866]
Epoch [70/120    avg_loss:0.277, val_acc:0.866]
Epoch [71/120    avg_loss:0.258, val_acc:0.867]
Epoch [72/120    avg_loss:0.274, val_acc:0.867]
Epoch [73/120    avg_loss:0.254, val_acc:0.868]
Epoch [74/120    avg_loss:0.273, val_acc:0.868]
Epoch [75/120    avg_loss:0.270, val_acc:0.867]
Epoch [76/120    avg_loss:0.266, val_acc:0.868]
Epoch [77/120    avg_loss:0.283, val_acc:0.868]
Epoch [78/120    avg_loss:0.269, val_acc:0.867]
Epoch [79/120    avg_loss:0.269, val_acc:0.867]
Epoch [80/120    avg_loss:0.265, val_acc:0.867]
Epoch [81/120    avg_loss:0.278, val_acc:0.867]
Epoch [82/120    avg_loss:0.289, val_acc:0.868]
Epoch [83/120    avg_loss:0.262, val_acc:0.868]
Epoch [84/120    avg_loss:0.270, val_acc:0.868]
Epoch [85/120    avg_loss:0.255, val_acc:0.867]
Epoch [86/120    avg_loss:0.264, val_acc:0.867]
Epoch [87/120    avg_loss:0.270, val_acc:0.868]
Epoch [88/120    avg_loss:0.279, val_acc:0.868]
Epoch [89/120    avg_loss:0.265, val_acc:0.868]
Epoch [90/120    avg_loss:0.271, val_acc:0.868]
Epoch [91/120    avg_loss:0.273, val_acc:0.868]
Epoch [92/120    avg_loss:0.257, val_acc:0.868]
Epoch [93/120    avg_loss:0.266, val_acc:0.868]
Epoch [94/120    avg_loss:0.252, val_acc:0.868]
Epoch [95/120    avg_loss:0.273, val_acc:0.868]
Epoch [96/120    avg_loss:0.281, val_acc:0.868]
Epoch [97/120    avg_loss:0.272, val_acc:0.868]
Epoch [98/120    avg_loss:0.257, val_acc:0.868]
Epoch [99/120    avg_loss:0.263, val_acc:0.868]
Epoch [100/120    avg_loss:0.267, val_acc:0.868]
Epoch [101/120    avg_loss:0.265, val_acc:0.868]
Epoch [102/120    avg_loss:0.262, val_acc:0.868]
Epoch [103/120    avg_loss:0.269, val_acc:0.868]
Epoch [104/120    avg_loss:0.277, val_acc:0.868]
Epoch [105/120    avg_loss:0.262, val_acc:0.868]
Epoch [106/120    avg_loss:0.267, val_acc:0.868]
Epoch [107/120    avg_loss:0.273, val_acc:0.868]
Epoch [108/120    avg_loss:0.279, val_acc:0.868]
Epoch [109/120    avg_loss:0.266, val_acc:0.868]
Epoch [110/120    avg_loss:0.255, val_acc:0.868]
Epoch [111/120    avg_loss:0.291, val_acc:0.868]
Epoch [112/120    avg_loss:0.264, val_acc:0.868]
Epoch [113/120    avg_loss:0.281, val_acc:0.868]
Epoch [114/120    avg_loss:0.262, val_acc:0.868]
Epoch [115/120    avg_loss:0.276, val_acc:0.868]
Epoch [116/120    avg_loss:0.265, val_acc:0.868]
Epoch [117/120    avg_loss:0.264, val_acc:0.868]
Epoch [118/120    avg_loss:0.276, val_acc:0.868]
Epoch [119/120    avg_loss:0.267, val_acc:0.868]
Epoch [120/120    avg_loss:0.266, val_acc:0.868]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5493     3    60    74     0     0     5   720    77]
 [    0     0 15307     0   161     0  2622     0     0     0]
 [    0     8     1  1782     3     0     1     0   224    17]
 [    0    31    33     0  2861     0    36     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   139     0    11     0  4620     0   108     0]
 [    0    35     0     0     0     0     2  1240    12     1]
 [    0   113    10   177    19     0   125     0  3127     0]
 [    0    15     0     0    20    25     0     0     0   859]]

Accuracy:
88.19318921263827

F1 scores:
[       nan 0.90591243 0.91159217 0.87891492 0.93481457 0.99051233
 0.75219798 0.97830375 0.80457996 0.91724506]

Kappa:
0.8478430316557245
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f46669ccb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.131, val_acc:0.232]
Epoch [2/120    avg_loss:1.847, val_acc:0.357]
Epoch [3/120    avg_loss:1.669, val_acc:0.571]
Epoch [4/120    avg_loss:1.468, val_acc:0.607]
Epoch [5/120    avg_loss:1.304, val_acc:0.632]
Epoch [6/120    avg_loss:1.148, val_acc:0.692]
Epoch [7/120    avg_loss:1.015, val_acc:0.726]
Epoch [8/120    avg_loss:0.912, val_acc:0.765]
Epoch [9/120    avg_loss:0.778, val_acc:0.689]
Epoch [10/120    avg_loss:0.687, val_acc:0.752]
Epoch [11/120    avg_loss:0.610, val_acc:0.765]
Epoch [12/120    avg_loss:0.489, val_acc:0.782]
Epoch [13/120    avg_loss:0.452, val_acc:0.797]
Epoch [14/120    avg_loss:0.427, val_acc:0.794]
Epoch [15/120    avg_loss:0.371, val_acc:0.784]
Epoch [16/120    avg_loss:0.346, val_acc:0.801]
Epoch [17/120    avg_loss:0.321, val_acc:0.818]
Epoch [18/120    avg_loss:0.295, val_acc:0.848]
Epoch [19/120    avg_loss:0.266, val_acc:0.858]
Epoch [20/120    avg_loss:0.250, val_acc:0.862]
Epoch [21/120    avg_loss:0.232, val_acc:0.873]
Epoch [22/120    avg_loss:0.235, val_acc:0.810]
Epoch [23/120    avg_loss:0.237, val_acc:0.819]
Epoch [24/120    avg_loss:0.213, val_acc:0.909]
Epoch [25/120    avg_loss:0.184, val_acc:0.911]
Epoch [26/120    avg_loss:0.152, val_acc:0.926]
Epoch [27/120    avg_loss:0.157, val_acc:0.940]
Epoch [28/120    avg_loss:0.153, val_acc:0.931]
Epoch [29/120    avg_loss:0.144, val_acc:0.889]
Epoch [30/120    avg_loss:0.138, val_acc:0.895]
Epoch [31/120    avg_loss:0.121, val_acc:0.948]
Epoch [32/120    avg_loss:0.115, val_acc:0.942]
Epoch [33/120    avg_loss:0.087, val_acc:0.961]
Epoch [34/120    avg_loss:0.103, val_acc:0.917]
Epoch [35/120    avg_loss:0.098, val_acc:0.970]
Epoch [36/120    avg_loss:0.097, val_acc:0.953]
Epoch [37/120    avg_loss:0.082, val_acc:0.958]
Epoch [38/120    avg_loss:0.076, val_acc:0.960]
Epoch [39/120    avg_loss:0.076, val_acc:0.958]
Epoch [40/120    avg_loss:0.058, val_acc:0.973]
Epoch [41/120    avg_loss:0.047, val_acc:0.974]
Epoch [42/120    avg_loss:0.057, val_acc:0.968]
Epoch [43/120    avg_loss:0.057, val_acc:0.972]
Epoch [44/120    avg_loss:0.077, val_acc:0.929]
Epoch [45/120    avg_loss:0.066, val_acc:0.973]
Epoch [46/120    avg_loss:0.040, val_acc:0.973]
Epoch [47/120    avg_loss:0.089, val_acc:0.942]
Epoch [48/120    avg_loss:0.095, val_acc:0.945]
Epoch [49/120    avg_loss:0.066, val_acc:0.957]
Epoch [50/120    avg_loss:0.046, val_acc:0.971]
Epoch [51/120    avg_loss:0.036, val_acc:0.983]
Epoch [52/120    avg_loss:0.039, val_acc:0.963]
Epoch [53/120    avg_loss:0.041, val_acc:0.974]
Epoch [54/120    avg_loss:0.045, val_acc:0.953]
Epoch [55/120    avg_loss:0.039, val_acc:0.983]
Epoch [56/120    avg_loss:0.033, val_acc:0.979]
Epoch [57/120    avg_loss:0.030, val_acc:0.978]
Epoch [58/120    avg_loss:0.050, val_acc:0.973]
Epoch [59/120    avg_loss:0.035, val_acc:0.964]
Epoch [60/120    avg_loss:0.025, val_acc:0.983]
Epoch [61/120    avg_loss:0.023, val_acc:0.967]
Epoch [62/120    avg_loss:0.024, val_acc:0.985]
Epoch [63/120    avg_loss:0.021, val_acc:0.983]
Epoch [64/120    avg_loss:0.022, val_acc:0.978]
Epoch [65/120    avg_loss:0.028, val_acc:0.981]
Epoch [66/120    avg_loss:0.020, val_acc:0.974]
Epoch [67/120    avg_loss:0.022, val_acc:0.958]
Epoch [68/120    avg_loss:0.016, val_acc:0.987]
Epoch [69/120    avg_loss:0.020, val_acc:0.989]
Epoch [70/120    avg_loss:0.018, val_acc:0.983]
Epoch [71/120    avg_loss:0.018, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.974]
Epoch [73/120    avg_loss:0.019, val_acc:0.983]
Epoch [74/120    avg_loss:0.017, val_acc:0.976]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.018, val_acc:0.978]
Epoch [77/120    avg_loss:0.033, val_acc:0.965]
Epoch [78/120    avg_loss:0.023, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.986]
Epoch [80/120    avg_loss:0.016, val_acc:0.984]
Epoch [81/120    avg_loss:0.012, val_acc:0.989]
Epoch [82/120    avg_loss:0.017, val_acc:0.979]
Epoch [83/120    avg_loss:0.022, val_acc:0.978]
Epoch [84/120    avg_loss:0.034, val_acc:0.973]
Epoch [85/120    avg_loss:0.016, val_acc:0.980]
Epoch [86/120    avg_loss:0.039, val_acc:0.974]
Epoch [87/120    avg_loss:0.033, val_acc:0.978]
Epoch [88/120    avg_loss:0.058, val_acc:0.916]
Epoch [89/120    avg_loss:0.204, val_acc:0.932]
Epoch [90/120    avg_loss:0.091, val_acc:0.969]
Epoch [91/120    avg_loss:0.046, val_acc:0.977]
Epoch [92/120    avg_loss:0.035, val_acc:0.981]
Epoch [93/120    avg_loss:0.058, val_acc:0.978]
Epoch [94/120    avg_loss:0.039, val_acc:0.978]
Epoch [95/120    avg_loss:0.019, val_acc:0.986]
Epoch [96/120    avg_loss:0.020, val_acc:0.987]
Epoch [97/120    avg_loss:0.017, val_acc:0.988]
Epoch [98/120    avg_loss:0.017, val_acc:0.988]
Epoch [99/120    avg_loss:0.013, val_acc:0.989]
Epoch [100/120    avg_loss:0.015, val_acc:0.989]
Epoch [101/120    avg_loss:0.015, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.015, val_acc:0.990]
Epoch [104/120    avg_loss:0.016, val_acc:0.989]
Epoch [105/120    avg_loss:0.016, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.989]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.015, val_acc:0.988]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.988]
Epoch [116/120    avg_loss:0.011, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.989]
Epoch [118/120    avg_loss:0.010, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6343     0     0     2     0     0    57    20    10]
 [    0     0 18045     0    29     0    15     0     1     0]
 [    0    15     0  1904     0     0     0     0   116     1]
 [    0    15     4     0  2935     0    15     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    49     0     1     0  4827     0     1     0]
 [    0    13     0     0     0     0     0  1272     0     5]
 [    0    64     0    21    57     0     0     0  3429     0]
 [    0     2     0     0    14    22     0     0     0   881]]

Accuracy:
98.6696551225508

F1 scores:
[       nan 0.9846321  0.99729192 0.96137339 0.97670549 0.99164134
 0.99167951 0.97136312 0.9603697  0.97026432]

Kappa:
0.982369463050614
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04acec8b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.177, val_acc:0.378]
Epoch [2/120    avg_loss:1.887, val_acc:0.198]
Epoch [3/120    avg_loss:1.689, val_acc:0.242]
Epoch [4/120    avg_loss:1.522, val_acc:0.314]
Epoch [5/120    avg_loss:1.370, val_acc:0.403]
Epoch [6/120    avg_loss:1.206, val_acc:0.424]
Epoch [7/120    avg_loss:1.059, val_acc:0.490]
Epoch [8/120    avg_loss:0.895, val_acc:0.530]
Epoch [9/120    avg_loss:0.789, val_acc:0.554]
Epoch [10/120    avg_loss:0.667, val_acc:0.614]
Epoch [11/120    avg_loss:0.639, val_acc:0.595]
Epoch [12/120    avg_loss:0.539, val_acc:0.750]
Epoch [13/120    avg_loss:0.466, val_acc:0.820]
Epoch [14/120    avg_loss:0.447, val_acc:0.835]
Epoch [15/120    avg_loss:0.391, val_acc:0.819]
Epoch [16/120    avg_loss:0.341, val_acc:0.862]
Epoch [17/120    avg_loss:0.373, val_acc:0.832]
Epoch [18/120    avg_loss:0.282, val_acc:0.859]
Epoch [19/120    avg_loss:0.248, val_acc:0.870]
Epoch [20/120    avg_loss:0.302, val_acc:0.830]
Epoch [21/120    avg_loss:0.251, val_acc:0.902]
Epoch [22/120    avg_loss:0.218, val_acc:0.863]
Epoch [23/120    avg_loss:0.168, val_acc:0.931]
Epoch [24/120    avg_loss:0.174, val_acc:0.925]
Epoch [25/120    avg_loss:0.166, val_acc:0.939]
Epoch [26/120    avg_loss:0.131, val_acc:0.923]
Epoch [27/120    avg_loss:0.131, val_acc:0.950]
Epoch [28/120    avg_loss:0.110, val_acc:0.967]
Epoch [29/120    avg_loss:0.105, val_acc:0.974]
Epoch [30/120    avg_loss:0.090, val_acc:0.962]
Epoch [31/120    avg_loss:0.088, val_acc:0.948]
Epoch [32/120    avg_loss:0.104, val_acc:0.957]
Epoch [33/120    avg_loss:0.097, val_acc:0.944]
Epoch [34/120    avg_loss:0.085, val_acc:0.966]
Epoch [35/120    avg_loss:0.094, val_acc:0.952]
Epoch [36/120    avg_loss:0.068, val_acc:0.969]
Epoch [37/120    avg_loss:0.072, val_acc:0.966]
Epoch [38/120    avg_loss:0.065, val_acc:0.969]
Epoch [39/120    avg_loss:0.048, val_acc:0.964]
Epoch [40/120    avg_loss:0.067, val_acc:0.963]
Epoch [41/120    avg_loss:0.055, val_acc:0.978]
Epoch [42/120    avg_loss:0.069, val_acc:0.957]
Epoch [43/120    avg_loss:0.089, val_acc:0.961]
Epoch [44/120    avg_loss:0.071, val_acc:0.945]
Epoch [45/120    avg_loss:0.066, val_acc:0.971]
Epoch [46/120    avg_loss:0.034, val_acc:0.975]
Epoch [47/120    avg_loss:0.042, val_acc:0.975]
Epoch [48/120    avg_loss:0.036, val_acc:0.973]
Epoch [49/120    avg_loss:0.045, val_acc:0.971]
Epoch [50/120    avg_loss:0.062, val_acc:0.969]
Epoch [51/120    avg_loss:0.035, val_acc:0.974]
Epoch [52/120    avg_loss:0.032, val_acc:0.979]
Epoch [53/120    avg_loss:0.038, val_acc:0.974]
Epoch [54/120    avg_loss:0.030, val_acc:0.979]
Epoch [55/120    avg_loss:0.020, val_acc:0.982]
Epoch [56/120    avg_loss:0.019, val_acc:0.983]
Epoch [57/120    avg_loss:0.020, val_acc:0.982]
Epoch [58/120    avg_loss:0.027, val_acc:0.982]
Epoch [59/120    avg_loss:0.018, val_acc:0.979]
Epoch [60/120    avg_loss:0.016, val_acc:0.975]
Epoch [61/120    avg_loss:0.013, val_acc:0.983]
Epoch [62/120    avg_loss:0.034, val_acc:0.959]
Epoch [63/120    avg_loss:0.028, val_acc:0.968]
Epoch [64/120    avg_loss:0.022, val_acc:0.977]
Epoch [65/120    avg_loss:0.015, val_acc:0.986]
Epoch [66/120    avg_loss:0.023, val_acc:0.986]
Epoch [67/120    avg_loss:0.020, val_acc:0.986]
Epoch [68/120    avg_loss:0.013, val_acc:0.984]
Epoch [69/120    avg_loss:0.023, val_acc:0.970]
Epoch [70/120    avg_loss:0.027, val_acc:0.979]
Epoch [71/120    avg_loss:0.017, val_acc:0.987]
Epoch [72/120    avg_loss:0.021, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.013, val_acc:0.986]
Epoch [75/120    avg_loss:0.014, val_acc:0.982]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.015, val_acc:0.986]
Epoch [87/120    avg_loss:0.016, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     0     0     0     0    32    34     2]
 [    0     0 18080     0     8     0     2     0     0     0]
 [    0     2     0  1990     0     0     0     0    40     4]
 [    0    23    14     0  2918     0     4     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     0     0  4856     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     4     0     1    52     0     0     0  3504    10]
 [    0     0     0     0    20    40     0     0     0   859]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99243665 0.99872949 0.98832878 0.97755444 0.98490566
 0.99712526 0.98774885 0.97849763 0.95763657]

Kappa:
0.9895553790141671
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b9a075be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.123, val_acc:0.065]
Epoch [2/120    avg_loss:1.878, val_acc:0.092]
Epoch [3/120    avg_loss:1.637, val_acc:0.166]
Epoch [4/120    avg_loss:1.488, val_acc:0.432]
Epoch [5/120    avg_loss:1.392, val_acc:0.605]
Epoch [6/120    avg_loss:1.249, val_acc:0.648]
Epoch [7/120    avg_loss:1.141, val_acc:0.581]
Epoch [8/120    avg_loss:1.009, val_acc:0.576]
Epoch [9/120    avg_loss:0.884, val_acc:0.531]
Epoch [10/120    avg_loss:0.775, val_acc:0.548]
Epoch [11/120    avg_loss:0.692, val_acc:0.550]
Epoch [12/120    avg_loss:0.586, val_acc:0.596]
Epoch [13/120    avg_loss:0.515, val_acc:0.691]
Epoch [14/120    avg_loss:0.433, val_acc:0.771]
Epoch [15/120    avg_loss:0.387, val_acc:0.804]
Epoch [16/120    avg_loss:0.348, val_acc:0.806]
Epoch [17/120    avg_loss:0.300, val_acc:0.860]
Epoch [18/120    avg_loss:0.325, val_acc:0.865]
Epoch [19/120    avg_loss:0.262, val_acc:0.904]
Epoch [20/120    avg_loss:0.243, val_acc:0.914]
Epoch [21/120    avg_loss:0.410, val_acc:0.842]
Epoch [22/120    avg_loss:0.265, val_acc:0.937]
Epoch [23/120    avg_loss:0.215, val_acc:0.902]
Epoch [24/120    avg_loss:0.199, val_acc:0.890]
Epoch [25/120    avg_loss:0.184, val_acc:0.947]
Epoch [26/120    avg_loss:0.156, val_acc:0.953]
Epoch [27/120    avg_loss:0.150, val_acc:0.966]
Epoch [28/120    avg_loss:0.144, val_acc:0.967]
Epoch [29/120    avg_loss:0.781, val_acc:0.493]
Epoch [30/120    avg_loss:1.715, val_acc:0.596]
Epoch [31/120    avg_loss:1.349, val_acc:0.639]
Epoch [32/120    avg_loss:1.249, val_acc:0.657]
Epoch [33/120    avg_loss:1.144, val_acc:0.702]
Epoch [34/120    avg_loss:1.061, val_acc:0.734]
Epoch [35/120    avg_loss:0.932, val_acc:0.758]
Epoch [36/120    avg_loss:0.871, val_acc:0.782]
Epoch [37/120    avg_loss:0.791, val_acc:0.805]
Epoch [38/120    avg_loss:0.748, val_acc:0.797]
Epoch [39/120    avg_loss:0.695, val_acc:0.808]
Epoch [40/120    avg_loss:0.641, val_acc:0.818]
Epoch [41/120    avg_loss:0.557, val_acc:0.811]
Epoch [42/120    avg_loss:0.540, val_acc:0.815]
Epoch [43/120    avg_loss:0.498, val_acc:0.822]
Epoch [44/120    avg_loss:0.521, val_acc:0.825]
Epoch [45/120    avg_loss:0.501, val_acc:0.828]
Epoch [46/120    avg_loss:0.506, val_acc:0.828]
Epoch [47/120    avg_loss:0.480, val_acc:0.838]
Epoch [48/120    avg_loss:0.483, val_acc:0.834]
Epoch [49/120    avg_loss:0.488, val_acc:0.827]
Epoch [50/120    avg_loss:0.457, val_acc:0.831]
Epoch [51/120    avg_loss:0.484, val_acc:0.832]
Epoch [52/120    avg_loss:0.481, val_acc:0.827]
Epoch [53/120    avg_loss:0.443, val_acc:0.830]
Epoch [54/120    avg_loss:0.458, val_acc:0.823]
Epoch [55/120    avg_loss:0.437, val_acc:0.827]
Epoch [56/120    avg_loss:0.444, val_acc:0.830]
Epoch [57/120    avg_loss:0.460, val_acc:0.832]
Epoch [58/120    avg_loss:0.451, val_acc:0.833]
Epoch [59/120    avg_loss:0.439, val_acc:0.834]
Epoch [60/120    avg_loss:0.436, val_acc:0.835]
Epoch [61/120    avg_loss:0.434, val_acc:0.832]
Epoch [62/120    avg_loss:0.432, val_acc:0.835]
Epoch [63/120    avg_loss:0.438, val_acc:0.837]
Epoch [64/120    avg_loss:0.419, val_acc:0.838]
Epoch [65/120    avg_loss:0.424, val_acc:0.837]
Epoch [66/120    avg_loss:0.431, val_acc:0.839]
Epoch [67/120    avg_loss:0.458, val_acc:0.838]
Epoch [68/120    avg_loss:0.441, val_acc:0.838]
Epoch [69/120    avg_loss:0.429, val_acc:0.838]
Epoch [70/120    avg_loss:0.426, val_acc:0.838]
Epoch [71/120    avg_loss:0.445, val_acc:0.838]
Epoch [72/120    avg_loss:0.423, val_acc:0.838]
Epoch [73/120    avg_loss:0.412, val_acc:0.837]
Epoch [74/120    avg_loss:0.441, val_acc:0.837]
Epoch [75/120    avg_loss:0.438, val_acc:0.837]
Epoch [76/120    avg_loss:0.434, val_acc:0.837]
Epoch [77/120    avg_loss:0.419, val_acc:0.838]
Epoch [78/120    avg_loss:0.427, val_acc:0.837]
Epoch [79/120    avg_loss:0.432, val_acc:0.838]
Epoch [80/120    avg_loss:0.432, val_acc:0.838]
Epoch [81/120    avg_loss:0.442, val_acc:0.838]
Epoch [82/120    avg_loss:0.431, val_acc:0.838]
Epoch [83/120    avg_loss:0.429, val_acc:0.838]
Epoch [84/120    avg_loss:0.443, val_acc:0.838]
Epoch [85/120    avg_loss:0.438, val_acc:0.838]
Epoch [86/120    avg_loss:0.435, val_acc:0.838]
Epoch [87/120    avg_loss:0.432, val_acc:0.838]
Epoch [88/120    avg_loss:0.426, val_acc:0.838]
Epoch [89/120    avg_loss:0.440, val_acc:0.838]
Epoch [90/120    avg_loss:0.419, val_acc:0.838]
Epoch [91/120    avg_loss:0.436, val_acc:0.838]
Epoch [92/120    avg_loss:0.454, val_acc:0.838]
Epoch [93/120    avg_loss:0.408, val_acc:0.838]
Epoch [94/120    avg_loss:0.427, val_acc:0.838]
Epoch [95/120    avg_loss:0.447, val_acc:0.838]
Epoch [96/120    avg_loss:0.426, val_acc:0.838]
Epoch [97/120    avg_loss:0.422, val_acc:0.838]
Epoch [98/120    avg_loss:0.431, val_acc:0.838]
Epoch [99/120    avg_loss:0.439, val_acc:0.838]
Epoch [100/120    avg_loss:0.425, val_acc:0.838]
Epoch [101/120    avg_loss:0.443, val_acc:0.838]
Epoch [102/120    avg_loss:0.428, val_acc:0.838]
Epoch [103/120    avg_loss:0.433, val_acc:0.838]
Epoch [104/120    avg_loss:0.443, val_acc:0.838]
Epoch [105/120    avg_loss:0.431, val_acc:0.838]
Epoch [106/120    avg_loss:0.441, val_acc:0.838]
Epoch [107/120    avg_loss:0.433, val_acc:0.838]
Epoch [108/120    avg_loss:0.433, val_acc:0.838]
Epoch [109/120    avg_loss:0.427, val_acc:0.838]
Epoch [110/120    avg_loss:0.439, val_acc:0.838]
Epoch [111/120    avg_loss:0.435, val_acc:0.838]
Epoch [112/120    avg_loss:0.452, val_acc:0.838]
Epoch [113/120    avg_loss:0.424, val_acc:0.838]
Epoch [114/120    avg_loss:0.430, val_acc:0.838]
Epoch [115/120    avg_loss:0.424, val_acc:0.838]
Epoch [116/120    avg_loss:0.429, val_acc:0.838]
Epoch [117/120    avg_loss:0.440, val_acc:0.838]
Epoch [118/120    avg_loss:0.422, val_acc:0.838]
Epoch [119/120    avg_loss:0.430, val_acc:0.838]
Epoch [120/120    avg_loss:0.451, val_acc:0.838]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5432     2    30    75     0   320     7   305   261]
 [    0     0 14304     0  1528     0  2258     0     0     0]
 [    0    27     0  1772     0     0    15     3   167    52]
 [    0    69   116     0  2611     0   135     0    13    28]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    14  1249    32     2     0  3541     0    40     0]
 [    0   113     0     0     0     0    20  1143     3    11]
 [    0   167    23    34    20     0    94     0  3228     5]
 [    0    21     0     1     6   118     0     0     0   773]]

Accuracy:
82.204227219049

F1 scores:
[       nan 0.88505092 0.84679138 0.90755442 0.72387025 0.95674487
 0.62889619 0.93573475 0.88112461 0.7545144 ]

Kappa:
0.7704708069090925
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f726ff56b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.207, val_acc:0.071]
Epoch [2/120    avg_loss:1.990, val_acc:0.102]
Epoch [3/120    avg_loss:1.789, val_acc:0.143]
Epoch [4/120    avg_loss:1.570, val_acc:0.227]
Epoch [5/120    avg_loss:1.418, val_acc:0.304]
Epoch [6/120    avg_loss:1.315, val_acc:0.353]
Epoch [7/120    avg_loss:1.182, val_acc:0.394]
Epoch [8/120    avg_loss:1.089, val_acc:0.399]
Epoch [9/120    avg_loss:0.962, val_acc:0.474]
Epoch [10/120    avg_loss:0.839, val_acc:0.568]
Epoch [11/120    avg_loss:0.681, val_acc:0.652]
Epoch [12/120    avg_loss:0.553, val_acc:0.700]
Epoch [13/120    avg_loss:0.517, val_acc:0.678]
Epoch [14/120    avg_loss:0.440, val_acc:0.821]
Epoch [15/120    avg_loss:0.353, val_acc:0.820]
Epoch [16/120    avg_loss:0.313, val_acc:0.785]
Epoch [17/120    avg_loss:0.330, val_acc:0.844]
Epoch [18/120    avg_loss:0.266, val_acc:0.872]
Epoch [19/120    avg_loss:0.237, val_acc:0.821]
Epoch [20/120    avg_loss:0.227, val_acc:0.890]
Epoch [21/120    avg_loss:0.190, val_acc:0.912]
Epoch [22/120    avg_loss:0.173, val_acc:0.887]
Epoch [23/120    avg_loss:0.172, val_acc:0.923]
Epoch [24/120    avg_loss:0.144, val_acc:0.918]
Epoch [25/120    avg_loss:0.185, val_acc:0.878]
Epoch [26/120    avg_loss:0.182, val_acc:0.929]
Epoch [27/120    avg_loss:0.133, val_acc:0.949]
Epoch [28/120    avg_loss:0.119, val_acc:0.937]
Epoch [29/120    avg_loss:0.119, val_acc:0.933]
Epoch [30/120    avg_loss:0.092, val_acc:0.961]
Epoch [31/120    avg_loss:0.094, val_acc:0.931]
Epoch [32/120    avg_loss:0.099, val_acc:0.944]
Epoch [33/120    avg_loss:0.078, val_acc:0.957]
Epoch [34/120    avg_loss:0.090, val_acc:0.955]
Epoch [35/120    avg_loss:0.093, val_acc:0.930]
Epoch [36/120    avg_loss:0.087, val_acc:0.966]
Epoch [37/120    avg_loss:0.061, val_acc:0.957]
Epoch [38/120    avg_loss:0.081, val_acc:0.966]
Epoch [39/120    avg_loss:0.072, val_acc:0.970]
Epoch [40/120    avg_loss:0.053, val_acc:0.966]
Epoch [41/120    avg_loss:0.045, val_acc:0.961]
Epoch [42/120    avg_loss:0.050, val_acc:0.970]
Epoch [43/120    avg_loss:0.042, val_acc:0.952]
Epoch [44/120    avg_loss:0.074, val_acc:0.967]
Epoch [45/120    avg_loss:0.044, val_acc:0.973]
Epoch [46/120    avg_loss:0.041, val_acc:0.975]
Epoch [47/120    avg_loss:0.031, val_acc:0.976]
Epoch [48/120    avg_loss:0.027, val_acc:0.974]
Epoch [49/120    avg_loss:0.027, val_acc:0.980]
Epoch [50/120    avg_loss:0.029, val_acc:0.977]
Epoch [51/120    avg_loss:0.030, val_acc:0.976]
Epoch [52/120    avg_loss:0.028, val_acc:0.958]
Epoch [53/120    avg_loss:0.026, val_acc:0.981]
Epoch [54/120    avg_loss:0.023, val_acc:0.974]
Epoch [55/120    avg_loss:0.016, val_acc:0.979]
Epoch [56/120    avg_loss:0.018, val_acc:0.977]
Epoch [57/120    avg_loss:0.015, val_acc:0.983]
Epoch [58/120    avg_loss:0.018, val_acc:0.980]
Epoch [59/120    avg_loss:0.019, val_acc:0.974]
Epoch [60/120    avg_loss:0.017, val_acc:0.975]
Epoch [61/120    avg_loss:0.016, val_acc:0.975]
Epoch [62/120    avg_loss:0.016, val_acc:0.980]
Epoch [63/120    avg_loss:0.020, val_acc:0.974]
Epoch [64/120    avg_loss:0.018, val_acc:0.977]
Epoch [65/120    avg_loss:0.013, val_acc:0.971]
Epoch [66/120    avg_loss:0.011, val_acc:0.984]
Epoch [67/120    avg_loss:0.013, val_acc:0.977]
Epoch [68/120    avg_loss:0.012, val_acc:0.973]
Epoch [69/120    avg_loss:0.015, val_acc:0.980]
Epoch [70/120    avg_loss:0.023, val_acc:0.961]
Epoch [71/120    avg_loss:0.014, val_acc:0.978]
Epoch [72/120    avg_loss:0.013, val_acc:0.978]
Epoch [73/120    avg_loss:0.010, val_acc:0.976]
Epoch [74/120    avg_loss:0.013, val_acc:0.952]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.019, val_acc:0.952]
Epoch [78/120    avg_loss:0.058, val_acc:0.965]
Epoch [79/120    avg_loss:0.038, val_acc:0.981]
Epoch [80/120    avg_loss:0.016, val_acc:0.980]
Epoch [81/120    avg_loss:0.030, val_acc:0.980]
Epoch [82/120    avg_loss:0.016, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.980]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.019, val_acc:0.985]
Epoch [87/120    avg_loss:0.020, val_acc:0.980]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.016, val_acc:0.974]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.980]
Epoch [94/120    avg_loss:0.019, val_acc:0.980]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.965]
Epoch [101/120    avg_loss:0.046, val_acc:0.953]
Epoch [102/120    avg_loss:0.047, val_acc:0.947]
Epoch [103/120    avg_loss:0.029, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.007, val_acc:0.979]
Epoch [107/120    avg_loss:0.006, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.007, val_acc:0.980]
Epoch [111/120    avg_loss:0.008, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.005, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.005, val_acc:0.981]
Epoch [117/120    avg_loss:0.005, val_acc:0.981]
Epoch [118/120    avg_loss:0.005, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.005, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6404     0     0     0     0     0     2    25     1]
 [    0     2 17988     0    85     0    15     0     0     0]
 [    0     8     0  1908     0     0     0     0   120     0]
 [    0     4     2     0  2964     0     0     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     1     7     0  4861     0     0     0]
 [    0    39     0     0     0     0     0  1251     0     0]
 [    0    40     1     5    33     0     0     0  3492     0]
 [    0     0     0     0    10    54     0     0     0   855]]

Accuracy:
98.87932904345311

F1 scores:
[       nan 0.99064119 0.99684123 0.96607595 0.9764454  0.97972973
 0.99671929 0.98387731 0.96865465 0.96338028]

Kappa:
0.9851605182562659
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c8cba5b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.120, val_acc:0.077]
Epoch [2/120    avg_loss:1.857, val_acc:0.328]
Epoch [3/120    avg_loss:1.691, val_acc:0.555]
Epoch [4/120    avg_loss:1.543, val_acc:0.641]
Epoch [5/120    avg_loss:1.369, val_acc:0.628]
Epoch [6/120    avg_loss:1.174, val_acc:0.634]
Epoch [7/120    avg_loss:1.030, val_acc:0.628]
Epoch [8/120    avg_loss:0.893, val_acc:0.657]
Epoch [9/120    avg_loss:0.803, val_acc:0.629]
Epoch [10/120    avg_loss:0.668, val_acc:0.733]
Epoch [11/120    avg_loss:0.557, val_acc:0.748]
Epoch [12/120    avg_loss:0.513, val_acc:0.774]
Epoch [13/120    avg_loss:0.440, val_acc:0.813]
Epoch [14/120    avg_loss:0.404, val_acc:0.811]
Epoch [15/120    avg_loss:0.373, val_acc:0.830]
Epoch [16/120    avg_loss:0.317, val_acc:0.821]
Epoch [17/120    avg_loss:0.741, val_acc:0.480]
Epoch [18/120    avg_loss:1.279, val_acc:0.527]
Epoch [19/120    avg_loss:1.154, val_acc:0.534]
Epoch [20/120    avg_loss:1.113, val_acc:0.394]
Epoch [21/120    avg_loss:1.083, val_acc:0.432]
Epoch [22/120    avg_loss:1.072, val_acc:0.474]
Epoch [23/120    avg_loss:1.023, val_acc:0.542]
Epoch [24/120    avg_loss:0.987, val_acc:0.557]
Epoch [25/120    avg_loss:0.969, val_acc:0.541]
Epoch [26/120    avg_loss:0.932, val_acc:0.560]
Epoch [27/120    avg_loss:0.932, val_acc:0.574]
Epoch [28/120    avg_loss:0.902, val_acc:0.578]
Epoch [29/120    avg_loss:0.852, val_acc:0.586]
Epoch [30/120    avg_loss:0.865, val_acc:0.589]
Epoch [31/120    avg_loss:0.864, val_acc:0.597]
Epoch [32/120    avg_loss:0.853, val_acc:0.603]
Epoch [33/120    avg_loss:0.838, val_acc:0.604]
Epoch [34/120    avg_loss:0.840, val_acc:0.603]
Epoch [35/120    avg_loss:0.842, val_acc:0.606]
Epoch [36/120    avg_loss:0.843, val_acc:0.605]
Epoch [37/120    avg_loss:0.825, val_acc:0.603]
Epoch [38/120    avg_loss:0.839, val_acc:0.613]
Epoch [39/120    avg_loss:0.828, val_acc:0.613]
Epoch [40/120    avg_loss:0.824, val_acc:0.603]
Epoch [41/120    avg_loss:0.830, val_acc:0.613]
Epoch [42/120    avg_loss:0.821, val_acc:0.613]
Epoch [43/120    avg_loss:0.824, val_acc:0.612]
Epoch [44/120    avg_loss:0.806, val_acc:0.609]
Epoch [45/120    avg_loss:0.819, val_acc:0.612]
Epoch [46/120    avg_loss:0.819, val_acc:0.608]
Epoch [47/120    avg_loss:0.805, val_acc:0.613]
Epoch [48/120    avg_loss:0.803, val_acc:0.608]
Epoch [49/120    avg_loss:0.819, val_acc:0.618]
Epoch [50/120    avg_loss:0.810, val_acc:0.610]
Epoch [51/120    avg_loss:0.808, val_acc:0.610]
Epoch [52/120    avg_loss:0.814, val_acc:0.610]
Epoch [53/120    avg_loss:0.803, val_acc:0.617]
Epoch [54/120    avg_loss:0.780, val_acc:0.613]
Epoch [55/120    avg_loss:0.788, val_acc:0.610]
Epoch [56/120    avg_loss:0.789, val_acc:0.610]
Epoch [57/120    avg_loss:0.786, val_acc:0.610]
Epoch [58/120    avg_loss:0.814, val_acc:0.611]
Epoch [59/120    avg_loss:0.792, val_acc:0.612]
Epoch [60/120    avg_loss:0.792, val_acc:0.611]
Epoch [61/120    avg_loss:0.790, val_acc:0.612]
Epoch [62/120    avg_loss:0.787, val_acc:0.612]
Epoch [63/120    avg_loss:0.798, val_acc:0.612]
Epoch [64/120    avg_loss:0.788, val_acc:0.610]
Epoch [65/120    avg_loss:0.791, val_acc:0.612]
Epoch [66/120    avg_loss:0.793, val_acc:0.610]
Epoch [67/120    avg_loss:0.801, val_acc:0.608]
Epoch [68/120    avg_loss:0.787, val_acc:0.608]
Epoch [69/120    avg_loss:0.799, val_acc:0.608]
Epoch [70/120    avg_loss:0.847, val_acc:0.608]
Epoch [71/120    avg_loss:0.806, val_acc:0.608]
Epoch [72/120    avg_loss:0.817, val_acc:0.608]
Epoch [73/120    avg_loss:0.787, val_acc:0.608]
Epoch [74/120    avg_loss:0.776, val_acc:0.609]
Epoch [75/120    avg_loss:0.813, val_acc:0.608]
Epoch [76/120    avg_loss:0.797, val_acc:0.609]
Epoch [77/120    avg_loss:0.817, val_acc:0.608]
Epoch [78/120    avg_loss:0.825, val_acc:0.608]
Epoch [79/120    avg_loss:0.802, val_acc:0.609]
Epoch [80/120    avg_loss:0.786, val_acc:0.609]
Epoch [81/120    avg_loss:0.798, val_acc:0.609]
Epoch [82/120    avg_loss:0.793, val_acc:0.609]
Epoch [83/120    avg_loss:0.792, val_acc:0.609]
Epoch [84/120    avg_loss:0.795, val_acc:0.609]
Epoch [85/120    avg_loss:0.794, val_acc:0.609]
Epoch [86/120    avg_loss:0.796, val_acc:0.609]
Epoch [87/120    avg_loss:0.802, val_acc:0.609]
Epoch [88/120    avg_loss:0.810, val_acc:0.609]
Epoch [89/120    avg_loss:0.804, val_acc:0.609]
Epoch [90/120    avg_loss:0.811, val_acc:0.609]
Epoch [91/120    avg_loss:0.795, val_acc:0.609]
Epoch [92/120    avg_loss:0.814, val_acc:0.609]
Epoch [93/120    avg_loss:0.781, val_acc:0.609]
Epoch [94/120    avg_loss:0.829, val_acc:0.609]
Epoch [95/120    avg_loss:0.804, val_acc:0.609]
Epoch [96/120    avg_loss:0.803, val_acc:0.609]
Epoch [97/120    avg_loss:0.797, val_acc:0.609]
Epoch [98/120    avg_loss:0.810, val_acc:0.609]
Epoch [99/120    avg_loss:0.791, val_acc:0.609]
Epoch [100/120    avg_loss:0.821, val_acc:0.609]
Epoch [101/120    avg_loss:0.792, val_acc:0.609]
Epoch [102/120    avg_loss:0.789, val_acc:0.609]
Epoch [103/120    avg_loss:0.789, val_acc:0.609]
Epoch [104/120    avg_loss:0.789, val_acc:0.609]
Epoch [105/120    avg_loss:0.794, val_acc:0.609]
Epoch [106/120    avg_loss:0.804, val_acc:0.609]
Epoch [107/120    avg_loss:0.797, val_acc:0.609]
Epoch [108/120    avg_loss:0.784, val_acc:0.609]
Epoch [109/120    avg_loss:0.797, val_acc:0.609]
Epoch [110/120    avg_loss:0.790, val_acc:0.609]
Epoch [111/120    avg_loss:0.818, val_acc:0.609]
Epoch [112/120    avg_loss:0.804, val_acc:0.609]
Epoch [113/120    avg_loss:0.784, val_acc:0.609]
Epoch [114/120    avg_loss:0.790, val_acc:0.609]
Epoch [115/120    avg_loss:0.793, val_acc:0.609]
Epoch [116/120    avg_loss:0.788, val_acc:0.609]
Epoch [117/120    avg_loss:0.806, val_acc:0.609]
Epoch [118/120    avg_loss:0.810, val_acc:0.609]
Epoch [119/120    avg_loss:0.795, val_acc:0.609]
Epoch [120/120    avg_loss:0.804, val_acc:0.609]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0]
 [   0 4470  507  167   65    0  753   11  224  235]
 [   0   49 9643    0 2408    0 5989    0    1    0]
 [   0   28    3 1662   15    0    4    0  258   66]
 [   0    4  526    0 1962    0  449    0   31    0]
 [   0    0    0    0    0 1302    0    3    0    0]
 [   0    0  208  241  159    0 4223    0   47    0]
 [   0  100    1    0   35    0    0 1138   15    1]
 [   0  559  195  254   78    0  346    0 2139    0]
 [   0   20    0    3   18   67    4    0    0  807]]

Accuracy:
65.90509242522835

F1 scores:
[       nan 0.76659235 0.66109073 0.7618611  0.50881743 0.97382199
 0.50738916 0.93202293 0.68055997 0.79585799]

Kappa:
0.5799096026700369
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60a2b0dbe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.089, val_acc:0.078]
Epoch [2/120    avg_loss:1.846, val_acc:0.109]
Epoch [3/120    avg_loss:1.677, val_acc:0.128]
Epoch [4/120    avg_loss:1.546, val_acc:0.204]
Epoch [5/120    avg_loss:1.402, val_acc:0.287]
Epoch [6/120    avg_loss:1.339, val_acc:0.355]
Epoch [7/120    avg_loss:1.261, val_acc:0.395]
Epoch [8/120    avg_loss:1.164, val_acc:0.441]
Epoch [9/120    avg_loss:1.067, val_acc:0.443]
Epoch [10/120    avg_loss:0.952, val_acc:0.500]
Epoch [11/120    avg_loss:0.852, val_acc:0.517]
Epoch [12/120    avg_loss:0.764, val_acc:0.593]
Epoch [13/120    avg_loss:0.646, val_acc:0.695]
Epoch [14/120    avg_loss:0.538, val_acc:0.746]
Epoch [15/120    avg_loss:0.511, val_acc:0.752]
Epoch [16/120    avg_loss:0.498, val_acc:0.768]
Epoch [17/120    avg_loss:0.463, val_acc:0.790]
Epoch [18/120    avg_loss:0.387, val_acc:0.804]
Epoch [19/120    avg_loss:0.359, val_acc:0.771]
Epoch [20/120    avg_loss:0.322, val_acc:0.763]
Epoch [21/120    avg_loss:0.292, val_acc:0.838]
Epoch [22/120    avg_loss:0.261, val_acc:0.888]
Epoch [23/120    avg_loss:0.274, val_acc:0.819]
Epoch [24/120    avg_loss:0.261, val_acc:0.866]
Epoch [25/120    avg_loss:0.214, val_acc:0.887]
Epoch [26/120    avg_loss:0.215, val_acc:0.894]
Epoch [27/120    avg_loss:0.226, val_acc:0.910]
Epoch [28/120    avg_loss:0.220, val_acc:0.913]
Epoch [29/120    avg_loss:0.216, val_acc:0.890]
Epoch [30/120    avg_loss:0.261, val_acc:0.913]
Epoch [31/120    avg_loss:0.154, val_acc:0.924]
Epoch [32/120    avg_loss:0.136, val_acc:0.923]
Epoch [33/120    avg_loss:0.116, val_acc:0.896]
Epoch [34/120    avg_loss:0.095, val_acc:0.952]
Epoch [35/120    avg_loss:0.137, val_acc:0.935]
Epoch [36/120    avg_loss:0.097, val_acc:0.957]
Epoch [37/120    avg_loss:0.093, val_acc:0.944]
Epoch [38/120    avg_loss:0.099, val_acc:0.940]
Epoch [39/120    avg_loss:0.085, val_acc:0.961]
Epoch [40/120    avg_loss:0.079, val_acc:0.934]
Epoch [41/120    avg_loss:0.095, val_acc:0.957]
Epoch [42/120    avg_loss:0.071, val_acc:0.937]
Epoch [43/120    avg_loss:0.080, val_acc:0.965]
Epoch [44/120    avg_loss:0.049, val_acc:0.958]
Epoch [45/120    avg_loss:0.085, val_acc:0.951]
Epoch [46/120    avg_loss:0.094, val_acc:0.956]
Epoch [47/120    avg_loss:0.098, val_acc:0.957]
Epoch [48/120    avg_loss:0.117, val_acc:0.920]
Epoch [49/120    avg_loss:0.125, val_acc:0.957]
Epoch [50/120    avg_loss:0.070, val_acc:0.957]
Epoch [51/120    avg_loss:0.039, val_acc:0.969]
Epoch [52/120    avg_loss:0.049, val_acc:0.963]
Epoch [53/120    avg_loss:0.085, val_acc:0.966]
Epoch [54/120    avg_loss:0.046, val_acc:0.961]
Epoch [55/120    avg_loss:0.045, val_acc:0.966]
Epoch [56/120    avg_loss:0.039, val_acc:0.973]
Epoch [57/120    avg_loss:0.033, val_acc:0.969]
Epoch [58/120    avg_loss:0.028, val_acc:0.961]
Epoch [59/120    avg_loss:0.035, val_acc:0.975]
Epoch [60/120    avg_loss:0.033, val_acc:0.974]
Epoch [61/120    avg_loss:0.040, val_acc:0.980]
Epoch [62/120    avg_loss:0.016, val_acc:0.977]
Epoch [63/120    avg_loss:0.017, val_acc:0.974]
Epoch [64/120    avg_loss:0.019, val_acc:0.975]
Epoch [65/120    avg_loss:0.027, val_acc:0.976]
Epoch [66/120    avg_loss:0.016, val_acc:0.972]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.045, val_acc:0.959]
Epoch [69/120    avg_loss:0.033, val_acc:0.973]
Epoch [70/120    avg_loss:0.019, val_acc:0.963]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.015, val_acc:0.971]
Epoch [74/120    avg_loss:0.013, val_acc:0.976]
Epoch [75/120    avg_loss:0.016, val_acc:0.980]
Epoch [76/120    avg_loss:0.019, val_acc:0.980]
Epoch [77/120    avg_loss:0.015, val_acc:0.976]
Epoch [78/120    avg_loss:0.014, val_acc:0.978]
Epoch [79/120    avg_loss:0.017, val_acc:0.978]
Epoch [80/120    avg_loss:0.012, val_acc:0.978]
Epoch [81/120    avg_loss:0.015, val_acc:0.979]
Epoch [82/120    avg_loss:0.037, val_acc:0.968]
Epoch [83/120    avg_loss:0.026, val_acc:0.918]
Epoch [84/120    avg_loss:0.026, val_acc:0.968]
Epoch [85/120    avg_loss:0.042, val_acc:0.976]
Epoch [86/120    avg_loss:0.020, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.019, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.019, val_acc:0.983]
Epoch [91/120    avg_loss:0.016, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.982]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.026, val_acc:0.974]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.982]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0     0     0     0     1    56     0]
 [    0     3 17845     0    10     0   232     0     0     0]
 [    0     8     0  1973     0     0     0     0    54     1]
 [    0    53    15     0  2871     0     9     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    17    10     0     0  4849     0     1     0]
 [    0     2     0     0     0     0     1  1287     0     0]
 [    0    48     0    33    32     0     0     0  3458     0]
 [    0     0     0     1    14    24     0     0     0   880]]

Accuracy:
98.43347070590221

F1 scores:
[       nan 0.98668937 0.9922985  0.9735998  0.97338532 0.99088838
 0.97281573 0.99844841 0.96538247 0.97777778]

Kappa:
0.9792830579448855
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e8d79eba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.149, val_acc:0.074]
Epoch [2/120    avg_loss:1.926, val_acc:0.122]
Epoch [3/120    avg_loss:1.761, val_acc:0.237]
Epoch [4/120    avg_loss:1.614, val_acc:0.454]
Epoch [5/120    avg_loss:1.418, val_acc:0.598]
Epoch [6/120    avg_loss:1.281, val_acc:0.687]
Epoch [7/120    avg_loss:1.149, val_acc:0.733]
Epoch [8/120    avg_loss:0.994, val_acc:0.760]
Epoch [9/120    avg_loss:0.928, val_acc:0.795]
Epoch [10/120    avg_loss:0.787, val_acc:0.737]
Epoch [11/120    avg_loss:0.672, val_acc:0.773]
Epoch [12/120    avg_loss:0.560, val_acc:0.778]
Epoch [13/120    avg_loss:0.503, val_acc:0.774]
Epoch [14/120    avg_loss:0.469, val_acc:0.777]
Epoch [15/120    avg_loss:0.393, val_acc:0.754]
Epoch [16/120    avg_loss:0.341, val_acc:0.784]
Epoch [17/120    avg_loss:0.319, val_acc:0.782]
Epoch [18/120    avg_loss:0.314, val_acc:0.769]
Epoch [19/120    avg_loss:0.297, val_acc:0.848]
Epoch [20/120    avg_loss:0.252, val_acc:0.828]
Epoch [21/120    avg_loss:0.249, val_acc:0.833]
Epoch [22/120    avg_loss:0.227, val_acc:0.886]
Epoch [23/120    avg_loss:0.203, val_acc:0.897]
Epoch [24/120    avg_loss:0.174, val_acc:0.911]
Epoch [25/120    avg_loss:0.210, val_acc:0.925]
Epoch [26/120    avg_loss:0.185, val_acc:0.915]
Epoch [27/120    avg_loss:0.151, val_acc:0.953]
Epoch [28/120    avg_loss:0.143, val_acc:0.929]
Epoch [29/120    avg_loss:0.146, val_acc:0.941]
Epoch [30/120    avg_loss:0.127, val_acc:0.941]
Epoch [31/120    avg_loss:0.133, val_acc:0.930]
Epoch [32/120    avg_loss:0.119, val_acc:0.937]
Epoch [33/120    avg_loss:0.104, val_acc:0.955]
Epoch [34/120    avg_loss:0.071, val_acc:0.930]
Epoch [35/120    avg_loss:0.060, val_acc:0.961]
Epoch [36/120    avg_loss:0.057, val_acc:0.965]
Epoch [37/120    avg_loss:0.057, val_acc:0.969]
Epoch [38/120    avg_loss:0.055, val_acc:0.975]
Epoch [39/120    avg_loss:0.048, val_acc:0.975]
Epoch [40/120    avg_loss:0.047, val_acc:0.953]
Epoch [41/120    avg_loss:0.098, val_acc:0.929]
Epoch [42/120    avg_loss:0.066, val_acc:0.971]
Epoch [43/120    avg_loss:0.048, val_acc:0.982]
Epoch [44/120    avg_loss:0.040, val_acc:0.979]
Epoch [45/120    avg_loss:0.036, val_acc:0.972]
Epoch [46/120    avg_loss:0.063, val_acc:0.946]
Epoch [47/120    avg_loss:0.047, val_acc:0.971]
Epoch [48/120    avg_loss:0.064, val_acc:0.969]
Epoch [49/120    avg_loss:0.076, val_acc:0.980]
Epoch [50/120    avg_loss:0.030, val_acc:0.976]
Epoch [51/120    avg_loss:0.027, val_acc:0.983]
Epoch [52/120    avg_loss:0.037, val_acc:0.972]
Epoch [53/120    avg_loss:0.037, val_acc:0.982]
Epoch [54/120    avg_loss:0.023, val_acc:0.982]
Epoch [55/120    avg_loss:0.024, val_acc:0.984]
Epoch [56/120    avg_loss:0.051, val_acc:0.983]
Epoch [57/120    avg_loss:0.041, val_acc:0.975]
Epoch [58/120    avg_loss:0.037, val_acc:0.981]
Epoch [59/120    avg_loss:0.024, val_acc:0.986]
Epoch [60/120    avg_loss:0.016, val_acc:0.987]
Epoch [61/120    avg_loss:0.012, val_acc:0.985]
Epoch [62/120    avg_loss:0.014, val_acc:0.983]
Epoch [63/120    avg_loss:0.015, val_acc:0.980]
Epoch [64/120    avg_loss:0.020, val_acc:0.984]
Epoch [65/120    avg_loss:0.014, val_acc:0.985]
Epoch [66/120    avg_loss:0.018, val_acc:0.971]
Epoch [67/120    avg_loss:0.019, val_acc:0.983]
Epoch [68/120    avg_loss:0.015, val_acc:0.982]
Epoch [69/120    avg_loss:0.014, val_acc:0.981]
Epoch [70/120    avg_loss:0.018, val_acc:0.973]
Epoch [71/120    avg_loss:0.021, val_acc:0.983]
Epoch [72/120    avg_loss:0.021, val_acc:0.986]
Epoch [73/120    avg_loss:0.026, val_acc:0.943]
Epoch [74/120    avg_loss:0.018, val_acc:0.983]
Epoch [75/120    avg_loss:0.012, val_acc:0.985]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.009, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     0     0     2     0     0     0    13     9]
 [    0     0 18018     0    49     0    16     0     7     0]
 [    0    10     0  1983     0     0     0     0    38     5]
 [    0    34    13     0  2899     0     2     0    23     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     3     0     0  4866     0     0     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0    28     0    15    46     0     0     0  3479     3]
 [    0     0     0     2    14    48     0     0     0   855]]

Accuracy:
99.05767237847348

F1 scores:
[       nan 0.99248819 0.99739828 0.98192622 0.96924106 0.98194131
 0.99692686 0.99961225 0.97573973 0.95424107]

Kappa:
0.9875187244467029
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8473856b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.217, val_acc:0.189]
Epoch [2/120    avg_loss:1.925, val_acc:0.173]
Epoch [3/120    avg_loss:1.685, val_acc:0.207]
Epoch [4/120    avg_loss:1.463, val_acc:0.295]
Epoch [5/120    avg_loss:1.305, val_acc:0.348]
Epoch [6/120    avg_loss:1.109, val_acc:0.481]
Epoch [7/120    avg_loss:0.997, val_acc:0.533]
Epoch [8/120    avg_loss:0.883, val_acc:0.745]
Epoch [9/120    avg_loss:0.786, val_acc:0.738]
Epoch [10/120    avg_loss:0.727, val_acc:0.724]
Epoch [11/120    avg_loss:0.593, val_acc:0.785]
Epoch [12/120    avg_loss:0.544, val_acc:0.729]
Epoch [13/120    avg_loss:0.485, val_acc:0.808]
Epoch [14/120    avg_loss:0.485, val_acc:0.785]
Epoch [15/120    avg_loss:0.422, val_acc:0.803]
Epoch [16/120    avg_loss:0.475, val_acc:0.784]
Epoch [17/120    avg_loss:0.419, val_acc:0.903]
Epoch [18/120    avg_loss:0.313, val_acc:0.879]
Epoch [19/120    avg_loss:0.259, val_acc:0.803]
Epoch [20/120    avg_loss:0.267, val_acc:0.815]
Epoch [21/120    avg_loss:0.232, val_acc:0.899]
Epoch [22/120    avg_loss:0.188, val_acc:0.901]
Epoch [23/120    avg_loss:0.178, val_acc:0.920]
Epoch [24/120    avg_loss:0.159, val_acc:0.923]
Epoch [25/120    avg_loss:0.149, val_acc:0.940]
Epoch [26/120    avg_loss:0.109, val_acc:0.940]
Epoch [27/120    avg_loss:0.136, val_acc:0.914]
Epoch [28/120    avg_loss:0.106, val_acc:0.905]
Epoch [29/120    avg_loss:0.107, val_acc:0.953]
Epoch [30/120    avg_loss:0.110, val_acc:0.939]
Epoch [31/120    avg_loss:0.098, val_acc:0.952]
Epoch [32/120    avg_loss:0.208, val_acc:0.848]
Epoch [33/120    avg_loss:0.218, val_acc:0.885]
Epoch [34/120    avg_loss:0.136, val_acc:0.940]
Epoch [35/120    avg_loss:0.103, val_acc:0.936]
Epoch [36/120    avg_loss:0.141, val_acc:0.927]
Epoch [37/120    avg_loss:0.106, val_acc:0.951]
Epoch [38/120    avg_loss:0.086, val_acc:0.941]
Epoch [39/120    avg_loss:0.061, val_acc:0.952]
Epoch [40/120    avg_loss:0.062, val_acc:0.950]
Epoch [41/120    avg_loss:0.073, val_acc:0.964]
Epoch [42/120    avg_loss:0.052, val_acc:0.959]
Epoch [43/120    avg_loss:0.051, val_acc:0.964]
Epoch [44/120    avg_loss:0.046, val_acc:0.957]
Epoch [45/120    avg_loss:0.034, val_acc:0.964]
Epoch [46/120    avg_loss:0.042, val_acc:0.957]
Epoch [47/120    avg_loss:0.064, val_acc:0.951]
Epoch [48/120    avg_loss:0.067, val_acc:0.963]
Epoch [49/120    avg_loss:0.037, val_acc:0.961]
Epoch [50/120    avg_loss:0.032, val_acc:0.967]
Epoch [51/120    avg_loss:0.033, val_acc:0.963]
Epoch [52/120    avg_loss:0.023, val_acc:0.973]
Epoch [53/120    avg_loss:0.034, val_acc:0.966]
Epoch [54/120    avg_loss:0.027, val_acc:0.973]
Epoch [55/120    avg_loss:0.027, val_acc:0.970]
Epoch [56/120    avg_loss:0.022, val_acc:0.971]
Epoch [57/120    avg_loss:0.030, val_acc:0.968]
Epoch [58/120    avg_loss:0.022, val_acc:0.973]
Epoch [59/120    avg_loss:0.020, val_acc:0.970]
Epoch [60/120    avg_loss:0.022, val_acc:0.973]
Epoch [61/120    avg_loss:0.022, val_acc:0.967]
Epoch [62/120    avg_loss:0.069, val_acc:0.963]
Epoch [63/120    avg_loss:0.031, val_acc:0.963]
Epoch [64/120    avg_loss:0.043, val_acc:0.968]
Epoch [65/120    avg_loss:0.022, val_acc:0.974]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.012, val_acc:0.975]
Epoch [68/120    avg_loss:0.019, val_acc:0.974]
Epoch [69/120    avg_loss:0.021, val_acc:0.972]
Epoch [70/120    avg_loss:0.027, val_acc:0.958]
Epoch [71/120    avg_loss:0.027, val_acc:0.971]
Epoch [72/120    avg_loss:0.013, val_acc:0.974]
Epoch [73/120    avg_loss:0.024, val_acc:0.974]
Epoch [74/120    avg_loss:0.016, val_acc:0.977]
Epoch [75/120    avg_loss:0.014, val_acc:0.977]
Epoch [76/120    avg_loss:0.012, val_acc:0.976]
Epoch [77/120    avg_loss:0.012, val_acc:0.979]
Epoch [78/120    avg_loss:0.016, val_acc:0.967]
Epoch [79/120    avg_loss:0.021, val_acc:0.972]
Epoch [80/120    avg_loss:0.009, val_acc:0.976]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.009, val_acc:0.980]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.012, val_acc:0.974]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.006, val_acc:0.975]
Epoch [88/120    avg_loss:0.007, val_acc:0.975]
Epoch [89/120    avg_loss:0.010, val_acc:0.974]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.975]
Epoch [92/120    avg_loss:0.006, val_acc:0.977]
Epoch [93/120    avg_loss:0.006, val_acc:0.973]
Epoch [94/120    avg_loss:0.007, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.977]
Epoch [96/120    avg_loss:0.007, val_acc:0.977]
Epoch [97/120    avg_loss:0.005, val_acc:0.979]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.005, val_acc:0.978]
Epoch [100/120    avg_loss:0.005, val_acc:0.978]
Epoch [101/120    avg_loss:0.006, val_acc:0.976]
Epoch [102/120    avg_loss:0.005, val_acc:0.977]
Epoch [103/120    avg_loss:0.004, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.007, val_acc:0.977]
Epoch [106/120    avg_loss:0.007, val_acc:0.977]
Epoch [107/120    avg_loss:0.006, val_acc:0.976]
Epoch [108/120    avg_loss:0.006, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.978]
Epoch [110/120    avg_loss:0.004, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.979]
Epoch [112/120    avg_loss:0.006, val_acc:0.979]
Epoch [113/120    avg_loss:0.005, val_acc:0.978]
Epoch [114/120    avg_loss:0.005, val_acc:0.978]
Epoch [115/120    avg_loss:0.005, val_acc:0.978]
Epoch [116/120    avg_loss:0.006, val_acc:0.978]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.004, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.004, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     1     0     0     0     0    20     0]
 [    0     3 18008     0    66     0    13     0     0     0]
 [    0     1     0  1923     2     0     0     0   106     4]
 [    0    27    11     0  2912     0     5     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     1     0     0  4866     0     1     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    18     0    40    55     0     0     0  3458     0]
 [    0     0     0     4    11    21     0     0     0   883]]

Accuracy:
98.94681030535271

F1 scores:
[       nan 0.99457028 0.99714832 0.96029963 0.96776338 0.99201824
 0.99692686 1.         0.9641712  0.97785161]

Kappa:
0.9860521352565425
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd89bac5ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.185, val_acc:0.052]
Epoch [2/120    avg_loss:1.923, val_acc:0.251]
Epoch [3/120    avg_loss:1.688, val_acc:0.233]
Epoch [4/120    avg_loss:1.545, val_acc:0.293]
Epoch [5/120    avg_loss:1.392, val_acc:0.313]
Epoch [6/120    avg_loss:1.260, val_acc:0.354]
Epoch [7/120    avg_loss:1.130, val_acc:0.442]
Epoch [8/120    avg_loss:1.005, val_acc:0.584]
Epoch [9/120    avg_loss:0.879, val_acc:0.599]
Epoch [10/120    avg_loss:0.756, val_acc:0.681]
Epoch [11/120    avg_loss:0.659, val_acc:0.767]
Epoch [12/120    avg_loss:0.578, val_acc:0.780]
Epoch [13/120    avg_loss:0.504, val_acc:0.794]
Epoch [14/120    avg_loss:0.483, val_acc:0.801]
Epoch [15/120    avg_loss:0.459, val_acc:0.803]
Epoch [16/120    avg_loss:0.413, val_acc:0.802]
Epoch [17/120    avg_loss:0.353, val_acc:0.787]
Epoch [18/120    avg_loss:0.916, val_acc:0.604]
Epoch [19/120    avg_loss:0.942, val_acc:0.684]
Epoch [20/120    avg_loss:0.694, val_acc:0.726]
Epoch [21/120    avg_loss:0.581, val_acc:0.716]
Epoch [22/120    avg_loss:0.517, val_acc:0.759]
Epoch [23/120    avg_loss:0.445, val_acc:0.764]
Epoch [24/120    avg_loss:0.410, val_acc:0.756]
Epoch [25/120    avg_loss:0.389, val_acc:0.794]
Epoch [26/120    avg_loss:0.372, val_acc:0.823]
Epoch [27/120    avg_loss:0.324, val_acc:0.838]
Epoch [28/120    avg_loss:0.319, val_acc:0.852]
Epoch [29/120    avg_loss:0.290, val_acc:0.866]
Epoch [30/120    avg_loss:0.268, val_acc:0.899]
Epoch [31/120    avg_loss:0.254, val_acc:0.897]
Epoch [32/120    avg_loss:0.240, val_acc:0.916]
Epoch [33/120    avg_loss:0.204, val_acc:0.930]
Epoch [34/120    avg_loss:0.162, val_acc:0.915]
Epoch [35/120    avg_loss:0.161, val_acc:0.946]
Epoch [36/120    avg_loss:0.166, val_acc:0.946]
Epoch [37/120    avg_loss:0.171, val_acc:0.952]
Epoch [38/120    avg_loss:0.148, val_acc:0.939]
Epoch [39/120    avg_loss:0.147, val_acc:0.955]
Epoch [40/120    avg_loss:0.136, val_acc:0.891]
Epoch [41/120    avg_loss:0.113, val_acc:0.963]
Epoch [42/120    avg_loss:0.148, val_acc:0.959]
Epoch [43/120    avg_loss:0.096, val_acc:0.967]
Epoch [44/120    avg_loss:0.097, val_acc:0.968]
Epoch [45/120    avg_loss:0.093, val_acc:0.954]
Epoch [46/120    avg_loss:0.103, val_acc:0.964]
Epoch [47/120    avg_loss:0.093, val_acc:0.970]
Epoch [48/120    avg_loss:0.088, val_acc:0.938]
Epoch [49/120    avg_loss:0.075, val_acc:0.962]
Epoch [50/120    avg_loss:0.072, val_acc:0.956]
Epoch [51/120    avg_loss:0.062, val_acc:0.960]
Epoch [52/120    avg_loss:0.061, val_acc:0.977]
Epoch [53/120    avg_loss:0.055, val_acc:0.980]
Epoch [54/120    avg_loss:0.056, val_acc:0.933]
Epoch [55/120    avg_loss:0.074, val_acc:0.952]
Epoch [56/120    avg_loss:0.054, val_acc:0.965]
Epoch [57/120    avg_loss:0.042, val_acc:0.978]
Epoch [58/120    avg_loss:0.044, val_acc:0.974]
Epoch [59/120    avg_loss:0.048, val_acc:0.924]
Epoch [60/120    avg_loss:0.065, val_acc:0.974]
Epoch [61/120    avg_loss:0.042, val_acc:0.977]
Epoch [62/120    avg_loss:0.045, val_acc:0.978]
Epoch [63/120    avg_loss:0.039, val_acc:0.979]
Epoch [64/120    avg_loss:0.045, val_acc:0.982]
Epoch [65/120    avg_loss:0.050, val_acc:0.976]
Epoch [66/120    avg_loss:0.030, val_acc:0.973]
Epoch [67/120    avg_loss:0.033, val_acc:0.978]
Epoch [68/120    avg_loss:0.032, val_acc:0.976]
Epoch [69/120    avg_loss:0.044, val_acc:0.975]
Epoch [70/120    avg_loss:0.026, val_acc:0.981]
Epoch [71/120    avg_loss:0.028, val_acc:0.985]
Epoch [72/120    avg_loss:0.024, val_acc:0.976]
Epoch [73/120    avg_loss:0.022, val_acc:0.985]
Epoch [74/120    avg_loss:0.026, val_acc:0.987]
Epoch [75/120    avg_loss:0.032, val_acc:0.963]
Epoch [76/120    avg_loss:0.037, val_acc:0.981]
Epoch [77/120    avg_loss:0.034, val_acc:0.986]
Epoch [78/120    avg_loss:0.023, val_acc:0.988]
Epoch [79/120    avg_loss:0.028, val_acc:0.987]
Epoch [80/120    avg_loss:0.213, val_acc:0.916]
Epoch [81/120    avg_loss:0.162, val_acc:0.924]
Epoch [82/120    avg_loss:0.082, val_acc:0.961]
Epoch [83/120    avg_loss:0.074, val_acc:0.974]
Epoch [84/120    avg_loss:0.045, val_acc:0.974]
Epoch [85/120    avg_loss:0.063, val_acc:0.946]
Epoch [86/120    avg_loss:0.051, val_acc:0.971]
Epoch [87/120    avg_loss:0.062, val_acc:0.971]
Epoch [88/120    avg_loss:0.053, val_acc:0.970]
Epoch [89/120    avg_loss:0.047, val_acc:0.974]
Epoch [90/120    avg_loss:0.025, val_acc:0.982]
Epoch [91/120    avg_loss:0.029, val_acc:0.974]
Epoch [92/120    avg_loss:0.023, val_acc:0.982]
Epoch [93/120    avg_loss:0.019, val_acc:0.984]
Epoch [94/120    avg_loss:0.028, val_acc:0.984]
Epoch [95/120    avg_loss:0.021, val_acc:0.985]
Epoch [96/120    avg_loss:0.015, val_acc:0.986]
Epoch [97/120    avg_loss:0.017, val_acc:0.986]
Epoch [98/120    avg_loss:0.020, val_acc:0.986]
Epoch [99/120    avg_loss:0.017, val_acc:0.986]
Epoch [100/120    avg_loss:0.016, val_acc:0.984]
Epoch [101/120    avg_loss:0.017, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.986]
Epoch [103/120    avg_loss:0.015, val_acc:0.985]
Epoch [104/120    avg_loss:0.019, val_acc:0.985]
Epoch [105/120    avg_loss:0.017, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.017, val_acc:0.986]
Epoch [110/120    avg_loss:0.018, val_acc:0.986]
Epoch [111/120    avg_loss:0.016, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.986]
Epoch [114/120    avg_loss:0.012, val_acc:0.986]
Epoch [115/120    avg_loss:0.017, val_acc:0.985]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.985]
Epoch [118/120    avg_loss:0.016, val_acc:0.985]
Epoch [119/120    avg_loss:0.016, val_acc:0.985]
Epoch [120/120    avg_loss:0.014, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6342     0    20     3     0     0    17    38    12]
 [    0     4 17945     0    81     0    60     0     0     0]
 [    0     8     0  1939     0     0     0     0    86     3]
 [    0    24     0     0  2932     1    11     1     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    13     1     0  4858     0     5     0]
 [    0     7     0     0     0     6     0  1276     0     1]
 [    0     3     0     7    48     0     0     0  3513     0]
 [    0     0     0     0    14    44     0     0     0   861]]

Accuracy:
98.74195647458608

F1 scores:
[       nan 0.98939158 0.9959485  0.96587796 0.96909602 0.98083427
 0.99072091 0.9876161  0.97366962 0.95879733]

Kappa:
0.9833575721427177
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68edd7fb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.177, val_acc:0.339]
Epoch [2/120    avg_loss:1.938, val_acc:0.477]
Epoch [3/120    avg_loss:1.763, val_acc:0.544]
Epoch [4/120    avg_loss:1.548, val_acc:0.608]
Epoch [5/120    avg_loss:1.389, val_acc:0.637]
Epoch [6/120    avg_loss:1.209, val_acc:0.638]
Epoch [7/120    avg_loss:1.087, val_acc:0.686]
Epoch [8/120    avg_loss:0.931, val_acc:0.654]
Epoch [9/120    avg_loss:0.787, val_acc:0.664]
Epoch [10/120    avg_loss:0.670, val_acc:0.690]
Epoch [11/120    avg_loss:0.651, val_acc:0.685]
Epoch [12/120    avg_loss:0.558, val_acc:0.698]
Epoch [13/120    avg_loss:0.480, val_acc:0.731]
Epoch [14/120    avg_loss:0.422, val_acc:0.774]
Epoch [15/120    avg_loss:0.377, val_acc:0.793]
Epoch [16/120    avg_loss:0.311, val_acc:0.873]
Epoch [17/120    avg_loss:0.301, val_acc:0.886]
Epoch [18/120    avg_loss:0.308, val_acc:0.890]
Epoch [19/120    avg_loss:0.291, val_acc:0.883]
Epoch [20/120    avg_loss:0.245, val_acc:0.902]
Epoch [21/120    avg_loss:0.189, val_acc:0.910]
Epoch [22/120    avg_loss:0.210, val_acc:0.941]
Epoch [23/120    avg_loss:0.211, val_acc:0.784]
Epoch [24/120    avg_loss:0.191, val_acc:0.947]
Epoch [25/120    avg_loss:0.152, val_acc:0.931]
Epoch [26/120    avg_loss:0.130, val_acc:0.948]
Epoch [27/120    avg_loss:0.129, val_acc:0.933]
Epoch [28/120    avg_loss:0.133, val_acc:0.813]
Epoch [29/120    avg_loss:0.301, val_acc:0.928]
Epoch [30/120    avg_loss:0.170, val_acc:0.947]
Epoch [31/120    avg_loss:0.121, val_acc:0.927]
Epoch [32/120    avg_loss:0.160, val_acc:0.958]
Epoch [33/120    avg_loss:0.103, val_acc:0.922]
Epoch [34/120    avg_loss:0.108, val_acc:0.951]
Epoch [35/120    avg_loss:0.093, val_acc:0.920]
Epoch [36/120    avg_loss:0.098, val_acc:0.958]
Epoch [37/120    avg_loss:0.079, val_acc:0.968]
Epoch [38/120    avg_loss:0.071, val_acc:0.962]
Epoch [39/120    avg_loss:0.070, val_acc:0.966]
Epoch [40/120    avg_loss:0.051, val_acc:0.949]
Epoch [41/120    avg_loss:0.049, val_acc:0.968]
Epoch [42/120    avg_loss:0.072, val_acc:0.960]
Epoch [43/120    avg_loss:0.058, val_acc:0.975]
Epoch [44/120    avg_loss:0.054, val_acc:0.968]
Epoch [45/120    avg_loss:0.067, val_acc:0.963]
Epoch [46/120    avg_loss:0.057, val_acc:0.932]
Epoch [47/120    avg_loss:0.091, val_acc:0.937]
Epoch [48/120    avg_loss:0.052, val_acc:0.970]
Epoch [49/120    avg_loss:0.040, val_acc:0.963]
Epoch [50/120    avg_loss:0.041, val_acc:0.967]
Epoch [51/120    avg_loss:0.033, val_acc:0.967]
Epoch [52/120    avg_loss:0.033, val_acc:0.965]
Epoch [53/120    avg_loss:0.051, val_acc:0.968]
Epoch [54/120    avg_loss:0.033, val_acc:0.974]
Epoch [55/120    avg_loss:0.041, val_acc:0.967]
Epoch [56/120    avg_loss:0.040, val_acc:0.970]
Epoch [57/120    avg_loss:0.035, val_acc:0.973]
Epoch [58/120    avg_loss:0.025, val_acc:0.973]
Epoch [59/120    avg_loss:0.017, val_acc:0.974]
Epoch [60/120    avg_loss:0.020, val_acc:0.973]
Epoch [61/120    avg_loss:0.017, val_acc:0.976]
Epoch [62/120    avg_loss:0.017, val_acc:0.975]
Epoch [63/120    avg_loss:0.019, val_acc:0.977]
Epoch [64/120    avg_loss:0.015, val_acc:0.977]
Epoch [65/120    avg_loss:0.016, val_acc:0.974]
Epoch [66/120    avg_loss:0.016, val_acc:0.975]
Epoch [67/120    avg_loss:0.015, val_acc:0.975]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.973]
Epoch [70/120    avg_loss:0.016, val_acc:0.975]
Epoch [71/120    avg_loss:0.019, val_acc:0.975]
Epoch [72/120    avg_loss:0.014, val_acc:0.974]
Epoch [73/120    avg_loss:0.017, val_acc:0.975]
Epoch [74/120    avg_loss:0.013, val_acc:0.975]
Epoch [75/120    avg_loss:0.016, val_acc:0.977]
Epoch [76/120    avg_loss:0.013, val_acc:0.975]
Epoch [77/120    avg_loss:0.016, val_acc:0.975]
Epoch [78/120    avg_loss:0.017, val_acc:0.974]
Epoch [79/120    avg_loss:0.017, val_acc:0.974]
Epoch [80/120    avg_loss:0.016, val_acc:0.977]
Epoch [81/120    avg_loss:0.012, val_acc:0.976]
Epoch [82/120    avg_loss:0.012, val_acc:0.978]
Epoch [83/120    avg_loss:0.016, val_acc:0.977]
Epoch [84/120    avg_loss:0.016, val_acc:0.977]
Epoch [85/120    avg_loss:0.012, val_acc:0.977]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.975]
Epoch [88/120    avg_loss:0.016, val_acc:0.974]
Epoch [89/120    avg_loss:0.014, val_acc:0.976]
Epoch [90/120    avg_loss:0.014, val_acc:0.974]
Epoch [91/120    avg_loss:0.011, val_acc:0.975]
Epoch [92/120    avg_loss:0.013, val_acc:0.974]
Epoch [93/120    avg_loss:0.016, val_acc:0.974]
Epoch [94/120    avg_loss:0.013, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.978]
Epoch [96/120    avg_loss:0.012, val_acc:0.975]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.013, val_acc:0.977]
Epoch [99/120    avg_loss:0.010, val_acc:0.976]
Epoch [100/120    avg_loss:0.010, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.976]
Epoch [102/120    avg_loss:0.012, val_acc:0.975]
Epoch [103/120    avg_loss:0.012, val_acc:0.975]
Epoch [104/120    avg_loss:0.014, val_acc:0.973]
Epoch [105/120    avg_loss:0.011, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.977]
Epoch [107/120    avg_loss:0.012, val_acc:0.978]
Epoch [108/120    avg_loss:0.011, val_acc:0.977]
Epoch [109/120    avg_loss:0.014, val_acc:0.975]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.011, val_acc:0.977]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.014, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.976]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.009, val_acc:0.979]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6388     0     0     0     0     1     0    43     0]
 [    0     6 17805     0    99     0   167     0    13     0]
 [    0     4     0  1905     2     0     0     0   125     0]
 [    0    21     9     0  2909     0     5     0    26     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     3     0     0  4862     0     9     0]
 [    0     1     0     0     0     0     0  1287     2     0]
 [    0     5     0    23    54     0     0     0  3489     0]
 [    0     0     0     0    14    15     0     0     0   890]]

Accuracy:
98.42624057069868

F1 scores:
[       nan 0.99369993 0.99170101 0.96042349 0.96165289 0.99428571
 0.98093413 0.99883586 0.95877988 0.98288239]

Kappa:
0.9792047041727221
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38625cebe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.125, val_acc:0.062]
Epoch [2/120    avg_loss:1.873, val_acc:0.123]
Epoch [3/120    avg_loss:1.687, val_acc:0.146]
Epoch [4/120    avg_loss:1.470, val_acc:0.256]
Epoch [5/120    avg_loss:1.354, val_acc:0.333]
Epoch [6/120    avg_loss:1.226, val_acc:0.413]
Epoch [7/120    avg_loss:1.063, val_acc:0.486]
Epoch [8/120    avg_loss:0.911, val_acc:0.505]
Epoch [9/120    avg_loss:0.793, val_acc:0.684]
Epoch [10/120    avg_loss:0.655, val_acc:0.718]
Epoch [11/120    avg_loss:0.554, val_acc:0.709]
Epoch [12/120    avg_loss:0.514, val_acc:0.770]
Epoch [13/120    avg_loss:0.469, val_acc:0.734]
Epoch [14/120    avg_loss:0.421, val_acc:0.781]
Epoch [15/120    avg_loss:0.378, val_acc:0.858]
Epoch [16/120    avg_loss:0.333, val_acc:0.811]
Epoch [17/120    avg_loss:0.315, val_acc:0.851]
Epoch [18/120    avg_loss:0.278, val_acc:0.829]
Epoch [19/120    avg_loss:0.259, val_acc:0.912]
Epoch [20/120    avg_loss:0.242, val_acc:0.928]
Epoch [21/120    avg_loss:0.208, val_acc:0.897]
Epoch [22/120    avg_loss:0.184, val_acc:0.904]
Epoch [23/120    avg_loss:0.166, val_acc:0.906]
Epoch [24/120    avg_loss:0.176, val_acc:0.918]
Epoch [25/120    avg_loss:0.213, val_acc:0.861]
Epoch [26/120    avg_loss:0.243, val_acc:0.886]
Epoch [27/120    avg_loss:0.174, val_acc:0.940]
Epoch [28/120    avg_loss:0.138, val_acc:0.935]
Epoch [29/120    avg_loss:0.103, val_acc:0.942]
Epoch [30/120    avg_loss:0.118, val_acc:0.952]
Epoch [31/120    avg_loss:0.123, val_acc:0.967]
Epoch [32/120    avg_loss:0.084, val_acc:0.969]
Epoch [33/120    avg_loss:0.072, val_acc:0.963]
Epoch [34/120    avg_loss:0.160, val_acc:0.918]
Epoch [35/120    avg_loss:0.162, val_acc:0.946]
Epoch [36/120    avg_loss:0.089, val_acc:0.972]
Epoch [37/120    avg_loss:0.079, val_acc:0.968]
Epoch [38/120    avg_loss:0.071, val_acc:0.966]
Epoch [39/120    avg_loss:0.076, val_acc:0.946]
Epoch [40/120    avg_loss:0.087, val_acc:0.956]
Epoch [41/120    avg_loss:0.067, val_acc:0.949]
Epoch [42/120    avg_loss:0.060, val_acc:0.952]
Epoch [43/120    avg_loss:0.059, val_acc:0.962]
Epoch [44/120    avg_loss:0.124, val_acc:0.946]
Epoch [45/120    avg_loss:0.077, val_acc:0.940]
Epoch [46/120    avg_loss:0.049, val_acc:0.973]
Epoch [47/120    avg_loss:0.041, val_acc:0.979]
Epoch [48/120    avg_loss:0.071, val_acc:0.973]
Epoch [49/120    avg_loss:0.044, val_acc:0.981]
Epoch [50/120    avg_loss:0.033, val_acc:0.983]
Epoch [51/120    avg_loss:0.033, val_acc:0.979]
Epoch [52/120    avg_loss:0.027, val_acc:0.932]
Epoch [53/120    avg_loss:0.050, val_acc:0.976]
Epoch [54/120    avg_loss:0.047, val_acc:0.949]
Epoch [55/120    avg_loss:0.049, val_acc:0.979]
Epoch [56/120    avg_loss:0.427, val_acc:0.810]
Epoch [57/120    avg_loss:0.326, val_acc:0.921]
Epoch [58/120    avg_loss:0.135, val_acc:0.963]
Epoch [59/120    avg_loss:0.074, val_acc:0.967]
Epoch [60/120    avg_loss:0.065, val_acc:0.970]
Epoch [61/120    avg_loss:0.043, val_acc:0.967]
Epoch [62/120    avg_loss:0.058, val_acc:0.971]
Epoch [63/120    avg_loss:0.041, val_acc:0.978]
Epoch [64/120    avg_loss:0.023, val_acc:0.978]
Epoch [65/120    avg_loss:0.023, val_acc:0.978]
Epoch [66/120    avg_loss:0.020, val_acc:0.979]
Epoch [67/120    avg_loss:0.019, val_acc:0.980]
Epoch [68/120    avg_loss:0.024, val_acc:0.981]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.021, val_acc:0.982]
Epoch [71/120    avg_loss:0.020, val_acc:0.981]
Epoch [72/120    avg_loss:0.021, val_acc:0.983]
Epoch [73/120    avg_loss:0.018, val_acc:0.983]
Epoch [74/120    avg_loss:0.017, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.984]
Epoch [76/120    avg_loss:0.020, val_acc:0.981]
Epoch [77/120    avg_loss:0.017, val_acc:0.982]
Epoch [78/120    avg_loss:0.016, val_acc:0.984]
Epoch [79/120    avg_loss:0.016, val_acc:0.985]
Epoch [80/120    avg_loss:0.016, val_acc:0.982]
Epoch [81/120    avg_loss:0.014, val_acc:0.983]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.016, val_acc:0.982]
Epoch [84/120    avg_loss:0.017, val_acc:0.982]
Epoch [85/120    avg_loss:0.013, val_acc:0.985]
Epoch [86/120    avg_loss:0.014, val_acc:0.983]
Epoch [87/120    avg_loss:0.014, val_acc:0.983]
Epoch [88/120    avg_loss:0.017, val_acc:0.982]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.020, val_acc:0.982]
Epoch [91/120    avg_loss:0.013, val_acc:0.981]
Epoch [92/120    avg_loss:0.014, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.982]
Epoch [94/120    avg_loss:0.015, val_acc:0.982]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.017, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.016, val_acc:0.983]
Epoch [102/120    avg_loss:0.015, val_acc:0.985]
Epoch [103/120    avg_loss:0.015, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.013, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.985]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.015, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0     0     0     0    33    20     4]
 [    0     0 18061     0    20     0     5     0     4     0]
 [    0     8     0  1972     0     0     0     0    54     2]
 [    0    13     6     0  2946     0     1     0     5     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    41     0     0     0  4837     0     0     0]
 [    0     5     0     0     0     0     1  1281     0     3]
 [    0    12     0    18    49     0     0     0  3492     0]
 [    0     1     0     0     2    40     0     0     0   876]]

Accuracy:
99.16130431639071

F1 scores:
[       nan 0.99252686 0.99790044 0.97963239 0.98380364 0.98490566
 0.99506274 0.98387097 0.97732997 0.97063712]

Kappa:
0.9888854822581805
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32b44bcb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.121, val_acc:0.241]
Epoch [2/120    avg_loss:1.884, val_acc:0.321]
Epoch [3/120    avg_loss:1.708, val_acc:0.325]
Epoch [4/120    avg_loss:1.529, val_acc:0.355]
Epoch [5/120    avg_loss:1.385, val_acc:0.420]
Epoch [6/120    avg_loss:1.266, val_acc:0.446]
Epoch [7/120    avg_loss:1.123, val_acc:0.495]
Epoch [8/120    avg_loss:0.933, val_acc:0.517]
Epoch [9/120    avg_loss:0.792, val_acc:0.562]
Epoch [10/120    avg_loss:0.689, val_acc:0.560]
Epoch [11/120    avg_loss:0.598, val_acc:0.603]
Epoch [12/120    avg_loss:0.501, val_acc:0.626]
Epoch [13/120    avg_loss:0.433, val_acc:0.690]
Epoch [14/120    avg_loss:0.397, val_acc:0.714]
Epoch [15/120    avg_loss:0.351, val_acc:0.764]
Epoch [16/120    avg_loss:0.321, val_acc:0.751]
Epoch [17/120    avg_loss:0.377, val_acc:0.790]
Epoch [18/120    avg_loss:0.271, val_acc:0.834]
Epoch [19/120    avg_loss:0.273, val_acc:0.825]
Epoch [20/120    avg_loss:0.239, val_acc:0.856]
Epoch [21/120    avg_loss:0.231, val_acc:0.929]
Epoch [22/120    avg_loss:0.214, val_acc:0.846]
Epoch [23/120    avg_loss:0.185, val_acc:0.947]
Epoch [24/120    avg_loss:0.179, val_acc:0.927]
Epoch [25/120    avg_loss:0.138, val_acc:0.882]
Epoch [26/120    avg_loss:0.127, val_acc:0.946]
Epoch [27/120    avg_loss:0.132, val_acc:0.959]
Epoch [28/120    avg_loss:0.118, val_acc:0.957]
Epoch [29/120    avg_loss:0.095, val_acc:0.964]
Epoch [30/120    avg_loss:0.100, val_acc:0.966]
Epoch [31/120    avg_loss:0.106, val_acc:0.939]
Epoch [32/120    avg_loss:0.106, val_acc:0.959]
Epoch [33/120    avg_loss:0.070, val_acc:0.960]
Epoch [34/120    avg_loss:0.088, val_acc:0.958]
Epoch [35/120    avg_loss:0.073, val_acc:0.967]
Epoch [36/120    avg_loss:0.065, val_acc:0.955]
Epoch [37/120    avg_loss:0.056, val_acc:0.975]
Epoch [38/120    avg_loss:0.066, val_acc:0.977]
Epoch [39/120    avg_loss:0.049, val_acc:0.977]
Epoch [40/120    avg_loss:0.050, val_acc:0.969]
Epoch [41/120    avg_loss:0.051, val_acc:0.972]
Epoch [42/120    avg_loss:0.051, val_acc:0.981]
Epoch [43/120    avg_loss:0.042, val_acc:0.974]
Epoch [44/120    avg_loss:0.038, val_acc:0.981]
Epoch [45/120    avg_loss:0.033, val_acc:0.977]
Epoch [46/120    avg_loss:0.027, val_acc:0.983]
Epoch [47/120    avg_loss:0.031, val_acc:0.981]
Epoch [48/120    avg_loss:0.036, val_acc:0.981]
Epoch [49/120    avg_loss:0.030, val_acc:0.976]
Epoch [50/120    avg_loss:0.026, val_acc:0.975]
Epoch [51/120    avg_loss:0.021, val_acc:0.984]
Epoch [52/120    avg_loss:0.016, val_acc:0.983]
Epoch [53/120    avg_loss:0.038, val_acc:0.984]
Epoch [54/120    avg_loss:0.031, val_acc:0.973]
Epoch [55/120    avg_loss:0.031, val_acc:0.974]
Epoch [56/120    avg_loss:0.028, val_acc:0.976]
Epoch [57/120    avg_loss:0.030, val_acc:0.957]
Epoch [58/120    avg_loss:0.023, val_acc:0.961]
Epoch [59/120    avg_loss:0.048, val_acc:0.973]
Epoch [60/120    avg_loss:0.046, val_acc:0.967]
Epoch [61/120    avg_loss:0.033, val_acc:0.968]
Epoch [62/120    avg_loss:0.025, val_acc:0.974]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.037, val_acc:0.974]
Epoch [67/120    avg_loss:0.043, val_acc:0.957]
Epoch [68/120    avg_loss:0.022, val_acc:0.986]
Epoch [69/120    avg_loss:0.031, val_acc:0.981]
Epoch [70/120    avg_loss:0.016, val_acc:0.979]
Epoch [71/120    avg_loss:0.016, val_acc:0.962]
Epoch [72/120    avg_loss:0.014, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.990]
Epoch [74/120    avg_loss:0.010, val_acc:0.988]
Epoch [75/120    avg_loss:0.047, val_acc:0.975]
Epoch [76/120    avg_loss:0.018, val_acc:0.986]
Epoch [77/120    avg_loss:0.032, val_acc:0.978]
Epoch [78/120    avg_loss:0.017, val_acc:0.976]
Epoch [79/120    avg_loss:0.011, val_acc:0.988]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.015, val_acc:0.983]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.021, val_acc:0.990]
Epoch [86/120    avg_loss:0.009, val_acc:0.989]
Epoch [87/120    avg_loss:0.014, val_acc:0.988]
Epoch [88/120    avg_loss:0.020, val_acc:0.980]
Epoch [89/120    avg_loss:0.011, val_acc:0.989]
Epoch [90/120    avg_loss:0.012, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.030, val_acc:0.983]
Epoch [94/120    avg_loss:0.023, val_acc:0.970]
Epoch [95/120    avg_loss:0.016, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.989]
Epoch [97/120    avg_loss:0.010, val_acc:0.989]
Epoch [98/120    avg_loss:0.020, val_acc:0.974]
Epoch [99/120    avg_loss:0.016, val_acc:0.990]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.011, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     0     0     0     0    15    10]
 [    0     6 18051     0     6     0    27     0     0     0]
 [    0     9     0  1960     0     0     0     0    67     0]
 [    0    10     1     0  2958     0     0     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4875     0     0     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0    25     0    26    47     0     0     0  3473     0]
 [    0     0     0     0     5    18     0     0     0   896]]

Accuracy:
99.32277733593618

F1 scores:
[       nan 0.99418108 0.99881035 0.97463948 0.98797595 0.99315068
 0.99672869 0.99883586 0.9743302  0.98138007]

Kappa:
0.9910288374957698
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a485c9b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.152, val_acc:0.198]
Epoch [2/120    avg_loss:1.904, val_acc:0.259]
Epoch [3/120    avg_loss:1.708, val_acc:0.297]
Epoch [4/120    avg_loss:1.554, val_acc:0.348]
Epoch [5/120    avg_loss:1.359, val_acc:0.382]
Epoch [6/120    avg_loss:1.273, val_acc:0.418]
Epoch [7/120    avg_loss:1.132, val_acc:0.437]
Epoch [8/120    avg_loss:1.011, val_acc:0.494]
Epoch [9/120    avg_loss:0.874, val_acc:0.568]
Epoch [10/120    avg_loss:0.733, val_acc:0.647]
Epoch [11/120    avg_loss:0.657, val_acc:0.721]
Epoch [12/120    avg_loss:0.606, val_acc:0.771]
Epoch [13/120    avg_loss:0.527, val_acc:0.793]
Epoch [14/120    avg_loss:0.467, val_acc:0.776]
Epoch [15/120    avg_loss:0.433, val_acc:0.754]
Epoch [16/120    avg_loss:0.379, val_acc:0.759]
Epoch [17/120    avg_loss:0.346, val_acc:0.840]
Epoch [18/120    avg_loss:0.339, val_acc:0.819]
Epoch [19/120    avg_loss:0.308, val_acc:0.858]
Epoch [20/120    avg_loss:0.262, val_acc:0.810]
Epoch [21/120    avg_loss:0.250, val_acc:0.866]
Epoch [22/120    avg_loss:0.241, val_acc:0.883]
Epoch [23/120    avg_loss:0.213, val_acc:0.892]
Epoch [24/120    avg_loss:0.233, val_acc:0.883]
Epoch [25/120    avg_loss:0.216, val_acc:0.892]
Epoch [26/120    avg_loss:0.175, val_acc:0.928]
Epoch [27/120    avg_loss:0.156, val_acc:0.927]
Epoch [28/120    avg_loss:0.145, val_acc:0.934]
Epoch [29/120    avg_loss:0.126, val_acc:0.926]
Epoch [30/120    avg_loss:0.125, val_acc:0.952]
Epoch [31/120    avg_loss:0.116, val_acc:0.955]
Epoch [32/120    avg_loss:0.100, val_acc:0.912]
Epoch [33/120    avg_loss:0.106, val_acc:0.956]
Epoch [34/120    avg_loss:0.098, val_acc:0.943]
Epoch [35/120    avg_loss:0.103, val_acc:0.964]
Epoch [36/120    avg_loss:0.089, val_acc:0.952]
Epoch [37/120    avg_loss:0.088, val_acc:0.974]
Epoch [38/120    avg_loss:0.096, val_acc:0.972]
Epoch [39/120    avg_loss:0.075, val_acc:0.968]
Epoch [40/120    avg_loss:0.068, val_acc:0.974]
Epoch [41/120    avg_loss:0.056, val_acc:0.976]
Epoch [42/120    avg_loss:0.065, val_acc:0.980]
Epoch [43/120    avg_loss:0.051, val_acc:0.979]
Epoch [44/120    avg_loss:0.046, val_acc:0.974]
Epoch [45/120    avg_loss:0.069, val_acc:0.967]
Epoch [46/120    avg_loss:0.145, val_acc:0.965]
Epoch [47/120    avg_loss:0.061, val_acc:0.969]
Epoch [48/120    avg_loss:0.066, val_acc:0.980]
Epoch [49/120    avg_loss:0.056, val_acc:0.970]
Epoch [50/120    avg_loss:0.037, val_acc:0.977]
Epoch [51/120    avg_loss:0.031, val_acc:0.980]
Epoch [52/120    avg_loss:0.038, val_acc:0.985]
Epoch [53/120    avg_loss:0.031, val_acc:0.977]
Epoch [54/120    avg_loss:0.044, val_acc:0.975]
Epoch [55/120    avg_loss:0.087, val_acc:0.959]
Epoch [56/120    avg_loss:0.262, val_acc:0.912]
Epoch [57/120    avg_loss:0.151, val_acc:0.964]
Epoch [58/120    avg_loss:0.130, val_acc:0.952]
Epoch [59/120    avg_loss:0.061, val_acc:0.965]
Epoch [60/120    avg_loss:0.056, val_acc:0.961]
Epoch [61/120    avg_loss:0.045, val_acc:0.979]
Epoch [62/120    avg_loss:0.048, val_acc:0.970]
Epoch [63/120    avg_loss:0.036, val_acc:0.980]
Epoch [64/120    avg_loss:0.035, val_acc:0.978]
Epoch [65/120    avg_loss:0.047, val_acc:0.974]
Epoch [66/120    avg_loss:0.033, val_acc:0.984]
Epoch [67/120    avg_loss:0.022, val_acc:0.986]
Epoch [68/120    avg_loss:0.025, val_acc:0.986]
Epoch [69/120    avg_loss:0.024, val_acc:0.986]
Epoch [70/120    avg_loss:0.023, val_acc:0.985]
Epoch [71/120    avg_loss:0.023, val_acc:0.985]
Epoch [72/120    avg_loss:0.018, val_acc:0.987]
Epoch [73/120    avg_loss:0.022, val_acc:0.986]
Epoch [74/120    avg_loss:0.019, val_acc:0.987]
Epoch [75/120    avg_loss:0.024, val_acc:0.986]
Epoch [76/120    avg_loss:0.017, val_acc:0.986]
Epoch [77/120    avg_loss:0.016, val_acc:0.986]
Epoch [78/120    avg_loss:0.015, val_acc:0.988]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.015, val_acc:0.988]
Epoch [81/120    avg_loss:0.021, val_acc:0.986]
Epoch [82/120    avg_loss:0.025, val_acc:0.987]
Epoch [83/120    avg_loss:0.018, val_acc:0.988]
Epoch [84/120    avg_loss:0.014, val_acc:0.989]
Epoch [85/120    avg_loss:0.021, val_acc:0.990]
Epoch [86/120    avg_loss:0.015, val_acc:0.988]
Epoch [87/120    avg_loss:0.014, val_acc:0.987]
Epoch [88/120    avg_loss:0.018, val_acc:0.988]
Epoch [89/120    avg_loss:0.017, val_acc:0.987]
Epoch [90/120    avg_loss:0.017, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.987]
Epoch [92/120    avg_loss:0.021, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.988]
Epoch [94/120    avg_loss:0.015, val_acc:0.986]
Epoch [95/120    avg_loss:0.017, val_acc:0.988]
Epoch [96/120    avg_loss:0.014, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.987]
Epoch [98/120    avg_loss:0.013, val_acc:0.987]
Epoch [99/120    avg_loss:0.015, val_acc:0.987]
Epoch [100/120    avg_loss:0.016, val_acc:0.987]
Epoch [101/120    avg_loss:0.013, val_acc:0.987]
Epoch [102/120    avg_loss:0.018, val_acc:0.987]
Epoch [103/120    avg_loss:0.017, val_acc:0.987]
Epoch [104/120    avg_loss:0.015, val_acc:0.987]
Epoch [105/120    avg_loss:0.017, val_acc:0.987]
Epoch [106/120    avg_loss:0.014, val_acc:0.987]
Epoch [107/120    avg_loss:0.016, val_acc:0.987]
Epoch [108/120    avg_loss:0.017, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.987]
Epoch [110/120    avg_loss:0.014, val_acc:0.987]
Epoch [111/120    avg_loss:0.014, val_acc:0.987]
Epoch [112/120    avg_loss:0.014, val_acc:0.987]
Epoch [113/120    avg_loss:0.016, val_acc:0.987]
Epoch [114/120    avg_loss:0.017, val_acc:0.987]
Epoch [115/120    avg_loss:0.012, val_acc:0.987]
Epoch [116/120    avg_loss:0.014, val_acc:0.987]
Epoch [117/120    avg_loss:0.016, val_acc:0.987]
Epoch [118/120    avg_loss:0.020, val_acc:0.988]
Epoch [119/120    avg_loss:0.021, val_acc:0.988]
Epoch [120/120    avg_loss:0.019, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0    12     0     0     0     6     3    12]
 [    0     1 18068     0    14     0     7     0     0     0]
 [    0    10     0  1990     0     0     0     0    36     0]
 [    0    22     8     4  2930     0     2     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29    12     0     0  4837     0     0     0]
 [    0     3     0     0     0     0     0  1286     0     1]
 [    0    39     0    33    55     0     0     0  3444     0]
 [    0     0     0     0     8    20     0     0     0   891]]

Accuracy:
99.17335454172994

F1 scores:
[       nan 0.9916318  0.99836994 0.97381943 0.98009701 0.99239544
 0.99485808 0.99612703 0.97563739 0.9775096 ]

Kappa:
0.9890444075852838
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93c13bcc50>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.237, val_acc:0.210]
Epoch [2/120    avg_loss:2.006, val_acc:0.247]
Epoch [3/120    avg_loss:1.802, val_acc:0.270]
Epoch [4/120    avg_loss:1.629, val_acc:0.302]
Epoch [5/120    avg_loss:1.453, val_acc:0.330]
Epoch [6/120    avg_loss:1.305, val_acc:0.388]
Epoch [7/120    avg_loss:1.200, val_acc:0.394]
Epoch [8/120    avg_loss:1.095, val_acc:0.420]
Epoch [9/120    avg_loss:0.938, val_acc:0.447]
Epoch [10/120    avg_loss:0.838, val_acc:0.590]
Epoch [11/120    avg_loss:0.717, val_acc:0.645]
Epoch [12/120    avg_loss:0.639, val_acc:0.754]
Epoch [13/120    avg_loss:0.525, val_acc:0.759]
Epoch [14/120    avg_loss:0.493, val_acc:0.781]
Epoch [15/120    avg_loss:0.438, val_acc:0.758]
Epoch [16/120    avg_loss:0.399, val_acc:0.749]
Epoch [17/120    avg_loss:0.386, val_acc:0.801]
Epoch [18/120    avg_loss:0.333, val_acc:0.872]
Epoch [19/120    avg_loss:0.282, val_acc:0.884]
Epoch [20/120    avg_loss:0.253, val_acc:0.920]
Epoch [21/120    avg_loss:0.325, val_acc:0.884]
Epoch [22/120    avg_loss:0.233, val_acc:0.936]
Epoch [23/120    avg_loss:0.248, val_acc:0.914]
Epoch [24/120    avg_loss:0.199, val_acc:0.923]
Epoch [25/120    avg_loss:0.164, val_acc:0.936]
Epoch [26/120    avg_loss:0.149, val_acc:0.948]
Epoch [27/120    avg_loss:0.120, val_acc:0.957]
Epoch [28/120    avg_loss:0.164, val_acc:0.943]
Epoch [29/120    avg_loss:0.123, val_acc:0.952]
Epoch [30/120    avg_loss:0.169, val_acc:0.941]
Epoch [31/120    avg_loss:0.137, val_acc:0.956]
Epoch [32/120    avg_loss:0.102, val_acc:0.962]
Epoch [33/120    avg_loss:0.076, val_acc:0.967]
Epoch [34/120    avg_loss:0.071, val_acc:0.974]
Epoch [35/120    avg_loss:0.071, val_acc:0.974]
Epoch [36/120    avg_loss:0.067, val_acc:0.970]
Epoch [37/120    avg_loss:0.063, val_acc:0.977]
Epoch [38/120    avg_loss:0.049, val_acc:0.973]
Epoch [39/120    avg_loss:0.061, val_acc:0.953]
Epoch [40/120    avg_loss:0.065, val_acc:0.965]
Epoch [41/120    avg_loss:0.057, val_acc:0.980]
Epoch [42/120    avg_loss:0.061, val_acc:0.975]
Epoch [43/120    avg_loss:0.048, val_acc:0.938]
Epoch [44/120    avg_loss:0.070, val_acc:0.963]
Epoch [45/120    avg_loss:0.047, val_acc:0.976]
Epoch [46/120    avg_loss:0.043, val_acc:0.983]
Epoch [47/120    avg_loss:0.034, val_acc:0.975]
Epoch [48/120    avg_loss:0.030, val_acc:0.982]
Epoch [49/120    avg_loss:0.052, val_acc:0.949]
Epoch [50/120    avg_loss:0.038, val_acc:0.984]
Epoch [51/120    avg_loss:0.028, val_acc:0.986]
Epoch [52/120    avg_loss:0.037, val_acc:0.987]
Epoch [53/120    avg_loss:0.019, val_acc:0.985]
Epoch [54/120    avg_loss:0.023, val_acc:0.986]
Epoch [55/120    avg_loss:0.026, val_acc:0.988]
Epoch [56/120    avg_loss:0.022, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.986]
Epoch [58/120    avg_loss:0.015, val_acc:0.984]
Epoch [59/120    avg_loss:0.020, val_acc:0.988]
Epoch [60/120    avg_loss:0.011, val_acc:0.989]
Epoch [61/120    avg_loss:0.013, val_acc:0.988]
Epoch [62/120    avg_loss:0.025, val_acc:0.982]
Epoch [63/120    avg_loss:0.013, val_acc:0.986]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.012, val_acc:0.987]
Epoch [67/120    avg_loss:0.016, val_acc:0.983]
Epoch [68/120    avg_loss:0.012, val_acc:0.988]
Epoch [69/120    avg_loss:0.010, val_acc:0.987]
Epoch [70/120    avg_loss:0.016, val_acc:0.991]
Epoch [71/120    avg_loss:0.041, val_acc:0.980]
Epoch [72/120    avg_loss:0.072, val_acc:0.967]
Epoch [73/120    avg_loss:0.036, val_acc:0.982]
Epoch [74/120    avg_loss:0.072, val_acc:0.974]
Epoch [75/120    avg_loss:0.052, val_acc:0.983]
Epoch [76/120    avg_loss:0.037, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.985]
Epoch [78/120    avg_loss:0.025, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.989]
Epoch [80/120    avg_loss:0.009, val_acc:0.991]
Epoch [81/120    avg_loss:0.015, val_acc:0.985]
Epoch [82/120    avg_loss:0.014, val_acc:0.985]
Epoch [83/120    avg_loss:0.015, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.991]
Epoch [85/120    avg_loss:0.009, val_acc:0.990]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.009, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.990]
Epoch [92/120    avg_loss:0.008, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.991]
Epoch [94/120    avg_loss:0.008, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.008, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.989]
Epoch [105/120    avg_loss:0.007, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     2     0     0     1    15     2]
 [    0     0 18051     0    34     0     5     0     0     0]
 [    0     0     0  2003     0     0     0     0    31     2]
 [    0    24    16     0  2914     0     4     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    12     0     0  4864     0     2     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     3     0    20    50     0     0     0  3494     4]
 [    0     0     0     0    14    44     0     0     0   861]]

Accuracy:
99.27698647964718

F1 scores:
[       nan 0.99634838 0.99847886 0.98403341 0.97360508 0.98342125
 0.99764127 0.99922481 0.9806343  0.96201117]

Kappa:
0.9904219670626129
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f18bda35b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.174, val_acc:0.082]
Epoch [2/120    avg_loss:1.948, val_acc:0.119]
Epoch [3/120    avg_loss:1.778, val_acc:0.151]
Epoch [4/120    avg_loss:1.651, val_acc:0.186]
Epoch [5/120    avg_loss:1.465, val_acc:0.294]
Epoch [6/120    avg_loss:1.333, val_acc:0.338]
Epoch [7/120    avg_loss:1.233, val_acc:0.378]
Epoch [8/120    avg_loss:1.111, val_acc:0.424]
Epoch [9/120    avg_loss:1.009, val_acc:0.636]
Epoch [10/120    avg_loss:0.864, val_acc:0.674]
Epoch [11/120    avg_loss:0.793, val_acc:0.748]
Epoch [12/120    avg_loss:0.648, val_acc:0.770]
Epoch [13/120    avg_loss:0.531, val_acc:0.815]
Epoch [14/120    avg_loss:0.462, val_acc:0.848]
Epoch [15/120    avg_loss:0.413, val_acc:0.817]
Epoch [16/120    avg_loss:0.383, val_acc:0.808]
Epoch [17/120    avg_loss:0.374, val_acc:0.852]
Epoch [18/120    avg_loss:0.330, val_acc:0.858]
Epoch [19/120    avg_loss:0.294, val_acc:0.888]
Epoch [20/120    avg_loss:0.342, val_acc:0.852]
Epoch [21/120    avg_loss:0.281, val_acc:0.903]
Epoch [22/120    avg_loss:0.229, val_acc:0.913]
Epoch [23/120    avg_loss:0.233, val_acc:0.880]
Epoch [24/120    avg_loss:0.221, val_acc:0.876]
Epoch [25/120    avg_loss:0.177, val_acc:0.925]
Epoch [26/120    avg_loss:0.148, val_acc:0.931]
Epoch [27/120    avg_loss:0.154, val_acc:0.951]
Epoch [28/120    avg_loss:0.177, val_acc:0.941]
Epoch [29/120    avg_loss:0.125, val_acc:0.931]
Epoch [30/120    avg_loss:0.108, val_acc:0.930]
Epoch [31/120    avg_loss:0.126, val_acc:0.935]
Epoch [32/120    avg_loss:0.111, val_acc:0.950]
Epoch [33/120    avg_loss:0.084, val_acc:0.962]
Epoch [34/120    avg_loss:0.089, val_acc:0.965]
Epoch [35/120    avg_loss:0.092, val_acc:0.954]
Epoch [36/120    avg_loss:0.087, val_acc:0.912]
Epoch [37/120    avg_loss:0.105, val_acc:0.953]
Epoch [38/120    avg_loss:0.086, val_acc:0.944]
Epoch [39/120    avg_loss:0.068, val_acc:0.941]
Epoch [40/120    avg_loss:0.062, val_acc:0.961]
Epoch [41/120    avg_loss:0.071, val_acc:0.970]
Epoch [42/120    avg_loss:0.093, val_acc:0.949]
Epoch [43/120    avg_loss:0.086, val_acc:0.962]
Epoch [44/120    avg_loss:0.051, val_acc:0.973]
Epoch [45/120    avg_loss:0.041, val_acc:0.968]
Epoch [46/120    avg_loss:0.070, val_acc:0.967]
Epoch [47/120    avg_loss:0.048, val_acc:0.978]
Epoch [48/120    avg_loss:0.042, val_acc:0.982]
Epoch [49/120    avg_loss:0.063, val_acc:0.965]
Epoch [50/120    avg_loss:0.061, val_acc:0.970]
Epoch [51/120    avg_loss:0.036, val_acc:0.975]
Epoch [52/120    avg_loss:0.037, val_acc:0.966]
Epoch [53/120    avg_loss:0.047, val_acc:0.974]
Epoch [54/120    avg_loss:0.032, val_acc:0.981]
Epoch [55/120    avg_loss:0.047, val_acc:0.958]
Epoch [56/120    avg_loss:0.046, val_acc:0.975]
Epoch [57/120    avg_loss:0.031, val_acc:0.980]
Epoch [58/120    avg_loss:0.020, val_acc:0.971]
Epoch [59/120    avg_loss:0.023, val_acc:0.970]
Epoch [60/120    avg_loss:0.032, val_acc:0.968]
Epoch [61/120    avg_loss:0.023, val_acc:0.977]
Epoch [62/120    avg_loss:0.036, val_acc:0.986]
Epoch [63/120    avg_loss:0.015, val_acc:0.985]
Epoch [64/120    avg_loss:0.013, val_acc:0.984]
Epoch [65/120    avg_loss:0.017, val_acc:0.985]
Epoch [66/120    avg_loss:0.015, val_acc:0.986]
Epoch [67/120    avg_loss:0.017, val_acc:0.984]
Epoch [68/120    avg_loss:0.015, val_acc:0.984]
Epoch [69/120    avg_loss:0.014, val_acc:0.985]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.014, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.019, val_acc:0.984]
Epoch [74/120    avg_loss:0.013, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.986]
Epoch [76/120    avg_loss:0.015, val_acc:0.984]
Epoch [77/120    avg_loss:0.012, val_acc:0.984]
Epoch [78/120    avg_loss:0.013, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.984]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.015, val_acc:0.987]
Epoch [83/120    avg_loss:0.013, val_acc:0.987]
Epoch [84/120    avg_loss:0.015, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.986]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.011, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.014, val_acc:0.985]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.985]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.012, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.984]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.012, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.984]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.015, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     0     0     0     0     3     7]
 [    0     2 18017     0    58     0    13     0     0     0]
 [    0    15     0  2000     0     0     0     0    21     0]
 [    0    41    14     0  2890     0     2     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     5     0     0  4860     0     0     0]
 [    0     2     0     0     0     0     1  1269     0    18]
 [    0    30     0    12    80     0     0     0  3442     7]
 [    0     0     0     0    19    51     0     0     0   849]]

Accuracy:
98.94199021521703

F1 scores:
[       nan 0.99227441 0.99723252 0.98692327 0.96029241 0.98083427
 0.99651425 0.99179367 0.97479468 0.94333333]

Kappa:
0.9859853074022599
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f918136fba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.181, val_acc:0.159]
Epoch [2/120    avg_loss:1.887, val_acc:0.170]
Epoch [3/120    avg_loss:1.653, val_acc:0.160]
Epoch [4/120    avg_loss:1.510, val_acc:0.256]
Epoch [5/120    avg_loss:1.393, val_acc:0.279]
Epoch [6/120    avg_loss:1.314, val_acc:0.351]
Epoch [7/120    avg_loss:1.156, val_acc:0.438]
Epoch [8/120    avg_loss:1.051, val_acc:0.450]
Epoch [9/120    avg_loss:0.955, val_acc:0.455]
Epoch [10/120    avg_loss:0.805, val_acc:0.520]
Epoch [11/120    avg_loss:0.685, val_acc:0.638]
Epoch [12/120    avg_loss:0.617, val_acc:0.767]
Epoch [13/120    avg_loss:0.528, val_acc:0.802]
Epoch [14/120    avg_loss:0.419, val_acc:0.851]
Epoch [15/120    avg_loss:0.413, val_acc:0.839]
Epoch [16/120    avg_loss:0.368, val_acc:0.864]
Epoch [17/120    avg_loss:0.474, val_acc:0.897]
Epoch [18/120    avg_loss:0.318, val_acc:0.853]
Epoch [19/120    avg_loss:0.308, val_acc:0.870]
Epoch [20/120    avg_loss:0.262, val_acc:0.924]
Epoch [21/120    avg_loss:0.218, val_acc:0.941]
Epoch [22/120    avg_loss:0.227, val_acc:0.951]
Epoch [23/120    avg_loss:0.182, val_acc:0.949]
Epoch [24/120    avg_loss:0.154, val_acc:0.853]
Epoch [25/120    avg_loss:0.148, val_acc:0.944]
Epoch [26/120    avg_loss:0.121, val_acc:0.962]
Epoch [27/120    avg_loss:0.138, val_acc:0.962]
Epoch [28/120    avg_loss:0.143, val_acc:0.940]
Epoch [29/120    avg_loss:0.126, val_acc:0.960]
Epoch [30/120    avg_loss:0.113, val_acc:0.971]
Epoch [31/120    avg_loss:0.100, val_acc:0.980]
Epoch [32/120    avg_loss:0.074, val_acc:0.983]
Epoch [33/120    avg_loss:0.071, val_acc:0.975]
Epoch [34/120    avg_loss:0.129, val_acc:0.946]
Epoch [35/120    avg_loss:0.098, val_acc:0.971]
Epoch [36/120    avg_loss:0.059, val_acc:0.979]
Epoch [37/120    avg_loss:0.107, val_acc:0.934]
Epoch [38/120    avg_loss:0.137, val_acc:0.979]
Epoch [39/120    avg_loss:0.058, val_acc:0.979]
Epoch [40/120    avg_loss:0.064, val_acc:0.972]
Epoch [41/120    avg_loss:0.047, val_acc:0.980]
Epoch [42/120    avg_loss:0.058, val_acc:0.952]
Epoch [43/120    avg_loss:0.088, val_acc:0.975]
Epoch [44/120    avg_loss:0.065, val_acc:0.976]
Epoch [45/120    avg_loss:0.042, val_acc:0.979]
Epoch [46/120    avg_loss:0.037, val_acc:0.986]
Epoch [47/120    avg_loss:0.034, val_acc:0.986]
Epoch [48/120    avg_loss:0.026, val_acc:0.986]
Epoch [49/120    avg_loss:0.025, val_acc:0.986]
Epoch [50/120    avg_loss:0.029, val_acc:0.985]
Epoch [51/120    avg_loss:0.029, val_acc:0.987]
Epoch [52/120    avg_loss:0.023, val_acc:0.986]
Epoch [53/120    avg_loss:0.023, val_acc:0.986]
Epoch [54/120    avg_loss:0.028, val_acc:0.986]
Epoch [55/120    avg_loss:0.024, val_acc:0.987]
Epoch [56/120    avg_loss:0.025, val_acc:0.987]
Epoch [57/120    avg_loss:0.024, val_acc:0.988]
Epoch [58/120    avg_loss:0.024, val_acc:0.986]
Epoch [59/120    avg_loss:0.024, val_acc:0.988]
Epoch [60/120    avg_loss:0.026, val_acc:0.987]
Epoch [61/120    avg_loss:0.019, val_acc:0.986]
Epoch [62/120    avg_loss:0.021, val_acc:0.986]
Epoch [63/120    avg_loss:0.022, val_acc:0.987]
Epoch [64/120    avg_loss:0.022, val_acc:0.986]
Epoch [65/120    avg_loss:0.018, val_acc:0.986]
Epoch [66/120    avg_loss:0.027, val_acc:0.987]
Epoch [67/120    avg_loss:0.019, val_acc:0.987]
Epoch [68/120    avg_loss:0.019, val_acc:0.987]
Epoch [69/120    avg_loss:0.021, val_acc:0.988]
Epoch [70/120    avg_loss:0.025, val_acc:0.988]
Epoch [71/120    avg_loss:0.016, val_acc:0.988]
Epoch [72/120    avg_loss:0.019, val_acc:0.987]
Epoch [73/120    avg_loss:0.022, val_acc:0.988]
Epoch [74/120    avg_loss:0.018, val_acc:0.986]
Epoch [75/120    avg_loss:0.018, val_acc:0.986]
Epoch [76/120    avg_loss:0.020, val_acc:0.987]
Epoch [77/120    avg_loss:0.019, val_acc:0.988]
Epoch [78/120    avg_loss:0.018, val_acc:0.986]
Epoch [79/120    avg_loss:0.017, val_acc:0.989]
Epoch [80/120    avg_loss:0.020, val_acc:0.988]
Epoch [81/120    avg_loss:0.014, val_acc:0.987]
Epoch [82/120    avg_loss:0.019, val_acc:0.988]
Epoch [83/120    avg_loss:0.016, val_acc:0.988]
Epoch [84/120    avg_loss:0.017, val_acc:0.989]
Epoch [85/120    avg_loss:0.019, val_acc:0.988]
Epoch [86/120    avg_loss:0.021, val_acc:0.989]
Epoch [87/120    avg_loss:0.019, val_acc:0.989]
Epoch [88/120    avg_loss:0.021, val_acc:0.986]
Epoch [89/120    avg_loss:0.022, val_acc:0.988]
Epoch [90/120    avg_loss:0.017, val_acc:0.989]
Epoch [91/120    avg_loss:0.022, val_acc:0.987]
Epoch [92/120    avg_loss:0.017, val_acc:0.987]
Epoch [93/120    avg_loss:0.017, val_acc:0.989]
Epoch [94/120    avg_loss:0.018, val_acc:0.989]
Epoch [95/120    avg_loss:0.015, val_acc:0.989]
Epoch [96/120    avg_loss:0.017, val_acc:0.989]
Epoch [97/120    avg_loss:0.023, val_acc:0.986]
Epoch [98/120    avg_loss:0.018, val_acc:0.986]
Epoch [99/120    avg_loss:0.018, val_acc:0.989]
Epoch [100/120    avg_loss:0.017, val_acc:0.989]
Epoch [101/120    avg_loss:0.016, val_acc:0.990]
Epoch [102/120    avg_loss:0.016, val_acc:0.989]
Epoch [103/120    avg_loss:0.016, val_acc:0.989]
Epoch [104/120    avg_loss:0.014, val_acc:0.989]
Epoch [105/120    avg_loss:0.016, val_acc:0.989]
Epoch [106/120    avg_loss:0.017, val_acc:0.988]
Epoch [107/120    avg_loss:0.016, val_acc:0.989]
Epoch [108/120    avg_loss:0.019, val_acc:0.988]
Epoch [109/120    avg_loss:0.015, val_acc:0.989]
Epoch [110/120    avg_loss:0.017, val_acc:0.988]
Epoch [111/120    avg_loss:0.015, val_acc:0.988]
Epoch [112/120    avg_loss:0.017, val_acc:0.988]
Epoch [113/120    avg_loss:0.014, val_acc:0.989]
Epoch [114/120    avg_loss:0.014, val_acc:0.989]
Epoch [115/120    avg_loss:0.017, val_acc:0.989]
Epoch [116/120    avg_loss:0.017, val_acc:0.989]
Epoch [117/120    avg_loss:0.015, val_acc:0.989]
Epoch [118/120    avg_loss:0.016, val_acc:0.989]
Epoch [119/120    avg_loss:0.014, val_acc:0.989]
Epoch [120/120    avg_loss:0.016, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     1    10     0     0     3    28    16]
 [    0     0 18012     0    63     0    11     0     4     0]
 [    0     9     0  1945     0     0     0     0    72    10]
 [    0    31    20     0  2885     0     9     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     7     0     0  4849     0     0     0]
 [    0     0     0     0     0     0     2  1271     0    17]
 [    0    25     0     1    63     0     0     0  3481     1]
 [    0     0     0     0    15    43     0     1     0   860]]

Accuracy:
98.76846697033234

F1 scores:
[       nan 0.99044363 0.99667995 0.97493734 0.96038615 0.98379193
 0.99476869 0.99103314 0.96923291 0.94349973]

Kappa:
0.9836870645397227
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd555b3fc50>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.193, val_acc:0.448]
Epoch [2/120    avg_loss:1.907, val_acc:0.146]
Epoch [3/120    avg_loss:1.697, val_acc:0.143]
Epoch [4/120    avg_loss:1.552, val_acc:0.165]
Epoch [5/120    avg_loss:1.457, val_acc:0.279]
Epoch [6/120    avg_loss:1.317, val_acc:0.350]
Epoch [7/120    avg_loss:1.203, val_acc:0.381]
Epoch [8/120    avg_loss:1.103, val_acc:0.441]
Epoch [9/120    avg_loss:0.974, val_acc:0.518]
Epoch [10/120    avg_loss:0.836, val_acc:0.647]
Epoch [11/120    avg_loss:0.923, val_acc:0.655]
Epoch [12/120    avg_loss:0.634, val_acc:0.732]
Epoch [13/120    avg_loss:0.538, val_acc:0.770]
Epoch [14/120    avg_loss:0.462, val_acc:0.780]
Epoch [15/120    avg_loss:0.421, val_acc:0.783]
Epoch [16/120    avg_loss:0.355, val_acc:0.797]
Epoch [17/120    avg_loss:0.360, val_acc:0.828]
Epoch [18/120    avg_loss:0.283, val_acc:0.819]
Epoch [19/120    avg_loss:0.251, val_acc:0.845]
Epoch [20/120    avg_loss:0.242, val_acc:0.864]
Epoch [21/120    avg_loss:0.232, val_acc:0.923]
Epoch [22/120    avg_loss:1.669, val_acc:0.593]
Epoch [23/120    avg_loss:1.364, val_acc:0.634]
Epoch [24/120    avg_loss:1.218, val_acc:0.647]
Epoch [25/120    avg_loss:1.128, val_acc:0.670]
Epoch [26/120    avg_loss:1.027, val_acc:0.720]
Epoch [27/120    avg_loss:0.944, val_acc:0.750]
Epoch [28/120    avg_loss:0.872, val_acc:0.754]
Epoch [29/120    avg_loss:0.824, val_acc:0.706]
Epoch [30/120    avg_loss:0.767, val_acc:0.757]
Epoch [31/120    avg_loss:0.739, val_acc:0.752]
Epoch [32/120    avg_loss:0.694, val_acc:0.764]
Epoch [33/120    avg_loss:0.629, val_acc:0.799]
Epoch [34/120    avg_loss:0.582, val_acc:0.712]
Epoch [35/120    avg_loss:0.541, val_acc:0.749]
Epoch [36/120    avg_loss:0.543, val_acc:0.754]
Epoch [37/120    avg_loss:0.553, val_acc:0.750]
Epoch [38/120    avg_loss:0.520, val_acc:0.766]
Epoch [39/120    avg_loss:0.527, val_acc:0.765]
Epoch [40/120    avg_loss:0.533, val_acc:0.777]
Epoch [41/120    avg_loss:0.530, val_acc:0.773]
Epoch [42/120    avg_loss:0.521, val_acc:0.773]
Epoch [43/120    avg_loss:0.506, val_acc:0.777]
Epoch [44/120    avg_loss:0.519, val_acc:0.773]
Epoch [45/120    avg_loss:0.510, val_acc:0.766]
Epoch [46/120    avg_loss:0.516, val_acc:0.755]
Epoch [47/120    avg_loss:0.495, val_acc:0.786]
Epoch [48/120    avg_loss:0.504, val_acc:0.785]
Epoch [49/120    avg_loss:0.473, val_acc:0.779]
Epoch [50/120    avg_loss:0.491, val_acc:0.779]
Epoch [51/120    avg_loss:0.473, val_acc:0.778]
Epoch [52/120    avg_loss:0.489, val_acc:0.777]
Epoch [53/120    avg_loss:0.500, val_acc:0.777]
Epoch [54/120    avg_loss:0.519, val_acc:0.768]
Epoch [55/120    avg_loss:0.482, val_acc:0.766]
Epoch [56/120    avg_loss:0.494, val_acc:0.769]
Epoch [57/120    avg_loss:0.503, val_acc:0.770]
Epoch [58/120    avg_loss:0.501, val_acc:0.772]
Epoch [59/120    avg_loss:0.488, val_acc:0.770]
Epoch [60/120    avg_loss:0.512, val_acc:0.778]
Epoch [61/120    avg_loss:0.471, val_acc:0.778]
Epoch [62/120    avg_loss:0.499, val_acc:0.778]
Epoch [63/120    avg_loss:0.496, val_acc:0.776]
Epoch [64/120    avg_loss:0.481, val_acc:0.776]
Epoch [65/120    avg_loss:0.506, val_acc:0.776]
Epoch [66/120    avg_loss:0.495, val_acc:0.776]
Epoch [67/120    avg_loss:0.505, val_acc:0.776]
Epoch [68/120    avg_loss:0.476, val_acc:0.776]
Epoch [69/120    avg_loss:0.492, val_acc:0.776]
Epoch [70/120    avg_loss:0.520, val_acc:0.777]
Epoch [71/120    avg_loss:0.497, val_acc:0.777]
Epoch [72/120    avg_loss:0.490, val_acc:0.778]
Epoch [73/120    avg_loss:0.480, val_acc:0.777]
Epoch [74/120    avg_loss:0.484, val_acc:0.777]
Epoch [75/120    avg_loss:0.489, val_acc:0.777]
Epoch [76/120    avg_loss:0.492, val_acc:0.777]
Epoch [77/120    avg_loss:0.492, val_acc:0.777]
Epoch [78/120    avg_loss:0.487, val_acc:0.777]
Epoch [79/120    avg_loss:0.493, val_acc:0.777]
Epoch [80/120    avg_loss:0.504, val_acc:0.777]
Epoch [81/120    avg_loss:0.482, val_acc:0.777]
Epoch [82/120    avg_loss:0.478, val_acc:0.777]
Epoch [83/120    avg_loss:0.482, val_acc:0.777]
Epoch [84/120    avg_loss:0.489, val_acc:0.777]
Epoch [85/120    avg_loss:0.484, val_acc:0.777]
Epoch [86/120    avg_loss:0.485, val_acc:0.777]
Epoch [87/120    avg_loss:0.466, val_acc:0.777]
Epoch [88/120    avg_loss:0.485, val_acc:0.777]
Epoch [89/120    avg_loss:0.506, val_acc:0.777]
Epoch [90/120    avg_loss:0.511, val_acc:0.777]
Epoch [91/120    avg_loss:0.486, val_acc:0.777]
Epoch [92/120    avg_loss:0.522, val_acc:0.777]
Epoch [93/120    avg_loss:0.478, val_acc:0.777]
Epoch [94/120    avg_loss:0.507, val_acc:0.777]
Epoch [95/120    avg_loss:0.486, val_acc:0.777]
Epoch [96/120    avg_loss:0.497, val_acc:0.777]
Epoch [97/120    avg_loss:0.478, val_acc:0.777]
Epoch [98/120    avg_loss:0.488, val_acc:0.777]
Epoch [99/120    avg_loss:0.479, val_acc:0.777]
Epoch [100/120    avg_loss:0.492, val_acc:0.777]
Epoch [101/120    avg_loss:0.483, val_acc:0.777]
Epoch [102/120    avg_loss:0.482, val_acc:0.777]
Epoch [103/120    avg_loss:0.499, val_acc:0.777]
Epoch [104/120    avg_loss:0.476, val_acc:0.777]
Epoch [105/120    avg_loss:0.469, val_acc:0.777]
Epoch [106/120    avg_loss:0.489, val_acc:0.777]
Epoch [107/120    avg_loss:0.490, val_acc:0.777]
Epoch [108/120    avg_loss:0.479, val_acc:0.777]
Epoch [109/120    avg_loss:0.498, val_acc:0.777]
Epoch [110/120    avg_loss:0.503, val_acc:0.777]
Epoch [111/120    avg_loss:0.506, val_acc:0.777]
Epoch [112/120    avg_loss:0.487, val_acc:0.777]
Epoch [113/120    avg_loss:0.494, val_acc:0.777]
Epoch [114/120    avg_loss:0.478, val_acc:0.777]
Epoch [115/120    avg_loss:0.506, val_acc:0.777]
Epoch [116/120    avg_loss:0.494, val_acc:0.777]
Epoch [117/120    avg_loss:0.506, val_acc:0.777]
Epoch [118/120    avg_loss:0.495, val_acc:0.777]
Epoch [119/120    avg_loss:0.491, val_acc:0.777]
Epoch [120/120    avg_loss:0.485, val_acc:0.777]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5188     5   290   145     0   322    10   392    80]
 [    0     0 11283     0    34     0  6773     0     0     0]
 [    0    18     0  1519    18     0     0     0   451    30]
 [    0   104   110    14  2259     4   450     0    30     1]
 [    0     1     0     0     0  1304     0     0     0     0]
 [    0     0   543     0    18     0  4133     0   184     0]
 [    0    36     0     0    41     0     8  1194     8     3]
 [    0    90     0    81    12     0   121     0  3267     0]
 [    0    13     0     0    24   109     0     2    16   755]]

Accuracy:
74.47521268647724

F1 scores:
[       nan 0.87325366 0.75142353 0.77106599 0.81803368 0.95811903
 0.49541504 0.95673077 0.82510418 0.84451902]

Kappa:
0.6819754641203373
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d80ae1b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.205, val_acc:0.138]
Epoch [2/120    avg_loss:2.002, val_acc:0.220]
Epoch [3/120    avg_loss:1.809, val_acc:0.193]
Epoch [4/120    avg_loss:1.629, val_acc:0.265]
Epoch [5/120    avg_loss:1.412, val_acc:0.302]
Epoch [6/120    avg_loss:1.311, val_acc:0.365]
Epoch [7/120    avg_loss:1.172, val_acc:0.415]
Epoch [8/120    avg_loss:1.066, val_acc:0.458]
Epoch [9/120    avg_loss:0.900, val_acc:0.474]
Epoch [10/120    avg_loss:0.800, val_acc:0.497]
Epoch [11/120    avg_loss:0.709, val_acc:0.526]
Epoch [12/120    avg_loss:0.600, val_acc:0.581]
Epoch [13/120    avg_loss:0.544, val_acc:0.692]
Epoch [14/120    avg_loss:0.488, val_acc:0.792]
Epoch [15/120    avg_loss:0.399, val_acc:0.869]
Epoch [16/120    avg_loss:0.357, val_acc:0.842]
Epoch [17/120    avg_loss:0.354, val_acc:0.830]
Epoch [18/120    avg_loss:0.321, val_acc:0.909]
Epoch [19/120    avg_loss:0.256, val_acc:0.938]
Epoch [20/120    avg_loss:0.224, val_acc:0.917]
Epoch [21/120    avg_loss:0.217, val_acc:0.934]
Epoch [22/120    avg_loss:0.201, val_acc:0.887]
Epoch [23/120    avg_loss:0.161, val_acc:0.940]
Epoch [24/120    avg_loss:0.194, val_acc:0.940]
Epoch [25/120    avg_loss:0.158, val_acc:0.958]
Epoch [26/120    avg_loss:0.151, val_acc:0.957]
Epoch [27/120    avg_loss:0.154, val_acc:0.957]
Epoch [28/120    avg_loss:0.159, val_acc:0.952]
Epoch [29/120    avg_loss:0.157, val_acc:0.948]
Epoch [30/120    avg_loss:0.132, val_acc:0.964]
Epoch [31/120    avg_loss:0.107, val_acc:0.976]
Epoch [32/120    avg_loss:0.088, val_acc:0.966]
Epoch [33/120    avg_loss:0.339, val_acc:0.460]
Epoch [34/120    avg_loss:1.441, val_acc:0.434]
Epoch [35/120    avg_loss:1.274, val_acc:0.426]
Epoch [36/120    avg_loss:1.215, val_acc:0.541]
Epoch [37/120    avg_loss:1.172, val_acc:0.493]
Epoch [38/120    avg_loss:1.087, val_acc:0.477]
Epoch [39/120    avg_loss:1.049, val_acc:0.507]
Epoch [40/120    avg_loss:0.972, val_acc:0.558]
Epoch [41/120    avg_loss:0.979, val_acc:0.580]
Epoch [42/120    avg_loss:0.921, val_acc:0.579]
Epoch [43/120    avg_loss:0.909, val_acc:0.610]
Epoch [44/120    avg_loss:0.870, val_acc:0.632]
Epoch [45/120    avg_loss:0.853, val_acc:0.646]
Epoch [46/120    avg_loss:0.831, val_acc:0.654]
Epoch [47/120    avg_loss:0.841, val_acc:0.658]
Epoch [48/120    avg_loss:0.829, val_acc:0.665]
Epoch [49/120    avg_loss:0.817, val_acc:0.664]
Epoch [50/120    avg_loss:0.820, val_acc:0.667]
Epoch [51/120    avg_loss:0.803, val_acc:0.667]
Epoch [52/120    avg_loss:0.835, val_acc:0.667]
Epoch [53/120    avg_loss:0.805, val_acc:0.677]
Epoch [54/120    avg_loss:0.813, val_acc:0.676]
Epoch [55/120    avg_loss:0.834, val_acc:0.679]
Epoch [56/120    avg_loss:0.802, val_acc:0.667]
Epoch [57/120    avg_loss:0.798, val_acc:0.688]
Epoch [58/120    avg_loss:0.790, val_acc:0.688]
Epoch [59/120    avg_loss:0.799, val_acc:0.688]
Epoch [60/120    avg_loss:0.796, val_acc:0.685]
Epoch [61/120    avg_loss:0.818, val_acc:0.683]
Epoch [62/120    avg_loss:0.781, val_acc:0.687]
Epoch [63/120    avg_loss:0.806, val_acc:0.689]
Epoch [64/120    avg_loss:0.826, val_acc:0.688]
Epoch [65/120    avg_loss:0.795, val_acc:0.689]
Epoch [66/120    avg_loss:0.787, val_acc:0.688]
Epoch [67/120    avg_loss:0.788, val_acc:0.688]
Epoch [68/120    avg_loss:0.825, val_acc:0.693]
Epoch [69/120    avg_loss:0.815, val_acc:0.694]
Epoch [70/120    avg_loss:0.776, val_acc:0.688]
Epoch [71/120    avg_loss:0.789, val_acc:0.688]
Epoch [72/120    avg_loss:0.802, val_acc:0.690]
Epoch [73/120    avg_loss:0.803, val_acc:0.691]
Epoch [74/120    avg_loss:0.790, val_acc:0.690]
Epoch [75/120    avg_loss:0.814, val_acc:0.691]
Epoch [76/120    avg_loss:0.785, val_acc:0.691]
Epoch [77/120    avg_loss:0.789, val_acc:0.690]
Epoch [78/120    avg_loss:0.834, val_acc:0.689]
Epoch [79/120    avg_loss:0.791, val_acc:0.690]
Epoch [80/120    avg_loss:0.813, val_acc:0.690]
Epoch [81/120    avg_loss:0.812, val_acc:0.690]
Epoch [82/120    avg_loss:0.788, val_acc:0.690]
Epoch [83/120    avg_loss:0.796, val_acc:0.690]
Epoch [84/120    avg_loss:0.782, val_acc:0.691]
Epoch [85/120    avg_loss:0.789, val_acc:0.691]
Epoch [86/120    avg_loss:0.779, val_acc:0.691]
Epoch [87/120    avg_loss:0.800, val_acc:0.691]
Epoch [88/120    avg_loss:0.805, val_acc:0.690]
Epoch [89/120    avg_loss:0.776, val_acc:0.690]
Epoch [90/120    avg_loss:0.783, val_acc:0.690]
Epoch [91/120    avg_loss:0.784, val_acc:0.689]
Epoch [92/120    avg_loss:0.785, val_acc:0.691]
Epoch [93/120    avg_loss:0.801, val_acc:0.690]
Epoch [94/120    avg_loss:0.788, val_acc:0.691]
Epoch [95/120    avg_loss:0.800, val_acc:0.690]
Epoch [96/120    avg_loss:0.787, val_acc:0.690]
Epoch [97/120    avg_loss:0.791, val_acc:0.690]
Epoch [98/120    avg_loss:0.790, val_acc:0.690]
Epoch [99/120    avg_loss:0.791, val_acc:0.690]
Epoch [100/120    avg_loss:0.782, val_acc:0.690]
Epoch [101/120    avg_loss:0.780, val_acc:0.690]
Epoch [102/120    avg_loss:0.787, val_acc:0.690]
Epoch [103/120    avg_loss:0.803, val_acc:0.690]
Epoch [104/120    avg_loss:0.788, val_acc:0.690]
Epoch [105/120    avg_loss:0.793, val_acc:0.690]
Epoch [106/120    avg_loss:0.790, val_acc:0.690]
Epoch [107/120    avg_loss:0.813, val_acc:0.690]
Epoch [108/120    avg_loss:0.776, val_acc:0.690]
Epoch [109/120    avg_loss:0.800, val_acc:0.690]
Epoch [110/120    avg_loss:0.791, val_acc:0.690]
Epoch [111/120    avg_loss:0.792, val_acc:0.690]
Epoch [112/120    avg_loss:0.795, val_acc:0.690]
Epoch [113/120    avg_loss:0.776, val_acc:0.690]
Epoch [114/120    avg_loss:0.779, val_acc:0.690]
Epoch [115/120    avg_loss:0.789, val_acc:0.690]
Epoch [116/120    avg_loss:0.783, val_acc:0.690]
Epoch [117/120    avg_loss:0.799, val_acc:0.690]
Epoch [118/120    avg_loss:0.785, val_acc:0.690]
Epoch [119/120    avg_loss:0.803, val_acc:0.690]
Epoch [120/120    avg_loss:0.788, val_acc:0.690]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4156   107   122   104     0  1128    59   534   222]
 [    0   214 12023     0   274     0  5283     0   296     0]
 [    0    19     0  1596     0     0    24     0   321    76]
 [    0    20   354     0  1797     0   773     0    26     2]
 [    0     0     0     0     0  1304     0     1     0     0]
 [    0     0   146    99   450     0  4168     0    15     0]
 [    0    94     0     0    53     0     0  1121     5    17]
 [    0   318    95   189    43     0   367     0  2559     0]
 [    0    29     0     6    20   117    17     0     0   730]]

Accuracy:
70.98546742824091

F1 scores:
[       nan 0.7367488  0.78033425 0.78853755 0.62909155 0.95671313
 0.50102176 0.90732497 0.69851235 0.74262462]

Kappa:
0.6361514133134917
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f80d5f2bb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.221, val_acc:0.111]
Epoch [2/120    avg_loss:1.945, val_acc:0.111]
Epoch [3/120    avg_loss:1.761, val_acc:0.134]
Epoch [4/120    avg_loss:1.630, val_acc:0.169]
Epoch [5/120    avg_loss:1.485, val_acc:0.490]
Epoch [6/120    avg_loss:1.330, val_acc:0.486]
Epoch [7/120    avg_loss:1.224, val_acc:0.466]
Epoch [8/120    avg_loss:1.127, val_acc:0.463]
Epoch [9/120    avg_loss:0.967, val_acc:0.504]
Epoch [10/120    avg_loss:0.830, val_acc:0.597]
Epoch [11/120    avg_loss:0.750, val_acc:0.672]
Epoch [12/120    avg_loss:0.647, val_acc:0.728]
Epoch [13/120    avg_loss:0.596, val_acc:0.713]
Epoch [14/120    avg_loss:0.497, val_acc:0.871]
Epoch [15/120    avg_loss:0.428, val_acc:0.847]
Epoch [16/120    avg_loss:0.384, val_acc:0.843]
Epoch [17/120    avg_loss:0.707, val_acc:0.750]
Epoch [18/120    avg_loss:0.749, val_acc:0.747]
Epoch [19/120    avg_loss:0.423, val_acc:0.857]
Epoch [20/120    avg_loss:0.323, val_acc:0.896]
Epoch [21/120    avg_loss:0.265, val_acc:0.895]
Epoch [22/120    avg_loss:0.302, val_acc:0.871]
Epoch [23/120    avg_loss:0.222, val_acc:0.916]
Epoch [24/120    avg_loss:0.220, val_acc:0.899]
Epoch [25/120    avg_loss:0.196, val_acc:0.907]
Epoch [26/120    avg_loss:0.161, val_acc:0.924]
Epoch [27/120    avg_loss:0.153, val_acc:0.919]
Epoch [28/120    avg_loss:0.159, val_acc:0.936]
Epoch [29/120    avg_loss:0.146, val_acc:0.955]
Epoch [30/120    avg_loss:0.102, val_acc:0.957]
Epoch [31/120    avg_loss:0.107, val_acc:0.908]
Epoch [32/120    avg_loss:0.101, val_acc:0.935]
Epoch [33/120    avg_loss:0.111, val_acc:0.952]
Epoch [34/120    avg_loss:0.097, val_acc:0.952]
Epoch [35/120    avg_loss:0.077, val_acc:0.966]
Epoch [36/120    avg_loss:0.068, val_acc:0.963]
Epoch [37/120    avg_loss:0.075, val_acc:0.962]
Epoch [38/120    avg_loss:0.383, val_acc:0.885]
Epoch [39/120    avg_loss:0.228, val_acc:0.944]
Epoch [40/120    avg_loss:0.119, val_acc:0.917]
Epoch [41/120    avg_loss:0.358, val_acc:0.915]
Epoch [42/120    avg_loss:0.159, val_acc:0.935]
Epoch [43/120    avg_loss:0.105, val_acc:0.951]
Epoch [44/120    avg_loss:0.089, val_acc:0.963]
Epoch [45/120    avg_loss:0.084, val_acc:0.959]
Epoch [46/120    avg_loss:0.082, val_acc:0.955]
Epoch [47/120    avg_loss:0.061, val_acc:0.930]
Epoch [48/120    avg_loss:0.090, val_acc:0.957]
Epoch [49/120    avg_loss:0.060, val_acc:0.976]
Epoch [50/120    avg_loss:0.044, val_acc:0.979]
Epoch [51/120    avg_loss:0.039, val_acc:0.980]
Epoch [52/120    avg_loss:0.047, val_acc:0.978]
Epoch [53/120    avg_loss:0.045, val_acc:0.977]
Epoch [54/120    avg_loss:0.041, val_acc:0.977]
Epoch [55/120    avg_loss:0.039, val_acc:0.981]
Epoch [56/120    avg_loss:0.040, val_acc:0.977]
Epoch [57/120    avg_loss:0.043, val_acc:0.980]
Epoch [58/120    avg_loss:0.039, val_acc:0.978]
Epoch [59/120    avg_loss:0.036, val_acc:0.978]
Epoch [60/120    avg_loss:0.031, val_acc:0.983]
Epoch [61/120    avg_loss:0.033, val_acc:0.984]
Epoch [62/120    avg_loss:0.038, val_acc:0.984]
Epoch [63/120    avg_loss:0.034, val_acc:0.981]
Epoch [64/120    avg_loss:0.043, val_acc:0.980]
Epoch [65/120    avg_loss:0.034, val_acc:0.984]
Epoch [66/120    avg_loss:0.036, val_acc:0.985]
Epoch [67/120    avg_loss:0.029, val_acc:0.985]
Epoch [68/120    avg_loss:0.028, val_acc:0.986]
Epoch [69/120    avg_loss:0.035, val_acc:0.984]
Epoch [70/120    avg_loss:0.038, val_acc:0.986]
Epoch [71/120    avg_loss:0.033, val_acc:0.985]
Epoch [72/120    avg_loss:0.033, val_acc:0.984]
Epoch [73/120    avg_loss:0.024, val_acc:0.985]
Epoch [74/120    avg_loss:0.034, val_acc:0.985]
Epoch [75/120    avg_loss:0.026, val_acc:0.985]
Epoch [76/120    avg_loss:0.033, val_acc:0.983]
Epoch [77/120    avg_loss:0.037, val_acc:0.980]
Epoch [78/120    avg_loss:0.030, val_acc:0.984]
Epoch [79/120    avg_loss:0.027, val_acc:0.985]
Epoch [80/120    avg_loss:0.030, val_acc:0.986]
Epoch [81/120    avg_loss:0.029, val_acc:0.980]
Epoch [82/120    avg_loss:0.032, val_acc:0.984]
Epoch [83/120    avg_loss:0.024, val_acc:0.983]
Epoch [84/120    avg_loss:0.029, val_acc:0.985]
Epoch [85/120    avg_loss:0.029, val_acc:0.984]
Epoch [86/120    avg_loss:0.028, val_acc:0.983]
Epoch [87/120    avg_loss:0.025, val_acc:0.986]
Epoch [88/120    avg_loss:0.024, val_acc:0.986]
Epoch [89/120    avg_loss:0.029, val_acc:0.984]
Epoch [90/120    avg_loss:0.025, val_acc:0.982]
Epoch [91/120    avg_loss:0.029, val_acc:0.984]
Epoch [92/120    avg_loss:0.024, val_acc:0.987]
Epoch [93/120    avg_loss:0.025, val_acc:0.987]
Epoch [94/120    avg_loss:0.028, val_acc:0.987]
Epoch [95/120    avg_loss:0.030, val_acc:0.986]
Epoch [96/120    avg_loss:0.027, val_acc:0.986]
Epoch [97/120    avg_loss:0.030, val_acc:0.985]
Epoch [98/120    avg_loss:0.027, val_acc:0.984]
Epoch [99/120    avg_loss:0.026, val_acc:0.986]
Epoch [100/120    avg_loss:0.022, val_acc:0.987]
Epoch [101/120    avg_loss:0.026, val_acc:0.987]
Epoch [102/120    avg_loss:0.022, val_acc:0.987]
Epoch [103/120    avg_loss:0.022, val_acc:0.987]
Epoch [104/120    avg_loss:0.026, val_acc:0.984]
Epoch [105/120    avg_loss:0.023, val_acc:0.986]
Epoch [106/120    avg_loss:0.022, val_acc:0.986]
Epoch [107/120    avg_loss:0.021, val_acc:0.987]
Epoch [108/120    avg_loss:0.025, val_acc:0.987]
Epoch [109/120    avg_loss:0.023, val_acc:0.986]
Epoch [110/120    avg_loss:0.022, val_acc:0.986]
Epoch [111/120    avg_loss:0.023, val_acc:0.984]
Epoch [112/120    avg_loss:0.025, val_acc:0.985]
Epoch [113/120    avg_loss:0.024, val_acc:0.986]
Epoch [114/120    avg_loss:0.024, val_acc:0.986]
Epoch [115/120    avg_loss:0.020, val_acc:0.988]
Epoch [116/120    avg_loss:0.024, val_acc:0.987]
Epoch [117/120    avg_loss:0.024, val_acc:0.986]
Epoch [118/120    avg_loss:0.020, val_acc:0.986]
Epoch [119/120    avg_loss:0.024, val_acc:0.986]
Epoch [120/120    avg_loss:0.019, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     0     0     0     0     0    23    29]
 [    0     0 18032     0    21     0    35     0     2     0]
 [    0    10     0  1987     0     0     0     0    39     0]
 [    0    10     4     0  2948     0     0     0     9     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     7     0     0  4853     0    12     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    12     1    31    65     0     2     0  3460     0]
 [    0     0     0     0     3    23     0     0     0   893]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.99345998 0.99809039 0.97857671 0.98119487 0.99126472
 0.99365274 1.         0.97245644 0.96959826]

Kappa:
0.9889894323244914
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f4b25fb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.203, val_acc:0.284]
Epoch [2/120    avg_loss:1.911, val_acc:0.214]
Epoch [3/120    avg_loss:1.699, val_acc:0.187]
Epoch [4/120    avg_loss:1.542, val_acc:0.215]
Epoch [5/120    avg_loss:1.396, val_acc:0.332]
Epoch [6/120    avg_loss:1.327, val_acc:0.379]
Epoch [7/120    avg_loss:1.196, val_acc:0.444]
Epoch [8/120    avg_loss:1.116, val_acc:0.638]
Epoch [9/120    avg_loss:0.979, val_acc:0.570]
Epoch [10/120    avg_loss:0.845, val_acc:0.746]
Epoch [11/120    avg_loss:0.748, val_acc:0.684]
Epoch [12/120    avg_loss:0.638, val_acc:0.756]
Epoch [13/120    avg_loss:0.550, val_acc:0.779]
Epoch [14/120    avg_loss:0.471, val_acc:0.805]
Epoch [15/120    avg_loss:0.653, val_acc:0.745]
Epoch [16/120    avg_loss:0.501, val_acc:0.812]
Epoch [17/120    avg_loss:0.434, val_acc:0.826]
Epoch [18/120    avg_loss:0.378, val_acc:0.826]
Epoch [19/120    avg_loss:0.304, val_acc:0.856]
Epoch [20/120    avg_loss:0.297, val_acc:0.892]
Epoch [21/120    avg_loss:0.235, val_acc:0.893]
Epoch [22/120    avg_loss:0.236, val_acc:0.929]
Epoch [23/120    avg_loss:0.203, val_acc:0.903]
Epoch [24/120    avg_loss:0.193, val_acc:0.934]
Epoch [25/120    avg_loss:0.186, val_acc:0.949]
Epoch [26/120    avg_loss:0.152, val_acc:0.947]
Epoch [27/120    avg_loss:0.183, val_acc:0.936]
Epoch [28/120    avg_loss:0.125, val_acc:0.954]
Epoch [29/120    avg_loss:0.138, val_acc:0.957]
Epoch [30/120    avg_loss:0.126, val_acc:0.951]
Epoch [31/120    avg_loss:0.118, val_acc:0.953]
Epoch [32/120    avg_loss:0.125, val_acc:0.964]
Epoch [33/120    avg_loss:0.095, val_acc:0.952]
Epoch [34/120    avg_loss:0.099, val_acc:0.969]
Epoch [35/120    avg_loss:0.080, val_acc:0.959]
Epoch [36/120    avg_loss:0.067, val_acc:0.951]
Epoch [37/120    avg_loss:0.110, val_acc:0.971]
Epoch [38/120    avg_loss:0.078, val_acc:0.960]
Epoch [39/120    avg_loss:0.101, val_acc:0.943]
Epoch [40/120    avg_loss:0.104, val_acc:0.971]
Epoch [41/120    avg_loss:0.072, val_acc:0.977]
Epoch [42/120    avg_loss:0.079, val_acc:0.964]
Epoch [43/120    avg_loss:0.103, val_acc:0.959]
Epoch [44/120    avg_loss:0.070, val_acc:0.969]
Epoch [45/120    avg_loss:0.053, val_acc:0.964]
Epoch [46/120    avg_loss:0.061, val_acc:0.967]
Epoch [47/120    avg_loss:0.053, val_acc:0.965]
Epoch [48/120    avg_loss:0.055, val_acc:0.965]
Epoch [49/120    avg_loss:0.052, val_acc:0.973]
Epoch [50/120    avg_loss:0.037, val_acc:0.972]
Epoch [51/120    avg_loss:0.046, val_acc:0.977]
Epoch [52/120    avg_loss:0.028, val_acc:0.980]
Epoch [53/120    avg_loss:0.035, val_acc:0.983]
Epoch [54/120    avg_loss:0.028, val_acc:0.977]
Epoch [55/120    avg_loss:0.063, val_acc:0.969]
Epoch [56/120    avg_loss:0.042, val_acc:0.975]
Epoch [57/120    avg_loss:0.039, val_acc:0.977]
Epoch [58/120    avg_loss:0.032, val_acc:0.980]
Epoch [59/120    avg_loss:0.032, val_acc:0.983]
Epoch [60/120    avg_loss:0.022, val_acc:0.983]
Epoch [61/120    avg_loss:0.029, val_acc:0.983]
Epoch [62/120    avg_loss:0.030, val_acc:0.976]
Epoch [63/120    avg_loss:0.035, val_acc:0.980]
Epoch [64/120    avg_loss:0.034, val_acc:0.984]
Epoch [65/120    avg_loss:0.023, val_acc:0.983]
Epoch [66/120    avg_loss:0.021, val_acc:0.980]
Epoch [67/120    avg_loss:0.025, val_acc:0.983]
Epoch [68/120    avg_loss:0.029, val_acc:0.983]
Epoch [69/120    avg_loss:0.019, val_acc:0.981]
Epoch [70/120    avg_loss:0.022, val_acc:0.982]
Epoch [71/120    avg_loss:0.021, val_acc:0.985]
Epoch [72/120    avg_loss:0.018, val_acc:0.982]
Epoch [73/120    avg_loss:0.077, val_acc:0.968]
Epoch [74/120    avg_loss:0.035, val_acc:0.984]
Epoch [75/120    avg_loss:0.028, val_acc:0.981]
Epoch [76/120    avg_loss:0.027, val_acc:0.977]
Epoch [77/120    avg_loss:0.025, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.020, val_acc:0.987]
Epoch [80/120    avg_loss:0.026, val_acc:0.984]
Epoch [81/120    avg_loss:0.026, val_acc:0.984]
Epoch [82/120    avg_loss:0.026, val_acc:0.984]
Epoch [83/120    avg_loss:0.013, val_acc:0.984]
Epoch [84/120    avg_loss:0.023, val_acc:0.984]
Epoch [85/120    avg_loss:0.013, val_acc:0.987]
Epoch [86/120    avg_loss:0.046, val_acc:0.972]
Epoch [87/120    avg_loss:0.588, val_acc:0.894]
Epoch [88/120    avg_loss:0.138, val_acc:0.950]
Epoch [89/120    avg_loss:0.100, val_acc:0.957]
Epoch [90/120    avg_loss:0.071, val_acc:0.949]
Epoch [91/120    avg_loss:0.054, val_acc:0.970]
Epoch [92/120    avg_loss:0.037, val_acc:0.973]
Epoch [93/120    avg_loss:0.058, val_acc:0.973]
Epoch [94/120    avg_loss:0.034, val_acc:0.972]
Epoch [95/120    avg_loss:0.038, val_acc:0.968]
Epoch [96/120    avg_loss:0.025, val_acc:0.977]
Epoch [97/120    avg_loss:0.027, val_acc:0.978]
Epoch [98/120    avg_loss:0.026, val_acc:0.978]
Epoch [99/120    avg_loss:0.021, val_acc:0.983]
Epoch [100/120    avg_loss:0.023, val_acc:0.983]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.018, val_acc:0.984]
Epoch [103/120    avg_loss:0.019, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.017, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.984]
Epoch [108/120    avg_loss:0.015, val_acc:0.985]
Epoch [109/120    avg_loss:0.017, val_acc:0.984]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.018, val_acc:0.984]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.016, val_acc:0.984]
Epoch [114/120    avg_loss:0.016, val_acc:0.985]
Epoch [115/120    avg_loss:0.014, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.017, val_acc:0.984]
Epoch [118/120    avg_loss:0.015, val_acc:0.984]
Epoch [119/120    avg_loss:0.014, val_acc:0.984]
Epoch [120/120    avg_loss:0.014, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     2     0     0     1     4    14]
 [    0     0 18035     0    50     0     5     0     0     0]
 [    0     2     0  1991     0     0     0     0    41     2]
 [    0    12    17     0  2934     0     4     0     3     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    28    11     0     0  4832     0     7     0]
 [    0     0     0     0     0     0     3  1280     0     7]
 [    0    36     0    29    59     0     0     0  3447     0]
 [    0     0     0     0    17    78     0     0     0   824]]

Accuracy:
98.95404044055624

F1 scores:
[       nan 0.99449314 0.99723528 0.97910007 0.97248923 0.97098214
 0.99403415 0.99572151 0.97469249 0.9321267 ]

Kappa:
0.9861417209782667
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a626b8c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.207, val_acc:0.120]
Epoch [2/120    avg_loss:1.940, val_acc:0.148]
Epoch [3/120    avg_loss:1.778, val_acc:0.186]
Epoch [4/120    avg_loss:1.625, val_acc:0.252]
Epoch [5/120    avg_loss:1.434, val_acc:0.277]
Epoch [6/120    avg_loss:1.280, val_acc:0.337]
Epoch [7/120    avg_loss:1.141, val_acc:0.391]
Epoch [8/120    avg_loss:0.991, val_acc:0.562]
Epoch [9/120    avg_loss:0.855, val_acc:0.659]
Epoch [10/120    avg_loss:0.768, val_acc:0.714]
Epoch [11/120    avg_loss:0.618, val_acc:0.761]
Epoch [12/120    avg_loss:0.562, val_acc:0.747]
Epoch [13/120    avg_loss:0.480, val_acc:0.807]
Epoch [14/120    avg_loss:0.399, val_acc:0.803]
Epoch [15/120    avg_loss:0.349, val_acc:0.810]
Epoch [16/120    avg_loss:0.321, val_acc:0.819]
Epoch [17/120    avg_loss:0.288, val_acc:0.839]
Epoch [18/120    avg_loss:0.263, val_acc:0.902]
Epoch [19/120    avg_loss:0.259, val_acc:0.881]
Epoch [20/120    avg_loss:0.228, val_acc:0.938]
Epoch [21/120    avg_loss:0.213, val_acc:0.924]
Epoch [22/120    avg_loss:0.143, val_acc:0.944]
Epoch [23/120    avg_loss:0.147, val_acc:0.946]
Epoch [24/120    avg_loss:0.120, val_acc:0.963]
Epoch [25/120    avg_loss:0.108, val_acc:0.924]
Epoch [26/120    avg_loss:0.101, val_acc:0.957]
Epoch [27/120    avg_loss:0.106, val_acc:0.953]
Epoch [28/120    avg_loss:0.094, val_acc:0.964]
Epoch [29/120    avg_loss:0.099, val_acc:0.948]
Epoch [30/120    avg_loss:0.105, val_acc:0.958]
Epoch [31/120    avg_loss:0.060, val_acc:0.969]
Epoch [32/120    avg_loss:0.062, val_acc:0.952]
Epoch [33/120    avg_loss:0.101, val_acc:0.928]
Epoch [34/120    avg_loss:0.070, val_acc:0.970]
Epoch [35/120    avg_loss:0.046, val_acc:0.966]
Epoch [36/120    avg_loss:0.052, val_acc:0.964]
Epoch [37/120    avg_loss:0.048, val_acc:0.976]
Epoch [38/120    avg_loss:0.058, val_acc:0.977]
Epoch [39/120    avg_loss:0.053, val_acc:0.977]
Epoch [40/120    avg_loss:0.043, val_acc:0.975]
Epoch [41/120    avg_loss:0.031, val_acc:0.969]
Epoch [42/120    avg_loss:0.056, val_acc:0.958]
Epoch [43/120    avg_loss:0.055, val_acc:0.978]
Epoch [44/120    avg_loss:0.055, val_acc:0.976]
Epoch [45/120    avg_loss:0.041, val_acc:0.974]
Epoch [46/120    avg_loss:0.040, val_acc:0.988]
Epoch [47/120    avg_loss:0.032, val_acc:0.973]
Epoch [48/120    avg_loss:0.062, val_acc:0.962]
Epoch [49/120    avg_loss:0.087, val_acc:0.944]
Epoch [50/120    avg_loss:0.058, val_acc:0.983]
Epoch [51/120    avg_loss:0.038, val_acc:0.924]
Epoch [52/120    avg_loss:0.059, val_acc:0.979]
Epoch [53/120    avg_loss:0.022, val_acc:0.984]
Epoch [54/120    avg_loss:0.030, val_acc:0.981]
Epoch [55/120    avg_loss:0.023, val_acc:0.983]
Epoch [56/120    avg_loss:0.026, val_acc:0.978]
Epoch [57/120    avg_loss:0.034, val_acc:0.989]
Epoch [58/120    avg_loss:0.018, val_acc:0.973]
Epoch [59/120    avg_loss:0.021, val_acc:0.977]
Epoch [60/120    avg_loss:0.030, val_acc:0.985]
Epoch [61/120    avg_loss:0.021, val_acc:0.982]
Epoch [62/120    avg_loss:0.016, val_acc:0.988]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.010, val_acc:0.989]
Epoch [65/120    avg_loss:0.010, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.986]
Epoch [67/120    avg_loss:0.012, val_acc:0.986]
Epoch [68/120    avg_loss:0.019, val_acc:0.989]
Epoch [69/120    avg_loss:0.024, val_acc:0.986]
Epoch [70/120    avg_loss:0.026, val_acc:0.984]
Epoch [71/120    avg_loss:0.030, val_acc:0.987]
Epoch [72/120    avg_loss:0.024, val_acc:0.965]
Epoch [73/120    avg_loss:0.025, val_acc:0.990]
Epoch [74/120    avg_loss:0.013, val_acc:0.990]
Epoch [75/120    avg_loss:0.013, val_acc:0.992]
Epoch [76/120    avg_loss:0.013, val_acc:0.991]
Epoch [77/120    avg_loss:0.009, val_acc:0.992]
Epoch [78/120    avg_loss:0.013, val_acc:0.992]
Epoch [79/120    avg_loss:0.008, val_acc:0.993]
Epoch [80/120    avg_loss:0.010, val_acc:0.993]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.007, val_acc:0.992]
Epoch [84/120    avg_loss:0.005, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.991]
Epoch [87/120    avg_loss:0.014, val_acc:0.962]
Epoch [88/120    avg_loss:0.008, val_acc:0.989]
Epoch [89/120    avg_loss:0.008, val_acc:0.992]
Epoch [90/120    avg_loss:0.029, val_acc:0.973]
Epoch [91/120    avg_loss:0.023, val_acc:0.991]
Epoch [92/120    avg_loss:0.014, val_acc:0.991]
Epoch [93/120    avg_loss:0.010, val_acc:0.991]
Epoch [94/120    avg_loss:0.008, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.995]
Epoch [96/120    avg_loss:0.005, val_acc:0.995]
Epoch [97/120    avg_loss:0.005, val_acc:0.995]
Epoch [98/120    avg_loss:0.005, val_acc:0.995]
Epoch [99/120    avg_loss:0.005, val_acc:0.995]
Epoch [100/120    avg_loss:0.004, val_acc:0.995]
Epoch [101/120    avg_loss:0.004, val_acc:0.995]
Epoch [102/120    avg_loss:0.005, val_acc:0.995]
Epoch [103/120    avg_loss:0.005, val_acc:0.995]
Epoch [104/120    avg_loss:0.005, val_acc:0.995]
Epoch [105/120    avg_loss:0.006, val_acc:0.995]
Epoch [106/120    avg_loss:0.004, val_acc:0.995]
Epoch [107/120    avg_loss:0.006, val_acc:0.995]
Epoch [108/120    avg_loss:0.006, val_acc:0.995]
Epoch [109/120    avg_loss:0.005, val_acc:0.995]
Epoch [110/120    avg_loss:0.004, val_acc:0.995]
Epoch [111/120    avg_loss:0.005, val_acc:0.994]
Epoch [112/120    avg_loss:0.004, val_acc:0.994]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.004, val_acc:0.995]
Epoch [117/120    avg_loss:0.005, val_acc:0.995]
Epoch [118/120    avg_loss:0.004, val_acc:0.995]
Epoch [119/120    avg_loss:0.004, val_acc:0.995]
Epoch [120/120    avg_loss:0.005, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     0     0     0     4     9     0]
 [    0     0 18070     0    16     0     4     0     0     0]
 [    0    10     0  1988     0     0     0     0    37     1]
 [    0    36    13     2  2904     0     5     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     6     0     0  4860     0     0     0]
 [    0     3     0     0     0     0     2  1285     0     0]
 [    0    17     0    17    41     0     0     0  3496     0]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
99.37338828236088

F1 scores:
[       nan 0.99388403 0.99875639 0.98197086 0.9789314  0.99504384
 0.99702534 0.99651028 0.98133333 0.99233297]

Kappa:
0.9916953440788091
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f79f96be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.181, val_acc:0.185]
Epoch [2/120    avg_loss:1.941, val_acc:0.238]
Epoch [3/120    avg_loss:1.822, val_acc:0.288]
Epoch [4/120    avg_loss:1.651, val_acc:0.323]
Epoch [5/120    avg_loss:1.488, val_acc:0.358]
Epoch [6/120    avg_loss:1.373, val_acc:0.418]
Epoch [7/120    avg_loss:1.213, val_acc:0.483]
Epoch [8/120    avg_loss:1.096, val_acc:0.521]
Epoch [9/120    avg_loss:1.006, val_acc:0.642]
Epoch [10/120    avg_loss:0.846, val_acc:0.727]
Epoch [11/120    avg_loss:0.725, val_acc:0.739]
Epoch [12/120    avg_loss:0.638, val_acc:0.771]
Epoch [13/120    avg_loss:0.540, val_acc:0.764]
Epoch [14/120    avg_loss:0.559, val_acc:0.794]
Epoch [15/120    avg_loss:0.493, val_acc:0.780]
Epoch [16/120    avg_loss:0.411, val_acc:0.821]
Epoch [17/120    avg_loss:0.360, val_acc:0.837]
Epoch [18/120    avg_loss:0.350, val_acc:0.870]
Epoch [19/120    avg_loss:0.321, val_acc:0.872]
Epoch [20/120    avg_loss:0.286, val_acc:0.866]
Epoch [21/120    avg_loss:0.273, val_acc:0.906]
Epoch [22/120    avg_loss:0.268, val_acc:0.905]
Epoch [23/120    avg_loss:0.230, val_acc:0.888]
Epoch [24/120    avg_loss:0.233, val_acc:0.863]
Epoch [25/120    avg_loss:0.192, val_acc:0.901]
Epoch [26/120    avg_loss:0.201, val_acc:0.939]
Epoch [27/120    avg_loss:0.187, val_acc:0.935]
Epoch [28/120    avg_loss:0.232, val_acc:0.900]
Epoch [29/120    avg_loss:0.196, val_acc:0.883]
Epoch [30/120    avg_loss:0.198, val_acc:0.916]
Epoch [31/120    avg_loss:0.172, val_acc:0.935]
Epoch [32/120    avg_loss:0.194, val_acc:0.935]
Epoch [33/120    avg_loss:0.151, val_acc:0.942]
Epoch [34/120    avg_loss:0.111, val_acc:0.950]
Epoch [35/120    avg_loss:0.090, val_acc:0.956]
Epoch [36/120    avg_loss:0.094, val_acc:0.963]
Epoch [37/120    avg_loss:0.127, val_acc:0.967]
Epoch [38/120    avg_loss:0.080, val_acc:0.940]
Epoch [39/120    avg_loss:0.070, val_acc:0.960]
Epoch [40/120    avg_loss:0.092, val_acc:0.938]
Epoch [41/120    avg_loss:0.056, val_acc:0.960]
Epoch [42/120    avg_loss:0.056, val_acc:0.976]
Epoch [43/120    avg_loss:0.049, val_acc:0.968]
Epoch [44/120    avg_loss:0.054, val_acc:0.979]
Epoch [45/120    avg_loss:0.042, val_acc:0.974]
Epoch [46/120    avg_loss:0.055, val_acc:0.967]
Epoch [47/120    avg_loss:0.062, val_acc:0.978]
Epoch [48/120    avg_loss:0.048, val_acc:0.937]
Epoch [49/120    avg_loss:0.058, val_acc:0.961]
Epoch [50/120    avg_loss:0.039, val_acc:0.979]
Epoch [51/120    avg_loss:0.042, val_acc:0.982]
Epoch [52/120    avg_loss:0.036, val_acc:0.975]
Epoch [53/120    avg_loss:0.041, val_acc:0.974]
Epoch [54/120    avg_loss:0.044, val_acc:0.978]
Epoch [55/120    avg_loss:0.047, val_acc:0.979]
Epoch [56/120    avg_loss:0.028, val_acc:0.985]
Epoch [57/120    avg_loss:0.025, val_acc:0.979]
Epoch [58/120    avg_loss:0.044, val_acc:0.964]
Epoch [59/120    avg_loss:0.052, val_acc:0.978]
Epoch [60/120    avg_loss:0.051, val_acc:0.971]
Epoch [61/120    avg_loss:0.039, val_acc:0.935]
Epoch [62/120    avg_loss:0.025, val_acc:0.983]
Epoch [63/120    avg_loss:0.019, val_acc:0.984]
Epoch [64/120    avg_loss:0.017, val_acc:0.984]
Epoch [65/120    avg_loss:0.018, val_acc:0.980]
Epoch [66/120    avg_loss:0.019, val_acc:0.980]
Epoch [67/120    avg_loss:0.031, val_acc:0.976]
Epoch [68/120    avg_loss:0.028, val_acc:0.984]
Epoch [69/120    avg_loss:0.024, val_acc:0.987]
Epoch [70/120    avg_loss:0.014, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.984]
Epoch [73/120    avg_loss:0.021, val_acc:0.986]
Epoch [74/120    avg_loss:0.014, val_acc:0.983]
Epoch [75/120    avg_loss:0.013, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.017, val_acc:0.982]
Epoch [78/120    avg_loss:0.018, val_acc:0.982]
Epoch [79/120    avg_loss:0.009, val_acc:0.986]
Epoch [80/120    avg_loss:0.028, val_acc:0.974]
Epoch [81/120    avg_loss:0.025, val_acc:0.976]
Epoch [82/120    avg_loss:0.014, val_acc:0.987]
Epoch [83/120    avg_loss:0.024, val_acc:0.980]
Epoch [84/120    avg_loss:0.027, val_acc:0.973]
Epoch [85/120    avg_loss:0.026, val_acc:0.988]
Epoch [86/120    avg_loss:0.012, val_acc:0.988]
Epoch [87/120    avg_loss:0.012, val_acc:0.989]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.987]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6420     0     0     0     0     0     8     0     4]
 [    0     2 18030     0    43     0    15     0     0     0]
 [    0     9     0  2011     0     0     0     0    15     1]
 [    0    44    20     0  2873     0     6     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     4     0     0  4869     0     0     0]
 [    0     3     0     0     0     0     0  1279     0     8]
 [    0     5     0    19    57     0     0     0  3490     0]
 [    0     0     0     1    15    55     0     0     0   848]]

Accuracy:
99.11310341503386

F1 scores:
[       nan 0.9941928  0.99764836 0.98796365 0.96409396 0.9793621
 0.99692875 0.99262709 0.98240676 0.95280899]

Kappa:
0.9882509653746473
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa1cb344be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.217, val_acc:0.133]
Epoch [2/120    avg_loss:2.004, val_acc:0.203]
Epoch [3/120    avg_loss:1.859, val_acc:0.247]
Epoch [4/120    avg_loss:1.750, val_acc:0.275]
Epoch [5/120    avg_loss:1.597, val_acc:0.282]
Epoch [6/120    avg_loss:1.516, val_acc:0.344]
Epoch [7/120    avg_loss:1.332, val_acc:0.345]
Epoch [8/120    avg_loss:1.227, val_acc:0.404]
Epoch [9/120    avg_loss:1.126, val_acc:0.451]
Epoch [10/120    avg_loss:0.998, val_acc:0.478]
Epoch [11/120    avg_loss:0.939, val_acc:0.470]
Epoch [12/120    avg_loss:0.761, val_acc:0.542]
Epoch [13/120    avg_loss:0.633, val_acc:0.669]
Epoch [14/120    avg_loss:0.587, val_acc:0.730]
Epoch [15/120    avg_loss:0.564, val_acc:0.771]
Epoch [16/120    avg_loss:0.474, val_acc:0.790]
Epoch [17/120    avg_loss:0.438, val_acc:0.786]
Epoch [18/120    avg_loss:0.392, val_acc:0.852]
Epoch [19/120    avg_loss:0.340, val_acc:0.823]
Epoch [20/120    avg_loss:0.320, val_acc:0.868]
Epoch [21/120    avg_loss:0.549, val_acc:0.720]
Epoch [22/120    avg_loss:0.592, val_acc:0.858]
Epoch [23/120    avg_loss:0.394, val_acc:0.861]
Epoch [24/120    avg_loss:0.381, val_acc:0.878]
Epoch [25/120    avg_loss:0.329, val_acc:0.878]
Epoch [26/120    avg_loss:0.303, val_acc:0.878]
Epoch [27/120    avg_loss:0.253, val_acc:0.923]
Epoch [28/120    avg_loss:0.235, val_acc:0.929]
Epoch [29/120    avg_loss:0.171, val_acc:0.924]
Epoch [30/120    avg_loss:0.149, val_acc:0.933]
Epoch [31/120    avg_loss:0.186, val_acc:0.901]
Epoch [32/120    avg_loss:0.169, val_acc:0.923]
Epoch [33/120    avg_loss:0.190, val_acc:0.896]
Epoch [34/120    avg_loss:0.141, val_acc:0.945]
Epoch [35/120    avg_loss:0.138, val_acc:0.951]
Epoch [36/120    avg_loss:0.097, val_acc:0.968]
Epoch [37/120    avg_loss:0.099, val_acc:0.958]
Epoch [38/120    avg_loss:0.086, val_acc:0.962]
Epoch [39/120    avg_loss:0.098, val_acc:0.964]
Epoch [40/120    avg_loss:0.062, val_acc:0.970]
Epoch [41/120    avg_loss:0.060, val_acc:0.968]
Epoch [42/120    avg_loss:0.055, val_acc:0.964]
Epoch [43/120    avg_loss:0.063, val_acc:0.965]
Epoch [44/120    avg_loss:0.077, val_acc:0.972]
Epoch [45/120    avg_loss:0.057, val_acc:0.973]
Epoch [46/120    avg_loss:0.055, val_acc:0.976]
Epoch [47/120    avg_loss:0.050, val_acc:0.969]
Epoch [48/120    avg_loss:0.034, val_acc:0.973]
Epoch [49/120    avg_loss:0.036, val_acc:0.982]
Epoch [50/120    avg_loss:0.038, val_acc:0.970]
Epoch [51/120    avg_loss:0.031, val_acc:0.980]
Epoch [52/120    avg_loss:0.025, val_acc:0.977]
Epoch [53/120    avg_loss:0.028, val_acc:0.980]
Epoch [54/120    avg_loss:0.032, val_acc:0.977]
Epoch [55/120    avg_loss:0.038, val_acc:0.968]
Epoch [56/120    avg_loss:0.024, val_acc:0.948]
Epoch [57/120    avg_loss:0.025, val_acc:0.982]
Epoch [58/120    avg_loss:0.027, val_acc:0.976]
Epoch [59/120    avg_loss:0.027, val_acc:0.981]
Epoch [60/120    avg_loss:0.023, val_acc:0.969]
Epoch [61/120    avg_loss:0.067, val_acc:0.968]
Epoch [62/120    avg_loss:0.088, val_acc:0.965]
Epoch [63/120    avg_loss:0.034, val_acc:0.964]
Epoch [64/120    avg_loss:0.032, val_acc:0.975]
Epoch [65/120    avg_loss:0.034, val_acc:0.974]
Epoch [66/120    avg_loss:0.032, val_acc:0.984]
Epoch [67/120    avg_loss:0.017, val_acc:0.983]
Epoch [68/120    avg_loss:0.018, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.017, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.014, val_acc:0.979]
Epoch [73/120    avg_loss:0.011, val_acc:0.983]
Epoch [74/120    avg_loss:0.032, val_acc:0.978]
Epoch [75/120    avg_loss:0.025, val_acc:0.976]
Epoch [76/120    avg_loss:0.012, val_acc:0.981]
Epoch [77/120    avg_loss:0.018, val_acc:0.982]
Epoch [78/120    avg_loss:0.016, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.017, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.010, val_acc:0.983]
Epoch [90/120    avg_loss:0.014, val_acc:0.979]
Epoch [91/120    avg_loss:0.011, val_acc:0.980]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.017, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.983]
Epoch [105/120    avg_loss:0.010, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.981]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     0     0     0     1    24     0]
 [    0     1 18011     0    57     0    20     0     1     0]
 [    0     7     0  1976     0     0     0     0    52     1]
 [    0    22    16     0  2901     0     6     0    26     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     1     0     0  4875     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    31     0     5    44     0     0     0  3491     0]
 [    0     0     0     0    16    74     0     1     0   828]]

Accuracy:
99.01429156725231

F1 scores:
[       nan 0.99333333 0.99731443 0.98357392 0.96861436 0.97242921
 0.99703446 0.99922541 0.97445918 0.94682676]

Kappa:
0.9869451327942403
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a55cacba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.195, val_acc:0.204]
Epoch [2/120    avg_loss:1.965, val_acc:0.194]
Epoch [3/120    avg_loss:1.804, val_acc:0.368]
Epoch [4/120    avg_loss:1.636, val_acc:0.404]
Epoch [5/120    avg_loss:1.489, val_acc:0.381]
Epoch [6/120    avg_loss:1.384, val_acc:0.522]
Epoch [7/120    avg_loss:1.264, val_acc:0.639]
Epoch [8/120    avg_loss:1.101, val_acc:0.596]
Epoch [9/120    avg_loss:0.949, val_acc:0.557]
Epoch [10/120    avg_loss:0.811, val_acc:0.583]
Epoch [11/120    avg_loss:0.690, val_acc:0.651]
Epoch [12/120    avg_loss:0.641, val_acc:0.672]
Epoch [13/120    avg_loss:0.536, val_acc:0.731]
Epoch [14/120    avg_loss:0.468, val_acc:0.772]
Epoch [15/120    avg_loss:0.408, val_acc:0.852]
Epoch [16/120    avg_loss:0.351, val_acc:0.879]
Epoch [17/120    avg_loss:0.399, val_acc:0.854]
Epoch [18/120    avg_loss:0.327, val_acc:0.913]
Epoch [19/120    avg_loss:0.313, val_acc:0.936]
Epoch [20/120    avg_loss:0.359, val_acc:0.466]
Epoch [21/120    avg_loss:0.835, val_acc:0.695]
Epoch [22/120    avg_loss:0.506, val_acc:0.835]
Epoch [23/120    avg_loss:0.368, val_acc:0.757]
Epoch [24/120    avg_loss:0.359, val_acc:0.877]
Epoch [25/120    avg_loss:0.702, val_acc:0.559]
Epoch [26/120    avg_loss:1.536, val_acc:0.571]
Epoch [27/120    avg_loss:1.353, val_acc:0.619]
Epoch [28/120    avg_loss:1.274, val_acc:0.639]
Epoch [29/120    avg_loss:1.190, val_acc:0.663]
Epoch [30/120    avg_loss:1.130, val_acc:0.684]
Epoch [31/120    avg_loss:1.056, val_acc:0.709]
Epoch [32/120    avg_loss:1.004, val_acc:0.703]
Epoch [33/120    avg_loss:0.962, val_acc:0.710]
Epoch [34/120    avg_loss:0.939, val_acc:0.714]
Epoch [35/120    avg_loss:0.941, val_acc:0.719]
Epoch [36/120    avg_loss:0.948, val_acc:0.716]
Epoch [37/120    avg_loss:0.938, val_acc:0.715]
Epoch [38/120    avg_loss:0.937, val_acc:0.724]
Epoch [39/120    avg_loss:0.926, val_acc:0.715]
Epoch [40/120    avg_loss:0.943, val_acc:0.711]
Epoch [41/120    avg_loss:0.923, val_acc:0.719]
Epoch [42/120    avg_loss:0.927, val_acc:0.725]
Epoch [43/120    avg_loss:0.938, val_acc:0.728]
Epoch [44/120    avg_loss:0.919, val_acc:0.724]
Epoch [45/120    avg_loss:0.884, val_acc:0.721]
Epoch [46/120    avg_loss:0.897, val_acc:0.719]
Epoch [47/120    avg_loss:0.892, val_acc:0.716]
Epoch [48/120    avg_loss:0.909, val_acc:0.716]
Epoch [49/120    avg_loss:0.901, val_acc:0.715]
Epoch [50/120    avg_loss:0.890, val_acc:0.718]
Epoch [51/120    avg_loss:0.871, val_acc:0.722]
Epoch [52/120    avg_loss:0.891, val_acc:0.719]
Epoch [53/120    avg_loss:0.891, val_acc:0.719]
Epoch [54/120    avg_loss:0.880, val_acc:0.720]
Epoch [55/120    avg_loss:0.856, val_acc:0.722]
Epoch [56/120    avg_loss:0.898, val_acc:0.724]
Epoch [57/120    avg_loss:0.888, val_acc:0.720]
Epoch [58/120    avg_loss:0.909, val_acc:0.719]
Epoch [59/120    avg_loss:0.866, val_acc:0.719]
Epoch [60/120    avg_loss:0.894, val_acc:0.719]
Epoch [61/120    avg_loss:0.882, val_acc:0.719]
Epoch [62/120    avg_loss:0.875, val_acc:0.719]
Epoch [63/120    avg_loss:0.888, val_acc:0.719]
Epoch [64/120    avg_loss:0.880, val_acc:0.719]
Epoch [65/120    avg_loss:0.869, val_acc:0.719]
Epoch [66/120    avg_loss:0.915, val_acc:0.719]
Epoch [67/120    avg_loss:0.890, val_acc:0.719]
Epoch [68/120    avg_loss:0.887, val_acc:0.719]
Epoch [69/120    avg_loss:0.916, val_acc:0.719]
Epoch [70/120    avg_loss:0.901, val_acc:0.719]
Epoch [71/120    avg_loss:0.899, val_acc:0.719]
Epoch [72/120    avg_loss:0.894, val_acc:0.719]
Epoch [73/120    avg_loss:0.886, val_acc:0.719]
Epoch [74/120    avg_loss:0.878, val_acc:0.719]
Epoch [75/120    avg_loss:0.884, val_acc:0.719]
Epoch [76/120    avg_loss:0.879, val_acc:0.719]
Epoch [77/120    avg_loss:0.873, val_acc:0.719]
Epoch [78/120    avg_loss:0.876, val_acc:0.719]
Epoch [79/120    avg_loss:0.921, val_acc:0.719]
Epoch [80/120    avg_loss:0.886, val_acc:0.719]
Epoch [81/120    avg_loss:0.880, val_acc:0.719]
Epoch [82/120    avg_loss:0.889, val_acc:0.719]
Epoch [83/120    avg_loss:0.869, val_acc:0.719]
Epoch [84/120    avg_loss:0.887, val_acc:0.719]
Epoch [85/120    avg_loss:0.892, val_acc:0.719]
Epoch [86/120    avg_loss:0.921, val_acc:0.719]
Epoch [87/120    avg_loss:0.861, val_acc:0.719]
Epoch [88/120    avg_loss:0.900, val_acc:0.719]
Epoch [89/120    avg_loss:0.885, val_acc:0.719]
Epoch [90/120    avg_loss:0.862, val_acc:0.719]
Epoch [91/120    avg_loss:0.883, val_acc:0.719]
Epoch [92/120    avg_loss:0.879, val_acc:0.719]
Epoch [93/120    avg_loss:0.875, val_acc:0.719]
Epoch [94/120    avg_loss:0.870, val_acc:0.719]
Epoch [95/120    avg_loss:0.888, val_acc:0.719]
Epoch [96/120    avg_loss:0.898, val_acc:0.719]
Epoch [97/120    avg_loss:0.896, val_acc:0.719]
Epoch [98/120    avg_loss:0.907, val_acc:0.719]
Epoch [99/120    avg_loss:0.892, val_acc:0.719]
Epoch [100/120    avg_loss:0.867, val_acc:0.719]
Epoch [101/120    avg_loss:0.892, val_acc:0.719]
Epoch [102/120    avg_loss:0.898, val_acc:0.719]
Epoch [103/120    avg_loss:0.896, val_acc:0.719]
Epoch [104/120    avg_loss:0.871, val_acc:0.719]
Epoch [105/120    avg_loss:0.862, val_acc:0.719]
Epoch [106/120    avg_loss:0.903, val_acc:0.719]
Epoch [107/120    avg_loss:0.893, val_acc:0.719]
Epoch [108/120    avg_loss:0.877, val_acc:0.719]
Epoch [109/120    avg_loss:0.917, val_acc:0.719]
Epoch [110/120    avg_loss:0.877, val_acc:0.719]
Epoch [111/120    avg_loss:0.886, val_acc:0.719]
Epoch [112/120    avg_loss:0.876, val_acc:0.719]
Epoch [113/120    avg_loss:0.911, val_acc:0.719]
Epoch [114/120    avg_loss:0.882, val_acc:0.719]
Epoch [115/120    avg_loss:0.891, val_acc:0.719]
Epoch [116/120    avg_loss:0.872, val_acc:0.719]
Epoch [117/120    avg_loss:0.881, val_acc:0.719]
Epoch [118/120    avg_loss:0.874, val_acc:0.719]
Epoch [119/120    avg_loss:0.887, val_acc:0.719]
Epoch [120/120    avg_loss:0.894, val_acc:0.719]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  3967    38    20   602   102   124   709   562   308]
 [    0     0 16519     0    61     0  1510     0     0     0]
 [    0    18     0  1571   120     0     7     0   124   196]
 [    0   134   260    29  1432     0   866     3   230    18]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0  2282    55   416     0  1934     0   191     0]
 [    0   170     0     0     1     0     0  1105     0    14]
 [    0   237     7    94   168     0    36     0  3008    21]
 [    0    22     0    11    14   188     0     0     0   684]]

Accuracy:
75.97667076374329

F1 scores:
[       nan 0.72258652 0.88821379 0.82337526 0.4949879  0.9
 0.41346873 0.71129707 0.78272183 0.63333333]

Kappa:
0.6800483155388628
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7504ab8ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.170, val_acc:0.143]
Epoch [2/120    avg_loss:1.933, val_acc:0.181]
Epoch [3/120    avg_loss:1.749, val_acc:0.189]
Epoch [4/120    avg_loss:1.572, val_acc:0.232]
Epoch [5/120    avg_loss:1.449, val_acc:0.279]
Epoch [6/120    avg_loss:1.310, val_acc:0.371]
Epoch [7/120    avg_loss:1.185, val_acc:0.477]
Epoch [8/120    avg_loss:1.018, val_acc:0.510]
Epoch [9/120    avg_loss:0.888, val_acc:0.669]
Epoch [10/120    avg_loss:0.783, val_acc:0.698]
Epoch [11/120    avg_loss:0.675, val_acc:0.731]
Epoch [12/120    avg_loss:0.563, val_acc:0.773]
Epoch [13/120    avg_loss:0.496, val_acc:0.773]
Epoch [14/120    avg_loss:0.429, val_acc:0.813]
Epoch [15/120    avg_loss:0.411, val_acc:0.826]
Epoch [16/120    avg_loss:0.337, val_acc:0.894]
Epoch [17/120    avg_loss:0.287, val_acc:0.891]
Epoch [18/120    avg_loss:0.321, val_acc:0.399]
Epoch [19/120    avg_loss:0.492, val_acc:0.826]
Epoch [20/120    avg_loss:0.300, val_acc:0.887]
Epoch [21/120    avg_loss:0.280, val_acc:0.901]
Epoch [22/120    avg_loss:0.277, val_acc:0.891]
Epoch [23/120    avg_loss:0.220, val_acc:0.909]
Epoch [24/120    avg_loss:0.177, val_acc:0.946]
Epoch [25/120    avg_loss:0.139, val_acc:0.951]
Epoch [26/120    avg_loss:0.146, val_acc:0.954]
Epoch [27/120    avg_loss:0.124, val_acc:0.967]
Epoch [28/120    avg_loss:0.123, val_acc:0.932]
Epoch [29/120    avg_loss:0.125, val_acc:0.960]
Epoch [30/120    avg_loss:0.127, val_acc:0.935]
Epoch [31/120    avg_loss:0.108, val_acc:0.945]
Epoch [32/120    avg_loss:0.100, val_acc:0.973]
Epoch [33/120    avg_loss:0.100, val_acc:0.971]
Epoch [34/120    avg_loss:0.084, val_acc:0.974]
Epoch [35/120    avg_loss:0.069, val_acc:0.976]
Epoch [36/120    avg_loss:0.059, val_acc:0.975]
Epoch [37/120    avg_loss:0.049, val_acc:0.977]
Epoch [38/120    avg_loss:0.064, val_acc:0.933]
Epoch [39/120    avg_loss:0.058, val_acc:0.978]
Epoch [40/120    avg_loss:0.064, val_acc:0.983]
Epoch [41/120    avg_loss:0.042, val_acc:0.981]
Epoch [42/120    avg_loss:0.086, val_acc:0.979]
Epoch [43/120    avg_loss:0.057, val_acc:0.965]
Epoch [44/120    avg_loss:0.057, val_acc:0.969]
Epoch [45/120    avg_loss:0.066, val_acc:0.937]
Epoch [46/120    avg_loss:0.054, val_acc:0.978]
Epoch [47/120    avg_loss:0.041, val_acc:0.970]
Epoch [48/120    avg_loss:0.033, val_acc:0.983]
Epoch [49/120    avg_loss:0.051, val_acc:0.944]
Epoch [50/120    avg_loss:0.519, val_acc:0.843]
Epoch [51/120    avg_loss:0.163, val_acc:0.924]
Epoch [52/120    avg_loss:0.097, val_acc:0.956]
Epoch [53/120    avg_loss:0.077, val_acc:0.970]
Epoch [54/120    avg_loss:0.087, val_acc:0.970]
Epoch [55/120    avg_loss:0.054, val_acc:0.977]
Epoch [56/120    avg_loss:0.053, val_acc:0.973]
Epoch [57/120    avg_loss:0.047, val_acc:0.975]
Epoch [58/120    avg_loss:0.067, val_acc:0.953]
Epoch [59/120    avg_loss:0.047, val_acc:0.961]
Epoch [60/120    avg_loss:0.093, val_acc:0.962]
Epoch [61/120    avg_loss:0.077, val_acc:0.970]
Epoch [62/120    avg_loss:0.044, val_acc:0.980]
Epoch [63/120    avg_loss:0.041, val_acc:0.981]
Epoch [64/120    avg_loss:0.037, val_acc:0.980]
Epoch [65/120    avg_loss:0.029, val_acc:0.979]
Epoch [66/120    avg_loss:0.027, val_acc:0.983]
Epoch [67/120    avg_loss:0.030, val_acc:0.979]
Epoch [68/120    avg_loss:0.035, val_acc:0.980]
Epoch [69/120    avg_loss:0.028, val_acc:0.978]
Epoch [70/120    avg_loss:0.032, val_acc:0.984]
Epoch [71/120    avg_loss:0.032, val_acc:0.982]
Epoch [72/120    avg_loss:0.026, val_acc:0.983]
Epoch [73/120    avg_loss:0.028, val_acc:0.984]
Epoch [74/120    avg_loss:0.024, val_acc:0.984]
Epoch [75/120    avg_loss:0.024, val_acc:0.985]
Epoch [76/120    avg_loss:0.030, val_acc:0.983]
Epoch [77/120    avg_loss:0.025, val_acc:0.985]
Epoch [78/120    avg_loss:0.027, val_acc:0.983]
Epoch [79/120    avg_loss:0.025, val_acc:0.985]
Epoch [80/120    avg_loss:0.021, val_acc:0.985]
Epoch [81/120    avg_loss:0.026, val_acc:0.985]
Epoch [82/120    avg_loss:0.020, val_acc:0.984]
Epoch [83/120    avg_loss:0.023, val_acc:0.984]
Epoch [84/120    avg_loss:0.026, val_acc:0.984]
Epoch [85/120    avg_loss:0.018, val_acc:0.984]
Epoch [86/120    avg_loss:0.022, val_acc:0.984]
Epoch [87/120    avg_loss:0.022, val_acc:0.984]
Epoch [88/120    avg_loss:0.021, val_acc:0.983]
Epoch [89/120    avg_loss:0.020, val_acc:0.984]
Epoch [90/120    avg_loss:0.021, val_acc:0.984]
Epoch [91/120    avg_loss:0.017, val_acc:0.984]
Epoch [92/120    avg_loss:0.020, val_acc:0.985]
Epoch [93/120    avg_loss:0.017, val_acc:0.985]
Epoch [94/120    avg_loss:0.018, val_acc:0.984]
Epoch [95/120    avg_loss:0.021, val_acc:0.985]
Epoch [96/120    avg_loss:0.019, val_acc:0.984]
Epoch [97/120    avg_loss:0.022, val_acc:0.984]
Epoch [98/120    avg_loss:0.017, val_acc:0.984]
Epoch [99/120    avg_loss:0.020, val_acc:0.983]
Epoch [100/120    avg_loss:0.021, val_acc:0.985]
Epoch [101/120    avg_loss:0.019, val_acc:0.984]
Epoch [102/120    avg_loss:0.018, val_acc:0.984]
Epoch [103/120    avg_loss:0.019, val_acc:0.984]
Epoch [104/120    avg_loss:0.020, val_acc:0.983]
Epoch [105/120    avg_loss:0.018, val_acc:0.985]
Epoch [106/120    avg_loss:0.016, val_acc:0.985]
Epoch [107/120    avg_loss:0.016, val_acc:0.985]
Epoch [108/120    avg_loss:0.015, val_acc:0.986]
Epoch [109/120    avg_loss:0.017, val_acc:0.984]
Epoch [110/120    avg_loss:0.018, val_acc:0.985]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.016, val_acc:0.988]
Epoch [113/120    avg_loss:0.020, val_acc:0.984]
Epoch [114/120    avg_loss:0.014, val_acc:0.987]
Epoch [115/120    avg_loss:0.018, val_acc:0.987]
Epoch [116/120    avg_loss:0.015, val_acc:0.988]
Epoch [117/120    avg_loss:0.020, val_acc:0.987]
Epoch [118/120    avg_loss:0.014, val_acc:0.986]
Epoch [119/120    avg_loss:0.019, val_acc:0.987]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6360     0    12     0     0    10     0    26    24]
 [    0     0 17969     0   104     0    15     0     2     0]
 [    0     0     0  1970     5     0     0     0    52     9]
 [    0    48    18     0  2870     0     8     0    25     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6    28     0     0  4838     0     2     4]
 [    0     1     0     0     0     0     2  1283     0     4]
 [    0     0     0    44    63     0     0     0  3448    16]
 [    0     0     0     0    14    78     0     0     0   827]]

Accuracy:
98.49854192273396

F1 scores:
[       nan 0.99057706 0.99598149 0.96332518 0.95222296 0.97098214
 0.99230848 0.99727944 0.96772383 0.9158361 ]

Kappa:
0.9801297703958196
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f632e258c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.199, val_acc:0.099]
Epoch [2/120    avg_loss:1.929, val_acc:0.093]
Epoch [3/120    avg_loss:1.756, val_acc:0.303]
Epoch [4/120    avg_loss:1.646, val_acc:0.522]
Epoch [5/120    avg_loss:1.494, val_acc:0.499]
Epoch [6/120    avg_loss:1.340, val_acc:0.480]
Epoch [7/120    avg_loss:1.190, val_acc:0.517]
Epoch [8/120    avg_loss:1.055, val_acc:0.579]
Epoch [9/120    avg_loss:0.941, val_acc:0.615]
Epoch [10/120    avg_loss:0.827, val_acc:0.630]
Epoch [11/120    avg_loss:0.716, val_acc:0.679]
Epoch [12/120    avg_loss:0.652, val_acc:0.692]
Epoch [13/120    avg_loss:0.552, val_acc:0.768]
Epoch [14/120    avg_loss:0.512, val_acc:0.747]
Epoch [15/120    avg_loss:0.409, val_acc:0.841]
Epoch [16/120    avg_loss:0.392, val_acc:0.858]
Epoch [17/120    avg_loss:0.370, val_acc:0.875]
Epoch [18/120    avg_loss:0.364, val_acc:0.849]
Epoch [19/120    avg_loss:0.303, val_acc:0.873]
Epoch [20/120    avg_loss:0.255, val_acc:0.884]
Epoch [21/120    avg_loss:0.359, val_acc:0.897]
Epoch [22/120    avg_loss:0.276, val_acc:0.921]
Epoch [23/120    avg_loss:0.249, val_acc:0.899]
Epoch [24/120    avg_loss:0.195, val_acc:0.937]
Epoch [25/120    avg_loss:0.203, val_acc:0.955]
Epoch [26/120    avg_loss:0.157, val_acc:0.940]
Epoch [27/120    avg_loss:0.165, val_acc:0.955]
Epoch [28/120    avg_loss:0.125, val_acc:0.948]
Epoch [29/120    avg_loss:0.121, val_acc:0.932]
Epoch [30/120    avg_loss:0.157, val_acc:0.946]
Epoch [31/120    avg_loss:0.137, val_acc:0.910]
Epoch [32/120    avg_loss:0.130, val_acc:0.964]
Epoch [33/120    avg_loss:0.096, val_acc:0.963]
Epoch [34/120    avg_loss:0.084, val_acc:0.935]
Epoch [35/120    avg_loss:0.063, val_acc:0.973]
Epoch [36/120    avg_loss:0.069, val_acc:0.974]
Epoch [37/120    avg_loss:0.087, val_acc:0.979]
Epoch [38/120    avg_loss:0.090, val_acc:0.971]
Epoch [39/120    avg_loss:0.079, val_acc:0.969]
Epoch [40/120    avg_loss:0.060, val_acc:0.969]
Epoch [41/120    avg_loss:0.065, val_acc:0.975]
Epoch [42/120    avg_loss:0.074, val_acc:0.964]
Epoch [43/120    avg_loss:0.104, val_acc:0.967]
Epoch [44/120    avg_loss:0.082, val_acc:0.963]
Epoch [45/120    avg_loss:0.067, val_acc:0.979]
Epoch [46/120    avg_loss:0.067, val_acc:0.965]
Epoch [47/120    avg_loss:0.063, val_acc:0.970]
Epoch [48/120    avg_loss:0.066, val_acc:0.956]
Epoch [49/120    avg_loss:0.054, val_acc:0.964]
Epoch [50/120    avg_loss:0.069, val_acc:0.962]
Epoch [51/120    avg_loss:0.058, val_acc:0.977]
Epoch [52/120    avg_loss:0.057, val_acc:0.980]
Epoch [53/120    avg_loss:0.051, val_acc:0.977]
Epoch [54/120    avg_loss:0.031, val_acc:0.982]
Epoch [55/120    avg_loss:0.063, val_acc:0.974]
Epoch [56/120    avg_loss:0.037, val_acc:0.987]
Epoch [57/120    avg_loss:0.023, val_acc:0.986]
Epoch [58/120    avg_loss:0.026, val_acc:0.982]
Epoch [59/120    avg_loss:0.020, val_acc:0.985]
Epoch [60/120    avg_loss:0.039, val_acc:0.957]
Epoch [61/120    avg_loss:0.035, val_acc:0.977]
Epoch [62/120    avg_loss:0.036, val_acc:0.986]
Epoch [63/120    avg_loss:0.026, val_acc:0.980]
Epoch [64/120    avg_loss:0.028, val_acc:0.983]
Epoch [65/120    avg_loss:0.030, val_acc:0.986]
Epoch [66/120    avg_loss:0.017, val_acc:0.986]
Epoch [67/120    avg_loss:0.019, val_acc:0.976]
Epoch [68/120    avg_loss:0.015, val_acc:0.984]
Epoch [69/120    avg_loss:0.016, val_acc:0.985]
Epoch [70/120    avg_loss:0.019, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.985]
Epoch [72/120    avg_loss:0.018, val_acc:0.986]
Epoch [73/120    avg_loss:0.012, val_acc:0.986]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.008, val_acc:0.989]
Epoch [78/120    avg_loss:0.010, val_acc:0.987]
Epoch [79/120    avg_loss:0.009, val_acc:0.987]
Epoch [80/120    avg_loss:0.009, val_acc:0.989]
Epoch [81/120    avg_loss:0.012, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.010, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.011, val_acc:0.989]
Epoch [99/120    avg_loss:0.013, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.010, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.989]
Epoch [108/120    avg_loss:0.011, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.989]
Epoch [110/120    avg_loss:0.012, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.010, val_acc:0.989]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.011, val_acc:0.989]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     5     0     0     1     0    20     0]
 [    0     2 18068     0    17     0     3     0     0     0]
 [    0     9     0  1997     0     0     0     0    26     4]
 [    0    27    17     0  2894     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     2     0     0  4861     0     0    14]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0    13     0    28    51     0     0     0  3478     1]
 [    0     0     0     0    17    68     0     0     0   834]]

Accuracy:
99.12033355023739

F1 scores:
[       nan 0.99402591 0.99889429 0.98180924 0.97260965 0.97460792
 0.99712821 0.99805825 0.97669194 0.93866066]

Kappa:
0.9883436491668838
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24a1a0bc18>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.261, val_acc:0.205]
Epoch [2/120    avg_loss:2.014, val_acc:0.229]
Epoch [3/120    avg_loss:1.836, val_acc:0.283]
Epoch [4/120    avg_loss:1.690, val_acc:0.315]
Epoch [5/120    avg_loss:1.506, val_acc:0.339]
Epoch [6/120    avg_loss:1.337, val_acc:0.368]
Epoch [7/120    avg_loss:1.185, val_acc:0.397]
Epoch [8/120    avg_loss:1.037, val_acc:0.398]
Epoch [9/120    avg_loss:0.922, val_acc:0.451]
Epoch [10/120    avg_loss:0.791, val_acc:0.530]
Epoch [11/120    avg_loss:0.722, val_acc:0.598]
Epoch [12/120    avg_loss:0.625, val_acc:0.665]
Epoch [13/120    avg_loss:0.587, val_acc:0.734]
Epoch [14/120    avg_loss:0.483, val_acc:0.795]
Epoch [15/120    avg_loss:0.420, val_acc:0.857]
Epoch [16/120    avg_loss:0.397, val_acc:0.870]
Epoch [17/120    avg_loss:0.369, val_acc:0.796]
Epoch [18/120    avg_loss:0.353, val_acc:0.852]
Epoch [19/120    avg_loss:0.302, val_acc:0.913]
Epoch [20/120    avg_loss:0.244, val_acc:0.911]
Epoch [21/120    avg_loss:0.253, val_acc:0.918]
Epoch [22/120    avg_loss:0.207, val_acc:0.934]
Epoch [23/120    avg_loss:0.181, val_acc:0.938]
Epoch [24/120    avg_loss:0.152, val_acc:0.939]
Epoch [25/120    avg_loss:0.147, val_acc:0.930]
Epoch [26/120    avg_loss:0.147, val_acc:0.939]
Epoch [27/120    avg_loss:0.151, val_acc:0.957]
Epoch [28/120    avg_loss:0.102, val_acc:0.966]
Epoch [29/120    avg_loss:0.117, val_acc:0.938]
Epoch [30/120    avg_loss:0.109, val_acc:0.946]
Epoch [31/120    avg_loss:0.120, val_acc:0.958]
Epoch [32/120    avg_loss:0.087, val_acc:0.967]
Epoch [33/120    avg_loss:0.077, val_acc:0.960]
Epoch [34/120    avg_loss:0.064, val_acc:0.957]
Epoch [35/120    avg_loss:0.065, val_acc:0.977]
Epoch [36/120    avg_loss:0.060, val_acc:0.983]
Epoch [37/120    avg_loss:0.074, val_acc:0.960]
Epoch [38/120    avg_loss:0.069, val_acc:0.960]
Epoch [39/120    avg_loss:0.079, val_acc:0.978]
Epoch [40/120    avg_loss:0.081, val_acc:0.883]
Epoch [41/120    avg_loss:0.076, val_acc:0.964]
Epoch [42/120    avg_loss:0.055, val_acc:0.983]
Epoch [43/120    avg_loss:0.061, val_acc:0.976]
Epoch [44/120    avg_loss:0.079, val_acc:0.980]
Epoch [45/120    avg_loss:0.047, val_acc:0.975]
Epoch [46/120    avg_loss:0.037, val_acc:0.984]
Epoch [47/120    avg_loss:0.031, val_acc:0.971]
Epoch [48/120    avg_loss:0.034, val_acc:0.981]
Epoch [49/120    avg_loss:0.028, val_acc:0.985]
Epoch [50/120    avg_loss:0.029, val_acc:0.989]
Epoch [51/120    avg_loss:0.030, val_acc:0.979]
Epoch [52/120    avg_loss:0.033, val_acc:0.981]
Epoch [53/120    avg_loss:0.917, val_acc:0.541]
Epoch [54/120    avg_loss:1.404, val_acc:0.434]
Epoch [55/120    avg_loss:1.297, val_acc:0.471]
Epoch [56/120    avg_loss:1.255, val_acc:0.431]
Epoch [57/120    avg_loss:1.186, val_acc:0.507]
Epoch [58/120    avg_loss:1.171, val_acc:0.490]
Epoch [59/120    avg_loss:1.125, val_acc:0.470]
Epoch [60/120    avg_loss:1.129, val_acc:0.468]
Epoch [61/120    avg_loss:1.068, val_acc:0.509]
Epoch [62/120    avg_loss:1.049, val_acc:0.523]
Epoch [63/120    avg_loss:1.072, val_acc:0.502]
Epoch [64/120    avg_loss:1.034, val_acc:0.517]
Epoch [65/120    avg_loss:1.006, val_acc:0.511]
Epoch [66/120    avg_loss:0.998, val_acc:0.511]
Epoch [67/120    avg_loss:0.986, val_acc:0.519]
Epoch [68/120    avg_loss:0.973, val_acc:0.531]
Epoch [69/120    avg_loss:0.979, val_acc:0.541]
Epoch [70/120    avg_loss:0.946, val_acc:0.536]
Epoch [71/120    avg_loss:0.963, val_acc:0.555]
Epoch [72/120    avg_loss:0.965, val_acc:0.554]
Epoch [73/120    avg_loss:0.972, val_acc:0.561]
Epoch [74/120    avg_loss:0.968, val_acc:0.541]
Epoch [75/120    avg_loss:0.965, val_acc:0.552]
Epoch [76/120    avg_loss:0.968, val_acc:0.559]
Epoch [77/120    avg_loss:0.931, val_acc:0.559]
Epoch [78/120    avg_loss:0.931, val_acc:0.560]
Epoch [79/120    avg_loss:0.960, val_acc:0.560]
Epoch [80/120    avg_loss:0.935, val_acc:0.559]
Epoch [81/120    avg_loss:0.930, val_acc:0.553]
Epoch [82/120    avg_loss:0.971, val_acc:0.559]
Epoch [83/120    avg_loss:0.962, val_acc:0.560]
Epoch [84/120    avg_loss:0.938, val_acc:0.560]
Epoch [85/120    avg_loss:0.939, val_acc:0.561]
Epoch [86/120    avg_loss:0.926, val_acc:0.556]
Epoch [87/120    avg_loss:0.954, val_acc:0.560]
Epoch [88/120    avg_loss:0.940, val_acc:0.559]
Epoch [89/120    avg_loss:0.950, val_acc:0.558]
Epoch [90/120    avg_loss:0.931, val_acc:0.558]
Epoch [91/120    avg_loss:1.004, val_acc:0.558]
Epoch [92/120    avg_loss:0.919, val_acc:0.558]
Epoch [93/120    avg_loss:0.933, val_acc:0.558]
Epoch [94/120    avg_loss:0.928, val_acc:0.559]
Epoch [95/120    avg_loss:0.938, val_acc:0.558]
Epoch [96/120    avg_loss:0.948, val_acc:0.559]
Epoch [97/120    avg_loss:0.948, val_acc:0.558]
Epoch [98/120    avg_loss:0.948, val_acc:0.558]
Epoch [99/120    avg_loss:0.935, val_acc:0.559]
Epoch [100/120    avg_loss:0.926, val_acc:0.559]
Epoch [101/120    avg_loss:0.955, val_acc:0.558]
Epoch [102/120    avg_loss:0.941, val_acc:0.558]
Epoch [103/120    avg_loss:0.950, val_acc:0.558]
Epoch [104/120    avg_loss:0.940, val_acc:0.558]
Epoch [105/120    avg_loss:0.936, val_acc:0.558]
Epoch [106/120    avg_loss:0.925, val_acc:0.558]
Epoch [107/120    avg_loss:0.955, val_acc:0.559]
Epoch [108/120    avg_loss:0.945, val_acc:0.558]
Epoch [109/120    avg_loss:0.918, val_acc:0.558]
Epoch [110/120    avg_loss:0.956, val_acc:0.558]
Epoch [111/120    avg_loss:0.959, val_acc:0.558]
Epoch [112/120    avg_loss:0.964, val_acc:0.558]
Epoch [113/120    avg_loss:0.935, val_acc:0.558]
Epoch [114/120    avg_loss:0.940, val_acc:0.558]
Epoch [115/120    avg_loss:0.939, val_acc:0.558]
Epoch [116/120    avg_loss:0.933, val_acc:0.558]
Epoch [117/120    avg_loss:0.940, val_acc:0.558]
Epoch [118/120    avg_loss:0.933, val_acc:0.558]
Epoch [119/120    avg_loss:0.950, val_acc:0.558]
Epoch [120/120    avg_loss:0.946, val_acc:0.558]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  2286   503   150   130     0  2120   484   558   201]
 [    0     0 11427    31   314     0  6318     0     0     0]
 [    0    18     0  1776     6     0    51     0   156    29]
 [    0    17   343     0  1555     0   999     0    35    23]
 [    0     0     0     0     0  1301     0     4     0     0]
 [    0     0   103   105   475     0  4131     0    64     0]
 [    0    96     3     1    57     0     0  1108     5    20]
 [    0   323   595   280    49     0  1082     0  1242     0]
 [    0    36     1    13    24   127    39     0     0   679]]

Accuracy:
61.46819945532982

F1 scores:
[       nan 0.49652476 0.73568324 0.80874317 0.55714798 0.95206733
 0.42114385 0.76784477 0.44112946 0.72581507]

Kappa:
0.5166511275647467
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbd7cb7fbe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.213, val_acc:0.206]
Epoch [2/120    avg_loss:1.961, val_acc:0.222]
Epoch [3/120    avg_loss:1.792, val_acc:0.255]
Epoch [4/120    avg_loss:1.658, val_acc:0.283]
Epoch [5/120    avg_loss:1.477, val_acc:0.294]
Epoch [6/120    avg_loss:1.357, val_acc:0.370]
Epoch [7/120    avg_loss:1.336, val_acc:0.385]
Epoch [8/120    avg_loss:1.159, val_acc:0.457]
Epoch [9/120    avg_loss:1.058, val_acc:0.524]
Epoch [10/120    avg_loss:0.973, val_acc:0.553]
Epoch [11/120    avg_loss:0.836, val_acc:0.602]
Epoch [12/120    avg_loss:0.729, val_acc:0.635]
Epoch [13/120    avg_loss:0.617, val_acc:0.694]
Epoch [14/120    avg_loss:0.532, val_acc:0.714]
Epoch [15/120    avg_loss:0.483, val_acc:0.806]
Epoch [16/120    avg_loss:0.485, val_acc:0.779]
Epoch [17/120    avg_loss:0.423, val_acc:0.773]
Epoch [18/120    avg_loss:0.392, val_acc:0.856]
Epoch [19/120    avg_loss:0.354, val_acc:0.859]
Epoch [20/120    avg_loss:0.340, val_acc:0.803]
Epoch [21/120    avg_loss:0.295, val_acc:0.882]
Epoch [22/120    avg_loss:0.268, val_acc:0.854]
Epoch [23/120    avg_loss:0.238, val_acc:0.889]
Epoch [24/120    avg_loss:0.221, val_acc:0.943]
Epoch [25/120    avg_loss:0.196, val_acc:0.934]
Epoch [26/120    avg_loss:0.203, val_acc:0.958]
Epoch [27/120    avg_loss:0.161, val_acc:0.944]
Epoch [28/120    avg_loss:0.161, val_acc:0.924]
Epoch [29/120    avg_loss:0.153, val_acc:0.955]
Epoch [30/120    avg_loss:0.127, val_acc:0.845]
Epoch [31/120    avg_loss:0.155, val_acc:0.954]
Epoch [32/120    avg_loss:0.104, val_acc:0.964]
Epoch [33/120    avg_loss:0.082, val_acc:0.971]
Epoch [34/120    avg_loss:0.069, val_acc:0.972]
Epoch [35/120    avg_loss:0.123, val_acc:0.963]
Epoch [36/120    avg_loss:0.111, val_acc:0.959]
Epoch [37/120    avg_loss:0.114, val_acc:0.970]
Epoch [38/120    avg_loss:0.095, val_acc:0.939]
Epoch [39/120    avg_loss:0.096, val_acc:0.955]
Epoch [40/120    avg_loss:0.072, val_acc:0.948]
Epoch [41/120    avg_loss:0.096, val_acc:0.917]
Epoch [42/120    avg_loss:0.064, val_acc:0.938]
Epoch [43/120    avg_loss:0.084, val_acc:0.971]
Epoch [44/120    avg_loss:0.065, val_acc:0.964]
Epoch [45/120    avg_loss:0.067, val_acc:0.949]
Epoch [46/120    avg_loss:0.059, val_acc:0.976]
Epoch [47/120    avg_loss:0.040, val_acc:0.985]
Epoch [48/120    avg_loss:0.040, val_acc:0.975]
Epoch [49/120    avg_loss:0.056, val_acc:0.934]
Epoch [50/120    avg_loss:0.041, val_acc:0.988]
Epoch [51/120    avg_loss:0.034, val_acc:0.984]
Epoch [52/120    avg_loss:0.029, val_acc:0.977]
Epoch [53/120    avg_loss:0.029, val_acc:0.983]
Epoch [54/120    avg_loss:0.050, val_acc:0.882]
Epoch [55/120    avg_loss:0.045, val_acc:0.982]
Epoch [56/120    avg_loss:0.023, val_acc:0.984]
Epoch [57/120    avg_loss:0.034, val_acc:0.975]
Epoch [58/120    avg_loss:0.049, val_acc:0.980]
Epoch [59/120    avg_loss:0.031, val_acc:0.984]
Epoch [60/120    avg_loss:0.030, val_acc:0.985]
Epoch [61/120    avg_loss:0.022, val_acc:0.991]
Epoch [62/120    avg_loss:0.017, val_acc:0.989]
Epoch [63/120    avg_loss:0.053, val_acc:0.971]
Epoch [64/120    avg_loss:0.038, val_acc:0.973]
Epoch [65/120    avg_loss:0.036, val_acc:0.972]
Epoch [66/120    avg_loss:0.025, val_acc:0.988]
Epoch [67/120    avg_loss:0.031, val_acc:0.974]
Epoch [68/120    avg_loss:0.131, val_acc:0.964]
Epoch [69/120    avg_loss:0.056, val_acc:0.980]
Epoch [70/120    avg_loss:0.040, val_acc:0.984]
Epoch [71/120    avg_loss:0.037, val_acc:0.982]
Epoch [72/120    avg_loss:0.027, val_acc:0.982]
Epoch [73/120    avg_loss:0.028, val_acc:0.984]
Epoch [74/120    avg_loss:0.025, val_acc:0.990]
Epoch [75/120    avg_loss:0.015, val_acc:0.990]
Epoch [76/120    avg_loss:0.014, val_acc:0.990]
Epoch [77/120    avg_loss:0.014, val_acc:0.989]
Epoch [78/120    avg_loss:0.016, val_acc:0.990]
Epoch [79/120    avg_loss:0.019, val_acc:0.990]
Epoch [80/120    avg_loss:0.013, val_acc:0.990]
Epoch [81/120    avg_loss:0.012, val_acc:0.990]
Epoch [82/120    avg_loss:0.013, val_acc:0.990]
Epoch [83/120    avg_loss:0.011, val_acc:0.989]
Epoch [84/120    avg_loss:0.016, val_acc:0.990]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.990]
Epoch [88/120    avg_loss:0.014, val_acc:0.990]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.011, val_acc:0.990]
Epoch [91/120    avg_loss:0.010, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.990]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.990]
Epoch [96/120    avg_loss:0.014, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.011, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.990]
Epoch [100/120    avg_loss:0.016, val_acc:0.990]
Epoch [101/120    avg_loss:0.012, val_acc:0.990]
Epoch [102/120    avg_loss:0.011, val_acc:0.990]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.015, val_acc:0.990]
Epoch [107/120    avg_loss:0.013, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.012, val_acc:0.990]
Epoch [113/120    avg_loss:0.014, val_acc:0.990]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.015, val_acc:0.990]
Epoch [116/120    avg_loss:0.016, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.015, val_acc:0.990]
Epoch [119/120    avg_loss:0.013, val_acc:0.990]
Epoch [120/120    avg_loss:0.013, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     0     1     0     0    51     7     6]
 [    0     2 18029     0    48     0    11     0     0     0]
 [    0     2     0  1993     0     0     0     0    37     4]
 [    0    27    24     2  2909     0     0     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     4     0     0  4861     0     0     0]
 [    0     0     0     0     0     0     3  1286     0     1]
 [    0    18     0    24    54     0     0     0  3475     0]
 [    0     0     0     0    14    85     0     0     0   820]]

Accuracy:
98.92029980960645

F1 scores:
[       nan 0.99112702 0.99728952 0.98201527 0.96999    0.96846011
 0.99682149 0.97906357 0.97887324 0.93714286]

Kappa:
0.9856985704249032
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f69052d8ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.151, val_acc:0.466]
Epoch [2/120    avg_loss:1.950, val_acc:0.507]
Epoch [3/120    avg_loss:1.773, val_acc:0.523]
Epoch [4/120    avg_loss:1.635, val_acc:0.516]
Epoch [5/120    avg_loss:1.481, val_acc:0.628]
Epoch [6/120    avg_loss:1.322, val_acc:0.667]
Epoch [7/120    avg_loss:1.208, val_acc:0.575]
Epoch [8/120    avg_loss:1.064, val_acc:0.609]
Epoch [9/120    avg_loss:0.902, val_acc:0.626]
Epoch [10/120    avg_loss:0.784, val_acc:0.661]
Epoch [11/120    avg_loss:0.691, val_acc:0.720]
Epoch [12/120    avg_loss:0.979, val_acc:0.466]
Epoch [13/120    avg_loss:1.156, val_acc:0.621]
Epoch [14/120    avg_loss:0.931, val_acc:0.717]
Epoch [15/120    avg_loss:0.846, val_acc:0.684]
Epoch [16/120    avg_loss:0.769, val_acc:0.740]
Epoch [17/120    avg_loss:0.667, val_acc:0.714]
Epoch [18/120    avg_loss:0.633, val_acc:0.733]
Epoch [19/120    avg_loss:0.557, val_acc:0.740]
Epoch [20/120    avg_loss:0.493, val_acc:0.738]
Epoch [21/120    avg_loss:0.476, val_acc:0.741]
Epoch [22/120    avg_loss:0.494, val_acc:0.795]
Epoch [23/120    avg_loss:0.464, val_acc:0.779]
Epoch [24/120    avg_loss:0.465, val_acc:0.784]
Epoch [25/120    avg_loss:0.397, val_acc:0.820]
Epoch [26/120    avg_loss:0.360, val_acc:0.799]
Epoch [27/120    avg_loss:0.327, val_acc:0.843]
Epoch [28/120    avg_loss:0.309, val_acc:0.852]
Epoch [29/120    avg_loss:0.311, val_acc:0.837]
Epoch [30/120    avg_loss:0.285, val_acc:0.881]
Epoch [31/120    avg_loss:0.263, val_acc:0.878]
Epoch [32/120    avg_loss:0.259, val_acc:0.883]
Epoch [33/120    avg_loss:0.269, val_acc:0.874]
Epoch [34/120    avg_loss:0.258, val_acc:0.884]
Epoch [35/120    avg_loss:0.229, val_acc:0.907]
Epoch [36/120    avg_loss:0.220, val_acc:0.912]
Epoch [37/120    avg_loss:0.200, val_acc:0.919]
Epoch [38/120    avg_loss:0.172, val_acc:0.937]
Epoch [39/120    avg_loss:0.180, val_acc:0.898]
Epoch [40/120    avg_loss:0.182, val_acc:0.936]
Epoch [41/120    avg_loss:0.178, val_acc:0.948]
Epoch [42/120    avg_loss:0.145, val_acc:0.946]
Epoch [43/120    avg_loss:0.163, val_acc:0.948]
Epoch [44/120    avg_loss:0.108, val_acc:0.954]
Epoch [45/120    avg_loss:0.115, val_acc:0.955]
Epoch [46/120    avg_loss:0.114, val_acc:0.957]
Epoch [47/120    avg_loss:0.125, val_acc:0.951]
Epoch [48/120    avg_loss:0.112, val_acc:0.944]
Epoch [49/120    avg_loss:0.094, val_acc:0.958]
Epoch [50/120    avg_loss:0.091, val_acc:0.964]
Epoch [51/120    avg_loss:0.091, val_acc:0.937]
Epoch [52/120    avg_loss:0.099, val_acc:0.939]
Epoch [53/120    avg_loss:0.093, val_acc:0.954]
Epoch [54/120    avg_loss:0.076, val_acc:0.956]
Epoch [55/120    avg_loss:0.067, val_acc:0.948]
Epoch [56/120    avg_loss:0.100, val_acc:0.967]
Epoch [57/120    avg_loss:0.073, val_acc:0.968]
Epoch [58/120    avg_loss:0.064, val_acc:0.956]
Epoch [59/120    avg_loss:0.098, val_acc:0.966]
Epoch [60/120    avg_loss:0.088, val_acc:0.976]
Epoch [61/120    avg_loss:0.060, val_acc:0.977]
Epoch [62/120    avg_loss:0.061, val_acc:0.972]
Epoch [63/120    avg_loss:0.071, val_acc:0.978]
Epoch [64/120    avg_loss:0.048, val_acc:0.980]
Epoch [65/120    avg_loss:0.042, val_acc:0.957]
Epoch [66/120    avg_loss:0.070, val_acc:0.958]
Epoch [67/120    avg_loss:0.051, val_acc:0.979]
Epoch [68/120    avg_loss:0.041, val_acc:0.978]
Epoch [69/120    avg_loss:0.042, val_acc:0.966]
Epoch [70/120    avg_loss:0.054, val_acc:0.974]
Epoch [71/120    avg_loss:0.035, val_acc:0.978]
Epoch [72/120    avg_loss:0.041, val_acc:0.978]
Epoch [73/120    avg_loss:0.044, val_acc:0.982]
Epoch [74/120    avg_loss:0.037, val_acc:0.979]
Epoch [75/120    avg_loss:0.027, val_acc:0.984]
Epoch [76/120    avg_loss:0.031, val_acc:0.982]
Epoch [77/120    avg_loss:0.034, val_acc:0.977]
Epoch [78/120    avg_loss:0.025, val_acc:0.980]
Epoch [79/120    avg_loss:0.020, val_acc:0.979]
Epoch [80/120    avg_loss:0.020, val_acc:0.983]
Epoch [81/120    avg_loss:0.023, val_acc:0.985]
Epoch [82/120    avg_loss:0.019, val_acc:0.978]
Epoch [83/120    avg_loss:0.027, val_acc:0.977]
Epoch [84/120    avg_loss:0.109, val_acc:0.963]
Epoch [85/120    avg_loss:0.083, val_acc:0.979]
Epoch [86/120    avg_loss:0.051, val_acc:0.969]
Epoch [87/120    avg_loss:0.057, val_acc:0.966]
Epoch [88/120    avg_loss:0.063, val_acc:0.983]
Epoch [89/120    avg_loss:0.051, val_acc:0.982]
Epoch [90/120    avg_loss:0.030, val_acc:0.984]
Epoch [91/120    avg_loss:0.023, val_acc:0.984]
Epoch [92/120    avg_loss:0.027, val_acc:0.982]
Epoch [93/120    avg_loss:0.040, val_acc:0.978]
Epoch [94/120    avg_loss:0.023, val_acc:0.986]
Epoch [95/120    avg_loss:0.019, val_acc:0.984]
Epoch [96/120    avg_loss:0.020, val_acc:0.984]
Epoch [97/120    avg_loss:0.017, val_acc:0.984]
Epoch [98/120    avg_loss:0.018, val_acc:0.982]
Epoch [99/120    avg_loss:0.016, val_acc:0.987]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.013, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.014, val_acc:0.986]
Epoch [105/120    avg_loss:0.015, val_acc:0.984]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.015, val_acc:0.987]
Epoch [109/120    avg_loss:0.015, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.986]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.015, val_acc:0.989]
Epoch [116/120    avg_loss:0.013, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.014, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6352     0     0     0     0     0    33    44     3]
 [    0     0 18006     0    76     0     8     0     0     0]
 [    0     8     0  1976     0     0     0     0    48     4]
 [    0    26    20     0  2872     0    10     0    44     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4    16     0     0  4855     0     1     2]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    17     0     4    48     0     0     0  3502     0]
 [    0     0     0     3    18    63     0     0     1   834]]

Accuracy:
98.79015737594293

F1 scores:
[       nan 0.98979353 0.99700997 0.97942999 0.95957234 0.97643098
 0.9957953  0.98698315 0.97129386 0.94611458]

Kappa:
0.9839805160860772
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b2ebc5ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.212, val_acc:0.176]
Epoch [2/120    avg_loss:2.004, val_acc:0.196]
Epoch [3/120    avg_loss:1.828, val_acc:0.240]
Epoch [4/120    avg_loss:1.682, val_acc:0.276]
Epoch [5/120    avg_loss:1.556, val_acc:0.342]
Epoch [6/120    avg_loss:1.370, val_acc:0.354]
Epoch [7/120    avg_loss:1.237, val_acc:0.403]
Epoch [8/120    avg_loss:1.152, val_acc:0.422]
Epoch [9/120    avg_loss:0.987, val_acc:0.480]
Epoch [10/120    avg_loss:0.847, val_acc:0.529]
Epoch [11/120    avg_loss:0.762, val_acc:0.637]
Epoch [12/120    avg_loss:0.633, val_acc:0.675]
Epoch [13/120    avg_loss:0.552, val_acc:0.760]
Epoch [14/120    avg_loss:0.497, val_acc:0.792]
Epoch [15/120    avg_loss:0.428, val_acc:0.819]
Epoch [16/120    avg_loss:0.380, val_acc:0.786]
Epoch [17/120    avg_loss:0.319, val_acc:0.833]
Epoch [18/120    avg_loss:0.300, val_acc:0.883]
Epoch [19/120    avg_loss:0.286, val_acc:0.821]
Epoch [20/120    avg_loss:0.255, val_acc:0.882]
Epoch [21/120    avg_loss:0.253, val_acc:0.869]
Epoch [22/120    avg_loss:0.237, val_acc:0.903]
Epoch [23/120    avg_loss:0.255, val_acc:0.911]
Epoch [24/120    avg_loss:0.175, val_acc:0.921]
Epoch [25/120    avg_loss:0.229, val_acc:0.866]
Epoch [26/120    avg_loss:0.137, val_acc:0.946]
Epoch [27/120    avg_loss:0.111, val_acc:0.957]
Epoch [28/120    avg_loss:0.097, val_acc:0.954]
Epoch [29/120    avg_loss:0.092, val_acc:0.944]
Epoch [30/120    avg_loss:0.096, val_acc:0.946]
Epoch [31/120    avg_loss:0.103, val_acc:0.962]
Epoch [32/120    avg_loss:0.092, val_acc:0.964]
Epoch [33/120    avg_loss:0.094, val_acc:0.957]
Epoch [34/120    avg_loss:0.092, val_acc:0.953]
Epoch [35/120    avg_loss:0.082, val_acc:0.973]
Epoch [36/120    avg_loss:0.070, val_acc:0.969]
Epoch [37/120    avg_loss:0.045, val_acc:0.980]
Epoch [38/120    avg_loss:0.075, val_acc:0.963]
Epoch [39/120    avg_loss:0.061, val_acc:0.970]
Epoch [40/120    avg_loss:0.052, val_acc:0.973]
Epoch [41/120    avg_loss:0.070, val_acc:0.961]
Epoch [42/120    avg_loss:0.066, val_acc:0.967]
Epoch [43/120    avg_loss:0.067, val_acc:0.969]
Epoch [44/120    avg_loss:0.050, val_acc:0.967]
Epoch [45/120    avg_loss:0.033, val_acc:0.986]
Epoch [46/120    avg_loss:0.028, val_acc:0.978]
Epoch [47/120    avg_loss:0.030, val_acc:0.987]
Epoch [48/120    avg_loss:0.022, val_acc:0.980]
Epoch [49/120    avg_loss:0.028, val_acc:0.958]
Epoch [50/120    avg_loss:0.038, val_acc:0.960]
Epoch [51/120    avg_loss:0.091, val_acc:0.936]
Epoch [52/120    avg_loss:0.142, val_acc:0.963]
Epoch [53/120    avg_loss:0.074, val_acc:0.930]
Epoch [54/120    avg_loss:0.056, val_acc:0.975]
Epoch [55/120    avg_loss:0.054, val_acc:0.959]
Epoch [56/120    avg_loss:0.063, val_acc:0.973]
Epoch [57/120    avg_loss:0.042, val_acc:0.980]
Epoch [58/120    avg_loss:0.031, val_acc:0.971]
Epoch [59/120    avg_loss:0.046, val_acc:0.944]
Epoch [60/120    avg_loss:0.063, val_acc:0.981]
Epoch [61/120    avg_loss:0.038, val_acc:0.977]
Epoch [62/120    avg_loss:0.027, val_acc:0.977]
Epoch [63/120    avg_loss:0.034, val_acc:0.977]
Epoch [64/120    avg_loss:0.025, val_acc:0.977]
Epoch [65/120    avg_loss:0.022, val_acc:0.976]
Epoch [66/120    avg_loss:0.023, val_acc:0.978]
Epoch [67/120    avg_loss:0.027, val_acc:0.983]
Epoch [68/120    avg_loss:0.021, val_acc:0.978]
Epoch [69/120    avg_loss:0.021, val_acc:0.983]
Epoch [70/120    avg_loss:0.020, val_acc:0.983]
Epoch [71/120    avg_loss:0.023, val_acc:0.980]
Epoch [72/120    avg_loss:0.020, val_acc:0.977]
Epoch [73/120    avg_loss:0.021, val_acc:0.982]
Epoch [74/120    avg_loss:0.018, val_acc:0.982]
Epoch [75/120    avg_loss:0.022, val_acc:0.982]
Epoch [76/120    avg_loss:0.018, val_acc:0.982]
Epoch [77/120    avg_loss:0.020, val_acc:0.982]
Epoch [78/120    avg_loss:0.019, val_acc:0.982]
Epoch [79/120    avg_loss:0.020, val_acc:0.982]
Epoch [80/120    avg_loss:0.019, val_acc:0.982]
Epoch [81/120    avg_loss:0.020, val_acc:0.982]
Epoch [82/120    avg_loss:0.016, val_acc:0.982]
Epoch [83/120    avg_loss:0.019, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.981]
Epoch [85/120    avg_loss:0.020, val_acc:0.980]
Epoch [86/120    avg_loss:0.020, val_acc:0.981]
Epoch [87/120    avg_loss:0.022, val_acc:0.981]
Epoch [88/120    avg_loss:0.022, val_acc:0.981]
Epoch [89/120    avg_loss:0.021, val_acc:0.981]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.020, val_acc:0.981]
Epoch [92/120    avg_loss:0.023, val_acc:0.981]
Epoch [93/120    avg_loss:0.019, val_acc:0.981]
Epoch [94/120    avg_loss:0.022, val_acc:0.981]
Epoch [95/120    avg_loss:0.022, val_acc:0.981]
Epoch [96/120    avg_loss:0.023, val_acc:0.981]
Epoch [97/120    avg_loss:0.022, val_acc:0.981]
Epoch [98/120    avg_loss:0.021, val_acc:0.981]
Epoch [99/120    avg_loss:0.018, val_acc:0.981]
Epoch [100/120    avg_loss:0.018, val_acc:0.981]
Epoch [101/120    avg_loss:0.019, val_acc:0.981]
Epoch [102/120    avg_loss:0.015, val_acc:0.981]
Epoch [103/120    avg_loss:0.019, val_acc:0.981]
Epoch [104/120    avg_loss:0.021, val_acc:0.981]
Epoch [105/120    avg_loss:0.018, val_acc:0.981]
Epoch [106/120    avg_loss:0.016, val_acc:0.981]
Epoch [107/120    avg_loss:0.016, val_acc:0.981]
Epoch [108/120    avg_loss:0.019, val_acc:0.981]
Epoch [109/120    avg_loss:0.019, val_acc:0.981]
Epoch [110/120    avg_loss:0.018, val_acc:0.981]
Epoch [111/120    avg_loss:0.020, val_acc:0.981]
Epoch [112/120    avg_loss:0.024, val_acc:0.981]
Epoch [113/120    avg_loss:0.016, val_acc:0.981]
Epoch [114/120    avg_loss:0.024, val_acc:0.981]
Epoch [115/120    avg_loss:0.022, val_acc:0.981]
Epoch [116/120    avg_loss:0.021, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.981]
Epoch [118/120    avg_loss:0.020, val_acc:0.981]
Epoch [119/120    avg_loss:0.020, val_acc:0.981]
Epoch [120/120    avg_loss:0.028, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6314     0     0     0     0     0     0    96    22]
 [    0     2 17972     0    37     0    79     0     0     0]
 [    0    10     0  1994     0     0     0     0    31     1]
 [    0    14    20     7  2889     0    19     0    21     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    41     0     8    72     0     0     0  3450     0]
 [    0     0     0     1    16    33     0     0     0   869]]

Accuracy:
98.71544597883981

F1 scores:
[       nan 0.98556154 0.99617538 0.98566485 0.96525226 0.98751419
 0.9900548  0.99961225 0.96247733 0.95810364]

Kappa:
0.9829976583527161
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe61e55ebe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.247, val_acc:0.052]
Epoch [2/120    avg_loss:2.002, val_acc:0.107]
Epoch [3/120    avg_loss:1.818, val_acc:0.144]
Epoch [4/120    avg_loss:1.647, val_acc:0.484]
Epoch [5/120    avg_loss:1.561, val_acc:0.510]
Epoch [6/120    avg_loss:1.431, val_acc:0.545]
Epoch [7/120    avg_loss:1.299, val_acc:0.622]
Epoch [8/120    avg_loss:1.222, val_acc:0.675]
Epoch [9/120    avg_loss:1.097, val_acc:0.687]
Epoch [10/120    avg_loss:0.995, val_acc:0.716]
Epoch [11/120    avg_loss:0.891, val_acc:0.747]
Epoch [12/120    avg_loss:0.753, val_acc:0.720]
Epoch [13/120    avg_loss:0.628, val_acc:0.768]
Epoch [14/120    avg_loss:0.564, val_acc:0.789]
Epoch [15/120    avg_loss:0.471, val_acc:0.766]
Epoch [16/120    avg_loss:0.436, val_acc:0.782]
Epoch [17/120    avg_loss:0.383, val_acc:0.812]
Epoch [18/120    avg_loss:0.344, val_acc:0.905]
Epoch [19/120    avg_loss:0.291, val_acc:0.902]
Epoch [20/120    avg_loss:0.269, val_acc:0.920]
Epoch [21/120    avg_loss:0.227, val_acc:0.953]
Epoch [22/120    avg_loss:0.205, val_acc:0.953]
Epoch [23/120    avg_loss:0.188, val_acc:0.949]
Epoch [24/120    avg_loss:0.197, val_acc:0.926]
Epoch [25/120    avg_loss:0.168, val_acc:0.951]
Epoch [26/120    avg_loss:0.146, val_acc:0.955]
Epoch [27/120    avg_loss:0.116, val_acc:0.971]
Epoch [28/120    avg_loss:0.130, val_acc:0.949]
Epoch [29/120    avg_loss:0.093, val_acc:0.962]
Epoch [30/120    avg_loss:0.099, val_acc:0.953]
Epoch [31/120    avg_loss:0.078, val_acc:0.969]
Epoch [32/120    avg_loss:0.108, val_acc:0.962]
Epoch [33/120    avg_loss:0.094, val_acc:0.971]
Epoch [34/120    avg_loss:0.079, val_acc:0.964]
Epoch [35/120    avg_loss:0.055, val_acc:0.969]
Epoch [36/120    avg_loss:0.068, val_acc:0.980]
Epoch [37/120    avg_loss:0.055, val_acc:0.983]
Epoch [38/120    avg_loss:0.057, val_acc:0.972]
Epoch [39/120    avg_loss:0.057, val_acc:0.956]
Epoch [40/120    avg_loss:0.056, val_acc:0.974]
Epoch [41/120    avg_loss:0.061, val_acc:0.981]
Epoch [42/120    avg_loss:0.045, val_acc:0.976]
Epoch [43/120    avg_loss:0.038, val_acc:0.984]
Epoch [44/120    avg_loss:0.039, val_acc:0.982]
Epoch [45/120    avg_loss:0.037, val_acc:0.987]
Epoch [46/120    avg_loss:0.057, val_acc:0.957]
Epoch [47/120    avg_loss:0.072, val_acc:0.957]
Epoch [48/120    avg_loss:0.053, val_acc:0.941]
Epoch [49/120    avg_loss:0.041, val_acc:0.970]
Epoch [50/120    avg_loss:0.054, val_acc:0.984]
Epoch [51/120    avg_loss:0.058, val_acc:0.984]
Epoch [52/120    avg_loss:0.034, val_acc:0.976]
Epoch [53/120    avg_loss:0.047, val_acc:0.970]
Epoch [54/120    avg_loss:0.032, val_acc:0.970]
Epoch [55/120    avg_loss:0.023, val_acc:0.984]
Epoch [56/120    avg_loss:0.042, val_acc:0.983]
Epoch [57/120    avg_loss:0.024, val_acc:0.984]
Epoch [58/120    avg_loss:0.024, val_acc:0.971]
Epoch [59/120    avg_loss:0.023, val_acc:0.984]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.016, val_acc:0.986]
Epoch [62/120    avg_loss:0.015, val_acc:0.987]
Epoch [63/120    avg_loss:0.014, val_acc:0.986]
Epoch [64/120    avg_loss:0.016, val_acc:0.987]
Epoch [65/120    avg_loss:0.014, val_acc:0.987]
Epoch [66/120    avg_loss:0.014, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.987]
Epoch [68/120    avg_loss:0.015, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.987]
Epoch [71/120    avg_loss:0.013, val_acc:0.988]
Epoch [72/120    avg_loss:0.012, val_acc:0.988]
Epoch [73/120    avg_loss:0.011, val_acc:0.987]
Epoch [74/120    avg_loss:0.013, val_acc:0.988]
Epoch [75/120    avg_loss:0.011, val_acc:0.988]
Epoch [76/120    avg_loss:0.012, val_acc:0.989]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.015, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.015, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.010, val_acc:0.988]
Epoch [84/120    avg_loss:0.013, val_acc:0.988]
Epoch [85/120    avg_loss:0.011, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.009, val_acc:0.989]
Epoch [88/120    avg_loss:0.015, val_acc:0.989]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.015, val_acc:0.989]
Epoch [91/120    avg_loss:0.010, val_acc:0.989]
Epoch [92/120    avg_loss:0.012, val_acc:0.989]
Epoch [93/120    avg_loss:0.010, val_acc:0.989]
Epoch [94/120    avg_loss:0.011, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.011, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.989]
Epoch [101/120    avg_loss:0.011, val_acc:0.989]
Epoch [102/120    avg_loss:0.010, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.013, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.989]
Epoch [109/120    avg_loss:0.011, val_acc:0.989]
Epoch [110/120    avg_loss:0.011, val_acc:0.989]
Epoch [111/120    avg_loss:0.014, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     0     0     0     0     4    30]
 [    0     2 18087     0     1     0     0     0     0     0]
 [    0     4     0  2014     0     0     0     0    18     0]
 [    0    23    20     2  2893     0    11     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15    11     0     0  4852     0     0     0]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     0     0    47    55     0     0     0  3469     0]
 [    0     0     0     0     2    39     0     0     0   878]]

Accuracy:
99.2504759839009

F1 scores:
[       nan 0.99510071 0.99895062 0.98004866 0.97686983 0.98527746
 0.99599713 0.9984472  0.97925194 0.96008748]

Kappa:
0.990065814296017
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff06441b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.163, val_acc:0.095]
Epoch [2/120    avg_loss:1.873, val_acc:0.128]
Epoch [3/120    avg_loss:1.654, val_acc:0.133]
Epoch [4/120    avg_loss:1.507, val_acc:0.165]
Epoch [5/120    avg_loss:1.395, val_acc:0.288]
Epoch [6/120    avg_loss:1.283, val_acc:0.324]
Epoch [7/120    avg_loss:1.167, val_acc:0.354]
Epoch [8/120    avg_loss:1.070, val_acc:0.387]
Epoch [9/120    avg_loss:0.983, val_acc:0.424]
Epoch [10/120    avg_loss:0.903, val_acc:0.501]
Epoch [11/120    avg_loss:0.798, val_acc:0.479]
Epoch [12/120    avg_loss:0.757, val_acc:0.617]
Epoch [13/120    avg_loss:0.659, val_acc:0.679]
Epoch [14/120    avg_loss:0.612, val_acc:0.672]
Epoch [15/120    avg_loss:0.545, val_acc:0.707]
Epoch [16/120    avg_loss:0.498, val_acc:0.670]
Epoch [17/120    avg_loss:0.436, val_acc:0.774]
Epoch [18/120    avg_loss:0.397, val_acc:0.785]
Epoch [19/120    avg_loss:0.386, val_acc:0.766]
Epoch [20/120    avg_loss:0.350, val_acc:0.841]
Epoch [21/120    avg_loss:0.287, val_acc:0.861]
Epoch [22/120    avg_loss:0.260, val_acc:0.773]
Epoch [23/120    avg_loss:0.246, val_acc:0.938]
Epoch [24/120    avg_loss:0.244, val_acc:0.919]
Epoch [25/120    avg_loss:0.187, val_acc:0.943]
Epoch [26/120    avg_loss:0.191, val_acc:0.900]
Epoch [27/120    avg_loss:0.196, val_acc:0.928]
Epoch [28/120    avg_loss:0.224, val_acc:0.946]
Epoch [29/120    avg_loss:0.186, val_acc:0.890]
Epoch [30/120    avg_loss:0.146, val_acc:0.928]
Epoch [31/120    avg_loss:0.136, val_acc:0.959]
Epoch [32/120    avg_loss:0.149, val_acc:0.956]
Epoch [33/120    avg_loss:0.148, val_acc:0.934]
Epoch [34/120    avg_loss:0.117, val_acc:0.953]
Epoch [35/120    avg_loss:0.113, val_acc:0.965]
Epoch [36/120    avg_loss:0.122, val_acc:0.914]
Epoch [37/120    avg_loss:0.098, val_acc:0.900]
Epoch [38/120    avg_loss:0.166, val_acc:0.922]
Epoch [39/120    avg_loss:0.167, val_acc:0.929]
Epoch [40/120    avg_loss:0.161, val_acc:0.924]
Epoch [41/120    avg_loss:0.112, val_acc:0.965]
Epoch [42/120    avg_loss:0.109, val_acc:0.931]
Epoch [43/120    avg_loss:0.078, val_acc:0.967]
Epoch [44/120    avg_loss:0.114, val_acc:0.879]
Epoch [45/120    avg_loss:0.102, val_acc:0.964]
Epoch [46/120    avg_loss:0.072, val_acc:0.977]
Epoch [47/120    avg_loss:0.055, val_acc:0.967]
Epoch [48/120    avg_loss:0.093, val_acc:0.940]
Epoch [49/120    avg_loss:0.062, val_acc:0.968]
Epoch [50/120    avg_loss:0.060, val_acc:0.956]
Epoch [51/120    avg_loss:0.055, val_acc:0.966]
Epoch [52/120    avg_loss:0.049, val_acc:0.980]
Epoch [53/120    avg_loss:0.044, val_acc:0.933]
Epoch [54/120    avg_loss:0.060, val_acc:0.978]
Epoch [55/120    avg_loss:0.041, val_acc:0.975]
Epoch [56/120    avg_loss:0.031, val_acc:0.967]
Epoch [57/120    avg_loss:0.027, val_acc:0.988]
Epoch [58/120    avg_loss:0.025, val_acc:0.984]
Epoch [59/120    avg_loss:0.021, val_acc:0.986]
Epoch [60/120    avg_loss:0.030, val_acc:0.970]
Epoch [61/120    avg_loss:0.023, val_acc:0.982]
Epoch [62/120    avg_loss:0.028, val_acc:0.976]
Epoch [63/120    avg_loss:0.031, val_acc:0.976]
Epoch [64/120    avg_loss:0.027, val_acc:0.984]
Epoch [65/120    avg_loss:0.029, val_acc:0.983]
Epoch [66/120    avg_loss:0.036, val_acc:0.986]
Epoch [67/120    avg_loss:0.037, val_acc:0.972]
Epoch [68/120    avg_loss:0.031, val_acc:0.979]
Epoch [69/120    avg_loss:0.031, val_acc:0.945]
Epoch [70/120    avg_loss:0.025, val_acc:0.984]
Epoch [71/120    avg_loss:0.019, val_acc:0.991]
Epoch [72/120    avg_loss:0.016, val_acc:0.988]
Epoch [73/120    avg_loss:0.013, val_acc:0.990]
Epoch [74/120    avg_loss:0.011, val_acc:0.988]
Epoch [75/120    avg_loss:0.014, val_acc:0.990]
Epoch [76/120    avg_loss:0.015, val_acc:0.988]
Epoch [77/120    avg_loss:0.011, val_acc:0.989]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.011, val_acc:0.988]
Epoch [81/120    avg_loss:0.012, val_acc:0.990]
Epoch [82/120    avg_loss:0.012, val_acc:0.991]
Epoch [83/120    avg_loss:0.011, val_acc:0.991]
Epoch [84/120    avg_loss:0.010, val_acc:0.991]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.989]
Epoch [87/120    avg_loss:0.010, val_acc:0.989]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.989]
Epoch [91/120    avg_loss:0.010, val_acc:0.989]
Epoch [92/120    avg_loss:0.010, val_acc:0.990]
Epoch [93/120    avg_loss:0.012, val_acc:0.989]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.990]
Epoch [97/120    avg_loss:0.010, val_acc:0.991]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.010, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.992]
Epoch [101/120    avg_loss:0.009, val_acc:0.991]
Epoch [102/120    avg_loss:0.011, val_acc:0.991]
Epoch [103/120    avg_loss:0.011, val_acc:0.989]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.989]
Epoch [106/120    avg_loss:0.009, val_acc:0.991]
Epoch [107/120    avg_loss:0.010, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.989]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.990]
Epoch [112/120    avg_loss:0.009, val_acc:0.990]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.011, val_acc:0.991]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.991]
Epoch [118/120    avg_loss:0.010, val_acc:0.991]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     0     0     0    10     0    62     1]
 [    0     3 18059     0    15     0    12     0     1     0]
 [    0     8     0  1942     0     0     0     0    85     1]
 [    0     2    10     0  2922     0    30     0     4     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    31     0     0     0  4827     0    20     0]
 [    0    23     0     0     0     0     0  1267     0     0]
 [    0    12     0    52    37     0     0     0  3470     0]
 [    0     1     0     0     0     3     0     0     0   915]]

Accuracy:
98.97091075603115

F1 scores:
[       nan 0.99049844 0.9980105  0.96377171 0.98284561 0.99885189
 0.98944348 0.99100508 0.96215167 0.99456522]

Kappa:
0.986362041186867
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ba1375b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.210, val_acc:0.099]
Epoch [2/120    avg_loss:1.970, val_acc:0.137]
Epoch [3/120    avg_loss:1.762, val_acc:0.168]
Epoch [4/120    avg_loss:1.581, val_acc:0.180]
Epoch [5/120    avg_loss:1.439, val_acc:0.240]
Epoch [6/120    avg_loss:1.302, val_acc:0.475]
Epoch [7/120    avg_loss:1.233, val_acc:0.677]
Epoch [8/120    avg_loss:1.100, val_acc:0.712]
Epoch [9/120    avg_loss:1.003, val_acc:0.682]
Epoch [10/120    avg_loss:0.894, val_acc:0.744]
Epoch [11/120    avg_loss:0.822, val_acc:0.698]
Epoch [12/120    avg_loss:0.731, val_acc:0.791]
Epoch [13/120    avg_loss:0.648, val_acc:0.773]
Epoch [14/120    avg_loss:0.543, val_acc:0.803]
Epoch [15/120    avg_loss:0.492, val_acc:0.800]
Epoch [16/120    avg_loss:0.460, val_acc:0.802]
Epoch [17/120    avg_loss:0.423, val_acc:0.796]
Epoch [18/120    avg_loss:0.383, val_acc:0.869]
Epoch [19/120    avg_loss:0.334, val_acc:0.851]
Epoch [20/120    avg_loss:0.311, val_acc:0.854]
Epoch [21/120    avg_loss:0.275, val_acc:0.884]
Epoch [22/120    avg_loss:0.279, val_acc:0.903]
Epoch [23/120    avg_loss:0.242, val_acc:0.903]
Epoch [24/120    avg_loss:0.214, val_acc:0.932]
Epoch [25/120    avg_loss:0.217, val_acc:0.914]
Epoch [26/120    avg_loss:0.198, val_acc:0.929]
Epoch [27/120    avg_loss:0.189, val_acc:0.939]
Epoch [28/120    avg_loss:0.166, val_acc:0.939]
Epoch [29/120    avg_loss:0.179, val_acc:0.919]
Epoch [30/120    avg_loss:0.211, val_acc:0.924]
Epoch [31/120    avg_loss:0.155, val_acc:0.959]
Epoch [32/120    avg_loss:0.152, val_acc:0.953]
Epoch [33/120    avg_loss:0.130, val_acc:0.926]
Epoch [34/120    avg_loss:0.143, val_acc:0.953]
Epoch [35/120    avg_loss:0.122, val_acc:0.960]
Epoch [36/120    avg_loss:0.103, val_acc:0.955]
Epoch [37/120    avg_loss:0.104, val_acc:0.942]
Epoch [38/120    avg_loss:0.081, val_acc:0.958]
Epoch [39/120    avg_loss:0.081, val_acc:0.968]
Epoch [40/120    avg_loss:0.079, val_acc:0.963]
Epoch [41/120    avg_loss:0.095, val_acc:0.950]
Epoch [42/120    avg_loss:0.076, val_acc:0.941]
Epoch [43/120    avg_loss:0.074, val_acc:0.961]
Epoch [44/120    avg_loss:0.081, val_acc:0.954]
Epoch [45/120    avg_loss:0.105, val_acc:0.942]
Epoch [46/120    avg_loss:0.080, val_acc:0.952]
Epoch [47/120    avg_loss:0.074, val_acc:0.951]
Epoch [48/120    avg_loss:0.092, val_acc:0.919]
Epoch [49/120    avg_loss:0.106, val_acc:0.956]
Epoch [50/120    avg_loss:0.081, val_acc:0.880]
Epoch [51/120    avg_loss:0.114, val_acc:0.944]
Epoch [52/120    avg_loss:0.062, val_acc:0.951]
Epoch [53/120    avg_loss:0.055, val_acc:0.970]
Epoch [54/120    avg_loss:0.037, val_acc:0.968]
Epoch [55/120    avg_loss:0.039, val_acc:0.970]
Epoch [56/120    avg_loss:0.040, val_acc:0.969]
Epoch [57/120    avg_loss:0.036, val_acc:0.970]
Epoch [58/120    avg_loss:0.036, val_acc:0.970]
Epoch [59/120    avg_loss:0.032, val_acc:0.969]
Epoch [60/120    avg_loss:0.031, val_acc:0.970]
Epoch [61/120    avg_loss:0.033, val_acc:0.973]
Epoch [62/120    avg_loss:0.036, val_acc:0.969]
Epoch [63/120    avg_loss:0.029, val_acc:0.970]
Epoch [64/120    avg_loss:0.030, val_acc:0.970]
Epoch [65/120    avg_loss:0.030, val_acc:0.975]
Epoch [66/120    avg_loss:0.029, val_acc:0.977]
Epoch [67/120    avg_loss:0.028, val_acc:0.975]
Epoch [68/120    avg_loss:0.027, val_acc:0.972]
Epoch [69/120    avg_loss:0.028, val_acc:0.972]
Epoch [70/120    avg_loss:0.031, val_acc:0.972]
Epoch [71/120    avg_loss:0.031, val_acc:0.968]
Epoch [72/120    avg_loss:0.027, val_acc:0.975]
Epoch [73/120    avg_loss:0.028, val_acc:0.969]
Epoch [74/120    avg_loss:0.027, val_acc:0.970]
Epoch [75/120    avg_loss:0.025, val_acc:0.972]
Epoch [76/120    avg_loss:0.033, val_acc:0.970]
Epoch [77/120    avg_loss:0.025, val_acc:0.972]
Epoch [78/120    avg_loss:0.027, val_acc:0.972]
Epoch [79/120    avg_loss:0.027, val_acc:0.975]
Epoch [80/120    avg_loss:0.026, val_acc:0.974]
Epoch [81/120    avg_loss:0.025, val_acc:0.974]
Epoch [82/120    avg_loss:0.025, val_acc:0.975]
Epoch [83/120    avg_loss:0.025, val_acc:0.975]
Epoch [84/120    avg_loss:0.027, val_acc:0.975]
Epoch [85/120    avg_loss:0.026, val_acc:0.974]
Epoch [86/120    avg_loss:0.025, val_acc:0.974]
Epoch [87/120    avg_loss:0.023, val_acc:0.973]
Epoch [88/120    avg_loss:0.024, val_acc:0.973]
Epoch [89/120    avg_loss:0.024, val_acc:0.973]
Epoch [90/120    avg_loss:0.024, val_acc:0.974]
Epoch [91/120    avg_loss:0.024, val_acc:0.974]
Epoch [92/120    avg_loss:0.027, val_acc:0.973]
Epoch [93/120    avg_loss:0.023, val_acc:0.974]
Epoch [94/120    avg_loss:0.025, val_acc:0.974]
Epoch [95/120    avg_loss:0.029, val_acc:0.974]
Epoch [96/120    avg_loss:0.026, val_acc:0.973]
Epoch [97/120    avg_loss:0.027, val_acc:0.973]
Epoch [98/120    avg_loss:0.024, val_acc:0.973]
Epoch [99/120    avg_loss:0.026, val_acc:0.972]
Epoch [100/120    avg_loss:0.027, val_acc:0.973]
Epoch [101/120    avg_loss:0.022, val_acc:0.974]
Epoch [102/120    avg_loss:0.031, val_acc:0.972]
Epoch [103/120    avg_loss:0.028, val_acc:0.972]
Epoch [104/120    avg_loss:0.027, val_acc:0.973]
Epoch [105/120    avg_loss:0.024, val_acc:0.972]
Epoch [106/120    avg_loss:0.029, val_acc:0.973]
Epoch [107/120    avg_loss:0.025, val_acc:0.973]
Epoch [108/120    avg_loss:0.024, val_acc:0.973]
Epoch [109/120    avg_loss:0.030, val_acc:0.973]
Epoch [110/120    avg_loss:0.031, val_acc:0.973]
Epoch [111/120    avg_loss:0.027, val_acc:0.973]
Epoch [112/120    avg_loss:0.026, val_acc:0.973]
Epoch [113/120    avg_loss:0.026, val_acc:0.973]
Epoch [114/120    avg_loss:0.024, val_acc:0.973]
Epoch [115/120    avg_loss:0.023, val_acc:0.973]
Epoch [116/120    avg_loss:0.030, val_acc:0.973]
Epoch [117/120    avg_loss:0.025, val_acc:0.973]
Epoch [118/120    avg_loss:0.024, val_acc:0.973]
Epoch [119/120    avg_loss:0.025, val_acc:0.973]
Epoch [120/120    avg_loss:0.025, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     0     0     3    27    63     0]
 [    0     0 17957     0    34     0    97     0     2     0]
 [    0    10     0  1874     0     0     0     0   149     3]
 [    0    17    15     0  2910     0    25     4     0     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   110     0     0     0  4768     0     0     0]
 [    0    10     0     0     0     0     0  1278     2     0]
 [    0    36     0    56    36     0     3     0  3440     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.29368809196733

F1 scores:
[       nan 0.98707568 0.99286741 0.94503278 0.97782258 0.99808795
 0.97564968 0.98345518 0.95198561 0.99510071]

Kappa:
0.9773911012066372
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03982deb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.202, val_acc:0.193]
Epoch [2/120    avg_loss:1.939, val_acc:0.175]
Epoch [3/120    avg_loss:1.763, val_acc:0.320]
Epoch [4/120    avg_loss:1.585, val_acc:0.518]
Epoch [5/120    avg_loss:1.430, val_acc:0.615]
Epoch [6/120    avg_loss:1.290, val_acc:0.656]
Epoch [7/120    avg_loss:1.183, val_acc:0.692]
Epoch [8/120    avg_loss:1.073, val_acc:0.697]
Epoch [9/120    avg_loss:0.957, val_acc:0.718]
Epoch [10/120    avg_loss:0.818, val_acc:0.688]
Epoch [11/120    avg_loss:0.757, val_acc:0.738]
Epoch [12/120    avg_loss:0.638, val_acc:0.751]
Epoch [13/120    avg_loss:0.555, val_acc:0.715]
Epoch [14/120    avg_loss:0.479, val_acc:0.740]
Epoch [15/120    avg_loss:0.450, val_acc:0.685]
Epoch [16/120    avg_loss:0.433, val_acc:0.627]
Epoch [17/120    avg_loss:0.411, val_acc:0.751]
Epoch [18/120    avg_loss:0.381, val_acc:0.756]
Epoch [19/120    avg_loss:0.348, val_acc:0.767]
Epoch [20/120    avg_loss:0.342, val_acc:0.776]
Epoch [21/120    avg_loss:0.309, val_acc:0.788]
Epoch [22/120    avg_loss:0.289, val_acc:0.782]
Epoch [23/120    avg_loss:0.299, val_acc:0.845]
Epoch [24/120    avg_loss:0.258, val_acc:0.689]
Epoch [25/120    avg_loss:0.263, val_acc:0.868]
Epoch [26/120    avg_loss:0.230, val_acc:0.878]
Epoch [27/120    avg_loss:0.227, val_acc:0.867]
Epoch [28/120    avg_loss:0.204, val_acc:0.927]
Epoch [29/120    avg_loss:0.176, val_acc:0.910]
Epoch [30/120    avg_loss:0.196, val_acc:0.915]
Epoch [31/120    avg_loss:0.169, val_acc:0.919]
Epoch [32/120    avg_loss:0.136, val_acc:0.935]
Epoch [33/120    avg_loss:0.156, val_acc:0.934]
Epoch [34/120    avg_loss:0.135, val_acc:0.915]
Epoch [35/120    avg_loss:0.112, val_acc:0.924]
Epoch [36/120    avg_loss:0.116, val_acc:0.935]
Epoch [37/120    avg_loss:0.131, val_acc:0.942]
Epoch [38/120    avg_loss:0.100, val_acc:0.939]
Epoch [39/120    avg_loss:0.153, val_acc:0.920]
Epoch [40/120    avg_loss:0.144, val_acc:0.904]
Epoch [41/120    avg_loss:0.109, val_acc:0.929]
Epoch [42/120    avg_loss:0.090, val_acc:0.921]
Epoch [43/120    avg_loss:0.074, val_acc:0.944]
Epoch [44/120    avg_loss:0.080, val_acc:0.952]
Epoch [45/120    avg_loss:0.105, val_acc:0.911]
Epoch [46/120    avg_loss:0.100, val_acc:0.957]
Epoch [47/120    avg_loss:0.127, val_acc:0.932]
Epoch [48/120    avg_loss:0.074, val_acc:0.955]
Epoch [49/120    avg_loss:0.067, val_acc:0.961]
Epoch [50/120    avg_loss:0.061, val_acc:0.965]
Epoch [51/120    avg_loss:0.075, val_acc:0.958]
Epoch [52/120    avg_loss:0.058, val_acc:0.959]
Epoch [53/120    avg_loss:0.043, val_acc:0.965]
Epoch [54/120    avg_loss:0.058, val_acc:0.978]
Epoch [55/120    avg_loss:0.066, val_acc:0.957]
Epoch [56/120    avg_loss:0.040, val_acc:0.964]
Epoch [57/120    avg_loss:0.065, val_acc:0.894]
Epoch [58/120    avg_loss:0.065, val_acc:0.961]
Epoch [59/120    avg_loss:0.046, val_acc:0.935]
Epoch [60/120    avg_loss:0.052, val_acc:0.965]
Epoch [61/120    avg_loss:0.034, val_acc:0.970]
Epoch [62/120    avg_loss:0.040, val_acc:0.970]
Epoch [63/120    avg_loss:0.032, val_acc:0.970]
Epoch [64/120    avg_loss:0.036, val_acc:0.969]
Epoch [65/120    avg_loss:0.038, val_acc:0.954]
Epoch [66/120    avg_loss:0.040, val_acc:0.974]
Epoch [67/120    avg_loss:0.056, val_acc:0.951]
Epoch [68/120    avg_loss:0.042, val_acc:0.970]
Epoch [69/120    avg_loss:0.028, val_acc:0.972]
Epoch [70/120    avg_loss:0.027, val_acc:0.972]
Epoch [71/120    avg_loss:0.019, val_acc:0.973]
Epoch [72/120    avg_loss:0.022, val_acc:0.975]
Epoch [73/120    avg_loss:0.022, val_acc:0.974]
Epoch [74/120    avg_loss:0.023, val_acc:0.975]
Epoch [75/120    avg_loss:0.021, val_acc:0.975]
Epoch [76/120    avg_loss:0.021, val_acc:0.976]
Epoch [77/120    avg_loss:0.023, val_acc:0.975]
Epoch [78/120    avg_loss:0.018, val_acc:0.975]
Epoch [79/120    avg_loss:0.019, val_acc:0.977]
Epoch [80/120    avg_loss:0.019, val_acc:0.976]
Epoch [81/120    avg_loss:0.020, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.976]
Epoch [83/120    avg_loss:0.019, val_acc:0.976]
Epoch [84/120    avg_loss:0.016, val_acc:0.977]
Epoch [85/120    avg_loss:0.018, val_acc:0.977]
Epoch [86/120    avg_loss:0.021, val_acc:0.977]
Epoch [87/120    avg_loss:0.015, val_acc:0.977]
Epoch [88/120    avg_loss:0.015, val_acc:0.977]
Epoch [89/120    avg_loss:0.017, val_acc:0.976]
Epoch [90/120    avg_loss:0.019, val_acc:0.976]
Epoch [91/120    avg_loss:0.018, val_acc:0.976]
Epoch [92/120    avg_loss:0.016, val_acc:0.976]
Epoch [93/120    avg_loss:0.014, val_acc:0.976]
Epoch [94/120    avg_loss:0.016, val_acc:0.976]
Epoch [95/120    avg_loss:0.015, val_acc:0.976]
Epoch [96/120    avg_loss:0.015, val_acc:0.976]
Epoch [97/120    avg_loss:0.018, val_acc:0.976]
Epoch [98/120    avg_loss:0.019, val_acc:0.976]
Epoch [99/120    avg_loss:0.020, val_acc:0.976]
Epoch [100/120    avg_loss:0.015, val_acc:0.976]
Epoch [101/120    avg_loss:0.018, val_acc:0.976]
Epoch [102/120    avg_loss:0.020, val_acc:0.976]
Epoch [103/120    avg_loss:0.019, val_acc:0.976]
Epoch [104/120    avg_loss:0.016, val_acc:0.976]
Epoch [105/120    avg_loss:0.017, val_acc:0.976]
Epoch [106/120    avg_loss:0.016, val_acc:0.976]
Epoch [107/120    avg_loss:0.023, val_acc:0.976]
Epoch [108/120    avg_loss:0.017, val_acc:0.976]
Epoch [109/120    avg_loss:0.018, val_acc:0.976]
Epoch [110/120    avg_loss:0.018, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.976]
Epoch [112/120    avg_loss:0.017, val_acc:0.976]
Epoch [113/120    avg_loss:0.019, val_acc:0.976]
Epoch [114/120    avg_loss:0.017, val_acc:0.976]
Epoch [115/120    avg_loss:0.016, val_acc:0.976]
Epoch [116/120    avg_loss:0.016, val_acc:0.976]
Epoch [117/120    avg_loss:0.016, val_acc:0.976]
Epoch [118/120    avg_loss:0.021, val_acc:0.976]
Epoch [119/120    avg_loss:0.016, val_acc:0.976]
Epoch [120/120    avg_loss:0.018, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     1     0     0     0    21    31    32]
 [    0     0 18017     0    27     0    42     0     4     0]
 [    0     4     0  1888     0     0     0     0   144     0]
 [    0    30     7     2  2925     0     7     0     0     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    68     1     0     0  4799     0    10     0]
 [    0    34     0     0     0     0     0  1253     3     0]
 [    0    11     0    85    30     0     2     0  3443     0]
 [    0     1     0     0     4    15     0     0     0   899]]

Accuracy:
98.51300219314102

F1 scores:
[       nan 0.98716852 0.99590957 0.94094194 0.98187311 0.99428571
 0.98663651 0.9773791  0.95559256 0.97136683]

Kappa:
0.9802959509843999
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe92b22b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.117, val_acc:0.142]
Epoch [2/120    avg_loss:1.861, val_acc:0.195]
Epoch [3/120    avg_loss:1.677, val_acc:0.238]
Epoch [4/120    avg_loss:1.503, val_acc:0.268]
Epoch [5/120    avg_loss:1.354, val_acc:0.301]
Epoch [6/120    avg_loss:1.220, val_acc:0.326]
Epoch [7/120    avg_loss:1.074, val_acc:0.436]
Epoch [8/120    avg_loss:0.986, val_acc:0.550]
Epoch [9/120    avg_loss:0.896, val_acc:0.660]
Epoch [10/120    avg_loss:0.804, val_acc:0.723]
Epoch [11/120    avg_loss:0.706, val_acc:0.757]
Epoch [12/120    avg_loss:0.614, val_acc:0.727]
Epoch [13/120    avg_loss:0.528, val_acc:0.777]
Epoch [14/120    avg_loss:0.488, val_acc:0.764]
Epoch [15/120    avg_loss:0.458, val_acc:0.750]
Epoch [16/120    avg_loss:0.417, val_acc:0.755]
Epoch [17/120    avg_loss:0.395, val_acc:0.780]
Epoch [18/120    avg_loss:0.360, val_acc:0.799]
Epoch [19/120    avg_loss:0.336, val_acc:0.813]
Epoch [20/120    avg_loss:0.318, val_acc:0.842]
Epoch [21/120    avg_loss:0.275, val_acc:0.859]
Epoch [22/120    avg_loss:0.278, val_acc:0.850]
Epoch [23/120    avg_loss:0.265, val_acc:0.864]
Epoch [24/120    avg_loss:0.244, val_acc:0.842]
Epoch [25/120    avg_loss:0.195, val_acc:0.931]
Epoch [26/120    avg_loss:0.192, val_acc:0.826]
Epoch [27/120    avg_loss:0.189, val_acc:0.919]
Epoch [28/120    avg_loss:0.184, val_acc:0.912]
Epoch [29/120    avg_loss:0.167, val_acc:0.936]
Epoch [30/120    avg_loss:0.150, val_acc:0.914]
Epoch [31/120    avg_loss:0.156, val_acc:0.941]
Epoch [32/120    avg_loss:0.132, val_acc:0.947]
Epoch [33/120    avg_loss:0.136, val_acc:0.919]
Epoch [34/120    avg_loss:0.140, val_acc:0.934]
Epoch [35/120    avg_loss:0.134, val_acc:0.936]
Epoch [36/120    avg_loss:0.116, val_acc:0.940]
Epoch [37/120    avg_loss:0.090, val_acc:0.938]
Epoch [38/120    avg_loss:0.134, val_acc:0.940]
Epoch [39/120    avg_loss:0.112, val_acc:0.928]
Epoch [40/120    avg_loss:0.124, val_acc:0.929]
Epoch [41/120    avg_loss:0.091, val_acc:0.955]
Epoch [42/120    avg_loss:0.093, val_acc:0.955]
Epoch [43/120    avg_loss:0.088, val_acc:0.951]
Epoch [44/120    avg_loss:0.082, val_acc:0.943]
Epoch [45/120    avg_loss:0.071, val_acc:0.954]
Epoch [46/120    avg_loss:0.068, val_acc:0.955]
Epoch [47/120    avg_loss:0.086, val_acc:0.965]
Epoch [48/120    avg_loss:0.050, val_acc:0.965]
Epoch [49/120    avg_loss:0.048, val_acc:0.958]
Epoch [50/120    avg_loss:0.048, val_acc:0.951]
Epoch [51/120    avg_loss:0.053, val_acc:0.926]
Epoch [52/120    avg_loss:0.051, val_acc:0.967]
Epoch [53/120    avg_loss:0.053, val_acc:0.955]
Epoch [54/120    avg_loss:0.071, val_acc:0.939]
Epoch [55/120    avg_loss:0.056, val_acc:0.965]
Epoch [56/120    avg_loss:0.049, val_acc:0.965]
Epoch [57/120    avg_loss:0.053, val_acc:0.969]
Epoch [58/120    avg_loss:0.030, val_acc:0.970]
Epoch [59/120    avg_loss:0.030, val_acc:0.965]
Epoch [60/120    avg_loss:0.035, val_acc:0.965]
Epoch [61/120    avg_loss:0.029, val_acc:0.977]
Epoch [62/120    avg_loss:0.031, val_acc:0.967]
Epoch [63/120    avg_loss:0.031, val_acc:0.956]
Epoch [64/120    avg_loss:0.042, val_acc:0.939]
Epoch [65/120    avg_loss:0.067, val_acc:0.959]
Epoch [66/120    avg_loss:0.037, val_acc:0.963]
Epoch [67/120    avg_loss:0.031, val_acc:0.972]
Epoch [68/120    avg_loss:0.032, val_acc:0.969]
Epoch [69/120    avg_loss:0.097, val_acc:0.940]
Epoch [70/120    avg_loss:0.069, val_acc:0.946]
Epoch [71/120    avg_loss:0.060, val_acc:0.968]
Epoch [72/120    avg_loss:0.035, val_acc:0.970]
Epoch [73/120    avg_loss:0.059, val_acc:0.947]
Epoch [74/120    avg_loss:0.035, val_acc:0.965]
Epoch [75/120    avg_loss:0.023, val_acc:0.971]
Epoch [76/120    avg_loss:0.030, val_acc:0.974]
Epoch [77/120    avg_loss:0.017, val_acc:0.972]
Epoch [78/120    avg_loss:0.015, val_acc:0.975]
Epoch [79/120    avg_loss:0.016, val_acc:0.975]
Epoch [80/120    avg_loss:0.019, val_acc:0.976]
Epoch [81/120    avg_loss:0.019, val_acc:0.976]
Epoch [82/120    avg_loss:0.016, val_acc:0.976]
Epoch [83/120    avg_loss:0.017, val_acc:0.977]
Epoch [84/120    avg_loss:0.016, val_acc:0.977]
Epoch [85/120    avg_loss:0.023, val_acc:0.976]
Epoch [86/120    avg_loss:0.014, val_acc:0.975]
Epoch [87/120    avg_loss:0.018, val_acc:0.975]
Epoch [88/120    avg_loss:0.016, val_acc:0.977]
Epoch [89/120    avg_loss:0.015, val_acc:0.978]
Epoch [90/120    avg_loss:0.014, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.978]
Epoch [92/120    avg_loss:0.015, val_acc:0.976]
Epoch [93/120    avg_loss:0.012, val_acc:0.976]
Epoch [94/120    avg_loss:0.014, val_acc:0.978]
Epoch [95/120    avg_loss:0.012, val_acc:0.979]
Epoch [96/120    avg_loss:0.014, val_acc:0.979]
Epoch [97/120    avg_loss:0.015, val_acc:0.979]
Epoch [98/120    avg_loss:0.012, val_acc:0.979]
Epoch [99/120    avg_loss:0.012, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.014, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.979]
Epoch [106/120    avg_loss:0.015, val_acc:0.980]
Epoch [107/120    avg_loss:0.015, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.979]
Epoch [111/120    avg_loss:0.012, val_acc:0.979]
Epoch [112/120    avg_loss:0.010, val_acc:0.979]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.015, val_acc:0.976]
Epoch [115/120    avg_loss:0.014, val_acc:0.977]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6283     0     0     0     0     0     1   145     3]
 [    0     0 18003     0    27     0    59     0     1     0]
 [    0    14     0  1842     0     0     0     0   180     0]
 [    0    12     7     4  2926     0    15     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    27     0    14     0  4836     0     1     0]
 [    0    10     0     0     0     0     0  1280     0     0]
 [    0    62     0    63    19     0    15     0  3412     0]
 [    0     0     0     0     6    11     0     0     0   902]]

Accuracy:
98.30332827223869

F1 scores:
[       nan 0.9807227  0.9966507  0.9338403  0.98122066 0.99580313
 0.98663674 0.99572151 0.93287765 0.98741106]

Kappa:
0.9775292511853614
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a1d21cb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.191, val_acc:0.138]
Epoch [2/120    avg_loss:1.940, val_acc:0.131]
Epoch [3/120    avg_loss:1.756, val_acc:0.155]
Epoch [4/120    avg_loss:1.610, val_acc:0.178]
Epoch [5/120    avg_loss:1.460, val_acc:0.218]
Epoch [6/120    avg_loss:1.388, val_acc:0.230]
Epoch [7/120    avg_loss:1.294, val_acc:0.242]
Epoch [8/120    avg_loss:1.203, val_acc:0.326]
Epoch [9/120    avg_loss:1.092, val_acc:0.433]
Epoch [10/120    avg_loss:0.969, val_acc:0.603]
Epoch [11/120    avg_loss:0.877, val_acc:0.558]
Epoch [12/120    avg_loss:0.820, val_acc:0.691]
Epoch [13/120    avg_loss:0.712, val_acc:0.710]
Epoch [14/120    avg_loss:0.619, val_acc:0.727]
Epoch [15/120    avg_loss:0.559, val_acc:0.773]
Epoch [16/120    avg_loss:0.493, val_acc:0.734]
Epoch [17/120    avg_loss:0.457, val_acc:0.793]
Epoch [18/120    avg_loss:0.405, val_acc:0.803]
Epoch [19/120    avg_loss:0.360, val_acc:0.808]
Epoch [20/120    avg_loss:0.342, val_acc:0.821]
Epoch [21/120    avg_loss:0.307, val_acc:0.840]
Epoch [22/120    avg_loss:0.300, val_acc:0.810]
Epoch [23/120    avg_loss:0.250, val_acc:0.903]
Epoch [24/120    avg_loss:0.242, val_acc:0.926]
Epoch [25/120    avg_loss:0.237, val_acc:0.931]
Epoch [26/120    avg_loss:0.221, val_acc:0.883]
Epoch [27/120    avg_loss:0.203, val_acc:0.934]
Epoch [28/120    avg_loss:0.180, val_acc:0.926]
Epoch [29/120    avg_loss:0.188, val_acc:0.938]
Epoch [30/120    avg_loss:0.208, val_acc:0.949]
Epoch [31/120    avg_loss:0.169, val_acc:0.944]
Epoch [32/120    avg_loss:0.129, val_acc:0.947]
Epoch [33/120    avg_loss:0.133, val_acc:0.931]
Epoch [34/120    avg_loss:0.115, val_acc:0.948]
Epoch [35/120    avg_loss:0.128, val_acc:0.964]
Epoch [36/120    avg_loss:0.106, val_acc:0.961]
Epoch [37/120    avg_loss:0.096, val_acc:0.964]
Epoch [38/120    avg_loss:0.084, val_acc:0.970]
Epoch [39/120    avg_loss:0.075, val_acc:0.958]
Epoch [40/120    avg_loss:0.087, val_acc:0.953]
Epoch [41/120    avg_loss:0.096, val_acc:0.935]
Epoch [42/120    avg_loss:0.084, val_acc:0.941]
Epoch [43/120    avg_loss:0.091, val_acc:0.948]
Epoch [44/120    avg_loss:0.059, val_acc:0.968]
Epoch [45/120    avg_loss:0.057, val_acc:0.970]
Epoch [46/120    avg_loss:0.049, val_acc:0.964]
Epoch [47/120    avg_loss:0.055, val_acc:0.969]
Epoch [48/120    avg_loss:0.051, val_acc:0.976]
Epoch [49/120    avg_loss:0.048, val_acc:0.969]
Epoch [50/120    avg_loss:0.039, val_acc:0.974]
Epoch [51/120    avg_loss:0.036, val_acc:0.977]
Epoch [52/120    avg_loss:0.130, val_acc:0.951]
Epoch [53/120    avg_loss:0.071, val_acc:0.967]
Epoch [54/120    avg_loss:0.044, val_acc:0.952]
Epoch [55/120    avg_loss:0.050, val_acc:0.978]
Epoch [56/120    avg_loss:0.036, val_acc:0.979]
Epoch [57/120    avg_loss:0.030, val_acc:0.982]
Epoch [58/120    avg_loss:0.030, val_acc:0.973]
Epoch [59/120    avg_loss:0.026, val_acc:0.983]
Epoch [60/120    avg_loss:0.026, val_acc:0.969]
Epoch [61/120    avg_loss:0.024, val_acc:0.982]
Epoch [62/120    avg_loss:0.018, val_acc:0.984]
Epoch [63/120    avg_loss:0.018, val_acc:0.983]
Epoch [64/120    avg_loss:0.019, val_acc:0.985]
Epoch [65/120    avg_loss:0.019, val_acc:0.979]
Epoch [66/120    avg_loss:0.039, val_acc:0.970]
Epoch [67/120    avg_loss:0.060, val_acc:0.955]
Epoch [68/120    avg_loss:0.050, val_acc:0.974]
Epoch [69/120    avg_loss:0.038, val_acc:0.972]
Epoch [70/120    avg_loss:0.050, val_acc:0.979]
Epoch [71/120    avg_loss:0.039, val_acc:0.973]
Epoch [72/120    avg_loss:0.057, val_acc:0.962]
Epoch [73/120    avg_loss:0.037, val_acc:0.965]
Epoch [74/120    avg_loss:0.022, val_acc:0.977]
Epoch [75/120    avg_loss:0.021, val_acc:0.981]
Epoch [76/120    avg_loss:0.017, val_acc:0.988]
Epoch [77/120    avg_loss:0.014, val_acc:0.982]
Epoch [78/120    avg_loss:0.037, val_acc:0.974]
Epoch [79/120    avg_loss:0.022, val_acc:0.985]
Epoch [80/120    avg_loss:0.017, val_acc:0.984]
Epoch [81/120    avg_loss:0.017, val_acc:0.989]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.015, val_acc:0.984]
Epoch [86/120    avg_loss:0.016, val_acc:0.978]
Epoch [87/120    avg_loss:0.038, val_acc:0.975]
Epoch [88/120    avg_loss:0.039, val_acc:0.985]
Epoch [89/120    avg_loss:0.041, val_acc:0.963]
Epoch [90/120    avg_loss:0.037, val_acc:0.975]
Epoch [91/120    avg_loss:0.018, val_acc:0.983]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.013, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.025, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.008, val_acc:0.989]
Epoch [105/120    avg_loss:0.007, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6386     0     0     0     0     1     1    36     8]
 [    0     0 17968     0    59     0    57     0     6     0]
 [    0     6     0  1942     0     0     0     0    88     0]
 [    0    21    15     0  2899     0    26     0    10     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    43     0     0     0  4835     0     0     0]
 [    0    23     2     0     0     0     0  1265     0     0]
 [    0    12     0    50    41     0     0     0  3468     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
98.75641674499313

F1 scores:
[       nan 0.99161491 0.99496096 0.96425025 0.97102663 0.99618321
 0.98703685 0.98982786 0.96615127 0.98965705]

Kappa:
0.9835308298731511
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3da391db70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.209, val_acc:0.082]
Epoch [2/120    avg_loss:1.992, val_acc:0.123]
Epoch [3/120    avg_loss:1.776, val_acc:0.232]
Epoch [4/120    avg_loss:1.593, val_acc:0.390]
Epoch [5/120    avg_loss:1.419, val_acc:0.466]
Epoch [6/120    avg_loss:1.279, val_acc:0.431]
Epoch [7/120    avg_loss:1.181, val_acc:0.537]
Epoch [8/120    avg_loss:1.011, val_acc:0.558]
Epoch [9/120    avg_loss:0.903, val_acc:0.623]
Epoch [10/120    avg_loss:0.771, val_acc:0.669]
Epoch [11/120    avg_loss:0.678, val_acc:0.652]
Epoch [12/120    avg_loss:0.620, val_acc:0.668]
Epoch [13/120    avg_loss:0.537, val_acc:0.718]
Epoch [14/120    avg_loss:0.481, val_acc:0.713]
Epoch [15/120    avg_loss:0.433, val_acc:0.753]
Epoch [16/120    avg_loss:0.389, val_acc:0.769]
Epoch [17/120    avg_loss:0.359, val_acc:0.806]
Epoch [18/120    avg_loss:0.333, val_acc:0.811]
Epoch [19/120    avg_loss:0.320, val_acc:0.859]
Epoch [20/120    avg_loss:0.335, val_acc:0.824]
Epoch [21/120    avg_loss:0.304, val_acc:0.847]
Epoch [22/120    avg_loss:0.272, val_acc:0.812]
Epoch [23/120    avg_loss:0.234, val_acc:0.887]
Epoch [24/120    avg_loss:0.203, val_acc:0.929]
Epoch [25/120    avg_loss:0.243, val_acc:0.905]
Epoch [26/120    avg_loss:0.190, val_acc:0.933]
Epoch [27/120    avg_loss:0.168, val_acc:0.942]
Epoch [28/120    avg_loss:0.159, val_acc:0.925]
Epoch [29/120    avg_loss:0.168, val_acc:0.948]
Epoch [30/120    avg_loss:0.135, val_acc:0.922]
Epoch [31/120    avg_loss:0.096, val_acc:0.947]
Epoch [32/120    avg_loss:0.110, val_acc:0.943]
Epoch [33/120    avg_loss:0.099, val_acc:0.944]
Epoch [34/120    avg_loss:0.101, val_acc:0.936]
Epoch [35/120    avg_loss:0.114, val_acc:0.955]
Epoch [36/120    avg_loss:0.104, val_acc:0.944]
Epoch [37/120    avg_loss:0.088, val_acc:0.965]
Epoch [38/120    avg_loss:0.097, val_acc:0.962]
Epoch [39/120    avg_loss:0.090, val_acc:0.961]
Epoch [40/120    avg_loss:0.092, val_acc:0.956]
Epoch [41/120    avg_loss:0.072, val_acc:0.948]
Epoch [42/120    avg_loss:0.063, val_acc:0.971]
Epoch [43/120    avg_loss:0.047, val_acc:0.973]
Epoch [44/120    avg_loss:0.044, val_acc:0.973]
Epoch [45/120    avg_loss:0.058, val_acc:0.955]
Epoch [46/120    avg_loss:0.087, val_acc:0.952]
Epoch [47/120    avg_loss:0.064, val_acc:0.970]
Epoch [48/120    avg_loss:0.065, val_acc:0.970]
Epoch [49/120    avg_loss:0.039, val_acc:0.968]
Epoch [50/120    avg_loss:0.037, val_acc:0.975]
Epoch [51/120    avg_loss:0.034, val_acc:0.977]
Epoch [52/120    avg_loss:0.035, val_acc:0.970]
Epoch [53/120    avg_loss:0.036, val_acc:0.975]
Epoch [54/120    avg_loss:0.027, val_acc:0.972]
Epoch [55/120    avg_loss:0.024, val_acc:0.975]
Epoch [56/120    avg_loss:0.022, val_acc:0.970]
Epoch [57/120    avg_loss:0.031, val_acc:0.965]
Epoch [58/120    avg_loss:0.039, val_acc:0.961]
Epoch [59/120    avg_loss:0.040, val_acc:0.962]
Epoch [60/120    avg_loss:0.043, val_acc:0.973]
Epoch [61/120    avg_loss:0.063, val_acc:0.956]
Epoch [62/120    avg_loss:0.070, val_acc:0.951]
Epoch [63/120    avg_loss:0.046, val_acc:0.958]
Epoch [64/120    avg_loss:0.032, val_acc:0.973]
Epoch [65/120    avg_loss:0.028, val_acc:0.977]
Epoch [66/120    avg_loss:0.023, val_acc:0.976]
Epoch [67/120    avg_loss:0.017, val_acc:0.977]
Epoch [68/120    avg_loss:0.016, val_acc:0.975]
Epoch [69/120    avg_loss:0.016, val_acc:0.975]
Epoch [70/120    avg_loss:0.019, val_acc:0.976]
Epoch [71/120    avg_loss:0.015, val_acc:0.975]
Epoch [72/120    avg_loss:0.017, val_acc:0.978]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.019, val_acc:0.977]
Epoch [76/120    avg_loss:0.016, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.980]
Epoch [78/120    avg_loss:0.015, val_acc:0.980]
Epoch [79/120    avg_loss:0.016, val_acc:0.980]
Epoch [80/120    avg_loss:0.014, val_acc:0.982]
Epoch [81/120    avg_loss:0.017, val_acc:0.980]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.015, val_acc:0.981]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.020, val_acc:0.981]
Epoch [88/120    avg_loss:0.013, val_acc:0.982]
Epoch [89/120    avg_loss:0.013, val_acc:0.979]
Epoch [90/120    avg_loss:0.018, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.979]
Epoch [92/120    avg_loss:0.017, val_acc:0.981]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.012, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.981]
Epoch [96/120    avg_loss:0.014, val_acc:0.981]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.012, val_acc:0.982]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.014, val_acc:0.982]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.014, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.982]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     2     0     7     0     0     0    53     7]
 [    0     0 17964     0    27     0    92     0     7     0]
 [    0     6     0  1886     0     0     0     0   144     0]
 [    0    23    16     0  2905     0    17     0     9     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     1     0  4859     0     5     0]
 [    0     7     0     0     0     0     0  1279     0     4]
 [    0    26     0    27    41     0     0     0  3477     0]
 [    0     0     0     0     4     7     0     0     0   908]]

Accuracy:
98.68170534789

F1 scores:
[       nan 0.989811   0.99564916 0.95517853 0.97532315 0.99732518
 0.9869998  0.99571818 0.95706028 0.98695652]

Kappa:
0.9825465321392582
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29c2d95b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.213, val_acc:0.197]
Epoch [2/120    avg_loss:1.988, val_acc:0.211]
Epoch [3/120    avg_loss:1.809, val_acc:0.224]
Epoch [4/120    avg_loss:1.649, val_acc:0.238]
Epoch [5/120    avg_loss:1.503, val_acc:0.298]
Epoch [6/120    avg_loss:1.338, val_acc:0.342]
Epoch [7/120    avg_loss:1.224, val_acc:0.438]
Epoch [8/120    avg_loss:1.059, val_acc:0.471]
Epoch [9/120    avg_loss:0.900, val_acc:0.620]
Epoch [10/120    avg_loss:0.782, val_acc:0.671]
Epoch [11/120    avg_loss:0.658, val_acc:0.634]
Epoch [12/120    avg_loss:0.585, val_acc:0.715]
Epoch [13/120    avg_loss:0.504, val_acc:0.751]
Epoch [14/120    avg_loss:0.482, val_acc:0.703]
Epoch [15/120    avg_loss:0.426, val_acc:0.754]
Epoch [16/120    avg_loss:0.384, val_acc:0.738]
Epoch [17/120    avg_loss:0.383, val_acc:0.749]
Epoch [18/120    avg_loss:0.374, val_acc:0.767]
Epoch [19/120    avg_loss:0.317, val_acc:0.789]
Epoch [20/120    avg_loss:0.292, val_acc:0.791]
Epoch [21/120    avg_loss:0.274, val_acc:0.803]
Epoch [22/120    avg_loss:0.263, val_acc:0.816]
Epoch [23/120    avg_loss:0.266, val_acc:0.851]
Epoch [24/120    avg_loss:0.261, val_acc:0.877]
Epoch [25/120    avg_loss:0.218, val_acc:0.882]
Epoch [26/120    avg_loss:0.194, val_acc:0.906]
Epoch [27/120    avg_loss:0.204, val_acc:0.910]
Epoch [28/120    avg_loss:0.166, val_acc:0.924]
Epoch [29/120    avg_loss:0.146, val_acc:0.914]
Epoch [30/120    avg_loss:0.149, val_acc:0.920]
Epoch [31/120    avg_loss:0.151, val_acc:0.942]
Epoch [32/120    avg_loss:0.147, val_acc:0.934]
Epoch [33/120    avg_loss:0.116, val_acc:0.956]
Epoch [34/120    avg_loss:0.104, val_acc:0.958]
Epoch [35/120    avg_loss:0.166, val_acc:0.947]
Epoch [36/120    avg_loss:0.100, val_acc:0.947]
Epoch [37/120    avg_loss:0.127, val_acc:0.953]
Epoch [38/120    avg_loss:0.084, val_acc:0.961]
Epoch [39/120    avg_loss:0.079, val_acc:0.955]
Epoch [40/120    avg_loss:0.086, val_acc:0.945]
Epoch [41/120    avg_loss:0.086, val_acc:0.892]
Epoch [42/120    avg_loss:0.063, val_acc:0.965]
Epoch [43/120    avg_loss:0.051, val_acc:0.965]
Epoch [44/120    avg_loss:0.058, val_acc:0.970]
Epoch [45/120    avg_loss:0.055, val_acc:0.969]
Epoch [46/120    avg_loss:0.081, val_acc:0.962]
Epoch [47/120    avg_loss:0.065, val_acc:0.975]
Epoch [48/120    avg_loss:0.049, val_acc:0.973]
Epoch [49/120    avg_loss:0.040, val_acc:0.911]
Epoch [50/120    avg_loss:0.043, val_acc:0.961]
Epoch [51/120    avg_loss:0.047, val_acc:0.970]
Epoch [52/120    avg_loss:0.070, val_acc:0.962]
Epoch [53/120    avg_loss:0.077, val_acc:0.957]
Epoch [54/120    avg_loss:0.138, val_acc:0.919]
Epoch [55/120    avg_loss:0.077, val_acc:0.960]
Epoch [56/120    avg_loss:0.053, val_acc:0.970]
Epoch [57/120    avg_loss:0.054, val_acc:0.962]
Epoch [58/120    avg_loss:0.045, val_acc:0.969]
Epoch [59/120    avg_loss:0.049, val_acc:0.971]
Epoch [60/120    avg_loss:0.033, val_acc:0.951]
Epoch [61/120    avg_loss:0.040, val_acc:0.972]
Epoch [62/120    avg_loss:0.026, val_acc:0.972]
Epoch [63/120    avg_loss:0.025, val_acc:0.975]
Epoch [64/120    avg_loss:0.022, val_acc:0.976]
Epoch [65/120    avg_loss:0.023, val_acc:0.975]
Epoch [66/120    avg_loss:0.021, val_acc:0.975]
Epoch [67/120    avg_loss:0.021, val_acc:0.975]
Epoch [68/120    avg_loss:0.021, val_acc:0.974]
Epoch [69/120    avg_loss:0.024, val_acc:0.975]
Epoch [70/120    avg_loss:0.024, val_acc:0.975]
Epoch [71/120    avg_loss:0.023, val_acc:0.976]
Epoch [72/120    avg_loss:0.020, val_acc:0.974]
Epoch [73/120    avg_loss:0.017, val_acc:0.975]
Epoch [74/120    avg_loss:0.018, val_acc:0.975]
Epoch [75/120    avg_loss:0.024, val_acc:0.976]
Epoch [76/120    avg_loss:0.021, val_acc:0.979]
Epoch [77/120    avg_loss:0.018, val_acc:0.976]
Epoch [78/120    avg_loss:0.018, val_acc:0.975]
Epoch [79/120    avg_loss:0.016, val_acc:0.975]
Epoch [80/120    avg_loss:0.017, val_acc:0.975]
Epoch [81/120    avg_loss:0.022, val_acc:0.976]
Epoch [82/120    avg_loss:0.016, val_acc:0.975]
Epoch [83/120    avg_loss:0.019, val_acc:0.975]
Epoch [84/120    avg_loss:0.016, val_acc:0.975]
Epoch [85/120    avg_loss:0.014, val_acc:0.974]
Epoch [86/120    avg_loss:0.018, val_acc:0.974]
Epoch [87/120    avg_loss:0.016, val_acc:0.977]
Epoch [88/120    avg_loss:0.014, val_acc:0.976]
Epoch [89/120    avg_loss:0.015, val_acc:0.976]
Epoch [90/120    avg_loss:0.015, val_acc:0.977]
Epoch [91/120    avg_loss:0.016, val_acc:0.977]
Epoch [92/120    avg_loss:0.016, val_acc:0.977]
Epoch [93/120    avg_loss:0.017, val_acc:0.977]
Epoch [94/120    avg_loss:0.016, val_acc:0.978]
Epoch [95/120    avg_loss:0.022, val_acc:0.979]
Epoch [96/120    avg_loss:0.016, val_acc:0.979]
Epoch [97/120    avg_loss:0.016, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.979]
Epoch [99/120    avg_loss:0.015, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.979]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.016, val_acc:0.979]
Epoch [103/120    avg_loss:0.015, val_acc:0.979]
Epoch [104/120    avg_loss:0.017, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.979]
Epoch [106/120    avg_loss:0.015, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.016, val_acc:0.979]
Epoch [109/120    avg_loss:0.015, val_acc:0.979]
Epoch [110/120    avg_loss:0.018, val_acc:0.979]
Epoch [111/120    avg_loss:0.016, val_acc:0.979]
Epoch [112/120    avg_loss:0.015, val_acc:0.979]
Epoch [113/120    avg_loss:0.016, val_acc:0.979]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.015, val_acc:0.979]
Epoch [116/120    avg_loss:0.016, val_acc:0.979]
Epoch [117/120    avg_loss:0.015, val_acc:0.979]
Epoch [118/120    avg_loss:0.017, val_acc:0.979]
Epoch [119/120    avg_loss:0.013, val_acc:0.979]
Epoch [120/120    avg_loss:0.023, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6343     0     0     0     0     8     6    71     4]
 [    0     0 18029     0     8     0    50     0     3     0]
 [    0     5     0  1880     0     0     0     0   151     0]
 [    0    13    11     0  2930     0    12     0     1     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    57     0     4     0  4815     0     2     0]
 [    0     1     0     0     0     0     0  1288     1     0]
 [    0    26     6    39    28     0     8     0  3464     0]
 [    0     1     0     0     0     7     0     0     0   911]]

Accuracy:
98.72749620417902

F1 scores:
[       nan 0.9894704  0.99627    0.95069532 0.98619993 0.99732518
 0.98556954 0.99690402 0.95374449 0.99075585]

Kappa:
0.9831354606390554
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f25e5900b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.117, val_acc:0.090]
Epoch [2/120    avg_loss:1.858, val_acc:0.130]
Epoch [3/120    avg_loss:1.689, val_acc:0.132]
Epoch [4/120    avg_loss:1.547, val_acc:0.215]
Epoch [5/120    avg_loss:1.423, val_acc:0.349]
Epoch [6/120    avg_loss:1.279, val_acc:0.434]
Epoch [7/120    avg_loss:1.137, val_acc:0.595]
Epoch [8/120    avg_loss:1.006, val_acc:0.538]
Epoch [9/120    avg_loss:0.891, val_acc:0.578]
Epoch [10/120    avg_loss:0.764, val_acc:0.673]
Epoch [11/120    avg_loss:0.670, val_acc:0.679]
Epoch [12/120    avg_loss:0.621, val_acc:0.747]
Epoch [13/120    avg_loss:0.547, val_acc:0.789]
Epoch [14/120    avg_loss:0.501, val_acc:0.770]
Epoch [15/120    avg_loss:0.512, val_acc:0.788]
Epoch [16/120    avg_loss:0.475, val_acc:0.766]
Epoch [17/120    avg_loss:0.406, val_acc:0.718]
Epoch [18/120    avg_loss:0.418, val_acc:0.761]
Epoch [19/120    avg_loss:0.358, val_acc:0.815]
Epoch [20/120    avg_loss:0.333, val_acc:0.779]
Epoch [21/120    avg_loss:0.321, val_acc:0.803]
Epoch [22/120    avg_loss:0.329, val_acc:0.825]
Epoch [23/120    avg_loss:0.322, val_acc:0.826]
Epoch [24/120    avg_loss:0.284, val_acc:0.808]
Epoch [25/120    avg_loss:0.254, val_acc:0.817]
Epoch [26/120    avg_loss:0.277, val_acc:0.798]
Epoch [27/120    avg_loss:0.290, val_acc:0.812]
Epoch [28/120    avg_loss:0.303, val_acc:0.749]
Epoch [29/120    avg_loss:0.255, val_acc:0.828]
Epoch [30/120    avg_loss:0.215, val_acc:0.873]
Epoch [31/120    avg_loss:0.208, val_acc:0.840]
Epoch [32/120    avg_loss:0.212, val_acc:0.840]
Epoch [33/120    avg_loss:0.196, val_acc:0.802]
Epoch [34/120    avg_loss:0.191, val_acc:0.922]
Epoch [35/120    avg_loss:0.181, val_acc:0.873]
Epoch [36/120    avg_loss:0.226, val_acc:0.889]
Epoch [37/120    avg_loss:0.214, val_acc:0.902]
Epoch [38/120    avg_loss:0.169, val_acc:0.861]
Epoch [39/120    avg_loss:0.123, val_acc:0.934]
Epoch [40/120    avg_loss:0.110, val_acc:0.897]
Epoch [41/120    avg_loss:0.126, val_acc:0.951]
Epoch [42/120    avg_loss:0.107, val_acc:0.961]
Epoch [43/120    avg_loss:0.099, val_acc:0.946]
Epoch [44/120    avg_loss:0.097, val_acc:0.951]
Epoch [45/120    avg_loss:0.086, val_acc:0.960]
Epoch [46/120    avg_loss:0.099, val_acc:0.939]
Epoch [47/120    avg_loss:0.085, val_acc:0.963]
Epoch [48/120    avg_loss:0.061, val_acc:0.942]
Epoch [49/120    avg_loss:0.076, val_acc:0.924]
Epoch [50/120    avg_loss:0.104, val_acc:0.916]
Epoch [51/120    avg_loss:0.085, val_acc:0.969]
Epoch [52/120    avg_loss:0.059, val_acc:0.954]
Epoch [53/120    avg_loss:0.054, val_acc:0.965]
Epoch [54/120    avg_loss:0.060, val_acc:0.953]
Epoch [55/120    avg_loss:0.055, val_acc:0.970]
Epoch [56/120    avg_loss:0.038, val_acc:0.970]
Epoch [57/120    avg_loss:0.048, val_acc:0.969]
Epoch [58/120    avg_loss:0.044, val_acc:0.952]
Epoch [59/120    avg_loss:0.047, val_acc:0.952]
Epoch [60/120    avg_loss:0.061, val_acc:0.954]
Epoch [61/120    avg_loss:0.059, val_acc:0.967]
Epoch [62/120    avg_loss:0.080, val_acc:0.966]
Epoch [63/120    avg_loss:0.051, val_acc:0.971]
Epoch [64/120    avg_loss:0.037, val_acc:0.981]
Epoch [65/120    avg_loss:0.029, val_acc:0.947]
Epoch [66/120    avg_loss:0.041, val_acc:0.963]
Epoch [67/120    avg_loss:0.034, val_acc:0.953]
Epoch [68/120    avg_loss:0.033, val_acc:0.966]
Epoch [69/120    avg_loss:0.034, val_acc:0.953]
Epoch [70/120    avg_loss:0.036, val_acc:0.965]
Epoch [71/120    avg_loss:0.025, val_acc:0.979]
Epoch [72/120    avg_loss:0.022, val_acc:0.977]
Epoch [73/120    avg_loss:0.020, val_acc:0.966]
Epoch [74/120    avg_loss:0.033, val_acc:0.960]
Epoch [75/120    avg_loss:0.027, val_acc:0.975]
Epoch [76/120    avg_loss:0.024, val_acc:0.970]
Epoch [77/120    avg_loss:0.022, val_acc:0.965]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.012, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.015, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.980]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.984]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.011, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     0     0     2     2    28     5]
 [    0     1 18013     0    43     0    30     0     3     0]
 [    0     3     0  1883     0     0     0     0   149     1]
 [    0    39     4     0  2911     0    10     0     3     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     5     0  4837     0    12     0]
 [    0    16     0     0     0     0     0  1272     2     0]
 [    0    16     0    41    24     0    16     0  3474     0]
 [    0     0     0     0    12    14     0     0     0   893]]

Accuracy:
98.77087701540019

F1 scores:
[       nan 0.99131918 0.99709391 0.9510101  0.97569968 0.99466463
 0.98987005 0.99219969 0.95940348 0.97970378]

Kappa:
0.9837181122667289
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f74e76b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.123, val_acc:0.121]
Epoch [2/120    avg_loss:1.864, val_acc:0.110]
Epoch [3/120    avg_loss:1.650, val_acc:0.215]
Epoch [4/120    avg_loss:1.483, val_acc:0.495]
Epoch [5/120    avg_loss:1.371, val_acc:0.611]
Epoch [6/120    avg_loss:1.259, val_acc:0.669]
Epoch [7/120    avg_loss:1.144, val_acc:0.640]
Epoch [8/120    avg_loss:1.055, val_acc:0.679]
Epoch [9/120    avg_loss:0.949, val_acc:0.700]
Epoch [10/120    avg_loss:0.813, val_acc:0.750]
Epoch [11/120    avg_loss:0.703, val_acc:0.750]
Epoch [12/120    avg_loss:0.633, val_acc:0.757]
Epoch [13/120    avg_loss:0.536, val_acc:0.764]
Epoch [14/120    avg_loss:0.480, val_acc:0.762]
Epoch [15/120    avg_loss:0.457, val_acc:0.737]
Epoch [16/120    avg_loss:0.393, val_acc:0.727]
Epoch [17/120    avg_loss:0.368, val_acc:0.784]
Epoch [18/120    avg_loss:0.380, val_acc:0.802]
Epoch [19/120    avg_loss:0.366, val_acc:0.800]
Epoch [20/120    avg_loss:0.352, val_acc:0.748]
Epoch [21/120    avg_loss:0.333, val_acc:0.822]
Epoch [22/120    avg_loss:0.304, val_acc:0.815]
Epoch [23/120    avg_loss:0.300, val_acc:0.873]
Epoch [24/120    avg_loss:0.266, val_acc:0.840]
Epoch [25/120    avg_loss:0.242, val_acc:0.892]
Epoch [26/120    avg_loss:0.190, val_acc:0.906]
Epoch [27/120    avg_loss:0.253, val_acc:0.908]
Epoch [28/120    avg_loss:0.225, val_acc:0.896]
Epoch [29/120    avg_loss:0.200, val_acc:0.934]
Epoch [30/120    avg_loss:0.190, val_acc:0.917]
Epoch [31/120    avg_loss:0.202, val_acc:0.922]
Epoch [32/120    avg_loss:0.207, val_acc:0.939]
Epoch [33/120    avg_loss:0.149, val_acc:0.941]
Epoch [34/120    avg_loss:0.131, val_acc:0.913]
Epoch [35/120    avg_loss:0.127, val_acc:0.951]
Epoch [36/120    avg_loss:0.111, val_acc:0.937]
Epoch [37/120    avg_loss:0.109, val_acc:0.955]
Epoch [38/120    avg_loss:0.083, val_acc:0.954]
Epoch [39/120    avg_loss:0.084, val_acc:0.954]
Epoch [40/120    avg_loss:0.107, val_acc:0.951]
Epoch [41/120    avg_loss:0.096, val_acc:0.918]
Epoch [42/120    avg_loss:0.101, val_acc:0.947]
Epoch [43/120    avg_loss:0.090, val_acc:0.960]
Epoch [44/120    avg_loss:0.073, val_acc:0.961]
Epoch [45/120    avg_loss:0.062, val_acc:0.967]
Epoch [46/120    avg_loss:0.054, val_acc:0.965]
Epoch [47/120    avg_loss:0.048, val_acc:0.970]
Epoch [48/120    avg_loss:0.068, val_acc:0.968]
Epoch [49/120    avg_loss:0.487, val_acc:0.526]
Epoch [50/120    avg_loss:1.192, val_acc:0.604]
Epoch [51/120    avg_loss:1.040, val_acc:0.613]
Epoch [52/120    avg_loss:0.980, val_acc:0.625]
Epoch [53/120    avg_loss:1.009, val_acc:0.632]
Epoch [54/120    avg_loss:0.959, val_acc:0.645]
Epoch [55/120    avg_loss:0.915, val_acc:0.597]
Epoch [56/120    avg_loss:0.948, val_acc:0.586]
Epoch [57/120    avg_loss:0.869, val_acc:0.583]
Epoch [58/120    avg_loss:0.854, val_acc:0.567]
Epoch [59/120    avg_loss:0.849, val_acc:0.550]
Epoch [60/120    avg_loss:0.819, val_acc:0.547]
Epoch [61/120    avg_loss:0.826, val_acc:0.592]
Epoch [62/120    avg_loss:0.791, val_acc:0.586]
Epoch [63/120    avg_loss:0.790, val_acc:0.582]
Epoch [64/120    avg_loss:0.766, val_acc:0.596]
Epoch [65/120    avg_loss:0.787, val_acc:0.595]
Epoch [66/120    avg_loss:0.804, val_acc:0.598]
Epoch [67/120    avg_loss:0.807, val_acc:0.599]
Epoch [68/120    avg_loss:0.785, val_acc:0.595]
Epoch [69/120    avg_loss:0.780, val_acc:0.594]
Epoch [70/120    avg_loss:0.769, val_acc:0.596]
Epoch [71/120    avg_loss:0.769, val_acc:0.595]
Epoch [72/120    avg_loss:0.776, val_acc:0.595]
Epoch [73/120    avg_loss:0.786, val_acc:0.598]
Epoch [74/120    avg_loss:0.807, val_acc:0.598]
Epoch [75/120    avg_loss:0.769, val_acc:0.597]
Epoch [76/120    avg_loss:0.806, val_acc:0.598]
Epoch [77/120    avg_loss:0.756, val_acc:0.595]
Epoch [78/120    avg_loss:0.772, val_acc:0.595]
Epoch [79/120    avg_loss:0.763, val_acc:0.595]
Epoch [80/120    avg_loss:0.749, val_acc:0.595]
Epoch [81/120    avg_loss:0.776, val_acc:0.595]
Epoch [82/120    avg_loss:0.782, val_acc:0.595]
Epoch [83/120    avg_loss:0.775, val_acc:0.595]
Epoch [84/120    avg_loss:0.773, val_acc:0.597]
Epoch [85/120    avg_loss:0.798, val_acc:0.600]
Epoch [86/120    avg_loss:0.752, val_acc:0.598]
Epoch [87/120    avg_loss:0.781, val_acc:0.598]
Epoch [88/120    avg_loss:0.818, val_acc:0.599]
Epoch [89/120    avg_loss:0.778, val_acc:0.599]
Epoch [90/120    avg_loss:0.793, val_acc:0.599]
Epoch [91/120    avg_loss:0.761, val_acc:0.599]
Epoch [92/120    avg_loss:0.785, val_acc:0.599]
Epoch [93/120    avg_loss:0.778, val_acc:0.599]
Epoch [94/120    avg_loss:0.761, val_acc:0.599]
Epoch [95/120    avg_loss:0.766, val_acc:0.599]
Epoch [96/120    avg_loss:0.759, val_acc:0.599]
Epoch [97/120    avg_loss:0.789, val_acc:0.599]
Epoch [98/120    avg_loss:0.760, val_acc:0.599]
Epoch [99/120    avg_loss:0.764, val_acc:0.599]
Epoch [100/120    avg_loss:0.778, val_acc:0.599]
Epoch [101/120    avg_loss:0.775, val_acc:0.599]
Epoch [102/120    avg_loss:0.780, val_acc:0.599]
Epoch [103/120    avg_loss:0.779, val_acc:0.599]
Epoch [104/120    avg_loss:0.785, val_acc:0.599]
Epoch [105/120    avg_loss:0.784, val_acc:0.599]
Epoch [106/120    avg_loss:0.788, val_acc:0.599]
Epoch [107/120    avg_loss:0.782, val_acc:0.598]
Epoch [108/120    avg_loss:0.792, val_acc:0.599]
Epoch [109/120    avg_loss:0.793, val_acc:0.599]
Epoch [110/120    avg_loss:0.772, val_acc:0.599]
Epoch [111/120    avg_loss:0.774, val_acc:0.599]
Epoch [112/120    avg_loss:0.777, val_acc:0.599]
Epoch [113/120    avg_loss:0.765, val_acc:0.599]
Epoch [114/120    avg_loss:0.773, val_acc:0.599]
Epoch [115/120    avg_loss:0.763, val_acc:0.599]
Epoch [116/120    avg_loss:0.747, val_acc:0.599]
Epoch [117/120    avg_loss:0.784, val_acc:0.599]
Epoch [118/120    avg_loss:0.763, val_acc:0.599]
Epoch [119/120    avg_loss:0.774, val_acc:0.599]
Epoch [120/120    avg_loss:0.790, val_acc:0.599]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0]
 [   0 3915  410    1   26    5 1471   31  422  151]
 [   0  736 8460    0 1689    0 7202    0    3    0]
 [   0   50    9 1587   49    0   55    0  270   16]
 [   0   27  193    0 2342    0  339    0   71    0]
 [   0    4    0    0    0 1300    0    1    0    0]
 [   0    9  393  244  292    0 3864    0   76    0]
 [   0  105    7    0   28    0    1 1124   25    0]
 [   0  205   94  192  242    0  475    0 2363    0]
 [   0    5    0    0   14   54   10    0    0  836]]

Accuracy:
62.157472344732845

F1 scores:
[       nan 0.68158078 0.61180214 0.7817734  0.6119676  0.97597598
 0.42241049 0.91905151 0.69489781 0.86992716]

Kappa:
0.5401755291850975
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5079ae8b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.015, val_acc:0.118]
Epoch [2/120    avg_loss:1.817, val_acc:0.164]
Epoch [3/120    avg_loss:1.628, val_acc:0.174]
Epoch [4/120    avg_loss:1.427, val_acc:0.206]
Epoch [5/120    avg_loss:1.343, val_acc:0.294]
Epoch [6/120    avg_loss:1.236, val_acc:0.432]
Epoch [7/120    avg_loss:1.146, val_acc:0.480]
Epoch [8/120    avg_loss:1.051, val_acc:0.533]
Epoch [9/120    avg_loss:0.923, val_acc:0.516]
Epoch [10/120    avg_loss:0.821, val_acc:0.586]
Epoch [11/120    avg_loss:0.701, val_acc:0.625]
Epoch [12/120    avg_loss:0.591, val_acc:0.748]
Epoch [13/120    avg_loss:0.529, val_acc:0.780]
Epoch [14/120    avg_loss:0.455, val_acc:0.728]
Epoch [15/120    avg_loss:0.418, val_acc:0.824]
Epoch [16/120    avg_loss:0.370, val_acc:0.833]
Epoch [17/120    avg_loss:0.342, val_acc:0.854]
Epoch [18/120    avg_loss:0.289, val_acc:0.864]
Epoch [19/120    avg_loss:0.259, val_acc:0.900]
Epoch [20/120    avg_loss:0.251, val_acc:0.855]
Epoch [21/120    avg_loss:0.227, val_acc:0.891]
Epoch [22/120    avg_loss:0.189, val_acc:0.922]
Epoch [23/120    avg_loss:0.212, val_acc:0.911]
Epoch [24/120    avg_loss:0.186, val_acc:0.910]
Epoch [25/120    avg_loss:0.189, val_acc:0.911]
Epoch [26/120    avg_loss:0.218, val_acc:0.897]
Epoch [27/120    avg_loss:0.200, val_acc:0.937]
Epoch [28/120    avg_loss:0.153, val_acc:0.911]
Epoch [29/120    avg_loss:0.154, val_acc:0.922]
Epoch [30/120    avg_loss:0.125, val_acc:0.932]
Epoch [31/120    avg_loss:0.125, val_acc:0.944]
Epoch [32/120    avg_loss:0.105, val_acc:0.943]
Epoch [33/120    avg_loss:0.131, val_acc:0.884]
Epoch [34/120    avg_loss:0.184, val_acc:0.929]
Epoch [35/120    avg_loss:0.141, val_acc:0.942]
Epoch [36/120    avg_loss:0.096, val_acc:0.948]
Epoch [37/120    avg_loss:0.087, val_acc:0.951]
Epoch [38/120    avg_loss:0.082, val_acc:0.961]
Epoch [39/120    avg_loss:0.077, val_acc:0.940]
Epoch [40/120    avg_loss:0.070, val_acc:0.941]
Epoch [41/120    avg_loss:0.090, val_acc:0.945]
Epoch [42/120    avg_loss:0.080, val_acc:0.948]
Epoch [43/120    avg_loss:0.074, val_acc:0.931]
Epoch [44/120    avg_loss:0.075, val_acc:0.958]
Epoch [45/120    avg_loss:0.047, val_acc:0.955]
Epoch [46/120    avg_loss:0.055, val_acc:0.963]
Epoch [47/120    avg_loss:0.044, val_acc:0.963]
Epoch [48/120    avg_loss:0.035, val_acc:0.961]
Epoch [49/120    avg_loss:0.053, val_acc:0.953]
Epoch [50/120    avg_loss:0.066, val_acc:0.959]
Epoch [51/120    avg_loss:0.062, val_acc:0.969]
Epoch [52/120    avg_loss:0.058, val_acc:0.965]
Epoch [53/120    avg_loss:0.072, val_acc:0.943]
Epoch [54/120    avg_loss:0.050, val_acc:0.968]
Epoch [55/120    avg_loss:0.054, val_acc:0.924]
Epoch [56/120    avg_loss:0.065, val_acc:0.962]
Epoch [57/120    avg_loss:0.043, val_acc:0.970]
Epoch [58/120    avg_loss:0.050, val_acc:0.965]
Epoch [59/120    avg_loss:0.074, val_acc:0.956]
Epoch [60/120    avg_loss:0.048, val_acc:0.965]
Epoch [61/120    avg_loss:0.044, val_acc:0.976]
Epoch [62/120    avg_loss:0.028, val_acc:0.970]
Epoch [63/120    avg_loss:0.062, val_acc:0.954]
Epoch [64/120    avg_loss:0.041, val_acc:0.973]
Epoch [65/120    avg_loss:0.029, val_acc:0.948]
Epoch [66/120    avg_loss:0.033, val_acc:0.971]
Epoch [67/120    avg_loss:0.034, val_acc:0.969]
Epoch [68/120    avg_loss:0.035, val_acc:0.969]
Epoch [69/120    avg_loss:0.057, val_acc:0.965]
Epoch [70/120    avg_loss:0.035, val_acc:0.971]
Epoch [71/120    avg_loss:0.021, val_acc:0.972]
Epoch [72/120    avg_loss:0.017, val_acc:0.973]
Epoch [73/120    avg_loss:0.015, val_acc:0.972]
Epoch [74/120    avg_loss:0.046, val_acc:0.961]
Epoch [75/120    avg_loss:0.027, val_acc:0.968]
Epoch [76/120    avg_loss:0.026, val_acc:0.970]
Epoch [77/120    avg_loss:0.019, val_acc:0.973]
Epoch [78/120    avg_loss:0.016, val_acc:0.971]
Epoch [79/120    avg_loss:0.018, val_acc:0.970]
Epoch [80/120    avg_loss:0.013, val_acc:0.974]
Epoch [81/120    avg_loss:0.014, val_acc:0.976]
Epoch [82/120    avg_loss:0.014, val_acc:0.975]
Epoch [83/120    avg_loss:0.013, val_acc:0.976]
Epoch [84/120    avg_loss:0.013, val_acc:0.971]
Epoch [85/120    avg_loss:0.014, val_acc:0.974]
Epoch [86/120    avg_loss:0.011, val_acc:0.974]
Epoch [87/120    avg_loss:0.012, val_acc:0.975]
Epoch [88/120    avg_loss:0.011, val_acc:0.975]
Epoch [89/120    avg_loss:0.013, val_acc:0.976]
Epoch [90/120    avg_loss:0.010, val_acc:0.975]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.010, val_acc:0.977]
Epoch [94/120    avg_loss:0.008, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.978]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.012, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.011, val_acc:0.977]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.980]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.008, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.011, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.011, val_acc:0.977]
Epoch [115/120    avg_loss:0.011, val_acc:0.976]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.012, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6381     1     0     3     0     0     5    36     6]
 [    0     0 18056     0    16     0    18     0     0     0]
 [    0     6     1  1888     0     0     0     0   141     0]
 [    0     7    12     0  2942     0    11     0     0     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    45     0     5     0  4823     0     5     0]
 [    0    10     0     0     0     0     0  1275     5     0]
 [    0    27     3    52    18     0     7     0  3464     0]
 [    0     1     0     0     4     5     0     0     0   909]]

Accuracy:
98.91547971947075

F1 scores:
[       nan 0.9920709  0.99734865 0.94969819 0.98724832 0.99808795
 0.99065421 0.9922179  0.95929106 0.9912759 ]

Kappa:
0.9856234589448606
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f150e7c50>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.199, val_acc:0.091]
Epoch [2/120    avg_loss:1.962, val_acc:0.125]
Epoch [3/120    avg_loss:1.744, val_acc:0.213]
Epoch [4/120    avg_loss:1.585, val_acc:0.232]
Epoch [5/120    avg_loss:1.434, val_acc:0.240]
Epoch [6/120    avg_loss:1.311, val_acc:0.260]
Epoch [7/120    avg_loss:1.233, val_acc:0.327]
Epoch [8/120    avg_loss:1.143, val_acc:0.367]
Epoch [9/120    avg_loss:1.038, val_acc:0.432]
Epoch [10/120    avg_loss:0.951, val_acc:0.665]
Epoch [11/120    avg_loss:0.874, val_acc:0.682]
Epoch [12/120    avg_loss:0.806, val_acc:0.646]
Epoch [13/120    avg_loss:0.728, val_acc:0.689]
Epoch [14/120    avg_loss:0.626, val_acc:0.749]
Epoch [15/120    avg_loss:0.555, val_acc:0.795]
Epoch [16/120    avg_loss:0.502, val_acc:0.793]
Epoch [17/120    avg_loss:0.452, val_acc:0.789]
Epoch [18/120    avg_loss:0.401, val_acc:0.796]
Epoch [19/120    avg_loss:0.367, val_acc:0.803]
Epoch [20/120    avg_loss:0.367, val_acc:0.767]
Epoch [21/120    avg_loss:0.325, val_acc:0.800]
Epoch [22/120    avg_loss:0.288, val_acc:0.788]
Epoch [23/120    avg_loss:0.273, val_acc:0.814]
Epoch [24/120    avg_loss:0.254, val_acc:0.795]
Epoch [25/120    avg_loss:0.300, val_acc:0.807]
Epoch [26/120    avg_loss:0.277, val_acc:0.839]
Epoch [27/120    avg_loss:0.223, val_acc:0.824]
Epoch [28/120    avg_loss:0.216, val_acc:0.885]
Epoch [29/120    avg_loss:0.192, val_acc:0.898]
Epoch [30/120    avg_loss:0.185, val_acc:0.887]
Epoch [31/120    avg_loss:0.169, val_acc:0.899]
Epoch [32/120    avg_loss:0.213, val_acc:0.879]
Epoch [33/120    avg_loss:0.163, val_acc:0.875]
Epoch [34/120    avg_loss:0.178, val_acc:0.914]
Epoch [35/120    avg_loss:0.128, val_acc:0.921]
Epoch [36/120    avg_loss:0.121, val_acc:0.838]
Epoch [37/120    avg_loss:0.115, val_acc:0.929]
Epoch [38/120    avg_loss:0.126, val_acc:0.953]
Epoch [39/120    avg_loss:0.098, val_acc:0.929]
Epoch [40/120    avg_loss:0.086, val_acc:0.957]
Epoch [41/120    avg_loss:0.085, val_acc:0.957]
Epoch [42/120    avg_loss:0.087, val_acc:0.955]
Epoch [43/120    avg_loss:0.070, val_acc:0.934]
Epoch [44/120    avg_loss:0.084, val_acc:0.917]
Epoch [45/120    avg_loss:0.076, val_acc:0.968]
Epoch [46/120    avg_loss:0.124, val_acc:0.942]
Epoch [47/120    avg_loss:0.104, val_acc:0.952]
Epoch [48/120    avg_loss:0.074, val_acc:0.957]
Epoch [49/120    avg_loss:0.079, val_acc:0.961]
Epoch [50/120    avg_loss:0.050, val_acc:0.971]
Epoch [51/120    avg_loss:0.037, val_acc:0.971]
Epoch [52/120    avg_loss:0.034, val_acc:0.976]
Epoch [53/120    avg_loss:0.062, val_acc:0.966]
Epoch [54/120    avg_loss:0.045, val_acc:0.971]
Epoch [55/120    avg_loss:0.068, val_acc:0.962]
Epoch [56/120    avg_loss:0.036, val_acc:0.985]
Epoch [57/120    avg_loss:0.032, val_acc:0.978]
Epoch [58/120    avg_loss:0.038, val_acc:0.963]
Epoch [59/120    avg_loss:0.035, val_acc:0.981]
Epoch [60/120    avg_loss:0.048, val_acc:0.976]
Epoch [61/120    avg_loss:0.028, val_acc:0.963]
Epoch [62/120    avg_loss:0.024, val_acc:0.985]
Epoch [63/120    avg_loss:0.022, val_acc:0.983]
Epoch [64/120    avg_loss:0.024, val_acc:0.970]
Epoch [65/120    avg_loss:0.034, val_acc:0.980]
Epoch [66/120    avg_loss:0.047, val_acc:0.959]
Epoch [67/120    avg_loss:0.085, val_acc:0.937]
Epoch [68/120    avg_loss:0.044, val_acc:0.965]
Epoch [69/120    avg_loss:0.051, val_acc:0.976]
Epoch [70/120    avg_loss:0.048, val_acc:0.971]
Epoch [71/120    avg_loss:0.058, val_acc:0.976]
Epoch [72/120    avg_loss:0.023, val_acc:0.978]
Epoch [73/120    avg_loss:0.037, val_acc:0.981]
Epoch [74/120    avg_loss:0.021, val_acc:0.978]
Epoch [75/120    avg_loss:0.024, val_acc:0.981]
Epoch [76/120    avg_loss:0.027, val_acc:0.989]
Epoch [77/120    avg_loss:0.025, val_acc:0.988]
Epoch [78/120    avg_loss:0.019, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.989]
Epoch [80/120    avg_loss:0.014, val_acc:0.990]
Epoch [81/120    avg_loss:0.012, val_acc:0.991]
Epoch [82/120    avg_loss:0.014, val_acc:0.989]
Epoch [83/120    avg_loss:0.013, val_acc:0.991]
Epoch [84/120    avg_loss:0.012, val_acc:0.989]
Epoch [85/120    avg_loss:0.013, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.991]
Epoch [87/120    avg_loss:0.014, val_acc:0.991]
Epoch [88/120    avg_loss:0.011, val_acc:0.990]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.990]
Epoch [91/120    avg_loss:0.013, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.989]
Epoch [94/120    avg_loss:0.012, val_acc:0.990]
Epoch [95/120    avg_loss:0.013, val_acc:0.989]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.989]
Epoch [98/120    avg_loss:0.011, val_acc:0.990]
Epoch [99/120    avg_loss:0.011, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.990]
Epoch [101/120    avg_loss:0.012, val_acc:0.990]
Epoch [102/120    avg_loss:0.013, val_acc:0.990]
Epoch [103/120    avg_loss:0.013, val_acc:0.990]
Epoch [104/120    avg_loss:0.011, val_acc:0.990]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.990]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.990]
Epoch [110/120    avg_loss:0.012, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.012, val_acc:0.990]
Epoch [113/120    avg_loss:0.014, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.014, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     0     1     0     0     0    95     0]
 [    0     2 18040     0    19     0    29     0     0     0]
 [    0    13     0  1900     0     0     0     0   123     0]
 [    0    13     8     0  2916     0    20     0    12     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    32     0     0     0  4846     0     0     0]
 [    0    11     0     0     0     0     0  1278     1     0]
 [    0    24     0    41    20     0     0     0  3484     2]
 [    0     2     0     0    14    15     0     0     0   888]]

Accuracy:
98.79497746607862

F1 scores:
[       nan 0.98745422 0.99751175 0.95549409 0.98148771 0.99428571
 0.99171186 0.9953271  0.95635465 0.98013245]

Kappa:
0.9840327518679042
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4af177eb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.180, val_acc:0.035]
Epoch [2/120    avg_loss:2.016, val_acc:0.065]
Epoch [3/120    avg_loss:1.883, val_acc:0.079]
Epoch [4/120    avg_loss:1.741, val_acc:0.135]
Epoch [5/120    avg_loss:1.568, val_acc:0.232]
Epoch [6/120    avg_loss:1.421, val_acc:0.339]
Epoch [7/120    avg_loss:1.285, val_acc:0.429]
Epoch [8/120    avg_loss:1.152, val_acc:0.427]
Epoch [9/120    avg_loss:1.073, val_acc:0.471]
Epoch [10/120    avg_loss:0.972, val_acc:0.491]
Epoch [11/120    avg_loss:0.861, val_acc:0.501]
Epoch [12/120    avg_loss:0.768, val_acc:0.497]
Epoch [13/120    avg_loss:0.675, val_acc:0.540]
Epoch [14/120    avg_loss:0.638, val_acc:0.617]
Epoch [15/120    avg_loss:0.533, val_acc:0.611]
Epoch [16/120    avg_loss:0.524, val_acc:0.684]
Epoch [17/120    avg_loss:0.449, val_acc:0.771]
Epoch [18/120    avg_loss:0.393, val_acc:0.786]
Epoch [19/120    avg_loss:0.377, val_acc:0.778]
Epoch [20/120    avg_loss:0.338, val_acc:0.844]
Epoch [21/120    avg_loss:0.304, val_acc:0.842]
Epoch [22/120    avg_loss:0.274, val_acc:0.834]
Epoch [23/120    avg_loss:0.257, val_acc:0.921]
Epoch [24/120    avg_loss:0.263, val_acc:0.851]
Epoch [25/120    avg_loss:0.229, val_acc:0.918]
Epoch [26/120    avg_loss:0.198, val_acc:0.933]
Epoch [27/120    avg_loss:0.196, val_acc:0.939]
Epoch [28/120    avg_loss:0.187, val_acc:0.905]
Epoch [29/120    avg_loss:0.147, val_acc:0.958]
Epoch [30/120    avg_loss:0.162, val_acc:0.948]
Epoch [31/120    avg_loss:0.138, val_acc:0.955]
Epoch [32/120    avg_loss:0.151, val_acc:0.917]
Epoch [33/120    avg_loss:0.194, val_acc:0.851]
Epoch [34/120    avg_loss:0.200, val_acc:0.895]
Epoch [35/120    avg_loss:0.149, val_acc:0.954]
Epoch [36/120    avg_loss:0.150, val_acc:0.951]
Epoch [37/120    avg_loss:0.134, val_acc:0.945]
Epoch [38/120    avg_loss:0.129, val_acc:0.969]
Epoch [39/120    avg_loss:0.097, val_acc:0.970]
Epoch [40/120    avg_loss:0.097, val_acc:0.938]
Epoch [41/120    avg_loss:0.117, val_acc:0.951]
Epoch [42/120    avg_loss:0.114, val_acc:0.960]
Epoch [43/120    avg_loss:0.126, val_acc:0.970]
Epoch [44/120    avg_loss:0.098, val_acc:0.973]
Epoch [45/120    avg_loss:0.111, val_acc:0.944]
Epoch [46/120    avg_loss:0.088, val_acc:0.966]
Epoch [47/120    avg_loss:0.081, val_acc:0.968]
Epoch [48/120    avg_loss:0.074, val_acc:0.975]
Epoch [49/120    avg_loss:0.068, val_acc:0.979]
Epoch [50/120    avg_loss:0.064, val_acc:0.970]
Epoch [51/120    avg_loss:0.041, val_acc:0.984]
Epoch [52/120    avg_loss:0.051, val_acc:0.981]
Epoch [53/120    avg_loss:0.049, val_acc:0.966]
Epoch [54/120    avg_loss:0.045, val_acc:0.977]
Epoch [55/120    avg_loss:0.058, val_acc:0.976]
Epoch [56/120    avg_loss:0.048, val_acc:0.974]
Epoch [57/120    avg_loss:0.046, val_acc:0.965]
Epoch [58/120    avg_loss:0.048, val_acc:0.962]
Epoch [59/120    avg_loss:0.059, val_acc:0.968]
Epoch [60/120    avg_loss:0.036, val_acc:0.981]
Epoch [61/120    avg_loss:0.028, val_acc:0.981]
Epoch [62/120    avg_loss:0.024, val_acc:0.969]
Epoch [63/120    avg_loss:0.031, val_acc:0.977]
Epoch [64/120    avg_loss:0.031, val_acc:0.981]
Epoch [65/120    avg_loss:0.023, val_acc:0.985]
Epoch [66/120    avg_loss:0.020, val_acc:0.985]
Epoch [67/120    avg_loss:0.017, val_acc:0.986]
Epoch [68/120    avg_loss:0.021, val_acc:0.986]
Epoch [69/120    avg_loss:0.015, val_acc:0.984]
Epoch [70/120    avg_loss:0.021, val_acc:0.984]
Epoch [71/120    avg_loss:0.017, val_acc:0.984]
Epoch [72/120    avg_loss:0.018, val_acc:0.986]
Epoch [73/120    avg_loss:0.015, val_acc:0.986]
Epoch [74/120    avg_loss:0.017, val_acc:0.984]
Epoch [75/120    avg_loss:0.015, val_acc:0.984]
Epoch [76/120    avg_loss:0.019, val_acc:0.985]
Epoch [77/120    avg_loss:0.016, val_acc:0.985]
Epoch [78/120    avg_loss:0.016, val_acc:0.983]
Epoch [79/120    avg_loss:0.014, val_acc:0.986]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.013, val_acc:0.986]
Epoch [82/120    avg_loss:0.020, val_acc:0.983]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.013, val_acc:0.986]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.015, val_acc:0.986]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.014, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.012, val_acc:0.988]
Epoch [93/120    avg_loss:0.018, val_acc:0.988]
Epoch [94/120    avg_loss:0.013, val_acc:0.985]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.017, val_acc:0.986]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.018, val_acc:0.985]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.017, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.986]
Epoch [102/120    avg_loss:0.015, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.014, val_acc:0.988]
Epoch [105/120    avg_loss:0.014, val_acc:0.989]
Epoch [106/120    avg_loss:0.017, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.987]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.989]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.015, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.013, val_acc:0.985]
Epoch [116/120    avg_loss:0.013, val_acc:0.987]
Epoch [117/120    avg_loss:0.011, val_acc:0.987]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.012, val_acc:0.989]
Epoch [120/120    avg_loss:0.013, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     1     0     0     0     4    37     1]
 [    0     5 18020     0    55     0    10     0     0     0]
 [    0     2     0  1932     0     0     0     0   100     2]
 [    0    31    15     5  2913     0     5     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     0     0     0  4848     0     0     0]
 [    0     4     0     0     0    10     0  1271     0     5]
 [    0     8     0    54    43     0     0     0  3464     2]
 [    0     0     0     0    14    17     0     0     0   888]]

Accuracy:
98.8841491335888

F1 scores:
[       nan 0.99277445 0.99681925 0.959285   0.97148574 0.98976109
 0.99538035 0.99103314 0.96557491 0.97743533]

Kappa:
0.985217692747571
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f201d30aba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.177, val_acc:0.090]
Epoch [2/120    avg_loss:1.965, val_acc:0.113]
Epoch [3/120    avg_loss:1.793, val_acc:0.129]
Epoch [4/120    avg_loss:1.660, val_acc:0.140]
Epoch [5/120    avg_loss:1.543, val_acc:0.152]
Epoch [6/120    avg_loss:1.444, val_acc:0.198]
Epoch [7/120    avg_loss:1.335, val_acc:0.330]
Epoch [8/120    avg_loss:1.240, val_acc:0.581]
Epoch [9/120    avg_loss:1.165, val_acc:0.627]
Epoch [10/120    avg_loss:1.065, val_acc:0.726]
Epoch [11/120    avg_loss:0.919, val_acc:0.691]
Epoch [12/120    avg_loss:0.797, val_acc:0.703]
Epoch [13/120    avg_loss:0.710, val_acc:0.717]
Epoch [14/120    avg_loss:0.623, val_acc:0.723]
Epoch [15/120    avg_loss:0.599, val_acc:0.735]
Epoch [16/120    avg_loss:0.511, val_acc:0.753]
Epoch [17/120    avg_loss:0.505, val_acc:0.763]
Epoch [18/120    avg_loss:0.427, val_acc:0.789]
Epoch [19/120    avg_loss:0.381, val_acc:0.821]
Epoch [20/120    avg_loss:0.343, val_acc:0.844]
Epoch [21/120    avg_loss:0.336, val_acc:0.878]
Epoch [22/120    avg_loss:0.284, val_acc:0.909]
Epoch [23/120    avg_loss:0.276, val_acc:0.860]
Epoch [24/120    avg_loss:0.237, val_acc:0.922]
Epoch [25/120    avg_loss:0.200, val_acc:0.921]
Epoch [26/120    avg_loss:0.174, val_acc:0.949]
Epoch [27/120    avg_loss:0.158, val_acc:0.928]
Epoch [28/120    avg_loss:0.186, val_acc:0.957]
Epoch [29/120    avg_loss:0.176, val_acc:0.924]
Epoch [30/120    avg_loss:0.201, val_acc:0.944]
Epoch [31/120    avg_loss:0.234, val_acc:0.891]
Epoch [32/120    avg_loss:0.208, val_acc:0.910]
Epoch [33/120    avg_loss:0.154, val_acc:0.948]
Epoch [34/120    avg_loss:0.111, val_acc:0.963]
Epoch [35/120    avg_loss:0.104, val_acc:0.954]
Epoch [36/120    avg_loss:0.111, val_acc:0.958]
Epoch [37/120    avg_loss:0.090, val_acc:0.961]
Epoch [38/120    avg_loss:0.076, val_acc:0.970]
Epoch [39/120    avg_loss:0.079, val_acc:0.966]
Epoch [40/120    avg_loss:0.085, val_acc:0.963]
Epoch [41/120    avg_loss:0.060, val_acc:0.972]
Epoch [42/120    avg_loss:0.054, val_acc:0.965]
Epoch [43/120    avg_loss:0.052, val_acc:0.971]
Epoch [44/120    avg_loss:0.046, val_acc:0.966]
Epoch [45/120    avg_loss:0.051, val_acc:0.959]
Epoch [46/120    avg_loss:0.051, val_acc:0.965]
Epoch [47/120    avg_loss:0.053, val_acc:0.967]
Epoch [48/120    avg_loss:0.043, val_acc:0.977]
Epoch [49/120    avg_loss:0.031, val_acc:0.965]
Epoch [50/120    avg_loss:0.043, val_acc:0.966]
Epoch [51/120    avg_loss:0.056, val_acc:0.972]
Epoch [52/120    avg_loss:0.041, val_acc:0.938]
Epoch [53/120    avg_loss:0.042, val_acc:0.958]
Epoch [54/120    avg_loss:0.042, val_acc:0.976]
Epoch [55/120    avg_loss:0.024, val_acc:0.976]
Epoch [56/120    avg_loss:0.031, val_acc:0.969]
Epoch [57/120    avg_loss:0.030, val_acc:0.978]
Epoch [58/120    avg_loss:0.021, val_acc:0.978]
Epoch [59/120    avg_loss:0.032, val_acc:0.961]
Epoch [60/120    avg_loss:0.040, val_acc:0.977]
Epoch [61/120    avg_loss:0.021, val_acc:0.975]
Epoch [62/120    avg_loss:0.021, val_acc:0.975]
Epoch [63/120    avg_loss:0.020, val_acc:0.974]
Epoch [64/120    avg_loss:0.018, val_acc:0.970]
Epoch [65/120    avg_loss:0.019, val_acc:0.978]
Epoch [66/120    avg_loss:0.017, val_acc:0.976]
Epoch [67/120    avg_loss:0.017, val_acc:0.976]
Epoch [68/120    avg_loss:0.015, val_acc:0.981]
Epoch [69/120    avg_loss:0.023, val_acc:0.968]
Epoch [70/120    avg_loss:0.025, val_acc:0.980]
Epoch [71/120    avg_loss:0.016, val_acc:0.982]
Epoch [72/120    avg_loss:0.009, val_acc:0.982]
Epoch [73/120    avg_loss:0.013, val_acc:0.981]
Epoch [74/120    avg_loss:0.021, val_acc:0.975]
Epoch [75/120    avg_loss:0.013, val_acc:0.977]
Epoch [76/120    avg_loss:0.078, val_acc:0.954]
Epoch [77/120    avg_loss:0.120, val_acc:0.960]
Epoch [78/120    avg_loss:0.078, val_acc:0.967]
Epoch [79/120    avg_loss:0.054, val_acc:0.963]
Epoch [80/120    avg_loss:0.073, val_acc:0.968]
Epoch [81/120    avg_loss:0.063, val_acc:0.962]
Epoch [82/120    avg_loss:0.037, val_acc:0.974]
Epoch [83/120    avg_loss:0.021, val_acc:0.977]
Epoch [84/120    avg_loss:0.015, val_acc:0.976]
Epoch [85/120    avg_loss:0.034, val_acc:0.933]
Epoch [86/120    avg_loss:0.027, val_acc:0.978]
Epoch [87/120    avg_loss:0.017, val_acc:0.976]
Epoch [88/120    avg_loss:0.014, val_acc:0.977]
Epoch [89/120    avg_loss:0.014, val_acc:0.978]
Epoch [90/120    avg_loss:0.014, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.012, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.978]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.979]
Epoch [98/120    avg_loss:0.012, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.976]
Epoch [100/120    avg_loss:0.009, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.011, val_acc:0.976]
Epoch [105/120    avg_loss:0.010, val_acc:0.976]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.977]
Epoch [108/120    avg_loss:0.011, val_acc:0.977]
Epoch [109/120    avg_loss:0.008, val_acc:0.977]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.977]
Epoch [112/120    avg_loss:0.013, val_acc:0.977]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.011, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.009, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.009, val_acc:0.977]
Epoch [119/120    avg_loss:0.009, val_acc:0.977]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     1     1     0     0    27    31    14]
 [    0     0 18050     0    29     0    10     0     1     0]
 [    0     4     0  1980     0     0     0     0    46     6]
 [    0    12     8     0  2904     0    25     0    11    12]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    27     1     0     0  4850     0     0     0]
 [    0    19     0     0     0     0     0  1270     0     1]
 [    0    42     0    64    53     0     0     0  3412     0]
 [    0     0     0     0     4    63     0     0     0   852]]

Accuracy:
98.76605692526451

F1 scores:
[       nan 0.98826455 0.99792674 0.97011269 0.97400637 0.97643098
 0.99354707 0.98183224 0.96493213 0.94456763]

Kappa:
0.9836513981803656
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa1d12c3ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.175, val_acc:0.139]
Epoch [2/120    avg_loss:1.976, val_acc:0.084]
Epoch [3/120    avg_loss:1.781, val_acc:0.098]
Epoch [4/120    avg_loss:1.634, val_acc:0.311]
Epoch [5/120    avg_loss:1.477, val_acc:0.394]
Epoch [6/120    avg_loss:1.379, val_acc:0.509]
Epoch [7/120    avg_loss:1.266, val_acc:0.455]
Epoch [8/120    avg_loss:1.177, val_acc:0.467]
Epoch [9/120    avg_loss:1.020, val_acc:0.446]
Epoch [10/120    avg_loss:0.915, val_acc:0.498]
Epoch [11/120    avg_loss:0.784, val_acc:0.509]
Epoch [12/120    avg_loss:0.690, val_acc:0.553]
Epoch [13/120    avg_loss:0.610, val_acc:0.617]
Epoch [14/120    avg_loss:0.542, val_acc:0.562]
Epoch [15/120    avg_loss:0.535, val_acc:0.601]
Epoch [16/120    avg_loss:0.469, val_acc:0.703]
Epoch [17/120    avg_loss:0.453, val_acc:0.683]
Epoch [18/120    avg_loss:0.376, val_acc:0.691]
Epoch [19/120    avg_loss:0.357, val_acc:0.701]
Epoch [20/120    avg_loss:0.376, val_acc:0.720]
Epoch [21/120    avg_loss:0.323, val_acc:0.731]
Epoch [22/120    avg_loss:0.314, val_acc:0.740]
Epoch [23/120    avg_loss:0.277, val_acc:0.785]
Epoch [24/120    avg_loss:0.297, val_acc:0.766]
Epoch [25/120    avg_loss:0.298, val_acc:0.757]
Epoch [26/120    avg_loss:0.262, val_acc:0.745]
Epoch [27/120    avg_loss:0.237, val_acc:0.802]
Epoch [28/120    avg_loss:0.269, val_acc:0.798]
Epoch [29/120    avg_loss:0.243, val_acc:0.816]
Epoch [30/120    avg_loss:0.206, val_acc:0.819]
Epoch [31/120    avg_loss:0.218, val_acc:0.872]
Epoch [32/120    avg_loss:0.177, val_acc:0.837]
Epoch [33/120    avg_loss:0.168, val_acc:0.880]
Epoch [34/120    avg_loss:0.181, val_acc:0.916]
Epoch [35/120    avg_loss:0.157, val_acc:0.932]
Epoch [36/120    avg_loss:0.149, val_acc:0.859]
Epoch [37/120    avg_loss:0.153, val_acc:0.950]
Epoch [38/120    avg_loss:0.152, val_acc:0.907]
Epoch [39/120    avg_loss:0.167, val_acc:0.945]
Epoch [40/120    avg_loss:0.115, val_acc:0.959]
Epoch [41/120    avg_loss:0.114, val_acc:0.946]
Epoch [42/120    avg_loss:0.085, val_acc:0.961]
Epoch [43/120    avg_loss:0.086, val_acc:0.940]
Epoch [44/120    avg_loss:0.088, val_acc:0.942]
Epoch [45/120    avg_loss:0.081, val_acc:0.959]
Epoch [46/120    avg_loss:0.084, val_acc:0.965]
Epoch [47/120    avg_loss:0.066, val_acc:0.961]
Epoch [48/120    avg_loss:0.102, val_acc:0.948]
Epoch [49/120    avg_loss:0.092, val_acc:0.943]
Epoch [50/120    avg_loss:0.070, val_acc:0.961]
Epoch [51/120    avg_loss:0.056, val_acc:0.958]
Epoch [52/120    avg_loss:0.057, val_acc:0.963]
Epoch [53/120    avg_loss:0.049, val_acc:0.953]
Epoch [54/120    avg_loss:0.065, val_acc:0.959]
Epoch [55/120    avg_loss:0.054, val_acc:0.957]
Epoch [56/120    avg_loss:0.064, val_acc:0.948]
Epoch [57/120    avg_loss:0.049, val_acc:0.971]
Epoch [58/120    avg_loss:0.057, val_acc:0.948]
Epoch [59/120    avg_loss:0.103, val_acc:0.921]
Epoch [60/120    avg_loss:0.074, val_acc:0.962]
Epoch [61/120    avg_loss:0.047, val_acc:0.963]
Epoch [62/120    avg_loss:0.045, val_acc:0.976]
Epoch [63/120    avg_loss:0.035, val_acc:0.976]
Epoch [64/120    avg_loss:0.026, val_acc:0.970]
Epoch [65/120    avg_loss:0.025, val_acc:0.972]
Epoch [66/120    avg_loss:0.035, val_acc:0.972]
Epoch [67/120    avg_loss:0.033, val_acc:0.970]
Epoch [68/120    avg_loss:0.094, val_acc:0.953]
Epoch [69/120    avg_loss:0.078, val_acc:0.932]
Epoch [70/120    avg_loss:0.058, val_acc:0.957]
Epoch [71/120    avg_loss:0.056, val_acc:0.966]
Epoch [72/120    avg_loss:0.042, val_acc:0.960]
Epoch [73/120    avg_loss:0.040, val_acc:0.970]
Epoch [74/120    avg_loss:0.036, val_acc:0.967]
Epoch [75/120    avg_loss:0.036, val_acc:0.976]
Epoch [76/120    avg_loss:0.022, val_acc:0.981]
Epoch [77/120    avg_loss:0.022, val_acc:0.972]
Epoch [78/120    avg_loss:0.016, val_acc:0.965]
Epoch [79/120    avg_loss:0.019, val_acc:0.979]
Epoch [80/120    avg_loss:0.023, val_acc:0.978]
Epoch [81/120    avg_loss:0.017, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.980]
Epoch [83/120    avg_loss:0.025, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.034, val_acc:0.975]
Epoch [87/120    avg_loss:0.019, val_acc:0.980]
Epoch [88/120    avg_loss:0.016, val_acc:0.979]
Epoch [89/120    avg_loss:0.038, val_acc:0.966]
Epoch [90/120    avg_loss:0.052, val_acc:0.954]
Epoch [91/120    avg_loss:0.052, val_acc:0.959]
Epoch [92/120    avg_loss:0.035, val_acc:0.969]
Epoch [93/120    avg_loss:0.022, val_acc:0.975]
Epoch [94/120    avg_loss:0.020, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.980]
Epoch [96/120    avg_loss:0.023, val_acc:0.976]
Epoch [97/120    avg_loss:0.033, val_acc:0.983]
Epoch [98/120    avg_loss:0.028, val_acc:0.976]
Epoch [99/120    avg_loss:0.017, val_acc:0.977]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.014, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.010, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.979]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.012, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.010, val_acc:0.979]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     0     1     0     0     2    35     9]
 [    0     0 18050     0    28     0    11     0     1     0]
 [    0     5     0  1907     0     0     0     0   117     7]
 [    0    18    10     0  2924     0    13     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4874     0     0     0]
 [    0    21     0     0     0     0     0  1255     1    13]
 [    0    17     0    53    49     0     0     0  3444     8]
 [    0     0     0     1    14    11     0     0     0   893]]

Accuracy:
98.9010194490637

F1 scores:
[       nan 0.9916136  0.99850639 0.95421566 0.97661991 0.99580313
 0.99713584 0.98547311 0.95986622 0.96592753]

Kappa:
0.9854401002330877
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9bfd06b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.241, val_acc:0.439]
Epoch [2/120    avg_loss:2.028, val_acc:0.444]
Epoch [3/120    avg_loss:1.869, val_acc:0.469]
Epoch [4/120    avg_loss:1.702, val_acc:0.514]
Epoch [5/120    avg_loss:1.553, val_acc:0.585]
Epoch [6/120    avg_loss:1.413, val_acc:0.622]
Epoch [7/120    avg_loss:1.271, val_acc:0.688]
Epoch [8/120    avg_loss:1.160, val_acc:0.698]
Epoch [9/120    avg_loss:1.050, val_acc:0.729]
Epoch [10/120    avg_loss:0.951, val_acc:0.618]
Epoch [11/120    avg_loss:0.788, val_acc:0.800]
Epoch [12/120    avg_loss:0.762, val_acc:0.758]
Epoch [13/120    avg_loss:0.698, val_acc:0.820]
Epoch [14/120    avg_loss:0.563, val_acc:0.842]
Epoch [15/120    avg_loss:0.531, val_acc:0.833]
Epoch [16/120    avg_loss:0.458, val_acc:0.839]
Epoch [17/120    avg_loss:0.411, val_acc:0.850]
Epoch [18/120    avg_loss:0.362, val_acc:0.843]
Epoch [19/120    avg_loss:0.367, val_acc:0.887]
Epoch [20/120    avg_loss:0.321, val_acc:0.874]
Epoch [21/120    avg_loss:0.335, val_acc:0.845]
Epoch [22/120    avg_loss:0.324, val_acc:0.884]
Epoch [23/120    avg_loss:0.301, val_acc:0.893]
Epoch [24/120    avg_loss:0.277, val_acc:0.880]
Epoch [25/120    avg_loss:0.233, val_acc:0.908]
Epoch [26/120    avg_loss:0.221, val_acc:0.928]
Epoch [27/120    avg_loss:0.251, val_acc:0.832]
Epoch [28/120    avg_loss:0.197, val_acc:0.930]
Epoch [29/120    avg_loss:0.152, val_acc:0.953]
Epoch [30/120    avg_loss:0.187, val_acc:0.926]
Epoch [31/120    avg_loss:0.144, val_acc:0.932]
Epoch [32/120    avg_loss:0.127, val_acc:0.959]
Epoch [33/120    avg_loss:0.109, val_acc:0.967]
Epoch [34/120    avg_loss:0.104, val_acc:0.966]
Epoch [35/120    avg_loss:0.093, val_acc:0.961]
Epoch [36/120    avg_loss:0.104, val_acc:0.962]
Epoch [37/120    avg_loss:0.080, val_acc:0.965]
Epoch [38/120    avg_loss:0.087, val_acc:0.950]
Epoch [39/120    avg_loss:0.076, val_acc:0.974]
Epoch [40/120    avg_loss:0.099, val_acc:0.932]
Epoch [41/120    avg_loss:0.084, val_acc:0.957]
Epoch [42/120    avg_loss:0.084, val_acc:0.954]
Epoch [43/120    avg_loss:0.070, val_acc:0.969]
Epoch [44/120    avg_loss:0.046, val_acc:0.968]
Epoch [45/120    avg_loss:0.050, val_acc:0.965]
Epoch [46/120    avg_loss:0.116, val_acc:0.948]
Epoch [47/120    avg_loss:0.067, val_acc:0.975]
Epoch [48/120    avg_loss:0.087, val_acc:0.959]
Epoch [49/120    avg_loss:0.086, val_acc:0.951]
Epoch [50/120    avg_loss:0.077, val_acc:0.973]
Epoch [51/120    avg_loss:0.055, val_acc:0.927]
Epoch [52/120    avg_loss:0.048, val_acc:0.965]
Epoch [53/120    avg_loss:0.071, val_acc:0.974]
Epoch [54/120    avg_loss:0.054, val_acc:0.970]
Epoch [55/120    avg_loss:0.037, val_acc:0.978]
Epoch [56/120    avg_loss:0.047, val_acc:0.977]
Epoch [57/120    avg_loss:0.033, val_acc:0.972]
Epoch [58/120    avg_loss:0.038, val_acc:0.983]
Epoch [59/120    avg_loss:0.033, val_acc:0.976]
Epoch [60/120    avg_loss:0.058, val_acc:0.978]
Epoch [61/120    avg_loss:0.057, val_acc:0.960]
Epoch [62/120    avg_loss:0.054, val_acc:0.954]
Epoch [63/120    avg_loss:0.036, val_acc:0.966]
Epoch [64/120    avg_loss:0.039, val_acc:0.976]
Epoch [65/120    avg_loss:0.029, val_acc:0.980]
Epoch [66/120    avg_loss:0.024, val_acc:0.976]
Epoch [67/120    avg_loss:0.023, val_acc:0.978]
Epoch [68/120    avg_loss:0.024, val_acc:0.976]
Epoch [69/120    avg_loss:0.019, val_acc:0.979]
Epoch [70/120    avg_loss:0.021, val_acc:0.976]
Epoch [71/120    avg_loss:0.022, val_acc:0.981]
Epoch [72/120    avg_loss:0.016, val_acc:0.979]
Epoch [73/120    avg_loss:0.015, val_acc:0.984]
Epoch [74/120    avg_loss:0.014, val_acc:0.986]
Epoch [75/120    avg_loss:0.016, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.016, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.014, val_acc:0.986]
Epoch [81/120    avg_loss:0.014, val_acc:0.986]
Epoch [82/120    avg_loss:0.015, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.014, val_acc:0.986]
Epoch [87/120    avg_loss:0.013, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.986]
Epoch [97/120    avg_loss:0.013, val_acc:0.988]
Epoch [98/120    avg_loss:0.011, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.011, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.012, val_acc:0.987]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.987]
Epoch [112/120    avg_loss:0.012, val_acc:0.987]
Epoch [113/120    avg_loss:0.010, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.987]
Epoch [116/120    avg_loss:0.011, val_acc:0.987]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.987]
Epoch [119/120    avg_loss:0.013, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     0     4    57     3]
 [    0     0 17944     0    22     0   120     0     4     0]
 [    0     6     0  1912     0     0     0     0   118     0]
 [    0    19    18     0  2926     0     8     0     1     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4855     0     0     0]
 [    0     4     0     0     0     0     0  1285     1     0]
 [    0    15     0    40    50     0     0     0  3466     0]
 [    0     1     0     0     3     7     0     0     0   908]]

Accuracy:
98.7371363844504

F1 scores:
[       nan 0.99151421 0.99481635 0.95887663 0.97974217 0.99732518
 0.98468715 0.99651028 0.96037684 0.99234973]

Kappa:
0.9832833081808575
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd6f2b58b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.221, val_acc:0.137]
Epoch [2/120    avg_loss:2.007, val_acc:0.150]
Epoch [3/120    avg_loss:1.849, val_acc:0.159]
Epoch [4/120    avg_loss:1.689, val_acc:0.192]
Epoch [5/120    avg_loss:1.519, val_acc:0.199]
Epoch [6/120    avg_loss:1.396, val_acc:0.241]
Epoch [7/120    avg_loss:1.248, val_acc:0.274]
Epoch [8/120    avg_loss:1.147, val_acc:0.356]
Epoch [9/120    avg_loss:1.040, val_acc:0.371]
Epoch [10/120    avg_loss:0.954, val_acc:0.414]
Epoch [11/120    avg_loss:0.830, val_acc:0.687]
Epoch [12/120    avg_loss:0.750, val_acc:0.796]
Epoch [13/120    avg_loss:0.705, val_acc:0.779]
Epoch [14/120    avg_loss:0.626, val_acc:0.726]
Epoch [15/120    avg_loss:0.547, val_acc:0.817]
Epoch [16/120    avg_loss:0.495, val_acc:0.817]
Epoch [17/120    avg_loss:0.429, val_acc:0.849]
Epoch [18/120    avg_loss:0.374, val_acc:0.804]
Epoch [19/120    avg_loss:0.361, val_acc:0.812]
Epoch [20/120    avg_loss:0.345, val_acc:0.822]
Epoch [21/120    avg_loss:0.285, val_acc:0.823]
Epoch [22/120    avg_loss:0.278, val_acc:0.823]
Epoch [23/120    avg_loss:0.307, val_acc:0.834]
Epoch [24/120    avg_loss:0.285, val_acc:0.863]
Epoch [25/120    avg_loss:0.253, val_acc:0.861]
Epoch [26/120    avg_loss:0.214, val_acc:0.903]
Epoch [27/120    avg_loss:0.257, val_acc:0.927]
Epoch [28/120    avg_loss:0.180, val_acc:0.949]
Epoch [29/120    avg_loss:0.135, val_acc:0.955]
Epoch [30/120    avg_loss:0.154, val_acc:0.947]
Epoch [31/120    avg_loss:0.531, val_acc:0.750]
Epoch [32/120    avg_loss:0.778, val_acc:0.737]
Epoch [33/120    avg_loss:0.485, val_acc:0.709]
Epoch [34/120    avg_loss:0.465, val_acc:0.747]
Epoch [35/120    avg_loss:0.352, val_acc:0.829]
Epoch [36/120    avg_loss:0.337, val_acc:0.843]
Epoch [37/120    avg_loss:0.224, val_acc:0.919]
Epoch [38/120    avg_loss:0.205, val_acc:0.891]
Epoch [39/120    avg_loss:0.232, val_acc:0.763]
Epoch [40/120    avg_loss:0.407, val_acc:0.874]
Epoch [41/120    avg_loss:0.197, val_acc:0.896]
Epoch [42/120    avg_loss:0.155, val_acc:0.953]
Epoch [43/120    avg_loss:0.140, val_acc:0.943]
Epoch [44/120    avg_loss:0.108, val_acc:0.961]
Epoch [45/120    avg_loss:0.096, val_acc:0.958]
Epoch [46/120    avg_loss:0.109, val_acc:0.962]
Epoch [47/120    avg_loss:0.097, val_acc:0.968]
Epoch [48/120    avg_loss:0.093, val_acc:0.959]
Epoch [49/120    avg_loss:0.092, val_acc:0.969]
Epoch [50/120    avg_loss:0.095, val_acc:0.963]
Epoch [51/120    avg_loss:0.093, val_acc:0.970]
Epoch [52/120    avg_loss:0.089, val_acc:0.964]
Epoch [53/120    avg_loss:0.105, val_acc:0.970]
Epoch [54/120    avg_loss:0.079, val_acc:0.967]
Epoch [55/120    avg_loss:0.099, val_acc:0.965]
Epoch [56/120    avg_loss:0.082, val_acc:0.966]
Epoch [57/120    avg_loss:0.075, val_acc:0.968]
Epoch [58/120    avg_loss:0.078, val_acc:0.974]
Epoch [59/120    avg_loss:0.078, val_acc:0.968]
Epoch [60/120    avg_loss:0.083, val_acc:0.973]
Epoch [61/120    avg_loss:0.086, val_acc:0.970]
Epoch [62/120    avg_loss:0.078, val_acc:0.970]
Epoch [63/120    avg_loss:0.069, val_acc:0.969]
Epoch [64/120    avg_loss:0.084, val_acc:0.971]
Epoch [65/120    avg_loss:0.063, val_acc:0.971]
Epoch [66/120    avg_loss:0.079, val_acc:0.972]
Epoch [67/120    avg_loss:0.072, val_acc:0.974]
Epoch [68/120    avg_loss:0.080, val_acc:0.970]
Epoch [69/120    avg_loss:0.069, val_acc:0.970]
Epoch [70/120    avg_loss:0.067, val_acc:0.975]
Epoch [71/120    avg_loss:0.066, val_acc:0.970]
Epoch [72/120    avg_loss:0.063, val_acc:0.972]
Epoch [73/120    avg_loss:0.062, val_acc:0.973]
Epoch [74/120    avg_loss:0.060, val_acc:0.974]
Epoch [75/120    avg_loss:0.064, val_acc:0.975]
Epoch [76/120    avg_loss:0.059, val_acc:0.973]
Epoch [77/120    avg_loss:0.058, val_acc:0.976]
Epoch [78/120    avg_loss:0.058, val_acc:0.975]
Epoch [79/120    avg_loss:0.060, val_acc:0.975]
Epoch [80/120    avg_loss:0.059, val_acc:0.976]
Epoch [81/120    avg_loss:0.060, val_acc:0.976]
Epoch [82/120    avg_loss:0.057, val_acc:0.975]
Epoch [83/120    avg_loss:0.062, val_acc:0.978]
Epoch [84/120    avg_loss:0.062, val_acc:0.977]
Epoch [85/120    avg_loss:0.057, val_acc:0.974]
Epoch [86/120    avg_loss:0.056, val_acc:0.974]
Epoch [87/120    avg_loss:0.057, val_acc:0.978]
Epoch [88/120    avg_loss:0.049, val_acc:0.980]
Epoch [89/120    avg_loss:0.052, val_acc:0.976]
Epoch [90/120    avg_loss:0.058, val_acc:0.979]
Epoch [91/120    avg_loss:0.055, val_acc:0.973]
Epoch [92/120    avg_loss:0.054, val_acc:0.978]
Epoch [93/120    avg_loss:0.066, val_acc:0.978]
Epoch [94/120    avg_loss:0.062, val_acc:0.976]
Epoch [95/120    avg_loss:0.062, val_acc:0.979]
Epoch [96/120    avg_loss:0.052, val_acc:0.978]
Epoch [97/120    avg_loss:0.054, val_acc:0.980]
Epoch [98/120    avg_loss:0.045, val_acc:0.979]
Epoch [99/120    avg_loss:0.044, val_acc:0.979]
Epoch [100/120    avg_loss:0.050, val_acc:0.980]
Epoch [101/120    avg_loss:0.046, val_acc:0.976]
Epoch [102/120    avg_loss:0.044, val_acc:0.979]
Epoch [103/120    avg_loss:0.046, val_acc:0.978]
Epoch [104/120    avg_loss:0.046, val_acc:0.979]
Epoch [105/120    avg_loss:0.046, val_acc:0.979]
Epoch [106/120    avg_loss:0.044, val_acc:0.979]
Epoch [107/120    avg_loss:0.046, val_acc:0.979]
Epoch [108/120    avg_loss:0.040, val_acc:0.979]
Epoch [109/120    avg_loss:0.047, val_acc:0.981]
Epoch [110/120    avg_loss:0.047, val_acc:0.979]
Epoch [111/120    avg_loss:0.041, val_acc:0.979]
Epoch [112/120    avg_loss:0.041, val_acc:0.979]
Epoch [113/120    avg_loss:0.046, val_acc:0.980]
Epoch [114/120    avg_loss:0.041, val_acc:0.979]
Epoch [115/120    avg_loss:0.040, val_acc:0.980]
Epoch [116/120    avg_loss:0.034, val_acc:0.978]
Epoch [117/120    avg_loss:0.041, val_acc:0.979]
Epoch [118/120    avg_loss:0.043, val_acc:0.979]
Epoch [119/120    avg_loss:0.039, val_acc:0.979]
Epoch [120/120    avg_loss:0.036, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6292     0     0     9     0     0    73    41    17]
 [    0     0 18035     0     2     0    49     0     4     0]
 [    0     0     0  1954     0     0     0     0    80     2]
 [    0    23     6     0  2912    14     5     0    10     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    31     1     0     0  4832     0    14     0]
 [    0    11     0     0     0     0     0  1279     0     0]
 [    0    25     0    59    29     0     0     0  3458     0]
 [    0     0     0     0     1    17     0     0     0   901]]

Accuracy:
98.73472633938255

F1 scores:
[       nan 0.98443245 0.99745589 0.96493827 0.98295359 0.98826202
 0.9897583  0.9682059  0.96349958 0.97881586]

Kappa:
0.983242834295991
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91a944bc18>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.150, val_acc:0.076]
Epoch [2/120    avg_loss:1.953, val_acc:0.102]
Epoch [3/120    avg_loss:1.771, val_acc:0.200]
Epoch [4/120    avg_loss:1.607, val_acc:0.234]
Epoch [5/120    avg_loss:1.491, val_acc:0.308]
Epoch [6/120    avg_loss:1.341, val_acc:0.369]
Epoch [7/120    avg_loss:1.190, val_acc:0.444]
Epoch [8/120    avg_loss:1.102, val_acc:0.466]
Epoch [9/120    avg_loss:0.981, val_acc:0.517]
Epoch [10/120    avg_loss:0.852, val_acc:0.579]
Epoch [11/120    avg_loss:0.736, val_acc:0.670]
Epoch [12/120    avg_loss:0.659, val_acc:0.725]
Epoch [13/120    avg_loss:0.572, val_acc:0.721]
Epoch [14/120    avg_loss:0.520, val_acc:0.760]
Epoch [15/120    avg_loss:0.458, val_acc:0.774]
Epoch [16/120    avg_loss:0.415, val_acc:0.806]
Epoch [17/120    avg_loss:0.395, val_acc:0.819]
Epoch [18/120    avg_loss:0.336, val_acc:0.832]
Epoch [19/120    avg_loss:0.331, val_acc:0.851]
Epoch [20/120    avg_loss:0.297, val_acc:0.867]
Epoch [21/120    avg_loss:0.282, val_acc:0.902]
Epoch [22/120    avg_loss:0.240, val_acc:0.877]
Epoch [23/120    avg_loss:0.237, val_acc:0.844]
Epoch [24/120    avg_loss:0.205, val_acc:0.939]
Epoch [25/120    avg_loss:0.191, val_acc:0.926]
Epoch [26/120    avg_loss:0.171, val_acc:0.931]
Epoch [27/120    avg_loss:0.175, val_acc:0.934]
Epoch [28/120    avg_loss:0.179, val_acc:0.954]
Epoch [29/120    avg_loss:0.184, val_acc:0.939]
Epoch [30/120    avg_loss:0.161, val_acc:0.938]
Epoch [31/120    avg_loss:0.129, val_acc:0.965]
Epoch [32/120    avg_loss:0.134, val_acc:0.948]
Epoch [33/120    avg_loss:0.152, val_acc:0.955]
Epoch [34/120    avg_loss:0.106, val_acc:0.950]
Epoch [35/120    avg_loss:0.085, val_acc:0.971]
Epoch [36/120    avg_loss:0.075, val_acc:0.960]
Epoch [37/120    avg_loss:0.138, val_acc:0.939]
Epoch [38/120    avg_loss:0.108, val_acc:0.935]
Epoch [39/120    avg_loss:0.096, val_acc:0.965]
Epoch [40/120    avg_loss:0.082, val_acc:0.970]
Epoch [41/120    avg_loss:0.077, val_acc:0.969]
Epoch [42/120    avg_loss:0.069, val_acc:0.957]
Epoch [43/120    avg_loss:0.065, val_acc:0.948]
Epoch [44/120    avg_loss:0.126, val_acc:0.944]
Epoch [45/120    avg_loss:0.066, val_acc:0.976]
Epoch [46/120    avg_loss:0.045, val_acc:0.976]
Epoch [47/120    avg_loss:0.072, val_acc:0.966]
Epoch [48/120    avg_loss:0.075, val_acc:0.967]
Epoch [49/120    avg_loss:0.053, val_acc:0.967]
Epoch [50/120    avg_loss:0.050, val_acc:0.979]
Epoch [51/120    avg_loss:0.038, val_acc:0.972]
Epoch [52/120    avg_loss:0.058, val_acc:0.979]
Epoch [53/120    avg_loss:0.053, val_acc:0.962]
Epoch [54/120    avg_loss:0.051, val_acc:0.982]
Epoch [55/120    avg_loss:0.034, val_acc:0.983]
Epoch [56/120    avg_loss:0.023, val_acc:0.983]
Epoch [57/120    avg_loss:0.031, val_acc:0.970]
Epoch [58/120    avg_loss:0.023, val_acc:0.972]
Epoch [59/120    avg_loss:0.028, val_acc:0.980]
Epoch [60/120    avg_loss:0.033, val_acc:0.958]
Epoch [61/120    avg_loss:0.023, val_acc:0.980]
Epoch [62/120    avg_loss:0.029, val_acc:0.985]
Epoch [63/120    avg_loss:0.058, val_acc:0.977]
Epoch [64/120    avg_loss:0.032, val_acc:0.975]
Epoch [65/120    avg_loss:0.022, val_acc:0.983]
Epoch [66/120    avg_loss:0.023, val_acc:0.978]
Epoch [67/120    avg_loss:0.021, val_acc:0.982]
Epoch [68/120    avg_loss:0.015, val_acc:0.985]
Epoch [69/120    avg_loss:0.016, val_acc:0.980]
Epoch [70/120    avg_loss:0.016, val_acc:0.986]
Epoch [71/120    avg_loss:0.018, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.982]
Epoch [73/120    avg_loss:0.020, val_acc:0.981]
Epoch [74/120    avg_loss:0.017, val_acc:0.985]
Epoch [75/120    avg_loss:0.016, val_acc:0.986]
Epoch [76/120    avg_loss:0.038, val_acc:0.980]
Epoch [77/120    avg_loss:0.021, val_acc:0.982]
Epoch [78/120    avg_loss:0.031, val_acc:0.980]
Epoch [79/120    avg_loss:0.025, val_acc:0.981]
Epoch [80/120    avg_loss:0.023, val_acc:0.987]
Epoch [81/120    avg_loss:0.029, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.987]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.016, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.016, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6337     0     0     0     0     0     0    86     9]
 [    0     0 18049     0    29     0    12     0     0     0]
 [    0     7     0  1926     0     0     0     0    99     4]
 [    0    23    16     0  2910     0     9     2    11     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     3     0     0  4866     0     0     0]
 [    0     7     0     0     0     0     0  1283     0     0]
 [    0    11     0    66    39     0     0     0  3455     0]
 [    0     0     0     0    14    16     0     0     0   889]]

Accuracy:
98.86004868291037

F1 scores:
[       nan 0.98884294 0.99817498 0.95559415 0.97585513 0.99390708
 0.99662058 0.99650485 0.95679867 0.97585071]

Kappa:
0.9848982860886286
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38a4328b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.214, val_acc:0.113]
Epoch [2/120    avg_loss:2.023, val_acc:0.100]
Epoch [3/120    avg_loss:1.873, val_acc:0.113]
Epoch [4/120    avg_loss:1.726, val_acc:0.133]
Epoch [5/120    avg_loss:1.596, val_acc:0.146]
Epoch [6/120    avg_loss:1.463, val_acc:0.189]
Epoch [7/120    avg_loss:1.334, val_acc:0.273]
Epoch [8/120    avg_loss:1.239, val_acc:0.326]
Epoch [9/120    avg_loss:1.174, val_acc:0.359]
Epoch [10/120    avg_loss:1.112, val_acc:0.445]
Epoch [11/120    avg_loss:1.037, val_acc:0.468]
Epoch [12/120    avg_loss:0.912, val_acc:0.512]
Epoch [13/120    avg_loss:0.820, val_acc:0.577]
Epoch [14/120    avg_loss:0.717, val_acc:0.596]
Epoch [15/120    avg_loss:0.648, val_acc:0.632]
Epoch [16/120    avg_loss:0.572, val_acc:0.633]
Epoch [17/120    avg_loss:0.560, val_acc:0.668]
Epoch [18/120    avg_loss:0.471, val_acc:0.738]
Epoch [19/120    avg_loss:0.421, val_acc:0.700]
Epoch [20/120    avg_loss:0.397, val_acc:0.715]
Epoch [21/120    avg_loss:0.357, val_acc:0.797]
Epoch [22/120    avg_loss:0.308, val_acc:0.765]
Epoch [23/120    avg_loss:0.313, val_acc:0.848]
Epoch [24/120    avg_loss:0.277, val_acc:0.826]
Epoch [25/120    avg_loss:0.272, val_acc:0.818]
Epoch [26/120    avg_loss:0.246, val_acc:0.869]
Epoch [27/120    avg_loss:0.227, val_acc:0.914]
Epoch [28/120    avg_loss:0.202, val_acc:0.923]
Epoch [29/120    avg_loss:0.209, val_acc:0.878]
Epoch [30/120    avg_loss:0.189, val_acc:0.913]
Epoch [31/120    avg_loss:0.156, val_acc:0.919]
Epoch [32/120    avg_loss:0.159, val_acc:0.899]
Epoch [33/120    avg_loss:0.141, val_acc:0.932]
Epoch [34/120    avg_loss:0.128, val_acc:0.896]
Epoch [35/120    avg_loss:0.171, val_acc:0.881]
Epoch [36/120    avg_loss:0.147, val_acc:0.933]
Epoch [37/120    avg_loss:0.120, val_acc:0.941]
Epoch [38/120    avg_loss:0.127, val_acc:0.952]
Epoch [39/120    avg_loss:0.100, val_acc:0.924]
Epoch [40/120    avg_loss:0.142, val_acc:0.924]
Epoch [41/120    avg_loss:0.144, val_acc:0.951]
Epoch [42/120    avg_loss:0.096, val_acc:0.959]
Epoch [43/120    avg_loss:0.096, val_acc:0.956]
Epoch [44/120    avg_loss:0.079, val_acc:0.957]
Epoch [45/120    avg_loss:0.072, val_acc:0.942]
Epoch [46/120    avg_loss:0.064, val_acc:0.950]
Epoch [47/120    avg_loss:0.082, val_acc:0.961]
Epoch [48/120    avg_loss:0.053, val_acc:0.974]
Epoch [49/120    avg_loss:0.054, val_acc:0.970]
Epoch [50/120    avg_loss:0.075, val_acc:0.962]
Epoch [51/120    avg_loss:0.076, val_acc:0.942]
Epoch [52/120    avg_loss:0.052, val_acc:0.953]
Epoch [53/120    avg_loss:0.069, val_acc:0.942]
Epoch [54/120    avg_loss:0.070, val_acc:0.924]
Epoch [55/120    avg_loss:0.056, val_acc:0.951]
Epoch [56/120    avg_loss:0.167, val_acc:0.939]
Epoch [57/120    avg_loss:0.118, val_acc:0.924]
Epoch [58/120    avg_loss:0.085, val_acc:0.962]
Epoch [59/120    avg_loss:0.059, val_acc:0.968]
Epoch [60/120    avg_loss:0.065, val_acc:0.961]
Epoch [61/120    avg_loss:0.047, val_acc:0.970]
Epoch [62/120    avg_loss:0.036, val_acc:0.971]
Epoch [63/120    avg_loss:0.037, val_acc:0.974]
Epoch [64/120    avg_loss:0.030, val_acc:0.975]
Epoch [65/120    avg_loss:0.028, val_acc:0.975]
Epoch [66/120    avg_loss:0.032, val_acc:0.976]
Epoch [67/120    avg_loss:0.027, val_acc:0.976]
Epoch [68/120    avg_loss:0.032, val_acc:0.976]
Epoch [69/120    avg_loss:0.028, val_acc:0.978]
Epoch [70/120    avg_loss:0.033, val_acc:0.977]
Epoch [71/120    avg_loss:0.026, val_acc:0.976]
Epoch [72/120    avg_loss:0.035, val_acc:0.977]
Epoch [73/120    avg_loss:0.026, val_acc:0.977]
Epoch [74/120    avg_loss:0.027, val_acc:0.979]
Epoch [75/120    avg_loss:0.024, val_acc:0.979]
Epoch [76/120    avg_loss:0.026, val_acc:0.979]
Epoch [77/120    avg_loss:0.025, val_acc:0.977]
Epoch [78/120    avg_loss:0.027, val_acc:0.979]
Epoch [79/120    avg_loss:0.027, val_acc:0.978]
Epoch [80/120    avg_loss:0.021, val_acc:0.978]
Epoch [81/120    avg_loss:0.025, val_acc:0.979]
Epoch [82/120    avg_loss:0.020, val_acc:0.979]
Epoch [83/120    avg_loss:0.022, val_acc:0.979]
Epoch [84/120    avg_loss:0.023, val_acc:0.979]
Epoch [85/120    avg_loss:0.026, val_acc:0.978]
Epoch [86/120    avg_loss:0.022, val_acc:0.979]
Epoch [87/120    avg_loss:0.020, val_acc:0.980]
Epoch [88/120    avg_loss:0.023, val_acc:0.980]
Epoch [89/120    avg_loss:0.023, val_acc:0.979]
Epoch [90/120    avg_loss:0.019, val_acc:0.980]
Epoch [91/120    avg_loss:0.020, val_acc:0.979]
Epoch [92/120    avg_loss:0.021, val_acc:0.980]
Epoch [93/120    avg_loss:0.020, val_acc:0.979]
Epoch [94/120    avg_loss:0.019, val_acc:0.979]
Epoch [95/120    avg_loss:0.017, val_acc:0.979]
Epoch [96/120    avg_loss:0.023, val_acc:0.978]
Epoch [97/120    avg_loss:0.017, val_acc:0.980]
Epoch [98/120    avg_loss:0.017, val_acc:0.979]
Epoch [99/120    avg_loss:0.018, val_acc:0.983]
Epoch [100/120    avg_loss:0.018, val_acc:0.981]
Epoch [101/120    avg_loss:0.019, val_acc:0.980]
Epoch [102/120    avg_loss:0.017, val_acc:0.980]
Epoch [103/120    avg_loss:0.019, val_acc:0.981]
Epoch [104/120    avg_loss:0.019, val_acc:0.979]
Epoch [105/120    avg_loss:0.017, val_acc:0.982]
Epoch [106/120    avg_loss:0.018, val_acc:0.981]
Epoch [107/120    avg_loss:0.018, val_acc:0.980]
Epoch [108/120    avg_loss:0.020, val_acc:0.981]
Epoch [109/120    avg_loss:0.017, val_acc:0.983]
Epoch [110/120    avg_loss:0.017, val_acc:0.981]
Epoch [111/120    avg_loss:0.018, val_acc:0.980]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.017, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.983]
Epoch [115/120    avg_loss:0.015, val_acc:0.983]
Epoch [116/120    avg_loss:0.019, val_acc:0.981]
Epoch [117/120    avg_loss:0.016, val_acc:0.983]
Epoch [118/120    avg_loss:0.015, val_acc:0.979]
Epoch [119/120    avg_loss:0.015, val_acc:0.980]
Epoch [120/120    avg_loss:0.021, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     0     0    62     2]
 [    0     0 17895     0    94     0    95     0     6     0]
 [    0     3     0  1988     0     0     0     0    45     0]
 [    0    18     3     0  2946     0     4     0     1     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     7     0  4847     0     2     0]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    31     0    56    58     0     0     0  3426     0]
 [    0     0     0     0    11    11     0     0     0   897]]

Accuracy:
98.70821584363628

F1 scores:
[       nan 0.99058878 0.99389059 0.9745098  0.96780552 0.99580313
 0.9867671  0.99805825 0.96330662 0.98679868]

Kappa:
0.9829154049608587
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fddef80db38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.182, val_acc:0.121]
Epoch [2/120    avg_loss:1.916, val_acc:0.117]
Epoch [3/120    avg_loss:1.732, val_acc:0.142]
Epoch [4/120    avg_loss:1.595, val_acc:0.153]
Epoch [5/120    avg_loss:1.457, val_acc:0.191]
Epoch [6/120    avg_loss:1.338, val_acc:0.242]
Epoch [7/120    avg_loss:1.264, val_acc:0.322]
Epoch [8/120    avg_loss:1.163, val_acc:0.364]
Epoch [9/120    avg_loss:1.077, val_acc:0.431]
Epoch [10/120    avg_loss:0.961, val_acc:0.487]
Epoch [11/120    avg_loss:0.839, val_acc:0.504]
Epoch [12/120    avg_loss:0.726, val_acc:0.532]
Epoch [13/120    avg_loss:0.638, val_acc:0.530]
Epoch [14/120    avg_loss:0.587, val_acc:0.644]
Epoch [15/120    avg_loss:0.487, val_acc:0.668]
Epoch [16/120    avg_loss:0.448, val_acc:0.692]
Epoch [17/120    avg_loss:0.411, val_acc:0.715]
Epoch [18/120    avg_loss:0.428, val_acc:0.735]
Epoch [19/120    avg_loss:0.395, val_acc:0.764]
Epoch [20/120    avg_loss:0.343, val_acc:0.798]
Epoch [21/120    avg_loss:0.345, val_acc:0.763]
Epoch [22/120    avg_loss:0.331, val_acc:0.777]
Epoch [23/120    avg_loss:0.279, val_acc:0.791]
Epoch [24/120    avg_loss:0.258, val_acc:0.782]
Epoch [25/120    avg_loss:0.254, val_acc:0.854]
Epoch [26/120    avg_loss:0.243, val_acc:0.828]
Epoch [27/120    avg_loss:0.266, val_acc:0.825]
Epoch [28/120    avg_loss:0.233, val_acc:0.802]
Epoch [29/120    avg_loss:0.213, val_acc:0.848]
Epoch [30/120    avg_loss:0.195, val_acc:0.916]
Epoch [31/120    avg_loss:0.209, val_acc:0.911]
Epoch [32/120    avg_loss:1.495, val_acc:0.637]
Epoch [33/120    avg_loss:1.437, val_acc:0.628]
Epoch [34/120    avg_loss:1.305, val_acc:0.585]
Epoch [35/120    avg_loss:1.214, val_acc:0.675]
Epoch [36/120    avg_loss:1.143, val_acc:0.476]
Epoch [37/120    avg_loss:1.086, val_acc:0.631]
Epoch [38/120    avg_loss:1.003, val_acc:0.628]
Epoch [39/120    avg_loss:0.934, val_acc:0.728]
Epoch [40/120    avg_loss:0.922, val_acc:0.704]
Epoch [41/120    avg_loss:0.908, val_acc:0.646]
Epoch [42/120    avg_loss:0.881, val_acc:0.744]
Epoch [43/120    avg_loss:0.825, val_acc:0.688]
Epoch [44/120    avg_loss:0.801, val_acc:0.584]
Epoch [45/120    avg_loss:0.799, val_acc:0.594]
Epoch [46/120    avg_loss:0.793, val_acc:0.618]
Epoch [47/120    avg_loss:0.794, val_acc:0.628]
Epoch [48/120    avg_loss:0.768, val_acc:0.602]
Epoch [49/120    avg_loss:0.784, val_acc:0.587]
Epoch [50/120    avg_loss:0.772, val_acc:0.617]
Epoch [51/120    avg_loss:0.760, val_acc:0.617]
Epoch [52/120    avg_loss:0.773, val_acc:0.628]
Epoch [53/120    avg_loss:0.785, val_acc:0.617]
Epoch [54/120    avg_loss:0.771, val_acc:0.633]
Epoch [55/120    avg_loss:0.755, val_acc:0.632]
Epoch [56/120    avg_loss:0.755, val_acc:0.623]
Epoch [57/120    avg_loss:0.750, val_acc:0.614]
Epoch [58/120    avg_loss:0.748, val_acc:0.628]
Epoch [59/120    avg_loss:0.757, val_acc:0.621]
Epoch [60/120    avg_loss:0.755, val_acc:0.625]
Epoch [61/120    avg_loss:0.733, val_acc:0.617]
Epoch [62/120    avg_loss:0.760, val_acc:0.617]
Epoch [63/120    avg_loss:0.752, val_acc:0.628]
Epoch [64/120    avg_loss:0.765, val_acc:0.623]
Epoch [65/120    avg_loss:0.742, val_acc:0.635]
Epoch [66/120    avg_loss:0.743, val_acc:0.625]
Epoch [67/120    avg_loss:0.744, val_acc:0.622]
Epoch [68/120    avg_loss:0.748, val_acc:0.632]
Epoch [69/120    avg_loss:0.745, val_acc:0.636]
Epoch [70/120    avg_loss:0.738, val_acc:0.633]
Epoch [71/120    avg_loss:0.751, val_acc:0.633]
Epoch [72/120    avg_loss:0.737, val_acc:0.633]
Epoch [73/120    avg_loss:0.759, val_acc:0.633]
Epoch [74/120    avg_loss:0.741, val_acc:0.633]
Epoch [75/120    avg_loss:0.757, val_acc:0.634]
Epoch [76/120    avg_loss:0.742, val_acc:0.634]
Epoch [77/120    avg_loss:0.745, val_acc:0.633]
Epoch [78/120    avg_loss:0.762, val_acc:0.633]
Epoch [79/120    avg_loss:0.760, val_acc:0.630]
Epoch [80/120    avg_loss:0.762, val_acc:0.633]
Epoch [81/120    avg_loss:0.761, val_acc:0.631]
Epoch [82/120    avg_loss:0.747, val_acc:0.630]
Epoch [83/120    avg_loss:0.759, val_acc:0.630]
Epoch [84/120    avg_loss:0.748, val_acc:0.630]
Epoch [85/120    avg_loss:0.740, val_acc:0.631]
Epoch [86/120    avg_loss:0.750, val_acc:0.631]
Epoch [87/120    avg_loss:0.770, val_acc:0.631]
Epoch [88/120    avg_loss:0.757, val_acc:0.631]
Epoch [89/120    avg_loss:0.754, val_acc:0.631]
Epoch [90/120    avg_loss:0.741, val_acc:0.631]
Epoch [91/120    avg_loss:0.741, val_acc:0.630]
Epoch [92/120    avg_loss:0.746, val_acc:0.630]
Epoch [93/120    avg_loss:0.743, val_acc:0.630]
Epoch [94/120    avg_loss:0.757, val_acc:0.631]
Epoch [95/120    avg_loss:0.748, val_acc:0.630]
Epoch [96/120    avg_loss:0.760, val_acc:0.630]
Epoch [97/120    avg_loss:0.754, val_acc:0.630]
Epoch [98/120    avg_loss:0.754, val_acc:0.630]
Epoch [99/120    avg_loss:0.750, val_acc:0.630]
Epoch [100/120    avg_loss:0.751, val_acc:0.630]
Epoch [101/120    avg_loss:0.763, val_acc:0.630]
Epoch [102/120    avg_loss:0.761, val_acc:0.630]
Epoch [103/120    avg_loss:0.744, val_acc:0.630]
Epoch [104/120    avg_loss:0.748, val_acc:0.630]
Epoch [105/120    avg_loss:0.743, val_acc:0.630]
Epoch [106/120    avg_loss:0.744, val_acc:0.630]
Epoch [107/120    avg_loss:0.746, val_acc:0.630]
Epoch [108/120    avg_loss:0.740, val_acc:0.630]
Epoch [109/120    avg_loss:0.738, val_acc:0.630]
Epoch [110/120    avg_loss:0.738, val_acc:0.630]
Epoch [111/120    avg_loss:0.759, val_acc:0.630]
Epoch [112/120    avg_loss:0.729, val_acc:0.630]
Epoch [113/120    avg_loss:0.753, val_acc:0.630]
Epoch [114/120    avg_loss:0.763, val_acc:0.630]
Epoch [115/120    avg_loss:0.754, val_acc:0.630]
Epoch [116/120    avg_loss:0.744, val_acc:0.630]
Epoch [117/120    avg_loss:0.751, val_acc:0.630]
Epoch [118/120    avg_loss:0.745, val_acc:0.630]
Epoch [119/120    avg_loss:0.761, val_acc:0.630]
Epoch [120/120    avg_loss:0.753, val_acc:0.630]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0]
 [   0 4951   61   51  253    8  234  360  476   38]
 [   0    0 8359    0   62    0 9669    0    0    0]
 [   0   56    0 1654    4    0   12    0  295   15]
 [   0   44 1006    0 1721    0   64    3  132    2]
 [   0    0    0    0    0 1305    0    0    0    0]
 [   0    0  858   49  138    0 3725   23   85    0]
 [   0   90    0    0    3    0    7 1185    5    0]
 [   0  107   45  187  215    0  122    3 2891    1]
 [   0   21    0    0    0   84    0    0    0  814]]

Accuracy:
64.11924902995686

F1 scores:
[       nan 0.84625246 0.58826841 0.83178275 0.64120715 0.96595115
 0.39816151 0.82751397 0.77558685 0.91000559]

Kappa:
0.5599630141086657
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f5130cc18>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.168, val_acc:0.539]
Epoch [2/120    avg_loss:2.015, val_acc:0.501]
Epoch [3/120    avg_loss:1.900, val_acc:0.533]
Epoch [4/120    avg_loss:1.753, val_acc:0.550]
Epoch [5/120    avg_loss:1.579, val_acc:0.550]
Epoch [6/120    avg_loss:1.454, val_acc:0.570]
Epoch [7/120    avg_loss:1.383, val_acc:0.592]
Epoch [8/120    avg_loss:1.234, val_acc:0.649]
Epoch [9/120    avg_loss:1.147, val_acc:0.648]
Epoch [10/120    avg_loss:1.082, val_acc:0.671]
Epoch [11/120    avg_loss:1.018, val_acc:0.760]
Epoch [12/120    avg_loss:0.930, val_acc:0.748]
Epoch [13/120    avg_loss:0.870, val_acc:0.801]
Epoch [14/120    avg_loss:0.772, val_acc:0.794]
Epoch [15/120    avg_loss:0.685, val_acc:0.754]
Epoch [16/120    avg_loss:0.612, val_acc:0.704]
Epoch [17/120    avg_loss:0.554, val_acc:0.737]
Epoch [18/120    avg_loss:0.454, val_acc:0.782]
Epoch [19/120    avg_loss:0.416, val_acc:0.830]
Epoch [20/120    avg_loss:0.376, val_acc:0.821]
Epoch [21/120    avg_loss:0.381, val_acc:0.833]
Epoch [22/120    avg_loss:0.327, val_acc:0.885]
Epoch [23/120    avg_loss:0.298, val_acc:0.862]
Epoch [24/120    avg_loss:0.273, val_acc:0.893]
Epoch [25/120    avg_loss:0.263, val_acc:0.874]
Epoch [26/120    avg_loss:0.246, val_acc:0.883]
Epoch [27/120    avg_loss:0.208, val_acc:0.883]
Epoch [28/120    avg_loss:0.190, val_acc:0.876]
Epoch [29/120    avg_loss:0.164, val_acc:0.957]
Epoch [30/120    avg_loss:0.166, val_acc:0.937]
Epoch [31/120    avg_loss:0.158, val_acc:0.947]
Epoch [32/120    avg_loss:0.139, val_acc:0.965]
Epoch [33/120    avg_loss:0.112, val_acc:0.940]
Epoch [34/120    avg_loss:0.123, val_acc:0.954]
Epoch [35/120    avg_loss:0.117, val_acc:0.963]
Epoch [36/120    avg_loss:0.132, val_acc:0.968]
Epoch [37/120    avg_loss:0.105, val_acc:0.943]
Epoch [38/120    avg_loss:0.086, val_acc:0.966]
Epoch [39/120    avg_loss:0.071, val_acc:0.966]
Epoch [40/120    avg_loss:0.072, val_acc:0.978]
Epoch [41/120    avg_loss:0.076, val_acc:0.966]
Epoch [42/120    avg_loss:0.077, val_acc:0.971]
Epoch [43/120    avg_loss:0.057, val_acc:0.971]
Epoch [44/120    avg_loss:0.055, val_acc:0.977]
Epoch [45/120    avg_loss:0.070, val_acc:0.975]
Epoch [46/120    avg_loss:0.056, val_acc:0.976]
Epoch [47/120    avg_loss:0.063, val_acc:0.975]
Epoch [48/120    avg_loss:0.042, val_acc:0.971]
Epoch [49/120    avg_loss:0.044, val_acc:0.971]
Epoch [50/120    avg_loss:0.036, val_acc:0.982]
Epoch [51/120    avg_loss:0.068, val_acc:0.977]
Epoch [52/120    avg_loss:0.052, val_acc:0.968]
Epoch [53/120    avg_loss:0.049, val_acc:0.986]
Epoch [54/120    avg_loss:0.039, val_acc:0.986]
Epoch [55/120    avg_loss:0.028, val_acc:0.989]
Epoch [56/120    avg_loss:0.023, val_acc:0.975]
Epoch [57/120    avg_loss:0.023, val_acc:0.986]
Epoch [58/120    avg_loss:0.039, val_acc:0.970]
Epoch [59/120    avg_loss:0.035, val_acc:0.984]
Epoch [60/120    avg_loss:0.030, val_acc:0.976]
Epoch [61/120    avg_loss:0.029, val_acc:0.984]
Epoch [62/120    avg_loss:0.025, val_acc:0.986]
Epoch [63/120    avg_loss:0.024, val_acc:0.987]
Epoch [64/120    avg_loss:0.028, val_acc:0.981]
Epoch [65/120    avg_loss:0.024, val_acc:0.986]
Epoch [66/120    avg_loss:0.020, val_acc:0.981]
Epoch [67/120    avg_loss:0.025, val_acc:0.985]
Epoch [68/120    avg_loss:0.026, val_acc:0.986]
Epoch [69/120    avg_loss:0.017, val_acc:0.991]
Epoch [70/120    avg_loss:0.013, val_acc:0.991]
Epoch [71/120    avg_loss:0.017, val_acc:0.989]
Epoch [72/120    avg_loss:0.013, val_acc:0.990]
Epoch [73/120    avg_loss:0.013, val_acc:0.991]
Epoch [74/120    avg_loss:0.013, val_acc:0.991]
Epoch [75/120    avg_loss:0.015, val_acc:0.992]
Epoch [76/120    avg_loss:0.013, val_acc:0.989]
Epoch [77/120    avg_loss:0.011, val_acc:0.990]
Epoch [78/120    avg_loss:0.013, val_acc:0.990]
Epoch [79/120    avg_loss:0.016, val_acc:0.992]
Epoch [80/120    avg_loss:0.013, val_acc:0.992]
Epoch [81/120    avg_loss:0.011, val_acc:0.992]
Epoch [82/120    avg_loss:0.012, val_acc:0.990]
Epoch [83/120    avg_loss:0.013, val_acc:0.992]
Epoch [84/120    avg_loss:0.010, val_acc:0.992]
Epoch [85/120    avg_loss:0.017, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.990]
Epoch [88/120    avg_loss:0.012, val_acc:0.991]
Epoch [89/120    avg_loss:0.010, val_acc:0.991]
Epoch [90/120    avg_loss:0.013, val_acc:0.991]
Epoch [91/120    avg_loss:0.011, val_acc:0.990]
Epoch [92/120    avg_loss:0.010, val_acc:0.989]
Epoch [93/120    avg_loss:0.016, val_acc:0.992]
Epoch [94/120    avg_loss:0.011, val_acc:0.988]
Epoch [95/120    avg_loss:0.011, val_acc:0.989]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.989]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.011, val_acc:0.989]
Epoch [100/120    avg_loss:0.012, val_acc:0.989]
Epoch [101/120    avg_loss:0.011, val_acc:0.989]
Epoch [102/120    avg_loss:0.010, val_acc:0.990]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.011, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.990]
Epoch [110/120    avg_loss:0.012, val_acc:0.990]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.012, val_acc:0.990]
Epoch [114/120    avg_loss:0.012, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.011, val_acc:0.990]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.010, val_acc:0.990]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6415     0     0     0     0     0    15     2     0]
 [    0     1 18056     0    19     0    14     0     0     0]
 [    0     0     0  1983     0     0     0     0    48     5]
 [    0    22     3     0  2938     0     3     0     4     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     4     0  4866     0     0     0]
 [    0    14     0     0     0     0     0  1276     0     0]
 [    0    38     0    44    45     0     0     0  3443     1]
 [    0     0     0     0     0     8     0     0     0   911]]

Accuracy:
99.27698647964718

F1 scores:
[       nan 0.99288036 0.99875543 0.97612602 0.98293744 0.99694423
 0.99702899 0.98876404 0.97425014 0.99129489]

Kappa:
0.9904210232501225
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f20b34be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.166, val_acc:0.074]
Epoch [2/120    avg_loss:1.985, val_acc:0.164]
Epoch [3/120    avg_loss:1.862, val_acc:0.231]
Epoch [4/120    avg_loss:1.772, val_acc:0.433]
Epoch [5/120    avg_loss:1.687, val_acc:0.449]
Epoch [6/120    avg_loss:1.541, val_acc:0.486]
Epoch [7/120    avg_loss:1.424, val_acc:0.556]
Epoch [8/120    avg_loss:1.302, val_acc:0.648]
Epoch [9/120    avg_loss:1.185, val_acc:0.639]
Epoch [10/120    avg_loss:1.087, val_acc:0.654]
Epoch [11/120    avg_loss:0.978, val_acc:0.603]
Epoch [12/120    avg_loss:0.845, val_acc:0.592]
Epoch [13/120    avg_loss:0.787, val_acc:0.609]
Epoch [14/120    avg_loss:0.627, val_acc:0.693]
Epoch [15/120    avg_loss:0.523, val_acc:0.739]
Epoch [16/120    avg_loss:0.478, val_acc:0.751]
Epoch [17/120    avg_loss:0.426, val_acc:0.801]
Epoch [18/120    avg_loss:0.388, val_acc:0.793]
Epoch [19/120    avg_loss:0.364, val_acc:0.830]
Epoch [20/120    avg_loss:0.330, val_acc:0.866]
Epoch [21/120    avg_loss:0.312, val_acc:0.834]
Epoch [22/120    avg_loss:0.492, val_acc:0.780]
Epoch [23/120    avg_loss:0.453, val_acc:0.834]
Epoch [24/120    avg_loss:0.357, val_acc:0.874]
Epoch [25/120    avg_loss:0.272, val_acc:0.888]
Epoch [26/120    avg_loss:0.210, val_acc:0.850]
Epoch [27/120    avg_loss:0.183, val_acc:0.956]
Epoch [28/120    avg_loss:0.196, val_acc:0.952]
Epoch [29/120    avg_loss:0.162, val_acc:0.937]
Epoch [30/120    avg_loss:0.150, val_acc:0.969]
Epoch [31/120    avg_loss:0.175, val_acc:0.932]
Epoch [32/120    avg_loss:0.124, val_acc:0.933]
Epoch [33/120    avg_loss:0.127, val_acc:0.964]
Epoch [34/120    avg_loss:0.119, val_acc:0.944]
Epoch [35/120    avg_loss:0.113, val_acc:0.961]
Epoch [36/120    avg_loss:0.103, val_acc:0.979]
Epoch [37/120    avg_loss:0.084, val_acc:0.957]
Epoch [38/120    avg_loss:0.078, val_acc:0.981]
Epoch [39/120    avg_loss:0.063, val_acc:0.977]
Epoch [40/120    avg_loss:0.071, val_acc:0.971]
Epoch [41/120    avg_loss:0.091, val_acc:0.956]
Epoch [42/120    avg_loss:0.119, val_acc:0.964]
Epoch [43/120    avg_loss:0.087, val_acc:0.957]
Epoch [44/120    avg_loss:0.087, val_acc:0.970]
Epoch [45/120    avg_loss:0.061, val_acc:0.964]
Epoch [46/120    avg_loss:0.060, val_acc:0.964]
Epoch [47/120    avg_loss:0.055, val_acc:0.975]
Epoch [48/120    avg_loss:0.110, val_acc:0.970]
Epoch [49/120    avg_loss:0.088, val_acc:0.972]
Epoch [50/120    avg_loss:0.071, val_acc:0.968]
Epoch [51/120    avg_loss:0.057, val_acc:0.979]
Epoch [52/120    avg_loss:0.042, val_acc:0.977]
Epoch [53/120    avg_loss:0.034, val_acc:0.982]
Epoch [54/120    avg_loss:0.036, val_acc:0.980]
Epoch [55/120    avg_loss:0.031, val_acc:0.984]
Epoch [56/120    avg_loss:0.030, val_acc:0.983]
Epoch [57/120    avg_loss:0.034, val_acc:0.982]
Epoch [58/120    avg_loss:0.033, val_acc:0.983]
Epoch [59/120    avg_loss:0.033, val_acc:0.980]
Epoch [60/120    avg_loss:0.026, val_acc:0.983]
Epoch [61/120    avg_loss:0.030, val_acc:0.982]
Epoch [62/120    avg_loss:0.026, val_acc:0.981]
Epoch [63/120    avg_loss:0.026, val_acc:0.982]
Epoch [64/120    avg_loss:0.027, val_acc:0.982]
Epoch [65/120    avg_loss:0.027, val_acc:0.981]
Epoch [66/120    avg_loss:0.027, val_acc:0.982]
Epoch [67/120    avg_loss:0.026, val_acc:0.982]
Epoch [68/120    avg_loss:0.026, val_acc:0.984]
Epoch [69/120    avg_loss:0.027, val_acc:0.980]
Epoch [70/120    avg_loss:0.021, val_acc:0.981]
Epoch [71/120    avg_loss:0.024, val_acc:0.981]
Epoch [72/120    avg_loss:0.028, val_acc:0.981]
Epoch [73/120    avg_loss:0.027, val_acc:0.981]
Epoch [74/120    avg_loss:0.027, val_acc:0.982]
Epoch [75/120    avg_loss:0.025, val_acc:0.981]
Epoch [76/120    avg_loss:0.024, val_acc:0.982]
Epoch [77/120    avg_loss:0.022, val_acc:0.982]
Epoch [78/120    avg_loss:0.024, val_acc:0.981]
Epoch [79/120    avg_loss:0.032, val_acc:0.981]
Epoch [80/120    avg_loss:0.030, val_acc:0.982]
Epoch [81/120    avg_loss:0.026, val_acc:0.982]
Epoch [82/120    avg_loss:0.022, val_acc:0.982]
Epoch [83/120    avg_loss:0.023, val_acc:0.983]
Epoch [84/120    avg_loss:0.023, val_acc:0.983]
Epoch [85/120    avg_loss:0.024, val_acc:0.983]
Epoch [86/120    avg_loss:0.021, val_acc:0.983]
Epoch [87/120    avg_loss:0.024, val_acc:0.983]
Epoch [88/120    avg_loss:0.022, val_acc:0.983]
Epoch [89/120    avg_loss:0.020, val_acc:0.983]
Epoch [90/120    avg_loss:0.025, val_acc:0.983]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.021, val_acc:0.983]
Epoch [93/120    avg_loss:0.029, val_acc:0.982]
Epoch [94/120    avg_loss:0.025, val_acc:0.983]
Epoch [95/120    avg_loss:0.027, val_acc:0.983]
Epoch [96/120    avg_loss:0.026, val_acc:0.983]
Epoch [97/120    avg_loss:0.023, val_acc:0.983]
Epoch [98/120    avg_loss:0.022, val_acc:0.983]
Epoch [99/120    avg_loss:0.021, val_acc:0.983]
Epoch [100/120    avg_loss:0.024, val_acc:0.983]
Epoch [101/120    avg_loss:0.024, val_acc:0.983]
Epoch [102/120    avg_loss:0.026, val_acc:0.983]
Epoch [103/120    avg_loss:0.023, val_acc:0.983]
Epoch [104/120    avg_loss:0.023, val_acc:0.983]
Epoch [105/120    avg_loss:0.025, val_acc:0.983]
Epoch [106/120    avg_loss:0.025, val_acc:0.983]
Epoch [107/120    avg_loss:0.019, val_acc:0.983]
Epoch [108/120    avg_loss:0.023, val_acc:0.983]
Epoch [109/120    avg_loss:0.025, val_acc:0.983]
Epoch [110/120    avg_loss:0.024, val_acc:0.983]
Epoch [111/120    avg_loss:0.025, val_acc:0.983]
Epoch [112/120    avg_loss:0.021, val_acc:0.983]
Epoch [113/120    avg_loss:0.023, val_acc:0.983]
Epoch [114/120    avg_loss:0.022, val_acc:0.983]
Epoch [115/120    avg_loss:0.026, val_acc:0.983]
Epoch [116/120    avg_loss:0.023, val_acc:0.983]
Epoch [117/120    avg_loss:0.026, val_acc:0.983]
Epoch [118/120    avg_loss:0.022, val_acc:0.983]
Epoch [119/120    avg_loss:0.024, val_acc:0.983]
Epoch [120/120    avg_loss:0.025, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     0     0     0     2    24     3]
 [    0     0 18057     0    21     0    10     0     2     0]
 [    0     2     0  1938     0     0     0     0    94     2]
 [    0    32    15     2  2911     0     5     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     7     0     0  4850     0     7     0]
 [    0    13     0     0     0     0     3  1272     0     2]
 [    0    26     0    40    63     0     0     0  3442     0]
 [    0     0     0     0    14    35     0     0     1   869]]

Accuracy:
98.92511989974213

F1 scores:
[       nan 0.99209792 0.99828616 0.9634601  0.97341582 0.98676749
 0.99528011 0.99219969 0.96306659 0.96824513]

Kappa:
0.9857552268359144
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc974425be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.184, val_acc:0.079]
Epoch [2/120    avg_loss:2.028, val_acc:0.070]
Epoch [3/120    avg_loss:1.923, val_acc:0.091]
Epoch [4/120    avg_loss:1.811, val_acc:0.158]
Epoch [5/120    avg_loss:1.707, val_acc:0.227]
Epoch [6/120    avg_loss:1.595, val_acc:0.285]
Epoch [7/120    avg_loss:1.459, val_acc:0.352]
Epoch [8/120    avg_loss:1.354, val_acc:0.390]
Epoch [9/120    avg_loss:1.201, val_acc:0.399]
Epoch [10/120    avg_loss:1.121, val_acc:0.441]
Epoch [11/120    avg_loss:0.997, val_acc:0.489]
Epoch [12/120    avg_loss:0.859, val_acc:0.638]
Epoch [13/120    avg_loss:0.779, val_acc:0.603]
Epoch [14/120    avg_loss:0.678, val_acc:0.715]
Epoch [15/120    avg_loss:0.602, val_acc:0.748]
Epoch [16/120    avg_loss:0.579, val_acc:0.753]
Epoch [17/120    avg_loss:0.542, val_acc:0.788]
Epoch [18/120    avg_loss:0.469, val_acc:0.749]
Epoch [19/120    avg_loss:0.441, val_acc:0.819]
Epoch [20/120    avg_loss:0.382, val_acc:0.815]
Epoch [21/120    avg_loss:0.348, val_acc:0.881]
Epoch [22/120    avg_loss:0.327, val_acc:0.877]
Epoch [23/120    avg_loss:0.290, val_acc:0.862]
Epoch [24/120    avg_loss:0.255, val_acc:0.878]
Epoch [25/120    avg_loss:0.232, val_acc:0.923]
Epoch [26/120    avg_loss:0.217, val_acc:0.928]
Epoch [27/120    avg_loss:0.229, val_acc:0.932]
Epoch [28/120    avg_loss:0.222, val_acc:0.938]
Epoch [29/120    avg_loss:0.184, val_acc:0.931]
Epoch [30/120    avg_loss:0.163, val_acc:0.955]
Epoch [31/120    avg_loss:0.124, val_acc:0.943]
Epoch [32/120    avg_loss:0.116, val_acc:0.950]
Epoch [33/120    avg_loss:0.134, val_acc:0.942]
Epoch [34/120    avg_loss:0.101, val_acc:0.965]
Epoch [35/120    avg_loss:0.084, val_acc:0.979]
Epoch [36/120    avg_loss:0.092, val_acc:0.966]
Epoch [37/120    avg_loss:0.089, val_acc:0.954]
Epoch [38/120    avg_loss:0.083, val_acc:0.963]
Epoch [39/120    avg_loss:0.080, val_acc:0.967]
Epoch [40/120    avg_loss:0.110, val_acc:0.972]
Epoch [41/120    avg_loss:0.074, val_acc:0.976]
Epoch [42/120    avg_loss:0.058, val_acc:0.973]
Epoch [43/120    avg_loss:0.051, val_acc:0.970]
Epoch [44/120    avg_loss:0.071, val_acc:0.965]
Epoch [45/120    avg_loss:0.073, val_acc:0.976]
Epoch [46/120    avg_loss:0.059, val_acc:0.976]
Epoch [47/120    avg_loss:0.054, val_acc:0.978]
Epoch [48/120    avg_loss:0.047, val_acc:0.976]
Epoch [49/120    avg_loss:0.035, val_acc:0.980]
Epoch [50/120    avg_loss:0.030, val_acc:0.982]
Epoch [51/120    avg_loss:0.028, val_acc:0.981]
Epoch [52/120    avg_loss:0.024, val_acc:0.982]
Epoch [53/120    avg_loss:0.027, val_acc:0.982]
Epoch [54/120    avg_loss:0.030, val_acc:0.982]
Epoch [55/120    avg_loss:0.027, val_acc:0.985]
Epoch [56/120    avg_loss:0.027, val_acc:0.984]
Epoch [57/120    avg_loss:0.027, val_acc:0.981]
Epoch [58/120    avg_loss:0.026, val_acc:0.983]
Epoch [59/120    avg_loss:0.029, val_acc:0.984]
Epoch [60/120    avg_loss:0.023, val_acc:0.986]
Epoch [61/120    avg_loss:0.027, val_acc:0.982]
Epoch [62/120    avg_loss:0.026, val_acc:0.984]
Epoch [63/120    avg_loss:0.024, val_acc:0.986]
Epoch [64/120    avg_loss:0.027, val_acc:0.984]
Epoch [65/120    avg_loss:0.024, val_acc:0.985]
Epoch [66/120    avg_loss:0.026, val_acc:0.985]
Epoch [67/120    avg_loss:0.023, val_acc:0.985]
Epoch [68/120    avg_loss:0.023, val_acc:0.982]
Epoch [69/120    avg_loss:0.026, val_acc:0.986]
Epoch [70/120    avg_loss:0.023, val_acc:0.985]
Epoch [71/120    avg_loss:0.022, val_acc:0.987]
Epoch [72/120    avg_loss:0.024, val_acc:0.986]
Epoch [73/120    avg_loss:0.022, val_acc:0.985]
Epoch [74/120    avg_loss:0.022, val_acc:0.984]
Epoch [75/120    avg_loss:0.022, val_acc:0.985]
Epoch [76/120    avg_loss:0.026, val_acc:0.984]
Epoch [77/120    avg_loss:0.023, val_acc:0.986]
Epoch [78/120    avg_loss:0.020, val_acc:0.985]
Epoch [79/120    avg_loss:0.020, val_acc:0.984]
Epoch [80/120    avg_loss:0.022, val_acc:0.985]
Epoch [81/120    avg_loss:0.027, val_acc:0.986]
Epoch [82/120    avg_loss:0.022, val_acc:0.984]
Epoch [83/120    avg_loss:0.021, val_acc:0.985]
Epoch [84/120    avg_loss:0.019, val_acc:0.986]
Epoch [85/120    avg_loss:0.019, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.986]
Epoch [87/120    avg_loss:0.018, val_acc:0.986]
Epoch [88/120    avg_loss:0.023, val_acc:0.986]
Epoch [89/120    avg_loss:0.021, val_acc:0.986]
Epoch [90/120    avg_loss:0.021, val_acc:0.986]
Epoch [91/120    avg_loss:0.018, val_acc:0.986]
Epoch [92/120    avg_loss:0.021, val_acc:0.986]
Epoch [93/120    avg_loss:0.022, val_acc:0.986]
Epoch [94/120    avg_loss:0.022, val_acc:0.986]
Epoch [95/120    avg_loss:0.020, val_acc:0.986]
Epoch [96/120    avg_loss:0.021, val_acc:0.986]
Epoch [97/120    avg_loss:0.020, val_acc:0.986]
Epoch [98/120    avg_loss:0.018, val_acc:0.986]
Epoch [99/120    avg_loss:0.019, val_acc:0.986]
Epoch [100/120    avg_loss:0.021, val_acc:0.986]
Epoch [101/120    avg_loss:0.022, val_acc:0.986]
Epoch [102/120    avg_loss:0.021, val_acc:0.986]
Epoch [103/120    avg_loss:0.019, val_acc:0.986]
Epoch [104/120    avg_loss:0.020, val_acc:0.986]
Epoch [105/120    avg_loss:0.023, val_acc:0.986]
Epoch [106/120    avg_loss:0.023, val_acc:0.986]
Epoch [107/120    avg_loss:0.020, val_acc:0.986]
Epoch [108/120    avg_loss:0.020, val_acc:0.986]
Epoch [109/120    avg_loss:0.023, val_acc:0.986]
Epoch [110/120    avg_loss:0.020, val_acc:0.986]
Epoch [111/120    avg_loss:0.022, val_acc:0.986]
Epoch [112/120    avg_loss:0.025, val_acc:0.986]
Epoch [113/120    avg_loss:0.021, val_acc:0.986]
Epoch [114/120    avg_loss:0.021, val_acc:0.986]
Epoch [115/120    avg_loss:0.025, val_acc:0.986]
Epoch [116/120    avg_loss:0.019, val_acc:0.986]
Epoch [117/120    avg_loss:0.018, val_acc:0.986]
Epoch [118/120    avg_loss:0.021, val_acc:0.986]
Epoch [119/120    avg_loss:0.019, val_acc:0.986]
Epoch [120/120    avg_loss:0.022, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     1     0     0     6    22    10]
 [    0     8 17984     0    78     0    18     0     2     0]
 [    0     9     0  2000     0     0     0     0    25     2]
 [    0    42    17     0  2881     0     4     0    26     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30    12     0     0  4834     0     2     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    13     0    46    48     0     0     0  3432    32]
 [    0     2     0     1    14    62     0     0     0   840]]

Accuracy:
98.71303593377196

F1 scores:
[       nan 0.99123963 0.99576424 0.97680098 0.96129463 0.97679641
 0.99321964 0.99767981 0.96949153 0.93074792]

Kappa:
0.9829589776266112
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f02a36ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.249, val_acc:0.472]
Epoch [2/120    avg_loss:2.035, val_acc:0.370]
Epoch [3/120    avg_loss:1.863, val_acc:0.466]
Epoch [4/120    avg_loss:1.707, val_acc:0.516]
Epoch [5/120    avg_loss:1.578, val_acc:0.401]
Epoch [6/120    avg_loss:1.424, val_acc:0.447]
Epoch [7/120    avg_loss:1.307, val_acc:0.470]
Epoch [8/120    avg_loss:1.194, val_acc:0.506]
Epoch [9/120    avg_loss:1.099, val_acc:0.548]
Epoch [10/120    avg_loss:0.962, val_acc:0.741]
Epoch [11/120    avg_loss:0.854, val_acc:0.703]
Epoch [12/120    avg_loss:0.722, val_acc:0.741]
Epoch [13/120    avg_loss:0.646, val_acc:0.728]
Epoch [14/120    avg_loss:0.575, val_acc:0.730]
Epoch [15/120    avg_loss:0.493, val_acc:0.796]
Epoch [16/120    avg_loss:0.467, val_acc:0.788]
Epoch [17/120    avg_loss:0.417, val_acc:0.795]
Epoch [18/120    avg_loss:0.376, val_acc:0.811]
Epoch [19/120    avg_loss:0.357, val_acc:0.848]
Epoch [20/120    avg_loss:0.304, val_acc:0.910]
Epoch [21/120    avg_loss:0.330, val_acc:0.825]
Epoch [22/120    avg_loss:0.306, val_acc:0.889]
Epoch [23/120    avg_loss:0.250, val_acc:0.889]
Epoch [24/120    avg_loss:0.206, val_acc:0.903]
Epoch [25/120    avg_loss:0.182, val_acc:0.921]
Epoch [26/120    avg_loss:0.180, val_acc:0.887]
Epoch [27/120    avg_loss:0.177, val_acc:0.934]
Epoch [28/120    avg_loss:0.149, val_acc:0.949]
Epoch [29/120    avg_loss:0.132, val_acc:0.953]
Epoch [30/120    avg_loss:0.154, val_acc:0.909]
Epoch [31/120    avg_loss:0.139, val_acc:0.954]
Epoch [32/120    avg_loss:0.144, val_acc:0.948]
Epoch [33/120    avg_loss:0.112, val_acc:0.954]
Epoch [34/120    avg_loss:0.099, val_acc:0.962]
Epoch [35/120    avg_loss:0.089, val_acc:0.960]
Epoch [36/120    avg_loss:0.097, val_acc:0.961]
Epoch [37/120    avg_loss:0.094, val_acc:0.951]
Epoch [38/120    avg_loss:0.090, val_acc:0.913]
Epoch [39/120    avg_loss:0.084, val_acc:0.940]
Epoch [40/120    avg_loss:0.061, val_acc:0.968]
Epoch [41/120    avg_loss:0.056, val_acc:0.973]
Epoch [42/120    avg_loss:0.048, val_acc:0.966]
Epoch [43/120    avg_loss:0.051, val_acc:0.976]
Epoch [44/120    avg_loss:0.046, val_acc:0.976]
Epoch [45/120    avg_loss:0.040, val_acc:0.981]
Epoch [46/120    avg_loss:0.044, val_acc:0.979]
Epoch [47/120    avg_loss:0.038, val_acc:0.967]
Epoch [48/120    avg_loss:0.033, val_acc:0.975]
Epoch [49/120    avg_loss:0.035, val_acc:0.966]
Epoch [50/120    avg_loss:0.045, val_acc:0.973]
Epoch [51/120    avg_loss:0.033, val_acc:0.980]
Epoch [52/120    avg_loss:0.035, val_acc:0.927]
Epoch [53/120    avg_loss:0.036, val_acc:0.982]
Epoch [54/120    avg_loss:0.030, val_acc:0.983]
Epoch [55/120    avg_loss:0.037, val_acc:0.969]
Epoch [56/120    avg_loss:0.024, val_acc:0.981]
Epoch [57/120    avg_loss:0.022, val_acc:0.981]
Epoch [58/120    avg_loss:0.023, val_acc:0.983]
Epoch [59/120    avg_loss:0.022, val_acc:0.971]
Epoch [60/120    avg_loss:0.029, val_acc:0.966]
Epoch [61/120    avg_loss:0.026, val_acc:0.977]
Epoch [62/120    avg_loss:0.031, val_acc:0.979]
Epoch [63/120    avg_loss:0.036, val_acc:0.981]
Epoch [64/120    avg_loss:0.032, val_acc:0.982]
Epoch [65/120    avg_loss:0.022, val_acc:0.976]
Epoch [66/120    avg_loss:0.044, val_acc:0.973]
Epoch [67/120    avg_loss:0.054, val_acc:0.972]
Epoch [68/120    avg_loss:0.038, val_acc:0.972]
Epoch [69/120    avg_loss:0.035, val_acc:0.975]
Epoch [70/120    avg_loss:0.036, val_acc:0.919]
Epoch [71/120    avg_loss:0.035, val_acc:0.972]
Epoch [72/120    avg_loss:0.020, val_acc:0.977]
Epoch [73/120    avg_loss:0.021, val_acc:0.981]
Epoch [74/120    avg_loss:0.019, val_acc:0.982]
Epoch [75/120    avg_loss:0.017, val_acc:0.982]
Epoch [76/120    avg_loss:0.016, val_acc:0.981]
Epoch [77/120    avg_loss:0.014, val_acc:0.981]
Epoch [78/120    avg_loss:0.015, val_acc:0.981]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.016, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.981]
Epoch [83/120    avg_loss:0.013, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.982]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.015, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.011, val_acc:0.982]
Epoch [93/120    avg_loss:0.011, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.021, val_acc:0.982]
Epoch [97/120    avg_loss:0.011, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.982]
Epoch [99/120    avg_loss:0.013, val_acc:0.982]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.014, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.015, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.013, val_acc:0.982]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6421     0     0     0     0     0     2     7     2]
 [    0     1 18058     0    27     0     4     0     0     0]
 [    0     8     0  1982     0     0     0     0    43     3]
 [    0    27    14     0  2888     0    12     0    29     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     1     0     0  4851     0    12     0]
 [    0     4     0     0     0     0     0  1278     0     8]
 [    0    18     0    27    49     0     0     0  3477     0]
 [    0     0     0     1    14    37     0     0     0   867]]

Accuracy:
99.11792350516954

F1 scores:
[       nan 0.99465572 0.99834144 0.97949098 0.9707563  0.98602191
 0.99558748 0.99455253 0.97408601 0.96279845]

Kappa:
0.9883107491206912
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50f593ab70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.221, val_acc:0.324]
Epoch [2/120    avg_loss:2.004, val_acc:0.285]
Epoch [3/120    avg_loss:1.861, val_acc:0.269]
Epoch [4/120    avg_loss:1.760, val_acc:0.318]
Epoch [5/120    avg_loss:1.638, val_acc:0.367]
Epoch [6/120    avg_loss:1.519, val_acc:0.540]
Epoch [7/120    avg_loss:1.358, val_acc:0.611]
Epoch [8/120    avg_loss:1.212, val_acc:0.589]
Epoch [9/120    avg_loss:1.135, val_acc:0.704]
Epoch [10/120    avg_loss:1.028, val_acc:0.756]
Epoch [11/120    avg_loss:0.960, val_acc:0.729]
Epoch [12/120    avg_loss:0.851, val_acc:0.752]
Epoch [13/120    avg_loss:0.777, val_acc:0.742]
Epoch [14/120    avg_loss:0.672, val_acc:0.722]
Epoch [15/120    avg_loss:0.591, val_acc:0.769]
Epoch [16/120    avg_loss:0.523, val_acc:0.723]
Epoch [17/120    avg_loss:0.447, val_acc:0.744]
Epoch [18/120    avg_loss:0.407, val_acc:0.813]
Epoch [19/120    avg_loss:0.372, val_acc:0.792]
Epoch [20/120    avg_loss:0.346, val_acc:0.827]
Epoch [21/120    avg_loss:0.331, val_acc:0.769]
Epoch [22/120    avg_loss:0.283, val_acc:0.831]
Epoch [23/120    avg_loss:0.292, val_acc:0.872]
Epoch [24/120    avg_loss:0.253, val_acc:0.901]
Epoch [25/120    avg_loss:0.238, val_acc:0.941]
Epoch [26/120    avg_loss:0.207, val_acc:0.940]
Epoch [27/120    avg_loss:0.191, val_acc:0.946]
Epoch [28/120    avg_loss:0.177, val_acc:0.945]
Epoch [29/120    avg_loss:0.168, val_acc:0.938]
Epoch [30/120    avg_loss:0.146, val_acc:0.966]
Epoch [31/120    avg_loss:0.131, val_acc:0.966]
Epoch [32/120    avg_loss:0.138, val_acc:0.951]
Epoch [33/120    avg_loss:0.140, val_acc:0.979]
Epoch [34/120    avg_loss:0.151, val_acc:0.965]
Epoch [35/120    avg_loss:0.119, val_acc:0.969]
Epoch [36/120    avg_loss:0.085, val_acc:0.975]
Epoch [37/120    avg_loss:0.068, val_acc:0.979]
Epoch [38/120    avg_loss:0.075, val_acc:0.969]
Epoch [39/120    avg_loss:0.077, val_acc:0.970]
Epoch [40/120    avg_loss:0.118, val_acc:0.963]
Epoch [41/120    avg_loss:0.139, val_acc:0.969]
Epoch [42/120    avg_loss:0.075, val_acc:0.978]
Epoch [43/120    avg_loss:0.069, val_acc:0.978]
Epoch [44/120    avg_loss:0.067, val_acc:0.966]
Epoch [45/120    avg_loss:0.059, val_acc:0.983]
Epoch [46/120    avg_loss:0.045, val_acc:0.986]
Epoch [47/120    avg_loss:0.055, val_acc:0.974]
Epoch [48/120    avg_loss:0.039, val_acc:0.986]
Epoch [49/120    avg_loss:0.040, val_acc:0.985]
Epoch [50/120    avg_loss:0.035, val_acc:0.987]
Epoch [51/120    avg_loss:0.040, val_acc:0.987]
Epoch [52/120    avg_loss:0.029, val_acc:0.986]
Epoch [53/120    avg_loss:0.039, val_acc:0.958]
Epoch [54/120    avg_loss:0.090, val_acc:0.969]
Epoch [55/120    avg_loss:0.067, val_acc:0.980]
Epoch [56/120    avg_loss:0.054, val_acc:0.985]
Epoch [57/120    avg_loss:0.037, val_acc:0.985]
Epoch [58/120    avg_loss:0.039, val_acc:0.986]
Epoch [59/120    avg_loss:0.031, val_acc:0.987]
Epoch [60/120    avg_loss:0.025, val_acc:0.987]
Epoch [61/120    avg_loss:0.030, val_acc:0.989]
Epoch [62/120    avg_loss:0.017, val_acc:0.990]
Epoch [63/120    avg_loss:0.021, val_acc:0.988]
Epoch [64/120    avg_loss:0.027, val_acc:0.988]
Epoch [65/120    avg_loss:0.013, val_acc:0.989]
Epoch [66/120    avg_loss:0.017, val_acc:0.988]
Epoch [67/120    avg_loss:0.016, val_acc:0.987]
Epoch [68/120    avg_loss:0.017, val_acc:0.990]
Epoch [69/120    avg_loss:0.015, val_acc:0.992]
Epoch [70/120    avg_loss:0.025, val_acc:0.987]
Epoch [71/120    avg_loss:0.026, val_acc:0.989]
Epoch [72/120    avg_loss:0.016, val_acc:0.991]
Epoch [73/120    avg_loss:0.013, val_acc:0.987]
Epoch [74/120    avg_loss:0.013, val_acc:0.992]
Epoch [75/120    avg_loss:0.011, val_acc:0.991]
Epoch [76/120    avg_loss:0.012, val_acc:0.987]
Epoch [77/120    avg_loss:0.015, val_acc:0.991]
Epoch [78/120    avg_loss:0.015, val_acc:0.989]
Epoch [79/120    avg_loss:0.010, val_acc:0.989]
Epoch [80/120    avg_loss:0.018, val_acc:0.989]
Epoch [81/120    avg_loss:0.013, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.990]
Epoch [84/120    avg_loss:0.012, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.976]
Epoch [86/120    avg_loss:0.010, val_acc:0.986]
Epoch [87/120    avg_loss:0.035, val_acc:0.966]
Epoch [88/120    avg_loss:0.034, val_acc:0.987]
Epoch [89/120    avg_loss:0.016, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.989]
Epoch [91/120    avg_loss:0.015, val_acc:0.989]
Epoch [92/120    avg_loss:0.012, val_acc:0.991]
Epoch [93/120    avg_loss:0.011, val_acc:0.991]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.991]
Epoch [96/120    avg_loss:0.010, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.992]
Epoch [98/120    avg_loss:0.009, val_acc:0.991]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.012, val_acc:0.992]
Epoch [101/120    avg_loss:0.011, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.009, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.009, val_acc:0.991]
Epoch [108/120    avg_loss:0.010, val_acc:0.992]
Epoch [109/120    avg_loss:0.009, val_acc:0.992]
Epoch [110/120    avg_loss:0.010, val_acc:0.992]
Epoch [111/120    avg_loss:0.007, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.009, val_acc:0.991]
Epoch [120/120    avg_loss:0.010, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     0     0     0     0     3    34     6]
 [    0    11 18030     0    42     0     7     0     0     0]
 [    0     8     0  1971     0     0     0     0    53     4]
 [    0     9    11     0  2928     0     3     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    31     0     0     0  4836     0     9     2]
 [    0     1     0     0     0     0     0  1287     0     2]
 [    0    28     0    31    51     0     0     0  3461     0]
 [    0     0     0     0    14    50     0     0     0   855]]

Accuracy:
98.96127057575977

F1 scores:
[       nan 0.99223482 0.99717936 0.97622585 0.97486266 0.98120301
 0.99465241 0.99767442 0.96824731 0.95637584]

Kappa:
0.9862386368928696
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4c30fdb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.206, val_acc:0.054]
Epoch [2/120    avg_loss:1.980, val_acc:0.098]
Epoch [3/120    avg_loss:1.788, val_acc:0.193]
Epoch [4/120    avg_loss:1.629, val_acc:0.438]
Epoch [5/120    avg_loss:1.506, val_acc:0.530]
Epoch [6/120    avg_loss:1.357, val_acc:0.595]
Epoch [7/120    avg_loss:1.206, val_acc:0.571]
Epoch [8/120    avg_loss:1.120, val_acc:0.636]
Epoch [9/120    avg_loss:1.012, val_acc:0.651]
Epoch [10/120    avg_loss:0.907, val_acc:0.732]
Epoch [11/120    avg_loss:0.783, val_acc:0.753]
Epoch [12/120    avg_loss:0.724, val_acc:0.802]
Epoch [13/120    avg_loss:0.660, val_acc:0.780]
Epoch [14/120    avg_loss:0.589, val_acc:0.793]
Epoch [15/120    avg_loss:0.535, val_acc:0.780]
Epoch [16/120    avg_loss:0.470, val_acc:0.824]
Epoch [17/120    avg_loss:0.413, val_acc:0.875]
Epoch [18/120    avg_loss:0.384, val_acc:0.786]
Epoch [19/120    avg_loss:0.421, val_acc:0.825]
Epoch [20/120    avg_loss:0.368, val_acc:0.839]
Epoch [21/120    avg_loss:0.302, val_acc:0.895]
Epoch [22/120    avg_loss:0.294, val_acc:0.889]
Epoch [23/120    avg_loss:0.270, val_acc:0.903]
Epoch [24/120    avg_loss:0.228, val_acc:0.890]
Epoch [25/120    avg_loss:0.225, val_acc:0.934]
Epoch [26/120    avg_loss:0.209, val_acc:0.919]
Epoch [27/120    avg_loss:0.210, val_acc:0.932]
Epoch [28/120    avg_loss:0.199, val_acc:0.908]
Epoch [29/120    avg_loss:0.337, val_acc:0.897]
Epoch [30/120    avg_loss:0.218, val_acc:0.898]
Epoch [31/120    avg_loss:0.158, val_acc:0.947]
Epoch [32/120    avg_loss:0.137, val_acc:0.940]
Epoch [33/120    avg_loss:0.157, val_acc:0.946]
Epoch [34/120    avg_loss:0.130, val_acc:0.955]
Epoch [35/120    avg_loss:0.104, val_acc:0.965]
Epoch [36/120    avg_loss:0.102, val_acc:0.951]
Epoch [37/120    avg_loss:0.115, val_acc:0.956]
Epoch [38/120    avg_loss:0.077, val_acc:0.971]
Epoch [39/120    avg_loss:0.075, val_acc:0.965]
Epoch [40/120    avg_loss:0.079, val_acc:0.961]
Epoch [41/120    avg_loss:0.067, val_acc:0.963]
Epoch [42/120    avg_loss:0.058, val_acc:0.976]
Epoch [43/120    avg_loss:0.057, val_acc:0.969]
Epoch [44/120    avg_loss:0.066, val_acc:0.976]
Epoch [45/120    avg_loss:0.070, val_acc:0.948]
Epoch [46/120    avg_loss:0.056, val_acc:0.963]
Epoch [47/120    avg_loss:0.047, val_acc:0.979]
Epoch [48/120    avg_loss:0.040, val_acc:0.971]
Epoch [49/120    avg_loss:0.047, val_acc:0.971]
Epoch [50/120    avg_loss:0.039, val_acc:0.976]
Epoch [51/120    avg_loss:0.052, val_acc:0.986]
Epoch [52/120    avg_loss:0.038, val_acc:0.983]
Epoch [53/120    avg_loss:0.058, val_acc:0.986]
Epoch [54/120    avg_loss:0.035, val_acc:0.985]
Epoch [55/120    avg_loss:0.028, val_acc:0.979]
Epoch [56/120    avg_loss:0.033, val_acc:0.982]
Epoch [57/120    avg_loss:0.027, val_acc:0.986]
Epoch [58/120    avg_loss:0.025, val_acc:0.974]
Epoch [59/120    avg_loss:0.024, val_acc:0.980]
Epoch [60/120    avg_loss:0.025, val_acc:0.983]
Epoch [61/120    avg_loss:0.021, val_acc:0.979]
Epoch [62/120    avg_loss:0.030, val_acc:0.981]
Epoch [63/120    avg_loss:0.034, val_acc:0.980]
Epoch [64/120    avg_loss:0.025, val_acc:0.980]
Epoch [65/120    avg_loss:0.021, val_acc:0.980]
Epoch [66/120    avg_loss:0.024, val_acc:0.986]
Epoch [67/120    avg_loss:0.023, val_acc:0.987]
Epoch [68/120    avg_loss:0.014, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.987]
Epoch [70/120    avg_loss:0.017, val_acc:0.987]
Epoch [71/120    avg_loss:0.014, val_acc:0.987]
Epoch [72/120    avg_loss:0.017, val_acc:0.986]
Epoch [73/120    avg_loss:0.013, val_acc:0.985]
Epoch [74/120    avg_loss:0.010, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.013, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.012, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.985]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.985]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.011, val_acc:0.985]
Epoch [98/120    avg_loss:0.013, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.014, val_acc:0.985]
Epoch [102/120    avg_loss:0.011, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.016, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.011, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6338     0     0     0     0     0     0    94     0]
 [    0     0 18028     0    39     0    15     0     8     0]
 [    0     9     0  1935     0     0     0     0    92     0]
 [    0    25    12     0  2905     0    15     0    14     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4868     0     1     1]
 [    0     2     0     5     5     0     0  1278     0     0]
 [    0     3     0     3    49     0     0     0  3514     2]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
98.92752994480998

F1 scores:
[       nan 0.98961668 0.99773092 0.97260618 0.97092246 0.9893859
 0.99590835 0.9953271  0.96353167 0.97444444]

Kappa:
0.9857952190326015
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd3f52bfba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.226, val_acc:0.210]
Epoch [2/120    avg_loss:2.026, val_acc:0.205]
Epoch [3/120    avg_loss:1.835, val_acc:0.177]
Epoch [4/120    avg_loss:1.673, val_acc:0.213]
Epoch [5/120    avg_loss:1.523, val_acc:0.270]
Epoch [6/120    avg_loss:1.411, val_acc:0.358]
Epoch [7/120    avg_loss:1.289, val_acc:0.391]
Epoch [8/120    avg_loss:1.178, val_acc:0.432]
Epoch [9/120    avg_loss:1.091, val_acc:0.411]
Epoch [10/120    avg_loss:0.988, val_acc:0.435]
Epoch [11/120    avg_loss:0.896, val_acc:0.491]
Epoch [12/120    avg_loss:0.781, val_acc:0.541]
Epoch [13/120    avg_loss:0.701, val_acc:0.676]
Epoch [14/120    avg_loss:0.617, val_acc:0.758]
Epoch [15/120    avg_loss:0.540, val_acc:0.767]
Epoch [16/120    avg_loss:0.440, val_acc:0.835]
Epoch [17/120    avg_loss:0.389, val_acc:0.860]
Epoch [18/120    avg_loss:0.356, val_acc:0.859]
Epoch [19/120    avg_loss:0.376, val_acc:0.815]
Epoch [20/120    avg_loss:0.295, val_acc:0.895]
Epoch [21/120    avg_loss:0.229, val_acc:0.923]
Epoch [22/120    avg_loss:0.224, val_acc:0.865]
Epoch [23/120    avg_loss:0.196, val_acc:0.925]
Epoch [24/120    avg_loss:0.166, val_acc:0.933]
Epoch [25/120    avg_loss:0.156, val_acc:0.931]
Epoch [26/120    avg_loss:0.177, val_acc:0.916]
Epoch [27/120    avg_loss:0.132, val_acc:0.940]
Epoch [28/120    avg_loss:0.107, val_acc:0.954]
Epoch [29/120    avg_loss:0.115, val_acc:0.954]
Epoch [30/120    avg_loss:0.115, val_acc:0.949]
Epoch [31/120    avg_loss:0.090, val_acc:0.967]
Epoch [32/120    avg_loss:0.079, val_acc:0.974]
Epoch [33/120    avg_loss:0.073, val_acc:0.976]
Epoch [34/120    avg_loss:0.076, val_acc:0.942]
Epoch [35/120    avg_loss:0.073, val_acc:0.959]
Epoch [36/120    avg_loss:0.085, val_acc:0.965]
Epoch [37/120    avg_loss:0.073, val_acc:0.965]
Epoch [38/120    avg_loss:0.063, val_acc:0.923]
Epoch [39/120    avg_loss:0.087, val_acc:0.970]
Epoch [40/120    avg_loss:0.061, val_acc:0.956]
Epoch [41/120    avg_loss:0.051, val_acc:0.977]
Epoch [42/120    avg_loss:0.053, val_acc:0.975]
Epoch [43/120    avg_loss:0.043, val_acc:0.984]
Epoch [44/120    avg_loss:0.040, val_acc:0.981]
Epoch [45/120    avg_loss:0.032, val_acc:0.978]
Epoch [46/120    avg_loss:0.029, val_acc:0.932]
Epoch [47/120    avg_loss:0.038, val_acc:0.976]
Epoch [48/120    avg_loss:0.038, val_acc:0.973]
Epoch [49/120    avg_loss:0.058, val_acc:0.964]
Epoch [50/120    avg_loss:0.060, val_acc:0.983]
Epoch [51/120    avg_loss:0.103, val_acc:0.964]
Epoch [52/120    avg_loss:0.077, val_acc:0.973]
Epoch [53/120    avg_loss:0.056, val_acc:0.974]
Epoch [54/120    avg_loss:0.035, val_acc:0.971]
Epoch [55/120    avg_loss:0.035, val_acc:0.972]
Epoch [56/120    avg_loss:0.037, val_acc:0.976]
Epoch [57/120    avg_loss:0.025, val_acc:0.981]
Epoch [58/120    avg_loss:0.024, val_acc:0.982]
Epoch [59/120    avg_loss:0.020, val_acc:0.983]
Epoch [60/120    avg_loss:0.019, val_acc:0.983]
Epoch [61/120    avg_loss:0.018, val_acc:0.982]
Epoch [62/120    avg_loss:0.023, val_acc:0.985]
Epoch [63/120    avg_loss:0.017, val_acc:0.985]
Epoch [64/120    avg_loss:0.017, val_acc:0.983]
Epoch [65/120    avg_loss:0.016, val_acc:0.985]
Epoch [66/120    avg_loss:0.017, val_acc:0.985]
Epoch [67/120    avg_loss:0.017, val_acc:0.984]
Epoch [68/120    avg_loss:0.018, val_acc:0.984]
Epoch [69/120    avg_loss:0.019, val_acc:0.987]
Epoch [70/120    avg_loss:0.020, val_acc:0.985]
Epoch [71/120    avg_loss:0.016, val_acc:0.983]
Epoch [72/120    avg_loss:0.018, val_acc:0.983]
Epoch [73/120    avg_loss:0.016, val_acc:0.983]
Epoch [74/120    avg_loss:0.018, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.984]
Epoch [76/120    avg_loss:0.015, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.016, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.986]
Epoch [81/120    avg_loss:0.015, val_acc:0.986]
Epoch [82/120    avg_loss:0.015, val_acc:0.986]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.015, val_acc:0.985]
Epoch [85/120    avg_loss:0.012, val_acc:0.986]
Epoch [86/120    avg_loss:0.015, val_acc:0.986]
Epoch [87/120    avg_loss:0.019, val_acc:0.986]
Epoch [88/120    avg_loss:0.016, val_acc:0.986]
Epoch [89/120    avg_loss:0.016, val_acc:0.986]
Epoch [90/120    avg_loss:0.014, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.985]
Epoch [92/120    avg_loss:0.015, val_acc:0.985]
Epoch [93/120    avg_loss:0.014, val_acc:0.985]
Epoch [94/120    avg_loss:0.012, val_acc:0.985]
Epoch [95/120    avg_loss:0.019, val_acc:0.986]
Epoch [96/120    avg_loss:0.014, val_acc:0.986]
Epoch [97/120    avg_loss:0.016, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.013, val_acc:0.986]
Epoch [100/120    avg_loss:0.020, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.013, val_acc:0.986]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.015, val_acc:0.986]
Epoch [107/120    avg_loss:0.022, val_acc:0.986]
Epoch [108/120    avg_loss:0.015, val_acc:0.986]
Epoch [109/120    avg_loss:0.013, val_acc:0.986]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.014, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.016, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.014, val_acc:0.986]
Epoch [116/120    avg_loss:0.017, val_acc:0.986]
Epoch [117/120    avg_loss:0.017, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.012, val_acc:0.986]
Epoch [120/120    avg_loss:0.020, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6265     0     0     2     0    14    29   111    11]
 [    0     0 18062     0    25     0     3     0     0     0]
 [    0     2     1  1897     3     0     0     0   131     2]
 [    0    21     3     0  2932     0     6     0     8     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4853     0    10     0]
 [    0     1     0     0     0     0     0  1285     3     1]
 [    0     8     0    33    55     0     0     0  3475     0]
 [    0     0     0     0     4    27     0     0     0   888]]

Accuracy:
98.72026606897549

F1 scores:
[       nan 0.98436641 0.99870062 0.95663137 0.97847489 0.98976109
 0.99507894 0.98694316 0.95088247 0.97421832]

Kappa:
0.9830487837903441
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6be45c9b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.150, val_acc:0.182]
Epoch [2/120    avg_loss:1.910, val_acc:0.165]
Epoch [3/120    avg_loss:1.766, val_acc:0.220]
Epoch [4/120    avg_loss:1.639, val_acc:0.227]
Epoch [5/120    avg_loss:1.488, val_acc:0.238]
Epoch [6/120    avg_loss:1.390, val_acc:0.260]
Epoch [7/120    avg_loss:1.297, val_acc:0.294]
Epoch [8/120    avg_loss:1.217, val_acc:0.339]
Epoch [9/120    avg_loss:1.119, val_acc:0.383]
Epoch [10/120    avg_loss:1.018, val_acc:0.408]
Epoch [11/120    avg_loss:0.945, val_acc:0.497]
Epoch [12/120    avg_loss:0.846, val_acc:0.496]
Epoch [13/120    avg_loss:0.750, val_acc:0.640]
Epoch [14/120    avg_loss:0.661, val_acc:0.657]
Epoch [15/120    avg_loss:0.583, val_acc:0.707]
Epoch [16/120    avg_loss:0.536, val_acc:0.718]
Epoch [17/120    avg_loss:0.470, val_acc:0.756]
Epoch [18/120    avg_loss:0.420, val_acc:0.778]
Epoch [19/120    avg_loss:0.350, val_acc:0.796]
Epoch [20/120    avg_loss:0.349, val_acc:0.800]
Epoch [21/120    avg_loss:0.350, val_acc:0.797]
Epoch [22/120    avg_loss:0.306, val_acc:0.844]
Epoch [23/120    avg_loss:0.260, val_acc:0.867]
Epoch [24/120    avg_loss:0.280, val_acc:0.892]
Epoch [25/120    avg_loss:0.247, val_acc:0.865]
Epoch [26/120    avg_loss:0.224, val_acc:0.878]
Epoch [27/120    avg_loss:0.196, val_acc:0.892]
Epoch [28/120    avg_loss:0.195, val_acc:0.927]
Epoch [29/120    avg_loss:0.188, val_acc:0.916]
Epoch [30/120    avg_loss:0.171, val_acc:0.950]
Epoch [31/120    avg_loss:0.164, val_acc:0.924]
Epoch [32/120    avg_loss:0.136, val_acc:0.937]
Epoch [33/120    avg_loss:0.165, val_acc:0.948]
Epoch [34/120    avg_loss:0.162, val_acc:0.905]
Epoch [35/120    avg_loss:0.140, val_acc:0.942]
Epoch [36/120    avg_loss:0.114, val_acc:0.949]
Epoch [37/120    avg_loss:0.126, val_acc:0.942]
Epoch [38/120    avg_loss:0.090, val_acc:0.969]
Epoch [39/120    avg_loss:0.117, val_acc:0.905]
Epoch [40/120    avg_loss:0.087, val_acc:0.966]
Epoch [41/120    avg_loss:0.093, val_acc:0.969]
Epoch [42/120    avg_loss:0.092, val_acc:0.933]
Epoch [43/120    avg_loss:0.130, val_acc:0.916]
Epoch [44/120    avg_loss:0.134, val_acc:0.942]
Epoch [45/120    avg_loss:0.096, val_acc:0.971]
Epoch [46/120    avg_loss:0.086, val_acc:0.932]
Epoch [47/120    avg_loss:0.070, val_acc:0.955]
Epoch [48/120    avg_loss:0.057, val_acc:0.975]
Epoch [49/120    avg_loss:0.070, val_acc:0.970]
Epoch [50/120    avg_loss:0.089, val_acc:0.969]
Epoch [51/120    avg_loss:0.072, val_acc:0.961]
Epoch [52/120    avg_loss:0.054, val_acc:0.976]
Epoch [53/120    avg_loss:0.055, val_acc:0.974]
Epoch [54/120    avg_loss:0.058, val_acc:0.981]
Epoch [55/120    avg_loss:0.041, val_acc:0.963]
Epoch [56/120    avg_loss:0.063, val_acc:0.976]
Epoch [57/120    avg_loss:0.070, val_acc:0.970]
Epoch [58/120    avg_loss:0.045, val_acc:0.980]
Epoch [59/120    avg_loss:0.055, val_acc:0.981]
Epoch [60/120    avg_loss:0.043, val_acc:0.972]
Epoch [61/120    avg_loss:0.035, val_acc:0.981]
Epoch [62/120    avg_loss:0.047, val_acc:0.973]
Epoch [63/120    avg_loss:0.035, val_acc:0.977]
Epoch [64/120    avg_loss:0.063, val_acc:0.960]
Epoch [65/120    avg_loss:0.148, val_acc:0.963]
Epoch [66/120    avg_loss:0.066, val_acc:0.981]
Epoch [67/120    avg_loss:0.048, val_acc:0.968]
Epoch [68/120    avg_loss:0.042, val_acc:0.982]
Epoch [69/120    avg_loss:0.061, val_acc:0.972]
Epoch [70/120    avg_loss:0.054, val_acc:0.981]
Epoch [71/120    avg_loss:0.031, val_acc:0.981]
Epoch [72/120    avg_loss:0.034, val_acc:0.986]
Epoch [73/120    avg_loss:0.022, val_acc:0.983]
Epoch [74/120    avg_loss:0.047, val_acc:0.985]
Epoch [75/120    avg_loss:0.052, val_acc:0.912]
Epoch [76/120    avg_loss:0.044, val_acc:0.987]
Epoch [77/120    avg_loss:0.036, val_acc:0.975]
Epoch [78/120    avg_loss:0.044, val_acc:0.986]
Epoch [79/120    avg_loss:0.044, val_acc:0.969]
Epoch [80/120    avg_loss:0.066, val_acc:0.965]
Epoch [81/120    avg_loss:0.050, val_acc:0.977]
Epoch [82/120    avg_loss:0.056, val_acc:0.983]
Epoch [83/120    avg_loss:0.029, val_acc:0.974]
Epoch [84/120    avg_loss:0.034, val_acc:0.986]
Epoch [85/120    avg_loss:0.031, val_acc:0.982]
Epoch [86/120    avg_loss:0.030, val_acc:0.983]
Epoch [87/120    avg_loss:0.049, val_acc:0.983]
Epoch [88/120    avg_loss:0.022, val_acc:0.985]
Epoch [89/120    avg_loss:0.021, val_acc:0.990]
Epoch [90/120    avg_loss:0.030, val_acc:0.955]
Epoch [91/120    avg_loss:0.032, val_acc:0.965]
Epoch [92/120    avg_loss:0.019, val_acc:0.986]
Epoch [93/120    avg_loss:0.021, val_acc:0.982]
Epoch [94/120    avg_loss:0.017, val_acc:0.988]
Epoch [95/120    avg_loss:0.018, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.988]
Epoch [98/120    avg_loss:0.017, val_acc:0.984]
Epoch [99/120    avg_loss:0.018, val_acc:0.974]
Epoch [100/120    avg_loss:0.016, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.022, val_acc:0.984]
Epoch [103/120    avg_loss:0.013, val_acc:0.989]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.008, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.990]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     2     0     0     0    39    12     1]
 [    0     2 17996     0    89     0     2     0     0     1]
 [    0     6     0  1997     0     0     0     0    31     2]
 [    0    38     6     0  2910     0     2     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10    12     0     0  4853     0     3     0]
 [    0     3     0     0     0     2     0  1285     0     0]
 [    0    32     0    34    34     0     0     1  3470     0]
 [    0     1     0     2    15    33     0     0     0   868]]

Accuracy:
98.96127057575977

F1 scores:
[       nan 0.98945082 0.99695308 0.9782023  0.96677741 0.98676749
 0.99702106 0.98279159 0.97705195 0.9692909 ]

Kappa:
0.9862486616730912
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22d4b32b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.099, val_acc:0.101]
Epoch [2/120    avg_loss:1.914, val_acc:0.122]
Epoch [3/120    avg_loss:1.801, val_acc:0.128]
Epoch [4/120    avg_loss:1.675, val_acc:0.162]
Epoch [5/120    avg_loss:1.524, val_acc:0.188]
Epoch [6/120    avg_loss:1.417, val_acc:0.352]
Epoch [7/120    avg_loss:1.326, val_acc:0.412]
Epoch [8/120    avg_loss:1.201, val_acc:0.431]
Epoch [9/120    avg_loss:1.103, val_acc:0.464]
Epoch [10/120    avg_loss:1.024, val_acc:0.494]
Epoch [11/120    avg_loss:0.963, val_acc:0.492]
Epoch [12/120    avg_loss:0.893, val_acc:0.509]
Epoch [13/120    avg_loss:0.774, val_acc:0.542]
Epoch [14/120    avg_loss:0.751, val_acc:0.551]
Epoch [15/120    avg_loss:0.660, val_acc:0.592]
Epoch [16/120    avg_loss:0.608, val_acc:0.719]
Epoch [17/120    avg_loss:0.541, val_acc:0.639]
Epoch [18/120    avg_loss:0.526, val_acc:0.665]
Epoch [19/120    avg_loss:0.456, val_acc:0.726]
Epoch [20/120    avg_loss:0.397, val_acc:0.698]
Epoch [21/120    avg_loss:0.388, val_acc:0.769]
Epoch [22/120    avg_loss:0.378, val_acc:0.812]
Epoch [23/120    avg_loss:0.353, val_acc:0.796]
Epoch [24/120    avg_loss:0.321, val_acc:0.842]
Epoch [25/120    avg_loss:0.290, val_acc:0.835]
Epoch [26/120    avg_loss:0.292, val_acc:0.844]
Epoch [27/120    avg_loss:0.263, val_acc:0.812]
Epoch [28/120    avg_loss:0.234, val_acc:0.900]
Epoch [29/120    avg_loss:0.227, val_acc:0.841]
Epoch [30/120    avg_loss:0.179, val_acc:0.918]
Epoch [31/120    avg_loss:0.216, val_acc:0.862]
Epoch [32/120    avg_loss:0.180, val_acc:0.934]
Epoch [33/120    avg_loss:0.171, val_acc:0.949]
Epoch [34/120    avg_loss:0.184, val_acc:0.921]
Epoch [35/120    avg_loss:0.172, val_acc:0.948]
Epoch [36/120    avg_loss:0.153, val_acc:0.950]
Epoch [37/120    avg_loss:0.121, val_acc:0.950]
Epoch [38/120    avg_loss:0.122, val_acc:0.966]
Epoch [39/120    avg_loss:0.098, val_acc:0.967]
Epoch [40/120    avg_loss:0.079, val_acc:0.965]
Epoch [41/120    avg_loss:0.102, val_acc:0.954]
Epoch [42/120    avg_loss:0.077, val_acc:0.971]
Epoch [43/120    avg_loss:0.076, val_acc:0.968]
Epoch [44/120    avg_loss:0.092, val_acc:0.971]
Epoch [45/120    avg_loss:0.068, val_acc:0.971]
Epoch [46/120    avg_loss:0.088, val_acc:0.952]
Epoch [47/120    avg_loss:0.079, val_acc:0.973]
Epoch [48/120    avg_loss:0.051, val_acc:0.972]
Epoch [49/120    avg_loss:0.047, val_acc:0.965]
Epoch [50/120    avg_loss:0.045, val_acc:0.981]
Epoch [51/120    avg_loss:0.049, val_acc:0.952]
Epoch [52/120    avg_loss:0.041, val_acc:0.979]
Epoch [53/120    avg_loss:0.053, val_acc:0.961]
Epoch [54/120    avg_loss:0.035, val_acc:0.982]
Epoch [55/120    avg_loss:0.049, val_acc:0.981]
Epoch [56/120    avg_loss:0.044, val_acc:0.969]
Epoch [57/120    avg_loss:0.039, val_acc:0.974]
Epoch [58/120    avg_loss:0.038, val_acc:0.981]
Epoch [59/120    avg_loss:0.038, val_acc:0.973]
Epoch [60/120    avg_loss:0.033, val_acc:0.975]
Epoch [61/120    avg_loss:0.051, val_acc:0.976]
Epoch [62/120    avg_loss:0.029, val_acc:0.976]
Epoch [63/120    avg_loss:0.029, val_acc:0.985]
Epoch [64/120    avg_loss:0.025, val_acc:0.981]
Epoch [65/120    avg_loss:0.049, val_acc:0.970]
Epoch [66/120    avg_loss:0.071, val_acc:0.968]
Epoch [67/120    avg_loss:0.104, val_acc:0.924]
Epoch [68/120    avg_loss:0.092, val_acc:0.972]
Epoch [69/120    avg_loss:0.054, val_acc:0.970]
Epoch [70/120    avg_loss:0.028, val_acc:0.979]
Epoch [71/120    avg_loss:0.027, val_acc:0.965]
Epoch [72/120    avg_loss:0.028, val_acc:0.973]
Epoch [73/120    avg_loss:0.033, val_acc:0.981]
Epoch [74/120    avg_loss:0.035, val_acc:0.986]
Epoch [75/120    avg_loss:0.026, val_acc:0.986]
Epoch [76/120    avg_loss:0.021, val_acc:0.979]
Epoch [77/120    avg_loss:0.027, val_acc:0.987]
Epoch [78/120    avg_loss:0.019, val_acc:0.986]
Epoch [79/120    avg_loss:0.018, val_acc:0.979]
Epoch [80/120    avg_loss:0.020, val_acc:0.984]
Epoch [81/120    avg_loss:0.026, val_acc:0.986]
Epoch [82/120    avg_loss:0.019, val_acc:0.983]
Epoch [83/120    avg_loss:0.016, val_acc:0.976]
Epoch [84/120    avg_loss:0.018, val_acc:0.981]
Epoch [85/120    avg_loss:0.017, val_acc:0.984]
Epoch [86/120    avg_loss:0.016, val_acc:0.981]
Epoch [87/120    avg_loss:0.017, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.987]
Epoch [89/120    avg_loss:0.012, val_acc:0.984]
Epoch [90/120    avg_loss:0.016, val_acc:0.982]
Epoch [91/120    avg_loss:0.017, val_acc:0.983]
Epoch [92/120    avg_loss:0.021, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.016, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.018, val_acc:0.977]
Epoch [99/120    avg_loss:0.012, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6344     0     0     0     0     0     5    80     3]
 [    0    10 18065     0    11     0     2     0     2     0]
 [    0     7     0  2014     0     0     0     0    11     4]
 [    0    42    12     0  2896     0     0     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    33     0     0     0  4843     0     2     0]
 [    0     2     0     0     0     1     1  1286     0     0]
 [    0    21     0    14    50     0     0     0  3486     0]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.0865929192876

F1 scores:
[       nan 0.98677866 0.9980663  0.99114173 0.97459196 0.98826202
 0.99609214 0.99651298 0.97184277 0.9716824 ]

Kappa:
0.9878942901021638
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff473a4bb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.120, val_acc:0.081]
Epoch [2/120    avg_loss:1.924, val_acc:0.115]
Epoch [3/120    avg_loss:1.785, val_acc:0.132]
Epoch [4/120    avg_loss:1.647, val_acc:0.197]
Epoch [5/120    avg_loss:1.513, val_acc:0.229]
Epoch [6/120    avg_loss:1.403, val_acc:0.256]
Epoch [7/120    avg_loss:1.340, val_acc:0.357]
Epoch [8/120    avg_loss:1.228, val_acc:0.408]
Epoch [9/120    avg_loss:1.123, val_acc:0.470]
Epoch [10/120    avg_loss:0.993, val_acc:0.476]
Epoch [11/120    avg_loss:0.887, val_acc:0.588]
Epoch [12/120    avg_loss:0.773, val_acc:0.661]
Epoch [13/120    avg_loss:0.689, val_acc:0.714]
Epoch [14/120    avg_loss:0.607, val_acc:0.751]
Epoch [15/120    avg_loss:0.497, val_acc:0.774]
Epoch [16/120    avg_loss:0.549, val_acc:0.672]
Epoch [17/120    avg_loss:0.506, val_acc:0.812]
Epoch [18/120    avg_loss:0.431, val_acc:0.812]
Epoch [19/120    avg_loss:0.375, val_acc:0.814]
Epoch [20/120    avg_loss:0.358, val_acc:0.827]
Epoch [21/120    avg_loss:0.313, val_acc:0.831]
Epoch [22/120    avg_loss:0.273, val_acc:0.899]
Epoch [23/120    avg_loss:0.241, val_acc:0.902]
Epoch [24/120    avg_loss:0.227, val_acc:0.904]
Epoch [25/120    avg_loss:0.235, val_acc:0.899]
Epoch [26/120    avg_loss:0.214, val_acc:0.924]
Epoch [27/120    avg_loss:0.170, val_acc:0.936]
Epoch [28/120    avg_loss:0.164, val_acc:0.899]
Epoch [29/120    avg_loss:0.252, val_acc:0.849]
Epoch [30/120    avg_loss:0.253, val_acc:0.872]
Epoch [31/120    avg_loss:0.214, val_acc:0.899]
Epoch [32/120    avg_loss:0.152, val_acc:0.932]
Epoch [33/120    avg_loss:0.148, val_acc:0.933]
Epoch [34/120    avg_loss:0.135, val_acc:0.948]
Epoch [35/120    avg_loss:0.105, val_acc:0.954]
Epoch [36/120    avg_loss:0.098, val_acc:0.951]
Epoch [37/120    avg_loss:0.125, val_acc:0.946]
Epoch [38/120    avg_loss:0.116, val_acc:0.954]
Epoch [39/120    avg_loss:0.088, val_acc:0.948]
Epoch [40/120    avg_loss:0.069, val_acc:0.942]
Epoch [41/120    avg_loss:0.073, val_acc:0.963]
Epoch [42/120    avg_loss:0.067, val_acc:0.956]
Epoch [43/120    avg_loss:0.060, val_acc:0.966]
Epoch [44/120    avg_loss:0.082, val_acc:0.965]
Epoch [45/120    avg_loss:0.054, val_acc:0.970]
Epoch [46/120    avg_loss:0.078, val_acc:0.953]
Epoch [47/120    avg_loss:0.060, val_acc:0.965]
Epoch [48/120    avg_loss:0.065, val_acc:0.962]
Epoch [49/120    avg_loss:0.060, val_acc:0.974]
Epoch [50/120    avg_loss:0.062, val_acc:0.980]
Epoch [51/120    avg_loss:0.048, val_acc:0.965]
Epoch [52/120    avg_loss:0.045, val_acc:0.967]
Epoch [53/120    avg_loss:0.033, val_acc:0.973]
Epoch [54/120    avg_loss:0.036, val_acc:0.958]
Epoch [55/120    avg_loss:0.109, val_acc:0.966]
Epoch [56/120    avg_loss:0.043, val_acc:0.977]
Epoch [57/120    avg_loss:0.034, val_acc:0.970]
Epoch [58/120    avg_loss:0.030, val_acc:0.971]
Epoch [59/120    avg_loss:0.056, val_acc:0.959]
Epoch [60/120    avg_loss:0.044, val_acc:0.983]
Epoch [61/120    avg_loss:0.025, val_acc:0.981]
Epoch [62/120    avg_loss:0.030, val_acc:0.980]
Epoch [63/120    avg_loss:0.029, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.970]
Epoch [66/120    avg_loss:0.033, val_acc:0.969]
Epoch [67/120    avg_loss:0.028, val_acc:0.979]
Epoch [68/120    avg_loss:0.048, val_acc:0.954]
Epoch [69/120    avg_loss:0.039, val_acc:0.963]
Epoch [70/120    avg_loss:0.046, val_acc:0.972]
Epoch [71/120    avg_loss:0.031, val_acc:0.975]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.014, val_acc:0.970]
Epoch [74/120    avg_loss:0.013, val_acc:0.977]
Epoch [75/120    avg_loss:0.012, val_acc:0.979]
Epoch [76/120    avg_loss:0.011, val_acc:0.980]
Epoch [77/120    avg_loss:0.010, val_acc:0.980]
Epoch [78/120    avg_loss:0.011, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.980]
Epoch [81/120    avg_loss:0.010, val_acc:0.980]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.013, val_acc:0.978]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.010, val_acc:0.979]
Epoch [88/120    avg_loss:0.012, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.979]
Epoch [90/120    avg_loss:0.011, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.979]
Epoch [92/120    avg_loss:0.011, val_acc:0.979]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.009, val_acc:0.979]
Epoch [95/120    avg_loss:0.010, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.013, val_acc:0.978]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.010, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.979]
Epoch [108/120    avg_loss:0.010, val_acc:0.979]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.979]
Epoch [111/120    avg_loss:0.009, val_acc:0.979]
Epoch [112/120    avg_loss:0.012, val_acc:0.979]
Epoch [113/120    avg_loss:0.011, val_acc:0.979]
Epoch [114/120    avg_loss:0.014, val_acc:0.979]
Epoch [115/120    avg_loss:0.014, val_acc:0.979]
Epoch [116/120    avg_loss:0.013, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.011, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     0     2     0     0     8    62     5]
 [    0     3 18023     0    44     0    20     0     0     0]
 [    0     7     0  1935     0     0     0     0    93     1]
 [    0    33    12     0  2896     0    12     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4868     0     1     0]
 [    0     1     0     0     0     0     0  1288     0     1]
 [    0    56     0    34    49     0     0     0  3431     1]
 [    0     0     0     0    14    17     0     0     0   888]]

Accuracy:
98.78533728580724

F1 scores:
[       nan 0.98626523 0.99756462 0.96629213 0.96904802 0.99352874
 0.99570464 0.99613302 0.9561098  0.9785124 ]

Kappa:
0.9839109470090048
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53fd6adac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.182, val_acc:0.088]
Epoch [2/120    avg_loss:1.959, val_acc:0.104]
Epoch [3/120    avg_loss:1.780, val_acc:0.182]
Epoch [4/120    avg_loss:1.625, val_acc:0.271]
Epoch [5/120    avg_loss:1.481, val_acc:0.294]
Epoch [6/120    avg_loss:1.382, val_acc:0.374]
Epoch [7/120    avg_loss:1.259, val_acc:0.503]
Epoch [8/120    avg_loss:1.126, val_acc:0.631]
Epoch [9/120    avg_loss:1.007, val_acc:0.649]
Epoch [10/120    avg_loss:0.860, val_acc:0.741]
Epoch [11/120    avg_loss:0.791, val_acc:0.790]
Epoch [12/120    avg_loss:0.681, val_acc:0.759]
Epoch [13/120    avg_loss:0.627, val_acc:0.773]
Epoch [14/120    avg_loss:0.539, val_acc:0.727]
Epoch [15/120    avg_loss:0.493, val_acc:0.744]
Epoch [16/120    avg_loss:0.435, val_acc:0.806]
Epoch [17/120    avg_loss:0.414, val_acc:0.808]
Epoch [18/120    avg_loss:0.404, val_acc:0.800]
Epoch [19/120    avg_loss:0.363, val_acc:0.815]
Epoch [20/120    avg_loss:0.333, val_acc:0.819]
Epoch [21/120    avg_loss:0.335, val_acc:0.831]
Epoch [22/120    avg_loss:0.279, val_acc:0.881]
Epoch [23/120    avg_loss:0.268, val_acc:0.845]
Epoch [24/120    avg_loss:0.278, val_acc:0.926]
Epoch [25/120    avg_loss:0.237, val_acc:0.843]
Epoch [26/120    avg_loss:0.213, val_acc:0.905]
Epoch [27/120    avg_loss:0.165, val_acc:0.941]
Epoch [28/120    avg_loss:0.156, val_acc:0.914]
Epoch [29/120    avg_loss:0.142, val_acc:0.951]
Epoch [30/120    avg_loss:0.116, val_acc:0.943]
Epoch [31/120    avg_loss:0.117, val_acc:0.947]
Epoch [32/120    avg_loss:0.111, val_acc:0.961]
Epoch [33/120    avg_loss:0.106, val_acc:0.963]
Epoch [34/120    avg_loss:0.094, val_acc:0.950]
Epoch [35/120    avg_loss:0.095, val_acc:0.957]
Epoch [36/120    avg_loss:0.084, val_acc:0.942]
Epoch [37/120    avg_loss:0.106, val_acc:0.948]
Epoch [38/120    avg_loss:0.102, val_acc:0.964]
Epoch [39/120    avg_loss:0.080, val_acc:0.972]
Epoch [40/120    avg_loss:0.061, val_acc:0.969]
Epoch [41/120    avg_loss:0.059, val_acc:0.968]
Epoch [42/120    avg_loss:0.052, val_acc:0.971]
Epoch [43/120    avg_loss:0.104, val_acc:0.964]
Epoch [44/120    avg_loss:0.072, val_acc:0.973]
Epoch [45/120    avg_loss:0.064, val_acc:0.963]
Epoch [46/120    avg_loss:0.068, val_acc:0.970]
Epoch [47/120    avg_loss:0.074, val_acc:0.971]
Epoch [48/120    avg_loss:0.059, val_acc:0.975]
Epoch [49/120    avg_loss:0.054, val_acc:0.974]
Epoch [50/120    avg_loss:0.047, val_acc:0.976]
Epoch [51/120    avg_loss:0.058, val_acc:0.974]
Epoch [52/120    avg_loss:0.055, val_acc:0.964]
Epoch [53/120    avg_loss:0.035, val_acc:0.978]
Epoch [54/120    avg_loss:0.041, val_acc:0.971]
Epoch [55/120    avg_loss:0.044, val_acc:0.977]
Epoch [56/120    avg_loss:0.031, val_acc:0.984]
Epoch [57/120    avg_loss:0.030, val_acc:0.980]
Epoch [58/120    avg_loss:0.027, val_acc:0.973]
Epoch [59/120    avg_loss:0.020, val_acc:0.983]
Epoch [60/120    avg_loss:0.022, val_acc:0.983]
Epoch [61/120    avg_loss:0.032, val_acc:0.929]
Epoch [62/120    avg_loss:0.091, val_acc:0.973]
Epoch [63/120    avg_loss:0.056, val_acc:0.964]
Epoch [64/120    avg_loss:0.048, val_acc:0.979]
Epoch [65/120    avg_loss:0.035, val_acc:0.970]
Epoch [66/120    avg_loss:0.039, val_acc:0.983]
Epoch [67/120    avg_loss:0.030, val_acc:0.977]
Epoch [68/120    avg_loss:0.040, val_acc:0.969]
Epoch [69/120    avg_loss:0.039, val_acc:0.984]
Epoch [70/120    avg_loss:0.022, val_acc:0.983]
Epoch [71/120    avg_loss:0.022, val_acc:0.984]
Epoch [72/120    avg_loss:0.019, val_acc:0.983]
Epoch [73/120    avg_loss:0.014, val_acc:0.982]
Epoch [74/120    avg_loss:0.028, val_acc:0.966]
Epoch [75/120    avg_loss:0.053, val_acc:0.981]
Epoch [76/120    avg_loss:0.027, val_acc:0.983]
Epoch [77/120    avg_loss:0.013, val_acc:0.986]
Epoch [78/120    avg_loss:0.023, val_acc:0.977]
Epoch [79/120    avg_loss:0.016, val_acc:0.983]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.019, val_acc:0.978]
Epoch [83/120    avg_loss:0.015, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.980]
Epoch [85/120    avg_loss:0.014, val_acc:0.984]
Epoch [86/120    avg_loss:0.013, val_acc:0.970]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.972]
Epoch [90/120    avg_loss:0.014, val_acc:0.986]
Epoch [91/120    avg_loss:0.012, val_acc:0.985]
Epoch [92/120    avg_loss:0.015, val_acc:0.985]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.015, val_acc:0.977]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.967]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.015, val_acc:0.930]
Epoch [102/120    avg_loss:1.210, val_acc:0.387]
Epoch [103/120    avg_loss:1.209, val_acc:0.494]
Epoch [104/120    avg_loss:1.123, val_acc:0.543]
Epoch [105/120    avg_loss:1.079, val_acc:0.531]
Epoch [106/120    avg_loss:1.040, val_acc:0.564]
Epoch [107/120    avg_loss:1.017, val_acc:0.553]
Epoch [108/120    avg_loss:0.973, val_acc:0.552]
Epoch [109/120    avg_loss:0.916, val_acc:0.613]
Epoch [110/120    avg_loss:0.897, val_acc:0.625]
Epoch [111/120    avg_loss:0.860, val_acc:0.635]
Epoch [112/120    avg_loss:0.845, val_acc:0.615]
Epoch [113/120    avg_loss:0.807, val_acc:0.665]
Epoch [114/120    avg_loss:0.764, val_acc:0.661]
Epoch [115/120    avg_loss:0.758, val_acc:0.673]
Epoch [116/120    avg_loss:0.764, val_acc:0.674]
Epoch [117/120    avg_loss:0.734, val_acc:0.682]
Epoch [118/120    avg_loss:0.726, val_acc:0.680]
Epoch [119/120    avg_loss:0.745, val_acc:0.688]
Epoch [120/120    avg_loss:0.738, val_acc:0.687]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4478   311    84    77     0   945    21   425    91]
 [    0     0 11072     0  2297     0  4721     0     0     0]
 [    0    18     1  1439     0     0    21     0   520    37]
 [    0    16   576     0  1756     0   607     0     5    12]
 [    0     0     0     0     0  1302     0     3     0     0]
 [    0     0   173    33   566     0  4106     0     0     0]
 [    0   142     1     0    15     0     0  1104    28     0]
 [    0   285   157    95    14     0   268     3  2749     0]
 [    0    43     0     1    14    44     7     0     2   808]]

Accuracy:
69.44303858482154

F1 scores:
[       nan 0.78465043 0.7288766  0.78036876 0.45545325 0.98227084
 0.52800103 0.91201983 0.75315068 0.86555972]

Kappa:
0.6188742003415937
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e0b44fba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.153, val_acc:0.097]
Epoch [2/120    avg_loss:1.937, val_acc:0.088]
Epoch [3/120    avg_loss:1.792, val_acc:0.108]
Epoch [4/120    avg_loss:1.695, val_acc:0.162]
Epoch [5/120    avg_loss:1.575, val_acc:0.191]
Epoch [6/120    avg_loss:1.476, val_acc:0.204]
Epoch [7/120    avg_loss:1.350, val_acc:0.239]
Epoch [8/120    avg_loss:1.235, val_acc:0.272]
Epoch [9/120    avg_loss:1.158, val_acc:0.292]
Epoch [10/120    avg_loss:1.115, val_acc:0.398]
Epoch [11/120    avg_loss:1.015, val_acc:0.414]
Epoch [12/120    avg_loss:0.890, val_acc:0.486]
Epoch [13/120    avg_loss:0.862, val_acc:0.533]
Epoch [14/120    avg_loss:0.736, val_acc:0.614]
Epoch [15/120    avg_loss:0.668, val_acc:0.639]
Epoch [16/120    avg_loss:0.627, val_acc:0.628]
Epoch [17/120    avg_loss:0.549, val_acc:0.687]
Epoch [18/120    avg_loss:0.491, val_acc:0.694]
Epoch [19/120    avg_loss:0.428, val_acc:0.718]
Epoch [20/120    avg_loss:0.452, val_acc:0.680]
Epoch [21/120    avg_loss:0.431, val_acc:0.768]
Epoch [22/120    avg_loss:0.376, val_acc:0.770]
Epoch [23/120    avg_loss:0.332, val_acc:0.796]
Epoch [24/120    avg_loss:0.325, val_acc:0.794]
Epoch [25/120    avg_loss:0.294, val_acc:0.802]
Epoch [26/120    avg_loss:0.276, val_acc:0.833]
Epoch [27/120    avg_loss:0.260, val_acc:0.859]
Epoch [28/120    avg_loss:0.222, val_acc:0.833]
Epoch [29/120    avg_loss:0.228, val_acc:0.871]
Epoch [30/120    avg_loss:0.192, val_acc:0.912]
Epoch [31/120    avg_loss:0.174, val_acc:0.893]
Epoch [32/120    avg_loss:0.188, val_acc:0.917]
Epoch [33/120    avg_loss:0.152, val_acc:0.908]
Epoch [34/120    avg_loss:0.110, val_acc:0.931]
Epoch [35/120    avg_loss:0.150, val_acc:0.915]
Epoch [36/120    avg_loss:0.144, val_acc:0.937]
Epoch [37/120    avg_loss:0.126, val_acc:0.944]
Epoch [38/120    avg_loss:0.103, val_acc:0.934]
Epoch [39/120    avg_loss:0.104, val_acc:0.925]
Epoch [40/120    avg_loss:0.105, val_acc:0.933]
Epoch [41/120    avg_loss:0.111, val_acc:0.951]
Epoch [42/120    avg_loss:0.090, val_acc:0.952]
Epoch [43/120    avg_loss:0.114, val_acc:0.943]
Epoch [44/120    avg_loss:0.116, val_acc:0.937]
Epoch [45/120    avg_loss:0.087, val_acc:0.950]
Epoch [46/120    avg_loss:0.064, val_acc:0.962]
Epoch [47/120    avg_loss:0.073, val_acc:0.951]
Epoch [48/120    avg_loss:0.057, val_acc:0.958]
Epoch [49/120    avg_loss:0.045, val_acc:0.970]
Epoch [50/120    avg_loss:0.043, val_acc:0.964]
Epoch [51/120    avg_loss:0.044, val_acc:0.958]
Epoch [52/120    avg_loss:0.049, val_acc:0.972]
Epoch [53/120    avg_loss:0.042, val_acc:0.977]
Epoch [54/120    avg_loss:0.037, val_acc:0.965]
Epoch [55/120    avg_loss:0.044, val_acc:0.970]
Epoch [56/120    avg_loss:0.033, val_acc:0.971]
Epoch [57/120    avg_loss:0.031, val_acc:0.975]
Epoch [58/120    avg_loss:0.028, val_acc:0.975]
Epoch [59/120    avg_loss:0.027, val_acc:0.975]
Epoch [60/120    avg_loss:0.043, val_acc:0.964]
Epoch [61/120    avg_loss:0.027, val_acc:0.970]
Epoch [62/120    avg_loss:0.044, val_acc:0.955]
Epoch [63/120    avg_loss:0.052, val_acc:0.926]
Epoch [64/120    avg_loss:0.071, val_acc:0.977]
Epoch [65/120    avg_loss:0.036, val_acc:0.966]
Epoch [66/120    avg_loss:0.037, val_acc:0.977]
Epoch [67/120    avg_loss:0.027, val_acc:0.976]
Epoch [68/120    avg_loss:0.025, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.977]
Epoch [70/120    avg_loss:0.028, val_acc:0.976]
Epoch [71/120    avg_loss:0.035, val_acc:0.970]
Epoch [72/120    avg_loss:0.031, val_acc:0.975]
Epoch [73/120    avg_loss:0.024, val_acc:0.968]
Epoch [74/120    avg_loss:0.028, val_acc:0.962]
Epoch [75/120    avg_loss:0.022, val_acc:0.974]
Epoch [76/120    avg_loss:0.025, val_acc:0.966]
Epoch [77/120    avg_loss:0.021, val_acc:0.959]
Epoch [78/120    avg_loss:0.029, val_acc:0.971]
Epoch [79/120    avg_loss:0.018, val_acc:0.977]
Epoch [80/120    avg_loss:0.015, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.010, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     0     0     0     0    35     0]
 [    0     0 18049     0    18     0    23     0     0     0]
 [    0     3     0  1992     1     0     0     0    39     1]
 [    0    30    20     0  2892     0     8     0    21     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     2     0     0  4874     0     0     0]
 [    0    12     0     0     0     0     3  1275     0     0]
 [    0    34     0    35    43     0     0     0  3459     0]
 [    0     0     0     0    14    40     0     1     0   864]]

Accuracy:
99.06972260381269

F1 scores:
[       nan 0.99116827 0.99825779 0.9800738  0.97373737 0.98490566
 0.9961169  0.99376461 0.97094737 0.96806723]

Kappa:
0.9876734122411654
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86305d4b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.137, val_acc:0.077]
Epoch [2/120    avg_loss:1.932, val_acc:0.108]
Epoch [3/120    avg_loss:1.795, val_acc:0.121]
Epoch [4/120    avg_loss:1.698, val_acc:0.128]
Epoch [5/120    avg_loss:1.579, val_acc:0.133]
Epoch [6/120    avg_loss:1.469, val_acc:0.232]
Epoch [7/120    avg_loss:1.377, val_acc:0.313]
Epoch [8/120    avg_loss:1.288, val_acc:0.388]
Epoch [9/120    avg_loss:1.190, val_acc:0.426]
Epoch [10/120    avg_loss:1.115, val_acc:0.431]
Epoch [11/120    avg_loss:1.033, val_acc:0.476]
Epoch [12/120    avg_loss:0.901, val_acc:0.464]
Epoch [13/120    avg_loss:0.801, val_acc:0.545]
Epoch [14/120    avg_loss:0.699, val_acc:0.580]
Epoch [15/120    avg_loss:0.620, val_acc:0.679]
Epoch [16/120    avg_loss:0.552, val_acc:0.715]
Epoch [17/120    avg_loss:0.511, val_acc:0.763]
Epoch [18/120    avg_loss:0.443, val_acc:0.813]
Epoch [19/120    avg_loss:0.420, val_acc:0.788]
Epoch [20/120    avg_loss:0.406, val_acc:0.828]
Epoch [21/120    avg_loss:0.358, val_acc:0.815]
Epoch [22/120    avg_loss:0.314, val_acc:0.855]
Epoch [23/120    avg_loss:0.291, val_acc:0.897]
Epoch [24/120    avg_loss:0.266, val_acc:0.928]
Epoch [25/120    avg_loss:0.240, val_acc:0.922]
Epoch [26/120    avg_loss:0.226, val_acc:0.934]
Epoch [27/120    avg_loss:0.218, val_acc:0.934]
Epoch [28/120    avg_loss:0.166, val_acc:0.934]
Epoch [29/120    avg_loss:0.206, val_acc:0.931]
Epoch [30/120    avg_loss:0.190, val_acc:0.874]
Epoch [31/120    avg_loss:0.178, val_acc:0.926]
Epoch [32/120    avg_loss:0.168, val_acc:0.932]
Epoch [33/120    avg_loss:0.165, val_acc:0.944]
Epoch [34/120    avg_loss:0.145, val_acc:0.957]
Epoch [35/120    avg_loss:0.129, val_acc:0.937]
Epoch [36/120    avg_loss:0.127, val_acc:0.947]
Epoch [37/120    avg_loss:0.130, val_acc:0.945]
Epoch [38/120    avg_loss:0.102, val_acc:0.957]
Epoch [39/120    avg_loss:0.088, val_acc:0.956]
Epoch [40/120    avg_loss:0.097, val_acc:0.975]
Epoch [41/120    avg_loss:0.093, val_acc:0.945]
Epoch [42/120    avg_loss:0.078, val_acc:0.974]
Epoch [43/120    avg_loss:0.057, val_acc:0.966]
Epoch [44/120    avg_loss:0.055, val_acc:0.958]
Epoch [45/120    avg_loss:0.083, val_acc:0.964]
Epoch [46/120    avg_loss:0.073, val_acc:0.972]
Epoch [47/120    avg_loss:0.162, val_acc:0.903]
Epoch [48/120    avg_loss:0.130, val_acc:0.945]
Epoch [49/120    avg_loss:0.097, val_acc:0.969]
Epoch [50/120    avg_loss:0.092, val_acc:0.955]
Epoch [51/120    avg_loss:0.070, val_acc:0.974]
Epoch [52/120    avg_loss:0.060, val_acc:0.972]
Epoch [53/120    avg_loss:0.066, val_acc:0.975]
Epoch [54/120    avg_loss:0.061, val_acc:0.975]
Epoch [55/120    avg_loss:0.042, val_acc:0.982]
Epoch [56/120    avg_loss:0.057, val_acc:0.967]
Epoch [57/120    avg_loss:0.035, val_acc:0.984]
Epoch [58/120    avg_loss:0.033, val_acc:0.977]
Epoch [59/120    avg_loss:0.055, val_acc:0.931]
Epoch [60/120    avg_loss:0.059, val_acc:0.977]
Epoch [61/120    avg_loss:0.038, val_acc:0.982]
Epoch [62/120    avg_loss:0.038, val_acc:0.980]
Epoch [63/120    avg_loss:0.039, val_acc:0.972]
Epoch [64/120    avg_loss:0.049, val_acc:0.963]
Epoch [65/120    avg_loss:0.048, val_acc:0.975]
Epoch [66/120    avg_loss:0.035, val_acc:0.981]
Epoch [67/120    avg_loss:0.027, val_acc:0.985]
Epoch [68/120    avg_loss:0.044, val_acc:0.976]
Epoch [69/120    avg_loss:0.031, val_acc:0.984]
Epoch [70/120    avg_loss:0.019, val_acc:0.988]
Epoch [71/120    avg_loss:0.025, val_acc:0.980]
Epoch [72/120    avg_loss:0.020, val_acc:0.985]
Epoch [73/120    avg_loss:0.014, val_acc:0.990]
Epoch [74/120    avg_loss:0.019, val_acc:0.983]
Epoch [75/120    avg_loss:0.029, val_acc:0.981]
Epoch [76/120    avg_loss:0.019, val_acc:0.984]
Epoch [77/120    avg_loss:0.020, val_acc:0.987]
Epoch [78/120    avg_loss:0.013, val_acc:0.985]
Epoch [79/120    avg_loss:0.020, val_acc:0.985]
Epoch [80/120    avg_loss:0.026, val_acc:0.957]
Epoch [81/120    avg_loss:0.030, val_acc:0.984]
Epoch [82/120    avg_loss:0.023, val_acc:0.989]
Epoch [83/120    avg_loss:0.026, val_acc:0.982]
Epoch [84/120    avg_loss:0.017, val_acc:0.985]
Epoch [85/120    avg_loss:0.012, val_acc:0.987]
Epoch [86/120    avg_loss:0.021, val_acc:0.987]
Epoch [87/120    avg_loss:0.015, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.987]
Epoch [91/120    avg_loss:0.011, val_acc:0.990]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.008, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.989]
Epoch [96/120    avg_loss:0.011, val_acc:0.989]
Epoch [97/120    avg_loss:0.010, val_acc:0.990]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.991]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.989]
Epoch [108/120    avg_loss:0.011, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.991]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.009, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.007, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     3     0    12     0     5     5]
 [    0     0 18068     0    16     0     6     0     0     0]
 [    0     7     0  1993     0     0     0     0    32     4]
 [    0    37    16     0  2884     0     8     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4859     0     0     0]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0    30     0    24    50     0     0     0  3467     0]
 [    0     0     0     0    17    59     0     0     0   843]]

Accuracy:
99.08418287421975

F1 scores:
[       nan 0.99233331 0.99842511 0.98346904 0.97071693 0.97789434
 0.99508499 0.99883586 0.97634469 0.95200452]

Kappa:
0.9878609160468433
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04a77b5b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.188, val_acc:0.106]
Epoch [2/120    avg_loss:1.995, val_acc:0.154]
Epoch [3/120    avg_loss:1.843, val_acc:0.158]
Epoch [4/120    avg_loss:1.713, val_acc:0.168]
Epoch [5/120    avg_loss:1.577, val_acc:0.194]
Epoch [6/120    avg_loss:1.485, val_acc:0.242]
Epoch [7/120    avg_loss:1.358, val_acc:0.358]
Epoch [8/120    avg_loss:1.253, val_acc:0.494]
Epoch [9/120    avg_loss:1.148, val_acc:0.524]
Epoch [10/120    avg_loss:1.075, val_acc:0.636]
Epoch [11/120    avg_loss:0.971, val_acc:0.622]
Epoch [12/120    avg_loss:0.813, val_acc:0.717]
Epoch [13/120    avg_loss:0.740, val_acc:0.704]
Epoch [14/120    avg_loss:0.648, val_acc:0.763]
Epoch [15/120    avg_loss:0.566, val_acc:0.774]
Epoch [16/120    avg_loss:0.507, val_acc:0.802]
Epoch [17/120    avg_loss:0.444, val_acc:0.836]
Epoch [18/120    avg_loss:0.411, val_acc:0.846]
Epoch [19/120    avg_loss:0.352, val_acc:0.825]
Epoch [20/120    avg_loss:0.337, val_acc:0.895]
Epoch [21/120    avg_loss:0.310, val_acc:0.909]
Epoch [22/120    avg_loss:0.280, val_acc:0.914]
Epoch [23/120    avg_loss:0.255, val_acc:0.907]
Epoch [24/120    avg_loss:0.242, val_acc:0.931]
Epoch [25/120    avg_loss:0.245, val_acc:0.925]
Epoch [26/120    avg_loss:0.203, val_acc:0.934]
Epoch [27/120    avg_loss:0.175, val_acc:0.954]
Epoch [28/120    avg_loss:0.167, val_acc:0.941]
Epoch [29/120    avg_loss:0.139, val_acc:0.952]
Epoch [30/120    avg_loss:0.154, val_acc:0.864]
Epoch [31/120    avg_loss:0.156, val_acc:0.967]
Epoch [32/120    avg_loss:0.130, val_acc:0.931]
Epoch [33/120    avg_loss:0.129, val_acc:0.917]
Epoch [34/120    avg_loss:0.108, val_acc:0.964]
Epoch [35/120    avg_loss:0.089, val_acc:0.960]
Epoch [36/120    avg_loss:0.076, val_acc:0.963]
Epoch [37/120    avg_loss:0.076, val_acc:0.965]
Epoch [38/120    avg_loss:0.077, val_acc:0.978]
Epoch [39/120    avg_loss:0.081, val_acc:0.983]
Epoch [40/120    avg_loss:0.076, val_acc:0.971]
Epoch [41/120    avg_loss:0.082, val_acc:0.976]
Epoch [42/120    avg_loss:0.090, val_acc:0.972]
Epoch [43/120    avg_loss:0.077, val_acc:0.921]
Epoch [44/120    avg_loss:0.059, val_acc:0.984]
Epoch [45/120    avg_loss:0.041, val_acc:0.987]
Epoch [46/120    avg_loss:0.041, val_acc:0.991]
Epoch [47/120    avg_loss:0.044, val_acc:0.954]
Epoch [48/120    avg_loss:0.076, val_acc:0.983]
Epoch [49/120    avg_loss:0.067, val_acc:0.990]
Epoch [50/120    avg_loss:0.036, val_acc:0.988]
Epoch [51/120    avg_loss:0.034, val_acc:0.982]
Epoch [52/120    avg_loss:0.034, val_acc:0.986]
Epoch [53/120    avg_loss:0.043, val_acc:0.990]
Epoch [54/120    avg_loss:0.053, val_acc:0.989]
Epoch [55/120    avg_loss:0.032, val_acc:0.987]
Epoch [56/120    avg_loss:0.027, val_acc:0.991]
Epoch [57/120    avg_loss:0.026, val_acc:0.990]
Epoch [58/120    avg_loss:0.019, val_acc:0.990]
Epoch [59/120    avg_loss:0.024, val_acc:0.991]
Epoch [60/120    avg_loss:0.036, val_acc:0.987]
Epoch [61/120    avg_loss:0.028, val_acc:0.990]
Epoch [62/120    avg_loss:0.027, val_acc:0.991]
Epoch [63/120    avg_loss:0.017, val_acc:0.985]
Epoch [64/120    avg_loss:0.014, val_acc:0.989]
Epoch [65/120    avg_loss:0.016, val_acc:0.992]
Epoch [66/120    avg_loss:0.017, val_acc:0.990]
Epoch [67/120    avg_loss:0.013, val_acc:0.992]
Epoch [68/120    avg_loss:0.011, val_acc:0.993]
Epoch [69/120    avg_loss:0.013, val_acc:0.991]
Epoch [70/120    avg_loss:0.019, val_acc:0.992]
Epoch [71/120    avg_loss:0.013, val_acc:0.990]
Epoch [72/120    avg_loss:0.011, val_acc:0.994]
Epoch [73/120    avg_loss:0.011, val_acc:0.989]
Epoch [74/120    avg_loss:0.015, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.990]
Epoch [76/120    avg_loss:0.014, val_acc:0.992]
Epoch [77/120    avg_loss:0.011, val_acc:0.993]
Epoch [78/120    avg_loss:0.018, val_acc:0.992]
Epoch [79/120    avg_loss:0.012, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.991]
Epoch [82/120    avg_loss:0.008, val_acc:0.992]
Epoch [83/120    avg_loss:0.028, val_acc:0.959]
Epoch [84/120    avg_loss:0.053, val_acc:0.985]
Epoch [85/120    avg_loss:0.019, val_acc:0.991]
Epoch [86/120    avg_loss:0.013, val_acc:0.993]
Epoch [87/120    avg_loss:0.010, val_acc:0.992]
Epoch [88/120    avg_loss:0.009, val_acc:0.992]
Epoch [89/120    avg_loss:0.012, val_acc:0.992]
Epoch [90/120    avg_loss:0.009, val_acc:0.991]
Epoch [91/120    avg_loss:0.009, val_acc:0.991]
Epoch [92/120    avg_loss:0.009, val_acc:0.991]
Epoch [93/120    avg_loss:0.009, val_acc:0.991]
Epoch [94/120    avg_loss:0.009, val_acc:0.991]
Epoch [95/120    avg_loss:0.009, val_acc:0.991]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.991]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.008, val_acc:0.992]
Epoch [100/120    avg_loss:0.009, val_acc:0.992]
Epoch [101/120    avg_loss:0.010, val_acc:0.992]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.008, val_acc:0.992]
Epoch [105/120    avg_loss:0.008, val_acc:0.992]
Epoch [106/120    avg_loss:0.008, val_acc:0.992]
Epoch [107/120    avg_loss:0.008, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.009, val_acc:0.992]
Epoch [110/120    avg_loss:0.009, val_acc:0.992]
Epoch [111/120    avg_loss:0.010, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     1     0     0     0    34     0]
 [    0     1 18048     0    27     0    14     0     0     0]
 [    0     5     0  2004     2     0     0     0    24     1]
 [    0    38    21     0  2879     0     6     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     7     0     0  4858     0     0     8]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    39     0    12    58     0     0     0  3462     0]
 [    0     0     0     1    14    32     0     0     1   871]]

Accuracy:
99.08418287421975

F1 scores:
[       nan 0.99086121 0.99811968 0.98719212 0.96724341 0.98788796
 0.99589996 0.99961225 0.97247191 0.96777778]

Kappa:
0.987865349949281
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf05724b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.227, val_acc:0.139]
Epoch [2/120    avg_loss:2.037, val_acc:0.165]
Epoch [3/120    avg_loss:1.880, val_acc:0.208]
Epoch [4/120    avg_loss:1.742, val_acc:0.289]
Epoch [5/120    avg_loss:1.569, val_acc:0.284]
Epoch [6/120    avg_loss:1.466, val_acc:0.314]
Epoch [7/120    avg_loss:1.376, val_acc:0.340]
Epoch [8/120    avg_loss:1.262, val_acc:0.378]
Epoch [9/120    avg_loss:1.191, val_acc:0.419]
Epoch [10/120    avg_loss:1.118, val_acc:0.408]
Epoch [11/120    avg_loss:1.034, val_acc:0.509]
Epoch [12/120    avg_loss:0.951, val_acc:0.624]
Epoch [13/120    avg_loss:0.843, val_acc:0.645]
Epoch [14/120    avg_loss:0.725, val_acc:0.690]
Epoch [15/120    avg_loss:0.654, val_acc:0.776]
Epoch [16/120    avg_loss:0.589, val_acc:0.772]
Epoch [17/120    avg_loss:0.568, val_acc:0.796]
Epoch [18/120    avg_loss:0.503, val_acc:0.821]
Epoch [19/120    avg_loss:0.447, val_acc:0.851]
Epoch [20/120    avg_loss:0.418, val_acc:0.821]
Epoch [21/120    avg_loss:0.389, val_acc:0.871]
Epoch [22/120    avg_loss:0.334, val_acc:0.835]
Epoch [23/120    avg_loss:0.280, val_acc:0.896]
Epoch [24/120    avg_loss:0.256, val_acc:0.896]
Epoch [25/120    avg_loss:0.229, val_acc:0.915]
Epoch [26/120    avg_loss:0.209, val_acc:0.926]
Epoch [27/120    avg_loss:0.181, val_acc:0.946]
Epoch [28/120    avg_loss:0.202, val_acc:0.942]
Epoch [29/120    avg_loss:0.185, val_acc:0.915]
Epoch [30/120    avg_loss:0.159, val_acc:0.959]
Epoch [31/120    avg_loss:0.153, val_acc:0.950]
Epoch [32/120    avg_loss:0.132, val_acc:0.944]
Epoch [33/120    avg_loss:0.176, val_acc:0.933]
Epoch [34/120    avg_loss:0.124, val_acc:0.955]
Epoch [35/120    avg_loss:0.107, val_acc:0.973]
Epoch [36/120    avg_loss:0.086, val_acc:0.944]
Epoch [37/120    avg_loss:0.103, val_acc:0.972]
Epoch [38/120    avg_loss:0.093, val_acc:0.970]
Epoch [39/120    avg_loss:0.076, val_acc:0.939]
Epoch [40/120    avg_loss:0.094, val_acc:0.964]
Epoch [41/120    avg_loss:0.078, val_acc:0.968]
Epoch [42/120    avg_loss:0.072, val_acc:0.978]
Epoch [43/120    avg_loss:0.056, val_acc:0.910]
Epoch [44/120    avg_loss:0.073, val_acc:0.962]
Epoch [45/120    avg_loss:0.060, val_acc:0.977]
Epoch [46/120    avg_loss:0.055, val_acc:0.974]
Epoch [47/120    avg_loss:0.043, val_acc:0.981]
Epoch [48/120    avg_loss:0.041, val_acc:0.981]
Epoch [49/120    avg_loss:0.044, val_acc:0.960]
Epoch [50/120    avg_loss:0.053, val_acc:0.967]
Epoch [51/120    avg_loss:0.047, val_acc:0.969]
Epoch [52/120    avg_loss:0.041, val_acc:0.972]
Epoch [53/120    avg_loss:0.031, val_acc:0.982]
Epoch [54/120    avg_loss:0.033, val_acc:0.973]
Epoch [55/120    avg_loss:0.026, val_acc:0.982]
Epoch [56/120    avg_loss:0.023, val_acc:0.976]
Epoch [57/120    avg_loss:0.026, val_acc:0.979]
Epoch [58/120    avg_loss:0.020, val_acc:0.985]
Epoch [59/120    avg_loss:0.018, val_acc:0.983]
Epoch [60/120    avg_loss:0.021, val_acc:0.982]
Epoch [61/120    avg_loss:0.029, val_acc:0.976]
Epoch [62/120    avg_loss:0.031, val_acc:0.973]
Epoch [63/120    avg_loss:0.024, val_acc:0.981]
Epoch [64/120    avg_loss:0.022, val_acc:0.986]
Epoch [65/120    avg_loss:0.019, val_acc:0.979]
Epoch [66/120    avg_loss:0.016, val_acc:0.982]
Epoch [67/120    avg_loss:0.016, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.020, val_acc:0.967]
Epoch [70/120    avg_loss:0.015, val_acc:0.984]
Epoch [71/120    avg_loss:0.022, val_acc:0.984]
Epoch [72/120    avg_loss:0.030, val_acc:0.984]
Epoch [73/120    avg_loss:0.022, val_acc:0.984]
Epoch [74/120    avg_loss:0.022, val_acc:0.983]
Epoch [75/120    avg_loss:0.023, val_acc:0.979]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.017, val_acc:0.979]
Epoch [78/120    avg_loss:0.022, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.985]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.010, val_acc:0.983]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.013, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.008, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     0     0     0     3     6     0]
 [    0     0 18018     0    36     0    36     0     0     0]
 [    0    11     0  1985     0     0     0     0    39     1]
 [    0    43    12     0  2881     0    10     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     9     0     0  4855     0     3     0]
 [    0     1     0     0     0     0     0  1281     0     8]
 [    0    47     0     0    49     0     0     0  3474     1]
 [    0     0     0     1    13    45     0     0     0   860]]

Accuracy:
99.00947147711662

F1 scores:
[       nan 0.9914332  0.99737068 0.98486728 0.96824063 0.98305085
 0.99294406 0.995338   0.97597977 0.96143097]

Kappa:
0.9868780788725763
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8510a3b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.225, val_acc:0.068]
Epoch [2/120    avg_loss:2.024, val_acc:0.075]
Epoch [3/120    avg_loss:1.866, val_acc:0.110]
Epoch [4/120    avg_loss:1.739, val_acc:0.311]
Epoch [5/120    avg_loss:1.585, val_acc:0.289]
Epoch [6/120    avg_loss:1.483, val_acc:0.369]
Epoch [7/120    avg_loss:1.388, val_acc:0.553]
Epoch [8/120    avg_loss:1.262, val_acc:0.563]
Epoch [9/120    avg_loss:1.133, val_acc:0.653]
Epoch [10/120    avg_loss:1.070, val_acc:0.674]
Epoch [11/120    avg_loss:1.027, val_acc:0.672]
Epoch [12/120    avg_loss:0.860, val_acc:0.593]
Epoch [13/120    avg_loss:0.802, val_acc:0.670]
Epoch [14/120    avg_loss:0.819, val_acc:0.689]
Epoch [15/120    avg_loss:0.677, val_acc:0.639]
Epoch [16/120    avg_loss:0.563, val_acc:0.763]
Epoch [17/120    avg_loss:0.558, val_acc:0.780]
Epoch [18/120    avg_loss:0.471, val_acc:0.798]
Epoch [19/120    avg_loss:0.460, val_acc:0.817]
Epoch [20/120    avg_loss:0.421, val_acc:0.842]
Epoch [21/120    avg_loss:0.354, val_acc:0.838]
Epoch [22/120    avg_loss:0.332, val_acc:0.885]
Epoch [23/120    avg_loss:0.312, val_acc:0.897]
Epoch [24/120    avg_loss:0.266, val_acc:0.899]
Epoch [25/120    avg_loss:0.243, val_acc:0.911]
Epoch [26/120    avg_loss:0.210, val_acc:0.906]
Epoch [27/120    avg_loss:0.209, val_acc:0.930]
Epoch [28/120    avg_loss:0.176, val_acc:0.935]
Epoch [29/120    avg_loss:0.192, val_acc:0.918]
Epoch [30/120    avg_loss:0.186, val_acc:0.917]
Epoch [31/120    avg_loss:0.198, val_acc:0.921]
Epoch [32/120    avg_loss:0.136, val_acc:0.908]
Epoch [33/120    avg_loss:0.127, val_acc:0.959]
Epoch [34/120    avg_loss:0.147, val_acc:0.951]
Epoch [35/120    avg_loss:0.998, val_acc:0.487]
Epoch [36/120    avg_loss:1.365, val_acc:0.492]
Epoch [37/120    avg_loss:1.267, val_acc:0.411]
Epoch [38/120    avg_loss:1.171, val_acc:0.472]
Epoch [39/120    avg_loss:1.120, val_acc:0.570]
Epoch [40/120    avg_loss:1.045, val_acc:0.579]
Epoch [41/120    avg_loss:0.964, val_acc:0.585]
Epoch [42/120    avg_loss:0.961, val_acc:0.607]
Epoch [43/120    avg_loss:0.897, val_acc:0.668]
Epoch [44/120    avg_loss:0.852, val_acc:0.656]
Epoch [45/120    avg_loss:0.761, val_acc:0.712]
Epoch [46/120    avg_loss:0.746, val_acc:0.742]
Epoch [47/120    avg_loss:0.710, val_acc:0.730]
Epoch [48/120    avg_loss:0.692, val_acc:0.742]
Epoch [49/120    avg_loss:0.654, val_acc:0.750]
Epoch [50/120    avg_loss:0.692, val_acc:0.745]
Epoch [51/120    avg_loss:0.660, val_acc:0.756]
Epoch [52/120    avg_loss:0.675, val_acc:0.764]
Epoch [53/120    avg_loss:0.665, val_acc:0.773]
Epoch [54/120    avg_loss:0.658, val_acc:0.777]
Epoch [55/120    avg_loss:0.652, val_acc:0.764]
Epoch [56/120    avg_loss:0.652, val_acc:0.773]
Epoch [57/120    avg_loss:0.650, val_acc:0.807]
Epoch [58/120    avg_loss:0.641, val_acc:0.799]
Epoch [59/120    avg_loss:0.628, val_acc:0.796]
Epoch [60/120    avg_loss:0.625, val_acc:0.799]
Epoch [61/120    avg_loss:0.620, val_acc:0.801]
Epoch [62/120    avg_loss:0.619, val_acc:0.800]
Epoch [63/120    avg_loss:0.612, val_acc:0.799]
Epoch [64/120    avg_loss:0.618, val_acc:0.801]
Epoch [65/120    avg_loss:0.630, val_acc:0.800]
Epoch [66/120    avg_loss:0.603, val_acc:0.802]
Epoch [67/120    avg_loss:0.615, val_acc:0.802]
Epoch [68/120    avg_loss:0.623, val_acc:0.803]
Epoch [69/120    avg_loss:0.620, val_acc:0.803]
Epoch [70/120    avg_loss:0.620, val_acc:0.804]
Epoch [71/120    avg_loss:0.613, val_acc:0.805]
Epoch [72/120    avg_loss:0.607, val_acc:0.802]
Epoch [73/120    avg_loss:0.628, val_acc:0.802]
Epoch [74/120    avg_loss:0.603, val_acc:0.802]
Epoch [75/120    avg_loss:0.628, val_acc:0.802]
Epoch [76/120    avg_loss:0.609, val_acc:0.802]
Epoch [77/120    avg_loss:0.607, val_acc:0.803]
Epoch [78/120    avg_loss:0.613, val_acc:0.804]
Epoch [79/120    avg_loss:0.593, val_acc:0.803]
Epoch [80/120    avg_loss:0.607, val_acc:0.804]
Epoch [81/120    avg_loss:0.613, val_acc:0.803]
Epoch [82/120    avg_loss:0.625, val_acc:0.804]
Epoch [83/120    avg_loss:0.615, val_acc:0.804]
Epoch [84/120    avg_loss:0.604, val_acc:0.803]
Epoch [85/120    avg_loss:0.608, val_acc:0.804]
Epoch [86/120    avg_loss:0.628, val_acc:0.804]
Epoch [87/120    avg_loss:0.602, val_acc:0.804]
Epoch [88/120    avg_loss:0.600, val_acc:0.804]
Epoch [89/120    avg_loss:0.621, val_acc:0.804]
Epoch [90/120    avg_loss:0.617, val_acc:0.804]
Epoch [91/120    avg_loss:0.605, val_acc:0.804]
Epoch [92/120    avg_loss:0.612, val_acc:0.804]
Epoch [93/120    avg_loss:0.637, val_acc:0.804]
Epoch [94/120    avg_loss:0.632, val_acc:0.804]
Epoch [95/120    avg_loss:0.606, val_acc:0.804]
Epoch [96/120    avg_loss:0.615, val_acc:0.804]
Epoch [97/120    avg_loss:0.611, val_acc:0.804]
Epoch [98/120    avg_loss:0.597, val_acc:0.804]
Epoch [99/120    avg_loss:0.609, val_acc:0.804]
Epoch [100/120    avg_loss:0.624, val_acc:0.804]
Epoch [101/120    avg_loss:0.636, val_acc:0.804]
Epoch [102/120    avg_loss:0.610, val_acc:0.804]
Epoch [103/120    avg_loss:0.616, val_acc:0.804]
Epoch [104/120    avg_loss:0.639, val_acc:0.804]
Epoch [105/120    avg_loss:0.613, val_acc:0.804]
Epoch [106/120    avg_loss:0.610, val_acc:0.804]
Epoch [107/120    avg_loss:0.623, val_acc:0.804]
Epoch [108/120    avg_loss:0.626, val_acc:0.804]
Epoch [109/120    avg_loss:0.613, val_acc:0.804]
Epoch [110/120    avg_loss:0.604, val_acc:0.804]
Epoch [111/120    avg_loss:0.618, val_acc:0.804]
Epoch [112/120    avg_loss:0.613, val_acc:0.804]
Epoch [113/120    avg_loss:0.593, val_acc:0.804]
Epoch [114/120    avg_loss:0.632, val_acc:0.804]
Epoch [115/120    avg_loss:0.587, val_acc:0.804]
Epoch [116/120    avg_loss:0.612, val_acc:0.804]
Epoch [117/120    avg_loss:0.638, val_acc:0.804]
Epoch [118/120    avg_loss:0.638, val_acc:0.804]
Epoch [119/120    avg_loss:0.609, val_acc:0.804]
Epoch [120/120    avg_loss:0.647, val_acc:0.804]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4991   303    74   176     0     1    55   643   189]
 [    0     0 13475     0    86     0  4529     0     0     0]
 [    0    22     2  1680     1     0     0     0   214   117]
 [    0    82   584     0  2022     0   242     0    39     3]
 [    0     0     0     0     0  1302     0     3     0     0]
 [    0     0   151   181    47     0  4478     0    21     0]
 [    0   100     0     6    29     0     0  1136     4    15]
 [    0   185   185   170    32     0    16     0  2983     0]
 [    0    24     0     5    16    97     0     0    15   762]]

Accuracy:
79.11936953221026

F1 scores:
[       nan 0.84335924 0.82189692 0.80924855 0.75153317 0.96301775
 0.63320136 0.91465378 0.7965287  0.76009975]

Kappa:
0.7329081485153999
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c6ce39be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.239, val_acc:0.168]
Epoch [2/120    avg_loss:2.074, val_acc:0.314]
Epoch [3/120    avg_loss:1.924, val_acc:0.481]
Epoch [4/120    avg_loss:1.777, val_acc:0.470]
Epoch [5/120    avg_loss:1.635, val_acc:0.391]
Epoch [6/120    avg_loss:1.488, val_acc:0.392]
Epoch [7/120    avg_loss:1.367, val_acc:0.530]
Epoch [8/120    avg_loss:1.269, val_acc:0.553]
Epoch [9/120    avg_loss:1.156, val_acc:0.638]
Epoch [10/120    avg_loss:1.025, val_acc:0.666]
Epoch [11/120    avg_loss:0.937, val_acc:0.662]
Epoch [12/120    avg_loss:0.871, val_acc:0.530]
Epoch [13/120    avg_loss:0.754, val_acc:0.644]
Epoch [14/120    avg_loss:0.682, val_acc:0.730]
Epoch [15/120    avg_loss:0.627, val_acc:0.736]
Epoch [16/120    avg_loss:0.549, val_acc:0.758]
Epoch [17/120    avg_loss:0.500, val_acc:0.774]
Epoch [18/120    avg_loss:0.474, val_acc:0.774]
Epoch [19/120    avg_loss:0.438, val_acc:0.802]
Epoch [20/120    avg_loss:0.401, val_acc:0.825]
Epoch [21/120    avg_loss:0.354, val_acc:0.841]
Epoch [22/120    avg_loss:0.324, val_acc:0.860]
Epoch [23/120    avg_loss:0.318, val_acc:0.880]
Epoch [24/120    avg_loss:0.269, val_acc:0.891]
Epoch [25/120    avg_loss:0.245, val_acc:0.920]
Epoch [26/120    avg_loss:0.236, val_acc:0.926]
Epoch [27/120    avg_loss:0.205, val_acc:0.927]
Epoch [28/120    avg_loss:0.205, val_acc:0.871]
Epoch [29/120    avg_loss:0.166, val_acc:0.933]
Epoch [30/120    avg_loss:0.178, val_acc:0.934]
Epoch [31/120    avg_loss:0.135, val_acc:0.943]
Epoch [32/120    avg_loss:0.121, val_acc:0.955]
Epoch [33/120    avg_loss:0.131, val_acc:0.927]
Epoch [34/120    avg_loss:0.103, val_acc:0.927]
Epoch [35/120    avg_loss:0.093, val_acc:0.957]
Epoch [36/120    avg_loss:0.107, val_acc:0.927]
Epoch [37/120    avg_loss:0.107, val_acc:0.954]
Epoch [38/120    avg_loss:0.137, val_acc:0.933]
Epoch [39/120    avg_loss:0.107, val_acc:0.963]
Epoch [40/120    avg_loss:0.073, val_acc:0.971]
Epoch [41/120    avg_loss:0.073, val_acc:0.954]
Epoch [42/120    avg_loss:0.060, val_acc:0.976]
Epoch [43/120    avg_loss:0.058, val_acc:0.968]
Epoch [44/120    avg_loss:0.087, val_acc:0.947]
Epoch [45/120    avg_loss:0.141, val_acc:0.954]
Epoch [46/120    avg_loss:0.080, val_acc:0.961]
Epoch [47/120    avg_loss:0.066, val_acc:0.951]
Epoch [48/120    avg_loss:0.047, val_acc:0.970]
Epoch [49/120    avg_loss:0.046, val_acc:0.975]
Epoch [50/120    avg_loss:0.040, val_acc:0.974]
Epoch [51/120    avg_loss:0.049, val_acc:0.975]
Epoch [52/120    avg_loss:0.037, val_acc:0.972]
Epoch [53/120    avg_loss:0.027, val_acc:0.978]
Epoch [54/120    avg_loss:0.032, val_acc:0.982]
Epoch [55/120    avg_loss:0.040, val_acc:0.969]
Epoch [56/120    avg_loss:0.036, val_acc:0.981]
Epoch [57/120    avg_loss:0.034, val_acc:0.977]
Epoch [58/120    avg_loss:0.029, val_acc:0.972]
Epoch [59/120    avg_loss:0.032, val_acc:0.974]
Epoch [60/120    avg_loss:0.032, val_acc:0.979]
Epoch [61/120    avg_loss:0.028, val_acc:0.974]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.022, val_acc:0.981]
Epoch [64/120    avg_loss:0.021, val_acc:0.981]
Epoch [65/120    avg_loss:0.028, val_acc:0.976]
Epoch [66/120    avg_loss:0.017, val_acc:0.978]
Epoch [67/120    avg_loss:0.015, val_acc:0.980]
Epoch [68/120    avg_loss:0.016, val_acc:0.982]
Epoch [69/120    avg_loss:0.013, val_acc:0.982]
Epoch [70/120    avg_loss:0.012, val_acc:0.984]
Epoch [71/120    avg_loss:0.014, val_acc:0.984]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.012, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.987]
Epoch [81/120    avg_loss:0.013, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.012, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.987]
Epoch [92/120    avg_loss:0.011, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.986]
Epoch [94/120    avg_loss:0.011, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.987]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.012, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.013, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.012, val_acc:0.986]
Epoch [109/120    avg_loss:0.013, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     0     7     0     0    46    15     5]
 [    0     8 18035     0    40     0     7     0     0     0]
 [    0    10     0  1973     0     0     0     0    39    14]
 [    0    33    14     0  2879     0     9     0    37     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4850     0     0    17]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    26     0    19    53     0     0     0  3473     0]
 [    0     0     0     0    14    52     0     0     0   853]]

Accuracy:
98.85281854770685

F1 scores:
[       nan 0.98834318 0.997787   0.9796425  0.96529757 0.98046582
 0.9954844  0.98248286 0.97351086 0.94358407]

Kappa:
0.9848053279959408
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd00ee0b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.177, val_acc:0.068]
Epoch [2/120    avg_loss:1.984, val_acc:0.073]
Epoch [3/120    avg_loss:1.859, val_acc:0.164]
Epoch [4/120    avg_loss:1.758, val_acc:0.345]
Epoch [5/120    avg_loss:1.608, val_acc:0.433]
Epoch [6/120    avg_loss:1.511, val_acc:0.556]
Epoch [7/120    avg_loss:1.391, val_acc:0.618]
Epoch [8/120    avg_loss:1.310, val_acc:0.669]
Epoch [9/120    avg_loss:1.236, val_acc:0.709]
Epoch [10/120    avg_loss:1.105, val_acc:0.764]
Epoch [11/120    avg_loss:1.022, val_acc:0.750]
Epoch [12/120    avg_loss:0.893, val_acc:0.755]
Epoch [13/120    avg_loss:0.804, val_acc:0.769]
Epoch [14/120    avg_loss:0.683, val_acc:0.755]
Epoch [15/120    avg_loss:0.587, val_acc:0.758]
Epoch [16/120    avg_loss:0.543, val_acc:0.764]
Epoch [17/120    avg_loss:0.481, val_acc:0.805]
Epoch [18/120    avg_loss:0.422, val_acc:0.842]
Epoch [19/120    avg_loss:0.437, val_acc:0.835]
Epoch [20/120    avg_loss:0.448, val_acc:0.826]
Epoch [21/120    avg_loss:0.376, val_acc:0.840]
Epoch [22/120    avg_loss:0.325, val_acc:0.903]
Epoch [23/120    avg_loss:0.292, val_acc:0.905]
Epoch [24/120    avg_loss:0.265, val_acc:0.911]
Epoch [25/120    avg_loss:0.303, val_acc:0.872]
Epoch [26/120    avg_loss:0.212, val_acc:0.922]
Epoch [27/120    avg_loss:0.230, val_acc:0.931]
Epoch [28/120    avg_loss:0.212, val_acc:0.942]
Epoch [29/120    avg_loss:0.205, val_acc:0.945]
Epoch [30/120    avg_loss:0.164, val_acc:0.926]
Epoch [31/120    avg_loss:0.148, val_acc:0.958]
Epoch [32/120    avg_loss:0.136, val_acc:0.941]
Epoch [33/120    avg_loss:0.140, val_acc:0.940]
Epoch [34/120    avg_loss:0.112, val_acc:0.960]
Epoch [35/120    avg_loss:0.132, val_acc:0.950]
Epoch [36/120    avg_loss:0.125, val_acc:0.923]
Epoch [37/120    avg_loss:0.128, val_acc:0.942]
Epoch [38/120    avg_loss:0.114, val_acc:0.959]
Epoch [39/120    avg_loss:0.081, val_acc:0.965]
Epoch [40/120    avg_loss:0.072, val_acc:0.971]
Epoch [41/120    avg_loss:0.086, val_acc:0.970]
Epoch [42/120    avg_loss:0.099, val_acc:0.960]
Epoch [43/120    avg_loss:0.074, val_acc:0.963]
Epoch [44/120    avg_loss:0.076, val_acc:0.975]
Epoch [45/120    avg_loss:0.074, val_acc:0.966]
Epoch [46/120    avg_loss:0.086, val_acc:0.970]
Epoch [47/120    avg_loss:1.097, val_acc:0.499]
Epoch [48/120    avg_loss:1.173, val_acc:0.544]
Epoch [49/120    avg_loss:1.110, val_acc:0.490]
Epoch [50/120    avg_loss:1.081, val_acc:0.515]
Epoch [51/120    avg_loss:1.007, val_acc:0.490]
Epoch [52/120    avg_loss:0.983, val_acc:0.540]
Epoch [53/120    avg_loss:0.950, val_acc:0.576]
Epoch [54/120    avg_loss:0.909, val_acc:0.572]
Epoch [55/120    avg_loss:0.875, val_acc:0.608]
Epoch [56/120    avg_loss:0.895, val_acc:0.605]
Epoch [57/120    avg_loss:0.840, val_acc:0.628]
Epoch [58/120    avg_loss:0.803, val_acc:0.623]
Epoch [59/120    avg_loss:0.810, val_acc:0.628]
Epoch [60/120    avg_loss:0.789, val_acc:0.637]
Epoch [61/120    avg_loss:0.761, val_acc:0.638]
Epoch [62/120    avg_loss:0.749, val_acc:0.645]
Epoch [63/120    avg_loss:0.775, val_acc:0.643]
Epoch [64/120    avg_loss:0.736, val_acc:0.661]
Epoch [65/120    avg_loss:0.740, val_acc:0.660]
Epoch [66/120    avg_loss:0.757, val_acc:0.661]
Epoch [67/120    avg_loss:0.739, val_acc:0.671]
Epoch [68/120    avg_loss:0.713, val_acc:0.675]
Epoch [69/120    avg_loss:0.710, val_acc:0.660]
Epoch [70/120    avg_loss:0.704, val_acc:0.670]
Epoch [71/120    avg_loss:0.695, val_acc:0.668]
Epoch [72/120    avg_loss:0.682, val_acc:0.672]
Epoch [73/120    avg_loss:0.693, val_acc:0.671]
Epoch [74/120    avg_loss:0.682, val_acc:0.674]
Epoch [75/120    avg_loss:0.680, val_acc:0.674]
Epoch [76/120    avg_loss:0.695, val_acc:0.674]
Epoch [77/120    avg_loss:0.679, val_acc:0.674]
Epoch [78/120    avg_loss:0.673, val_acc:0.674]
Epoch [79/120    avg_loss:0.676, val_acc:0.675]
Epoch [80/120    avg_loss:0.669, val_acc:0.680]
Epoch [81/120    avg_loss:0.694, val_acc:0.678]
Epoch [82/120    avg_loss:0.696, val_acc:0.678]
Epoch [83/120    avg_loss:0.675, val_acc:0.681]
Epoch [84/120    avg_loss:0.689, val_acc:0.683]
Epoch [85/120    avg_loss:0.673, val_acc:0.681]
Epoch [86/120    avg_loss:0.684, val_acc:0.683]
Epoch [87/120    avg_loss:0.698, val_acc:0.683]
Epoch [88/120    avg_loss:0.676, val_acc:0.682]
Epoch [89/120    avg_loss:0.684, val_acc:0.682]
Epoch [90/120    avg_loss:0.681, val_acc:0.681]
Epoch [91/120    avg_loss:0.701, val_acc:0.681]
Epoch [92/120    avg_loss:0.675, val_acc:0.681]
Epoch [93/120    avg_loss:0.688, val_acc:0.681]
Epoch [94/120    avg_loss:0.671, val_acc:0.681]
Epoch [95/120    avg_loss:0.703, val_acc:0.681]
Epoch [96/120    avg_loss:0.668, val_acc:0.680]
Epoch [97/120    avg_loss:0.671, val_acc:0.681]
Epoch [98/120    avg_loss:0.667, val_acc:0.681]
Epoch [99/120    avg_loss:0.673, val_acc:0.681]
Epoch [100/120    avg_loss:0.676, val_acc:0.680]
Epoch [101/120    avg_loss:0.695, val_acc:0.680]
Epoch [102/120    avg_loss:0.675, val_acc:0.681]
Epoch [103/120    avg_loss:0.709, val_acc:0.681]
Epoch [104/120    avg_loss:0.677, val_acc:0.681]
Epoch [105/120    avg_loss:0.677, val_acc:0.680]
Epoch [106/120    avg_loss:0.681, val_acc:0.680]
Epoch [107/120    avg_loss:0.673, val_acc:0.681]
Epoch [108/120    avg_loss:0.668, val_acc:0.680]
Epoch [109/120    avg_loss:0.685, val_acc:0.680]
Epoch [110/120    avg_loss:0.672, val_acc:0.680]
Epoch [111/120    avg_loss:0.667, val_acc:0.680]
Epoch [112/120    avg_loss:0.670, val_acc:0.680]
Epoch [113/120    avg_loss:0.683, val_acc:0.680]
Epoch [114/120    avg_loss:0.682, val_acc:0.681]
Epoch [115/120    avg_loss:0.704, val_acc:0.681]
Epoch [116/120    avg_loss:0.688, val_acc:0.681]
Epoch [117/120    avg_loss:0.673, val_acc:0.681]
Epoch [118/120    avg_loss:0.682, val_acc:0.680]
Epoch [119/120    avg_loss:0.673, val_acc:0.681]
Epoch [120/120    avg_loss:0.688, val_acc:0.681]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  3992   449   184   251     0   581   195   407   373]
 [    0     2 10935     0  1925     0  5228     0     0     0]
 [    0     2     0  1850     0     0    19     0    80    85]
 [    0    56   252     0  2316     0   309     0    33     6]
 [    0     0     0     0     0  1302     0     3     0     0]
 [    0     0   206   249    85     0  4248     0    90     0]
 [    0   109     0     0    25     0     0  1132     1    23]
 [    0   133   232   285    53     0    56     0  2809     3]
 [    0    20     1    10    19   137     0     0     1   731]]

Accuracy:
70.65047116381076

F1 scores:
[       nan 0.74297413 0.72501243 0.80190724 0.60580696 0.94897959
 0.55460539 0.86412214 0.8034897  0.68317757]

Kappa:
0.6360759731067115
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8248bbdb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.229, val_acc:0.110]
Epoch [2/120    avg_loss:2.001, val_acc:0.152]
Epoch [3/120    avg_loss:1.874, val_acc:0.167]
Epoch [4/120    avg_loss:1.712, val_acc:0.194]
Epoch [5/120    avg_loss:1.573, val_acc:0.249]
Epoch [6/120    avg_loss:1.450, val_acc:0.361]
Epoch [7/120    avg_loss:1.317, val_acc:0.392]
Epoch [8/120    avg_loss:1.198, val_acc:0.470]
Epoch [9/120    avg_loss:1.118, val_acc:0.546]
Epoch [10/120    avg_loss:1.005, val_acc:0.615]
Epoch [11/120    avg_loss:0.930, val_acc:0.629]
Epoch [12/120    avg_loss:0.820, val_acc:0.671]
Epoch [13/120    avg_loss:0.680, val_acc:0.663]
Epoch [14/120    avg_loss:0.589, val_acc:0.747]
Epoch [15/120    avg_loss:0.558, val_acc:0.737]
Epoch [16/120    avg_loss:0.458, val_acc:0.800]
Epoch [17/120    avg_loss:0.414, val_acc:0.795]
Epoch [18/120    avg_loss:0.354, val_acc:0.889]
Epoch [19/120    avg_loss:0.353, val_acc:0.875]
Epoch [20/120    avg_loss:0.283, val_acc:0.886]
Epoch [21/120    avg_loss:0.268, val_acc:0.876]
Epoch [22/120    avg_loss:0.229, val_acc:0.902]
Epoch [23/120    avg_loss:0.218, val_acc:0.921]
Epoch [24/120    avg_loss:0.255, val_acc:0.895]
Epoch [25/120    avg_loss:0.195, val_acc:0.938]
Epoch [26/120    avg_loss:0.187, val_acc:0.951]
Epoch [27/120    avg_loss:0.160, val_acc:0.947]
Epoch [28/120    avg_loss:0.137, val_acc:0.946]
Epoch [29/120    avg_loss:0.131, val_acc:0.961]
Epoch [30/120    avg_loss:0.116, val_acc:0.941]
Epoch [31/120    avg_loss:0.137, val_acc:0.965]
Epoch [32/120    avg_loss:0.122, val_acc:0.955]
Epoch [33/120    avg_loss:0.074, val_acc:0.954]
Epoch [34/120    avg_loss:0.098, val_acc:0.954]
Epoch [35/120    avg_loss:0.105, val_acc:0.965]
Epoch [36/120    avg_loss:0.072, val_acc:0.955]
Epoch [37/120    avg_loss:0.065, val_acc:0.969]
Epoch [38/120    avg_loss:0.107, val_acc:0.936]
Epoch [39/120    avg_loss:0.090, val_acc:0.970]
Epoch [40/120    avg_loss:0.079, val_acc:0.968]
Epoch [41/120    avg_loss:0.054, val_acc:0.972]
Epoch [42/120    avg_loss:0.087, val_acc:0.958]
Epoch [43/120    avg_loss:0.050, val_acc:0.976]
Epoch [44/120    avg_loss:0.043, val_acc:0.979]
Epoch [45/120    avg_loss:0.088, val_acc:0.963]
Epoch [46/120    avg_loss:0.106, val_acc:0.966]
Epoch [47/120    avg_loss:0.051, val_acc:0.978]
Epoch [48/120    avg_loss:0.046, val_acc:0.974]
Epoch [49/120    avg_loss:0.048, val_acc:0.986]
Epoch [50/120    avg_loss:0.041, val_acc:0.983]
Epoch [51/120    avg_loss:0.046, val_acc:0.948]
Epoch [52/120    avg_loss:0.032, val_acc:0.977]
Epoch [53/120    avg_loss:0.083, val_acc:0.952]
Epoch [54/120    avg_loss:0.049, val_acc:0.968]
Epoch [55/120    avg_loss:0.037, val_acc:0.976]
Epoch [56/120    avg_loss:0.039, val_acc:0.986]
Epoch [57/120    avg_loss:0.030, val_acc:0.976]
Epoch [58/120    avg_loss:0.034, val_acc:0.979]
Epoch [59/120    avg_loss:0.059, val_acc:0.972]
Epoch [60/120    avg_loss:0.059, val_acc:0.974]
Epoch [61/120    avg_loss:0.067, val_acc:0.963]
Epoch [62/120    avg_loss:0.064, val_acc:0.947]
Epoch [63/120    avg_loss:0.098, val_acc:0.965]
Epoch [64/120    avg_loss:0.054, val_acc:0.957]
Epoch [65/120    avg_loss:0.030, val_acc:0.981]
Epoch [66/120    avg_loss:0.027, val_acc:0.981]
Epoch [67/120    avg_loss:0.028, val_acc:0.975]
Epoch [68/120    avg_loss:0.022, val_acc:0.980]
Epoch [69/120    avg_loss:0.040, val_acc:0.976]
Epoch [70/120    avg_loss:0.021, val_acc:0.981]
Epoch [71/120    avg_loss:0.020, val_acc:0.985]
Epoch [72/120    avg_loss:0.014, val_acc:0.986]
Epoch [73/120    avg_loss:0.016, val_acc:0.986]
Epoch [74/120    avg_loss:0.017, val_acc:0.988]
Epoch [75/120    avg_loss:0.014, val_acc:0.987]
Epoch [76/120    avg_loss:0.016, val_acc:0.988]
Epoch [77/120    avg_loss:0.013, val_acc:0.988]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.987]
Epoch [80/120    avg_loss:0.012, val_acc:0.987]
Epoch [81/120    avg_loss:0.011, val_acc:0.988]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.013, val_acc:0.987]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.012, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.987]
Epoch [89/120    avg_loss:0.012, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.987]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.987]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.011, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.987]
Epoch [102/120    avg_loss:0.011, val_acc:0.987]
Epoch [103/120    avg_loss:0.012, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.987]
Epoch [105/120    avg_loss:0.011, val_acc:0.987]
Epoch [106/120    avg_loss:0.011, val_acc:0.987]
Epoch [107/120    avg_loss:0.011, val_acc:0.987]
Epoch [108/120    avg_loss:0.011, val_acc:0.987]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.012, val_acc:0.987]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.987]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.012, val_acc:0.987]
Epoch [115/120    avg_loss:0.012, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.987]
Epoch [118/120    avg_loss:0.013, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6404     0     0     1     0     0    14    11     2]
 [    0     0 18025     0    49     0    16     0     0     0]
 [    0     0     1  1985     2     0     0     0    46     2]
 [    0    25    12     0  2905     0     0     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     1     0     0  4873     0     0     2]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     5     0    21    56     0     0     0  3488     1]
 [    0     0     0     0    13    47     0     0     0   859]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.99549199 0.99778577 0.9819441  0.96865622 0.98231088
 0.9978499  0.99421519 0.97621047 0.96192609]

Kappa:
0.9885096799743166
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f2b6e4c50>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.178, val_acc:0.098]
Epoch [2/120    avg_loss:1.975, val_acc:0.110]
Epoch [3/120    avg_loss:1.797, val_acc:0.101]
Epoch [4/120    avg_loss:1.649, val_acc:0.143]
Epoch [5/120    avg_loss:1.533, val_acc:0.274]
Epoch [6/120    avg_loss:1.387, val_acc:0.287]
Epoch [7/120    avg_loss:1.330, val_acc:0.416]
Epoch [8/120    avg_loss:1.226, val_acc:0.451]
Epoch [9/120    avg_loss:1.108, val_acc:0.469]
Epoch [10/120    avg_loss:1.013, val_acc:0.519]
Epoch [11/120    avg_loss:0.934, val_acc:0.539]
Epoch [12/120    avg_loss:0.815, val_acc:0.579]
Epoch [13/120    avg_loss:0.707, val_acc:0.698]
Epoch [14/120    avg_loss:0.636, val_acc:0.667]
Epoch [15/120    avg_loss:0.570, val_acc:0.732]
Epoch [16/120    avg_loss:0.496, val_acc:0.761]
Epoch [17/120    avg_loss:0.488, val_acc:0.735]
Epoch [18/120    avg_loss:0.463, val_acc:0.767]
Epoch [19/120    avg_loss:0.376, val_acc:0.791]
Epoch [20/120    avg_loss:0.317, val_acc:0.823]
Epoch [21/120    avg_loss:0.302, val_acc:0.872]
Epoch [22/120    avg_loss:0.273, val_acc:0.877]
Epoch [23/120    avg_loss:0.270, val_acc:0.879]
Epoch [24/120    avg_loss:0.249, val_acc:0.936]
Epoch [25/120    avg_loss:1.642, val_acc:0.581]
Epoch [26/120    avg_loss:1.485, val_acc:0.586]
Epoch [27/120    avg_loss:1.389, val_acc:0.606]
Epoch [28/120    avg_loss:1.279, val_acc:0.632]
Epoch [29/120    avg_loss:1.227, val_acc:0.646]
Epoch [30/120    avg_loss:1.198, val_acc:0.638]
Epoch [31/120    avg_loss:1.128, val_acc:0.670]
Epoch [32/120    avg_loss:1.092, val_acc:0.674]
Epoch [33/120    avg_loss:1.042, val_acc:0.647]
Epoch [34/120    avg_loss:1.013, val_acc:0.677]
Epoch [35/120    avg_loss:0.967, val_acc:0.703]
Epoch [36/120    avg_loss:0.958, val_acc:0.724]
Epoch [37/120    avg_loss:0.924, val_acc:0.738]
Epoch [38/120    avg_loss:0.897, val_acc:0.741]
Epoch [39/120    avg_loss:0.873, val_acc:0.741]
Epoch [40/120    avg_loss:0.863, val_acc:0.746]
Epoch [41/120    avg_loss:0.881, val_acc:0.744]
Epoch [42/120    avg_loss:0.870, val_acc:0.747]
Epoch [43/120    avg_loss:0.868, val_acc:0.745]
Epoch [44/120    avg_loss:0.867, val_acc:0.742]
Epoch [45/120    avg_loss:0.852, val_acc:0.742]
Epoch [46/120    avg_loss:0.845, val_acc:0.745]
Epoch [47/120    avg_loss:0.865, val_acc:0.745]
Epoch [48/120    avg_loss:0.851, val_acc:0.748]
Epoch [49/120    avg_loss:0.848, val_acc:0.747]
Epoch [50/120    avg_loss:0.846, val_acc:0.750]
Epoch [51/120    avg_loss:0.856, val_acc:0.751]
Epoch [52/120    avg_loss:0.830, val_acc:0.752]
Epoch [53/120    avg_loss:0.828, val_acc:0.753]
Epoch [54/120    avg_loss:0.843, val_acc:0.751]
Epoch [55/120    avg_loss:0.844, val_acc:0.753]
Epoch [56/120    avg_loss:0.845, val_acc:0.751]
Epoch [57/120    avg_loss:0.825, val_acc:0.750]
Epoch [58/120    avg_loss:0.841, val_acc:0.749]
Epoch [59/120    avg_loss:0.817, val_acc:0.750]
Epoch [60/120    avg_loss:0.821, val_acc:0.749]
Epoch [61/120    avg_loss:0.831, val_acc:0.751]
Epoch [62/120    avg_loss:0.823, val_acc:0.751]
Epoch [63/120    avg_loss:0.842, val_acc:0.752]
Epoch [64/120    avg_loss:0.830, val_acc:0.752]
Epoch [65/120    avg_loss:0.841, val_acc:0.752]
Epoch [66/120    avg_loss:0.826, val_acc:0.752]
Epoch [67/120    avg_loss:0.824, val_acc:0.752]
Epoch [68/120    avg_loss:0.840, val_acc:0.752]
Epoch [69/120    avg_loss:0.834, val_acc:0.752]
Epoch [70/120    avg_loss:0.811, val_acc:0.752]
Epoch [71/120    avg_loss:0.836, val_acc:0.752]
Epoch [72/120    avg_loss:0.838, val_acc:0.752]
Epoch [73/120    avg_loss:0.833, val_acc:0.752]
Epoch [74/120    avg_loss:0.826, val_acc:0.753]
Epoch [75/120    avg_loss:0.846, val_acc:0.753]
Epoch [76/120    avg_loss:0.824, val_acc:0.753]
Epoch [77/120    avg_loss:0.810, val_acc:0.753]
Epoch [78/120    avg_loss:0.830, val_acc:0.753]
Epoch [79/120    avg_loss:0.824, val_acc:0.753]
Epoch [80/120    avg_loss:0.824, val_acc:0.753]
Epoch [81/120    avg_loss:0.837, val_acc:0.753]
Epoch [82/120    avg_loss:0.835, val_acc:0.753]
Epoch [83/120    avg_loss:0.832, val_acc:0.753]
Epoch [84/120    avg_loss:0.832, val_acc:0.753]
Epoch [85/120    avg_loss:0.831, val_acc:0.753]
Epoch [86/120    avg_loss:0.855, val_acc:0.753]
Epoch [87/120    avg_loss:0.836, val_acc:0.753]
Epoch [88/120    avg_loss:0.835, val_acc:0.753]
Epoch [89/120    avg_loss:0.843, val_acc:0.753]
Epoch [90/120    avg_loss:0.834, val_acc:0.753]
Epoch [91/120    avg_loss:0.822, val_acc:0.753]
Epoch [92/120    avg_loss:0.830, val_acc:0.753]
Epoch [93/120    avg_loss:0.828, val_acc:0.753]
Epoch [94/120    avg_loss:0.826, val_acc:0.753]
Epoch [95/120    avg_loss:0.825, val_acc:0.753]
Epoch [96/120    avg_loss:0.831, val_acc:0.753]
Epoch [97/120    avg_loss:0.839, val_acc:0.753]
Epoch [98/120    avg_loss:0.832, val_acc:0.753]
Epoch [99/120    avg_loss:0.829, val_acc:0.753]
Epoch [100/120    avg_loss:0.861, val_acc:0.753]
Epoch [101/120    avg_loss:0.828, val_acc:0.753]
Epoch [102/120    avg_loss:0.843, val_acc:0.753]
Epoch [103/120    avg_loss:0.825, val_acc:0.753]
Epoch [104/120    avg_loss:0.828, val_acc:0.753]
Epoch [105/120    avg_loss:0.844, val_acc:0.753]
Epoch [106/120    avg_loss:0.841, val_acc:0.753]
Epoch [107/120    avg_loss:0.827, val_acc:0.753]
Epoch [108/120    avg_loss:0.840, val_acc:0.753]
Epoch [109/120    avg_loss:0.838, val_acc:0.753]
Epoch [110/120    avg_loss:0.822, val_acc:0.753]
Epoch [111/120    avg_loss:0.832, val_acc:0.753]
Epoch [112/120    avg_loss:0.817, val_acc:0.753]
Epoch [113/120    avg_loss:0.824, val_acc:0.753]
Epoch [114/120    avg_loss:0.839, val_acc:0.753]
Epoch [115/120    avg_loss:0.836, val_acc:0.753]
Epoch [116/120    avg_loss:0.824, val_acc:0.753]
Epoch [117/120    avg_loss:0.838, val_acc:0.753]
Epoch [118/120    avg_loss:0.842, val_acc:0.753]
Epoch [119/120    avg_loss:0.831, val_acc:0.753]
Epoch [120/120    avg_loss:0.826, val_acc:0.753]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4006     0    38  1114    93    18   207   418   538]
 [    0     0 16250     0     5     0  1834     0     1     0]
 [    0    51     0  1472   204     0     0     0   182   127]
 [    0    60   334    40  1654     0   490     0   385     9]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0  1774    44   126     0  2756     0   178     0]
 [    0    62     0     0    40     0     0  1181     1     6]
 [    0   195     0   123   119     0    41     0  3092     1]
 [    0     5     0     8    28   150     0     0     1   727]]

Accuracy:
78.18909213602295

F1 scores:
[       nan 0.74109703 0.8916813  0.78277054 0.52826573 0.9148265
 0.55026455 0.88200149 0.78988377 0.62483885]

Kappa:
0.7117522335240729
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f365f273c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.176, val_acc:0.085]
Epoch [2/120    avg_loss:1.951, val_acc:0.124]
Epoch [3/120    avg_loss:1.774, val_acc:0.136]
Epoch [4/120    avg_loss:1.652, val_acc:0.172]
Epoch [5/120    avg_loss:1.518, val_acc:0.252]
Epoch [6/120    avg_loss:1.430, val_acc:0.350]
Epoch [7/120    avg_loss:1.337, val_acc:0.367]
Epoch [8/120    avg_loss:1.312, val_acc:0.351]
Epoch [9/120    avg_loss:1.223, val_acc:0.415]
Epoch [10/120    avg_loss:1.118, val_acc:0.439]
Epoch [11/120    avg_loss:1.061, val_acc:0.434]
Epoch [12/120    avg_loss:0.990, val_acc:0.463]
Epoch [13/120    avg_loss:0.912, val_acc:0.472]
Epoch [14/120    avg_loss:0.840, val_acc:0.499]
Epoch [15/120    avg_loss:0.723, val_acc:0.519]
Epoch [16/120    avg_loss:0.682, val_acc:0.534]
Epoch [17/120    avg_loss:0.660, val_acc:0.576]
Epoch [18/120    avg_loss:0.585, val_acc:0.669]
Epoch [19/120    avg_loss:0.527, val_acc:0.712]
Epoch [20/120    avg_loss:0.509, val_acc:0.673]
Epoch [21/120    avg_loss:0.467, val_acc:0.728]
Epoch [22/120    avg_loss:0.428, val_acc:0.753]
Epoch [23/120    avg_loss:0.393, val_acc:0.812]
Epoch [24/120    avg_loss:0.341, val_acc:0.802]
Epoch [25/120    avg_loss:0.328, val_acc:0.829]
Epoch [26/120    avg_loss:0.303, val_acc:0.826]
Epoch [27/120    avg_loss:0.293, val_acc:0.855]
Epoch [28/120    avg_loss:0.246, val_acc:0.928]
Epoch [29/120    avg_loss:0.244, val_acc:0.885]
Epoch [30/120    avg_loss:0.279, val_acc:0.890]
Epoch [31/120    avg_loss:0.220, val_acc:0.951]
Epoch [32/120    avg_loss:0.176, val_acc:0.933]
Epoch [33/120    avg_loss:0.176, val_acc:0.941]
Epoch [34/120    avg_loss:0.143, val_acc:0.959]
Epoch [35/120    avg_loss:0.135, val_acc:0.967]
Epoch [36/120    avg_loss:0.117, val_acc:0.976]
Epoch [37/120    avg_loss:0.130, val_acc:0.959]
Epoch [38/120    avg_loss:0.139, val_acc:0.971]
Epoch [39/120    avg_loss:0.110, val_acc:0.967]
Epoch [40/120    avg_loss:0.115, val_acc:0.966]
Epoch [41/120    avg_loss:0.110, val_acc:0.970]
Epoch [42/120    avg_loss:0.085, val_acc:0.976]
Epoch [43/120    avg_loss:0.092, val_acc:0.973]
Epoch [44/120    avg_loss:0.102, val_acc:0.961]
Epoch [45/120    avg_loss:0.086, val_acc:0.980]
Epoch [46/120    avg_loss:0.078, val_acc:0.974]
Epoch [47/120    avg_loss:0.069, val_acc:0.970]
Epoch [48/120    avg_loss:0.061, val_acc:0.959]
Epoch [49/120    avg_loss:0.066, val_acc:0.946]
Epoch [50/120    avg_loss:0.051, val_acc:0.979]
Epoch [51/120    avg_loss:0.044, val_acc:0.983]
Epoch [52/120    avg_loss:0.038, val_acc:0.988]
Epoch [53/120    avg_loss:0.044, val_acc:0.981]
Epoch [54/120    avg_loss:0.036, val_acc:0.985]
Epoch [55/120    avg_loss:0.032, val_acc:0.983]
Epoch [56/120    avg_loss:0.026, val_acc:0.989]
Epoch [57/120    avg_loss:0.023, val_acc:0.984]
Epoch [58/120    avg_loss:0.039, val_acc:0.970]
Epoch [59/120    avg_loss:0.047, val_acc:0.985]
Epoch [60/120    avg_loss:0.029, val_acc:0.986]
Epoch [61/120    avg_loss:0.025, val_acc:0.987]
Epoch [62/120    avg_loss:0.023, val_acc:0.986]
Epoch [63/120    avg_loss:0.025, val_acc:0.989]
Epoch [64/120    avg_loss:0.034, val_acc:0.981]
Epoch [65/120    avg_loss:0.022, val_acc:0.988]
Epoch [66/120    avg_loss:0.023, val_acc:0.985]
Epoch [67/120    avg_loss:0.014, val_acc:0.990]
Epoch [68/120    avg_loss:0.019, val_acc:0.990]
Epoch [69/120    avg_loss:0.017, val_acc:0.987]
Epoch [70/120    avg_loss:0.014, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.019, val_acc:0.986]
Epoch [73/120    avg_loss:0.044, val_acc:0.988]
Epoch [74/120    avg_loss:0.036, val_acc:0.988]
Epoch [75/120    avg_loss:0.022, val_acc:0.986]
Epoch [76/120    avg_loss:0.019, val_acc:0.989]
Epoch [77/120    avg_loss:0.019, val_acc:0.989]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.023, val_acc:0.986]
Epoch [80/120    avg_loss:0.023, val_acc:0.987]
Epoch [81/120    avg_loss:0.017, val_acc:0.991]
Epoch [82/120    avg_loss:0.013, val_acc:0.990]
Epoch [83/120    avg_loss:0.009, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.989]
Epoch [86/120    avg_loss:0.035, val_acc:0.980]
Epoch [87/120    avg_loss:0.032, val_acc:0.986]
Epoch [88/120    avg_loss:0.022, val_acc:0.991]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.014, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.992]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.013, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.992]
Epoch [96/120    avg_loss:0.012, val_acc:0.987]
Epoch [97/120    avg_loss:0.018, val_acc:0.992]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.972]
Epoch [101/120    avg_loss:0.015, val_acc:0.988]
Epoch [102/120    avg_loss:0.018, val_acc:0.986]
Epoch [103/120    avg_loss:0.206, val_acc:0.799]
Epoch [104/120    avg_loss:0.394, val_acc:0.861]
Epoch [105/120    avg_loss:0.206, val_acc:0.928]
Epoch [106/120    avg_loss:0.130, val_acc:0.965]
Epoch [107/120    avg_loss:0.136, val_acc:0.957]
Epoch [108/120    avg_loss:0.097, val_acc:0.967]
Epoch [109/120    avg_loss:0.052, val_acc:0.969]
Epoch [110/120    avg_loss:0.068, val_acc:0.934]
Epoch [111/120    avg_loss:0.055, val_acc:0.980]
Epoch [112/120    avg_loss:0.039, val_acc:0.984]
Epoch [113/120    avg_loss:0.035, val_acc:0.984]
Epoch [114/120    avg_loss:0.026, val_acc:0.985]
Epoch [115/120    avg_loss:0.027, val_acc:0.983]
Epoch [116/120    avg_loss:0.026, val_acc:0.984]
Epoch [117/120    avg_loss:0.024, val_acc:0.986]
Epoch [118/120    avg_loss:0.024, val_acc:0.982]
Epoch [119/120    avg_loss:0.026, val_acc:0.983]
Epoch [120/120    avg_loss:0.024, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6279     0     0     4     0     0     4   138     7]
 [    0    10 17872     0   108     0    97     0     3     0]
 [    0     6     0  1931     4     0     0     0    87     8]
 [    0    44    18     0  2869     0     8     0    33     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     5     1     0  4846     0    19     0]
 [    0     1     0     0     0     0     0  1287     0     2]
 [    0    81     0    14    25     0     0     0  3451     0]
 [    0     0     0     0    14    41     0     0     0   864]]

Accuracy:
98.09847444147206

F1 scores:
[       nan 0.97704816 0.99324756 0.96889112 0.95681174 0.98453414
 0.98606165 0.99728787 0.94522049 0.96      ]

Kappa:
0.974854581700246
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd4a7c8b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.223, val_acc:0.323]
Epoch [2/120    avg_loss:2.069, val_acc:0.350]
Epoch [3/120    avg_loss:1.941, val_acc:0.196]
Epoch [4/120    avg_loss:1.820, val_acc:0.149]
Epoch [5/120    avg_loss:1.695, val_acc:0.191]
Epoch [6/120    avg_loss:1.554, val_acc:0.336]
Epoch [7/120    avg_loss:1.461, val_acc:0.289]
Epoch [8/120    avg_loss:1.339, val_acc:0.381]
Epoch [9/120    avg_loss:1.242, val_acc:0.424]
Epoch [10/120    avg_loss:1.094, val_acc:0.453]
Epoch [11/120    avg_loss:0.987, val_acc:0.445]
Epoch [12/120    avg_loss:0.888, val_acc:0.526]
Epoch [13/120    avg_loss:0.769, val_acc:0.572]
Epoch [14/120    avg_loss:0.677, val_acc:0.635]
Epoch [15/120    avg_loss:0.635, val_acc:0.656]
Epoch [16/120    avg_loss:0.552, val_acc:0.708]
Epoch [17/120    avg_loss:0.513, val_acc:0.753]
Epoch [18/120    avg_loss:0.447, val_acc:0.792]
Epoch [19/120    avg_loss:0.383, val_acc:0.811]
Epoch [20/120    avg_loss:0.365, val_acc:0.883]
Epoch [21/120    avg_loss:0.319, val_acc:0.888]
Epoch [22/120    avg_loss:0.281, val_acc:0.891]
Epoch [23/120    avg_loss:0.251, val_acc:0.924]
Epoch [24/120    avg_loss:0.229, val_acc:0.901]
Epoch [25/120    avg_loss:0.235, val_acc:0.947]
Epoch [26/120    avg_loss:0.192, val_acc:0.931]
Epoch [27/120    avg_loss:0.159, val_acc:0.938]
Epoch [28/120    avg_loss:0.154, val_acc:0.951]
Epoch [29/120    avg_loss:0.136, val_acc:0.933]
Epoch [30/120    avg_loss:0.111, val_acc:0.944]
Epoch [31/120    avg_loss:0.131, val_acc:0.956]
Epoch [32/120    avg_loss:0.116, val_acc:0.552]
Epoch [33/120    avg_loss:0.344, val_acc:0.841]
Epoch [34/120    avg_loss:0.187, val_acc:0.944]
Epoch [35/120    avg_loss:0.225, val_acc:0.938]
Epoch [36/120    avg_loss:0.109, val_acc:0.957]
Epoch [37/120    avg_loss:0.134, val_acc:0.898]
Epoch [38/120    avg_loss:0.230, val_acc:0.938]
Epoch [39/120    avg_loss:0.128, val_acc:0.928]
Epoch [40/120    avg_loss:0.108, val_acc:0.951]
Epoch [41/120    avg_loss:0.095, val_acc:0.950]
Epoch [42/120    avg_loss:0.097, val_acc:0.957]
Epoch [43/120    avg_loss:0.088, val_acc:0.949]
Epoch [44/120    avg_loss:0.071, val_acc:0.964]
Epoch [45/120    avg_loss:0.055, val_acc:0.970]
Epoch [46/120    avg_loss:0.067, val_acc:0.964]
Epoch [47/120    avg_loss:0.064, val_acc:0.964]
Epoch [48/120    avg_loss:0.044, val_acc:0.945]
Epoch [49/120    avg_loss:0.063, val_acc:0.967]
Epoch [50/120    avg_loss:0.065, val_acc:0.962]
Epoch [51/120    avg_loss:0.059, val_acc:0.972]
Epoch [52/120    avg_loss:0.048, val_acc:0.970]
Epoch [53/120    avg_loss:0.045, val_acc:0.974]
Epoch [54/120    avg_loss:0.046, val_acc:0.977]
Epoch [55/120    avg_loss:0.054, val_acc:0.965]
Epoch [56/120    avg_loss:0.035, val_acc:0.977]
Epoch [57/120    avg_loss:0.045, val_acc:0.965]
Epoch [58/120    avg_loss:0.040, val_acc:0.978]
Epoch [59/120    avg_loss:0.025, val_acc:0.975]
Epoch [60/120    avg_loss:0.025, val_acc:0.978]
Epoch [61/120    avg_loss:0.042, val_acc:0.972]
Epoch [62/120    avg_loss:0.025, val_acc:0.977]
Epoch [63/120    avg_loss:0.027, val_acc:0.975]
Epoch [64/120    avg_loss:0.032, val_acc:0.963]
Epoch [65/120    avg_loss:0.034, val_acc:0.977]
Epoch [66/120    avg_loss:0.031, val_acc:0.977]
Epoch [67/120    avg_loss:0.029, val_acc:0.981]
Epoch [68/120    avg_loss:0.022, val_acc:0.976]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.020, val_acc:0.977]
Epoch [71/120    avg_loss:0.021, val_acc:0.978]
Epoch [72/120    avg_loss:0.020, val_acc:0.979]
Epoch [73/120    avg_loss:0.018, val_acc:0.977]
Epoch [74/120    avg_loss:0.013, val_acc:0.979]
Epoch [75/120    avg_loss:0.018, val_acc:0.983]
Epoch [76/120    avg_loss:0.014, val_acc:0.984]
Epoch [77/120    avg_loss:0.016, val_acc:0.983]
Epoch [78/120    avg_loss:0.014, val_acc:0.983]
Epoch [79/120    avg_loss:0.016, val_acc:0.982]
Epoch [80/120    avg_loss:0.021, val_acc:0.977]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.016, val_acc:0.977]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.017, val_acc:0.977]
Epoch [89/120    avg_loss:0.029, val_acc:0.970]
Epoch [90/120    avg_loss:0.015, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.018, val_acc:0.971]
Epoch [96/120    avg_loss:0.026, val_acc:0.978]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.012, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.984]
Epoch [105/120    avg_loss:0.028, val_acc:0.966]
Epoch [106/120    avg_loss:0.022, val_acc:0.978]
Epoch [107/120    avg_loss:0.029, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.012, val_acc:0.976]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.973]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6429     0     0     0     0     0     3     0     0]
 [    0     8 18039     0    26     0    17     0     0     0]
 [    0     9     0  2001     0     0     0     0    26     0]
 [    0    21    13    15  2906     0    11     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2     1    13     0     0  4861     0     1     0]
 [    0     2     0     0     0     0     6  1282     0     0]
 [    0    10     0    37    51     0     0     0  3473     0]
 [    0     0     0     0     2    32     0     0     0   885]]

Accuracy:
99.24806593883307

F1 scores:
[       nan 0.99574073 0.99820159 0.97562165 0.97565889 0.98788796
 0.99478154 0.99572816 0.98148933 0.98115299]

Kappa:
0.9900394825949314
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa9e082ab38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.233, val_acc:0.051]
Epoch [2/120    avg_loss:2.074, val_acc:0.080]
Epoch [3/120    avg_loss:1.968, val_acc:0.097]
Epoch [4/120    avg_loss:1.857, val_acc:0.127]
Epoch [5/120    avg_loss:1.768, val_acc:0.137]
Epoch [6/120    avg_loss:1.619, val_acc:0.163]
Epoch [7/120    avg_loss:1.506, val_acc:0.218]
Epoch [8/120    avg_loss:1.407, val_acc:0.324]
Epoch [9/120    avg_loss:1.317, val_acc:0.385]
Epoch [10/120    avg_loss:1.217, val_acc:0.412]
Epoch [11/120    avg_loss:1.137, val_acc:0.439]
Epoch [12/120    avg_loss:1.020, val_acc:0.444]
Epoch [13/120    avg_loss:0.931, val_acc:0.478]
Epoch [14/120    avg_loss:0.831, val_acc:0.582]
Epoch [15/120    avg_loss:0.739, val_acc:0.612]
Epoch [16/120    avg_loss:0.690, val_acc:0.649]
Epoch [17/120    avg_loss:0.629, val_acc:0.731]
Epoch [18/120    avg_loss:0.561, val_acc:0.730]
Epoch [19/120    avg_loss:0.557, val_acc:0.750]
Epoch [20/120    avg_loss:0.463, val_acc:0.783]
Epoch [21/120    avg_loss:0.420, val_acc:0.807]
Epoch [22/120    avg_loss:0.399, val_acc:0.842]
Epoch [23/120    avg_loss:0.360, val_acc:0.815]
Epoch [24/120    avg_loss:0.396, val_acc:0.821]
Epoch [25/120    avg_loss:0.345, val_acc:0.847]
Epoch [26/120    avg_loss:0.309, val_acc:0.857]
Epoch [27/120    avg_loss:0.289, val_acc:0.856]
Epoch [28/120    avg_loss:0.382, val_acc:0.867]
Epoch [29/120    avg_loss:0.250, val_acc:0.891]
Epoch [30/120    avg_loss:0.230, val_acc:0.917]
Epoch [31/120    avg_loss:0.216, val_acc:0.923]
Epoch [32/120    avg_loss:0.288, val_acc:0.908]
Epoch [33/120    avg_loss:0.215, val_acc:0.891]
Epoch [34/120    avg_loss:0.180, val_acc:0.911]
Epoch [35/120    avg_loss:0.211, val_acc:0.933]
Epoch [36/120    avg_loss:0.144, val_acc:0.951]
Epoch [37/120    avg_loss:0.132, val_acc:0.956]
Epoch [38/120    avg_loss:0.127, val_acc:0.943]
Epoch [39/120    avg_loss:0.131, val_acc:0.952]
Epoch [40/120    avg_loss:0.134, val_acc:0.954]
Epoch [41/120    avg_loss:0.118, val_acc:0.951]
Epoch [42/120    avg_loss:0.100, val_acc:0.963]
Epoch [43/120    avg_loss:0.086, val_acc:0.969]
Epoch [44/120    avg_loss:0.105, val_acc:0.965]
Epoch [45/120    avg_loss:0.103, val_acc:0.970]
Epoch [46/120    avg_loss:0.121, val_acc:0.941]
Epoch [47/120    avg_loss:0.102, val_acc:0.972]
Epoch [48/120    avg_loss:0.081, val_acc:0.968]
Epoch [49/120    avg_loss:0.066, val_acc:0.962]
Epoch [50/120    avg_loss:0.051, val_acc:0.977]
Epoch [51/120    avg_loss:0.081, val_acc:0.973]
Epoch [52/120    avg_loss:0.060, val_acc:0.971]
Epoch [53/120    avg_loss:0.050, val_acc:0.974]
Epoch [54/120    avg_loss:0.069, val_acc:0.970]
Epoch [55/120    avg_loss:0.050, val_acc:0.980]
Epoch [56/120    avg_loss:0.047, val_acc:0.982]
Epoch [57/120    avg_loss:0.045, val_acc:0.981]
Epoch [58/120    avg_loss:0.047, val_acc:0.982]
Epoch [59/120    avg_loss:0.067, val_acc:0.978]
Epoch [60/120    avg_loss:0.052, val_acc:0.980]
Epoch [61/120    avg_loss:0.064, val_acc:0.979]
Epoch [62/120    avg_loss:0.053, val_acc:0.983]
Epoch [63/120    avg_loss:0.043, val_acc:0.983]
Epoch [64/120    avg_loss:0.035, val_acc:0.981]
Epoch [65/120    avg_loss:0.047, val_acc:0.982]
Epoch [66/120    avg_loss:0.036, val_acc:0.984]
Epoch [67/120    avg_loss:0.036, val_acc:0.976]
Epoch [68/120    avg_loss:0.035, val_acc:0.983]
Epoch [69/120    avg_loss:0.035, val_acc:0.975]
Epoch [70/120    avg_loss:0.035, val_acc:0.982]
Epoch [71/120    avg_loss:0.027, val_acc:0.980]
Epoch [72/120    avg_loss:0.040, val_acc:0.983]
Epoch [73/120    avg_loss:0.048, val_acc:0.966]
Epoch [74/120    avg_loss:0.026, val_acc:0.988]
Epoch [75/120    avg_loss:0.018, val_acc:0.988]
Epoch [76/120    avg_loss:0.036, val_acc:0.983]
Epoch [77/120    avg_loss:0.030, val_acc:0.987]
Epoch [78/120    avg_loss:0.026, val_acc:0.984]
Epoch [79/120    avg_loss:0.030, val_acc:0.980]
Epoch [80/120    avg_loss:0.024, val_acc:0.987]
Epoch [81/120    avg_loss:0.024, val_acc:0.982]
Epoch [82/120    avg_loss:0.046, val_acc:0.980]
Epoch [83/120    avg_loss:0.034, val_acc:0.987]
Epoch [84/120    avg_loss:0.019, val_acc:0.986]
Epoch [85/120    avg_loss:0.017, val_acc:0.985]
Epoch [86/120    avg_loss:0.017, val_acc:0.984]
Epoch [87/120    avg_loss:0.015, val_acc:0.989]
Epoch [88/120    avg_loss:0.021, val_acc:0.987]
Epoch [89/120    avg_loss:0.016, val_acc:0.989]
Epoch [90/120    avg_loss:0.016, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.987]
Epoch [92/120    avg_loss:0.015, val_acc:0.990]
Epoch [93/120    avg_loss:0.022, val_acc:0.986]
Epoch [94/120    avg_loss:0.021, val_acc:0.989]
Epoch [95/120    avg_loss:0.015, val_acc:0.983]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.056, val_acc:0.984]
Epoch [98/120    avg_loss:0.020, val_acc:0.982]
Epoch [99/120    avg_loss:0.014, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.987]
Epoch [103/120    avg_loss:0.013, val_acc:0.989]
Epoch [104/120    avg_loss:0.013, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.008, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.008, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.991]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     0     0     0     0    18     0]
 [    0     2 18021     0    58     0     8     0     0     1]
 [    0     3     0  2016     2     0     0     0    14     1]
 [    0    43    19     1  2874     0     8     0    25     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5    16     0     0  4855     0     0     2]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    18     0    22    57     0     0     0  3474     0]
 [    0     0     0     0    14    44     0     0     0   861]]

Accuracy:
99.07695273901622

F1 scores:
[       nan 0.99349442 0.99742632 0.9855781  0.96168646 0.98342125
 0.99599959 1.         0.97831597 0.96416573]

Kappa:
0.9877741088591313
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff73e9bb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.207, val_acc:0.157]
Epoch [2/120    avg_loss:2.030, val_acc:0.156]
Epoch [3/120    avg_loss:1.901, val_acc:0.200]
Epoch [4/120    avg_loss:1.776, val_acc:0.229]
Epoch [5/120    avg_loss:1.645, val_acc:0.248]
Epoch [6/120    avg_loss:1.499, val_acc:0.305]
Epoch [7/120    avg_loss:1.384, val_acc:0.358]
Epoch [8/120    avg_loss:1.256, val_acc:0.388]
Epoch [9/120    avg_loss:1.140, val_acc:0.450]
Epoch [10/120    avg_loss:1.042, val_acc:0.470]
Epoch [11/120    avg_loss:0.925, val_acc:0.457]
Epoch [12/120    avg_loss:0.833, val_acc:0.515]
Epoch [13/120    avg_loss:0.725, val_acc:0.533]
Epoch [14/120    avg_loss:0.693, val_acc:0.589]
Epoch [15/120    avg_loss:0.608, val_acc:0.642]
Epoch [16/120    avg_loss:0.511, val_acc:0.672]
Epoch [17/120    avg_loss:0.471, val_acc:0.672]
Epoch [18/120    avg_loss:0.431, val_acc:0.756]
Epoch [19/120    avg_loss:0.366, val_acc:0.766]
Epoch [20/120    avg_loss:0.360, val_acc:0.812]
Epoch [21/120    avg_loss:0.349, val_acc:0.840]
Epoch [22/120    avg_loss:0.480, val_acc:0.809]
Epoch [23/120    avg_loss:0.341, val_acc:0.825]
Epoch [24/120    avg_loss:0.258, val_acc:0.856]
Epoch [25/120    avg_loss:0.238, val_acc:0.877]
Epoch [26/120    avg_loss:0.221, val_acc:0.908]
Epoch [27/120    avg_loss:0.209, val_acc:0.915]
Epoch [28/120    avg_loss:0.170, val_acc:0.918]
Epoch [29/120    avg_loss:0.175, val_acc:0.925]
Epoch [30/120    avg_loss:0.195, val_acc:0.935]
Epoch [31/120    avg_loss:0.165, val_acc:0.938]
Epoch [32/120    avg_loss:0.155, val_acc:0.942]
Epoch [33/120    avg_loss:0.125, val_acc:0.944]
Epoch [34/120    avg_loss:0.125, val_acc:0.968]
Epoch [35/120    avg_loss:0.117, val_acc:0.969]
Epoch [36/120    avg_loss:0.097, val_acc:0.971]
Epoch [37/120    avg_loss:0.074, val_acc:0.973]
Epoch [38/120    avg_loss:0.109, val_acc:0.928]
Epoch [39/120    avg_loss:0.111, val_acc:0.980]
Epoch [40/120    avg_loss:0.088, val_acc:0.979]
Epoch [41/120    avg_loss:0.070, val_acc:0.976]
Epoch [42/120    avg_loss:0.064, val_acc:0.981]
Epoch [43/120    avg_loss:0.066, val_acc:0.977]
Epoch [44/120    avg_loss:0.075, val_acc:0.973]
Epoch [45/120    avg_loss:0.057, val_acc:0.975]
Epoch [46/120    avg_loss:0.055, val_acc:0.986]
Epoch [47/120    avg_loss:0.055, val_acc:0.977]
Epoch [48/120    avg_loss:0.074, val_acc:0.965]
Epoch [49/120    avg_loss:0.066, val_acc:0.973]
Epoch [50/120    avg_loss:0.057, val_acc:0.941]
Epoch [51/120    avg_loss:0.075, val_acc:0.957]
Epoch [52/120    avg_loss:0.056, val_acc:0.983]
Epoch [53/120    avg_loss:0.052, val_acc:0.981]
Epoch [54/120    avg_loss:0.040, val_acc:0.973]
Epoch [55/120    avg_loss:0.034, val_acc:0.979]
Epoch [56/120    avg_loss:0.069, val_acc:0.982]
Epoch [57/120    avg_loss:0.042, val_acc:0.990]
Epoch [58/120    avg_loss:0.032, val_acc:0.984]
Epoch [59/120    avg_loss:0.037, val_acc:0.981]
Epoch [60/120    avg_loss:0.033, val_acc:0.985]
Epoch [61/120    avg_loss:0.036, val_acc:0.992]
Epoch [62/120    avg_loss:0.036, val_acc:0.987]
Epoch [63/120    avg_loss:0.026, val_acc:0.990]
Epoch [64/120    avg_loss:0.030, val_acc:0.984]
Epoch [65/120    avg_loss:0.030, val_acc:0.951]
Epoch [66/120    avg_loss:0.042, val_acc:0.953]
Epoch [67/120    avg_loss:0.033, val_acc:0.989]
Epoch [68/120    avg_loss:0.024, val_acc:0.986]
Epoch [69/120    avg_loss:0.021, val_acc:0.988]
Epoch [70/120    avg_loss:0.022, val_acc:0.990]
Epoch [71/120    avg_loss:0.015, val_acc:0.990]
Epoch [72/120    avg_loss:0.015, val_acc:0.993]
Epoch [73/120    avg_loss:0.020, val_acc:0.990]
Epoch [74/120    avg_loss:0.023, val_acc:0.982]
Epoch [75/120    avg_loss:0.032, val_acc:0.995]
Epoch [76/120    avg_loss:0.017, val_acc:0.989]
Epoch [77/120    avg_loss:0.013, val_acc:0.976]
Epoch [78/120    avg_loss:0.019, val_acc:0.987]
Epoch [79/120    avg_loss:0.015, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.993]
Epoch [81/120    avg_loss:0.009, val_acc:0.992]
Epoch [82/120    avg_loss:0.014, val_acc:0.990]
Epoch [83/120    avg_loss:0.013, val_acc:0.986]
Epoch [84/120    avg_loss:0.048, val_acc:0.987]
Epoch [85/120    avg_loss:0.918, val_acc:0.470]
Epoch [86/120    avg_loss:1.343, val_acc:0.434]
Epoch [87/120    avg_loss:1.221, val_acc:0.402]
Epoch [88/120    avg_loss:1.185, val_acc:0.405]
Epoch [89/120    avg_loss:1.193, val_acc:0.383]
Epoch [90/120    avg_loss:1.133, val_acc:0.405]
Epoch [91/120    avg_loss:1.103, val_acc:0.421]
Epoch [92/120    avg_loss:1.109, val_acc:0.421]
Epoch [93/120    avg_loss:1.096, val_acc:0.418]
Epoch [94/120    avg_loss:1.090, val_acc:0.401]
Epoch [95/120    avg_loss:1.067, val_acc:0.451]
Epoch [96/120    avg_loss:1.111, val_acc:0.424]
Epoch [97/120    avg_loss:1.075, val_acc:0.432]
Epoch [98/120    avg_loss:1.064, val_acc:0.416]
Epoch [99/120    avg_loss:1.010, val_acc:0.400]
Epoch [100/120    avg_loss:1.017, val_acc:0.457]
Epoch [101/120    avg_loss:1.045, val_acc:0.438]
Epoch [102/120    avg_loss:1.039, val_acc:0.427]
Epoch [103/120    avg_loss:1.047, val_acc:0.434]
Epoch [104/120    avg_loss:1.047, val_acc:0.438]
Epoch [105/120    avg_loss:1.044, val_acc:0.433]
Epoch [106/120    avg_loss:1.005, val_acc:0.435]
Epoch [107/120    avg_loss:1.034, val_acc:0.433]
Epoch [108/120    avg_loss:1.042, val_acc:0.436]
Epoch [109/120    avg_loss:1.010, val_acc:0.435]
Epoch [110/120    avg_loss:1.015, val_acc:0.435]
Epoch [111/120    avg_loss:1.005, val_acc:0.436]
Epoch [112/120    avg_loss:1.024, val_acc:0.434]
Epoch [113/120    avg_loss:1.003, val_acc:0.436]
Epoch [114/120    avg_loss:1.047, val_acc:0.434]
Epoch [115/120    avg_loss:1.017, val_acc:0.435]
Epoch [116/120    avg_loss:1.042, val_acc:0.435]
Epoch [117/120    avg_loss:1.001, val_acc:0.433]
Epoch [118/120    avg_loss:1.041, val_acc:0.432]
Epoch [119/120    avg_loss:0.986, val_acc:0.434]
Epoch [120/120    avg_loss:1.017, val_acc:0.434]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0]
 [   0 2618  378    0  236    6 2320  328   65  481]
 [   0 3591 5853    0   79    0 8551    0   16    0]
 [   0    7    7 1633   11    0    1    0  163  214]
 [   0  420  302    0 1503    0  716    0   27    4]
 [   0    0    0    0    0 1305    0    0    0    0]
 [   0    3  830  230  443    0 3291    0   81    0]
 [   0  120    0   15    3    0    0 1084    0   68]
 [   0   64  280  151   69    0  710    0 2293    4]
 [   0   13    0    5   24  145    1    0   12  719]]

Accuracy:
48.921504832140364

F1 scores:
[       nan 0.39463371 0.45477855 0.802457   0.56292135 0.94530967
 0.32157514 0.80236862 0.73635196 0.59692819]

Kappa:
0.3882439554454834
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f193b759be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.254, val_acc:0.210]
Epoch [2/120    avg_loss:2.066, val_acc:0.220]
Epoch [3/120    avg_loss:1.894, val_acc:0.231]
Epoch [4/120    avg_loss:1.738, val_acc:0.249]
Epoch [5/120    avg_loss:1.626, val_acc:0.263]
Epoch [6/120    avg_loss:1.466, val_acc:0.289]
Epoch [7/120    avg_loss:1.376, val_acc:0.309]
Epoch [8/120    avg_loss:1.311, val_acc:0.312]
Epoch [9/120    avg_loss:1.224, val_acc:0.417]
Epoch [10/120    avg_loss:1.140, val_acc:0.511]
Epoch [11/120    avg_loss:1.051, val_acc:0.474]
Epoch [12/120    avg_loss:0.943, val_acc:0.483]
Epoch [13/120    avg_loss:0.800, val_acc:0.540]
Epoch [14/120    avg_loss:0.720, val_acc:0.660]
Epoch [15/120    avg_loss:0.623, val_acc:0.676]
Epoch [16/120    avg_loss:0.562, val_acc:0.694]
Epoch [17/120    avg_loss:0.529, val_acc:0.700]
Epoch [18/120    avg_loss:0.476, val_acc:0.731]
Epoch [19/120    avg_loss:0.433, val_acc:0.749]
Epoch [20/120    avg_loss:0.402, val_acc:0.787]
Epoch [21/120    avg_loss:0.367, val_acc:0.808]
Epoch [22/120    avg_loss:0.327, val_acc:0.806]
Epoch [23/120    avg_loss:0.576, val_acc:0.765]
Epoch [24/120    avg_loss:0.385, val_acc:0.788]
Epoch [25/120    avg_loss:0.315, val_acc:0.811]
Epoch [26/120    avg_loss:0.276, val_acc:0.836]
Epoch [27/120    avg_loss:0.241, val_acc:0.832]
Epoch [28/120    avg_loss:0.264, val_acc:0.832]
Epoch [29/120    avg_loss:0.255, val_acc:0.831]
Epoch [30/120    avg_loss:0.239, val_acc:0.841]
Epoch [31/120    avg_loss:0.210, val_acc:0.885]
Epoch [32/120    avg_loss:0.176, val_acc:0.898]
Epoch [33/120    avg_loss:0.169, val_acc:0.914]
Epoch [34/120    avg_loss:0.170, val_acc:0.921]
Epoch [35/120    avg_loss:0.145, val_acc:0.938]
Epoch [36/120    avg_loss:0.466, val_acc:0.794]
Epoch [37/120    avg_loss:0.387, val_acc:0.849]
Epoch [38/120    avg_loss:0.224, val_acc:0.919]
Epoch [39/120    avg_loss:0.208, val_acc:0.838]
Epoch [40/120    avg_loss:0.153, val_acc:0.956]
Epoch [41/120    avg_loss:0.129, val_acc:0.954]
Epoch [42/120    avg_loss:0.123, val_acc:0.955]
Epoch [43/120    avg_loss:0.100, val_acc:0.961]
Epoch [44/120    avg_loss:0.108, val_acc:0.949]
Epoch [45/120    avg_loss:0.096, val_acc:0.964]
Epoch [46/120    avg_loss:0.079, val_acc:0.971]
Epoch [47/120    avg_loss:0.079, val_acc:0.962]
Epoch [48/120    avg_loss:0.082, val_acc:0.967]
Epoch [49/120    avg_loss:0.094, val_acc:0.963]
Epoch [50/120    avg_loss:0.070, val_acc:0.968]
Epoch [51/120    avg_loss:0.063, val_acc:0.971]
Epoch [52/120    avg_loss:0.091, val_acc:0.957]
Epoch [53/120    avg_loss:0.071, val_acc:0.958]
Epoch [54/120    avg_loss:0.077, val_acc:0.956]
Epoch [55/120    avg_loss:0.076, val_acc:0.945]
Epoch [56/120    avg_loss:0.072, val_acc:0.961]
Epoch [57/120    avg_loss:0.060, val_acc:0.977]
Epoch [58/120    avg_loss:0.070, val_acc:0.972]
Epoch [59/120    avg_loss:0.058, val_acc:0.977]
Epoch [60/120    avg_loss:0.059, val_acc:0.977]
Epoch [61/120    avg_loss:0.047, val_acc:0.969]
Epoch [62/120    avg_loss:0.048, val_acc:0.982]
Epoch [63/120    avg_loss:0.039, val_acc:0.982]
Epoch [64/120    avg_loss:0.033, val_acc:0.984]
Epoch [65/120    avg_loss:0.036, val_acc:0.966]
Epoch [66/120    avg_loss:0.088, val_acc:0.976]
Epoch [67/120    avg_loss:0.043, val_acc:0.978]
Epoch [68/120    avg_loss:0.042, val_acc:0.981]
Epoch [69/120    avg_loss:0.029, val_acc:0.988]
Epoch [70/120    avg_loss:0.038, val_acc:0.984]
Epoch [71/120    avg_loss:0.057, val_acc:0.966]
Epoch [72/120    avg_loss:0.041, val_acc:0.983]
Epoch [73/120    avg_loss:0.030, val_acc:0.985]
Epoch [74/120    avg_loss:0.024, val_acc:0.989]
Epoch [75/120    avg_loss:0.033, val_acc:0.984]
Epoch [76/120    avg_loss:0.028, val_acc:0.987]
Epoch [77/120    avg_loss:0.030, val_acc:0.990]
Epoch [78/120    avg_loss:0.031, val_acc:0.984]
Epoch [79/120    avg_loss:0.041, val_acc:0.981]
Epoch [80/120    avg_loss:0.032, val_acc:0.983]
Epoch [81/120    avg_loss:0.025, val_acc:0.980]
Epoch [82/120    avg_loss:0.024, val_acc:0.986]
Epoch [83/120    avg_loss:0.030, val_acc:0.977]
Epoch [84/120    avg_loss:0.020, val_acc:0.990]
Epoch [85/120    avg_loss:0.022, val_acc:0.961]
Epoch [86/120    avg_loss:0.030, val_acc:0.985]
Epoch [87/120    avg_loss:0.025, val_acc:0.989]
Epoch [88/120    avg_loss:0.019, val_acc:0.987]
Epoch [89/120    avg_loss:0.014, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.989]
Epoch [91/120    avg_loss:0.013, val_acc:0.989]
Epoch [92/120    avg_loss:0.013, val_acc:0.990]
Epoch [93/120    avg_loss:0.012, val_acc:0.987]
Epoch [94/120    avg_loss:0.020, val_acc:0.984]
Epoch [95/120    avg_loss:0.028, val_acc:0.976]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.009, val_acc:0.993]
Epoch [101/120    avg_loss:0.013, val_acc:0.990]
Epoch [102/120    avg_loss:0.012, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.989]
Epoch [104/120    avg_loss:0.012, val_acc:0.990]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.017, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.992]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.024, val_acc:0.987]
Epoch [113/120    avg_loss:0.028, val_acc:0.971]
Epoch [114/120    avg_loss:0.028, val_acc:0.983]
Epoch [115/120    avg_loss:0.020, val_acc:0.982]
Epoch [116/120    avg_loss:0.019, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.990]
Epoch [119/120    avg_loss:0.010, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     0     0     0     1    38     0]
 [    0     2 18051     0    25     0    12     0     0     0]
 [    0    12     0  2008     0     0     0     0    13     3]
 [    0    35    20     0  2879     0    11     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     4     0     0  4854     0     1     2]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0    27     0     5    75     0     0     0  3464     0]
 [    0     0     0     0    14    90     0     0     0   815]]

Accuracy:
98.94440026028487

F1 scores:
[       nan 0.99108596 0.99789928 0.99087096 0.96529757 0.96666667
 0.99518196 0.99805976 0.97385437 0.93516925]

Kappa:
0.9860110837986207
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa67f778b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.203, val_acc:0.183]
Epoch [2/120    avg_loss:2.010, val_acc:0.128]
Epoch [3/120    avg_loss:1.876, val_acc:0.132]
Epoch [4/120    avg_loss:1.727, val_acc:0.155]
Epoch [5/120    avg_loss:1.604, val_acc:0.214]
Epoch [6/120    avg_loss:1.507, val_acc:0.358]
Epoch [7/120    avg_loss:1.404, val_acc:0.365]
Epoch [8/120    avg_loss:1.295, val_acc:0.449]
Epoch [9/120    avg_loss:1.167, val_acc:0.514]
Epoch [10/120    avg_loss:1.051, val_acc:0.564]
Epoch [11/120    avg_loss:0.927, val_acc:0.620]
Epoch [12/120    avg_loss:0.790, val_acc:0.622]
Epoch [13/120    avg_loss:0.708, val_acc:0.628]
Epoch [14/120    avg_loss:0.629, val_acc:0.689]
Epoch [15/120    avg_loss:0.548, val_acc:0.752]
Epoch [16/120    avg_loss:0.476, val_acc:0.780]
Epoch [17/120    avg_loss:0.432, val_acc:0.806]
Epoch [18/120    avg_loss:0.416, val_acc:0.851]
Epoch [19/120    avg_loss:0.341, val_acc:0.883]
Epoch [20/120    avg_loss:0.386, val_acc:0.884]
Epoch [21/120    avg_loss:0.276, val_acc:0.894]
Epoch [22/120    avg_loss:0.259, val_acc:0.928]
Epoch [23/120    avg_loss:0.218, val_acc:0.941]
Epoch [24/120    avg_loss:0.212, val_acc:0.915]
Epoch [25/120    avg_loss:0.196, val_acc:0.941]
Epoch [26/120    avg_loss:0.198, val_acc:0.939]
Epoch [27/120    avg_loss:0.173, val_acc:0.906]
Epoch [28/120    avg_loss:0.156, val_acc:0.955]
Epoch [29/120    avg_loss:0.174, val_acc:0.953]
Epoch [30/120    avg_loss:0.133, val_acc:0.951]
Epoch [31/120    avg_loss:0.128, val_acc:0.955]
Epoch [32/120    avg_loss:0.122, val_acc:0.962]
Epoch [33/120    avg_loss:0.095, val_acc:0.965]
Epoch [34/120    avg_loss:0.085, val_acc:0.971]
Epoch [35/120    avg_loss:0.070, val_acc:0.973]
Epoch [36/120    avg_loss:0.081, val_acc:0.969]
Epoch [37/120    avg_loss:0.079, val_acc:0.960]
Epoch [38/120    avg_loss:0.066, val_acc:0.974]
Epoch [39/120    avg_loss:0.075, val_acc:0.966]
Epoch [40/120    avg_loss:0.065, val_acc:0.957]
Epoch [41/120    avg_loss:0.066, val_acc:0.973]
Epoch [42/120    avg_loss:0.069, val_acc:0.966]
Epoch [43/120    avg_loss:0.068, val_acc:0.977]
Epoch [44/120    avg_loss:0.089, val_acc:0.966]
Epoch [45/120    avg_loss:0.118, val_acc:0.951]
Epoch [46/120    avg_loss:0.126, val_acc:0.961]
Epoch [47/120    avg_loss:0.083, val_acc:0.957]
Epoch [48/120    avg_loss:0.076, val_acc:0.974]
Epoch [49/120    avg_loss:0.062, val_acc:0.979]
Epoch [50/120    avg_loss:0.059, val_acc:0.975]
Epoch [51/120    avg_loss:0.049, val_acc:0.979]
Epoch [52/120    avg_loss:0.045, val_acc:0.978]
Epoch [53/120    avg_loss:0.048, val_acc:0.972]
Epoch [54/120    avg_loss:0.053, val_acc:0.977]
Epoch [55/120    avg_loss:0.038, val_acc:0.976]
Epoch [56/120    avg_loss:0.031, val_acc:0.971]
Epoch [57/120    avg_loss:0.035, val_acc:0.984]
Epoch [58/120    avg_loss:0.025, val_acc:0.986]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.053, val_acc:0.971]
Epoch [61/120    avg_loss:0.037, val_acc:0.978]
Epoch [62/120    avg_loss:0.031, val_acc:0.984]
Epoch [63/120    avg_loss:0.021, val_acc:0.981]
Epoch [64/120    avg_loss:0.021, val_acc:0.982]
Epoch [65/120    avg_loss:0.021, val_acc:0.985]
Epoch [66/120    avg_loss:0.028, val_acc:0.984]
Epoch [67/120    avg_loss:0.021, val_acc:0.990]
Epoch [68/120    avg_loss:0.019, val_acc:0.987]
Epoch [69/120    avg_loss:0.018, val_acc:0.983]
Epoch [70/120    avg_loss:0.016, val_acc:0.987]
Epoch [71/120    avg_loss:0.016, val_acc:0.984]
Epoch [72/120    avg_loss:0.026, val_acc:0.984]
Epoch [73/120    avg_loss:0.017, val_acc:0.986]
Epoch [74/120    avg_loss:0.016, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.989]
Epoch [78/120    avg_loss:0.015, val_acc:0.988]
Epoch [79/120    avg_loss:0.018, val_acc:0.986]
Epoch [80/120    avg_loss:0.018, val_acc:0.962]
Epoch [81/120    avg_loss:0.020, val_acc:0.987]
Epoch [82/120    avg_loss:0.013, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.008, val_acc:0.989]
Epoch [88/120    avg_loss:0.008, val_acc:0.989]
Epoch [89/120    avg_loss:0.008, val_acc:0.989]
Epoch [90/120    avg_loss:0.009, val_acc:0.989]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.008, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.009, val_acc:0.990]
Epoch [97/120    avg_loss:0.010, val_acc:0.990]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6414     0     0     0     0     0     0    17     1]
 [    0     2 18049     0    22     0    17     0     0     0]
 [    0    10     0  2009     0     0     0     0    12     5]
 [    0    48    21     0  2876     0     0     0    26     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     3     0     1     0     0  4870     0     0     4]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0    17    50     0     0     0  3500     0]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.26493625430795

F1 scores:
[       nan 0.99341749 0.9982854  0.98892444 0.96932929 0.9893859
 0.99723559 0.9992242  0.98231827 0.97066962]

Kappa:
0.9902606560900689
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4737c7b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.173, val_acc:0.152]
Epoch [2/120    avg_loss:1.977, val_acc:0.177]
Epoch [3/120    avg_loss:1.818, val_acc:0.181]
Epoch [4/120    avg_loss:1.691, val_acc:0.218]
Epoch [5/120    avg_loss:1.560, val_acc:0.243]
Epoch [6/120    avg_loss:1.442, val_acc:0.254]
Epoch [7/120    avg_loss:1.357, val_acc:0.300]
Epoch [8/120    avg_loss:1.238, val_acc:0.408]
Epoch [9/120    avg_loss:1.182, val_acc:0.398]
Epoch [10/120    avg_loss:1.081, val_acc:0.412]
Epoch [11/120    avg_loss:0.953, val_acc:0.475]
Epoch [12/120    avg_loss:0.867, val_acc:0.510]
Epoch [13/120    avg_loss:0.731, val_acc:0.588]
Epoch [14/120    avg_loss:0.632, val_acc:0.668]
Epoch [15/120    avg_loss:0.533, val_acc:0.747]
Epoch [16/120    avg_loss:0.535, val_acc:0.755]
Epoch [17/120    avg_loss:0.447, val_acc:0.788]
Epoch [18/120    avg_loss:0.766, val_acc:0.680]
Epoch [19/120    avg_loss:0.840, val_acc:0.710]
Epoch [20/120    avg_loss:0.543, val_acc:0.814]
Epoch [21/120    avg_loss:0.457, val_acc:0.783]
Epoch [22/120    avg_loss:0.363, val_acc:0.844]
Epoch [23/120    avg_loss:0.285, val_acc:0.911]
Epoch [24/120    avg_loss:0.233, val_acc:0.934]
Epoch [25/120    avg_loss:0.227, val_acc:0.919]
Epoch [26/120    avg_loss:0.783, val_acc:0.587]
Epoch [27/120    avg_loss:0.998, val_acc:0.651]
Epoch [28/120    avg_loss:0.834, val_acc:0.690]
Epoch [29/120    avg_loss:0.774, val_acc:0.688]
Epoch [30/120    avg_loss:0.673, val_acc:0.694]
Epoch [31/120    avg_loss:0.557, val_acc:0.830]
Epoch [32/120    avg_loss:0.509, val_acc:0.806]
Epoch [33/120    avg_loss:0.389, val_acc:0.874]
Epoch [34/120    avg_loss:0.342, val_acc:0.876]
Epoch [35/120    avg_loss:0.331, val_acc:0.857]
Epoch [36/120    avg_loss:0.626, val_acc:0.765]
Epoch [37/120    avg_loss:0.434, val_acc:0.762]
Epoch [38/120    avg_loss:0.365, val_acc:0.834]
Epoch [39/120    avg_loss:0.341, val_acc:0.845]
Epoch [40/120    avg_loss:0.328, val_acc:0.851]
Epoch [41/120    avg_loss:0.293, val_acc:0.848]
Epoch [42/120    avg_loss:0.301, val_acc:0.863]
Epoch [43/120    avg_loss:0.290, val_acc:0.870]
Epoch [44/120    avg_loss:0.303, val_acc:0.874]
Epoch [45/120    avg_loss:0.276, val_acc:0.877]
Epoch [46/120    avg_loss:0.279, val_acc:0.892]
Epoch [47/120    avg_loss:0.262, val_acc:0.897]
Epoch [48/120    avg_loss:0.268, val_acc:0.890]
Epoch [49/120    avg_loss:0.253, val_acc:0.887]
Epoch [50/120    avg_loss:0.249, val_acc:0.905]
Epoch [51/120    avg_loss:0.252, val_acc:0.909]
Epoch [52/120    avg_loss:0.262, val_acc:0.908]
Epoch [53/120    avg_loss:0.242, val_acc:0.907]
Epoch [54/120    avg_loss:0.252, val_acc:0.910]
Epoch [55/120    avg_loss:0.238, val_acc:0.909]
Epoch [56/120    avg_loss:0.262, val_acc:0.908]
Epoch [57/120    avg_loss:0.231, val_acc:0.911]
Epoch [58/120    avg_loss:0.259, val_acc:0.911]
Epoch [59/120    avg_loss:0.238, val_acc:0.908]
Epoch [60/120    avg_loss:0.233, val_acc:0.909]
Epoch [61/120    avg_loss:0.227, val_acc:0.909]
Epoch [62/120    avg_loss:0.229, val_acc:0.909]
Epoch [63/120    avg_loss:0.250, val_acc:0.909]
Epoch [64/120    avg_loss:0.226, val_acc:0.909]
Epoch [65/120    avg_loss:0.242, val_acc:0.909]
Epoch [66/120    avg_loss:0.229, val_acc:0.909]
Epoch [67/120    avg_loss:0.233, val_acc:0.909]
Epoch [68/120    avg_loss:0.229, val_acc:0.909]
Epoch [69/120    avg_loss:0.233, val_acc:0.909]
Epoch [70/120    avg_loss:0.229, val_acc:0.909]
Epoch [71/120    avg_loss:0.235, val_acc:0.909]
Epoch [72/120    avg_loss:0.234, val_acc:0.909]
Epoch [73/120    avg_loss:0.217, val_acc:0.909]
Epoch [74/120    avg_loss:0.236, val_acc:0.909]
Epoch [75/120    avg_loss:0.246, val_acc:0.909]
Epoch [76/120    avg_loss:0.223, val_acc:0.909]
Epoch [77/120    avg_loss:0.231, val_acc:0.909]
Epoch [78/120    avg_loss:0.225, val_acc:0.909]
Epoch [79/120    avg_loss:0.233, val_acc:0.909]
Epoch [80/120    avg_loss:0.219, val_acc:0.909]
Epoch [81/120    avg_loss:0.231, val_acc:0.909]
Epoch [82/120    avg_loss:0.253, val_acc:0.909]
Epoch [83/120    avg_loss:0.238, val_acc:0.909]
Epoch [84/120    avg_loss:0.241, val_acc:0.909]
Epoch [85/120    avg_loss:0.228, val_acc:0.909]
Epoch [86/120    avg_loss:0.231, val_acc:0.909]
Epoch [87/120    avg_loss:0.222, val_acc:0.909]
Epoch [88/120    avg_loss:0.230, val_acc:0.909]
Epoch [89/120    avg_loss:0.245, val_acc:0.909]
Epoch [90/120    avg_loss:0.233, val_acc:0.909]
Epoch [91/120    avg_loss:0.237, val_acc:0.909]
Epoch [92/120    avg_loss:0.260, val_acc:0.909]
Epoch [93/120    avg_loss:0.235, val_acc:0.909]
Epoch [94/120    avg_loss:0.244, val_acc:0.909]
Epoch [95/120    avg_loss:0.232, val_acc:0.909]
Epoch [96/120    avg_loss:0.226, val_acc:0.909]
Epoch [97/120    avg_loss:0.233, val_acc:0.909]
Epoch [98/120    avg_loss:0.233, val_acc:0.909]
Epoch [99/120    avg_loss:0.241, val_acc:0.909]
Epoch [100/120    avg_loss:0.239, val_acc:0.909]
Epoch [101/120    avg_loss:0.228, val_acc:0.909]
Epoch [102/120    avg_loss:0.241, val_acc:0.909]
Epoch [103/120    avg_loss:0.228, val_acc:0.909]
Epoch [104/120    avg_loss:0.248, val_acc:0.909]
Epoch [105/120    avg_loss:0.235, val_acc:0.909]
Epoch [106/120    avg_loss:0.235, val_acc:0.909]
Epoch [107/120    avg_loss:0.245, val_acc:0.909]
Epoch [108/120    avg_loss:0.237, val_acc:0.909]
Epoch [109/120    avg_loss:0.222, val_acc:0.909]
Epoch [110/120    avg_loss:0.230, val_acc:0.909]
Epoch [111/120    avg_loss:0.226, val_acc:0.909]
Epoch [112/120    avg_loss:0.230, val_acc:0.909]
Epoch [113/120    avg_loss:0.233, val_acc:0.909]
Epoch [114/120    avg_loss:0.243, val_acc:0.909]
Epoch [115/120    avg_loss:0.237, val_acc:0.909]
Epoch [116/120    avg_loss:0.228, val_acc:0.909]
Epoch [117/120    avg_loss:0.235, val_acc:0.909]
Epoch [118/120    avg_loss:0.234, val_acc:0.909]
Epoch [119/120    avg_loss:0.241, val_acc:0.909]
Epoch [120/120    avg_loss:0.236, val_acc:0.909]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5283     0     0   328     0     0    30   782     9]
 [    0     0 17033     0   293     0   763     0     1     0]
 [    0     6     0  1790     9     0     0     0   157    74]
 [    0    82    19     1  2791     0    16     0    55     8]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0   233    10     4     0  4570     0    59     2]
 [    0    10     0     0     0     0     4  1233     0    43]
 [    0    38     0    18    68     0     0     0  3447     0]
 [    0     6     0     5    13    71     0     0     3   821]]

Accuracy:
92.2348347914106

F1 scores:
[       nan 0.89111917 0.96299647 0.92746114 0.86168571 0.97275103
 0.89336331 0.96592244 0.85374613 0.8743344 ]

Kappa:
0.8982148226126372
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7fb665be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.187, val_acc:0.125]
Epoch [2/120    avg_loss:1.944, val_acc:0.166]
Epoch [3/120    avg_loss:1.813, val_acc:0.199]
Epoch [4/120    avg_loss:1.676, val_acc:0.273]
Epoch [5/120    avg_loss:1.533, val_acc:0.300]
Epoch [6/120    avg_loss:1.390, val_acc:0.331]
Epoch [7/120    avg_loss:1.301, val_acc:0.381]
Epoch [8/120    avg_loss:1.213, val_acc:0.372]
Epoch [9/120    avg_loss:1.107, val_acc:0.421]
Epoch [10/120    avg_loss:0.999, val_acc:0.446]
Epoch [11/120    avg_loss:0.937, val_acc:0.480]
Epoch [12/120    avg_loss:0.863, val_acc:0.576]
Epoch [13/120    avg_loss:0.729, val_acc:0.639]
Epoch [14/120    avg_loss:0.644, val_acc:0.721]
Epoch [15/120    avg_loss:0.546, val_acc:0.814]
Epoch [16/120    avg_loss:0.490, val_acc:0.900]
Epoch [17/120    avg_loss:0.394, val_acc:0.860]
Epoch [18/120    avg_loss:0.340, val_acc:0.895]
Epoch [19/120    avg_loss:0.339, val_acc:0.917]
Epoch [20/120    avg_loss:0.297, val_acc:0.918]
Epoch [21/120    avg_loss:0.317, val_acc:0.913]
Epoch [22/120    avg_loss:0.249, val_acc:0.931]
Epoch [23/120    avg_loss:0.494, val_acc:0.174]
Epoch [24/120    avg_loss:1.263, val_acc:0.475]
Epoch [25/120    avg_loss:0.742, val_acc:0.746]
Epoch [26/120    avg_loss:0.466, val_acc:0.865]
Epoch [27/120    avg_loss:0.285, val_acc:0.931]
Epoch [28/120    avg_loss:0.268, val_acc:0.923]
Epoch [29/120    avg_loss:0.253, val_acc:0.939]
Epoch [30/120    avg_loss:0.208, val_acc:0.925]
Epoch [31/120    avg_loss:0.179, val_acc:0.953]
Epoch [32/120    avg_loss:0.273, val_acc:0.938]
Epoch [33/120    avg_loss:0.223, val_acc:0.929]
Epoch [34/120    avg_loss:0.213, val_acc:0.957]
Epoch [35/120    avg_loss:0.151, val_acc:0.952]
Epoch [36/120    avg_loss:0.128, val_acc:0.968]
Epoch [37/120    avg_loss:0.122, val_acc:0.972]
Epoch [38/120    avg_loss:0.096, val_acc:0.969]
Epoch [39/120    avg_loss:0.139, val_acc:0.954]
Epoch [40/120    avg_loss:0.103, val_acc:0.970]
Epoch [41/120    avg_loss:0.079, val_acc:0.981]
Epoch [42/120    avg_loss:0.092, val_acc:0.963]
Epoch [43/120    avg_loss:0.082, val_acc:0.962]
Epoch [44/120    avg_loss:0.081, val_acc:0.976]
Epoch [45/120    avg_loss:1.750, val_acc:0.183]
Epoch [46/120    avg_loss:1.494, val_acc:0.240]
Epoch [47/120    avg_loss:1.396, val_acc:0.292]
Epoch [48/120    avg_loss:1.314, val_acc:0.412]
Epoch [49/120    avg_loss:1.233, val_acc:0.496]
Epoch [50/120    avg_loss:1.172, val_acc:0.438]
Epoch [51/120    avg_loss:1.136, val_acc:0.522]
Epoch [52/120    avg_loss:1.092, val_acc:0.419]
Epoch [53/120    avg_loss:1.017, val_acc:0.675]
Epoch [54/120    avg_loss:0.969, val_acc:0.673]
Epoch [55/120    avg_loss:0.941, val_acc:0.609]
Epoch [56/120    avg_loss:0.934, val_acc:0.616]
Epoch [57/120    avg_loss:0.932, val_acc:0.607]
Epoch [58/120    avg_loss:0.943, val_acc:0.610]
Epoch [59/120    avg_loss:0.934, val_acc:0.562]
Epoch [60/120    avg_loss:0.935, val_acc:0.628]
Epoch [61/120    avg_loss:0.926, val_acc:0.611]
Epoch [62/120    avg_loss:0.925, val_acc:0.579]
Epoch [63/120    avg_loss:0.899, val_acc:0.620]
Epoch [64/120    avg_loss:0.882, val_acc:0.630]
Epoch [65/120    avg_loss:0.918, val_acc:0.602]
Epoch [66/120    avg_loss:0.879, val_acc:0.579]
Epoch [67/120    avg_loss:0.880, val_acc:0.548]
Epoch [68/120    avg_loss:0.891, val_acc:0.552]
Epoch [69/120    avg_loss:0.865, val_acc:0.566]
Epoch [70/120    avg_loss:0.885, val_acc:0.573]
Epoch [71/120    avg_loss:0.889, val_acc:0.588]
Epoch [72/120    avg_loss:0.869, val_acc:0.589]
Epoch [73/120    avg_loss:0.900, val_acc:0.591]
Epoch [74/120    avg_loss:0.888, val_acc:0.585]
Epoch [75/120    avg_loss:0.886, val_acc:0.591]
Epoch [76/120    avg_loss:0.874, val_acc:0.598]
Epoch [77/120    avg_loss:0.889, val_acc:0.594]
Epoch [78/120    avg_loss:0.869, val_acc:0.593]
Epoch [79/120    avg_loss:0.866, val_acc:0.592]
Epoch [80/120    avg_loss:0.870, val_acc:0.592]
Epoch [81/120    avg_loss:0.889, val_acc:0.594]
Epoch [82/120    avg_loss:0.876, val_acc:0.594]
Epoch [83/120    avg_loss:0.876, val_acc:0.595]
Epoch [84/120    avg_loss:0.878, val_acc:0.595]
Epoch [85/120    avg_loss:0.891, val_acc:0.592]
Epoch [86/120    avg_loss:0.857, val_acc:0.594]
Epoch [87/120    avg_loss:0.898, val_acc:0.595]
Epoch [88/120    avg_loss:0.890, val_acc:0.595]
Epoch [89/120    avg_loss:0.885, val_acc:0.595]
Epoch [90/120    avg_loss:0.857, val_acc:0.595]
Epoch [91/120    avg_loss:0.858, val_acc:0.595]
Epoch [92/120    avg_loss:0.865, val_acc:0.595]
Epoch [93/120    avg_loss:0.871, val_acc:0.596]
Epoch [94/120    avg_loss:0.891, val_acc:0.596]
Epoch [95/120    avg_loss:0.877, val_acc:0.596]
Epoch [96/120    avg_loss:0.878, val_acc:0.596]
Epoch [97/120    avg_loss:0.863, val_acc:0.596]
Epoch [98/120    avg_loss:0.891, val_acc:0.596]
Epoch [99/120    avg_loss:0.870, val_acc:0.595]
Epoch [100/120    avg_loss:0.855, val_acc:0.595]
Epoch [101/120    avg_loss:0.877, val_acc:0.595]
Epoch [102/120    avg_loss:0.875, val_acc:0.595]
Epoch [103/120    avg_loss:0.881, val_acc:0.595]
Epoch [104/120    avg_loss:0.890, val_acc:0.595]
Epoch [105/120    avg_loss:0.872, val_acc:0.596]
Epoch [106/120    avg_loss:0.890, val_acc:0.596]
Epoch [107/120    avg_loss:0.885, val_acc:0.596]
Epoch [108/120    avg_loss:0.882, val_acc:0.596]
Epoch [109/120    avg_loss:0.884, val_acc:0.596]
Epoch [110/120    avg_loss:0.891, val_acc:0.596]
Epoch [111/120    avg_loss:0.894, val_acc:0.596]
Epoch [112/120    avg_loss:0.866, val_acc:0.596]
Epoch [113/120    avg_loss:0.880, val_acc:0.596]
Epoch [114/120    avg_loss:0.857, val_acc:0.596]
Epoch [115/120    avg_loss:0.876, val_acc:0.596]
Epoch [116/120    avg_loss:0.888, val_acc:0.596]
Epoch [117/120    avg_loss:0.888, val_acc:0.596]
Epoch [118/120    avg_loss:0.885, val_acc:0.596]
Epoch [119/120    avg_loss:0.891, val_acc:0.596]
Epoch [120/120    avg_loss:0.870, val_acc:0.596]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  2858   374    43   433     9   157   331  1311   916]
 [    0    10 11577     0    33     0  6466     0     4     0]
 [    0    12    33  1106   220     0    13     0   364   288]
 [    0   126   286     8  1992     0   380     0   168    12]
 [    0     0     0     1     0  1304     0     0     0     0]
 [    0    11   765     0    23     0  3826     0   253     0]
 [    0    74     0     0     8     0     8  1177     0    23]
 [    0    60    27    80     2     0   124     0  3226    52]
 [    0    17     0    18    21   205     0     1     3   654]]

Accuracy:
66.80644928060156

F1 scores:
[       nan 0.59541667 0.74325886 0.67193196 0.69845722 0.92383989
 0.48271511 0.84101465 0.72494382 0.45670391]

Kappa:
0.584692501374772
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d7e312ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.179, val_acc:0.129]
Epoch [2/120    avg_loss:1.996, val_acc:0.093]
Epoch [3/120    avg_loss:1.850, val_acc:0.091]
Epoch [4/120    avg_loss:1.726, val_acc:0.121]
Epoch [5/120    avg_loss:1.606, val_acc:0.218]
Epoch [6/120    avg_loss:1.506, val_acc:0.300]
Epoch [7/120    avg_loss:1.402, val_acc:0.304]
Epoch [8/120    avg_loss:1.303, val_acc:0.332]
Epoch [9/120    avg_loss:1.243, val_acc:0.345]
Epoch [10/120    avg_loss:1.123, val_acc:0.424]
Epoch [11/120    avg_loss:1.091, val_acc:0.411]
Epoch [12/120    avg_loss:0.989, val_acc:0.441]
Epoch [13/120    avg_loss:0.896, val_acc:0.433]
Epoch [14/120    avg_loss:0.817, val_acc:0.515]
Epoch [15/120    avg_loss:0.725, val_acc:0.592]
Epoch [16/120    avg_loss:0.655, val_acc:0.661]
Epoch [17/120    avg_loss:0.526, val_acc:0.739]
Epoch [18/120    avg_loss:0.492, val_acc:0.780]
Epoch [19/120    avg_loss:0.483, val_acc:0.762]
Epoch [20/120    avg_loss:0.387, val_acc:0.852]
Epoch [21/120    avg_loss:0.341, val_acc:0.864]
Epoch [22/120    avg_loss:0.326, val_acc:0.848]
Epoch [23/120    avg_loss:0.305, val_acc:0.848]
Epoch [24/120    avg_loss:0.295, val_acc:0.897]
Epoch [25/120    avg_loss:0.243, val_acc:0.872]
Epoch [26/120    avg_loss:0.297, val_acc:0.876]
Epoch [27/120    avg_loss:0.214, val_acc:0.889]
Epoch [28/120    avg_loss:0.214, val_acc:0.880]
Epoch [29/120    avg_loss:0.196, val_acc:0.921]
Epoch [30/120    avg_loss:0.169, val_acc:0.924]
Epoch [31/120    avg_loss:0.186, val_acc:0.941]
Epoch [32/120    avg_loss:0.164, val_acc:0.923]
Epoch [33/120    avg_loss:0.141, val_acc:0.937]
Epoch [34/120    avg_loss:0.124, val_acc:0.947]
Epoch [35/120    avg_loss:0.130, val_acc:0.948]
Epoch [36/120    avg_loss:0.107, val_acc:0.950]
Epoch [37/120    avg_loss:0.121, val_acc:0.953]
Epoch [38/120    avg_loss:0.087, val_acc:0.933]
Epoch [39/120    avg_loss:0.100, val_acc:0.942]
Epoch [40/120    avg_loss:0.098, val_acc:0.950]
Epoch [41/120    avg_loss:0.086, val_acc:0.936]
Epoch [42/120    avg_loss:0.102, val_acc:0.960]
Epoch [43/120    avg_loss:0.079, val_acc:0.965]
Epoch [44/120    avg_loss:0.062, val_acc:0.969]
Epoch [45/120    avg_loss:0.061, val_acc:0.963]
Epoch [46/120    avg_loss:0.053, val_acc:0.968]
Epoch [47/120    avg_loss:0.080, val_acc:0.961]
Epoch [48/120    avg_loss:0.043, val_acc:0.970]
Epoch [49/120    avg_loss:0.042, val_acc:0.967]
Epoch [50/120    avg_loss:0.040, val_acc:0.977]
Epoch [51/120    avg_loss:0.028, val_acc:0.976]
Epoch [52/120    avg_loss:0.036, val_acc:0.973]
Epoch [53/120    avg_loss:0.040, val_acc:0.964]
Epoch [54/120    avg_loss:0.038, val_acc:0.969]
Epoch [55/120    avg_loss:0.040, val_acc:0.970]
Epoch [56/120    avg_loss:0.033, val_acc:0.970]
Epoch [57/120    avg_loss:0.040, val_acc:0.985]
Epoch [58/120    avg_loss:0.030, val_acc:0.963]
Epoch [59/120    avg_loss:0.035, val_acc:0.982]
Epoch [60/120    avg_loss:0.045, val_acc:0.956]
Epoch [61/120    avg_loss:0.036, val_acc:0.970]
Epoch [62/120    avg_loss:0.054, val_acc:0.971]
Epoch [63/120    avg_loss:0.048, val_acc:0.976]
Epoch [64/120    avg_loss:0.048, val_acc:0.968]
Epoch [65/120    avg_loss:0.038, val_acc:0.984]
Epoch [66/120    avg_loss:0.034, val_acc:0.980]
Epoch [67/120    avg_loss:0.043, val_acc:0.946]
Epoch [68/120    avg_loss:0.047, val_acc:0.972]
Epoch [69/120    avg_loss:0.033, val_acc:0.981]
Epoch [70/120    avg_loss:0.043, val_acc:0.969]
Epoch [71/120    avg_loss:0.034, val_acc:0.984]
Epoch [72/120    avg_loss:0.029, val_acc:0.985]
Epoch [73/120    avg_loss:0.023, val_acc:0.982]
Epoch [74/120    avg_loss:0.020, val_acc:0.987]
Epoch [75/120    avg_loss:0.016, val_acc:0.988]
Epoch [76/120    avg_loss:0.018, val_acc:0.987]
Epoch [77/120    avg_loss:0.022, val_acc:0.987]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.022, val_acc:0.986]
Epoch [80/120    avg_loss:0.018, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.988]
Epoch [82/120    avg_loss:0.018, val_acc:0.987]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.017, val_acc:0.988]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.016, val_acc:0.988]
Epoch [87/120    avg_loss:0.017, val_acc:0.988]
Epoch [88/120    avg_loss:0.015, val_acc:0.988]
Epoch [89/120    avg_loss:0.016, val_acc:0.989]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.988]
Epoch [92/120    avg_loss:0.015, val_acc:0.989]
Epoch [93/120    avg_loss:0.016, val_acc:0.989]
Epoch [94/120    avg_loss:0.013, val_acc:0.989]
Epoch [95/120    avg_loss:0.013, val_acc:0.990]
Epoch [96/120    avg_loss:0.017, val_acc:0.988]
Epoch [97/120    avg_loss:0.013, val_acc:0.990]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.021, val_acc:0.990]
Epoch [100/120    avg_loss:0.013, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.989]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.016, val_acc:0.988]
Epoch [105/120    avg_loss:0.012, val_acc:0.987]
Epoch [106/120    avg_loss:0.015, val_acc:0.988]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.989]
Epoch [110/120    avg_loss:0.015, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.988]
Epoch [112/120    avg_loss:0.016, val_acc:0.988]
Epoch [113/120    avg_loss:0.019, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.012, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.013, val_acc:0.988]
Epoch [119/120    avg_loss:0.015, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0     0     0    32     0]
 [    0     0 18057     0    11     0    22     0     0     0]
 [    0     0     1  2001     0     0     0     0    31     3]
 [    0    21    22     0  2890     0    12     0    26     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    13     0     0  4862     0     2     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0    18     0    25    53     0     0     0  3475     0]
 [    0     0     0     0    17    63     0     0     0   839]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99448372 0.99842415 0.98208589 0.97257277 0.97643098
 0.99468085 0.99883586 0.97379851 0.95178673]

Kappa:
0.9879609100582799
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb3088ab70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.199, val_acc:0.214]
Epoch [2/120    avg_loss:1.980, val_acc:0.197]
Epoch [3/120    avg_loss:1.814, val_acc:0.262]
Epoch [4/120    avg_loss:1.717, val_acc:0.278]
Epoch [5/120    avg_loss:1.586, val_acc:0.281]
Epoch [6/120    avg_loss:1.471, val_acc:0.308]
Epoch [7/120    avg_loss:1.388, val_acc:0.323]
Epoch [8/120    avg_loss:1.234, val_acc:0.352]
Epoch [9/120    avg_loss:1.162, val_acc:0.444]
Epoch [10/120    avg_loss:1.025, val_acc:0.545]
Epoch [11/120    avg_loss:0.946, val_acc:0.580]
Epoch [12/120    avg_loss:0.841, val_acc:0.694]
Epoch [13/120    avg_loss:0.681, val_acc:0.749]
Epoch [14/120    avg_loss:0.615, val_acc:0.750]
Epoch [15/120    avg_loss:0.544, val_acc:0.782]
Epoch [16/120    avg_loss:0.463, val_acc:0.805]
Epoch [17/120    avg_loss:0.479, val_acc:0.779]
Epoch [18/120    avg_loss:0.425, val_acc:0.796]
Epoch [19/120    avg_loss:0.374, val_acc:0.839]
Epoch [20/120    avg_loss:0.346, val_acc:0.846]
Epoch [21/120    avg_loss:0.286, val_acc:0.878]
Epoch [22/120    avg_loss:0.266, val_acc:0.852]
Epoch [23/120    avg_loss:0.270, val_acc:0.864]
Epoch [24/120    avg_loss:0.204, val_acc:0.905]
Epoch [25/120    avg_loss:0.227, val_acc:0.910]
Epoch [26/120    avg_loss:0.215, val_acc:0.935]
Epoch [27/120    avg_loss:0.179, val_acc:0.897]
Epoch [28/120    avg_loss:0.203, val_acc:0.929]
Epoch [29/120    avg_loss:0.157, val_acc:0.941]
Epoch [30/120    avg_loss:0.148, val_acc:0.934]
Epoch [31/120    avg_loss:0.195, val_acc:0.929]
Epoch [32/120    avg_loss:0.123, val_acc:0.931]
Epoch [33/120    avg_loss:0.130, val_acc:0.940]
Epoch [34/120    avg_loss:0.117, val_acc:0.949]
Epoch [35/120    avg_loss:0.111, val_acc:0.964]
Epoch [36/120    avg_loss:0.095, val_acc:0.967]
Epoch [37/120    avg_loss:0.080, val_acc:0.963]
Epoch [38/120    avg_loss:0.073, val_acc:0.967]
Epoch [39/120    avg_loss:0.076, val_acc:0.961]
Epoch [40/120    avg_loss:0.073, val_acc:0.964]
Epoch [41/120    avg_loss:0.127, val_acc:0.939]
Epoch [42/120    avg_loss:0.090, val_acc:0.964]
Epoch [43/120    avg_loss:0.069, val_acc:0.970]
Epoch [44/120    avg_loss:0.052, val_acc:0.977]
Epoch [45/120    avg_loss:0.037, val_acc:0.970]
Epoch [46/120    avg_loss:0.041, val_acc:0.972]
Epoch [47/120    avg_loss:0.043, val_acc:0.977]
Epoch [48/120    avg_loss:0.037, val_acc:0.980]
Epoch [49/120    avg_loss:0.038, val_acc:0.977]
Epoch [50/120    avg_loss:0.050, val_acc:0.975]
Epoch [51/120    avg_loss:0.034, val_acc:0.971]
Epoch [52/120    avg_loss:0.044, val_acc:0.975]
Epoch [53/120    avg_loss:0.038, val_acc:0.973]
Epoch [54/120    avg_loss:0.046, val_acc:0.975]
Epoch [55/120    avg_loss:0.035, val_acc:0.974]
Epoch [56/120    avg_loss:0.030, val_acc:0.980]
Epoch [57/120    avg_loss:0.025, val_acc:0.977]
Epoch [58/120    avg_loss:0.066, val_acc:0.965]
Epoch [59/120    avg_loss:0.039, val_acc:0.973]
Epoch [60/120    avg_loss:0.028, val_acc:0.984]
Epoch [61/120    avg_loss:0.026, val_acc:0.980]
Epoch [62/120    avg_loss:0.023, val_acc:0.979]
Epoch [63/120    avg_loss:0.070, val_acc:0.977]
Epoch [64/120    avg_loss:0.064, val_acc:0.966]
Epoch [65/120    avg_loss:0.042, val_acc:0.979]
Epoch [66/120    avg_loss:0.084, val_acc:0.951]
Epoch [67/120    avg_loss:0.082, val_acc:0.956]
Epoch [68/120    avg_loss:0.044, val_acc:0.974]
Epoch [69/120    avg_loss:0.055, val_acc:0.971]
Epoch [70/120    avg_loss:0.034, val_acc:0.969]
Epoch [71/120    avg_loss:0.043, val_acc:0.970]
Epoch [72/120    avg_loss:0.022, val_acc:0.984]
Epoch [73/120    avg_loss:0.034, val_acc:0.970]
Epoch [74/120    avg_loss:0.022, val_acc:0.981]
Epoch [75/120    avg_loss:0.026, val_acc:0.979]
Epoch [76/120    avg_loss:0.018, val_acc:0.977]
Epoch [77/120    avg_loss:0.022, val_acc:0.983]
Epoch [78/120    avg_loss:0.025, val_acc:0.966]
Epoch [79/120    avg_loss:0.027, val_acc:0.977]
Epoch [80/120    avg_loss:0.014, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.021, val_acc:0.981]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.015, val_acc:0.985]
Epoch [85/120    avg_loss:0.014, val_acc:0.982]
Epoch [86/120    avg_loss:0.028, val_acc:0.977]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.015, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.011, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6342     0     2     1     0     0     0    75    12]
 [    0     2 18046     0    28     0    14     0     0     0]
 [    0    10     0  1995     0     0     0     0    30     1]
 [    0    12    21     0  2879     0     7     0    53     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     1     0     0  4845     0     0    15]
 [    0     0     0     0     0     0     0  1280     0    10]
 [    0     0     0    11    65     0     0     0  3491     4]
 [    0     0     0     6    11    50     0     0     0   852]]

Accuracy:
98.89619935892802

F1 scores:
[       nan 0.99109236 0.99773318 0.98494199 0.96675621 0.98120301
 0.99445813 0.99610895 0.96703601 0.93987865]

Kappa:
0.9853772471181406
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b353a8b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.244, val_acc:0.194]
Epoch [2/120    avg_loss:2.055, val_acc:0.221]
Epoch [3/120    avg_loss:1.913, val_acc:0.195]
Epoch [4/120    avg_loss:1.784, val_acc:0.203]
Epoch [5/120    avg_loss:1.663, val_acc:0.216]
Epoch [6/120    avg_loss:1.514, val_acc:0.293]
Epoch [7/120    avg_loss:1.381, val_acc:0.532]
Epoch [8/120    avg_loss:1.249, val_acc:0.622]
Epoch [9/120    avg_loss:1.134, val_acc:0.672]
Epoch [10/120    avg_loss:0.995, val_acc:0.679]
Epoch [11/120    avg_loss:0.883, val_acc:0.704]
Epoch [12/120    avg_loss:0.784, val_acc:0.730]
Epoch [13/120    avg_loss:0.708, val_acc:0.731]
Epoch [14/120    avg_loss:0.627, val_acc:0.762]
Epoch [15/120    avg_loss:0.585, val_acc:0.779]
Epoch [16/120    avg_loss:0.518, val_acc:0.840]
Epoch [17/120    avg_loss:0.429, val_acc:0.786]
Epoch [18/120    avg_loss:0.407, val_acc:0.811]
Epoch [19/120    avg_loss:0.359, val_acc:0.835]
Epoch [20/120    avg_loss:0.327, val_acc:0.885]
Epoch [21/120    avg_loss:0.284, val_acc:0.892]
Epoch [22/120    avg_loss:0.225, val_acc:0.927]
Epoch [23/120    avg_loss:0.236, val_acc:0.871]
Epoch [24/120    avg_loss:0.230, val_acc:0.882]
Epoch [25/120    avg_loss:0.223, val_acc:0.918]
Epoch [26/120    avg_loss:0.257, val_acc:0.935]
Epoch [27/120    avg_loss:0.217, val_acc:0.906]
Epoch [28/120    avg_loss:0.196, val_acc:0.951]
Epoch [29/120    avg_loss:0.190, val_acc:0.919]
Epoch [30/120    avg_loss:0.184, val_acc:0.933]
Epoch [31/120    avg_loss:0.145, val_acc:0.944]
Epoch [32/120    avg_loss:0.139, val_acc:0.931]
Epoch [33/120    avg_loss:0.153, val_acc:0.944]
Epoch [34/120    avg_loss:0.104, val_acc:0.964]
Epoch [35/120    avg_loss:0.130, val_acc:0.966]
Epoch [36/120    avg_loss:0.110, val_acc:0.966]
Epoch [37/120    avg_loss:0.089, val_acc:0.964]
Epoch [38/120    avg_loss:0.106, val_acc:0.965]
Epoch [39/120    avg_loss:0.088, val_acc:0.962]
Epoch [40/120    avg_loss:0.075, val_acc:0.973]
Epoch [41/120    avg_loss:0.054, val_acc:0.969]
Epoch [42/120    avg_loss:0.081, val_acc:0.965]
Epoch [43/120    avg_loss:0.064, val_acc:0.970]
Epoch [44/120    avg_loss:0.051, val_acc:0.962]
Epoch [45/120    avg_loss:0.059, val_acc:0.959]
Epoch [46/120    avg_loss:0.058, val_acc:0.973]
Epoch [47/120    avg_loss:0.075, val_acc:0.967]
Epoch [48/120    avg_loss:0.043, val_acc:0.977]
Epoch [49/120    avg_loss:0.039, val_acc:0.983]
Epoch [50/120    avg_loss:0.034, val_acc:0.982]
Epoch [51/120    avg_loss:0.044, val_acc:0.977]
Epoch [52/120    avg_loss:0.038, val_acc:0.984]
Epoch [53/120    avg_loss:0.048, val_acc:0.979]
Epoch [54/120    avg_loss:0.045, val_acc:0.977]
Epoch [55/120    avg_loss:0.045, val_acc:0.978]
Epoch [56/120    avg_loss:0.048, val_acc:0.972]
Epoch [57/120    avg_loss:0.033, val_acc:0.980]
Epoch [58/120    avg_loss:0.035, val_acc:0.984]
Epoch [59/120    avg_loss:0.032, val_acc:0.987]
Epoch [60/120    avg_loss:0.037, val_acc:0.985]
Epoch [61/120    avg_loss:0.047, val_acc:0.983]
Epoch [62/120    avg_loss:0.028, val_acc:0.990]
Epoch [63/120    avg_loss:0.023, val_acc:0.989]
Epoch [64/120    avg_loss:0.020, val_acc:0.984]
Epoch [65/120    avg_loss:0.025, val_acc:0.984]
Epoch [66/120    avg_loss:0.023, val_acc:0.983]
Epoch [67/120    avg_loss:0.024, val_acc:0.985]
Epoch [68/120    avg_loss:0.025, val_acc:0.982]
Epoch [69/120    avg_loss:0.018, val_acc:0.989]
Epoch [70/120    avg_loss:0.017, val_acc:0.991]
Epoch [71/120    avg_loss:0.015, val_acc:0.990]
Epoch [72/120    avg_loss:0.033, val_acc:0.984]
Epoch [73/120    avg_loss:0.021, val_acc:0.983]
Epoch [74/120    avg_loss:0.052, val_acc:0.977]
Epoch [75/120    avg_loss:0.035, val_acc:0.981]
Epoch [76/120    avg_loss:0.027, val_acc:0.980]
Epoch [77/120    avg_loss:0.030, val_acc:0.977]
Epoch [78/120    avg_loss:0.048, val_acc:0.980]
Epoch [79/120    avg_loss:0.030, val_acc:0.985]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.022, val_acc:0.976]
Epoch [82/120    avg_loss:0.019, val_acc:0.983]
Epoch [83/120    avg_loss:0.017, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.989]
Epoch [85/120    avg_loss:0.011, val_acc:0.990]
Epoch [86/120    avg_loss:0.010, val_acc:0.990]
Epoch [87/120    avg_loss:0.011, val_acc:0.990]
Epoch [88/120    avg_loss:0.013, val_acc:0.989]
Epoch [89/120    avg_loss:0.012, val_acc:0.989]
Epoch [90/120    avg_loss:0.009, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.990]
Epoch [92/120    avg_loss:0.012, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.990]
Epoch [97/120    avg_loss:0.009, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.010, val_acc:0.990]
Epoch [106/120    avg_loss:0.013, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.011, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.012, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.013, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.010, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6418     0     0     2     0     0     1    11     0]
 [    0     0 18079     0     4     0     7     0     0     0]
 [    0     3     0  2002     2     0     0     0    28     1]
 [    0    23    21    14  2890     0     7     0    16     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2    29     0     0  4846     0     1     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     3     0    23    73     0     0     0  3460    12]
 [    0     0     0     1    15    35     0     0     0   868]]

Accuracy:
99.18781481213699

F1 scores:
[       nan 0.99666123 0.99906057 0.97539586 0.9701242  0.98676749
 0.99527624 0.99883676 0.97643573 0.96283971]

Kappa:
0.9892368100644582
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9714456ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.215, val_acc:0.080]
Epoch [2/120    avg_loss:1.965, val_acc:0.086]
Epoch [3/120    avg_loss:1.802, val_acc:0.095]
Epoch [4/120    avg_loss:1.650, val_acc:0.161]
Epoch [5/120    avg_loss:1.526, val_acc:0.198]
Epoch [6/120    avg_loss:1.421, val_acc:0.294]
Epoch [7/120    avg_loss:1.306, val_acc:0.401]
Epoch [8/120    avg_loss:1.292, val_acc:0.378]
Epoch [9/120    avg_loss:1.132, val_acc:0.480]
Epoch [10/120    avg_loss:0.996, val_acc:0.503]
Epoch [11/120    avg_loss:0.871, val_acc:0.498]
Epoch [12/120    avg_loss:0.802, val_acc:0.537]
Epoch [13/120    avg_loss:0.697, val_acc:0.536]
Epoch [14/120    avg_loss:0.616, val_acc:0.598]
Epoch [15/120    avg_loss:0.546, val_acc:0.568]
Epoch [16/120    avg_loss:0.496, val_acc:0.718]
Epoch [17/120    avg_loss:0.460, val_acc:0.765]
Epoch [18/120    avg_loss:0.396, val_acc:0.790]
Epoch [19/120    avg_loss:0.390, val_acc:0.801]
Epoch [20/120    avg_loss:0.353, val_acc:0.817]
Epoch [21/120    avg_loss:0.302, val_acc:0.888]
Epoch [22/120    avg_loss:0.290, val_acc:0.824]
Epoch [23/120    avg_loss:0.338, val_acc:0.837]
Epoch [24/120    avg_loss:0.248, val_acc:0.894]
Epoch [25/120    avg_loss:0.234, val_acc:0.929]
Epoch [26/120    avg_loss:0.213, val_acc:0.898]
Epoch [27/120    avg_loss:0.189, val_acc:0.902]
Epoch [28/120    avg_loss:0.156, val_acc:0.917]
Epoch [29/120    avg_loss:0.157, val_acc:0.947]
Epoch [30/120    avg_loss:0.140, val_acc:0.948]
Epoch [31/120    avg_loss:0.161, val_acc:0.947]
Epoch [32/120    avg_loss:0.141, val_acc:0.945]
Epoch [33/120    avg_loss:0.143, val_acc:0.930]
Epoch [34/120    avg_loss:0.116, val_acc:0.958]
Epoch [35/120    avg_loss:0.145, val_acc:0.962]
Epoch [36/120    avg_loss:0.121, val_acc:0.961]
Epoch [37/120    avg_loss:0.086, val_acc:0.957]
Epoch [38/120    avg_loss:0.106, val_acc:0.948]
Epoch [39/120    avg_loss:0.076, val_acc:0.953]
Epoch [40/120    avg_loss:0.079, val_acc:0.969]
Epoch [41/120    avg_loss:0.137, val_acc:0.943]
Epoch [42/120    avg_loss:0.115, val_acc:0.962]
Epoch [43/120    avg_loss:0.103, val_acc:0.956]
Epoch [44/120    avg_loss:0.091, val_acc:0.962]
Epoch [45/120    avg_loss:0.074, val_acc:0.956]
Epoch [46/120    avg_loss:0.066, val_acc:0.969]
Epoch [47/120    avg_loss:0.078, val_acc:0.970]
Epoch [48/120    avg_loss:0.113, val_acc:0.944]
Epoch [49/120    avg_loss:0.070, val_acc:0.974]
Epoch [50/120    avg_loss:0.050, val_acc:0.971]
Epoch [51/120    avg_loss:0.051, val_acc:0.973]
Epoch [52/120    avg_loss:0.055, val_acc:0.971]
Epoch [53/120    avg_loss:0.052, val_acc:0.976]
Epoch [54/120    avg_loss:0.050, val_acc:0.955]
Epoch [55/120    avg_loss:0.064, val_acc:0.978]
Epoch [56/120    avg_loss:0.056, val_acc:0.943]
Epoch [57/120    avg_loss:0.043, val_acc:0.972]
Epoch [58/120    avg_loss:0.032, val_acc:0.981]
Epoch [59/120    avg_loss:0.029, val_acc:0.979]
Epoch [60/120    avg_loss:0.028, val_acc:0.981]
Epoch [61/120    avg_loss:0.045, val_acc:0.969]
Epoch [62/120    avg_loss:0.036, val_acc:0.973]
Epoch [63/120    avg_loss:0.036, val_acc:0.980]
Epoch [64/120    avg_loss:0.057, val_acc:0.978]
Epoch [65/120    avg_loss:0.058, val_acc:0.971]
Epoch [66/120    avg_loss:0.041, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.988]
Epoch [68/120    avg_loss:0.026, val_acc:0.980]
Epoch [69/120    avg_loss:0.030, val_acc:0.986]
Epoch [70/120    avg_loss:0.025, val_acc:0.971]
Epoch [71/120    avg_loss:0.037, val_acc:0.973]
Epoch [72/120    avg_loss:0.023, val_acc:0.980]
Epoch [73/120    avg_loss:0.023, val_acc:0.970]
Epoch [74/120    avg_loss:0.016, val_acc:0.984]
Epoch [75/120    avg_loss:0.018, val_acc:0.971]
Epoch [76/120    avg_loss:0.021, val_acc:0.976]
Epoch [77/120    avg_loss:0.019, val_acc:0.985]
Epoch [78/120    avg_loss:0.021, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.987]
Epoch [81/120    avg_loss:0.014, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.987]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.011, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     0     1     0     0    20    34    15]
 [    0     2 18033     0    34     0    18     0     3     0]
 [    0     5     0  1929     0     0     0     0   101     1]
 [    0    14    13     0  2920     0     5     0    18     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    48     1     0     0  4814     0    15     0]
 [    0    29     0     0     0     0     0  1258     0     3]
 [    0    46     3    70    40     0     0     0  3412     0]
 [    0     0     0     0    12     6     0     0     0   901]]

Accuracy:
98.65278480707589

F1 scores:
[       nan 0.9871218  0.99665626 0.95589693 0.97675197 0.99770642
 0.99104478 0.97975078 0.95387196 0.97881586]

Kappa:
0.9821463513889205
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3cdd1ebba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.259, val_acc:0.505]
Epoch [2/120    avg_loss:2.057, val_acc:0.495]
Epoch [3/120    avg_loss:1.900, val_acc:0.463]
Epoch [4/120    avg_loss:1.727, val_acc:0.392]
Epoch [5/120    avg_loss:1.576, val_acc:0.332]
Epoch [6/120    avg_loss:1.419, val_acc:0.331]
Epoch [7/120    avg_loss:1.274, val_acc:0.380]
Epoch [8/120    avg_loss:1.162, val_acc:0.509]
Epoch [9/120    avg_loss:1.054, val_acc:0.432]
Epoch [10/120    avg_loss:0.953, val_acc:0.481]
Epoch [11/120    avg_loss:0.862, val_acc:0.555]
Epoch [12/120    avg_loss:0.778, val_acc:0.578]
Epoch [13/120    avg_loss:0.677, val_acc:0.615]
Epoch [14/120    avg_loss:0.601, val_acc:0.649]
Epoch [15/120    avg_loss:0.587, val_acc:0.733]
Epoch [16/120    avg_loss:0.541, val_acc:0.683]
Epoch [17/120    avg_loss:0.477, val_acc:0.729]
Epoch [18/120    avg_loss:0.433, val_acc:0.738]
Epoch [19/120    avg_loss:0.415, val_acc:0.688]
Epoch [20/120    avg_loss:0.391, val_acc:0.743]
Epoch [21/120    avg_loss:0.343, val_acc:0.786]
Epoch [22/120    avg_loss:0.329, val_acc:0.760]
Epoch [23/120    avg_loss:0.332, val_acc:0.759]
Epoch [24/120    avg_loss:0.321, val_acc:0.746]
Epoch [25/120    avg_loss:0.298, val_acc:0.748]
Epoch [26/120    avg_loss:0.262, val_acc:0.797]
Epoch [27/120    avg_loss:0.252, val_acc:0.821]
Epoch [28/120    avg_loss:0.242, val_acc:0.814]
Epoch [29/120    avg_loss:0.230, val_acc:0.809]
Epoch [30/120    avg_loss:0.230, val_acc:0.819]
Epoch [31/120    avg_loss:0.209, val_acc:0.819]
Epoch [32/120    avg_loss:0.205, val_acc:0.823]
Epoch [33/120    avg_loss:0.194, val_acc:0.834]
Epoch [34/120    avg_loss:0.183, val_acc:0.848]
Epoch [35/120    avg_loss:0.180, val_acc:0.828]
Epoch [36/120    avg_loss:0.178, val_acc:0.862]
Epoch [37/120    avg_loss:0.166, val_acc:0.903]
Epoch [38/120    avg_loss:0.145, val_acc:0.862]
Epoch [39/120    avg_loss:0.128, val_acc:0.925]
Epoch [40/120    avg_loss:0.154, val_acc:0.922]
Epoch [41/120    avg_loss:0.138, val_acc:0.949]
Epoch [42/120    avg_loss:0.146, val_acc:0.901]
Epoch [43/120    avg_loss:0.114, val_acc:0.935]
Epoch [44/120    avg_loss:0.109, val_acc:0.953]
Epoch [45/120    avg_loss:0.111, val_acc:0.931]
Epoch [46/120    avg_loss:0.099, val_acc:0.961]
Epoch [47/120    avg_loss:0.095, val_acc:0.972]
Epoch [48/120    avg_loss:0.096, val_acc:0.957]
Epoch [49/120    avg_loss:0.104, val_acc:0.975]
Epoch [50/120    avg_loss:0.076, val_acc:0.963]
Epoch [51/120    avg_loss:0.066, val_acc:0.973]
Epoch [52/120    avg_loss:0.066, val_acc:0.968]
Epoch [53/120    avg_loss:0.059, val_acc:0.979]
Epoch [54/120    avg_loss:0.066, val_acc:0.973]
Epoch [55/120    avg_loss:0.062, val_acc:0.968]
Epoch [56/120    avg_loss:0.062, val_acc:0.977]
Epoch [57/120    avg_loss:0.047, val_acc:0.982]
Epoch [58/120    avg_loss:0.042, val_acc:0.972]
Epoch [59/120    avg_loss:0.060, val_acc:0.974]
Epoch [60/120    avg_loss:0.049, val_acc:0.977]
Epoch [61/120    avg_loss:0.034, val_acc:0.978]
Epoch [62/120    avg_loss:0.029, val_acc:0.980]
Epoch [63/120    avg_loss:0.072, val_acc:0.974]
Epoch [64/120    avg_loss:0.098, val_acc:0.956]
Epoch [65/120    avg_loss:0.071, val_acc:0.981]
Epoch [66/120    avg_loss:0.048, val_acc:0.974]
Epoch [67/120    avg_loss:0.057, val_acc:0.974]
Epoch [68/120    avg_loss:0.058, val_acc:0.981]
Epoch [69/120    avg_loss:0.042, val_acc:0.957]
Epoch [70/120    avg_loss:0.055, val_acc:0.964]
Epoch [71/120    avg_loss:0.043, val_acc:0.983]
Epoch [72/120    avg_loss:0.032, val_acc:0.985]
Epoch [73/120    avg_loss:0.025, val_acc:0.985]
Epoch [74/120    avg_loss:0.028, val_acc:0.985]
Epoch [75/120    avg_loss:0.033, val_acc:0.986]
Epoch [76/120    avg_loss:0.023, val_acc:0.986]
Epoch [77/120    avg_loss:0.023, val_acc:0.987]
Epoch [78/120    avg_loss:0.025, val_acc:0.986]
Epoch [79/120    avg_loss:0.022, val_acc:0.988]
Epoch [80/120    avg_loss:0.020, val_acc:0.987]
Epoch [81/120    avg_loss:0.021, val_acc:0.983]
Epoch [82/120    avg_loss:0.026, val_acc:0.985]
Epoch [83/120    avg_loss:0.024, val_acc:0.987]
Epoch [84/120    avg_loss:0.019, val_acc:0.986]
Epoch [85/120    avg_loss:0.023, val_acc:0.987]
Epoch [86/120    avg_loss:0.020, val_acc:0.987]
Epoch [87/120    avg_loss:0.021, val_acc:0.986]
Epoch [88/120    avg_loss:0.020, val_acc:0.988]
Epoch [89/120    avg_loss:0.020, val_acc:0.987]
Epoch [90/120    avg_loss:0.021, val_acc:0.988]
Epoch [91/120    avg_loss:0.018, val_acc:0.988]
Epoch [92/120    avg_loss:0.020, val_acc:0.987]
Epoch [93/120    avg_loss:0.018, val_acc:0.988]
Epoch [94/120    avg_loss:0.018, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.988]
Epoch [96/120    avg_loss:0.017, val_acc:0.989]
Epoch [97/120    avg_loss:0.019, val_acc:0.985]
Epoch [98/120    avg_loss:0.017, val_acc:0.987]
Epoch [99/120    avg_loss:0.020, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.986]
Epoch [101/120    avg_loss:0.018, val_acc:0.986]
Epoch [102/120    avg_loss:0.018, val_acc:0.987]
Epoch [103/120    avg_loss:0.017, val_acc:0.987]
Epoch [104/120    avg_loss:0.014, val_acc:0.987]
Epoch [105/120    avg_loss:0.017, val_acc:0.984]
Epoch [106/120    avg_loss:0.016, val_acc:0.987]
Epoch [107/120    avg_loss:0.020, val_acc:0.985]
Epoch [108/120    avg_loss:0.017, val_acc:0.987]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.017, val_acc:0.988]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.017, val_acc:0.988]
Epoch [113/120    avg_loss:0.016, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.018, val_acc:0.988]
Epoch [116/120    avg_loss:0.017, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.988]
Epoch [118/120    avg_loss:0.017, val_acc:0.988]
Epoch [119/120    avg_loss:0.017, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6353     0     0     1     0     0     3    58    17]
 [    0     0 17926     0    37     0   123     0     4     0]
 [    0     4     0  1917     0     0     0     0   114     1]
 [    0    14     5     0  2932     0    14     0     0     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     5     0  4844     0     7     0]
 [    0     8     0     0     0     0     0  1279     0     3]
 [    0     9     0    28    21     0    33     0  3479     1]
 [    0     2     0     0     0    11     0     0     0   906]]

Accuracy:
98.6696551225508

F1 scores:
[       nan 0.99095305 0.99470077 0.9630746  0.98257373 0.99580313
 0.97937727 0.99455677 0.96197981 0.97734628]

Kappa:
0.9823972620656176
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f535513eac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.196, val_acc:0.117]
Epoch [2/120    avg_loss:2.028, val_acc:0.110]
Epoch [3/120    avg_loss:1.849, val_acc:0.156]
Epoch [4/120    avg_loss:1.696, val_acc:0.246]
Epoch [5/120    avg_loss:1.553, val_acc:0.280]
Epoch [6/120    avg_loss:1.446, val_acc:0.318]
Epoch [7/120    avg_loss:1.341, val_acc:0.323]
Epoch [8/120    avg_loss:1.297, val_acc:0.358]
Epoch [9/120    avg_loss:1.169, val_acc:0.438]
Epoch [10/120    avg_loss:1.084, val_acc:0.491]
Epoch [11/120    avg_loss:0.968, val_acc:0.488]
Epoch [12/120    avg_loss:0.899, val_acc:0.503]
Epoch [13/120    avg_loss:0.785, val_acc:0.521]
Epoch [14/120    avg_loss:0.732, val_acc:0.522]
Epoch [15/120    avg_loss:0.666, val_acc:0.542]
Epoch [16/120    avg_loss:0.603, val_acc:0.568]
Epoch [17/120    avg_loss:0.502, val_acc:0.597]
Epoch [18/120    avg_loss:0.463, val_acc:0.591]
Epoch [19/120    avg_loss:0.434, val_acc:0.603]
Epoch [20/120    avg_loss:0.412, val_acc:0.634]
Epoch [21/120    avg_loss:0.387, val_acc:0.700]
Epoch [22/120    avg_loss:0.351, val_acc:0.713]
Epoch [23/120    avg_loss:0.331, val_acc:0.748]
Epoch [24/120    avg_loss:0.310, val_acc:0.735]
Epoch [25/120    avg_loss:0.300, val_acc:0.752]
Epoch [26/120    avg_loss:0.308, val_acc:0.746]
Epoch [27/120    avg_loss:0.383, val_acc:0.672]
Epoch [28/120    avg_loss:0.318, val_acc:0.790]
Epoch [29/120    avg_loss:0.260, val_acc:0.802]
Epoch [30/120    avg_loss:0.275, val_acc:0.767]
Epoch [31/120    avg_loss:0.274, val_acc:0.772]
Epoch [32/120    avg_loss:0.264, val_acc:0.788]
Epoch [33/120    avg_loss:0.228, val_acc:0.811]
Epoch [34/120    avg_loss:0.227, val_acc:0.754]
Epoch [35/120    avg_loss:0.253, val_acc:0.782]
Epoch [36/120    avg_loss:0.202, val_acc:0.819]
Epoch [37/120    avg_loss:0.184, val_acc:0.842]
Epoch [38/120    avg_loss:0.174, val_acc:0.833]
Epoch [39/120    avg_loss:0.195, val_acc:0.855]
Epoch [40/120    avg_loss:0.182, val_acc:0.877]
Epoch [41/120    avg_loss:0.171, val_acc:0.903]
Epoch [42/120    avg_loss:0.149, val_acc:0.910]
Epoch [43/120    avg_loss:0.151, val_acc:0.927]
Epoch [44/120    avg_loss:0.146, val_acc:0.907]
Epoch [45/120    avg_loss:0.138, val_acc:0.938]
Epoch [46/120    avg_loss:0.129, val_acc:0.916]
Epoch [47/120    avg_loss:0.126, val_acc:0.940]
Epoch [48/120    avg_loss:0.118, val_acc:0.957]
Epoch [49/120    avg_loss:0.099, val_acc:0.942]
Epoch [50/120    avg_loss:0.089, val_acc:0.949]
Epoch [51/120    avg_loss:0.091, val_acc:0.951]
Epoch [52/120    avg_loss:0.081, val_acc:0.943]
Epoch [53/120    avg_loss:0.086, val_acc:0.968]
Epoch [54/120    avg_loss:0.075, val_acc:0.972]
Epoch [55/120    avg_loss:0.067, val_acc:0.963]
Epoch [56/120    avg_loss:0.102, val_acc:0.944]
Epoch [57/120    avg_loss:0.103, val_acc:0.958]
Epoch [58/120    avg_loss:0.085, val_acc:0.964]
Epoch [59/120    avg_loss:0.073, val_acc:0.958]
Epoch [60/120    avg_loss:0.058, val_acc:0.927]
Epoch [61/120    avg_loss:0.070, val_acc:0.953]
Epoch [62/120    avg_loss:0.067, val_acc:0.970]
Epoch [63/120    avg_loss:0.067, val_acc:0.948]
Epoch [64/120    avg_loss:0.071, val_acc:0.972]
Epoch [65/120    avg_loss:0.053, val_acc:0.972]
Epoch [66/120    avg_loss:0.050, val_acc:0.949]
Epoch [67/120    avg_loss:0.059, val_acc:0.961]
Epoch [68/120    avg_loss:0.059, val_acc:0.981]
Epoch [69/120    avg_loss:0.043, val_acc:0.982]
Epoch [70/120    avg_loss:0.040, val_acc:0.975]
Epoch [71/120    avg_loss:0.035, val_acc:0.986]
Epoch [72/120    avg_loss:0.036, val_acc:0.983]
Epoch [73/120    avg_loss:0.031, val_acc:0.981]
Epoch [74/120    avg_loss:0.035, val_acc:0.982]
Epoch [75/120    avg_loss:0.045, val_acc:0.984]
Epoch [76/120    avg_loss:0.036, val_acc:0.973]
Epoch [77/120    avg_loss:0.049, val_acc:0.968]
Epoch [78/120    avg_loss:0.098, val_acc:0.963]
Epoch [79/120    avg_loss:0.056, val_acc:0.973]
Epoch [80/120    avg_loss:0.054, val_acc:0.960]
Epoch [81/120    avg_loss:0.043, val_acc:0.977]
Epoch [82/120    avg_loss:0.029, val_acc:0.978]
Epoch [83/120    avg_loss:0.027, val_acc:0.981]
Epoch [84/120    avg_loss:0.022, val_acc:0.975]
Epoch [85/120    avg_loss:0.020, val_acc:0.983]
Epoch [86/120    avg_loss:0.019, val_acc:0.983]
Epoch [87/120    avg_loss:0.018, val_acc:0.985]
Epoch [88/120    avg_loss:0.018, val_acc:0.987]
Epoch [89/120    avg_loss:0.017, val_acc:0.986]
Epoch [90/120    avg_loss:0.017, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.986]
Epoch [92/120    avg_loss:0.016, val_acc:0.984]
Epoch [93/120    avg_loss:0.015, val_acc:0.987]
Epoch [94/120    avg_loss:0.016, val_acc:0.987]
Epoch [95/120    avg_loss:0.015, val_acc:0.986]
Epoch [96/120    avg_loss:0.015, val_acc:0.986]
Epoch [97/120    avg_loss:0.015, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.987]
Epoch [99/120    avg_loss:0.014, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.985]
Epoch [101/120    avg_loss:0.014, val_acc:0.985]
Epoch [102/120    avg_loss:0.015, val_acc:0.986]
Epoch [103/120    avg_loss:0.015, val_acc:0.985]
Epoch [104/120    avg_loss:0.014, val_acc:0.985]
Epoch [105/120    avg_loss:0.015, val_acc:0.987]
Epoch [106/120    avg_loss:0.014, val_acc:0.984]
Epoch [107/120    avg_loss:0.014, val_acc:0.985]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.986]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.986]
Epoch [112/120    avg_loss:0.015, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.987]
Epoch [114/120    avg_loss:0.014, val_acc:0.987]
Epoch [115/120    avg_loss:0.015, val_acc:0.985]
Epoch [116/120    avg_loss:0.014, val_acc:0.984]
Epoch [117/120    avg_loss:0.022, val_acc:0.985]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.016, val_acc:0.987]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0     0     0     0     1    28    12]
 [    0     1 18021     0    45     0    23     0     0     0]
 [    0     8     0  1936     0     0     0     0    92     0]
 [    0    10    10     0  2917     0    33     0     2     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     6     0  4862     0     0     0]
 [    0    35     0     0     0     0     0  1255     0     0]
 [    0    26     0   116    37     0    12     0  3380     0]
 [    0     5     0     0     9    12     0     0     0   893]]

Accuracy:
98.71544597883981

F1 scores:
[       nan 0.99023861 0.99753674 0.94716243 0.97460742 0.99542334
 0.99143556 0.98586017 0.95574721 0.97916667]

Kappa:
0.982985166539288
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67d3582b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.188, val_acc:0.188]
Epoch [2/120    avg_loss:1.950, val_acc:0.107]
Epoch [3/120    avg_loss:1.770, val_acc:0.125]
Epoch [4/120    avg_loss:1.632, val_acc:0.175]
Epoch [5/120    avg_loss:1.500, val_acc:0.239]
Epoch [6/120    avg_loss:1.388, val_acc:0.292]
Epoch [7/120    avg_loss:1.295, val_acc:0.315]
Epoch [8/120    avg_loss:1.206, val_acc:0.448]
Epoch [9/120    avg_loss:1.099, val_acc:0.474]
Epoch [10/120    avg_loss:0.983, val_acc:0.493]
Epoch [11/120    avg_loss:0.884, val_acc:0.500]
Epoch [12/120    avg_loss:0.789, val_acc:0.500]
Epoch [13/120    avg_loss:0.722, val_acc:0.528]
Epoch [14/120    avg_loss:0.699, val_acc:0.519]
Epoch [15/120    avg_loss:0.649, val_acc:0.507]
Epoch [16/120    avg_loss:0.580, val_acc:0.535]
Epoch [17/120    avg_loss:0.530, val_acc:0.562]
Epoch [18/120    avg_loss:0.472, val_acc:0.664]
Epoch [19/120    avg_loss:0.419, val_acc:0.775]
Epoch [20/120    avg_loss:0.407, val_acc:0.790]
Epoch [21/120    avg_loss:0.378, val_acc:0.740]
Epoch [22/120    avg_loss:0.530, val_acc:0.629]
Epoch [23/120    avg_loss:0.385, val_acc:0.810]
Epoch [24/120    avg_loss:0.350, val_acc:0.793]
Epoch [25/120    avg_loss:0.313, val_acc:0.828]
Epoch [26/120    avg_loss:0.284, val_acc:0.823]
Epoch [27/120    avg_loss:0.262, val_acc:0.841]
Epoch [28/120    avg_loss:0.245, val_acc:0.842]
Epoch [29/120    avg_loss:0.223, val_acc:0.906]
Epoch [30/120    avg_loss:0.226, val_acc:0.876]
Epoch [31/120    avg_loss:0.222, val_acc:0.895]
Epoch [32/120    avg_loss:0.171, val_acc:0.932]
Epoch [33/120    avg_loss:0.172, val_acc:0.944]
Epoch [34/120    avg_loss:0.149, val_acc:0.947]
Epoch [35/120    avg_loss:0.130, val_acc:0.951]
Epoch [36/120    avg_loss:0.145, val_acc:0.931]
Epoch [37/120    avg_loss:0.136, val_acc:0.939]
Epoch [38/120    avg_loss:0.133, val_acc:0.945]
Epoch [39/120    avg_loss:0.111, val_acc:0.946]
Epoch [40/120    avg_loss:0.105, val_acc:0.932]
Epoch [41/120    avg_loss:0.121, val_acc:0.958]
Epoch [42/120    avg_loss:0.110, val_acc:0.963]
Epoch [43/120    avg_loss:0.083, val_acc:0.942]
Epoch [44/120    avg_loss:0.093, val_acc:0.938]
Epoch [45/120    avg_loss:0.084, val_acc:0.939]
Epoch [46/120    avg_loss:0.070, val_acc:0.956]
Epoch [47/120    avg_loss:0.072, val_acc:0.952]
Epoch [48/120    avg_loss:0.065, val_acc:0.960]
Epoch [49/120    avg_loss:0.076, val_acc:0.953]
Epoch [50/120    avg_loss:0.091, val_acc:0.962]
Epoch [51/120    avg_loss:0.060, val_acc:0.915]
Epoch [52/120    avg_loss:0.082, val_acc:0.959]
Epoch [53/120    avg_loss:0.075, val_acc:0.963]
Epoch [54/120    avg_loss:0.050, val_acc:0.968]
Epoch [55/120    avg_loss:0.050, val_acc:0.971]
Epoch [56/120    avg_loss:0.043, val_acc:0.976]
Epoch [57/120    avg_loss:0.056, val_acc:0.970]
Epoch [58/120    avg_loss:0.091, val_acc:0.957]
Epoch [59/120    avg_loss:0.052, val_acc:0.971]
Epoch [60/120    avg_loss:0.062, val_acc:0.979]
Epoch [61/120    avg_loss:0.044, val_acc:0.974]
Epoch [62/120    avg_loss:0.055, val_acc:0.979]
Epoch [63/120    avg_loss:0.038, val_acc:0.972]
Epoch [64/120    avg_loss:0.045, val_acc:0.983]
Epoch [65/120    avg_loss:0.027, val_acc:0.980]
Epoch [66/120    avg_loss:0.036, val_acc:0.973]
Epoch [67/120    avg_loss:0.033, val_acc:0.979]
Epoch [68/120    avg_loss:0.048, val_acc:0.982]
Epoch [69/120    avg_loss:0.046, val_acc:0.970]
Epoch [70/120    avg_loss:0.031, val_acc:0.979]
Epoch [71/120    avg_loss:0.032, val_acc:0.975]
Epoch [72/120    avg_loss:0.031, val_acc:0.969]
Epoch [73/120    avg_loss:0.046, val_acc:0.957]
Epoch [74/120    avg_loss:0.031, val_acc:0.975]
Epoch [75/120    avg_loss:0.022, val_acc:0.984]
Epoch [76/120    avg_loss:0.019, val_acc:0.978]
Epoch [77/120    avg_loss:0.018, val_acc:0.973]
Epoch [78/120    avg_loss:0.022, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.987]
Epoch [80/120    avg_loss:0.021, val_acc:0.983]
Epoch [81/120    avg_loss:0.019, val_acc:0.976]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.015, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.978]
Epoch [85/120    avg_loss:0.011, val_acc:0.988]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.987]
Epoch [89/120    avg_loss:0.012, val_acc:0.984]
Epoch [90/120    avg_loss:0.015, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.016, val_acc:0.976]
Epoch [93/120    avg_loss:0.014, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.014, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.013, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.015, val_acc:0.985]
Epoch [102/120    avg_loss:0.027, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     0     0     0     0    21    13]
 [    0     0 18051     0    15     0    21     0     3     0]
 [    0    15     0  1890     0     0     0     0   131     0]
 [    0     7     6     0  2927     0    30     0     1     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    16     0     3     0  4858     0     0     0]
 [    0    18     0     0     0     0     0  1270     2     0]
 [    0    36     0    39    36     0    19     0  3440     1]
 [    0     0     0     0     0    11     0     0     0   908]]

Accuracy:
98.92511989974213

F1 scores:
[       nan 0.99140002 0.99831319 0.95334174 0.98336973 0.99580313
 0.99082195 0.9921875  0.95968754 0.98588491]

Kappa:
0.9857559781274666
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8489362b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.218, val_acc:0.448]
Epoch [2/120    avg_loss:2.035, val_acc:0.470]
Epoch [3/120    avg_loss:1.912, val_acc:0.512]
Epoch [4/120    avg_loss:1.809, val_acc:0.531]
Epoch [5/120    avg_loss:1.689, val_acc:0.532]
Epoch [6/120    avg_loss:1.565, val_acc:0.549]
Epoch [7/120    avg_loss:1.425, val_acc:0.597]
Epoch [8/120    avg_loss:1.318, val_acc:0.637]
Epoch [9/120    avg_loss:1.237, val_acc:0.690]
Epoch [10/120    avg_loss:1.129, val_acc:0.743]
Epoch [11/120    avg_loss:1.040, val_acc:0.733]
Epoch [12/120    avg_loss:0.962, val_acc:0.745]
Epoch [13/120    avg_loss:0.837, val_acc:0.787]
Epoch [14/120    avg_loss:0.725, val_acc:0.779]
Epoch [15/120    avg_loss:0.668, val_acc:0.762]
Epoch [16/120    avg_loss:0.611, val_acc:0.781]
Epoch [17/120    avg_loss:0.511, val_acc:0.788]
Epoch [18/120    avg_loss:0.479, val_acc:0.777]
Epoch [19/120    avg_loss:0.448, val_acc:0.779]
Epoch [20/120    avg_loss:0.400, val_acc:0.806]
Epoch [21/120    avg_loss:0.369, val_acc:0.814]
Epoch [22/120    avg_loss:0.339, val_acc:0.823]
Epoch [23/120    avg_loss:0.299, val_acc:0.831]
Epoch [24/120    avg_loss:0.270, val_acc:0.885]
Epoch [25/120    avg_loss:0.245, val_acc:0.860]
Epoch [26/120    avg_loss:0.232, val_acc:0.882]
Epoch [27/120    avg_loss:0.191, val_acc:0.912]
Epoch [28/120    avg_loss:0.184, val_acc:0.858]
Epoch [29/120    avg_loss:0.184, val_acc:0.937]
Epoch [30/120    avg_loss:0.149, val_acc:0.927]
Epoch [31/120    avg_loss:0.138, val_acc:0.924]
Epoch [32/120    avg_loss:0.134, val_acc:0.923]
Epoch [33/120    avg_loss:0.154, val_acc:0.952]
Epoch [34/120    avg_loss:0.115, val_acc:0.946]
Epoch [35/120    avg_loss:0.093, val_acc:0.951]
Epoch [36/120    avg_loss:0.084, val_acc:0.954]
Epoch [37/120    avg_loss:0.116, val_acc:0.958]
Epoch [38/120    avg_loss:0.084, val_acc:0.953]
Epoch [39/120    avg_loss:0.066, val_acc:0.950]
Epoch [40/120    avg_loss:0.059, val_acc:0.966]
Epoch [41/120    avg_loss:0.067, val_acc:0.962]
Epoch [42/120    avg_loss:0.067, val_acc:0.962]
Epoch [43/120    avg_loss:0.047, val_acc:0.973]
Epoch [44/120    avg_loss:0.039, val_acc:0.976]
Epoch [45/120    avg_loss:0.036, val_acc:0.973]
Epoch [46/120    avg_loss:0.041, val_acc:0.973]
Epoch [47/120    avg_loss:0.046, val_acc:0.964]
Epoch [48/120    avg_loss:0.036, val_acc:0.971]
Epoch [49/120    avg_loss:0.044, val_acc:0.978]
Epoch [50/120    avg_loss:0.051, val_acc:0.964]
Epoch [51/120    avg_loss:0.060, val_acc:0.969]
Epoch [52/120    avg_loss:0.038, val_acc:0.971]
Epoch [53/120    avg_loss:0.047, val_acc:0.963]
Epoch [54/120    avg_loss:0.043, val_acc:0.973]
Epoch [55/120    avg_loss:0.024, val_acc:0.978]
Epoch [56/120    avg_loss:0.024, val_acc:0.976]
Epoch [57/120    avg_loss:0.023, val_acc:0.972]
Epoch [58/120    avg_loss:0.018, val_acc:0.979]
Epoch [59/120    avg_loss:0.018, val_acc:0.970]
Epoch [60/120    avg_loss:0.030, val_acc:0.978]
Epoch [61/120    avg_loss:0.020, val_acc:0.980]
Epoch [62/120    avg_loss:0.022, val_acc:0.980]
Epoch [63/120    avg_loss:0.022, val_acc:0.980]
Epoch [64/120    avg_loss:0.026, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.981]
Epoch [66/120    avg_loss:0.017, val_acc:0.977]
Epoch [67/120    avg_loss:0.020, val_acc:0.976]
Epoch [68/120    avg_loss:0.031, val_acc:0.972]
Epoch [69/120    avg_loss:0.031, val_acc:0.979]
Epoch [70/120    avg_loss:0.019, val_acc:0.976]
Epoch [71/120    avg_loss:0.017, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.978]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.020, val_acc:0.966]
Epoch [76/120    avg_loss:0.039, val_acc:0.967]
Epoch [77/120    avg_loss:0.028, val_acc:0.978]
Epoch [78/120    avg_loss:0.018, val_acc:0.979]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.013, val_acc:0.979]
Epoch [81/120    avg_loss:0.010, val_acc:0.979]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.981]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.009, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.984]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     1     4     0     0     0    56    21]
 [    0     1 18035     0    40     0    12     0     2     0]
 [    0     6     1  1914     0     0     0     0   115     0]
 [    0    13    12     0  2922     0    18     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    46     0     1     0  4802     0    29     0]
 [    0    21     0     0     0     0     0  1269     0     0]
 [    0    71     6    52    42     0     0     0  3400     0]
 [    0     3     0     0     4    18     0     0     0   894]]

Accuracy:
98.54915286915866

F1 scores:
[       nan 0.98472513 0.99668417 0.95628279 0.9764411  0.99315068
 0.98908342 0.99179367 0.94733909 0.97385621]

Kappa:
0.9807713552368555
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe546519b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.195, val_acc:0.085]
Epoch [2/120    avg_loss:1.991, val_acc:0.098]
Epoch [3/120    avg_loss:1.840, val_acc:0.115]
Epoch [4/120    avg_loss:1.758, val_acc:0.146]
Epoch [5/120    avg_loss:1.613, val_acc:0.193]
Epoch [6/120    avg_loss:1.510, val_acc:0.302]
Epoch [7/120    avg_loss:1.390, val_acc:0.351]
Epoch [8/120    avg_loss:1.306, val_acc:0.421]
Epoch [9/120    avg_loss:1.173, val_acc:0.452]
Epoch [10/120    avg_loss:1.017, val_acc:0.458]
Epoch [11/120    avg_loss:0.903, val_acc:0.491]
Epoch [12/120    avg_loss:0.836, val_acc:0.542]
Epoch [13/120    avg_loss:0.761, val_acc:0.568]
Epoch [14/120    avg_loss:0.699, val_acc:0.588]
Epoch [15/120    avg_loss:0.619, val_acc:0.635]
Epoch [16/120    avg_loss:0.574, val_acc:0.664]
Epoch [17/120    avg_loss:0.512, val_acc:0.726]
Epoch [18/120    avg_loss:0.441, val_acc:0.724]
Epoch [19/120    avg_loss:0.438, val_acc:0.740]
Epoch [20/120    avg_loss:0.376, val_acc:0.749]
Epoch [21/120    avg_loss:0.362, val_acc:0.745]
Epoch [22/120    avg_loss:0.342, val_acc:0.760]
Epoch [23/120    avg_loss:0.304, val_acc:0.878]
Epoch [24/120    avg_loss:0.294, val_acc:0.924]
Epoch [25/120    avg_loss:0.257, val_acc:0.919]
Epoch [26/120    avg_loss:0.255, val_acc:0.904]
Epoch [27/120    avg_loss:0.220, val_acc:0.900]
Epoch [28/120    avg_loss:0.216, val_acc:0.886]
Epoch [29/120    avg_loss:0.217, val_acc:0.878]
Epoch [30/120    avg_loss:0.193, val_acc:0.898]
Epoch [31/120    avg_loss:0.162, val_acc:0.895]
Epoch [32/120    avg_loss:0.176, val_acc:0.917]
Epoch [33/120    avg_loss:0.163, val_acc:0.941]
Epoch [34/120    avg_loss:0.153, val_acc:0.957]
Epoch [35/120    avg_loss:0.157, val_acc:0.956]
Epoch [36/120    avg_loss:0.152, val_acc:0.938]
Epoch [37/120    avg_loss:0.110, val_acc:0.973]
Epoch [38/120    avg_loss:0.096, val_acc:0.965]
Epoch [39/120    avg_loss:0.108, val_acc:0.944]
Epoch [40/120    avg_loss:0.119, val_acc:0.958]
Epoch [41/120    avg_loss:0.102, val_acc:0.952]
Epoch [42/120    avg_loss:0.089, val_acc:0.963]
Epoch [43/120    avg_loss:0.072, val_acc:0.967]
Epoch [44/120    avg_loss:0.086, val_acc:0.927]
Epoch [45/120    avg_loss:0.127, val_acc:0.960]
Epoch [46/120    avg_loss:0.121, val_acc:0.968]
Epoch [47/120    avg_loss:0.094, val_acc:0.927]
Epoch [48/120    avg_loss:0.076, val_acc:0.960]
Epoch [49/120    avg_loss:0.069, val_acc:0.923]
Epoch [50/120    avg_loss:0.063, val_acc:0.973]
Epoch [51/120    avg_loss:0.059, val_acc:0.932]
Epoch [52/120    avg_loss:0.068, val_acc:0.964]
Epoch [53/120    avg_loss:0.082, val_acc:0.973]
Epoch [54/120    avg_loss:0.055, val_acc:0.977]
Epoch [55/120    avg_loss:0.037, val_acc:0.978]
Epoch [56/120    avg_loss:0.046, val_acc:0.978]
Epoch [57/120    avg_loss:0.051, val_acc:0.975]
Epoch [58/120    avg_loss:0.034, val_acc:0.980]
Epoch [59/120    avg_loss:0.035, val_acc:0.980]
Epoch [60/120    avg_loss:0.039, val_acc:0.981]
Epoch [61/120    avg_loss:0.035, val_acc:0.973]
Epoch [62/120    avg_loss:0.041, val_acc:0.976]
Epoch [63/120    avg_loss:0.031, val_acc:0.981]
Epoch [64/120    avg_loss:0.026, val_acc:0.974]
Epoch [65/120    avg_loss:0.030, val_acc:0.976]
Epoch [66/120    avg_loss:0.029, val_acc:0.978]
Epoch [67/120    avg_loss:0.036, val_acc:0.972]
Epoch [68/120    avg_loss:0.029, val_acc:0.972]
Epoch [69/120    avg_loss:0.049, val_acc:0.939]
Epoch [70/120    avg_loss:0.052, val_acc:0.948]
Epoch [71/120    avg_loss:0.067, val_acc:0.971]
Epoch [72/120    avg_loss:0.037, val_acc:0.971]
Epoch [73/120    avg_loss:0.029, val_acc:0.978]
Epoch [74/120    avg_loss:0.031, val_acc:0.982]
Epoch [75/120    avg_loss:0.026, val_acc:0.978]
Epoch [76/120    avg_loss:0.018, val_acc:0.983]
Epoch [77/120    avg_loss:0.016, val_acc:0.952]
Epoch [78/120    avg_loss:0.022, val_acc:0.983]
Epoch [79/120    avg_loss:0.024, val_acc:0.980]
Epoch [80/120    avg_loss:0.023, val_acc:0.981]
Epoch [81/120    avg_loss:0.020, val_acc:0.975]
Epoch [82/120    avg_loss:0.023, val_acc:0.975]
Epoch [83/120    avg_loss:0.018, val_acc:0.983]
Epoch [84/120    avg_loss:0.019, val_acc:0.981]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.015, val_acc:0.982]
Epoch [87/120    avg_loss:0.012, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.015, val_acc:0.986]
Epoch [90/120    avg_loss:0.012, val_acc:0.983]
Epoch [91/120    avg_loss:0.024, val_acc:0.981]
Epoch [92/120    avg_loss:0.026, val_acc:0.984]
Epoch [93/120    avg_loss:0.027, val_acc:0.978]
Epoch [94/120    avg_loss:0.017, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.984]
Epoch [98/120    avg_loss:0.020, val_acc:0.966]
Epoch [99/120    avg_loss:0.038, val_acc:0.973]
Epoch [100/120    avg_loss:0.021, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6245     0     1     0     0     0     9   129    48]
 [    0     0 18056     0    29     0     0     0     5     0]
 [    0     7     0  1936     0     0     0     0    93     0]
 [    0    13     5     1  2926     0    21     0     1     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0    10     0  4835     0     4     0]
 [    0    19     0     0     0     0     0  1268     3     0]
 [    0    22     1    96    23     0    17     0  3406     6]
 [    0     8     0     0     0     6     0     0     0   905]]

Accuracy:
98.52746246354808

F1 scores:
[       nan 0.97991527 0.99809292 0.95135135 0.98187919 0.99770642
 0.99169316 0.98792365 0.94453688 0.96123208]

Kappa:
0.9804945846462703
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0172ffb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.104, val_acc:0.167]
Epoch [2/120    avg_loss:1.892, val_acc:0.176]
Epoch [3/120    avg_loss:1.729, val_acc:0.164]
Epoch [4/120    avg_loss:1.573, val_acc:0.171]
Epoch [5/120    avg_loss:1.430, val_acc:0.223]
Epoch [6/120    avg_loss:1.344, val_acc:0.347]
Epoch [7/120    avg_loss:1.245, val_acc:0.372]
Epoch [8/120    avg_loss:1.179, val_acc:0.421]
Epoch [9/120    avg_loss:1.100, val_acc:0.413]
Epoch [10/120    avg_loss:1.022, val_acc:0.463]
Epoch [11/120    avg_loss:0.934, val_acc:0.465]
Epoch [12/120    avg_loss:0.866, val_acc:0.485]
Epoch [13/120    avg_loss:0.791, val_acc:0.578]
Epoch [14/120    avg_loss:0.741, val_acc:0.547]
Epoch [15/120    avg_loss:0.688, val_acc:0.608]
Epoch [16/120    avg_loss:0.629, val_acc:0.604]
Epoch [17/120    avg_loss:0.562, val_acc:0.652]
Epoch [18/120    avg_loss:0.504, val_acc:0.667]
Epoch [19/120    avg_loss:0.456, val_acc:0.689]
Epoch [20/120    avg_loss:0.424, val_acc:0.747]
Epoch [21/120    avg_loss:0.396, val_acc:0.714]
Epoch [22/120    avg_loss:0.427, val_acc:0.722]
Epoch [23/120    avg_loss:0.372, val_acc:0.772]
Epoch [24/120    avg_loss:0.349, val_acc:0.767]
Epoch [25/120    avg_loss:0.291, val_acc:0.784]
Epoch [26/120    avg_loss:0.300, val_acc:0.830]
Epoch [27/120    avg_loss:0.308, val_acc:0.801]
Epoch [28/120    avg_loss:0.255, val_acc:0.843]
Epoch [29/120    avg_loss:0.236, val_acc:0.901]
Epoch [30/120    avg_loss:0.238, val_acc:0.862]
Epoch [31/120    avg_loss:0.231, val_acc:0.888]
Epoch [32/120    avg_loss:0.219, val_acc:0.898]
Epoch [33/120    avg_loss:0.209, val_acc:0.886]
Epoch [34/120    avg_loss:0.161, val_acc:0.930]
Epoch [35/120    avg_loss:0.137, val_acc:0.903]
Epoch [36/120    avg_loss:0.126, val_acc:0.938]
Epoch [37/120    avg_loss:0.125, val_acc:0.912]
Epoch [38/120    avg_loss:0.116, val_acc:0.955]
Epoch [39/120    avg_loss:0.103, val_acc:0.954]
Epoch [40/120    avg_loss:0.100, val_acc:0.940]
Epoch [41/120    avg_loss:0.139, val_acc:0.915]
Epoch [42/120    avg_loss:0.138, val_acc:0.919]
Epoch [43/120    avg_loss:0.117, val_acc:0.943]
Epoch [44/120    avg_loss:0.096, val_acc:0.954]
Epoch [45/120    avg_loss:0.096, val_acc:0.944]
Epoch [46/120    avg_loss:0.081, val_acc:0.964]
Epoch [47/120    avg_loss:0.069, val_acc:0.959]
Epoch [48/120    avg_loss:0.061, val_acc:0.967]
Epoch [49/120    avg_loss:0.053, val_acc:0.966]
Epoch [50/120    avg_loss:0.051, val_acc:0.964]
Epoch [51/120    avg_loss:0.055, val_acc:0.961]
Epoch [52/120    avg_loss:0.066, val_acc:0.961]
Epoch [53/120    avg_loss:0.052, val_acc:0.966]
Epoch [54/120    avg_loss:0.099, val_acc:0.941]
Epoch [55/120    avg_loss:0.054, val_acc:0.969]
Epoch [56/120    avg_loss:0.054, val_acc:0.964]
Epoch [57/120    avg_loss:0.063, val_acc:0.963]
Epoch [58/120    avg_loss:0.062, val_acc:0.937]
Epoch [59/120    avg_loss:0.044, val_acc:0.958]
Epoch [60/120    avg_loss:0.037, val_acc:0.972]
Epoch [61/120    avg_loss:0.059, val_acc:0.954]
Epoch [62/120    avg_loss:0.051, val_acc:0.960]
Epoch [63/120    avg_loss:0.061, val_acc:0.968]
Epoch [64/120    avg_loss:0.039, val_acc:0.968]
Epoch [65/120    avg_loss:0.039, val_acc:0.971]
Epoch [66/120    avg_loss:0.035, val_acc:0.957]
Epoch [67/120    avg_loss:0.037, val_acc:0.967]
Epoch [68/120    avg_loss:0.029, val_acc:0.973]
Epoch [69/120    avg_loss:0.024, val_acc:0.963]
Epoch [70/120    avg_loss:0.032, val_acc:0.963]
Epoch [71/120    avg_loss:0.022, val_acc:0.977]
Epoch [72/120    avg_loss:0.017, val_acc:0.975]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.027, val_acc:0.922]
Epoch [75/120    avg_loss:0.026, val_acc:0.965]
Epoch [76/120    avg_loss:0.025, val_acc:0.971]
Epoch [77/120    avg_loss:0.017, val_acc:0.972]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.021, val_acc:0.970]
Epoch [80/120    avg_loss:0.017, val_acc:0.981]
Epoch [81/120    avg_loss:0.024, val_acc:0.976]
Epoch [82/120    avg_loss:0.021, val_acc:0.975]
Epoch [83/120    avg_loss:0.015, val_acc:0.979]
Epoch [84/120    avg_loss:0.019, val_acc:0.975]
Epoch [85/120    avg_loss:0.012, val_acc:0.975]
Epoch [86/120    avg_loss:0.017, val_acc:0.963]
Epoch [87/120    avg_loss:0.014, val_acc:0.978]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.975]
Epoch [90/120    avg_loss:0.011, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.977]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.009, val_acc:0.979]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.007, val_acc:0.980]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.008, val_acc:0.979]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.980]
Epoch [107/120    avg_loss:0.009, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.982]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     1     0     0     0     0    12    44     5]
 [    0     0 18059     0    30     0     0     0     1     0]
 [    0    18     0  1952     0     0     0     0    66     0]
 [    0     9     8     0  2919     2    28     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    93     0     0     0  4785     0     0     0]
 [    0    27     0     0     0    10     0  1253     0     0]
 [    0    66     2    64    45     0     3     0  3391     0]
 [    0     0     0     0     2     9     0     0     0   908]]

Accuracy:
98.67206516761864

F1 scores:
[       nan 0.98591549 0.99627617 0.96347483 0.97821716 0.99201824
 0.98720858 0.98082192 0.95858657 0.98910675]

Kappa:
0.9823881228767201
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf10c5cb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.244, val_acc:0.072]
Epoch [2/120    avg_loss:1.999, val_acc:0.135]
Epoch [3/120    avg_loss:1.823, val_acc:0.263]
Epoch [4/120    avg_loss:1.677, val_acc:0.393]
Epoch [5/120    avg_loss:1.539, val_acc:0.456]
Epoch [6/120    avg_loss:1.423, val_acc:0.471]
Epoch [7/120    avg_loss:1.319, val_acc:0.554]
Epoch [8/120    avg_loss:1.247, val_acc:0.588]
Epoch [9/120    avg_loss:1.155, val_acc:0.625]
Epoch [10/120    avg_loss:1.044, val_acc:0.688]
Epoch [11/120    avg_loss:0.969, val_acc:0.695]
Epoch [12/120    avg_loss:0.888, val_acc:0.747]
Epoch [13/120    avg_loss:0.800, val_acc:0.736]
Epoch [14/120    avg_loss:0.729, val_acc:0.813]
Epoch [15/120    avg_loss:0.654, val_acc:0.813]
Epoch [16/120    avg_loss:0.588, val_acc:0.777]
Epoch [17/120    avg_loss:0.523, val_acc:0.835]
Epoch [18/120    avg_loss:0.490, val_acc:0.794]
Epoch [19/120    avg_loss:0.474, val_acc:0.726]
Epoch [20/120    avg_loss:0.438, val_acc:0.792]
Epoch [21/120    avg_loss:0.409, val_acc:0.794]
Epoch [22/120    avg_loss:0.367, val_acc:0.816]
Epoch [23/120    avg_loss:0.363, val_acc:0.808]
Epoch [24/120    avg_loss:0.326, val_acc:0.806]
Epoch [25/120    avg_loss:0.329, val_acc:0.828]
Epoch [26/120    avg_loss:0.286, val_acc:0.845]
Epoch [27/120    avg_loss:0.286, val_acc:0.847]
Epoch [28/120    avg_loss:0.255, val_acc:0.858]
Epoch [29/120    avg_loss:0.229, val_acc:0.866]
Epoch [30/120    avg_loss:0.237, val_acc:0.867]
Epoch [31/120    avg_loss:0.196, val_acc:0.872]
Epoch [32/120    avg_loss:0.202, val_acc:0.907]
Epoch [33/120    avg_loss:0.196, val_acc:0.927]
Epoch [34/120    avg_loss:0.184, val_acc:0.854]
Epoch [35/120    avg_loss:0.238, val_acc:0.860]
Epoch [36/120    avg_loss:0.181, val_acc:0.907]
Epoch [37/120    avg_loss:0.186, val_acc:0.933]
Epoch [38/120    avg_loss:0.144, val_acc:0.930]
Epoch [39/120    avg_loss:0.119, val_acc:0.945]
Epoch [40/120    avg_loss:0.132, val_acc:0.948]
Epoch [41/120    avg_loss:0.174, val_acc:0.924]
Epoch [42/120    avg_loss:0.131, val_acc:0.948]
Epoch [43/120    avg_loss:0.123, val_acc:0.968]
Epoch [44/120    avg_loss:0.107, val_acc:0.956]
Epoch [45/120    avg_loss:0.093, val_acc:0.926]
Epoch [46/120    avg_loss:0.106, val_acc:0.948]
Epoch [47/120    avg_loss:0.105, val_acc:0.950]
Epoch [48/120    avg_loss:0.101, val_acc:0.941]
Epoch [49/120    avg_loss:0.110, val_acc:0.965]
Epoch [50/120    avg_loss:0.074, val_acc:0.967]
Epoch [51/120    avg_loss:0.076, val_acc:0.959]
Epoch [52/120    avg_loss:0.068, val_acc:0.967]
Epoch [53/120    avg_loss:0.055, val_acc:0.965]
Epoch [54/120    avg_loss:0.070, val_acc:0.971]
Epoch [55/120    avg_loss:0.051, val_acc:0.971]
Epoch [56/120    avg_loss:0.067, val_acc:0.953]
Epoch [57/120    avg_loss:0.084, val_acc:0.968]
Epoch [58/120    avg_loss:0.062, val_acc:0.948]
Epoch [59/120    avg_loss:0.061, val_acc:0.964]
Epoch [60/120    avg_loss:0.043, val_acc:0.963]
Epoch [61/120    avg_loss:0.038, val_acc:0.965]
Epoch [62/120    avg_loss:0.045, val_acc:0.968]
Epoch [63/120    avg_loss:0.069, val_acc:0.958]
Epoch [64/120    avg_loss:0.077, val_acc:0.971]
Epoch [65/120    avg_loss:0.055, val_acc:0.978]
Epoch [66/120    avg_loss:0.041, val_acc:0.958]
Epoch [67/120    avg_loss:0.049, val_acc:0.944]
Epoch [68/120    avg_loss:0.046, val_acc:0.973]
Epoch [69/120    avg_loss:0.033, val_acc:0.972]
Epoch [70/120    avg_loss:0.036, val_acc:0.973]
Epoch [71/120    avg_loss:0.043, val_acc:0.957]
Epoch [72/120    avg_loss:0.074, val_acc:0.928]
Epoch [73/120    avg_loss:0.103, val_acc:0.919]
Epoch [74/120    avg_loss:0.056, val_acc:0.969]
Epoch [75/120    avg_loss:0.054, val_acc:0.962]
Epoch [76/120    avg_loss:0.071, val_acc:0.974]
Epoch [77/120    avg_loss:0.048, val_acc:0.979]
Epoch [78/120    avg_loss:0.034, val_acc:0.973]
Epoch [79/120    avg_loss:0.032, val_acc:0.975]
Epoch [80/120    avg_loss:0.033, val_acc:0.976]
Epoch [81/120    avg_loss:0.025, val_acc:0.974]
Epoch [82/120    avg_loss:0.024, val_acc:0.972]
Epoch [83/120    avg_loss:0.028, val_acc:0.969]
Epoch [84/120    avg_loss:0.026, val_acc:0.968]
Epoch [85/120    avg_loss:0.024, val_acc:0.981]
Epoch [86/120    avg_loss:0.022, val_acc:0.979]
Epoch [87/120    avg_loss:0.019, val_acc:0.980]
Epoch [88/120    avg_loss:0.015, val_acc:0.976]
Epoch [89/120    avg_loss:0.018, val_acc:0.980]
Epoch [90/120    avg_loss:0.026, val_acc:0.979]
Epoch [91/120    avg_loss:0.023, val_acc:0.978]
Epoch [92/120    avg_loss:0.027, val_acc:0.972]
Epoch [93/120    avg_loss:0.028, val_acc:0.976]
Epoch [94/120    avg_loss:0.018, val_acc:0.981]
Epoch [95/120    avg_loss:0.017, val_acc:0.981]
Epoch [96/120    avg_loss:0.021, val_acc:0.982]
Epoch [97/120    avg_loss:0.014, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.021, val_acc:0.976]
Epoch [100/120    avg_loss:0.025, val_acc:0.978]
Epoch [101/120    avg_loss:0.030, val_acc:0.982]
Epoch [102/120    avg_loss:0.016, val_acc:0.973]
Epoch [103/120    avg_loss:0.031, val_acc:0.981]
Epoch [104/120    avg_loss:0.015, val_acc:0.979]
Epoch [105/120    avg_loss:0.015, val_acc:0.978]
Epoch [106/120    avg_loss:0.017, val_acc:0.975]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.020, val_acc:0.978]
Epoch [111/120    avg_loss:0.015, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.973]
Epoch [113/120    avg_loss:0.018, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.970]
Epoch [115/120    avg_loss:0.012, val_acc:0.979]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.007, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.010, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     0     0     1    14     2     3]
 [    0     0 18020     0    33     0    36     0     1     0]
 [    0     2     0  1910     0     0     0     0   124     0]
 [    0    25     5     0  2922     0    19     0     0     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    46     0    15     0  4817     0     0     0]
 [    0    14     0     0     0     0     0  1276     0     0]
 [    0    14     2    59    37     0     1     0  3458     0]
 [    0     0     0     0     0    19     0     0     0   900]]

Accuracy:
98.86004868291037

F1 scores:
[       nan 0.9941856  0.99659873 0.95380774 0.97742097 0.99277292
 0.98789992 0.98914729 0.96646171 0.98738343]

Kappa:
0.9848958086806819
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa378152b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.204, val_acc:0.136]
Epoch [2/120    avg_loss:1.979, val_acc:0.242]
Epoch [3/120    avg_loss:1.785, val_acc:0.235]
Epoch [4/120    avg_loss:1.610, val_acc:0.232]
Epoch [5/120    avg_loss:1.483, val_acc:0.257]
Epoch [6/120    avg_loss:1.384, val_acc:0.285]
Epoch [7/120    avg_loss:1.312, val_acc:0.324]
Epoch [8/120    avg_loss:1.220, val_acc:0.362]
Epoch [9/120    avg_loss:1.112, val_acc:0.402]
Epoch [10/120    avg_loss:1.040, val_acc:0.438]
Epoch [11/120    avg_loss:0.950, val_acc:0.468]
Epoch [12/120    avg_loss:0.897, val_acc:0.481]
Epoch [13/120    avg_loss:0.810, val_acc:0.582]
Epoch [14/120    avg_loss:0.777, val_acc:0.583]
Epoch [15/120    avg_loss:0.685, val_acc:0.608]
Epoch [16/120    avg_loss:0.688, val_acc:0.592]
Epoch [17/120    avg_loss:0.615, val_acc:0.655]
Epoch [18/120    avg_loss:0.588, val_acc:0.644]
Epoch [19/120    avg_loss:0.527, val_acc:0.613]
Epoch [20/120    avg_loss:0.474, val_acc:0.645]
Epoch [21/120    avg_loss:0.415, val_acc:0.710]
Epoch [22/120    avg_loss:0.494, val_acc:0.718]
Epoch [23/120    avg_loss:0.498, val_acc:0.716]
Epoch [24/120    avg_loss:0.499, val_acc:0.682]
Epoch [25/120    avg_loss:0.378, val_acc:0.687]
Epoch [26/120    avg_loss:0.348, val_acc:0.747]
Epoch [27/120    avg_loss:0.359, val_acc:0.741]
Epoch [28/120    avg_loss:0.307, val_acc:0.764]
Epoch [29/120    avg_loss:0.308, val_acc:0.797]
Epoch [30/120    avg_loss:0.272, val_acc:0.795]
Epoch [31/120    avg_loss:0.258, val_acc:0.801]
Epoch [32/120    avg_loss:0.251, val_acc:0.828]
Epoch [33/120    avg_loss:0.233, val_acc:0.836]
Epoch [34/120    avg_loss:0.210, val_acc:0.848]
Epoch [35/120    avg_loss:0.204, val_acc:0.855]
Epoch [36/120    avg_loss:0.186, val_acc:0.868]
Epoch [37/120    avg_loss:0.171, val_acc:0.923]
Epoch [38/120    avg_loss:0.150, val_acc:0.936]
Epoch [39/120    avg_loss:0.148, val_acc:0.918]
Epoch [40/120    avg_loss:0.364, val_acc:0.419]
Epoch [41/120    avg_loss:0.506, val_acc:0.836]
Epoch [42/120    avg_loss:0.239, val_acc:0.820]
Epoch [43/120    avg_loss:0.216, val_acc:0.918]
Epoch [44/120    avg_loss:0.158, val_acc:0.910]
Epoch [45/120    avg_loss:0.138, val_acc:0.912]
Epoch [46/120    avg_loss:0.358, val_acc:0.901]
Epoch [47/120    avg_loss:0.185, val_acc:0.927]
Epoch [48/120    avg_loss:0.162, val_acc:0.887]
Epoch [49/120    avg_loss:1.144, val_acc:0.338]
Epoch [50/120    avg_loss:0.915, val_acc:0.446]
Epoch [51/120    avg_loss:0.773, val_acc:0.480]
Epoch [52/120    avg_loss:0.670, val_acc:0.477]
Epoch [53/120    avg_loss:0.627, val_acc:0.490]
Epoch [54/120    avg_loss:0.602, val_acc:0.485]
Epoch [55/120    avg_loss:0.575, val_acc:0.487]
Epoch [56/120    avg_loss:0.606, val_acc:0.492]
Epoch [57/120    avg_loss:0.565, val_acc:0.494]
Epoch [58/120    avg_loss:0.559, val_acc:0.506]
Epoch [59/120    avg_loss:0.552, val_acc:0.507]
Epoch [60/120    avg_loss:0.556, val_acc:0.511]
Epoch [61/120    avg_loss:0.639, val_acc:0.623]
Epoch [62/120    avg_loss:0.562, val_acc:0.519]
Epoch [63/120    avg_loss:0.522, val_acc:0.516]
Epoch [64/120    avg_loss:0.530, val_acc:0.525]
Epoch [65/120    avg_loss:0.530, val_acc:0.524]
Epoch [66/120    avg_loss:0.527, val_acc:0.520]
Epoch [67/120    avg_loss:0.512, val_acc:0.522]
Epoch [68/120    avg_loss:0.534, val_acc:0.519]
Epoch [69/120    avg_loss:0.514, val_acc:0.522]
Epoch [70/120    avg_loss:0.520, val_acc:0.526]
Epoch [71/120    avg_loss:0.505, val_acc:0.526]
Epoch [72/120    avg_loss:0.513, val_acc:0.526]
Epoch [73/120    avg_loss:0.502, val_acc:0.522]
Epoch [74/120    avg_loss:0.514, val_acc:0.525]
Epoch [75/120    avg_loss:0.498, val_acc:0.525]
Epoch [76/120    avg_loss:0.492, val_acc:0.527]
Epoch [77/120    avg_loss:0.495, val_acc:0.527]
Epoch [78/120    avg_loss:0.493, val_acc:0.527]
Epoch [79/120    avg_loss:0.495, val_acc:0.527]
Epoch [80/120    avg_loss:0.536, val_acc:0.526]
Epoch [81/120    avg_loss:0.491, val_acc:0.525]
Epoch [82/120    avg_loss:0.501, val_acc:0.526]
Epoch [83/120    avg_loss:0.501, val_acc:0.527]
Epoch [84/120    avg_loss:0.506, val_acc:0.525]
Epoch [85/120    avg_loss:0.514, val_acc:0.525]
Epoch [86/120    avg_loss:0.502, val_acc:0.526]
Epoch [87/120    avg_loss:0.499, val_acc:0.527]
Epoch [88/120    avg_loss:0.494, val_acc:0.527]
Epoch [89/120    avg_loss:0.493, val_acc:0.528]
Epoch [90/120    avg_loss:0.505, val_acc:0.528]
Epoch [91/120    avg_loss:0.480, val_acc:0.528]
Epoch [92/120    avg_loss:0.498, val_acc:0.528]
Epoch [93/120    avg_loss:0.494, val_acc:0.528]
Epoch [94/120    avg_loss:0.508, val_acc:0.528]
Epoch [95/120    avg_loss:0.493, val_acc:0.527]
Epoch [96/120    avg_loss:0.515, val_acc:0.528]
Epoch [97/120    avg_loss:0.496, val_acc:0.528]
Epoch [98/120    avg_loss:0.503, val_acc:0.527]
Epoch [99/120    avg_loss:0.491, val_acc:0.527]
Epoch [100/120    avg_loss:0.522, val_acc:0.527]
Epoch [101/120    avg_loss:0.498, val_acc:0.527]
Epoch [102/120    avg_loss:0.493, val_acc:0.527]
Epoch [103/120    avg_loss:0.503, val_acc:0.527]
Epoch [104/120    avg_loss:0.524, val_acc:0.527]
Epoch [105/120    avg_loss:0.501, val_acc:0.527]
Epoch [106/120    avg_loss:0.504, val_acc:0.527]
Epoch [107/120    avg_loss:0.496, val_acc:0.527]
Epoch [108/120    avg_loss:0.493, val_acc:0.527]
Epoch [109/120    avg_loss:0.495, val_acc:0.527]
Epoch [110/120    avg_loss:0.518, val_acc:0.527]
Epoch [111/120    avg_loss:0.496, val_acc:0.527]
Epoch [112/120    avg_loss:0.506, val_acc:0.527]
Epoch [113/120    avg_loss:0.499, val_acc:0.527]
Epoch [114/120    avg_loss:0.490, val_acc:0.527]
Epoch [115/120    avg_loss:0.493, val_acc:0.527]
Epoch [116/120    avg_loss:0.490, val_acc:0.527]
Epoch [117/120    avg_loss:0.507, val_acc:0.527]
Epoch [118/120    avg_loss:0.516, val_acc:0.527]
Epoch [119/120    avg_loss:0.500, val_acc:0.527]
Epoch [120/120    avg_loss:0.505, val_acc:0.527]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5251    22   251    67     0   112   158   265   306]
 [    0     0  2315     0  2757     0 13018     0     0     0]
 [    0    30     0  1722     0     0     0     0   282     2]
 [    0    14    89     0  2514     0   335     0     0    20]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     8    10     0   635     0  4188    11    26     0]
 [    0    44     0     0     2     0     0  1240     3     1]
 [    0   272    13   174     1     0   154    11  2945     1]
 [    0    47     2     0     1    32     0     0     0   837]]

Accuracy:
53.784975779047066

F1 scores:
[       nan 0.86807737 0.22540285 0.82333254 0.56185049 0.98788796
 0.36923077 0.91512915 0.83051325 0.80249281]

Kappa:
0.47468987098281235
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efca64feb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 12410==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.208, val_acc:0.036]
Epoch [2/120    avg_loss:2.025, val_acc:0.093]
Epoch [3/120    avg_loss:1.836, val_acc:0.115]
Epoch [4/120    avg_loss:1.690, val_acc:0.213]
Epoch [5/120    avg_loss:1.531, val_acc:0.292]
Epoch [6/120    avg_loss:1.386, val_acc:0.353]
Epoch [7/120    avg_loss:1.271, val_acc:0.376]
Epoch [8/120    avg_loss:1.175, val_acc:0.453]
Epoch [9/120    avg_loss:1.081, val_acc:0.516]
Epoch [10/120    avg_loss:0.943, val_acc:0.671]
Epoch [11/120    avg_loss:0.856, val_acc:0.713]
Epoch [12/120    avg_loss:0.758, val_acc:0.664]
Epoch [13/120    avg_loss:0.693, val_acc:0.722]
Epoch [14/120    avg_loss:0.642, val_acc:0.771]
Epoch [15/120    avg_loss:0.582, val_acc:0.790]
Epoch [16/120    avg_loss:0.531, val_acc:0.828]
Epoch [17/120    avg_loss:0.478, val_acc:0.787]
Epoch [18/120    avg_loss:0.425, val_acc:0.807]
Epoch [19/120    avg_loss:0.394, val_acc:0.863]
Epoch [20/120    avg_loss:0.354, val_acc:0.808]
Epoch [21/120    avg_loss:0.338, val_acc:0.776]
Epoch [22/120    avg_loss:0.346, val_acc:0.832]
Epoch [23/120    avg_loss:0.330, val_acc:0.847]
Epoch [24/120    avg_loss:0.297, val_acc:0.827]
Epoch [25/120    avg_loss:0.267, val_acc:0.879]
Epoch [26/120    avg_loss:0.249, val_acc:0.899]
Epoch [27/120    avg_loss:0.223, val_acc:0.902]
Epoch [28/120    avg_loss:0.207, val_acc:0.916]
Epoch [29/120    avg_loss:0.195, val_acc:0.908]
Epoch [30/120    avg_loss:0.228, val_acc:0.935]
Epoch [31/120    avg_loss:0.210, val_acc:0.870]
Epoch [32/120    avg_loss:0.192, val_acc:0.939]
Epoch [33/120    avg_loss:0.174, val_acc:0.922]
Epoch [34/120    avg_loss:0.207, val_acc:0.948]
Epoch [35/120    avg_loss:0.139, val_acc:0.936]
Epoch [36/120    avg_loss:0.121, val_acc:0.943]
Epoch [37/120    avg_loss:0.121, val_acc:0.958]
Epoch [38/120    avg_loss:0.124, val_acc:0.958]
Epoch [39/120    avg_loss:0.135, val_acc:0.955]
Epoch [40/120    avg_loss:0.099, val_acc:0.958]
Epoch [41/120    avg_loss:0.095, val_acc:0.955]
Epoch [42/120    avg_loss:0.108, val_acc:0.961]
Epoch [43/120    avg_loss:0.089, val_acc:0.961]
Epoch [44/120    avg_loss:0.083, val_acc:0.973]
Epoch [45/120    avg_loss:0.074, val_acc:0.968]
Epoch [46/120    avg_loss:0.089, val_acc:0.963]
Epoch [47/120    avg_loss:0.070, val_acc:0.973]
Epoch [48/120    avg_loss:0.057, val_acc:0.970]
Epoch [49/120    avg_loss:0.062, val_acc:0.974]
Epoch [50/120    avg_loss:0.048, val_acc:0.971]
Epoch [51/120    avg_loss:0.059, val_acc:0.972]
Epoch [52/120    avg_loss:0.075, val_acc:0.978]
Epoch [53/120    avg_loss:0.055, val_acc:0.969]
Epoch [54/120    avg_loss:0.102, val_acc:0.968]
Epoch [55/120    avg_loss:0.066, val_acc:0.977]
Epoch [56/120    avg_loss:0.043, val_acc:0.972]
Epoch [57/120    avg_loss:0.038, val_acc:0.984]
Epoch [58/120    avg_loss:0.032, val_acc:0.973]
Epoch [59/120    avg_loss:0.035, val_acc:0.981]
Epoch [60/120    avg_loss:0.031, val_acc:0.962]
Epoch [61/120    avg_loss:0.028, val_acc:0.978]
Epoch [62/120    avg_loss:0.026, val_acc:0.979]
Epoch [63/120    avg_loss:0.029, val_acc:0.986]
Epoch [64/120    avg_loss:0.022, val_acc:0.984]
Epoch [65/120    avg_loss:0.019, val_acc:0.970]
Epoch [66/120    avg_loss:0.033, val_acc:0.980]
Epoch [67/120    avg_loss:0.031, val_acc:0.983]
Epoch [68/120    avg_loss:0.025, val_acc:0.984]
Epoch [69/120    avg_loss:0.019, val_acc:0.986]
Epoch [70/120    avg_loss:0.020, val_acc:0.979]
Epoch [71/120    avg_loss:0.033, val_acc:0.982]
Epoch [72/120    avg_loss:0.021, val_acc:0.980]
Epoch [73/120    avg_loss:0.055, val_acc:0.972]
Epoch [74/120    avg_loss:0.048, val_acc:0.950]
Epoch [75/120    avg_loss:0.065, val_acc:0.978]
Epoch [76/120    avg_loss:0.034, val_acc:0.982]
Epoch [77/120    avg_loss:0.024, val_acc:0.983]
Epoch [78/120    avg_loss:0.023, val_acc:0.984]
Epoch [79/120    avg_loss:0.018, val_acc:0.980]
Epoch [80/120    avg_loss:0.016, val_acc:0.982]
Epoch [81/120    avg_loss:0.015, val_acc:0.977]
Epoch [82/120    avg_loss:0.018, val_acc:0.980]
Epoch [83/120    avg_loss:0.016, val_acc:0.983]
Epoch [84/120    avg_loss:0.016, val_acc:0.987]
Epoch [85/120    avg_loss:0.013, val_acc:0.985]
Epoch [86/120    avg_loss:0.013, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.011, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.012, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.987]
Epoch [105/120    avg_loss:0.012, val_acc:0.987]
Epoch [106/120    avg_loss:0.010, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.987]
Epoch [109/120    avg_loss:0.010, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.010, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.011, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     0     0     0     0     0    40    18]
 [    0     0 18055     0    20     0    14     0     1     0]
 [    0     2     0  1929     0     0     0     0   105     0]
 [    0    18    11     0  2938     0     5     0     0     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4863     0     0     0]
 [    0     2     0     0     0     0     0  1287     1     0]
 [    0    18     0    48    50     0     1     0  3454     0]
 [    0     2     0     0     4     6     0     0     0   907]]

Accuracy:
99.0817728291519

F1 scores:
[       nan 0.99221669 0.99831357 0.96137553 0.98195187 0.99770642
 0.9964143  0.99883586 0.96319018 0.98373102]

Kappa:
0.9878341148545845
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb0a0522b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.245, val_acc:0.083]
Epoch [2/120    avg_loss:2.070, val_acc:0.086]
Epoch [3/120    avg_loss:1.902, val_acc:0.211]
Epoch [4/120    avg_loss:1.758, val_acc:0.236]
Epoch [5/120    avg_loss:1.610, val_acc:0.273]
Epoch [6/120    avg_loss:1.480, val_acc:0.283]
Epoch [7/120    avg_loss:1.365, val_acc:0.318]
Epoch [8/120    avg_loss:1.289, val_acc:0.357]
Epoch [9/120    avg_loss:1.194, val_acc:0.423]
Epoch [10/120    avg_loss:1.094, val_acc:0.452]
Epoch [11/120    avg_loss:1.014, val_acc:0.470]
Epoch [12/120    avg_loss:0.918, val_acc:0.521]
Epoch [13/120    avg_loss:0.870, val_acc:0.610]
Epoch [14/120    avg_loss:0.807, val_acc:0.602]
Epoch [15/120    avg_loss:0.739, val_acc:0.639]
Epoch [16/120    avg_loss:0.669, val_acc:0.744]
Epoch [17/120    avg_loss:0.590, val_acc:0.748]
Epoch [18/120    avg_loss:0.531, val_acc:0.756]
Epoch [19/120    avg_loss:0.475, val_acc:0.843]
Epoch [20/120    avg_loss:0.426, val_acc:0.778]
Epoch [21/120    avg_loss:0.392, val_acc:0.875]
Epoch [22/120    avg_loss:0.383, val_acc:0.902]
Epoch [23/120    avg_loss:0.325, val_acc:0.923]
Epoch [24/120    avg_loss:0.281, val_acc:0.927]
Epoch [25/120    avg_loss:0.246, val_acc:0.934]
Epoch [26/120    avg_loss:0.225, val_acc:0.946]
Epoch [27/120    avg_loss:0.195, val_acc:0.955]
Epoch [28/120    avg_loss:0.220, val_acc:0.943]
Epoch [29/120    avg_loss:0.216, val_acc:0.901]
Epoch [30/120    avg_loss:0.175, val_acc:0.952]
Epoch [31/120    avg_loss:0.147, val_acc:0.953]
Epoch [32/120    avg_loss:0.167, val_acc:0.962]
Epoch [33/120    avg_loss:0.135, val_acc:0.943]
Epoch [34/120    avg_loss:0.141, val_acc:0.955]
Epoch [35/120    avg_loss:0.113, val_acc:0.941]
Epoch [36/120    avg_loss:0.113, val_acc:0.964]
Epoch [37/120    avg_loss:0.095, val_acc:0.973]
Epoch [38/120    avg_loss:0.099, val_acc:0.976]
Epoch [39/120    avg_loss:0.095, val_acc:0.979]
Epoch [40/120    avg_loss:0.089, val_acc:0.968]
Epoch [41/120    avg_loss:0.075, val_acc:0.968]
Epoch [42/120    avg_loss:0.066, val_acc:0.963]
Epoch [43/120    avg_loss:0.058, val_acc:0.976]
Epoch [44/120    avg_loss:0.069, val_acc:0.974]
Epoch [45/120    avg_loss:0.071, val_acc:0.978]
Epoch [46/120    avg_loss:0.053, val_acc:0.978]
Epoch [47/120    avg_loss:0.061, val_acc:0.970]
Epoch [48/120    avg_loss:0.056, val_acc:0.981]
Epoch [49/120    avg_loss:0.046, val_acc:0.980]
Epoch [50/120    avg_loss:0.035, val_acc:0.975]
Epoch [51/120    avg_loss:0.041, val_acc:0.984]
Epoch [52/120    avg_loss:0.065, val_acc:0.970]
Epoch [53/120    avg_loss:0.051, val_acc:0.979]
Epoch [54/120    avg_loss:0.041, val_acc:0.977]
Epoch [55/120    avg_loss:0.045, val_acc:0.980]
Epoch [56/120    avg_loss:0.043, val_acc:0.984]
Epoch [57/120    avg_loss:0.038, val_acc:0.986]
Epoch [58/120    avg_loss:0.038, val_acc:0.987]
Epoch [59/120    avg_loss:0.041, val_acc:0.980]
Epoch [60/120    avg_loss:0.056, val_acc:0.970]
Epoch [61/120    avg_loss:0.035, val_acc:0.971]
Epoch [62/120    avg_loss:0.049, val_acc:0.942]
Epoch [63/120    avg_loss:0.038, val_acc:0.986]
Epoch [64/120    avg_loss:0.029, val_acc:0.975]
Epoch [65/120    avg_loss:0.028, val_acc:0.988]
Epoch [66/120    avg_loss:0.025, val_acc:0.985]
Epoch [67/120    avg_loss:0.021, val_acc:0.983]
Epoch [68/120    avg_loss:0.030, val_acc:0.967]
Epoch [69/120    avg_loss:0.028, val_acc:0.988]
Epoch [70/120    avg_loss:0.026, val_acc:0.955]
Epoch [71/120    avg_loss:0.053, val_acc:0.969]
Epoch [72/120    avg_loss:0.050, val_acc:0.982]
Epoch [73/120    avg_loss:0.022, val_acc:0.982]
Epoch [74/120    avg_loss:0.023, val_acc:0.958]
Epoch [75/120    avg_loss:0.019, val_acc:0.985]
Epoch [76/120    avg_loss:0.017, val_acc:0.981]
Epoch [77/120    avg_loss:0.025, val_acc:0.984]
Epoch [78/120    avg_loss:0.020, val_acc:0.988]
Epoch [79/120    avg_loss:0.015, val_acc:0.983]
Epoch [80/120    avg_loss:0.020, val_acc:0.974]
Epoch [81/120    avg_loss:0.018, val_acc:0.985]
Epoch [82/120    avg_loss:0.014, val_acc:0.988]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.032, val_acc:0.957]
Epoch [85/120    avg_loss:0.039, val_acc:0.983]
Epoch [86/120    avg_loss:0.035, val_acc:0.984]
Epoch [87/120    avg_loss:0.018, val_acc:0.988]
Epoch [88/120    avg_loss:0.020, val_acc:0.991]
Epoch [89/120    avg_loss:0.016, val_acc:0.982]
Epoch [90/120    avg_loss:0.012, val_acc:0.991]
Epoch [91/120    avg_loss:0.013, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.990]
Epoch [93/120    avg_loss:0.011, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.017, val_acc:0.993]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.989]
Epoch [101/120    avg_loss:0.011, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.993]
Epoch [103/120    avg_loss:0.007, val_acc:0.993]
Epoch [104/120    avg_loss:0.008, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.011, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.993]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.006, val_acc:0.993]
Epoch [120/120    avg_loss:0.005, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     0     0     0     0     3    15     9]
 [    0     1 18059     0    24     0     0     0     2     4]
 [    0     7     0  1911     0     0     0     0   117     1]
 [    0    21    10     0  2916     0     9     0    14     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4856     0     3     0]
 [    0    21     0     0     0     0     0  1268     0     1]
 [    0    37     0    21    48     0     0     0  3465     0]
 [    0     0     0     0    16    24     0     0     0   879]]

Accuracy:
98.96609066589545

F1 scores:
[       nan 0.9911792  0.99834153 0.96320565 0.97590361 0.99088838
 0.99681823 0.99023819 0.96424099 0.96859504]

Kappa:
0.9862967435243414
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1c5c764be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.232, val_acc:0.157]
Epoch [2/120    avg_loss:2.009, val_acc:0.186]
Epoch [3/120    avg_loss:1.828, val_acc:0.182]
Epoch [4/120    avg_loss:1.696, val_acc:0.187]
Epoch [5/120    avg_loss:1.556, val_acc:0.250]
Epoch [6/120    avg_loss:1.448, val_acc:0.315]
Epoch [7/120    avg_loss:1.338, val_acc:0.380]
Epoch [8/120    avg_loss:1.252, val_acc:0.508]
Epoch [9/120    avg_loss:1.146, val_acc:0.667]
Epoch [10/120    avg_loss:1.051, val_acc:0.727]
Epoch [11/120    avg_loss:0.913, val_acc:0.738]
Epoch [12/120    avg_loss:0.811, val_acc:0.707]
Epoch [13/120    avg_loss:0.729, val_acc:0.812]
Epoch [14/120    avg_loss:0.641, val_acc:0.786]
Epoch [15/120    avg_loss:0.595, val_acc:0.827]
Epoch [16/120    avg_loss:0.538, val_acc:0.810]
Epoch [17/120    avg_loss:0.478, val_acc:0.843]
Epoch [18/120    avg_loss:0.425, val_acc:0.817]
Epoch [19/120    avg_loss:0.374, val_acc:0.840]
Epoch [20/120    avg_loss:0.365, val_acc:0.868]
Epoch [21/120    avg_loss:0.322, val_acc:0.831]
Epoch [22/120    avg_loss:0.300, val_acc:0.872]
Epoch [23/120    avg_loss:0.278, val_acc:0.907]
Epoch [24/120    avg_loss:0.247, val_acc:0.897]
Epoch [25/120    avg_loss:0.252, val_acc:0.918]
Epoch [26/120    avg_loss:0.284, val_acc:0.893]
Epoch [27/120    avg_loss:0.251, val_acc:0.924]
Epoch [28/120    avg_loss:0.206, val_acc:0.905]
Epoch [29/120    avg_loss:0.177, val_acc:0.954]
Epoch [30/120    avg_loss:0.162, val_acc:0.953]
Epoch [31/120    avg_loss:0.133, val_acc:0.962]
Epoch [32/120    avg_loss:0.121, val_acc:0.963]
Epoch [33/120    avg_loss:0.116, val_acc:0.966]
Epoch [34/120    avg_loss:0.132, val_acc:0.932]
Epoch [35/120    avg_loss:0.109, val_acc:0.931]
Epoch [36/120    avg_loss:0.152, val_acc:0.944]
Epoch [37/120    avg_loss:0.109, val_acc:0.958]
Epoch [38/120    avg_loss:0.104, val_acc:0.959]
Epoch [39/120    avg_loss:0.081, val_acc:0.958]
Epoch [40/120    avg_loss:0.072, val_acc:0.965]
Epoch [41/120    avg_loss:0.073, val_acc:0.952]
Epoch [42/120    avg_loss:0.092, val_acc:0.966]
Epoch [43/120    avg_loss:0.072, val_acc:0.959]
Epoch [44/120    avg_loss:0.068, val_acc:0.970]
Epoch [45/120    avg_loss:0.055, val_acc:0.973]
Epoch [46/120    avg_loss:0.063, val_acc:0.968]
Epoch [47/120    avg_loss:0.067, val_acc:0.974]
Epoch [48/120    avg_loss:0.044, val_acc:0.980]
Epoch [49/120    avg_loss:0.058, val_acc:0.976]
Epoch [50/120    avg_loss:0.055, val_acc:0.967]
Epoch [51/120    avg_loss:0.053, val_acc:0.976]
Epoch [52/120    avg_loss:0.042, val_acc:0.974]
Epoch [53/120    avg_loss:0.078, val_acc:0.322]
Epoch [54/120    avg_loss:0.506, val_acc:0.875]
Epoch [55/120    avg_loss:0.262, val_acc:0.930]
Epoch [56/120    avg_loss:0.157, val_acc:0.951]
Epoch [57/120    avg_loss:0.151, val_acc:0.947]
Epoch [58/120    avg_loss:0.141, val_acc:0.960]
Epoch [59/120    avg_loss:0.108, val_acc:0.968]
Epoch [60/120    avg_loss:0.069, val_acc:0.970]
Epoch [61/120    avg_loss:0.053, val_acc:0.963]
Epoch [62/120    avg_loss:0.046, val_acc:0.979]
Epoch [63/120    avg_loss:0.041, val_acc:0.977]
Epoch [64/120    avg_loss:0.043, val_acc:0.979]
Epoch [65/120    avg_loss:0.037, val_acc:0.978]
Epoch [66/120    avg_loss:0.037, val_acc:0.979]
Epoch [67/120    avg_loss:0.039, val_acc:0.978]
Epoch [68/120    avg_loss:0.036, val_acc:0.976]
Epoch [69/120    avg_loss:0.031, val_acc:0.980]
Epoch [70/120    avg_loss:0.041, val_acc:0.979]
Epoch [71/120    avg_loss:0.032, val_acc:0.981]
Epoch [72/120    avg_loss:0.033, val_acc:0.978]
Epoch [73/120    avg_loss:0.032, val_acc:0.980]
Epoch [74/120    avg_loss:0.033, val_acc:0.984]
Epoch [75/120    avg_loss:0.031, val_acc:0.985]
Epoch [76/120    avg_loss:0.030, val_acc:0.985]
Epoch [77/120    avg_loss:0.033, val_acc:0.984]
Epoch [78/120    avg_loss:0.031, val_acc:0.983]
Epoch [79/120    avg_loss:0.033, val_acc:0.984]
Epoch [80/120    avg_loss:0.033, val_acc:0.982]
Epoch [81/120    avg_loss:0.034, val_acc:0.984]
Epoch [82/120    avg_loss:0.030, val_acc:0.983]
Epoch [83/120    avg_loss:0.030, val_acc:0.984]
Epoch [84/120    avg_loss:0.034, val_acc:0.983]
Epoch [85/120    avg_loss:0.029, val_acc:0.983]
Epoch [86/120    avg_loss:0.028, val_acc:0.984]
Epoch [87/120    avg_loss:0.029, val_acc:0.983]
Epoch [88/120    avg_loss:0.029, val_acc:0.983]
Epoch [89/120    avg_loss:0.030, val_acc:0.984]
Epoch [90/120    avg_loss:0.028, val_acc:0.984]
Epoch [91/120    avg_loss:0.026, val_acc:0.984]
Epoch [92/120    avg_loss:0.032, val_acc:0.984]
Epoch [93/120    avg_loss:0.025, val_acc:0.984]
Epoch [94/120    avg_loss:0.024, val_acc:0.984]
Epoch [95/120    avg_loss:0.025, val_acc:0.984]
Epoch [96/120    avg_loss:0.028, val_acc:0.983]
Epoch [97/120    avg_loss:0.026, val_acc:0.983]
Epoch [98/120    avg_loss:0.027, val_acc:0.984]
Epoch [99/120    avg_loss:0.028, val_acc:0.984]
Epoch [100/120    avg_loss:0.025, val_acc:0.984]
Epoch [101/120    avg_loss:0.024, val_acc:0.985]
Epoch [102/120    avg_loss:0.027, val_acc:0.985]
Epoch [103/120    avg_loss:0.028, val_acc:0.986]
Epoch [104/120    avg_loss:0.026, val_acc:0.985]
Epoch [105/120    avg_loss:0.030, val_acc:0.984]
Epoch [106/120    avg_loss:0.026, val_acc:0.984]
Epoch [107/120    avg_loss:0.028, val_acc:0.985]
Epoch [108/120    avg_loss:0.025, val_acc:0.984]
Epoch [109/120    avg_loss:0.027, val_acc:0.984]
Epoch [110/120    avg_loss:0.028, val_acc:0.985]
Epoch [111/120    avg_loss:0.024, val_acc:0.985]
Epoch [112/120    avg_loss:0.026, val_acc:0.985]
Epoch [113/120    avg_loss:0.028, val_acc:0.984]
Epoch [114/120    avg_loss:0.026, val_acc:0.983]
Epoch [115/120    avg_loss:0.026, val_acc:0.984]
Epoch [116/120    avg_loss:0.023, val_acc:0.984]
Epoch [117/120    avg_loss:0.027, val_acc:0.984]
Epoch [118/120    avg_loss:0.028, val_acc:0.984]
Epoch [119/120    avg_loss:0.026, val_acc:0.985]
Epoch [120/120    avg_loss:0.023, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     2     0     0    14    47     0]
 [    0     3 17876     0    56     0   153     0     2     0]
 [    0     5     0  1942     0     0     0     0    86     3]
 [    0    35     7     0  2915     0     6     0     7     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4868     0     7     0]
 [    0    21     0     0     0     0     0  1269     0     0]
 [    0    13     0    80    55     0     0     0  3414     9]
 [    0     0     0     0    10    14     0     0     0   895]]

Accuracy:
98.45757115658063

F1 scores:
[       nan 0.98912875 0.99377363 0.95712173 0.97004992 0.99466463
 0.98293791 0.9863972  0.95710681 0.97921225]

Kappa:
0.9796048071020644
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47cc5fec18>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.132, val_acc:0.109]
Epoch [2/120    avg_loss:1.906, val_acc:0.100]
Epoch [3/120    avg_loss:1.729, val_acc:0.123]
Epoch [4/120    avg_loss:1.610, val_acc:0.128]
Epoch [5/120    avg_loss:1.478, val_acc:0.227]
Epoch [6/120    avg_loss:1.402, val_acc:0.251]
Epoch [7/120    avg_loss:1.314, val_acc:0.283]
Epoch [8/120    avg_loss:1.238, val_acc:0.302]
Epoch [9/120    avg_loss:1.167, val_acc:0.360]
Epoch [10/120    avg_loss:1.076, val_acc:0.400]
Epoch [11/120    avg_loss:0.999, val_acc:0.411]
Epoch [12/120    avg_loss:0.967, val_acc:0.434]
Epoch [13/120    avg_loss:0.872, val_acc:0.438]
Epoch [14/120    avg_loss:0.796, val_acc:0.443]
Epoch [15/120    avg_loss:0.748, val_acc:0.458]
Epoch [16/120    avg_loss:0.693, val_acc:0.501]
Epoch [17/120    avg_loss:0.665, val_acc:0.524]
Epoch [18/120    avg_loss:0.617, val_acc:0.605]
Epoch [19/120    avg_loss:0.582, val_acc:0.507]
Epoch [20/120    avg_loss:0.575, val_acc:0.723]
Epoch [21/120    avg_loss:0.522, val_acc:0.785]
Epoch [22/120    avg_loss:0.467, val_acc:0.804]
Epoch [23/120    avg_loss:0.426, val_acc:0.835]
Epoch [24/120    avg_loss:0.406, val_acc:0.797]
Epoch [25/120    avg_loss:0.384, val_acc:0.782]
Epoch [26/120    avg_loss:0.344, val_acc:0.814]
Epoch [27/120    avg_loss:0.334, val_acc:0.828]
Epoch [28/120    avg_loss:0.298, val_acc:0.818]
Epoch [29/120    avg_loss:0.298, val_acc:0.812]
Epoch [30/120    avg_loss:0.304, val_acc:0.827]
Epoch [31/120    avg_loss:0.271, val_acc:0.825]
Epoch [32/120    avg_loss:0.271, val_acc:0.828]
Epoch [33/120    avg_loss:0.277, val_acc:0.834]
Epoch [34/120    avg_loss:0.258, val_acc:0.794]
Epoch [35/120    avg_loss:0.269, val_acc:0.838]
Epoch [36/120    avg_loss:0.252, val_acc:0.835]
Epoch [37/120    avg_loss:0.228, val_acc:0.832]
Epoch [38/120    avg_loss:0.222, val_acc:0.837]
Epoch [39/120    avg_loss:0.204, val_acc:0.828]
Epoch [40/120    avg_loss:0.198, val_acc:0.836]
Epoch [41/120    avg_loss:0.188, val_acc:0.850]
Epoch [42/120    avg_loss:0.186, val_acc:0.850]
Epoch [43/120    avg_loss:0.188, val_acc:0.837]
Epoch [44/120    avg_loss:0.192, val_acc:0.846]
Epoch [45/120    avg_loss:0.168, val_acc:0.868]
Epoch [46/120    avg_loss:0.171, val_acc:0.862]
Epoch [47/120    avg_loss:0.150, val_acc:0.883]
Epoch [48/120    avg_loss:0.135, val_acc:0.932]
Epoch [49/120    avg_loss:0.137, val_acc:0.934]
Epoch [50/120    avg_loss:0.127, val_acc:0.935]
Epoch [51/120    avg_loss:0.110, val_acc:0.947]
Epoch [52/120    avg_loss:0.121, val_acc:0.938]
Epoch [53/120    avg_loss:0.126, val_acc:0.959]
Epoch [54/120    avg_loss:0.088, val_acc:0.960]
Epoch [55/120    avg_loss:0.087, val_acc:0.963]
Epoch [56/120    avg_loss:0.084, val_acc:0.962]
Epoch [57/120    avg_loss:0.108, val_acc:0.936]
Epoch [58/120    avg_loss:0.094, val_acc:0.948]
Epoch [59/120    avg_loss:0.083, val_acc:0.963]
Epoch [60/120    avg_loss:0.056, val_acc:0.968]
Epoch [61/120    avg_loss:0.050, val_acc:0.973]
Epoch [62/120    avg_loss:0.078, val_acc:0.950]
Epoch [63/120    avg_loss:0.099, val_acc:0.968]
Epoch [64/120    avg_loss:0.086, val_acc:0.945]
Epoch [65/120    avg_loss:0.088, val_acc:0.954]
Epoch [66/120    avg_loss:0.099, val_acc:0.929]
Epoch [67/120    avg_loss:0.094, val_acc:0.962]
Epoch [68/120    avg_loss:0.059, val_acc:0.966]
Epoch [69/120    avg_loss:0.053, val_acc:0.963]
Epoch [70/120    avg_loss:0.055, val_acc:0.955]
Epoch [71/120    avg_loss:0.048, val_acc:0.968]
Epoch [72/120    avg_loss:0.042, val_acc:0.969]
Epoch [73/120    avg_loss:0.036, val_acc:0.972]
Epoch [74/120    avg_loss:0.048, val_acc:0.974]
Epoch [75/120    avg_loss:0.029, val_acc:0.969]
Epoch [76/120    avg_loss:0.037, val_acc:0.970]
Epoch [77/120    avg_loss:0.033, val_acc:0.979]
Epoch [78/120    avg_loss:0.029, val_acc:0.973]
Epoch [79/120    avg_loss:0.023, val_acc:0.973]
Epoch [80/120    avg_loss:0.021, val_acc:0.970]
Epoch [81/120    avg_loss:0.023, val_acc:0.975]
Epoch [82/120    avg_loss:0.038, val_acc:0.972]
Epoch [83/120    avg_loss:0.024, val_acc:0.973]
Epoch [84/120    avg_loss:0.022, val_acc:0.971]
Epoch [85/120    avg_loss:0.024, val_acc:0.968]
Epoch [86/120    avg_loss:0.019, val_acc:0.967]
Epoch [87/120    avg_loss:0.020, val_acc:0.976]
Epoch [88/120    avg_loss:0.024, val_acc:0.969]
Epoch [89/120    avg_loss:0.026, val_acc:0.977]
Epoch [90/120    avg_loss:0.018, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.013, val_acc:0.980]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.980]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.017, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.980]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.012, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.010, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.010, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.012, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     1     0     0    10    41     3]
 [    0     2 18050     0    16     0    22     0     0     0]
 [    0     7     0  1922     0     0     0     0   103     4]
 [    0    18    11     0  2938     0     1     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0    16     0     0     0     4     0  1268     2     0]
 [    0    38     0    68    50     0     0     0  3415     0]
 [    0     1     0     1    12    17     0     0     0   888]]

Accuracy:
98.8841491335888

F1 scores:
[       nan 0.98937243 0.99828549 0.95455674 0.98113208 0.99201824
 0.99651925 0.98753894 0.95711883 0.97905182]

Kappa:
0.9852152448209127
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f578fa6fb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.255, val_acc:0.139]
Epoch [2/120    avg_loss:2.042, val_acc:0.147]
Epoch [3/120    avg_loss:1.889, val_acc:0.159]
Epoch [4/120    avg_loss:1.745, val_acc:0.223]
Epoch [5/120    avg_loss:1.597, val_acc:0.254]
Epoch [6/120    avg_loss:1.489, val_acc:0.321]
Epoch [7/120    avg_loss:1.349, val_acc:0.347]
Epoch [8/120    avg_loss:1.259, val_acc:0.383]
Epoch [9/120    avg_loss:1.151, val_acc:0.434]
Epoch [10/120    avg_loss:1.068, val_acc:0.473]
Epoch [11/120    avg_loss:0.957, val_acc:0.505]
Epoch [12/120    avg_loss:0.841, val_acc:0.502]
Epoch [13/120    avg_loss:0.761, val_acc:0.515]
Epoch [14/120    avg_loss:0.706, val_acc:0.546]
Epoch [15/120    avg_loss:0.634, val_acc:0.583]
Epoch [16/120    avg_loss:0.574, val_acc:0.593]
Epoch [17/120    avg_loss:0.513, val_acc:0.613]
Epoch [18/120    avg_loss:0.516, val_acc:0.675]
Epoch [19/120    avg_loss:0.444, val_acc:0.606]
Epoch [20/120    avg_loss:0.422, val_acc:0.716]
Epoch [21/120    avg_loss:0.368, val_acc:0.733]
Epoch [22/120    avg_loss:0.347, val_acc:0.777]
Epoch [23/120    avg_loss:0.326, val_acc:0.746]
Epoch [24/120    avg_loss:0.305, val_acc:0.756]
Epoch [25/120    avg_loss:0.311, val_acc:0.789]
Epoch [26/120    avg_loss:0.293, val_acc:0.779]
Epoch [27/120    avg_loss:0.316, val_acc:0.776]
Epoch [28/120    avg_loss:0.293, val_acc:0.798]
Epoch [29/120    avg_loss:0.231, val_acc:0.801]
Epoch [30/120    avg_loss:0.226, val_acc:0.826]
Epoch [31/120    avg_loss:0.203, val_acc:0.865]
Epoch [32/120    avg_loss:0.187, val_acc:0.890]
Epoch [33/120    avg_loss:0.192, val_acc:0.812]
Epoch [34/120    avg_loss:0.172, val_acc:0.904]
Epoch [35/120    avg_loss:0.164, val_acc:0.896]
Epoch [36/120    avg_loss:0.147, val_acc:0.940]
Epoch [37/120    avg_loss:0.125, val_acc:0.938]
Epoch [38/120    avg_loss:0.124, val_acc:0.953]
Epoch [39/120    avg_loss:0.119, val_acc:0.914]
Epoch [40/120    avg_loss:0.138, val_acc:0.954]
Epoch [41/120    avg_loss:0.095, val_acc:0.922]
Epoch [42/120    avg_loss:0.112, val_acc:0.931]
Epoch [43/120    avg_loss:0.102, val_acc:0.950]
Epoch [44/120    avg_loss:0.081, val_acc:0.960]
Epoch [45/120    avg_loss:0.080, val_acc:0.960]
Epoch [46/120    avg_loss:0.068, val_acc:0.966]
Epoch [47/120    avg_loss:0.068, val_acc:0.969]
Epoch [48/120    avg_loss:0.047, val_acc:0.971]
Epoch [49/120    avg_loss:0.058, val_acc:0.963]
Epoch [50/120    avg_loss:0.051, val_acc:0.968]
Epoch [51/120    avg_loss:0.052, val_acc:0.934]
Epoch [52/120    avg_loss:0.068, val_acc:0.961]
Epoch [53/120    avg_loss:0.061, val_acc:0.967]
Epoch [54/120    avg_loss:0.037, val_acc:0.980]
Epoch [55/120    avg_loss:0.037, val_acc:0.978]
Epoch [56/120    avg_loss:0.146, val_acc:0.922]
Epoch [57/120    avg_loss:0.147, val_acc:0.943]
Epoch [58/120    avg_loss:0.083, val_acc:0.973]
Epoch [59/120    avg_loss:0.060, val_acc:0.966]
Epoch [60/120    avg_loss:0.072, val_acc:0.970]
Epoch [61/120    avg_loss:0.075, val_acc:0.973]
Epoch [62/120    avg_loss:0.048, val_acc:0.976]
Epoch [63/120    avg_loss:0.047, val_acc:0.962]
Epoch [64/120    avg_loss:0.036, val_acc:0.973]
Epoch [65/120    avg_loss:0.041, val_acc:0.970]
Epoch [66/120    avg_loss:0.028, val_acc:0.973]
Epoch [67/120    avg_loss:0.049, val_acc:0.971]
Epoch [68/120    avg_loss:0.039, val_acc:0.975]
Epoch [69/120    avg_loss:0.027, val_acc:0.979]
Epoch [70/120    avg_loss:0.025, val_acc:0.981]
Epoch [71/120    avg_loss:0.030, val_acc:0.980]
Epoch [72/120    avg_loss:0.023, val_acc:0.982]
Epoch [73/120    avg_loss:0.022, val_acc:0.978]
Epoch [74/120    avg_loss:0.023, val_acc:0.977]
Epoch [75/120    avg_loss:0.019, val_acc:0.977]
Epoch [76/120    avg_loss:0.020, val_acc:0.977]
Epoch [77/120    avg_loss:0.021, val_acc:0.977]
Epoch [78/120    avg_loss:0.021, val_acc:0.978]
Epoch [79/120    avg_loss:0.020, val_acc:0.978]
Epoch [80/120    avg_loss:0.022, val_acc:0.977]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.976]
Epoch [83/120    avg_loss:0.020, val_acc:0.977]
Epoch [84/120    avg_loss:0.018, val_acc:0.977]
Epoch [85/120    avg_loss:0.018, val_acc:0.976]
Epoch [86/120    avg_loss:0.016, val_acc:0.976]
Epoch [87/120    avg_loss:0.019, val_acc:0.977]
Epoch [88/120    avg_loss:0.018, val_acc:0.977]
Epoch [89/120    avg_loss:0.018, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.976]
Epoch [91/120    avg_loss:0.016, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.976]
Epoch [93/120    avg_loss:0.019, val_acc:0.977]
Epoch [94/120    avg_loss:0.022, val_acc:0.976]
Epoch [95/120    avg_loss:0.019, val_acc:0.976]
Epoch [96/120    avg_loss:0.018, val_acc:0.976]
Epoch [97/120    avg_loss:0.017, val_acc:0.976]
Epoch [98/120    avg_loss:0.022, val_acc:0.976]
Epoch [99/120    avg_loss:0.020, val_acc:0.976]
Epoch [100/120    avg_loss:0.017, val_acc:0.976]
Epoch [101/120    avg_loss:0.019, val_acc:0.976]
Epoch [102/120    avg_loss:0.020, val_acc:0.976]
Epoch [103/120    avg_loss:0.018, val_acc:0.976]
Epoch [104/120    avg_loss:0.017, val_acc:0.976]
Epoch [105/120    avg_loss:0.017, val_acc:0.976]
Epoch [106/120    avg_loss:0.019, val_acc:0.976]
Epoch [107/120    avg_loss:0.021, val_acc:0.976]
Epoch [108/120    avg_loss:0.016, val_acc:0.976]
Epoch [109/120    avg_loss:0.018, val_acc:0.976]
Epoch [110/120    avg_loss:0.023, val_acc:0.976]
Epoch [111/120    avg_loss:0.019, val_acc:0.976]
Epoch [112/120    avg_loss:0.017, val_acc:0.976]
Epoch [113/120    avg_loss:0.016, val_acc:0.976]
Epoch [114/120    avg_loss:0.018, val_acc:0.976]
Epoch [115/120    avg_loss:0.019, val_acc:0.976]
Epoch [116/120    avg_loss:0.018, val_acc:0.976]
Epoch [117/120    avg_loss:0.019, val_acc:0.976]
Epoch [118/120    avg_loss:0.018, val_acc:0.976]
Epoch [119/120    avg_loss:0.015, val_acc:0.976]
Epoch [120/120    avg_loss:0.020, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6308     0     0     2     0     0    18    92    12]
 [    0     0 18034     0    28     0    20     0     8     0]
 [    0    12     0  1915     0     0     0     0   108     1]
 [    0    25     5     6  2925     0     3     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     0     3     0  4850     0     5     0]
 [    0    19     0     0     0     2     1  1268     0     0]
 [    0    47     0    50    51     0     0     0  3423     0]
 [    0     0     0     0    11    29     0     0     0   879]]

Accuracy:
98.58771359024414

F1 scores:
[       nan 0.982325   0.99775927 0.9558273  0.97630174 0.98826202
 0.99466776 0.98447205 0.94925125 0.969129  ]

Kappa:
0.9812927136542036
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0b608f4b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.210, val_acc:0.094]
Epoch [2/120    avg_loss:1.995, val_acc:0.117]
Epoch [3/120    avg_loss:1.846, val_acc:0.168]
Epoch [4/120    avg_loss:1.725, val_acc:0.179]
Epoch [5/120    avg_loss:1.596, val_acc:0.209]
Epoch [6/120    avg_loss:1.468, val_acc:0.211]
Epoch [7/120    avg_loss:1.372, val_acc:0.232]
Epoch [8/120    avg_loss:1.299, val_acc:0.290]
Epoch [9/120    avg_loss:1.199, val_acc:0.341]
Epoch [10/120    avg_loss:1.103, val_acc:0.419]
Epoch [11/120    avg_loss:1.023, val_acc:0.439]
Epoch [12/120    avg_loss:0.921, val_acc:0.453]
Epoch [13/120    avg_loss:0.860, val_acc:0.463]
Epoch [14/120    avg_loss:0.788, val_acc:0.503]
Epoch [15/120    avg_loss:0.715, val_acc:0.595]
Epoch [16/120    avg_loss:0.639, val_acc:0.608]
Epoch [17/120    avg_loss:0.586, val_acc:0.681]
Epoch [18/120    avg_loss:0.554, val_acc:0.665]
Epoch [19/120    avg_loss:0.479, val_acc:0.716]
Epoch [20/120    avg_loss:0.438, val_acc:0.721]
Epoch [21/120    avg_loss:0.412, val_acc:0.698]
Epoch [22/120    avg_loss:0.373, val_acc:0.746]
Epoch [23/120    avg_loss:0.338, val_acc:0.773]
Epoch [24/120    avg_loss:0.340, val_acc:0.844]
Epoch [25/120    avg_loss:0.315, val_acc:0.870]
Epoch [26/120    avg_loss:0.265, val_acc:0.808]
Epoch [27/120    avg_loss:0.276, val_acc:0.838]
Epoch [28/120    avg_loss:0.239, val_acc:0.902]
Epoch [29/120    avg_loss:0.223, val_acc:0.878]
Epoch [30/120    avg_loss:0.201, val_acc:0.824]
Epoch [31/120    avg_loss:0.170, val_acc:0.930]
Epoch [32/120    avg_loss:0.185, val_acc:0.852]
Epoch [33/120    avg_loss:0.151, val_acc:0.937]
Epoch [34/120    avg_loss:0.156, val_acc:0.946]
Epoch [35/120    avg_loss:0.148, val_acc:0.927]
Epoch [36/120    avg_loss:0.131, val_acc:0.950]
Epoch [37/120    avg_loss:0.129, val_acc:0.936]
Epoch [38/120    avg_loss:0.138, val_acc:0.924]
Epoch [39/120    avg_loss:0.124, val_acc:0.944]
Epoch [40/120    avg_loss:0.122, val_acc:0.947]
Epoch [41/120    avg_loss:0.095, val_acc:0.967]
Epoch [42/120    avg_loss:0.076, val_acc:0.961]
Epoch [43/120    avg_loss:0.092, val_acc:0.943]
Epoch [44/120    avg_loss:0.102, val_acc:0.953]
Epoch [45/120    avg_loss:0.083, val_acc:0.967]
Epoch [46/120    avg_loss:0.095, val_acc:0.938]
Epoch [47/120    avg_loss:0.088, val_acc:0.966]
Epoch [48/120    avg_loss:0.087, val_acc:0.933]
Epoch [49/120    avg_loss:0.068, val_acc:0.964]
Epoch [50/120    avg_loss:0.056, val_acc:0.944]
Epoch [51/120    avg_loss:0.057, val_acc:0.973]
Epoch [52/120    avg_loss:0.075, val_acc:0.970]
Epoch [53/120    avg_loss:0.045, val_acc:0.967]
Epoch [54/120    avg_loss:0.056, val_acc:0.941]
Epoch [55/120    avg_loss:0.061, val_acc:0.948]
Epoch [56/120    avg_loss:0.038, val_acc:0.967]
Epoch [57/120    avg_loss:0.046, val_acc:0.973]
Epoch [58/120    avg_loss:0.031, val_acc:0.965]
Epoch [59/120    avg_loss:0.037, val_acc:0.968]
Epoch [60/120    avg_loss:0.045, val_acc:0.975]
Epoch [61/120    avg_loss:0.038, val_acc:0.971]
Epoch [62/120    avg_loss:0.036, val_acc:0.978]
Epoch [63/120    avg_loss:0.035, val_acc:0.979]
Epoch [64/120    avg_loss:0.044, val_acc:0.971]
Epoch [65/120    avg_loss:0.034, val_acc:0.975]
Epoch [66/120    avg_loss:0.029, val_acc:0.978]
Epoch [67/120    avg_loss:0.029, val_acc:0.975]
Epoch [68/120    avg_loss:0.023, val_acc:0.975]
Epoch [69/120    avg_loss:0.045, val_acc:0.970]
Epoch [70/120    avg_loss:0.051, val_acc:0.965]
Epoch [71/120    avg_loss:0.046, val_acc:0.969]
Epoch [72/120    avg_loss:0.033, val_acc:0.978]
Epoch [73/120    avg_loss:0.021, val_acc:0.975]
Epoch [74/120    avg_loss:0.018, val_acc:0.954]
Epoch [75/120    avg_loss:0.021, val_acc:0.974]
Epoch [76/120    avg_loss:0.021, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.985]
Epoch [78/120    avg_loss:0.017, val_acc:0.980]
Epoch [79/120    avg_loss:0.017, val_acc:0.973]
Epoch [80/120    avg_loss:0.015, val_acc:0.971]
Epoch [81/120    avg_loss:0.026, val_acc:0.978]
Epoch [82/120    avg_loss:0.037, val_acc:0.973]
Epoch [83/120    avg_loss:0.019, val_acc:0.975]
Epoch [84/120    avg_loss:0.015, val_acc:0.975]
Epoch [85/120    avg_loss:0.016, val_acc:0.960]
Epoch [86/120    avg_loss:0.138, val_acc:0.932]
Epoch [87/120    avg_loss:0.096, val_acc:0.968]
Epoch [88/120    avg_loss:0.065, val_acc:0.951]
Epoch [89/120    avg_loss:0.045, val_acc:0.963]
Epoch [90/120    avg_loss:0.036, val_acc:0.970]
Epoch [91/120    avg_loss:0.027, val_acc:0.977]
Epoch [92/120    avg_loss:0.019, val_acc:0.981]
Epoch [93/120    avg_loss:0.024, val_acc:0.977]
Epoch [94/120    avg_loss:0.020, val_acc:0.983]
Epoch [95/120    avg_loss:0.017, val_acc:0.981]
Epoch [96/120    avg_loss:0.016, val_acc:0.981]
Epoch [97/120    avg_loss:0.017, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.014, val_acc:0.983]
Epoch [101/120    avg_loss:0.021, val_acc:0.981]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.017, val_acc:0.978]
Epoch [104/120    avg_loss:0.014, val_acc:0.980]
Epoch [105/120    avg_loss:0.012, val_acc:0.980]
Epoch [106/120    avg_loss:0.016, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.980]
Epoch [108/120    avg_loss:0.016, val_acc:0.980]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.016, val_acc:0.980]
Epoch [112/120    avg_loss:0.015, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.980]
Epoch [115/120    avg_loss:0.013, val_acc:0.980]
Epoch [116/120    avg_loss:0.014, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.980]
Epoch [118/120    avg_loss:0.016, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     2     0     0     0    26    33     9]
 [    0     0 18037     0    43     0     9     0     1     0]
 [    0     8     0  1950     0     0     0     0    76     2]
 [    0    35    11     6  2899     0    14     0     6     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     5     0     0  4870     0     0     0]
 [    0    23     0     0     0     1     0  1264     1     1]
 [    0    38     0    34    45     0     0     0  3454     0]
 [    0     0     0     0     2    15     0     0     0   902]]

Accuracy:
98.91547971947075

F1 scores:
[       nan 0.98650954 0.99814615 0.96702207 0.97265559 0.99390708
 0.99682735 0.97984496 0.96723607 0.98364231]

Kappa:
0.985634153821727
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f842a341be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.210, val_acc:0.052]
Epoch [2/120    avg_loss:1.973, val_acc:0.067]
Epoch [3/120    avg_loss:1.872, val_acc:0.083]
Epoch [4/120    avg_loss:1.736, val_acc:0.115]
Epoch [5/120    avg_loss:1.633, val_acc:0.134]
Epoch [6/120    avg_loss:1.543, val_acc:0.150]
Epoch [7/120    avg_loss:1.438, val_acc:0.159]
Epoch [8/120    avg_loss:1.343, val_acc:0.194]
Epoch [9/120    avg_loss:1.294, val_acc:0.228]
Epoch [10/120    avg_loss:1.248, val_acc:0.288]
Epoch [11/120    avg_loss:1.184, val_acc:0.389]
Epoch [12/120    avg_loss:1.137, val_acc:0.398]
Epoch [13/120    avg_loss:1.051, val_acc:0.425]
Epoch [14/120    avg_loss:0.986, val_acc:0.448]
Epoch [15/120    avg_loss:0.914, val_acc:0.453]
Epoch [16/120    avg_loss:0.857, val_acc:0.458]
Epoch [17/120    avg_loss:0.806, val_acc:0.535]
Epoch [18/120    avg_loss:0.735, val_acc:0.632]
Epoch [19/120    avg_loss:0.684, val_acc:0.696]
Epoch [20/120    avg_loss:0.601, val_acc:0.750]
Epoch [21/120    avg_loss:0.551, val_acc:0.790]
Epoch [22/120    avg_loss:0.465, val_acc:0.808]
Epoch [23/120    avg_loss:0.424, val_acc:0.818]
Epoch [24/120    avg_loss:0.364, val_acc:0.842]
Epoch [25/120    avg_loss:0.328, val_acc:0.873]
Epoch [26/120    avg_loss:0.325, val_acc:0.852]
Epoch [27/120    avg_loss:0.278, val_acc:0.909]
Epoch [28/120    avg_loss:0.268, val_acc:0.934]
Epoch [29/120    avg_loss:0.227, val_acc:0.929]
Epoch [30/120    avg_loss:0.208, val_acc:0.925]
Epoch [31/120    avg_loss:0.174, val_acc:0.946]
Epoch [32/120    avg_loss:0.179, val_acc:0.932]
Epoch [33/120    avg_loss:0.149, val_acc:0.947]
Epoch [34/120    avg_loss:0.134, val_acc:0.954]
Epoch [35/120    avg_loss:0.144, val_acc:0.943]
Epoch [36/120    avg_loss:0.179, val_acc:0.962]
Epoch [37/120    avg_loss:0.120, val_acc:0.968]
Epoch [38/120    avg_loss:0.101, val_acc:0.963]
Epoch [39/120    avg_loss:0.099, val_acc:0.948]
Epoch [40/120    avg_loss:0.396, val_acc:0.711]
Epoch [41/120    avg_loss:0.706, val_acc:0.756]
Epoch [42/120    avg_loss:0.511, val_acc:0.828]
Epoch [43/120    avg_loss:0.359, val_acc:0.879]
Epoch [44/120    avg_loss:0.262, val_acc:0.899]
Epoch [45/120    avg_loss:0.232, val_acc:0.920]
Epoch [46/120    avg_loss:0.181, val_acc:0.939]
Epoch [47/120    avg_loss:0.166, val_acc:0.943]
Epoch [48/120    avg_loss:0.150, val_acc:0.907]
Epoch [49/120    avg_loss:0.123, val_acc:0.931]
Epoch [50/120    avg_loss:0.106, val_acc:0.957]
Epoch [51/120    avg_loss:0.082, val_acc:0.962]
Epoch [52/120    avg_loss:0.074, val_acc:0.968]
Epoch [53/120    avg_loss:0.065, val_acc:0.968]
Epoch [54/120    avg_loss:0.069, val_acc:0.964]
Epoch [55/120    avg_loss:0.067, val_acc:0.968]
Epoch [56/120    avg_loss:0.070, val_acc:0.968]
Epoch [57/120    avg_loss:0.066, val_acc:0.971]
Epoch [58/120    avg_loss:0.067, val_acc:0.965]
Epoch [59/120    avg_loss:0.065, val_acc:0.967]
Epoch [60/120    avg_loss:0.068, val_acc:0.970]
Epoch [61/120    avg_loss:0.060, val_acc:0.971]
Epoch [62/120    avg_loss:0.058, val_acc:0.969]
Epoch [63/120    avg_loss:0.060, val_acc:0.965]
Epoch [64/120    avg_loss:0.057, val_acc:0.972]
Epoch [65/120    avg_loss:0.057, val_acc:0.973]
Epoch [66/120    avg_loss:0.056, val_acc:0.971]
Epoch [67/120    avg_loss:0.065, val_acc:0.972]
Epoch [68/120    avg_loss:0.051, val_acc:0.975]
Epoch [69/120    avg_loss:0.056, val_acc:0.969]
Epoch [70/120    avg_loss:0.048, val_acc:0.974]
Epoch [71/120    avg_loss:0.050, val_acc:0.973]
Epoch [72/120    avg_loss:0.053, val_acc:0.968]
Epoch [73/120    avg_loss:0.052, val_acc:0.972]
Epoch [74/120    avg_loss:0.046, val_acc:0.973]
Epoch [75/120    avg_loss:0.056, val_acc:0.973]
Epoch [76/120    avg_loss:0.047, val_acc:0.969]
Epoch [77/120    avg_loss:0.051, val_acc:0.974]
Epoch [78/120    avg_loss:0.044, val_acc:0.977]
Epoch [79/120    avg_loss:0.051, val_acc:0.978]
Epoch [80/120    avg_loss:0.045, val_acc:0.968]
Epoch [81/120    avg_loss:0.049, val_acc:0.975]
Epoch [82/120    avg_loss:0.047, val_acc:0.977]
Epoch [83/120    avg_loss:0.045, val_acc:0.976]
Epoch [84/120    avg_loss:0.046, val_acc:0.973]
Epoch [85/120    avg_loss:0.041, val_acc:0.974]
Epoch [86/120    avg_loss:0.047, val_acc:0.974]
Epoch [87/120    avg_loss:0.039, val_acc:0.978]
Epoch [88/120    avg_loss:0.048, val_acc:0.977]
Epoch [89/120    avg_loss:0.041, val_acc:0.974]
Epoch [90/120    avg_loss:0.043, val_acc:0.976]
Epoch [91/120    avg_loss:0.039, val_acc:0.978]
Epoch [92/120    avg_loss:0.037, val_acc:0.978]
Epoch [93/120    avg_loss:0.040, val_acc:0.978]
Epoch [94/120    avg_loss:0.039, val_acc:0.980]
Epoch [95/120    avg_loss:0.035, val_acc:0.978]
Epoch [96/120    avg_loss:0.034, val_acc:0.978]
Epoch [97/120    avg_loss:0.037, val_acc:0.979]
Epoch [98/120    avg_loss:0.037, val_acc:0.977]
Epoch [99/120    avg_loss:0.034, val_acc:0.979]
Epoch [100/120    avg_loss:0.034, val_acc:0.979]
Epoch [101/120    avg_loss:0.035, val_acc:0.978]
Epoch [102/120    avg_loss:0.034, val_acc:0.978]
Epoch [103/120    avg_loss:0.032, val_acc:0.980]
Epoch [104/120    avg_loss:0.033, val_acc:0.974]
Epoch [105/120    avg_loss:0.038, val_acc:0.980]
Epoch [106/120    avg_loss:0.034, val_acc:0.978]
Epoch [107/120    avg_loss:0.041, val_acc:0.966]
Epoch [108/120    avg_loss:0.035, val_acc:0.979]
Epoch [109/120    avg_loss:0.032, val_acc:0.982]
Epoch [110/120    avg_loss:0.033, val_acc:0.979]
Epoch [111/120    avg_loss:0.034, val_acc:0.981]
Epoch [112/120    avg_loss:0.032, val_acc:0.979]
Epoch [113/120    avg_loss:0.031, val_acc:0.980]
Epoch [114/120    avg_loss:0.033, val_acc:0.980]
Epoch [115/120    avg_loss:0.033, val_acc:0.980]
Epoch [116/120    avg_loss:0.029, val_acc:0.981]
Epoch [117/120    avg_loss:0.030, val_acc:0.980]
Epoch [118/120    avg_loss:0.031, val_acc:0.983]
Epoch [119/120    avg_loss:0.031, val_acc:0.978]
Epoch [120/120    avg_loss:0.028, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6249     0     0     3     0     0     4   152    24]
 [    0     0 17954     0    32     0    96     0     8     0]
 [    0     1     0  1881     0     0     0     0   148     6]
 [    0    17     5     0  2930     0     6     0    10     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     0     0     0  4850     0     7     0]
 [    0    28     0     0     0     0     0  1256     0     6]
 [    0    49     0    24    53     0     0     0  3445     0]
 [    0     0     0     0     1    11     0     0     0   907]]

Accuracy:
98.27440773142457

F1 scores:
[       nan 0.97824045 0.99550873 0.95458006 0.97813387 0.99580313
 0.98677518 0.98509804 0.93856423 0.9721329 ]

Kappa:
0.9771631569009732
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9295800b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.231, val_acc:0.093]
Epoch [2/120    avg_loss:2.085, val_acc:0.097]
Epoch [3/120    avg_loss:1.975, val_acc:0.097]
Epoch [4/120    avg_loss:1.862, val_acc:0.107]
Epoch [5/120    avg_loss:1.753, val_acc:0.126]
Epoch [6/120    avg_loss:1.642, val_acc:0.176]
Epoch [7/120    avg_loss:1.510, val_acc:0.260]
Epoch [8/120    avg_loss:1.380, val_acc:0.432]
Epoch [9/120    avg_loss:1.246, val_acc:0.558]
Epoch [10/120    avg_loss:1.147, val_acc:0.560]
Epoch [11/120    avg_loss:1.014, val_acc:0.572]
Epoch [12/120    avg_loss:0.920, val_acc:0.583]
Epoch [13/120    avg_loss:0.812, val_acc:0.656]
Epoch [14/120    avg_loss:0.754, val_acc:0.685]
Epoch [15/120    avg_loss:0.667, val_acc:0.722]
Epoch [16/120    avg_loss:0.591, val_acc:0.782]
Epoch [17/120    avg_loss:0.563, val_acc:0.802]
Epoch [18/120    avg_loss:0.499, val_acc:0.767]
Epoch [19/120    avg_loss:0.487, val_acc:0.793]
Epoch [20/120    avg_loss:0.433, val_acc:0.779]
Epoch [21/120    avg_loss:0.415, val_acc:0.818]
Epoch [22/120    avg_loss:0.379, val_acc:0.823]
Epoch [23/120    avg_loss:0.346, val_acc:0.824]
Epoch [24/120    avg_loss:0.316, val_acc:0.831]
Epoch [25/120    avg_loss:0.279, val_acc:0.862]
Epoch [26/120    avg_loss:0.283, val_acc:0.877]
Epoch [27/120    avg_loss:0.261, val_acc:0.897]
Epoch [28/120    avg_loss:0.231, val_acc:0.912]
Epoch [29/120    avg_loss:0.197, val_acc:0.922]
Epoch [30/120    avg_loss:0.180, val_acc:0.907]
Epoch [31/120    avg_loss:0.182, val_acc:0.925]
Epoch [32/120    avg_loss:0.156, val_acc:0.926]
Epoch [33/120    avg_loss:0.158, val_acc:0.947]
Epoch [34/120    avg_loss:0.146, val_acc:0.957]
Epoch [35/120    avg_loss:0.184, val_acc:0.943]
Epoch [36/120    avg_loss:0.135, val_acc:0.958]
Epoch [37/120    avg_loss:0.115, val_acc:0.965]
Epoch [38/120    avg_loss:0.129, val_acc:0.960]
Epoch [39/120    avg_loss:0.130, val_acc:0.960]
Epoch [40/120    avg_loss:0.127, val_acc:0.963]
Epoch [41/120    avg_loss:0.089, val_acc:0.970]
Epoch [42/120    avg_loss:0.070, val_acc:0.970]
Epoch [43/120    avg_loss:0.066, val_acc:0.977]
Epoch [44/120    avg_loss:0.067, val_acc:0.984]
Epoch [45/120    avg_loss:0.063, val_acc:0.972]
Epoch [46/120    avg_loss:0.096, val_acc:0.955]
Epoch [47/120    avg_loss:0.087, val_acc:0.958]
Epoch [48/120    avg_loss:0.124, val_acc:0.956]
Epoch [49/120    avg_loss:0.091, val_acc:0.980]
Epoch [50/120    avg_loss:0.073, val_acc:0.976]
Epoch [51/120    avg_loss:0.066, val_acc:0.978]
Epoch [52/120    avg_loss:0.048, val_acc:0.970]
Epoch [53/120    avg_loss:0.070, val_acc:0.925]
Epoch [54/120    avg_loss:0.090, val_acc:0.970]
Epoch [55/120    avg_loss:0.045, val_acc:0.986]
Epoch [56/120    avg_loss:0.039, val_acc:0.983]
Epoch [57/120    avg_loss:0.037, val_acc:0.974]
Epoch [58/120    avg_loss:0.037, val_acc:0.985]
Epoch [59/120    avg_loss:0.038, val_acc:0.983]
Epoch [60/120    avg_loss:0.034, val_acc:0.971]
Epoch [61/120    avg_loss:0.058, val_acc:0.987]
Epoch [62/120    avg_loss:0.037, val_acc:0.988]
Epoch [63/120    avg_loss:0.026, val_acc:0.988]
Epoch [64/120    avg_loss:0.046, val_acc:0.963]
Epoch [65/120    avg_loss:0.047, val_acc:0.988]
Epoch [66/120    avg_loss:0.026, val_acc:0.968]
Epoch [67/120    avg_loss:0.029, val_acc:0.980]
Epoch [68/120    avg_loss:0.031, val_acc:0.986]
Epoch [69/120    avg_loss:0.023, val_acc:0.967]
Epoch [70/120    avg_loss:0.019, val_acc:0.989]
Epoch [71/120    avg_loss:0.020, val_acc:0.989]
Epoch [72/120    avg_loss:0.023, val_acc:0.980]
Epoch [73/120    avg_loss:0.022, val_acc:0.989]
Epoch [74/120    avg_loss:0.027, val_acc:0.986]
Epoch [75/120    avg_loss:0.023, val_acc:0.981]
Epoch [76/120    avg_loss:0.015, val_acc:0.994]
Epoch [77/120    avg_loss:0.016, val_acc:0.994]
Epoch [78/120    avg_loss:0.021, val_acc:0.991]
Epoch [79/120    avg_loss:0.020, val_acc:0.983]
Epoch [80/120    avg_loss:0.014, val_acc:0.993]
Epoch [81/120    avg_loss:0.012, val_acc:0.989]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.017, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.013, val_acc:0.988]
Epoch [86/120    avg_loss:0.012, val_acc:0.991]
Epoch [87/120    avg_loss:0.013, val_acc:0.991]
Epoch [88/120    avg_loss:0.022, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.991]
Epoch [90/120    avg_loss:0.040, val_acc:0.951]
Epoch [91/120    avg_loss:0.042, val_acc:0.988]
Epoch [92/120    avg_loss:0.018, val_acc:0.993]
Epoch [93/120    avg_loss:0.013, val_acc:0.994]
Epoch [94/120    avg_loss:0.012, val_acc:0.993]
Epoch [95/120    avg_loss:0.010, val_acc:0.993]
Epoch [96/120    avg_loss:0.011, val_acc:0.993]
Epoch [97/120    avg_loss:0.009, val_acc:0.994]
Epoch [98/120    avg_loss:0.010, val_acc:0.996]
Epoch [99/120    avg_loss:0.010, val_acc:0.996]
Epoch [100/120    avg_loss:0.010, val_acc:0.994]
Epoch [101/120    avg_loss:0.009, val_acc:0.995]
Epoch [102/120    avg_loss:0.008, val_acc:0.996]
Epoch [103/120    avg_loss:0.010, val_acc:0.995]
Epoch [104/120    avg_loss:0.010, val_acc:0.995]
Epoch [105/120    avg_loss:0.009, val_acc:0.995]
Epoch [106/120    avg_loss:0.010, val_acc:0.995]
Epoch [107/120    avg_loss:0.011, val_acc:0.996]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.008, val_acc:0.995]
Epoch [110/120    avg_loss:0.010, val_acc:0.994]
Epoch [111/120    avg_loss:0.008, val_acc:0.995]
Epoch [112/120    avg_loss:0.008, val_acc:0.995]
Epoch [113/120    avg_loss:0.010, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.996]
Epoch [115/120    avg_loss:0.010, val_acc:0.996]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.007, val_acc:0.995]
Epoch [118/120    avg_loss:0.009, val_acc:0.994]
Epoch [119/120    avg_loss:0.008, val_acc:0.993]
Epoch [120/120    avg_loss:0.008, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     0     0     0     6    53     0]
 [    0     4 18025     0    56     0     5     0     0     0]
 [    0     2     5  1935     0     0     0     0    93     1]
 [    0     6     6     0  2956     0     2     0     0     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4865     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    40     0     7    38     0     0     1  3485     0]
 [    0     0     0     0    11    29     0     0     0   879]]

Accuracy:
99.08418287421975

F1 scores:
[       nan 0.99136657 0.99753729 0.97285068 0.97994364 0.98901099
 0.99794872 0.99729416 0.96778673 0.97612438]

Kappa:
0.987869684679226
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea6a5d6be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.186, val_acc:0.079]
Epoch [2/120    avg_loss:1.995, val_acc:0.102]
Epoch [3/120    avg_loss:1.863, val_acc:0.127]
Epoch [4/120    avg_loss:1.749, val_acc:0.133]
Epoch [5/120    avg_loss:1.623, val_acc:0.134]
Epoch [6/120    avg_loss:1.540, val_acc:0.140]
Epoch [7/120    avg_loss:1.422, val_acc:0.232]
Epoch [8/120    avg_loss:1.332, val_acc:0.273]
Epoch [9/120    avg_loss:1.248, val_acc:0.299]
Epoch [10/120    avg_loss:1.159, val_acc:0.330]
Epoch [11/120    avg_loss:1.106, val_acc:0.401]
Epoch [12/120    avg_loss:1.024, val_acc:0.433]
Epoch [13/120    avg_loss:0.953, val_acc:0.486]
Epoch [14/120    avg_loss:0.828, val_acc:0.507]
Epoch [15/120    avg_loss:0.738, val_acc:0.564]
Epoch [16/120    avg_loss:0.664, val_acc:0.631]
Epoch [17/120    avg_loss:0.568, val_acc:0.665]
Epoch [18/120    avg_loss:0.549, val_acc:0.692]
Epoch [19/120    avg_loss:0.478, val_acc:0.732]
Epoch [20/120    avg_loss:0.432, val_acc:0.829]
Epoch [21/120    avg_loss:0.399, val_acc:0.864]
Epoch [22/120    avg_loss:0.409, val_acc:0.846]
Epoch [23/120    avg_loss:0.358, val_acc:0.892]
Epoch [24/120    avg_loss:0.302, val_acc:0.873]
Epoch [25/120    avg_loss:0.299, val_acc:0.853]
Epoch [26/120    avg_loss:0.246, val_acc:0.914]
Epoch [27/120    avg_loss:0.226, val_acc:0.938]
Epoch [28/120    avg_loss:0.224, val_acc:0.916]
Epoch [29/120    avg_loss:0.204, val_acc:0.948]
Epoch [30/120    avg_loss:0.174, val_acc:0.941]
Epoch [31/120    avg_loss:0.173, val_acc:0.957]
Epoch [32/120    avg_loss:0.164, val_acc:0.950]
Epoch [33/120    avg_loss:0.128, val_acc:0.953]
Epoch [34/120    avg_loss:0.118, val_acc:0.935]
Epoch [35/120    avg_loss:0.136, val_acc:0.933]
Epoch [36/120    avg_loss:0.133, val_acc:0.948]
Epoch [37/120    avg_loss:0.128, val_acc:0.955]
Epoch [38/120    avg_loss:0.129, val_acc:0.938]
Epoch [39/120    avg_loss:0.102, val_acc:0.968]
Epoch [40/120    avg_loss:0.087, val_acc:0.971]
Epoch [41/120    avg_loss:0.097, val_acc:0.956]
Epoch [42/120    avg_loss:0.081, val_acc:0.975]
Epoch [43/120    avg_loss:0.080, val_acc:0.963]
Epoch [44/120    avg_loss:0.079, val_acc:0.974]
Epoch [45/120    avg_loss:0.071, val_acc:0.953]
Epoch [46/120    avg_loss:0.075, val_acc:0.981]
Epoch [47/120    avg_loss:0.062, val_acc:0.973]
Epoch [48/120    avg_loss:0.068, val_acc:0.979]
Epoch [49/120    avg_loss:0.056, val_acc:0.981]
Epoch [50/120    avg_loss:0.059, val_acc:0.964]
Epoch [51/120    avg_loss:0.058, val_acc:0.970]
Epoch [52/120    avg_loss:0.037, val_acc:0.980]
Epoch [53/120    avg_loss:0.053, val_acc:0.967]
Epoch [54/120    avg_loss:0.072, val_acc:0.970]
Epoch [55/120    avg_loss:0.055, val_acc:0.972]
Epoch [56/120    avg_loss:0.075, val_acc:0.962]
Epoch [57/120    avg_loss:0.045, val_acc:0.974]
Epoch [58/120    avg_loss:0.040, val_acc:0.983]
Epoch [59/120    avg_loss:0.040, val_acc:0.986]
Epoch [60/120    avg_loss:0.043, val_acc:0.978]
Epoch [61/120    avg_loss:0.032, val_acc:0.978]
Epoch [62/120    avg_loss:0.036, val_acc:0.973]
Epoch [63/120    avg_loss:0.024, val_acc:0.980]
Epoch [64/120    avg_loss:0.023, val_acc:0.986]
Epoch [65/120    avg_loss:0.020, val_acc:0.982]
Epoch [66/120    avg_loss:0.017, val_acc:0.969]
Epoch [67/120    avg_loss:0.022, val_acc:0.986]
Epoch [68/120    avg_loss:0.019, val_acc:0.983]
Epoch [69/120    avg_loss:0.018, val_acc:0.988]
Epoch [70/120    avg_loss:0.022, val_acc:0.988]
Epoch [71/120    avg_loss:0.046, val_acc:0.970]
Epoch [72/120    avg_loss:0.033, val_acc:0.985]
Epoch [73/120    avg_loss:0.020, val_acc:0.990]
Epoch [74/120    avg_loss:0.017, val_acc:0.988]
Epoch [75/120    avg_loss:0.016, val_acc:0.987]
Epoch [76/120    avg_loss:0.015, val_acc:0.985]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.021, val_acc:0.983]
Epoch [79/120    avg_loss:0.015, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.989]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.010, val_acc:0.989]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.017, val_acc:0.988]
Epoch [87/120    avg_loss:0.012, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.009, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.010, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.011, val_acc:0.989]
Epoch [118/120    avg_loss:0.010, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0     0     6    42     2]
 [    0     0 18078     0    11     0     0     0     1     0]
 [    0     2     3  1978     0     0     0     0    49     4]
 [    0    26     5     0  2933     0     3     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     1     0     0  4874     0     0     0]
 [    0    15     0     0     0     0     0  1273     1     1]
 [    0    28     0    44    47     0     0     0  3444     8]
 [    0     0     0     0     7    19     0     0     0   893]]

Accuracy:
99.19745499240835

F1 scores:
[       nan 0.99060924 0.99936427 0.97462429 0.98257956 0.99277292
 0.99928242 0.9910471  0.96836778 0.97755884]

Kappa:
0.9893653040560808
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28615e3be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.204, val_acc:0.105]
Epoch [2/120    avg_loss:1.995, val_acc:0.079]
Epoch [3/120    avg_loss:1.869, val_acc:0.119]
Epoch [4/120    avg_loss:1.743, val_acc:0.134]
Epoch [5/120    avg_loss:1.593, val_acc:0.135]
Epoch [6/120    avg_loss:1.467, val_acc:0.143]
Epoch [7/120    avg_loss:1.365, val_acc:0.189]
Epoch [8/120    avg_loss:1.304, val_acc:0.243]
Epoch [9/120    avg_loss:1.246, val_acc:0.354]
Epoch [10/120    avg_loss:1.189, val_acc:0.354]
Epoch [11/120    avg_loss:1.115, val_acc:0.388]
Epoch [12/120    avg_loss:1.046, val_acc:0.397]
Epoch [13/120    avg_loss:0.967, val_acc:0.420]
Epoch [14/120    avg_loss:0.869, val_acc:0.470]
Epoch [15/120    avg_loss:0.793, val_acc:0.522]
Epoch [16/120    avg_loss:0.689, val_acc:0.553]
Epoch [17/120    avg_loss:0.628, val_acc:0.633]
Epoch [18/120    avg_loss:0.565, val_acc:0.629]
Epoch [19/120    avg_loss:0.501, val_acc:0.678]
Epoch [20/120    avg_loss:0.454, val_acc:0.691]
Epoch [21/120    avg_loss:0.413, val_acc:0.723]
Epoch [22/120    avg_loss:0.444, val_acc:0.724]
Epoch [23/120    avg_loss:0.412, val_acc:0.717]
Epoch [24/120    avg_loss:0.363, val_acc:0.762]
Epoch [25/120    avg_loss:0.341, val_acc:0.773]
Epoch [26/120    avg_loss:0.314, val_acc:0.788]
Epoch [27/120    avg_loss:0.293, val_acc:0.784]
Epoch [28/120    avg_loss:0.268, val_acc:0.794]
Epoch [29/120    avg_loss:0.255, val_acc:0.798]
Epoch [30/120    avg_loss:0.232, val_acc:0.803]
Epoch [31/120    avg_loss:0.225, val_acc:0.820]
Epoch [32/120    avg_loss:0.238, val_acc:0.853]
Epoch [33/120    avg_loss:0.218, val_acc:0.885]
Epoch [34/120    avg_loss:0.207, val_acc:0.881]
Epoch [35/120    avg_loss:0.247, val_acc:0.851]
Epoch [36/120    avg_loss:0.208, val_acc:0.895]
Epoch [37/120    avg_loss:0.165, val_acc:0.928]
Epoch [38/120    avg_loss:0.150, val_acc:0.902]
Epoch [39/120    avg_loss:0.157, val_acc:0.931]
Epoch [40/120    avg_loss:0.137, val_acc:0.946]
Epoch [41/120    avg_loss:0.143, val_acc:0.939]
Epoch [42/120    avg_loss:0.137, val_acc:0.952]
Epoch [43/120    avg_loss:0.097, val_acc:0.959]
Epoch [44/120    avg_loss:0.096, val_acc:0.968]
Epoch [45/120    avg_loss:0.093, val_acc:0.966]
Epoch [46/120    avg_loss:0.090, val_acc:0.952]
Epoch [47/120    avg_loss:0.093, val_acc:0.947]
Epoch [48/120    avg_loss:0.106, val_acc:0.943]
Epoch [49/120    avg_loss:0.083, val_acc:0.965]
Epoch [50/120    avg_loss:0.082, val_acc:0.960]
Epoch [51/120    avg_loss:0.088, val_acc:0.956]
Epoch [52/120    avg_loss:0.065, val_acc:0.965]
Epoch [53/120    avg_loss:0.063, val_acc:0.950]
Epoch [54/120    avg_loss:0.049, val_acc:0.977]
Epoch [55/120    avg_loss:0.063, val_acc:0.970]
Epoch [56/120    avg_loss:0.055, val_acc:0.972]
Epoch [57/120    avg_loss:0.074, val_acc:0.959]
Epoch [58/120    avg_loss:0.069, val_acc:0.964]
Epoch [59/120    avg_loss:0.065, val_acc:0.974]
Epoch [60/120    avg_loss:0.056, val_acc:0.953]
Epoch [61/120    avg_loss:0.073, val_acc:0.966]
Epoch [62/120    avg_loss:0.046, val_acc:0.968]
Epoch [63/120    avg_loss:0.059, val_acc:0.955]
Epoch [64/120    avg_loss:0.046, val_acc:0.973]
Epoch [65/120    avg_loss:0.049, val_acc:0.966]
Epoch [66/120    avg_loss:0.069, val_acc:0.966]
Epoch [67/120    avg_loss:0.035, val_acc:0.974]
Epoch [68/120    avg_loss:0.039, val_acc:0.979]
Epoch [69/120    avg_loss:0.028, val_acc:0.979]
Epoch [70/120    avg_loss:0.025, val_acc:0.979]
Epoch [71/120    avg_loss:0.024, val_acc:0.978]
Epoch [72/120    avg_loss:0.024, val_acc:0.979]
Epoch [73/120    avg_loss:0.024, val_acc:0.980]
Epoch [74/120    avg_loss:0.025, val_acc:0.981]
Epoch [75/120    avg_loss:0.023, val_acc:0.982]
Epoch [76/120    avg_loss:0.023, val_acc:0.982]
Epoch [77/120    avg_loss:0.022, val_acc:0.980]
Epoch [78/120    avg_loss:0.019, val_acc:0.981]
Epoch [79/120    avg_loss:0.020, val_acc:0.981]
Epoch [80/120    avg_loss:0.022, val_acc:0.982]
Epoch [81/120    avg_loss:0.020, val_acc:0.982]
Epoch [82/120    avg_loss:0.022, val_acc:0.981]
Epoch [83/120    avg_loss:0.020, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.981]
Epoch [85/120    avg_loss:0.023, val_acc:0.981]
Epoch [86/120    avg_loss:0.019, val_acc:0.981]
Epoch [87/120    avg_loss:0.020, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.982]
Epoch [89/120    avg_loss:0.020, val_acc:0.982]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.021, val_acc:0.981]
Epoch [92/120    avg_loss:0.020, val_acc:0.982]
Epoch [93/120    avg_loss:0.017, val_acc:0.982]
Epoch [94/120    avg_loss:0.017, val_acc:0.979]
Epoch [95/120    avg_loss:0.024, val_acc:0.981]
Epoch [96/120    avg_loss:0.021, val_acc:0.983]
Epoch [97/120    avg_loss:0.019, val_acc:0.982]
Epoch [98/120    avg_loss:0.019, val_acc:0.983]
Epoch [99/120    avg_loss:0.017, val_acc:0.982]
Epoch [100/120    avg_loss:0.017, val_acc:0.980]
Epoch [101/120    avg_loss:0.017, val_acc:0.979]
Epoch [102/120    avg_loss:0.018, val_acc:0.981]
Epoch [103/120    avg_loss:0.019, val_acc:0.983]
Epoch [104/120    avg_loss:0.022, val_acc:0.979]
Epoch [105/120    avg_loss:0.018, val_acc:0.979]
Epoch [106/120    avg_loss:0.016, val_acc:0.981]
Epoch [107/120    avg_loss:0.017, val_acc:0.980]
Epoch [108/120    avg_loss:0.015, val_acc:0.980]
Epoch [109/120    avg_loss:0.019, val_acc:0.984]
Epoch [110/120    avg_loss:0.015, val_acc:0.982]
Epoch [111/120    avg_loss:0.016, val_acc:0.981]
Epoch [112/120    avg_loss:0.017, val_acc:0.979]
Epoch [113/120    avg_loss:0.016, val_acc:0.980]
Epoch [114/120    avg_loss:0.015, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.016, val_acc:0.980]
Epoch [117/120    avg_loss:0.017, val_acc:0.979]
Epoch [118/120    avg_loss:0.014, val_acc:0.979]
Epoch [119/120    avg_loss:0.015, val_acc:0.980]
Epoch [120/120    avg_loss:0.017, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     7    19    10]
 [    0     0 18040     0    44     0     6     0     0     0]
 [    0    15     0  1939     0     0     0     0    79     3]
 [    0    18    10     0  2921     0     3     0    17     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2    46     2     0     0  4827     0     1     0]
 [    0     9     0     0     0     2     0  1278     0     1]
 [    0    71     0    58    39     0     0     0  3403     0]
 [    0     2     0     0    11    17     0     0     0   889]]

Accuracy:
98.80702769141783

F1 scores:
[       nan 0.98818076 0.99707069 0.96109046 0.97578086 0.99277292
 0.99382335 0.99262136 0.95994358 0.97424658]

Kappa:
0.9841887720892154
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f035ba50c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 13210==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.206, val_acc:0.040]
Epoch [2/120    avg_loss:2.038, val_acc:0.070]
Epoch [3/120    avg_loss:1.892, val_acc:0.099]
Epoch [4/120    avg_loss:1.741, val_acc:0.135]
Epoch [5/120    avg_loss:1.573, val_acc:0.160]
Epoch [6/120    avg_loss:1.458, val_acc:0.235]
Epoch [7/120    avg_loss:1.362, val_acc:0.298]
Epoch [8/120    avg_loss:1.223, val_acc:0.508]
Epoch [9/120    avg_loss:1.117, val_acc:0.508]
Epoch [10/120    avg_loss:1.011, val_acc:0.543]
Epoch [11/120    avg_loss:0.934, val_acc:0.631]
Epoch [12/120    avg_loss:0.824, val_acc:0.680]
Epoch [13/120    avg_loss:0.727, val_acc:0.708]
Epoch [14/120    avg_loss:0.686, val_acc:0.759]
Epoch [15/120    avg_loss:0.587, val_acc:0.748]
Epoch [16/120    avg_loss:0.530, val_acc:0.795]
Epoch [17/120    avg_loss:0.525, val_acc:0.804]
Epoch [18/120    avg_loss:0.452, val_acc:0.823]
Epoch [19/120    avg_loss:0.398, val_acc:0.856]
Epoch [20/120    avg_loss:0.358, val_acc:0.866]
Epoch [21/120    avg_loss:0.303, val_acc:0.887]
Epoch [22/120    avg_loss:0.293, val_acc:0.898]
Epoch [23/120    avg_loss:0.275, val_acc:0.930]
Epoch [24/120    avg_loss:0.260, val_acc:0.928]
Epoch [25/120    avg_loss:0.267, val_acc:0.871]
Epoch [26/120    avg_loss:0.253, val_acc:0.933]
Epoch [27/120    avg_loss:0.209, val_acc:0.919]
Epoch [28/120    avg_loss:0.179, val_acc:0.886]
Epoch [29/120    avg_loss:0.176, val_acc:0.946]
Epoch [30/120    avg_loss:0.152, val_acc:0.931]
Epoch [31/120    avg_loss:0.150, val_acc:0.944]
Epoch [32/120    avg_loss:0.123, val_acc:0.961]
Epoch [33/120    avg_loss:0.119, val_acc:0.941]
Epoch [34/120    avg_loss:0.138, val_acc:0.952]
Epoch [35/120    avg_loss:0.161, val_acc:0.956]
Epoch [36/120    avg_loss:0.099, val_acc:0.961]
Epoch [37/120    avg_loss:0.100, val_acc:0.969]
Epoch [38/120    avg_loss:0.094, val_acc:0.963]
Epoch [39/120    avg_loss:0.100, val_acc:0.971]
Epoch [40/120    avg_loss:0.085, val_acc:0.979]
Epoch [41/120    avg_loss:0.125, val_acc:0.932]
Epoch [42/120    avg_loss:0.114, val_acc:0.948]
Epoch [43/120    avg_loss:0.111, val_acc:0.965]
Epoch [44/120    avg_loss:0.102, val_acc:0.962]
Epoch [45/120    avg_loss:0.076, val_acc:0.972]
Epoch [46/120    avg_loss:0.069, val_acc:0.966]
Epoch [47/120    avg_loss:0.064, val_acc:0.978]
Epoch [48/120    avg_loss:0.073, val_acc:0.966]
Epoch [49/120    avg_loss:0.066, val_acc:0.973]
Epoch [50/120    avg_loss:0.070, val_acc:0.975]
Epoch [51/120    avg_loss:0.053, val_acc:0.974]
Epoch [52/120    avg_loss:0.041, val_acc:0.984]
Epoch [53/120    avg_loss:0.045, val_acc:0.975]
Epoch [54/120    avg_loss:0.050, val_acc:0.978]
Epoch [55/120    avg_loss:0.052, val_acc:0.968]
Epoch [56/120    avg_loss:0.049, val_acc:0.978]
Epoch [57/120    avg_loss:0.036, val_acc:0.975]
Epoch [58/120    avg_loss:0.030, val_acc:0.978]
Epoch [59/120    avg_loss:0.041, val_acc:0.983]
Epoch [60/120    avg_loss:0.029, val_acc:0.986]
Epoch [61/120    avg_loss:0.030, val_acc:0.985]
Epoch [62/120    avg_loss:0.025, val_acc:0.981]
Epoch [63/120    avg_loss:0.041, val_acc:0.981]
Epoch [64/120    avg_loss:0.029, val_acc:0.984]
Epoch [65/120    avg_loss:0.034, val_acc:0.983]
Epoch [66/120    avg_loss:0.027, val_acc:0.979]
Epoch [67/120    avg_loss:0.051, val_acc:0.971]
Epoch [68/120    avg_loss:0.041, val_acc:0.976]
Epoch [69/120    avg_loss:0.053, val_acc:0.955]
Epoch [70/120    avg_loss:0.038, val_acc:0.977]
Epoch [71/120    avg_loss:0.028, val_acc:0.984]
Epoch [72/120    avg_loss:0.025, val_acc:0.983]
Epoch [73/120    avg_loss:0.020, val_acc:0.981]
Epoch [74/120    avg_loss:0.021, val_acc:0.981]
Epoch [75/120    avg_loss:0.017, val_acc:0.983]
Epoch [76/120    avg_loss:0.016, val_acc:0.984]
Epoch [77/120    avg_loss:0.017, val_acc:0.983]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.014, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.016, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.985]
Epoch [84/120    avg_loss:0.015, val_acc:0.984]
Epoch [85/120    avg_loss:0.016, val_acc:0.984]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.984]
Epoch [89/120    avg_loss:0.015, val_acc:0.984]
Epoch [90/120    avg_loss:0.014, val_acc:0.984]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.014, val_acc:0.984]
Epoch [93/120    avg_loss:0.015, val_acc:0.984]
Epoch [94/120    avg_loss:0.014, val_acc:0.984]
Epoch [95/120    avg_loss:0.013, val_acc:0.984]
Epoch [96/120    avg_loss:0.016, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.984]
Epoch [98/120    avg_loss:0.014, val_acc:0.984]
Epoch [99/120    avg_loss:0.014, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.017, val_acc:0.984]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.013, val_acc:0.984]
Epoch [106/120    avg_loss:0.014, val_acc:0.984]
Epoch [107/120    avg_loss:0.013, val_acc:0.984]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.014, val_acc:0.984]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.016, val_acc:0.984]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.014, val_acc:0.984]
Epoch [120/120    avg_loss:0.015, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6344     0     0     0     0     0     4    84     0]
 [    0     0 17916     0    95     0    79     0     0     0]
 [    0     3     0  1901     0     0     0     0   131     1]
 [    0    40     6     0  2904     0    22     0     0     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    18     0    21     0  4837     0     2     0]
 [    0     5     0     0     0     0     0  1284     1     0]
 [    0    28     0    56    45     0     1     0  3436     5]
 [    0     0     0     0    13    18     0     0     0   888]]

Accuracy:
98.36598944400261

F1 scores:
[       nan 0.98723934 0.99450458 0.95216629 0.96       0.99315068
 0.98543343 0.99612102 0.95114187 0.97959184]

Kappa:
0.9783823907600342
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f131fd27c50>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.233, val_acc:0.070]
Epoch [2/120    avg_loss:2.030, val_acc:0.067]
Epoch [3/120    avg_loss:1.905, val_acc:0.088]
Epoch [4/120    avg_loss:1.794, val_acc:0.104]
Epoch [5/120    avg_loss:1.661, val_acc:0.242]
Epoch [6/120    avg_loss:1.533, val_acc:0.286]
Epoch [7/120    avg_loss:1.438, val_acc:0.383]
Epoch [8/120    avg_loss:1.312, val_acc:0.492]
Epoch [9/120    avg_loss:1.156, val_acc:0.436]
Epoch [10/120    avg_loss:1.085, val_acc:0.556]
Epoch [11/120    avg_loss:0.944, val_acc:0.567]
Epoch [12/120    avg_loss:0.852, val_acc:0.568]
Epoch [13/120    avg_loss:0.781, val_acc:0.603]
Epoch [14/120    avg_loss:0.701, val_acc:0.598]
Epoch [15/120    avg_loss:0.644, val_acc:0.603]
Epoch [16/120    avg_loss:0.560, val_acc:0.612]
Epoch [17/120    avg_loss:0.510, val_acc:0.652]
Epoch [18/120    avg_loss:0.473, val_acc:0.685]
Epoch [19/120    avg_loss:0.441, val_acc:0.667]
Epoch [20/120    avg_loss:0.416, val_acc:0.726]
Epoch [21/120    avg_loss:0.372, val_acc:0.740]
Epoch [22/120    avg_loss:0.344, val_acc:0.762]
Epoch [23/120    avg_loss:0.328, val_acc:0.762]
Epoch [24/120    avg_loss:0.291, val_acc:0.805]
Epoch [25/120    avg_loss:0.317, val_acc:0.806]
Epoch [26/120    avg_loss:0.280, val_acc:0.815]
Epoch [27/120    avg_loss:0.269, val_acc:0.820]
Epoch [28/120    avg_loss:0.229, val_acc:0.829]
Epoch [29/120    avg_loss:0.235, val_acc:0.866]
Epoch [30/120    avg_loss:0.201, val_acc:0.858]
Epoch [31/120    avg_loss:0.232, val_acc:0.891]
Epoch [32/120    avg_loss:0.188, val_acc:0.910]
Epoch [33/120    avg_loss:0.156, val_acc:0.916]
Epoch [34/120    avg_loss:0.166, val_acc:0.936]
Epoch [35/120    avg_loss:0.173, val_acc:0.824]
Epoch [36/120    avg_loss:0.161, val_acc:0.942]
Epoch [37/120    avg_loss:0.128, val_acc:0.932]
Epoch [38/120    avg_loss:0.171, val_acc:0.943]
Epoch [39/120    avg_loss:0.132, val_acc:0.936]
Epoch [40/120    avg_loss:0.132, val_acc:0.965]
Epoch [41/120    avg_loss:0.099, val_acc:0.950]
Epoch [42/120    avg_loss:0.128, val_acc:0.924]
Epoch [43/120    avg_loss:0.643, val_acc:0.633]
Epoch [44/120    avg_loss:0.996, val_acc:0.634]
Epoch [45/120    avg_loss:0.750, val_acc:0.715]
Epoch [46/120    avg_loss:0.574, val_acc:0.743]
Epoch [47/120    avg_loss:0.501, val_acc:0.736]
Epoch [48/120    avg_loss:0.431, val_acc:0.796]
Epoch [49/120    avg_loss:0.461, val_acc:0.780]
Epoch [50/120    avg_loss:0.398, val_acc:0.795]
Epoch [51/120    avg_loss:0.355, val_acc:0.834]
Epoch [52/120    avg_loss:0.421, val_acc:0.741]
Epoch [53/120    avg_loss:0.466, val_acc:0.768]
Epoch [54/120    avg_loss:0.358, val_acc:0.798]
Epoch [55/120    avg_loss:0.315, val_acc:0.814]
Epoch [56/120    avg_loss:0.311, val_acc:0.823]
Epoch [57/120    avg_loss:0.288, val_acc:0.848]
Epoch [58/120    avg_loss:0.298, val_acc:0.861]
Epoch [59/120    avg_loss:0.279, val_acc:0.863]
Epoch [60/120    avg_loss:0.267, val_acc:0.875]
Epoch [61/120    avg_loss:0.267, val_acc:0.871]
Epoch [62/120    avg_loss:0.279, val_acc:0.868]
Epoch [63/120    avg_loss:0.269, val_acc:0.874]
Epoch [64/120    avg_loss:0.257, val_acc:0.873]
Epoch [65/120    avg_loss:0.240, val_acc:0.864]
Epoch [66/120    avg_loss:0.247, val_acc:0.879]
Epoch [67/120    avg_loss:0.244, val_acc:0.883]
Epoch [68/120    avg_loss:0.241, val_acc:0.883]
Epoch [69/120    avg_loss:0.228, val_acc:0.883]
Epoch [70/120    avg_loss:0.242, val_acc:0.883]
Epoch [71/120    avg_loss:0.265, val_acc:0.887]
Epoch [72/120    avg_loss:0.240, val_acc:0.883]
Epoch [73/120    avg_loss:0.238, val_acc:0.883]
Epoch [74/120    avg_loss:0.247, val_acc:0.881]
Epoch [75/120    avg_loss:0.226, val_acc:0.880]
Epoch [76/120    avg_loss:0.250, val_acc:0.882]
Epoch [77/120    avg_loss:0.233, val_acc:0.885]
Epoch [78/120    avg_loss:0.232, val_acc:0.884]
Epoch [79/120    avg_loss:0.226, val_acc:0.885]
Epoch [80/120    avg_loss:0.236, val_acc:0.885]
Epoch [81/120    avg_loss:0.234, val_acc:0.885]
Epoch [82/120    avg_loss:0.238, val_acc:0.884]
Epoch [83/120    avg_loss:0.241, val_acc:0.884]
Epoch [84/120    avg_loss:0.231, val_acc:0.884]
Epoch [85/120    avg_loss:0.230, val_acc:0.884]
Epoch [86/120    avg_loss:0.235, val_acc:0.882]
Epoch [87/120    avg_loss:0.236, val_acc:0.883]
Epoch [88/120    avg_loss:0.226, val_acc:0.883]
Epoch [89/120    avg_loss:0.226, val_acc:0.883]
Epoch [90/120    avg_loss:0.243, val_acc:0.883]
Epoch [91/120    avg_loss:0.243, val_acc:0.883]
Epoch [92/120    avg_loss:0.240, val_acc:0.882]
Epoch [93/120    avg_loss:0.229, val_acc:0.883]
Epoch [94/120    avg_loss:0.235, val_acc:0.883]
Epoch [95/120    avg_loss:0.234, val_acc:0.883]
Epoch [96/120    avg_loss:0.243, val_acc:0.883]
Epoch [97/120    avg_loss:0.254, val_acc:0.883]
Epoch [98/120    avg_loss:0.247, val_acc:0.883]
Epoch [99/120    avg_loss:0.238, val_acc:0.883]
Epoch [100/120    avg_loss:0.241, val_acc:0.883]
Epoch [101/120    avg_loss:0.236, val_acc:0.883]
Epoch [102/120    avg_loss:0.237, val_acc:0.883]
Epoch [103/120    avg_loss:0.241, val_acc:0.883]
Epoch [104/120    avg_loss:0.251, val_acc:0.883]
Epoch [105/120    avg_loss:0.231, val_acc:0.883]
Epoch [106/120    avg_loss:0.236, val_acc:0.883]
Epoch [107/120    avg_loss:0.223, val_acc:0.883]
Epoch [108/120    avg_loss:0.242, val_acc:0.883]
Epoch [109/120    avg_loss:0.232, val_acc:0.883]
Epoch [110/120    avg_loss:0.231, val_acc:0.883]
Epoch [111/120    avg_loss:0.241, val_acc:0.883]
Epoch [112/120    avg_loss:0.232, val_acc:0.883]
Epoch [113/120    avg_loss:0.242, val_acc:0.883]
Epoch [114/120    avg_loss:0.246, val_acc:0.883]
Epoch [115/120    avg_loss:0.241, val_acc:0.883]
Epoch [116/120    avg_loss:0.225, val_acc:0.883]
Epoch [117/120    avg_loss:0.224, val_acc:0.883]
Epoch [118/120    avg_loss:0.228, val_acc:0.883]
Epoch [119/120    avg_loss:0.225, val_acc:0.883]
Epoch [120/120    avg_loss:0.231, val_acc:0.883]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5103   261   263    76     0    19    42   493   175]
 [    0     0 17020     0   192     0   878     0     0     0]
 [    0     7     1  1880     0     0     0     0   103    45]
 [    0    43    30     0  2851     0    23     0    23     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     9    16     0     0     0  4762     0    91     0]
 [    0    18     0     0     0     0     1  1271     0     0]
 [    0   125    55    87    51     0    29     0  3224     0]
 [    0    18     0     1    21    56     0     0     1   822]]

Accuracy:
92.1553033041718

F1 scores:
[       nan 0.86822629 0.95960308 0.88118116 0.92519877 0.97899475
 0.899339   0.9765655  0.8590461  0.83749363]

Kappa:
0.897105414627341
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe54c056ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.213, val_acc:0.154]
Epoch [2/120    avg_loss:2.041, val_acc:0.189]
Epoch [3/120    avg_loss:1.895, val_acc:0.202]
Epoch [4/120    avg_loss:1.759, val_acc:0.240]
Epoch [5/120    avg_loss:1.656, val_acc:0.252]
Epoch [6/120    avg_loss:1.478, val_acc:0.268]
Epoch [7/120    avg_loss:1.356, val_acc:0.323]
Epoch [8/120    avg_loss:1.271, val_acc:0.365]
Epoch [9/120    avg_loss:1.150, val_acc:0.376]
Epoch [10/120    avg_loss:1.072, val_acc:0.397]
Epoch [11/120    avg_loss:0.977, val_acc:0.441]
Epoch [12/120    avg_loss:0.853, val_acc:0.498]
Epoch [13/120    avg_loss:0.772, val_acc:0.547]
Epoch [14/120    avg_loss:0.693, val_acc:0.608]
Epoch [15/120    avg_loss:0.651, val_acc:0.619]
Epoch [16/120    avg_loss:0.588, val_acc:0.633]
Epoch [17/120    avg_loss:0.521, val_acc:0.651]
Epoch [18/120    avg_loss:0.498, val_acc:0.738]
Epoch [19/120    avg_loss:0.439, val_acc:0.724]
Epoch [20/120    avg_loss:0.431, val_acc:0.704]
Epoch [21/120    avg_loss:0.449, val_acc:0.700]
Epoch [22/120    avg_loss:0.400, val_acc:0.731]
Epoch [23/120    avg_loss:0.360, val_acc:0.778]
Epoch [24/120    avg_loss:0.352, val_acc:0.773]
Epoch [25/120    avg_loss:0.311, val_acc:0.738]
Epoch [26/120    avg_loss:0.295, val_acc:0.781]
Epoch [27/120    avg_loss:0.265, val_acc:0.764]
Epoch [28/120    avg_loss:0.277, val_acc:0.797]
Epoch [29/120    avg_loss:0.259, val_acc:0.796]
Epoch [30/120    avg_loss:0.234, val_acc:0.801]
Epoch [31/120    avg_loss:0.214, val_acc:0.823]
Epoch [32/120    avg_loss:0.217, val_acc:0.856]
Epoch [33/120    avg_loss:0.198, val_acc:0.885]
Epoch [34/120    avg_loss:0.212, val_acc:0.891]
Epoch [35/120    avg_loss:0.191, val_acc:0.897]
Epoch [36/120    avg_loss:0.165, val_acc:0.924]
Epoch [37/120    avg_loss:0.180, val_acc:0.922]
Epoch [38/120    avg_loss:0.157, val_acc:0.907]
Epoch [39/120    avg_loss:0.167, val_acc:0.892]
Epoch [40/120    avg_loss:0.148, val_acc:0.916]
Epoch [41/120    avg_loss:0.129, val_acc:0.906]
Epoch [42/120    avg_loss:0.143, val_acc:0.934]
Epoch [43/120    avg_loss:0.116, val_acc:0.941]
Epoch [44/120    avg_loss:0.096, val_acc:0.953]
Epoch [45/120    avg_loss:0.104, val_acc:0.953]
Epoch [46/120    avg_loss:0.083, val_acc:0.966]
Epoch [47/120    avg_loss:0.103, val_acc:0.960]
Epoch [48/120    avg_loss:0.076, val_acc:0.966]
Epoch [49/120    avg_loss:0.069, val_acc:0.969]
Epoch [50/120    avg_loss:0.063, val_acc:0.968]
Epoch [51/120    avg_loss:0.069, val_acc:0.960]
Epoch [52/120    avg_loss:0.179, val_acc:0.830]
Epoch [53/120    avg_loss:0.271, val_acc:0.916]
Epoch [54/120    avg_loss:0.142, val_acc:0.921]
Epoch [55/120    avg_loss:0.121, val_acc:0.926]
Epoch [56/120    avg_loss:0.130, val_acc:0.966]
Epoch [57/120    avg_loss:0.075, val_acc:0.958]
Epoch [58/120    avg_loss:0.074, val_acc:0.966]
Epoch [59/120    avg_loss:0.061, val_acc:0.964]
Epoch [60/120    avg_loss:0.067, val_acc:0.956]
Epoch [61/120    avg_loss:0.100, val_acc:0.898]
Epoch [62/120    avg_loss:0.130, val_acc:0.921]
Epoch [63/120    avg_loss:0.084, val_acc:0.947]
Epoch [64/120    avg_loss:0.068, val_acc:0.956]
Epoch [65/120    avg_loss:0.056, val_acc:0.959]
Epoch [66/120    avg_loss:0.052, val_acc:0.964]
Epoch [67/120    avg_loss:0.056, val_acc:0.965]
Epoch [68/120    avg_loss:0.056, val_acc:0.966]
Epoch [69/120    avg_loss:0.045, val_acc:0.968]
Epoch [70/120    avg_loss:0.046, val_acc:0.967]
Epoch [71/120    avg_loss:0.045, val_acc:0.966]
Epoch [72/120    avg_loss:0.047, val_acc:0.964]
Epoch [73/120    avg_loss:0.044, val_acc:0.965]
Epoch [74/120    avg_loss:0.044, val_acc:0.967]
Epoch [75/120    avg_loss:0.039, val_acc:0.968]
Epoch [76/120    avg_loss:0.041, val_acc:0.968]
Epoch [77/120    avg_loss:0.040, val_acc:0.968]
Epoch [78/120    avg_loss:0.040, val_acc:0.968]
Epoch [79/120    avg_loss:0.041, val_acc:0.968]
Epoch [80/120    avg_loss:0.045, val_acc:0.968]
Epoch [81/120    avg_loss:0.037, val_acc:0.968]
Epoch [82/120    avg_loss:0.046, val_acc:0.969]
Epoch [83/120    avg_loss:0.041, val_acc:0.969]
Epoch [84/120    avg_loss:0.040, val_acc:0.969]
Epoch [85/120    avg_loss:0.036, val_acc:0.969]
Epoch [86/120    avg_loss:0.042, val_acc:0.969]
Epoch [87/120    avg_loss:0.045, val_acc:0.969]
Epoch [88/120    avg_loss:0.040, val_acc:0.967]
Epoch [89/120    avg_loss:0.038, val_acc:0.967]
Epoch [90/120    avg_loss:0.041, val_acc:0.967]
Epoch [91/120    avg_loss:0.040, val_acc:0.967]
Epoch [92/120    avg_loss:0.045, val_acc:0.967]
Epoch [93/120    avg_loss:0.040, val_acc:0.967]
Epoch [94/120    avg_loss:0.039, val_acc:0.968]
Epoch [95/120    avg_loss:0.038, val_acc:0.969]
Epoch [96/120    avg_loss:0.040, val_acc:0.968]
Epoch [97/120    avg_loss:0.044, val_acc:0.968]
Epoch [98/120    avg_loss:0.039, val_acc:0.968]
Epoch [99/120    avg_loss:0.040, val_acc:0.968]
Epoch [100/120    avg_loss:0.042, val_acc:0.968]
Epoch [101/120    avg_loss:0.038, val_acc:0.968]
Epoch [102/120    avg_loss:0.040, val_acc:0.968]
Epoch [103/120    avg_loss:0.040, val_acc:0.969]
Epoch [104/120    avg_loss:0.047, val_acc:0.969]
Epoch [105/120    avg_loss:0.037, val_acc:0.968]
Epoch [106/120    avg_loss:0.040, val_acc:0.968]
Epoch [107/120    avg_loss:0.037, val_acc:0.968]
Epoch [108/120    avg_loss:0.038, val_acc:0.968]
Epoch [109/120    avg_loss:0.041, val_acc:0.969]
Epoch [110/120    avg_loss:0.034, val_acc:0.969]
Epoch [111/120    avg_loss:0.042, val_acc:0.968]
Epoch [112/120    avg_loss:0.037, val_acc:0.968]
Epoch [113/120    avg_loss:0.041, val_acc:0.969]
Epoch [114/120    avg_loss:0.036, val_acc:0.969]
Epoch [115/120    avg_loss:0.038, val_acc:0.969]
Epoch [116/120    avg_loss:0.039, val_acc:0.969]
Epoch [117/120    avg_loss:0.036, val_acc:0.969]
Epoch [118/120    avg_loss:0.039, val_acc:0.969]
Epoch [119/120    avg_loss:0.040, val_acc:0.969]
Epoch [120/120    avg_loss:0.042, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     0     3     0     4    17    45    27]
 [    0     0 17824     0    78     0   180     0     6     2]
 [    0     3     0  1997     0     0     0     0    33     3]
 [    0    38     6     0  2915     0     1     0    11     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    41     3     0     0  4809     0    25     0]
 [    0     7     0     0     0     0     0  1277     0     6]
 [    0    36     0    28    59     0     0     0  3448     0]
 [    0     0     0     0    14    38     0     0     0   867]]

Accuracy:
98.27681777649242

F1 scores:
[       nan 0.9859944  0.99129613 0.98277559 0.96507201 0.98564955
 0.97427066 0.98839009 0.96596162 0.95013699]

Kappa:
0.9772223482012996
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f09dc95fb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.196, val_acc:0.082]
Epoch [2/120    avg_loss:1.993, val_acc:0.188]
Epoch [3/120    avg_loss:1.875, val_acc:0.354]
Epoch [4/120    avg_loss:1.741, val_acc:0.408]
Epoch [5/120    avg_loss:1.633, val_acc:0.491]
Epoch [6/120    avg_loss:1.532, val_acc:0.503]
Epoch [7/120    avg_loss:1.414, val_acc:0.516]
Epoch [8/120    avg_loss:1.328, val_acc:0.502]
Epoch [9/120    avg_loss:1.266, val_acc:0.364]
Epoch [10/120    avg_loss:1.207, val_acc:0.389]
Epoch [11/120    avg_loss:1.094, val_acc:0.466]
Epoch [12/120    avg_loss:0.986, val_acc:0.555]
Epoch [13/120    avg_loss:0.886, val_acc:0.588]
Epoch [14/120    avg_loss:0.768, val_acc:0.603]
Epoch [15/120    avg_loss:0.695, val_acc:0.666]
Epoch [16/120    avg_loss:0.571, val_acc:0.683]
Epoch [17/120    avg_loss:0.531, val_acc:0.713]
Epoch [18/120    avg_loss:0.507, val_acc:0.728]
Epoch [19/120    avg_loss:0.428, val_acc:0.798]
Epoch [20/120    avg_loss:0.382, val_acc:0.807]
Epoch [21/120    avg_loss:0.357, val_acc:0.813]
Epoch [22/120    avg_loss:0.332, val_acc:0.896]
Epoch [23/120    avg_loss:0.300, val_acc:0.871]
Epoch [24/120    avg_loss:0.275, val_acc:0.922]
Epoch [25/120    avg_loss:0.286, val_acc:0.919]
Epoch [26/120    avg_loss:0.250, val_acc:0.928]
Epoch [27/120    avg_loss:0.211, val_acc:0.947]
Epoch [28/120    avg_loss:0.178, val_acc:0.949]
Epoch [29/120    avg_loss:0.306, val_acc:0.905]
Epoch [30/120    avg_loss:0.223, val_acc:0.924]
Epoch [31/120    avg_loss:0.183, val_acc:0.946]
Epoch [32/120    avg_loss:0.144, val_acc:0.947]
Epoch [33/120    avg_loss:0.128, val_acc:0.948]
Epoch [34/120    avg_loss:0.134, val_acc:0.961]
Epoch [35/120    avg_loss:0.124, val_acc:0.957]
Epoch [36/120    avg_loss:0.097, val_acc:0.966]
Epoch [37/120    avg_loss:0.119, val_acc:0.743]
Epoch [38/120    avg_loss:1.358, val_acc:0.547]
Epoch [39/120    avg_loss:0.967, val_acc:0.622]
Epoch [40/120    avg_loss:0.730, val_acc:0.682]
Epoch [41/120    avg_loss:0.620, val_acc:0.709]
Epoch [42/120    avg_loss:0.574, val_acc:0.710]
Epoch [43/120    avg_loss:0.479, val_acc:0.799]
Epoch [44/120    avg_loss:0.416, val_acc:0.792]
Epoch [45/120    avg_loss:0.342, val_acc:0.828]
Epoch [46/120    avg_loss:0.307, val_acc:0.788]
Epoch [47/120    avg_loss:0.320, val_acc:0.827]
Epoch [48/120    avg_loss:0.260, val_acc:0.919]
Epoch [49/120    avg_loss:0.230, val_acc:0.934]
Epoch [50/120    avg_loss:0.197, val_acc:0.932]
Epoch [51/120    avg_loss:0.179, val_acc:0.930]
Epoch [52/120    avg_loss:0.164, val_acc:0.928]
Epoch [53/120    avg_loss:0.157, val_acc:0.944]
Epoch [54/120    avg_loss:0.155, val_acc:0.938]
Epoch [55/120    avg_loss:0.159, val_acc:0.945]
Epoch [56/120    avg_loss:0.146, val_acc:0.941]
Epoch [57/120    avg_loss:0.151, val_acc:0.944]
Epoch [58/120    avg_loss:0.158, val_acc:0.947]
Epoch [59/120    avg_loss:0.148, val_acc:0.951]
Epoch [60/120    avg_loss:0.145, val_acc:0.947]
Epoch [61/120    avg_loss:0.163, val_acc:0.953]
Epoch [62/120    avg_loss:0.134, val_acc:0.951]
Epoch [63/120    avg_loss:0.137, val_acc:0.952]
Epoch [64/120    avg_loss:0.140, val_acc:0.952]
Epoch [65/120    avg_loss:0.129, val_acc:0.952]
Epoch [66/120    avg_loss:0.137, val_acc:0.951]
Epoch [67/120    avg_loss:0.136, val_acc:0.951]
Epoch [68/120    avg_loss:0.140, val_acc:0.952]
Epoch [69/120    avg_loss:0.125, val_acc:0.950]
Epoch [70/120    avg_loss:0.138, val_acc:0.951]
Epoch [71/120    avg_loss:0.134, val_acc:0.951]
Epoch [72/120    avg_loss:0.127, val_acc:0.952]
Epoch [73/120    avg_loss:0.123, val_acc:0.951]
Epoch [74/120    avg_loss:0.131, val_acc:0.950]
Epoch [75/120    avg_loss:0.129, val_acc:0.950]
Epoch [76/120    avg_loss:0.130, val_acc:0.951]
Epoch [77/120    avg_loss:0.127, val_acc:0.951]
Epoch [78/120    avg_loss:0.136, val_acc:0.951]
Epoch [79/120    avg_loss:0.119, val_acc:0.951]
Epoch [80/120    avg_loss:0.132, val_acc:0.951]
Epoch [81/120    avg_loss:0.131, val_acc:0.950]
Epoch [82/120    avg_loss:0.134, val_acc:0.950]
Epoch [83/120    avg_loss:0.126, val_acc:0.951]
Epoch [84/120    avg_loss:0.123, val_acc:0.951]
Epoch [85/120    avg_loss:0.138, val_acc:0.951]
Epoch [86/120    avg_loss:0.129, val_acc:0.951]
Epoch [87/120    avg_loss:0.133, val_acc:0.951]
Epoch [88/120    avg_loss:0.133, val_acc:0.951]
Epoch [89/120    avg_loss:0.132, val_acc:0.951]
Epoch [90/120    avg_loss:0.132, val_acc:0.951]
Epoch [91/120    avg_loss:0.137, val_acc:0.951]
Epoch [92/120    avg_loss:0.132, val_acc:0.951]
Epoch [93/120    avg_loss:0.132, val_acc:0.951]
Epoch [94/120    avg_loss:0.125, val_acc:0.951]
Epoch [95/120    avg_loss:0.124, val_acc:0.951]
Epoch [96/120    avg_loss:0.131, val_acc:0.951]
Epoch [97/120    avg_loss:0.126, val_acc:0.951]
Epoch [98/120    avg_loss:0.124, val_acc:0.951]
Epoch [99/120    avg_loss:0.143, val_acc:0.951]
Epoch [100/120    avg_loss:0.128, val_acc:0.951]
Epoch [101/120    avg_loss:0.141, val_acc:0.951]
Epoch [102/120    avg_loss:0.123, val_acc:0.951]
Epoch [103/120    avg_loss:0.130, val_acc:0.951]
Epoch [104/120    avg_loss:0.128, val_acc:0.951]
Epoch [105/120    avg_loss:0.126, val_acc:0.951]
Epoch [106/120    avg_loss:0.127, val_acc:0.951]
Epoch [107/120    avg_loss:0.133, val_acc:0.951]
Epoch [108/120    avg_loss:0.139, val_acc:0.951]
Epoch [109/120    avg_loss:0.137, val_acc:0.951]
Epoch [110/120    avg_loss:0.142, val_acc:0.951]
Epoch [111/120    avg_loss:0.122, val_acc:0.951]
Epoch [112/120    avg_loss:0.133, val_acc:0.951]
Epoch [113/120    avg_loss:0.130, val_acc:0.951]
Epoch [114/120    avg_loss:0.137, val_acc:0.951]
Epoch [115/120    avg_loss:0.127, val_acc:0.951]
Epoch [116/120    avg_loss:0.129, val_acc:0.951]
Epoch [117/120    avg_loss:0.138, val_acc:0.951]
Epoch [118/120    avg_loss:0.129, val_acc:0.951]
Epoch [119/120    avg_loss:0.139, val_acc:0.951]
Epoch [120/120    avg_loss:0.131, val_acc:0.951]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5877     0     0   112     9     9    52   222   151]
 [    0     2 17722     0   145     0   221     0     0     0]
 [    0    17     0  1873     0     0     0     0   132    14]
 [    0    80    23     0  2808     0    33     0    24     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    50     0     1     0  4827     0     0     0]
 [    0    16     0     0     0     0     0  1274     0     0]
 [    0   157     0    50    71     0     1     0  3292     0]
 [    0     8     0     1    12    54     0     0     0   844]]

Accuracy:
95.97281469163474

F1 scores:
[       nan 0.93367225 0.98771074 0.9459596  0.91749714 0.97643098
 0.96840205 0.97400612 0.90926668 0.873706  ]

Kappa:
0.9468500640638883
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff64479fbe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.196, val_acc:0.207]
Epoch [2/120    avg_loss:2.016, val_acc:0.184]
Epoch [3/120    avg_loss:1.877, val_acc:0.195]
Epoch [4/120    avg_loss:1.712, val_acc:0.200]
Epoch [5/120    avg_loss:1.584, val_acc:0.216]
Epoch [6/120    avg_loss:1.480, val_acc:0.257]
Epoch [7/120    avg_loss:1.372, val_acc:0.319]
Epoch [8/120    avg_loss:1.286, val_acc:0.372]
Epoch [9/120    avg_loss:1.231, val_acc:0.398]
Epoch [10/120    avg_loss:1.139, val_acc:0.362]
Epoch [11/120    avg_loss:1.060, val_acc:0.427]
Epoch [12/120    avg_loss:1.014, val_acc:0.447]
Epoch [13/120    avg_loss:0.916, val_acc:0.461]
Epoch [14/120    avg_loss:0.855, val_acc:0.473]
Epoch [15/120    avg_loss:0.775, val_acc:0.544]
Epoch [16/120    avg_loss:0.701, val_acc:0.572]
Epoch [17/120    avg_loss:0.678, val_acc:0.584]
Epoch [18/120    avg_loss:0.635, val_acc:0.667]
Epoch [19/120    avg_loss:0.585, val_acc:0.625]
Epoch [20/120    avg_loss:0.514, val_acc:0.683]
Epoch [21/120    avg_loss:0.489, val_acc:0.741]
Epoch [22/120    avg_loss:0.447, val_acc:0.753]
Epoch [23/120    avg_loss:0.392, val_acc:0.778]
Epoch [24/120    avg_loss:0.384, val_acc:0.791]
Epoch [25/120    avg_loss:0.413, val_acc:0.849]
Epoch [26/120    avg_loss:0.317, val_acc:0.878]
Epoch [27/120    avg_loss:0.278, val_acc:0.928]
Epoch [28/120    avg_loss:0.237, val_acc:0.840]
Epoch [29/120    avg_loss:0.212, val_acc:0.922]
Epoch [30/120    avg_loss:0.207, val_acc:0.932]
Epoch [31/120    avg_loss:0.223, val_acc:0.875]
Epoch [32/120    avg_loss:0.260, val_acc:0.929]
Epoch [33/120    avg_loss:0.197, val_acc:0.951]
Epoch [34/120    avg_loss:0.148, val_acc:0.965]
Epoch [35/120    avg_loss:0.138, val_acc:0.952]
Epoch [36/120    avg_loss:0.110, val_acc:0.953]
Epoch [37/120    avg_loss:0.110, val_acc:0.963]
Epoch [38/120    avg_loss:0.094, val_acc:0.962]
Epoch [39/120    avg_loss:0.108, val_acc:0.964]
Epoch [40/120    avg_loss:0.093, val_acc:0.968]
Epoch [41/120    avg_loss:0.095, val_acc:0.924]
Epoch [42/120    avg_loss:0.096, val_acc:0.963]
Epoch [43/120    avg_loss:0.083, val_acc:0.972]
Epoch [44/120    avg_loss:0.092, val_acc:0.954]
Epoch [45/120    avg_loss:0.100, val_acc:0.978]
Epoch [46/120    avg_loss:0.063, val_acc:0.982]
Epoch [47/120    avg_loss:0.051, val_acc:0.980]
Epoch [48/120    avg_loss:0.045, val_acc:0.981]
Epoch [49/120    avg_loss:0.076, val_acc:0.955]
Epoch [50/120    avg_loss:0.077, val_acc:0.947]
Epoch [51/120    avg_loss:0.061, val_acc:0.973]
Epoch [52/120    avg_loss:0.047, val_acc:0.981]
Epoch [53/120    avg_loss:0.042, val_acc:0.990]
Epoch [54/120    avg_loss:0.032, val_acc:0.987]
Epoch [55/120    avg_loss:0.031, val_acc:0.991]
Epoch [56/120    avg_loss:0.029, val_acc:0.985]
Epoch [57/120    avg_loss:0.032, val_acc:0.986]
Epoch [58/120    avg_loss:0.028, val_acc:0.978]
Epoch [59/120    avg_loss:0.025, val_acc:0.987]
Epoch [60/120    avg_loss:0.028, val_acc:0.976]
Epoch [61/120    avg_loss:0.039, val_acc:0.987]
Epoch [62/120    avg_loss:0.053, val_acc:0.984]
Epoch [63/120    avg_loss:0.066, val_acc:0.968]
Epoch [64/120    avg_loss:0.046, val_acc:0.975]
Epoch [65/120    avg_loss:0.032, val_acc:0.984]
Epoch [66/120    avg_loss:0.026, val_acc:0.986]
Epoch [67/120    avg_loss:0.024, val_acc:0.989]
Epoch [68/120    avg_loss:0.018, val_acc:0.988]
Epoch [69/120    avg_loss:0.016, val_acc:0.989]
Epoch [70/120    avg_loss:0.014, val_acc:0.990]
Epoch [71/120    avg_loss:0.016, val_acc:0.990]
Epoch [72/120    avg_loss:0.017, val_acc:0.991]
Epoch [73/120    avg_loss:0.016, val_acc:0.991]
Epoch [74/120    avg_loss:0.013, val_acc:0.991]
Epoch [75/120    avg_loss:0.020, val_acc:0.991]
Epoch [76/120    avg_loss:0.013, val_acc:0.991]
Epoch [77/120    avg_loss:0.015, val_acc:0.991]
Epoch [78/120    avg_loss:0.015, val_acc:0.991]
Epoch [79/120    avg_loss:0.012, val_acc:0.991]
Epoch [80/120    avg_loss:0.014, val_acc:0.991]
Epoch [81/120    avg_loss:0.012, val_acc:0.991]
Epoch [82/120    avg_loss:0.012, val_acc:0.991]
Epoch [83/120    avg_loss:0.012, val_acc:0.991]
Epoch [84/120    avg_loss:0.013, val_acc:0.991]
Epoch [85/120    avg_loss:0.012, val_acc:0.991]
Epoch [86/120    avg_loss:0.012, val_acc:0.991]
Epoch [87/120    avg_loss:0.012, val_acc:0.991]
Epoch [88/120    avg_loss:0.012, val_acc:0.991]
Epoch [89/120    avg_loss:0.013, val_acc:0.991]
Epoch [90/120    avg_loss:0.014, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.991]
Epoch [92/120    avg_loss:0.013, val_acc:0.991]
Epoch [93/120    avg_loss:0.014, val_acc:0.992]
Epoch [94/120    avg_loss:0.013, val_acc:0.991]
Epoch [95/120    avg_loss:0.011, val_acc:0.991]
Epoch [96/120    avg_loss:0.012, val_acc:0.991]
Epoch [97/120    avg_loss:0.014, val_acc:0.991]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.012, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.990]
Epoch [101/120    avg_loss:0.011, val_acc:0.991]
Epoch [102/120    avg_loss:0.011, val_acc:0.991]
Epoch [103/120    avg_loss:0.012, val_acc:0.991]
Epoch [104/120    avg_loss:0.012, val_acc:0.991]
Epoch [105/120    avg_loss:0.010, val_acc:0.991]
Epoch [106/120    avg_loss:0.012, val_acc:0.991]
Epoch [107/120    avg_loss:0.011, val_acc:0.991]
Epoch [108/120    avg_loss:0.011, val_acc:0.991]
Epoch [109/120    avg_loss:0.010, val_acc:0.991]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.991]
Epoch [112/120    avg_loss:0.011, val_acc:0.991]
Epoch [113/120    avg_loss:0.011, val_acc:0.991]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.011, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.992]
Epoch [117/120    avg_loss:0.012, val_acc:0.992]
Epoch [118/120    avg_loss:0.012, val_acc:0.991]
Epoch [119/120    avg_loss:0.011, val_acc:0.991]
Epoch [120/120    avg_loss:0.010, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     1     0     0     6    11     1]
 [    0     2 18052     0    34     0     2     0     0     0]
 [    0     2     0  1987     3     0     0     0    42     2]
 [    0    17     9     0  2922     0     3     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     5     0     0  4854     0     0     0]
 [    0    16     0     0     0     0     0  1272     0     2]
 [    0    33     0    41    49     0     0     0  3448     0]
 [    0     0     0     0    15    38     0     0     0   866]]

Accuracy:
99.0986431446268

F1 scores:
[       nan 0.99310879 0.99817528 0.97665274 0.97464977 0.98564955
 0.99702167 0.99065421 0.97222614 0.96759777]

Kappa:
0.9880563126210355
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c42a49be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.241, val_acc:0.198]
Epoch [2/120    avg_loss:2.022, val_acc:0.164]
Epoch [3/120    avg_loss:1.873, val_acc:0.156]
Epoch [4/120    avg_loss:1.724, val_acc:0.186]
Epoch [5/120    avg_loss:1.607, val_acc:0.379]
Epoch [6/120    avg_loss:1.513, val_acc:0.450]
Epoch [7/120    avg_loss:1.410, val_acc:0.568]
Epoch [8/120    avg_loss:1.357, val_acc:0.566]
Epoch [9/120    avg_loss:1.230, val_acc:0.516]
Epoch [10/120    avg_loss:1.138, val_acc:0.621]
Epoch [11/120    avg_loss:1.054, val_acc:0.615]
Epoch [12/120    avg_loss:0.989, val_acc:0.565]
Epoch [13/120    avg_loss:0.891, val_acc:0.567]
Epoch [14/120    avg_loss:0.831, val_acc:0.507]
Epoch [15/120    avg_loss:0.766, val_acc:0.537]
Epoch [16/120    avg_loss:0.720, val_acc:0.557]
Epoch [17/120    avg_loss:0.648, val_acc:0.573]
Epoch [18/120    avg_loss:0.612, val_acc:0.604]
Epoch [19/120    avg_loss:0.566, val_acc:0.627]
Epoch [20/120    avg_loss:0.522, val_acc:0.634]
Epoch [21/120    avg_loss:0.492, val_acc:0.652]
Epoch [22/120    avg_loss:0.479, val_acc:0.728]
Epoch [23/120    avg_loss:0.450, val_acc:0.706]
Epoch [24/120    avg_loss:0.406, val_acc:0.744]
Epoch [25/120    avg_loss:0.408, val_acc:0.761]
Epoch [26/120    avg_loss:0.375, val_acc:0.753]
Epoch [27/120    avg_loss:0.361, val_acc:0.801]
Epoch [28/120    avg_loss:0.359, val_acc:0.792]
Epoch [29/120    avg_loss:0.308, val_acc:0.808]
Epoch [30/120    avg_loss:0.276, val_acc:0.815]
Epoch [31/120    avg_loss:0.268, val_acc:0.823]
Epoch [32/120    avg_loss:0.239, val_acc:0.835]
Epoch [33/120    avg_loss:0.227, val_acc:0.903]
Epoch [34/120    avg_loss:0.227, val_acc:0.817]
Epoch [35/120    avg_loss:0.183, val_acc:0.934]
Epoch [36/120    avg_loss:0.199, val_acc:0.905]
Epoch [37/120    avg_loss:0.209, val_acc:0.934]
Epoch [38/120    avg_loss:0.215, val_acc:0.887]
Epoch [39/120    avg_loss:0.183, val_acc:0.927]
Epoch [40/120    avg_loss:0.157, val_acc:0.930]
Epoch [41/120    avg_loss:0.134, val_acc:0.953]
Epoch [42/120    avg_loss:0.118, val_acc:0.944]
Epoch [43/120    avg_loss:0.169, val_acc:0.948]
Epoch [44/120    avg_loss:0.142, val_acc:0.927]
Epoch [45/120    avg_loss:0.137, val_acc:0.958]
Epoch [46/120    avg_loss:0.118, val_acc:0.942]
Epoch [47/120    avg_loss:0.097, val_acc:0.962]
Epoch [48/120    avg_loss:0.083, val_acc:0.974]
Epoch [49/120    avg_loss:0.105, val_acc:0.962]
Epoch [50/120    avg_loss:0.120, val_acc:0.953]
Epoch [51/120    avg_loss:0.103, val_acc:0.962]
Epoch [52/120    avg_loss:0.078, val_acc:0.972]
Epoch [53/120    avg_loss:0.070, val_acc:0.963]
Epoch [54/120    avg_loss:0.069, val_acc:0.970]
Epoch [55/120    avg_loss:0.055, val_acc:0.961]
Epoch [56/120    avg_loss:0.049, val_acc:0.975]
Epoch [57/120    avg_loss:0.038, val_acc:0.980]
Epoch [58/120    avg_loss:0.054, val_acc:0.970]
Epoch [59/120    avg_loss:0.077, val_acc:0.969]
Epoch [60/120    avg_loss:0.057, val_acc:0.971]
Epoch [61/120    avg_loss:0.046, val_acc:0.971]
Epoch [62/120    avg_loss:0.069, val_acc:0.956]
Epoch [63/120    avg_loss:0.050, val_acc:0.973]
Epoch [64/120    avg_loss:0.040, val_acc:0.974]
Epoch [65/120    avg_loss:0.034, val_acc:0.978]
Epoch [66/120    avg_loss:0.041, val_acc:0.976]
Epoch [67/120    avg_loss:0.027, val_acc:0.985]
Epoch [68/120    avg_loss:0.035, val_acc:0.978]
Epoch [69/120    avg_loss:0.038, val_acc:0.982]
Epoch [70/120    avg_loss:0.059, val_acc:0.959]
Epoch [71/120    avg_loss:0.047, val_acc:0.978]
Epoch [72/120    avg_loss:0.067, val_acc:0.968]
Epoch [73/120    avg_loss:0.081, val_acc:0.971]
Epoch [74/120    avg_loss:0.051, val_acc:0.980]
Epoch [75/120    avg_loss:0.038, val_acc:0.971]
Epoch [76/120    avg_loss:0.029, val_acc:0.971]
Epoch [77/120    avg_loss:0.026, val_acc:0.978]
Epoch [78/120    avg_loss:0.033, val_acc:0.983]
Epoch [79/120    avg_loss:0.024, val_acc:0.977]
Epoch [80/120    avg_loss:0.025, val_acc:0.978]
Epoch [81/120    avg_loss:0.016, val_acc:0.982]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.016, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.985]
Epoch [86/120    avg_loss:0.016, val_acc:0.987]
Epoch [87/120    avg_loss:0.014, val_acc:0.983]
Epoch [88/120    avg_loss:0.015, val_acc:0.988]
Epoch [89/120    avg_loss:0.013, val_acc:0.987]
Epoch [90/120    avg_loss:0.013, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.989]
Epoch [92/120    avg_loss:0.013, val_acc:0.987]
Epoch [93/120    avg_loss:0.013, val_acc:0.989]
Epoch [94/120    avg_loss:0.011, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.987]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.013, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.011, val_acc:0.987]
Epoch [101/120    avg_loss:0.012, val_acc:0.988]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.013, val_acc:0.987]
Epoch [104/120    avg_loss:0.012, val_acc:0.986]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.011, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.012, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.989]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6343     0     0     0     0     0    39    50     0]
 [    0     5 18015     0    52     0    14     0     4     0]
 [    0    11     0  1961     0     0     0     0    63     1]
 [    0    33    13     0  2902     0     9     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4859     0     0     0]
 [    0     5     0     0     0     0     0  1280     0     5]
 [    0     5     0    21    36     0     0     0  3509     0]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
98.93235003494566

F1 scores:
[       nan 0.98846813 0.99703905 0.97610752 0.97121821 0.98901099
 0.99569672 0.98121886 0.97310039 0.97279289]

Kappa:
0.9858603379874331
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc11ed4aba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.215, val_acc:0.072]
Epoch [2/120    avg_loss:2.015, val_acc:0.071]
Epoch [3/120    avg_loss:1.887, val_acc:0.090]
Epoch [4/120    avg_loss:1.777, val_acc:0.132]
Epoch [5/120    avg_loss:1.655, val_acc:0.208]
Epoch [6/120    avg_loss:1.503, val_acc:0.225]
Epoch [7/120    avg_loss:1.417, val_acc:0.295]
Epoch [8/120    avg_loss:1.332, val_acc:0.331]
Epoch [9/120    avg_loss:1.250, val_acc:0.378]
Epoch [10/120    avg_loss:1.182, val_acc:0.399]
Epoch [11/120    avg_loss:1.076, val_acc:0.481]
Epoch [12/120    avg_loss:0.999, val_acc:0.484]
Epoch [13/120    avg_loss:0.893, val_acc:0.480]
Epoch [14/120    avg_loss:0.773, val_acc:0.520]
Epoch [15/120    avg_loss:0.678, val_acc:0.565]
Epoch [16/120    avg_loss:0.603, val_acc:0.637]
Epoch [17/120    avg_loss:0.559, val_acc:0.708]
Epoch [18/120    avg_loss:0.536, val_acc:0.741]
Epoch [19/120    avg_loss:0.435, val_acc:0.757]
Epoch [20/120    avg_loss:0.399, val_acc:0.795]
Epoch [21/120    avg_loss:0.395, val_acc:0.770]
Epoch [22/120    avg_loss:0.335, val_acc:0.775]
Epoch [23/120    avg_loss:0.329, val_acc:0.866]
Epoch [24/120    avg_loss:0.304, val_acc:0.875]
Epoch [25/120    avg_loss:0.247, val_acc:0.929]
Epoch [26/120    avg_loss:0.231, val_acc:0.948]
Epoch [27/120    avg_loss:0.198, val_acc:0.929]
Epoch [28/120    avg_loss:0.185, val_acc:0.928]
Epoch [29/120    avg_loss:0.162, val_acc:0.952]
Epoch [30/120    avg_loss:0.149, val_acc:0.937]
Epoch [31/120    avg_loss:0.163, val_acc:0.900]
Epoch [32/120    avg_loss:0.144, val_acc:0.911]
Epoch [33/120    avg_loss:0.145, val_acc:0.962]
Epoch [34/120    avg_loss:0.126, val_acc:0.962]
Epoch [35/120    avg_loss:0.136, val_acc:0.953]
Epoch [36/120    avg_loss:0.105, val_acc:0.969]
Epoch [37/120    avg_loss:0.092, val_acc:0.966]
Epoch [38/120    avg_loss:0.122, val_acc:0.973]
Epoch [39/120    avg_loss:0.098, val_acc:0.947]
Epoch [40/120    avg_loss:0.067, val_acc:0.973]
Epoch [41/120    avg_loss:0.070, val_acc:0.972]
Epoch [42/120    avg_loss:0.062, val_acc:0.972]
Epoch [43/120    avg_loss:0.067, val_acc:0.953]
Epoch [44/120    avg_loss:0.060, val_acc:0.966]
Epoch [45/120    avg_loss:0.065, val_acc:0.964]
Epoch [46/120    avg_loss:0.080, val_acc:0.967]
Epoch [47/120    avg_loss:0.054, val_acc:0.953]
Epoch [48/120    avg_loss:0.062, val_acc:0.973]
Epoch [49/120    avg_loss:0.051, val_acc:0.978]
Epoch [50/120    avg_loss:0.049, val_acc:0.976]
Epoch [51/120    avg_loss:0.043, val_acc:0.976]
Epoch [52/120    avg_loss:0.038, val_acc:0.983]
Epoch [53/120    avg_loss:0.035, val_acc:0.980]
Epoch [54/120    avg_loss:0.039, val_acc:0.973]
Epoch [55/120    avg_loss:0.082, val_acc:0.870]
Epoch [56/120    avg_loss:0.121, val_acc:0.926]
Epoch [57/120    avg_loss:0.077, val_acc:0.972]
Epoch [58/120    avg_loss:0.040, val_acc:0.970]
Epoch [59/120    avg_loss:0.035, val_acc:0.974]
Epoch [60/120    avg_loss:0.035, val_acc:0.974]
Epoch [61/120    avg_loss:0.049, val_acc:0.978]
Epoch [62/120    avg_loss:0.031, val_acc:0.978]
Epoch [63/120    avg_loss:0.030, val_acc:0.974]
Epoch [64/120    avg_loss:0.038, val_acc:0.968]
Epoch [65/120    avg_loss:0.032, val_acc:0.965]
Epoch [66/120    avg_loss:0.032, val_acc:0.982]
Epoch [67/120    avg_loss:0.019, val_acc:0.982]
Epoch [68/120    avg_loss:0.019, val_acc:0.981]
Epoch [69/120    avg_loss:0.020, val_acc:0.980]
Epoch [70/120    avg_loss:0.017, val_acc:0.980]
Epoch [71/120    avg_loss:0.019, val_acc:0.981]
Epoch [72/120    avg_loss:0.018, val_acc:0.982]
Epoch [73/120    avg_loss:0.017, val_acc:0.980]
Epoch [74/120    avg_loss:0.017, val_acc:0.980]
Epoch [75/120    avg_loss:0.017, val_acc:0.981]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.017, val_acc:0.983]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.016, val_acc:0.984]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.016, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.018, val_acc:0.983]
Epoch [87/120    avg_loss:0.016, val_acc:0.983]
Epoch [88/120    avg_loss:0.014, val_acc:0.980]
Epoch [89/120    avg_loss:0.015, val_acc:0.984]
Epoch [90/120    avg_loss:0.014, val_acc:0.984]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.016, val_acc:0.983]
Epoch [94/120    avg_loss:0.015, val_acc:0.984]
Epoch [95/120    avg_loss:0.015, val_acc:0.983]
Epoch [96/120    avg_loss:0.014, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.984]
Epoch [99/120    avg_loss:0.014, val_acc:0.983]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.986]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.015, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.013, val_acc:0.984]
Epoch [108/120    avg_loss:0.015, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.983]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.016, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     0     0     0     0     6    37     2]
 [    0     2 18048     0    33     0     4     0     3     0]
 [    0     8     0  1949     0     0     0     0    77     2]
 [    0    14    11     0  2904     0    25     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     6     0     0  4846     0     5     0]
 [    0     0     0     0     0     0     3  1279     0     8]
 [    0    13     0    50    51     0     0     0  3455     2]
 [    0     0     0     0    11    25     0     0     0   883]]

Accuracy:
98.94681030535271

F1 scores:
[       nan 0.99362166 0.99795411 0.96461272 0.97270139 0.99051233
 0.99343993 0.99339806 0.96427575 0.97246696]

Kappa:
0.9860459274322406
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc90831fc18>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.212, val_acc:0.033]
Epoch [2/120    avg_loss:2.024, val_acc:0.102]
Epoch [3/120    avg_loss:1.884, val_acc:0.189]
Epoch [4/120    avg_loss:1.770, val_acc:0.212]
Epoch [5/120    avg_loss:1.674, val_acc:0.232]
Epoch [6/120    avg_loss:1.533, val_acc:0.239]
Epoch [7/120    avg_loss:1.449, val_acc:0.266]
Epoch [8/120    avg_loss:1.351, val_acc:0.303]
Epoch [9/120    avg_loss:1.279, val_acc:0.336]
Epoch [10/120    avg_loss:1.208, val_acc:0.375]
Epoch [11/120    avg_loss:1.146, val_acc:0.389]
Epoch [12/120    avg_loss:1.062, val_acc:0.396]
Epoch [13/120    avg_loss:0.994, val_acc:0.435]
Epoch [14/120    avg_loss:0.959, val_acc:0.457]
Epoch [15/120    avg_loss:0.919, val_acc:0.453]
Epoch [16/120    avg_loss:0.827, val_acc:0.486]
Epoch [17/120    avg_loss:0.761, val_acc:0.507]
Epoch [18/120    avg_loss:0.706, val_acc:0.502]
Epoch [19/120    avg_loss:0.655, val_acc:0.535]
Epoch [20/120    avg_loss:0.616, val_acc:0.542]
Epoch [21/120    avg_loss:0.561, val_acc:0.645]
Epoch [22/120    avg_loss:0.540, val_acc:0.697]
Epoch [23/120    avg_loss:0.517, val_acc:0.734]
Epoch [24/120    avg_loss:0.460, val_acc:0.743]
Epoch [25/120    avg_loss:0.398, val_acc:0.796]
Epoch [26/120    avg_loss:0.372, val_acc:0.753]
Epoch [27/120    avg_loss:0.383, val_acc:0.809]
Epoch [28/120    avg_loss:0.349, val_acc:0.794]
Epoch [29/120    avg_loss:0.317, val_acc:0.811]
Epoch [30/120    avg_loss:0.294, val_acc:0.802]
Epoch [31/120    avg_loss:0.289, val_acc:0.786]
Epoch [32/120    avg_loss:0.257, val_acc:0.828]
Epoch [33/120    avg_loss:0.244, val_acc:0.877]
Epoch [34/120    avg_loss:0.232, val_acc:0.865]
Epoch [35/120    avg_loss:0.295, val_acc:0.880]
Epoch [36/120    avg_loss:0.265, val_acc:0.896]
Epoch [37/120    avg_loss:0.189, val_acc:0.922]
Epoch [38/120    avg_loss:0.184, val_acc:0.914]
Epoch [39/120    avg_loss:0.151, val_acc:0.916]
Epoch [40/120    avg_loss:0.157, val_acc:0.941]
Epoch [41/120    avg_loss:0.147, val_acc:0.950]
Epoch [42/120    avg_loss:0.143, val_acc:0.954]
Epoch [43/120    avg_loss:0.124, val_acc:0.950]
Epoch [44/120    avg_loss:0.127, val_acc:0.952]
Epoch [45/120    avg_loss:0.110, val_acc:0.947]
Epoch [46/120    avg_loss:0.098, val_acc:0.953]
Epoch [47/120    avg_loss:0.102, val_acc:0.961]
Epoch [48/120    avg_loss:0.097, val_acc:0.964]
Epoch [49/120    avg_loss:0.088, val_acc:0.958]
Epoch [50/120    avg_loss:0.095, val_acc:0.958]
Epoch [51/120    avg_loss:0.097, val_acc:0.928]
Epoch [52/120    avg_loss:0.099, val_acc:0.942]
Epoch [53/120    avg_loss:0.105, val_acc:0.963]
Epoch [54/120    avg_loss:0.105, val_acc:0.953]
Epoch [55/120    avg_loss:0.071, val_acc:0.976]
Epoch [56/120    avg_loss:0.070, val_acc:0.969]
Epoch [57/120    avg_loss:0.061, val_acc:0.958]
Epoch [58/120    avg_loss:0.068, val_acc:0.972]
Epoch [59/120    avg_loss:0.052, val_acc:0.979]
Epoch [60/120    avg_loss:0.048, val_acc:0.970]
Epoch [61/120    avg_loss:0.048, val_acc:0.972]
Epoch [62/120    avg_loss:0.053, val_acc:0.972]
Epoch [63/120    avg_loss:0.046, val_acc:0.970]
Epoch [64/120    avg_loss:0.039, val_acc:0.972]
Epoch [65/120    avg_loss:0.039, val_acc:0.975]
Epoch [66/120    avg_loss:0.049, val_acc:0.979]
Epoch [67/120    avg_loss:0.047, val_acc:0.971]
Epoch [68/120    avg_loss:0.037, val_acc:0.974]
Epoch [69/120    avg_loss:0.033, val_acc:0.975]
Epoch [70/120    avg_loss:0.032, val_acc:0.981]
Epoch [71/120    avg_loss:0.033, val_acc:0.972]
Epoch [72/120    avg_loss:0.035, val_acc:0.969]
Epoch [73/120    avg_loss:0.045, val_acc:0.972]
Epoch [74/120    avg_loss:0.059, val_acc:0.971]
Epoch [75/120    avg_loss:0.049, val_acc:0.974]
Epoch [76/120    avg_loss:0.039, val_acc:0.972]
Epoch [77/120    avg_loss:0.033, val_acc:0.978]
Epoch [78/120    avg_loss:0.027, val_acc:0.982]
Epoch [79/120    avg_loss:0.022, val_acc:0.980]
Epoch [80/120    avg_loss:0.028, val_acc:0.978]
Epoch [81/120    avg_loss:0.025, val_acc:0.982]
Epoch [82/120    avg_loss:0.025, val_acc:0.979]
Epoch [83/120    avg_loss:0.024, val_acc:0.977]
Epoch [84/120    avg_loss:0.024, val_acc:0.982]
Epoch [85/120    avg_loss:0.022, val_acc:0.983]
Epoch [86/120    avg_loss:0.018, val_acc:0.980]
Epoch [87/120    avg_loss:0.020, val_acc:0.955]
Epoch [88/120    avg_loss:0.037, val_acc:0.970]
Epoch [89/120    avg_loss:0.033, val_acc:0.984]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.025, val_acc:0.985]
Epoch [92/120    avg_loss:0.021, val_acc:0.981]
Epoch [93/120    avg_loss:0.022, val_acc:0.979]
Epoch [94/120    avg_loss:0.019, val_acc:0.978]
Epoch [95/120    avg_loss:0.018, val_acc:0.985]
Epoch [96/120    avg_loss:0.017, val_acc:0.978]
Epoch [97/120    avg_loss:0.019, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.986]
Epoch [99/120    avg_loss:0.015, val_acc:0.973]
Epoch [100/120    avg_loss:0.014, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.987]
Epoch [106/120    avg_loss:0.014, val_acc:0.984]
Epoch [107/120    avg_loss:0.022, val_acc:0.972]
Epoch [108/120    avg_loss:0.028, val_acc:0.971]
Epoch [109/120    avg_loss:0.019, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.017, val_acc:0.977]
Epoch [113/120    avg_loss:0.014, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.015, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     0     1     0     0    11    53    13]
 [    0     0 18060     0     0     0    30     0     0     0]
 [    0     6     0  1895     0     0     0     0   132     3]
 [    0     8    16     0  2920     0     2     0    14    12]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     0     0     0  4861     0     0     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    29     0    14    49     0     0     0  3472     7]
 [    0     1     0     0     4    49     0     0     1   864]]

Accuracy:
98.86004868291037

F1 scores:
[       nan 0.99049104 0.99825885 0.96070976 0.98217289 0.98157202
 0.99498516 0.9953668  0.95871876 0.94997251]

Kappa:
0.9848940708070951
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd20a7deb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.199, val_acc:0.172]
Epoch [2/120    avg_loss:1.980, val_acc:0.202]
Epoch [3/120    avg_loss:1.865, val_acc:0.229]
Epoch [4/120    avg_loss:1.744, val_acc:0.264]
Epoch [5/120    avg_loss:1.648, val_acc:0.278]
Epoch [6/120    avg_loss:1.525, val_acc:0.286]
Epoch [7/120    avg_loss:1.415, val_acc:0.318]
Epoch [8/120    avg_loss:1.355, val_acc:0.360]
Epoch [9/120    avg_loss:1.250, val_acc:0.413]
Epoch [10/120    avg_loss:1.193, val_acc:0.432]
Epoch [11/120    avg_loss:1.099, val_acc:0.453]
Epoch [12/120    avg_loss:1.055, val_acc:0.473]
Epoch [13/120    avg_loss:0.974, val_acc:0.477]
Epoch [14/120    avg_loss:0.897, val_acc:0.498]
Epoch [15/120    avg_loss:0.811, val_acc:0.536]
Epoch [16/120    avg_loss:0.723, val_acc:0.563]
Epoch [17/120    avg_loss:0.685, val_acc:0.597]
Epoch [18/120    avg_loss:0.666, val_acc:0.624]
Epoch [19/120    avg_loss:0.578, val_acc:0.680]
Epoch [20/120    avg_loss:0.498, val_acc:0.768]
Epoch [21/120    avg_loss:0.452, val_acc:0.790]
Epoch [22/120    avg_loss:0.439, val_acc:0.807]
Epoch [23/120    avg_loss:0.362, val_acc:0.826]
Epoch [24/120    avg_loss:0.373, val_acc:0.822]
Epoch [25/120    avg_loss:0.353, val_acc:0.812]
Epoch [26/120    avg_loss:0.320, val_acc:0.840]
Epoch [27/120    avg_loss:0.300, val_acc:0.900]
Epoch [28/120    avg_loss:0.246, val_acc:0.925]
Epoch [29/120    avg_loss:0.218, val_acc:0.916]
Epoch [30/120    avg_loss:0.201, val_acc:0.946]
Epoch [31/120    avg_loss:0.174, val_acc:0.953]
Epoch [32/120    avg_loss:0.173, val_acc:0.938]
Epoch [33/120    avg_loss:0.159, val_acc:0.953]
Epoch [34/120    avg_loss:0.149, val_acc:0.964]
Epoch [35/120    avg_loss:0.116, val_acc:0.957]
Epoch [36/120    avg_loss:0.147, val_acc:0.960]
Epoch [37/120    avg_loss:0.164, val_acc:0.947]
Epoch [38/120    avg_loss:0.158, val_acc:0.957]
Epoch [39/120    avg_loss:0.125, val_acc:0.955]
Epoch [40/120    avg_loss:0.106, val_acc:0.972]
Epoch [41/120    avg_loss:0.093, val_acc:0.975]
Epoch [42/120    avg_loss:0.085, val_acc:0.969]
Epoch [43/120    avg_loss:0.078, val_acc:0.975]
Epoch [44/120    avg_loss:0.084, val_acc:0.973]
Epoch [45/120    avg_loss:0.074, val_acc:0.969]
Epoch [46/120    avg_loss:0.060, val_acc:0.966]
Epoch [47/120    avg_loss:0.047, val_acc:0.982]
Epoch [48/120    avg_loss:0.047, val_acc:0.973]
Epoch [49/120    avg_loss:0.051, val_acc:0.965]
Epoch [50/120    avg_loss:0.049, val_acc:0.984]
Epoch [51/120    avg_loss:0.039, val_acc:0.984]
Epoch [52/120    avg_loss:0.042, val_acc:0.984]
Epoch [53/120    avg_loss:0.044, val_acc:0.981]
Epoch [54/120    avg_loss:0.034, val_acc:0.986]
Epoch [55/120    avg_loss:0.034, val_acc:0.985]
Epoch [56/120    avg_loss:0.037, val_acc:0.974]
Epoch [57/120    avg_loss:0.033, val_acc:0.973]
Epoch [58/120    avg_loss:0.067, val_acc:0.976]
Epoch [59/120    avg_loss:0.051, val_acc:0.974]
Epoch [60/120    avg_loss:0.043, val_acc:0.982]
Epoch [61/120    avg_loss:0.028, val_acc:0.984]
Epoch [62/120    avg_loss:0.023, val_acc:0.982]
Epoch [63/120    avg_loss:0.022, val_acc:0.982]
Epoch [64/120    avg_loss:0.024, val_acc:0.948]
Epoch [65/120    avg_loss:0.021, val_acc:0.978]
Epoch [66/120    avg_loss:0.021, val_acc:0.984]
Epoch [67/120    avg_loss:0.017, val_acc:0.983]
Epoch [68/120    avg_loss:0.016, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.988]
Epoch [70/120    avg_loss:0.013, val_acc:0.988]
Epoch [71/120    avg_loss:0.016, val_acc:0.987]
Epoch [72/120    avg_loss:0.013, val_acc:0.987]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.013, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.987]
Epoch [76/120    avg_loss:0.014, val_acc:0.985]
Epoch [77/120    avg_loss:0.014, val_acc:0.988]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.986]
Epoch [80/120    avg_loss:0.012, val_acc:0.987]
Epoch [81/120    avg_loss:0.013, val_acc:0.989]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.987]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.015, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.987]
Epoch [87/120    avg_loss:0.012, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.987]
Epoch [89/120    avg_loss:0.016, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.987]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.014, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.013, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.013, val_acc:0.987]
Epoch [101/120    avg_loss:0.013, val_acc:0.986]
Epoch [102/120    avg_loss:0.015, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.988]
Epoch [104/120    avg_loss:0.012, val_acc:0.987]
Epoch [105/120    avg_loss:0.011, val_acc:0.989]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.989]
Epoch [108/120    avg_loss:0.010, val_acc:0.989]
Epoch [109/120    avg_loss:0.014, val_acc:0.989]
Epoch [110/120    avg_loss:0.010, val_acc:0.987]
Epoch [111/120    avg_loss:0.011, val_acc:0.987]
Epoch [112/120    avg_loss:0.013, val_acc:0.987]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.011, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.013, val_acc:0.987]
Epoch [118/120    avg_loss:0.011, val_acc:0.987]
Epoch [119/120    avg_loss:0.012, val_acc:0.987]
Epoch [120/120    avg_loss:0.012, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6423     0     0     0     0     0     0     7     2]
 [    0     0 18006     0    44     0    40     0     0     0]
 [    0    10     0  1947     0     0     0     0    79     0]
 [    0    30    23     0  2885     0     8     0    25     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    33     5     0     0  4840     0     0     0]
 [    0    13     0     0     0     0     0  1272     0     5]
 [    0    12     0    20    56     0     0     1  3481     1]
 [    0     0     0     0    14    26     0     0     0   879]]

Accuracy:
98.90342949413154

F1 scores:
[       nan 0.99427245 0.99612746 0.97155689 0.9663373  0.99013657
 0.99119394 0.99258681 0.97193913 0.97288323]

Kappa:
0.9854707748269025
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92d3ea9b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.236, val_acc:0.034]
Epoch [2/120    avg_loss:2.039, val_acc:0.067]
Epoch [3/120    avg_loss:1.897, val_acc:0.087]
Epoch [4/120    avg_loss:1.803, val_acc:0.141]
Epoch [5/120    avg_loss:1.691, val_acc:0.166]
Epoch [6/120    avg_loss:1.578, val_acc:0.185]
Epoch [7/120    avg_loss:1.494, val_acc:0.322]
Epoch [8/120    avg_loss:1.354, val_acc:0.377]
Epoch [9/120    avg_loss:1.281, val_acc:0.405]
Epoch [10/120    avg_loss:1.170, val_acc:0.399]
Epoch [11/120    avg_loss:1.097, val_acc:0.389]
Epoch [12/120    avg_loss:0.995, val_acc:0.475]
Epoch [13/120    avg_loss:0.907, val_acc:0.482]
Epoch [14/120    avg_loss:0.838, val_acc:0.501]
Epoch [15/120    avg_loss:0.803, val_acc:0.507]
Epoch [16/120    avg_loss:0.677, val_acc:0.542]
Epoch [17/120    avg_loss:0.625, val_acc:0.587]
Epoch [18/120    avg_loss:0.598, val_acc:0.759]
Epoch [19/120    avg_loss:0.496, val_acc:0.709]
Epoch [20/120    avg_loss:0.480, val_acc:0.757]
Epoch [21/120    avg_loss:0.434, val_acc:0.770]
Epoch [22/120    avg_loss:0.390, val_acc:0.853]
Epoch [23/120    avg_loss:0.343, val_acc:0.875]
Epoch [24/120    avg_loss:0.320, val_acc:0.865]
Epoch [25/120    avg_loss:0.275, val_acc:0.894]
Epoch [26/120    avg_loss:0.274, val_acc:0.914]
Epoch [27/120    avg_loss:0.246, val_acc:0.921]
Epoch [28/120    avg_loss:0.230, val_acc:0.922]
Epoch [29/120    avg_loss:0.283, val_acc:0.878]
Epoch [30/120    avg_loss:0.383, val_acc:0.820]
Epoch [31/120    avg_loss:0.239, val_acc:0.907]
Epoch [32/120    avg_loss:0.222, val_acc:0.898]
Epoch [33/120    avg_loss:0.165, val_acc:0.947]
Epoch [34/120    avg_loss:0.161, val_acc:0.924]
Epoch [35/120    avg_loss:0.174, val_acc:0.942]
Epoch [36/120    avg_loss:0.129, val_acc:0.953]
Epoch [37/120    avg_loss:0.132, val_acc:0.967]
Epoch [38/120    avg_loss:0.118, val_acc:0.969]
Epoch [39/120    avg_loss:0.138, val_acc:0.924]
Epoch [40/120    avg_loss:0.161, val_acc:0.931]
Epoch [41/120    avg_loss:0.098, val_acc:0.972]
Epoch [42/120    avg_loss:0.097, val_acc:0.957]
Epoch [43/120    avg_loss:0.122, val_acc:0.971]
Epoch [44/120    avg_loss:0.101, val_acc:0.970]
Epoch [45/120    avg_loss:0.076, val_acc:0.945]
Epoch [46/120    avg_loss:0.076, val_acc:0.972]
Epoch [47/120    avg_loss:0.062, val_acc:0.977]
Epoch [48/120    avg_loss:0.061, val_acc:0.983]
Epoch [49/120    avg_loss:0.060, val_acc:0.949]
Epoch [50/120    avg_loss:0.068, val_acc:0.978]
Epoch [51/120    avg_loss:0.060, val_acc:0.966]
Epoch [52/120    avg_loss:0.051, val_acc:0.978]
Epoch [53/120    avg_loss:0.038, val_acc:0.972]
Epoch [54/120    avg_loss:0.050, val_acc:0.981]
Epoch [55/120    avg_loss:0.038, val_acc:0.965]
Epoch [56/120    avg_loss:0.061, val_acc:0.950]
Epoch [57/120    avg_loss:0.054, val_acc:0.972]
Epoch [58/120    avg_loss:0.054, val_acc:0.985]
Epoch [59/120    avg_loss:0.048, val_acc:0.976]
Epoch [60/120    avg_loss:0.039, val_acc:0.973]
Epoch [61/120    avg_loss:0.049, val_acc:0.966]
Epoch [62/120    avg_loss:0.040, val_acc:0.968]
Epoch [63/120    avg_loss:0.034, val_acc:0.979]
Epoch [64/120    avg_loss:0.032, val_acc:0.978]
Epoch [65/120    avg_loss:0.042, val_acc:0.986]
Epoch [66/120    avg_loss:0.036, val_acc:0.991]
Epoch [67/120    avg_loss:0.030, val_acc:0.987]
Epoch [68/120    avg_loss:0.048, val_acc:0.975]
Epoch [69/120    avg_loss:0.058, val_acc:0.984]
Epoch [70/120    avg_loss:0.048, val_acc:0.976]
Epoch [71/120    avg_loss:0.046, val_acc:0.973]
Epoch [72/120    avg_loss:0.044, val_acc:0.982]
Epoch [73/120    avg_loss:1.081, val_acc:0.490]
Epoch [74/120    avg_loss:1.197, val_acc:0.526]
Epoch [75/120    avg_loss:1.151, val_acc:0.548]
Epoch [76/120    avg_loss:1.073, val_acc:0.566]
Epoch [77/120    avg_loss:1.035, val_acc:0.580]
Epoch [78/120    avg_loss:0.993, val_acc:0.588]
Epoch [79/120    avg_loss:0.934, val_acc:0.566]
Epoch [80/120    avg_loss:0.929, val_acc:0.606]
Epoch [81/120    avg_loss:0.909, val_acc:0.602]
Epoch [82/120    avg_loss:0.904, val_acc:0.608]
Epoch [83/120    avg_loss:0.894, val_acc:0.609]
Epoch [84/120    avg_loss:0.878, val_acc:0.621]
Epoch [85/120    avg_loss:0.872, val_acc:0.634]
Epoch [86/120    avg_loss:0.866, val_acc:0.641]
Epoch [87/120    avg_loss:0.863, val_acc:0.626]
Epoch [88/120    avg_loss:0.886, val_acc:0.657]
Epoch [89/120    avg_loss:0.874, val_acc:0.641]
Epoch [90/120    avg_loss:0.867, val_acc:0.634]
Epoch [91/120    avg_loss:0.839, val_acc:0.658]
Epoch [92/120    avg_loss:0.857, val_acc:0.654]
Epoch [93/120    avg_loss:0.826, val_acc:0.656]
Epoch [94/120    avg_loss:0.824, val_acc:0.659]
Epoch [95/120    avg_loss:0.818, val_acc:0.659]
Epoch [96/120    avg_loss:0.851, val_acc:0.659]
Epoch [97/120    avg_loss:0.836, val_acc:0.659]
Epoch [98/120    avg_loss:0.836, val_acc:0.661]
Epoch [99/120    avg_loss:0.809, val_acc:0.659]
Epoch [100/120    avg_loss:0.823, val_acc:0.658]
Epoch [101/120    avg_loss:0.821, val_acc:0.659]
Epoch [102/120    avg_loss:0.841, val_acc:0.658]
Epoch [103/120    avg_loss:0.817, val_acc:0.661]
Epoch [104/120    avg_loss:0.824, val_acc:0.661]
Epoch [105/120    avg_loss:0.830, val_acc:0.661]
Epoch [106/120    avg_loss:0.831, val_acc:0.661]
Epoch [107/120    avg_loss:0.825, val_acc:0.660]
Epoch [108/120    avg_loss:0.836, val_acc:0.663]
Epoch [109/120    avg_loss:0.827, val_acc:0.661]
Epoch [110/120    avg_loss:0.830, val_acc:0.662]
Epoch [111/120    avg_loss:0.812, val_acc:0.661]
Epoch [112/120    avg_loss:0.829, val_acc:0.661]
Epoch [113/120    avg_loss:0.816, val_acc:0.661]
Epoch [114/120    avg_loss:0.810, val_acc:0.661]
Epoch [115/120    avg_loss:0.816, val_acc:0.661]
Epoch [116/120    avg_loss:0.827, val_acc:0.661]
Epoch [117/120    avg_loss:0.818, val_acc:0.661]
Epoch [118/120    avg_loss:0.817, val_acc:0.661]
Epoch [119/120    avg_loss:0.828, val_acc:0.661]
Epoch [120/120    avg_loss:0.822, val_acc:0.661]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  3286  1114     0    56     0  1067   437   185   287]
 [    0     0 14401     0   525     0  3164     0     0     0]
 [    0    32     6  1756     5     0    60     0    36   141]
 [    0     7  1012     0  1830     0   116     0     0     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     9  1129   183   361     0  3101     0    95     0]
 [    0   126     1     0    39     0     0  1076    22    26]
 [    0   601   498   304    84     0   156     0  1927     1]
 [    0    18    11     0     7    55     6     0     0   822]]

Accuracy:
71.10596968163304

F1 scores:
[       nan 0.62524974 0.794275   0.82075251 0.62255486 0.9793621
 0.49426203 0.76774884 0.66038382 0.74625511]

Kappa:
0.6182449992930167
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa7d3f21b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 14170==>0.01M
----------Training process----------
Epoch [1/120    avg_loss:2.252, val_acc:0.191]
Epoch [2/120    avg_loss:2.048, val_acc:0.272]
Epoch [3/120    avg_loss:1.887, val_acc:0.310]
Epoch [4/120    avg_loss:1.733, val_acc:0.280]
Epoch [5/120    avg_loss:1.626, val_acc:0.273]
Epoch [6/120    avg_loss:1.512, val_acc:0.247]
Epoch [7/120    avg_loss:1.422, val_acc:0.268]
Epoch [8/120    avg_loss:1.322, val_acc:0.263]
Epoch [9/120    avg_loss:1.206, val_acc:0.292]
Epoch [10/120    avg_loss:1.137, val_acc:0.393]
Epoch [11/120    avg_loss:1.052, val_acc:0.506]
Epoch [12/120    avg_loss:1.000, val_acc:0.605]
Epoch [13/120    avg_loss:0.903, val_acc:0.636]
Epoch [14/120    avg_loss:0.850, val_acc:0.681]
Epoch [15/120    avg_loss:0.801, val_acc:0.723]
Epoch [16/120    avg_loss:0.739, val_acc:0.726]
Epoch [17/120    avg_loss:0.694, val_acc:0.800]
Epoch [18/120    avg_loss:0.643, val_acc:0.807]
Epoch [19/120    avg_loss:0.593, val_acc:0.781]
Epoch [20/120    avg_loss:0.515, val_acc:0.797]
Epoch [21/120    avg_loss:0.472, val_acc:0.847]
Epoch [22/120    avg_loss:0.451, val_acc:0.817]
Epoch [23/120    avg_loss:0.395, val_acc:0.876]
Epoch [24/120    avg_loss:0.421, val_acc:0.862]
Epoch [25/120    avg_loss:0.361, val_acc:0.938]
Epoch [26/120    avg_loss:0.304, val_acc:0.917]
Epoch [27/120    avg_loss:0.261, val_acc:0.932]
Epoch [28/120    avg_loss:0.242, val_acc:0.926]
Epoch [29/120    avg_loss:0.209, val_acc:0.946]
Epoch [30/120    avg_loss:0.183, val_acc:0.933]
Epoch [31/120    avg_loss:0.184, val_acc:0.950]
Epoch [32/120    avg_loss:0.159, val_acc:0.925]
Epoch [33/120    avg_loss:0.152, val_acc:0.961]
Epoch [34/120    avg_loss:0.421, val_acc:0.333]
Epoch [35/120    avg_loss:1.644, val_acc:0.610]
Epoch [36/120    avg_loss:1.326, val_acc:0.627]
Epoch [37/120    avg_loss:1.206, val_acc:0.642]
Epoch [38/120    avg_loss:1.122, val_acc:0.663]
Epoch [39/120    avg_loss:1.096, val_acc:0.662]
Epoch [40/120    avg_loss:1.024, val_acc:0.678]
Epoch [41/120    avg_loss:0.992, val_acc:0.700]
Epoch [42/120    avg_loss:0.926, val_acc:0.708]
Epoch [43/120    avg_loss:0.907, val_acc:0.711]
Epoch [44/120    avg_loss:0.860, val_acc:0.698]
Epoch [45/120    avg_loss:0.870, val_acc:0.552]
Epoch [46/120    avg_loss:0.816, val_acc:0.646]
Epoch [47/120    avg_loss:0.793, val_acc:0.698]
Epoch [48/120    avg_loss:0.783, val_acc:0.703]
Epoch [49/120    avg_loss:0.807, val_acc:0.693]
Epoch [50/120    avg_loss:0.797, val_acc:0.693]
Epoch [51/120    avg_loss:0.781, val_acc:0.703]
Epoch [52/120    avg_loss:0.776, val_acc:0.702]
Epoch [53/120    avg_loss:0.792, val_acc:0.698]
Epoch [54/120    avg_loss:0.785, val_acc:0.692]
Epoch [55/120    avg_loss:0.779, val_acc:0.699]
Epoch [56/120    avg_loss:0.768, val_acc:0.693]
Epoch [57/120    avg_loss:0.769, val_acc:0.688]
Epoch [58/120    avg_loss:0.756, val_acc:0.707]
Epoch [59/120    avg_loss:0.765, val_acc:0.690]
Epoch [60/120    avg_loss:0.749, val_acc:0.688]
Epoch [61/120    avg_loss:0.749, val_acc:0.692]
Epoch [62/120    avg_loss:0.757, val_acc:0.692]
Epoch [63/120    avg_loss:0.742, val_acc:0.693]
Epoch [64/120    avg_loss:0.755, val_acc:0.692]
Epoch [65/120    avg_loss:0.762, val_acc:0.692]
Epoch [66/120    avg_loss:0.757, val_acc:0.691]
Epoch [67/120    avg_loss:0.737, val_acc:0.695]
Epoch [68/120    avg_loss:0.758, val_acc:0.692]
Epoch [69/120    avg_loss:0.745, val_acc:0.690]
Epoch [70/120    avg_loss:0.745, val_acc:0.693]
Epoch [71/120    avg_loss:0.757, val_acc:0.691]
Epoch [72/120    avg_loss:0.759, val_acc:0.693]
Epoch [73/120    avg_loss:0.759, val_acc:0.693]
Epoch [74/120    avg_loss:0.745, val_acc:0.693]
Epoch [75/120    avg_loss:0.755, val_acc:0.693]
Epoch [76/120    avg_loss:0.768, val_acc:0.694]
Epoch [77/120    avg_loss:0.761, val_acc:0.694]
Epoch [78/120    avg_loss:0.772, val_acc:0.694]
Epoch [79/120    avg_loss:0.758, val_acc:0.694]
Epoch [80/120    avg_loss:0.762, val_acc:0.694]
Epoch [81/120    avg_loss:0.756, val_acc:0.694]
Epoch [82/120    avg_loss:0.750, val_acc:0.694]
Epoch [83/120    avg_loss:0.746, val_acc:0.694]
Epoch [84/120    avg_loss:0.770, val_acc:0.694]
Epoch [85/120    avg_loss:0.746, val_acc:0.694]
Epoch [86/120    avg_loss:0.740, val_acc:0.694]
Epoch [87/120    avg_loss:0.739, val_acc:0.694]
Epoch [88/120    avg_loss:0.749, val_acc:0.694]
Epoch [89/120    avg_loss:0.751, val_acc:0.694]
Epoch [90/120    avg_loss:0.747, val_acc:0.694]
Epoch [91/120    avg_loss:0.746, val_acc:0.694]
Epoch [92/120    avg_loss:0.739, val_acc:0.694]
Epoch [93/120    avg_loss:0.759, val_acc:0.694]
Epoch [94/120    avg_loss:0.768, val_acc:0.694]
Epoch [95/120    avg_loss:0.760, val_acc:0.694]
Epoch [96/120    avg_loss:0.759, val_acc:0.694]
Epoch [97/120    avg_loss:0.752, val_acc:0.694]
Epoch [98/120    avg_loss:0.743, val_acc:0.694]
Epoch [99/120    avg_loss:0.760, val_acc:0.694]
Epoch [100/120    avg_loss:0.751, val_acc:0.694]
Epoch [101/120    avg_loss:0.754, val_acc:0.694]
Epoch [102/120    avg_loss:0.733, val_acc:0.694]
Epoch [103/120    avg_loss:0.768, val_acc:0.694]
Epoch [104/120    avg_loss:0.775, val_acc:0.694]
Epoch [105/120    avg_loss:0.762, val_acc:0.694]
Epoch [106/120    avg_loss:0.755, val_acc:0.694]
Epoch [107/120    avg_loss:0.764, val_acc:0.694]
Epoch [108/120    avg_loss:0.764, val_acc:0.694]
Epoch [109/120    avg_loss:0.765, val_acc:0.694]
Epoch [110/120    avg_loss:0.744, val_acc:0.694]
Epoch [111/120    avg_loss:0.750, val_acc:0.694]
Epoch [112/120    avg_loss:0.752, val_acc:0.694]
Epoch [113/120    avg_loss:0.746, val_acc:0.694]
Epoch [114/120    avg_loss:0.756, val_acc:0.694]
Epoch [115/120    avg_loss:0.744, val_acc:0.694]
Epoch [116/120    avg_loss:0.764, val_acc:0.694]
Epoch [117/120    avg_loss:0.751, val_acc:0.694]
Epoch [118/120    avg_loss:0.759, val_acc:0.694]
Epoch [119/120    avg_loss:0.753, val_acc:0.694]
Epoch [120/120    avg_loss:0.747, val_acc:0.694]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4423   288    40   226     0   355    36   638   426]
 [    0     0 15466     0   190     0  2434     0     0     0]
 [    0    20     2  1182   110     0    66     1   517   138]
 [    0    48   146    10  2127     0   482     0   150     9]
 [    0     3     0     0     0  1300     0     0     2     0]
 [    0    19  1920     2     0     0  2770     0   167     0]
 [    0   119     0    17    54     0     4  1089     7     0]
 [    0   140     2   289     3     0   208     1  2926     2]
 [    0   207     0     1    14    12     0     4     2   679]]

Accuracy:
77.02986045839057

F1 scores:
[       nan 0.7752169  0.86127972 0.66088901 0.74683989 0.99350401
 0.49477539 0.89962825 0.73333333 0.62494248]

Kappa:
0.6969771912360855
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f498ecf4b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.259, val_acc:0.134]
Epoch [2/120    avg_loss:2.117, val_acc:0.158]
Epoch [3/120    avg_loss:1.969, val_acc:0.212]
Epoch [4/120    avg_loss:1.840, val_acc:0.203]
Epoch [5/120    avg_loss:1.742, val_acc:0.208]
Epoch [6/120    avg_loss:1.653, val_acc:0.320]
Epoch [7/120    avg_loss:1.564, val_acc:0.495]
Epoch [8/120    avg_loss:1.437, val_acc:0.538]
Epoch [9/120    avg_loss:1.347, val_acc:0.559]
Epoch [10/120    avg_loss:1.258, val_acc:0.612]
Epoch [11/120    avg_loss:1.165, val_acc:0.505]
Epoch [12/120    avg_loss:1.071, val_acc:0.553]
Epoch [13/120    avg_loss:0.998, val_acc:0.553]
Epoch [14/120    avg_loss:0.916, val_acc:0.558]
Epoch [15/120    avg_loss:0.827, val_acc:0.614]
Epoch [16/120    avg_loss:0.773, val_acc:0.628]
Epoch [17/120    avg_loss:0.681, val_acc:0.669]
Epoch [18/120    avg_loss:0.633, val_acc:0.663]
Epoch [19/120    avg_loss:0.559, val_acc:0.685]
Epoch [20/120    avg_loss:0.502, val_acc:0.707]
Epoch [21/120    avg_loss:0.458, val_acc:0.758]
Epoch [22/120    avg_loss:0.415, val_acc:0.783]
Epoch [23/120    avg_loss:0.388, val_acc:0.786]
Epoch [24/120    avg_loss:0.397, val_acc:0.822]
Epoch [25/120    avg_loss:0.316, val_acc:0.847]
Epoch [26/120    avg_loss:0.295, val_acc:0.850]
Epoch [27/120    avg_loss:0.294, val_acc:0.907]
Epoch [28/120    avg_loss:0.244, val_acc:0.938]
Epoch [29/120    avg_loss:0.234, val_acc:0.948]
Epoch [30/120    avg_loss:0.193, val_acc:0.941]
Epoch [31/120    avg_loss:0.207, val_acc:0.935]
Epoch [32/120    avg_loss:0.178, val_acc:0.947]
Epoch [33/120    avg_loss:0.151, val_acc:0.955]
Epoch [34/120    avg_loss:0.137, val_acc:0.948]
Epoch [35/120    avg_loss:0.142, val_acc:0.945]
Epoch [36/120    avg_loss:0.139, val_acc:0.960]
Epoch [37/120    avg_loss:0.133, val_acc:0.956]
Epoch [38/120    avg_loss:0.098, val_acc:0.970]
Epoch [39/120    avg_loss:0.087, val_acc:0.949]
Epoch [40/120    avg_loss:0.087, val_acc:0.971]
Epoch [41/120    avg_loss:0.093, val_acc:0.956]
Epoch [42/120    avg_loss:0.114, val_acc:0.966]
Epoch [43/120    avg_loss:0.119, val_acc:0.941]
Epoch [44/120    avg_loss:0.097, val_acc:0.969]
Epoch [45/120    avg_loss:0.093, val_acc:0.969]
Epoch [46/120    avg_loss:0.070, val_acc:0.965]
Epoch [47/120    avg_loss:0.071, val_acc:0.971]
Epoch [48/120    avg_loss:0.048, val_acc:0.978]
Epoch [49/120    avg_loss:0.057, val_acc:0.978]
Epoch [50/120    avg_loss:0.052, val_acc:0.965]
Epoch [51/120    avg_loss:0.052, val_acc:0.984]
Epoch [52/120    avg_loss:0.040, val_acc:0.978]
Epoch [53/120    avg_loss:0.048, val_acc:0.982]
Epoch [54/120    avg_loss:0.044, val_acc:0.981]
Epoch [55/120    avg_loss:0.038, val_acc:0.978]
Epoch [56/120    avg_loss:0.047, val_acc:0.991]
Epoch [57/120    avg_loss:0.034, val_acc:0.985]
Epoch [58/120    avg_loss:0.037, val_acc:0.979]
Epoch [59/120    avg_loss:0.032, val_acc:0.984]
Epoch [60/120    avg_loss:0.034, val_acc:0.983]
Epoch [61/120    avg_loss:0.040, val_acc:0.972]
Epoch [62/120    avg_loss:0.037, val_acc:0.985]
Epoch [63/120    avg_loss:0.032, val_acc:0.988]
Epoch [64/120    avg_loss:0.023, val_acc:0.988]
Epoch [65/120    avg_loss:0.027, val_acc:0.984]
Epoch [66/120    avg_loss:0.023, val_acc:0.991]
Epoch [67/120    avg_loss:0.028, val_acc:0.986]
Epoch [68/120    avg_loss:0.025, val_acc:0.982]
Epoch [69/120    avg_loss:0.021, val_acc:0.991]
Epoch [70/120    avg_loss:0.023, val_acc:0.988]
Epoch [71/120    avg_loss:0.024, val_acc:0.987]
Epoch [72/120    avg_loss:0.016, val_acc:0.984]
Epoch [73/120    avg_loss:0.020, val_acc:0.979]
Epoch [74/120    avg_loss:0.034, val_acc:0.978]
Epoch [75/120    avg_loss:0.020, val_acc:0.984]
Epoch [76/120    avg_loss:0.022, val_acc:0.985]
Epoch [77/120    avg_loss:0.017, val_acc:0.985]
Epoch [78/120    avg_loss:0.018, val_acc:0.987]
Epoch [79/120    avg_loss:0.020, val_acc:0.989]
Epoch [80/120    avg_loss:0.017, val_acc:0.986]
Epoch [81/120    avg_loss:0.018, val_acc:0.991]
Epoch [82/120    avg_loss:0.014, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.991]
Epoch [84/120    avg_loss:0.015, val_acc:0.991]
Epoch [85/120    avg_loss:0.015, val_acc:0.990]
Epoch [86/120    avg_loss:0.016, val_acc:0.986]
Epoch [87/120    avg_loss:0.016, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.991]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.018, val_acc:0.983]
Epoch [91/120    avg_loss:0.023, val_acc:0.989]
Epoch [92/120    avg_loss:0.060, val_acc:0.962]
Epoch [93/120    avg_loss:0.115, val_acc:0.931]
Epoch [94/120    avg_loss:0.069, val_acc:0.976]
Epoch [95/120    avg_loss:0.044, val_acc:0.962]
Epoch [96/120    avg_loss:0.118, val_acc:0.973]
Epoch [97/120    avg_loss:0.041, val_acc:0.975]
Epoch [98/120    avg_loss:0.032, val_acc:0.980]
Epoch [99/120    avg_loss:0.026, val_acc:0.982]
Epoch [100/120    avg_loss:0.020, val_acc:0.982]
Epoch [101/120    avg_loss:0.022, val_acc:0.982]
Epoch [102/120    avg_loss:0.021, val_acc:0.983]
Epoch [103/120    avg_loss:0.022, val_acc:0.983]
Epoch [104/120    avg_loss:0.023, val_acc:0.982]
Epoch [105/120    avg_loss:0.025, val_acc:0.984]
Epoch [106/120    avg_loss:0.021, val_acc:0.984]
Epoch [107/120    avg_loss:0.020, val_acc:0.984]
Epoch [108/120    avg_loss:0.018, val_acc:0.982]
Epoch [109/120    avg_loss:0.019, val_acc:0.983]
Epoch [110/120    avg_loss:0.016, val_acc:0.984]
Epoch [111/120    avg_loss:0.020, val_acc:0.984]
Epoch [112/120    avg_loss:0.021, val_acc:0.983]
Epoch [113/120    avg_loss:0.020, val_acc:0.983]
Epoch [114/120    avg_loss:0.018, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.023, val_acc:0.983]
Epoch [117/120    avg_loss:0.016, val_acc:0.983]
Epoch [118/120    avg_loss:0.019, val_acc:0.983]
Epoch [119/120    avg_loss:0.015, val_acc:0.983]
Epoch [120/120    avg_loss:0.018, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     0     3     0     0     0    47    35]
 [    0     2 17985     0    97     0     5     0     1     0]
 [    0    12     0  1991     0     0     0     0    33     0]
 [    0    33    18     0  2884     0     8     0    27     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     7     0     0  4854     0     0     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0    45     0    30    52     0     0     0  3442     2]
 [    0     0     0    10    14    69     0     0     0   826]]

Accuracy:
98.62386426626178

F1 scores:
[       nan 0.98624815 0.99612296 0.97741777 0.95782132 0.97424412
 0.99599877 0.9992242  0.96671816 0.92600897]

Kappa:
0.9817803245087553
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9174418ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.269, val_acc:0.054]
Epoch [2/120    avg_loss:2.124, val_acc:0.072]
Epoch [3/120    avg_loss:2.035, val_acc:0.072]
Epoch [4/120    avg_loss:1.947, val_acc:0.096]
Epoch [5/120    avg_loss:1.841, val_acc:0.365]
Epoch [6/120    avg_loss:1.747, val_acc:0.472]
Epoch [7/120    avg_loss:1.665, val_acc:0.497]
Epoch [8/120    avg_loss:1.543, val_acc:0.533]
Epoch [9/120    avg_loss:1.445, val_acc:0.617]
Epoch [10/120    avg_loss:1.347, val_acc:0.637]
Epoch [11/120    avg_loss:1.267, val_acc:0.660]
Epoch [12/120    avg_loss:1.179, val_acc:0.619]
Epoch [13/120    avg_loss:1.094, val_acc:0.642]
Epoch [14/120    avg_loss:1.028, val_acc:0.628]
Epoch [15/120    avg_loss:0.939, val_acc:0.678]
Epoch [16/120    avg_loss:0.855, val_acc:0.671]
Epoch [17/120    avg_loss:0.779, val_acc:0.727]
Epoch [18/120    avg_loss:0.756, val_acc:0.744]
Epoch [19/120    avg_loss:0.667, val_acc:0.716]
Epoch [20/120    avg_loss:0.588, val_acc:0.762]
Epoch [21/120    avg_loss:0.513, val_acc:0.745]
Epoch [22/120    avg_loss:0.491, val_acc:0.778]
Epoch [23/120    avg_loss:0.470, val_acc:0.805]
Epoch [24/120    avg_loss:0.467, val_acc:0.784]
Epoch [25/120    avg_loss:0.404, val_acc:0.843]
Epoch [26/120    avg_loss:0.373, val_acc:0.822]
Epoch [27/120    avg_loss:0.314, val_acc:0.858]
Epoch [28/120    avg_loss:0.274, val_acc:0.921]
Epoch [29/120    avg_loss:0.274, val_acc:0.844]
Epoch [30/120    avg_loss:0.248, val_acc:0.890]
Epoch [31/120    avg_loss:0.231, val_acc:0.930]
Epoch [32/120    avg_loss:0.266, val_acc:0.882]
Epoch [33/120    avg_loss:0.226, val_acc:0.922]
Epoch [34/120    avg_loss:0.185, val_acc:0.946]
Epoch [35/120    avg_loss:0.170, val_acc:0.952]
Epoch [36/120    avg_loss:0.148, val_acc:0.970]
Epoch [37/120    avg_loss:0.132, val_acc:0.923]
Epoch [38/120    avg_loss:0.145, val_acc:0.928]
Epoch [39/120    avg_loss:0.129, val_acc:0.966]
Epoch [40/120    avg_loss:0.118, val_acc:0.974]
Epoch [41/120    avg_loss:0.128, val_acc:0.941]
Epoch [42/120    avg_loss:0.111, val_acc:0.968]
Epoch [43/120    avg_loss:0.104, val_acc:0.963]
Epoch [44/120    avg_loss:0.134, val_acc:0.971]
Epoch [45/120    avg_loss:0.127, val_acc:0.942]
Epoch [46/120    avg_loss:0.124, val_acc:0.970]
Epoch [47/120    avg_loss:0.085, val_acc:0.972]
Epoch [48/120    avg_loss:0.069, val_acc:0.977]
Epoch [49/120    avg_loss:0.063, val_acc:0.947]
Epoch [50/120    avg_loss:0.067, val_acc:0.976]
Epoch [51/120    avg_loss:0.054, val_acc:0.984]
Epoch [52/120    avg_loss:0.060, val_acc:0.968]
Epoch [53/120    avg_loss:0.056, val_acc:0.985]
Epoch [54/120    avg_loss:0.074, val_acc:0.953]
Epoch [55/120    avg_loss:0.064, val_acc:0.967]
Epoch [56/120    avg_loss:0.061, val_acc:0.973]
Epoch [57/120    avg_loss:0.058, val_acc:0.977]
Epoch [58/120    avg_loss:0.032, val_acc:0.974]
Epoch [59/120    avg_loss:0.044, val_acc:0.982]
Epoch [60/120    avg_loss:0.040, val_acc:0.987]
Epoch [61/120    avg_loss:0.030, val_acc:0.984]
Epoch [62/120    avg_loss:0.031, val_acc:0.986]
Epoch [63/120    avg_loss:0.032, val_acc:0.986]
Epoch [64/120    avg_loss:0.040, val_acc:0.984]
Epoch [65/120    avg_loss:0.036, val_acc:0.985]
Epoch [66/120    avg_loss:0.034, val_acc:0.988]
Epoch [67/120    avg_loss:0.048, val_acc:0.981]
Epoch [68/120    avg_loss:0.039, val_acc:0.984]
Epoch [69/120    avg_loss:0.040, val_acc:0.987]
Epoch [70/120    avg_loss:0.029, val_acc:0.988]
Epoch [71/120    avg_loss:0.025, val_acc:0.987]
Epoch [72/120    avg_loss:0.025, val_acc:0.978]
Epoch [73/120    avg_loss:0.024, val_acc:0.988]
Epoch [74/120    avg_loss:0.022, val_acc:0.985]
Epoch [75/120    avg_loss:0.034, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.986]
Epoch [77/120    avg_loss:0.017, val_acc:0.987]
Epoch [78/120    avg_loss:0.016, val_acc:0.991]
Epoch [79/120    avg_loss:0.021, val_acc:0.988]
Epoch [80/120    avg_loss:0.019, val_acc:0.988]
Epoch [81/120    avg_loss:0.033, val_acc:0.987]
Epoch [82/120    avg_loss:0.016, val_acc:0.988]
Epoch [83/120    avg_loss:0.018, val_acc:0.990]
Epoch [84/120    avg_loss:0.017, val_acc:0.988]
Epoch [85/120    avg_loss:0.020, val_acc:0.985]
Epoch [86/120    avg_loss:0.021, val_acc:0.990]
Epoch [87/120    avg_loss:0.019, val_acc:0.991]
Epoch [88/120    avg_loss:0.019, val_acc:0.980]
Epoch [89/120    avg_loss:0.086, val_acc:0.970]
Epoch [90/120    avg_loss:0.031, val_acc:0.984]
Epoch [91/120    avg_loss:0.033, val_acc:0.983]
Epoch [92/120    avg_loss:0.017, val_acc:0.985]
Epoch [93/120    avg_loss:0.018, val_acc:0.987]
Epoch [94/120    avg_loss:0.015, val_acc:0.991]
Epoch [95/120    avg_loss:0.013, val_acc:0.991]
Epoch [96/120    avg_loss:0.011, val_acc:0.991]
Epoch [97/120    avg_loss:0.010, val_acc:0.991]
Epoch [98/120    avg_loss:0.014, val_acc:0.987]
Epoch [99/120    avg_loss:0.014, val_acc:0.991]
Epoch [100/120    avg_loss:0.011, val_acc:0.989]
Epoch [101/120    avg_loss:0.010, val_acc:0.989]
Epoch [102/120    avg_loss:0.013, val_acc:0.991]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.991]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.016, val_acc:0.988]
Epoch [108/120    avg_loss:0.026, val_acc:0.983]
Epoch [109/120    avg_loss:0.017, val_acc:0.985]
Epoch [110/120    avg_loss:0.016, val_acc:0.987]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.013, val_acc:0.991]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.011, val_acc:0.991]
Epoch [115/120    avg_loss:0.013, val_acc:0.990]
Epoch [116/120    avg_loss:0.011, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.991]
Epoch [118/120    avg_loss:0.007, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     2     0     0     6    17     4]
 [    0     4 18054     0    25     0     2     0     0     5]
 [    0    13     0  1999     0     0     0     0    23     1]
 [    0    22    19     0  2888     0    10     0    27     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2    20     0     0     0  4855     0     1     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    34     0     5    40     0     0     0  3492     0]
 [    0     0     0     8    14    19     0     0     0   878]]

Accuracy:
99.20709517267973

F1 scores:
[       nan 0.99194423 0.9979272  0.98764822 0.9722269  0.99277292
 0.99640841 0.99767981 0.97938578 0.9685604 ]

Kappa:
0.9894918419470734
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24ec167b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.231, val_acc:0.125]
Epoch [2/120    avg_loss:2.078, val_acc:0.152]
Epoch [3/120    avg_loss:1.946, val_acc:0.156]
Epoch [4/120    avg_loss:1.850, val_acc:0.166]
Epoch [5/120    avg_loss:1.733, val_acc:0.245]
Epoch [6/120    avg_loss:1.650, val_acc:0.250]
Epoch [7/120    avg_loss:1.580, val_acc:0.251]
Epoch [8/120    avg_loss:1.498, val_acc:0.259]
Epoch [9/120    avg_loss:1.385, val_acc:0.293]
Epoch [10/120    avg_loss:1.289, val_acc:0.331]
Epoch [11/120    avg_loss:1.188, val_acc:0.404]
Epoch [12/120    avg_loss:1.134, val_acc:0.549]
Epoch [13/120    avg_loss:1.078, val_acc:0.532]
Epoch [14/120    avg_loss:1.032, val_acc:0.667]
Epoch [15/120    avg_loss:0.918, val_acc:0.694]
Epoch [16/120    avg_loss:0.876, val_acc:0.703]
Epoch [17/120    avg_loss:0.818, val_acc:0.730]
Epoch [18/120    avg_loss:0.737, val_acc:0.747]
Epoch [19/120    avg_loss:0.657, val_acc:0.750]
Epoch [20/120    avg_loss:0.630, val_acc:0.747]
Epoch [21/120    avg_loss:0.594, val_acc:0.753]
Epoch [22/120    avg_loss:0.829, val_acc:0.800]
Epoch [23/120    avg_loss:0.548, val_acc:0.716]
Epoch [24/120    avg_loss:0.494, val_acc:0.787]
Epoch [25/120    avg_loss:0.750, val_acc:0.785]
Epoch [26/120    avg_loss:0.485, val_acc:0.774]
Epoch [27/120    avg_loss:0.386, val_acc:0.810]
Epoch [28/120    avg_loss:0.353, val_acc:0.819]
Epoch [29/120    avg_loss:0.290, val_acc:0.816]
Epoch [30/120    avg_loss:0.261, val_acc:0.828]
Epoch [31/120    avg_loss:0.250, val_acc:0.879]
Epoch [32/120    avg_loss:0.231, val_acc:0.911]
Epoch [33/120    avg_loss:0.205, val_acc:0.933]
Epoch [34/120    avg_loss:0.191, val_acc:0.923]
Epoch [35/120    avg_loss:0.165, val_acc:0.955]
Epoch [36/120    avg_loss:0.165, val_acc:0.906]
Epoch [37/120    avg_loss:0.163, val_acc:0.956]
Epoch [38/120    avg_loss:0.373, val_acc:0.916]
Epoch [39/120    avg_loss:0.183, val_acc:0.890]
Epoch [40/120    avg_loss:0.142, val_acc:0.956]
Epoch [41/120    avg_loss:0.131, val_acc:0.927]
Epoch [42/120    avg_loss:0.121, val_acc:0.959]
Epoch [43/120    avg_loss:0.127, val_acc:0.927]
Epoch [44/120    avg_loss:0.142, val_acc:0.947]
Epoch [45/120    avg_loss:0.094, val_acc:0.950]
Epoch [46/120    avg_loss:0.085, val_acc:0.965]
Epoch [47/120    avg_loss:0.090, val_acc:0.957]
Epoch [48/120    avg_loss:0.132, val_acc:0.943]
Epoch [49/120    avg_loss:0.157, val_acc:0.953]
Epoch [50/120    avg_loss:0.091, val_acc:0.972]
Epoch [51/120    avg_loss:0.073, val_acc:0.970]
Epoch [52/120    avg_loss:0.060, val_acc:0.974]
Epoch [53/120    avg_loss:0.054, val_acc:0.973]
Epoch [54/120    avg_loss:0.054, val_acc:0.978]
Epoch [55/120    avg_loss:0.066, val_acc:0.970]
Epoch [56/120    avg_loss:0.090, val_acc:0.969]
Epoch [57/120    avg_loss:0.064, val_acc:0.977]
Epoch [58/120    avg_loss:0.053, val_acc:0.980]
Epoch [59/120    avg_loss:0.041, val_acc:0.983]
Epoch [60/120    avg_loss:0.044, val_acc:0.980]
Epoch [61/120    avg_loss:0.040, val_acc:0.985]
Epoch [62/120    avg_loss:0.065, val_acc:0.966]
Epoch [63/120    avg_loss:0.057, val_acc:0.974]
Epoch [64/120    avg_loss:0.057, val_acc:0.976]
Epoch [65/120    avg_loss:0.038, val_acc:0.981]
Epoch [66/120    avg_loss:0.034, val_acc:0.981]
Epoch [67/120    avg_loss:0.032, val_acc:0.984]
Epoch [68/120    avg_loss:0.037, val_acc:0.980]
Epoch [69/120    avg_loss:0.033, val_acc:0.983]
Epoch [70/120    avg_loss:0.028, val_acc:0.979]
Epoch [71/120    avg_loss:0.029, val_acc:0.980]
Epoch [72/120    avg_loss:0.024, val_acc:0.981]
Epoch [73/120    avg_loss:0.030, val_acc:0.980]
Epoch [74/120    avg_loss:0.028, val_acc:0.982]
Epoch [75/120    avg_loss:0.022, val_acc:0.984]
Epoch [76/120    avg_loss:0.024, val_acc:0.984]
Epoch [77/120    avg_loss:0.020, val_acc:0.984]
Epoch [78/120    avg_loss:0.018, val_acc:0.984]
Epoch [79/120    avg_loss:0.017, val_acc:0.984]
Epoch [80/120    avg_loss:0.018, val_acc:0.984]
Epoch [81/120    avg_loss:0.017, val_acc:0.984]
Epoch [82/120    avg_loss:0.017, val_acc:0.986]
Epoch [83/120    avg_loss:0.018, val_acc:0.984]
Epoch [84/120    avg_loss:0.018, val_acc:0.984]
Epoch [85/120    avg_loss:0.017, val_acc:0.984]
Epoch [86/120    avg_loss:0.019, val_acc:0.984]
Epoch [87/120    avg_loss:0.021, val_acc:0.985]
Epoch [88/120    avg_loss:0.021, val_acc:0.984]
Epoch [89/120    avg_loss:0.017, val_acc:0.984]
Epoch [90/120    avg_loss:0.019, val_acc:0.985]
Epoch [91/120    avg_loss:0.018, val_acc:0.985]
Epoch [92/120    avg_loss:0.017, val_acc:0.984]
Epoch [93/120    avg_loss:0.018, val_acc:0.985]
Epoch [94/120    avg_loss:0.017, val_acc:0.985]
Epoch [95/120    avg_loss:0.016, val_acc:0.984]
Epoch [96/120    avg_loss:0.018, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.984]
Epoch [98/120    avg_loss:0.018, val_acc:0.984]
Epoch [99/120    avg_loss:0.016, val_acc:0.984]
Epoch [100/120    avg_loss:0.019, val_acc:0.984]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.015, val_acc:0.985]
Epoch [106/120    avg_loss:0.016, val_acc:0.985]
Epoch [107/120    avg_loss:0.017, val_acc:0.985]
Epoch [108/120    avg_loss:0.015, val_acc:0.985]
Epoch [109/120    avg_loss:0.018, val_acc:0.985]
Epoch [110/120    avg_loss:0.015, val_acc:0.985]
Epoch [111/120    avg_loss:0.017, val_acc:0.985]
Epoch [112/120    avg_loss:0.016, val_acc:0.985]
Epoch [113/120    avg_loss:0.017, val_acc:0.985]
Epoch [114/120    avg_loss:0.017, val_acc:0.985]
Epoch [115/120    avg_loss:0.018, val_acc:0.985]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.017, val_acc:0.985]
Epoch [118/120    avg_loss:0.016, val_acc:0.985]
Epoch [119/120    avg_loss:0.017, val_acc:0.985]
Epoch [120/120    avg_loss:0.018, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     0    20     0     0     1    36     5]
 [    0     0 18030     0    58     0     2     0     0     0]
 [    0    11     0  1992     0     0     0     0    32     1]
 [    0    28    22     0  2887     0     8     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20    23     0     0  4835     0     0     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    21     0    31    64     0     0     0  3455     0]
 [    0     1     0     0    15    20     0     2     0   881]]

Accuracy:
98.9178897645386

F1 scores:
[       nan 0.99043769 0.99717936 0.97599216 0.95977394 0.99239544
 0.99454901 0.99845081 0.97036933 0.97509685]

Kappa:
0.9856650649467529
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a25879ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.185, val_acc:0.089]
Epoch [2/120    avg_loss:2.015, val_acc:0.069]
Epoch [3/120    avg_loss:1.897, val_acc:0.101]
Epoch [4/120    avg_loss:1.799, val_acc:0.134]
Epoch [5/120    avg_loss:1.721, val_acc:0.159]
Epoch [6/120    avg_loss:1.589, val_acc:0.184]
Epoch [7/120    avg_loss:1.513, val_acc:0.220]
Epoch [8/120    avg_loss:1.446, val_acc:0.232]
Epoch [9/120    avg_loss:1.341, val_acc:0.271]
Epoch [10/120    avg_loss:1.285, val_acc:0.296]
Epoch [11/120    avg_loss:1.218, val_acc:0.328]
Epoch [12/120    avg_loss:1.147, val_acc:0.387]
Epoch [13/120    avg_loss:1.070, val_acc:0.491]
Epoch [14/120    avg_loss:1.002, val_acc:0.534]
Epoch [15/120    avg_loss:0.976, val_acc:0.528]
Epoch [16/120    avg_loss:0.880, val_acc:0.655]
Epoch [17/120    avg_loss:0.826, val_acc:0.594]
Epoch [18/120    avg_loss:0.776, val_acc:0.686]
Epoch [19/120    avg_loss:0.698, val_acc:0.706]
Epoch [20/120    avg_loss:0.620, val_acc:0.719]
Epoch [21/120    avg_loss:0.597, val_acc:0.771]
Epoch [22/120    avg_loss:0.541, val_acc:0.772]
Epoch [23/120    avg_loss:0.484, val_acc:0.802]
Epoch [24/120    avg_loss:0.486, val_acc:0.830]
Epoch [25/120    avg_loss:0.420, val_acc:0.819]
Epoch [26/120    avg_loss:0.424, val_acc:0.803]
Epoch [27/120    avg_loss:0.376, val_acc:0.834]
Epoch [28/120    avg_loss:0.335, val_acc:0.829]
Epoch [29/120    avg_loss:0.303, val_acc:0.853]
Epoch [30/120    avg_loss:0.302, val_acc:0.833]
Epoch [31/120    avg_loss:0.600, val_acc:0.730]
Epoch [32/120    avg_loss:0.594, val_acc:0.788]
Epoch [33/120    avg_loss:0.464, val_acc:0.797]
Epoch [34/120    avg_loss:0.426, val_acc:0.857]
Epoch [35/120    avg_loss:0.357, val_acc:0.859]
Epoch [36/120    avg_loss:0.313, val_acc:0.834]
Epoch [37/120    avg_loss:0.314, val_acc:0.841]
Epoch [38/120    avg_loss:0.307, val_acc:0.854]
Epoch [39/120    avg_loss:0.251, val_acc:0.887]
Epoch [40/120    avg_loss:0.239, val_acc:0.872]
Epoch [41/120    avg_loss:0.240, val_acc:0.911]
Epoch [42/120    avg_loss:0.207, val_acc:0.928]
Epoch [43/120    avg_loss:0.233, val_acc:0.917]
Epoch [44/120    avg_loss:0.192, val_acc:0.934]
Epoch [45/120    avg_loss:0.205, val_acc:0.944]
Epoch [46/120    avg_loss:0.150, val_acc:0.941]
Epoch [47/120    avg_loss:0.154, val_acc:0.942]
Epoch [48/120    avg_loss:0.128, val_acc:0.954]
Epoch [49/120    avg_loss:0.129, val_acc:0.957]
Epoch [50/120    avg_loss:0.107, val_acc:0.964]
Epoch [51/120    avg_loss:0.110, val_acc:0.958]
Epoch [52/120    avg_loss:0.094, val_acc:0.970]
Epoch [53/120    avg_loss:0.084, val_acc:0.967]
Epoch [54/120    avg_loss:0.103, val_acc:0.960]
Epoch [55/120    avg_loss:0.115, val_acc:0.964]
Epoch [56/120    avg_loss:0.109, val_acc:0.974]
Epoch [57/120    avg_loss:0.090, val_acc:0.974]
Epoch [58/120    avg_loss:0.086, val_acc:0.958]
Epoch [59/120    avg_loss:0.090, val_acc:0.970]
Epoch [60/120    avg_loss:0.079, val_acc:0.977]
Epoch [61/120    avg_loss:0.066, val_acc:0.964]
Epoch [62/120    avg_loss:0.048, val_acc:0.979]
Epoch [63/120    avg_loss:0.051, val_acc:0.972]
Epoch [64/120    avg_loss:0.047, val_acc:0.970]
Epoch [65/120    avg_loss:0.042, val_acc:0.979]
Epoch [66/120    avg_loss:0.042, val_acc:0.967]
Epoch [67/120    avg_loss:0.046, val_acc:0.977]
Epoch [68/120    avg_loss:0.063, val_acc:0.963]
Epoch [69/120    avg_loss:0.056, val_acc:0.972]
Epoch [70/120    avg_loss:0.060, val_acc:0.975]
Epoch [71/120    avg_loss:0.082, val_acc:0.964]
Epoch [72/120    avg_loss:0.072, val_acc:0.961]
Epoch [73/120    avg_loss:0.052, val_acc:0.979]
Epoch [74/120    avg_loss:0.044, val_acc:0.982]
Epoch [75/120    avg_loss:0.040, val_acc:0.981]
Epoch [76/120    avg_loss:0.040, val_acc:0.982]
Epoch [77/120    avg_loss:0.049, val_acc:0.970]
Epoch [78/120    avg_loss:0.073, val_acc:0.980]
Epoch [79/120    avg_loss:0.035, val_acc:0.971]
Epoch [80/120    avg_loss:0.051, val_acc:0.977]
Epoch [81/120    avg_loss:0.031, val_acc:0.984]
Epoch [82/120    avg_loss:0.043, val_acc:0.982]
Epoch [83/120    avg_loss:0.035, val_acc:0.980]
Epoch [84/120    avg_loss:0.034, val_acc:0.978]
Epoch [85/120    avg_loss:0.023, val_acc:0.981]
Epoch [86/120    avg_loss:0.023, val_acc:0.981]
Epoch [87/120    avg_loss:0.026, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.982]
Epoch [89/120    avg_loss:0.030, val_acc:0.978]
Epoch [90/120    avg_loss:0.049, val_acc:0.978]
Epoch [91/120    avg_loss:0.029, val_acc:0.978]
Epoch [92/120    avg_loss:0.023, val_acc:0.975]
Epoch [93/120    avg_loss:0.029, val_acc:0.964]
Epoch [94/120    avg_loss:0.023, val_acc:0.983]
Epoch [95/120    avg_loss:0.017, val_acc:0.982]
Epoch [96/120    avg_loss:0.018, val_acc:0.979]
Epoch [97/120    avg_loss:0.019, val_acc:0.980]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.020, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.981]
Epoch [102/120    avg_loss:0.014, val_acc:0.981]
Epoch [103/120    avg_loss:0.018, val_acc:0.981]
Epoch [104/120    avg_loss:0.017, val_acc:0.980]
Epoch [105/120    avg_loss:0.014, val_acc:0.980]
Epoch [106/120    avg_loss:0.016, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.015, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.981]
Epoch [112/120    avg_loss:0.014, val_acc:0.981]
Epoch [113/120    avg_loss:0.015, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.013, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.015, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     0     0     0     0    37     2]
 [    0     1 18066     0    20     0     3     0     0     0]
 [    0     9     0  1992     1     0     0     0    30     4]
 [    0    29    20     0  2888     0     8     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    14     0     0  4863     0     0     0]
 [    0     0     0     0     0     0     3  1282     0     5]
 [    0    37     0     2    58     0     0     0  3474     0]
 [    0     0     0     0    14    34     0     0     1   870]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.99108596 0.99875612 0.9851632  0.97026709 0.9871407
 0.99702717 0.99688958 0.97310924 0.96666667]

Kappa:
0.9885023171108337
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb0daa48b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.260, val_acc:0.122]
Epoch [2/120    avg_loss:2.107, val_acc:0.091]
Epoch [3/120    avg_loss:1.986, val_acc:0.159]
Epoch [4/120    avg_loss:1.842, val_acc:0.130]
Epoch [5/120    avg_loss:1.735, val_acc:0.169]
Epoch [6/120    avg_loss:1.644, val_acc:0.261]
Epoch [7/120    avg_loss:1.541, val_acc:0.315]
Epoch [8/120    avg_loss:1.480, val_acc:0.347]
Epoch [9/120    avg_loss:1.419, val_acc:0.350]
Epoch [10/120    avg_loss:1.330, val_acc:0.344]
Epoch [11/120    avg_loss:1.227, val_acc:0.374]
Epoch [12/120    avg_loss:1.147, val_acc:0.368]
Epoch [13/120    avg_loss:1.066, val_acc:0.380]
Epoch [14/120    avg_loss:1.026, val_acc:0.505]
Epoch [15/120    avg_loss:0.933, val_acc:0.547]
Epoch [16/120    avg_loss:0.870, val_acc:0.561]
Epoch [17/120    avg_loss:0.783, val_acc:0.539]
Epoch [18/120    avg_loss:0.733, val_acc:0.587]
Epoch [19/120    avg_loss:0.644, val_acc:0.583]
Epoch [20/120    avg_loss:0.589, val_acc:0.640]
Epoch [21/120    avg_loss:0.519, val_acc:0.715]
Epoch [22/120    avg_loss:0.499, val_acc:0.736]
Epoch [23/120    avg_loss:0.465, val_acc:0.788]
Epoch [24/120    avg_loss:0.419, val_acc:0.779]
Epoch [25/120    avg_loss:0.371, val_acc:0.806]
Epoch [26/120    avg_loss:0.339, val_acc:0.872]
Epoch [27/120    avg_loss:0.325, val_acc:0.851]
Epoch [28/120    avg_loss:0.282, val_acc:0.890]
Epoch [29/120    avg_loss:0.262, val_acc:0.913]
Epoch [30/120    avg_loss:0.238, val_acc:0.892]
Epoch [31/120    avg_loss:0.229, val_acc:0.922]
Epoch [32/120    avg_loss:0.228, val_acc:0.918]
Epoch [33/120    avg_loss:0.219, val_acc:0.909]
Epoch [34/120    avg_loss:0.205, val_acc:0.925]
Epoch [35/120    avg_loss:0.165, val_acc:0.946]
Epoch [36/120    avg_loss:0.169, val_acc:0.914]
Epoch [37/120    avg_loss:0.264, val_acc:0.852]
Epoch [38/120    avg_loss:0.214, val_acc:0.939]
Epoch [39/120    avg_loss:0.152, val_acc:0.955]
Epoch [40/120    avg_loss:0.136, val_acc:0.929]
Epoch [41/120    avg_loss:0.157, val_acc:0.948]
Epoch [42/120    avg_loss:0.131, val_acc:0.941]
Epoch [43/120    avg_loss:0.102, val_acc:0.963]
Epoch [44/120    avg_loss:0.098, val_acc:0.969]
Epoch [45/120    avg_loss:0.094, val_acc:0.955]
Epoch [46/120    avg_loss:0.102, val_acc:0.953]
Epoch [47/120    avg_loss:0.105, val_acc:0.963]
Epoch [48/120    avg_loss:0.076, val_acc:0.961]
Epoch [49/120    avg_loss:0.073, val_acc:0.966]
Epoch [50/120    avg_loss:0.085, val_acc:0.929]
Epoch [51/120    avg_loss:0.102, val_acc:0.949]
Epoch [52/120    avg_loss:0.076, val_acc:0.957]
Epoch [53/120    avg_loss:0.060, val_acc:0.967]
Epoch [54/120    avg_loss:0.079, val_acc:0.943]
Epoch [55/120    avg_loss:0.087, val_acc:0.951]
Epoch [56/120    avg_loss:0.062, val_acc:0.972]
Epoch [57/120    avg_loss:0.047, val_acc:0.972]
Epoch [58/120    avg_loss:0.051, val_acc:0.977]
Epoch [59/120    avg_loss:0.074, val_acc:0.961]
Epoch [60/120    avg_loss:0.057, val_acc:0.968]
Epoch [61/120    avg_loss:0.036, val_acc:0.981]
Epoch [62/120    avg_loss:0.045, val_acc:0.966]
Epoch [63/120    avg_loss:0.045, val_acc:0.978]
Epoch [64/120    avg_loss:0.040, val_acc:0.981]
Epoch [65/120    avg_loss:0.034, val_acc:0.983]
Epoch [66/120    avg_loss:0.039, val_acc:0.979]
Epoch [67/120    avg_loss:0.028, val_acc:0.979]
Epoch [68/120    avg_loss:0.027, val_acc:0.979]
Epoch [69/120    avg_loss:0.028, val_acc:0.974]
Epoch [70/120    avg_loss:0.027, val_acc:0.977]
Epoch [71/120    avg_loss:0.035, val_acc:0.978]
Epoch [72/120    avg_loss:0.046, val_acc:0.981]
Epoch [73/120    avg_loss:0.046, val_acc:0.979]
Epoch [74/120    avg_loss:0.042, val_acc:0.983]
Epoch [75/120    avg_loss:0.023, val_acc:0.976]
Epoch [76/120    avg_loss:0.027, val_acc:0.973]
Epoch [77/120    avg_loss:0.039, val_acc:0.972]
Epoch [78/120    avg_loss:0.035, val_acc:0.971]
Epoch [79/120    avg_loss:0.037, val_acc:0.978]
Epoch [80/120    avg_loss:0.042, val_acc:0.976]
Epoch [81/120    avg_loss:0.038, val_acc:0.978]
Epoch [82/120    avg_loss:0.033, val_acc:0.978]
Epoch [83/120    avg_loss:0.044, val_acc:0.979]
Epoch [84/120    avg_loss:0.034, val_acc:0.977]
Epoch [85/120    avg_loss:0.028, val_acc:0.984]
Epoch [86/120    avg_loss:0.022, val_acc:0.985]
Epoch [87/120    avg_loss:0.016, val_acc:0.981]
Epoch [88/120    avg_loss:0.020, val_acc:0.986]
Epoch [89/120    avg_loss:0.018, val_acc:0.976]
Epoch [90/120    avg_loss:0.018, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.014, val_acc:0.985]
Epoch [94/120    avg_loss:0.014, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.012, val_acc:0.986]
Epoch [97/120    avg_loss:0.012, val_acc:0.986]
Epoch [98/120    avg_loss:0.015, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.978]
Epoch [102/120    avg_loss:0.017, val_acc:0.982]
Epoch [103/120    avg_loss:0.016, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.987]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.989]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.015, val_acc:0.984]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.012, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.017, val_acc:0.956]
Epoch [118/120    avg_loss:0.186, val_acc:0.939]
Epoch [119/120    avg_loss:0.127, val_acc:0.952]
Epoch [120/120    avg_loss:0.073, val_acc:0.942]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5872     0    88    90     0     0     9   239   134]
 [    0     0 18047     0     8     0    33     0     2     0]
 [    0    12     0  2006     0     0     0     0    11     7]
 [    0    49    33     0  2862     0     3     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30   237     1     0  4542     0    68     0]
 [    0     3     0     0     0     0     2  1265     7    13]
 [    0    39     0   362    89     0     0     0  3081     0]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
96.06921649434845

F1 scores:
[       nan 0.94656242 0.99707182 0.84838232 0.94831014 0.99126472
 0.96045676 0.98673947 0.87978298 0.90230179]

Kappa:
0.9480265897345735
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9139ea1b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.188, val_acc:0.061]
Epoch [2/120    avg_loss:2.001, val_acc:0.067]
Epoch [3/120    avg_loss:1.887, val_acc:0.101]
Epoch [4/120    avg_loss:1.792, val_acc:0.123]
Epoch [5/120    avg_loss:1.705, val_acc:0.134]
Epoch [6/120    avg_loss:1.636, val_acc:0.156]
Epoch [7/120    avg_loss:1.527, val_acc:0.226]
Epoch [8/120    avg_loss:1.421, val_acc:0.267]
Epoch [9/120    avg_loss:1.352, val_acc:0.337]
Epoch [10/120    avg_loss:1.288, val_acc:0.360]
Epoch [11/120    avg_loss:1.180, val_acc:0.366]
Epoch [12/120    avg_loss:1.072, val_acc:0.393]
Epoch [13/120    avg_loss:1.012, val_acc:0.433]
Epoch [14/120    avg_loss:0.909, val_acc:0.539]
Epoch [15/120    avg_loss:0.853, val_acc:0.625]
Epoch [16/120    avg_loss:0.769, val_acc:0.703]
Epoch [17/120    avg_loss:0.705, val_acc:0.694]
Epoch [18/120    avg_loss:0.640, val_acc:0.765]
Epoch [19/120    avg_loss:0.589, val_acc:0.775]
Epoch [20/120    avg_loss:0.521, val_acc:0.828]
Epoch [21/120    avg_loss:0.480, val_acc:0.785]
Epoch [22/120    avg_loss:0.433, val_acc:0.803]
Epoch [23/120    avg_loss:0.410, val_acc:0.821]
Epoch [24/120    avg_loss:0.385, val_acc:0.821]
Epoch [25/120    avg_loss:0.392, val_acc:0.814]
Epoch [26/120    avg_loss:0.360, val_acc:0.826]
Epoch [27/120    avg_loss:0.315, val_acc:0.808]
Epoch [28/120    avg_loss:0.331, val_acc:0.837]
Epoch [29/120    avg_loss:0.272, val_acc:0.847]
Epoch [30/120    avg_loss:0.262, val_acc:0.857]
Epoch [31/120    avg_loss:0.240, val_acc:0.879]
Epoch [32/120    avg_loss:0.222, val_acc:0.900]
Epoch [33/120    avg_loss:0.174, val_acc:0.925]
Epoch [34/120    avg_loss:0.187, val_acc:0.926]
Epoch [35/120    avg_loss:0.206, val_acc:0.884]
Epoch [36/120    avg_loss:0.226, val_acc:0.882]
Epoch [37/120    avg_loss:0.170, val_acc:0.922]
Epoch [38/120    avg_loss:0.168, val_acc:0.939]
Epoch [39/120    avg_loss:0.144, val_acc:0.944]
Epoch [40/120    avg_loss:0.117, val_acc:0.932]
Epoch [41/120    avg_loss:0.356, val_acc:0.881]
Epoch [42/120    avg_loss:0.189, val_acc:0.934]
Epoch [43/120    avg_loss:0.152, val_acc:0.851]
Epoch [44/120    avg_loss:0.108, val_acc:0.950]
Epoch [45/120    avg_loss:0.109, val_acc:0.941]
Epoch [46/120    avg_loss:0.096, val_acc:0.963]
Epoch [47/120    avg_loss:0.083, val_acc:0.967]
Epoch [48/120    avg_loss:0.068, val_acc:0.970]
Epoch [49/120    avg_loss:0.103, val_acc:0.958]
Epoch [50/120    avg_loss:0.074, val_acc:0.966]
Epoch [51/120    avg_loss:0.070, val_acc:0.962]
Epoch [52/120    avg_loss:0.064, val_acc:0.976]
Epoch [53/120    avg_loss:0.053, val_acc:0.973]
Epoch [54/120    avg_loss:0.065, val_acc:0.973]
Epoch [55/120    avg_loss:0.050, val_acc:0.976]
Epoch [56/120    avg_loss:0.040, val_acc:0.972]
Epoch [57/120    avg_loss:0.047, val_acc:0.958]
Epoch [58/120    avg_loss:0.081, val_acc:0.972]
Epoch [59/120    avg_loss:0.058, val_acc:0.971]
Epoch [60/120    avg_loss:0.079, val_acc:0.965]
Epoch [61/120    avg_loss:0.073, val_acc:0.960]
Epoch [62/120    avg_loss:0.041, val_acc:0.972]
Epoch [63/120    avg_loss:0.037, val_acc:0.976]
Epoch [64/120    avg_loss:0.039, val_acc:0.972]
Epoch [65/120    avg_loss:0.046, val_acc:0.966]
Epoch [66/120    avg_loss:0.034, val_acc:0.978]
Epoch [67/120    avg_loss:0.027, val_acc:0.978]
Epoch [68/120    avg_loss:0.025, val_acc:0.980]
Epoch [69/120    avg_loss:0.027, val_acc:0.974]
Epoch [70/120    avg_loss:0.073, val_acc:0.953]
Epoch [71/120    avg_loss:0.073, val_acc:0.976]
Epoch [72/120    avg_loss:0.043, val_acc:0.977]
Epoch [73/120    avg_loss:0.171, val_acc:0.881]
Epoch [74/120    avg_loss:0.282, val_acc:0.918]
Epoch [75/120    avg_loss:0.223, val_acc:0.924]
Epoch [76/120    avg_loss:0.139, val_acc:0.923]
Epoch [77/120    avg_loss:0.120, val_acc:0.957]
Epoch [78/120    avg_loss:0.090, val_acc:0.934]
Epoch [79/120    avg_loss:0.147, val_acc:0.917]
Epoch [80/120    avg_loss:0.094, val_acc:0.952]
Epoch [81/120    avg_loss:0.063, val_acc:0.973]
Epoch [82/120    avg_loss:0.043, val_acc:0.976]
Epoch [83/120    avg_loss:0.038, val_acc:0.978]
Epoch [84/120    avg_loss:0.039, val_acc:0.980]
Epoch [85/120    avg_loss:0.039, val_acc:0.979]
Epoch [86/120    avg_loss:0.030, val_acc:0.978]
Epoch [87/120    avg_loss:0.033, val_acc:0.981]
Epoch [88/120    avg_loss:0.034, val_acc:0.980]
Epoch [89/120    avg_loss:0.030, val_acc:0.980]
Epoch [90/120    avg_loss:0.030, val_acc:0.978]
Epoch [91/120    avg_loss:0.030, val_acc:0.979]
Epoch [92/120    avg_loss:0.029, val_acc:0.979]
Epoch [93/120    avg_loss:0.035, val_acc:0.978]
Epoch [94/120    avg_loss:0.040, val_acc:0.982]
Epoch [95/120    avg_loss:0.025, val_acc:0.981]
Epoch [96/120    avg_loss:0.031, val_acc:0.980]
Epoch [97/120    avg_loss:0.031, val_acc:0.978]
Epoch [98/120    avg_loss:0.029, val_acc:0.980]
Epoch [99/120    avg_loss:0.032, val_acc:0.981]
Epoch [100/120    avg_loss:0.027, val_acc:0.982]
Epoch [101/120    avg_loss:0.033, val_acc:0.981]
Epoch [102/120    avg_loss:0.027, val_acc:0.980]
Epoch [103/120    avg_loss:0.026, val_acc:0.981]
Epoch [104/120    avg_loss:0.028, val_acc:0.982]
Epoch [105/120    avg_loss:0.024, val_acc:0.981]
Epoch [106/120    avg_loss:0.027, val_acc:0.981]
Epoch [107/120    avg_loss:0.024, val_acc:0.983]
Epoch [108/120    avg_loss:0.025, val_acc:0.983]
Epoch [109/120    avg_loss:0.024, val_acc:0.981]
Epoch [110/120    avg_loss:0.025, val_acc:0.981]
Epoch [111/120    avg_loss:0.021, val_acc:0.984]
Epoch [112/120    avg_loss:0.024, val_acc:0.983]
Epoch [113/120    avg_loss:0.025, val_acc:0.979]
Epoch [114/120    avg_loss:0.023, val_acc:0.981]
Epoch [115/120    avg_loss:0.024, val_acc:0.980]
Epoch [116/120    avg_loss:0.024, val_acc:0.981]
Epoch [117/120    avg_loss:0.022, val_acc:0.981]
Epoch [118/120    avg_loss:0.023, val_acc:0.981]
Epoch [119/120    avg_loss:0.027, val_acc:0.978]
Epoch [120/120    avg_loss:0.024, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     0     0     0    25    68     0]
 [    0     5 18013     0    29     0    43     0     0     0]
 [    0    11     0  1979     0     0     0     0    46     0]
 [    0    19    20     0  2906     0    13     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     1     0     0  4836     0    25     3]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    27     0    22    53     0     0     0  3469     0]
 [    0     0     0     1    14    25     0     0     0   879]]

Accuracy:
98.850408502639

F1 scores:
[       nan 0.98792176 0.99695594 0.97994553 0.97288249 0.99051233
 0.98996929 0.99040307 0.96454887 0.97612438]

Kappa:
0.9847753278631898
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91c1721c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.256, val_acc:0.372]
Epoch [2/120    avg_loss:2.117, val_acc:0.481]
Epoch [3/120    avg_loss:1.974, val_acc:0.378]
Epoch [4/120    avg_loss:1.877, val_acc:0.254]
Epoch [5/120    avg_loss:1.785, val_acc:0.248]
Epoch [6/120    avg_loss:1.700, val_acc:0.259]
Epoch [7/120    avg_loss:1.588, val_acc:0.259]
Epoch [8/120    avg_loss:1.497, val_acc:0.259]
Epoch [9/120    avg_loss:1.354, val_acc:0.292]
Epoch [10/120    avg_loss:1.246, val_acc:0.343]
Epoch [11/120    avg_loss:1.171, val_acc:0.379]
Epoch [12/120    avg_loss:1.066, val_acc:0.467]
Epoch [13/120    avg_loss:1.004, val_acc:0.491]
Epoch [14/120    avg_loss:0.903, val_acc:0.512]
Epoch [15/120    avg_loss:0.838, val_acc:0.511]
Epoch [16/120    avg_loss:0.721, val_acc:0.540]
Epoch [17/120    avg_loss:0.636, val_acc:0.573]
Epoch [18/120    avg_loss:0.597, val_acc:0.609]
Epoch [19/120    avg_loss:0.544, val_acc:0.584]
Epoch [20/120    avg_loss:0.505, val_acc:0.673]
Epoch [21/120    avg_loss:0.493, val_acc:0.695]
Epoch [22/120    avg_loss:0.417, val_acc:0.745]
Epoch [23/120    avg_loss:0.409, val_acc:0.748]
Epoch [24/120    avg_loss:0.420, val_acc:0.727]
Epoch [25/120    avg_loss:0.373, val_acc:0.766]
Epoch [26/120    avg_loss:0.302, val_acc:0.798]
Epoch [27/120    avg_loss:0.279, val_acc:0.795]
Epoch [28/120    avg_loss:0.250, val_acc:0.837]
Epoch [29/120    avg_loss:0.276, val_acc:0.799]
Epoch [30/120    avg_loss:0.228, val_acc:0.828]
Epoch [31/120    avg_loss:0.217, val_acc:0.906]
Epoch [32/120    avg_loss:0.192, val_acc:0.930]
Epoch [33/120    avg_loss:0.186, val_acc:0.880]
Epoch [34/120    avg_loss:0.248, val_acc:0.889]
Epoch [35/120    avg_loss:0.253, val_acc:0.934]
Epoch [36/120    avg_loss:0.167, val_acc:0.927]
Epoch [37/120    avg_loss:0.143, val_acc:0.885]
Epoch [38/120    avg_loss:0.149, val_acc:0.951]
Epoch [39/120    avg_loss:0.134, val_acc:0.959]
Epoch [40/120    avg_loss:0.119, val_acc:0.948]
Epoch [41/120    avg_loss:0.134, val_acc:0.952]
Epoch [42/120    avg_loss:0.113, val_acc:0.966]
Epoch [43/120    avg_loss:0.091, val_acc:0.965]
Epoch [44/120    avg_loss:0.086, val_acc:0.949]
Epoch [45/120    avg_loss:0.079, val_acc:0.964]
Epoch [46/120    avg_loss:0.081, val_acc:0.963]
Epoch [47/120    avg_loss:0.070, val_acc:0.968]
Epoch [48/120    avg_loss:0.063, val_acc:0.974]
Epoch [49/120    avg_loss:0.071, val_acc:0.973]
Epoch [50/120    avg_loss:0.048, val_acc:0.975]
Epoch [51/120    avg_loss:0.046, val_acc:0.976]
Epoch [52/120    avg_loss:0.048, val_acc:0.972]
Epoch [53/120    avg_loss:0.076, val_acc:0.962]
Epoch [54/120    avg_loss:0.050, val_acc:0.976]
Epoch [55/120    avg_loss:0.046, val_acc:0.973]
Epoch [56/120    avg_loss:0.044, val_acc:0.966]
Epoch [57/120    avg_loss:0.036, val_acc:0.978]
Epoch [58/120    avg_loss:0.037, val_acc:0.971]
Epoch [59/120    avg_loss:0.034, val_acc:0.979]
Epoch [60/120    avg_loss:0.033, val_acc:0.980]
Epoch [61/120    avg_loss:0.027, val_acc:0.974]
Epoch [62/120    avg_loss:0.023, val_acc:0.982]
Epoch [63/120    avg_loss:0.034, val_acc:0.968]
Epoch [64/120    avg_loss:0.032, val_acc:0.982]
Epoch [65/120    avg_loss:0.033, val_acc:0.972]
Epoch [66/120    avg_loss:0.024, val_acc:0.980]
Epoch [67/120    avg_loss:0.020, val_acc:0.978]
Epoch [68/120    avg_loss:0.017, val_acc:0.983]
Epoch [69/120    avg_loss:0.026, val_acc:0.978]
Epoch [70/120    avg_loss:0.020, val_acc:0.983]
Epoch [71/120    avg_loss:0.018, val_acc:0.972]
Epoch [72/120    avg_loss:0.024, val_acc:0.978]
Epoch [73/120    avg_loss:0.018, val_acc:0.983]
Epoch [74/120    avg_loss:0.018, val_acc:0.983]
Epoch [75/120    avg_loss:0.035, val_acc:0.975]
Epoch [76/120    avg_loss:0.049, val_acc:0.972]
Epoch [77/120    avg_loss:0.033, val_acc:0.972]
Epoch [78/120    avg_loss:0.021, val_acc:0.979]
Epoch [79/120    avg_loss:0.017, val_acc:0.981]
Epoch [80/120    avg_loss:0.020, val_acc:0.972]
Epoch [81/120    avg_loss:0.019, val_acc:0.972]
Epoch [82/120    avg_loss:0.018, val_acc:0.979]
Epoch [83/120    avg_loss:0.015, val_acc:0.979]
Epoch [84/120    avg_loss:0.024, val_acc:0.980]
Epoch [85/120    avg_loss:0.018, val_acc:0.983]
Epoch [86/120    avg_loss:0.034, val_acc:0.968]
Epoch [87/120    avg_loss:0.034, val_acc:0.984]
Epoch [88/120    avg_loss:0.018, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.014, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.014, val_acc:0.976]
Epoch [94/120    avg_loss:0.015, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.982]
Epoch [96/120    avg_loss:0.064, val_acc:0.954]
Epoch [97/120    avg_loss:0.110, val_acc:0.958]
Epoch [98/120    avg_loss:0.063, val_acc:0.974]
Epoch [99/120    avg_loss:0.041, val_acc:0.965]
Epoch [100/120    avg_loss:0.029, val_acc:0.972]
Epoch [101/120    avg_loss:0.031, val_acc:0.972]
Epoch [102/120    avg_loss:0.022, val_acc:0.975]
Epoch [103/120    avg_loss:0.028, val_acc:0.977]
Epoch [104/120    avg_loss:0.020, val_acc:0.980]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6386     0     0     0     0     0    22    23     1]
 [    0     6 18004     0    73     0     7     0     0     0]
 [    0    10     0  2013     0     0     0     0    12     1]
 [    0    36    21     0  2879     0    10     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4857     0     0     2]
 [    0     0     0     0     0     0     4  1280     0     6]
 [    0    11     0    22    65     0     0     0  3473     0]
 [    0     0     0     3    15    13     0     0     0   888]]

Accuracy:
99.01670161232015

F1 scores:
[       nan 0.99153792 0.99651298 0.98821797 0.95902732 0.99504384
 0.99569496 0.98765432 0.97762139 0.97743533]

Kappa:
0.9869773112575654
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f84172f5be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.267, val_acc:0.083]
Epoch [2/120    avg_loss:2.065, val_acc:0.110]
Epoch [3/120    avg_loss:1.929, val_acc:0.113]
Epoch [4/120    avg_loss:1.830, val_acc:0.127]
Epoch [5/120    avg_loss:1.685, val_acc:0.129]
Epoch [6/120    avg_loss:1.586, val_acc:0.135]
Epoch [7/120    avg_loss:1.491, val_acc:0.180]
Epoch [8/120    avg_loss:1.395, val_acc:0.234]
Epoch [9/120    avg_loss:1.322, val_acc:0.337]
Epoch [10/120    avg_loss:1.251, val_acc:0.394]
Epoch [11/120    avg_loss:1.175, val_acc:0.441]
Epoch [12/120    avg_loss:1.109, val_acc:0.450]
Epoch [13/120    avg_loss:1.034, val_acc:0.497]
Epoch [14/120    avg_loss:0.952, val_acc:0.491]
Epoch [15/120    avg_loss:0.814, val_acc:0.517]
Epoch [16/120    avg_loss:0.769, val_acc:0.516]
Epoch [17/120    avg_loss:0.710, val_acc:0.497]
Epoch [18/120    avg_loss:0.607, val_acc:0.545]
Epoch [19/120    avg_loss:0.529, val_acc:0.612]
Epoch [20/120    avg_loss:0.532, val_acc:0.597]
Epoch [21/120    avg_loss:0.555, val_acc:0.636]
Epoch [22/120    avg_loss:0.472, val_acc:0.697]
Epoch [23/120    avg_loss:0.369, val_acc:0.736]
Epoch [24/120    avg_loss:0.361, val_acc:0.795]
Epoch [25/120    avg_loss:0.362, val_acc:0.787]
Epoch [26/120    avg_loss:0.300, val_acc:0.827]
Epoch [27/120    avg_loss:0.288, val_acc:0.849]
Epoch [28/120    avg_loss:0.272, val_acc:0.933]
Epoch [29/120    avg_loss:0.244, val_acc:0.920]
Epoch [30/120    avg_loss:0.252, val_acc:0.904]
Epoch [31/120    avg_loss:1.124, val_acc:0.481]
Epoch [32/120    avg_loss:0.762, val_acc:0.701]
Epoch [33/120    avg_loss:0.571, val_acc:0.687]
Epoch [34/120    avg_loss:0.437, val_acc:0.752]
Epoch [35/120    avg_loss:0.400, val_acc:0.765]
Epoch [36/120    avg_loss:0.343, val_acc:0.769]
Epoch [37/120    avg_loss:0.298, val_acc:0.822]
Epoch [38/120    avg_loss:0.296, val_acc:0.826]
Epoch [39/120    avg_loss:0.274, val_acc:0.829]
Epoch [40/120    avg_loss:0.241, val_acc:0.850]
Epoch [41/120    avg_loss:0.221, val_acc:0.877]
Epoch [42/120    avg_loss:0.207, val_acc:0.877]
Epoch [43/120    avg_loss:0.200, val_acc:0.873]
Epoch [44/120    avg_loss:0.188, val_acc:0.883]
Epoch [45/120    avg_loss:0.189, val_acc:0.886]
Epoch [46/120    avg_loss:0.174, val_acc:0.892]
Epoch [47/120    avg_loss:0.169, val_acc:0.893]
Epoch [48/120    avg_loss:0.175, val_acc:0.896]
Epoch [49/120    avg_loss:0.184, val_acc:0.897]
Epoch [50/120    avg_loss:0.175, val_acc:0.905]
Epoch [51/120    avg_loss:0.161, val_acc:0.907]
Epoch [52/120    avg_loss:0.173, val_acc:0.909]
Epoch [53/120    avg_loss:0.161, val_acc:0.909]
Epoch [54/120    avg_loss:0.156, val_acc:0.917]
Epoch [55/120    avg_loss:0.164, val_acc:0.917]
Epoch [56/120    avg_loss:0.157, val_acc:0.916]
Epoch [57/120    avg_loss:0.152, val_acc:0.915]
Epoch [58/120    avg_loss:0.159, val_acc:0.915]
Epoch [59/120    avg_loss:0.156, val_acc:0.915]
Epoch [60/120    avg_loss:0.161, val_acc:0.915]
Epoch [61/120    avg_loss:0.158, val_acc:0.915]
Epoch [62/120    avg_loss:0.154, val_acc:0.915]
Epoch [63/120    avg_loss:0.168, val_acc:0.915]
Epoch [64/120    avg_loss:0.157, val_acc:0.915]
Epoch [65/120    avg_loss:0.158, val_acc:0.915]
Epoch [66/120    avg_loss:0.155, val_acc:0.915]
Epoch [67/120    avg_loss:0.161, val_acc:0.915]
Epoch [68/120    avg_loss:0.148, val_acc:0.915]
Epoch [69/120    avg_loss:0.162, val_acc:0.915]
Epoch [70/120    avg_loss:0.154, val_acc:0.915]
Epoch [71/120    avg_loss:0.154, val_acc:0.915]
Epoch [72/120    avg_loss:0.148, val_acc:0.915]
Epoch [73/120    avg_loss:0.154, val_acc:0.915]
Epoch [74/120    avg_loss:0.169, val_acc:0.915]
Epoch [75/120    avg_loss:0.166, val_acc:0.915]
Epoch [76/120    avg_loss:0.169, val_acc:0.915]
Epoch [77/120    avg_loss:0.164, val_acc:0.915]
Epoch [78/120    avg_loss:0.165, val_acc:0.915]
Epoch [79/120    avg_loss:0.158, val_acc:0.915]
Epoch [80/120    avg_loss:0.157, val_acc:0.915]
Epoch [81/120    avg_loss:0.152, val_acc:0.915]
Epoch [82/120    avg_loss:0.153, val_acc:0.915]
Epoch [83/120    avg_loss:0.153, val_acc:0.915]
Epoch [84/120    avg_loss:0.156, val_acc:0.915]
Epoch [85/120    avg_loss:0.151, val_acc:0.915]
Epoch [86/120    avg_loss:0.148, val_acc:0.915]
Epoch [87/120    avg_loss:0.151, val_acc:0.915]
Epoch [88/120    avg_loss:0.155, val_acc:0.915]
Epoch [89/120    avg_loss:0.152, val_acc:0.915]
Epoch [90/120    avg_loss:0.161, val_acc:0.915]
Epoch [91/120    avg_loss:0.150, val_acc:0.915]
Epoch [92/120    avg_loss:0.159, val_acc:0.915]
Epoch [93/120    avg_loss:0.157, val_acc:0.915]
Epoch [94/120    avg_loss:0.156, val_acc:0.915]
Epoch [95/120    avg_loss:0.159, val_acc:0.915]
Epoch [96/120    avg_loss:0.155, val_acc:0.915]
Epoch [97/120    avg_loss:0.149, val_acc:0.915]
Epoch [98/120    avg_loss:0.157, val_acc:0.915]
Epoch [99/120    avg_loss:0.152, val_acc:0.915]
Epoch [100/120    avg_loss:0.152, val_acc:0.915]
Epoch [101/120    avg_loss:0.166, val_acc:0.915]
Epoch [102/120    avg_loss:0.151, val_acc:0.915]
Epoch [103/120    avg_loss:0.154, val_acc:0.915]
Epoch [104/120    avg_loss:0.151, val_acc:0.915]
Epoch [105/120    avg_loss:0.149, val_acc:0.915]
Epoch [106/120    avg_loss:0.155, val_acc:0.915]
Epoch [107/120    avg_loss:0.161, val_acc:0.915]
Epoch [108/120    avg_loss:0.151, val_acc:0.915]
Epoch [109/120    avg_loss:0.152, val_acc:0.915]
Epoch [110/120    avg_loss:0.157, val_acc:0.915]
Epoch [111/120    avg_loss:0.157, val_acc:0.915]
Epoch [112/120    avg_loss:0.158, val_acc:0.915]
Epoch [113/120    avg_loss:0.152, val_acc:0.915]
Epoch [114/120    avg_loss:0.154, val_acc:0.915]
Epoch [115/120    avg_loss:0.153, val_acc:0.915]
Epoch [116/120    avg_loss:0.154, val_acc:0.915]
Epoch [117/120    avg_loss:0.152, val_acc:0.915]
Epoch [118/120    avg_loss:0.158, val_acc:0.915]
Epoch [119/120    avg_loss:0.161, val_acc:0.915]
Epoch [120/120    avg_loss:0.147, val_acc:0.915]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6188     0    41    18     0     0    20   142    23]
 [    0     0 15434     0    11     0  2644     0     1     0]
 [    0     5     0  1902     0     0     0     0   118    11]
 [    0    47    37     1  2858     0     4     0    24     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   188    27     0     0  4571     0    92     0]
 [    0     6     0     0     0     0     0  1279     0     5]
 [    0    61     0    85    59     0     3     0  3363     0]
 [    0     0     0     0    14    35     0     0     0   870]]

Accuracy:
91.02740221242138

F1 scores:
[       nan 0.97150483 0.91463451 0.92961877 0.96358732 0.98676749
 0.75553719 0.98802626 0.91998359 0.95133953]

Kappa:
0.8840254417388723
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16959d7b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.217, val_acc:0.052]
Epoch [2/120    avg_loss:2.053, val_acc:0.080]
Epoch [3/120    avg_loss:1.944, val_acc:0.099]
Epoch [4/120    avg_loss:1.814, val_acc:0.124]
Epoch [5/120    avg_loss:1.757, val_acc:0.141]
Epoch [6/120    avg_loss:1.635, val_acc:0.152]
Epoch [7/120    avg_loss:1.530, val_acc:0.171]
Epoch [8/120    avg_loss:1.456, val_acc:0.203]
Epoch [9/120    avg_loss:1.352, val_acc:0.265]
Epoch [10/120    avg_loss:1.274, val_acc:0.365]
Epoch [11/120    avg_loss:1.195, val_acc:0.427]
Epoch [12/120    avg_loss:1.097, val_acc:0.448]
Epoch [13/120    avg_loss:1.028, val_acc:0.497]
Epoch [14/120    avg_loss:0.965, val_acc:0.453]
Epoch [15/120    avg_loss:0.908, val_acc:0.522]
Epoch [16/120    avg_loss:0.821, val_acc:0.512]
Epoch [17/120    avg_loss:0.708, val_acc:0.543]
Epoch [18/120    avg_loss:0.642, val_acc:0.580]
Epoch [19/120    avg_loss:0.536, val_acc:0.593]
Epoch [20/120    avg_loss:0.510, val_acc:0.598]
Epoch [21/120    avg_loss:0.490, val_acc:0.642]
Epoch [22/120    avg_loss:0.438, val_acc:0.664]
Epoch [23/120    avg_loss:0.411, val_acc:0.722]
Epoch [24/120    avg_loss:0.920, val_acc:0.497]
Epoch [25/120    avg_loss:0.781, val_acc:0.618]
Epoch [26/120    avg_loss:0.496, val_acc:0.642]
Epoch [27/120    avg_loss:0.456, val_acc:0.650]
Epoch [28/120    avg_loss:0.359, val_acc:0.743]
Epoch [29/120    avg_loss:0.325, val_acc:0.722]
Epoch [30/120    avg_loss:0.315, val_acc:0.775]
Epoch [31/120    avg_loss:0.284, val_acc:0.773]
Epoch [32/120    avg_loss:0.290, val_acc:0.809]
Epoch [33/120    avg_loss:0.294, val_acc:0.827]
Epoch [34/120    avg_loss:0.259, val_acc:0.837]
Epoch [35/120    avg_loss:0.239, val_acc:0.829]
Epoch [36/120    avg_loss:0.228, val_acc:0.832]
Epoch [37/120    avg_loss:0.236, val_acc:0.858]
Epoch [38/120    avg_loss:0.236, val_acc:0.891]
Epoch [39/120    avg_loss:0.210, val_acc:0.840]
Epoch [40/120    avg_loss:0.206, val_acc:0.890]
Epoch [41/120    avg_loss:0.165, val_acc:0.922]
Epoch [42/120    avg_loss:0.179, val_acc:0.890]
Epoch [43/120    avg_loss:0.150, val_acc:0.944]
Epoch [44/120    avg_loss:0.145, val_acc:0.948]
Epoch [45/120    avg_loss:0.132, val_acc:0.953]
Epoch [46/120    avg_loss:0.126, val_acc:0.892]
Epoch [47/120    avg_loss:0.118, val_acc:0.956]
Epoch [48/120    avg_loss:0.107, val_acc:0.947]
Epoch [49/120    avg_loss:0.080, val_acc:0.968]
Epoch [50/120    avg_loss:0.073, val_acc:0.973]
Epoch [51/120    avg_loss:0.057, val_acc:0.974]
Epoch [52/120    avg_loss:0.056, val_acc:0.977]
Epoch [53/120    avg_loss:0.057, val_acc:0.967]
Epoch [54/120    avg_loss:0.071, val_acc:0.968]
Epoch [55/120    avg_loss:0.070, val_acc:0.956]
Epoch [56/120    avg_loss:0.109, val_acc:0.950]
Epoch [57/120    avg_loss:0.073, val_acc:0.968]
Epoch [58/120    avg_loss:0.056, val_acc:0.972]
Epoch [59/120    avg_loss:0.046, val_acc:0.980]
Epoch [60/120    avg_loss:0.044, val_acc:0.972]
Epoch [61/120    avg_loss:0.046, val_acc:0.980]
Epoch [62/120    avg_loss:0.050, val_acc:0.984]
Epoch [63/120    avg_loss:0.039, val_acc:0.985]
Epoch [64/120    avg_loss:0.048, val_acc:0.940]
Epoch [65/120    avg_loss:0.069, val_acc:0.964]
Epoch [66/120    avg_loss:0.044, val_acc:0.968]
Epoch [67/120    avg_loss:0.029, val_acc:0.948]
Epoch [68/120    avg_loss:0.037, val_acc:0.985]
Epoch [69/120    avg_loss:0.028, val_acc:0.984]
Epoch [70/120    avg_loss:0.022, val_acc:0.983]
Epoch [71/120    avg_loss:0.025, val_acc:0.983]
Epoch [72/120    avg_loss:0.026, val_acc:0.976]
Epoch [73/120    avg_loss:0.023, val_acc:0.981]
Epoch [74/120    avg_loss:0.024, val_acc:0.977]
Epoch [75/120    avg_loss:0.019, val_acc:0.983]
Epoch [76/120    avg_loss:0.029, val_acc:0.983]
Epoch [77/120    avg_loss:0.033, val_acc:0.973]
Epoch [78/120    avg_loss:0.026, val_acc:0.978]
Epoch [79/120    avg_loss:0.028, val_acc:0.988]
Epoch [80/120    avg_loss:0.038, val_acc:0.968]
Epoch [81/120    avg_loss:0.044, val_acc:0.980]
Epoch [82/120    avg_loss:0.049, val_acc:0.983]
Epoch [83/120    avg_loss:0.040, val_acc:0.973]
Epoch [84/120    avg_loss:0.054, val_acc:0.961]
Epoch [85/120    avg_loss:0.053, val_acc:0.966]
Epoch [86/120    avg_loss:0.040, val_acc:0.974]
Epoch [87/120    avg_loss:0.027, val_acc:0.983]
Epoch [88/120    avg_loss:0.018, val_acc:0.985]
Epoch [89/120    avg_loss:0.025, val_acc:0.987]
Epoch [90/120    avg_loss:0.018, val_acc:0.977]
Epoch [91/120    avg_loss:0.018, val_acc:0.944]
Epoch [92/120    avg_loss:0.016, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.012, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.011, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.011, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     0     0     0     0    40     0]
 [    0     4 18060     0     9     0    14     0     3     0]
 [    0    11     0  1986     0     0     0     1    38     0]
 [    0    16     7     0  2944     0     1     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     8     0     0  4858     0     0     0]
 [    0     0     0     0     0     4     0  1284     0     2]
 [    0    41     0    18    44     0     0     0  3467     1]
 [    0     0     0     3    14    13     0     0     0   889]]

Accuracy:
99.25770611910443

F1 scores:
[       nan 0.99131514 0.99864525 0.98049864 0.98412168 0.99352874
 0.99641062 0.99728155 0.97346624 0.98177802]

Kappa:
0.9901643191753867
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9212615b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 15290==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.224, val_acc:0.172]
Epoch [2/120    avg_loss:1.963, val_acc:0.148]
Epoch [3/120    avg_loss:1.807, val_acc:0.173]
Epoch [4/120    avg_loss:1.686, val_acc:0.250]
Epoch [5/120    avg_loss:1.562, val_acc:0.206]
Epoch [6/120    avg_loss:1.462, val_acc:0.223]
Epoch [7/120    avg_loss:1.362, val_acc:0.235]
Epoch [8/120    avg_loss:1.293, val_acc:0.255]
Epoch [9/120    avg_loss:1.223, val_acc:0.297]
Epoch [10/120    avg_loss:1.158, val_acc:0.338]
Epoch [11/120    avg_loss:1.060, val_acc:0.413]
Epoch [12/120    avg_loss:0.984, val_acc:0.478]
Epoch [13/120    avg_loss:0.933, val_acc:0.502]
Epoch [14/120    avg_loss:0.847, val_acc:0.539]
Epoch [15/120    avg_loss:0.801, val_acc:0.549]
Epoch [16/120    avg_loss:0.738, val_acc:0.603]
Epoch [17/120    avg_loss:0.697, val_acc:0.677]
Epoch [18/120    avg_loss:0.604, val_acc:0.690]
Epoch [19/120    avg_loss:0.567, val_acc:0.758]
Epoch [20/120    avg_loss:0.502, val_acc:0.816]
Epoch [21/120    avg_loss:0.471, val_acc:0.784]
Epoch [22/120    avg_loss:0.440, val_acc:0.798]
Epoch [23/120    avg_loss:0.373, val_acc:0.843]
Epoch [24/120    avg_loss:0.356, val_acc:0.816]
Epoch [25/120    avg_loss:0.341, val_acc:0.829]
Epoch [26/120    avg_loss:0.317, val_acc:0.846]
Epoch [27/120    avg_loss:0.293, val_acc:0.855]
Epoch [28/120    avg_loss:0.302, val_acc:0.856]
Epoch [29/120    avg_loss:0.267, val_acc:0.852]
Epoch [30/120    avg_loss:0.258, val_acc:0.872]
Epoch [31/120    avg_loss:0.233, val_acc:0.850]
Epoch [32/120    avg_loss:0.206, val_acc:0.896]
Epoch [33/120    avg_loss:0.198, val_acc:0.883]
Epoch [34/120    avg_loss:0.198, val_acc:0.901]
Epoch [35/120    avg_loss:0.217, val_acc:0.890]
Epoch [36/120    avg_loss:0.194, val_acc:0.873]
Epoch [37/120    avg_loss:0.172, val_acc:0.921]
Epoch [38/120    avg_loss:0.163, val_acc:0.860]
Epoch [39/120    avg_loss:0.176, val_acc:0.914]
Epoch [40/120    avg_loss:0.137, val_acc:0.926]
Epoch [41/120    avg_loss:0.155, val_acc:0.898]
Epoch [42/120    avg_loss:0.147, val_acc:0.941]
Epoch [43/120    avg_loss:0.141, val_acc:0.952]
Epoch [44/120    avg_loss:0.107, val_acc:0.957]
Epoch [45/120    avg_loss:0.112, val_acc:0.939]
Epoch [46/120    avg_loss:0.088, val_acc:0.964]
Epoch [47/120    avg_loss:0.095, val_acc:0.956]
Epoch [48/120    avg_loss:0.083, val_acc:0.975]
Epoch [49/120    avg_loss:0.071, val_acc:0.910]
Epoch [50/120    avg_loss:0.109, val_acc:0.956]
Epoch [51/120    avg_loss:0.079, val_acc:0.976]
Epoch [52/120    avg_loss:0.064, val_acc:0.975]
Epoch [53/120    avg_loss:0.076, val_acc:0.960]
Epoch [54/120    avg_loss:0.079, val_acc:0.953]
Epoch [55/120    avg_loss:0.081, val_acc:0.949]
Epoch [56/120    avg_loss:0.077, val_acc:0.953]
Epoch [57/120    avg_loss:0.055, val_acc:0.971]
Epoch [58/120    avg_loss:0.052, val_acc:0.974]
Epoch [59/120    avg_loss:0.045, val_acc:0.976]
Epoch [60/120    avg_loss:0.065, val_acc:0.970]
Epoch [61/120    avg_loss:0.056, val_acc:0.978]
Epoch [62/120    avg_loss:0.042, val_acc:0.979]
Epoch [63/120    avg_loss:0.035, val_acc:0.984]
Epoch [64/120    avg_loss:0.058, val_acc:0.971]
Epoch [65/120    avg_loss:0.067, val_acc:0.970]
Epoch [66/120    avg_loss:0.056, val_acc:0.975]
Epoch [67/120    avg_loss:0.048, val_acc:0.977]
Epoch [68/120    avg_loss:0.054, val_acc:0.978]
Epoch [69/120    avg_loss:0.037, val_acc:0.983]
Epoch [70/120    avg_loss:0.033, val_acc:0.977]
Epoch [71/120    avg_loss:0.038, val_acc:0.984]
Epoch [72/120    avg_loss:0.029, val_acc:0.978]
Epoch [73/120    avg_loss:0.034, val_acc:0.985]
Epoch [74/120    avg_loss:0.044, val_acc:0.962]
Epoch [75/120    avg_loss:0.054, val_acc:0.978]
Epoch [76/120    avg_loss:0.036, val_acc:0.982]
Epoch [77/120    avg_loss:0.028, val_acc:0.985]
Epoch [78/120    avg_loss:0.026, val_acc:0.979]
Epoch [79/120    avg_loss:0.027, val_acc:0.990]
Epoch [80/120    avg_loss:0.025, val_acc:0.985]
Epoch [81/120    avg_loss:0.019, val_acc:0.984]
Epoch [82/120    avg_loss:0.019, val_acc:0.986]
Epoch [83/120    avg_loss:0.022, val_acc:0.980]
Epoch [84/120    avg_loss:0.042, val_acc:0.984]
Epoch [85/120    avg_loss:0.029, val_acc:0.986]
Epoch [86/120    avg_loss:0.019, val_acc:0.979]
Epoch [87/120    avg_loss:0.026, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.987]
Epoch [90/120    avg_loss:0.020, val_acc:0.968]
Epoch [91/120    avg_loss:0.037, val_acc:0.985]
Epoch [92/120    avg_loss:0.053, val_acc:0.959]
Epoch [93/120    avg_loss:0.050, val_acc:0.978]
Epoch [94/120    avg_loss:0.027, val_acc:0.982]
Epoch [95/120    avg_loss:0.020, val_acc:0.979]
Epoch [96/120    avg_loss:0.034, val_acc:0.985]
Epoch [97/120    avg_loss:0.024, val_acc:0.987]
Epoch [98/120    avg_loss:0.022, val_acc:0.988]
Epoch [99/120    avg_loss:0.021, val_acc:0.990]
Epoch [100/120    avg_loss:0.016, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.987]
Epoch [102/120    avg_loss:0.015, val_acc:0.982]
Epoch [103/120    avg_loss:0.019, val_acc:0.981]
Epoch [104/120    avg_loss:0.014, val_acc:0.989]
Epoch [105/120    avg_loss:0.011, val_acc:0.987]
Epoch [106/120    avg_loss:0.014, val_acc:0.989]
Epoch [107/120    avg_loss:0.014, val_acc:0.979]
Epoch [108/120    avg_loss:0.016, val_acc:0.992]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.989]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.992]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.993]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.014, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.991]
Epoch [120/120    avg_loss:0.008, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6388     0     0     2     0     0     4    35     3]
 [    0     0 18014     0    40     0    35     0     1     0]
 [    0     7     0  2004     0     0     0     0    23     2]
 [    0    33    23     0  2878     0     8     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     9     0     0  4860     0     0     3]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    21     0     7    57     0     0     0  3486     0]
 [    0     0     0     0    15    25     0     0     0   879]]

Accuracy:
99.06008242354132

F1 scores:
[       nan 0.99184846 0.99709407 0.98816568 0.96512408 0.99051233
 0.99376342 0.99806427 0.97565071 0.97288323]

Kappa:
0.9875509835722067
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1432319ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.165, val_acc:0.072]
Epoch [2/120    avg_loss:2.006, val_acc:0.068]
Epoch [3/120    avg_loss:1.900, val_acc:0.084]
Epoch [4/120    avg_loss:1.827, val_acc:0.109]
Epoch [5/120    avg_loss:1.755, val_acc:0.128]
Epoch [6/120    avg_loss:1.656, val_acc:0.131]
Epoch [7/120    avg_loss:1.591, val_acc:0.149]
Epoch [8/120    avg_loss:1.482, val_acc:0.234]
Epoch [9/120    avg_loss:1.404, val_acc:0.310]
Epoch [10/120    avg_loss:1.345, val_acc:0.358]
Epoch [11/120    avg_loss:1.273, val_acc:0.366]
Epoch [12/120    avg_loss:1.241, val_acc:0.411]
Epoch [13/120    avg_loss:1.169, val_acc:0.397]
Epoch [14/120    avg_loss:1.088, val_acc:0.440]
Epoch [15/120    avg_loss:1.032, val_acc:0.474]
Epoch [16/120    avg_loss:0.917, val_acc:0.504]
Epoch [17/120    avg_loss:0.850, val_acc:0.583]
Epoch [18/120    avg_loss:0.747, val_acc:0.685]
Epoch [19/120    avg_loss:0.665, val_acc:0.734]
Epoch [20/120    avg_loss:0.617, val_acc:0.778]
Epoch [21/120    avg_loss:0.534, val_acc:0.784]
Epoch [22/120    avg_loss:0.488, val_acc:0.804]
Epoch [23/120    avg_loss:0.432, val_acc:0.794]
Epoch [24/120    avg_loss:0.371, val_acc:0.834]
Epoch [25/120    avg_loss:0.336, val_acc:0.878]
Epoch [26/120    avg_loss:0.309, val_acc:0.871]
Epoch [27/120    avg_loss:0.306, val_acc:0.891]
Epoch [28/120    avg_loss:0.278, val_acc:0.880]
Epoch [29/120    avg_loss:0.256, val_acc:0.926]
Epoch [30/120    avg_loss:0.207, val_acc:0.928]
Epoch [31/120    avg_loss:0.203, val_acc:0.926]
Epoch [32/120    avg_loss:0.512, val_acc:0.493]
Epoch [33/120    avg_loss:1.322, val_acc:0.500]
Epoch [34/120    avg_loss:1.174, val_acc:0.560]
Epoch [35/120    avg_loss:1.067, val_acc:0.629]
Epoch [36/120    avg_loss:1.008, val_acc:0.619]
Epoch [37/120    avg_loss:0.938, val_acc:0.593]
Epoch [38/120    avg_loss:0.896, val_acc:0.707]
Epoch [39/120    avg_loss:0.818, val_acc:0.751]
Epoch [40/120    avg_loss:0.796, val_acc:0.704]
Epoch [41/120    avg_loss:0.797, val_acc:0.715]
Epoch [42/120    avg_loss:0.770, val_acc:0.675]
Epoch [43/120    avg_loss:0.705, val_acc:0.725]
Epoch [44/120    avg_loss:0.606, val_acc:0.736]
Epoch [45/120    avg_loss:0.599, val_acc:0.755]
Epoch [46/120    avg_loss:0.587, val_acc:0.739]
Epoch [47/120    avg_loss:0.564, val_acc:0.760]
Epoch [48/120    avg_loss:0.589, val_acc:0.759]
Epoch [49/120    avg_loss:0.599, val_acc:0.754]
Epoch [50/120    avg_loss:0.570, val_acc:0.747]
Epoch [51/120    avg_loss:0.541, val_acc:0.760]
Epoch [52/120    avg_loss:0.547, val_acc:0.766]
Epoch [53/120    avg_loss:0.550, val_acc:0.747]
Epoch [54/120    avg_loss:0.541, val_acc:0.771]
Epoch [55/120    avg_loss:0.540, val_acc:0.769]
Epoch [56/120    avg_loss:0.527, val_acc:0.759]
Epoch [57/120    avg_loss:0.502, val_acc:0.766]
Epoch [58/120    avg_loss:0.499, val_acc:0.765]
Epoch [59/120    avg_loss:0.503, val_acc:0.762]
Epoch [60/120    avg_loss:0.497, val_acc:0.763]
Epoch [61/120    avg_loss:0.506, val_acc:0.760]
Epoch [62/120    avg_loss:0.508, val_acc:0.764]
Epoch [63/120    avg_loss:0.505, val_acc:0.762]
Epoch [64/120    avg_loss:0.509, val_acc:0.766]
Epoch [65/120    avg_loss:0.512, val_acc:0.763]
Epoch [66/120    avg_loss:0.498, val_acc:0.760]
Epoch [67/120    avg_loss:0.497, val_acc:0.763]
Epoch [68/120    avg_loss:0.502, val_acc:0.761]
Epoch [69/120    avg_loss:0.509, val_acc:0.762]
Epoch [70/120    avg_loss:0.516, val_acc:0.762]
Epoch [71/120    avg_loss:0.512, val_acc:0.762]
Epoch [72/120    avg_loss:0.515, val_acc:0.761]
Epoch [73/120    avg_loss:0.488, val_acc:0.762]
Epoch [74/120    avg_loss:0.508, val_acc:0.762]
Epoch [75/120    avg_loss:0.504, val_acc:0.762]
Epoch [76/120    avg_loss:0.513, val_acc:0.762]
Epoch [77/120    avg_loss:0.494, val_acc:0.762]
Epoch [78/120    avg_loss:0.515, val_acc:0.762]
Epoch [79/120    avg_loss:0.517, val_acc:0.762]
Epoch [80/120    avg_loss:0.512, val_acc:0.763]
Epoch [81/120    avg_loss:0.494, val_acc:0.762]
Epoch [82/120    avg_loss:0.483, val_acc:0.763]
Epoch [83/120    avg_loss:0.502, val_acc:0.763]
Epoch [84/120    avg_loss:0.511, val_acc:0.763]
Epoch [85/120    avg_loss:0.506, val_acc:0.763]
Epoch [86/120    avg_loss:0.504, val_acc:0.763]
Epoch [87/120    avg_loss:0.501, val_acc:0.763]
Epoch [88/120    avg_loss:0.510, val_acc:0.763]
Epoch [89/120    avg_loss:0.498, val_acc:0.763]
Epoch [90/120    avg_loss:0.505, val_acc:0.763]
Epoch [91/120    avg_loss:0.505, val_acc:0.763]
Epoch [92/120    avg_loss:0.507, val_acc:0.763]
Epoch [93/120    avg_loss:0.484, val_acc:0.763]
Epoch [94/120    avg_loss:0.523, val_acc:0.763]
Epoch [95/120    avg_loss:0.504, val_acc:0.763]
Epoch [96/120    avg_loss:0.493, val_acc:0.763]
Epoch [97/120    avg_loss:0.507, val_acc:0.763]
Epoch [98/120    avg_loss:0.499, val_acc:0.763]
Epoch [99/120    avg_loss:0.497, val_acc:0.763]
Epoch [100/120    avg_loss:0.511, val_acc:0.763]
Epoch [101/120    avg_loss:0.515, val_acc:0.763]
Epoch [102/120    avg_loss:0.499, val_acc:0.763]
Epoch [103/120    avg_loss:0.506, val_acc:0.763]
Epoch [104/120    avg_loss:0.497, val_acc:0.763]
Epoch [105/120    avg_loss:0.499, val_acc:0.763]
Epoch [106/120    avg_loss:0.504, val_acc:0.763]
Epoch [107/120    avg_loss:0.503, val_acc:0.763]
Epoch [108/120    avg_loss:0.481, val_acc:0.763]
Epoch [109/120    avg_loss:0.515, val_acc:0.763]
Epoch [110/120    avg_loss:0.503, val_acc:0.763]
Epoch [111/120    avg_loss:0.505, val_acc:0.763]
Epoch [112/120    avg_loss:0.495, val_acc:0.763]
Epoch [113/120    avg_loss:0.478, val_acc:0.763]
Epoch [114/120    avg_loss:0.489, val_acc:0.763]
Epoch [115/120    avg_loss:0.499, val_acc:0.763]
Epoch [116/120    avg_loss:0.502, val_acc:0.763]
Epoch [117/120    avg_loss:0.497, val_acc:0.763]
Epoch [118/120    avg_loss:0.486, val_acc:0.763]
Epoch [119/120    avg_loss:0.526, val_acc:0.763]
Epoch [120/120    avg_loss:0.523, val_acc:0.763]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4813     0   258   407     0     0     1   578   375]
 [    0     0 13121     0   682     0  4287     0     0     0]
 [    0     0     0  1723     2     0     0     0   220    91]
 [    0    97    83     0  2660     0    49     0    79     4]
 [    0     0     0     0     0  1301     0     0     0     4]
 [    0    10   329     0    91     0  4401     0    47     0]
 [    0    15     0     0     0     0     0  1267     0     8]
 [    0    23     0   143    87     0     0     0  3318     0]
 [    0    27     0    14    27   123     0     0     1   727]]

Accuracy:
80.32921215626732

F1 scores:
[       nan 0.84312867 0.82983904 0.82558697 0.76789838 0.95346281
 0.64649284 0.99061767 0.84924494 0.68327068]

Kappa:
0.7521305499998208
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79d9ed2b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.237, val_acc:0.192]
Epoch [2/120    avg_loss:2.044, val_acc:0.191]
Epoch [3/120    avg_loss:1.905, val_acc:0.223]
Epoch [4/120    avg_loss:1.805, val_acc:0.234]
Epoch [5/120    avg_loss:1.705, val_acc:0.239]
Epoch [6/120    avg_loss:1.628, val_acc:0.243]
Epoch [7/120    avg_loss:1.517, val_acc:0.279]
Epoch [8/120    avg_loss:1.435, val_acc:0.316]
Epoch [9/120    avg_loss:1.369, val_acc:0.330]
Epoch [10/120    avg_loss:1.293, val_acc:0.341]
Epoch [11/120    avg_loss:1.231, val_acc:0.385]
Epoch [12/120    avg_loss:1.170, val_acc:0.406]
Epoch [13/120    avg_loss:1.070, val_acc:0.447]
Epoch [14/120    avg_loss:1.018, val_acc:0.449]
Epoch [15/120    avg_loss:0.964, val_acc:0.465]
Epoch [16/120    avg_loss:0.870, val_acc:0.468]
Epoch [17/120    avg_loss:0.846, val_acc:0.535]
Epoch [18/120    avg_loss:0.816, val_acc:0.651]
Epoch [19/120    avg_loss:0.713, val_acc:0.677]
Epoch [20/120    avg_loss:0.665, val_acc:0.669]
Epoch [21/120    avg_loss:0.642, val_acc:0.716]
Epoch [22/120    avg_loss:0.573, val_acc:0.711]
Epoch [23/120    avg_loss:0.584, val_acc:0.744]
Epoch [24/120    avg_loss:0.557, val_acc:0.702]
Epoch [25/120    avg_loss:0.438, val_acc:0.755]
Epoch [26/120    avg_loss:0.383, val_acc:0.760]
Epoch [27/120    avg_loss:0.369, val_acc:0.786]
Epoch [28/120    avg_loss:0.358, val_acc:0.811]
Epoch [29/120    avg_loss:0.319, val_acc:0.866]
Epoch [30/120    avg_loss:0.314, val_acc:0.841]
Epoch [31/120    avg_loss:0.277, val_acc:0.873]
Epoch [32/120    avg_loss:0.231, val_acc:0.911]
Epoch [33/120    avg_loss:0.232, val_acc:0.900]
Epoch [34/120    avg_loss:0.233, val_acc:0.932]
Epoch [35/120    avg_loss:0.186, val_acc:0.946]
Epoch [36/120    avg_loss:0.185, val_acc:0.931]
Epoch [37/120    avg_loss:0.208, val_acc:0.940]
Epoch [38/120    avg_loss:0.172, val_acc:0.957]
Epoch [39/120    avg_loss:0.149, val_acc:0.949]
Epoch [40/120    avg_loss:0.133, val_acc:0.951]
Epoch [41/120    avg_loss:0.111, val_acc:0.958]
Epoch [42/120    avg_loss:0.088, val_acc:0.960]
Epoch [43/120    avg_loss:0.093, val_acc:0.970]
Epoch [44/120    avg_loss:0.083, val_acc:0.952]
Epoch [45/120    avg_loss:0.116, val_acc:0.963]
Epoch [46/120    avg_loss:0.090, val_acc:0.963]
Epoch [47/120    avg_loss:0.081, val_acc:0.956]
Epoch [48/120    avg_loss:0.083, val_acc:0.951]
Epoch [49/120    avg_loss:0.079, val_acc:0.966]
Epoch [50/120    avg_loss:0.077, val_acc:0.973]
Epoch [51/120    avg_loss:0.062, val_acc:0.974]
Epoch [52/120    avg_loss:0.066, val_acc:0.973]
Epoch [53/120    avg_loss:0.057, val_acc:0.974]
Epoch [54/120    avg_loss:0.047, val_acc:0.970]
Epoch [55/120    avg_loss:0.055, val_acc:0.980]
Epoch [56/120    avg_loss:0.058, val_acc:0.969]
Epoch [57/120    avg_loss:0.067, val_acc:0.975]
Epoch [58/120    avg_loss:0.076, val_acc:0.962]
Epoch [59/120    avg_loss:0.138, val_acc:0.952]
Epoch [60/120    avg_loss:0.080, val_acc:0.972]
Epoch [61/120    avg_loss:0.052, val_acc:0.973]
Epoch [62/120    avg_loss:0.042, val_acc:0.978]
Epoch [63/120    avg_loss:0.043, val_acc:0.966]
Epoch [64/120    avg_loss:0.046, val_acc:0.972]
Epoch [65/120    avg_loss:0.041, val_acc:0.974]
Epoch [66/120    avg_loss:0.054, val_acc:0.974]
Epoch [67/120    avg_loss:0.075, val_acc:0.953]
Epoch [68/120    avg_loss:0.061, val_acc:0.967]
Epoch [69/120    avg_loss:0.056, val_acc:0.970]
Epoch [70/120    avg_loss:0.039, val_acc:0.971]
Epoch [71/120    avg_loss:0.030, val_acc:0.977]
Epoch [72/120    avg_loss:0.029, val_acc:0.976]
Epoch [73/120    avg_loss:0.027, val_acc:0.976]
Epoch [74/120    avg_loss:0.027, val_acc:0.975]
Epoch [75/120    avg_loss:0.024, val_acc:0.977]
Epoch [76/120    avg_loss:0.024, val_acc:0.978]
Epoch [77/120    avg_loss:0.025, val_acc:0.980]
Epoch [78/120    avg_loss:0.021, val_acc:0.980]
Epoch [79/120    avg_loss:0.021, val_acc:0.978]
Epoch [80/120    avg_loss:0.023, val_acc:0.979]
Epoch [81/120    avg_loss:0.024, val_acc:0.979]
Epoch [82/120    avg_loss:0.022, val_acc:0.979]
Epoch [83/120    avg_loss:0.020, val_acc:0.980]
Epoch [84/120    avg_loss:0.022, val_acc:0.980]
Epoch [85/120    avg_loss:0.021, val_acc:0.980]
Epoch [86/120    avg_loss:0.021, val_acc:0.981]
Epoch [87/120    avg_loss:0.022, val_acc:0.980]
Epoch [88/120    avg_loss:0.026, val_acc:0.979]
Epoch [89/120    avg_loss:0.020, val_acc:0.980]
Epoch [90/120    avg_loss:0.023, val_acc:0.982]
Epoch [91/120    avg_loss:0.021, val_acc:0.981]
Epoch [92/120    avg_loss:0.019, val_acc:0.981]
Epoch [93/120    avg_loss:0.021, val_acc:0.982]
Epoch [94/120    avg_loss:0.019, val_acc:0.981]
Epoch [95/120    avg_loss:0.023, val_acc:0.978]
Epoch [96/120    avg_loss:0.020, val_acc:0.981]
Epoch [97/120    avg_loss:0.018, val_acc:0.981]
Epoch [98/120    avg_loss:0.020, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.981]
Epoch [100/120    avg_loss:0.020, val_acc:0.981]
Epoch [101/120    avg_loss:0.021, val_acc:0.981]
Epoch [102/120    avg_loss:0.016, val_acc:0.981]
Epoch [103/120    avg_loss:0.019, val_acc:0.982]
Epoch [104/120    avg_loss:0.019, val_acc:0.981]
Epoch [105/120    avg_loss:0.018, val_acc:0.982]
Epoch [106/120    avg_loss:0.017, val_acc:0.981]
Epoch [107/120    avg_loss:0.016, val_acc:0.980]
Epoch [108/120    avg_loss:0.020, val_acc:0.981]
Epoch [109/120    avg_loss:0.019, val_acc:0.981]
Epoch [110/120    avg_loss:0.017, val_acc:0.980]
Epoch [111/120    avg_loss:0.020, val_acc:0.980]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.018, val_acc:0.981]
Epoch [114/120    avg_loss:0.018, val_acc:0.981]
Epoch [115/120    avg_loss:0.018, val_acc:0.981]
Epoch [116/120    avg_loss:0.016, val_acc:0.981]
Epoch [117/120    avg_loss:0.016, val_acc:0.981]
Epoch [118/120    avg_loss:0.016, val_acc:0.981]
Epoch [119/120    avg_loss:0.016, val_acc:0.981]
Epoch [120/120    avg_loss:0.018, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     6     0     0     5    23     3]
 [    0     2 18026     0    27     0    35     0     0     0]
 [    0    14     0  2002     0     0     0     0    18     2]
 [    0    46    18     0  2871     0    10     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    33    10     2     0     0  4833     0     0     0]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0    33     0    36    68     0     0     0  3434     0]
 [    0     0     0     8    21    62     0     0     0   828]]

Accuracy:
98.75882679006098

F1 scores:
[       nan 0.9872636  0.99745463 0.98041136 0.96261526 0.97679641
 0.99057184 0.99573478 0.97101654 0.94305239]

Kappa:
0.9835563814685507
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc840a4be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.340, val_acc:0.091]
Epoch [2/120    avg_loss:2.156, val_acc:0.102]
Epoch [3/120    avg_loss:2.011, val_acc:0.093]
Epoch [4/120    avg_loss:1.890, val_acc:0.119]
Epoch [5/120    avg_loss:1.784, val_acc:0.130]
Epoch [6/120    avg_loss:1.699, val_acc:0.133]
Epoch [7/120    avg_loss:1.574, val_acc:0.134]
Epoch [8/120    avg_loss:1.491, val_acc:0.174]
Epoch [9/120    avg_loss:1.430, val_acc:0.262]
Epoch [10/120    avg_loss:1.341, val_acc:0.317]
Epoch [11/120    avg_loss:1.295, val_acc:0.345]
Epoch [12/120    avg_loss:1.225, val_acc:0.390]
Epoch [13/120    avg_loss:1.167, val_acc:0.404]
Epoch [14/120    avg_loss:1.089, val_acc:0.434]
Epoch [15/120    avg_loss:0.969, val_acc:0.455]
Epoch [16/120    avg_loss:0.899, val_acc:0.490]
Epoch [17/120    avg_loss:0.808, val_acc:0.492]
Epoch [18/120    avg_loss:0.740, val_acc:0.522]
Epoch [19/120    avg_loss:0.694, val_acc:0.526]
Epoch [20/120    avg_loss:0.662, val_acc:0.543]
Epoch [21/120    avg_loss:0.546, val_acc:0.564]
Epoch [22/120    avg_loss:0.490, val_acc:0.566]
Epoch [23/120    avg_loss:0.428, val_acc:0.585]
Epoch [24/120    avg_loss:0.399, val_acc:0.629]
Epoch [25/120    avg_loss:0.386, val_acc:0.745]
Epoch [26/120    avg_loss:0.343, val_acc:0.779]
Epoch [27/120    avg_loss:0.314, val_acc:0.806]
Epoch [28/120    avg_loss:0.264, val_acc:0.820]
Epoch [29/120    avg_loss:0.246, val_acc:0.886]
Epoch [30/120    avg_loss:0.229, val_acc:0.903]
Epoch [31/120    avg_loss:0.222, val_acc:0.885]
Epoch [32/120    avg_loss:0.230, val_acc:0.872]
Epoch [33/120    avg_loss:0.337, val_acc:0.896]
Epoch [34/120    avg_loss:0.276, val_acc:0.924]
Epoch [35/120    avg_loss:0.215, val_acc:0.912]
Epoch [36/120    avg_loss:0.171, val_acc:0.938]
Epoch [37/120    avg_loss:0.157, val_acc:0.933]
Epoch [38/120    avg_loss:0.141, val_acc:0.936]
Epoch [39/120    avg_loss:0.134, val_acc:0.958]
Epoch [40/120    avg_loss:0.125, val_acc:0.950]
Epoch [41/120    avg_loss:0.140, val_acc:0.953]
Epoch [42/120    avg_loss:0.104, val_acc:0.974]
Epoch [43/120    avg_loss:0.090, val_acc:0.966]
Epoch [44/120    avg_loss:0.084, val_acc:0.954]
Epoch [45/120    avg_loss:0.076, val_acc:0.945]
Epoch [46/120    avg_loss:0.090, val_acc:0.969]
Epoch [47/120    avg_loss:0.067, val_acc:0.978]
Epoch [48/120    avg_loss:0.064, val_acc:0.974]
Epoch [49/120    avg_loss:0.059, val_acc:0.980]
Epoch [50/120    avg_loss:0.065, val_acc:0.981]
Epoch [51/120    avg_loss:0.059, val_acc:0.967]
Epoch [52/120    avg_loss:0.060, val_acc:0.981]
Epoch [53/120    avg_loss:0.050, val_acc:0.978]
Epoch [54/120    avg_loss:0.039, val_acc:0.980]
Epoch [55/120    avg_loss:0.044, val_acc:0.979]
Epoch [56/120    avg_loss:0.038, val_acc:0.978]
Epoch [57/120    avg_loss:0.036, val_acc:0.981]
Epoch [58/120    avg_loss:0.042, val_acc:0.983]
Epoch [59/120    avg_loss:0.029, val_acc:0.984]
Epoch [60/120    avg_loss:0.056, val_acc:0.978]
Epoch [61/120    avg_loss:0.049, val_acc:0.981]
Epoch [62/120    avg_loss:0.033, val_acc:0.977]
Epoch [63/120    avg_loss:0.038, val_acc:0.981]
Epoch [64/120    avg_loss:0.036, val_acc:0.976]
Epoch [65/120    avg_loss:0.040, val_acc:0.979]
Epoch [66/120    avg_loss:0.030, val_acc:0.981]
Epoch [67/120    avg_loss:0.035, val_acc:0.978]
Epoch [68/120    avg_loss:0.036, val_acc:0.984]
Epoch [69/120    avg_loss:0.023, val_acc:0.984]
Epoch [70/120    avg_loss:0.019, val_acc:0.981]
Epoch [71/120    avg_loss:0.029, val_acc:0.983]
Epoch [72/120    avg_loss:0.021, val_acc:0.987]
Epoch [73/120    avg_loss:0.021, val_acc:0.974]
Epoch [74/120    avg_loss:0.026, val_acc:0.981]
Epoch [75/120    avg_loss:0.018, val_acc:0.984]
Epoch [76/120    avg_loss:0.018, val_acc:0.986]
Epoch [77/120    avg_loss:0.019, val_acc:0.983]
Epoch [78/120    avg_loss:0.018, val_acc:0.982]
Epoch [79/120    avg_loss:0.016, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.014, val_acc:0.984]
Epoch [82/120    avg_loss:0.024, val_acc:0.982]
Epoch [83/120    avg_loss:0.045, val_acc:0.983]
Epoch [84/120    avg_loss:0.040, val_acc:0.981]
Epoch [85/120    avg_loss:0.027, val_acc:0.982]
Epoch [86/120    avg_loss:0.016, val_acc:0.983]
Epoch [87/120    avg_loss:0.017, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.983]
Epoch [89/120    avg_loss:0.017, val_acc:0.984]
Epoch [90/120    avg_loss:0.015, val_acc:0.984]
Epoch [91/120    avg_loss:0.018, val_acc:0.985]
Epoch [92/120    avg_loss:0.019, val_acc:0.984]
Epoch [93/120    avg_loss:0.016, val_acc:0.984]
Epoch [94/120    avg_loss:0.021, val_acc:0.984]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.985]
Epoch [97/120    avg_loss:0.019, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.984]
Epoch [99/120    avg_loss:0.016, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.984]
Epoch [102/120    avg_loss:0.014, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.017, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.985]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.013, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.013, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.012, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     0     0     0     0    53    10     3]
 [    0     3 18003     0    64     0    20     0     0     0]
 [    0     2     0  1992     0     0     0     0    40     2]
 [    0    38    25    15  2881     0     2     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4875     0     0     1]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    21     0    19    53     0     0     0  3478     0]
 [    0     0     0     0    14    78     0     0     0   827]]

Accuracy:
98.84076832236764

F1 scores:
[       nan 0.98950804 0.99684385 0.98079764 0.96290107 0.97098214
 0.99744246 0.97792998 0.97834037 0.94406393]

Kappa:
0.9846501656077505
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f46e61dab70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.246, val_acc:0.070]
Epoch [2/120    avg_loss:2.062, val_acc:0.081]
Epoch [3/120    avg_loss:1.927, val_acc:0.096]
Epoch [4/120    avg_loss:1.815, val_acc:0.138]
Epoch [5/120    avg_loss:1.712, val_acc:0.138]
Epoch [6/120    avg_loss:1.618, val_acc:0.135]
Epoch [7/120    avg_loss:1.510, val_acc:0.135]
Epoch [8/120    avg_loss:1.430, val_acc:0.159]
Epoch [9/120    avg_loss:1.380, val_acc:0.362]
Epoch [10/120    avg_loss:1.299, val_acc:0.507]
Epoch [11/120    avg_loss:1.255, val_acc:0.570]
Epoch [12/120    avg_loss:1.184, val_acc:0.618]
Epoch [13/120    avg_loss:1.096, val_acc:0.625]
Epoch [14/120    avg_loss:1.041, val_acc:0.669]
Epoch [15/120    avg_loss:0.973, val_acc:0.699]
Epoch [16/120    avg_loss:0.860, val_acc:0.713]
Epoch [17/120    avg_loss:0.759, val_acc:0.731]
Epoch [18/120    avg_loss:0.709, val_acc:0.710]
Epoch [19/120    avg_loss:0.666, val_acc:0.777]
Epoch [20/120    avg_loss:0.589, val_acc:0.757]
Epoch [21/120    avg_loss:0.564, val_acc:0.790]
Epoch [22/120    avg_loss:0.496, val_acc:0.791]
Epoch [23/120    avg_loss:0.461, val_acc:0.822]
Epoch [24/120    avg_loss:0.495, val_acc:0.815]
Epoch [25/120    avg_loss:0.439, val_acc:0.809]
Epoch [26/120    avg_loss:0.404, val_acc:0.821]
Epoch [27/120    avg_loss:0.368, val_acc:0.822]
Epoch [28/120    avg_loss:0.328, val_acc:0.852]
Epoch [29/120    avg_loss:0.303, val_acc:0.852]
Epoch [30/120    avg_loss:0.261, val_acc:0.859]
Epoch [31/120    avg_loss:0.223, val_acc:0.896]
Epoch [32/120    avg_loss:0.244, val_acc:0.887]
Epoch [33/120    avg_loss:0.223, val_acc:0.939]
Epoch [34/120    avg_loss:0.250, val_acc:0.914]
Epoch [35/120    avg_loss:0.215, val_acc:0.888]
Epoch [36/120    avg_loss:0.177, val_acc:0.927]
Epoch [37/120    avg_loss:0.196, val_acc:0.946]
Epoch [38/120    avg_loss:0.159, val_acc:0.904]
Epoch [39/120    avg_loss:0.173, val_acc:0.921]
Epoch [40/120    avg_loss:0.181, val_acc:0.953]
Epoch [41/120    avg_loss:0.140, val_acc:0.954]
Epoch [42/120    avg_loss:0.120, val_acc:0.954]
Epoch [43/120    avg_loss:0.114, val_acc:0.943]
Epoch [44/120    avg_loss:0.108, val_acc:0.961]
Epoch [45/120    avg_loss:0.103, val_acc:0.969]
Epoch [46/120    avg_loss:0.112, val_acc:0.957]
Epoch [47/120    avg_loss:0.110, val_acc:0.925]
Epoch [48/120    avg_loss:0.110, val_acc:0.953]
Epoch [49/120    avg_loss:0.087, val_acc:0.960]
Epoch [50/120    avg_loss:0.116, val_acc:0.887]
Epoch [51/120    avg_loss:0.231, val_acc:0.954]
Epoch [52/120    avg_loss:0.102, val_acc:0.966]
Epoch [53/120    avg_loss:0.071, val_acc:0.971]
Epoch [54/120    avg_loss:0.071, val_acc:0.974]
Epoch [55/120    avg_loss:0.065, val_acc:0.966]
Epoch [56/120    avg_loss:0.056, val_acc:0.981]
Epoch [57/120    avg_loss:0.056, val_acc:0.972]
Epoch [58/120    avg_loss:0.061, val_acc:0.967]
Epoch [59/120    avg_loss:0.051, val_acc:0.970]
Epoch [60/120    avg_loss:0.114, val_acc:0.959]
Epoch [61/120    avg_loss:0.060, val_acc:0.967]
Epoch [62/120    avg_loss:0.058, val_acc:0.974]
Epoch [63/120    avg_loss:0.046, val_acc:0.985]
Epoch [64/120    avg_loss:0.056, val_acc:0.972]
Epoch [65/120    avg_loss:0.047, val_acc:0.980]
Epoch [66/120    avg_loss:0.037, val_acc:0.982]
Epoch [67/120    avg_loss:0.053, val_acc:0.981]
Epoch [68/120    avg_loss:0.050, val_acc:0.984]
Epoch [69/120    avg_loss:0.041, val_acc:0.985]
Epoch [70/120    avg_loss:0.034, val_acc:0.986]
Epoch [71/120    avg_loss:0.036, val_acc:0.982]
Epoch [72/120    avg_loss:0.032, val_acc:0.984]
Epoch [73/120    avg_loss:0.035, val_acc:0.986]
Epoch [74/120    avg_loss:0.033, val_acc:0.982]
Epoch [75/120    avg_loss:0.028, val_acc:0.985]
Epoch [76/120    avg_loss:0.033, val_acc:0.985]
Epoch [77/120    avg_loss:0.023, val_acc:0.986]
Epoch [78/120    avg_loss:0.027, val_acc:0.978]
Epoch [79/120    avg_loss:0.029, val_acc:0.985]
Epoch [80/120    avg_loss:0.022, val_acc:0.986]
Epoch [81/120    avg_loss:0.018, val_acc:0.988]
Epoch [82/120    avg_loss:0.019, val_acc:0.988]
Epoch [83/120    avg_loss:0.019, val_acc:0.990]
Epoch [84/120    avg_loss:0.017, val_acc:0.988]
Epoch [85/120    avg_loss:0.022, val_acc:0.987]
Epoch [86/120    avg_loss:0.019, val_acc:0.978]
Epoch [87/120    avg_loss:0.040, val_acc:0.979]
Epoch [88/120    avg_loss:0.030, val_acc:0.979]
Epoch [89/120    avg_loss:0.024, val_acc:0.984]
Epoch [90/120    avg_loss:0.019, val_acc:0.989]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.018, val_acc:0.988]
Epoch [93/120    avg_loss:0.020, val_acc:0.985]
Epoch [94/120    avg_loss:0.023, val_acc:0.977]
Epoch [95/120    avg_loss:0.027, val_acc:0.968]
Epoch [96/120    avg_loss:0.042, val_acc:0.979]
Epoch [97/120    avg_loss:0.022, val_acc:0.984]
Epoch [98/120    avg_loss:0.017, val_acc:0.987]
Epoch [99/120    avg_loss:0.015, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.987]
Epoch [101/120    avg_loss:0.021, val_acc:0.986]
Epoch [102/120    avg_loss:0.014, val_acc:0.988]
Epoch [103/120    avg_loss:0.018, val_acc:0.986]
Epoch [104/120    avg_loss:0.014, val_acc:0.987]
Epoch [105/120    avg_loss:0.017, val_acc:0.987]
Epoch [106/120    avg_loss:0.013, val_acc:0.987]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.025, val_acc:0.987]
Epoch [110/120    avg_loss:0.012, val_acc:0.987]
Epoch [111/120    avg_loss:0.013, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.987]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.014, val_acc:0.988]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.013, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.012, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6415     0     0     0     0     0     0    15     2]
 [    0     0 18064     0    21     0     5     0     0     0]
 [    0     2     0  2030     2     0     0     0     1     1]
 [    0    37    23     0  2878     0     4     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15    12     0     0  4836     0     0    15]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     0     0    37    60     0     0     0  3474     0]
 [    0     0     0     5    18    57     0     1     0   838]]

Accuracy:
99.12274359530524

F1 scores:
[       nan 0.9956542  0.99823165 0.98543689 0.9672324  0.97862767
 0.99475471 0.99922481 0.97983359 0.94369369]

Kappa:
0.9883744678453185
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb366122be0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.267, val_acc:0.181]
Epoch [2/120    avg_loss:2.066, val_acc:0.190]
Epoch [3/120    avg_loss:1.936, val_acc:0.199]
Epoch [4/120    avg_loss:1.854, val_acc:0.228]
Epoch [5/120    avg_loss:1.748, val_acc:0.260]
Epoch [6/120    avg_loss:1.659, val_acc:0.280]
Epoch [7/120    avg_loss:1.544, val_acc:0.307]
Epoch [8/120    avg_loss:1.422, val_acc:0.334]
Epoch [9/120    avg_loss:1.328, val_acc:0.384]
Epoch [10/120    avg_loss:1.249, val_acc:0.408]
Epoch [11/120    avg_loss:1.131, val_acc:0.491]
Epoch [12/120    avg_loss:1.027, val_acc:0.478]
Epoch [13/120    avg_loss:0.948, val_acc:0.491]
Epoch [14/120    avg_loss:0.872, val_acc:0.537]
Epoch [15/120    avg_loss:0.786, val_acc:0.591]
Epoch [16/120    avg_loss:0.685, val_acc:0.626]
Epoch [17/120    avg_loss:0.620, val_acc:0.616]
Epoch [18/120    avg_loss:0.548, val_acc:0.680]
Epoch [19/120    avg_loss:0.504, val_acc:0.691]
Epoch [20/120    avg_loss:0.452, val_acc:0.720]
Epoch [21/120    avg_loss:0.417, val_acc:0.791]
Epoch [22/120    avg_loss:0.394, val_acc:0.796]
Epoch [23/120    avg_loss:0.355, val_acc:0.814]
Epoch [24/120    avg_loss:0.326, val_acc:0.837]
Epoch [25/120    avg_loss:0.294, val_acc:0.866]
Epoch [26/120    avg_loss:0.261, val_acc:0.878]
Epoch [27/120    avg_loss:0.254, val_acc:0.919]
Epoch [28/120    avg_loss:0.228, val_acc:0.922]
Epoch [29/120    avg_loss:0.213, val_acc:0.913]
Epoch [30/120    avg_loss:0.184, val_acc:0.934]
Epoch [31/120    avg_loss:0.166, val_acc:0.931]
Epoch [32/120    avg_loss:0.162, val_acc:0.937]
Epoch [33/120    avg_loss:0.134, val_acc:0.944]
Epoch [34/120    avg_loss:0.130, val_acc:0.944]
Epoch [35/120    avg_loss:0.153, val_acc:0.934]
Epoch [36/120    avg_loss:0.127, val_acc:0.939]
Epoch [37/120    avg_loss:0.115, val_acc:0.950]
Epoch [38/120    avg_loss:0.109, val_acc:0.957]
Epoch [39/120    avg_loss:0.091, val_acc:0.965]
Epoch [40/120    avg_loss:0.092, val_acc:0.956]
Epoch [41/120    avg_loss:0.094, val_acc:0.962]
Epoch [42/120    avg_loss:0.106, val_acc:0.931]
Epoch [43/120    avg_loss:0.089, val_acc:0.978]
Epoch [44/120    avg_loss:0.066, val_acc:0.979]
Epoch [45/120    avg_loss:0.072, val_acc:0.971]
Epoch [46/120    avg_loss:0.076, val_acc:0.963]
Epoch [47/120    avg_loss:0.137, val_acc:0.938]
Epoch [48/120    avg_loss:0.134, val_acc:0.944]
Epoch [49/120    avg_loss:0.108, val_acc:0.960]
Epoch [50/120    avg_loss:0.075, val_acc:0.968]
Epoch [51/120    avg_loss:0.070, val_acc:0.966]
Epoch [52/120    avg_loss:0.047, val_acc:0.973]
Epoch [53/120    avg_loss:0.055, val_acc:0.968]
Epoch [54/120    avg_loss:0.046, val_acc:0.962]
Epoch [55/120    avg_loss:0.048, val_acc:0.974]
Epoch [56/120    avg_loss:0.040, val_acc:0.977]
Epoch [57/120    avg_loss:0.036, val_acc:0.982]
Epoch [58/120    avg_loss:0.033, val_acc:0.985]
Epoch [59/120    avg_loss:0.041, val_acc:0.980]
Epoch [60/120    avg_loss:0.058, val_acc:0.962]
Epoch [61/120    avg_loss:0.055, val_acc:0.974]
Epoch [62/120    avg_loss:0.048, val_acc:0.978]
Epoch [63/120    avg_loss:0.028, val_acc:0.978]
Epoch [64/120    avg_loss:0.037, val_acc:0.982]
Epoch [65/120    avg_loss:0.027, val_acc:0.984]
Epoch [66/120    avg_loss:0.027, val_acc:0.984]
Epoch [67/120    avg_loss:0.033, val_acc:0.973]
Epoch [68/120    avg_loss:0.035, val_acc:0.979]
Epoch [69/120    avg_loss:0.025, val_acc:0.984]
Epoch [70/120    avg_loss:0.033, val_acc:0.959]
Epoch [71/120    avg_loss:0.041, val_acc:0.983]
Epoch [72/120    avg_loss:0.026, val_acc:0.984]
Epoch [73/120    avg_loss:0.025, val_acc:0.984]
Epoch [74/120    avg_loss:0.021, val_acc:0.985]
Epoch [75/120    avg_loss:0.020, val_acc:0.987]
Epoch [76/120    avg_loss:0.022, val_acc:0.988]
Epoch [77/120    avg_loss:0.019, val_acc:0.986]
Epoch [78/120    avg_loss:0.017, val_acc:0.986]
Epoch [79/120    avg_loss:0.018, val_acc:0.988]
Epoch [80/120    avg_loss:0.017, val_acc:0.989]
Epoch [81/120    avg_loss:0.019, val_acc:0.988]
Epoch [82/120    avg_loss:0.020, val_acc:0.986]
Epoch [83/120    avg_loss:0.020, val_acc:0.988]
Epoch [84/120    avg_loss:0.015, val_acc:0.987]
Epoch [85/120    avg_loss:0.016, val_acc:0.987]
Epoch [86/120    avg_loss:0.017, val_acc:0.986]
Epoch [87/120    avg_loss:0.015, val_acc:0.987]
Epoch [88/120    avg_loss:0.015, val_acc:0.988]
Epoch [89/120    avg_loss:0.019, val_acc:0.988]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.015, val_acc:0.985]
Epoch [93/120    avg_loss:0.013, val_acc:0.987]
Epoch [94/120    avg_loss:0.013, val_acc:0.987]
Epoch [95/120    avg_loss:0.013, val_acc:0.987]
Epoch [96/120    avg_loss:0.016, val_acc:0.987]
Epoch [97/120    avg_loss:0.015, val_acc:0.987]
Epoch [98/120    avg_loss:0.020, val_acc:0.986]
Epoch [99/120    avg_loss:0.018, val_acc:0.986]
Epoch [100/120    avg_loss:0.014, val_acc:0.986]
Epoch [101/120    avg_loss:0.015, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.986]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.018, val_acc:0.987]
Epoch [106/120    avg_loss:0.013, val_acc:0.986]
Epoch [107/120    avg_loss:0.013, val_acc:0.986]
Epoch [108/120    avg_loss:0.014, val_acc:0.986]
Epoch [109/120    avg_loss:0.017, val_acc:0.986]
Epoch [110/120    avg_loss:0.014, val_acc:0.986]
Epoch [111/120    avg_loss:0.014, val_acc:0.986]
Epoch [112/120    avg_loss:0.015, val_acc:0.986]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.014, val_acc:0.986]
Epoch [115/120    avg_loss:0.015, val_acc:0.986]
Epoch [116/120    avg_loss:0.014, val_acc:0.986]
Epoch [117/120    avg_loss:0.013, val_acc:0.986]
Epoch [118/120    avg_loss:0.015, val_acc:0.986]
Epoch [119/120    avg_loss:0.015, val_acc:0.986]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     0     1     0     0    17    49     0]
 [    0    11 17991     0    51     0    37     0     0     0]
 [    0    12     0  2007     0     0     0     0    16     1]
 [    0    44    26     0  2865     0     5     0    32     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4850     0     0    15]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     7     0     8    61     0     0     0  3495     0]
 [    0     0     0     2    19    55     0     0     0   843]]

Accuracy:
98.83112814209626

F1 scores:
[       nan 0.98904514 0.9961794  0.9903775  0.95995979 0.9793621
 0.99283521 0.9922899  0.97584811 0.94665918]

Kappa:
0.9845215582667773
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f21d4a37b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.234, val_acc:0.116]
Epoch [2/120    avg_loss:2.074, val_acc:0.116]
Epoch [3/120    avg_loss:1.976, val_acc:0.196]
Epoch [4/120    avg_loss:1.873, val_acc:0.220]
Epoch [5/120    avg_loss:1.759, val_acc:0.291]
Epoch [6/120    avg_loss:1.665, val_acc:0.293]
Epoch [7/120    avg_loss:1.511, val_acc:0.334]
Epoch [8/120    avg_loss:1.438, val_acc:0.355]
Epoch [9/120    avg_loss:1.348, val_acc:0.370]
Epoch [10/120    avg_loss:1.230, val_acc:0.421]
Epoch [11/120    avg_loss:1.099, val_acc:0.437]
Epoch [12/120    avg_loss:1.006, val_acc:0.479]
Epoch [13/120    avg_loss:0.885, val_acc:0.547]
Epoch [14/120    avg_loss:0.813, val_acc:0.570]
Epoch [15/120    avg_loss:0.721, val_acc:0.611]
Epoch [16/120    avg_loss:0.675, val_acc:0.629]
Epoch [17/120    avg_loss:0.602, val_acc:0.660]
Epoch [18/120    avg_loss:0.539, val_acc:0.761]
Epoch [19/120    avg_loss:0.497, val_acc:0.801]
Epoch [20/120    avg_loss:0.436, val_acc:0.878]
Epoch [21/120    avg_loss:0.395, val_acc:0.859]
Epoch [22/120    avg_loss:0.345, val_acc:0.899]
Epoch [23/120    avg_loss:0.361, val_acc:0.914]
Epoch [24/120    avg_loss:0.283, val_acc:0.943]
Epoch [25/120    avg_loss:0.248, val_acc:0.939]
Epoch [26/120    avg_loss:0.234, val_acc:0.897]
Epoch [27/120    avg_loss:0.213, val_acc:0.948]
Epoch [28/120    avg_loss:1.149, val_acc:0.430]
Epoch [29/120    avg_loss:1.515, val_acc:0.466]
Epoch [30/120    avg_loss:1.317, val_acc:0.488]
Epoch [31/120    avg_loss:1.227, val_acc:0.542]
Epoch [32/120    avg_loss:1.111, val_acc:0.570]
Epoch [33/120    avg_loss:1.072, val_acc:0.619]
Epoch [34/120    avg_loss:0.958, val_acc:0.597]
Epoch [35/120    avg_loss:0.875, val_acc:0.678]
Epoch [36/120    avg_loss:0.842, val_acc:0.691]
Epoch [37/120    avg_loss:0.812, val_acc:0.691]
Epoch [38/120    avg_loss:0.754, val_acc:0.666]
Epoch [39/120    avg_loss:0.770, val_acc:0.752]
Epoch [40/120    avg_loss:0.666, val_acc:0.834]
Epoch [41/120    avg_loss:0.612, val_acc:0.811]
Epoch [42/120    avg_loss:0.579, val_acc:0.825]
Epoch [43/120    avg_loss:0.626, val_acc:0.824]
Epoch [44/120    avg_loss:0.568, val_acc:0.832]
Epoch [45/120    avg_loss:0.600, val_acc:0.841]
Epoch [46/120    avg_loss:0.570, val_acc:0.828]
Epoch [47/120    avg_loss:0.545, val_acc:0.837]
Epoch [48/120    avg_loss:0.557, val_acc:0.847]
Epoch [49/120    avg_loss:0.546, val_acc:0.840]
Epoch [50/120    avg_loss:0.551, val_acc:0.848]
Epoch [51/120    avg_loss:0.538, val_acc:0.835]
Epoch [52/120    avg_loss:0.542, val_acc:0.850]
Epoch [53/120    avg_loss:0.539, val_acc:0.857]
Epoch [54/120    avg_loss:0.543, val_acc:0.860]
Epoch [55/120    avg_loss:0.526, val_acc:0.857]
Epoch [56/120    avg_loss:0.517, val_acc:0.859]
Epoch [57/120    avg_loss:0.519, val_acc:0.857]
Epoch [58/120    avg_loss:0.529, val_acc:0.859]
Epoch [59/120    avg_loss:0.512, val_acc:0.861]
Epoch [60/120    avg_loss:0.511, val_acc:0.861]
Epoch [61/120    avg_loss:0.536, val_acc:0.861]
Epoch [62/120    avg_loss:0.518, val_acc:0.862]
Epoch [63/120    avg_loss:0.515, val_acc:0.862]
Epoch [64/120    avg_loss:0.524, val_acc:0.858]
Epoch [65/120    avg_loss:0.535, val_acc:0.861]
Epoch [66/120    avg_loss:0.530, val_acc:0.865]
Epoch [67/120    avg_loss:0.522, val_acc:0.865]
Epoch [68/120    avg_loss:0.514, val_acc:0.864]
Epoch [69/120    avg_loss:0.510, val_acc:0.865]
Epoch [70/120    avg_loss:0.513, val_acc:0.864]
Epoch [71/120    avg_loss:0.512, val_acc:0.864]
Epoch [72/120    avg_loss:0.515, val_acc:0.863]
Epoch [73/120    avg_loss:0.537, val_acc:0.862]
Epoch [74/120    avg_loss:0.504, val_acc:0.861]
Epoch [75/120    avg_loss:0.526, val_acc:0.861]
Epoch [76/120    avg_loss:0.537, val_acc:0.861]
Epoch [77/120    avg_loss:0.516, val_acc:0.862]
Epoch [78/120    avg_loss:0.532, val_acc:0.862]
Epoch [79/120    avg_loss:0.527, val_acc:0.862]
Epoch [80/120    avg_loss:0.512, val_acc:0.862]
Epoch [81/120    avg_loss:0.533, val_acc:0.862]
Epoch [82/120    avg_loss:0.521, val_acc:0.862]
Epoch [83/120    avg_loss:0.525, val_acc:0.862]
Epoch [84/120    avg_loss:0.508, val_acc:0.862]
Epoch [85/120    avg_loss:0.518, val_acc:0.862]
Epoch [86/120    avg_loss:0.511, val_acc:0.862]
Epoch [87/120    avg_loss:0.512, val_acc:0.862]
Epoch [88/120    avg_loss:0.524, val_acc:0.862]
Epoch [89/120    avg_loss:0.521, val_acc:0.862]
Epoch [90/120    avg_loss:0.550, val_acc:0.862]
Epoch [91/120    avg_loss:0.526, val_acc:0.862]
Epoch [92/120    avg_loss:0.520, val_acc:0.862]
Epoch [93/120    avg_loss:0.515, val_acc:0.862]
Epoch [94/120    avg_loss:0.533, val_acc:0.862]
Epoch [95/120    avg_loss:0.520, val_acc:0.862]
Epoch [96/120    avg_loss:0.524, val_acc:0.862]
Epoch [97/120    avg_loss:0.513, val_acc:0.862]
Epoch [98/120    avg_loss:0.514, val_acc:0.862]
Epoch [99/120    avg_loss:0.519, val_acc:0.862]
Epoch [100/120    avg_loss:0.501, val_acc:0.862]
Epoch [101/120    avg_loss:0.524, val_acc:0.862]
Epoch [102/120    avg_loss:0.510, val_acc:0.862]
Epoch [103/120    avg_loss:0.528, val_acc:0.862]
Epoch [104/120    avg_loss:0.519, val_acc:0.862]
Epoch [105/120    avg_loss:0.519, val_acc:0.862]
Epoch [106/120    avg_loss:0.540, val_acc:0.862]
Epoch [107/120    avg_loss:0.523, val_acc:0.862]
Epoch [108/120    avg_loss:0.528, val_acc:0.862]
Epoch [109/120    avg_loss:0.531, val_acc:0.862]
Epoch [110/120    avg_loss:0.528, val_acc:0.862]
Epoch [111/120    avg_loss:0.523, val_acc:0.862]
Epoch [112/120    avg_loss:0.521, val_acc:0.862]
Epoch [113/120    avg_loss:0.524, val_acc:0.862]
Epoch [114/120    avg_loss:0.523, val_acc:0.862]
Epoch [115/120    avg_loss:0.527, val_acc:0.862]
Epoch [116/120    avg_loss:0.517, val_acc:0.862]
Epoch [117/120    avg_loss:0.514, val_acc:0.862]
Epoch [118/120    avg_loss:0.541, val_acc:0.862]
Epoch [119/120    avg_loss:0.534, val_acc:0.862]
Epoch [120/120    avg_loss:0.513, val_acc:0.862]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4301     0   280   564     0     0   454   613   220]
 [    0   166 16254   101   983     0   429     0   157     0]
 [    0    23     2  1784    17     0     0     0    46   164]
 [    0   277    55     4  2434     0    92     0    92    18]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     4    23     0   100     0  4673     0    78     0]
 [    0    10     0     0    53     0     1  1187     0    39]
 [    0   299     0   190    69     0     0     0  3013     0]
 [    0    10     0    12    36   151     0     3     7   700]]

Accuracy:
85.92051671366255

F1 scores:
[       nan 0.74657178 0.94434116 0.80962106 0.67349198 0.94530967
 0.92782686 0.80913429 0.79530157 0.67961165]

Kappa:
0.8179147133237871
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ab2063b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.212, val_acc:0.152]
Epoch [2/120    avg_loss:2.042, val_acc:0.124]
Epoch [3/120    avg_loss:1.950, val_acc:0.104]
Epoch [4/120    avg_loss:1.870, val_acc:0.109]
Epoch [5/120    avg_loss:1.779, val_acc:0.124]
Epoch [6/120    avg_loss:1.666, val_acc:0.165]
Epoch [7/120    avg_loss:1.577, val_acc:0.262]
Epoch [8/120    avg_loss:1.482, val_acc:0.303]
Epoch [9/120    avg_loss:1.394, val_acc:0.346]
Epoch [10/120    avg_loss:1.303, val_acc:0.382]
Epoch [11/120    avg_loss:1.208, val_acc:0.436]
Epoch [12/120    avg_loss:1.108, val_acc:0.519]
Epoch [13/120    avg_loss:0.978, val_acc:0.519]
Epoch [14/120    avg_loss:0.873, val_acc:0.504]
Epoch [15/120    avg_loss:0.788, val_acc:0.555]
Epoch [16/120    avg_loss:0.712, val_acc:0.589]
Epoch [17/120    avg_loss:0.634, val_acc:0.614]
Epoch [18/120    avg_loss:0.574, val_acc:0.641]
Epoch [19/120    avg_loss:0.524, val_acc:0.712]
Epoch [20/120    avg_loss:0.481, val_acc:0.778]
Epoch [21/120    avg_loss:0.422, val_acc:0.772]
Epoch [22/120    avg_loss:0.426, val_acc:0.823]
Epoch [23/120    avg_loss:0.375, val_acc:0.814]
Epoch [24/120    avg_loss:0.335, val_acc:0.856]
Epoch [25/120    avg_loss:0.294, val_acc:0.918]
Epoch [26/120    avg_loss:0.273, val_acc:0.855]
Epoch [27/120    avg_loss:0.227, val_acc:0.893]
Epoch [28/120    avg_loss:0.214, val_acc:0.923]
Epoch [29/120    avg_loss:0.238, val_acc:0.918]
Epoch [30/120    avg_loss:0.242, val_acc:0.883]
Epoch [31/120    avg_loss:0.211, val_acc:0.942]
Epoch [32/120    avg_loss:0.189, val_acc:0.945]
Epoch [33/120    avg_loss:0.171, val_acc:0.841]
Epoch [34/120    avg_loss:0.195, val_acc:0.949]
Epoch [35/120    avg_loss:0.148, val_acc:0.953]
Epoch [36/120    avg_loss:0.166, val_acc:0.960]
Epoch [37/120    avg_loss:0.140, val_acc:0.952]
Epoch [38/120    avg_loss:0.142, val_acc:0.967]
Epoch [39/120    avg_loss:0.121, val_acc:0.966]
Epoch [40/120    avg_loss:0.099, val_acc:0.958]
Epoch [41/120    avg_loss:0.142, val_acc:0.959]
Epoch [42/120    avg_loss:0.131, val_acc:0.960]
Epoch [43/120    avg_loss:0.095, val_acc:0.961]
Epoch [44/120    avg_loss:0.111, val_acc:0.951]
Epoch [45/120    avg_loss:0.111, val_acc:0.963]
Epoch [46/120    avg_loss:0.132, val_acc:0.938]
Epoch [47/120    avg_loss:0.146, val_acc:0.950]
Epoch [48/120    avg_loss:0.101, val_acc:0.970]
Epoch [49/120    avg_loss:0.087, val_acc:0.966]
Epoch [50/120    avg_loss:0.086, val_acc:0.955]
Epoch [51/120    avg_loss:0.093, val_acc:0.956]
Epoch [52/120    avg_loss:0.075, val_acc:0.969]
Epoch [53/120    avg_loss:0.066, val_acc:0.966]
Epoch [54/120    avg_loss:0.084, val_acc:0.966]
Epoch [55/120    avg_loss:0.076, val_acc:0.972]
Epoch [56/120    avg_loss:0.068, val_acc:0.972]
Epoch [57/120    avg_loss:0.059, val_acc:0.965]
Epoch [58/120    avg_loss:0.068, val_acc:0.965]
Epoch [59/120    avg_loss:0.047, val_acc:0.973]
Epoch [60/120    avg_loss:0.057, val_acc:0.972]
Epoch [61/120    avg_loss:0.056, val_acc:0.976]
Epoch [62/120    avg_loss:0.047, val_acc:0.972]
Epoch [63/120    avg_loss:0.044, val_acc:0.979]
Epoch [64/120    avg_loss:0.054, val_acc:0.976]
Epoch [65/120    avg_loss:0.054, val_acc:0.966]
Epoch [66/120    avg_loss:0.042, val_acc:0.972]
Epoch [67/120    avg_loss:0.023, val_acc:0.984]
Epoch [68/120    avg_loss:0.030, val_acc:0.975]
Epoch [69/120    avg_loss:0.040, val_acc:0.972]
Epoch [70/120    avg_loss:0.044, val_acc:0.970]
Epoch [71/120    avg_loss:0.034, val_acc:0.972]
Epoch [72/120    avg_loss:0.023, val_acc:0.978]
Epoch [73/120    avg_loss:0.031, val_acc:0.975]
Epoch [74/120    avg_loss:0.041, val_acc:0.979]
Epoch [75/120    avg_loss:0.027, val_acc:0.976]
Epoch [76/120    avg_loss:0.031, val_acc:0.984]
Epoch [77/120    avg_loss:0.030, val_acc:0.981]
Epoch [78/120    avg_loss:0.022, val_acc:0.984]
Epoch [79/120    avg_loss:0.027, val_acc:0.974]
Epoch [80/120    avg_loss:0.025, val_acc:0.981]
Epoch [81/120    avg_loss:0.026, val_acc:0.982]
Epoch [82/120    avg_loss:0.023, val_acc:0.981]
Epoch [83/120    avg_loss:0.015, val_acc:0.984]
Epoch [84/120    avg_loss:0.026, val_acc:0.983]
Epoch [85/120    avg_loss:0.034, val_acc:0.972]
Epoch [86/120    avg_loss:0.066, val_acc:0.970]
Epoch [87/120    avg_loss:0.027, val_acc:0.978]
Epoch [88/120    avg_loss:0.022, val_acc:0.972]
Epoch [89/120    avg_loss:0.017, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.984]
Epoch [91/120    avg_loss:0.016, val_acc:0.983]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.027, val_acc:0.979]
Epoch [94/120    avg_loss:0.022, val_acc:0.978]
Epoch [95/120    avg_loss:0.018, val_acc:0.981]
Epoch [96/120    avg_loss:0.014, val_acc:0.982]
Epoch [97/120    avg_loss:0.021, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.987]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.014, val_acc:0.986]
Epoch [105/120    avg_loss:0.014, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.010, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     0     0     1     0    47     0]
 [    0     0 18060     0    24     0     6     0     0     0]
 [    0     3     0  2013     0     0     0     0    16     4]
 [    0    33    20     0  2885     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4860     0     0    14]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    40     0    33    60     0     0     0  3438     0]
 [    0     0     0     0    17    47     0     0     0   855]]

Accuracy:
99.02634179259152

F1 scores:
[       nan 0.99038163 0.99850722 0.98628123 0.96844579 0.98231088
 0.99671862 0.99961225 0.96858712 0.95370887]

Kappa:
0.9870986476588437
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f519bc99ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.167, val_acc:0.128]
Epoch [2/120    avg_loss:1.977, val_acc:0.129]
Epoch [3/120    avg_loss:1.853, val_acc:0.185]
Epoch [4/120    avg_loss:1.761, val_acc:0.198]
Epoch [5/120    avg_loss:1.662, val_acc:0.189]
Epoch [6/120    avg_loss:1.561, val_acc:0.230]
Epoch [7/120    avg_loss:1.470, val_acc:0.242]
Epoch [8/120    avg_loss:1.387, val_acc:0.278]
Epoch [9/120    avg_loss:1.352, val_acc:0.370]
Epoch [10/120    avg_loss:1.260, val_acc:0.423]
Epoch [11/120    avg_loss:1.206, val_acc:0.391]
Epoch [12/120    avg_loss:1.077, val_acc:0.417]
Epoch [13/120    avg_loss:0.961, val_acc:0.470]
Epoch [14/120    avg_loss:0.910, val_acc:0.466]
Epoch [15/120    avg_loss:0.839, val_acc:0.495]
Epoch [16/120    avg_loss:0.729, val_acc:0.546]
Epoch [17/120    avg_loss:0.651, val_acc:0.597]
Epoch [18/120    avg_loss:0.601, val_acc:0.565]
Epoch [19/120    avg_loss:0.533, val_acc:0.650]
Epoch [20/120    avg_loss:0.465, val_acc:0.678]
Epoch [21/120    avg_loss:0.449, val_acc:0.774]
Epoch [22/120    avg_loss:0.397, val_acc:0.805]
Epoch [23/120    avg_loss:0.403, val_acc:0.816]
Epoch [24/120    avg_loss:0.337, val_acc:0.838]
Epoch [25/120    avg_loss:0.321, val_acc:0.860]
Epoch [26/120    avg_loss:0.293, val_acc:0.844]
Epoch [27/120    avg_loss:0.256, val_acc:0.887]
Epoch [28/120    avg_loss:0.243, val_acc:0.894]
Epoch [29/120    avg_loss:0.210, val_acc:0.946]
Epoch [30/120    avg_loss:0.191, val_acc:0.888]
Epoch [31/120    avg_loss:0.200, val_acc:0.952]
Epoch [32/120    avg_loss:0.192, val_acc:0.953]
Epoch [33/120    avg_loss:0.195, val_acc:0.887]
Epoch [34/120    avg_loss:0.153, val_acc:0.957]
Epoch [35/120    avg_loss:0.136, val_acc:0.956]
Epoch [36/120    avg_loss:0.115, val_acc:0.934]
Epoch [37/120    avg_loss:0.136, val_acc:0.944]
Epoch [38/120    avg_loss:0.137, val_acc:0.955]
Epoch [39/120    avg_loss:0.109, val_acc:0.940]
Epoch [40/120    avg_loss:0.106, val_acc:0.956]
Epoch [41/120    avg_loss:0.122, val_acc:0.977]
Epoch [42/120    avg_loss:0.095, val_acc:0.974]
Epoch [43/120    avg_loss:0.107, val_acc:0.963]
Epoch [44/120    avg_loss:0.110, val_acc:0.972]
Epoch [45/120    avg_loss:0.111, val_acc:0.968]
Epoch [46/120    avg_loss:0.083, val_acc:0.972]
Epoch [47/120    avg_loss:0.087, val_acc:0.972]
Epoch [48/120    avg_loss:0.081, val_acc:0.979]
Epoch [49/120    avg_loss:0.065, val_acc:0.977]
Epoch [50/120    avg_loss:0.052, val_acc:0.978]
Epoch [51/120    avg_loss:0.045, val_acc:0.980]
Epoch [52/120    avg_loss:0.050, val_acc:0.984]
Epoch [53/120    avg_loss:0.048, val_acc:0.976]
Epoch [54/120    avg_loss:0.046, val_acc:0.985]
Epoch [55/120    avg_loss:0.062, val_acc:0.960]
Epoch [56/120    avg_loss:0.051, val_acc:0.979]
Epoch [57/120    avg_loss:0.066, val_acc:0.977]
Epoch [58/120    avg_loss:0.066, val_acc:0.975]
Epoch [59/120    avg_loss:0.049, val_acc:0.978]
Epoch [60/120    avg_loss:0.048, val_acc:0.984]
Epoch [61/120    avg_loss:0.048, val_acc:0.980]
Epoch [62/120    avg_loss:0.060, val_acc:0.984]
Epoch [63/120    avg_loss:0.069, val_acc:0.981]
Epoch [64/120    avg_loss:0.068, val_acc:0.979]
Epoch [65/120    avg_loss:0.053, val_acc:0.988]
Epoch [66/120    avg_loss:0.035, val_acc:0.987]
Epoch [67/120    avg_loss:0.033, val_acc:0.990]
Epoch [68/120    avg_loss:0.055, val_acc:0.971]
Epoch [69/120    avg_loss:0.034, val_acc:0.983]
Epoch [70/120    avg_loss:0.026, val_acc:0.987]
Epoch [71/120    avg_loss:0.031, val_acc:0.988]
Epoch [72/120    avg_loss:0.037, val_acc:0.984]
Epoch [73/120    avg_loss:0.027, val_acc:0.989]
Epoch [74/120    avg_loss:0.079, val_acc:0.983]
Epoch [75/120    avg_loss:0.073, val_acc:0.976]
Epoch [76/120    avg_loss:0.070, val_acc:0.980]
Epoch [77/120    avg_loss:0.048, val_acc:0.991]
Epoch [78/120    avg_loss:0.033, val_acc:0.988]
Epoch [79/120    avg_loss:0.025, val_acc:0.989]
Epoch [80/120    avg_loss:0.024, val_acc:0.990]
Epoch [81/120    avg_loss:0.020, val_acc:0.991]
Epoch [82/120    avg_loss:0.023, val_acc:0.991]
Epoch [83/120    avg_loss:0.018, val_acc:0.991]
Epoch [84/120    avg_loss:0.019, val_acc:0.987]
Epoch [85/120    avg_loss:0.041, val_acc:0.986]
Epoch [86/120    avg_loss:0.035, val_acc:0.960]
Epoch [87/120    avg_loss:0.024, val_acc:0.991]
Epoch [88/120    avg_loss:0.016, val_acc:0.990]
Epoch [89/120    avg_loss:0.022, val_acc:0.992]
Epoch [90/120    avg_loss:0.020, val_acc:0.991]
Epoch [91/120    avg_loss:0.019, val_acc:0.991]
Epoch [92/120    avg_loss:0.044, val_acc:0.978]
Epoch [93/120    avg_loss:0.067, val_acc:0.987]
Epoch [94/120    avg_loss:0.024, val_acc:0.985]
Epoch [95/120    avg_loss:0.018, val_acc:0.991]
Epoch [96/120    avg_loss:0.018, val_acc:0.991]
Epoch [97/120    avg_loss:0.034, val_acc:0.975]
Epoch [98/120    avg_loss:0.031, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.989]
Epoch [100/120    avg_loss:0.018, val_acc:0.991]
Epoch [101/120    avg_loss:0.014, val_acc:0.993]
Epoch [102/120    avg_loss:0.013, val_acc:0.993]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.011, val_acc:0.993]
Epoch [105/120    avg_loss:0.012, val_acc:0.991]
Epoch [106/120    avg_loss:0.015, val_acc:0.991]
Epoch [107/120    avg_loss:0.026, val_acc:0.993]
Epoch [108/120    avg_loss:0.020, val_acc:0.993]
Epoch [109/120    avg_loss:0.011, val_acc:0.991]
Epoch [110/120    avg_loss:0.012, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.991]
Epoch [112/120    avg_loss:0.014, val_acc:0.987]
Epoch [113/120    avg_loss:0.016, val_acc:0.987]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.015, val_acc:0.975]
Epoch [116/120    avg_loss:0.051, val_acc:0.980]
Epoch [117/120    avg_loss:0.024, val_acc:0.982]
Epoch [118/120    avg_loss:0.025, val_acc:0.987]
Epoch [119/120    avg_loss:0.025, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     0     0     0     0     0    82     0]
 [    0     1 17944     0    32     0   113     0     0     0]
 [    0     6     0  1989     0     0     0     0    40     1]
 [    0    20    16     0  2896     0    13     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     9     0     0  4869     0     0     0]
 [    0     9     0     0     0     0     3  1275     0     3]
 [    0     5     0    46    54     0     0     0  3453    13]
 [    0     0     0     0    15    41     0     0     0   863]]

Accuracy:
98.67688525775432

F1 scores:
[       nan 0.99040786 0.99550624 0.975      0.97034679 0.98453414
 0.98602673 0.99415205 0.96277708 0.9594219 ]

Kappa:
0.9824925945576467
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fafd2c81b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.239, val_acc:0.089]
Epoch [2/120    avg_loss:2.035, val_acc:0.116]
Epoch [3/120    avg_loss:1.913, val_acc:0.104]
Epoch [4/120    avg_loss:1.804, val_acc:0.117]
Epoch [5/120    avg_loss:1.707, val_acc:0.122]
Epoch [6/120    avg_loss:1.624, val_acc:0.141]
Epoch [7/120    avg_loss:1.496, val_acc:0.212]
Epoch [8/120    avg_loss:1.425, val_acc:0.235]
Epoch [9/120    avg_loss:1.357, val_acc:0.278]
Epoch [10/120    avg_loss:1.280, val_acc:0.331]
Epoch [11/120    avg_loss:1.263, val_acc:0.324]
Epoch [12/120    avg_loss:1.169, val_acc:0.403]
Epoch [13/120    avg_loss:1.125, val_acc:0.411]
Epoch [14/120    avg_loss:1.045, val_acc:0.421]
Epoch [15/120    avg_loss:0.976, val_acc:0.430]
Epoch [16/120    avg_loss:0.890, val_acc:0.453]
Epoch [17/120    avg_loss:0.843, val_acc:0.464]
Epoch [18/120    avg_loss:0.779, val_acc:0.464]
Epoch [19/120    avg_loss:0.690, val_acc:0.483]
Epoch [20/120    avg_loss:1.234, val_acc:0.300]
Epoch [21/120    avg_loss:1.273, val_acc:0.355]
Epoch [22/120    avg_loss:1.151, val_acc:0.403]
Epoch [23/120    avg_loss:1.065, val_acc:0.471]
Epoch [24/120    avg_loss:1.013, val_acc:0.625]
Epoch [25/120    avg_loss:0.954, val_acc:0.620]
Epoch [26/120    avg_loss:0.900, val_acc:0.640]
Epoch [27/120    avg_loss:0.826, val_acc:0.648]
Epoch [28/120    avg_loss:0.770, val_acc:0.671]
Epoch [29/120    avg_loss:0.764, val_acc:0.747]
Epoch [30/120    avg_loss:0.713, val_acc:0.718]
Epoch [31/120    avg_loss:0.669, val_acc:0.759]
Epoch [32/120    avg_loss:0.648, val_acc:0.723]
Epoch [33/120    avg_loss:0.679, val_acc:0.772]
Epoch [34/120    avg_loss:0.623, val_acc:0.760]
Epoch [35/120    avg_loss:0.664, val_acc:0.727]
Epoch [36/120    avg_loss:0.546, val_acc:0.776]
Epoch [37/120    avg_loss:0.501, val_acc:0.755]
Epoch [38/120    avg_loss:0.510, val_acc:0.784]
Epoch [39/120    avg_loss:0.475, val_acc:0.784]
Epoch [40/120    avg_loss:0.456, val_acc:0.790]
Epoch [41/120    avg_loss:0.440, val_acc:0.802]
Epoch [42/120    avg_loss:0.414, val_acc:0.766]
Epoch [43/120    avg_loss:0.446, val_acc:0.802]
Epoch [44/120    avg_loss:0.406, val_acc:0.799]
Epoch [45/120    avg_loss:0.414, val_acc:0.800]
Epoch [46/120    avg_loss:0.458, val_acc:0.796]
Epoch [47/120    avg_loss:0.357, val_acc:0.816]
Epoch [48/120    avg_loss:0.386, val_acc:0.752]
Epoch [49/120    avg_loss:0.361, val_acc:0.806]
Epoch [50/120    avg_loss:0.358, val_acc:0.823]
Epoch [51/120    avg_loss:0.288, val_acc:0.827]
Epoch [52/120    avg_loss:0.295, val_acc:0.858]
Epoch [53/120    avg_loss:0.305, val_acc:0.801]
Epoch [54/120    avg_loss:0.313, val_acc:0.840]
Epoch [55/120    avg_loss:0.285, val_acc:0.882]
Epoch [56/120    avg_loss:0.265, val_acc:0.864]
Epoch [57/120    avg_loss:0.295, val_acc:0.823]
Epoch [58/120    avg_loss:0.351, val_acc:0.834]
Epoch [59/120    avg_loss:0.268, val_acc:0.890]
Epoch [60/120    avg_loss:0.228, val_acc:0.875]
Epoch [61/120    avg_loss:0.211, val_acc:0.844]
Epoch [62/120    avg_loss:0.220, val_acc:0.909]
Epoch [63/120    avg_loss:0.283, val_acc:0.878]
Epoch [64/120    avg_loss:0.237, val_acc:0.896]
Epoch [65/120    avg_loss:0.211, val_acc:0.870]
Epoch [66/120    avg_loss:0.192, val_acc:0.909]
Epoch [67/120    avg_loss:0.193, val_acc:0.905]
Epoch [68/120    avg_loss:0.176, val_acc:0.925]
Epoch [69/120    avg_loss:0.183, val_acc:0.894]
Epoch [70/120    avg_loss:0.167, val_acc:0.924]
Epoch [71/120    avg_loss:0.171, val_acc:0.927]
Epoch [72/120    avg_loss:0.135, val_acc:0.933]
Epoch [73/120    avg_loss:0.127, val_acc:0.905]
Epoch [74/120    avg_loss:0.137, val_acc:0.885]
Epoch [75/120    avg_loss:0.128, val_acc:0.943]
Epoch [76/120    avg_loss:0.149, val_acc:0.891]
Epoch [77/120    avg_loss:0.125, val_acc:0.947]
Epoch [78/120    avg_loss:0.111, val_acc:0.935]
Epoch [79/120    avg_loss:0.100, val_acc:0.950]
Epoch [80/120    avg_loss:0.083, val_acc:0.949]
Epoch [81/120    avg_loss:0.088, val_acc:0.945]
Epoch [82/120    avg_loss:0.089, val_acc:0.954]
Epoch [83/120    avg_loss:0.068, val_acc:0.950]
Epoch [84/120    avg_loss:0.098, val_acc:0.935]
Epoch [85/120    avg_loss:0.131, val_acc:0.949]
Epoch [86/120    avg_loss:0.104, val_acc:0.949]
Epoch [87/120    avg_loss:0.087, val_acc:0.958]
Epoch [88/120    avg_loss:0.096, val_acc:0.947]
Epoch [89/120    avg_loss:0.077, val_acc:0.950]
Epoch [90/120    avg_loss:0.064, val_acc:0.966]
Epoch [91/120    avg_loss:0.062, val_acc:0.965]
Epoch [92/120    avg_loss:0.079, val_acc:0.957]
Epoch [93/120    avg_loss:0.084, val_acc:0.960]
Epoch [94/120    avg_loss:0.071, val_acc:0.952]
Epoch [95/120    avg_loss:0.069, val_acc:0.957]
Epoch [96/120    avg_loss:0.073, val_acc:0.964]
Epoch [97/120    avg_loss:0.056, val_acc:0.966]
Epoch [98/120    avg_loss:0.049, val_acc:0.966]
Epoch [99/120    avg_loss:0.058, val_acc:0.956]
Epoch [100/120    avg_loss:0.067, val_acc:0.941]
Epoch [101/120    avg_loss:0.046, val_acc:0.963]
Epoch [102/120    avg_loss:0.034, val_acc:0.972]
Epoch [103/120    avg_loss:0.036, val_acc:0.978]
Epoch [104/120    avg_loss:0.031, val_acc:0.974]
Epoch [105/120    avg_loss:0.030, val_acc:0.968]
Epoch [106/120    avg_loss:0.038, val_acc:0.970]
Epoch [107/120    avg_loss:0.044, val_acc:0.968]
Epoch [108/120    avg_loss:0.033, val_acc:0.972]
Epoch [109/120    avg_loss:0.045, val_acc:0.969]
Epoch [110/120    avg_loss:0.043, val_acc:0.957]
Epoch [111/120    avg_loss:0.034, val_acc:0.973]
Epoch [112/120    avg_loss:0.027, val_acc:0.975]
Epoch [113/120    avg_loss:0.024, val_acc:0.977]
Epoch [114/120    avg_loss:0.024, val_acc:0.963]
Epoch [115/120    avg_loss:0.029, val_acc:0.972]
Epoch [116/120    avg_loss:0.123, val_acc:0.947]
Epoch [117/120    avg_loss:0.061, val_acc:0.966]
Epoch [118/120    avg_loss:0.041, val_acc:0.966]
Epoch [119/120    avg_loss:0.036, val_acc:0.966]
Epoch [120/120    avg_loss:0.040, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5955     0     0   112     0     0    26   207   132]
 [    0     0 17993     0    37     0    60     0     0     0]
 [    0    11     0  1979     0     0     0     0    33    13]
 [    0    66    20     0  2834     0    29     0    23     0]
 [    0     0     0     0     0  1301     0     0     0     4]
 [    0    11     0     0     0     0  4867     0     0     0]
 [    0     4     0     0     0     0     7  1249     0    30]
 [    0    48     0    23    60     0     0     0  3440     0]
 [    0     5     0     1    21    99     0     1     0   792]]

Accuracy:
97.38992119152628

F1 scores:
[       nan 0.95036706 0.99675927 0.97994553 0.93903247 0.96192237
 0.98912712 0.97349961 0.94583448 0.83809524]

Kappa:
0.9654755259102289
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8989f1cb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 16570==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.267, val_acc:0.541]
Epoch [2/120    avg_loss:2.072, val_acc:0.443]
Epoch [3/120    avg_loss:1.943, val_acc:0.435]
Epoch [4/120    avg_loss:1.862, val_acc:0.465]
Epoch [5/120    avg_loss:1.764, val_acc:0.528]
Epoch [6/120    avg_loss:1.677, val_acc:0.532]
Epoch [7/120    avg_loss:1.548, val_acc:0.544]
Epoch [8/120    avg_loss:1.456, val_acc:0.572]
Epoch [9/120    avg_loss:1.355, val_acc:0.584]
Epoch [10/120    avg_loss:1.292, val_acc:0.615]
Epoch [11/120    avg_loss:1.267, val_acc:0.634]
Epoch [12/120    avg_loss:1.156, val_acc:0.681]
Epoch [13/120    avg_loss:1.142, val_acc:0.727]
Epoch [14/120    avg_loss:1.065, val_acc:0.752]
Epoch [15/120    avg_loss:0.998, val_acc:0.783]
Epoch [16/120    avg_loss:0.898, val_acc:0.737]
Epoch [17/120    avg_loss:0.849, val_acc:0.781]
Epoch [18/120    avg_loss:0.794, val_acc:0.779]
Epoch [19/120    avg_loss:0.676, val_acc:0.796]
Epoch [20/120    avg_loss:0.599, val_acc:0.804]
Epoch [21/120    avg_loss:0.542, val_acc:0.805]
Epoch [22/120    avg_loss:0.485, val_acc:0.809]
Epoch [23/120    avg_loss:0.451, val_acc:0.826]
Epoch [24/120    avg_loss:0.426, val_acc:0.803]
Epoch [25/120    avg_loss:0.372, val_acc:0.847]
Epoch [26/120    avg_loss:0.349, val_acc:0.859]
Epoch [27/120    avg_loss:0.403, val_acc:0.841]
Epoch [28/120    avg_loss:0.342, val_acc:0.886]
Epoch [29/120    avg_loss:0.338, val_acc:0.768]
Epoch [30/120    avg_loss:0.435, val_acc:0.861]
Epoch [31/120    avg_loss:0.294, val_acc:0.914]
Epoch [32/120    avg_loss:0.246, val_acc:0.896]
Epoch [33/120    avg_loss:0.240, val_acc:0.932]
Epoch [34/120    avg_loss:0.196, val_acc:0.941]
Epoch [35/120    avg_loss:0.188, val_acc:0.944]
Epoch [36/120    avg_loss:0.175, val_acc:0.935]
Epoch [37/120    avg_loss:0.139, val_acc:0.946]
Epoch [38/120    avg_loss:0.123, val_acc:0.947]
Epoch [39/120    avg_loss:0.115, val_acc:0.921]
Epoch [40/120    avg_loss:0.108, val_acc:0.963]
Epoch [41/120    avg_loss:0.118, val_acc:0.913]
Epoch [42/120    avg_loss:0.107, val_acc:0.946]
Epoch [43/120    avg_loss:0.101, val_acc:0.935]
Epoch [44/120    avg_loss:0.147, val_acc:0.956]
Epoch [45/120    avg_loss:0.116, val_acc:0.941]
Epoch [46/120    avg_loss:0.108, val_acc:0.966]
Epoch [47/120    avg_loss:0.102, val_acc:0.958]
Epoch [48/120    avg_loss:0.084, val_acc:0.962]
Epoch [49/120    avg_loss:0.069, val_acc:0.972]
Epoch [50/120    avg_loss:0.066, val_acc:0.977]
Epoch [51/120    avg_loss:0.075, val_acc:0.953]
Epoch [52/120    avg_loss:0.058, val_acc:0.958]
Epoch [53/120    avg_loss:0.065, val_acc:0.977]
Epoch [54/120    avg_loss:0.045, val_acc:0.971]
Epoch [55/120    avg_loss:0.057, val_acc:0.966]
Epoch [56/120    avg_loss:0.052, val_acc:0.974]
Epoch [57/120    avg_loss:0.042, val_acc:0.978]
Epoch [58/120    avg_loss:0.038, val_acc:0.981]
Epoch [59/120    avg_loss:0.054, val_acc:0.953]
Epoch [60/120    avg_loss:0.056, val_acc:0.973]
Epoch [61/120    avg_loss:0.041, val_acc:0.978]
Epoch [62/120    avg_loss:0.038, val_acc:0.973]
Epoch [63/120    avg_loss:0.043, val_acc:0.973]
Epoch [64/120    avg_loss:0.034, val_acc:0.983]
Epoch [65/120    avg_loss:0.041, val_acc:0.978]
Epoch [66/120    avg_loss:0.028, val_acc:0.976]
Epoch [67/120    avg_loss:0.031, val_acc:0.980]
Epoch [68/120    avg_loss:0.030, val_acc:0.980]
Epoch [69/120    avg_loss:0.027, val_acc:0.980]
Epoch [70/120    avg_loss:0.027, val_acc:0.979]
Epoch [71/120    avg_loss:0.044, val_acc:0.983]
Epoch [72/120    avg_loss:0.029, val_acc:0.984]
Epoch [73/120    avg_loss:0.026, val_acc:0.979]
Epoch [74/120    avg_loss:0.021, val_acc:0.984]
Epoch [75/120    avg_loss:0.020, val_acc:0.982]
Epoch [76/120    avg_loss:0.024, val_acc:0.984]
Epoch [77/120    avg_loss:0.021, val_acc:0.982]
Epoch [78/120    avg_loss:0.025, val_acc:0.974]
Epoch [79/120    avg_loss:0.037, val_acc:0.977]
Epoch [80/120    avg_loss:0.024, val_acc:0.984]
Epoch [81/120    avg_loss:0.021, val_acc:0.982]
Epoch [82/120    avg_loss:0.023, val_acc:0.983]
Epoch [83/120    avg_loss:0.016, val_acc:0.980]
Epoch [84/120    avg_loss:0.024, val_acc:0.984]
Epoch [85/120    avg_loss:0.018, val_acc:0.985]
Epoch [86/120    avg_loss:0.032, val_acc:0.982]
Epoch [87/120    avg_loss:0.027, val_acc:0.981]
Epoch [88/120    avg_loss:0.023, val_acc:0.984]
Epoch [89/120    avg_loss:0.016, val_acc:0.984]
Epoch [90/120    avg_loss:0.014, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.987]
Epoch [93/120    avg_loss:0.014, val_acc:0.985]
Epoch [94/120    avg_loss:0.014, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.014, val_acc:0.972]
Epoch [97/120    avg_loss:0.014, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.987]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.013, val_acc:0.987]
Epoch [107/120    avg_loss:0.028, val_acc:0.970]
Epoch [108/120    avg_loss:0.019, val_acc:0.947]
Epoch [109/120    avg_loss:0.013, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.015, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.013, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     0     0     0     0    29     0]
 [    0     1 18025     0    58     0     6     0     0     0]
 [    0     9     0  2005     0     0     0     0    22     0]
 [    0    26    18     0  2894     0     7     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     0     0     0  4853     0     2     3]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0    22     0    24    58     0     0     0  3467     0]
 [    0     0     0     0    16    38     0     0     0   865]]

Accuracy:
99.06249246860916

F1 scores:
[       nan 0.99325215 0.997151   0.98646986 0.96498833 0.98564955
 0.99599795 0.99883586 0.97415004 0.96702068]

Kappa:
0.987580272408809
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feedb6bf908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.679, val_acc:0.679]
Epoch [2/120    avg_loss:1.010, val_acc:0.743]
Epoch [3/120    avg_loss:0.654, val_acc:0.738]
Epoch [4/120    avg_loss:0.514, val_acc:0.796]
Epoch [5/120    avg_loss:0.424, val_acc:0.833]
Epoch [6/120    avg_loss:0.368, val_acc:0.832]
Epoch [7/120    avg_loss:0.335, val_acc:0.853]
Epoch [8/120    avg_loss:0.323, val_acc:0.793]
Epoch [9/120    avg_loss:0.275, val_acc:0.856]
Epoch [10/120    avg_loss:0.238, val_acc:0.917]
Epoch [11/120    avg_loss:0.178, val_acc:0.880]
Epoch [12/120    avg_loss:0.214, val_acc:0.895]
Epoch [13/120    avg_loss:0.221, val_acc:0.868]
Epoch [14/120    avg_loss:0.225, val_acc:0.828]
Epoch [15/120    avg_loss:0.223, val_acc:0.915]
Epoch [16/120    avg_loss:0.161, val_acc:0.907]
Epoch [17/120    avg_loss:0.114, val_acc:0.958]
Epoch [18/120    avg_loss:0.127, val_acc:0.955]
Epoch [19/120    avg_loss:0.153, val_acc:0.898]
Epoch [20/120    avg_loss:0.116, val_acc:0.967]
Epoch [21/120    avg_loss:0.111, val_acc:0.917]
Epoch [22/120    avg_loss:0.097, val_acc:0.959]
Epoch [23/120    avg_loss:0.081, val_acc:0.939]
Epoch [24/120    avg_loss:0.084, val_acc:0.920]
Epoch [25/120    avg_loss:0.083, val_acc:0.956]
Epoch [26/120    avg_loss:0.077, val_acc:0.965]
Epoch [27/120    avg_loss:0.042, val_acc:0.979]
Epoch [28/120    avg_loss:0.061, val_acc:0.961]
Epoch [29/120    avg_loss:0.070, val_acc:0.974]
Epoch [30/120    avg_loss:0.047, val_acc:0.953]
Epoch [31/120    avg_loss:0.058, val_acc:0.976]
Epoch [32/120    avg_loss:0.119, val_acc:0.952]
Epoch [33/120    avg_loss:0.053, val_acc:0.963]
Epoch [34/120    avg_loss:0.045, val_acc:0.967]
Epoch [35/120    avg_loss:0.053, val_acc:0.974]
Epoch [36/120    avg_loss:0.043, val_acc:0.975]
Epoch [37/120    avg_loss:0.029, val_acc:0.972]
Epoch [38/120    avg_loss:0.029, val_acc:0.955]
Epoch [39/120    avg_loss:0.061, val_acc:0.943]
Epoch [40/120    avg_loss:0.035, val_acc:0.977]
Epoch [41/120    avg_loss:0.028, val_acc:0.976]
Epoch [42/120    avg_loss:0.021, val_acc:0.981]
Epoch [43/120    avg_loss:0.018, val_acc:0.979]
Epoch [44/120    avg_loss:0.016, val_acc:0.980]
Epoch [45/120    avg_loss:0.018, val_acc:0.980]
Epoch [46/120    avg_loss:0.015, val_acc:0.982]
Epoch [47/120    avg_loss:0.011, val_acc:0.980]
Epoch [48/120    avg_loss:0.017, val_acc:0.981]
Epoch [49/120    avg_loss:0.014, val_acc:0.981]
Epoch [50/120    avg_loss:0.014, val_acc:0.983]
Epoch [51/120    avg_loss:0.016, val_acc:0.984]
Epoch [52/120    avg_loss:0.013, val_acc:0.982]
Epoch [53/120    avg_loss:0.018, val_acc:0.983]
Epoch [54/120    avg_loss:0.013, val_acc:0.982]
Epoch [55/120    avg_loss:0.018, val_acc:0.983]
Epoch [56/120    avg_loss:0.011, val_acc:0.983]
Epoch [57/120    avg_loss:0.012, val_acc:0.983]
Epoch [58/120    avg_loss:0.012, val_acc:0.982]
Epoch [59/120    avg_loss:0.012, val_acc:0.979]
Epoch [60/120    avg_loss:0.016, val_acc:0.983]
Epoch [61/120    avg_loss:0.015, val_acc:0.983]
Epoch [62/120    avg_loss:0.012, val_acc:0.985]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.012, val_acc:0.984]
Epoch [65/120    avg_loss:0.013, val_acc:0.984]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.011, val_acc:0.984]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.013, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.012, val_acc:0.984]
Epoch [73/120    avg_loss:0.012, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.018, val_acc:0.985]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.989]
Epoch [82/120    avg_loss:0.010, val_acc:0.989]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.009, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.009, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.011, val_acc:0.984]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.989]
Epoch [108/120    avg_loss:0.011, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.989]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.009, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.989]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     0     0     0     4    15    49     3]
 [    0     0 17987     0    15     0    79     0     9     0]
 [    0     1     0  1909     0     0     0     0   124     2]
 [    0     9     3     0  2934     0    16     0     5     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     1     0  4868     0     0     0]
 [    0    11     0     0     0     0     2  1277     0     0]
 [    0     8     1    41    38     0     8     0  3475     0]
 [    0     1     0     0     8     8     0     0     0   902]]

Accuracy:
98.8552285927747

F1 scores:
[       nan 0.99212353 0.99678581 0.95785248 0.98324397 0.99694423
 0.98792491 0.98915569 0.96087377 0.98525396]

Kappa:
0.9848448637790113
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f630c9eb908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.757, val_acc:0.692]
Epoch [2/120    avg_loss:1.006, val_acc:0.762]
Epoch [3/120    avg_loss:0.637, val_acc:0.765]
Epoch [4/120    avg_loss:0.515, val_acc:0.712]
Epoch [5/120    avg_loss:0.429, val_acc:0.835]
Epoch [6/120    avg_loss:0.379, val_acc:0.809]
Epoch [7/120    avg_loss:0.376, val_acc:0.850]
Epoch [8/120    avg_loss:0.345, val_acc:0.795]
Epoch [9/120    avg_loss:0.300, val_acc:0.895]
Epoch [10/120    avg_loss:0.331, val_acc:0.850]
Epoch [11/120    avg_loss:0.232, val_acc:0.916]
Epoch [12/120    avg_loss:0.231, val_acc:0.863]
Epoch [13/120    avg_loss:0.214, val_acc:0.925]
Epoch [14/120    avg_loss:0.187, val_acc:0.878]
Epoch [15/120    avg_loss:0.197, val_acc:0.873]
Epoch [16/120    avg_loss:0.191, val_acc:0.894]
Epoch [17/120    avg_loss:0.145, val_acc:0.940]
Epoch [18/120    avg_loss:0.118, val_acc:0.940]
Epoch [19/120    avg_loss:0.134, val_acc:0.920]
Epoch [20/120    avg_loss:0.170, val_acc:0.944]
Epoch [21/120    avg_loss:0.121, val_acc:0.935]
Epoch [22/120    avg_loss:0.104, val_acc:0.941]
Epoch [23/120    avg_loss:0.096, val_acc:0.940]
Epoch [24/120    avg_loss:0.171, val_acc:0.928]
Epoch [25/120    avg_loss:0.120, val_acc:0.947]
Epoch [26/120    avg_loss:0.105, val_acc:0.963]
Epoch [27/120    avg_loss:0.087, val_acc:0.949]
Epoch [28/120    avg_loss:0.084, val_acc:0.954]
Epoch [29/120    avg_loss:0.083, val_acc:0.940]
Epoch [30/120    avg_loss:0.121, val_acc:0.830]
Epoch [31/120    avg_loss:0.062, val_acc:0.971]
Epoch [32/120    avg_loss:0.072, val_acc:0.953]
Epoch [33/120    avg_loss:0.079, val_acc:0.940]
Epoch [34/120    avg_loss:0.139, val_acc:0.859]
Epoch [35/120    avg_loss:0.124, val_acc:0.962]
Epoch [36/120    avg_loss:0.075, val_acc:0.967]
Epoch [37/120    avg_loss:0.039, val_acc:0.972]
Epoch [38/120    avg_loss:0.037, val_acc:0.974]
Epoch [39/120    avg_loss:0.049, val_acc:0.974]
Epoch [40/120    avg_loss:0.068, val_acc:0.958]
Epoch [41/120    avg_loss:0.031, val_acc:0.971]
Epoch [42/120    avg_loss:0.030, val_acc:0.967]
Epoch [43/120    avg_loss:0.038, val_acc:0.942]
Epoch [44/120    avg_loss:0.032, val_acc:0.975]
Epoch [45/120    avg_loss:0.023, val_acc:0.967]
Epoch [46/120    avg_loss:0.032, val_acc:0.968]
Epoch [47/120    avg_loss:0.046, val_acc:0.967]
Epoch [48/120    avg_loss:0.028, val_acc:0.977]
Epoch [49/120    avg_loss:0.024, val_acc:0.938]
Epoch [50/120    avg_loss:0.031, val_acc:0.973]
Epoch [51/120    avg_loss:0.031, val_acc:0.959]
Epoch [52/120    avg_loss:0.029, val_acc:0.971]
Epoch [53/120    avg_loss:0.029, val_acc:0.975]
Epoch [54/120    avg_loss:0.024, val_acc:0.961]
Epoch [55/120    avg_loss:0.028, val_acc:0.970]
Epoch [56/120    avg_loss:0.037, val_acc:0.971]
Epoch [57/120    avg_loss:0.021, val_acc:0.977]
Epoch [58/120    avg_loss:0.020, val_acc:0.972]
Epoch [59/120    avg_loss:0.018, val_acc:0.976]
Epoch [60/120    avg_loss:0.029, val_acc:0.965]
Epoch [61/120    avg_loss:0.015, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.980]
Epoch [63/120    avg_loss:0.015, val_acc:0.971]
Epoch [64/120    avg_loss:0.010, val_acc:0.972]
Epoch [65/120    avg_loss:0.011, val_acc:0.980]
Epoch [66/120    avg_loss:0.010, val_acc:0.980]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.010, val_acc:0.978]
Epoch [69/120    avg_loss:0.013, val_acc:0.975]
Epoch [70/120    avg_loss:0.012, val_acc:0.983]
Epoch [71/120    avg_loss:0.010, val_acc:0.980]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.978]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.006, val_acc:0.977]
Epoch [76/120    avg_loss:0.020, val_acc:0.973]
Epoch [77/120    avg_loss:0.152, val_acc:0.894]
Epoch [78/120    avg_loss:0.125, val_acc:0.960]
Epoch [79/120    avg_loss:0.214, val_acc:0.941]
Epoch [80/120    avg_loss:0.082, val_acc:0.963]
Epoch [81/120    avg_loss:0.038, val_acc:0.963]
Epoch [82/120    avg_loss:0.030, val_acc:0.974]
Epoch [83/120    avg_loss:0.023, val_acc:0.970]
Epoch [84/120    avg_loss:0.019, val_acc:0.981]
Epoch [85/120    avg_loss:0.034, val_acc:0.936]
Epoch [86/120    avg_loss:0.037, val_acc:0.966]
Epoch [87/120    avg_loss:0.028, val_acc:0.962]
Epoch [88/120    avg_loss:0.043, val_acc:0.975]
Epoch [89/120    avg_loss:0.019, val_acc:0.979]
Epoch [90/120    avg_loss:0.016, val_acc:0.980]
Epoch [91/120    avg_loss:0.013, val_acc:0.980]
Epoch [92/120    avg_loss:0.015, val_acc:0.980]
Epoch [93/120    avg_loss:0.014, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.007, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     0     0     2     0    51     2]
 [    0     0 17682     0   158     0   241     0     9     0]
 [    0    10     0  1921     0     0     0     0   105     0]
 [    0    14     3     0  2947     0     4     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     0     0     0  4848     0     0     0]
 [    0    57     0     0     0     0     0  1232     1     0]
 [    0    38     6    50    46     0    11     0  3419     1]
 [    0     2     0     0    14    18     0     0     0   885]]

Accuracy:
97.8863904755019

F1 scores:
[       nan 0.98638824 0.9875178  0.95882206 0.96040411 0.99315068
 0.97115385 0.97700238 0.95516134 0.9789823 ]

Kappa:
0.9720946707258451
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15f40fe940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.657, val_acc:0.634]
Epoch [2/120    avg_loss:1.004, val_acc:0.771]
Epoch [3/120    avg_loss:0.713, val_acc:0.671]
Epoch [4/120    avg_loss:0.549, val_acc:0.750]
Epoch [5/120    avg_loss:0.491, val_acc:0.687]
Epoch [6/120    avg_loss:0.400, val_acc:0.709]
Epoch [7/120    avg_loss:0.343, val_acc:0.793]
Epoch [8/120    avg_loss:0.306, val_acc:0.831]
Epoch [9/120    avg_loss:0.266, val_acc:0.801]
Epoch [10/120    avg_loss:0.241, val_acc:0.896]
Epoch [11/120    avg_loss:0.231, val_acc:0.891]
Epoch [12/120    avg_loss:0.245, val_acc:0.894]
Epoch [13/120    avg_loss:0.199, val_acc:0.903]
Epoch [14/120    avg_loss:0.166, val_acc:0.918]
Epoch [15/120    avg_loss:0.164, val_acc:0.904]
Epoch [16/120    avg_loss:0.233, val_acc:0.911]
Epoch [17/120    avg_loss:0.162, val_acc:0.849]
Epoch [18/120    avg_loss:0.175, val_acc:0.876]
Epoch [19/120    avg_loss:0.177, val_acc:0.915]
Epoch [20/120    avg_loss:0.130, val_acc:0.932]
Epoch [21/120    avg_loss:0.110, val_acc:0.909]
Epoch [22/120    avg_loss:0.092, val_acc:0.924]
Epoch [23/120    avg_loss:0.124, val_acc:0.910]
Epoch [24/120    avg_loss:0.110, val_acc:0.891]
Epoch [25/120    avg_loss:0.121, val_acc:0.943]
Epoch [26/120    avg_loss:0.105, val_acc:0.964]
Epoch [27/120    avg_loss:0.089, val_acc:0.929]
Epoch [28/120    avg_loss:0.091, val_acc:0.857]
Epoch [29/120    avg_loss:0.101, val_acc:0.954]
Epoch [30/120    avg_loss:0.063, val_acc:0.938]
Epoch [31/120    avg_loss:0.107, val_acc:0.893]
Epoch [32/120    avg_loss:0.087, val_acc:0.968]
Epoch [33/120    avg_loss:0.063, val_acc:0.947]
Epoch [34/120    avg_loss:0.065, val_acc:0.945]
Epoch [35/120    avg_loss:0.060, val_acc:0.895]
Epoch [36/120    avg_loss:0.086, val_acc:0.961]
Epoch [37/120    avg_loss:0.054, val_acc:0.966]
Epoch [38/120    avg_loss:0.119, val_acc:0.943]
Epoch [39/120    avg_loss:0.086, val_acc:0.951]
Epoch [40/120    avg_loss:0.038, val_acc:0.968]
Epoch [41/120    avg_loss:0.060, val_acc:0.941]
Epoch [42/120    avg_loss:0.040, val_acc:0.952]
Epoch [43/120    avg_loss:0.044, val_acc:0.965]
Epoch [44/120    avg_loss:0.035, val_acc:0.968]
Epoch [45/120    avg_loss:0.065, val_acc:0.946]
Epoch [46/120    avg_loss:0.065, val_acc:0.915]
Epoch [47/120    avg_loss:0.077, val_acc:0.964]
Epoch [48/120    avg_loss:0.062, val_acc:0.963]
Epoch [49/120    avg_loss:0.069, val_acc:0.965]
Epoch [50/120    avg_loss:0.068, val_acc:0.978]
Epoch [51/120    avg_loss:0.032, val_acc:0.977]
Epoch [52/120    avg_loss:0.061, val_acc:0.834]
Epoch [53/120    avg_loss:0.043, val_acc:0.978]
Epoch [54/120    avg_loss:0.043, val_acc:0.979]
Epoch [55/120    avg_loss:0.022, val_acc:0.966]
Epoch [56/120    avg_loss:0.038, val_acc:0.944]
Epoch [57/120    avg_loss:0.070, val_acc:0.963]
Epoch [58/120    avg_loss:0.068, val_acc:0.868]
Epoch [59/120    avg_loss:0.093, val_acc:0.937]
Epoch [60/120    avg_loss:0.045, val_acc:0.964]
Epoch [61/120    avg_loss:0.036, val_acc:0.976]
Epoch [62/120    avg_loss:0.032, val_acc:0.973]
Epoch [63/120    avg_loss:0.032, val_acc:0.963]
Epoch [64/120    avg_loss:0.032, val_acc:0.956]
Epoch [65/120    avg_loss:0.024, val_acc:0.976]
Epoch [66/120    avg_loss:0.039, val_acc:0.975]
Epoch [67/120    avg_loss:0.069, val_acc:0.970]
Epoch [68/120    avg_loss:0.032, val_acc:0.981]
Epoch [69/120    avg_loss:0.027, val_acc:0.981]
Epoch [70/120    avg_loss:0.027, val_acc:0.981]
Epoch [71/120    avg_loss:0.022, val_acc:0.981]
Epoch [72/120    avg_loss:0.022, val_acc:0.983]
Epoch [73/120    avg_loss:0.020, val_acc:0.983]
Epoch [74/120    avg_loss:0.023, val_acc:0.982]
Epoch [75/120    avg_loss:0.028, val_acc:0.984]
Epoch [76/120    avg_loss:0.024, val_acc:0.984]
Epoch [77/120    avg_loss:0.019, val_acc:0.982]
Epoch [78/120    avg_loss:0.019, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.984]
Epoch [80/120    avg_loss:0.019, val_acc:0.984]
Epoch [81/120    avg_loss:0.015, val_acc:0.986]
Epoch [82/120    avg_loss:0.016, val_acc:0.984]
Epoch [83/120    avg_loss:0.025, val_acc:0.981]
Epoch [84/120    avg_loss:0.018, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.016, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.982]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.016, val_acc:0.984]
Epoch [94/120    avg_loss:0.015, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.984]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.013, val_acc:0.985]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.017, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.013, val_acc:0.985]
Epoch [104/120    avg_loss:0.017, val_acc:0.985]
Epoch [105/120    avg_loss:0.014, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.014, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.985]
Epoch [111/120    avg_loss:0.013, val_acc:0.985]
Epoch [112/120    avg_loss:0.015, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.016, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     1     4     0     0     4     0    23     4]
 [    0     0 17939     0    27     0   121     0     3     0]
 [    0     2     0  1893     0     0     0     0   137     4]
 [    0     5     4     0  2925     2    21     0    14     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0    10     0     0     0     0     0  1275     2     3]
 [    0    10     4    80    30     0    12     0  3435     0]
 [    0     0     0     0     0    22     0     0     0   897]]

Accuracy:
98.6696551225508

F1 scores:
[       nan 0.99509918 0.99556024 0.94343384 0.98253275 0.99088838
 0.98385795 0.99415205 0.95589258 0.98140044]

Kappa:
0.9823964139968174
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7696f0a908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.691, val_acc:0.677]
Epoch [2/120    avg_loss:0.967, val_acc:0.776]
Epoch [3/120    avg_loss:0.648, val_acc:0.767]
Epoch [4/120    avg_loss:0.525, val_acc:0.759]
Epoch [5/120    avg_loss:0.391, val_acc:0.835]
Epoch [6/120    avg_loss:0.374, val_acc:0.859]
Epoch [7/120    avg_loss:0.328, val_acc:0.776]
Epoch [8/120    avg_loss:0.307, val_acc:0.880]
Epoch [9/120    avg_loss:0.267, val_acc:0.893]
Epoch [10/120    avg_loss:0.247, val_acc:0.852]
Epoch [11/120    avg_loss:0.254, val_acc:0.831]
Epoch [12/120    avg_loss:0.200, val_acc:0.911]
Epoch [13/120    avg_loss:0.183, val_acc:0.889]
Epoch [14/120    avg_loss:0.171, val_acc:0.850]
Epoch [15/120    avg_loss:0.160, val_acc:0.918]
Epoch [16/120    avg_loss:0.144, val_acc:0.897]
Epoch [17/120    avg_loss:0.156, val_acc:0.855]
Epoch [18/120    avg_loss:0.143, val_acc:0.914]
Epoch [19/120    avg_loss:0.234, val_acc:0.847]
Epoch [20/120    avg_loss:0.144, val_acc:0.925]
Epoch [21/120    avg_loss:0.125, val_acc:0.906]
Epoch [22/120    avg_loss:0.080, val_acc:0.956]
Epoch [23/120    avg_loss:0.108, val_acc:0.926]
Epoch [24/120    avg_loss:0.140, val_acc:0.928]
Epoch [25/120    avg_loss:0.104, val_acc:0.935]
Epoch [26/120    avg_loss:0.071, val_acc:0.950]
Epoch [27/120    avg_loss:0.071, val_acc:0.948]
Epoch [28/120    avg_loss:0.087, val_acc:0.941]
Epoch [29/120    avg_loss:0.101, val_acc:0.945]
Epoch [30/120    avg_loss:0.058, val_acc:0.954]
Epoch [31/120    avg_loss:0.088, val_acc:0.958]
Epoch [32/120    avg_loss:0.074, val_acc:0.932]
Epoch [33/120    avg_loss:0.061, val_acc:0.957]
Epoch [34/120    avg_loss:0.048, val_acc:0.969]
Epoch [35/120    avg_loss:0.070, val_acc:0.949]
Epoch [36/120    avg_loss:0.100, val_acc:0.963]
Epoch [37/120    avg_loss:0.054, val_acc:0.968]
Epoch [38/120    avg_loss:0.050, val_acc:0.966]
Epoch [39/120    avg_loss:0.077, val_acc:0.963]
Epoch [40/120    avg_loss:0.069, val_acc:0.968]
Epoch [41/120    avg_loss:0.050, val_acc:0.964]
Epoch [42/120    avg_loss:0.028, val_acc:0.970]
Epoch [43/120    avg_loss:0.032, val_acc:0.969]
Epoch [44/120    avg_loss:0.058, val_acc:0.950]
Epoch [45/120    avg_loss:0.028, val_acc:0.966]
Epoch [46/120    avg_loss:0.025, val_acc:0.974]
Epoch [47/120    avg_loss:0.050, val_acc:0.940]
Epoch [48/120    avg_loss:0.083, val_acc:0.966]
Epoch [49/120    avg_loss:0.033, val_acc:0.975]
Epoch [50/120    avg_loss:0.019, val_acc:0.971]
Epoch [51/120    avg_loss:0.030, val_acc:0.971]
Epoch [52/120    avg_loss:0.043, val_acc:0.960]
Epoch [53/120    avg_loss:0.030, val_acc:0.963]
Epoch [54/120    avg_loss:0.023, val_acc:0.978]
Epoch [55/120    avg_loss:0.033, val_acc:0.972]
Epoch [56/120    avg_loss:0.020, val_acc:0.968]
Epoch [57/120    avg_loss:0.028, val_acc:0.972]
Epoch [58/120    avg_loss:0.021, val_acc:0.973]
Epoch [59/120    avg_loss:0.021, val_acc:0.978]
Epoch [60/120    avg_loss:0.015, val_acc:0.974]
Epoch [61/120    avg_loss:0.024, val_acc:0.962]
Epoch [62/120    avg_loss:0.032, val_acc:0.972]
Epoch [63/120    avg_loss:0.029, val_acc:0.971]
Epoch [64/120    avg_loss:0.027, val_acc:0.973]
Epoch [65/120    avg_loss:0.052, val_acc:0.916]
Epoch [66/120    avg_loss:0.098, val_acc:0.964]
Epoch [67/120    avg_loss:0.034, val_acc:0.969]
Epoch [68/120    avg_loss:0.022, val_acc:0.965]
Epoch [69/120    avg_loss:0.028, val_acc:0.973]
Epoch [70/120    avg_loss:0.016, val_acc:0.975]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.010, val_acc:0.977]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.011, val_acc:0.977]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.010, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.978]
Epoch [79/120    avg_loss:0.006, val_acc:0.979]
Epoch [80/120    avg_loss:0.007, val_acc:0.978]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.008, val_acc:0.979]
Epoch [84/120    avg_loss:0.008, val_acc:0.980]
Epoch [85/120    avg_loss:0.007, val_acc:0.979]
Epoch [86/120    avg_loss:0.007, val_acc:0.978]
Epoch [87/120    avg_loss:0.011, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.014, val_acc:0.979]
Epoch [91/120    avg_loss:0.007, val_acc:0.981]
Epoch [92/120    avg_loss:0.006, val_acc:0.982]
Epoch [93/120    avg_loss:0.005, val_acc:0.982]
Epoch [94/120    avg_loss:0.005, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.008, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.004, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.005, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.004, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     0     0     0     2     9    43     8]
 [    0     0 17665     0    76     0   347     0     1     1]
 [    0     5     0  1870     0     0     0     0   161     0]
 [    0    15     2     0  2947     0     4     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     2     0  4874     0     0     0]
 [    0    26     0     0     0     0     0  1263     0     1]
 [    0    25     5    52    35     0     1     0  3453     0]
 [    0     0     0     0     4     6     0     0     0   909]]

Accuracy:
97.9827922782156

F1 scores:
[       nan 0.9896683  0.98786489 0.94492168 0.97647449 0.99770642
 0.9645755  0.98594848 0.95518672 0.98750679]

Kappa:
0.9733817604515461
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6502bab898>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.652, val_acc:0.667]
Epoch [2/120    avg_loss:1.038, val_acc:0.714]
Epoch [3/120    avg_loss:0.723, val_acc:0.760]
Epoch [4/120    avg_loss:0.581, val_acc:0.780]
Epoch [5/120    avg_loss:0.456, val_acc:0.793]
Epoch [6/120    avg_loss:0.394, val_acc:0.803]
Epoch [7/120    avg_loss:0.311, val_acc:0.794]
Epoch [8/120    avg_loss:0.262, val_acc:0.863]
Epoch [9/120    avg_loss:0.255, val_acc:0.885]
Epoch [10/120    avg_loss:0.248, val_acc:0.847]
Epoch [11/120    avg_loss:0.225, val_acc:0.885]
Epoch [12/120    avg_loss:0.190, val_acc:0.914]
Epoch [13/120    avg_loss:0.230, val_acc:0.850]
Epoch [14/120    avg_loss:0.185, val_acc:0.891]
Epoch [15/120    avg_loss:0.198, val_acc:0.812]
Epoch [16/120    avg_loss:0.173, val_acc:0.902]
Epoch [17/120    avg_loss:0.147, val_acc:0.831]
Epoch [18/120    avg_loss:0.209, val_acc:0.918]
Epoch [19/120    avg_loss:0.150, val_acc:0.916]
Epoch [20/120    avg_loss:0.108, val_acc:0.951]
Epoch [21/120    avg_loss:0.085, val_acc:0.947]
Epoch [22/120    avg_loss:0.107, val_acc:0.947]
Epoch [23/120    avg_loss:0.119, val_acc:0.940]
Epoch [24/120    avg_loss:0.078, val_acc:0.919]
Epoch [25/120    avg_loss:0.315, val_acc:0.770]
Epoch [26/120    avg_loss:0.359, val_acc:0.803]
Epoch [27/120    avg_loss:0.226, val_acc:0.877]
Epoch [28/120    avg_loss:0.232, val_acc:0.886]
Epoch [29/120    avg_loss:0.174, val_acc:0.907]
Epoch [30/120    avg_loss:0.139, val_acc:0.882]
Epoch [31/120    avg_loss:0.098, val_acc:0.893]
Epoch [32/120    avg_loss:0.142, val_acc:0.944]
Epoch [33/120    avg_loss:0.086, val_acc:0.960]
Epoch [34/120    avg_loss:0.081, val_acc:0.935]
Epoch [35/120    avg_loss:0.075, val_acc:0.954]
Epoch [36/120    avg_loss:0.078, val_acc:0.931]
Epoch [37/120    avg_loss:0.068, val_acc:0.953]
Epoch [38/120    avg_loss:0.055, val_acc:0.953]
Epoch [39/120    avg_loss:0.056, val_acc:0.954]
Epoch [40/120    avg_loss:0.068, val_acc:0.956]
Epoch [41/120    avg_loss:0.051, val_acc:0.955]
Epoch [42/120    avg_loss:0.061, val_acc:0.920]
Epoch [43/120    avg_loss:0.051, val_acc:0.940]
Epoch [44/120    avg_loss:0.030, val_acc:0.960]
Epoch [45/120    avg_loss:0.048, val_acc:0.962]
Epoch [46/120    avg_loss:0.042, val_acc:0.961]
Epoch [47/120    avg_loss:0.049, val_acc:0.962]
Epoch [48/120    avg_loss:0.061, val_acc:0.963]
Epoch [49/120    avg_loss:0.045, val_acc:0.968]
Epoch [50/120    avg_loss:0.031, val_acc:0.972]
Epoch [51/120    avg_loss:0.018, val_acc:0.969]
Epoch [52/120    avg_loss:0.032, val_acc:0.930]
Epoch [53/120    avg_loss:0.069, val_acc:0.957]
Epoch [54/120    avg_loss:0.025, val_acc:0.964]
Epoch [55/120    avg_loss:0.022, val_acc:0.972]
Epoch [56/120    avg_loss:0.020, val_acc:0.972]
Epoch [57/120    avg_loss:0.014, val_acc:0.969]
Epoch [58/120    avg_loss:0.045, val_acc:0.948]
Epoch [59/120    avg_loss:0.035, val_acc:0.970]
Epoch [60/120    avg_loss:0.041, val_acc:0.960]
Epoch [61/120    avg_loss:0.033, val_acc:0.980]
Epoch [62/120    avg_loss:0.043, val_acc:0.959]
Epoch [63/120    avg_loss:0.032, val_acc:0.965]
Epoch [64/120    avg_loss:0.021, val_acc:0.975]
Epoch [65/120    avg_loss:0.020, val_acc:0.978]
Epoch [66/120    avg_loss:0.017, val_acc:0.972]
Epoch [67/120    avg_loss:0.019, val_acc:0.978]
Epoch [68/120    avg_loss:0.013, val_acc:0.976]
Epoch [69/120    avg_loss:0.016, val_acc:0.969]
Epoch [70/120    avg_loss:0.011, val_acc:0.974]
Epoch [71/120    avg_loss:0.008, val_acc:0.977]
Epoch [72/120    avg_loss:0.013, val_acc:0.975]
Epoch [73/120    avg_loss:0.013, val_acc:0.978]
Epoch [74/120    avg_loss:0.006, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.005, val_acc:0.979]
Epoch [77/120    avg_loss:0.006, val_acc:0.979]
Epoch [78/120    avg_loss:0.005, val_acc:0.979]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.009, val_acc:0.979]
Epoch [81/120    avg_loss:0.004, val_acc:0.980]
Epoch [82/120    avg_loss:0.004, val_acc:0.980]
Epoch [83/120    avg_loss:0.006, val_acc:0.980]
Epoch [84/120    avg_loss:0.009, val_acc:0.978]
Epoch [85/120    avg_loss:0.010, val_acc:0.979]
Epoch [86/120    avg_loss:0.005, val_acc:0.980]
Epoch [87/120    avg_loss:0.006, val_acc:0.979]
Epoch [88/120    avg_loss:0.007, val_acc:0.979]
Epoch [89/120    avg_loss:0.004, val_acc:0.979]
Epoch [90/120    avg_loss:0.006, val_acc:0.979]
Epoch [91/120    avg_loss:0.005, val_acc:0.981]
Epoch [92/120    avg_loss:0.006, val_acc:0.980]
Epoch [93/120    avg_loss:0.004, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.004, val_acc:0.980]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.005, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.981]
Epoch [100/120    avg_loss:0.005, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.981]
Epoch [104/120    avg_loss:0.005, val_acc:0.981]
Epoch [105/120    avg_loss:0.005, val_acc:0.980]
Epoch [106/120    avg_loss:0.005, val_acc:0.982]
Epoch [107/120    avg_loss:0.004, val_acc:0.982]
Epoch [108/120    avg_loss:0.004, val_acc:0.982]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.004, val_acc:0.982]
Epoch [111/120    avg_loss:0.003, val_acc:0.979]
Epoch [112/120    avg_loss:0.003, val_acc:0.981]
Epoch [113/120    avg_loss:0.004, val_acc:0.979]
Epoch [114/120    avg_loss:0.004, val_acc:0.981]
Epoch [115/120    avg_loss:0.004, val_acc:0.980]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.004, val_acc:0.981]
Epoch [118/120    avg_loss:0.004, val_acc:0.979]
Epoch [119/120    avg_loss:0.004, val_acc:0.979]
Epoch [120/120    avg_loss:0.005, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6351     0     0     0     0     6    14    57     4]
 [    0     0 18044     0    34     0     6     0     6     0]
 [    0     7     0  1932     0     0     0     0    96     1]
 [    0    26     5     0  2901     0    27     0     8     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    39     0    15     0  4822     0     2     0]
 [    0    14     0     0     0     0     3  1273     0     0]
 [    0    33     0    86    42     0     1     0  3409     0]
 [    0     0     0     0    14     2     0     0     0   903]]

Accuracy:
98.66724507748295

F1 scores:
[       nan 0.98748348 0.9975123  0.95313271 0.97055872 0.9992343
 0.98983886 0.98797051 0.95369982 0.98580786]

Kappa:
0.9823405103167239
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28917ec7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.679, val_acc:0.411]
Epoch [2/120    avg_loss:0.910, val_acc:0.728]
Epoch [3/120    avg_loss:0.584, val_acc:0.818]
Epoch [4/120    avg_loss:0.480, val_acc:0.791]
Epoch [5/120    avg_loss:0.388, val_acc:0.829]
Epoch [6/120    avg_loss:0.354, val_acc:0.790]
Epoch [7/120    avg_loss:0.321, val_acc:0.847]
Epoch [8/120    avg_loss:0.288, val_acc:0.854]
Epoch [9/120    avg_loss:0.236, val_acc:0.909]
Epoch [10/120    avg_loss:0.256, val_acc:0.873]
Epoch [11/120    avg_loss:0.215, val_acc:0.890]
Epoch [12/120    avg_loss:0.216, val_acc:0.925]
Epoch [13/120    avg_loss:0.226, val_acc:0.925]
Epoch [14/120    avg_loss:0.200, val_acc:0.887]
Epoch [15/120    avg_loss:0.168, val_acc:0.925]
Epoch [16/120    avg_loss:0.195, val_acc:0.906]
Epoch [17/120    avg_loss:0.186, val_acc:0.911]
Epoch [18/120    avg_loss:0.203, val_acc:0.895]
Epoch [19/120    avg_loss:0.201, val_acc:0.945]
Epoch [20/120    avg_loss:0.127, val_acc:0.938]
Epoch [21/120    avg_loss:0.123, val_acc:0.945]
Epoch [22/120    avg_loss:0.116, val_acc:0.894]
Epoch [23/120    avg_loss:0.092, val_acc:0.959]
Epoch [24/120    avg_loss:0.090, val_acc:0.945]
Epoch [25/120    avg_loss:0.079, val_acc:0.935]
Epoch [26/120    avg_loss:0.149, val_acc:0.929]
Epoch [27/120    avg_loss:0.110, val_acc:0.934]
Epoch [28/120    avg_loss:0.093, val_acc:0.962]
Epoch [29/120    avg_loss:0.081, val_acc:0.919]
Epoch [30/120    avg_loss:0.082, val_acc:0.948]
Epoch [31/120    avg_loss:0.119, val_acc:0.960]
Epoch [32/120    avg_loss:0.058, val_acc:0.916]
Epoch [33/120    avg_loss:0.068, val_acc:0.952]
Epoch [34/120    avg_loss:0.073, val_acc:0.938]
Epoch [35/120    avg_loss:0.066, val_acc:0.970]
Epoch [36/120    avg_loss:0.055, val_acc:0.960]
Epoch [37/120    avg_loss:0.045, val_acc:0.982]
Epoch [38/120    avg_loss:0.095, val_acc:0.954]
Epoch [39/120    avg_loss:0.090, val_acc:0.966]
Epoch [40/120    avg_loss:0.057, val_acc:0.964]
Epoch [41/120    avg_loss:0.054, val_acc:0.961]
Epoch [42/120    avg_loss:0.043, val_acc:0.978]
Epoch [43/120    avg_loss:0.033, val_acc:0.981]
Epoch [44/120    avg_loss:0.081, val_acc:0.957]
Epoch [45/120    avg_loss:0.039, val_acc:0.978]
Epoch [46/120    avg_loss:0.081, val_acc:0.964]
Epoch [47/120    avg_loss:0.024, val_acc:0.980]
Epoch [48/120    avg_loss:0.031, val_acc:0.918]
Epoch [49/120    avg_loss:0.049, val_acc:0.979]
Epoch [50/120    avg_loss:0.032, val_acc:0.959]
Epoch [51/120    avg_loss:0.040, val_acc:0.973]
Epoch [52/120    avg_loss:0.027, val_acc:0.978]
Epoch [53/120    avg_loss:0.019, val_acc:0.976]
Epoch [54/120    avg_loss:0.023, val_acc:0.978]
Epoch [55/120    avg_loss:0.018, val_acc:0.979]
Epoch [56/120    avg_loss:0.015, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.981]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.013, val_acc:0.984]
Epoch [60/120    avg_loss:0.024, val_acc:0.984]
Epoch [61/120    avg_loss:0.015, val_acc:0.987]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.018, val_acc:0.983]
Epoch [64/120    avg_loss:0.016, val_acc:0.979]
Epoch [65/120    avg_loss:0.019, val_acc:0.985]
Epoch [66/120    avg_loss:0.015, val_acc:0.987]
Epoch [67/120    avg_loss:0.019, val_acc:0.986]
Epoch [68/120    avg_loss:0.018, val_acc:0.986]
Epoch [69/120    avg_loss:0.016, val_acc:0.988]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.012, val_acc:0.987]
Epoch [72/120    avg_loss:0.012, val_acc:0.988]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.012, val_acc:0.989]
Epoch [75/120    avg_loss:0.013, val_acc:0.988]
Epoch [76/120    avg_loss:0.019, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.011, val_acc:0.987]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.015, val_acc:0.987]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.010, val_acc:0.989]
Epoch [84/120    avg_loss:0.013, val_acc:0.989]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.013, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.988]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.011, val_acc:0.990]
Epoch [91/120    avg_loss:0.013, val_acc:0.988]
Epoch [92/120    avg_loss:0.011, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.991]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.011, val_acc:0.989]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.011, val_acc:0.989]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.991]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.016, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.989]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.989]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.989]
Epoch [113/120    avg_loss:0.008, val_acc:0.990]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.991]
Epoch [116/120    avg_loss:0.008, val_acc:0.991]
Epoch [117/120    avg_loss:0.010, val_acc:0.991]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     3     0     0    17     0    26     1]
 [    0     0 17321     0    81     0   687     0     1     0]
 [    0     1     0  1898     0     0     0     0   137     0]
 [    0     5     5     0  2909     2    29     3    15     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4869     0     9     0]
 [    0     6     0     0     0     3     0  1281     0     0]
 [    0     8     2   135    39     0     8     0  3379     0]
 [    0     0     0     0    11     4     0     0     0   904]]

Accuracy:
97.00672402573929

F1 scores:
[       nan 0.99478071 0.97809024 0.93222004 0.9677312  0.99656357
 0.9284897  0.995338   0.9467638  0.98905908]

Kappa:
0.9606463720554154
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4463888d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.701, val_acc:0.638]
Epoch [2/120    avg_loss:0.998, val_acc:0.554]
Epoch [3/120    avg_loss:0.717, val_acc:0.636]
Epoch [4/120    avg_loss:0.539, val_acc:0.784]
Epoch [5/120    avg_loss:0.476, val_acc:0.787]
Epoch [6/120    avg_loss:0.446, val_acc:0.726]
Epoch [7/120    avg_loss:0.368, val_acc:0.800]
Epoch [8/120    avg_loss:0.302, val_acc:0.862]
Epoch [9/120    avg_loss:0.236, val_acc:0.855]
Epoch [10/120    avg_loss:0.303, val_acc:0.821]
Epoch [11/120    avg_loss:0.285, val_acc:0.876]
Epoch [12/120    avg_loss:0.243, val_acc:0.818]
Epoch [13/120    avg_loss:0.242, val_acc:0.813]
Epoch [14/120    avg_loss:0.206, val_acc:0.923]
Epoch [15/120    avg_loss:0.188, val_acc:0.894]
Epoch [16/120    avg_loss:0.158, val_acc:0.899]
Epoch [17/120    avg_loss:0.122, val_acc:0.919]
Epoch [18/120    avg_loss:0.119, val_acc:0.926]
Epoch [19/120    avg_loss:0.124, val_acc:0.929]
Epoch [20/120    avg_loss:0.110, val_acc:0.921]
Epoch [21/120    avg_loss:0.136, val_acc:0.950]
Epoch [22/120    avg_loss:0.122, val_acc:0.900]
Epoch [23/120    avg_loss:0.098, val_acc:0.942]
Epoch [24/120    avg_loss:0.076, val_acc:0.959]
Epoch [25/120    avg_loss:0.063, val_acc:0.950]
Epoch [26/120    avg_loss:0.097, val_acc:0.915]
Epoch [27/120    avg_loss:0.061, val_acc:0.930]
Epoch [28/120    avg_loss:0.065, val_acc:0.955]
Epoch [29/120    avg_loss:0.072, val_acc:0.947]
Epoch [30/120    avg_loss:0.056, val_acc:0.958]
Epoch [31/120    avg_loss:0.084, val_acc:0.918]
Epoch [32/120    avg_loss:0.048, val_acc:0.962]
Epoch [33/120    avg_loss:0.073, val_acc:0.935]
Epoch [34/120    avg_loss:0.047, val_acc:0.962]
Epoch [35/120    avg_loss:0.052, val_acc:0.968]
Epoch [36/120    avg_loss:0.040, val_acc:0.973]
Epoch [37/120    avg_loss:0.044, val_acc:0.925]
Epoch [38/120    avg_loss:0.047, val_acc:0.938]
Epoch [39/120    avg_loss:0.089, val_acc:0.935]
Epoch [40/120    avg_loss:0.061, val_acc:0.873]
Epoch [41/120    avg_loss:0.083, val_acc:0.950]
Epoch [42/120    avg_loss:0.044, val_acc:0.955]
Epoch [43/120    avg_loss:0.042, val_acc:0.967]
Epoch [44/120    avg_loss:0.023, val_acc:0.973]
Epoch [45/120    avg_loss:0.028, val_acc:0.974]
Epoch [46/120    avg_loss:0.031, val_acc:0.966]
Epoch [47/120    avg_loss:0.028, val_acc:0.973]
Epoch [48/120    avg_loss:0.097, val_acc:0.969]
Epoch [49/120    avg_loss:0.035, val_acc:0.970]
Epoch [50/120    avg_loss:0.044, val_acc:0.969]
Epoch [51/120    avg_loss:0.038, val_acc:0.974]
Epoch [52/120    avg_loss:0.022, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.975]
Epoch [54/120    avg_loss:0.017, val_acc:0.978]
Epoch [55/120    avg_loss:0.013, val_acc:0.972]
Epoch [56/120    avg_loss:0.013, val_acc:0.976]
Epoch [57/120    avg_loss:0.019, val_acc:0.974]
Epoch [58/120    avg_loss:0.018, val_acc:0.939]
Epoch [59/120    avg_loss:0.016, val_acc:0.976]
Epoch [60/120    avg_loss:0.012, val_acc:0.978]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.006, val_acc:0.978]
Epoch [63/120    avg_loss:0.011, val_acc:0.982]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.029, val_acc:0.977]
Epoch [66/120    avg_loss:0.015, val_acc:0.975]
Epoch [67/120    avg_loss:0.018, val_acc:0.980]
Epoch [68/120    avg_loss:0.022, val_acc:0.958]
Epoch [69/120    avg_loss:0.024, val_acc:0.969]
Epoch [70/120    avg_loss:0.021, val_acc:0.971]
Epoch [71/120    avg_loss:0.020, val_acc:0.971]
Epoch [72/120    avg_loss:0.150, val_acc:0.872]
Epoch [73/120    avg_loss:0.086, val_acc:0.965]
Epoch [74/120    avg_loss:0.038, val_acc:0.965]
Epoch [75/120    avg_loss:0.023, val_acc:0.964]
Epoch [76/120    avg_loss:0.023, val_acc:0.974]
Epoch [77/120    avg_loss:0.029, val_acc:0.973]
Epoch [78/120    avg_loss:0.024, val_acc:0.973]
Epoch [79/120    avg_loss:0.021, val_acc:0.976]
Epoch [80/120    avg_loss:0.018, val_acc:0.974]
Epoch [81/120    avg_loss:0.017, val_acc:0.974]
Epoch [82/120    avg_loss:0.018, val_acc:0.975]
Epoch [83/120    avg_loss:0.014, val_acc:0.975]
Epoch [84/120    avg_loss:0.016, val_acc:0.967]
Epoch [85/120    avg_loss:0.014, val_acc:0.976]
Epoch [86/120    avg_loss:0.017, val_acc:0.976]
Epoch [87/120    avg_loss:0.016, val_acc:0.977]
Epoch [88/120    avg_loss:0.014, val_acc:0.977]
Epoch [89/120    avg_loss:0.013, val_acc:0.975]
Epoch [90/120    avg_loss:0.013, val_acc:0.976]
Epoch [91/120    avg_loss:0.013, val_acc:0.977]
Epoch [92/120    avg_loss:0.015, val_acc:0.976]
Epoch [93/120    avg_loss:0.011, val_acc:0.976]
Epoch [94/120    avg_loss:0.013, val_acc:0.976]
Epoch [95/120    avg_loss:0.015, val_acc:0.976]
Epoch [96/120    avg_loss:0.012, val_acc:0.976]
Epoch [97/120    avg_loss:0.016, val_acc:0.976]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.977]
Epoch [102/120    avg_loss:0.013, val_acc:0.977]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.014, val_acc:0.978]
Epoch [107/120    avg_loss:0.014, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.018, val_acc:0.978]
Epoch [111/120    avg_loss:0.015, val_acc:0.978]
Epoch [112/120    avg_loss:0.012, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.978]
Epoch [114/120    avg_loss:0.016, val_acc:0.978]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.019, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.978]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     3     0     0     8     4    40    10]
 [    0     0 17884     0    36     0   166     0     4     0]
 [    0     5     0  1900     0     0     0     0   131     0]
 [    0    26     9     0  2915     0    17     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4872     0     2     0]
 [    0    44     0     0     0     0     0  1246     0     0]
 [    0    25     4    51    36     0    12     0  3443     0]
 [    0     0     0     0    12    11     0     0     0   896]]

Accuracy:
98.39732002988455

F1 scores:
[       nan 0.98720831 0.99380401 0.95238095 0.97638587 0.99580313
 0.97900131 0.98110236 0.95731962 0.98030635]

Kappa:
0.978799157659594
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12ccbf8400>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.682, val_acc:0.683]
Epoch [2/120    avg_loss:0.996, val_acc:0.740]
Epoch [3/120    avg_loss:0.665, val_acc:0.791]
Epoch [4/120    avg_loss:0.536, val_acc:0.786]
Epoch [5/120    avg_loss:0.411, val_acc:0.834]
Epoch [6/120    avg_loss:0.404, val_acc:0.867]
Epoch [7/120    avg_loss:0.302, val_acc:0.780]
Epoch [8/120    avg_loss:0.277, val_acc:0.844]
Epoch [9/120    avg_loss:0.264, val_acc:0.882]
Epoch [10/120    avg_loss:0.271, val_acc:0.893]
Epoch [11/120    avg_loss:0.235, val_acc:0.888]
Epoch [12/120    avg_loss:0.242, val_acc:0.880]
Epoch [13/120    avg_loss:0.202, val_acc:0.875]
Epoch [14/120    avg_loss:0.176, val_acc:0.891]
Epoch [15/120    avg_loss:0.194, val_acc:0.911]
Epoch [16/120    avg_loss:0.149, val_acc:0.908]
Epoch [17/120    avg_loss:0.164, val_acc:0.924]
Epoch [18/120    avg_loss:0.135, val_acc:0.912]
Epoch [19/120    avg_loss:0.131, val_acc:0.933]
Epoch [20/120    avg_loss:0.139, val_acc:0.932]
Epoch [21/120    avg_loss:0.163, val_acc:0.933]
Epoch [22/120    avg_loss:0.103, val_acc:0.959]
Epoch [23/120    avg_loss:0.164, val_acc:0.878]
Epoch [24/120    avg_loss:0.142, val_acc:0.916]
Epoch [25/120    avg_loss:0.096, val_acc:0.944]
Epoch [26/120    avg_loss:0.092, val_acc:0.934]
Epoch [27/120    avg_loss:0.115, val_acc:0.959]
Epoch [28/120    avg_loss:0.077, val_acc:0.957]
Epoch [29/120    avg_loss:0.070, val_acc:0.962]
Epoch [30/120    avg_loss:0.086, val_acc:0.953]
Epoch [31/120    avg_loss:0.093, val_acc:0.944]
Epoch [32/120    avg_loss:0.081, val_acc:0.956]
Epoch [33/120    avg_loss:0.035, val_acc:0.950]
Epoch [34/120    avg_loss:0.050, val_acc:0.971]
Epoch [35/120    avg_loss:0.059, val_acc:0.966]
Epoch [36/120    avg_loss:0.039, val_acc:0.969]
Epoch [37/120    avg_loss:0.070, val_acc:0.957]
Epoch [38/120    avg_loss:0.050, val_acc:0.951]
Epoch [39/120    avg_loss:0.034, val_acc:0.972]
Epoch [40/120    avg_loss:0.054, val_acc:0.953]
Epoch [41/120    avg_loss:0.054, val_acc:0.965]
Epoch [42/120    avg_loss:0.037, val_acc:0.966]
Epoch [43/120    avg_loss:0.086, val_acc:0.964]
Epoch [44/120    avg_loss:0.042, val_acc:0.968]
Epoch [45/120    avg_loss:0.048, val_acc:0.924]
Epoch [46/120    avg_loss:0.068, val_acc:0.968]
Epoch [47/120    avg_loss:0.033, val_acc:0.971]
Epoch [48/120    avg_loss:0.028, val_acc:0.936]
Epoch [49/120    avg_loss:0.022, val_acc:0.976]
Epoch [50/120    avg_loss:0.027, val_acc:0.978]
Epoch [51/120    avg_loss:0.020, val_acc:0.976]
Epoch [52/120    avg_loss:0.038, val_acc:0.975]
Epoch [53/120    avg_loss:0.052, val_acc:0.951]
Epoch [54/120    avg_loss:0.027, val_acc:0.976]
Epoch [55/120    avg_loss:0.018, val_acc:0.981]
Epoch [56/120    avg_loss:0.022, val_acc:0.981]
Epoch [57/120    avg_loss:0.020, val_acc:0.972]
Epoch [58/120    avg_loss:0.062, val_acc:0.976]
Epoch [59/120    avg_loss:0.023, val_acc:0.958]
Epoch [60/120    avg_loss:0.028, val_acc:0.981]
Epoch [61/120    avg_loss:0.030, val_acc:0.968]
Epoch [62/120    avg_loss:0.026, val_acc:0.986]
Epoch [63/120    avg_loss:0.036, val_acc:0.978]
Epoch [64/120    avg_loss:0.025, val_acc:0.983]
Epoch [65/120    avg_loss:0.019, val_acc:0.982]
Epoch [66/120    avg_loss:0.019, val_acc:0.981]
Epoch [67/120    avg_loss:0.012, val_acc:0.978]
Epoch [68/120    avg_loss:0.040, val_acc:0.980]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.029, val_acc:0.976]
Epoch [71/120    avg_loss:0.012, val_acc:0.973]
Epoch [72/120    avg_loss:0.010, val_acc:0.980]
Epoch [73/120    avg_loss:0.015, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.966]
Epoch [75/120    avg_loss:0.019, val_acc:0.937]
Epoch [76/120    avg_loss:0.018, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.013, val_acc:0.985]
Epoch [83/120    avg_loss:0.008, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.010, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.009, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     1     0     0    28     1    44     2]
 [    0     0 18020     0    43     0    25     0     2     0]
 [    0    10     0  1949     0     0     0     0    77     0]
 [    0    11     7     0  2925     0    25     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     1     0  4877     0     0     0]
 [    0    42     0     0     0     0     0  1247     1     0]
 [    0    24     8    69    35     0     2     0  3433     0]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
98.8552285927747

F1 scores:
[       nan 0.98733981 0.99764706 0.96128237 0.97891566 0.99504384
 0.99176411 0.98266351 0.96310843 0.99016393]

Kappa:
0.9848377311764267
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd78b45c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.697, val_acc:0.657]
Epoch [2/120    avg_loss:1.020, val_acc:0.702]
Epoch [3/120    avg_loss:0.669, val_acc:0.733]
Epoch [4/120    avg_loss:0.551, val_acc:0.744]
Epoch [5/120    avg_loss:0.372, val_acc:0.834]
Epoch [6/120    avg_loss:0.379, val_acc:0.823]
Epoch [7/120    avg_loss:0.277, val_acc:0.877]
Epoch [8/120    avg_loss:0.268, val_acc:0.906]
Epoch [9/120    avg_loss:0.282, val_acc:0.845]
Epoch [10/120    avg_loss:0.250, val_acc:0.916]
Epoch [11/120    avg_loss:0.217, val_acc:0.683]
Epoch [12/120    avg_loss:0.190, val_acc:0.901]
Epoch [13/120    avg_loss:0.174, val_acc:0.907]
Epoch [14/120    avg_loss:0.162, val_acc:0.914]
Epoch [15/120    avg_loss:0.136, val_acc:0.850]
Epoch [16/120    avg_loss:0.137, val_acc:0.929]
Epoch [17/120    avg_loss:0.156, val_acc:0.951]
Epoch [18/120    avg_loss:0.131, val_acc:0.919]
Epoch [19/120    avg_loss:0.119, val_acc:0.950]
Epoch [20/120    avg_loss:0.127, val_acc:0.938]
Epoch [21/120    avg_loss:0.083, val_acc:0.942]
Epoch [22/120    avg_loss:0.103, val_acc:0.946]
Epoch [23/120    avg_loss:0.078, val_acc:0.927]
Epoch [24/120    avg_loss:0.101, val_acc:0.937]
Epoch [25/120    avg_loss:0.116, val_acc:0.956]
Epoch [26/120    avg_loss:0.076, val_acc:0.932]
Epoch [27/120    avg_loss:0.074, val_acc:0.948]
Epoch [28/120    avg_loss:0.058, val_acc:0.947]
Epoch [29/120    avg_loss:0.060, val_acc:0.959]
Epoch [30/120    avg_loss:0.061, val_acc:0.957]
Epoch [31/120    avg_loss:0.075, val_acc:0.951]
Epoch [32/120    avg_loss:0.051, val_acc:0.972]
Epoch [33/120    avg_loss:0.057, val_acc:0.970]
Epoch [34/120    avg_loss:0.025, val_acc:0.973]
Epoch [35/120    avg_loss:0.039, val_acc:0.974]
Epoch [36/120    avg_loss:0.039, val_acc:0.915]
Epoch [37/120    avg_loss:0.031, val_acc:0.976]
Epoch [38/120    avg_loss:0.033, val_acc:0.978]
Epoch [39/120    avg_loss:0.048, val_acc:0.970]
Epoch [40/120    avg_loss:0.034, val_acc:0.957]
Epoch [41/120    avg_loss:0.032, val_acc:0.967]
Epoch [42/120    avg_loss:0.022, val_acc:0.979]
Epoch [43/120    avg_loss:0.017, val_acc:0.982]
Epoch [44/120    avg_loss:0.105, val_acc:0.914]
Epoch [45/120    avg_loss:0.119, val_acc:0.954]
Epoch [46/120    avg_loss:0.054, val_acc:0.975]
Epoch [47/120    avg_loss:0.038, val_acc:0.970]
Epoch [48/120    avg_loss:0.022, val_acc:0.977]
Epoch [49/120    avg_loss:0.030, val_acc:0.968]
Epoch [50/120    avg_loss:0.017, val_acc:0.970]
Epoch [51/120    avg_loss:0.013, val_acc:0.974]
Epoch [52/120    avg_loss:0.030, val_acc:0.974]
Epoch [53/120    avg_loss:0.017, val_acc:0.974]
Epoch [54/120    avg_loss:0.043, val_acc:0.968]
Epoch [55/120    avg_loss:0.141, val_acc:0.961]
Epoch [56/120    avg_loss:0.048, val_acc:0.975]
Epoch [57/120    avg_loss:0.027, val_acc:0.983]
Epoch [58/120    avg_loss:0.023, val_acc:0.982]
Epoch [59/120    avg_loss:0.023, val_acc:0.980]
Epoch [60/120    avg_loss:0.015, val_acc:0.984]
Epoch [61/120    avg_loss:0.019, val_acc:0.984]
Epoch [62/120    avg_loss:0.017, val_acc:0.982]
Epoch [63/120    avg_loss:0.018, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.983]
Epoch [65/120    avg_loss:0.016, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.986]
Epoch [68/120    avg_loss:0.015, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.012, val_acc:0.984]
Epoch [72/120    avg_loss:0.015, val_acc:0.985]
Epoch [73/120    avg_loss:0.011, val_acc:0.984]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.012, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.017, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     6     0     0     0     1     0    26     5]
 [    0     0 17833     0    93     0   159     0     5     0]
 [    0     0     0  1949     0     0     0     0    86     1]
 [    0    24     0     0  2929     0    13     0     5     1]
 [    0     2     0     0     0  1303     0     0     0     0]
 [    0     0    15     0     5     0  4857     0     1     0]
 [    0    12     0     0     0     0     0  1273     0     5]
 [    0     3     0   111    32     0     6     0  3419     0]
 [    0     5     0     0     4     0     0     0     0   910]]

Accuracy:
98.49131178753044

F1 scores:
[       nan 0.99347421 0.99226575 0.95166016 0.97067109 0.99923313
 0.97982651 0.99336715 0.96133839 0.98859316]

Kappa:
0.9800588963304698
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7267595898>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.680, val_acc:0.675]
Epoch [2/120    avg_loss:1.025, val_acc:0.628]
Epoch [3/120    avg_loss:0.689, val_acc:0.762]
Epoch [4/120    avg_loss:0.510, val_acc:0.799]
Epoch [5/120    avg_loss:0.433, val_acc:0.785]
Epoch [6/120    avg_loss:0.350, val_acc:0.842]
Epoch [7/120    avg_loss:0.302, val_acc:0.788]
Epoch [8/120    avg_loss:0.297, val_acc:0.814]
Epoch [9/120    avg_loss:0.260, val_acc:0.877]
Epoch [10/120    avg_loss:0.298, val_acc:0.842]
Epoch [11/120    avg_loss:0.256, val_acc:0.877]
Epoch [12/120    avg_loss:0.239, val_acc:0.917]
Epoch [13/120    avg_loss:0.231, val_acc:0.870]
Epoch [14/120    avg_loss:0.200, val_acc:0.902]
Epoch [15/120    avg_loss:0.198, val_acc:0.901]
Epoch [16/120    avg_loss:0.180, val_acc:0.905]
Epoch [17/120    avg_loss:0.156, val_acc:0.886]
Epoch [18/120    avg_loss:0.153, val_acc:0.916]
Epoch [19/120    avg_loss:0.145, val_acc:0.896]
Epoch [20/120    avg_loss:0.120, val_acc:0.901]
Epoch [21/120    avg_loss:0.139, val_acc:0.936]
Epoch [22/120    avg_loss:0.117, val_acc:0.924]
Epoch [23/120    avg_loss:0.130, val_acc:0.942]
Epoch [24/120    avg_loss:0.093, val_acc:0.945]
Epoch [25/120    avg_loss:0.080, val_acc:0.935]
Epoch [26/120    avg_loss:0.087, val_acc:0.803]
Epoch [27/120    avg_loss:0.117, val_acc:0.901]
Epoch [28/120    avg_loss:0.104, val_acc:0.935]
Epoch [29/120    avg_loss:0.089, val_acc:0.958]
Epoch [30/120    avg_loss:0.066, val_acc:0.962]
Epoch [31/120    avg_loss:0.053, val_acc:0.956]
Epoch [32/120    avg_loss:0.089, val_acc:0.937]
Epoch [33/120    avg_loss:0.057, val_acc:0.963]
Epoch [34/120    avg_loss:0.073, val_acc:0.954]
Epoch [35/120    avg_loss:0.065, val_acc:0.934]
Epoch [36/120    avg_loss:0.051, val_acc:0.956]
Epoch [37/120    avg_loss:0.083, val_acc:0.936]
Epoch [38/120    avg_loss:0.064, val_acc:0.896]
Epoch [39/120    avg_loss:0.053, val_acc:0.966]
Epoch [40/120    avg_loss:0.035, val_acc:0.965]
Epoch [41/120    avg_loss:0.054, val_acc:0.951]
Epoch [42/120    avg_loss:0.069, val_acc:0.913]
Epoch [43/120    avg_loss:0.062, val_acc:0.938]
Epoch [44/120    avg_loss:0.086, val_acc:0.956]
Epoch [45/120    avg_loss:0.046, val_acc:0.963]
Epoch [46/120    avg_loss:0.062, val_acc:0.952]
Epoch [47/120    avg_loss:0.022, val_acc:0.965]
Epoch [48/120    avg_loss:0.028, val_acc:0.961]
Epoch [49/120    avg_loss:0.032, val_acc:0.967]
Epoch [50/120    avg_loss:0.031, val_acc:0.970]
Epoch [51/120    avg_loss:0.020, val_acc:0.981]
Epoch [52/120    avg_loss:0.022, val_acc:0.980]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.022, val_acc:0.969]
Epoch [55/120    avg_loss:0.028, val_acc:0.972]
Epoch [56/120    avg_loss:0.035, val_acc:0.979]
Epoch [57/120    avg_loss:0.028, val_acc:0.979]
Epoch [58/120    avg_loss:0.019, val_acc:0.980]
Epoch [59/120    avg_loss:0.025, val_acc:0.959]
Epoch [60/120    avg_loss:0.046, val_acc:0.976]
Epoch [61/120    avg_loss:0.028, val_acc:0.971]
Epoch [62/120    avg_loss:0.057, val_acc:0.939]
Epoch [63/120    avg_loss:0.034, val_acc:0.980]
Epoch [64/120    avg_loss:0.025, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.982]
Epoch [66/120    avg_loss:0.020, val_acc:0.980]
Epoch [67/120    avg_loss:0.011, val_acc:0.980]
Epoch [68/120    avg_loss:0.014, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.965]
Epoch [70/120    avg_loss:0.017, val_acc:0.981]
Epoch [71/120    avg_loss:0.025, val_acc:0.973]
Epoch [72/120    avg_loss:0.029, val_acc:0.976]
Epoch [73/120    avg_loss:0.016, val_acc:0.975]
Epoch [74/120    avg_loss:0.045, val_acc:0.974]
Epoch [75/120    avg_loss:0.011, val_acc:0.974]
Epoch [76/120    avg_loss:0.011, val_acc:0.978]
Epoch [77/120    avg_loss:0.007, val_acc:0.981]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.008, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.012, val_acc:0.975]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.019, val_acc:0.969]
Epoch [85/120    avg_loss:0.011, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.984]
Epoch [91/120    avg_loss:0.005, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.973]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.986]
Epoch [103/120    avg_loss:0.003, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.003, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     0     6    57     1]
 [    0     0 18052     0    13     0    23     0     2     0]
 [    0     1     0  1892     0     0     0     0   142     1]
 [    0     8     6     0  2942     1     7     5     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4874     0     0     0]
 [    0    17     0     0     0     0     0  1273     0     0]
 [    0    33     0    77    33     0     1     0  3427     0]
 [    0     0     0     0    11     5     0     0     0   903]]

Accuracy:
98.89860940399586

F1 scores:
[       nan 0.99043471 0.99867227 0.94481898 0.98542958 0.99770642
 0.99642237 0.98912199 0.95181225 0.98958904]

Kappa:
0.9854088774839964
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f447b5940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.683, val_acc:0.475]
Epoch [2/120    avg_loss:0.973, val_acc:0.696]
Epoch [3/120    avg_loss:0.678, val_acc:0.647]
Epoch [4/120    avg_loss:0.571, val_acc:0.727]
Epoch [5/120    avg_loss:0.475, val_acc:0.748]
Epoch [6/120    avg_loss:0.459, val_acc:0.813]
Epoch [7/120    avg_loss:0.315, val_acc:0.854]
Epoch [8/120    avg_loss:0.342, val_acc:0.809]
Epoch [9/120    avg_loss:0.298, val_acc:0.885]
Epoch [10/120    avg_loss:0.299, val_acc:0.847]
Epoch [11/120    avg_loss:0.247, val_acc:0.883]
Epoch [12/120    avg_loss:0.275, val_acc:0.904]
Epoch [13/120    avg_loss:0.223, val_acc:0.882]
Epoch [14/120    avg_loss:0.224, val_acc:0.784]
Epoch [15/120    avg_loss:0.231, val_acc:0.892]
Epoch [16/120    avg_loss:0.219, val_acc:0.798]
Epoch [17/120    avg_loss:0.167, val_acc:0.914]
Epoch [18/120    avg_loss:0.183, val_acc:0.908]
Epoch [19/120    avg_loss:0.220, val_acc:0.863]
Epoch [20/120    avg_loss:0.144, val_acc:0.900]
Epoch [21/120    avg_loss:0.136, val_acc:0.838]
Epoch [22/120    avg_loss:0.144, val_acc:0.892]
Epoch [23/120    avg_loss:0.114, val_acc:0.941]
Epoch [24/120    avg_loss:0.111, val_acc:0.940]
Epoch [25/120    avg_loss:0.101, val_acc:0.939]
Epoch [26/120    avg_loss:0.142, val_acc:0.919]
Epoch [27/120    avg_loss:0.110, val_acc:0.874]
Epoch [28/120    avg_loss:0.117, val_acc:0.919]
Epoch [29/120    avg_loss:0.112, val_acc:0.938]
Epoch [30/120    avg_loss:0.075, val_acc:0.914]
Epoch [31/120    avg_loss:0.056, val_acc:0.947]
Epoch [32/120    avg_loss:0.055, val_acc:0.948]
Epoch [33/120    avg_loss:0.090, val_acc:0.891]
Epoch [34/120    avg_loss:0.066, val_acc:0.938]
Epoch [35/120    avg_loss:0.054, val_acc:0.965]
Epoch [36/120    avg_loss:0.055, val_acc:0.951]
Epoch [37/120    avg_loss:0.044, val_acc:0.969]
Epoch [38/120    avg_loss:0.039, val_acc:0.900]
Epoch [39/120    avg_loss:0.036, val_acc:0.961]
Epoch [40/120    avg_loss:0.069, val_acc:0.973]
Epoch [41/120    avg_loss:0.044, val_acc:0.957]
Epoch [42/120    avg_loss:0.062, val_acc:0.964]
Epoch [43/120    avg_loss:0.084, val_acc:0.959]
Epoch [44/120    avg_loss:0.046, val_acc:0.950]
Epoch [45/120    avg_loss:0.042, val_acc:0.961]
Epoch [46/120    avg_loss:0.053, val_acc:0.964]
Epoch [47/120    avg_loss:0.049, val_acc:0.965]
Epoch [48/120    avg_loss:0.045, val_acc:0.885]
Epoch [49/120    avg_loss:0.051, val_acc:0.965]
Epoch [50/120    avg_loss:0.036, val_acc:0.949]
Epoch [51/120    avg_loss:0.043, val_acc:0.961]
Epoch [52/120    avg_loss:0.035, val_acc:0.978]
Epoch [53/120    avg_loss:0.025, val_acc:0.979]
Epoch [54/120    avg_loss:0.029, val_acc:0.975]
Epoch [55/120    avg_loss:0.039, val_acc:0.971]
Epoch [56/120    avg_loss:0.052, val_acc:0.965]
Epoch [57/120    avg_loss:0.020, val_acc:0.970]
Epoch [58/120    avg_loss:0.019, val_acc:0.974]
Epoch [59/120    avg_loss:0.026, val_acc:0.940]
Epoch [60/120    avg_loss:0.024, val_acc:0.981]
Epoch [61/120    avg_loss:0.022, val_acc:0.979]
Epoch [62/120    avg_loss:0.018, val_acc:0.968]
Epoch [63/120    avg_loss:0.046, val_acc:0.933]
Epoch [64/120    avg_loss:0.030, val_acc:0.969]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.025, val_acc:0.987]
Epoch [67/120    avg_loss:0.042, val_acc:0.963]
Epoch [68/120    avg_loss:0.012, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.977]
Epoch [70/120    avg_loss:0.020, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.024, val_acc:0.976]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.014, val_acc:0.982]
Epoch [75/120    avg_loss:0.012, val_acc:0.981]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.016, val_acc:0.978]
Epoch [78/120    avg_loss:0.036, val_acc:0.970]
Epoch [79/120    avg_loss:0.008, val_acc:0.977]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.007, val_acc:0.980]
Epoch [82/120    avg_loss:0.012, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.007, val_acc:0.982]
Epoch [88/120    avg_loss:0.005, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     2     0     0     0     0    60     0]
 [    0     0 18020     0    36     0    32     0     2     0]
 [    0     5     0  1980     0     0     0     0    47     4]
 [    0    12     0     0  2931     0    10     1    15     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     1     0  4868     0     3     0]
 [    0     5     0     0     0     0     2  1281     0     2]
 [    0    14     0    73    52     0     2     0  3415    15]
 [    0     0     0     0    14     9     0     0     0   896]]

Accuracy:
98.97091075603115

F1 scores:
[       nan 0.99236641 0.99789567 0.96797849 0.97602398 0.99656357
 0.99428105 0.99611198 0.96021369 0.97444263]

Kappa:
0.9863747223868234
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea00703908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.766, val_acc:0.654]
Epoch [2/120    avg_loss:1.009, val_acc:0.720]
Epoch [3/120    avg_loss:0.680, val_acc:0.648]
Epoch [4/120    avg_loss:0.530, val_acc:0.597]
Epoch [5/120    avg_loss:0.433, val_acc:0.641]
Epoch [6/120    avg_loss:0.392, val_acc:0.697]
Epoch [7/120    avg_loss:0.336, val_acc:0.852]
Epoch [8/120    avg_loss:0.272, val_acc:0.785]
Epoch [9/120    avg_loss:0.256, val_acc:0.852]
Epoch [10/120    avg_loss:0.270, val_acc:0.878]
Epoch [11/120    avg_loss:0.246, val_acc:0.801]
Epoch [12/120    avg_loss:0.216, val_acc:0.905]
Epoch [13/120    avg_loss:0.175, val_acc:0.877]
Epoch [14/120    avg_loss:0.177, val_acc:0.928]
Epoch [15/120    avg_loss:0.159, val_acc:0.870]
Epoch [16/120    avg_loss:0.131, val_acc:0.908]
Epoch [17/120    avg_loss:0.105, val_acc:0.951]
Epoch [18/120    avg_loss:0.100, val_acc:0.929]
Epoch [19/120    avg_loss:0.210, val_acc:0.825]
Epoch [20/120    avg_loss:0.126, val_acc:0.865]
Epoch [21/120    avg_loss:0.351, val_acc:0.467]
Epoch [22/120    avg_loss:0.857, val_acc:0.744]
Epoch [23/120    avg_loss:0.396, val_acc:0.831]
Epoch [24/120    avg_loss:0.314, val_acc:0.901]
Epoch [25/120    avg_loss:0.318, val_acc:0.784]
Epoch [26/120    avg_loss:0.245, val_acc:0.884]
Epoch [27/120    avg_loss:0.209, val_acc:0.942]
Epoch [28/120    avg_loss:0.158, val_acc:0.935]
Epoch [29/120    avg_loss:0.137, val_acc:0.964]
Epoch [30/120    avg_loss:0.112, val_acc:0.961]
Epoch [31/120    avg_loss:0.095, val_acc:0.940]
Epoch [32/120    avg_loss:0.108, val_acc:0.977]
Epoch [33/120    avg_loss:0.098, val_acc:0.940]
Epoch [34/120    avg_loss:0.083, val_acc:0.933]
Epoch [35/120    avg_loss:0.067, val_acc:0.957]
Epoch [36/120    avg_loss:0.729, val_acc:0.630]
Epoch [37/120    avg_loss:0.833, val_acc:0.722]
Epoch [38/120    avg_loss:0.715, val_acc:0.687]
Epoch [39/120    avg_loss:0.624, val_acc:0.757]
Epoch [40/120    avg_loss:0.617, val_acc:0.742]
Epoch [41/120    avg_loss:0.585, val_acc:0.689]
Epoch [42/120    avg_loss:0.523, val_acc:0.738]
Epoch [43/120    avg_loss:0.530, val_acc:0.731]
Epoch [44/120    avg_loss:0.516, val_acc:0.653]
Epoch [45/120    avg_loss:0.456, val_acc:0.758]
Epoch [46/120    avg_loss:0.466, val_acc:0.778]
Epoch [47/120    avg_loss:0.406, val_acc:0.789]
Epoch [48/120    avg_loss:0.397, val_acc:0.794]
Epoch [49/120    avg_loss:0.398, val_acc:0.776]
Epoch [50/120    avg_loss:0.374, val_acc:0.789]
Epoch [51/120    avg_loss:0.394, val_acc:0.784]
Epoch [52/120    avg_loss:0.391, val_acc:0.779]
Epoch [53/120    avg_loss:0.377, val_acc:0.789]
Epoch [54/120    avg_loss:0.397, val_acc:0.796]
Epoch [55/120    avg_loss:0.380, val_acc:0.794]
Epoch [56/120    avg_loss:0.392, val_acc:0.784]
Epoch [57/120    avg_loss:0.392, val_acc:0.788]
Epoch [58/120    avg_loss:0.375, val_acc:0.800]
Epoch [59/120    avg_loss:0.364, val_acc:0.799]
Epoch [60/120    avg_loss:0.367, val_acc:0.796]
Epoch [61/120    avg_loss:0.359, val_acc:0.798]
Epoch [62/120    avg_loss:0.354, val_acc:0.799]
Epoch [63/120    avg_loss:0.387, val_acc:0.798]
Epoch [64/120    avg_loss:0.387, val_acc:0.801]
Epoch [65/120    avg_loss:0.356, val_acc:0.799]
Epoch [66/120    avg_loss:0.348, val_acc:0.799]
Epoch [67/120    avg_loss:0.356, val_acc:0.797]
Epoch [68/120    avg_loss:0.347, val_acc:0.797]
Epoch [69/120    avg_loss:0.370, val_acc:0.799]
Epoch [70/120    avg_loss:0.355, val_acc:0.798]
Epoch [71/120    avg_loss:0.343, val_acc:0.797]
Epoch [72/120    avg_loss:0.377, val_acc:0.796]
Epoch [73/120    avg_loss:0.350, val_acc:0.796]
Epoch [74/120    avg_loss:0.367, val_acc:0.797]
Epoch [75/120    avg_loss:0.371, val_acc:0.797]
Epoch [76/120    avg_loss:0.358, val_acc:0.798]
Epoch [77/120    avg_loss:0.350, val_acc:0.798]
Epoch [78/120    avg_loss:0.354, val_acc:0.798]
Epoch [79/120    avg_loss:0.359, val_acc:0.798]
Epoch [80/120    avg_loss:0.357, val_acc:0.798]
Epoch [81/120    avg_loss:0.369, val_acc:0.797]
Epoch [82/120    avg_loss:0.364, val_acc:0.797]
Epoch [83/120    avg_loss:0.363, val_acc:0.797]
Epoch [84/120    avg_loss:0.347, val_acc:0.797]
Epoch [85/120    avg_loss:0.336, val_acc:0.797]
Epoch [86/120    avg_loss:0.344, val_acc:0.797]
Epoch [87/120    avg_loss:0.356, val_acc:0.797]
Epoch [88/120    avg_loss:0.352, val_acc:0.797]
Epoch [89/120    avg_loss:0.377, val_acc:0.797]
Epoch [90/120    avg_loss:0.366, val_acc:0.797]
Epoch [91/120    avg_loss:0.365, val_acc:0.797]
Epoch [92/120    avg_loss:0.367, val_acc:0.797]
Epoch [93/120    avg_loss:0.356, val_acc:0.797]
Epoch [94/120    avg_loss:0.356, val_acc:0.797]
Epoch [95/120    avg_loss:0.370, val_acc:0.797]
Epoch [96/120    avg_loss:0.361, val_acc:0.797]
Epoch [97/120    avg_loss:0.355, val_acc:0.797]
Epoch [98/120    avg_loss:0.383, val_acc:0.797]
Epoch [99/120    avg_loss:0.358, val_acc:0.797]
Epoch [100/120    avg_loss:0.361, val_acc:0.797]
Epoch [101/120    avg_loss:0.359, val_acc:0.797]
Epoch [102/120    avg_loss:0.363, val_acc:0.797]
Epoch [103/120    avg_loss:0.382, val_acc:0.797]
Epoch [104/120    avg_loss:0.347, val_acc:0.797]
Epoch [105/120    avg_loss:0.349, val_acc:0.797]
Epoch [106/120    avg_loss:0.355, val_acc:0.797]
Epoch [107/120    avg_loss:0.348, val_acc:0.797]
Epoch [108/120    avg_loss:0.350, val_acc:0.797]
Epoch [109/120    avg_loss:0.348, val_acc:0.797]
Epoch [110/120    avg_loss:0.369, val_acc:0.797]
Epoch [111/120    avg_loss:0.342, val_acc:0.797]
Epoch [112/120    avg_loss:0.340, val_acc:0.797]
Epoch [113/120    avg_loss:0.386, val_acc:0.797]
Epoch [114/120    avg_loss:0.378, val_acc:0.797]
Epoch [115/120    avg_loss:0.351, val_acc:0.797]
Epoch [116/120    avg_loss:0.357, val_acc:0.797]
Epoch [117/120    avg_loss:0.352, val_acc:0.797]
Epoch [118/120    avg_loss:0.368, val_acc:0.797]
Epoch [119/120    avg_loss:0.362, val_acc:0.797]
Epoch [120/120    avg_loss:0.366, val_acc:0.797]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5212   175    79    75     0   280    69   415   127]
 [    0     9 13988     0   448     0  3645     0     0     0]
 [    0     4     0  1818     1     0     1     0   192    20]
 [    0    23   127     0  2742     0    49     0    20    11]
 [    0     0     0     0     0  1304     0     1     0     0]
 [    0     0   702     0     1     0  4106     0    69     0]
 [    0    59     0     0     0     0     5  1206    20     0]
 [    0   170    37    77    78     0    76    18  3114     1]
 [    0    14     0     0    14    31     0     0     0   860]]

Accuracy:
82.78504808039911

F1 scores:
[       nan 0.87427661 0.84471149 0.90673317 0.86621387 0.98787879
 0.6297546  0.93343653 0.8415079  0.8875129 ]

Kappa:
0.7792516632214482
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f879c834908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.750, val_acc:0.703]
Epoch [2/120    avg_loss:1.022, val_acc:0.625]
Epoch [3/120    avg_loss:0.690, val_acc:0.689]
Epoch [4/120    avg_loss:0.526, val_acc:0.792]
Epoch [5/120    avg_loss:0.393, val_acc:0.826]
Epoch [6/120    avg_loss:0.338, val_acc:0.863]
Epoch [7/120    avg_loss:0.288, val_acc:0.889]
Epoch [8/120    avg_loss:0.324, val_acc:0.807]
Epoch [9/120    avg_loss:0.289, val_acc:0.878]
Epoch [10/120    avg_loss:0.233, val_acc:0.877]
Epoch [11/120    avg_loss:0.263, val_acc:0.887]
Epoch [12/120    avg_loss:0.231, val_acc:0.867]
Epoch [13/120    avg_loss:0.218, val_acc:0.886]
Epoch [14/120    avg_loss:0.194, val_acc:0.892]
Epoch [15/120    avg_loss:0.147, val_acc:0.898]
Epoch [16/120    avg_loss:0.165, val_acc:0.921]
Epoch [17/120    avg_loss:0.160, val_acc:0.894]
Epoch [18/120    avg_loss:0.167, val_acc:0.921]
Epoch [19/120    avg_loss:0.148, val_acc:0.938]
Epoch [20/120    avg_loss:0.169, val_acc:0.911]
Epoch [21/120    avg_loss:0.100, val_acc:0.945]
Epoch [22/120    avg_loss:0.099, val_acc:0.899]
Epoch [23/120    avg_loss:0.119, val_acc:0.916]
Epoch [24/120    avg_loss:0.106, val_acc:0.930]
Epoch [25/120    avg_loss:0.088, val_acc:0.938]
Epoch [26/120    avg_loss:0.093, val_acc:0.960]
Epoch [27/120    avg_loss:0.082, val_acc:0.943]
Epoch [28/120    avg_loss:0.082, val_acc:0.952]
Epoch [29/120    avg_loss:0.130, val_acc:0.887]
Epoch [30/120    avg_loss:0.209, val_acc:0.945]
Epoch [31/120    avg_loss:0.077, val_acc:0.953]
Epoch [32/120    avg_loss:0.053, val_acc:0.968]
Epoch [33/120    avg_loss:0.040, val_acc:0.963]
Epoch [34/120    avg_loss:0.053, val_acc:0.974]
Epoch [35/120    avg_loss:0.035, val_acc:0.968]
Epoch [36/120    avg_loss:0.038, val_acc:0.980]
Epoch [37/120    avg_loss:0.031, val_acc:0.969]
Epoch [38/120    avg_loss:0.027, val_acc:0.980]
Epoch [39/120    avg_loss:0.023, val_acc:0.980]
Epoch [40/120    avg_loss:0.043, val_acc:0.939]
Epoch [41/120    avg_loss:0.225, val_acc:0.942]
Epoch [42/120    avg_loss:0.063, val_acc:0.968]
Epoch [43/120    avg_loss:0.049, val_acc:0.968]
Epoch [44/120    avg_loss:0.039, val_acc:0.970]
Epoch [45/120    avg_loss:0.040, val_acc:0.957]
Epoch [46/120    avg_loss:0.037, val_acc:0.977]
Epoch [47/120    avg_loss:0.027, val_acc:0.980]
Epoch [48/120    avg_loss:0.021, val_acc:0.978]
Epoch [49/120    avg_loss:0.022, val_acc:0.983]
Epoch [50/120    avg_loss:0.018, val_acc:0.983]
Epoch [51/120    avg_loss:0.016, val_acc:0.982]
Epoch [52/120    avg_loss:0.013, val_acc:0.985]
Epoch [53/120    avg_loss:0.014, val_acc:0.983]
Epoch [54/120    avg_loss:0.038, val_acc:0.948]
Epoch [55/120    avg_loss:0.042, val_acc:0.983]
Epoch [56/120    avg_loss:0.030, val_acc:0.983]
Epoch [57/120    avg_loss:0.021, val_acc:0.985]
Epoch [58/120    avg_loss:0.011, val_acc:0.992]
Epoch [59/120    avg_loss:0.029, val_acc:0.978]
Epoch [60/120    avg_loss:0.016, val_acc:0.980]
Epoch [61/120    avg_loss:0.010, val_acc:0.985]
Epoch [62/120    avg_loss:0.024, val_acc:0.965]
Epoch [63/120    avg_loss:0.016, val_acc:0.984]
Epoch [64/120    avg_loss:0.010, val_acc:0.984]
Epoch [65/120    avg_loss:0.008, val_acc:0.984]
Epoch [66/120    avg_loss:0.012, val_acc:0.990]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.010, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.019, val_acc:0.990]
Epoch [71/120    avg_loss:0.006, val_acc:0.988]
Epoch [72/120    avg_loss:0.006, val_acc:0.989]
Epoch [73/120    avg_loss:0.004, val_acc:0.989]
Epoch [74/120    avg_loss:0.004, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.004, val_acc:0.990]
Epoch [78/120    avg_loss:0.004, val_acc:0.990]
Epoch [79/120    avg_loss:0.004, val_acc:0.990]
Epoch [80/120    avg_loss:0.003, val_acc:0.991]
Epoch [81/120    avg_loss:0.004, val_acc:0.989]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.005, val_acc:0.989]
Epoch [86/120    avg_loss:0.004, val_acc:0.989]
Epoch [87/120    avg_loss:0.003, val_acc:0.989]
Epoch [88/120    avg_loss:0.004, val_acc:0.989]
Epoch [89/120    avg_loss:0.003, val_acc:0.989]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.989]
Epoch [95/120    avg_loss:0.003, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.003, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     0     0     0     6     0    68     1]
 [    0     0 18064     0    25     0     1     0     0     0]
 [    0    10     0  1947     0     0     0     0    76     3]
 [    0    13     9     0  2932     0     7     0     7     4]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0    23     0     0     0  4838     0    17     0]
 [    0    17     0     0     0     0     2  1271     0     0]
 [    0    20     0    31    33     0     1     0  3486     0]
 [    0     0     0     0    14    21     0     0     0   884]]

Accuracy:
99.01188152218447

F1 scores:
[       nan 0.98949335 0.99839717 0.97010463 0.98125837 0.99163498
 0.99414364 0.99258102 0.9649827  0.97571744]

Kappa:
0.9869053117946632
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb6fd0c58d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.751, val_acc:0.652]
Epoch [2/120    avg_loss:1.065, val_acc:0.703]
Epoch [3/120    avg_loss:0.699, val_acc:0.752]
Epoch [4/120    avg_loss:0.578, val_acc:0.817]
Epoch [5/120    avg_loss:0.491, val_acc:0.843]
Epoch [6/120    avg_loss:0.417, val_acc:0.843]
Epoch [7/120    avg_loss:0.340, val_acc:0.805]
Epoch [8/120    avg_loss:0.283, val_acc:0.805]
Epoch [9/120    avg_loss:0.328, val_acc:0.807]
Epoch [10/120    avg_loss:0.347, val_acc:0.888]
Epoch [11/120    avg_loss:0.264, val_acc:0.687]
Epoch [12/120    avg_loss:0.295, val_acc:0.913]
Epoch [13/120    avg_loss:0.230, val_acc:0.839]
Epoch [14/120    avg_loss:0.240, val_acc:0.821]
Epoch [15/120    avg_loss:0.219, val_acc:0.879]
Epoch [16/120    avg_loss:0.153, val_acc:0.933]
Epoch [17/120    avg_loss:0.173, val_acc:0.905]
Epoch [18/120    avg_loss:0.144, val_acc:0.876]
Epoch [19/120    avg_loss:0.163, val_acc:0.924]
Epoch [20/120    avg_loss:0.168, val_acc:0.912]
Epoch [21/120    avg_loss:0.140, val_acc:0.902]
Epoch [22/120    avg_loss:0.121, val_acc:0.843]
Epoch [23/120    avg_loss:0.122, val_acc:0.956]
Epoch [24/120    avg_loss:0.067, val_acc:0.966]
Epoch [25/120    avg_loss:0.099, val_acc:0.873]
Epoch [26/120    avg_loss:0.088, val_acc:0.945]
Epoch [27/120    avg_loss:0.143, val_acc:0.917]
Epoch [28/120    avg_loss:0.098, val_acc:0.938]
Epoch [29/120    avg_loss:0.110, val_acc:0.932]
Epoch [30/120    avg_loss:0.070, val_acc:0.938]
Epoch [31/120    avg_loss:0.065, val_acc:0.943]
Epoch [32/120    avg_loss:0.058, val_acc:0.885]
Epoch [33/120    avg_loss:0.060, val_acc:0.955]
Epoch [34/120    avg_loss:0.044, val_acc:0.937]
Epoch [35/120    avg_loss:0.035, val_acc:0.972]
Epoch [36/120    avg_loss:0.026, val_acc:0.970]
Epoch [37/120    avg_loss:0.044, val_acc:0.955]
Epoch [38/120    avg_loss:0.038, val_acc:0.923]
Epoch [39/120    avg_loss:0.084, val_acc:0.948]
Epoch [40/120    avg_loss:0.030, val_acc:0.953]
Epoch [41/120    avg_loss:0.049, val_acc:0.959]
Epoch [42/120    avg_loss:0.027, val_acc:0.972]
Epoch [43/120    avg_loss:0.043, val_acc:0.975]
Epoch [44/120    avg_loss:0.072, val_acc:0.956]
Epoch [45/120    avg_loss:0.093, val_acc:0.930]
Epoch [46/120    avg_loss:0.060, val_acc:0.948]
Epoch [47/120    avg_loss:0.038, val_acc:0.971]
Epoch [48/120    avg_loss:0.032, val_acc:0.964]
Epoch [49/120    avg_loss:0.016, val_acc:0.974]
Epoch [50/120    avg_loss:0.030, val_acc:0.969]
Epoch [51/120    avg_loss:0.026, val_acc:0.953]
Epoch [52/120    avg_loss:0.024, val_acc:0.973]
Epoch [53/120    avg_loss:0.035, val_acc:0.969]
Epoch [54/120    avg_loss:0.026, val_acc:0.973]
Epoch [55/120    avg_loss:0.021, val_acc:0.972]
Epoch [56/120    avg_loss:0.011, val_acc:0.973]
Epoch [57/120    avg_loss:0.011, val_acc:0.980]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.006, val_acc:0.979]
Epoch [60/120    avg_loss:0.008, val_acc:0.979]
Epoch [61/120    avg_loss:0.013, val_acc:0.981]
Epoch [62/120    avg_loss:0.008, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.982]
Epoch [64/120    avg_loss:0.008, val_acc:0.983]
Epoch [65/120    avg_loss:0.006, val_acc:0.983]
Epoch [66/120    avg_loss:0.008, val_acc:0.983]
Epoch [67/120    avg_loss:0.007, val_acc:0.983]
Epoch [68/120    avg_loss:0.007, val_acc:0.981]
Epoch [69/120    avg_loss:0.007, val_acc:0.983]
Epoch [70/120    avg_loss:0.006, val_acc:0.983]
Epoch [71/120    avg_loss:0.006, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.979]
Epoch [73/120    avg_loss:0.006, val_acc:0.982]
Epoch [74/120    avg_loss:0.005, val_acc:0.983]
Epoch [75/120    avg_loss:0.006, val_acc:0.981]
Epoch [76/120    avg_loss:0.008, val_acc:0.982]
Epoch [77/120    avg_loss:0.007, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.981]
Epoch [85/120    avg_loss:0.007, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.005, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.007, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.982]
Epoch [105/120    avg_loss:0.005, val_acc:0.982]
Epoch [106/120    avg_loss:0.004, val_acc:0.982]
Epoch [107/120    avg_loss:0.005, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.982]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     0     1     0     2     0    74     0]
 [    0     0 18024     0    45     0    21     0     0     0]
 [    0     4     0  1922     0     0     0     0   105     5]
 [    0    10     5     0  2947     0     1     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    34     0     0     0  4833     0    11     0]
 [    0     6     0     0     0     0     0  1282     0     2]
 [    0    37     0    49    36     0     0     0  3447     2]
 [    0     0     0     0    14    21     0     0     0   884]]

Accuracy:
98.80943773648568

F1 scores:
[       nan 0.98956711 0.99709568 0.95932119 0.97988362 0.99201824
 0.99291217 0.99688958 0.95564181 0.97410468]

Kappa:
0.984229037652579
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff3565459e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.767, val_acc:0.592]
Epoch [2/120    avg_loss:1.095, val_acc:0.706]
Epoch [3/120    avg_loss:0.787, val_acc:0.729]
Epoch [4/120    avg_loss:0.579, val_acc:0.718]
Epoch [5/120    avg_loss:0.539, val_acc:0.791]
Epoch [6/120    avg_loss:0.417, val_acc:0.779]
Epoch [7/120    avg_loss:0.342, val_acc:0.828]
Epoch [8/120    avg_loss:0.323, val_acc:0.793]
Epoch [9/120    avg_loss:0.254, val_acc:0.874]
Epoch [10/120    avg_loss:0.265, val_acc:0.870]
Epoch [11/120    avg_loss:0.301, val_acc:0.866]
Epoch [12/120    avg_loss:0.257, val_acc:0.921]
Epoch [13/120    avg_loss:0.225, val_acc:0.891]
Epoch [14/120    avg_loss:0.161, val_acc:0.836]
Epoch [15/120    avg_loss:0.146, val_acc:0.933]
Epoch [16/120    avg_loss:0.168, val_acc:0.938]
Epoch [17/120    avg_loss:0.181, val_acc:0.860]
Epoch [18/120    avg_loss:0.142, val_acc:0.856]
Epoch [19/120    avg_loss:0.115, val_acc:0.945]
Epoch [20/120    avg_loss:0.130, val_acc:0.923]
Epoch [21/120    avg_loss:0.094, val_acc:0.914]
Epoch [22/120    avg_loss:0.131, val_acc:0.943]
Epoch [23/120    avg_loss:0.135, val_acc:0.951]
Epoch [24/120    avg_loss:0.092, val_acc:0.914]
Epoch [25/120    avg_loss:0.074, val_acc:0.967]
Epoch [26/120    avg_loss:0.049, val_acc:0.956]
Epoch [27/120    avg_loss:0.237, val_acc:0.869]
Epoch [28/120    avg_loss:0.124, val_acc:0.952]
Epoch [29/120    avg_loss:0.069, val_acc:0.943]
Epoch [30/120    avg_loss:0.063, val_acc:0.953]
Epoch [31/120    avg_loss:0.069, val_acc:0.965]
Epoch [32/120    avg_loss:0.042, val_acc:0.954]
Epoch [33/120    avg_loss:0.053, val_acc:0.974]
Epoch [34/120    avg_loss:0.167, val_acc:0.939]
Epoch [35/120    avg_loss:0.092, val_acc:0.967]
Epoch [36/120    avg_loss:0.069, val_acc:0.867]
Epoch [37/120    avg_loss:0.071, val_acc:0.971]
Epoch [38/120    avg_loss:0.069, val_acc:0.966]
Epoch [39/120    avg_loss:0.084, val_acc:0.883]
Epoch [40/120    avg_loss:0.066, val_acc:0.966]
Epoch [41/120    avg_loss:0.044, val_acc:0.946]
Epoch [42/120    avg_loss:0.049, val_acc:0.969]
Epoch [43/120    avg_loss:0.041, val_acc:0.975]
Epoch [44/120    avg_loss:0.039, val_acc:0.971]
Epoch [45/120    avg_loss:0.049, val_acc:0.966]
Epoch [46/120    avg_loss:0.059, val_acc:0.970]
Epoch [47/120    avg_loss:0.024, val_acc:0.986]
Epoch [48/120    avg_loss:0.029, val_acc:0.987]
Epoch [49/120    avg_loss:0.126, val_acc:0.945]
Epoch [50/120    avg_loss:0.066, val_acc:0.940]
Epoch [51/120    avg_loss:0.041, val_acc:0.982]
Epoch [52/120    avg_loss:0.025, val_acc:0.975]
Epoch [53/120    avg_loss:0.024, val_acc:0.977]
Epoch [54/120    avg_loss:0.076, val_acc:0.950]
Epoch [55/120    avg_loss:0.048, val_acc:0.965]
Epoch [56/120    avg_loss:0.039, val_acc:0.976]
Epoch [57/120    avg_loss:0.024, val_acc:0.987]
Epoch [58/120    avg_loss:0.026, val_acc:0.990]
Epoch [59/120    avg_loss:0.018, val_acc:0.989]
Epoch [60/120    avg_loss:0.019, val_acc:0.979]
Epoch [61/120    avg_loss:0.017, val_acc:0.987]
Epoch [62/120    avg_loss:0.016, val_acc:0.983]
Epoch [63/120    avg_loss:0.014, val_acc:0.981]
Epoch [64/120    avg_loss:0.021, val_acc:0.987]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.019, val_acc:0.979]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.015, val_acc:0.990]
Epoch [69/120    avg_loss:0.018, val_acc:0.983]
Epoch [70/120    avg_loss:0.021, val_acc:0.990]
Epoch [71/120    avg_loss:0.012, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.990]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.005, val_acc:0.990]
Epoch [78/120    avg_loss:0.006, val_acc:0.983]
Epoch [79/120    avg_loss:0.039, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.012, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.009, val_acc:0.987]
Epoch [87/120    avg_loss:0.017, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.940]
Epoch [89/120    avg_loss:0.020, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.012, val_acc:0.989]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.012, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.993]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.014, val_acc:0.984]
Epoch [100/120    avg_loss:0.024, val_acc:0.967]
Epoch [101/120    avg_loss:0.035, val_acc:0.950]
Epoch [102/120    avg_loss:0.042, val_acc:0.985]
Epoch [103/120    avg_loss:0.018, val_acc:0.985]
Epoch [104/120    avg_loss:0.013, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.002, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6386     0     0     0     0     0     4    42     0]
 [    0     0 18025     0    19     0    39     0     7     0]
 [    0     0     0  1993     0     0     0     0    39     4]
 [    0    27     5     0  2927     0     3     0     8     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     3     0     0     0     0     0  1287     0     0]
 [    0    11     0    36    41     0     3     0  3474     6]
 [    0     0     0     0    12    18     0     0     0   889]]

Accuracy:
99.20227508254405

F1 scores:
[       nan 0.99323431 0.99806202 0.98056581 0.98040529 0.99315068
 0.99520359 0.99728787 0.97270055 0.97692308]

Kappa:
0.9894364110333504
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48780c9898>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.728, val_acc:0.683]
Epoch [2/120    avg_loss:1.071, val_acc:0.458]
Epoch [3/120    avg_loss:0.690, val_acc:0.701]
Epoch [4/120    avg_loss:0.557, val_acc:0.763]
Epoch [5/120    avg_loss:0.497, val_acc:0.772]
Epoch [6/120    avg_loss:0.439, val_acc:0.790]
Epoch [7/120    avg_loss:0.416, val_acc:0.869]
Epoch [8/120    avg_loss:0.336, val_acc:0.829]
Epoch [9/120    avg_loss:0.290, val_acc:0.763]
Epoch [10/120    avg_loss:0.279, val_acc:0.857]
Epoch [11/120    avg_loss:0.237, val_acc:0.833]
Epoch [12/120    avg_loss:0.212, val_acc:0.821]
Epoch [13/120    avg_loss:0.203, val_acc:0.903]
Epoch [14/120    avg_loss:0.176, val_acc:0.902]
Epoch [15/120    avg_loss:0.170, val_acc:0.929]
Epoch [16/120    avg_loss:0.152, val_acc:0.882]
Epoch [17/120    avg_loss:0.162, val_acc:0.931]
Epoch [18/120    avg_loss:0.146, val_acc:0.944]
Epoch [19/120    avg_loss:0.131, val_acc:0.894]
Epoch [20/120    avg_loss:0.189, val_acc:0.906]
Epoch [21/120    avg_loss:0.127, val_acc:0.931]
Epoch [22/120    avg_loss:0.121, val_acc:0.925]
Epoch [23/120    avg_loss:0.124, val_acc:0.871]
Epoch [24/120    avg_loss:0.109, val_acc:0.952]
Epoch [25/120    avg_loss:0.150, val_acc:0.907]
Epoch [26/120    avg_loss:0.135, val_acc:0.921]
Epoch [27/120    avg_loss:0.118, val_acc:0.950]
Epoch [28/120    avg_loss:0.098, val_acc:0.945]
Epoch [29/120    avg_loss:0.061, val_acc:0.960]
Epoch [30/120    avg_loss:0.055, val_acc:0.961]
Epoch [31/120    avg_loss:0.095, val_acc:0.959]
Epoch [32/120    avg_loss:0.066, val_acc:0.952]
Epoch [33/120    avg_loss:0.067, val_acc:0.959]
Epoch [34/120    avg_loss:0.076, val_acc:0.950]
Epoch [35/120    avg_loss:0.055, val_acc:0.947]
Epoch [36/120    avg_loss:0.071, val_acc:0.935]
Epoch [37/120    avg_loss:0.074, val_acc:0.936]
Epoch [38/120    avg_loss:0.048, val_acc:0.951]
Epoch [39/120    avg_loss:0.053, val_acc:0.963]
Epoch [40/120    avg_loss:0.033, val_acc:0.969]
Epoch [41/120    avg_loss:0.035, val_acc:0.971]
Epoch [42/120    avg_loss:0.042, val_acc:0.966]
Epoch [43/120    avg_loss:0.029, val_acc:0.972]
Epoch [44/120    avg_loss:0.045, val_acc:0.956]
Epoch [45/120    avg_loss:0.031, val_acc:0.974]
Epoch [46/120    avg_loss:0.025, val_acc:0.974]
Epoch [47/120    avg_loss:0.023, val_acc:0.974]
Epoch [48/120    avg_loss:0.035, val_acc:0.975]
Epoch [49/120    avg_loss:0.059, val_acc:0.943]
Epoch [50/120    avg_loss:0.064, val_acc:0.973]
Epoch [51/120    avg_loss:0.030, val_acc:0.960]
Epoch [52/120    avg_loss:0.024, val_acc:0.982]
Epoch [53/120    avg_loss:0.023, val_acc:0.984]
Epoch [54/120    avg_loss:0.029, val_acc:0.974]
Epoch [55/120    avg_loss:0.030, val_acc:0.980]
Epoch [56/120    avg_loss:0.028, val_acc:0.972]
Epoch [57/120    avg_loss:0.019, val_acc:0.949]
Epoch [58/120    avg_loss:0.014, val_acc:0.983]
Epoch [59/120    avg_loss:0.060, val_acc:0.960]
Epoch [60/120    avg_loss:0.066, val_acc:0.916]
Epoch [61/120    avg_loss:0.051, val_acc:0.961]
Epoch [62/120    avg_loss:0.027, val_acc:0.975]
Epoch [63/120    avg_loss:0.066, val_acc:0.974]
Epoch [64/120    avg_loss:0.020, val_acc:0.978]
Epoch [65/120    avg_loss:0.018, val_acc:0.977]
Epoch [66/120    avg_loss:0.019, val_acc:0.977]
Epoch [67/120    avg_loss:0.017, val_acc:0.978]
Epoch [68/120    avg_loss:0.013, val_acc:0.979]
Epoch [69/120    avg_loss:0.010, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.985]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.010, val_acc:0.983]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.982]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     3     0     0     6     1    62     4]
 [    0     2 18001     0    32     0    53     0     2     0]
 [    0     3     0  1919     0     0     0     0   109     5]
 [    0    36    11     0  2903     0     5     0    16     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     1     0     0  4868     0     2     0]
 [    0    16     0     0     0     0     0  1267     0     7]
 [    0    15     0    40    48     0     0     0  3467     1]
 [    0     2     0     0    14    17     0     0     0   886]]

Accuracy:
98.74436651965392

F1 scores:
[       nan 0.98833774 0.99703675 0.95973993 0.97269224 0.99352874
 0.99245668 0.99061767 0.95919214 0.97202414]

Kappa:
0.9833725592106766
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f00c744f978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.767, val_acc:0.498]
Epoch [2/120    avg_loss:1.054, val_acc:0.679]
Epoch [3/120    avg_loss:0.750, val_acc:0.620]
Epoch [4/120    avg_loss:0.577, val_acc:0.682]
Epoch [5/120    avg_loss:0.477, val_acc:0.795]
Epoch [6/120    avg_loss:0.381, val_acc:0.713]
Epoch [7/120    avg_loss:0.349, val_acc:0.788]
Epoch [8/120    avg_loss:0.311, val_acc:0.807]
Epoch [9/120    avg_loss:0.331, val_acc:0.806]
Epoch [10/120    avg_loss:0.336, val_acc:0.778]
Epoch [11/120    avg_loss:0.300, val_acc:0.888]
Epoch [12/120    avg_loss:0.296, val_acc:0.802]
Epoch [13/120    avg_loss:0.227, val_acc:0.907]
Epoch [14/120    avg_loss:0.196, val_acc:0.917]
Epoch [15/120    avg_loss:0.163, val_acc:0.921]
Epoch [16/120    avg_loss:0.172, val_acc:0.915]
Epoch [17/120    avg_loss:0.130, val_acc:0.891]
Epoch [18/120    avg_loss:0.149, val_acc:0.928]
Epoch [19/120    avg_loss:0.156, val_acc:0.918]
Epoch [20/120    avg_loss:0.150, val_acc:0.897]
Epoch [21/120    avg_loss:0.119, val_acc:0.953]
Epoch [22/120    avg_loss:0.155, val_acc:0.899]
Epoch [23/120    avg_loss:0.187, val_acc:0.943]
Epoch [24/120    avg_loss:0.110, val_acc:0.907]
Epoch [25/120    avg_loss:0.116, val_acc:0.898]
Epoch [26/120    avg_loss:0.078, val_acc:0.955]
Epoch [27/120    avg_loss:0.117, val_acc:0.911]
Epoch [28/120    avg_loss:0.186, val_acc:0.950]
Epoch [29/120    avg_loss:0.106, val_acc:0.935]
Epoch [30/120    avg_loss:0.105, val_acc:0.961]
Epoch [31/120    avg_loss:0.083, val_acc:0.911]
Epoch [32/120    avg_loss:0.081, val_acc:0.958]
Epoch [33/120    avg_loss:0.081, val_acc:0.949]
Epoch [34/120    avg_loss:0.077, val_acc:0.965]
Epoch [35/120    avg_loss:0.048, val_acc:0.972]
Epoch [36/120    avg_loss:0.053, val_acc:0.981]
Epoch [37/120    avg_loss:0.056, val_acc:0.968]
Epoch [38/120    avg_loss:0.071, val_acc:0.886]
Epoch [39/120    avg_loss:0.064, val_acc:0.937]
Epoch [40/120    avg_loss:0.051, val_acc:0.977]
Epoch [41/120    avg_loss:0.031, val_acc:0.978]
Epoch [42/120    avg_loss:0.047, val_acc:0.963]
Epoch [43/120    avg_loss:0.044, val_acc:0.967]
Epoch [44/120    avg_loss:0.062, val_acc:0.963]
Epoch [45/120    avg_loss:0.085, val_acc:0.968]
Epoch [46/120    avg_loss:0.043, val_acc:0.975]
Epoch [47/120    avg_loss:0.054, val_acc:0.973]
Epoch [48/120    avg_loss:0.030, val_acc:0.975]
Epoch [49/120    avg_loss:0.032, val_acc:0.981]
Epoch [50/120    avg_loss:0.040, val_acc:0.979]
Epoch [51/120    avg_loss:0.047, val_acc:0.975]
Epoch [52/120    avg_loss:0.025, val_acc:0.983]
Epoch [53/120    avg_loss:0.032, val_acc:0.977]
Epoch [54/120    avg_loss:0.036, val_acc:0.983]
Epoch [55/120    avg_loss:0.021, val_acc:0.984]
Epoch [56/120    avg_loss:0.020, val_acc:0.981]
Epoch [57/120    avg_loss:0.028, val_acc:0.979]
Epoch [58/120    avg_loss:0.022, val_acc:0.958]
Epoch [59/120    avg_loss:0.029, val_acc:0.976]
Epoch [60/120    avg_loss:0.021, val_acc:0.968]
Epoch [61/120    avg_loss:0.018, val_acc:0.984]
Epoch [62/120    avg_loss:0.084, val_acc:0.943]
Epoch [63/120    avg_loss:0.047, val_acc:0.968]
Epoch [64/120    avg_loss:0.033, val_acc:0.988]
Epoch [65/120    avg_loss:0.030, val_acc:0.973]
Epoch [66/120    avg_loss:0.018, val_acc:0.981]
Epoch [67/120    avg_loss:0.036, val_acc:0.970]
Epoch [68/120    avg_loss:0.028, val_acc:0.979]
Epoch [69/120    avg_loss:0.013, val_acc:0.982]
Epoch [70/120    avg_loss:0.018, val_acc:0.986]
Epoch [71/120    avg_loss:0.021, val_acc:0.967]
Epoch [72/120    avg_loss:0.138, val_acc:0.936]
Epoch [73/120    avg_loss:0.058, val_acc:0.973]
Epoch [74/120    avg_loss:0.049, val_acc:0.927]
Epoch [75/120    avg_loss:0.030, val_acc:0.989]
Epoch [76/120    avg_loss:0.032, val_acc:0.978]
Epoch [77/120    avg_loss:0.023, val_acc:0.983]
Epoch [78/120    avg_loss:0.022, val_acc:0.981]
Epoch [79/120    avg_loss:0.019, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.968]
Epoch [81/120    avg_loss:0.012, val_acc:0.981]
Epoch [82/120    avg_loss:0.025, val_acc:0.990]
Epoch [83/120    avg_loss:0.020, val_acc:0.973]
Epoch [84/120    avg_loss:0.037, val_acc:0.977]
Epoch [85/120    avg_loss:0.090, val_acc:0.948]
Epoch [86/120    avg_loss:0.035, val_acc:0.983]
Epoch [87/120    avg_loss:0.019, val_acc:0.982]
Epoch [88/120    avg_loss:0.013, val_acc:0.992]
Epoch [89/120    avg_loss:0.048, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.984]
Epoch [91/120    avg_loss:0.019, val_acc:0.988]
Epoch [92/120    avg_loss:0.018, val_acc:0.983]
Epoch [93/120    avg_loss:0.012, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.989]
Epoch [96/120    avg_loss:0.010, val_acc:0.990]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.006, val_acc:0.993]
Epoch [99/120    avg_loss:0.013, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.979]
Epoch [101/120    avg_loss:0.008, val_acc:0.968]
Epoch [102/120    avg_loss:0.041, val_acc:0.932]
Epoch [103/120    avg_loss:0.056, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.015, val_acc:0.968]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.021, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.989]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.017, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.977]
Epoch [112/120    avg_loss:0.014, val_acc:0.991]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.008, val_acc:0.993]
Epoch [115/120    avg_loss:0.007, val_acc:0.993]
Epoch [116/120    avg_loss:0.005, val_acc:0.993]
Epoch [117/120    avg_loss:0.009, val_acc:0.993]
Epoch [118/120    avg_loss:0.011, val_acc:0.993]
Epoch [119/120    avg_loss:0.006, val_acc:0.993]
Epoch [120/120    avg_loss:0.007, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     2     0     0     1     1    46     0]
 [    0     1 18002     0    11     0    68     0     7     1]
 [    0    12     0  1976     0     0     0     0    48     0]
 [    0    20     4     0  2920     0    22     0     3     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     1     0]
 [    0     2     0     0     0     0     0  1287     0     1]
 [    0     5     0    12    35     0     0     0  3516     3]
 [    0     0     0     0     1    17     0     0     0   901]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99299829 0.99745124 0.98161947 0.98333053 0.99352874
 0.9906561  0.99844841 0.97775306 0.98577681]

Kappa:
0.9895659488775821
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b5e3fc908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.792, val_acc:0.676]
Epoch [2/120    avg_loss:1.085, val_acc:0.732]
Epoch [3/120    avg_loss:0.724, val_acc:0.728]
Epoch [4/120    avg_loss:0.555, val_acc:0.794]
Epoch [5/120    avg_loss:0.438, val_acc:0.810]
Epoch [6/120    avg_loss:0.396, val_acc:0.836]
Epoch [7/120    avg_loss:0.340, val_acc:0.881]
Epoch [8/120    avg_loss:0.282, val_acc:0.841]
Epoch [9/120    avg_loss:0.250, val_acc:0.846]
Epoch [10/120    avg_loss:0.238, val_acc:0.874]
Epoch [11/120    avg_loss:0.219, val_acc:0.867]
Epoch [12/120    avg_loss:0.261, val_acc:0.918]
Epoch [13/120    avg_loss:0.193, val_acc:0.899]
Epoch [14/120    avg_loss:0.164, val_acc:0.925]
Epoch [15/120    avg_loss:0.181, val_acc:0.887]
Epoch [16/120    avg_loss:0.150, val_acc:0.947]
Epoch [17/120    avg_loss:0.142, val_acc:0.921]
Epoch [18/120    avg_loss:0.100, val_acc:0.953]
Epoch [19/120    avg_loss:0.089, val_acc:0.954]
Epoch [20/120    avg_loss:0.093, val_acc:0.951]
Epoch [21/120    avg_loss:0.071, val_acc:0.953]
Epoch [22/120    avg_loss:0.100, val_acc:0.941]
Epoch [23/120    avg_loss:0.085, val_acc:0.929]
Epoch [24/120    avg_loss:0.086, val_acc:0.951]
Epoch [25/120    avg_loss:0.094, val_acc:0.952]
Epoch [26/120    avg_loss:0.082, val_acc:0.933]
Epoch [27/120    avg_loss:0.068, val_acc:0.966]
Epoch [28/120    avg_loss:0.078, val_acc:0.950]
Epoch [29/120    avg_loss:0.106, val_acc:0.897]
Epoch [30/120    avg_loss:0.055, val_acc:0.963]
Epoch [31/120    avg_loss:0.054, val_acc:0.966]
Epoch [32/120    avg_loss:0.053, val_acc:0.966]
Epoch [33/120    avg_loss:0.060, val_acc:0.964]
Epoch [34/120    avg_loss:0.034, val_acc:0.976]
Epoch [35/120    avg_loss:0.047, val_acc:0.955]
Epoch [36/120    avg_loss:0.056, val_acc:0.975]
Epoch [37/120    avg_loss:0.032, val_acc:0.968]
Epoch [38/120    avg_loss:0.031, val_acc:0.961]
Epoch [39/120    avg_loss:0.032, val_acc:0.972]
Epoch [40/120    avg_loss:0.035, val_acc:0.966]
Epoch [41/120    avg_loss:0.067, val_acc:0.964]
Epoch [42/120    avg_loss:0.020, val_acc:0.979]
Epoch [43/120    avg_loss:0.028, val_acc:0.974]
Epoch [44/120    avg_loss:0.038, val_acc:0.956]
Epoch [45/120    avg_loss:0.070, val_acc:0.935]
Epoch [46/120    avg_loss:0.052, val_acc:0.977]
Epoch [47/120    avg_loss:0.020, val_acc:0.974]
Epoch [48/120    avg_loss:0.030, val_acc:0.959]
Epoch [49/120    avg_loss:0.021, val_acc:0.980]
Epoch [50/120    avg_loss:0.035, val_acc:0.977]
Epoch [51/120    avg_loss:0.012, val_acc:0.980]
Epoch [52/120    avg_loss:0.009, val_acc:0.975]
Epoch [53/120    avg_loss:0.017, val_acc:0.980]
Epoch [54/120    avg_loss:0.012, val_acc:0.977]
Epoch [55/120    avg_loss:0.039, val_acc:0.972]
Epoch [56/120    avg_loss:0.021, val_acc:0.977]
Epoch [57/120    avg_loss:0.016, val_acc:0.977]
Epoch [58/120    avg_loss:0.010, val_acc:0.979]
Epoch [59/120    avg_loss:0.009, val_acc:0.978]
Epoch [60/120    avg_loss:0.008, val_acc:0.984]
Epoch [61/120    avg_loss:0.007, val_acc:0.980]
Epoch [62/120    avg_loss:0.007, val_acc:0.980]
Epoch [63/120    avg_loss:0.009, val_acc:0.978]
Epoch [64/120    avg_loss:0.008, val_acc:0.980]
Epoch [65/120    avg_loss:0.010, val_acc:0.978]
Epoch [66/120    avg_loss:0.007, val_acc:0.982]
Epoch [67/120    avg_loss:0.007, val_acc:0.981]
Epoch [68/120    avg_loss:0.004, val_acc:0.981]
Epoch [69/120    avg_loss:0.005, val_acc:0.983]
Epoch [70/120    avg_loss:0.005, val_acc:0.981]
Epoch [71/120    avg_loss:0.004, val_acc:0.983]
Epoch [72/120    avg_loss:0.043, val_acc:0.957]
Epoch [73/120    avg_loss:0.840, val_acc:0.451]
Epoch [74/120    avg_loss:0.862, val_acc:0.554]
Epoch [75/120    avg_loss:0.842, val_acc:0.587]
Epoch [76/120    avg_loss:0.811, val_acc:0.577]
Epoch [77/120    avg_loss:0.805, val_acc:0.600]
Epoch [78/120    avg_loss:0.756, val_acc:0.608]
Epoch [79/120    avg_loss:0.747, val_acc:0.617]
Epoch [80/120    avg_loss:0.744, val_acc:0.654]
Epoch [81/120    avg_loss:0.720, val_acc:0.654]
Epoch [82/120    avg_loss:0.701, val_acc:0.674]
Epoch [83/120    avg_loss:0.697, val_acc:0.684]
Epoch [84/120    avg_loss:0.657, val_acc:0.683]
Epoch [85/120    avg_loss:0.618, val_acc:0.707]
Epoch [86/120    avg_loss:0.622, val_acc:0.706]
Epoch [87/120    avg_loss:0.601, val_acc:0.726]
Epoch [88/120    avg_loss:0.585, val_acc:0.729]
Epoch [89/120    avg_loss:0.622, val_acc:0.730]
Epoch [90/120    avg_loss:0.590, val_acc:0.734]
Epoch [91/120    avg_loss:0.572, val_acc:0.726]
Epoch [92/120    avg_loss:0.576, val_acc:0.728]
Epoch [93/120    avg_loss:0.590, val_acc:0.731]
Epoch [94/120    avg_loss:0.571, val_acc:0.730]
Epoch [95/120    avg_loss:0.570, val_acc:0.732]
Epoch [96/120    avg_loss:0.582, val_acc:0.734]
Epoch [97/120    avg_loss:0.583, val_acc:0.736]
Epoch [98/120    avg_loss:0.563, val_acc:0.730]
Epoch [99/120    avg_loss:0.579, val_acc:0.742]
Epoch [100/120    avg_loss:0.564, val_acc:0.742]
Epoch [101/120    avg_loss:0.597, val_acc:0.740]
Epoch [102/120    avg_loss:0.585, val_acc:0.741]
Epoch [103/120    avg_loss:0.572, val_acc:0.740]
Epoch [104/120    avg_loss:0.560, val_acc:0.739]
Epoch [105/120    avg_loss:0.574, val_acc:0.739]
Epoch [106/120    avg_loss:0.570, val_acc:0.739]
Epoch [107/120    avg_loss:0.571, val_acc:0.739]
Epoch [108/120    avg_loss:0.565, val_acc:0.738]
Epoch [109/120    avg_loss:0.573, val_acc:0.738]
Epoch [110/120    avg_loss:0.580, val_acc:0.738]
Epoch [111/120    avg_loss:0.567, val_acc:0.738]
Epoch [112/120    avg_loss:0.581, val_acc:0.738]
Epoch [113/120    avg_loss:0.563, val_acc:0.738]
Epoch [114/120    avg_loss:0.577, val_acc:0.738]
Epoch [115/120    avg_loss:0.574, val_acc:0.738]
Epoch [116/120    avg_loss:0.584, val_acc:0.738]
Epoch [117/120    avg_loss:0.599, val_acc:0.738]
Epoch [118/120    avg_loss:0.577, val_acc:0.738]
Epoch [119/120    avg_loss:0.544, val_acc:0.738]
Epoch [120/120    avg_loss:0.580, val_acc:0.738]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4535   919   134   278     0   395    14    98    59]
 [    0     8 12705     0  1560     0  3817     0     0     0]
 [    0    64     9  1631     6     0     0     0   312    14]
 [    0    23   202     0  2635     0   100     0     0    12]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   562     0   255     0  3989     0    72     0]
 [    0   131     3     0    20     0     0  1081    55     0]
 [    0   191   224    35   445     0    94     0  2582     0]
 [    0    39    39     0     2    78     2     0     0   759]]

Accuracy:
75.24642710818692

F1 scores:
[       nan 0.79401208 0.7758068  0.85036496 0.64480607 0.97098214
 0.60097928 0.90649895 0.77189836 0.86103233]

Kappa:
0.6841044133184775
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74c2e8f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.694, val_acc:0.676]
Epoch [2/120    avg_loss:1.016, val_acc:0.704]
Epoch [3/120    avg_loss:0.722, val_acc:0.739]
Epoch [4/120    avg_loss:0.497, val_acc:0.792]
Epoch [5/120    avg_loss:0.456, val_acc:0.832]
Epoch [6/120    avg_loss:0.405, val_acc:0.732]
Epoch [7/120    avg_loss:0.304, val_acc:0.860]
Epoch [8/120    avg_loss:0.275, val_acc:0.875]
Epoch [9/120    avg_loss:0.257, val_acc:0.876]
Epoch [10/120    avg_loss:0.283, val_acc:0.892]
Epoch [11/120    avg_loss:0.286, val_acc:0.852]
Epoch [12/120    avg_loss:0.218, val_acc:0.901]
Epoch [13/120    avg_loss:0.193, val_acc:0.892]
Epoch [14/120    avg_loss:0.176, val_acc:0.930]
Epoch [15/120    avg_loss:0.159, val_acc:0.949]
Epoch [16/120    avg_loss:0.146, val_acc:0.934]
Epoch [17/120    avg_loss:0.187, val_acc:0.931]
Epoch [18/120    avg_loss:0.153, val_acc:0.825]
Epoch [19/120    avg_loss:0.121, val_acc:0.906]
Epoch [20/120    avg_loss:0.096, val_acc:0.940]
Epoch [21/120    avg_loss:0.179, val_acc:0.921]
Epoch [22/120    avg_loss:0.118, val_acc:0.882]
Epoch [23/120    avg_loss:0.117, val_acc:0.916]
Epoch [24/120    avg_loss:0.115, val_acc:0.940]
Epoch [25/120    avg_loss:0.116, val_acc:0.916]
Epoch [26/120    avg_loss:0.119, val_acc:0.953]
Epoch [27/120    avg_loss:0.076, val_acc:0.893]
Epoch [28/120    avg_loss:0.103, val_acc:0.870]
Epoch [29/120    avg_loss:0.084, val_acc:0.959]
Epoch [30/120    avg_loss:0.077, val_acc:0.925]
Epoch [31/120    avg_loss:0.070, val_acc:0.962]
Epoch [32/120    avg_loss:0.081, val_acc:0.967]
Epoch [33/120    avg_loss:0.083, val_acc:0.918]
Epoch [34/120    avg_loss:0.087, val_acc:0.953]
Epoch [35/120    avg_loss:0.068, val_acc:0.968]
Epoch [36/120    avg_loss:0.051, val_acc:0.965]
Epoch [37/120    avg_loss:0.052, val_acc:0.938]
Epoch [38/120    avg_loss:0.060, val_acc:0.958]
Epoch [39/120    avg_loss:0.049, val_acc:0.974]
Epoch [40/120    avg_loss:0.100, val_acc:0.956]
Epoch [41/120    avg_loss:0.063, val_acc:0.888]
Epoch [42/120    avg_loss:0.056, val_acc:0.961]
Epoch [43/120    avg_loss:0.046, val_acc:0.968]
Epoch [44/120    avg_loss:0.056, val_acc:0.978]
Epoch [45/120    avg_loss:0.033, val_acc:0.977]
Epoch [46/120    avg_loss:0.037, val_acc:0.952]
Epoch [47/120    avg_loss:0.029, val_acc:0.980]
Epoch [48/120    avg_loss:0.020, val_acc:0.977]
Epoch [49/120    avg_loss:0.023, val_acc:0.969]
Epoch [50/120    avg_loss:0.028, val_acc:0.973]
Epoch [51/120    avg_loss:0.029, val_acc:0.969]
Epoch [52/120    avg_loss:0.023, val_acc:0.982]
Epoch [53/120    avg_loss:0.017, val_acc:0.978]
Epoch [54/120    avg_loss:0.045, val_acc:0.964]
Epoch [55/120    avg_loss:0.032, val_acc:0.983]
Epoch [56/120    avg_loss:0.028, val_acc:0.985]
Epoch [57/120    avg_loss:0.015, val_acc:0.983]
Epoch [58/120    avg_loss:0.017, val_acc:0.983]
Epoch [59/120    avg_loss:0.016, val_acc:0.974]
Epoch [60/120    avg_loss:0.024, val_acc:0.967]
Epoch [61/120    avg_loss:0.012, val_acc:0.978]
Epoch [62/120    avg_loss:0.014, val_acc:0.954]
Epoch [63/120    avg_loss:0.046, val_acc:0.983]
Epoch [64/120    avg_loss:0.020, val_acc:0.986]
Epoch [65/120    avg_loss:0.016, val_acc:0.991]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.020, val_acc:0.975]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.015, val_acc:0.983]
Epoch [70/120    avg_loss:0.013, val_acc:0.986]
Epoch [71/120    avg_loss:0.027, val_acc:0.984]
Epoch [72/120    avg_loss:0.009, val_acc:0.984]
Epoch [73/120    avg_loss:0.011, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.017, val_acc:0.987]
Epoch [76/120    avg_loss:0.027, val_acc:0.978]
Epoch [77/120    avg_loss:0.019, val_acc:0.984]
Epoch [78/120    avg_loss:0.019, val_acc:0.981]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.008, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     1     0     0     1    61     0]
 [    0     0 17759     0   268     0    59     0     4     0]
 [    0     4     0  1959     0     0     0     0    71     2]
 [    0    28     2     0  2912     0     9     0    18     3]
 [    0     0     0     0     0  1288     0     0     0    17]
 [    0     0     4     0     0     0  4867     0     7     0]
 [    0    16     0     0     0     0     2  1269     0     3]
 [    0     9     2    49    62     0     5     0  3443     1]
 [    0     0     1     0    14     1     0     0     0   903]]

Accuracy:
98.25512737088184

F1 scores:
[       nan 0.99066729 0.99051815 0.96884273 0.93498154 0.99306091
 0.99124236 0.99140625 0.95972125 0.97727273]

Kappa:
0.9769623809246168
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b7dacb898>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.781, val_acc:0.637]
Epoch [2/120    avg_loss:1.067, val_acc:0.631]
Epoch [3/120    avg_loss:0.670, val_acc:0.736]
Epoch [4/120    avg_loss:0.536, val_acc:0.805]
Epoch [5/120    avg_loss:0.488, val_acc:0.836]
Epoch [6/120    avg_loss:0.404, val_acc:0.728]
Epoch [7/120    avg_loss:0.368, val_acc:0.850]
Epoch [8/120    avg_loss:0.311, val_acc:0.867]
Epoch [9/120    avg_loss:0.250, val_acc:0.877]
Epoch [10/120    avg_loss:0.241, val_acc:0.910]
Epoch [11/120    avg_loss:0.206, val_acc:0.861]
Epoch [12/120    avg_loss:0.187, val_acc:0.935]
Epoch [13/120    avg_loss:0.203, val_acc:0.809]
Epoch [14/120    avg_loss:0.182, val_acc:0.926]
Epoch [15/120    avg_loss:0.153, val_acc:0.911]
Epoch [16/120    avg_loss:0.132, val_acc:0.956]
Epoch [17/120    avg_loss:0.101, val_acc:0.947]
Epoch [18/120    avg_loss:0.083, val_acc:0.960]
Epoch [19/120    avg_loss:0.097, val_acc:0.953]
Epoch [20/120    avg_loss:0.097, val_acc:0.950]
Epoch [21/120    avg_loss:0.090, val_acc:0.931]
Epoch [22/120    avg_loss:0.088, val_acc:0.908]
Epoch [23/120    avg_loss:0.092, val_acc:0.933]
Epoch [24/120    avg_loss:0.083, val_acc:0.880]
Epoch [25/120    avg_loss:0.063, val_acc:0.963]
Epoch [26/120    avg_loss:0.127, val_acc:0.925]
Epoch [27/120    avg_loss:0.131, val_acc:0.957]
Epoch [28/120    avg_loss:0.066, val_acc:0.960]
Epoch [29/120    avg_loss:0.058, val_acc:0.963]
Epoch [30/120    avg_loss:0.034, val_acc:0.971]
Epoch [31/120    avg_loss:0.060, val_acc:0.952]
Epoch [32/120    avg_loss:0.055, val_acc:0.948]
Epoch [33/120    avg_loss:0.027, val_acc:0.966]
Epoch [34/120    avg_loss:0.028, val_acc:0.960]
Epoch [35/120    avg_loss:0.023, val_acc:0.971]
Epoch [36/120    avg_loss:0.054, val_acc:0.790]
Epoch [37/120    avg_loss:0.110, val_acc:0.949]
Epoch [38/120    avg_loss:0.087, val_acc:0.955]
Epoch [39/120    avg_loss:0.053, val_acc:0.964]
Epoch [40/120    avg_loss:0.040, val_acc:0.961]
Epoch [41/120    avg_loss:0.020, val_acc:0.960]
Epoch [42/120    avg_loss:0.014, val_acc:0.969]
Epoch [43/120    avg_loss:0.023, val_acc:0.969]
Epoch [44/120    avg_loss:0.037, val_acc:0.968]
Epoch [45/120    avg_loss:0.016, val_acc:0.973]
Epoch [46/120    avg_loss:0.014, val_acc:0.972]
Epoch [47/120    avg_loss:0.010, val_acc:0.974]
Epoch [48/120    avg_loss:0.008, val_acc:0.975]
Epoch [49/120    avg_loss:0.011, val_acc:0.977]
Epoch [50/120    avg_loss:0.021, val_acc:0.912]
Epoch [51/120    avg_loss:0.023, val_acc:0.969]
Epoch [52/120    avg_loss:0.020, val_acc:0.974]
Epoch [53/120    avg_loss:0.011, val_acc:0.972]
Epoch [54/120    avg_loss:0.007, val_acc:0.976]
Epoch [55/120    avg_loss:0.011, val_acc:0.978]
Epoch [56/120    avg_loss:0.006, val_acc:0.974]
Epoch [57/120    avg_loss:0.010, val_acc:0.970]
Epoch [58/120    avg_loss:0.018, val_acc:0.968]
Epoch [59/120    avg_loss:0.007, val_acc:0.972]
Epoch [60/120    avg_loss:0.008, val_acc:0.971]
Epoch [61/120    avg_loss:0.013, val_acc:0.973]
Epoch [62/120    avg_loss:0.010, val_acc:0.978]
Epoch [63/120    avg_loss:0.011, val_acc:0.969]
Epoch [64/120    avg_loss:0.018, val_acc:0.974]
Epoch [65/120    avg_loss:0.010, val_acc:0.978]
Epoch [66/120    avg_loss:0.011, val_acc:0.978]
Epoch [67/120    avg_loss:0.014, val_acc:0.974]
Epoch [68/120    avg_loss:0.009, val_acc:0.982]
Epoch [69/120    avg_loss:0.013, val_acc:0.959]
Epoch [70/120    avg_loss:0.007, val_acc:0.977]
Epoch [71/120    avg_loss:0.006, val_acc:0.976]
Epoch [72/120    avg_loss:0.004, val_acc:0.982]
Epoch [73/120    avg_loss:0.008, val_acc:0.978]
Epoch [74/120    avg_loss:0.005, val_acc:0.982]
Epoch [75/120    avg_loss:0.013, val_acc:0.977]
Epoch [76/120    avg_loss:0.026, val_acc:0.975]
Epoch [77/120    avg_loss:0.018, val_acc:0.975]
Epoch [78/120    avg_loss:0.006, val_acc:0.977]
Epoch [79/120    avg_loss:0.008, val_acc:0.978]
Epoch [80/120    avg_loss:0.004, val_acc:0.978]
Epoch [81/120    avg_loss:0.005, val_acc:0.979]
Epoch [82/120    avg_loss:0.007, val_acc:0.961]
Epoch [83/120    avg_loss:0.032, val_acc:0.974]
Epoch [84/120    avg_loss:0.011, val_acc:0.968]
Epoch [85/120    avg_loss:0.007, val_acc:0.976]
Epoch [86/120    avg_loss:0.005, val_acc:0.975]
Epoch [87/120    avg_loss:0.005, val_acc:0.976]
Epoch [88/120    avg_loss:0.006, val_acc:0.976]
Epoch [89/120    avg_loss:0.003, val_acc:0.977]
Epoch [90/120    avg_loss:0.004, val_acc:0.976]
Epoch [91/120    avg_loss:0.006, val_acc:0.975]
Epoch [92/120    avg_loss:0.004, val_acc:0.976]
Epoch [93/120    avg_loss:0.003, val_acc:0.978]
Epoch [94/120    avg_loss:0.004, val_acc:0.978]
Epoch [95/120    avg_loss:0.004, val_acc:0.976]
Epoch [96/120    avg_loss:0.003, val_acc:0.977]
Epoch [97/120    avg_loss:0.004, val_acc:0.976]
Epoch [98/120    avg_loss:0.003, val_acc:0.977]
Epoch [99/120    avg_loss:0.003, val_acc:0.976]
Epoch [100/120    avg_loss:0.004, val_acc:0.977]
Epoch [101/120    avg_loss:0.003, val_acc:0.977]
Epoch [102/120    avg_loss:0.003, val_acc:0.977]
Epoch [103/120    avg_loss:0.003, val_acc:0.977]
Epoch [104/120    avg_loss:0.003, val_acc:0.977]
Epoch [105/120    avg_loss:0.003, val_acc:0.977]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.003, val_acc:0.977]
Epoch [108/120    avg_loss:0.003, val_acc:0.977]
Epoch [109/120    avg_loss:0.003, val_acc:0.977]
Epoch [110/120    avg_loss:0.003, val_acc:0.977]
Epoch [111/120    avg_loss:0.004, val_acc:0.977]
Epoch [112/120    avg_loss:0.005, val_acc:0.977]
Epoch [113/120    avg_loss:0.003, val_acc:0.977]
Epoch [114/120    avg_loss:0.004, val_acc:0.977]
Epoch [115/120    avg_loss:0.003, val_acc:0.977]
Epoch [116/120    avg_loss:0.003, val_acc:0.977]
Epoch [117/120    avg_loss:0.003, val_acc:0.977]
Epoch [118/120    avg_loss:0.005, val_acc:0.977]
Epoch [119/120    avg_loss:0.004, val_acc:0.977]
Epoch [120/120    avg_loss:0.003, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     0     0     1     8    50     0]
 [    0     0 18005     0    23     0    56     0     6     0]
 [    0    11     0  1966     0     0     0     0    54     5]
 [    0     5     6     0  2940     3     3     0     4    11]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    36     0     3     0  4829     0    10     0]
 [    0    17     0     0     0     0     0  1267     1     5]
 [    0     2     0    49    53     0     0     0  3466     1]
 [    0     0     0     0    14    13     0     0     0   892]]

Accuracy:
98.91547971947075

F1 scores:
[       nan 0.99267913 0.9964856  0.97062454 0.97918401 0.99390708
 0.98883997 0.98791423 0.96788607 0.97326787]

Kappa:
0.9856368763018719
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92fe1eb9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.761, val_acc:0.375]
Epoch [2/120    avg_loss:1.144, val_acc:0.633]
Epoch [3/120    avg_loss:0.727, val_acc:0.760]
Epoch [4/120    avg_loss:0.546, val_acc:0.769]
Epoch [5/120    avg_loss:0.489, val_acc:0.798]
Epoch [6/120    avg_loss:0.386, val_acc:0.860]
Epoch [7/120    avg_loss:0.416, val_acc:0.780]
Epoch [8/120    avg_loss:0.350, val_acc:0.861]
Epoch [9/120    avg_loss:0.277, val_acc:0.868]
Epoch [10/120    avg_loss:0.288, val_acc:0.727]
Epoch [11/120    avg_loss:0.327, val_acc:0.862]
Epoch [12/120    avg_loss:0.203, val_acc:0.915]
Epoch [13/120    avg_loss:0.161, val_acc:0.915]
Epoch [14/120    avg_loss:0.144, val_acc:0.943]
Epoch [15/120    avg_loss:0.178, val_acc:0.935]
Epoch [16/120    avg_loss:0.181, val_acc:0.919]
Epoch [17/120    avg_loss:0.134, val_acc:0.940]
Epoch [18/120    avg_loss:0.130, val_acc:0.945]
Epoch [19/120    avg_loss:0.137, val_acc:0.952]
Epoch [20/120    avg_loss:0.100, val_acc:0.964]
Epoch [21/120    avg_loss:0.076, val_acc:0.958]
Epoch [22/120    avg_loss:0.146, val_acc:0.955]
Epoch [23/120    avg_loss:0.084, val_acc:0.955]
Epoch [24/120    avg_loss:0.102, val_acc:0.947]
Epoch [25/120    avg_loss:0.067, val_acc:0.962]
Epoch [26/120    avg_loss:0.113, val_acc:0.917]
Epoch [27/120    avg_loss:0.121, val_acc:0.952]
Epoch [28/120    avg_loss:0.079, val_acc:0.942]
Epoch [29/120    avg_loss:0.107, val_acc:0.971]
Epoch [30/120    avg_loss:0.097, val_acc:0.951]
Epoch [31/120    avg_loss:0.116, val_acc:0.851]
Epoch [32/120    avg_loss:0.069, val_acc:0.978]
Epoch [33/120    avg_loss:0.041, val_acc:0.978]
Epoch [34/120    avg_loss:0.038, val_acc:0.962]
Epoch [35/120    avg_loss:0.034, val_acc:0.986]
Epoch [36/120    avg_loss:0.058, val_acc:0.975]
Epoch [37/120    avg_loss:0.049, val_acc:0.974]
Epoch [38/120    avg_loss:0.033, val_acc:0.980]
Epoch [39/120    avg_loss:0.035, val_acc:0.979]
Epoch [40/120    avg_loss:0.034, val_acc:0.966]
Epoch [41/120    avg_loss:0.025, val_acc:0.980]
Epoch [42/120    avg_loss:0.578, val_acc:0.253]
Epoch [43/120    avg_loss:1.497, val_acc:0.644]
Epoch [44/120    avg_loss:1.225, val_acc:0.670]
Epoch [45/120    avg_loss:1.010, val_acc:0.710]
Epoch [46/120    avg_loss:0.924, val_acc:0.691]
Epoch [47/120    avg_loss:0.815, val_acc:0.659]
Epoch [48/120    avg_loss:0.771, val_acc:0.765]
Epoch [49/120    avg_loss:0.742, val_acc:0.789]
Epoch [50/120    avg_loss:0.634, val_acc:0.795]
Epoch [51/120    avg_loss:0.615, val_acc:0.800]
Epoch [52/120    avg_loss:0.593, val_acc:0.794]
Epoch [53/120    avg_loss:0.592, val_acc:0.806]
Epoch [54/120    avg_loss:0.566, val_acc:0.810]
Epoch [55/120    avg_loss:0.591, val_acc:0.808]
Epoch [56/120    avg_loss:0.547, val_acc:0.812]
Epoch [57/120    avg_loss:0.588, val_acc:0.811]
Epoch [58/120    avg_loss:0.531, val_acc:0.815]
Epoch [59/120    avg_loss:0.541, val_acc:0.796]
Epoch [60/120    avg_loss:0.497, val_acc:0.816]
Epoch [61/120    avg_loss:0.515, val_acc:0.822]
Epoch [62/120    avg_loss:0.526, val_acc:0.820]
Epoch [63/120    avg_loss:0.504, val_acc:0.821]
Epoch [64/120    avg_loss:0.497, val_acc:0.820]
Epoch [65/120    avg_loss:0.499, val_acc:0.819]
Epoch [66/120    avg_loss:0.495, val_acc:0.819]
Epoch [67/120    avg_loss:0.488, val_acc:0.818]
Epoch [68/120    avg_loss:0.511, val_acc:0.819]
Epoch [69/120    avg_loss:0.523, val_acc:0.820]
Epoch [70/120    avg_loss:0.553, val_acc:0.818]
Epoch [71/120    avg_loss:0.484, val_acc:0.820]
Epoch [72/120    avg_loss:0.530, val_acc:0.817]
Epoch [73/120    avg_loss:0.495, val_acc:0.821]
Epoch [74/120    avg_loss:0.482, val_acc:0.821]
Epoch [75/120    avg_loss:0.501, val_acc:0.820]
Epoch [76/120    avg_loss:0.516, val_acc:0.820]
Epoch [77/120    avg_loss:0.490, val_acc:0.820]
Epoch [78/120    avg_loss:0.518, val_acc:0.820]
Epoch [79/120    avg_loss:0.497, val_acc:0.820]
Epoch [80/120    avg_loss:0.500, val_acc:0.820]
Epoch [81/120    avg_loss:0.513, val_acc:0.821]
Epoch [82/120    avg_loss:0.528, val_acc:0.821]
Epoch [83/120    avg_loss:0.522, val_acc:0.822]
Epoch [84/120    avg_loss:0.506, val_acc:0.823]
Epoch [85/120    avg_loss:0.525, val_acc:0.822]
Epoch [86/120    avg_loss:0.483, val_acc:0.822]
Epoch [87/120    avg_loss:0.499, val_acc:0.824]
Epoch [88/120    avg_loss:0.467, val_acc:0.824]
Epoch [89/120    avg_loss:0.495, val_acc:0.824]
Epoch [90/120    avg_loss:0.495, val_acc:0.824]
Epoch [91/120    avg_loss:0.508, val_acc:0.824]
Epoch [92/120    avg_loss:0.519, val_acc:0.824]
Epoch [93/120    avg_loss:0.529, val_acc:0.824]
Epoch [94/120    avg_loss:0.506, val_acc:0.824]
Epoch [95/120    avg_loss:0.509, val_acc:0.824]
Epoch [96/120    avg_loss:0.482, val_acc:0.824]
Epoch [97/120    avg_loss:0.506, val_acc:0.824]
Epoch [98/120    avg_loss:0.467, val_acc:0.824]
Epoch [99/120    avg_loss:0.470, val_acc:0.824]
Epoch [100/120    avg_loss:0.500, val_acc:0.824]
Epoch [101/120    avg_loss:0.509, val_acc:0.824]
Epoch [102/120    avg_loss:0.472, val_acc:0.824]
Epoch [103/120    avg_loss:0.497, val_acc:0.824]
Epoch [104/120    avg_loss:0.490, val_acc:0.824]
Epoch [105/120    avg_loss:0.483, val_acc:0.824]
Epoch [106/120    avg_loss:0.496, val_acc:0.824]
Epoch [107/120    avg_loss:0.501, val_acc:0.824]
Epoch [108/120    avg_loss:0.521, val_acc:0.824]
Epoch [109/120    avg_loss:0.501, val_acc:0.824]
Epoch [110/120    avg_loss:0.457, val_acc:0.824]
Epoch [111/120    avg_loss:0.503, val_acc:0.824]
Epoch [112/120    avg_loss:0.496, val_acc:0.824]
Epoch [113/120    avg_loss:0.523, val_acc:0.824]
Epoch [114/120    avg_loss:0.496, val_acc:0.824]
Epoch [115/120    avg_loss:0.496, val_acc:0.824]
Epoch [116/120    avg_loss:0.495, val_acc:0.824]
Epoch [117/120    avg_loss:0.517, val_acc:0.824]
Epoch [118/120    avg_loss:0.514, val_acc:0.824]
Epoch [119/120    avg_loss:0.521, val_acc:0.824]
Epoch [120/120    avg_loss:0.528, val_acc:0.824]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5654     1   140    31     0   263    43   290    10]
 [    0     0 16828     0     0     0  1262     0     0     0]
 [    0    47    69  1553     2     0    12     0   332    21]
 [    0    79   175    15  2291     0   261     0   150     1]
 [    0     1     0     2     0  1302     0     0     0     0]
 [    0     0  1785     2     0     0  2922     0   169     0]
 [    0    71     0     0     0     0    17  1202     0     0]
 [    0   122     0   104    17     0   133     0  3195     0]
 [    0    26     0     6    19   102     0     2     0   764]]

Accuracy:
86.06511941773311

F1 scores:
[       nan 0.90958816 0.91090181 0.80508035 0.85933983 0.96124031
 0.59950759 0.94757588 0.82911639 0.8909621 ]

Kappa:
0.8138068763742294
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f941be989b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.827, val_acc:0.640]
Epoch [2/120    avg_loss:1.130, val_acc:0.508]
Epoch [3/120    avg_loss:0.795, val_acc:0.765]
Epoch [4/120    avg_loss:0.604, val_acc:0.686]
Epoch [5/120    avg_loss:0.510, val_acc:0.796]
Epoch [6/120    avg_loss:0.448, val_acc:0.834]
Epoch [7/120    avg_loss:0.408, val_acc:0.877]
Epoch [8/120    avg_loss:0.299, val_acc:0.917]
Epoch [9/120    avg_loss:0.272, val_acc:0.866]
Epoch [10/120    avg_loss:0.280, val_acc:0.889]
Epoch [11/120    avg_loss:0.277, val_acc:0.861]
Epoch [12/120    avg_loss:0.209, val_acc:0.891]
Epoch [13/120    avg_loss:0.300, val_acc:0.851]
Epoch [14/120    avg_loss:0.243, val_acc:0.913]
Epoch [15/120    avg_loss:0.243, val_acc:0.908]
Epoch [16/120    avg_loss:0.206, val_acc:0.922]
Epoch [17/120    avg_loss:0.187, val_acc:0.950]
Epoch [18/120    avg_loss:0.157, val_acc:0.893]
Epoch [19/120    avg_loss:0.121, val_acc:0.931]
Epoch [20/120    avg_loss:0.134, val_acc:0.945]
Epoch [21/120    avg_loss:0.100, val_acc:0.929]
Epoch [22/120    avg_loss:0.100, val_acc:0.957]
Epoch [23/120    avg_loss:0.090, val_acc:0.930]
Epoch [24/120    avg_loss:0.142, val_acc:0.872]
Epoch [25/120    avg_loss:0.210, val_acc:0.898]
Epoch [26/120    avg_loss:0.092, val_acc:0.949]
Epoch [27/120    avg_loss:0.096, val_acc:0.836]
Epoch [28/120    avg_loss:0.118, val_acc:0.940]
Epoch [29/120    avg_loss:0.099, val_acc:0.934]
Epoch [30/120    avg_loss:0.099, val_acc:0.928]
Epoch [31/120    avg_loss:0.057, val_acc:0.961]
Epoch [32/120    avg_loss:0.058, val_acc:0.956]
Epoch [33/120    avg_loss:0.071, val_acc:0.973]
Epoch [34/120    avg_loss:0.046, val_acc:0.975]
Epoch [35/120    avg_loss:0.037, val_acc:0.978]
Epoch [36/120    avg_loss:0.073, val_acc:0.961]
Epoch [37/120    avg_loss:0.094, val_acc:0.964]
Epoch [38/120    avg_loss:0.046, val_acc:0.975]
Epoch [39/120    avg_loss:0.032, val_acc:0.962]
Epoch [40/120    avg_loss:0.033, val_acc:0.974]
Epoch [41/120    avg_loss:0.038, val_acc:0.977]
Epoch [42/120    avg_loss:0.026, val_acc:0.976]
Epoch [43/120    avg_loss:0.022, val_acc:0.984]
Epoch [44/120    avg_loss:0.029, val_acc:0.977]
Epoch [45/120    avg_loss:0.019, val_acc:0.985]
Epoch [46/120    avg_loss:0.022, val_acc:0.977]
Epoch [47/120    avg_loss:0.021, val_acc:0.980]
Epoch [48/120    avg_loss:0.028, val_acc:0.984]
Epoch [49/120    avg_loss:0.015, val_acc:0.984]
Epoch [50/120    avg_loss:0.014, val_acc:0.990]
Epoch [51/120    avg_loss:0.012, val_acc:0.977]
Epoch [52/120    avg_loss:0.016, val_acc:0.984]
Epoch [53/120    avg_loss:0.011, val_acc:0.982]
Epoch [54/120    avg_loss:0.010, val_acc:0.990]
Epoch [55/120    avg_loss:0.012, val_acc:0.979]
Epoch [56/120    avg_loss:0.010, val_acc:0.983]
Epoch [57/120    avg_loss:0.027, val_acc:0.973]
Epoch [58/120    avg_loss:0.045, val_acc:0.975]
Epoch [59/120    avg_loss:0.032, val_acc:0.924]
Epoch [60/120    avg_loss:0.027, val_acc:0.987]
Epoch [61/120    avg_loss:0.010, val_acc:0.976]
Epoch [62/120    avg_loss:0.008, val_acc:0.986]
Epoch [63/120    avg_loss:0.016, val_acc:0.957]
Epoch [64/120    avg_loss:0.007, val_acc:0.988]
Epoch [65/120    avg_loss:0.014, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.991]
Epoch [67/120    avg_loss:0.013, val_acc:0.980]
Epoch [68/120    avg_loss:0.006, val_acc:0.987]
Epoch [69/120    avg_loss:0.010, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.990]
Epoch [71/120    avg_loss:0.009, val_acc:0.987]
Epoch [72/120    avg_loss:0.005, val_acc:0.987]
Epoch [73/120    avg_loss:0.003, val_acc:0.987]
Epoch [74/120    avg_loss:0.006, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.985]
Epoch [76/120    avg_loss:0.005, val_acc:0.986]
Epoch [77/120    avg_loss:0.004, val_acc:0.991]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.004, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.004, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.003, val_acc:0.990]
Epoch [86/120    avg_loss:0.004, val_acc:0.992]
Epoch [87/120    avg_loss:0.003, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.977]
Epoch [90/120    avg_loss:0.036, val_acc:0.981]
Epoch [91/120    avg_loss:0.227, val_acc:0.955]
Epoch [92/120    avg_loss:0.056, val_acc:0.982]
Epoch [93/120    avg_loss:0.027, val_acc:0.986]
Epoch [94/120    avg_loss:0.031, val_acc:0.914]
Epoch [95/120    avg_loss:0.023, val_acc:0.988]
Epoch [96/120    avg_loss:0.016, val_acc:0.987]
Epoch [97/120    avg_loss:0.012, val_acc:0.973]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.013, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     0     0     0     3    15    34     0]
 [    0     0 18033     0    14     0    28     0    15     0]
 [    0    12     0  1998     0     0     0     0    20     6]
 [    0    41    13     0  2889     0     9     7    13     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     3     0     0     0  4847     0    28     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    14     0    41    47     0     0     0  3469     0]
 [    0     2     0     0    16    18     0     0     0   883]]

Accuracy:
99.0359819728629

F1 scores:
[       nan 0.99060632 0.99798002 0.9806135  0.9730549  0.99276742
 0.99272913 0.99154497 0.97034965 0.97622996]

Kappa:
0.9872315609791359
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0638fb2940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.769, val_acc:0.560]
Epoch [2/120    avg_loss:1.024, val_acc:0.737]
Epoch [3/120    avg_loss:0.696, val_acc:0.796]
Epoch [4/120    avg_loss:0.583, val_acc:0.777]
Epoch [5/120    avg_loss:0.448, val_acc:0.847]
Epoch [6/120    avg_loss:0.375, val_acc:0.887]
Epoch [7/120    avg_loss:0.373, val_acc:0.791]
Epoch [8/120    avg_loss:0.310, val_acc:0.874]
Epoch [9/120    avg_loss:0.326, val_acc:0.851]
Epoch [10/120    avg_loss:0.305, val_acc:0.890]
Epoch [11/120    avg_loss:0.207, val_acc:0.887]
Epoch [12/120    avg_loss:0.335, val_acc:0.884]
Epoch [13/120    avg_loss:0.256, val_acc:0.904]
Epoch [14/120    avg_loss:0.195, val_acc:0.850]
Epoch [15/120    avg_loss:0.183, val_acc:0.888]
Epoch [16/120    avg_loss:0.178, val_acc:0.941]
Epoch [17/120    avg_loss:0.152, val_acc:0.940]
Epoch [18/120    avg_loss:0.122, val_acc:0.938]
Epoch [19/120    avg_loss:0.167, val_acc:0.937]
Epoch [20/120    avg_loss:0.119, val_acc:0.919]
Epoch [21/120    avg_loss:0.110, val_acc:0.923]
Epoch [22/120    avg_loss:0.096, val_acc:0.953]
Epoch [23/120    avg_loss:0.103, val_acc:0.951]
Epoch [24/120    avg_loss:0.089, val_acc:0.897]
Epoch [25/120    avg_loss:0.112, val_acc:0.943]
Epoch [26/120    avg_loss:0.084, val_acc:0.951]
Epoch [27/120    avg_loss:0.083, val_acc:0.941]
Epoch [28/120    avg_loss:0.067, val_acc:0.933]
Epoch [29/120    avg_loss:0.082, val_acc:0.949]
Epoch [30/120    avg_loss:0.122, val_acc:0.953]
Epoch [31/120    avg_loss:0.107, val_acc:0.956]
Epoch [32/120    avg_loss:0.064, val_acc:0.960]
Epoch [33/120    avg_loss:0.070, val_acc:0.946]
Epoch [34/120    avg_loss:0.099, val_acc:0.967]
Epoch [35/120    avg_loss:0.073, val_acc:0.952]
Epoch [36/120    avg_loss:0.095, val_acc:0.965]
Epoch [37/120    avg_loss:0.064, val_acc:0.960]
Epoch [38/120    avg_loss:0.084, val_acc:0.960]
Epoch [39/120    avg_loss:0.039, val_acc:0.971]
Epoch [40/120    avg_loss:0.038, val_acc:0.973]
Epoch [41/120    avg_loss:0.036, val_acc:0.978]
Epoch [42/120    avg_loss:0.051, val_acc:0.968]
Epoch [43/120    avg_loss:0.026, val_acc:0.974]
Epoch [44/120    avg_loss:0.031, val_acc:0.961]
Epoch [45/120    avg_loss:0.029, val_acc:0.975]
Epoch [46/120    avg_loss:0.062, val_acc:0.942]
Epoch [47/120    avg_loss:0.055, val_acc:0.971]
Epoch [48/120    avg_loss:0.041, val_acc:0.969]
Epoch [49/120    avg_loss:0.028, val_acc:0.968]
Epoch [50/120    avg_loss:0.034, val_acc:0.961]
Epoch [51/120    avg_loss:0.032, val_acc:0.975]
Epoch [52/120    avg_loss:0.019, val_acc:0.951]
Epoch [53/120    avg_loss:0.032, val_acc:0.970]
Epoch [54/120    avg_loss:0.041, val_acc:0.967]
Epoch [55/120    avg_loss:0.020, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.978]
Epoch [57/120    avg_loss:0.014, val_acc:0.980]
Epoch [58/120    avg_loss:0.013, val_acc:0.979]
Epoch [59/120    avg_loss:0.015, val_acc:0.979]
Epoch [60/120    avg_loss:0.016, val_acc:0.977]
Epoch [61/120    avg_loss:0.013, val_acc:0.978]
Epoch [62/120    avg_loss:0.013, val_acc:0.977]
Epoch [63/120    avg_loss:0.010, val_acc:0.978]
Epoch [64/120    avg_loss:0.015, val_acc:0.978]
Epoch [65/120    avg_loss:0.010, val_acc:0.980]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.012, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.011, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.980]
Epoch [75/120    avg_loss:0.008, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.982]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.008, val_acc:0.982]
Epoch [82/120    avg_loss:0.009, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.013, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.980]
Epoch [88/120    avg_loss:0.010, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.980]
Epoch [91/120    avg_loss:0.011, val_acc:0.980]
Epoch [92/120    avg_loss:0.010, val_acc:0.979]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.009, val_acc:0.980]
Epoch [95/120    avg_loss:0.008, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.011, val_acc:0.981]
Epoch [98/120    avg_loss:0.009, val_acc:0.981]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.021, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.981]
Epoch [106/120    avg_loss:0.012, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.016, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.009, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     1     1     0     0     0    60    11]
 [    0     0 18061     0    17     0     5     0     7     0]
 [    0     0     0  1954     0     0     0     0    75     7]
 [    0    23    10     0  2915     0     4     0    17     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     1     0     0     0     0     0  1281     0     8]
 [    0     0     0    70    66     0     3     0  3424     8]
 [    0     2     0     0    14    56     0     0     0   847]]

Accuracy:
98.86486877304606

F1 scores:
[       nan 0.99227588 0.99892149 0.96232455 0.97410192 0.97899475
 0.99856646 0.99649942 0.9569592  0.9395452 ]

Kappa:
0.9849636684324964
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51aff7e940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.816, val_acc:0.408]
Epoch [2/120    avg_loss:1.089, val_acc:0.686]
Epoch [3/120    avg_loss:0.772, val_acc:0.760]
Epoch [4/120    avg_loss:0.549, val_acc:0.800]
Epoch [5/120    avg_loss:0.441, val_acc:0.816]
Epoch [6/120    avg_loss:0.396, val_acc:0.867]
Epoch [7/120    avg_loss:0.343, val_acc:0.888]
Epoch [8/120    avg_loss:0.282, val_acc:0.877]
Epoch [9/120    avg_loss:0.260, val_acc:0.885]
Epoch [10/120    avg_loss:0.275, val_acc:0.861]
Epoch [11/120    avg_loss:0.254, val_acc:0.904]
Epoch [12/120    avg_loss:0.181, val_acc:0.904]
Epoch [13/120    avg_loss:0.160, val_acc:0.873]
Epoch [14/120    avg_loss:0.177, val_acc:0.932]
Epoch [15/120    avg_loss:0.160, val_acc:0.935]
Epoch [16/120    avg_loss:0.134, val_acc:0.942]
Epoch [17/120    avg_loss:0.203, val_acc:0.893]
Epoch [18/120    avg_loss:0.173, val_acc:0.903]
Epoch [19/120    avg_loss:0.150, val_acc:0.945]
Epoch [20/120    avg_loss:0.092, val_acc:0.951]
Epoch [21/120    avg_loss:0.090, val_acc:0.960]
Epoch [22/120    avg_loss:0.093, val_acc:0.918]
Epoch [23/120    avg_loss:0.077, val_acc:0.940]
Epoch [24/120    avg_loss:0.060, val_acc:0.964]
Epoch [25/120    avg_loss:0.055, val_acc:0.967]
Epoch [26/120    avg_loss:0.055, val_acc:0.967]
Epoch [27/120    avg_loss:0.068, val_acc:0.959]
Epoch [28/120    avg_loss:0.082, val_acc:0.963]
Epoch [29/120    avg_loss:0.158, val_acc:0.952]
Epoch [30/120    avg_loss:0.079, val_acc:0.935]
Epoch [31/120    avg_loss:0.082, val_acc:0.961]
Epoch [32/120    avg_loss:0.060, val_acc:0.965]
Epoch [33/120    avg_loss:0.072, val_acc:0.955]
Epoch [34/120    avg_loss:0.077, val_acc:0.965]
Epoch [35/120    avg_loss:0.066, val_acc:0.948]
Epoch [36/120    avg_loss:0.065, val_acc:0.947]
Epoch [37/120    avg_loss:0.035, val_acc:0.963]
Epoch [38/120    avg_loss:0.060, val_acc:0.956]
Epoch [39/120    avg_loss:0.054, val_acc:0.965]
Epoch [40/120    avg_loss:0.031, val_acc:0.982]
Epoch [41/120    avg_loss:0.026, val_acc:0.980]
Epoch [42/120    avg_loss:0.022, val_acc:0.979]
Epoch [43/120    avg_loss:0.021, val_acc:0.981]
Epoch [44/120    avg_loss:0.018, val_acc:0.981]
Epoch [45/120    avg_loss:0.020, val_acc:0.982]
Epoch [46/120    avg_loss:0.017, val_acc:0.981]
Epoch [47/120    avg_loss:0.019, val_acc:0.982]
Epoch [48/120    avg_loss:0.023, val_acc:0.982]
Epoch [49/120    avg_loss:0.014, val_acc:0.980]
Epoch [50/120    avg_loss:0.024, val_acc:0.983]
Epoch [51/120    avg_loss:0.014, val_acc:0.982]
Epoch [52/120    avg_loss:0.022, val_acc:0.982]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.017, val_acc:0.983]
Epoch [55/120    avg_loss:0.015, val_acc:0.982]
Epoch [56/120    avg_loss:0.018, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.014, val_acc:0.982]
Epoch [59/120    avg_loss:0.017, val_acc:0.982]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.015, val_acc:0.984]
Epoch [62/120    avg_loss:0.017, val_acc:0.983]
Epoch [63/120    avg_loss:0.015, val_acc:0.983]
Epoch [64/120    avg_loss:0.014, val_acc:0.982]
Epoch [65/120    avg_loss:0.011, val_acc:0.983]
Epoch [66/120    avg_loss:0.015, val_acc:0.982]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.014, val_acc:0.982]
Epoch [69/120    avg_loss:0.019, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.014, val_acc:0.982]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.013, val_acc:0.983]
Epoch [74/120    avg_loss:0.016, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.982]
Epoch [76/120    avg_loss:0.014, val_acc:0.982]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.014, val_acc:0.983]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.011, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.012, val_acc:0.983]
Epoch [88/120    avg_loss:0.016, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.982]
Epoch [92/120    avg_loss:0.023, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.985]
Epoch [98/120    avg_loss:0.013, val_acc:0.986]
Epoch [99/120    avg_loss:0.016, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.011, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.985]
Epoch [111/120    avg_loss:0.015, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.015, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     0     0     0     0    39     0]
 [    0     1 18013     0    45     0    30     0     0     1]
 [    0     0     0  2022     0     0     0     0     7     7]
 [    0    30    11     0  2893     0     9     0    28     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4865     0    11     0]
 [    0     0     0     0     0     0     2  1282     0     6]
 [    0     7     0    22    59     0     0     0  3476     7]
 [    0     0     0     0    14    35     0     0     0   870]]

Accuracy:
99.0986431446268

F1 scores:
[       nan 0.99401384 0.99750803 0.99117647 0.96707337 0.98676749
 0.99448078 0.99688958 0.97476164 0.96079514]

Kappa:
0.988064933353615
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fabd2b0b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.834, val_acc:0.609]
Epoch [2/120    avg_loss:1.118, val_acc:0.513]
Epoch [3/120    avg_loss:0.716, val_acc:0.761]
Epoch [4/120    avg_loss:0.507, val_acc:0.831]
Epoch [5/120    avg_loss:0.452, val_acc:0.851]
Epoch [6/120    avg_loss:0.362, val_acc:0.833]
Epoch [7/120    avg_loss:0.309, val_acc:0.870]
Epoch [8/120    avg_loss:0.282, val_acc:0.861]
Epoch [9/120    avg_loss:0.256, val_acc:0.831]
Epoch [10/120    avg_loss:0.240, val_acc:0.911]
Epoch [11/120    avg_loss:1.055, val_acc:0.605]
Epoch [12/120    avg_loss:1.418, val_acc:0.701]
Epoch [13/120    avg_loss:1.245, val_acc:0.713]
Epoch [14/120    avg_loss:1.025, val_acc:0.715]
Epoch [15/120    avg_loss:0.901, val_acc:0.777]
Epoch [16/120    avg_loss:0.856, val_acc:0.778]
Epoch [17/120    avg_loss:0.737, val_acc:0.797]
Epoch [18/120    avg_loss:0.715, val_acc:0.804]
Epoch [19/120    avg_loss:0.629, val_acc:0.772]
Epoch [20/120    avg_loss:0.639, val_acc:0.817]
Epoch [21/120    avg_loss:0.586, val_acc:0.819]
Epoch [22/120    avg_loss:0.518, val_acc:0.821]
Epoch [23/120    avg_loss:0.528, val_acc:0.757]
Epoch [24/120    avg_loss:0.497, val_acc:0.839]
Epoch [25/120    avg_loss:0.446, val_acc:0.844]
Epoch [26/120    avg_loss:0.442, val_acc:0.836]
Epoch [27/120    avg_loss:0.448, val_acc:0.838]
Epoch [28/120    avg_loss:0.485, val_acc:0.839]
Epoch [29/120    avg_loss:0.429, val_acc:0.838]
Epoch [30/120    avg_loss:0.451, val_acc:0.838]
Epoch [31/120    avg_loss:0.414, val_acc:0.841]
Epoch [32/120    avg_loss:0.421, val_acc:0.837]
Epoch [33/120    avg_loss:0.421, val_acc:0.840]
Epoch [34/120    avg_loss:0.375, val_acc:0.844]
Epoch [35/120    avg_loss:0.403, val_acc:0.849]
Epoch [36/120    avg_loss:0.416, val_acc:0.840]
Epoch [37/120    avg_loss:0.427, val_acc:0.841]
Epoch [38/120    avg_loss:0.403, val_acc:0.842]
Epoch [39/120    avg_loss:0.393, val_acc:0.845]
Epoch [40/120    avg_loss:0.429, val_acc:0.847]
Epoch [41/120    avg_loss:0.404, val_acc:0.849]
Epoch [42/120    avg_loss:0.399, val_acc:0.849]
Epoch [43/120    avg_loss:0.434, val_acc:0.845]
Epoch [44/120    avg_loss:0.404, val_acc:0.844]
Epoch [45/120    avg_loss:0.405, val_acc:0.848]
Epoch [46/120    avg_loss:0.421, val_acc:0.848]
Epoch [47/120    avg_loss:0.417, val_acc:0.848]
Epoch [48/120    avg_loss:0.417, val_acc:0.844]
Epoch [49/120    avg_loss:0.404, val_acc:0.846]
Epoch [50/120    avg_loss:0.390, val_acc:0.846]
Epoch [51/120    avg_loss:0.409, val_acc:0.846]
Epoch [52/120    avg_loss:0.416, val_acc:0.847]
Epoch [53/120    avg_loss:0.392, val_acc:0.847]
Epoch [54/120    avg_loss:0.402, val_acc:0.847]
Epoch [55/120    avg_loss:0.403, val_acc:0.846]
Epoch [56/120    avg_loss:0.424, val_acc:0.846]
Epoch [57/120    avg_loss:0.429, val_acc:0.846]
Epoch [58/120    avg_loss:0.396, val_acc:0.845]
Epoch [59/120    avg_loss:0.404, val_acc:0.846]
Epoch [60/120    avg_loss:0.429, val_acc:0.846]
Epoch [61/120    avg_loss:0.398, val_acc:0.846]
Epoch [62/120    avg_loss:0.398, val_acc:0.845]
Epoch [63/120    avg_loss:0.415, val_acc:0.845]
Epoch [64/120    avg_loss:0.390, val_acc:0.845]
Epoch [65/120    avg_loss:0.417, val_acc:0.845]
Epoch [66/120    avg_loss:0.393, val_acc:0.845]
Epoch [67/120    avg_loss:0.404, val_acc:0.845]
Epoch [68/120    avg_loss:0.436, val_acc:0.845]
Epoch [69/120    avg_loss:0.395, val_acc:0.845]
Epoch [70/120    avg_loss:0.420, val_acc:0.846]
Epoch [71/120    avg_loss:0.443, val_acc:0.845]
Epoch [72/120    avg_loss:0.404, val_acc:0.846]
Epoch [73/120    avg_loss:0.402, val_acc:0.846]
Epoch [74/120    avg_loss:0.433, val_acc:0.845]
Epoch [75/120    avg_loss:0.396, val_acc:0.845]
Epoch [76/120    avg_loss:0.407, val_acc:0.845]
Epoch [77/120    avg_loss:0.399, val_acc:0.845]
Epoch [78/120    avg_loss:0.396, val_acc:0.845]
Epoch [79/120    avg_loss:0.431, val_acc:0.845]
Epoch [80/120    avg_loss:0.423, val_acc:0.845]
Epoch [81/120    avg_loss:0.380, val_acc:0.845]
Epoch [82/120    avg_loss:0.423, val_acc:0.845]
Epoch [83/120    avg_loss:0.402, val_acc:0.845]
Epoch [84/120    avg_loss:0.380, val_acc:0.845]
Epoch [85/120    avg_loss:0.448, val_acc:0.845]
Epoch [86/120    avg_loss:0.405, val_acc:0.845]
Epoch [87/120    avg_loss:0.371, val_acc:0.845]
Epoch [88/120    avg_loss:0.382, val_acc:0.845]
Epoch [89/120    avg_loss:0.408, val_acc:0.845]
Epoch [90/120    avg_loss:0.411, val_acc:0.845]
Epoch [91/120    avg_loss:0.413, val_acc:0.845]
Epoch [92/120    avg_loss:0.434, val_acc:0.845]
Epoch [93/120    avg_loss:0.405, val_acc:0.845]
Epoch [94/120    avg_loss:0.431, val_acc:0.845]
Epoch [95/120    avg_loss:0.420, val_acc:0.845]
Epoch [96/120    avg_loss:0.444, val_acc:0.845]
Epoch [97/120    avg_loss:0.427, val_acc:0.845]
Epoch [98/120    avg_loss:0.371, val_acc:0.845]
Epoch [99/120    avg_loss:0.377, val_acc:0.845]
Epoch [100/120    avg_loss:0.410, val_acc:0.845]
Epoch [101/120    avg_loss:0.384, val_acc:0.845]
Epoch [102/120    avg_loss:0.388, val_acc:0.845]
Epoch [103/120    avg_loss:0.397, val_acc:0.845]
Epoch [104/120    avg_loss:0.406, val_acc:0.845]
Epoch [105/120    avg_loss:0.393, val_acc:0.845]
Epoch [106/120    avg_loss:0.403, val_acc:0.845]
Epoch [107/120    avg_loss:0.388, val_acc:0.845]
Epoch [108/120    avg_loss:0.423, val_acc:0.845]
Epoch [109/120    avg_loss:0.410, val_acc:0.845]
Epoch [110/120    avg_loss:0.401, val_acc:0.845]
Epoch [111/120    avg_loss:0.393, val_acc:0.845]
Epoch [112/120    avg_loss:0.421, val_acc:0.845]
Epoch [113/120    avg_loss:0.394, val_acc:0.845]
Epoch [114/120    avg_loss:0.414, val_acc:0.845]
Epoch [115/120    avg_loss:0.411, val_acc:0.845]
Epoch [116/120    avg_loss:0.424, val_acc:0.845]
Epoch [117/120    avg_loss:0.414, val_acc:0.845]
Epoch [118/120    avg_loss:0.392, val_acc:0.845]
Epoch [119/120    avg_loss:0.387, val_acc:0.845]
Epoch [120/120    avg_loss:0.409, val_acc:0.845]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5642     7     0   138     0   405     0   210    30]
 [    0     5 14949     0     7     0  3129     0     0     0]
 [    0    20     0  1636    12     0     1     0   331    36]
 [    0    71   238     0  2456     0   143     2    61     1]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     4   870    23     3     0  3855     0   123     0]
 [    0   122     0     0     2     0    13  1143     0    10]
 [    0   125     0    26    63     0   191     0  3163     3]
 [    0    20     0     1    22   110     0     0     0   766]]

Accuracy:
84.14431349866243

F1 scores:
[       nan 0.90700104 0.87538795 0.87909726 0.86555066 0.95917617
 0.61117717 0.93880903 0.84810296 0.86749717]

Kappa:
0.7940111970428743
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f66b01a9860>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.750, val_acc:0.416]
Epoch [2/120    avg_loss:1.116, val_acc:0.600]
Epoch [3/120    avg_loss:0.757, val_acc:0.808]
Epoch [4/120    avg_loss:0.558, val_acc:0.752]
Epoch [5/120    avg_loss:0.529, val_acc:0.819]
Epoch [6/120    avg_loss:0.409, val_acc:0.808]
Epoch [7/120    avg_loss:0.326, val_acc:0.865]
Epoch [8/120    avg_loss:0.311, val_acc:0.879]
Epoch [9/120    avg_loss:0.281, val_acc:0.786]
Epoch [10/120    avg_loss:0.248, val_acc:0.889]
Epoch [11/120    avg_loss:0.241, val_acc:0.905]
Epoch [12/120    avg_loss:0.229, val_acc:0.820]
Epoch [13/120    avg_loss:0.256, val_acc:0.847]
Epoch [14/120    avg_loss:0.181, val_acc:0.917]
Epoch [15/120    avg_loss:0.207, val_acc:0.910]
Epoch [16/120    avg_loss:0.141, val_acc:0.913]
Epoch [17/120    avg_loss:0.146, val_acc:0.938]
Epoch [18/120    avg_loss:0.132, val_acc:0.916]
Epoch [19/120    avg_loss:0.124, val_acc:0.942]
Epoch [20/120    avg_loss:0.084, val_acc:0.952]
Epoch [21/120    avg_loss:0.121, val_acc:0.937]
Epoch [22/120    avg_loss:0.144, val_acc:0.956]
Epoch [23/120    avg_loss:0.081, val_acc:0.936]
Epoch [24/120    avg_loss:0.111, val_acc:0.948]
Epoch [25/120    avg_loss:0.093, val_acc:0.930]
Epoch [26/120    avg_loss:0.080, val_acc:0.940]
Epoch [27/120    avg_loss:0.108, val_acc:0.818]
Epoch [28/120    avg_loss:0.140, val_acc:0.957]
Epoch [29/120    avg_loss:0.098, val_acc:0.965]
Epoch [30/120    avg_loss:0.061, val_acc:0.961]
Epoch [31/120    avg_loss:0.056, val_acc:0.969]
Epoch [32/120    avg_loss:0.061, val_acc:0.965]
Epoch [33/120    avg_loss:0.060, val_acc:0.959]
Epoch [34/120    avg_loss:0.042, val_acc:0.971]
Epoch [35/120    avg_loss:0.034, val_acc:0.951]
Epoch [36/120    avg_loss:0.036, val_acc:0.961]
Epoch [37/120    avg_loss:0.087, val_acc:0.960]
Epoch [38/120    avg_loss:0.062, val_acc:0.970]
Epoch [39/120    avg_loss:0.049, val_acc:0.955]
Epoch [40/120    avg_loss:0.059, val_acc:0.976]
Epoch [41/120    avg_loss:0.028, val_acc:0.980]
Epoch [42/120    avg_loss:0.045, val_acc:0.978]
Epoch [43/120    avg_loss:0.028, val_acc:0.982]
Epoch [44/120    avg_loss:0.021, val_acc:0.979]
Epoch [45/120    avg_loss:0.027, val_acc:0.969]
Epoch [46/120    avg_loss:0.018, val_acc:0.982]
Epoch [47/120    avg_loss:0.025, val_acc:0.989]
Epoch [48/120    avg_loss:0.016, val_acc:0.978]
Epoch [49/120    avg_loss:0.015, val_acc:0.986]
Epoch [50/120    avg_loss:0.023, val_acc:0.982]
Epoch [51/120    avg_loss:0.018, val_acc:0.987]
Epoch [52/120    avg_loss:0.020, val_acc:0.965]
Epoch [53/120    avg_loss:0.016, val_acc:0.977]
Epoch [54/120    avg_loss:0.012, val_acc:0.988]
Epoch [55/120    avg_loss:0.023, val_acc:0.944]
Epoch [56/120    avg_loss:0.037, val_acc:0.986]
Epoch [57/120    avg_loss:0.027, val_acc:0.985]
Epoch [58/120    avg_loss:0.082, val_acc:0.961]
Epoch [59/120    avg_loss:0.021, val_acc:0.978]
Epoch [60/120    avg_loss:0.028, val_acc:0.986]
Epoch [61/120    avg_loss:0.012, val_acc:0.986]
Epoch [62/120    avg_loss:0.013, val_acc:0.985]
Epoch [63/120    avg_loss:0.011, val_acc:0.987]
Epoch [64/120    avg_loss:0.009, val_acc:0.987]
Epoch [65/120    avg_loss:0.009, val_acc:0.987]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.987]
Epoch [68/120    avg_loss:0.009, val_acc:0.987]
Epoch [69/120    avg_loss:0.012, val_acc:0.986]
Epoch [70/120    avg_loss:0.012, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.009, val_acc:0.988]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.007, val_acc:0.988]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.006, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.009, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.011, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0    10     3    29     8]
 [    0    11 17950     0    83     0    45     0     1     0]
 [    0     4     0  1922     0     0     0     0   104     6]
 [    0    27     5     0  2896     0    17     0    24     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     9     0     0     0     0     1  1278     0     2]
 [    0    37     0    44    44     0     1     0  3445     0]
 [    0     0     0     0    14    67     0     0     0   838]]

Accuracy:
98.55638300436219

F1 scores:
[       nan 0.98930398 0.99597725 0.96051974 0.9638875  0.97497198
 0.99247202 0.99416569 0.9604126  0.94369369]

Kappa:
0.9808955074824952
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faff2584978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.831, val_acc:0.543]
Epoch [2/120    avg_loss:1.161, val_acc:0.622]
Epoch [3/120    avg_loss:0.821, val_acc:0.755]
Epoch [4/120    avg_loss:0.620, val_acc:0.796]
Epoch [5/120    avg_loss:0.483, val_acc:0.763]
Epoch [6/120    avg_loss:0.497, val_acc:0.773]
Epoch [7/120    avg_loss:0.449, val_acc:0.835]
Epoch [8/120    avg_loss:0.352, val_acc:0.803]
Epoch [9/120    avg_loss:0.363, val_acc:0.797]
Epoch [10/120    avg_loss:0.320, val_acc:0.800]
Epoch [11/120    avg_loss:0.327, val_acc:0.881]
Epoch [12/120    avg_loss:0.262, val_acc:0.910]
Epoch [13/120    avg_loss:0.214, val_acc:0.944]
Epoch [14/120    avg_loss:0.212, val_acc:0.900]
Epoch [15/120    avg_loss:0.212, val_acc:0.893]
Epoch [16/120    avg_loss:0.196, val_acc:0.861]
Epoch [17/120    avg_loss:0.173, val_acc:0.851]
Epoch [18/120    avg_loss:0.144, val_acc:0.795]
Epoch [19/120    avg_loss:0.132, val_acc:0.919]
Epoch [20/120    avg_loss:0.121, val_acc:0.938]
Epoch [21/120    avg_loss:0.102, val_acc:0.925]
Epoch [22/120    avg_loss:0.088, val_acc:0.895]
Epoch [23/120    avg_loss:0.087, val_acc:0.940]
Epoch [24/120    avg_loss:0.113, val_acc:0.909]
Epoch [25/120    avg_loss:0.110, val_acc:0.961]
Epoch [26/120    avg_loss:0.083, val_acc:0.955]
Epoch [27/120    avg_loss:0.088, val_acc:0.955]
Epoch [28/120    avg_loss:0.060, val_acc:0.964]
Epoch [29/120    avg_loss:0.060, val_acc:0.879]
Epoch [30/120    avg_loss:0.050, val_acc:0.954]
Epoch [31/120    avg_loss:0.066, val_acc:0.963]
Epoch [32/120    avg_loss:0.081, val_acc:0.932]
Epoch [33/120    avg_loss:0.055, val_acc:0.959]
Epoch [34/120    avg_loss:0.094, val_acc:0.956]
Epoch [35/120    avg_loss:0.071, val_acc:0.947]
Epoch [36/120    avg_loss:0.060, val_acc:0.949]
Epoch [37/120    avg_loss:0.094, val_acc:0.968]
Epoch [38/120    avg_loss:0.054, val_acc:0.952]
Epoch [39/120    avg_loss:0.050, val_acc:0.964]
Epoch [40/120    avg_loss:0.042, val_acc:0.977]
Epoch [41/120    avg_loss:0.038, val_acc:0.975]
Epoch [42/120    avg_loss:0.026, val_acc:0.976]
Epoch [43/120    avg_loss:0.041, val_acc:0.972]
Epoch [44/120    avg_loss:0.030, val_acc:0.973]
Epoch [45/120    avg_loss:0.037, val_acc:0.955]
Epoch [46/120    avg_loss:0.028, val_acc:0.977]
Epoch [47/120    avg_loss:0.032, val_acc:0.978]
Epoch [48/120    avg_loss:0.026, val_acc:0.971]
Epoch [49/120    avg_loss:0.025, val_acc:0.981]
Epoch [50/120    avg_loss:0.031, val_acc:0.974]
Epoch [51/120    avg_loss:0.030, val_acc:0.958]
Epoch [52/120    avg_loss:0.019, val_acc:0.982]
Epoch [53/120    avg_loss:0.035, val_acc:0.982]
Epoch [54/120    avg_loss:0.015, val_acc:0.982]
Epoch [55/120    avg_loss:0.016, val_acc:0.977]
Epoch [56/120    avg_loss:0.034, val_acc:0.969]
Epoch [57/120    avg_loss:0.027, val_acc:0.982]
Epoch [58/120    avg_loss:0.031, val_acc:0.960]
Epoch [59/120    avg_loss:0.014, val_acc:0.987]
Epoch [60/120    avg_loss:0.013, val_acc:0.983]
Epoch [61/120    avg_loss:0.015, val_acc:0.978]
Epoch [62/120    avg_loss:0.026, val_acc:0.874]
Epoch [63/120    avg_loss:0.142, val_acc:0.964]
Epoch [64/120    avg_loss:0.028, val_acc:0.977]
Epoch [65/120    avg_loss:0.023, val_acc:0.977]
Epoch [66/120    avg_loss:0.014, val_acc:0.976]
Epoch [67/120    avg_loss:0.016, val_acc:0.981]
Epoch [68/120    avg_loss:0.018, val_acc:0.923]
Epoch [69/120    avg_loss:0.027, val_acc:0.983]
Epoch [70/120    avg_loss:0.026, val_acc:0.964]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.014, val_acc:0.983]
Epoch [73/120    avg_loss:0.012, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.008, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.010, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     0     0     0    18     0    55     0]
 [    0     0 17992     0    60     0    38     0     0     0]
 [    0     0     0  1983     0     0     0     0    45     8]
 [    0    24     7     0  2915     0     9     0    15     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     1     0     0     0     0     2  1283     0     4]
 [    0     2     0     3    39     0     0     0  3512    15]
 [    0     0     0     0     5    27     4     0     0   883]]

Accuracy:
99.07213264888054

F1 scores:
[       nan 0.99219847 0.99709053 0.98607658 0.97312636 0.98976109
 0.99256997 0.99727944 0.97555556 0.96450027]

Kappa:
0.9877178705296904
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7bc27de908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.740, val_acc:0.339]
Epoch [2/120    avg_loss:1.077, val_acc:0.685]
Epoch [3/120    avg_loss:0.726, val_acc:0.709]
Epoch [4/120    avg_loss:0.587, val_acc:0.812]
Epoch [5/120    avg_loss:0.439, val_acc:0.854]
Epoch [6/120    avg_loss:0.404, val_acc:0.874]
Epoch [7/120    avg_loss:0.389, val_acc:0.863]
Epoch [8/120    avg_loss:0.350, val_acc:0.869]
Epoch [9/120    avg_loss:0.303, val_acc:0.808]
Epoch [10/120    avg_loss:0.280, val_acc:0.868]
Epoch [11/120    avg_loss:0.246, val_acc:0.868]
Epoch [12/120    avg_loss:0.188, val_acc:0.893]
Epoch [13/120    avg_loss:0.186, val_acc:0.891]
Epoch [14/120    avg_loss:0.214, val_acc:0.929]
Epoch [15/120    avg_loss:0.148, val_acc:0.932]
Epoch [16/120    avg_loss:0.606, val_acc:0.484]
Epoch [17/120    avg_loss:0.805, val_acc:0.718]
Epoch [18/120    avg_loss:0.607, val_acc:0.766]
Epoch [19/120    avg_loss:0.564, val_acc:0.804]
Epoch [20/120    avg_loss:0.475, val_acc:0.808]
Epoch [21/120    avg_loss:0.457, val_acc:0.783]
Epoch [22/120    avg_loss:0.426, val_acc:0.875]
Epoch [23/120    avg_loss:0.399, val_acc:0.774]
Epoch [24/120    avg_loss:0.359, val_acc:0.864]
Epoch [25/120    avg_loss:0.423, val_acc:0.792]
Epoch [26/120    avg_loss:0.301, val_acc:0.863]
Epoch [27/120    avg_loss:0.285, val_acc:0.715]
Epoch [28/120    avg_loss:0.324, val_acc:0.825]
Epoch [29/120    avg_loss:0.255, val_acc:0.877]
Epoch [30/120    avg_loss:0.217, val_acc:0.888]
Epoch [31/120    avg_loss:0.207, val_acc:0.890]
Epoch [32/120    avg_loss:0.225, val_acc:0.889]
Epoch [33/120    avg_loss:0.218, val_acc:0.898]
Epoch [34/120    avg_loss:0.205, val_acc:0.893]
Epoch [35/120    avg_loss:0.189, val_acc:0.895]
Epoch [36/120    avg_loss:0.190, val_acc:0.889]
Epoch [37/120    avg_loss:0.188, val_acc:0.900]
Epoch [38/120    avg_loss:0.181, val_acc:0.894]
Epoch [39/120    avg_loss:0.181, val_acc:0.899]
Epoch [40/120    avg_loss:0.193, val_acc:0.893]
Epoch [41/120    avg_loss:0.167, val_acc:0.898]
Epoch [42/120    avg_loss:0.177, val_acc:0.901]
Epoch [43/120    avg_loss:0.174, val_acc:0.900]
Epoch [44/120    avg_loss:0.166, val_acc:0.898]
Epoch [45/120    avg_loss:0.185, val_acc:0.899]
Epoch [46/120    avg_loss:0.166, val_acc:0.899]
Epoch [47/120    avg_loss:0.156, val_acc:0.899]
Epoch [48/120    avg_loss:0.173, val_acc:0.899]
Epoch [49/120    avg_loss:0.173, val_acc:0.900]
Epoch [50/120    avg_loss:0.173, val_acc:0.902]
Epoch [51/120    avg_loss:0.169, val_acc:0.899]
Epoch [52/120    avg_loss:0.171, val_acc:0.896]
Epoch [53/120    avg_loss:0.166, val_acc:0.899]
Epoch [54/120    avg_loss:0.154, val_acc:0.900]
Epoch [55/120    avg_loss:0.176, val_acc:0.901]
Epoch [56/120    avg_loss:0.168, val_acc:0.900]
Epoch [57/120    avg_loss:0.180, val_acc:0.900]
Epoch [58/120    avg_loss:0.153, val_acc:0.900]
Epoch [59/120    avg_loss:0.175, val_acc:0.900]
Epoch [60/120    avg_loss:0.160, val_acc:0.900]
Epoch [61/120    avg_loss:0.173, val_acc:0.900]
Epoch [62/120    avg_loss:0.167, val_acc:0.900]
Epoch [63/120    avg_loss:0.176, val_acc:0.899]
Epoch [64/120    avg_loss:0.178, val_acc:0.899]
Epoch [65/120    avg_loss:0.152, val_acc:0.899]
Epoch [66/120    avg_loss:0.175, val_acc:0.899]
Epoch [67/120    avg_loss:0.177, val_acc:0.899]
Epoch [68/120    avg_loss:0.182, val_acc:0.899]
Epoch [69/120    avg_loss:0.162, val_acc:0.899]
Epoch [70/120    avg_loss:0.175, val_acc:0.899]
Epoch [71/120    avg_loss:0.177, val_acc:0.899]
Epoch [72/120    avg_loss:0.161, val_acc:0.899]
Epoch [73/120    avg_loss:0.184, val_acc:0.899]
Epoch [74/120    avg_loss:0.172, val_acc:0.899]
Epoch [75/120    avg_loss:0.178, val_acc:0.899]
Epoch [76/120    avg_loss:0.158, val_acc:0.899]
Epoch [77/120    avg_loss:0.159, val_acc:0.899]
Epoch [78/120    avg_loss:0.168, val_acc:0.899]
Epoch [79/120    avg_loss:0.167, val_acc:0.899]
Epoch [80/120    avg_loss:0.157, val_acc:0.899]
Epoch [81/120    avg_loss:0.163, val_acc:0.899]
Epoch [82/120    avg_loss:0.161, val_acc:0.899]
Epoch [83/120    avg_loss:0.164, val_acc:0.899]
Epoch [84/120    avg_loss:0.167, val_acc:0.899]
Epoch [85/120    avg_loss:0.171, val_acc:0.899]
Epoch [86/120    avg_loss:0.158, val_acc:0.899]
Epoch [87/120    avg_loss:0.166, val_acc:0.899]
Epoch [88/120    avg_loss:0.177, val_acc:0.899]
Epoch [89/120    avg_loss:0.177, val_acc:0.899]
Epoch [90/120    avg_loss:0.173, val_acc:0.899]
Epoch [91/120    avg_loss:0.181, val_acc:0.899]
Epoch [92/120    avg_loss:0.160, val_acc:0.899]
Epoch [93/120    avg_loss:0.163, val_acc:0.899]
Epoch [94/120    avg_loss:0.151, val_acc:0.899]
Epoch [95/120    avg_loss:0.162, val_acc:0.899]
Epoch [96/120    avg_loss:0.170, val_acc:0.899]
Epoch [97/120    avg_loss:0.155, val_acc:0.899]
Epoch [98/120    avg_loss:0.158, val_acc:0.899]
Epoch [99/120    avg_loss:0.170, val_acc:0.899]
Epoch [100/120    avg_loss:0.161, val_acc:0.899]
Epoch [101/120    avg_loss:0.171, val_acc:0.899]
Epoch [102/120    avg_loss:0.167, val_acc:0.899]
Epoch [103/120    avg_loss:0.157, val_acc:0.899]
Epoch [104/120    avg_loss:0.169, val_acc:0.899]
Epoch [105/120    avg_loss:0.189, val_acc:0.899]
Epoch [106/120    avg_loss:0.166, val_acc:0.899]
Epoch [107/120    avg_loss:0.161, val_acc:0.899]
Epoch [108/120    avg_loss:0.173, val_acc:0.899]
Epoch [109/120    avg_loss:0.167, val_acc:0.899]
Epoch [110/120    avg_loss:0.176, val_acc:0.899]
Epoch [111/120    avg_loss:0.175, val_acc:0.899]
Epoch [112/120    avg_loss:0.162, val_acc:0.899]
Epoch [113/120    avg_loss:0.165, val_acc:0.899]
Epoch [114/120    avg_loss:0.170, val_acc:0.899]
Epoch [115/120    avg_loss:0.158, val_acc:0.899]
Epoch [116/120    avg_loss:0.159, val_acc:0.899]
Epoch [117/120    avg_loss:0.166, val_acc:0.899]
Epoch [118/120    avg_loss:0.152, val_acc:0.899]
Epoch [119/120    avg_loss:0.173, val_acc:0.899]
Epoch [120/120    avg_loss:0.181, val_acc:0.899]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5963     6    15    61     0    65    19   243    60]
 [    0     0 15891     0    56     0  2140     0     3     0]
 [    0     4     0  1946     0     0     0     0    59    27]
 [    0   111   118     0  2638     0    70     0    31     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   156     2     0     0  4681     0    39     0]
 [    0    68     0     0     0     0     4  1216     1     1]
 [    0    70     0     4    49     0     6     0  3441     1]
 [    0    23     0     1    13    49     0     0     1   832]]

Accuracy:
91.37203865712289

F1 scores:
[       nan 0.94120432 0.92764368 0.97202797 0.91138366 0.98157202
 0.79044242 0.96316832 0.93138449 0.90238612]

Kappa:
0.8878739420263005
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff990d0f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.756, val_acc:0.447]
Epoch [2/120    avg_loss:1.085, val_acc:0.705]
Epoch [3/120    avg_loss:0.777, val_acc:0.687]
Epoch [4/120    avg_loss:0.560, val_acc:0.762]
Epoch [5/120    avg_loss:0.420, val_acc:0.789]
Epoch [6/120    avg_loss:0.430, val_acc:0.773]
Epoch [7/120    avg_loss:0.404, val_acc:0.759]
Epoch [8/120    avg_loss:0.310, val_acc:0.871]
Epoch [9/120    avg_loss:0.271, val_acc:0.843]
Epoch [10/120    avg_loss:0.214, val_acc:0.908]
Epoch [11/120    avg_loss:0.264, val_acc:0.879]
Epoch [12/120    avg_loss:0.188, val_acc:0.894]
Epoch [13/120    avg_loss:0.207, val_acc:0.897]
Epoch [14/120    avg_loss:0.194, val_acc:0.893]
Epoch [15/120    avg_loss:0.155, val_acc:0.899]
Epoch [16/120    avg_loss:0.615, val_acc:0.845]
Epoch [17/120    avg_loss:0.320, val_acc:0.841]
Epoch [18/120    avg_loss:0.261, val_acc:0.782]
Epoch [19/120    avg_loss:0.461, val_acc:0.213]
Epoch [20/120    avg_loss:1.327, val_acc:0.629]
Epoch [21/120    avg_loss:0.998, val_acc:0.650]
Epoch [22/120    avg_loss:0.793, val_acc:0.744]
Epoch [23/120    avg_loss:0.730, val_acc:0.759]
Epoch [24/120    avg_loss:0.569, val_acc:0.745]
Epoch [25/120    avg_loss:0.571, val_acc:0.770]
Epoch [26/120    avg_loss:0.561, val_acc:0.781]
Epoch [27/120    avg_loss:0.522, val_acc:0.801]
Epoch [28/120    avg_loss:0.533, val_acc:0.794]
Epoch [29/120    avg_loss:0.549, val_acc:0.786]
Epoch [30/120    avg_loss:0.507, val_acc:0.808]
Epoch [31/120    avg_loss:0.496, val_acc:0.807]
Epoch [32/120    avg_loss:0.499, val_acc:0.808]
Epoch [33/120    avg_loss:0.488, val_acc:0.815]
Epoch [34/120    avg_loss:0.468, val_acc:0.806]
Epoch [35/120    avg_loss:0.468, val_acc:0.812]
Epoch [36/120    avg_loss:0.465, val_acc:0.830]
Epoch [37/120    avg_loss:0.470, val_acc:0.830]
Epoch [38/120    avg_loss:0.453, val_acc:0.836]
Epoch [39/120    avg_loss:0.445, val_acc:0.834]
Epoch [40/120    avg_loss:0.445, val_acc:0.831]
Epoch [41/120    avg_loss:0.423, val_acc:0.826]
Epoch [42/120    avg_loss:0.429, val_acc:0.826]
Epoch [43/120    avg_loss:0.471, val_acc:0.826]
Epoch [44/120    avg_loss:0.449, val_acc:0.831]
Epoch [45/120    avg_loss:0.424, val_acc:0.832]
Epoch [46/120    avg_loss:0.462, val_acc:0.833]
Epoch [47/120    avg_loss:0.432, val_acc:0.833]
Epoch [48/120    avg_loss:0.441, val_acc:0.831]
Epoch [49/120    avg_loss:0.433, val_acc:0.835]
Epoch [50/120    avg_loss:0.442, val_acc:0.834]
Epoch [51/120    avg_loss:0.416, val_acc:0.832]
Epoch [52/120    avg_loss:0.442, val_acc:0.831]
Epoch [53/120    avg_loss:0.426, val_acc:0.832]
Epoch [54/120    avg_loss:0.438, val_acc:0.831]
Epoch [55/120    avg_loss:0.437, val_acc:0.831]
Epoch [56/120    avg_loss:0.444, val_acc:0.831]
Epoch [57/120    avg_loss:0.479, val_acc:0.829]
Epoch [58/120    avg_loss:0.453, val_acc:0.830]
Epoch [59/120    avg_loss:0.457, val_acc:0.830]
Epoch [60/120    avg_loss:0.454, val_acc:0.829]
Epoch [61/120    avg_loss:0.435, val_acc:0.828]
Epoch [62/120    avg_loss:0.449, val_acc:0.829]
Epoch [63/120    avg_loss:0.421, val_acc:0.829]
Epoch [64/120    avg_loss:0.460, val_acc:0.829]
Epoch [65/120    avg_loss:0.431, val_acc:0.829]
Epoch [66/120    avg_loss:0.449, val_acc:0.829]
Epoch [67/120    avg_loss:0.450, val_acc:0.829]
Epoch [68/120    avg_loss:0.434, val_acc:0.829]
Epoch [69/120    avg_loss:0.434, val_acc:0.829]
Epoch [70/120    avg_loss:0.460, val_acc:0.828]
Epoch [71/120    avg_loss:0.430, val_acc:0.828]
Epoch [72/120    avg_loss:0.452, val_acc:0.828]
Epoch [73/120    avg_loss:0.428, val_acc:0.827]
Epoch [74/120    avg_loss:0.441, val_acc:0.828]
Epoch [75/120    avg_loss:0.445, val_acc:0.827]
Epoch [76/120    avg_loss:0.440, val_acc:0.827]
Epoch [77/120    avg_loss:0.412, val_acc:0.827]
Epoch [78/120    avg_loss:0.421, val_acc:0.827]
Epoch [79/120    avg_loss:0.453, val_acc:0.827]
Epoch [80/120    avg_loss:0.443, val_acc:0.827]
Epoch [81/120    avg_loss:0.451, val_acc:0.827]
Epoch [82/120    avg_loss:0.421, val_acc:0.827]
Epoch [83/120    avg_loss:0.422, val_acc:0.827]
Epoch [84/120    avg_loss:0.444, val_acc:0.827]
Epoch [85/120    avg_loss:0.428, val_acc:0.827]
Epoch [86/120    avg_loss:0.422, val_acc:0.827]
Epoch [87/120    avg_loss:0.448, val_acc:0.827]
Epoch [88/120    avg_loss:0.432, val_acc:0.827]
Epoch [89/120    avg_loss:0.439, val_acc:0.827]
Epoch [90/120    avg_loss:0.455, val_acc:0.827]
Epoch [91/120    avg_loss:0.461, val_acc:0.827]
Epoch [92/120    avg_loss:0.423, val_acc:0.827]
Epoch [93/120    avg_loss:0.432, val_acc:0.827]
Epoch [94/120    avg_loss:0.476, val_acc:0.827]
Epoch [95/120    avg_loss:0.430, val_acc:0.827]
Epoch [96/120    avg_loss:0.439, val_acc:0.827]
Epoch [97/120    avg_loss:0.440, val_acc:0.827]
Epoch [98/120    avg_loss:0.425, val_acc:0.827]
Epoch [99/120    avg_loss:0.440, val_acc:0.827]
Epoch [100/120    avg_loss:0.438, val_acc:0.827]
Epoch [101/120    avg_loss:0.460, val_acc:0.827]
Epoch [102/120    avg_loss:0.439, val_acc:0.827]
Epoch [103/120    avg_loss:0.450, val_acc:0.827]
Epoch [104/120    avg_loss:0.437, val_acc:0.827]
Epoch [105/120    avg_loss:0.426, val_acc:0.827]
Epoch [106/120    avg_loss:0.426, val_acc:0.827]
Epoch [107/120    avg_loss:0.461, val_acc:0.827]
Epoch [108/120    avg_loss:0.437, val_acc:0.827]
Epoch [109/120    avg_loss:0.424, val_acc:0.827]
Epoch [110/120    avg_loss:0.443, val_acc:0.827]
Epoch [111/120    avg_loss:0.454, val_acc:0.827]
Epoch [112/120    avg_loss:0.448, val_acc:0.827]
Epoch [113/120    avg_loss:0.445, val_acc:0.827]
Epoch [114/120    avg_loss:0.425, val_acc:0.827]
Epoch [115/120    avg_loss:0.431, val_acc:0.827]
Epoch [116/120    avg_loss:0.443, val_acc:0.827]
Epoch [117/120    avg_loss:0.432, val_acc:0.827]
Epoch [118/120    avg_loss:0.437, val_acc:0.827]
Epoch [119/120    avg_loss:0.424, val_acc:0.827]
Epoch [120/120    avg_loss:0.445, val_acc:0.827]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5471     0     0   119     0   164    58   547    73]
 [    0     2 15479     0     0     0  2609     0     0     0]
 [    0    53     0  1560     6     0     0    25   376    16]
 [    0    80   152     0  2541     0   143     4    52     0]
 [    0     0     0     3     0  1302     0     0     0     0]
 [    0     0   745     0     4     0  3983     0   146     0]
 [    0    39     0     0     5     0    14  1231     1     0]
 [    0   166     0    56    36     0   143     5  3165     0]
 [    0    63     0     0    14    68     0     0     1   773]]

Accuracy:
85.5686501337575

F1 scores:
[       nan 0.88915976 0.89821853 0.85362517 0.89204845 0.97345794
 0.66750461 0.94221202 0.80544599 0.86805166]

Kappa:
0.8120904504300119
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5834720940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.765, val_acc:0.642]
Epoch [2/120    avg_loss:1.184, val_acc:0.719]
Epoch [3/120    avg_loss:0.876, val_acc:0.720]
Epoch [4/120    avg_loss:0.616, val_acc:0.744]
Epoch [5/120    avg_loss:0.540, val_acc:0.819]
Epoch [6/120    avg_loss:0.435, val_acc:0.832]
Epoch [7/120    avg_loss:0.359, val_acc:0.831]
Epoch [8/120    avg_loss:0.346, val_acc:0.857]
Epoch [9/120    avg_loss:0.314, val_acc:0.864]
Epoch [10/120    avg_loss:0.315, val_acc:0.895]
Epoch [11/120    avg_loss:0.367, val_acc:0.866]
Epoch [12/120    avg_loss:0.248, val_acc:0.900]
Epoch [13/120    avg_loss:0.199, val_acc:0.908]
Epoch [14/120    avg_loss:0.182, val_acc:0.935]
Epoch [15/120    avg_loss:0.121, val_acc:0.918]
Epoch [16/120    avg_loss:0.144, val_acc:0.902]
Epoch [17/120    avg_loss:0.144, val_acc:0.929]
Epoch [18/120    avg_loss:0.136, val_acc:0.941]
Epoch [19/120    avg_loss:0.154, val_acc:0.923]
Epoch [20/120    avg_loss:0.114, val_acc:0.923]
Epoch [21/120    avg_loss:0.127, val_acc:0.942]
Epoch [22/120    avg_loss:0.128, val_acc:0.935]
Epoch [23/120    avg_loss:0.107, val_acc:0.923]
Epoch [24/120    avg_loss:0.094, val_acc:0.932]
Epoch [25/120    avg_loss:0.073, val_acc:0.960]
Epoch [26/120    avg_loss:0.077, val_acc:0.958]
Epoch [27/120    avg_loss:0.066, val_acc:0.965]
Epoch [28/120    avg_loss:0.030, val_acc:0.968]
Epoch [29/120    avg_loss:0.042, val_acc:0.963]
Epoch [30/120    avg_loss:0.038, val_acc:0.959]
Epoch [31/120    avg_loss:0.078, val_acc:0.884]
Epoch [32/120    avg_loss:0.061, val_acc:0.963]
Epoch [33/120    avg_loss:0.042, val_acc:0.963]
Epoch [34/120    avg_loss:0.039, val_acc:0.938]
Epoch [35/120    avg_loss:0.058, val_acc:0.969]
Epoch [36/120    avg_loss:0.051, val_acc:0.957]
Epoch [37/120    avg_loss:0.033, val_acc:0.977]
Epoch [38/120    avg_loss:0.084, val_acc:0.969]
Epoch [39/120    avg_loss:0.035, val_acc:0.966]
Epoch [40/120    avg_loss:0.033, val_acc:0.971]
Epoch [41/120    avg_loss:0.024, val_acc:0.975]
Epoch [42/120    avg_loss:0.017, val_acc:0.974]
Epoch [43/120    avg_loss:0.021, val_acc:0.974]
Epoch [44/120    avg_loss:0.036, val_acc:0.982]
Epoch [45/120    avg_loss:0.034, val_acc:0.958]
Epoch [46/120    avg_loss:0.026, val_acc:0.980]
Epoch [47/120    avg_loss:0.017, val_acc:0.970]
Epoch [48/120    avg_loss:0.097, val_acc:0.969]
Epoch [49/120    avg_loss:0.041, val_acc:0.960]
Epoch [50/120    avg_loss:0.028, val_acc:0.980]
Epoch [51/120    avg_loss:0.056, val_acc:0.981]
Epoch [52/120    avg_loss:0.030, val_acc:0.971]
Epoch [53/120    avg_loss:0.021, val_acc:0.976]
Epoch [54/120    avg_loss:0.021, val_acc:0.981]
Epoch [55/120    avg_loss:0.024, val_acc:0.985]
Epoch [56/120    avg_loss:0.027, val_acc:0.973]
Epoch [57/120    avg_loss:0.010, val_acc:0.985]
Epoch [58/120    avg_loss:0.014, val_acc:0.984]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.024, val_acc:0.972]
Epoch [61/120    avg_loss:0.021, val_acc:0.983]
Epoch [62/120    avg_loss:0.010, val_acc:0.986]
Epoch [63/120    avg_loss:0.016, val_acc:0.987]
Epoch [64/120    avg_loss:0.008, val_acc:0.987]
Epoch [65/120    avg_loss:0.006, val_acc:0.988]
Epoch [66/120    avg_loss:0.012, val_acc:0.978]
Epoch [67/120    avg_loss:0.006, val_acc:0.988]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.008, val_acc:0.988]
Epoch [70/120    avg_loss:0.016, val_acc:0.990]
Epoch [71/120    avg_loss:0.008, val_acc:0.990]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.004, val_acc:0.989]
Epoch [74/120    avg_loss:0.005, val_acc:0.987]
Epoch [75/120    avg_loss:0.004, val_acc:0.989]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.004, val_acc:0.989]
Epoch [80/120    avg_loss:0.004, val_acc:0.990]
Epoch [81/120    avg_loss:0.007, val_acc:0.987]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.986]
Epoch [84/120    avg_loss:0.013, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.333, val_acc:0.860]
Epoch [88/120    avg_loss:0.178, val_acc:0.950]
Epoch [89/120    avg_loss:0.082, val_acc:0.973]
Epoch [90/120    avg_loss:0.028, val_acc:0.959]
Epoch [91/120    avg_loss:0.043, val_acc:0.969]
Epoch [92/120    avg_loss:0.017, val_acc:0.991]
Epoch [93/120    avg_loss:0.012, val_acc:0.990]
Epoch [94/120    avg_loss:0.014, val_acc:0.989]
Epoch [95/120    avg_loss:0.013, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.993]
Epoch [97/120    avg_loss:0.017, val_acc:0.918]
Epoch [98/120    avg_loss:0.014, val_acc:0.991]
Epoch [99/120    avg_loss:0.005, val_acc:0.993]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.995]
Epoch [103/120    avg_loss:0.004, val_acc:0.993]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.994]
Epoch [107/120    avg_loss:0.004, val_acc:0.993]
Epoch [108/120    avg_loss:0.006, val_acc:0.993]
Epoch [109/120    avg_loss:0.004, val_acc:0.994]
Epoch [110/120    avg_loss:0.008, val_acc:0.994]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.005, val_acc:0.995]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.003, val_acc:0.993]
Epoch [115/120    avg_loss:0.002, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.002, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6296     0     0     1     0    50     6    75     4]
 [    0     0 18057     0    14     0    15     0     4     0]
 [    0     2     0  2006     0     0     0     0    24     4]
 [    0    15     3     0  2914     0    17     0    21     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     1     0]
 [    0     0     0     0     0     0     1  1286     0     3]
 [    0     0     5    10    37     0     0     0  3517     2]
 [    0     0     0     0    13    33     0     0     0   873]]

Accuracy:
99.12756368544092

F1 scores:
[       nan 0.98799529 0.99886599 0.99012833 0.9793312  0.98751419
 0.99146168 0.99612703 0.9751837  0.96624239]

Kappa:
0.9884447333222778
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f63997908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.837, val_acc:0.292]
Epoch [2/120    avg_loss:1.222, val_acc:0.620]
Epoch [3/120    avg_loss:0.881, val_acc:0.620]
Epoch [4/120    avg_loss:0.721, val_acc:0.764]
Epoch [5/120    avg_loss:0.577, val_acc:0.747]
Epoch [6/120    avg_loss:0.477, val_acc:0.734]
Epoch [7/120    avg_loss:0.407, val_acc:0.776]
Epoch [8/120    avg_loss:0.385, val_acc:0.790]
Epoch [9/120    avg_loss:0.339, val_acc:0.843]
Epoch [10/120    avg_loss:0.310, val_acc:0.760]
Epoch [11/120    avg_loss:0.263, val_acc:0.890]
Epoch [12/120    avg_loss:0.220, val_acc:0.872]
Epoch [13/120    avg_loss:0.196, val_acc:0.839]
Epoch [14/120    avg_loss:0.152, val_acc:0.921]
Epoch [15/120    avg_loss:0.186, val_acc:0.890]
Epoch [16/120    avg_loss:0.174, val_acc:0.903]
Epoch [17/120    avg_loss:0.256, val_acc:0.883]
Epoch [18/120    avg_loss:0.177, val_acc:0.892]
Epoch [19/120    avg_loss:0.122, val_acc:0.951]
Epoch [20/120    avg_loss:0.096, val_acc:0.954]
Epoch [21/120    avg_loss:0.096, val_acc:0.931]
Epoch [22/120    avg_loss:0.120, val_acc:0.902]
Epoch [23/120    avg_loss:0.082, val_acc:0.969]
Epoch [24/120    avg_loss:0.080, val_acc:0.951]
Epoch [25/120    avg_loss:0.090, val_acc:0.963]
Epoch [26/120    avg_loss:0.059, val_acc:0.967]
Epoch [27/120    avg_loss:0.051, val_acc:0.972]
Epoch [28/120    avg_loss:0.065, val_acc:0.950]
Epoch [29/120    avg_loss:0.051, val_acc:0.979]
Epoch [30/120    avg_loss:0.070, val_acc:0.974]
Epoch [31/120    avg_loss:0.082, val_acc:0.954]
Epoch [32/120    avg_loss:0.060, val_acc:0.985]
Epoch [33/120    avg_loss:0.034, val_acc:0.978]
Epoch [34/120    avg_loss:0.038, val_acc:0.976]
Epoch [35/120    avg_loss:0.040, val_acc:0.978]
Epoch [36/120    avg_loss:0.188, val_acc:0.942]
Epoch [37/120    avg_loss:0.070, val_acc:0.970]
Epoch [38/120    avg_loss:0.052, val_acc:0.961]
Epoch [39/120    avg_loss:0.029, val_acc:0.983]
Epoch [40/120    avg_loss:0.019, val_acc:0.980]
Epoch [41/120    avg_loss:0.056, val_acc:0.945]
Epoch [42/120    avg_loss:0.127, val_acc:0.954]
Epoch [43/120    avg_loss:0.066, val_acc:0.970]
Epoch [44/120    avg_loss:0.046, val_acc:0.978]
Epoch [45/120    avg_loss:0.024, val_acc:0.986]
Epoch [46/120    avg_loss:0.026, val_acc:0.965]
Epoch [47/120    avg_loss:0.021, val_acc:0.982]
Epoch [48/120    avg_loss:0.023, val_acc:0.984]
Epoch [49/120    avg_loss:0.019, val_acc:0.981]
Epoch [50/120    avg_loss:0.017, val_acc:0.940]
Epoch [51/120    avg_loss:0.026, val_acc:0.979]
Epoch [52/120    avg_loss:0.017, val_acc:0.983]
Epoch [53/120    avg_loss:0.014, val_acc:0.985]
Epoch [54/120    avg_loss:0.009, val_acc:0.986]
Epoch [55/120    avg_loss:0.013, val_acc:0.983]
Epoch [56/120    avg_loss:0.017, val_acc:0.984]
Epoch [57/120    avg_loss:0.015, val_acc:0.970]
Epoch [58/120    avg_loss:0.040, val_acc:0.985]
Epoch [59/120    avg_loss:0.014, val_acc:0.986]
Epoch [60/120    avg_loss:0.015, val_acc:0.987]
Epoch [61/120    avg_loss:0.012, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.987]
Epoch [63/120    avg_loss:0.012, val_acc:0.988]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.987]
Epoch [66/120    avg_loss:0.010, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.988]
Epoch [68/120    avg_loss:0.009, val_acc:0.989]
Epoch [69/120    avg_loss:0.007, val_acc:0.989]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.006, val_acc:0.989]
Epoch [72/120    avg_loss:0.006, val_acc:0.989]
Epoch [73/120    avg_loss:0.006, val_acc:0.989]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.008, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.010, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.008, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.991]
Epoch [98/120    avg_loss:0.006, val_acc:0.991]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.004, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0    13     0     0    12     2    34     0]
 [    0     0 18069     0     7     0    14     0     0     0]
 [    0     2     0  2013     0     0     0     0    18     3]
 [    0    23     0     0  2942     0     0     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15    18     0     0  4842     0     3     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     8     0    26    70     0     0     0  3458     9]
 [    0     0     0     0    14    52     0     0     0   853]]

Accuracy:
99.15166413611935

F1 scores:
[       nan 0.99267685 0.99900481 0.98051632 0.97985012 0.98046582
 0.99363842 0.99844961 0.97532083 0.95520717]

Kappa:
0.988761190733667
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e7b43d978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.774, val_acc:0.615]
Epoch [2/120    avg_loss:1.160, val_acc:0.683]
Epoch [3/120    avg_loss:0.832, val_acc:0.786]
Epoch [4/120    avg_loss:0.554, val_acc:0.793]
Epoch [5/120    avg_loss:0.488, val_acc:0.858]
Epoch [6/120    avg_loss:0.470, val_acc:0.820]
Epoch [7/120    avg_loss:0.369, val_acc:0.815]
Epoch [8/120    avg_loss:0.362, val_acc:0.854]
Epoch [9/120    avg_loss:0.304, val_acc:0.830]
Epoch [10/120    avg_loss:0.379, val_acc:0.794]
Epoch [11/120    avg_loss:0.410, val_acc:0.813]
Epoch [12/120    avg_loss:0.315, val_acc:0.831]
Epoch [13/120    avg_loss:0.257, val_acc:0.884]
Epoch [14/120    avg_loss:0.239, val_acc:0.865]
Epoch [15/120    avg_loss:0.179, val_acc:0.927]
Epoch [16/120    avg_loss:0.180, val_acc:0.936]
Epoch [17/120    avg_loss:0.129, val_acc:0.832]
Epoch [18/120    avg_loss:0.129, val_acc:0.943]
Epoch [19/120    avg_loss:0.122, val_acc:0.943]
Epoch [20/120    avg_loss:0.101, val_acc:0.942]
Epoch [21/120    avg_loss:0.102, val_acc:0.946]
Epoch [22/120    avg_loss:0.131, val_acc:0.958]
Epoch [23/120    avg_loss:0.123, val_acc:0.953]
Epoch [24/120    avg_loss:0.059, val_acc:0.955]
Epoch [25/120    avg_loss:0.138, val_acc:0.945]
Epoch [26/120    avg_loss:0.065, val_acc:0.955]
Epoch [27/120    avg_loss:0.076, val_acc:0.956]
Epoch [28/120    avg_loss:0.073, val_acc:0.950]
Epoch [29/120    avg_loss:0.049, val_acc:0.964]
Epoch [30/120    avg_loss:0.044, val_acc:0.935]
Epoch [31/120    avg_loss:0.112, val_acc:0.971]
Epoch [32/120    avg_loss:0.042, val_acc:0.978]
Epoch [33/120    avg_loss:0.061, val_acc:0.981]
Epoch [34/120    avg_loss:0.220, val_acc:0.961]
Epoch [35/120    avg_loss:0.153, val_acc:0.947]
Epoch [36/120    avg_loss:0.097, val_acc:0.947]
Epoch [37/120    avg_loss:0.039, val_acc:0.980]
Epoch [38/120    avg_loss:0.044, val_acc:0.950]
Epoch [39/120    avg_loss:0.045, val_acc:0.973]
Epoch [40/120    avg_loss:0.042, val_acc:0.977]
Epoch [41/120    avg_loss:0.034, val_acc:0.985]
Epoch [42/120    avg_loss:0.027, val_acc:0.977]
Epoch [43/120    avg_loss:0.024, val_acc:0.979]
Epoch [44/120    avg_loss:0.022, val_acc:0.961]
Epoch [45/120    avg_loss:0.020, val_acc:0.983]
Epoch [46/120    avg_loss:0.019, val_acc:0.985]
Epoch [47/120    avg_loss:0.028, val_acc:0.979]
Epoch [48/120    avg_loss:0.033, val_acc:0.964]
Epoch [49/120    avg_loss:0.024, val_acc:0.978]
Epoch [50/120    avg_loss:0.026, val_acc:0.953]
Epoch [51/120    avg_loss:0.022, val_acc:0.979]
Epoch [52/120    avg_loss:0.020, val_acc:0.980]
Epoch [53/120    avg_loss:0.014, val_acc:0.987]
Epoch [54/120    avg_loss:0.014, val_acc:0.985]
Epoch [55/120    avg_loss:0.009, val_acc:0.987]
Epoch [56/120    avg_loss:0.033, val_acc:0.978]
Epoch [57/120    avg_loss:0.045, val_acc:0.975]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.008, val_acc:0.985]
Epoch [60/120    avg_loss:0.014, val_acc:0.961]
Epoch [61/120    avg_loss:0.009, val_acc:0.991]
Epoch [62/120    avg_loss:0.006, val_acc:0.985]
Epoch [63/120    avg_loss:0.012, val_acc:0.987]
Epoch [64/120    avg_loss:0.009, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.006, val_acc:0.990]
Epoch [67/120    avg_loss:0.007, val_acc:0.988]
Epoch [68/120    avg_loss:0.005, val_acc:0.988]
Epoch [69/120    avg_loss:0.005, val_acc:0.987]
Epoch [70/120    avg_loss:0.007, val_acc:0.991]
Epoch [71/120    avg_loss:0.004, val_acc:0.986]
Epoch [72/120    avg_loss:0.005, val_acc:0.987]
Epoch [73/120    avg_loss:0.005, val_acc:0.988]
Epoch [74/120    avg_loss:0.005, val_acc:0.989]
Epoch [75/120    avg_loss:0.004, val_acc:0.987]
Epoch [76/120    avg_loss:0.004, val_acc:0.988]
Epoch [77/120    avg_loss:0.004, val_acc:0.988]
Epoch [78/120    avg_loss:0.005, val_acc:0.987]
Epoch [79/120    avg_loss:0.004, val_acc:0.987]
Epoch [80/120    avg_loss:0.004, val_acc:0.988]
Epoch [81/120    avg_loss:0.004, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.004, val_acc:0.989]
Epoch [84/120    avg_loss:0.003, val_acc:0.988]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.003, val_acc:0.988]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.003, val_acc:0.989]
Epoch [91/120    avg_loss:0.004, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.003, val_acc:0.989]
Epoch [94/120    avg_loss:0.003, val_acc:0.989]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.003, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.989]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.003, val_acc:0.989]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     1     1     0    11     0    27     0]
 [    0     0 18068     0    11     0    10     0     1     0]
 [    0     1     0  2024     1     0     0     0     4     6]
 [    0    19    12     0  2913     0     3     0    23     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2    12     0     0  4861     0     3     0]
 [    0     1     0     0     0     0     3  1285     0     1]
 [    0     1     0    36    64     0     0     0  3443    27]
 [    0     0     0     0    16    40     0     0     0   863]]

Accuracy:
99.1829947220013

F1 scores:
[       nan 0.99517359 0.99900476 0.98515454 0.97457344 0.98490566
 0.99549457 0.99805825 0.9736991  0.94939494]

Kappa:
0.9891763352440837
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c85500940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.816, val_acc:0.602]
Epoch [2/120    avg_loss:1.127, val_acc:0.715]
Epoch [3/120    avg_loss:0.826, val_acc:0.716]
Epoch [4/120    avg_loss:0.623, val_acc:0.786]
Epoch [5/120    avg_loss:0.506, val_acc:0.749]
Epoch [6/120    avg_loss:0.423, val_acc:0.827]
Epoch [7/120    avg_loss:0.382, val_acc:0.867]
Epoch [8/120    avg_loss:0.312, val_acc:0.800]
Epoch [9/120    avg_loss:0.323, val_acc:0.889]
Epoch [10/120    avg_loss:0.283, val_acc:0.867]
Epoch [11/120    avg_loss:0.230, val_acc:0.891]
Epoch [12/120    avg_loss:0.231, val_acc:0.872]
Epoch [13/120    avg_loss:0.195, val_acc:0.925]
Epoch [14/120    avg_loss:0.161, val_acc:0.944]
Epoch [15/120    avg_loss:0.150, val_acc:0.938]
Epoch [16/120    avg_loss:0.136, val_acc:0.855]
Epoch [17/120    avg_loss:0.146, val_acc:0.942]
Epoch [18/120    avg_loss:0.156, val_acc:0.937]
Epoch [19/120    avg_loss:0.212, val_acc:0.944]
Epoch [20/120    avg_loss:0.115, val_acc:0.966]
Epoch [21/120    avg_loss:0.135, val_acc:0.943]
Epoch [22/120    avg_loss:0.080, val_acc:0.963]
Epoch [23/120    avg_loss:0.099, val_acc:0.931]
Epoch [24/120    avg_loss:0.305, val_acc:0.912]
Epoch [25/120    avg_loss:0.110, val_acc:0.962]
Epoch [26/120    avg_loss:0.097, val_acc:0.961]
Epoch [27/120    avg_loss:0.098, val_acc:0.969]
Epoch [28/120    avg_loss:0.076, val_acc:0.948]
Epoch [29/120    avg_loss:0.063, val_acc:0.963]
Epoch [30/120    avg_loss:0.071, val_acc:0.970]
Epoch [31/120    avg_loss:0.055, val_acc:0.976]
Epoch [32/120    avg_loss:0.034, val_acc:0.974]
Epoch [33/120    avg_loss:0.041, val_acc:0.979]
Epoch [34/120    avg_loss:0.030, val_acc:0.970]
Epoch [35/120    avg_loss:0.032, val_acc:0.984]
Epoch [36/120    avg_loss:0.031, val_acc:0.969]
Epoch [37/120    avg_loss:0.046, val_acc:0.975]
Epoch [38/120    avg_loss:0.056, val_acc:0.964]
Epoch [39/120    avg_loss:0.038, val_acc:0.980]
Epoch [40/120    avg_loss:0.026, val_acc:0.963]
Epoch [41/120    avg_loss:0.035, val_acc:0.965]
Epoch [42/120    avg_loss:0.029, val_acc:0.979]
Epoch [43/120    avg_loss:0.017, val_acc:0.983]
Epoch [44/120    avg_loss:0.041, val_acc:0.920]
Epoch [45/120    avg_loss:0.042, val_acc:0.980]
Epoch [46/120    avg_loss:0.028, val_acc:0.985]
Epoch [47/120    avg_loss:0.017, val_acc:0.988]
Epoch [48/120    avg_loss:0.026, val_acc:0.988]
Epoch [49/120    avg_loss:0.058, val_acc:0.986]
Epoch [50/120    avg_loss:0.027, val_acc:0.985]
Epoch [51/120    avg_loss:0.012, val_acc:0.988]
Epoch [52/120    avg_loss:0.019, val_acc:0.979]
Epoch [53/120    avg_loss:0.012, val_acc:0.991]
Epoch [54/120    avg_loss:0.010, val_acc:0.991]
Epoch [55/120    avg_loss:0.013, val_acc:0.988]
Epoch [56/120    avg_loss:0.012, val_acc:0.990]
Epoch [57/120    avg_loss:0.032, val_acc:0.985]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.015, val_acc:0.980]
Epoch [60/120    avg_loss:0.010, val_acc:0.987]
Epoch [61/120    avg_loss:0.009, val_acc:0.991]
Epoch [62/120    avg_loss:0.009, val_acc:0.991]
Epoch [63/120    avg_loss:0.064, val_acc:0.980]
Epoch [64/120    avg_loss:0.022, val_acc:0.987]
Epoch [65/120    avg_loss:0.011, val_acc:0.989]
Epoch [66/120    avg_loss:0.019, val_acc:0.979]
Epoch [67/120    avg_loss:0.010, val_acc:0.989]
Epoch [68/120    avg_loss:0.054, val_acc:0.989]
Epoch [69/120    avg_loss:0.021, val_acc:0.984]
Epoch [70/120    avg_loss:0.014, val_acc:0.990]
Epoch [71/120    avg_loss:0.040, val_acc:0.969]
Epoch [72/120    avg_loss:0.025, val_acc:0.979]
Epoch [73/120    avg_loss:0.016, val_acc:0.976]
Epoch [74/120    avg_loss:0.013, val_acc:0.989]
Epoch [75/120    avg_loss:0.013, val_acc:0.990]
Epoch [76/120    avg_loss:0.011, val_acc:0.991]
Epoch [77/120    avg_loss:0.007, val_acc:0.991]
Epoch [78/120    avg_loss:0.007, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.991]
Epoch [80/120    avg_loss:0.007, val_acc:0.992]
Epoch [81/120    avg_loss:0.008, val_acc:0.992]
Epoch [82/120    avg_loss:0.008, val_acc:0.991]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.005, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.007, val_acc:0.992]
Epoch [89/120    avg_loss:0.005, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.006, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.993]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.007, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.004, val_acc:0.992]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.004, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.993]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.005, val_acc:0.992]
Epoch [113/120    avg_loss:0.004, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.004, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6419     0     0     2     0     0     0    11     0]
 [    0     1 18035     0    47     0     3     0     0     4]
 [    0     7     0  2014     4     0     0     0     4     7]
 [    0    24     9     0  2915     0     3     0    20     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     4     0     0  4850     0    16     1]
 [    0     2     0     0     0     0     2  1277     0     9]
 [    0     0     0    19    53     0     0     0  3484    15]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.23360566842601

F1 scores:
[       nan 0.99635235 0.99803547 0.98895163 0.97053438 0.98901099
 0.99630238 0.99493572 0.98057979 0.95633188]

Kappa:
0.98984943484597
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f73c2a738d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.811, val_acc:0.458]
Epoch [2/120    avg_loss:1.086, val_acc:0.641]
Epoch [3/120    avg_loss:0.732, val_acc:0.700]
Epoch [4/120    avg_loss:0.590, val_acc:0.784]
Epoch [5/120    avg_loss:0.499, val_acc:0.770]
Epoch [6/120    avg_loss:0.427, val_acc:0.859]
Epoch [7/120    avg_loss:0.410, val_acc:0.826]
Epoch [8/120    avg_loss:0.335, val_acc:0.847]
Epoch [9/120    avg_loss:0.285, val_acc:0.855]
Epoch [10/120    avg_loss:0.293, val_acc:0.884]
Epoch [11/120    avg_loss:0.273, val_acc:0.862]
Epoch [12/120    avg_loss:0.232, val_acc:0.862]
Epoch [13/120    avg_loss:0.211, val_acc:0.878]
Epoch [14/120    avg_loss:0.188, val_acc:0.914]
Epoch [15/120    avg_loss:0.182, val_acc:0.832]
Epoch [16/120    avg_loss:0.159, val_acc:0.949]
Epoch [17/120    avg_loss:0.152, val_acc:0.954]
Epoch [18/120    avg_loss:0.142, val_acc:0.958]
Epoch [19/120    avg_loss:0.117, val_acc:0.954]
Epoch [20/120    avg_loss:0.159, val_acc:0.946]
Epoch [21/120    avg_loss:0.137, val_acc:0.940]
Epoch [22/120    avg_loss:0.083, val_acc:0.963]
Epoch [23/120    avg_loss:0.109, val_acc:0.965]
Epoch [24/120    avg_loss:0.083, val_acc:0.961]
Epoch [25/120    avg_loss:0.096, val_acc:0.937]
Epoch [26/120    avg_loss:0.083, val_acc:0.950]
Epoch [27/120    avg_loss:0.077, val_acc:0.835]
Epoch [28/120    avg_loss:0.072, val_acc:0.964]
Epoch [29/120    avg_loss:0.053, val_acc:0.960]
Epoch [30/120    avg_loss:0.064, val_acc:0.968]
Epoch [31/120    avg_loss:0.048, val_acc:0.963]
Epoch [32/120    avg_loss:0.039, val_acc:0.974]
Epoch [33/120    avg_loss:0.055, val_acc:0.949]
Epoch [34/120    avg_loss:0.047, val_acc:0.971]
Epoch [35/120    avg_loss:0.029, val_acc:0.969]
Epoch [36/120    avg_loss:0.053, val_acc:0.951]
Epoch [37/120    avg_loss:0.035, val_acc:0.976]
Epoch [38/120    avg_loss:0.021, val_acc:0.976]
Epoch [39/120    avg_loss:0.024, val_acc:0.977]
Epoch [40/120    avg_loss:0.029, val_acc:0.975]
Epoch [41/120    avg_loss:0.038, val_acc:0.975]
Epoch [42/120    avg_loss:0.023, val_acc:0.981]
Epoch [43/120    avg_loss:0.021, val_acc:0.972]
Epoch [44/120    avg_loss:0.021, val_acc:0.972]
Epoch [45/120    avg_loss:0.021, val_acc:0.976]
Epoch [46/120    avg_loss:0.023, val_acc:0.979]
Epoch [47/120    avg_loss:0.024, val_acc:0.969]
Epoch [48/120    avg_loss:0.043, val_acc:0.967]
Epoch [49/120    avg_loss:0.027, val_acc:0.981]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.016, val_acc:0.971]
Epoch [52/120    avg_loss:0.018, val_acc:0.962]
Epoch [53/120    avg_loss:0.014, val_acc:0.984]
Epoch [54/120    avg_loss:0.016, val_acc:0.982]
Epoch [55/120    avg_loss:0.024, val_acc:0.980]
Epoch [56/120    avg_loss:0.015, val_acc:0.985]
Epoch [57/120    avg_loss:0.023, val_acc:0.984]
Epoch [58/120    avg_loss:0.015, val_acc:0.983]
Epoch [59/120    avg_loss:0.008, val_acc:0.963]
Epoch [60/120    avg_loss:0.022, val_acc:0.980]
Epoch [61/120    avg_loss:0.017, val_acc:0.981]
Epoch [62/120    avg_loss:0.009, val_acc:0.986]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.015, val_acc:0.986]
Epoch [65/120    avg_loss:0.171, val_acc:0.980]
Epoch [66/120    avg_loss:0.024, val_acc:0.967]
Epoch [67/120    avg_loss:0.029, val_acc:0.980]
Epoch [68/120    avg_loss:0.022, val_acc:0.980]
Epoch [69/120    avg_loss:0.026, val_acc:0.974]
Epoch [70/120    avg_loss:0.038, val_acc:0.975]
Epoch [71/120    avg_loss:0.016, val_acc:0.980]
Epoch [72/120    avg_loss:0.015, val_acc:0.982]
Epoch [73/120    avg_loss:0.014, val_acc:0.986]
Epoch [74/120    avg_loss:0.014, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.982]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.004, val_acc:0.987]
Epoch [78/120    avg_loss:0.004, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.040, val_acc:0.935]
Epoch [81/120    avg_loss:0.108, val_acc:0.970]
Epoch [82/120    avg_loss:0.039, val_acc:0.940]
Epoch [83/120    avg_loss:0.049, val_acc:0.979]
Epoch [84/120    avg_loss:0.018, val_acc:0.980]
Epoch [85/120    avg_loss:0.015, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.004, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.003, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.003, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     0     0     0    21     0    39     0]
 [    0     0 17998     0    83     0     9     0     0     0]
 [    0     5     0  2005     5     0     0     0    14     7]
 [    0    28    15     0  2909     0     5     0    14     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1     0     0     0     0  4848     0    29     0]
 [    0     3     0     0     0     0     2  1282     0     3]
 [    0    21     0    10    34     0     0     0  3506     0]
 [    0     0    10     0    14    30     0     0     0   865]]

Accuracy:
99.02875183765937

F1 scores:
[       nan 0.99082569 0.99676017 0.98987904 0.96692704 0.98863636
 0.99313736 0.99688958 0.97755472 0.9637883 ]

Kappa:
0.9871394469905884
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e345168d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.852, val_acc:0.502]
Epoch [2/120    avg_loss:1.275, val_acc:0.622]
Epoch [3/120    avg_loss:0.826, val_acc:0.800]
Epoch [4/120    avg_loss:0.671, val_acc:0.741]
Epoch [5/120    avg_loss:0.556, val_acc:0.783]
Epoch [6/120    avg_loss:0.415, val_acc:0.748]
Epoch [7/120    avg_loss:0.381, val_acc:0.834]
Epoch [8/120    avg_loss:0.325, val_acc:0.793]
Epoch [9/120    avg_loss:0.383, val_acc:0.889]
Epoch [10/120    avg_loss:0.264, val_acc:0.920]
Epoch [11/120    avg_loss:0.273, val_acc:0.858]
Epoch [12/120    avg_loss:0.220, val_acc:0.858]
Epoch [13/120    avg_loss:0.202, val_acc:0.893]
Epoch [14/120    avg_loss:0.210, val_acc:0.805]
Epoch [15/120    avg_loss:0.137, val_acc:0.932]
Epoch [16/120    avg_loss:0.153, val_acc:0.899]
Epoch [17/120    avg_loss:0.234, val_acc:0.936]
Epoch [18/120    avg_loss:0.209, val_acc:0.938]
Epoch [19/120    avg_loss:0.151, val_acc:0.865]
Epoch [20/120    avg_loss:0.120, val_acc:0.889]
Epoch [21/120    avg_loss:0.083, val_acc:0.957]
Epoch [22/120    avg_loss:0.089, val_acc:0.940]
Epoch [23/120    avg_loss:0.069, val_acc:0.955]
Epoch [24/120    avg_loss:0.090, val_acc:0.962]
Epoch [25/120    avg_loss:0.063, val_acc:0.971]
Epoch [26/120    avg_loss:0.071, val_acc:0.967]
Epoch [27/120    avg_loss:0.042, val_acc:0.964]
Epoch [28/120    avg_loss:0.067, val_acc:0.964]
Epoch [29/120    avg_loss:0.054, val_acc:0.976]
Epoch [30/120    avg_loss:0.045, val_acc:0.972]
Epoch [31/120    avg_loss:0.076, val_acc:0.974]
Epoch [32/120    avg_loss:0.047, val_acc:0.973]
Epoch [33/120    avg_loss:0.051, val_acc:0.964]
Epoch [34/120    avg_loss:0.228, val_acc:0.931]
Epoch [35/120    avg_loss:0.091, val_acc:0.968]
Epoch [36/120    avg_loss:0.042, val_acc:0.967]
Epoch [37/120    avg_loss:0.057, val_acc:0.980]
Epoch [38/120    avg_loss:0.033, val_acc:0.941]
Epoch [39/120    avg_loss:0.024, val_acc:0.978]
Epoch [40/120    avg_loss:0.020, val_acc:0.984]
Epoch [41/120    avg_loss:0.134, val_acc:0.945]
Epoch [42/120    avg_loss:0.153, val_acc:0.969]
Epoch [43/120    avg_loss:0.066, val_acc:0.967]
Epoch [44/120    avg_loss:0.037, val_acc:0.970]
Epoch [45/120    avg_loss:0.159, val_acc:0.971]
Epoch [46/120    avg_loss:0.047, val_acc:0.973]
Epoch [47/120    avg_loss:0.085, val_acc:0.961]
Epoch [48/120    avg_loss:0.086, val_acc:0.965]
Epoch [49/120    avg_loss:0.036, val_acc:0.983]
Epoch [50/120    avg_loss:0.056, val_acc:0.972]
Epoch [51/120    avg_loss:0.036, val_acc:0.975]
Epoch [52/120    avg_loss:0.018, val_acc:0.982]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.013, val_acc:0.985]
Epoch [55/120    avg_loss:0.012, val_acc:0.985]
Epoch [56/120    avg_loss:0.014, val_acc:0.987]
Epoch [57/120    avg_loss:0.011, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.986]
Epoch [59/120    avg_loss:0.009, val_acc:0.985]
Epoch [60/120    avg_loss:0.011, val_acc:0.985]
Epoch [61/120    avg_loss:0.009, val_acc:0.986]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.010, val_acc:0.987]
Epoch [65/120    avg_loss:0.010, val_acc:0.988]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.009, val_acc:0.988]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.006, val_acc:0.987]
Epoch [71/120    avg_loss:0.008, val_acc:0.987]
Epoch [72/120    avg_loss:0.010, val_acc:0.988]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     4     5     0     4     8    54     1]
 [    0     0 18064     0    19     0     4     0     3     0]
 [    0     5     0  2013     1     0     0     0    17     0]
 [    0    33    21     0  2892     0     7     0    19     0]
 [    0     0     0     0     0  1277     0     0    28     0]
 [    0     0     0    12     0     0  4855     0    10     1]
 [    0     6     0     0     0     0     0  1281     0     3]
 [    0     0     0    12    54     0     0     0  3481    24]
 [    0     0     0     7     5    60     0     0     0   847]]

Accuracy:
98.97091075603115

F1 scores:
[       nan 0.99064838 0.99870076 0.98579824 0.97242771 0.9666919
 0.99610176 0.9934083  0.96923291 0.94373259]

Kappa:
0.986365536879675
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12adf628d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.843, val_acc:0.411]
Epoch [2/120    avg_loss:1.167, val_acc:0.571]
Epoch [3/120    avg_loss:0.805, val_acc:0.617]
Epoch [4/120    avg_loss:0.656, val_acc:0.721]
Epoch [5/120    avg_loss:0.499, val_acc:0.736]
Epoch [6/120    avg_loss:0.456, val_acc:0.836]
Epoch [7/120    avg_loss:0.438, val_acc:0.823]
Epoch [8/120    avg_loss:0.372, val_acc:0.819]
Epoch [9/120    avg_loss:0.307, val_acc:0.840]
Epoch [10/120    avg_loss:0.267, val_acc:0.798]
Epoch [11/120    avg_loss:0.274, val_acc:0.889]
Epoch [12/120    avg_loss:0.269, val_acc:0.847]
Epoch [13/120    avg_loss:0.258, val_acc:0.896]
Epoch [14/120    avg_loss:0.231, val_acc:0.810]
Epoch [15/120    avg_loss:0.187, val_acc:0.906]
Epoch [16/120    avg_loss:0.160, val_acc:0.906]
Epoch [17/120    avg_loss:0.165, val_acc:0.874]
Epoch [18/120    avg_loss:0.177, val_acc:0.889]
Epoch [19/120    avg_loss:0.135, val_acc:0.929]
Epoch [20/120    avg_loss:0.158, val_acc:0.901]
Epoch [21/120    avg_loss:0.137, val_acc:0.934]
Epoch [22/120    avg_loss:0.117, val_acc:0.936]
Epoch [23/120    avg_loss:0.111, val_acc:0.940]
Epoch [24/120    avg_loss:0.115, val_acc:0.947]
Epoch [25/120    avg_loss:0.090, val_acc:0.952]
Epoch [26/120    avg_loss:0.093, val_acc:0.966]
Epoch [27/120    avg_loss:0.066, val_acc:0.944]
Epoch [28/120    avg_loss:0.088, val_acc:0.946]
Epoch [29/120    avg_loss:0.157, val_acc:0.934]
Epoch [30/120    avg_loss:0.109, val_acc:0.912]
Epoch [31/120    avg_loss:0.080, val_acc:0.925]
Epoch [32/120    avg_loss:0.077, val_acc:0.950]
Epoch [33/120    avg_loss:0.120, val_acc:0.956]
Epoch [34/120    avg_loss:0.087, val_acc:0.966]
Epoch [35/120    avg_loss:0.060, val_acc:0.959]
Epoch [36/120    avg_loss:0.105, val_acc:0.957]
Epoch [37/120    avg_loss:0.047, val_acc:0.962]
Epoch [38/120    avg_loss:0.042, val_acc:0.957]
Epoch [39/120    avg_loss:0.047, val_acc:0.952]
Epoch [40/120    avg_loss:0.036, val_acc:0.972]
Epoch [41/120    avg_loss:0.039, val_acc:0.969]
Epoch [42/120    avg_loss:0.089, val_acc:0.965]
Epoch [43/120    avg_loss:0.052, val_acc:0.970]
Epoch [44/120    avg_loss:0.092, val_acc:0.945]
Epoch [45/120    avg_loss:0.112, val_acc:0.963]
Epoch [46/120    avg_loss:0.037, val_acc:0.972]
Epoch [47/120    avg_loss:0.045, val_acc:0.968]
Epoch [48/120    avg_loss:0.040, val_acc:0.940]
Epoch [49/120    avg_loss:0.041, val_acc:0.973]
Epoch [50/120    avg_loss:0.032, val_acc:0.982]
Epoch [51/120    avg_loss:0.046, val_acc:0.952]
Epoch [52/120    avg_loss:0.032, val_acc:0.957]
Epoch [53/120    avg_loss:0.030, val_acc:0.972]
Epoch [54/120    avg_loss:0.045, val_acc:0.980]
Epoch [55/120    avg_loss:0.029, val_acc:0.972]
Epoch [56/120    avg_loss:0.025, val_acc:0.977]
Epoch [57/120    avg_loss:0.027, val_acc:0.979]
Epoch [58/120    avg_loss:0.065, val_acc:0.973]
Epoch [59/120    avg_loss:0.026, val_acc:0.940]
Epoch [60/120    avg_loss:0.068, val_acc:0.926]
Epoch [61/120    avg_loss:0.042, val_acc:0.952]
Epoch [62/120    avg_loss:0.037, val_acc:0.942]
Epoch [63/120    avg_loss:0.029, val_acc:0.981]
Epoch [64/120    avg_loss:0.023, val_acc:0.980]
Epoch [65/120    avg_loss:0.021, val_acc:0.983]
Epoch [66/120    avg_loss:0.014, val_acc:0.982]
Epoch [67/120    avg_loss:0.018, val_acc:0.981]
Epoch [68/120    avg_loss:0.016, val_acc:0.980]
Epoch [69/120    avg_loss:0.024, val_acc:0.981]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.015, val_acc:0.982]
Epoch [72/120    avg_loss:0.021, val_acc:0.978]
Epoch [73/120    avg_loss:0.011, val_acc:0.980]
Epoch [74/120    avg_loss:0.018, val_acc:0.983]
Epoch [75/120    avg_loss:0.012, val_acc:0.980]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.015, val_acc:0.985]
Epoch [78/120    avg_loss:0.015, val_acc:0.980]
Epoch [79/120    avg_loss:0.014, val_acc:0.982]
Epoch [80/120    avg_loss:0.015, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.983]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.019, val_acc:0.979]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.014, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.015, val_acc:0.980]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.981]
Epoch [93/120    avg_loss:0.017, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.981]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.981]
Epoch [101/120    avg_loss:0.013, val_acc:0.981]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.017, val_acc:0.981]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.014, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.010, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.981]
Epoch [116/120    avg_loss:0.011, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6316     0     0     0     0    30     0    64    22]
 [    0     0 18000     0    83     0     7     0     0     0]
 [    0     0     0  2009     1     0     0     0    18     8]
 [    0    27    21     0  2905     0     0     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4847     0    31     0]
 [    0     1     0     0     0     0     0  1286     0     3]
 [    0     3     0    20    74     0     0     0  3470     4]
 [    0     0     0     0    12    35     0     0     1   871]]

Accuracy:
98.83353818716411

F1 scores:
[       nan 0.98849675 0.99692614 0.98843788 0.96080701 0.98676749
 0.99303421 0.9984472  0.96738221 0.95347564]

Kappa:
0.9845594711243821
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f569d666978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.841, val_acc:0.551]
Epoch [2/120    avg_loss:1.228, val_acc:0.663]
Epoch [3/120    avg_loss:0.870, val_acc:0.703]
Epoch [4/120    avg_loss:0.625, val_acc:0.776]
Epoch [5/120    avg_loss:0.505, val_acc:0.801]
Epoch [6/120    avg_loss:0.510, val_acc:0.786]
Epoch [7/120    avg_loss:0.393, val_acc:0.827]
Epoch [8/120    avg_loss:0.338, val_acc:0.862]
Epoch [9/120    avg_loss:0.250, val_acc:0.889]
Epoch [10/120    avg_loss:0.349, val_acc:0.862]
Epoch [11/120    avg_loss:0.285, val_acc:0.871]
Epoch [12/120    avg_loss:0.224, val_acc:0.840]
Epoch [13/120    avg_loss:0.230, val_acc:0.889]
Epoch [14/120    avg_loss:0.281, val_acc:0.738]
Epoch [15/120    avg_loss:0.280, val_acc:0.878]
Epoch [16/120    avg_loss:0.198, val_acc:0.890]
Epoch [17/120    avg_loss:0.146, val_acc:0.927]
Epoch [18/120    avg_loss:0.133, val_acc:0.941]
Epoch [19/120    avg_loss:0.121, val_acc:0.952]
Epoch [20/120    avg_loss:0.102, val_acc:0.961]
Epoch [21/120    avg_loss:0.078, val_acc:0.950]
Epoch [22/120    avg_loss:0.070, val_acc:0.940]
Epoch [23/120    avg_loss:0.109, val_acc:0.925]
Epoch [24/120    avg_loss:0.086, val_acc:0.959]
Epoch [25/120    avg_loss:0.100, val_acc:0.934]
Epoch [26/120    avg_loss:0.095, val_acc:0.886]
Epoch [27/120    avg_loss:0.135, val_acc:0.958]
Epoch [28/120    avg_loss:0.052, val_acc:0.971]
Epoch [29/120    avg_loss:0.051, val_acc:0.964]
Epoch [30/120    avg_loss:0.056, val_acc:0.967]
Epoch [31/120    avg_loss:0.105, val_acc:0.967]
Epoch [32/120    avg_loss:0.042, val_acc:0.971]
Epoch [33/120    avg_loss:0.047, val_acc:0.940]
Epoch [34/120    avg_loss:0.042, val_acc:0.982]
Epoch [35/120    avg_loss:0.041, val_acc:0.973]
Epoch [36/120    avg_loss:0.036, val_acc:0.890]
Epoch [37/120    avg_loss:0.112, val_acc:0.950]
Epoch [38/120    avg_loss:0.059, val_acc:0.913]
Epoch [39/120    avg_loss:0.155, val_acc:0.930]
Epoch [40/120    avg_loss:0.069, val_acc:0.967]
Epoch [41/120    avg_loss:0.033, val_acc:0.972]
Epoch [42/120    avg_loss:0.029, val_acc:0.976]
Epoch [43/120    avg_loss:0.030, val_acc:0.962]
Epoch [44/120    avg_loss:0.023, val_acc:0.972]
Epoch [45/120    avg_loss:0.022, val_acc:0.978]
Epoch [46/120    avg_loss:0.031, val_acc:0.979]
Epoch [47/120    avg_loss:0.028, val_acc:0.962]
Epoch [48/120    avg_loss:0.032, val_acc:0.980]
Epoch [49/120    avg_loss:0.018, val_acc:0.982]
Epoch [50/120    avg_loss:0.017, val_acc:0.981]
Epoch [51/120    avg_loss:0.014, val_acc:0.984]
Epoch [52/120    avg_loss:0.010, val_acc:0.985]
Epoch [53/120    avg_loss:0.011, val_acc:0.984]
Epoch [54/120    avg_loss:0.011, val_acc:0.985]
Epoch [55/120    avg_loss:0.011, val_acc:0.984]
Epoch [56/120    avg_loss:0.010, val_acc:0.985]
Epoch [57/120    avg_loss:0.014, val_acc:0.984]
Epoch [58/120    avg_loss:0.013, val_acc:0.983]
Epoch [59/120    avg_loss:0.011, val_acc:0.984]
Epoch [60/120    avg_loss:0.010, val_acc:0.984]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.009, val_acc:0.983]
Epoch [63/120    avg_loss:0.012, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.985]
Epoch [65/120    avg_loss:0.009, val_acc:0.985]
Epoch [66/120    avg_loss:0.012, val_acc:0.985]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.013, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.008, val_acc:0.985]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.987]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.014, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6405     0     0     1     0    15     3     6     2]
 [    0     0 18047     0    18     0    20     0     5     0]
 [    0    17     0  2009     1     0     0     0     3     6]
 [    0    35    17     0  2890     0     9     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4872     0     6     0]
 [    0     3     0     0     0     0     2  1283     0     2]
 [    0     0     0    24    55     0     0     0  3484     8]
 [    0     0     0     0    14    70     0     0     0   835]]

Accuracy:
99.12515364037307

F1 scores:
[       nan 0.99363947 0.99834043 0.98746621 0.97126533 0.9738806
 0.99469171 0.99611801 0.98196167 0.94243792]

Kappa:
0.9884097122498776
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7926f099b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.743, val_acc:0.354]
Epoch [2/120    avg_loss:1.116, val_acc:0.677]
Epoch [3/120    avg_loss:0.801, val_acc:0.774]
Epoch [4/120    avg_loss:0.607, val_acc:0.828]
Epoch [5/120    avg_loss:0.473, val_acc:0.854]
Epoch [6/120    avg_loss:0.416, val_acc:0.841]
Epoch [7/120    avg_loss:0.418, val_acc:0.801]
Epoch [8/120    avg_loss:0.396, val_acc:0.883]
Epoch [9/120    avg_loss:0.294, val_acc:0.876]
Epoch [10/120    avg_loss:0.465, val_acc:0.321]
Epoch [11/120    avg_loss:1.105, val_acc:0.695]
Epoch [12/120    avg_loss:0.615, val_acc:0.772]
Epoch [13/120    avg_loss:0.446, val_acc:0.756]
Epoch [14/120    avg_loss:0.405, val_acc:0.826]
Epoch [15/120    avg_loss:0.309, val_acc:0.785]
Epoch [16/120    avg_loss:0.281, val_acc:0.793]
Epoch [17/120    avg_loss:0.319, val_acc:0.817]
Epoch [18/120    avg_loss:0.251, val_acc:0.858]
Epoch [19/120    avg_loss:0.239, val_acc:0.866]
Epoch [20/120    avg_loss:0.238, val_acc:0.902]
Epoch [21/120    avg_loss:0.179, val_acc:0.900]
Epoch [22/120    avg_loss:0.184, val_acc:0.895]
Epoch [23/120    avg_loss:0.193, val_acc:0.895]
Epoch [24/120    avg_loss:0.182, val_acc:0.925]
Epoch [25/120    avg_loss:0.190, val_acc:0.928]
Epoch [26/120    avg_loss:0.148, val_acc:0.936]
Epoch [27/120    avg_loss:0.143, val_acc:0.897]
Epoch [28/120    avg_loss:0.153, val_acc:0.935]
Epoch [29/120    avg_loss:0.113, val_acc:0.943]
Epoch [30/120    avg_loss:0.095, val_acc:0.938]
Epoch [31/120    avg_loss:0.114, val_acc:0.929]
Epoch [32/120    avg_loss:0.087, val_acc:0.949]
Epoch [33/120    avg_loss:0.098, val_acc:0.921]
Epoch [34/120    avg_loss:0.108, val_acc:0.931]
Epoch [35/120    avg_loss:0.104, val_acc:0.942]
Epoch [36/120    avg_loss:0.108, val_acc:0.953]
Epoch [37/120    avg_loss:0.080, val_acc:0.916]
Epoch [38/120    avg_loss:0.084, val_acc:0.958]
Epoch [39/120    avg_loss:0.064, val_acc:0.968]
Epoch [40/120    avg_loss:0.059, val_acc:0.968]
Epoch [41/120    avg_loss:0.053, val_acc:0.967]
Epoch [42/120    avg_loss:0.053, val_acc:0.966]
Epoch [43/120    avg_loss:0.057, val_acc:0.956]
Epoch [44/120    avg_loss:0.093, val_acc:0.969]
Epoch [45/120    avg_loss:0.052, val_acc:0.954]
Epoch [46/120    avg_loss:0.058, val_acc:0.942]
Epoch [47/120    avg_loss:0.069, val_acc:0.960]
Epoch [48/120    avg_loss:0.064, val_acc:0.962]
Epoch [49/120    avg_loss:0.068, val_acc:0.976]
Epoch [50/120    avg_loss:0.041, val_acc:0.965]
Epoch [51/120    avg_loss:0.032, val_acc:0.947]
Epoch [52/120    avg_loss:0.041, val_acc:0.977]
Epoch [53/120    avg_loss:0.046, val_acc:0.961]
Epoch [54/120    avg_loss:0.087, val_acc:0.971]
Epoch [55/120    avg_loss:0.036, val_acc:0.986]
Epoch [56/120    avg_loss:0.073, val_acc:0.959]
Epoch [57/120    avg_loss:0.052, val_acc:0.907]
Epoch [58/120    avg_loss:0.044, val_acc:0.982]
Epoch [59/120    avg_loss:0.026, val_acc:0.969]
Epoch [60/120    avg_loss:0.036, val_acc:0.966]
Epoch [61/120    avg_loss:0.025, val_acc:0.984]
Epoch [62/120    avg_loss:0.019, val_acc:0.988]
Epoch [63/120    avg_loss:0.024, val_acc:0.968]
Epoch [64/120    avg_loss:0.025, val_acc:0.984]
Epoch [65/120    avg_loss:0.020, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.962]
Epoch [67/120    avg_loss:0.013, val_acc:0.988]
Epoch [68/120    avg_loss:0.014, val_acc:0.973]
Epoch [69/120    avg_loss:0.017, val_acc:0.978]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.021, val_acc:0.958]
Epoch [74/120    avg_loss:0.018, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.986]
Epoch [76/120    avg_loss:0.024, val_acc:0.980]
Epoch [77/120    avg_loss:0.015, val_acc:0.988]
Epoch [78/120    avg_loss:0.045, val_acc:0.963]
Epoch [79/120    avg_loss:0.018, val_acc:0.983]
Epoch [80/120    avg_loss:0.016, val_acc:0.971]
Epoch [81/120    avg_loss:0.045, val_acc:0.923]
Epoch [82/120    avg_loss:0.136, val_acc:0.972]
Epoch [83/120    avg_loss:0.031, val_acc:0.966]
Epoch [84/120    avg_loss:0.029, val_acc:0.973]
Epoch [85/120    avg_loss:0.020, val_acc:0.968]
Epoch [86/120    avg_loss:0.016, val_acc:0.960]
Epoch [87/120    avg_loss:0.019, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.976]
Epoch [89/120    avg_loss:0.031, val_acc:0.980]
Epoch [90/120    avg_loss:0.014, val_acc:0.984]
Epoch [91/120    avg_loss:0.012, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6381     0     0     5     0     0     8    38     0]
 [    0     0 17897     0    37     0   154     0     2     0]
 [    0     1     0  1992     6     0     0     0    36     1]
 [    0    25    19     0  2904     0     0     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4849     0    29     0]
 [    0     9     0     0     0     0     0  1275     0     6]
 [    0     2     0     1    71     0     0     0  3493     4]
 [    0     0     0     0    14    38     0     0     0   867]]

Accuracy:
98.72267611404334

F1 scores:
[       nan 0.99315175 0.99411209 0.98883098 0.96655017 0.98564955
 0.98147961 0.99106102 0.97122202 0.96494157]

Kappa:
0.9831050790806547
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f33df984908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.840, val_acc:0.308]
Epoch [2/120    avg_loss:1.199, val_acc:0.730]
Epoch [3/120    avg_loss:0.823, val_acc:0.634]
Epoch [4/120    avg_loss:0.615, val_acc:0.699]
Epoch [5/120    avg_loss:0.471, val_acc:0.852]
Epoch [6/120    avg_loss:0.456, val_acc:0.843]
Epoch [7/120    avg_loss:0.373, val_acc:0.851]
Epoch [8/120    avg_loss:0.322, val_acc:0.818]
Epoch [9/120    avg_loss:0.328, val_acc:0.905]
Epoch [10/120    avg_loss:0.253, val_acc:0.844]
Epoch [11/120    avg_loss:0.240, val_acc:0.827]
Epoch [12/120    avg_loss:0.214, val_acc:0.908]
Epoch [13/120    avg_loss:0.218, val_acc:0.846]
Epoch [14/120    avg_loss:0.173, val_acc:0.931]
Epoch [15/120    avg_loss:0.214, val_acc:0.889]
Epoch [16/120    avg_loss:0.174, val_acc:0.925]
Epoch [17/120    avg_loss:0.342, val_acc:0.539]
Epoch [18/120    avg_loss:1.386, val_acc:0.655]
Epoch [19/120    avg_loss:1.092, val_acc:0.740]
Epoch [20/120    avg_loss:0.940, val_acc:0.686]
Epoch [21/120    avg_loss:0.912, val_acc:0.747]
Epoch [22/120    avg_loss:0.759, val_acc:0.755]
Epoch [23/120    avg_loss:0.725, val_acc:0.758]
Epoch [24/120    avg_loss:0.631, val_acc:0.777]
Epoch [25/120    avg_loss:0.634, val_acc:0.772]
Epoch [26/120    avg_loss:0.568, val_acc:0.754]
Epoch [27/120    avg_loss:0.506, val_acc:0.741]
Epoch [28/120    avg_loss:0.484, val_acc:0.774]
Epoch [29/120    avg_loss:0.454, val_acc:0.792]
Epoch [30/120    avg_loss:0.452, val_acc:0.797]
Epoch [31/120    avg_loss:0.468, val_acc:0.801]
Epoch [32/120    avg_loss:0.432, val_acc:0.798]
Epoch [33/120    avg_loss:0.393, val_acc:0.791]
Epoch [34/120    avg_loss:0.394, val_acc:0.780]
Epoch [35/120    avg_loss:0.426, val_acc:0.783]
Epoch [36/120    avg_loss:0.370, val_acc:0.785]
Epoch [37/120    avg_loss:0.416, val_acc:0.802]
Epoch [38/120    avg_loss:0.384, val_acc:0.799]
Epoch [39/120    avg_loss:0.386, val_acc:0.798]
Epoch [40/120    avg_loss:0.375, val_acc:0.799]
Epoch [41/120    avg_loss:0.385, val_acc:0.802]
Epoch [42/120    avg_loss:0.379, val_acc:0.801]
Epoch [43/120    avg_loss:0.388, val_acc:0.798]
Epoch [44/120    avg_loss:0.391, val_acc:0.802]
Epoch [45/120    avg_loss:0.398, val_acc:0.803]
Epoch [46/120    avg_loss:0.359, val_acc:0.802]
Epoch [47/120    avg_loss:0.382, val_acc:0.805]
Epoch [48/120    avg_loss:0.356, val_acc:0.802]
Epoch [49/120    avg_loss:0.367, val_acc:0.803]
Epoch [50/120    avg_loss:0.368, val_acc:0.806]
Epoch [51/120    avg_loss:0.398, val_acc:0.807]
Epoch [52/120    avg_loss:0.372, val_acc:0.804]
Epoch [53/120    avg_loss:0.379, val_acc:0.810]
Epoch [54/120    avg_loss:0.384, val_acc:0.810]
Epoch [55/120    avg_loss:0.367, val_acc:0.808]
Epoch [56/120    avg_loss:0.387, val_acc:0.807]
Epoch [57/120    avg_loss:0.406, val_acc:0.807]
Epoch [58/120    avg_loss:0.408, val_acc:0.807]
Epoch [59/120    avg_loss:0.378, val_acc:0.807]
Epoch [60/120    avg_loss:0.401, val_acc:0.807]
Epoch [61/120    avg_loss:0.379, val_acc:0.807]
Epoch [62/120    avg_loss:0.369, val_acc:0.807]
Epoch [63/120    avg_loss:0.341, val_acc:0.807]
Epoch [64/120    avg_loss:0.390, val_acc:0.807]
Epoch [65/120    avg_loss:0.375, val_acc:0.807]
Epoch [66/120    avg_loss:0.370, val_acc:0.807]
Epoch [67/120    avg_loss:0.396, val_acc:0.807]
Epoch [68/120    avg_loss:0.378, val_acc:0.807]
Epoch [69/120    avg_loss:0.385, val_acc:0.807]
Epoch [70/120    avg_loss:0.354, val_acc:0.807]
Epoch [71/120    avg_loss:0.374, val_acc:0.807]
Epoch [72/120    avg_loss:0.364, val_acc:0.807]
Epoch [73/120    avg_loss:0.353, val_acc:0.807]
Epoch [74/120    avg_loss:0.397, val_acc:0.807]
Epoch [75/120    avg_loss:0.376, val_acc:0.807]
Epoch [76/120    avg_loss:0.370, val_acc:0.807]
Epoch [77/120    avg_loss:0.392, val_acc:0.807]
Epoch [78/120    avg_loss:0.390, val_acc:0.807]
Epoch [79/120    avg_loss:0.385, val_acc:0.807]
Epoch [80/120    avg_loss:0.389, val_acc:0.807]
Epoch [81/120    avg_loss:0.391, val_acc:0.807]
Epoch [82/120    avg_loss:0.393, val_acc:0.807]
Epoch [83/120    avg_loss:0.387, val_acc:0.807]
Epoch [84/120    avg_loss:0.356, val_acc:0.807]
Epoch [85/120    avg_loss:0.402, val_acc:0.807]
Epoch [86/120    avg_loss:0.370, val_acc:0.807]
Epoch [87/120    avg_loss:0.364, val_acc:0.807]
Epoch [88/120    avg_loss:0.362, val_acc:0.807]
Epoch [89/120    avg_loss:0.410, val_acc:0.807]
Epoch [90/120    avg_loss:0.373, val_acc:0.807]
Epoch [91/120    avg_loss:0.367, val_acc:0.807]
Epoch [92/120    avg_loss:0.366, val_acc:0.807]
Epoch [93/120    avg_loss:0.385, val_acc:0.807]
Epoch [94/120    avg_loss:0.376, val_acc:0.807]
Epoch [95/120    avg_loss:0.366, val_acc:0.807]
Epoch [96/120    avg_loss:0.360, val_acc:0.807]
Epoch [97/120    avg_loss:0.384, val_acc:0.807]
Epoch [98/120    avg_loss:0.365, val_acc:0.807]
Epoch [99/120    avg_loss:0.366, val_acc:0.807]
Epoch [100/120    avg_loss:0.373, val_acc:0.807]
Epoch [101/120    avg_loss:0.382, val_acc:0.807]
Epoch [102/120    avg_loss:0.368, val_acc:0.807]
Epoch [103/120    avg_loss:0.376, val_acc:0.807]
Epoch [104/120    avg_loss:0.366, val_acc:0.807]
Epoch [105/120    avg_loss:0.379, val_acc:0.807]
Epoch [106/120    avg_loss:0.385, val_acc:0.807]
Epoch [107/120    avg_loss:0.364, val_acc:0.807]
Epoch [108/120    avg_loss:0.391, val_acc:0.807]
Epoch [109/120    avg_loss:0.406, val_acc:0.807]
Epoch [110/120    avg_loss:0.427, val_acc:0.807]
Epoch [111/120    avg_loss:0.371, val_acc:0.807]
Epoch [112/120    avg_loss:0.370, val_acc:0.807]
Epoch [113/120    avg_loss:0.388, val_acc:0.807]
Epoch [114/120    avg_loss:0.371, val_acc:0.807]
Epoch [115/120    avg_loss:0.357, val_acc:0.807]
Epoch [116/120    avg_loss:0.349, val_acc:0.807]
Epoch [117/120    avg_loss:0.377, val_acc:0.807]
Epoch [118/120    avg_loss:0.394, val_acc:0.807]
Epoch [119/120    avg_loss:0.396, val_acc:0.807]
Epoch [120/120    avg_loss:0.396, val_acc:0.807]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5836     1     0    73     0   152     0   306    64]
 [    0     0 14320     0     7     0  3763     0     0     0]
 [    0    45     0  1536     2     0     1     0   342   110]
 [    0   131    86     0  2411     0   257     0    63    24]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   672     9     0     0  4035     0   162     0]
 [    0    74     0     0     0     0     9  1205     0     2]
 [    0   182     1     0    44     0    72     0  3255    17]
 [    0    23     0     3    16    98     0     0     0   779]]

Accuracy:
83.5851830429229

F1 scores:
[       nan 0.9173937  0.86343081 0.85714286 0.87276018 0.96381093
 0.61289588 0.96593186 0.84556436 0.81357702]

Kappa:
0.7888131652040298
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7ba2c69b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.833, val_acc:0.625]
Epoch [2/120    avg_loss:1.107, val_acc:0.707]
Epoch [3/120    avg_loss:0.761, val_acc:0.733]
Epoch [4/120    avg_loss:0.698, val_acc:0.741]
Epoch [5/120    avg_loss:0.500, val_acc:0.771]
Epoch [6/120    avg_loss:0.418, val_acc:0.875]
Epoch [7/120    avg_loss:0.379, val_acc:0.847]
Epoch [8/120    avg_loss:0.312, val_acc:0.804]
Epoch [9/120    avg_loss:0.326, val_acc:0.908]
Epoch [10/120    avg_loss:0.282, val_acc:0.927]
Epoch [11/120    avg_loss:0.327, val_acc:0.910]
Epoch [12/120    avg_loss:0.280, val_acc:0.814]
Epoch [13/120    avg_loss:0.185, val_acc:0.845]
Epoch [14/120    avg_loss:0.195, val_acc:0.929]
Epoch [15/120    avg_loss:0.176, val_acc:0.937]
Epoch [16/120    avg_loss:0.204, val_acc:0.925]
Epoch [17/120    avg_loss:0.130, val_acc:0.915]
Epoch [18/120    avg_loss:0.240, val_acc:0.532]
Epoch [19/120    avg_loss:0.828, val_acc:0.729]
Epoch [20/120    avg_loss:0.598, val_acc:0.759]
Epoch [21/120    avg_loss:0.525, val_acc:0.836]
Epoch [22/120    avg_loss:0.463, val_acc:0.779]
Epoch [23/120    avg_loss:0.448, val_acc:0.817]
Epoch [24/120    avg_loss:0.359, val_acc:0.859]
Epoch [25/120    avg_loss:0.336, val_acc:0.846]
Epoch [26/120    avg_loss:0.347, val_acc:0.859]
Epoch [27/120    avg_loss:0.330, val_acc:0.848]
Epoch [28/120    avg_loss:0.344, val_acc:0.876]
Epoch [29/120    avg_loss:0.252, val_acc:0.886]
Epoch [30/120    avg_loss:0.231, val_acc:0.889]
Epoch [31/120    avg_loss:0.220, val_acc:0.889]
Epoch [32/120    avg_loss:0.229, val_acc:0.894]
Epoch [33/120    avg_loss:0.226, val_acc:0.889]
Epoch [34/120    avg_loss:0.192, val_acc:0.902]
Epoch [35/120    avg_loss:0.219, val_acc:0.893]
Epoch [36/120    avg_loss:0.201, val_acc:0.908]
Epoch [37/120    avg_loss:0.194, val_acc:0.902]
Epoch [38/120    avg_loss:0.196, val_acc:0.909]
Epoch [39/120    avg_loss:0.193, val_acc:0.912]
Epoch [40/120    avg_loss:0.196, val_acc:0.916]
Epoch [41/120    avg_loss:0.190, val_acc:0.922]
Epoch [42/120    avg_loss:0.188, val_acc:0.922]
Epoch [43/120    avg_loss:0.195, val_acc:0.921]
Epoch [44/120    avg_loss:0.185, val_acc:0.921]
Epoch [45/120    avg_loss:0.166, val_acc:0.921]
Epoch [46/120    avg_loss:0.183, val_acc:0.919]
Epoch [47/120    avg_loss:0.172, val_acc:0.920]
Epoch [48/120    avg_loss:0.193, val_acc:0.919]
Epoch [49/120    avg_loss:0.169, val_acc:0.918]
Epoch [50/120    avg_loss:0.177, val_acc:0.920]
Epoch [51/120    avg_loss:0.166, val_acc:0.920]
Epoch [52/120    avg_loss:0.161, val_acc:0.922]
Epoch [53/120    avg_loss:0.172, val_acc:0.922]
Epoch [54/120    avg_loss:0.175, val_acc:0.921]
Epoch [55/120    avg_loss:0.185, val_acc:0.921]
Epoch [56/120    avg_loss:0.183, val_acc:0.921]
Epoch [57/120    avg_loss:0.176, val_acc:0.921]
Epoch [58/120    avg_loss:0.172, val_acc:0.921]
Epoch [59/120    avg_loss:0.167, val_acc:0.921]
Epoch [60/120    avg_loss:0.168, val_acc:0.921]
Epoch [61/120    avg_loss:0.164, val_acc:0.922]
Epoch [62/120    avg_loss:0.175, val_acc:0.922]
Epoch [63/120    avg_loss:0.168, val_acc:0.922]
Epoch [64/120    avg_loss:0.172, val_acc:0.922]
Epoch [65/120    avg_loss:0.168, val_acc:0.922]
Epoch [66/120    avg_loss:0.179, val_acc:0.922]
Epoch [67/120    avg_loss:0.170, val_acc:0.922]
Epoch [68/120    avg_loss:0.165, val_acc:0.922]
Epoch [69/120    avg_loss:0.176, val_acc:0.922]
Epoch [70/120    avg_loss:0.177, val_acc:0.922]
Epoch [71/120    avg_loss:0.165, val_acc:0.922]
Epoch [72/120    avg_loss:0.177, val_acc:0.922]
Epoch [73/120    avg_loss:0.181, val_acc:0.922]
Epoch [74/120    avg_loss:0.166, val_acc:0.922]
Epoch [75/120    avg_loss:0.183, val_acc:0.922]
Epoch [76/120    avg_loss:0.169, val_acc:0.922]
Epoch [77/120    avg_loss:0.176, val_acc:0.922]
Epoch [78/120    avg_loss:0.176, val_acc:0.922]
Epoch [79/120    avg_loss:0.172, val_acc:0.922]
Epoch [80/120    avg_loss:0.170, val_acc:0.922]
Epoch [81/120    avg_loss:0.169, val_acc:0.922]
Epoch [82/120    avg_loss:0.166, val_acc:0.922]
Epoch [83/120    avg_loss:0.183, val_acc:0.922]
Epoch [84/120    avg_loss:0.167, val_acc:0.922]
Epoch [85/120    avg_loss:0.163, val_acc:0.922]
Epoch [86/120    avg_loss:0.169, val_acc:0.922]
Epoch [87/120    avg_loss:0.172, val_acc:0.922]
Epoch [88/120    avg_loss:0.172, val_acc:0.922]
Epoch [89/120    avg_loss:0.169, val_acc:0.922]
Epoch [90/120    avg_loss:0.171, val_acc:0.922]
Epoch [91/120    avg_loss:0.190, val_acc:0.922]
Epoch [92/120    avg_loss:0.168, val_acc:0.922]
Epoch [93/120    avg_loss:0.173, val_acc:0.922]
Epoch [94/120    avg_loss:0.184, val_acc:0.922]
Epoch [95/120    avg_loss:0.175, val_acc:0.922]
Epoch [96/120    avg_loss:0.164, val_acc:0.922]
Epoch [97/120    avg_loss:0.154, val_acc:0.922]
Epoch [98/120    avg_loss:0.179, val_acc:0.922]
Epoch [99/120    avg_loss:0.179, val_acc:0.922]
Epoch [100/120    avg_loss:0.178, val_acc:0.922]
Epoch [101/120    avg_loss:0.173, val_acc:0.922]
Epoch [102/120    avg_loss:0.176, val_acc:0.922]
Epoch [103/120    avg_loss:0.166, val_acc:0.922]
Epoch [104/120    avg_loss:0.172, val_acc:0.922]
Epoch [105/120    avg_loss:0.173, val_acc:0.922]
Epoch [106/120    avg_loss:0.161, val_acc:0.922]
Epoch [107/120    avg_loss:0.145, val_acc:0.922]
Epoch [108/120    avg_loss:0.176, val_acc:0.922]
Epoch [109/120    avg_loss:0.165, val_acc:0.922]
Epoch [110/120    avg_loss:0.180, val_acc:0.922]
Epoch [111/120    avg_loss:0.187, val_acc:0.922]
Epoch [112/120    avg_loss:0.163, val_acc:0.922]
Epoch [113/120    avg_loss:0.171, val_acc:0.922]
Epoch [114/120    avg_loss:0.175, val_acc:0.922]
Epoch [115/120    avg_loss:0.170, val_acc:0.922]
Epoch [116/120    avg_loss:0.179, val_acc:0.922]
Epoch [117/120    avg_loss:0.168, val_acc:0.922]
Epoch [118/120    avg_loss:0.171, val_acc:0.922]
Epoch [119/120    avg_loss:0.170, val_acc:0.922]
Epoch [120/120    avg_loss:0.164, val_acc:0.922]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5988     0    22   101     0    25    67   167    62]
 [    0     0 17470     0   509     0   111     0     0     0]
 [    0    13     0  1949     0     0     0     0    27    47]
 [    0   135    38     1  2722     0    62     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    52    12     7     0  4782     0    25     0]
 [    0    13     0     0     0     0    11  1248     2    16]
 [    0    81     0    15    78     0     7     0  3390     0]
 [    0    17    20     5    25    66     0     0     0   786]]

Accuracy:
95.53418648928735

F1 scores:
[       nan 0.94455399 0.97953462 0.96485149 0.84876832 0.97533632
 0.96840826 0.95815739 0.94219011 0.85901639]

Kappa:
0.9411961555793857
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbaa852d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.856, val_acc:0.323]
Epoch [2/120    avg_loss:1.200, val_acc:0.575]
Epoch [3/120    avg_loss:0.875, val_acc:0.595]
Epoch [4/120    avg_loss:0.706, val_acc:0.736]
Epoch [5/120    avg_loss:0.594, val_acc:0.737]
Epoch [6/120    avg_loss:0.423, val_acc:0.793]
Epoch [7/120    avg_loss:0.424, val_acc:0.735]
Epoch [8/120    avg_loss:0.350, val_acc:0.761]
Epoch [9/120    avg_loss:0.312, val_acc:0.854]
Epoch [10/120    avg_loss:0.280, val_acc:0.860]
Epoch [11/120    avg_loss:0.255, val_acc:0.884]
Epoch [12/120    avg_loss:0.320, val_acc:0.906]
Epoch [13/120    avg_loss:0.309, val_acc:0.886]
Epoch [14/120    avg_loss:0.217, val_acc:0.878]
Epoch [15/120    avg_loss:0.191, val_acc:0.925]
Epoch [16/120    avg_loss:0.171, val_acc:0.941]
Epoch [17/120    avg_loss:0.180, val_acc:0.944]
Epoch [18/120    avg_loss:0.159, val_acc:0.958]
Epoch [19/120    avg_loss:0.137, val_acc:0.949]
Epoch [20/120    avg_loss:0.118, val_acc:0.933]
Epoch [21/120    avg_loss:0.130, val_acc:0.932]
Epoch [22/120    avg_loss:0.143, val_acc:0.920]
Epoch [23/120    avg_loss:0.203, val_acc:0.945]
Epoch [24/120    avg_loss:0.140, val_acc:0.916]
Epoch [25/120    avg_loss:0.106, val_acc:0.953]
Epoch [26/120    avg_loss:0.120, val_acc:0.936]
Epoch [27/120    avg_loss:0.114, val_acc:0.933]
Epoch [28/120    avg_loss:0.126, val_acc:0.816]
Epoch [29/120    avg_loss:0.116, val_acc:0.964]
Epoch [30/120    avg_loss:0.088, val_acc:0.951]
Epoch [31/120    avg_loss:0.084, val_acc:0.897]
Epoch [32/120    avg_loss:0.120, val_acc:0.954]
Epoch [33/120    avg_loss:0.859, val_acc:0.564]
Epoch [34/120    avg_loss:0.996, val_acc:0.669]
Epoch [35/120    avg_loss:0.839, val_acc:0.707]
Epoch [36/120    avg_loss:0.731, val_acc:0.729]
Epoch [37/120    avg_loss:0.688, val_acc:0.765]
Epoch [38/120    avg_loss:0.573, val_acc:0.763]
Epoch [39/120    avg_loss:0.508, val_acc:0.748]
Epoch [40/120    avg_loss:0.543, val_acc:0.796]
Epoch [41/120    avg_loss:0.460, val_acc:0.791]
Epoch [42/120    avg_loss:0.433, val_acc:0.815]
Epoch [43/120    avg_loss:0.402, val_acc:0.841]
Epoch [44/120    avg_loss:0.412, val_acc:0.839]
Epoch [45/120    avg_loss:0.347, val_acc:0.843]
Epoch [46/120    avg_loss:0.342, val_acc:0.845]
Epoch [47/120    avg_loss:0.342, val_acc:0.852]
Epoch [48/120    avg_loss:0.322, val_acc:0.855]
Epoch [49/120    avg_loss:0.321, val_acc:0.850]
Epoch [50/120    avg_loss:0.327, val_acc:0.854]
Epoch [51/120    avg_loss:0.318, val_acc:0.849]
Epoch [52/120    avg_loss:0.328, val_acc:0.856]
Epoch [53/120    avg_loss:0.355, val_acc:0.849]
Epoch [54/120    avg_loss:0.323, val_acc:0.862]
Epoch [55/120    avg_loss:0.316, val_acc:0.866]
Epoch [56/120    avg_loss:0.293, val_acc:0.863]
Epoch [57/120    avg_loss:0.283, val_acc:0.863]
Epoch [58/120    avg_loss:0.304, val_acc:0.861]
Epoch [59/120    avg_loss:0.293, val_acc:0.861]
Epoch [60/120    avg_loss:0.295, val_acc:0.862]
Epoch [61/120    avg_loss:0.306, val_acc:0.865]
Epoch [62/120    avg_loss:0.296, val_acc:0.863]
Epoch [63/120    avg_loss:0.314, val_acc:0.865]
Epoch [64/120    avg_loss:0.311, val_acc:0.863]
Epoch [65/120    avg_loss:0.283, val_acc:0.864]
Epoch [66/120    avg_loss:0.274, val_acc:0.862]
Epoch [67/120    avg_loss:0.301, val_acc:0.866]
Epoch [68/120    avg_loss:0.272, val_acc:0.858]
Epoch [69/120    avg_loss:0.291, val_acc:0.859]
Epoch [70/120    avg_loss:0.288, val_acc:0.862]
Epoch [71/120    avg_loss:0.305, val_acc:0.862]
Epoch [72/120    avg_loss:0.299, val_acc:0.862]
Epoch [73/120    avg_loss:0.304, val_acc:0.864]
Epoch [74/120    avg_loss:0.296, val_acc:0.865]
Epoch [75/120    avg_loss:0.306, val_acc:0.865]
Epoch [76/120    avg_loss:0.313, val_acc:0.865]
Epoch [77/120    avg_loss:0.308, val_acc:0.865]
Epoch [78/120    avg_loss:0.308, val_acc:0.864]
Epoch [79/120    avg_loss:0.290, val_acc:0.866]
Epoch [80/120    avg_loss:0.267, val_acc:0.865]
Epoch [81/120    avg_loss:0.309, val_acc:0.864]
Epoch [82/120    avg_loss:0.307, val_acc:0.864]
Epoch [83/120    avg_loss:0.296, val_acc:0.864]
Epoch [84/120    avg_loss:0.314, val_acc:0.864]
Epoch [85/120    avg_loss:0.287, val_acc:0.864]
Epoch [86/120    avg_loss:0.302, val_acc:0.865]
Epoch [87/120    avg_loss:0.343, val_acc:0.865]
Epoch [88/120    avg_loss:0.299, val_acc:0.865]
Epoch [89/120    avg_loss:0.312, val_acc:0.865]
Epoch [90/120    avg_loss:0.295, val_acc:0.865]
Epoch [91/120    avg_loss:0.302, val_acc:0.865]
Epoch [92/120    avg_loss:0.281, val_acc:0.865]
Epoch [93/120    avg_loss:0.283, val_acc:0.865]
Epoch [94/120    avg_loss:0.293, val_acc:0.865]
Epoch [95/120    avg_loss:0.285, val_acc:0.865]
Epoch [96/120    avg_loss:0.281, val_acc:0.865]
Epoch [97/120    avg_loss:0.299, val_acc:0.865]
Epoch [98/120    avg_loss:0.318, val_acc:0.865]
Epoch [99/120    avg_loss:0.284, val_acc:0.865]
Epoch [100/120    avg_loss:0.299, val_acc:0.865]
Epoch [101/120    avg_loss:0.303, val_acc:0.865]
Epoch [102/120    avg_loss:0.282, val_acc:0.865]
Epoch [103/120    avg_loss:0.308, val_acc:0.865]
Epoch [104/120    avg_loss:0.309, val_acc:0.865]
Epoch [105/120    avg_loss:0.306, val_acc:0.865]
Epoch [106/120    avg_loss:0.284, val_acc:0.865]
Epoch [107/120    avg_loss:0.292, val_acc:0.865]
Epoch [108/120    avg_loss:0.280, val_acc:0.865]
Epoch [109/120    avg_loss:0.323, val_acc:0.865]
Epoch [110/120    avg_loss:0.274, val_acc:0.865]
Epoch [111/120    avg_loss:0.291, val_acc:0.865]
Epoch [112/120    avg_loss:0.291, val_acc:0.865]
Epoch [113/120    avg_loss:0.308, val_acc:0.865]
Epoch [114/120    avg_loss:0.287, val_acc:0.865]
Epoch [115/120    avg_loss:0.281, val_acc:0.865]
Epoch [116/120    avg_loss:0.297, val_acc:0.865]
Epoch [117/120    avg_loss:0.305, val_acc:0.865]
Epoch [118/120    avg_loss:0.287, val_acc:0.865]
Epoch [119/120    avg_loss:0.295, val_acc:0.865]
Epoch [120/120    avg_loss:0.288, val_acc:0.865]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5584     0    12   332     0     0    44   396    64]
 [    0     0 17407     0   342     0   341     0     0     0]
 [    0     2     0  1955    11     0     0     0    32    36]
 [    0   157    89     3  2570     0    95     0    57     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1   126     0    38     0  4713     0     0     0]
 [    0    10     0     0     0     0     3  1267     0    10]
 [    0    70     6     1    69     0     0     0  3425     0]
 [    0    20     6     6    20   110     2     3     9   743]]

Accuracy:
93.91704624876485

F1 scores:
[       nan 0.90974259 0.97452693 0.97433342 0.80893925 0.95955882
 0.9395933  0.97311828 0.91455274 0.83812747]

Kappa:
0.9199044517775719
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fce6cfb5940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.848, val_acc:0.606]
Epoch [2/120    avg_loss:1.153, val_acc:0.761]
Epoch [3/120    avg_loss:0.891, val_acc:0.764]
Epoch [4/120    avg_loss:0.610, val_acc:0.788]
Epoch [5/120    avg_loss:0.498, val_acc:0.779]
Epoch [6/120    avg_loss:0.455, val_acc:0.824]
Epoch [7/120    avg_loss:0.409, val_acc:0.824]
Epoch [8/120    avg_loss:0.321, val_acc:0.890]
Epoch [9/120    avg_loss:0.376, val_acc:0.860]
Epoch [10/120    avg_loss:0.286, val_acc:0.893]
Epoch [11/120    avg_loss:0.216, val_acc:0.904]
Epoch [12/120    avg_loss:0.259, val_acc:0.882]
Epoch [13/120    avg_loss:0.627, val_acc:0.591]
Epoch [14/120    avg_loss:0.618, val_acc:0.647]
Epoch [15/120    avg_loss:0.498, val_acc:0.800]
Epoch [16/120    avg_loss:0.394, val_acc:0.786]
Epoch [17/120    avg_loss:0.314, val_acc:0.904]
Epoch [18/120    avg_loss:0.274, val_acc:0.859]
Epoch [19/120    avg_loss:0.251, val_acc:0.827]
Epoch [20/120    avg_loss:0.235, val_acc:0.889]
Epoch [21/120    avg_loss:0.197, val_acc:0.941]
Epoch [22/120    avg_loss:0.165, val_acc:0.943]
Epoch [23/120    avg_loss:0.162, val_acc:0.927]
Epoch [24/120    avg_loss:0.132, val_acc:0.914]
Epoch [25/120    avg_loss:0.148, val_acc:0.927]
Epoch [26/120    avg_loss:0.130, val_acc:0.937]
Epoch [27/120    avg_loss:0.139, val_acc:0.900]
Epoch [28/120    avg_loss:0.151, val_acc:0.938]
Epoch [29/120    avg_loss:0.136, val_acc:0.928]
Epoch [30/120    avg_loss:0.110, val_acc:0.948]
Epoch [31/120    avg_loss:0.134, val_acc:0.914]
Epoch [32/120    avg_loss:0.136, val_acc:0.962]
Epoch [33/120    avg_loss:0.096, val_acc:0.939]
Epoch [34/120    avg_loss:0.098, val_acc:0.933]
Epoch [35/120    avg_loss:0.093, val_acc:0.967]
Epoch [36/120    avg_loss:0.081, val_acc:0.973]
Epoch [37/120    avg_loss:0.061, val_acc:0.965]
Epoch [38/120    avg_loss:0.057, val_acc:0.968]
Epoch [39/120    avg_loss:0.054, val_acc:0.930]
Epoch [40/120    avg_loss:0.047, val_acc:0.980]
Epoch [41/120    avg_loss:0.037, val_acc:0.969]
Epoch [42/120    avg_loss:0.046, val_acc:0.972]
Epoch [43/120    avg_loss:0.052, val_acc:0.965]
Epoch [44/120    avg_loss:0.111, val_acc:0.949]
Epoch [45/120    avg_loss:0.054, val_acc:0.970]
Epoch [46/120    avg_loss:0.068, val_acc:0.923]
Epoch [47/120    avg_loss:0.096, val_acc:0.911]
Epoch [48/120    avg_loss:0.060, val_acc:0.973]
Epoch [49/120    avg_loss:0.064, val_acc:0.965]
Epoch [50/120    avg_loss:0.057, val_acc:0.979]
Epoch [51/120    avg_loss:0.063, val_acc:0.939]
Epoch [52/120    avg_loss:0.070, val_acc:0.966]
Epoch [53/120    avg_loss:0.034, val_acc:0.973]
Epoch [54/120    avg_loss:0.032, val_acc:0.983]
Epoch [55/120    avg_loss:0.021, val_acc:0.984]
Epoch [56/120    avg_loss:0.024, val_acc:0.984]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.018, val_acc:0.985]
Epoch [59/120    avg_loss:0.022, val_acc:0.985]
Epoch [60/120    avg_loss:0.019, val_acc:0.985]
Epoch [61/120    avg_loss:0.022, val_acc:0.985]
Epoch [62/120    avg_loss:0.021, val_acc:0.985]
Epoch [63/120    avg_loss:0.018, val_acc:0.984]
Epoch [64/120    avg_loss:0.019, val_acc:0.988]
Epoch [65/120    avg_loss:0.017, val_acc:0.987]
Epoch [66/120    avg_loss:0.017, val_acc:0.988]
Epoch [67/120    avg_loss:0.014, val_acc:0.986]
Epoch [68/120    avg_loss:0.022, val_acc:0.987]
Epoch [69/120    avg_loss:0.021, val_acc:0.985]
Epoch [70/120    avg_loss:0.018, val_acc:0.989]
Epoch [71/120    avg_loss:0.020, val_acc:0.985]
Epoch [72/120    avg_loss:0.013, val_acc:0.989]
Epoch [73/120    avg_loss:0.015, val_acc:0.989]
Epoch [74/120    avg_loss:0.012, val_acc:0.989]
Epoch [75/120    avg_loss:0.015, val_acc:0.989]
Epoch [76/120    avg_loss:0.014, val_acc:0.988]
Epoch [77/120    avg_loss:0.014, val_acc:0.988]
Epoch [78/120    avg_loss:0.016, val_acc:0.991]
Epoch [79/120    avg_loss:0.015, val_acc:0.991]
Epoch [80/120    avg_loss:0.024, val_acc:0.986]
Epoch [81/120    avg_loss:0.015, val_acc:0.991]
Epoch [82/120    avg_loss:0.013, val_acc:0.989]
Epoch [83/120    avg_loss:0.015, val_acc:0.989]
Epoch [84/120    avg_loss:0.015, val_acc:0.990]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.016, val_acc:0.989]
Epoch [87/120    avg_loss:0.017, val_acc:0.987]
Epoch [88/120    avg_loss:0.015, val_acc:0.989]
Epoch [89/120    avg_loss:0.012, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.989]
Epoch [91/120    avg_loss:0.013, val_acc:0.987]
Epoch [92/120    avg_loss:0.013, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.988]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.014, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.014, val_acc:0.988]
Epoch [111/120    avg_loss:0.014, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.013, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.988]
Epoch [116/120    avg_loss:0.013, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.988]
Epoch [120/120    avg_loss:0.011, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     0     4     0     5    31     3     0]
 [    0     4 17941     0    78     0    67     0     0     0]
 [    0     0     0  2020     6     0     0     0     2     8]
 [    0    26    16     0  2885     0    12     0    32     1]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4876     0     0     2]
 [    0     7     0     0     0     0     8  1273     0     2]
 [    0     0     0    10    59     0     0     0  3475    27]
 [    0     0     0     0    29    58     0     1     0   831]]

Accuracy:
98.79497746607862

F1 scores:
[       nan 0.99377819 0.99542264 0.99360551 0.95640643 0.97749437
 0.99045298 0.98111753 0.98122265 0.92745536]

Kappa:
0.9840562074477102
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb82f57e978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.834, val_acc:0.287]
Epoch [2/120    avg_loss:1.242, val_acc:0.602]
Epoch [3/120    avg_loss:0.899, val_acc:0.619]
Epoch [4/120    avg_loss:0.715, val_acc:0.690]
Epoch [5/120    avg_loss:0.616, val_acc:0.732]
Epoch [6/120    avg_loss:0.534, val_acc:0.799]
Epoch [7/120    avg_loss:0.425, val_acc:0.878]
Epoch [8/120    avg_loss:0.399, val_acc:0.768]
Epoch [9/120    avg_loss:0.371, val_acc:0.905]
Epoch [10/120    avg_loss:0.266, val_acc:0.851]
Epoch [11/120    avg_loss:0.316, val_acc:0.881]
Epoch [12/120    avg_loss:0.243, val_acc:0.878]
Epoch [13/120    avg_loss:0.232, val_acc:0.883]
Epoch [14/120    avg_loss:0.216, val_acc:0.920]
Epoch [15/120    avg_loss:0.231, val_acc:0.942]
Epoch [16/120    avg_loss:0.205, val_acc:0.906]
Epoch [17/120    avg_loss:0.164, val_acc:0.946]
Epoch [18/120    avg_loss:0.126, val_acc:0.933]
Epoch [19/120    avg_loss:0.147, val_acc:0.907]
Epoch [20/120    avg_loss:0.126, val_acc:0.946]
Epoch [21/120    avg_loss:0.103, val_acc:0.953]
Epoch [22/120    avg_loss:0.071, val_acc:0.966]
Epoch [23/120    avg_loss:0.065, val_acc:0.961]
Epoch [24/120    avg_loss:0.062, val_acc:0.966]
Epoch [25/120    avg_loss:0.086, val_acc:0.966]
Epoch [26/120    avg_loss:0.075, val_acc:0.969]
Epoch [27/120    avg_loss:0.106, val_acc:0.965]
Epoch [28/120    avg_loss:0.120, val_acc:0.953]
Epoch [29/120    avg_loss:0.079, val_acc:0.959]
Epoch [30/120    avg_loss:0.060, val_acc:0.957]
Epoch [31/120    avg_loss:0.090, val_acc:0.949]
Epoch [32/120    avg_loss:0.061, val_acc:0.952]
Epoch [33/120    avg_loss:0.038, val_acc:0.951]
Epoch [34/120    avg_loss:0.043, val_acc:0.983]
Epoch [35/120    avg_loss:0.083, val_acc:0.938]
Epoch [36/120    avg_loss:0.074, val_acc:0.971]
Epoch [37/120    avg_loss:0.064, val_acc:0.978]
Epoch [38/120    avg_loss:0.106, val_acc:0.965]
Epoch [39/120    avg_loss:0.054, val_acc:0.906]
Epoch [40/120    avg_loss:0.097, val_acc:0.968]
Epoch [41/120    avg_loss:0.050, val_acc:0.975]
Epoch [42/120    avg_loss:0.058, val_acc:0.900]
Epoch [43/120    avg_loss:0.110, val_acc:0.931]
Epoch [44/120    avg_loss:0.097, val_acc:0.971]
Epoch [45/120    avg_loss:0.038, val_acc:0.982]
Epoch [46/120    avg_loss:0.028, val_acc:0.980]
Epoch [47/120    avg_loss:0.030, val_acc:0.979]
Epoch [48/120    avg_loss:0.018, val_acc:0.988]
Epoch [49/120    avg_loss:0.021, val_acc:0.986]
Epoch [50/120    avg_loss:0.016, val_acc:0.991]
Epoch [51/120    avg_loss:0.015, val_acc:0.991]
Epoch [52/120    avg_loss:0.015, val_acc:0.990]
Epoch [53/120    avg_loss:0.016, val_acc:0.987]
Epoch [54/120    avg_loss:0.015, val_acc:0.988]
Epoch [55/120    avg_loss:0.019, val_acc:0.987]
Epoch [56/120    avg_loss:0.013, val_acc:0.989]
Epoch [57/120    avg_loss:0.016, val_acc:0.988]
Epoch [58/120    avg_loss:0.013, val_acc:0.988]
Epoch [59/120    avg_loss:0.018, val_acc:0.986]
Epoch [60/120    avg_loss:0.020, val_acc:0.987]
Epoch [61/120    avg_loss:0.016, val_acc:0.988]
Epoch [62/120    avg_loss:0.012, val_acc:0.989]
Epoch [63/120    avg_loss:0.013, val_acc:0.989]
Epoch [64/120    avg_loss:0.014, val_acc:0.988]
Epoch [65/120    avg_loss:0.016, val_acc:0.988]
Epoch [66/120    avg_loss:0.013, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.986]
Epoch [68/120    avg_loss:0.013, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.986]
Epoch [70/120    avg_loss:0.014, val_acc:0.987]
Epoch [71/120    avg_loss:0.012, val_acc:0.987]
Epoch [72/120    avg_loss:0.014, val_acc:0.987]
Epoch [73/120    avg_loss:0.015, val_acc:0.988]
Epoch [74/120    avg_loss:0.014, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.988]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.016, val_acc:0.989]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.013, val_acc:0.989]
Epoch [81/120    avg_loss:0.017, val_acc:0.989]
Epoch [82/120    avg_loss:0.011, val_acc:0.989]
Epoch [83/120    avg_loss:0.019, val_acc:0.989]
Epoch [84/120    avg_loss:0.012, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.012, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.990]
Epoch [88/120    avg_loss:0.013, val_acc:0.989]
Epoch [89/120    avg_loss:0.011, val_acc:0.989]
Epoch [90/120    avg_loss:0.013, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.989]
Epoch [92/120    avg_loss:0.014, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.014, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.012, val_acc:0.989]
Epoch [97/120    avg_loss:0.012, val_acc:0.989]
Epoch [98/120    avg_loss:0.018, val_acc:0.989]
Epoch [99/120    avg_loss:0.016, val_acc:0.989]
Epoch [100/120    avg_loss:0.015, val_acc:0.989]
Epoch [101/120    avg_loss:0.017, val_acc:0.989]
Epoch [102/120    avg_loss:0.015, val_acc:0.989]
Epoch [103/120    avg_loss:0.013, val_acc:0.989]
Epoch [104/120    avg_loss:0.015, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.989]
Epoch [106/120    avg_loss:0.013, val_acc:0.989]
Epoch [107/120    avg_loss:0.011, val_acc:0.989]
Epoch [108/120    avg_loss:0.011, val_acc:0.989]
Epoch [109/120    avg_loss:0.010, val_acc:0.989]
Epoch [110/120    avg_loss:0.013, val_acc:0.989]
Epoch [111/120    avg_loss:0.013, val_acc:0.989]
Epoch [112/120    avg_loss:0.011, val_acc:0.989]
Epoch [113/120    avg_loss:0.014, val_acc:0.989]
Epoch [114/120    avg_loss:0.013, val_acc:0.989]
Epoch [115/120    avg_loss:0.015, val_acc:0.989]
Epoch [116/120    avg_loss:0.014, val_acc:0.989]
Epoch [117/120    avg_loss:0.012, val_acc:0.989]
Epoch [118/120    avg_loss:0.012, val_acc:0.989]
Epoch [119/120    avg_loss:0.011, val_acc:0.989]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     0     0     4     0    18     0]
 [    0     0 18022     0    65     0     2     0     1     0]
 [    0     0     0  2018     4     0     0     0     7     7]
 [    0    33    18     0  2878     0    10     1    32     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     1     0     0     0  4867     0     0    10]
 [    0     0     0     0     0     0     3  1282     0     5]
 [    0     7     0     0    55     0     0     0  3489    20]
 [    0     0     0     0    14    43     0     0     0   862]]

Accuracy:
99.12997373050877

F1 scores:
[       nan 0.99518708 0.9975921  0.99555994 0.96125585 0.98340875
 0.99692749 0.99650214 0.98033155 0.94517544]

Kappa:
0.9884777477677655
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7ad9d0908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.845, val_acc:0.332]
Epoch [2/120    avg_loss:1.309, val_acc:0.647]
Epoch [3/120    avg_loss:0.967, val_acc:0.745]
Epoch [4/120    avg_loss:0.789, val_acc:0.765]
Epoch [5/120    avg_loss:0.601, val_acc:0.825]
Epoch [6/120    avg_loss:0.515, val_acc:0.844]
Epoch [7/120    avg_loss:0.427, val_acc:0.800]
Epoch [8/120    avg_loss:0.368, val_acc:0.822]
Epoch [9/120    avg_loss:0.334, val_acc:0.885]
Epoch [10/120    avg_loss:0.312, val_acc:0.856]
Epoch [11/120    avg_loss:0.255, val_acc:0.834]
Epoch [12/120    avg_loss:0.242, val_acc:0.881]
Epoch [13/120    avg_loss:0.242, val_acc:0.891]
Epoch [14/120    avg_loss:0.201, val_acc:0.927]
Epoch [15/120    avg_loss:0.171, val_acc:0.883]
Epoch [16/120    avg_loss:0.140, val_acc:0.939]
Epoch [17/120    avg_loss:0.159, val_acc:0.919]
Epoch [18/120    avg_loss:0.132, val_acc:0.916]
Epoch [19/120    avg_loss:0.126, val_acc:0.929]
Epoch [20/120    avg_loss:0.119, val_acc:0.947]
Epoch [21/120    avg_loss:0.110, val_acc:0.955]
Epoch [22/120    avg_loss:0.102, val_acc:0.953]
Epoch [23/120    avg_loss:0.088, val_acc:0.932]
Epoch [24/120    avg_loss:0.156, val_acc:0.914]
Epoch [25/120    avg_loss:0.128, val_acc:0.856]
Epoch [26/120    avg_loss:0.486, val_acc:0.794]
Epoch [27/120    avg_loss:0.285, val_acc:0.824]
Epoch [28/120    avg_loss:0.139, val_acc:0.897]
Epoch [29/120    avg_loss:0.119, val_acc:0.932]
Epoch [30/120    avg_loss:0.082, val_acc:0.948]
Epoch [31/120    avg_loss:0.055, val_acc:0.934]
Epoch [32/120    avg_loss:0.087, val_acc:0.949]
Epoch [33/120    avg_loss:0.076, val_acc:0.961]
Epoch [34/120    avg_loss:0.051, val_acc:0.960]
Epoch [35/120    avg_loss:0.047, val_acc:0.962]
Epoch [36/120    avg_loss:0.054, val_acc:0.952]
Epoch [37/120    avg_loss:0.043, val_acc:0.960]
Epoch [38/120    avg_loss:0.035, val_acc:0.969]
Epoch [39/120    avg_loss:0.037, val_acc:0.961]
Epoch [40/120    avg_loss:0.031, val_acc:0.977]
Epoch [41/120    avg_loss:0.023, val_acc:0.966]
Epoch [42/120    avg_loss:0.047, val_acc:0.976]
Epoch [43/120    avg_loss:0.026, val_acc:0.973]
Epoch [44/120    avg_loss:0.015, val_acc:0.973]
Epoch [45/120    avg_loss:0.017, val_acc:0.974]
Epoch [46/120    avg_loss:0.061, val_acc:0.967]
Epoch [47/120    avg_loss:0.019, val_acc:0.974]
Epoch [48/120    avg_loss:0.040, val_acc:0.971]
Epoch [49/120    avg_loss:0.027, val_acc:0.975]
Epoch [50/120    avg_loss:0.019, val_acc:0.973]
Epoch [51/120    avg_loss:0.011, val_acc:0.978]
Epoch [52/120    avg_loss:0.013, val_acc:0.977]
Epoch [53/120    avg_loss:0.012, val_acc:0.976]
Epoch [54/120    avg_loss:0.017, val_acc:0.969]
Epoch [55/120    avg_loss:0.016, val_acc:0.962]
Epoch [56/120    avg_loss:0.015, val_acc:0.979]
Epoch [57/120    avg_loss:0.015, val_acc:0.980]
Epoch [58/120    avg_loss:0.107, val_acc:0.961]
Epoch [59/120    avg_loss:0.057, val_acc:0.972]
Epoch [60/120    avg_loss:0.032, val_acc:0.965]
Epoch [61/120    avg_loss:0.023, val_acc:0.973]
Epoch [62/120    avg_loss:0.022, val_acc:0.979]
Epoch [63/120    avg_loss:0.009, val_acc:0.979]
Epoch [64/120    avg_loss:0.008, val_acc:0.979]
Epoch [65/120    avg_loss:0.009, val_acc:0.981]
Epoch [66/120    avg_loss:0.010, val_acc:0.979]
Epoch [67/120    avg_loss:0.019, val_acc:0.967]
Epoch [68/120    avg_loss:0.011, val_acc:0.979]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.006, val_acc:0.983]
Epoch [71/120    avg_loss:0.010, val_acc:0.981]
Epoch [72/120    avg_loss:0.117, val_acc:0.909]
Epoch [73/120    avg_loss:0.048, val_acc:0.955]
Epoch [74/120    avg_loss:0.054, val_acc:0.980]
Epoch [75/120    avg_loss:0.023, val_acc:0.937]
Epoch [76/120    avg_loss:0.016, val_acc:0.979]
Epoch [77/120    avg_loss:0.011, val_acc:0.983]
Epoch [78/120    avg_loss:0.011, val_acc:0.973]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.007, val_acc:0.982]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.004, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.006, val_acc:0.976]
Epoch [85/120    avg_loss:0.008, val_acc:0.975]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.985]
Epoch [88/120    avg_loss:0.004, val_acc:0.985]
Epoch [89/120    avg_loss:0.004, val_acc:0.989]
Epoch [90/120    avg_loss:0.003, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.003, val_acc:0.986]
Epoch [97/120    avg_loss:0.003, val_acc:0.985]
Epoch [98/120    avg_loss:0.003, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.003, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.985]
Epoch [103/120    avg_loss:0.003, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.002, val_acc:0.986]
Epoch [106/120    avg_loss:0.003, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.002, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.002, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.002, val_acc:0.986]
Epoch [116/120    avg_loss:0.003, val_acc:0.986]
Epoch [117/120    avg_loss:0.002, val_acc:0.986]
Epoch [118/120    avg_loss:0.002, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.985]
Epoch [120/120    avg_loss:0.002, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     0     4     0    10     2    25     6]
 [    0     0 18038     0    44     0     5     0     0     3]
 [    0    20     0  2010     2     0     0     0     1     3]
 [    0    24    17     0  2906     0     2     0    23     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     0     0  4862     0    11     5]
 [    0     0     0     0     0     0     2  1281     0     7]
 [    0     3     0     2    47     0     0     0  3505    14]
 [    0     0     0     0    14    19     0     0     0   886]]

Accuracy:
99.23601571349384

F1 scores:
[       nan 0.99269279 0.99809102 0.993083   0.97044582 0.99200609
 0.99641357 0.99572483 0.98234305 0.9604336 ]

Kappa:
0.989880917039854
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7facedcfd940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.841, val_acc:0.395]
Epoch [2/120    avg_loss:1.205, val_acc:0.573]
Epoch [3/120    avg_loss:0.911, val_acc:0.628]
Epoch [4/120    avg_loss:0.703, val_acc:0.740]
Epoch [5/120    avg_loss:0.574, val_acc:0.829]
Epoch [6/120    avg_loss:0.514, val_acc:0.733]
Epoch [7/120    avg_loss:0.383, val_acc:0.745]
Epoch [8/120    avg_loss:0.370, val_acc:0.799]
Epoch [9/120    avg_loss:0.347, val_acc:0.790]
Epoch [10/120    avg_loss:0.313, val_acc:0.852]
Epoch [11/120    avg_loss:0.297, val_acc:0.857]
Epoch [12/120    avg_loss:0.280, val_acc:0.902]
Epoch [13/120    avg_loss:0.255, val_acc:0.891]
Epoch [14/120    avg_loss:0.264, val_acc:0.834]
Epoch [15/120    avg_loss:0.196, val_acc:0.913]
Epoch [16/120    avg_loss:0.190, val_acc:0.903]
Epoch [17/120    avg_loss:0.149, val_acc:0.913]
Epoch [18/120    avg_loss:0.182, val_acc:0.905]
Epoch [19/120    avg_loss:0.161, val_acc:0.908]
Epoch [20/120    avg_loss:0.202, val_acc:0.928]
Epoch [21/120    avg_loss:0.131, val_acc:0.922]
Epoch [22/120    avg_loss:0.120, val_acc:0.889]
Epoch [23/120    avg_loss:0.112, val_acc:0.905]
Epoch [24/120    avg_loss:0.108, val_acc:0.957]
Epoch [25/120    avg_loss:0.126, val_acc:0.937]
Epoch [26/120    avg_loss:0.858, val_acc:0.715]
Epoch [27/120    avg_loss:0.524, val_acc:0.818]
Epoch [28/120    avg_loss:0.334, val_acc:0.910]
Epoch [29/120    avg_loss:0.255, val_acc:0.864]
Epoch [30/120    avg_loss:0.190, val_acc:0.940]
Epoch [31/120    avg_loss:0.132, val_acc:0.936]
Epoch [32/120    avg_loss:0.154, val_acc:0.943]
Epoch [33/120    avg_loss:0.135, val_acc:0.951]
Epoch [34/120    avg_loss:0.081, val_acc:0.916]
Epoch [35/120    avg_loss:0.105, val_acc:0.963]
Epoch [36/120    avg_loss:0.071, val_acc:0.957]
Epoch [37/120    avg_loss:0.109, val_acc:0.921]
Epoch [38/120    avg_loss:0.067, val_acc:0.897]
Epoch [39/120    avg_loss:0.095, val_acc:0.964]
Epoch [40/120    avg_loss:0.090, val_acc:0.964]
Epoch [41/120    avg_loss:0.051, val_acc:0.965]
Epoch [42/120    avg_loss:0.099, val_acc:0.932]
Epoch [43/120    avg_loss:0.051, val_acc:0.970]
Epoch [44/120    avg_loss:0.047, val_acc:0.948]
Epoch [45/120    avg_loss:0.083, val_acc:0.915]
Epoch [46/120    avg_loss:0.052, val_acc:0.963]
Epoch [47/120    avg_loss:0.046, val_acc:0.964]
Epoch [48/120    avg_loss:0.045, val_acc:0.976]
Epoch [49/120    avg_loss:0.033, val_acc:0.954]
Epoch [50/120    avg_loss:0.037, val_acc:0.969]
Epoch [51/120    avg_loss:0.026, val_acc:0.977]
Epoch [52/120    avg_loss:0.035, val_acc:0.964]
Epoch [53/120    avg_loss:0.050, val_acc:0.972]
Epoch [54/120    avg_loss:0.085, val_acc:0.806]
Epoch [55/120    avg_loss:0.074, val_acc:0.971]
Epoch [56/120    avg_loss:0.044, val_acc:0.974]
Epoch [57/120    avg_loss:0.036, val_acc:0.974]
Epoch [58/120    avg_loss:0.085, val_acc:0.861]
Epoch [59/120    avg_loss:0.191, val_acc:0.944]
Epoch [60/120    avg_loss:0.110, val_acc:0.943]
Epoch [61/120    avg_loss:0.055, val_acc:0.971]
Epoch [62/120    avg_loss:0.038, val_acc:0.969]
Epoch [63/120    avg_loss:0.028, val_acc:0.969]
Epoch [64/120    avg_loss:0.039, val_acc:0.967]
Epoch [65/120    avg_loss:0.034, val_acc:0.977]
Epoch [66/120    avg_loss:0.023, val_acc:0.979]
Epoch [67/120    avg_loss:0.021, val_acc:0.978]
Epoch [68/120    avg_loss:0.021, val_acc:0.979]
Epoch [69/120    avg_loss:0.018, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.978]
Epoch [71/120    avg_loss:0.019, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.977]
Epoch [73/120    avg_loss:0.021, val_acc:0.980]
Epoch [74/120    avg_loss:0.020, val_acc:0.977]
Epoch [75/120    avg_loss:0.017, val_acc:0.982]
Epoch [76/120    avg_loss:0.021, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.980]
Epoch [78/120    avg_loss:0.016, val_acc:0.981]
Epoch [79/120    avg_loss:0.016, val_acc:0.982]
Epoch [80/120    avg_loss:0.019, val_acc:0.979]
Epoch [81/120    avg_loss:0.016, val_acc:0.979]
Epoch [82/120    avg_loss:0.016, val_acc:0.979]
Epoch [83/120    avg_loss:0.020, val_acc:0.982]
Epoch [84/120    avg_loss:0.015, val_acc:0.982]
Epoch [85/120    avg_loss:0.018, val_acc:0.983]
Epoch [86/120    avg_loss:0.015, val_acc:0.980]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.016, val_acc:0.981]
Epoch [89/120    avg_loss:0.017, val_acc:0.982]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.019, val_acc:0.980]
Epoch [92/120    avg_loss:0.014, val_acc:0.982]
Epoch [93/120    avg_loss:0.014, val_acc:0.982]
Epoch [94/120    avg_loss:0.013, val_acc:0.982]
Epoch [95/120    avg_loss:0.013, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.011, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.984]
Epoch [101/120    avg_loss:0.011, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.985]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.985]
Epoch [107/120    avg_loss:0.014, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.984]
Epoch [109/120    avg_loss:0.014, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.015, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.012, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     0     0     5    13    37     0]
 [    0     0 18000     0    16     0    74     0     0     0]
 [    0     0     0  2011     2     0     0     0    18     5]
 [    0    52    20     0  2870     0     4     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4877     0     0     0]
 [    0     0     0     0     0     0     1  1286     0     3]
 [    0     0     0     3    55     0     0     0  3490    23]
 [    0     0     0     0    20    66     0     0     0   833]]

Accuracy:
98.92993998987781

F1 scores:
[       nan 0.99168027 0.99695375 0.99284127 0.96714406 0.97533632
 0.99136091 0.99343376 0.97731728 0.93438026]

Kappa:
0.985830906112867
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20e72b5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.837, val_acc:0.614]
Epoch [2/120    avg_loss:1.251, val_acc:0.602]
Epoch [3/120    avg_loss:0.814, val_acc:0.779]
Epoch [4/120    avg_loss:0.616, val_acc:0.827]
Epoch [5/120    avg_loss:0.487, val_acc:0.752]
Epoch [6/120    avg_loss:0.490, val_acc:0.837]
Epoch [7/120    avg_loss:0.412, val_acc:0.868]
Epoch [8/120    avg_loss:0.361, val_acc:0.687]
Epoch [9/120    avg_loss:0.345, val_acc:0.900]
Epoch [10/120    avg_loss:0.375, val_acc:0.878]
Epoch [11/120    avg_loss:0.320, val_acc:0.842]
Epoch [12/120    avg_loss:0.277, val_acc:0.859]
Epoch [13/120    avg_loss:0.266, val_acc:0.919]
Epoch [14/120    avg_loss:0.210, val_acc:0.928]
Epoch [15/120    avg_loss:0.228, val_acc:0.928]
Epoch [16/120    avg_loss:0.199, val_acc:0.862]
Epoch [17/120    avg_loss:0.188, val_acc:0.936]
Epoch [18/120    avg_loss:0.155, val_acc:0.903]
Epoch [19/120    avg_loss:0.176, val_acc:0.930]
Epoch [20/120    avg_loss:0.151, val_acc:0.942]
Epoch [21/120    avg_loss:0.138, val_acc:0.951]
Epoch [22/120    avg_loss:0.110, val_acc:0.926]
Epoch [23/120    avg_loss:0.150, val_acc:0.924]
Epoch [24/120    avg_loss:0.124, val_acc:0.949]
Epoch [25/120    avg_loss:0.161, val_acc:0.931]
Epoch [26/120    avg_loss:0.107, val_acc:0.960]
Epoch [27/120    avg_loss:0.136, val_acc:0.945]
Epoch [28/120    avg_loss:0.359, val_acc:0.922]
Epoch [29/120    avg_loss:0.130, val_acc:0.942]
Epoch [30/120    avg_loss:0.112, val_acc:0.934]
Epoch [31/120    avg_loss:0.126, val_acc:0.953]
Epoch [32/120    avg_loss:0.107, val_acc:0.970]
Epoch [33/120    avg_loss:0.065, val_acc:0.964]
Epoch [34/120    avg_loss:0.077, val_acc:0.968]
Epoch [35/120    avg_loss:0.079, val_acc:0.941]
Epoch [36/120    avg_loss:0.065, val_acc:0.959]
Epoch [37/120    avg_loss:0.120, val_acc:0.965]
Epoch [38/120    avg_loss:0.063, val_acc:0.974]
Epoch [39/120    avg_loss:0.053, val_acc:0.919]
Epoch [40/120    avg_loss:0.057, val_acc:0.970]
Epoch [41/120    avg_loss:0.056, val_acc:0.969]
Epoch [42/120    avg_loss:0.051, val_acc:0.922]
Epoch [43/120    avg_loss:0.072, val_acc:0.965]
Epoch [44/120    avg_loss:0.046, val_acc:0.967]
Epoch [45/120    avg_loss:0.040, val_acc:0.973]
Epoch [46/120    avg_loss:0.039, val_acc:0.953]
Epoch [47/120    avg_loss:0.032, val_acc:0.984]
Epoch [48/120    avg_loss:0.045, val_acc:0.970]
Epoch [49/120    avg_loss:0.044, val_acc:0.975]
Epoch [50/120    avg_loss:0.052, val_acc:0.964]
Epoch [51/120    avg_loss:0.053, val_acc:0.971]
Epoch [52/120    avg_loss:0.022, val_acc:0.981]
Epoch [53/120    avg_loss:0.040, val_acc:0.977]
Epoch [54/120    avg_loss:0.053, val_acc:0.968]
Epoch [55/120    avg_loss:0.031, val_acc:0.985]
Epoch [56/120    avg_loss:0.027, val_acc:0.979]
Epoch [57/120    avg_loss:0.048, val_acc:0.966]
Epoch [58/120    avg_loss:0.027, val_acc:0.970]
Epoch [59/120    avg_loss:0.020, val_acc:0.982]
Epoch [60/120    avg_loss:0.015, val_acc:0.980]
Epoch [61/120    avg_loss:0.015, val_acc:0.986]
Epoch [62/120    avg_loss:0.017, val_acc:0.981]
Epoch [63/120    avg_loss:0.042, val_acc:0.933]
Epoch [64/120    avg_loss:0.080, val_acc:0.969]
Epoch [65/120    avg_loss:0.021, val_acc:0.979]
Epoch [66/120    avg_loss:0.022, val_acc:0.979]
Epoch [67/120    avg_loss:0.015, val_acc:0.976]
Epoch [68/120    avg_loss:0.015, val_acc:0.982]
Epoch [69/120    avg_loss:0.017, val_acc:0.980]
Epoch [70/120    avg_loss:0.015, val_acc:0.985]
Epoch [71/120    avg_loss:0.022, val_acc:0.984]
Epoch [72/120    avg_loss:0.014, val_acc:0.984]
Epoch [73/120    avg_loss:0.012, val_acc:0.987]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.032, val_acc:0.986]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.016, val_acc:0.973]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.989]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6422     0     0     0     0     8     0     2     0]
 [    0     0 18055     0    17     0    18     0     0     0]
 [    0     4     0  2029     2     0     0     0     0     1]
 [    0    37    23     0  2846     0     7     0    56     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4874     0     0     3]
 [    0     0     0     0     0     0     6  1284     0     0]
 [    0     1     0    13    66     0     0     0  3490     1]
 [    0     0     0     0    14    72     0     2     0   831]]

Accuracy:
99.13961391078013

F1 scores:
[       nan 0.99596774 0.99836877 0.99509564 0.96197397 0.97315436
 0.99560821 0.99689441 0.98047479 0.94539249]

Kappa:
0.9885989900808613
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd98efb6908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.801, val_acc:0.547]
Epoch [2/120    avg_loss:1.231, val_acc:0.668]
Epoch [3/120    avg_loss:0.881, val_acc:0.759]
Epoch [4/120    avg_loss:0.641, val_acc:0.698]
Epoch [5/120    avg_loss:0.513, val_acc:0.851]
Epoch [6/120    avg_loss:0.437, val_acc:0.826]
Epoch [7/120    avg_loss:0.391, val_acc:0.783]
Epoch [8/120    avg_loss:0.336, val_acc:0.858]
Epoch [9/120    avg_loss:0.276, val_acc:0.914]
Epoch [10/120    avg_loss:0.294, val_acc:0.884]
Epoch [11/120    avg_loss:0.341, val_acc:0.902]
Epoch [12/120    avg_loss:0.278, val_acc:0.881]
Epoch [13/120    avg_loss:0.243, val_acc:0.913]
Epoch [14/120    avg_loss:0.199, val_acc:0.926]
Epoch [15/120    avg_loss:0.254, val_acc:0.814]
Epoch [16/120    avg_loss:0.258, val_acc:0.935]
Epoch [17/120    avg_loss:0.199, val_acc:0.863]
Epoch [18/120    avg_loss:0.124, val_acc:0.952]
Epoch [19/120    avg_loss:0.128, val_acc:0.940]
Epoch [20/120    avg_loss:0.144, val_acc:0.915]
Epoch [21/120    avg_loss:0.101, val_acc:0.958]
Epoch [22/120    avg_loss:0.142, val_acc:0.953]
Epoch [23/120    avg_loss:0.103, val_acc:0.947]
Epoch [24/120    avg_loss:0.090, val_acc:0.957]
Epoch [25/120    avg_loss:0.110, val_acc:0.968]
Epoch [26/120    avg_loss:0.203, val_acc:0.915]
Epoch [27/120    avg_loss:0.148, val_acc:0.927]
Epoch [28/120    avg_loss:0.100, val_acc:0.937]
Epoch [29/120    avg_loss:0.087, val_acc:0.943]
Epoch [30/120    avg_loss:0.104, val_acc:0.934]
Epoch [31/120    avg_loss:0.481, val_acc:0.421]
Epoch [32/120    avg_loss:1.028, val_acc:0.578]
Epoch [33/120    avg_loss:0.845, val_acc:0.629]
Epoch [34/120    avg_loss:0.753, val_acc:0.671]
Epoch [35/120    avg_loss:0.723, val_acc:0.666]
Epoch [36/120    avg_loss:0.672, val_acc:0.691]
Epoch [37/120    avg_loss:0.623, val_acc:0.790]
Epoch [38/120    avg_loss:0.564, val_acc:0.763]
Epoch [39/120    avg_loss:0.538, val_acc:0.774]
Epoch [40/120    avg_loss:0.483, val_acc:0.797]
Epoch [41/120    avg_loss:0.487, val_acc:0.800]
Epoch [42/120    avg_loss:0.485, val_acc:0.801]
Epoch [43/120    avg_loss:0.468, val_acc:0.798]
Epoch [44/120    avg_loss:0.480, val_acc:0.816]
Epoch [45/120    avg_loss:0.476, val_acc:0.816]
Epoch [46/120    avg_loss:0.485, val_acc:0.793]
Epoch [47/120    avg_loss:0.445, val_acc:0.819]
Epoch [48/120    avg_loss:0.435, val_acc:0.822]
Epoch [49/120    avg_loss:0.469, val_acc:0.824]
Epoch [50/120    avg_loss:0.444, val_acc:0.809]
Epoch [51/120    avg_loss:0.455, val_acc:0.830]
Epoch [52/120    avg_loss:0.407, val_acc:0.829]
Epoch [53/120    avg_loss:0.460, val_acc:0.832]
Epoch [54/120    avg_loss:0.438, val_acc:0.830]
Epoch [55/120    avg_loss:0.447, val_acc:0.833]
Epoch [56/120    avg_loss:0.443, val_acc:0.833]
Epoch [57/120    avg_loss:0.451, val_acc:0.829]
Epoch [58/120    avg_loss:0.415, val_acc:0.829]
Epoch [59/120    avg_loss:0.418, val_acc:0.830]
Epoch [60/120    avg_loss:0.456, val_acc:0.830]
Epoch [61/120    avg_loss:0.416, val_acc:0.829]
Epoch [62/120    avg_loss:0.396, val_acc:0.828]
Epoch [63/120    avg_loss:0.420, val_acc:0.831]
Epoch [64/120    avg_loss:0.425, val_acc:0.829]
Epoch [65/120    avg_loss:0.396, val_acc:0.829]
Epoch [66/120    avg_loss:0.426, val_acc:0.830]
Epoch [67/120    avg_loss:0.431, val_acc:0.830]
Epoch [68/120    avg_loss:0.413, val_acc:0.829]
Epoch [69/120    avg_loss:0.401, val_acc:0.829]
Epoch [70/120    avg_loss:0.425, val_acc:0.830]
Epoch [71/120    avg_loss:0.412, val_acc:0.830]
Epoch [72/120    avg_loss:0.408, val_acc:0.831]
Epoch [73/120    avg_loss:0.392, val_acc:0.830]
Epoch [74/120    avg_loss:0.415, val_acc:0.831]
Epoch [75/120    avg_loss:0.413, val_acc:0.830]
Epoch [76/120    avg_loss:0.417, val_acc:0.830]
Epoch [77/120    avg_loss:0.418, val_acc:0.830]
Epoch [78/120    avg_loss:0.414, val_acc:0.831]
Epoch [79/120    avg_loss:0.427, val_acc:0.831]
Epoch [80/120    avg_loss:0.417, val_acc:0.831]
Epoch [81/120    avg_loss:0.438, val_acc:0.831]
Epoch [82/120    avg_loss:0.421, val_acc:0.831]
Epoch [83/120    avg_loss:0.427, val_acc:0.831]
Epoch [84/120    avg_loss:0.445, val_acc:0.831]
Epoch [85/120    avg_loss:0.439, val_acc:0.831]
Epoch [86/120    avg_loss:0.420, val_acc:0.831]
Epoch [87/120    avg_loss:0.431, val_acc:0.831]
Epoch [88/120    avg_loss:0.429, val_acc:0.831]
Epoch [89/120    avg_loss:0.416, val_acc:0.831]
Epoch [90/120    avg_loss:0.414, val_acc:0.831]
Epoch [91/120    avg_loss:0.401, val_acc:0.831]
Epoch [92/120    avg_loss:0.433, val_acc:0.831]
Epoch [93/120    avg_loss:0.417, val_acc:0.831]
Epoch [94/120    avg_loss:0.436, val_acc:0.831]
Epoch [95/120    avg_loss:0.423, val_acc:0.831]
Epoch [96/120    avg_loss:0.443, val_acc:0.831]
Epoch [97/120    avg_loss:0.427, val_acc:0.831]
Epoch [98/120    avg_loss:0.430, val_acc:0.831]
Epoch [99/120    avg_loss:0.425, val_acc:0.831]
Epoch [100/120    avg_loss:0.416, val_acc:0.831]
Epoch [101/120    avg_loss:0.430, val_acc:0.831]
Epoch [102/120    avg_loss:0.416, val_acc:0.831]
Epoch [103/120    avg_loss:0.434, val_acc:0.831]
Epoch [104/120    avg_loss:0.416, val_acc:0.831]
Epoch [105/120    avg_loss:0.443, val_acc:0.831]
Epoch [106/120    avg_loss:0.439, val_acc:0.831]
Epoch [107/120    avg_loss:0.422, val_acc:0.831]
Epoch [108/120    avg_loss:0.447, val_acc:0.831]
Epoch [109/120    avg_loss:0.426, val_acc:0.831]
Epoch [110/120    avg_loss:0.433, val_acc:0.831]
Epoch [111/120    avg_loss:0.428, val_acc:0.831]
Epoch [112/120    avg_loss:0.450, val_acc:0.831]
Epoch [113/120    avg_loss:0.422, val_acc:0.831]
Epoch [114/120    avg_loss:0.433, val_acc:0.831]
Epoch [115/120    avg_loss:0.417, val_acc:0.831]
Epoch [116/120    avg_loss:0.413, val_acc:0.831]
Epoch [117/120    avg_loss:0.445, val_acc:0.831]
Epoch [118/120    avg_loss:0.394, val_acc:0.831]
Epoch [119/120    avg_loss:0.432, val_acc:0.831]
Epoch [120/120    avg_loss:0.413, val_acc:0.831]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5003    19   208   365     0    63    43   630   101]
 [    0    33 14470     0   323     0  3264     0     0     0]
 [    0     7     0  1802    12     0     0     0   166    49]
 [    0   131   126     0  2566     0    96     0    50     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   158     0    21     0  4699     0     0     0]
 [    0    32     0     0     0     0     7  1227     0    24]
 [    0   122     1    65    71     0    13     0  3299     0]
 [    0    25     1     5    21   134     4     2    10   717]]

Accuracy:
84.56366134046706

F1 scores:
[       nan 0.8490454  0.88057204 0.87560739 0.80806172 0.95116618
 0.72159091 0.95784543 0.85399948 0.79095422]

Kappa:
0.802666259210041
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc752a8908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.721, val_acc:0.634]
Epoch [2/120    avg_loss:1.170, val_acc:0.653]
Epoch [3/120    avg_loss:0.845, val_acc:0.652]
Epoch [4/120    avg_loss:0.643, val_acc:0.799]
Epoch [5/120    avg_loss:0.494, val_acc:0.759]
Epoch [6/120    avg_loss:0.471, val_acc:0.870]
Epoch [7/120    avg_loss:0.385, val_acc:0.820]
Epoch [8/120    avg_loss:0.341, val_acc:0.799]
Epoch [9/120    avg_loss:0.318, val_acc:0.913]
Epoch [10/120    avg_loss:0.337, val_acc:0.857]
Epoch [11/120    avg_loss:0.292, val_acc:0.897]
Epoch [12/120    avg_loss:0.236, val_acc:0.832]
Epoch [13/120    avg_loss:0.241, val_acc:0.916]
Epoch [14/120    avg_loss:0.248, val_acc:0.908]
Epoch [15/120    avg_loss:0.205, val_acc:0.930]
Epoch [16/120    avg_loss:0.217, val_acc:0.902]
Epoch [17/120    avg_loss:0.425, val_acc:0.836]
Epoch [18/120    avg_loss:0.290, val_acc:0.924]
Epoch [19/120    avg_loss:0.191, val_acc:0.934]
Epoch [20/120    avg_loss:0.150, val_acc:0.948]
Epoch [21/120    avg_loss:0.161, val_acc:0.943]
Epoch [22/120    avg_loss:0.134, val_acc:0.914]
Epoch [23/120    avg_loss:0.101, val_acc:0.950]
Epoch [24/120    avg_loss:0.089, val_acc:0.956]
Epoch [25/120    avg_loss:0.107, val_acc:0.955]
Epoch [26/120    avg_loss:0.130, val_acc:0.941]
Epoch [27/120    avg_loss:0.127, val_acc:0.962]
Epoch [28/120    avg_loss:0.096, val_acc:0.952]
Epoch [29/120    avg_loss:0.079, val_acc:0.966]
Epoch [30/120    avg_loss:0.109, val_acc:0.805]
Epoch [31/120    avg_loss:0.149, val_acc:0.951]
Epoch [32/120    avg_loss:0.103, val_acc:0.959]
Epoch [33/120    avg_loss:0.066, val_acc:0.970]
Epoch [34/120    avg_loss:0.081, val_acc:0.979]
Epoch [35/120    avg_loss:0.053, val_acc:0.940]
Epoch [36/120    avg_loss:0.073, val_acc:0.901]
Epoch [37/120    avg_loss:0.141, val_acc:0.929]
Epoch [38/120    avg_loss:0.063, val_acc:0.975]
Epoch [39/120    avg_loss:0.057, val_acc:0.961]
Epoch [40/120    avg_loss:0.047, val_acc:0.966]
Epoch [41/120    avg_loss:0.044, val_acc:0.979]
Epoch [42/120    avg_loss:0.035, val_acc:0.974]
Epoch [43/120    avg_loss:0.037, val_acc:0.968]
Epoch [44/120    avg_loss:0.033, val_acc:0.977]
Epoch [45/120    avg_loss:0.061, val_acc:0.969]
Epoch [46/120    avg_loss:0.041, val_acc:0.979]
Epoch [47/120    avg_loss:0.038, val_acc:0.952]
Epoch [48/120    avg_loss:0.036, val_acc:0.965]
Epoch [49/120    avg_loss:0.073, val_acc:0.973]
Epoch [50/120    avg_loss:0.035, val_acc:0.983]
Epoch [51/120    avg_loss:0.023, val_acc:0.976]
Epoch [52/120    avg_loss:0.030, val_acc:0.985]
Epoch [53/120    avg_loss:0.025, val_acc:0.979]
Epoch [54/120    avg_loss:0.023, val_acc:0.986]
Epoch [55/120    avg_loss:0.024, val_acc:0.983]
Epoch [56/120    avg_loss:0.023, val_acc:0.988]
Epoch [57/120    avg_loss:0.036, val_acc:0.908]
Epoch [58/120    avg_loss:0.036, val_acc:0.984]
Epoch [59/120    avg_loss:0.024, val_acc:0.985]
Epoch [60/120    avg_loss:0.017, val_acc:0.984]
Epoch [61/120    avg_loss:0.012, val_acc:0.987]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.032, val_acc:0.987]
Epoch [64/120    avg_loss:0.016, val_acc:0.976]
Epoch [65/120    avg_loss:0.012, val_acc:0.986]
Epoch [66/120    avg_loss:0.015, val_acc:0.985]
Epoch [67/120    avg_loss:0.012, val_acc:0.982]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.016, val_acc:0.986]
Epoch [70/120    avg_loss:0.012, val_acc:0.985]
Epoch [71/120    avg_loss:0.009, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.989]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.012, val_acc:0.989]
Epoch [76/120    avg_loss:0.007, val_acc:0.989]
Epoch [77/120    avg_loss:0.006, val_acc:0.989]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.008, val_acc:0.991]
Epoch [80/120    avg_loss:0.007, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.991]
Epoch [82/120    avg_loss:0.006, val_acc:0.990]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.006, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.991]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.009, val_acc:0.991]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.992]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.008, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.991]
Epoch [98/120    avg_loss:0.006, val_acc:0.991]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.991]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.008, val_acc:0.991]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0     0     0    11     0    46     0]
 [    0     0 18039     0    50     0     1     0     0     0]
 [    0     1     0  1980     4     0     0     0    50     1]
 [    0    39    25     0  2887     0     3     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0     7     0    19    66     0     0     0  3472     7]
 [    0     0     0     0    15    66     0     0     0   838]]

Accuracy:
98.92993998987781

F1 scores:
[       nan 0.99190913 0.99759436 0.98141264 0.96329663 0.97533632
 0.99733607 0.9984472  0.97023893 0.94742793]

Kappa:
0.9858229326778122
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6db5421940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.857, val_acc:0.597]
Epoch [2/120    avg_loss:1.221, val_acc:0.567]
Epoch [3/120    avg_loss:0.867, val_acc:0.676]
Epoch [4/120    avg_loss:0.669, val_acc:0.739]
Epoch [5/120    avg_loss:0.620, val_acc:0.761]
Epoch [6/120    avg_loss:0.536, val_acc:0.824]
Epoch [7/120    avg_loss:0.382, val_acc:0.826]
Epoch [8/120    avg_loss:0.833, val_acc:0.678]
Epoch [9/120    avg_loss:0.619, val_acc:0.808]
Epoch [10/120    avg_loss:0.439, val_acc:0.801]
Epoch [11/120    avg_loss:0.358, val_acc:0.791]
Epoch [12/120    avg_loss:0.701, val_acc:0.540]
Epoch [13/120    avg_loss:0.910, val_acc:0.639]
Epoch [14/120    avg_loss:0.764, val_acc:0.739]
Epoch [15/120    avg_loss:0.644, val_acc:0.786]
Epoch [16/120    avg_loss:0.659, val_acc:0.784]
Epoch [17/120    avg_loss:0.494, val_acc:0.777]
Epoch [18/120    avg_loss:0.522, val_acc:0.745]
Epoch [19/120    avg_loss:0.421, val_acc:0.803]
Epoch [20/120    avg_loss:0.436, val_acc:0.791]
Epoch [21/120    avg_loss:0.388, val_acc:0.823]
Epoch [22/120    avg_loss:0.353, val_acc:0.844]
Epoch [23/120    avg_loss:0.349, val_acc:0.853]
Epoch [24/120    avg_loss:0.320, val_acc:0.864]
Epoch [25/120    avg_loss:0.301, val_acc:0.855]
Epoch [26/120    avg_loss:0.316, val_acc:0.877]
Epoch [27/120    avg_loss:0.303, val_acc:0.857]
Epoch [28/120    avg_loss:0.291, val_acc:0.878]
Epoch [29/120    avg_loss:0.287, val_acc:0.873]
Epoch [30/120    avg_loss:0.288, val_acc:0.866]
Epoch [31/120    avg_loss:0.297, val_acc:0.873]
Epoch [32/120    avg_loss:0.283, val_acc:0.890]
Epoch [33/120    avg_loss:0.252, val_acc:0.877]
Epoch [34/120    avg_loss:0.273, val_acc:0.893]
Epoch [35/120    avg_loss:0.243, val_acc:0.862]
Epoch [36/120    avg_loss:0.268, val_acc:0.891]
Epoch [37/120    avg_loss:0.258, val_acc:0.878]
Epoch [38/120    avg_loss:0.256, val_acc:0.862]
Epoch [39/120    avg_loss:0.261, val_acc:0.859]
Epoch [40/120    avg_loss:0.261, val_acc:0.874]
Epoch [41/120    avg_loss:0.243, val_acc:0.886]
Epoch [42/120    avg_loss:0.256, val_acc:0.879]
Epoch [43/120    avg_loss:0.249, val_acc:0.880]
Epoch [44/120    avg_loss:0.249, val_acc:0.879]
Epoch [45/120    avg_loss:0.233, val_acc:0.895]
Epoch [46/120    avg_loss:0.279, val_acc:0.890]
Epoch [47/120    avg_loss:0.222, val_acc:0.888]
Epoch [48/120    avg_loss:0.231, val_acc:0.902]
Epoch [49/120    avg_loss:0.224, val_acc:0.903]
Epoch [50/120    avg_loss:0.239, val_acc:0.900]
Epoch [51/120    avg_loss:0.214, val_acc:0.884]
Epoch [52/120    avg_loss:0.238, val_acc:0.877]
Epoch [53/120    avg_loss:0.217, val_acc:0.882]
Epoch [54/120    avg_loss:0.215, val_acc:0.909]
Epoch [55/120    avg_loss:0.201, val_acc:0.874]
Epoch [56/120    avg_loss:0.214, val_acc:0.897]
Epoch [57/120    avg_loss:0.226, val_acc:0.885]
Epoch [58/120    avg_loss:0.211, val_acc:0.887]
Epoch [59/120    avg_loss:0.210, val_acc:0.894]
Epoch [60/120    avg_loss:0.178, val_acc:0.876]
Epoch [61/120    avg_loss:0.206, val_acc:0.903]
Epoch [62/120    avg_loss:0.208, val_acc:0.903]
Epoch [63/120    avg_loss:0.185, val_acc:0.905]
Epoch [64/120    avg_loss:0.199, val_acc:0.889]
Epoch [65/120    avg_loss:0.193, val_acc:0.909]
Epoch [66/120    avg_loss:0.201, val_acc:0.916]
Epoch [67/120    avg_loss:0.205, val_acc:0.895]
Epoch [68/120    avg_loss:0.216, val_acc:0.904]
Epoch [69/120    avg_loss:0.188, val_acc:0.916]
Epoch [70/120    avg_loss:0.185, val_acc:0.911]
Epoch [71/120    avg_loss:0.180, val_acc:0.903]
Epoch [72/120    avg_loss:0.168, val_acc:0.910]
Epoch [73/120    avg_loss:0.177, val_acc:0.922]
Epoch [74/120    avg_loss:0.171, val_acc:0.900]
Epoch [75/120    avg_loss:0.168, val_acc:0.922]
Epoch [76/120    avg_loss:0.173, val_acc:0.920]
Epoch [77/120    avg_loss:0.164, val_acc:0.921]
Epoch [78/120    avg_loss:0.170, val_acc:0.921]
Epoch [79/120    avg_loss:0.164, val_acc:0.910]
Epoch [80/120    avg_loss:0.173, val_acc:0.917]
Epoch [81/120    avg_loss:0.168, val_acc:0.908]
Epoch [82/120    avg_loss:0.156, val_acc:0.912]
Epoch [83/120    avg_loss:0.161, val_acc:0.932]
Epoch [84/120    avg_loss:0.180, val_acc:0.929]
Epoch [85/120    avg_loss:0.172, val_acc:0.908]
Epoch [86/120    avg_loss:0.167, val_acc:0.932]
Epoch [87/120    avg_loss:0.158, val_acc:0.940]
Epoch [88/120    avg_loss:0.168, val_acc:0.915]
Epoch [89/120    avg_loss:0.139, val_acc:0.913]
Epoch [90/120    avg_loss:0.166, val_acc:0.922]
Epoch [91/120    avg_loss:0.164, val_acc:0.930]
Epoch [92/120    avg_loss:0.150, val_acc:0.912]
Epoch [93/120    avg_loss:0.150, val_acc:0.933]
Epoch [94/120    avg_loss:0.153, val_acc:0.928]
Epoch [95/120    avg_loss:0.163, val_acc:0.927]
Epoch [96/120    avg_loss:0.155, val_acc:0.927]
Epoch [97/120    avg_loss:0.135, val_acc:0.911]
Epoch [98/120    avg_loss:0.118, val_acc:0.938]
Epoch [99/120    avg_loss:0.143, val_acc:0.912]
Epoch [100/120    avg_loss:0.140, val_acc:0.930]
Epoch [101/120    avg_loss:0.137, val_acc:0.943]
Epoch [102/120    avg_loss:0.124, val_acc:0.941]
Epoch [103/120    avg_loss:0.128, val_acc:0.941]
Epoch [104/120    avg_loss:0.137, val_acc:0.942]
Epoch [105/120    avg_loss:0.125, val_acc:0.939]
Epoch [106/120    avg_loss:0.126, val_acc:0.936]
Epoch [107/120    avg_loss:0.128, val_acc:0.941]
Epoch [108/120    avg_loss:0.123, val_acc:0.941]
Epoch [109/120    avg_loss:0.134, val_acc:0.941]
Epoch [110/120    avg_loss:0.116, val_acc:0.941]
Epoch [111/120    avg_loss:0.120, val_acc:0.941]
Epoch [112/120    avg_loss:0.119, val_acc:0.941]
Epoch [113/120    avg_loss:0.126, val_acc:0.938]
Epoch [114/120    avg_loss:0.122, val_acc:0.944]
Epoch [115/120    avg_loss:0.126, val_acc:0.941]
Epoch [116/120    avg_loss:0.119, val_acc:0.941]
Epoch [117/120    avg_loss:0.129, val_acc:0.936]
Epoch [118/120    avg_loss:0.127, val_acc:0.937]
Epoch [119/120    avg_loss:0.111, val_acc:0.941]
Epoch [120/120    avg_loss:0.112, val_acc:0.939]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5653     4    63   266     0     0    54   299    93]
 [    0     1 17272     0   115     0   701     0     1     0]
 [    0    12     0  1987     0     0     0     0    22    15]
 [    0    79    64     0  2726     0    27     0    76     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     4     0  4873     0     0     0]
 [    0     1     0     0     0     0     5  1280     0     4]
 [    0    56     0     0    57     0     0     0  3458     0]
 [    0     7     2     5    23    84     0     0     0   798]]

Accuracy:
94.84009350974864

F1 scores:
[       nan 0.92361735 0.97491039 0.97140064 0.88463411 0.9688196
 0.92960702 0.97560976 0.93119698 0.87260798]

Kappa:
0.9322591635239984
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f43201b8908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.824, val_acc:0.475]
Epoch [2/120    avg_loss:1.157, val_acc:0.688]
Epoch [3/120    avg_loss:0.802, val_acc:0.687]
Epoch [4/120    avg_loss:0.647, val_acc:0.802]
Epoch [5/120    avg_loss:0.574, val_acc:0.788]
Epoch [6/120    avg_loss:0.478, val_acc:0.828]
Epoch [7/120    avg_loss:0.398, val_acc:0.855]
Epoch [8/120    avg_loss:0.342, val_acc:0.765]
Epoch [9/120    avg_loss:0.295, val_acc:0.849]
Epoch [10/120    avg_loss:0.273, val_acc:0.851]
Epoch [11/120    avg_loss:0.296, val_acc:0.921]
Epoch [12/120    avg_loss:0.224, val_acc:0.910]
Epoch [13/120    avg_loss:0.208, val_acc:0.864]
Epoch [14/120    avg_loss:0.209, val_acc:0.904]
Epoch [15/120    avg_loss:0.246, val_acc:0.901]
Epoch [16/120    avg_loss:0.200, val_acc:0.933]
Epoch [17/120    avg_loss:0.159, val_acc:0.925]
Epoch [18/120    avg_loss:0.147, val_acc:0.947]
Epoch [19/120    avg_loss:0.187, val_acc:0.929]
Epoch [20/120    avg_loss:0.181, val_acc:0.946]
Epoch [21/120    avg_loss:0.122, val_acc:0.951]
Epoch [22/120    avg_loss:0.077, val_acc:0.949]
Epoch [23/120    avg_loss:0.090, val_acc:0.957]
Epoch [24/120    avg_loss:0.280, val_acc:0.929]
Epoch [25/120    avg_loss:0.173, val_acc:0.946]
Epoch [26/120    avg_loss:0.135, val_acc:0.949]
Epoch [27/120    avg_loss:0.105, val_acc:0.946]
Epoch [28/120    avg_loss:0.058, val_acc:0.952]
Epoch [29/120    avg_loss:0.078, val_acc:0.976]
Epoch [30/120    avg_loss:0.084, val_acc:0.963]
Epoch [31/120    avg_loss:0.056, val_acc:0.946]
Epoch [32/120    avg_loss:0.063, val_acc:0.975]
Epoch [33/120    avg_loss:0.067, val_acc:0.977]
Epoch [34/120    avg_loss:0.041, val_acc:0.986]
Epoch [35/120    avg_loss:0.040, val_acc:0.973]
Epoch [36/120    avg_loss:0.039, val_acc:0.976]
Epoch [37/120    avg_loss:0.040, val_acc:0.969]
Epoch [38/120    avg_loss:0.045, val_acc:0.984]
Epoch [39/120    avg_loss:0.023, val_acc:0.979]
Epoch [40/120    avg_loss:0.058, val_acc:0.978]
Epoch [41/120    avg_loss:0.033, val_acc:0.932]
Epoch [42/120    avg_loss:0.042, val_acc:0.979]
Epoch [43/120    avg_loss:0.052, val_acc:0.957]
Epoch [44/120    avg_loss:0.049, val_acc:0.982]
Epoch [45/120    avg_loss:0.046, val_acc:0.961]
Epoch [46/120    avg_loss:0.041, val_acc:0.987]
Epoch [47/120    avg_loss:0.036, val_acc:0.971]
Epoch [48/120    avg_loss:0.023, val_acc:0.986]
Epoch [49/120    avg_loss:0.032, val_acc:0.984]
Epoch [50/120    avg_loss:0.024, val_acc:0.980]
Epoch [51/120    avg_loss:0.034, val_acc:0.973]
Epoch [52/120    avg_loss:0.132, val_acc:0.956]
Epoch [53/120    avg_loss:0.078, val_acc:0.984]
Epoch [54/120    avg_loss:0.029, val_acc:0.986]
Epoch [55/120    avg_loss:0.028, val_acc:0.972]
Epoch [56/120    avg_loss:0.025, val_acc:0.982]
Epoch [57/120    avg_loss:0.018, val_acc:0.986]
Epoch [58/120    avg_loss:0.012, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.986]
Epoch [60/120    avg_loss:0.011, val_acc:0.986]
Epoch [61/120    avg_loss:0.011, val_acc:0.992]
Epoch [62/120    avg_loss:0.008, val_acc:0.989]
Epoch [63/120    avg_loss:0.014, val_acc:0.990]
Epoch [64/120    avg_loss:0.017, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.979]
Epoch [66/120    avg_loss:0.023, val_acc:0.980]
Epoch [67/120    avg_loss:0.135, val_acc:0.952]
Epoch [68/120    avg_loss:0.041, val_acc:0.974]
Epoch [69/120    avg_loss:0.028, val_acc:0.982]
Epoch [70/120    avg_loss:0.020, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.017, val_acc:0.983]
Epoch [73/120    avg_loss:0.012, val_acc:0.989]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.011, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.989]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.990]
Epoch [94/120    avg_loss:0.010, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     2     0    19     0    16     0]
 [    0     5 18054     0    24     0     7     0     0     0]
 [    0     0     0  2027     1     0     0     0     4     4]
 [    0    28    17     0  2898     0     7     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4874     0     0     2]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0     4     2    25    43     0     0     0  3492     5]
 [    0     0     0     0    22    55     0     0     0   842]]

Accuracy:
99.22637553322248

F1 scores:
[       nan 0.99424751 0.99842389 0.99168297 0.97215699 0.9793621
 0.9962187  0.99805825 0.98296974 0.9476646 ]

Kappa:
0.989750323350609
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f051577f860>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.781, val_acc:0.198]
Epoch [2/120    avg_loss:1.310, val_acc:0.396]
Epoch [3/120    avg_loss:1.016, val_acc:0.676]
Epoch [4/120    avg_loss:0.794, val_acc:0.702]
Epoch [5/120    avg_loss:0.642, val_acc:0.724]
Epoch [6/120    avg_loss:0.507, val_acc:0.784]
Epoch [7/120    avg_loss:0.426, val_acc:0.798]
Epoch [8/120    avg_loss:0.413, val_acc:0.750]
Epoch [9/120    avg_loss:0.360, val_acc:0.823]
Epoch [10/120    avg_loss:0.311, val_acc:0.830]
Epoch [11/120    avg_loss:0.268, val_acc:0.851]
Epoch [12/120    avg_loss:0.244, val_acc:0.912]
Epoch [13/120    avg_loss:0.217, val_acc:0.908]
Epoch [14/120    avg_loss:0.211, val_acc:0.877]
Epoch [15/120    avg_loss:0.273, val_acc:0.906]
Epoch [16/120    avg_loss:0.204, val_acc:0.808]
Epoch [17/120    avg_loss:0.158, val_acc:0.927]
Epoch [18/120    avg_loss:0.157, val_acc:0.898]
Epoch [19/120    avg_loss:0.144, val_acc:0.941]
Epoch [20/120    avg_loss:0.122, val_acc:0.930]
Epoch [21/120    avg_loss:0.116, val_acc:0.935]
Epoch [22/120    avg_loss:0.118, val_acc:0.936]
Epoch [23/120    avg_loss:0.111, val_acc:0.913]
Epoch [24/120    avg_loss:0.075, val_acc:0.961]
Epoch [25/120    avg_loss:0.075, val_acc:0.953]
Epoch [26/120    avg_loss:0.084, val_acc:0.964]
Epoch [27/120    avg_loss:0.083, val_acc:0.823]
Epoch [28/120    avg_loss:0.083, val_acc:0.970]
Epoch [29/120    avg_loss:0.079, val_acc:0.943]
Epoch [30/120    avg_loss:0.068, val_acc:0.961]
Epoch [31/120    avg_loss:0.044, val_acc:0.959]
Epoch [32/120    avg_loss:0.064, val_acc:0.975]
Epoch [33/120    avg_loss:0.077, val_acc:0.956]
Epoch [34/120    avg_loss:0.086, val_acc:0.877]
Epoch [35/120    avg_loss:0.089, val_acc:0.964]
Epoch [36/120    avg_loss:0.050, val_acc:0.965]
Epoch [37/120    avg_loss:0.039, val_acc:0.979]
Epoch [38/120    avg_loss:0.071, val_acc:0.965]
Epoch [39/120    avg_loss:0.057, val_acc:0.973]
Epoch [40/120    avg_loss:0.057, val_acc:0.942]
Epoch [41/120    avg_loss:0.040, val_acc:0.979]
Epoch [42/120    avg_loss:0.030, val_acc:0.977]
Epoch [43/120    avg_loss:0.029, val_acc:0.979]
Epoch [44/120    avg_loss:0.040, val_acc:0.976]
Epoch [45/120    avg_loss:0.029, val_acc:0.976]
Epoch [46/120    avg_loss:0.023, val_acc:0.948]
Epoch [47/120    avg_loss:0.029, val_acc:0.977]
Epoch [48/120    avg_loss:0.014, val_acc:0.978]
Epoch [49/120    avg_loss:0.019, val_acc:0.975]
Epoch [50/120    avg_loss:0.024, val_acc:0.979]
Epoch [51/120    avg_loss:0.012, val_acc:0.980]
Epoch [52/120    avg_loss:0.010, val_acc:0.979]
Epoch [53/120    avg_loss:0.012, val_acc:0.982]
Epoch [54/120    avg_loss:0.015, val_acc:0.982]
Epoch [55/120    avg_loss:0.011, val_acc:0.981]
Epoch [56/120    avg_loss:0.013, val_acc:0.984]
Epoch [57/120    avg_loss:0.012, val_acc:0.984]
Epoch [58/120    avg_loss:0.007, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.013, val_acc:0.984]
Epoch [61/120    avg_loss:0.009, val_acc:0.985]
Epoch [62/120    avg_loss:0.011, val_acc:0.984]
Epoch [63/120    avg_loss:0.011, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.985]
Epoch [65/120    avg_loss:0.008, val_acc:0.984]
Epoch [66/120    avg_loss:0.007, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.008, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.009, val_acc:0.984]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     0     0     0     0     0     7    53    23]
 [    0     0 17941     0   116     0    33     0     0     0]
 [    0     6     0  1929     0     0     0     0   101     0]
 [    0    14    10     0  2923     0    19     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     3     0  4868     0     2     0]
 [    0    26     0     0     0     0     0  1263     1     0]
 [    0    18     2    41    27     0     9     0  3474     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.72990624924687

F1 scores:
[       nan 0.98855586 0.99539503 0.96305542 0.96772058 0.99808795
 0.99276027 0.98671875 0.96446419 0.9827957 ]

Kappa:
0.9831948716585359
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f495c1d2908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.890, val_acc:0.497]
Epoch [2/120    avg_loss:1.338, val_acc:0.688]
Epoch [3/120    avg_loss:1.010, val_acc:0.687]
Epoch [4/120    avg_loss:0.781, val_acc:0.600]
Epoch [5/120    avg_loss:0.641, val_acc:0.736]
Epoch [6/120    avg_loss:0.504, val_acc:0.721]
Epoch [7/120    avg_loss:0.422, val_acc:0.795]
Epoch [8/120    avg_loss:0.354, val_acc:0.755]
Epoch [9/120    avg_loss:0.346, val_acc:0.884]
Epoch [10/120    avg_loss:0.284, val_acc:0.840]
Epoch [11/120    avg_loss:0.241, val_acc:0.896]
Epoch [12/120    avg_loss:0.201, val_acc:0.898]
Epoch [13/120    avg_loss:0.174, val_acc:0.834]
Epoch [14/120    avg_loss:0.187, val_acc:0.879]
Epoch [15/120    avg_loss:0.191, val_acc:0.881]
Epoch [16/120    avg_loss:0.127, val_acc:0.902]
Epoch [17/120    avg_loss:0.127, val_acc:0.923]
Epoch [18/120    avg_loss:0.114, val_acc:0.921]
Epoch [19/120    avg_loss:0.119, val_acc:0.943]
Epoch [20/120    avg_loss:0.084, val_acc:0.958]
Epoch [21/120    avg_loss:0.132, val_acc:0.881]
Epoch [22/120    avg_loss:0.140, val_acc:0.926]
Epoch [23/120    avg_loss:0.135, val_acc:0.928]
Epoch [24/120    avg_loss:0.072, val_acc:0.952]
Epoch [25/120    avg_loss:0.059, val_acc:0.942]
Epoch [26/120    avg_loss:0.080, val_acc:0.967]
Epoch [27/120    avg_loss:0.071, val_acc:0.927]
Epoch [28/120    avg_loss:0.084, val_acc:0.917]
Epoch [29/120    avg_loss:0.058, val_acc:0.975]
Epoch [30/120    avg_loss:0.081, val_acc:0.954]
Epoch [31/120    avg_loss:0.055, val_acc:0.963]
Epoch [32/120    avg_loss:0.060, val_acc:0.942]
Epoch [33/120    avg_loss:0.047, val_acc:0.970]
Epoch [34/120    avg_loss:0.045, val_acc:0.962]
Epoch [35/120    avg_loss:0.049, val_acc:0.967]
Epoch [36/120    avg_loss:0.041, val_acc:0.970]
Epoch [37/120    avg_loss:0.038, val_acc:0.951]
Epoch [38/120    avg_loss:0.039, val_acc:0.958]
Epoch [39/120    avg_loss:0.045, val_acc:0.973]
Epoch [40/120    avg_loss:0.039, val_acc:0.968]
Epoch [41/120    avg_loss:0.039, val_acc:0.956]
Epoch [42/120    avg_loss:0.033, val_acc:0.972]
Epoch [43/120    avg_loss:0.025, val_acc:0.975]
Epoch [44/120    avg_loss:0.016, val_acc:0.983]
Epoch [45/120    avg_loss:0.016, val_acc:0.984]
Epoch [46/120    avg_loss:0.012, val_acc:0.983]
Epoch [47/120    avg_loss:0.019, val_acc:0.981]
Epoch [48/120    avg_loss:0.015, val_acc:0.984]
Epoch [49/120    avg_loss:0.018, val_acc:0.984]
Epoch [50/120    avg_loss:0.015, val_acc:0.985]
Epoch [51/120    avg_loss:0.009, val_acc:0.985]
Epoch [52/120    avg_loss:0.015, val_acc:0.984]
Epoch [53/120    avg_loss:0.009, val_acc:0.984]
Epoch [54/120    avg_loss:0.013, val_acc:0.986]
Epoch [55/120    avg_loss:0.015, val_acc:0.986]
Epoch [56/120    avg_loss:0.011, val_acc:0.988]
Epoch [57/120    avg_loss:0.012, val_acc:0.984]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.015, val_acc:0.984]
Epoch [60/120    avg_loss:0.012, val_acc:0.988]
Epoch [61/120    avg_loss:0.017, val_acc:0.985]
Epoch [62/120    avg_loss:0.017, val_acc:0.985]
Epoch [63/120    avg_loss:0.015, val_acc:0.985]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.013, val_acc:0.984]
Epoch [66/120    avg_loss:0.013, val_acc:0.985]
Epoch [67/120    avg_loss:0.009, val_acc:0.985]
Epoch [68/120    avg_loss:0.013, val_acc:0.985]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.013, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.984]
Epoch [76/120    avg_loss:0.029, val_acc:0.984]
Epoch [77/120    avg_loss:0.013, val_acc:0.984]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.984]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.013, val_acc:0.985]
Epoch [93/120    avg_loss:0.010, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.011, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.985]
Epoch [111/120    avg_loss:0.016, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.985]
Epoch [114/120    avg_loss:0.012, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.014, val_acc:0.985]
Epoch [120/120    avg_loss:0.012, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     8     0     0     1     3    37     3]
 [    0     1 17999     0    27     0    53     0    10     0]
 [    0     3     0  1909     0     0     0     0   122     2]
 [    0    21     5     0  2930     0     3     0    10     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    28     0     4     0  4845     0     1     0]
 [    0    34     0     0     0     0     0  1254     0     2]
 [    0    15     0    54    41     0     0     0  3460     1]
 [    0     0     0     0    10    13     0     0     0   896]]

Accuracy:
98.75882679006098

F1 scores:
[       nan 0.99022195 0.99656719 0.95283254 0.97927807 0.99504384
 0.99079755 0.98468787 0.95964499 0.98138007]

Kappa:
0.9835612247479931
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b64122940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.938, val_acc:0.470]
Epoch [2/120    avg_loss:1.343, val_acc:0.593]
Epoch [3/120    avg_loss:1.017, val_acc:0.665]
Epoch [4/120    avg_loss:0.830, val_acc:0.722]
Epoch [5/120    avg_loss:0.708, val_acc:0.639]
Epoch [6/120    avg_loss:0.571, val_acc:0.632]
Epoch [7/120    avg_loss:0.550, val_acc:0.684]
Epoch [8/120    avg_loss:0.439, val_acc:0.803]
Epoch [9/120    avg_loss:0.380, val_acc:0.821]
Epoch [10/120    avg_loss:0.376, val_acc:0.817]
Epoch [11/120    avg_loss:0.326, val_acc:0.768]
Epoch [12/120    avg_loss:0.314, val_acc:0.875]
Epoch [13/120    avg_loss:0.278, val_acc:0.809]
Epoch [14/120    avg_loss:0.258, val_acc:0.818]
Epoch [15/120    avg_loss:0.226, val_acc:0.886]
Epoch [16/120    avg_loss:0.208, val_acc:0.858]
Epoch [17/120    avg_loss:0.211, val_acc:0.861]
Epoch [18/120    avg_loss:0.176, val_acc:0.884]
Epoch [19/120    avg_loss:0.183, val_acc:0.873]
Epoch [20/120    avg_loss:0.143, val_acc:0.887]
Epoch [21/120    avg_loss:0.170, val_acc:0.885]
Epoch [22/120    avg_loss:0.153, val_acc:0.940]
Epoch [23/120    avg_loss:0.165, val_acc:0.936]
Epoch [24/120    avg_loss:0.157, val_acc:0.928]
Epoch [25/120    avg_loss:0.134, val_acc:0.888]
Epoch [26/120    avg_loss:0.148, val_acc:0.868]
Epoch [27/120    avg_loss:0.133, val_acc:0.912]
Epoch [28/120    avg_loss:0.099, val_acc:0.941]
Epoch [29/120    avg_loss:0.079, val_acc:0.959]
Epoch [30/120    avg_loss:0.076, val_acc:0.961]
Epoch [31/120    avg_loss:0.094, val_acc:0.968]
Epoch [32/120    avg_loss:0.067, val_acc:0.962]
Epoch [33/120    avg_loss:0.070, val_acc:0.948]
Epoch [34/120    avg_loss:0.078, val_acc:0.964]
Epoch [35/120    avg_loss:0.057, val_acc:0.947]
Epoch [36/120    avg_loss:0.106, val_acc:0.947]
Epoch [37/120    avg_loss:0.073, val_acc:0.967]
Epoch [38/120    avg_loss:0.049, val_acc:0.960]
Epoch [39/120    avg_loss:0.048, val_acc:0.923]
Epoch [40/120    avg_loss:0.086, val_acc:0.952]
Epoch [41/120    avg_loss:0.089, val_acc:0.953]
Epoch [42/120    avg_loss:0.054, val_acc:0.975]
Epoch [43/120    avg_loss:0.051, val_acc:0.965]
Epoch [44/120    avg_loss:0.050, val_acc:0.953]
Epoch [45/120    avg_loss:0.060, val_acc:0.959]
Epoch [46/120    avg_loss:0.091, val_acc:0.949]
Epoch [47/120    avg_loss:0.055, val_acc:0.965]
Epoch [48/120    avg_loss:0.045, val_acc:0.963]
Epoch [49/120    avg_loss:0.052, val_acc:0.969]
Epoch [50/120    avg_loss:0.050, val_acc:0.968]
Epoch [51/120    avg_loss:0.042, val_acc:0.958]
Epoch [52/120    avg_loss:0.065, val_acc:0.943]
Epoch [53/120    avg_loss:0.081, val_acc:0.937]
Epoch [54/120    avg_loss:0.043, val_acc:0.977]
Epoch [55/120    avg_loss:0.039, val_acc:0.975]
Epoch [56/120    avg_loss:0.055, val_acc:0.957]
Epoch [57/120    avg_loss:0.038, val_acc:0.958]
Epoch [58/120    avg_loss:0.034, val_acc:0.965]
Epoch [59/120    avg_loss:0.024, val_acc:0.979]
Epoch [60/120    avg_loss:0.026, val_acc:0.972]
Epoch [61/120    avg_loss:0.022, val_acc:0.980]
Epoch [62/120    avg_loss:0.013, val_acc:0.976]
Epoch [63/120    avg_loss:0.020, val_acc:0.974]
Epoch [64/120    avg_loss:0.028, val_acc:0.976]
Epoch [65/120    avg_loss:0.020, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.981]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.011, val_acc:0.981]
Epoch [69/120    avg_loss:0.010, val_acc:0.983]
Epoch [70/120    avg_loss:0.018, val_acc:0.946]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.029, val_acc:0.943]
Epoch [74/120    avg_loss:0.031, val_acc:0.979]
Epoch [75/120    avg_loss:0.018, val_acc:0.973]
Epoch [76/120    avg_loss:0.033, val_acc:0.974]
Epoch [77/120    avg_loss:0.024, val_acc:0.968]
Epoch [78/120    avg_loss:0.015, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.006, val_acc:0.981]
Epoch [81/120    avg_loss:0.010, val_acc:0.978]
Epoch [82/120    avg_loss:0.024, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.965]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.010, val_acc:0.980]
Epoch [86/120    avg_loss:0.008, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.008, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.980]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.004, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.982]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.005, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.007, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.981]
Epoch [105/120    avg_loss:0.006, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.004, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.981]
Epoch [113/120    avg_loss:0.007, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.005, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.005, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6297     2     4     0     0    30     0    84    15]
 [    0     0 17956     0    51     0    78     0     5     0]
 [    0    13     0  1900     0     0     0     0   123     0]
 [    0    14     6     0  2942     0     7     0     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     0     0     0  4857     0     0     0]
 [    0    21     0     0     0     0     0  1265     4     0]
 [    0    37     0    37    26     0    24     0  3447     0]
 [    0     0     0     0     7    18     0     0     0   894]]

Accuracy:
98.48167160725906

F1 scores:
[       nan 0.98283128 0.99548164 0.95549409 0.98099366 0.99315068
 0.98379583 0.99021526 0.95273632 0.97758338]

Kappa:
0.9799022639941511
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fef1f2219b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.866, val_acc:0.322]
Epoch [2/120    avg_loss:1.319, val_acc:0.695]
Epoch [3/120    avg_loss:1.060, val_acc:0.613]
Epoch [4/120    avg_loss:0.774, val_acc:0.749]
Epoch [5/120    avg_loss:0.633, val_acc:0.716]
Epoch [6/120    avg_loss:0.547, val_acc:0.726]
Epoch [7/120    avg_loss:0.413, val_acc:0.789]
Epoch [8/120    avg_loss:0.360, val_acc:0.785]
Epoch [9/120    avg_loss:0.334, val_acc:0.792]
Epoch [10/120    avg_loss:0.320, val_acc:0.885]
Epoch [11/120    avg_loss:0.290, val_acc:0.914]
Epoch [12/120    avg_loss:0.266, val_acc:0.857]
Epoch [13/120    avg_loss:0.232, val_acc:0.887]
Epoch [14/120    avg_loss:0.237, val_acc:0.876]
Epoch [15/120    avg_loss:0.193, val_acc:0.911]
Epoch [16/120    avg_loss:0.194, val_acc:0.867]
Epoch [17/120    avg_loss:0.190, val_acc:0.928]
Epoch [18/120    avg_loss:0.163, val_acc:0.914]
Epoch [19/120    avg_loss:0.129, val_acc:0.943]
Epoch [20/120    avg_loss:0.124, val_acc:0.954]
Epoch [21/120    avg_loss:0.117, val_acc:0.951]
Epoch [22/120    avg_loss:0.101, val_acc:0.938]
Epoch [23/120    avg_loss:0.123, val_acc:0.936]
Epoch [24/120    avg_loss:0.188, val_acc:0.410]
Epoch [25/120    avg_loss:0.545, val_acc:0.876]
Epoch [26/120    avg_loss:0.215, val_acc:0.883]
Epoch [27/120    avg_loss:0.185, val_acc:0.941]
Epoch [28/120    avg_loss:0.159, val_acc:0.945]
Epoch [29/120    avg_loss:0.147, val_acc:0.952]
Epoch [30/120    avg_loss:0.122, val_acc:0.930]
Epoch [31/120    avg_loss:0.122, val_acc:0.937]
Epoch [32/120    avg_loss:0.079, val_acc:0.943]
Epoch [33/120    avg_loss:0.113, val_acc:0.953]
Epoch [34/120    avg_loss:0.064, val_acc:0.965]
Epoch [35/120    avg_loss:0.062, val_acc:0.965]
Epoch [36/120    avg_loss:0.054, val_acc:0.968]
Epoch [37/120    avg_loss:0.055, val_acc:0.969]
Epoch [38/120    avg_loss:0.054, val_acc:0.970]
Epoch [39/120    avg_loss:0.066, val_acc:0.969]
Epoch [40/120    avg_loss:0.053, val_acc:0.970]
Epoch [41/120    avg_loss:0.052, val_acc:0.965]
Epoch [42/120    avg_loss:0.053, val_acc:0.970]
Epoch [43/120    avg_loss:0.044, val_acc:0.970]
Epoch [44/120    avg_loss:0.047, val_acc:0.970]
Epoch [45/120    avg_loss:0.055, val_acc:0.968]
Epoch [46/120    avg_loss:0.043, val_acc:0.970]
Epoch [47/120    avg_loss:0.046, val_acc:0.972]
Epoch [48/120    avg_loss:0.041, val_acc:0.967]
Epoch [49/120    avg_loss:0.048, val_acc:0.973]
Epoch [50/120    avg_loss:0.046, val_acc:0.973]
Epoch [51/120    avg_loss:0.039, val_acc:0.975]
Epoch [52/120    avg_loss:0.043, val_acc:0.973]
Epoch [53/120    avg_loss:0.041, val_acc:0.972]
Epoch [54/120    avg_loss:0.044, val_acc:0.969]
Epoch [55/120    avg_loss:0.050, val_acc:0.971]
Epoch [56/120    avg_loss:0.040, val_acc:0.973]
Epoch [57/120    avg_loss:0.042, val_acc:0.971]
Epoch [58/120    avg_loss:0.046, val_acc:0.970]
Epoch [59/120    avg_loss:0.047, val_acc:0.971]
Epoch [60/120    avg_loss:0.034, val_acc:0.971]
Epoch [61/120    avg_loss:0.038, val_acc:0.970]
Epoch [62/120    avg_loss:0.048, val_acc:0.971]
Epoch [63/120    avg_loss:0.045, val_acc:0.972]
Epoch [64/120    avg_loss:0.037, val_acc:0.975]
Epoch [65/120    avg_loss:0.043, val_acc:0.973]
Epoch [66/120    avg_loss:0.041, val_acc:0.978]
Epoch [67/120    avg_loss:0.037, val_acc:0.972]
Epoch [68/120    avg_loss:0.038, val_acc:0.973]
Epoch [69/120    avg_loss:0.036, val_acc:0.979]
Epoch [70/120    avg_loss:0.031, val_acc:0.977]
Epoch [71/120    avg_loss:0.033, val_acc:0.978]
Epoch [72/120    avg_loss:0.037, val_acc:0.975]
Epoch [73/120    avg_loss:0.034, val_acc:0.978]
Epoch [74/120    avg_loss:0.034, val_acc:0.976]
Epoch [75/120    avg_loss:0.044, val_acc:0.975]
Epoch [76/120    avg_loss:0.028, val_acc:0.978]
Epoch [77/120    avg_loss:0.034, val_acc:0.974]
Epoch [78/120    avg_loss:0.036, val_acc:0.976]
Epoch [79/120    avg_loss:0.037, val_acc:0.976]
Epoch [80/120    avg_loss:0.035, val_acc:0.976]
Epoch [81/120    avg_loss:0.029, val_acc:0.977]
Epoch [82/120    avg_loss:0.026, val_acc:0.980]
Epoch [83/120    avg_loss:0.026, val_acc:0.977]
Epoch [84/120    avg_loss:0.030, val_acc:0.981]
Epoch [85/120    avg_loss:0.044, val_acc:0.976]
Epoch [86/120    avg_loss:0.034, val_acc:0.979]
Epoch [87/120    avg_loss:0.037, val_acc:0.979]
Epoch [88/120    avg_loss:0.045, val_acc:0.978]
Epoch [89/120    avg_loss:0.031, val_acc:0.973]
Epoch [90/120    avg_loss:0.030, val_acc:0.978]
Epoch [91/120    avg_loss:0.027, val_acc:0.977]
Epoch [92/120    avg_loss:0.030, val_acc:0.976]
Epoch [93/120    avg_loss:0.032, val_acc:0.979]
Epoch [94/120    avg_loss:0.026, val_acc:0.982]
Epoch [95/120    avg_loss:0.033, val_acc:0.977]
Epoch [96/120    avg_loss:0.030, val_acc:0.978]
Epoch [97/120    avg_loss:0.026, val_acc:0.980]
Epoch [98/120    avg_loss:0.032, val_acc:0.983]
Epoch [99/120    avg_loss:0.050, val_acc:0.976]
Epoch [100/120    avg_loss:0.030, val_acc:0.983]
Epoch [101/120    avg_loss:0.031, val_acc:0.977]
Epoch [102/120    avg_loss:0.026, val_acc:0.980]
Epoch [103/120    avg_loss:0.030, val_acc:0.968]
Epoch [104/120    avg_loss:0.034, val_acc:0.980]
Epoch [105/120    avg_loss:0.020, val_acc:0.977]
Epoch [106/120    avg_loss:0.023, val_acc:0.977]
Epoch [107/120    avg_loss:0.020, val_acc:0.977]
Epoch [108/120    avg_loss:0.025, val_acc:0.979]
Epoch [109/120    avg_loss:0.028, val_acc:0.979]
Epoch [110/120    avg_loss:0.025, val_acc:0.981]
Epoch [111/120    avg_loss:0.024, val_acc:0.979]
Epoch [112/120    avg_loss:0.029, val_acc:0.975]
Epoch [113/120    avg_loss:0.027, val_acc:0.981]
Epoch [114/120    avg_loss:0.027, val_acc:0.980]
Epoch [115/120    avg_loss:0.028, val_acc:0.979]
Epoch [116/120    avg_loss:0.022, val_acc:0.979]
Epoch [117/120    avg_loss:0.025, val_acc:0.979]
Epoch [118/120    avg_loss:0.025, val_acc:0.980]
Epoch [119/120    avg_loss:0.021, val_acc:0.980]
Epoch [120/120    avg_loss:0.028, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6307     0     2     1     0     2    42    77     1]
 [    0     0 17940     0    15     0   133     0     2     0]
 [    0     1     0  1870     0     0     0     0   164     1]
 [    0    15     6     0  2933     0    14     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0    12     0     0     0     0     0  1277     1     0]
 [    0     4     0    98    32     0     3     0  3433     1]
 [    0     0     1     0     2    13     0     0     0   903]]

Accuracy:
98.42865061576651

F1 scores:
[       nan 0.98770652 0.99564337 0.9335996  0.98505458 0.99504384
 0.98414622 0.97891913 0.94625138 0.9890471 ]

Kappa:
0.9792125764656332
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb04e2ba908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.821, val_acc:0.200]
Epoch [2/120    avg_loss:1.336, val_acc:0.642]
Epoch [3/120    avg_loss:1.017, val_acc:0.725]
Epoch [4/120    avg_loss:0.827, val_acc:0.701]
Epoch [5/120    avg_loss:0.619, val_acc:0.760]
Epoch [6/120    avg_loss:0.477, val_acc:0.784]
Epoch [7/120    avg_loss:0.392, val_acc:0.785]
Epoch [8/120    avg_loss:0.336, val_acc:0.807]
Epoch [9/120    avg_loss:0.336, val_acc:0.841]
Epoch [10/120    avg_loss:0.243, val_acc:0.895]
Epoch [11/120    avg_loss:0.268, val_acc:0.897]
Epoch [12/120    avg_loss:0.280, val_acc:0.869]
Epoch [13/120    avg_loss:0.247, val_acc:0.907]
Epoch [14/120    avg_loss:0.217, val_acc:0.843]
Epoch [15/120    avg_loss:0.229, val_acc:0.833]
Epoch [16/120    avg_loss:0.235, val_acc:0.860]
Epoch [17/120    avg_loss:0.244, val_acc:0.878]
Epoch [18/120    avg_loss:0.191, val_acc:0.801]
Epoch [19/120    avg_loss:0.141, val_acc:0.887]
Epoch [20/120    avg_loss:0.142, val_acc:0.941]
Epoch [21/120    avg_loss:0.129, val_acc:0.959]
Epoch [22/120    avg_loss:0.154, val_acc:0.903]
Epoch [23/120    avg_loss:0.240, val_acc:0.865]
Epoch [24/120    avg_loss:0.237, val_acc:0.884]
Epoch [25/120    avg_loss:0.193, val_acc:0.924]
Epoch [26/120    avg_loss:0.191, val_acc:0.891]
Epoch [27/120    avg_loss:0.143, val_acc:0.925]
Epoch [28/120    avg_loss:0.140, val_acc:0.840]
Epoch [29/120    avg_loss:0.118, val_acc:0.930]
Epoch [30/120    avg_loss:0.108, val_acc:0.946]
Epoch [31/120    avg_loss:0.120, val_acc:0.905]
Epoch [32/120    avg_loss:0.113, val_acc:0.945]
Epoch [33/120    avg_loss:0.081, val_acc:0.933]
Epoch [34/120    avg_loss:0.091, val_acc:0.949]
Epoch [35/120    avg_loss:0.066, val_acc:0.961]
Epoch [36/120    avg_loss:0.057, val_acc:0.963]
Epoch [37/120    avg_loss:0.052, val_acc:0.961]
Epoch [38/120    avg_loss:0.042, val_acc:0.964]
Epoch [39/120    avg_loss:0.049, val_acc:0.965]
Epoch [40/120    avg_loss:0.053, val_acc:0.973]
Epoch [41/120    avg_loss:0.061, val_acc:0.964]
Epoch [42/120    avg_loss:0.045, val_acc:0.968]
Epoch [43/120    avg_loss:0.052, val_acc:0.967]
Epoch [44/120    avg_loss:0.054, val_acc:0.967]
Epoch [45/120    avg_loss:0.044, val_acc:0.968]
Epoch [46/120    avg_loss:0.055, val_acc:0.969]
Epoch [47/120    avg_loss:0.048, val_acc:0.963]
Epoch [48/120    avg_loss:0.040, val_acc:0.967]
Epoch [49/120    avg_loss:0.041, val_acc:0.970]
Epoch [50/120    avg_loss:0.041, val_acc:0.972]
Epoch [51/120    avg_loss:0.044, val_acc:0.970]
Epoch [52/120    avg_loss:0.044, val_acc:0.966]
Epoch [53/120    avg_loss:0.041, val_acc:0.965]
Epoch [54/120    avg_loss:0.035, val_acc:0.967]
Epoch [55/120    avg_loss:0.038, val_acc:0.970]
Epoch [56/120    avg_loss:0.040, val_acc:0.970]
Epoch [57/120    avg_loss:0.034, val_acc:0.970]
Epoch [58/120    avg_loss:0.040, val_acc:0.970]
Epoch [59/120    avg_loss:0.045, val_acc:0.970]
Epoch [60/120    avg_loss:0.038, val_acc:0.968]
Epoch [61/120    avg_loss:0.043, val_acc:0.969]
Epoch [62/120    avg_loss:0.050, val_acc:0.970]
Epoch [63/120    avg_loss:0.039, val_acc:0.970]
Epoch [64/120    avg_loss:0.044, val_acc:0.970]
Epoch [65/120    avg_loss:0.036, val_acc:0.970]
Epoch [66/120    avg_loss:0.043, val_acc:0.968]
Epoch [67/120    avg_loss:0.044, val_acc:0.968]
Epoch [68/120    avg_loss:0.042, val_acc:0.968]
Epoch [69/120    avg_loss:0.045, val_acc:0.968]
Epoch [70/120    avg_loss:0.040, val_acc:0.968]
Epoch [71/120    avg_loss:0.036, val_acc:0.968]
Epoch [72/120    avg_loss:0.034, val_acc:0.968]
Epoch [73/120    avg_loss:0.037, val_acc:0.968]
Epoch [74/120    avg_loss:0.043, val_acc:0.968]
Epoch [75/120    avg_loss:0.047, val_acc:0.968]
Epoch [76/120    avg_loss:0.047, val_acc:0.968]
Epoch [77/120    avg_loss:0.038, val_acc:0.968]
Epoch [78/120    avg_loss:0.037, val_acc:0.968]
Epoch [79/120    avg_loss:0.035, val_acc:0.968]
Epoch [80/120    avg_loss:0.039, val_acc:0.968]
Epoch [81/120    avg_loss:0.035, val_acc:0.968]
Epoch [82/120    avg_loss:0.040, val_acc:0.968]
Epoch [83/120    avg_loss:0.036, val_acc:0.968]
Epoch [84/120    avg_loss:0.041, val_acc:0.968]
Epoch [85/120    avg_loss:0.052, val_acc:0.968]
Epoch [86/120    avg_loss:0.041, val_acc:0.968]
Epoch [87/120    avg_loss:0.043, val_acc:0.968]
Epoch [88/120    avg_loss:0.036, val_acc:0.968]
Epoch [89/120    avg_loss:0.037, val_acc:0.968]
Epoch [90/120    avg_loss:0.042, val_acc:0.968]
Epoch [91/120    avg_loss:0.037, val_acc:0.968]
Epoch [92/120    avg_loss:0.038, val_acc:0.968]
Epoch [93/120    avg_loss:0.037, val_acc:0.968]
Epoch [94/120    avg_loss:0.041, val_acc:0.968]
Epoch [95/120    avg_loss:0.039, val_acc:0.968]
Epoch [96/120    avg_loss:0.035, val_acc:0.968]
Epoch [97/120    avg_loss:0.039, val_acc:0.968]
Epoch [98/120    avg_loss:0.042, val_acc:0.968]
Epoch [99/120    avg_loss:0.039, val_acc:0.968]
Epoch [100/120    avg_loss:0.045, val_acc:0.968]
Epoch [101/120    avg_loss:0.049, val_acc:0.968]
Epoch [102/120    avg_loss:0.042, val_acc:0.968]
Epoch [103/120    avg_loss:0.049, val_acc:0.968]
Epoch [104/120    avg_loss:0.044, val_acc:0.968]
Epoch [105/120    avg_loss:0.042, val_acc:0.968]
Epoch [106/120    avg_loss:0.035, val_acc:0.968]
Epoch [107/120    avg_loss:0.039, val_acc:0.968]
Epoch [108/120    avg_loss:0.034, val_acc:0.968]
Epoch [109/120    avg_loss:0.051, val_acc:0.968]
Epoch [110/120    avg_loss:0.045, val_acc:0.968]
Epoch [111/120    avg_loss:0.041, val_acc:0.968]
Epoch [112/120    avg_loss:0.034, val_acc:0.968]
Epoch [113/120    avg_loss:0.038, val_acc:0.968]
Epoch [114/120    avg_loss:0.053, val_acc:0.968]
Epoch [115/120    avg_loss:0.032, val_acc:0.968]
Epoch [116/120    avg_loss:0.033, val_acc:0.968]
Epoch [117/120    avg_loss:0.042, val_acc:0.968]
Epoch [118/120    avg_loss:0.034, val_acc:0.968]
Epoch [119/120    avg_loss:0.037, val_acc:0.968]
Epoch [120/120    avg_loss:0.043, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6295     0     0     0     0    24    15    97     1]
 [    0     0 17730     0    53     0   303     0     4     0]
 [    0     4     0  1882     0     0     0     0   149     1]
 [    0    20     6     0  2919     0    21     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     1     2     0  4863     0     0     0]
 [    0    22     0     0     0     0     0  1267     1     0]
 [    0    31     3    78    25     0    13     0  3421     0]
 [    0     0     0     0    14     2     0     0     0   903]]

Accuracy:
97.81167907839877

F1 scores:
[       nan 0.98328647 0.98936972 0.94170628 0.9754386  0.9992343
 0.96277965 0.98522551 0.94437543 0.98796499]

Kappa:
0.9711037395362494
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b2b748940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.825, val_acc:0.294]
Epoch [2/120    avg_loss:1.372, val_acc:0.453]
Epoch [3/120    avg_loss:1.079, val_acc:0.701]
Epoch [4/120    avg_loss:0.822, val_acc:0.717]
Epoch [5/120    avg_loss:0.601, val_acc:0.762]
Epoch [6/120    avg_loss:0.526, val_acc:0.789]
Epoch [7/120    avg_loss:0.451, val_acc:0.803]
Epoch [8/120    avg_loss:0.410, val_acc:0.823]
Epoch [9/120    avg_loss:0.331, val_acc:0.859]
Epoch [10/120    avg_loss:0.281, val_acc:0.823]
Epoch [11/120    avg_loss:0.278, val_acc:0.853]
Epoch [12/120    avg_loss:0.364, val_acc:0.886]
Epoch [13/120    avg_loss:0.272, val_acc:0.874]
Epoch [14/120    avg_loss:0.246, val_acc:0.921]
Epoch [15/120    avg_loss:0.213, val_acc:0.931]
Epoch [16/120    avg_loss:0.220, val_acc:0.900]
Epoch [17/120    avg_loss:0.172, val_acc:0.927]
Epoch [18/120    avg_loss:0.159, val_acc:0.937]
Epoch [19/120    avg_loss:0.139, val_acc:0.927]
Epoch [20/120    avg_loss:0.195, val_acc:0.916]
Epoch [21/120    avg_loss:0.142, val_acc:0.935]
Epoch [22/120    avg_loss:0.123, val_acc:0.938]
Epoch [23/120    avg_loss:0.118, val_acc:0.946]
Epoch [24/120    avg_loss:0.123, val_acc:0.943]
Epoch [25/120    avg_loss:0.117, val_acc:0.920]
Epoch [26/120    avg_loss:0.111, val_acc:0.951]
Epoch [27/120    avg_loss:0.087, val_acc:0.960]
Epoch [28/120    avg_loss:0.142, val_acc:0.893]
Epoch [29/120    avg_loss:0.116, val_acc:0.928]
Epoch [30/120    avg_loss:0.080, val_acc:0.943]
Epoch [31/120    avg_loss:0.090, val_acc:0.961]
Epoch [32/120    avg_loss:0.083, val_acc:0.940]
Epoch [33/120    avg_loss:0.068, val_acc:0.952]
Epoch [34/120    avg_loss:0.062, val_acc:0.961]
Epoch [35/120    avg_loss:0.058, val_acc:0.959]
Epoch [36/120    avg_loss:0.052, val_acc:0.976]
Epoch [37/120    avg_loss:0.076, val_acc:0.970]
Epoch [38/120    avg_loss:0.055, val_acc:0.975]
Epoch [39/120    avg_loss:0.042, val_acc:0.973]
Epoch [40/120    avg_loss:0.081, val_acc:0.965]
Epoch [41/120    avg_loss:0.078, val_acc:0.951]
Epoch [42/120    avg_loss:0.069, val_acc:0.960]
Epoch [43/120    avg_loss:0.069, val_acc:0.972]
Epoch [44/120    avg_loss:0.038, val_acc:0.974]
Epoch [45/120    avg_loss:0.085, val_acc:0.959]
Epoch [46/120    avg_loss:0.078, val_acc:0.965]
Epoch [47/120    avg_loss:0.060, val_acc:0.965]
Epoch [48/120    avg_loss:0.038, val_acc:0.970]
Epoch [49/120    avg_loss:0.032, val_acc:0.955]
Epoch [50/120    avg_loss:0.031, val_acc:0.976]
Epoch [51/120    avg_loss:0.018, val_acc:0.981]
Epoch [52/120    avg_loss:0.014, val_acc:0.980]
Epoch [53/120    avg_loss:0.020, val_acc:0.979]
Epoch [54/120    avg_loss:0.022, val_acc:0.979]
Epoch [55/120    avg_loss:0.017, val_acc:0.983]
Epoch [56/120    avg_loss:0.015, val_acc:0.984]
Epoch [57/120    avg_loss:0.017, val_acc:0.984]
Epoch [58/120    avg_loss:0.019, val_acc:0.982]
Epoch [59/120    avg_loss:0.019, val_acc:0.981]
Epoch [60/120    avg_loss:0.017, val_acc:0.986]
Epoch [61/120    avg_loss:0.018, val_acc:0.981]
Epoch [62/120    avg_loss:0.015, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.982]
Epoch [64/120    avg_loss:0.021, val_acc:0.982]
Epoch [65/120    avg_loss:0.011, val_acc:0.983]
Epoch [66/120    avg_loss:0.018, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.982]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.015, val_acc:0.984]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.016, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.982]
Epoch [73/120    avg_loss:0.021, val_acc:0.982]
Epoch [74/120    avg_loss:0.015, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.013, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.014, val_acc:0.982]
Epoch [81/120    avg_loss:0.013, val_acc:0.983]
Epoch [82/120    avg_loss:0.013, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.014, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.984]
Epoch [89/120    avg_loss:0.016, val_acc:0.984]
Epoch [90/120    avg_loss:0.014, val_acc:0.984]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.013, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.018, val_acc:0.984]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.984]
Epoch [98/120    avg_loss:0.014, val_acc:0.984]
Epoch [99/120    avg_loss:0.017, val_acc:0.984]
Epoch [100/120    avg_loss:0.012, val_acc:0.984]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.015, val_acc:0.984]
Epoch [104/120    avg_loss:0.016, val_acc:0.984]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.014, val_acc:0.984]
Epoch [107/120    avg_loss:0.014, val_acc:0.984]
Epoch [108/120    avg_loss:0.014, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.013, val_acc:0.984]
Epoch [112/120    avg_loss:0.016, val_acc:0.984]
Epoch [113/120    avg_loss:0.014, val_acc:0.984]
Epoch [114/120    avg_loss:0.014, val_acc:0.984]
Epoch [115/120    avg_loss:0.015, val_acc:0.984]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.015, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     0     1     0    19     0    39    12]
 [    0     0 17892     0    67     0   129     0     2     0]
 [    0     6     0  1926     0     0     1     0   103     0]
 [    0    14    10     0  2929     0    14     0     1     4]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0    25     0     0     0  4853     0     0     0]
 [    0    42     0     0     0     1     0  1247     0     0]
 [    0    16    19    72     4     0     1     0  3457     2]
 [    0     3     0     0     8     1     0     0     0   907]]

Accuracy:
98.51300219314102

F1 scores:
[       nan 0.98819326 0.99300699 0.95488349 0.97943488 0.99885101
 0.98089944 0.98305085 0.96389237 0.98319783]

Kappa:
0.9803231408710187
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd3bc4c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.002, val_acc:0.582]
Epoch [2/120    avg_loss:1.399, val_acc:0.688]
Epoch [3/120    avg_loss:1.068, val_acc:0.739]
Epoch [4/120    avg_loss:0.763, val_acc:0.692]
Epoch [5/120    avg_loss:0.582, val_acc:0.789]
Epoch [6/120    avg_loss:0.456, val_acc:0.815]
Epoch [7/120    avg_loss:0.439, val_acc:0.869]
Epoch [8/120    avg_loss:0.332, val_acc:0.857]
Epoch [9/120    avg_loss:0.272, val_acc:0.850]
Epoch [10/120    avg_loss:0.231, val_acc:0.900]
Epoch [11/120    avg_loss:0.243, val_acc:0.914]
Epoch [12/120    avg_loss:0.226, val_acc:0.917]
Epoch [13/120    avg_loss:0.198, val_acc:0.924]
Epoch [14/120    avg_loss:0.149, val_acc:0.889]
Epoch [15/120    avg_loss:0.182, val_acc:0.918]
Epoch [16/120    avg_loss:0.163, val_acc:0.924]
Epoch [17/120    avg_loss:0.121, val_acc:0.929]
Epoch [18/120    avg_loss:0.139, val_acc:0.947]
Epoch [19/120    avg_loss:0.145, val_acc:0.950]
Epoch [20/120    avg_loss:0.092, val_acc:0.955]
Epoch [21/120    avg_loss:0.097, val_acc:0.883]
Epoch [22/120    avg_loss:0.113, val_acc:0.924]
Epoch [23/120    avg_loss:0.094, val_acc:0.951]
Epoch [24/120    avg_loss:0.083, val_acc:0.956]
Epoch [25/120    avg_loss:0.065, val_acc:0.958]
Epoch [26/120    avg_loss:0.064, val_acc:0.953]
Epoch [27/120    avg_loss:0.056, val_acc:0.972]
Epoch [28/120    avg_loss:0.046, val_acc:0.963]
Epoch [29/120    avg_loss:0.052, val_acc:0.967]
Epoch [30/120    avg_loss:0.045, val_acc:0.962]
Epoch [31/120    avg_loss:0.050, val_acc:0.966]
Epoch [32/120    avg_loss:0.058, val_acc:0.969]
Epoch [33/120    avg_loss:0.059, val_acc:0.970]
Epoch [34/120    avg_loss:0.043, val_acc:0.938]
Epoch [35/120    avg_loss:0.056, val_acc:0.967]
Epoch [36/120    avg_loss:0.059, val_acc:0.974]
Epoch [37/120    avg_loss:0.044, val_acc:0.967]
Epoch [38/120    avg_loss:0.035, val_acc:0.961]
Epoch [39/120    avg_loss:0.071, val_acc:0.938]
Epoch [40/120    avg_loss:0.052, val_acc:0.968]
Epoch [41/120    avg_loss:0.057, val_acc:0.956]
Epoch [42/120    avg_loss:0.032, val_acc:0.971]
Epoch [43/120    avg_loss:0.026, val_acc:0.973]
Epoch [44/120    avg_loss:0.036, val_acc:0.974]
Epoch [45/120    avg_loss:0.023, val_acc:0.977]
Epoch [46/120    avg_loss:0.022, val_acc:0.974]
Epoch [47/120    avg_loss:0.037, val_acc:0.948]
Epoch [48/120    avg_loss:0.018, val_acc:0.973]
Epoch [49/120    avg_loss:0.020, val_acc:0.980]
Epoch [50/120    avg_loss:0.019, val_acc:0.979]
Epoch [51/120    avg_loss:0.028, val_acc:0.979]
Epoch [52/120    avg_loss:0.017, val_acc:0.980]
Epoch [53/120    avg_loss:0.017, val_acc:0.961]
Epoch [54/120    avg_loss:0.018, val_acc:0.980]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.038, val_acc:0.973]
Epoch [57/120    avg_loss:0.025, val_acc:0.979]
Epoch [58/120    avg_loss:0.019, val_acc:0.982]
Epoch [59/120    avg_loss:0.042, val_acc:0.974]
Epoch [60/120    avg_loss:0.061, val_acc:0.965]
Epoch [61/120    avg_loss:0.059, val_acc:0.976]
Epoch [62/120    avg_loss:0.020, val_acc:0.979]
Epoch [63/120    avg_loss:0.014, val_acc:0.983]
Epoch [64/120    avg_loss:0.027, val_acc:0.966]
Epoch [65/120    avg_loss:0.021, val_acc:0.972]
Epoch [66/120    avg_loss:0.012, val_acc:0.983]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.024, val_acc:0.976]
Epoch [69/120    avg_loss:0.021, val_acc:0.983]
Epoch [70/120    avg_loss:0.015, val_acc:0.975]
Epoch [71/120    avg_loss:0.016, val_acc:0.979]
Epoch [72/120    avg_loss:0.014, val_acc:0.984]
Epoch [73/120    avg_loss:0.018, val_acc:0.979]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.018, val_acc:0.981]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.016, val_acc:0.984]
Epoch [78/120    avg_loss:0.018, val_acc:0.984]
Epoch [79/120    avg_loss:0.017, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.990]
Epoch [81/120    avg_loss:0.010, val_acc:0.953]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.015, val_acc:0.975]
Epoch [84/120    avg_loss:0.008, val_acc:0.979]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     0     0     0    27    46    20]
 [    0     0 18019     0    22     0    46     0     3     0]
 [    0     4     0  1908     0     0     0     0   124     0]
 [    0    35     5     0  2909     0    17     0     4     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     1     0  4851     0    21     0]
 [    0    23     0     0     0     0     0  1266     1     0]
 [    0    30     0    59    38     0     3     0  3441     0]
 [    0     1     0     0     5     6     0     0     0   907]]

Accuracy:
98.67929530282217

F1 scores:
[       nan 0.98554104 0.99775741 0.95328504 0.97830839 0.99770642
 0.99050536 0.98025552 0.95437526 0.98160173]

Kappa:
0.9825103310582388
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb50b3b1908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.899, val_acc:0.537]
Epoch [2/120    avg_loss:1.368, val_acc:0.663]
Epoch [3/120    avg_loss:1.007, val_acc:0.672]
Epoch [4/120    avg_loss:0.805, val_acc:0.741]
Epoch [5/120    avg_loss:0.663, val_acc:0.620]
Epoch [6/120    avg_loss:0.548, val_acc:0.676]
Epoch [7/120    avg_loss:0.449, val_acc:0.771]
Epoch [8/120    avg_loss:0.419, val_acc:0.823]
Epoch [9/120    avg_loss:0.340, val_acc:0.785]
Epoch [10/120    avg_loss:0.288, val_acc:0.898]
Epoch [11/120    avg_loss:0.262, val_acc:0.868]
Epoch [12/120    avg_loss:0.231, val_acc:0.852]
Epoch [13/120    avg_loss:0.241, val_acc:0.894]
Epoch [14/120    avg_loss:0.204, val_acc:0.902]
Epoch [15/120    avg_loss:0.196, val_acc:0.891]
Epoch [16/120    avg_loss:0.167, val_acc:0.928]
Epoch [17/120    avg_loss:0.153, val_acc:0.940]
Epoch [18/120    avg_loss:0.152, val_acc:0.899]
Epoch [19/120    avg_loss:0.134, val_acc:0.926]
Epoch [20/120    avg_loss:0.125, val_acc:0.917]
Epoch [21/120    avg_loss:0.105, val_acc:0.947]
Epoch [22/120    avg_loss:0.128, val_acc:0.942]
Epoch [23/120    avg_loss:0.129, val_acc:0.946]
Epoch [24/120    avg_loss:0.101, val_acc:0.945]
Epoch [25/120    avg_loss:0.087, val_acc:0.964]
Epoch [26/120    avg_loss:0.107, val_acc:0.965]
Epoch [27/120    avg_loss:0.081, val_acc:0.946]
Epoch [28/120    avg_loss:0.065, val_acc:0.965]
Epoch [29/120    avg_loss:0.086, val_acc:0.958]
Epoch [30/120    avg_loss:0.067, val_acc:0.947]
Epoch [31/120    avg_loss:0.084, val_acc:0.954]
Epoch [32/120    avg_loss:0.073, val_acc:0.968]
Epoch [33/120    avg_loss:0.049, val_acc:0.970]
Epoch [34/120    avg_loss:0.040, val_acc:0.951]
Epoch [35/120    avg_loss:0.059, val_acc:0.953]
Epoch [36/120    avg_loss:0.055, val_acc:0.956]
Epoch [37/120    avg_loss:0.079, val_acc:0.963]
Epoch [38/120    avg_loss:0.055, val_acc:0.943]
Epoch [39/120    avg_loss:0.049, val_acc:0.971]
Epoch [40/120    avg_loss:0.075, val_acc:0.960]
Epoch [41/120    avg_loss:0.041, val_acc:0.949]
Epoch [42/120    avg_loss:0.065, val_acc:0.961]
Epoch [43/120    avg_loss:0.060, val_acc:0.967]
Epoch [44/120    avg_loss:0.040, val_acc:0.932]
Epoch [45/120    avg_loss:0.060, val_acc:0.957]
Epoch [46/120    avg_loss:0.038, val_acc:0.965]
Epoch [47/120    avg_loss:0.030, val_acc:0.946]
Epoch [48/120    avg_loss:0.038, val_acc:0.955]
Epoch [49/120    avg_loss:0.039, val_acc:0.960]
Epoch [50/120    avg_loss:0.025, val_acc:0.961]
Epoch [51/120    avg_loss:0.031, val_acc:0.969]
Epoch [52/120    avg_loss:0.021, val_acc:0.970]
Epoch [53/120    avg_loss:0.018, val_acc:0.975]
Epoch [54/120    avg_loss:0.012, val_acc:0.975]
Epoch [55/120    avg_loss:0.011, val_acc:0.978]
Epoch [56/120    avg_loss:0.017, val_acc:0.976]
Epoch [57/120    avg_loss:0.016, val_acc:0.979]
Epoch [58/120    avg_loss:0.014, val_acc:0.978]
Epoch [59/120    avg_loss:0.012, val_acc:0.979]
Epoch [60/120    avg_loss:0.020, val_acc:0.976]
Epoch [61/120    avg_loss:0.011, val_acc:0.979]
Epoch [62/120    avg_loss:0.010, val_acc:0.979]
Epoch [63/120    avg_loss:0.012, val_acc:0.979]
Epoch [64/120    avg_loss:0.012, val_acc:0.978]
Epoch [65/120    avg_loss:0.011, val_acc:0.979]
Epoch [66/120    avg_loss:0.016, val_acc:0.979]
Epoch [67/120    avg_loss:0.011, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.979]
Epoch [69/120    avg_loss:0.011, val_acc:0.978]
Epoch [70/120    avg_loss:0.013, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.979]
Epoch [72/120    avg_loss:0.010, val_acc:0.979]
Epoch [73/120    avg_loss:0.012, val_acc:0.979]
Epoch [74/120    avg_loss:0.016, val_acc:0.977]
Epoch [75/120    avg_loss:0.015, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.008, val_acc:0.979]
Epoch [81/120    avg_loss:0.016, val_acc:0.977]
Epoch [82/120    avg_loss:0.010, val_acc:0.978]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.977]
Epoch [85/120    avg_loss:0.014, val_acc:0.976]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.014, val_acc:0.977]
Epoch [89/120    avg_loss:0.011, val_acc:0.977]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.977]
Epoch [93/120    avg_loss:0.010, val_acc:0.977]
Epoch [94/120    avg_loss:0.011, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.977]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.978]
Epoch [103/120    avg_loss:0.013, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.007, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.012, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.010, val_acc:0.978]
Epoch [110/120    avg_loss:0.013, val_acc:0.978]
Epoch [111/120    avg_loss:0.008, val_acc:0.978]
Epoch [112/120    avg_loss:0.008, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     3     0     0     0     2    54     1]
 [    0     1 17919     0    78     0    88     0     4     0]
 [    0     4     0  1927     0     0     0     0   104     1]
 [    0    33     7     0  2923     0     5     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    32     0     1     0  4839     0     6     0]
 [    0    10     0     0     0     0     0  1278     2     0]
 [    0    10     0   111    40     0     0     0  3410     0]
 [    0     0     0     0     0     1     0     0     0   918]]

Accuracy:
98.54915286915866

F1 scores:
[       nan 0.99082569 0.99417443 0.94530292 0.97206518 0.999617
 0.98654434 0.99455253 0.95331283 0.99782609]

Kappa:
0.9808032641410536
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65747c78d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.836, val_acc:0.264]
Epoch [2/120    avg_loss:1.318, val_acc:0.585]
Epoch [3/120    avg_loss:1.013, val_acc:0.617]
Epoch [4/120    avg_loss:0.774, val_acc:0.581]
Epoch [5/120    avg_loss:0.603, val_acc:0.713]
Epoch [6/120    avg_loss:0.503, val_acc:0.752]
Epoch [7/120    avg_loss:0.418, val_acc:0.766]
Epoch [8/120    avg_loss:0.385, val_acc:0.779]
Epoch [9/120    avg_loss:0.361, val_acc:0.831]
Epoch [10/120    avg_loss:0.310, val_acc:0.747]
Epoch [11/120    avg_loss:0.273, val_acc:0.850]
Epoch [12/120    avg_loss:0.272, val_acc:0.808]
Epoch [13/120    avg_loss:0.222, val_acc:0.910]
Epoch [14/120    avg_loss:0.210, val_acc:0.910]
Epoch [15/120    avg_loss:0.201, val_acc:0.928]
Epoch [16/120    avg_loss:0.202, val_acc:0.912]
Epoch [17/120    avg_loss:0.161, val_acc:0.938]
Epoch [18/120    avg_loss:0.155, val_acc:0.914]
Epoch [19/120    avg_loss:0.146, val_acc:0.947]
Epoch [20/120    avg_loss:0.100, val_acc:0.958]
Epoch [21/120    avg_loss:0.162, val_acc:0.933]
Epoch [22/120    avg_loss:0.129, val_acc:0.914]
Epoch [23/120    avg_loss:0.130, val_acc:0.933]
Epoch [24/120    avg_loss:0.113, val_acc:0.939]
Epoch [25/120    avg_loss:0.115, val_acc:0.927]
Epoch [26/120    avg_loss:0.090, val_acc:0.951]
Epoch [27/120    avg_loss:0.079, val_acc:0.943]
Epoch [28/120    avg_loss:0.071, val_acc:0.916]
Epoch [29/120    avg_loss:0.069, val_acc:0.970]
Epoch [30/120    avg_loss:0.060, val_acc:0.957]
Epoch [31/120    avg_loss:0.062, val_acc:0.934]
Epoch [32/120    avg_loss:0.046, val_acc:0.965]
Epoch [33/120    avg_loss:0.070, val_acc:0.956]
Epoch [34/120    avg_loss:0.078, val_acc:0.956]
Epoch [35/120    avg_loss:0.112, val_acc:0.942]
Epoch [36/120    avg_loss:0.068, val_acc:0.968]
Epoch [37/120    avg_loss:0.047, val_acc:0.970]
Epoch [38/120    avg_loss:0.040, val_acc:0.973]
Epoch [39/120    avg_loss:0.037, val_acc:0.938]
Epoch [40/120    avg_loss:0.067, val_acc:0.967]
Epoch [41/120    avg_loss:0.050, val_acc:0.965]
Epoch [42/120    avg_loss:0.046, val_acc:0.965]
Epoch [43/120    avg_loss:0.038, val_acc:0.976]
Epoch [44/120    avg_loss:0.024, val_acc:0.971]
Epoch [45/120    avg_loss:0.027, val_acc:0.969]
Epoch [46/120    avg_loss:0.034, val_acc:0.970]
Epoch [47/120    avg_loss:0.030, val_acc:0.965]
Epoch [48/120    avg_loss:0.030, val_acc:0.969]
Epoch [49/120    avg_loss:0.018, val_acc:0.974]
Epoch [50/120    avg_loss:0.016, val_acc:0.978]
Epoch [51/120    avg_loss:0.013, val_acc:0.966]
Epoch [52/120    avg_loss:0.017, val_acc:0.964]
Epoch [53/120    avg_loss:0.022, val_acc:0.965]
Epoch [54/120    avg_loss:0.022, val_acc:0.975]
Epoch [55/120    avg_loss:0.017, val_acc:0.972]
Epoch [56/120    avg_loss:0.017, val_acc:0.974]
Epoch [57/120    avg_loss:0.015, val_acc:0.979]
Epoch [58/120    avg_loss:0.013, val_acc:0.980]
Epoch [59/120    avg_loss:0.011, val_acc:0.983]
Epoch [60/120    avg_loss:0.010, val_acc:0.979]
Epoch [61/120    avg_loss:0.026, val_acc:0.966]
Epoch [62/120    avg_loss:0.035, val_acc:0.935]
Epoch [63/120    avg_loss:0.036, val_acc:0.971]
Epoch [64/120    avg_loss:0.026, val_acc:0.974]
Epoch [65/120    avg_loss:0.009, val_acc:0.975]
Epoch [66/120    avg_loss:0.008, val_acc:0.977]
Epoch [67/120    avg_loss:0.014, val_acc:0.976]
Epoch [68/120    avg_loss:0.029, val_acc:0.970]
Epoch [69/120    avg_loss:0.018, val_acc:0.970]
Epoch [70/120    avg_loss:0.012, val_acc:0.972]
Epoch [71/120    avg_loss:0.009, val_acc:0.975]
Epoch [72/120    avg_loss:0.017, val_acc:0.973]
Epoch [73/120    avg_loss:0.015, val_acc:0.981]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.979]
Epoch [76/120    avg_loss:0.006, val_acc:0.979]
Epoch [77/120    avg_loss:0.005, val_acc:0.979]
Epoch [78/120    avg_loss:0.007, val_acc:0.979]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.006, val_acc:0.981]
Epoch [82/120    avg_loss:0.005, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.981]
Epoch [85/120    avg_loss:0.006, val_acc:0.981]
Epoch [86/120    avg_loss:0.010, val_acc:0.979]
Epoch [87/120    avg_loss:0.004, val_acc:0.979]
Epoch [88/120    avg_loss:0.005, val_acc:0.980]
Epoch [89/120    avg_loss:0.006, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.979]
Epoch [91/120    avg_loss:0.006, val_acc:0.979]
Epoch [92/120    avg_loss:0.005, val_acc:0.979]
Epoch [93/120    avg_loss:0.005, val_acc:0.979]
Epoch [94/120    avg_loss:0.004, val_acc:0.979]
Epoch [95/120    avg_loss:0.005, val_acc:0.979]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.006, val_acc:0.980]
Epoch [98/120    avg_loss:0.005, val_acc:0.980]
Epoch [99/120    avg_loss:0.006, val_acc:0.980]
Epoch [100/120    avg_loss:0.006, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.980]
Epoch [102/120    avg_loss:0.004, val_acc:0.980]
Epoch [103/120    avg_loss:0.008, val_acc:0.980]
Epoch [104/120    avg_loss:0.004, val_acc:0.980]
Epoch [105/120    avg_loss:0.005, val_acc:0.979]
Epoch [106/120    avg_loss:0.005, val_acc:0.979]
Epoch [107/120    avg_loss:0.005, val_acc:0.980]
Epoch [108/120    avg_loss:0.005, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.979]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.005, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     4     0     0     0    17    61    14]
 [    0     0 18057     0    25     0     1     0     7     0]
 [    0     8     0  1930     0     0     0     0    97     1]
 [    0    23     5     2  2897     0    37     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0    15     0     0     0     0     0  1275     0     0]
 [    0    33     0    68    30     0     0     0  3440     0]
 [    0     2     0     0    16     5     0     0     0   896]]

Accuracy:
98.81907791675705

F1 scores:
[       nan 0.98622461 0.99864502 0.95544554 0.97542088 0.99808795
 0.99499131 0.98760651 0.95808383 0.9776323 ]

Kappa:
0.9843545443455969
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3b1b95908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.830, val_acc:0.394]
Epoch [2/120    avg_loss:1.302, val_acc:0.680]
Epoch [3/120    avg_loss:1.005, val_acc:0.664]
Epoch [4/120    avg_loss:0.741, val_acc:0.732]
Epoch [5/120    avg_loss:0.574, val_acc:0.782]
Epoch [6/120    avg_loss:0.531, val_acc:0.695]
Epoch [7/120    avg_loss:0.410, val_acc:0.799]
Epoch [8/120    avg_loss:0.383, val_acc:0.770]
Epoch [9/120    avg_loss:0.373, val_acc:0.749]
Epoch [10/120    avg_loss:0.299, val_acc:0.873]
Epoch [11/120    avg_loss:0.299, val_acc:0.819]
Epoch [12/120    avg_loss:0.261, val_acc:0.878]
Epoch [13/120    avg_loss:0.257, val_acc:0.877]
Epoch [14/120    avg_loss:0.219, val_acc:0.875]
Epoch [15/120    avg_loss:0.229, val_acc:0.897]
Epoch [16/120    avg_loss:0.174, val_acc:0.907]
Epoch [17/120    avg_loss:0.210, val_acc:0.838]
Epoch [18/120    avg_loss:0.211, val_acc:0.911]
Epoch [19/120    avg_loss:0.167, val_acc:0.907]
Epoch [20/120    avg_loss:0.149, val_acc:0.921]
Epoch [21/120    avg_loss:0.145, val_acc:0.906]
Epoch [22/120    avg_loss:0.100, val_acc:0.961]
Epoch [23/120    avg_loss:0.126, val_acc:0.963]
Epoch [24/120    avg_loss:0.081, val_acc:0.930]
Epoch [25/120    avg_loss:0.128, val_acc:0.955]
Epoch [26/120    avg_loss:0.082, val_acc:0.960]
Epoch [27/120    avg_loss:0.098, val_acc:0.926]
Epoch [28/120    avg_loss:0.133, val_acc:0.958]
Epoch [29/120    avg_loss:0.088, val_acc:0.955]
Epoch [30/120    avg_loss:0.062, val_acc:0.968]
Epoch [31/120    avg_loss:0.096, val_acc:0.932]
Epoch [32/120    avg_loss:0.088, val_acc:0.956]
Epoch [33/120    avg_loss:0.053, val_acc:0.947]
Epoch [34/120    avg_loss:0.047, val_acc:0.964]
Epoch [35/120    avg_loss:0.036, val_acc:0.977]
Epoch [36/120    avg_loss:0.038, val_acc:0.970]
Epoch [37/120    avg_loss:0.092, val_acc:0.972]
Epoch [38/120    avg_loss:0.058, val_acc:0.938]
Epoch [39/120    avg_loss:0.132, val_acc:0.961]
Epoch [40/120    avg_loss:0.090, val_acc:0.934]
Epoch [41/120    avg_loss:0.046, val_acc:0.974]
Epoch [42/120    avg_loss:0.053, val_acc:0.970]
Epoch [43/120    avg_loss:0.053, val_acc:0.954]
Epoch [44/120    avg_loss:0.052, val_acc:0.949]
Epoch [45/120    avg_loss:0.030, val_acc:0.972]
Epoch [46/120    avg_loss:0.059, val_acc:0.978]
Epoch [47/120    avg_loss:0.047, val_acc:0.975]
Epoch [48/120    avg_loss:0.035, val_acc:0.974]
Epoch [49/120    avg_loss:0.025, val_acc:0.979]
Epoch [50/120    avg_loss:0.016, val_acc:0.979]
Epoch [51/120    avg_loss:0.032, val_acc:0.970]
Epoch [52/120    avg_loss:0.026, val_acc:0.962]
Epoch [53/120    avg_loss:0.030, val_acc:0.898]
Epoch [54/120    avg_loss:0.077, val_acc:0.968]
Epoch [55/120    avg_loss:0.043, val_acc:0.944]
Epoch [56/120    avg_loss:0.040, val_acc:0.971]
Epoch [57/120    avg_loss:0.033, val_acc:0.973]
Epoch [58/120    avg_loss:0.035, val_acc:0.980]
Epoch [59/120    avg_loss:0.022, val_acc:0.982]
Epoch [60/120    avg_loss:0.032, val_acc:0.975]
Epoch [61/120    avg_loss:0.020, val_acc:0.980]
Epoch [62/120    avg_loss:0.157, val_acc:0.955]
Epoch [63/120    avg_loss:0.106, val_acc:0.944]
Epoch [64/120    avg_loss:0.076, val_acc:0.974]
Epoch [65/120    avg_loss:0.043, val_acc:0.967]
Epoch [66/120    avg_loss:0.044, val_acc:0.956]
Epoch [67/120    avg_loss:0.031, val_acc:0.981]
Epoch [68/120    avg_loss:0.036, val_acc:0.983]
Epoch [69/120    avg_loss:0.021, val_acc:0.972]
Epoch [70/120    avg_loss:0.027, val_acc:0.967]
Epoch [71/120    avg_loss:0.041, val_acc:0.978]
Epoch [72/120    avg_loss:0.024, val_acc:0.984]
Epoch [73/120    avg_loss:0.016, val_acc:0.987]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.012, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.979]
Epoch [77/120    avg_loss:0.031, val_acc:0.927]
Epoch [78/120    avg_loss:0.022, val_acc:0.982]
Epoch [79/120    avg_loss:0.033, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.982]
Epoch [81/120    avg_loss:0.167, val_acc:0.874]
Epoch [82/120    avg_loss:0.140, val_acc:0.964]
Epoch [83/120    avg_loss:0.050, val_acc:0.979]
Epoch [84/120    avg_loss:0.033, val_acc:0.977]
Epoch [85/120    avg_loss:0.035, val_acc:0.942]
Epoch [86/120    avg_loss:0.056, val_acc:0.978]
Epoch [87/120    avg_loss:0.019, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.018, val_acc:0.984]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.987]
Epoch [94/120    avg_loss:0.013, val_acc:0.989]
Epoch [95/120    avg_loss:0.013, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.989]
Epoch [97/120    avg_loss:0.011, val_acc:0.989]
Epoch [98/120    avg_loss:0.016, val_acc:0.988]
Epoch [99/120    avg_loss:0.013, val_acc:0.987]
Epoch [100/120    avg_loss:0.013, val_acc:0.990]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.989]
Epoch [104/120    avg_loss:0.011, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.991]
Epoch [107/120    avg_loss:0.013, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.989]
Epoch [109/120    avg_loss:0.012, val_acc:0.988]
Epoch [110/120    avg_loss:0.013, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.009, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     1     0     0     7     3    41     0]
 [    0     2 17920     0    35     0   125     0     8     0]
 [    0     0     0  1951     0     0     0     0    82     3]
 [    0    12     6     0  2934     0    11     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     1     0  4866     0     1     0]
 [    0     5     0     0     0     0     0  1284     0     1]
 [    0     1     0    51    42     0     0     0  3477     0]
 [    0     0     0     0    14     6     0     0     0   899]]

Accuracy:
98.850408502639

F1 scores:
[       nan 0.99438903 0.99483706 0.96608071 0.97832611 0.99770642
 0.98432285 0.99650757 0.967715   0.98520548]

Kappa:
0.9847919178657323
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03fbadd978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.902, val_acc:0.247]
Epoch [2/120    avg_loss:1.372, val_acc:0.601]
Epoch [3/120    avg_loss:1.079, val_acc:0.691]
Epoch [4/120    avg_loss:0.899, val_acc:0.743]
Epoch [5/120    avg_loss:0.730, val_acc:0.617]
Epoch [6/120    avg_loss:0.650, val_acc:0.727]
Epoch [7/120    avg_loss:0.479, val_acc:0.650]
Epoch [8/120    avg_loss:0.440, val_acc:0.781]
Epoch [9/120    avg_loss:0.381, val_acc:0.814]
Epoch [10/120    avg_loss:0.336, val_acc:0.820]
Epoch [11/120    avg_loss:0.329, val_acc:0.868]
Epoch [12/120    avg_loss:0.278, val_acc:0.876]
Epoch [13/120    avg_loss:0.257, val_acc:0.899]
Epoch [14/120    avg_loss:0.223, val_acc:0.911]
Epoch [15/120    avg_loss:0.186, val_acc:0.927]
Epoch [16/120    avg_loss:0.209, val_acc:0.863]
Epoch [17/120    avg_loss:0.209, val_acc:0.937]
Epoch [18/120    avg_loss:0.131, val_acc:0.945]
Epoch [19/120    avg_loss:0.139, val_acc:0.945]
Epoch [20/120    avg_loss:0.151, val_acc:0.926]
Epoch [21/120    avg_loss:0.134, val_acc:0.939]
Epoch [22/120    avg_loss:0.124, val_acc:0.951]
Epoch [23/120    avg_loss:0.117, val_acc:0.957]
Epoch [24/120    avg_loss:0.123, val_acc:0.961]
Epoch [25/120    avg_loss:0.096, val_acc:0.943]
Epoch [26/120    avg_loss:0.064, val_acc:0.966]
Epoch [27/120    avg_loss:0.079, val_acc:0.938]
Epoch [28/120    avg_loss:0.088, val_acc:0.885]
Epoch [29/120    avg_loss:0.125, val_acc:0.964]
Epoch [30/120    avg_loss:0.063, val_acc:0.933]
Epoch [31/120    avg_loss:0.067, val_acc:0.962]
Epoch [32/120    avg_loss:0.058, val_acc:0.941]
Epoch [33/120    avg_loss:0.075, val_acc:0.967]
Epoch [34/120    avg_loss:0.051, val_acc:0.966]
Epoch [35/120    avg_loss:0.031, val_acc:0.977]
Epoch [36/120    avg_loss:0.044, val_acc:0.970]
Epoch [37/120    avg_loss:0.061, val_acc:0.932]
Epoch [38/120    avg_loss:0.056, val_acc:0.955]
Epoch [39/120    avg_loss:0.047, val_acc:0.965]
Epoch [40/120    avg_loss:0.062, val_acc:0.973]
Epoch [41/120    avg_loss:0.044, val_acc:0.977]
Epoch [42/120    avg_loss:0.042, val_acc:0.976]
Epoch [43/120    avg_loss:0.037, val_acc:0.977]
Epoch [44/120    avg_loss:0.046, val_acc:0.956]
Epoch [45/120    avg_loss:0.111, val_acc:0.926]
Epoch [46/120    avg_loss:0.073, val_acc:0.970]
Epoch [47/120    avg_loss:0.050, val_acc:0.974]
Epoch [48/120    avg_loss:0.026, val_acc:0.974]
Epoch [49/120    avg_loss:0.036, val_acc:0.949]
Epoch [50/120    avg_loss:0.038, val_acc:0.971]
Epoch [51/120    avg_loss:0.019, val_acc:0.979]
Epoch [52/120    avg_loss:0.023, val_acc:0.959]
Epoch [53/120    avg_loss:0.023, val_acc:0.983]
Epoch [54/120    avg_loss:0.072, val_acc:0.976]
Epoch [55/120    avg_loss:0.062, val_acc:0.961]
Epoch [56/120    avg_loss:0.049, val_acc:0.966]
Epoch [57/120    avg_loss:0.056, val_acc:0.967]
Epoch [58/120    avg_loss:0.033, val_acc:0.980]
Epoch [59/120    avg_loss:0.015, val_acc:0.981]
Epoch [60/120    avg_loss:0.024, val_acc:0.974]
Epoch [61/120    avg_loss:0.027, val_acc:0.976]
Epoch [62/120    avg_loss:0.030, val_acc:0.986]
Epoch [63/120    avg_loss:0.014, val_acc:0.976]
Epoch [64/120    avg_loss:0.029, val_acc:0.972]
Epoch [65/120    avg_loss:0.031, val_acc:0.984]
Epoch [66/120    avg_loss:0.040, val_acc:0.962]
Epoch [67/120    avg_loss:0.039, val_acc:0.980]
Epoch [68/120    avg_loss:0.033, val_acc:0.979]
Epoch [69/120    avg_loss:0.024, val_acc:0.983]
Epoch [70/120    avg_loss:0.020, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.990]
Epoch [72/120    avg_loss:0.012, val_acc:0.984]
Epoch [73/120    avg_loss:0.026, val_acc:0.980]
Epoch [74/120    avg_loss:0.018, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.014, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.981]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.016, val_acc:0.975]
Epoch [82/120    avg_loss:0.028, val_acc:0.983]
Epoch [83/120    avg_loss:0.049, val_acc:0.976]
Epoch [84/120    avg_loss:0.017, val_acc:0.981]
Epoch [85/120    avg_loss:0.019, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.012, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.014, val_acc:0.985]
Epoch [91/120    avg_loss:0.009, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.015, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.013, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.012, val_acc:0.987]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     0     0     0     4     1    54     7]
 [    0     2 17736     0    65     0   284     0     3     0]
 [    0     0     0  1920     0     0     0     0   110     6]
 [    0    16     0     0  2927     0    19     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4874     0     4     0]
 [    0    20     0     0     0     0     0  1269     1     0]
 [    0     7     0    30    49     0     4     0  3481     0]
 [    0     0     0     0     8    21     0     0     0   890]]

Accuracy:
98.25271732581399

F1 scores:
[       nan 0.99135716 0.99011891 0.9633718  0.97226374 0.99201824
 0.96869721 0.99140625 0.96279906 0.97534247]

Kappa:
0.9769305459003353
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac8016c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.944, val_acc:0.194]
Epoch [2/120    avg_loss:1.474, val_acc:0.506]
Epoch [3/120    avg_loss:1.137, val_acc:0.562]
Epoch [4/120    avg_loss:0.850, val_acc:0.629]
Epoch [5/120    avg_loss:0.670, val_acc:0.706]
Epoch [6/120    avg_loss:0.549, val_acc:0.703]
Epoch [7/120    avg_loss:0.467, val_acc:0.792]
Epoch [8/120    avg_loss:0.396, val_acc:0.801]
Epoch [9/120    avg_loss:0.350, val_acc:0.873]
Epoch [10/120    avg_loss:0.310, val_acc:0.853]
Epoch [11/120    avg_loss:0.256, val_acc:0.850]
Epoch [12/120    avg_loss:0.255, val_acc:0.914]
Epoch [13/120    avg_loss:0.191, val_acc:0.885]
Epoch [14/120    avg_loss:0.158, val_acc:0.931]
Epoch [15/120    avg_loss:0.189, val_acc:0.869]
Epoch [16/120    avg_loss:0.167, val_acc:0.927]
Epoch [17/120    avg_loss:0.113, val_acc:0.934]
Epoch [18/120    avg_loss:0.104, val_acc:0.942]
Epoch [19/120    avg_loss:0.128, val_acc:0.941]
Epoch [20/120    avg_loss:0.129, val_acc:0.870]
Epoch [21/120    avg_loss:0.152, val_acc:0.929]
Epoch [22/120    avg_loss:0.073, val_acc:0.947]
Epoch [23/120    avg_loss:0.093, val_acc:0.943]
Epoch [24/120    avg_loss:0.056, val_acc:0.961]
Epoch [25/120    avg_loss:0.051, val_acc:0.969]
Epoch [26/120    avg_loss:0.058, val_acc:0.952]
Epoch [27/120    avg_loss:0.047, val_acc:0.946]
Epoch [28/120    avg_loss:0.063, val_acc:0.964]
Epoch [29/120    avg_loss:0.044, val_acc:0.968]
Epoch [30/120    avg_loss:0.035, val_acc:0.966]
Epoch [31/120    avg_loss:0.042, val_acc:0.942]
Epoch [32/120    avg_loss:0.026, val_acc:0.970]
Epoch [33/120    avg_loss:0.027, val_acc:0.973]
Epoch [34/120    avg_loss:0.029, val_acc:0.971]
Epoch [35/120    avg_loss:0.028, val_acc:0.976]
Epoch [36/120    avg_loss:0.026, val_acc:0.973]
Epoch [37/120    avg_loss:0.019, val_acc:0.975]
Epoch [38/120    avg_loss:0.020, val_acc:0.959]
Epoch [39/120    avg_loss:0.030, val_acc:0.976]
Epoch [40/120    avg_loss:0.022, val_acc:0.968]
Epoch [41/120    avg_loss:0.018, val_acc:0.979]
Epoch [42/120    avg_loss:0.021, val_acc:0.971]
Epoch [43/120    avg_loss:0.034, val_acc:0.971]
Epoch [44/120    avg_loss:0.022, val_acc:0.975]
Epoch [45/120    avg_loss:0.010, val_acc:0.976]
Epoch [46/120    avg_loss:0.021, val_acc:0.979]
Epoch [47/120    avg_loss:0.028, val_acc:0.942]
Epoch [48/120    avg_loss:0.022, val_acc:0.967]
Epoch [49/120    avg_loss:0.015, val_acc:0.975]
Epoch [50/120    avg_loss:0.009, val_acc:0.976]
Epoch [51/120    avg_loss:0.020, val_acc:0.978]
Epoch [52/120    avg_loss:0.017, val_acc:0.979]
Epoch [53/120    avg_loss:0.011, val_acc:0.977]
Epoch [54/120    avg_loss:0.019, val_acc:0.971]
Epoch [55/120    avg_loss:0.009, val_acc:0.978]
Epoch [56/120    avg_loss:0.010, val_acc:0.978]
Epoch [57/120    avg_loss:0.007, val_acc:0.978]
Epoch [58/120    avg_loss:0.009, val_acc:0.978]
Epoch [59/120    avg_loss:0.007, val_acc:0.979]
Epoch [60/120    avg_loss:0.009, val_acc:0.978]
Epoch [61/120    avg_loss:0.006, val_acc:0.979]
Epoch [62/120    avg_loss:0.005, val_acc:0.979]
Epoch [63/120    avg_loss:0.007, val_acc:0.979]
Epoch [64/120    avg_loss:0.006, val_acc:0.978]
Epoch [65/120    avg_loss:0.006, val_acc:0.979]
Epoch [66/120    avg_loss:0.005, val_acc:0.978]
Epoch [67/120    avg_loss:0.005, val_acc:0.979]
Epoch [68/120    avg_loss:0.006, val_acc:0.979]
Epoch [69/120    avg_loss:0.006, val_acc:0.979]
Epoch [70/120    avg_loss:0.006, val_acc:0.979]
Epoch [71/120    avg_loss:0.006, val_acc:0.979]
Epoch [72/120    avg_loss:0.007, val_acc:0.979]
Epoch [73/120    avg_loss:0.006, val_acc:0.979]
Epoch [74/120    avg_loss:0.008, val_acc:0.979]
Epoch [75/120    avg_loss:0.005, val_acc:0.979]
Epoch [76/120    avg_loss:0.006, val_acc:0.979]
Epoch [77/120    avg_loss:0.007, val_acc:0.979]
Epoch [78/120    avg_loss:0.005, val_acc:0.979]
Epoch [79/120    avg_loss:0.005, val_acc:0.979]
Epoch [80/120    avg_loss:0.006, val_acc:0.979]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.008, val_acc:0.979]
Epoch [83/120    avg_loss:0.005, val_acc:0.980]
Epoch [84/120    avg_loss:0.005, val_acc:0.980]
Epoch [85/120    avg_loss:0.005, val_acc:0.980]
Epoch [86/120    avg_loss:0.006, val_acc:0.980]
Epoch [87/120    avg_loss:0.005, val_acc:0.979]
Epoch [88/120    avg_loss:0.006, val_acc:0.980]
Epoch [89/120    avg_loss:0.005, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.979]
Epoch [91/120    avg_loss:0.004, val_acc:0.979]
Epoch [92/120    avg_loss:0.005, val_acc:0.979]
Epoch [93/120    avg_loss:0.005, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.980]
Epoch [95/120    avg_loss:0.005, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.004, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.980]
Epoch [99/120    avg_loss:0.005, val_acc:0.980]
Epoch [100/120    avg_loss:0.006, val_acc:0.980]
Epoch [101/120    avg_loss:0.008, val_acc:0.980]
Epoch [102/120    avg_loss:0.005, val_acc:0.979]
Epoch [103/120    avg_loss:0.005, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.005, val_acc:0.980]
Epoch [106/120    avg_loss:0.005, val_acc:0.979]
Epoch [107/120    avg_loss:0.005, val_acc:0.979]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.006, val_acc:0.980]
Epoch [110/120    avg_loss:0.004, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.979]
Epoch [112/120    avg_loss:0.005, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.005, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.005, val_acc:0.979]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.004, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.004, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     0     0     1     5    42     0]
 [    0     0 18055     0    19     0     7     0     9     0]
 [    0     9     0  1982     0     0     0     0    40     5]
 [    0    36    12     0  2918     0     2     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4839     0    16     0]
 [    0     8     0     0     0     0     0  1281     0     1]
 [    0    21     0    81    44     0     0     0  3424     1]
 [    0     0     0     0    13    14     0     0     0   892]]

Accuracy:
99.00465138698094

F1 scores:
[       nan 0.9905353  0.99806523 0.96706514 0.97820986 0.99466463
 0.99496248 0.99456522 0.96382829 0.98075866]

Kappa:
0.9868109317106336
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12a7528a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.903, val_acc:0.260]
Epoch [2/120    avg_loss:1.394, val_acc:0.536]
Epoch [3/120    avg_loss:1.067, val_acc:0.752]
Epoch [4/120    avg_loss:0.850, val_acc:0.643]
Epoch [5/120    avg_loss:0.687, val_acc:0.750]
Epoch [6/120    avg_loss:0.556, val_acc:0.795]
Epoch [7/120    avg_loss:0.464, val_acc:0.738]
Epoch [8/120    avg_loss:0.404, val_acc:0.779]
Epoch [9/120    avg_loss:0.325, val_acc:0.816]
Epoch [10/120    avg_loss:0.326, val_acc:0.853]
Epoch [11/120    avg_loss:0.296, val_acc:0.843]
Epoch [12/120    avg_loss:0.275, val_acc:0.872]
Epoch [13/120    avg_loss:0.240, val_acc:0.881]
Epoch [14/120    avg_loss:0.225, val_acc:0.894]
Epoch [15/120    avg_loss:0.192, val_acc:0.927]
Epoch [16/120    avg_loss:0.193, val_acc:0.882]
Epoch [17/120    avg_loss:0.176, val_acc:0.921]
Epoch [18/120    avg_loss:0.151, val_acc:0.900]
Epoch [19/120    avg_loss:0.151, val_acc:0.962]
Epoch [20/120    avg_loss:0.108, val_acc:0.959]
Epoch [21/120    avg_loss:0.099, val_acc:0.967]
Epoch [22/120    avg_loss:0.092, val_acc:0.959]
Epoch [23/120    avg_loss:0.097, val_acc:0.959]
Epoch [24/120    avg_loss:0.085, val_acc:0.916]
Epoch [25/120    avg_loss:0.094, val_acc:0.903]
Epoch [26/120    avg_loss:0.086, val_acc:0.974]
Epoch [27/120    avg_loss:0.065, val_acc:0.976]
Epoch [28/120    avg_loss:0.048, val_acc:0.946]
Epoch [29/120    avg_loss:0.057, val_acc:0.969]
Epoch [30/120    avg_loss:0.061, val_acc:0.966]
Epoch [31/120    avg_loss:0.045, val_acc:0.977]
Epoch [32/120    avg_loss:0.050, val_acc:0.946]
Epoch [33/120    avg_loss:0.058, val_acc:0.953]
Epoch [34/120    avg_loss:0.057, val_acc:0.966]
Epoch [35/120    avg_loss:0.042, val_acc:0.938]
Epoch [36/120    avg_loss:0.071, val_acc:0.970]
Epoch [37/120    avg_loss:0.052, val_acc:0.959]
Epoch [38/120    avg_loss:0.050, val_acc:0.964]
Epoch [39/120    avg_loss:0.060, val_acc:0.963]
Epoch [40/120    avg_loss:0.124, val_acc:0.910]
Epoch [41/120    avg_loss:0.068, val_acc:0.946]
Epoch [42/120    avg_loss:0.040, val_acc:0.976]
Epoch [43/120    avg_loss:0.106, val_acc:0.968]
Epoch [44/120    avg_loss:0.115, val_acc:0.957]
Epoch [45/120    avg_loss:0.041, val_acc:0.974]
Epoch [46/120    avg_loss:0.038, val_acc:0.976]
Epoch [47/120    avg_loss:0.033, val_acc:0.979]
Epoch [48/120    avg_loss:0.026, val_acc:0.978]
Epoch [49/120    avg_loss:0.028, val_acc:0.981]
Epoch [50/120    avg_loss:0.025, val_acc:0.983]
Epoch [51/120    avg_loss:0.031, val_acc:0.981]
Epoch [52/120    avg_loss:0.023, val_acc:0.982]
Epoch [53/120    avg_loss:0.021, val_acc:0.981]
Epoch [54/120    avg_loss:0.020, val_acc:0.981]
Epoch [55/120    avg_loss:0.033, val_acc:0.981]
Epoch [56/120    avg_loss:0.022, val_acc:0.983]
Epoch [57/120    avg_loss:0.024, val_acc:0.981]
Epoch [58/120    avg_loss:0.021, val_acc:0.984]
Epoch [59/120    avg_loss:0.020, val_acc:0.983]
Epoch [60/120    avg_loss:0.021, val_acc:0.985]
Epoch [61/120    avg_loss:0.021, val_acc:0.987]
Epoch [62/120    avg_loss:0.015, val_acc:0.986]
Epoch [63/120    avg_loss:0.018, val_acc:0.987]
Epoch [64/120    avg_loss:0.016, val_acc:0.985]
Epoch [65/120    avg_loss:0.017, val_acc:0.985]
Epoch [66/120    avg_loss:0.020, val_acc:0.986]
Epoch [67/120    avg_loss:0.020, val_acc:0.988]
Epoch [68/120    avg_loss:0.027, val_acc:0.981]
Epoch [69/120    avg_loss:0.015, val_acc:0.985]
Epoch [70/120    avg_loss:0.018, val_acc:0.987]
Epoch [71/120    avg_loss:0.017, val_acc:0.986]
Epoch [72/120    avg_loss:0.014, val_acc:0.985]
Epoch [73/120    avg_loss:0.020, val_acc:0.985]
Epoch [74/120    avg_loss:0.019, val_acc:0.985]
Epoch [75/120    avg_loss:0.014, val_acc:0.986]
Epoch [76/120    avg_loss:0.015, val_acc:0.984]
Epoch [77/120    avg_loss:0.013, val_acc:0.987]
Epoch [78/120    avg_loss:0.021, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.015, val_acc:0.990]
Epoch [81/120    avg_loss:0.015, val_acc:0.986]
Epoch [82/120    avg_loss:0.013, val_acc:0.988]
Epoch [83/120    avg_loss:0.015, val_acc:0.987]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.987]
Epoch [86/120    avg_loss:0.012, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.985]
Epoch [88/120    avg_loss:0.022, val_acc:0.985]
Epoch [89/120    avg_loss:0.019, val_acc:0.983]
Epoch [90/120    avg_loss:0.017, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.014, val_acc:0.989]
Epoch [94/120    avg_loss:0.015, val_acc:0.989]
Epoch [95/120    avg_loss:0.010, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.014, val_acc:0.985]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.018, val_acc:0.987]
Epoch [106/120    avg_loss:0.015, val_acc:0.987]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.012, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.012, val_acc:0.987]
Epoch [112/120    avg_loss:0.012, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.987]
Epoch [116/120    avg_loss:0.015, val_acc:0.987]
Epoch [117/120    avg_loss:0.015, val_acc:0.987]
Epoch [118/120    avg_loss:0.012, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6260     0     4     0     0     0    24   137     7]
 [    0     2 18006     0    39     0    42     0     1     0]
 [    0     0     0  1973     0     0     0     0    62     1]
 [    0     7     5     1  2924     0    25     0     5     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     3     0     0  4852     0    15     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0     7     0    66    58     0     1     0  3437     2]
 [    0     0     0     0     6    23     0     0     0   890]]

Accuracy:
98.65760489721158

F1 scores:
[       nan 0.98512865 0.99731369 0.96644624 0.97482914 0.99126472
 0.99040621 0.9903957  0.9510238  0.97587719]

Kappa:
0.9822334664332217
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9cb6056940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.872, val_acc:0.296]
Epoch [2/120    avg_loss:1.423, val_acc:0.647]
Epoch [3/120    avg_loss:1.129, val_acc:0.657]
Epoch [4/120    avg_loss:0.941, val_acc:0.730]
Epoch [5/120    avg_loss:0.762, val_acc:0.693]
Epoch [6/120    avg_loss:0.609, val_acc:0.712]
Epoch [7/120    avg_loss:0.533, val_acc:0.789]
Epoch [8/120    avg_loss:0.402, val_acc:0.780]
Epoch [9/120    avg_loss:0.377, val_acc:0.868]
Epoch [10/120    avg_loss:0.295, val_acc:0.897]
Epoch [11/120    avg_loss:0.291, val_acc:0.868]
Epoch [12/120    avg_loss:0.231, val_acc:0.909]
Epoch [13/120    avg_loss:0.237, val_acc:0.928]
Epoch [14/120    avg_loss:0.223, val_acc:0.942]
Epoch [15/120    avg_loss:0.152, val_acc:0.935]
Epoch [16/120    avg_loss:0.193, val_acc:0.921]
Epoch [17/120    avg_loss:0.165, val_acc:0.925]
Epoch [18/120    avg_loss:0.155, val_acc:0.943]
Epoch [19/120    avg_loss:0.117, val_acc:0.951]
Epoch [20/120    avg_loss:0.124, val_acc:0.884]
Epoch [21/120    avg_loss:0.113, val_acc:0.940]
Epoch [22/120    avg_loss:0.196, val_acc:0.882]
Epoch [23/120    avg_loss:0.127, val_acc:0.952]
Epoch [24/120    avg_loss:0.122, val_acc:0.939]
Epoch [25/120    avg_loss:0.109, val_acc:0.953]
Epoch [26/120    avg_loss:0.088, val_acc:0.950]
Epoch [27/120    avg_loss:0.099, val_acc:0.962]
Epoch [28/120    avg_loss:0.067, val_acc:0.962]
Epoch [29/120    avg_loss:0.091, val_acc:0.958]
Epoch [30/120    avg_loss:0.069, val_acc:0.948]
Epoch [31/120    avg_loss:0.054, val_acc:0.968]
Epoch [32/120    avg_loss:0.066, val_acc:0.947]
Epoch [33/120    avg_loss:0.054, val_acc:0.967]
Epoch [34/120    avg_loss:0.045, val_acc:0.961]
Epoch [35/120    avg_loss:0.035, val_acc:0.970]
Epoch [36/120    avg_loss:0.057, val_acc:0.947]
Epoch [37/120    avg_loss:0.264, val_acc:0.776]
Epoch [38/120    avg_loss:0.154, val_acc:0.947]
Epoch [39/120    avg_loss:0.087, val_acc:0.917]
Epoch [40/120    avg_loss:0.065, val_acc:0.942]
Epoch [41/120    avg_loss:0.054, val_acc:0.966]
Epoch [42/120    avg_loss:0.061, val_acc:0.953]
Epoch [43/120    avg_loss:0.100, val_acc:0.963]
Epoch [44/120    avg_loss:0.048, val_acc:0.954]
Epoch [45/120    avg_loss:0.045, val_acc:0.968]
Epoch [46/120    avg_loss:0.039, val_acc:0.968]
Epoch [47/120    avg_loss:0.030, val_acc:0.979]
Epoch [48/120    avg_loss:0.027, val_acc:0.971]
Epoch [49/120    avg_loss:0.043, val_acc:0.971]
Epoch [50/120    avg_loss:0.057, val_acc:0.974]
Epoch [51/120    avg_loss:0.033, val_acc:0.975]
Epoch [52/120    avg_loss:0.017, val_acc:0.965]
Epoch [53/120    avg_loss:0.017, val_acc:0.973]
Epoch [54/120    avg_loss:0.017, val_acc:0.963]
Epoch [55/120    avg_loss:0.021, val_acc:0.965]
Epoch [56/120    avg_loss:0.041, val_acc:0.951]
Epoch [57/120    avg_loss:0.037, val_acc:0.955]
Epoch [58/120    avg_loss:0.025, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.978]
Epoch [60/120    avg_loss:0.016, val_acc:0.982]
Epoch [61/120    avg_loss:0.011, val_acc:0.968]
Epoch [62/120    avg_loss:0.016, val_acc:0.965]
Epoch [63/120    avg_loss:0.013, val_acc:0.980]
Epoch [64/120    avg_loss:0.021, val_acc:0.977]
Epoch [65/120    avg_loss:1.510, val_acc:0.652]
Epoch [66/120    avg_loss:0.771, val_acc:0.699]
Epoch [67/120    avg_loss:0.534, val_acc:0.718]
Epoch [68/120    avg_loss:0.452, val_acc:0.827]
Epoch [69/120    avg_loss:0.347, val_acc:0.793]
Epoch [70/120    avg_loss:0.342, val_acc:0.863]
Epoch [71/120    avg_loss:0.331, val_acc:0.890]
Epoch [72/120    avg_loss:0.296, val_acc:0.864]
Epoch [73/120    avg_loss:0.260, val_acc:0.885]
Epoch [74/120    avg_loss:0.209, val_acc:0.907]
Epoch [75/120    avg_loss:0.200, val_acc:0.896]
Epoch [76/120    avg_loss:0.177, val_acc:0.926]
Epoch [77/120    avg_loss:0.177, val_acc:0.922]
Epoch [78/120    avg_loss:0.165, val_acc:0.924]
Epoch [79/120    avg_loss:0.172, val_acc:0.927]
Epoch [80/120    avg_loss:0.169, val_acc:0.926]
Epoch [81/120    avg_loss:0.147, val_acc:0.924]
Epoch [82/120    avg_loss:0.155, val_acc:0.921]
Epoch [83/120    avg_loss:0.149, val_acc:0.922]
Epoch [84/120    avg_loss:0.163, val_acc:0.930]
Epoch [85/120    avg_loss:0.158, val_acc:0.920]
Epoch [86/120    avg_loss:0.152, val_acc:0.920]
Epoch [87/120    avg_loss:0.152, val_acc:0.928]
Epoch [88/120    avg_loss:0.144, val_acc:0.931]
Epoch [89/120    avg_loss:0.141, val_acc:0.932]
Epoch [90/120    avg_loss:0.124, val_acc:0.933]
Epoch [91/120    avg_loss:0.138, val_acc:0.933]
Epoch [92/120    avg_loss:0.141, val_acc:0.936]
Epoch [93/120    avg_loss:0.133, val_acc:0.933]
Epoch [94/120    avg_loss:0.145, val_acc:0.935]
Epoch [95/120    avg_loss:0.142, val_acc:0.934]
Epoch [96/120    avg_loss:0.132, val_acc:0.931]
Epoch [97/120    avg_loss:0.145, val_acc:0.935]
Epoch [98/120    avg_loss:0.129, val_acc:0.931]
Epoch [99/120    avg_loss:0.145, val_acc:0.929]
Epoch [100/120    avg_loss:0.130, val_acc:0.931]
Epoch [101/120    avg_loss:0.135, val_acc:0.931]
Epoch [102/120    avg_loss:0.134, val_acc:0.931]
Epoch [103/120    avg_loss:0.136, val_acc:0.932]
Epoch [104/120    avg_loss:0.127, val_acc:0.932]
Epoch [105/120    avg_loss:0.136, val_acc:0.932]
Epoch [106/120    avg_loss:0.134, val_acc:0.932]
Epoch [107/120    avg_loss:0.122, val_acc:0.932]
Epoch [108/120    avg_loss:0.137, val_acc:0.932]
Epoch [109/120    avg_loss:0.135, val_acc:0.933]
Epoch [110/120    avg_loss:0.132, val_acc:0.933]
Epoch [111/120    avg_loss:0.129, val_acc:0.933]
Epoch [112/120    avg_loss:0.141, val_acc:0.933]
Epoch [113/120    avg_loss:0.130, val_acc:0.934]
Epoch [114/120    avg_loss:0.142, val_acc:0.934]
Epoch [115/120    avg_loss:0.138, val_acc:0.934]
Epoch [116/120    avg_loss:0.134, val_acc:0.934]
Epoch [117/120    avg_loss:0.141, val_acc:0.934]
Epoch [118/120    avg_loss:0.128, val_acc:0.934]
Epoch [119/120    avg_loss:0.140, val_acc:0.934]
Epoch [120/120    avg_loss:0.118, val_acc:0.934]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5805    13    11     3     0     0    14   584     2]
 [    0     2 17586     0   227     0   275     0     0     0]
 [    0    19     1  1746     0     0     0     0   260    10]
 [    0    38    57     0  2857     0     4     0    15     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4709     0   163     0]
 [    0    52     0     0     0     3     0  1221    14     0]
 [    0    67     6   192    54     0    19     0  3215    18]
 [    0     7     0     0    15    26     0     0     1   870]]

Accuracy:
94.74851179717061

F1 scores:
[       nan 0.9346321  0.98374962 0.87496868 0.93244125 0.98901099
 0.9527567  0.96712871 0.82193532 0.95604396]

Kappa:
0.9307967053740372
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88e325e940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.894, val_acc:0.289]
Epoch [2/120    avg_loss:1.385, val_acc:0.405]
Epoch [3/120    avg_loss:1.078, val_acc:0.590]
Epoch [4/120    avg_loss:0.807, val_acc:0.654]
Epoch [5/120    avg_loss:0.623, val_acc:0.710]
Epoch [6/120    avg_loss:0.546, val_acc:0.793]
Epoch [7/120    avg_loss:0.404, val_acc:0.754]
Epoch [8/120    avg_loss:0.349, val_acc:0.834]
Epoch [9/120    avg_loss:0.336, val_acc:0.854]
Epoch [10/120    avg_loss:0.324, val_acc:0.871]
Epoch [11/120    avg_loss:0.262, val_acc:0.862]
Epoch [12/120    avg_loss:0.242, val_acc:0.932]
Epoch [13/120    avg_loss:0.199, val_acc:0.919]
Epoch [14/120    avg_loss:0.195, val_acc:0.924]
Epoch [15/120    avg_loss:0.185, val_acc:0.915]
Epoch [16/120    avg_loss:0.165, val_acc:0.960]
Epoch [17/120    avg_loss:0.131, val_acc:0.915]
Epoch [18/120    avg_loss:0.137, val_acc:0.935]
Epoch [19/120    avg_loss:0.191, val_acc:0.923]
Epoch [20/120    avg_loss:0.158, val_acc:0.894]
Epoch [21/120    avg_loss:0.152, val_acc:0.943]
Epoch [22/120    avg_loss:0.167, val_acc:0.908]
Epoch [23/120    avg_loss:0.130, val_acc:0.895]
Epoch [24/120    avg_loss:0.118, val_acc:0.954]
Epoch [25/120    avg_loss:0.101, val_acc:0.920]
Epoch [26/120    avg_loss:0.087, val_acc:0.968]
Epoch [27/120    avg_loss:0.068, val_acc:0.963]
Epoch [28/120    avg_loss:0.089, val_acc:0.969]
Epoch [29/120    avg_loss:0.081, val_acc:0.951]
Epoch [30/120    avg_loss:0.074, val_acc:0.896]
Epoch [31/120    avg_loss:0.064, val_acc:0.962]
Epoch [32/120    avg_loss:0.061, val_acc:0.968]
Epoch [33/120    avg_loss:0.039, val_acc:0.971]
Epoch [34/120    avg_loss:0.050, val_acc:0.971]
Epoch [35/120    avg_loss:0.278, val_acc:0.847]
Epoch [36/120    avg_loss:0.217, val_acc:0.876]
Epoch [37/120    avg_loss:0.118, val_acc:0.965]
Epoch [38/120    avg_loss:0.081, val_acc:0.967]
Epoch [39/120    avg_loss:0.073, val_acc:0.964]
Epoch [40/120    avg_loss:0.080, val_acc:0.981]
Epoch [41/120    avg_loss:0.049, val_acc:0.982]
Epoch [42/120    avg_loss:0.042, val_acc:0.973]
Epoch [43/120    avg_loss:0.041, val_acc:0.972]
Epoch [44/120    avg_loss:0.076, val_acc:0.954]
Epoch [45/120    avg_loss:0.055, val_acc:0.883]
Epoch [46/120    avg_loss:0.036, val_acc:0.958]
Epoch [47/120    avg_loss:0.020, val_acc:0.983]
Epoch [48/120    avg_loss:0.035, val_acc:0.972]
Epoch [49/120    avg_loss:0.094, val_acc:0.978]
Epoch [50/120    avg_loss:0.054, val_acc:0.980]
Epoch [51/120    avg_loss:0.027, val_acc:0.983]
Epoch [52/120    avg_loss:0.043, val_acc:0.973]
Epoch [53/120    avg_loss:0.028, val_acc:0.976]
Epoch [54/120    avg_loss:0.017, val_acc:0.986]
Epoch [55/120    avg_loss:0.028, val_acc:0.978]
Epoch [56/120    avg_loss:0.016, val_acc:0.984]
Epoch [57/120    avg_loss:0.025, val_acc:0.975]
Epoch [58/120    avg_loss:0.034, val_acc:0.982]
Epoch [59/120    avg_loss:0.028, val_acc:0.980]
Epoch [60/120    avg_loss:0.021, val_acc:0.992]
Epoch [61/120    avg_loss:0.014, val_acc:0.981]
Epoch [62/120    avg_loss:0.015, val_acc:0.973]
Epoch [63/120    avg_loss:0.018, val_acc:0.984]
Epoch [64/120    avg_loss:0.013, val_acc:0.984]
Epoch [65/120    avg_loss:0.015, val_acc:0.969]
Epoch [66/120    avg_loss:0.009, val_acc:0.981]
Epoch [67/120    avg_loss:0.018, val_acc:0.981]
Epoch [68/120    avg_loss:0.037, val_acc:0.905]
Epoch [69/120    avg_loss:0.057, val_acc:0.979]
Epoch [70/120    avg_loss:0.025, val_acc:0.941]
Epoch [71/120    avg_loss:0.026, val_acc:0.974]
Epoch [72/120    avg_loss:0.025, val_acc:0.979]
Epoch [73/120    avg_loss:0.027, val_acc:0.976]
Epoch [74/120    avg_loss:0.014, val_acc:0.983]
Epoch [75/120    avg_loss:0.015, val_acc:0.984]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.006, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.015, val_acc:0.987]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.012, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.012, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     1     0     1     0    35     0]
 [    0     0 17925     0    23     0   127     0    15     0]
 [    0     4     0  1931     0     0     0     0   100     1]
 [    0    25     2     0  2921     0    14     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4871     0     7     0]
 [    0     3     0     0     0     0     0  1286     0     1]
 [    0     1     0    44    52     0     0     0  3472     2]
 [    0     0     0     0    13     6     0     0     0   900]]

Accuracy:
98.82630805196058

F1 scores:
[       nan 0.99455677 0.9953633  0.96285216 0.97659646 0.99770642
 0.9849358  0.9984472  0.9631068  0.98738343]

Kappa:
0.9844729134044926
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa433c92908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.897, val_acc:0.580]
Epoch [2/120    avg_loss:1.375, val_acc:0.708]
Epoch [3/120    avg_loss:1.026, val_acc:0.618]
Epoch [4/120    avg_loss:0.759, val_acc:0.751]
Epoch [5/120    avg_loss:0.574, val_acc:0.668]
Epoch [6/120    avg_loss:0.512, val_acc:0.613]
Epoch [7/120    avg_loss:0.435, val_acc:0.758]
Epoch [8/120    avg_loss:0.383, val_acc:0.798]
Epoch [9/120    avg_loss:0.359, val_acc:0.849]
Epoch [10/120    avg_loss:0.337, val_acc:0.800]
Epoch [11/120    avg_loss:0.293, val_acc:0.833]
Epoch [12/120    avg_loss:0.253, val_acc:0.872]
Epoch [13/120    avg_loss:0.253, val_acc:0.834]
Epoch [14/120    avg_loss:0.223, val_acc:0.796]
Epoch [15/120    avg_loss:0.236, val_acc:0.872]
Epoch [16/120    avg_loss:0.222, val_acc:0.872]
Epoch [17/120    avg_loss:0.205, val_acc:0.891]
Epoch [18/120    avg_loss:0.178, val_acc:0.867]
Epoch [19/120    avg_loss:0.157, val_acc:0.905]
Epoch [20/120    avg_loss:0.135, val_acc:0.892]
Epoch [21/120    avg_loss:0.129, val_acc:0.876]
Epoch [22/120    avg_loss:0.139, val_acc:0.943]
Epoch [23/120    avg_loss:0.110, val_acc:0.919]
Epoch [24/120    avg_loss:0.125, val_acc:0.943]
Epoch [25/120    avg_loss:0.082, val_acc:0.948]
Epoch [26/120    avg_loss:0.095, val_acc:0.940]
Epoch [27/120    avg_loss:0.119, val_acc:0.948]
Epoch [28/120    avg_loss:0.083, val_acc:0.944]
Epoch [29/120    avg_loss:0.092, val_acc:0.922]
Epoch [30/120    avg_loss:0.093, val_acc:0.938]
Epoch [31/120    avg_loss:0.071, val_acc:0.955]
Epoch [32/120    avg_loss:0.099, val_acc:0.940]
Epoch [33/120    avg_loss:0.072, val_acc:0.958]
Epoch [34/120    avg_loss:0.073, val_acc:0.953]
Epoch [35/120    avg_loss:0.063, val_acc:0.958]
Epoch [36/120    avg_loss:0.067, val_acc:0.960]
Epoch [37/120    avg_loss:0.068, val_acc:0.943]
Epoch [38/120    avg_loss:0.058, val_acc:0.958]
Epoch [39/120    avg_loss:0.050, val_acc:0.965]
Epoch [40/120    avg_loss:0.059, val_acc:0.968]
Epoch [41/120    avg_loss:0.032, val_acc:0.980]
Epoch [42/120    avg_loss:0.042, val_acc:0.917]
Epoch [43/120    avg_loss:0.040, val_acc:0.978]
Epoch [44/120    avg_loss:0.025, val_acc:0.973]
Epoch [45/120    avg_loss:0.039, val_acc:0.972]
Epoch [46/120    avg_loss:0.041, val_acc:0.974]
Epoch [47/120    avg_loss:0.022, val_acc:0.978]
Epoch [48/120    avg_loss:0.034, val_acc:0.968]
Epoch [49/120    avg_loss:0.020, val_acc:0.973]
Epoch [50/120    avg_loss:0.026, val_acc:0.967]
Epoch [51/120    avg_loss:0.051, val_acc:0.961]
Epoch [52/120    avg_loss:0.061, val_acc:0.965]
Epoch [53/120    avg_loss:0.028, val_acc:0.975]
Epoch [54/120    avg_loss:0.028, val_acc:0.977]
Epoch [55/120    avg_loss:0.021, val_acc:0.979]
Epoch [56/120    avg_loss:0.020, val_acc:0.978]
Epoch [57/120    avg_loss:0.022, val_acc:0.978]
Epoch [58/120    avg_loss:0.015, val_acc:0.980]
Epoch [59/120    avg_loss:0.018, val_acc:0.979]
Epoch [60/120    avg_loss:0.014, val_acc:0.982]
Epoch [61/120    avg_loss:0.016, val_acc:0.979]
Epoch [62/120    avg_loss:0.013, val_acc:0.983]
Epoch [63/120    avg_loss:0.010, val_acc:0.983]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.982]
Epoch [66/120    avg_loss:0.018, val_acc:0.983]
Epoch [67/120    avg_loss:0.015, val_acc:0.981]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.980]
Epoch [72/120    avg_loss:0.008, val_acc:0.982]
Epoch [73/120    avg_loss:0.008, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.980]
Epoch [80/120    avg_loss:0.009, val_acc:0.980]
Epoch [81/120    avg_loss:0.012, val_acc:0.983]
Epoch [82/120    avg_loss:0.018, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.982]
Epoch [85/120    avg_loss:0.011, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.980]
Epoch [91/120    avg_loss:0.011, val_acc:0.980]
Epoch [92/120    avg_loss:0.007, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.981]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.010, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6322     0    11     0     0     0     0    95     4]
 [    0     0 18029     0    40     0    10     0    10     1]
 [    0     1     0  1942     0     0     0     0    89     4]
 [    0    10    10     0  2921     3     6     0    12    10]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4870     0     4     0]
 [    0     3     0     0     0     0     1  1286     0     0]
 [    0     4     0    33    47     0     0     0  3474    13]
 [    0     0     0     0     9    25     0     0     0   885]]

Accuracy:
98.89378931386017

F1 scores:
[       nan 0.98997808 0.99803482 0.96472926 0.975455   0.9893859
 0.99743984 0.9984472  0.95768436 0.96405229]

Kappa:
0.9853532822411888
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb69eca5898>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.926, val_acc:0.557]
Epoch [2/120    avg_loss:1.450, val_acc:0.615]
Epoch [3/120    avg_loss:1.145, val_acc:0.682]
Epoch [4/120    avg_loss:0.906, val_acc:0.741]
Epoch [5/120    avg_loss:0.725, val_acc:0.682]
Epoch [6/120    avg_loss:0.603, val_acc:0.740]
Epoch [7/120    avg_loss:0.477, val_acc:0.757]
Epoch [8/120    avg_loss:0.462, val_acc:0.824]
Epoch [9/120    avg_loss:0.371, val_acc:0.810]
Epoch [10/120    avg_loss:0.300, val_acc:0.818]
Epoch [11/120    avg_loss:0.267, val_acc:0.824]
Epoch [12/120    avg_loss:0.263, val_acc:0.891]
Epoch [13/120    avg_loss:0.244, val_acc:0.765]
Epoch [14/120    avg_loss:0.263, val_acc:0.907]
Epoch [15/120    avg_loss:0.213, val_acc:0.897]
Epoch [16/120    avg_loss:0.237, val_acc:0.916]
Epoch [17/120    avg_loss:0.162, val_acc:0.935]
Epoch [18/120    avg_loss:0.158, val_acc:0.887]
Epoch [19/120    avg_loss:0.153, val_acc:0.899]
Epoch [20/120    avg_loss:0.143, val_acc:0.944]
Epoch [21/120    avg_loss:0.097, val_acc:0.932]
Epoch [22/120    avg_loss:0.106, val_acc:0.943]
Epoch [23/120    avg_loss:0.115, val_acc:0.943]
Epoch [24/120    avg_loss:0.135, val_acc:0.928]
Epoch [25/120    avg_loss:0.101, val_acc:0.950]
Epoch [26/120    avg_loss:0.065, val_acc:0.939]
Epoch [27/120    avg_loss:0.086, val_acc:0.920]
Epoch [28/120    avg_loss:0.094, val_acc:0.955]
Epoch [29/120    avg_loss:0.078, val_acc:0.908]
Epoch [30/120    avg_loss:0.124, val_acc:0.941]
Epoch [31/120    avg_loss:0.111, val_acc:0.949]
Epoch [32/120    avg_loss:0.067, val_acc:0.963]
Epoch [33/120    avg_loss:0.083, val_acc:0.938]
Epoch [34/120    avg_loss:0.072, val_acc:0.955]
Epoch [35/120    avg_loss:0.077, val_acc:0.958]
Epoch [36/120    avg_loss:0.055, val_acc:0.955]
Epoch [37/120    avg_loss:0.057, val_acc:0.963]
Epoch [38/120    avg_loss:0.052, val_acc:0.968]
Epoch [39/120    avg_loss:0.031, val_acc:0.973]
Epoch [40/120    avg_loss:0.040, val_acc:0.952]
Epoch [41/120    avg_loss:0.061, val_acc:0.955]
Epoch [42/120    avg_loss:0.037, val_acc:0.973]
Epoch [43/120    avg_loss:0.035, val_acc:0.968]
Epoch [44/120    avg_loss:0.031, val_acc:0.972]
Epoch [45/120    avg_loss:0.021, val_acc:0.978]
Epoch [46/120    avg_loss:0.039, val_acc:0.956]
Epoch [47/120    avg_loss:0.022, val_acc:0.973]
Epoch [48/120    avg_loss:0.019, val_acc:0.976]
Epoch [49/120    avg_loss:0.030, val_acc:0.943]
Epoch [50/120    avg_loss:0.046, val_acc:0.963]
Epoch [51/120    avg_loss:0.040, val_acc:0.973]
Epoch [52/120    avg_loss:0.022, val_acc:0.978]
Epoch [53/120    avg_loss:0.043, val_acc:0.957]
Epoch [54/120    avg_loss:0.039, val_acc:0.979]
Epoch [55/120    avg_loss:0.033, val_acc:0.971]
Epoch [56/120    avg_loss:0.038, val_acc:0.971]
Epoch [57/120    avg_loss:0.018, val_acc:0.978]
Epoch [58/120    avg_loss:0.018, val_acc:0.962]
Epoch [59/120    avg_loss:0.019, val_acc:0.982]
Epoch [60/120    avg_loss:0.014, val_acc:0.982]
Epoch [61/120    avg_loss:0.025, val_acc:0.978]
Epoch [62/120    avg_loss:0.047, val_acc:0.952]
Epoch [63/120    avg_loss:0.198, val_acc:0.953]
Epoch [64/120    avg_loss:0.105, val_acc:0.931]
Epoch [65/120    avg_loss:0.044, val_acc:0.962]
Epoch [66/120    avg_loss:0.048, val_acc:0.961]
Epoch [67/120    avg_loss:0.032, val_acc:0.974]
Epoch [68/120    avg_loss:0.037, val_acc:0.946]
Epoch [69/120    avg_loss:0.084, val_acc:0.959]
Epoch [70/120    avg_loss:0.032, val_acc:0.981]
Epoch [71/120    avg_loss:0.019, val_acc:0.970]
Epoch [72/120    avg_loss:0.024, val_acc:0.976]
Epoch [73/120    avg_loss:0.015, val_acc:0.982]
Epoch [74/120    avg_loss:0.014, val_acc:0.972]
Epoch [75/120    avg_loss:0.010, val_acc:0.978]
Epoch [76/120    avg_loss:0.025, val_acc:0.967]
Epoch [77/120    avg_loss:0.021, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.010, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.008, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.005, val_acc:0.987]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.019, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.989]
Epoch [108/120    avg_loss:0.003, val_acc:0.987]
Epoch [109/120    avg_loss:0.013, val_acc:0.972]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0     1     0     0    11     0    41     8]
 [    0     2 18065     0    20     0     0     0     3     0]
 [    0     2     0  1963     0     0     0     0    62     9]
 [    0    29     7     0  2910     0     6     0     6    14]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     3     0     0  4844     0    17     0]
 [    0     4     0     0     0    14     0  1269     2     1]
 [    0    36     0    12    43     0     0     0  3479     1]
 [    0     0     0     2    10    20     0     0     0   887]]

Accuracy:
99.0359819728629

F1 scores:
[       nan 0.98959304 0.99872844 0.97734628 0.97732997 0.9871407
 0.99476332 0.99179367 0.96894583 0.9646547 ]

Kappa:
0.9872261780665975
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f861d8a98d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.894, val_acc:0.364]
Epoch [2/120    avg_loss:1.373, val_acc:0.451]
Epoch [3/120    avg_loss:1.075, val_acc:0.522]
Epoch [4/120    avg_loss:0.827, val_acc:0.603]
Epoch [5/120    avg_loss:0.741, val_acc:0.652]
Epoch [6/120    avg_loss:0.575, val_acc:0.718]
Epoch [7/120    avg_loss:0.496, val_acc:0.795]
Epoch [8/120    avg_loss:0.433, val_acc:0.772]
Epoch [9/120    avg_loss:0.367, val_acc:0.863]
Epoch [10/120    avg_loss:0.347, val_acc:0.881]
Epoch [11/120    avg_loss:0.266, val_acc:0.794]
Epoch [12/120    avg_loss:1.042, val_acc:0.520]
Epoch [13/120    avg_loss:0.898, val_acc:0.664]
Epoch [14/120    avg_loss:0.609, val_acc:0.740]
Epoch [15/120    avg_loss:0.504, val_acc:0.746]
Epoch [16/120    avg_loss:0.406, val_acc:0.787]
Epoch [17/120    avg_loss:0.355, val_acc:0.801]
Epoch [18/120    avg_loss:0.325, val_acc:0.792]
Epoch [19/120    avg_loss:0.308, val_acc:0.749]
Epoch [20/120    avg_loss:0.256, val_acc:0.845]
Epoch [21/120    avg_loss:0.264, val_acc:0.838]
Epoch [22/120    avg_loss:0.243, val_acc:0.839]
Epoch [23/120    avg_loss:0.209, val_acc:0.913]
Epoch [24/120    avg_loss:0.186, val_acc:0.908]
Epoch [25/120    avg_loss:0.233, val_acc:0.877]
Epoch [26/120    avg_loss:0.211, val_acc:0.805]
Epoch [27/120    avg_loss:0.185, val_acc:0.915]
Epoch [28/120    avg_loss:0.193, val_acc:0.865]
Epoch [29/120    avg_loss:0.216, val_acc:0.821]
Epoch [30/120    avg_loss:0.170, val_acc:0.919]
Epoch [31/120    avg_loss:0.145, val_acc:0.888]
Epoch [32/120    avg_loss:0.141, val_acc:0.937]
Epoch [33/120    avg_loss:0.109, val_acc:0.931]
Epoch [34/120    avg_loss:0.141, val_acc:0.921]
Epoch [35/120    avg_loss:0.121, val_acc:0.943]
Epoch [36/120    avg_loss:0.102, val_acc:0.946]
Epoch [37/120    avg_loss:0.092, val_acc:0.935]
Epoch [38/120    avg_loss:0.162, val_acc:0.945]
Epoch [39/120    avg_loss:0.087, val_acc:0.946]
Epoch [40/120    avg_loss:0.085, val_acc:0.932]
Epoch [41/120    avg_loss:0.114, val_acc:0.944]
Epoch [42/120    avg_loss:0.087, val_acc:0.933]
Epoch [43/120    avg_loss:0.104, val_acc:0.938]
Epoch [44/120    avg_loss:0.067, val_acc:0.954]
Epoch [45/120    avg_loss:0.073, val_acc:0.958]
Epoch [46/120    avg_loss:0.058, val_acc:0.954]
Epoch [47/120    avg_loss:0.069, val_acc:0.953]
Epoch [48/120    avg_loss:0.086, val_acc:0.958]
Epoch [49/120    avg_loss:0.061, val_acc:0.967]
Epoch [50/120    avg_loss:0.085, val_acc:0.913]
Epoch [51/120    avg_loss:0.058, val_acc:0.959]
Epoch [52/120    avg_loss:0.046, val_acc:0.943]
Epoch [53/120    avg_loss:0.068, val_acc:0.952]
Epoch [54/120    avg_loss:0.055, val_acc:0.961]
Epoch [55/120    avg_loss:0.044, val_acc:0.955]
Epoch [56/120    avg_loss:0.070, val_acc:0.922]
Epoch [57/120    avg_loss:0.082, val_acc:0.952]
Epoch [58/120    avg_loss:0.056, val_acc:0.966]
Epoch [59/120    avg_loss:0.057, val_acc:0.963]
Epoch [60/120    avg_loss:0.033, val_acc:0.962]
Epoch [61/120    avg_loss:0.041, val_acc:0.966]
Epoch [62/120    avg_loss:0.033, val_acc:0.962]
Epoch [63/120    avg_loss:0.027, val_acc:0.967]
Epoch [64/120    avg_loss:0.021, val_acc:0.970]
Epoch [65/120    avg_loss:0.029, val_acc:0.969]
Epoch [66/120    avg_loss:0.023, val_acc:0.969]
Epoch [67/120    avg_loss:0.018, val_acc:0.970]
Epoch [68/120    avg_loss:0.020, val_acc:0.969]
Epoch [69/120    avg_loss:0.019, val_acc:0.970]
Epoch [70/120    avg_loss:0.018, val_acc:0.970]
Epoch [71/120    avg_loss:0.018, val_acc:0.970]
Epoch [72/120    avg_loss:0.021, val_acc:0.968]
Epoch [73/120    avg_loss:0.018, val_acc:0.970]
Epoch [74/120    avg_loss:0.027, val_acc:0.974]
Epoch [75/120    avg_loss:0.017, val_acc:0.971]
Epoch [76/120    avg_loss:0.017, val_acc:0.971]
Epoch [77/120    avg_loss:0.018, val_acc:0.971]
Epoch [78/120    avg_loss:0.016, val_acc:0.972]
Epoch [79/120    avg_loss:0.018, val_acc:0.970]
Epoch [80/120    avg_loss:0.018, val_acc:0.973]
Epoch [81/120    avg_loss:0.016, val_acc:0.973]
Epoch [82/120    avg_loss:0.013, val_acc:0.971]
Epoch [83/120    avg_loss:0.022, val_acc:0.972]
Epoch [84/120    avg_loss:0.017, val_acc:0.973]
Epoch [85/120    avg_loss:0.017, val_acc:0.972]
Epoch [86/120    avg_loss:0.023, val_acc:0.976]
Epoch [87/120    avg_loss:0.018, val_acc:0.973]
Epoch [88/120    avg_loss:0.024, val_acc:0.974]
Epoch [89/120    avg_loss:0.015, val_acc:0.973]
Epoch [90/120    avg_loss:0.012, val_acc:0.973]
Epoch [91/120    avg_loss:0.014, val_acc:0.973]
Epoch [92/120    avg_loss:0.020, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.974]
Epoch [94/120    avg_loss:0.018, val_acc:0.974]
Epoch [95/120    avg_loss:0.017, val_acc:0.974]
Epoch [96/120    avg_loss:0.019, val_acc:0.974]
Epoch [97/120    avg_loss:0.018, val_acc:0.974]
Epoch [98/120    avg_loss:0.014, val_acc:0.975]
Epoch [99/120    avg_loss:0.013, val_acc:0.974]
Epoch [100/120    avg_loss:0.019, val_acc:0.972]
Epoch [101/120    avg_loss:0.016, val_acc:0.973]
Epoch [102/120    avg_loss:0.017, val_acc:0.976]
Epoch [103/120    avg_loss:0.014, val_acc:0.975]
Epoch [104/120    avg_loss:0.012, val_acc:0.975]
Epoch [105/120    avg_loss:0.019, val_acc:0.975]
Epoch [106/120    avg_loss:0.011, val_acc:0.973]
Epoch [107/120    avg_loss:0.018, val_acc:0.973]
Epoch [108/120    avg_loss:0.015, val_acc:0.973]
Epoch [109/120    avg_loss:0.014, val_acc:0.974]
Epoch [110/120    avg_loss:0.019, val_acc:0.976]
Epoch [111/120    avg_loss:0.015, val_acc:0.974]
Epoch [112/120    avg_loss:0.020, val_acc:0.975]
Epoch [113/120    avg_loss:0.014, val_acc:0.975]
Epoch [114/120    avg_loss:0.013, val_acc:0.976]
Epoch [115/120    avg_loss:0.014, val_acc:0.974]
Epoch [116/120    avg_loss:0.018, val_acc:0.975]
Epoch [117/120    avg_loss:0.022, val_acc:0.975]
Epoch [118/120    avg_loss:0.015, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.973]
Epoch [120/120    avg_loss:0.019, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6328     0     3     0     0     0     0   101     0]
 [    0     0 17454     0    30     0   606     0     0     0]
 [    0     5     0  1918     0     0     0     0   101    12]
 [    0    14     1     2  2914     0    19     0    19     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0    44     0     0     0     0     2  1235     7     2]
 [    0     0     0    69    46     0    51     0  3346    59]
 [    0     0     0     0    16    18     0     0     0   885]]

Accuracy:
97.02841443134987

F1 scores:
[       nan 0.98697653 0.98207905 0.95233366 0.974908   0.99315068
 0.93471383 0.97821782 0.93620593 0.94148936]

Kappa:
0.960878643194188
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff15e7e978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.902, val_acc:0.298]
Epoch [2/120    avg_loss:1.413, val_acc:0.413]
Epoch [3/120    avg_loss:1.084, val_acc:0.425]
Epoch [4/120    avg_loss:0.887, val_acc:0.467]
Epoch [5/120    avg_loss:0.712, val_acc:0.498]
Epoch [6/120    avg_loss:0.586, val_acc:0.697]
Epoch [7/120    avg_loss:0.502, val_acc:0.667]
Epoch [8/120    avg_loss:0.482, val_acc:0.812]
Epoch [9/120    avg_loss:0.417, val_acc:0.729]
Epoch [10/120    avg_loss:0.355, val_acc:0.792]
Epoch [11/120    avg_loss:0.295, val_acc:0.894]
Epoch [12/120    avg_loss:0.272, val_acc:0.907]
Epoch [13/120    avg_loss:0.245, val_acc:0.868]
Epoch [14/120    avg_loss:0.229, val_acc:0.892]
Epoch [15/120    avg_loss:0.179, val_acc:0.897]
Epoch [16/120    avg_loss:0.165, val_acc:0.931]
Epoch [17/120    avg_loss:0.146, val_acc:0.939]
Epoch [18/120    avg_loss:0.204, val_acc:0.792]
Epoch [19/120    avg_loss:0.186, val_acc:0.918]
Epoch [20/120    avg_loss:0.136, val_acc:0.931]
Epoch [21/120    avg_loss:0.129, val_acc:0.934]
Epoch [22/120    avg_loss:0.097, val_acc:0.888]
Epoch [23/120    avg_loss:0.126, val_acc:0.943]
Epoch [24/120    avg_loss:0.089, val_acc:0.945]
Epoch [25/120    avg_loss:0.077, val_acc:0.959]
Epoch [26/120    avg_loss:0.088, val_acc:0.964]
Epoch [27/120    avg_loss:0.057, val_acc:0.966]
Epoch [28/120    avg_loss:0.163, val_acc:0.921]
Epoch [29/120    avg_loss:0.093, val_acc:0.956]
Epoch [30/120    avg_loss:0.074, val_acc:0.953]
Epoch [31/120    avg_loss:0.057, val_acc:0.983]
Epoch [32/120    avg_loss:0.051, val_acc:0.975]
Epoch [33/120    avg_loss:0.058, val_acc:0.976]
Epoch [34/120    avg_loss:0.053, val_acc:0.976]
Epoch [35/120    avg_loss:0.036, val_acc:0.974]
Epoch [36/120    avg_loss:0.034, val_acc:0.983]
Epoch [37/120    avg_loss:0.034, val_acc:0.980]
Epoch [38/120    avg_loss:0.024, val_acc:0.978]
Epoch [39/120    avg_loss:0.018, val_acc:0.985]
Epoch [40/120    avg_loss:0.023, val_acc:0.981]
Epoch [41/120    avg_loss:0.032, val_acc:0.974]
Epoch [42/120    avg_loss:0.034, val_acc:0.978]
Epoch [43/120    avg_loss:0.042, val_acc:0.966]
Epoch [44/120    avg_loss:0.040, val_acc:0.903]
Epoch [45/120    avg_loss:0.051, val_acc:0.977]
Epoch [46/120    avg_loss:0.037, val_acc:0.982]
Epoch [47/120    avg_loss:0.025, val_acc:0.987]
Epoch [48/120    avg_loss:0.022, val_acc:0.966]
Epoch [49/120    avg_loss:0.051, val_acc:0.954]
Epoch [50/120    avg_loss:0.054, val_acc:0.963]
Epoch [51/120    avg_loss:0.054, val_acc:0.978]
Epoch [52/120    avg_loss:0.035, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.981]
Epoch [54/120    avg_loss:0.021, val_acc:0.983]
Epoch [55/120    avg_loss:0.018, val_acc:0.989]
Epoch [56/120    avg_loss:0.013, val_acc:0.989]
Epoch [57/120    avg_loss:0.012, val_acc:0.989]
Epoch [58/120    avg_loss:0.014, val_acc:0.983]
Epoch [59/120    avg_loss:0.007, val_acc:0.993]
Epoch [60/120    avg_loss:0.011, val_acc:0.978]
Epoch [61/120    avg_loss:0.013, val_acc:0.980]
Epoch [62/120    avg_loss:0.013, val_acc:0.990]
Epoch [63/120    avg_loss:0.015, val_acc:0.985]
Epoch [64/120    avg_loss:0.018, val_acc:0.964]
Epoch [65/120    avg_loss:0.028, val_acc:0.980]
Epoch [66/120    avg_loss:0.034, val_acc:0.987]
Epoch [67/120    avg_loss:0.009, val_acc:0.991]
Epoch [68/120    avg_loss:0.017, val_acc:0.993]
Epoch [69/120    avg_loss:0.031, val_acc:0.968]
Epoch [70/120    avg_loss:0.056, val_acc:0.562]
Epoch [71/120    avg_loss:0.151, val_acc:0.958]
Epoch [72/120    avg_loss:0.049, val_acc:0.973]
Epoch [73/120    avg_loss:0.028, val_acc:0.984]
Epoch [74/120    avg_loss:0.041, val_acc:0.980]
Epoch [75/120    avg_loss:0.020, val_acc:0.962]
Epoch [76/120    avg_loss:0.025, val_acc:0.991]
Epoch [77/120    avg_loss:0.013, val_acc:0.965]
Epoch [78/120    avg_loss:0.024, val_acc:0.986]
Epoch [79/120    avg_loss:0.032, val_acc:0.982]
Epoch [80/120    avg_loss:0.019, val_acc:0.980]
Epoch [81/120    avg_loss:0.017, val_acc:0.990]
Epoch [82/120    avg_loss:0.008, val_acc:0.991]
Epoch [83/120    avg_loss:0.010, val_acc:0.991]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.008, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.991]
Epoch [87/120    avg_loss:0.008, val_acc:0.991]
Epoch [88/120    avg_loss:0.007, val_acc:0.991]
Epoch [89/120    avg_loss:0.010, val_acc:0.992]
Epoch [90/120    avg_loss:0.007, val_acc:0.991]
Epoch [91/120    avg_loss:0.010, val_acc:0.992]
Epoch [92/120    avg_loss:0.007, val_acc:0.993]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.009, val_acc:0.991]
Epoch [96/120    avg_loss:0.009, val_acc:0.991]
Epoch [97/120    avg_loss:0.009, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.009, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.009, val_acc:0.991]
Epoch [102/120    avg_loss:0.009, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.010, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.011, val_acc:0.991]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.009, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.991]
Epoch [111/120    avg_loss:0.010, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.008, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.007, val_acc:0.991]
Epoch [116/120    avg_loss:0.008, val_acc:0.991]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.991]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     4     2     0     0     0    59     0]
 [    0     1 17968     0    68     0    50     0     3     0]
 [    0     1     0  1947     0     0     0     0    85     3]
 [    0    18     8     2  2929     0     0     0    12     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     2     0  4839     0     8     0]
 [    0     9     0     0     0     5     0  1266     0    10]
 [    0     9     0    58    23     0     0     0  3480     1]
 [    0     0     0     0    14    15     0     0     0   890]]

Accuracy:
98.79015737594293

F1 scores:
[       nan 0.99197632 0.99559496 0.96219422 0.97470882 0.99239544
 0.99088768 0.99061033 0.96425603 0.97480832]

Kappa:
0.9839843472387461
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3ede048d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.810, val_acc:0.279]
Epoch [2/120    avg_loss:1.375, val_acc:0.385]
Epoch [3/120    avg_loss:1.069, val_acc:0.693]
Epoch [4/120    avg_loss:0.830, val_acc:0.652]
Epoch [5/120    avg_loss:0.666, val_acc:0.693]
Epoch [6/120    avg_loss:0.516, val_acc:0.704]
Epoch [7/120    avg_loss:0.430, val_acc:0.785]
Epoch [8/120    avg_loss:0.363, val_acc:0.831]
Epoch [9/120    avg_loss:0.323, val_acc:0.863]
Epoch [10/120    avg_loss:0.287, val_acc:0.887]
Epoch [11/120    avg_loss:0.295, val_acc:0.780]
Epoch [12/120    avg_loss:0.367, val_acc:0.900]
Epoch [13/120    avg_loss:0.234, val_acc:0.907]
Epoch [14/120    avg_loss:0.176, val_acc:0.912]
Epoch [15/120    avg_loss:0.156, val_acc:0.923]
Epoch [16/120    avg_loss:0.161, val_acc:0.910]
Epoch [17/120    avg_loss:0.195, val_acc:0.934]
Epoch [18/120    avg_loss:0.151, val_acc:0.953]
Epoch [19/120    avg_loss:0.140, val_acc:0.926]
Epoch [20/120    avg_loss:0.094, val_acc:0.948]
Epoch [21/120    avg_loss:0.105, val_acc:0.957]
Epoch [22/120    avg_loss:0.106, val_acc:0.905]
Epoch [23/120    avg_loss:0.078, val_acc:0.953]
Epoch [24/120    avg_loss:0.109, val_acc:0.935]
Epoch [25/120    avg_loss:0.108, val_acc:0.919]
Epoch [26/120    avg_loss:0.073, val_acc:0.973]
Epoch [27/120    avg_loss:0.083, val_acc:0.915]
Epoch [28/120    avg_loss:0.063, val_acc:0.965]
Epoch [29/120    avg_loss:0.070, val_acc:0.943]
Epoch [30/120    avg_loss:0.055, val_acc:0.966]
Epoch [31/120    avg_loss:0.050, val_acc:0.944]
Epoch [32/120    avg_loss:0.060, val_acc:0.974]
Epoch [33/120    avg_loss:0.045, val_acc:0.977]
Epoch [34/120    avg_loss:0.039, val_acc:0.953]
Epoch [35/120    avg_loss:0.076, val_acc:0.963]
Epoch [36/120    avg_loss:0.122, val_acc:0.954]
Epoch [37/120    avg_loss:0.106, val_acc:0.965]
Epoch [38/120    avg_loss:0.062, val_acc:0.970]
Epoch [39/120    avg_loss:0.058, val_acc:0.967]
Epoch [40/120    avg_loss:0.091, val_acc:0.958]
Epoch [41/120    avg_loss:0.070, val_acc:0.958]
Epoch [42/120    avg_loss:0.065, val_acc:0.957]
Epoch [43/120    avg_loss:0.048, val_acc:0.978]
Epoch [44/120    avg_loss:0.032, val_acc:0.978]
Epoch [45/120    avg_loss:0.029, val_acc:0.983]
Epoch [46/120    avg_loss:0.045, val_acc:0.975]
Epoch [47/120    avg_loss:0.050, val_acc:0.972]
Epoch [48/120    avg_loss:0.025, val_acc:0.983]
Epoch [49/120    avg_loss:0.016, val_acc:0.984]
Epoch [50/120    avg_loss:0.017, val_acc:0.986]
Epoch [51/120    avg_loss:0.023, val_acc:0.985]
Epoch [52/120    avg_loss:0.017, val_acc:0.988]
Epoch [53/120    avg_loss:0.018, val_acc:0.979]
Epoch [54/120    avg_loss:0.013, val_acc:0.987]
Epoch [55/120    avg_loss:0.010, val_acc:0.987]
Epoch [56/120    avg_loss:0.013, val_acc:0.980]
Epoch [57/120    avg_loss:0.026, val_acc:0.987]
Epoch [58/120    avg_loss:0.015, val_acc:0.991]
Epoch [59/120    avg_loss:0.008, val_acc:0.990]
Epoch [60/120    avg_loss:0.014, val_acc:0.983]
Epoch [61/120    avg_loss:0.015, val_acc:0.987]
Epoch [62/120    avg_loss:0.015, val_acc:0.933]
Epoch [63/120    avg_loss:0.011, val_acc:0.993]
Epoch [64/120    avg_loss:0.009, val_acc:0.991]
Epoch [65/120    avg_loss:0.009, val_acc:0.989]
Epoch [66/120    avg_loss:0.009, val_acc:0.971]
Epoch [67/120    avg_loss:0.009, val_acc:0.991]
Epoch [68/120    avg_loss:0.030, val_acc:0.977]
Epoch [69/120    avg_loss:0.015, val_acc:0.985]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.021, val_acc:0.988]
Epoch [74/120    avg_loss:0.026, val_acc:0.983]
Epoch [75/120    avg_loss:0.083, val_acc:0.963]
Epoch [76/120    avg_loss:0.052, val_acc:0.962]
Epoch [77/120    avg_loss:0.023, val_acc:0.982]
Epoch [78/120    avg_loss:0.015, val_acc:0.983]
Epoch [79/120    avg_loss:0.013, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.988]
Epoch [83/120    avg_loss:0.013, val_acc:0.987]
Epoch [84/120    avg_loss:0.012, val_acc:0.987]
Epoch [85/120    avg_loss:0.015, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.987]
Epoch [87/120    avg_loss:0.010, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.012, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6327     0     0     1     0     0     6    98     0]
 [    0     0 17883     0   145     0    59     0     3     0]
 [    0     5     0  1970     0     0     0     0    54     7]
 [    0     7     1     2  2955     0     0     0     6     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     1     0  4860     0    12     0]
 [    0     5     0     0     0     0     0  1283     0     2]
 [    0     6     0    54    67     0     0     0  3437     7]
 [    0     0     0     0    16    10     0     0     0   893]]

Accuracy:
98.60217386065119

F1 scores:
[       nan 0.98998592 0.99407988 0.96996553 0.95988306 0.99618321
 0.99214045 0.99495929 0.95724829 0.97648989]

Kappa:
0.9815235786825071
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb1d2082908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.912, val_acc:0.278]
Epoch [2/120    avg_loss:1.383, val_acc:0.448]
Epoch [3/120    avg_loss:1.096, val_acc:0.644]
Epoch [4/120    avg_loss:0.870, val_acc:0.686]
Epoch [5/120    avg_loss:0.685, val_acc:0.728]
Epoch [6/120    avg_loss:0.561, val_acc:0.786]
Epoch [7/120    avg_loss:0.452, val_acc:0.828]
Epoch [8/120    avg_loss:0.386, val_acc:0.750]
Epoch [9/120    avg_loss:0.333, val_acc:0.848]
Epoch [10/120    avg_loss:0.270, val_acc:0.881]
Epoch [11/120    avg_loss:0.260, val_acc:0.895]
Epoch [12/120    avg_loss:0.213, val_acc:0.921]
Epoch [13/120    avg_loss:0.179, val_acc:0.926]
Epoch [14/120    avg_loss:0.166, val_acc:0.873]
Epoch [15/120    avg_loss:0.148, val_acc:0.937]
Epoch [16/120    avg_loss:0.137, val_acc:0.905]
Epoch [17/120    avg_loss:0.246, val_acc:0.905]
Epoch [18/120    avg_loss:0.150, val_acc:0.878]
Epoch [19/120    avg_loss:0.157, val_acc:0.920]
Epoch [20/120    avg_loss:0.110, val_acc:0.928]
Epoch [21/120    avg_loss:0.106, val_acc:0.922]
Epoch [22/120    avg_loss:0.114, val_acc:0.926]
Epoch [23/120    avg_loss:0.085, val_acc:0.956]
Epoch [24/120    avg_loss:0.079, val_acc:0.954]
Epoch [25/120    avg_loss:0.072, val_acc:0.963]
Epoch [26/120    avg_loss:0.058, val_acc:0.964]
Epoch [27/120    avg_loss:0.055, val_acc:0.968]
Epoch [28/120    avg_loss:0.043, val_acc:0.958]
Epoch [29/120    avg_loss:0.071, val_acc:0.955]
Epoch [30/120    avg_loss:0.054, val_acc:0.950]
Epoch [31/120    avg_loss:0.129, val_acc:0.955]
Epoch [32/120    avg_loss:0.059, val_acc:0.968]
Epoch [33/120    avg_loss:0.056, val_acc:0.969]
Epoch [34/120    avg_loss:0.037, val_acc:0.970]
Epoch [35/120    avg_loss:0.039, val_acc:0.976]
Epoch [36/120    avg_loss:0.036, val_acc:0.976]
Epoch [37/120    avg_loss:0.031, val_acc:0.975]
Epoch [38/120    avg_loss:0.037, val_acc:0.977]
Epoch [39/120    avg_loss:0.060, val_acc:0.961]
Epoch [40/120    avg_loss:0.024, val_acc:0.975]
Epoch [41/120    avg_loss:0.041, val_acc:0.973]
Epoch [42/120    avg_loss:0.030, val_acc:0.970]
Epoch [43/120    avg_loss:0.047, val_acc:0.970]
Epoch [44/120    avg_loss:0.038, val_acc:0.971]
Epoch [45/120    avg_loss:0.024, val_acc:0.981]
Epoch [46/120    avg_loss:0.023, val_acc:0.965]
Epoch [47/120    avg_loss:0.052, val_acc:0.955]
Epoch [48/120    avg_loss:0.031, val_acc:0.968]
Epoch [49/120    avg_loss:0.061, val_acc:0.977]
Epoch [50/120    avg_loss:0.029, val_acc:0.979]
Epoch [51/120    avg_loss:0.023, val_acc:0.974]
Epoch [52/120    avg_loss:0.024, val_acc:0.971]
Epoch [53/120    avg_loss:0.023, val_acc:0.974]
Epoch [54/120    avg_loss:0.016, val_acc:0.972]
Epoch [55/120    avg_loss:0.016, val_acc:0.983]
Epoch [56/120    avg_loss:0.055, val_acc:0.966]
Epoch [57/120    avg_loss:0.031, val_acc:0.981]
Epoch [58/120    avg_loss:0.013, val_acc:0.987]
Epoch [59/120    avg_loss:0.011, val_acc:0.986]
Epoch [60/120    avg_loss:0.009, val_acc:0.984]
Epoch [61/120    avg_loss:0.010, val_acc:0.970]
Epoch [62/120    avg_loss:0.009, val_acc:0.986]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.009, val_acc:0.985]
Epoch [65/120    avg_loss:0.097, val_acc:0.928]
Epoch [66/120    avg_loss:0.071, val_acc:0.970]
Epoch [67/120    avg_loss:0.022, val_acc:0.978]
Epoch [68/120    avg_loss:0.023, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.019, val_acc:0.986]
Epoch [72/120    avg_loss:0.025, val_acc:0.955]
Epoch [73/120    avg_loss:0.011, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.019, val_acc:0.985]
Epoch [78/120    avg_loss:0.017, val_acc:0.980]
Epoch [79/120    avg_loss:0.015, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.016, val_acc:0.939]
Epoch [82/120    avg_loss:0.067, val_acc:0.986]
Epoch [83/120    avg_loss:0.015, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.978]
Epoch [86/120    avg_loss:0.013, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     0     0     0     0     1    51     0]
 [    0     0 18062     0     3     0    15     0    10     0]
 [    0     1     0  1974     0     0     0     0    60     1]
 [    0    19     1     0  2943     0     0     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     2     0     0  4856     0     1     0]
 [    0     1     0     0     0     0     0  1286     0     3]
 [    0     1     0    47    58     0     0     0  3463     2]
 [    0     0     0     0    13    19     0     0     0   887]]

Accuracy:
99.18781481213699

F1 scores:
[       nan 0.99423407 0.99867301 0.97265336 0.9828018  0.99277292
 0.99620474 0.99805976 0.96677834 0.97848869]

Kappa:
0.9892396129545277
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28bc3279b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.882, val_acc:0.259]
Epoch [2/120    avg_loss:1.440, val_acc:0.348]
Epoch [3/120    avg_loss:1.126, val_acc:0.479]
Epoch [4/120    avg_loss:0.804, val_acc:0.551]
Epoch [5/120    avg_loss:0.636, val_acc:0.651]
Epoch [6/120    avg_loss:0.559, val_acc:0.748]
Epoch [7/120    avg_loss:0.509, val_acc:0.760]
Epoch [8/120    avg_loss:0.426, val_acc:0.775]
Epoch [9/120    avg_loss:0.379, val_acc:0.816]
Epoch [10/120    avg_loss:0.332, val_acc:0.862]
Epoch [11/120    avg_loss:0.332, val_acc:0.870]
Epoch [12/120    avg_loss:0.283, val_acc:0.861]
Epoch [13/120    avg_loss:0.270, val_acc:0.896]
Epoch [14/120    avg_loss:0.238, val_acc:0.872]
Epoch [15/120    avg_loss:0.192, val_acc:0.922]
Epoch [16/120    avg_loss:0.171, val_acc:0.917]
Epoch [17/120    avg_loss:0.140, val_acc:0.908]
Epoch [18/120    avg_loss:0.126, val_acc:0.937]
Epoch [19/120    avg_loss:0.167, val_acc:0.894]
Epoch [20/120    avg_loss:0.135, val_acc:0.900]
Epoch [21/120    avg_loss:0.139, val_acc:0.934]
Epoch [22/120    avg_loss:0.114, val_acc:0.948]
Epoch [23/120    avg_loss:0.091, val_acc:0.939]
Epoch [24/120    avg_loss:0.128, val_acc:0.900]
Epoch [25/120    avg_loss:0.120, val_acc:0.918]
Epoch [26/120    avg_loss:0.082, val_acc:0.954]
Epoch [27/120    avg_loss:0.081, val_acc:0.966]
Epoch [28/120    avg_loss:0.062, val_acc:0.974]
Epoch [29/120    avg_loss:0.058, val_acc:0.941]
Epoch [30/120    avg_loss:0.078, val_acc:0.961]
Epoch [31/120    avg_loss:0.076, val_acc:0.934]
Epoch [32/120    avg_loss:0.068, val_acc:0.965]
Epoch [33/120    avg_loss:0.046, val_acc:0.966]
Epoch [34/120    avg_loss:0.051, val_acc:0.965]
Epoch [35/120    avg_loss:0.057, val_acc:0.973]
Epoch [36/120    avg_loss:0.036, val_acc:0.972]
Epoch [37/120    avg_loss:0.030, val_acc:0.979]
Epoch [38/120    avg_loss:0.044, val_acc:0.976]
Epoch [39/120    avg_loss:0.029, val_acc:0.970]
Epoch [40/120    avg_loss:0.037, val_acc:0.952]
Epoch [41/120    avg_loss:0.032, val_acc:0.939]
Epoch [42/120    avg_loss:0.037, val_acc:0.962]
Epoch [43/120    avg_loss:0.028, val_acc:0.969]
Epoch [44/120    avg_loss:0.024, val_acc:0.977]
Epoch [45/120    avg_loss:0.016, val_acc:0.982]
Epoch [46/120    avg_loss:0.028, val_acc:0.965]
Epoch [47/120    avg_loss:0.027, val_acc:0.977]
Epoch [48/120    avg_loss:0.017, val_acc:0.976]
Epoch [49/120    avg_loss:0.018, val_acc:0.969]
Epoch [50/120    avg_loss:0.018, val_acc:0.980]
Epoch [51/120    avg_loss:0.022, val_acc:0.978]
Epoch [52/120    avg_loss:0.027, val_acc:0.963]
Epoch [53/120    avg_loss:0.018, val_acc:0.985]
Epoch [54/120    avg_loss:0.014, val_acc:0.983]
Epoch [55/120    avg_loss:0.055, val_acc:0.965]
Epoch [56/120    avg_loss:0.032, val_acc:0.973]
Epoch [57/120    avg_loss:0.014, val_acc:0.979]
Epoch [58/120    avg_loss:0.027, val_acc:0.953]
Epoch [59/120    avg_loss:0.037, val_acc:0.979]
Epoch [60/120    avg_loss:0.024, val_acc:0.979]
Epoch [61/120    avg_loss:0.018, val_acc:0.977]
Epoch [62/120    avg_loss:0.012, val_acc:0.973]
Epoch [63/120    avg_loss:0.018, val_acc:0.975]
Epoch [64/120    avg_loss:0.011, val_acc:0.982]
Epoch [65/120    avg_loss:0.018, val_acc:0.970]
Epoch [66/120    avg_loss:0.022, val_acc:0.974]
Epoch [67/120    avg_loss:0.010, val_acc:0.982]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.010, val_acc:0.986]
Epoch [70/120    avg_loss:0.007, val_acc:0.985]
Epoch [71/120    avg_loss:0.008, val_acc:0.982]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.007, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     0     0     9     0    16     0]
 [    0     1 18057     0    22     0     9     0     1     0]
 [    0     0     0  1961     0     0     0     0    75     0]
 [    0    37    10     0  2899     0     4     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    27     5     0     0  4845     0     1     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0     3     0    12    54     0     0     0  3499     3]
 [    0     0     0     0    15    43     0     0     0   861]]

Accuracy:
99.10828332489818

F1 scores:
[       nan 0.99479854 0.99806544 0.97708022 0.97249245 0.98379193
 0.99435608 0.99961225 0.97397356 0.965788  ]

Kappa:
0.9881822027237298
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3712eb7978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.961, val_acc:0.302]
Epoch [2/120    avg_loss:1.480, val_acc:0.359]
Epoch [3/120    avg_loss:1.152, val_acc:0.415]
Epoch [4/120    avg_loss:0.909, val_acc:0.678]
Epoch [5/120    avg_loss:0.736, val_acc:0.732]
Epoch [6/120    avg_loss:0.586, val_acc:0.801]
Epoch [7/120    avg_loss:0.493, val_acc:0.812]
Epoch [8/120    avg_loss:0.409, val_acc:0.834]
Epoch [9/120    avg_loss:0.361, val_acc:0.853]
Epoch [10/120    avg_loss:0.379, val_acc:0.848]
Epoch [11/120    avg_loss:0.339, val_acc:0.864]
Epoch [12/120    avg_loss:0.277, val_acc:0.879]
Epoch [13/120    avg_loss:0.238, val_acc:0.882]
Epoch [14/120    avg_loss:0.221, val_acc:0.917]
Epoch [15/120    avg_loss:0.205, val_acc:0.900]
Epoch [16/120    avg_loss:0.157, val_acc:0.940]
Epoch [17/120    avg_loss:0.155, val_acc:0.923]
Epoch [18/120    avg_loss:0.152, val_acc:0.947]
Epoch [19/120    avg_loss:0.148, val_acc:0.943]
Epoch [20/120    avg_loss:0.137, val_acc:0.924]
Epoch [21/120    avg_loss:0.193, val_acc:0.934]
Epoch [22/120    avg_loss:0.146, val_acc:0.904]
Epoch [23/120    avg_loss:0.134, val_acc:0.951]
Epoch [24/120    avg_loss:0.124, val_acc:0.958]
Epoch [25/120    avg_loss:0.108, val_acc:0.935]
Epoch [26/120    avg_loss:0.119, val_acc:0.870]
Epoch [27/120    avg_loss:0.119, val_acc:0.974]
Epoch [28/120    avg_loss:0.119, val_acc:0.854]
Epoch [29/120    avg_loss:0.104, val_acc:0.953]
Epoch [30/120    avg_loss:0.076, val_acc:0.970]
Epoch [31/120    avg_loss:0.056, val_acc:0.972]
Epoch [32/120    avg_loss:0.091, val_acc:0.975]
Epoch [33/120    avg_loss:0.060, val_acc:0.959]
Epoch [34/120    avg_loss:0.048, val_acc:0.982]
Epoch [35/120    avg_loss:0.044, val_acc:0.973]
Epoch [36/120    avg_loss:0.063, val_acc:0.959]
Epoch [37/120    avg_loss:0.048, val_acc:0.981]
Epoch [38/120    avg_loss:0.056, val_acc:0.976]
Epoch [39/120    avg_loss:0.056, val_acc:0.964]
Epoch [40/120    avg_loss:0.073, val_acc:0.949]
Epoch [41/120    avg_loss:0.059, val_acc:0.977]
Epoch [42/120    avg_loss:0.040, val_acc:0.976]
Epoch [43/120    avg_loss:0.055, val_acc:0.959]
Epoch [44/120    avg_loss:0.074, val_acc:0.954]
Epoch [45/120    avg_loss:0.045, val_acc:0.950]
Epoch [46/120    avg_loss:0.047, val_acc:0.927]
Epoch [47/120    avg_loss:0.044, val_acc:0.982]
Epoch [48/120    avg_loss:0.024, val_acc:0.979]
Epoch [49/120    avg_loss:0.019, val_acc:0.984]
Epoch [50/120    avg_loss:0.047, val_acc:0.971]
Epoch [51/120    avg_loss:0.027, val_acc:0.953]
Epoch [52/120    avg_loss:0.035, val_acc:0.966]
Epoch [53/120    avg_loss:0.087, val_acc:0.943]
Epoch [54/120    avg_loss:0.061, val_acc:0.967]
Epoch [55/120    avg_loss:0.043, val_acc:0.980]
Epoch [56/120    avg_loss:0.027, val_acc:0.981]
Epoch [57/120    avg_loss:0.037, val_acc:0.956]
Epoch [58/120    avg_loss:0.519, val_acc:0.877]
Epoch [59/120    avg_loss:0.156, val_acc:0.923]
Epoch [60/120    avg_loss:0.096, val_acc:0.978]
Epoch [61/120    avg_loss:0.045, val_acc:0.980]
Epoch [62/120    avg_loss:0.032, val_acc:0.967]
Epoch [63/120    avg_loss:0.026, val_acc:0.985]
Epoch [64/120    avg_loss:0.023, val_acc:0.985]
Epoch [65/120    avg_loss:0.020, val_acc:0.985]
Epoch [66/120    avg_loss:0.020, val_acc:0.984]
Epoch [67/120    avg_loss:0.020, val_acc:0.985]
Epoch [68/120    avg_loss:0.021, val_acc:0.980]
Epoch [69/120    avg_loss:0.019, val_acc:0.986]
Epoch [70/120    avg_loss:0.017, val_acc:0.984]
Epoch [71/120    avg_loss:0.020, val_acc:0.984]
Epoch [72/120    avg_loss:0.020, val_acc:0.986]
Epoch [73/120    avg_loss:0.020, val_acc:0.986]
Epoch [74/120    avg_loss:0.019, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.987]
Epoch [76/120    avg_loss:0.018, val_acc:0.988]
Epoch [77/120    avg_loss:0.015, val_acc:0.987]
Epoch [78/120    avg_loss:0.015, val_acc:0.987]
Epoch [79/120    avg_loss:0.019, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.987]
Epoch [81/120    avg_loss:0.016, val_acc:0.985]
Epoch [82/120    avg_loss:0.016, val_acc:0.988]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.014, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.016, val_acc:0.988]
Epoch [87/120    avg_loss:0.013, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.988]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.015, val_acc:0.988]
Epoch [91/120    avg_loss:0.015, val_acc:0.988]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.015, val_acc:0.988]
Epoch [94/120    avg_loss:0.016, val_acc:0.987]
Epoch [95/120    avg_loss:0.015, val_acc:0.989]
Epoch [96/120    avg_loss:0.018, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.989]
Epoch [98/120    avg_loss:0.012, val_acc:0.989]
Epoch [99/120    avg_loss:0.013, val_acc:0.989]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.013, val_acc:0.988]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.017, val_acc:0.987]
Epoch [104/120    avg_loss:0.013, val_acc:0.987]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.012, val_acc:0.989]
Epoch [107/120    avg_loss:0.016, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.989]
Epoch [109/120    avg_loss:0.013, val_acc:0.989]
Epoch [110/120    avg_loss:0.012, val_acc:0.989]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.015, val_acc:0.989]
Epoch [113/120    avg_loss:0.011, val_acc:0.989]
Epoch [114/120    avg_loss:0.013, val_acc:0.989]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.012, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.989]
Epoch [118/120    avg_loss:0.011, val_acc:0.989]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     1     1     0     1     0    35     1]
 [    0     0 17989     0    56     0    39     0     6     0]
 [    0     0     0  1991     1     0     0     0    37     7]
 [    0    25    16     0  2920     0     3     0     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     5     0     0  4867     0     3     0]
 [    0     9     0     0     0     0     0  1274     0     7]
 [    0     5     0    20    70     0     0     0  3469     7]
 [    0     0     0     0    13    22     0     0     0   884]]

Accuracy:
99.03357192779505

F1 scores:
[       nan 0.99393657 0.99667572 0.98248211 0.96800928 0.99164134
 0.99448304 0.99375975 0.97320802 0.96876712]

Kappa:
0.9872055857258744
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d874096d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.943, val_acc:0.282]
Epoch [2/120    avg_loss:1.454, val_acc:0.599]
Epoch [3/120    avg_loss:1.113, val_acc:0.661]
Epoch [4/120    avg_loss:0.864, val_acc:0.583]
Epoch [5/120    avg_loss:0.708, val_acc:0.714]
Epoch [6/120    avg_loss:0.552, val_acc:0.716]
Epoch [7/120    avg_loss:0.466, val_acc:0.753]
Epoch [8/120    avg_loss:0.448, val_acc:0.783]
Epoch [9/120    avg_loss:0.426, val_acc:0.807]
Epoch [10/120    avg_loss:0.339, val_acc:0.806]
Epoch [11/120    avg_loss:0.286, val_acc:0.894]
Epoch [12/120    avg_loss:0.234, val_acc:0.905]
Epoch [13/120    avg_loss:0.247, val_acc:0.875]
Epoch [14/120    avg_loss:0.252, val_acc:0.891]
Epoch [15/120    avg_loss:0.201, val_acc:0.904]
Epoch [16/120    avg_loss:0.159, val_acc:0.881]
Epoch [17/120    avg_loss:0.188, val_acc:0.939]
Epoch [18/120    avg_loss:0.160, val_acc:0.938]
Epoch [19/120    avg_loss:0.142, val_acc:0.933]
Epoch [20/120    avg_loss:0.149, val_acc:0.860]
Epoch [21/120    avg_loss:0.140, val_acc:0.928]
Epoch [22/120    avg_loss:0.177, val_acc:0.926]
Epoch [23/120    avg_loss:0.100, val_acc:0.949]
Epoch [24/120    avg_loss:0.102, val_acc:0.959]
Epoch [25/120    avg_loss:0.102, val_acc:0.943]
Epoch [26/120    avg_loss:0.086, val_acc:0.915]
Epoch [27/120    avg_loss:0.094, val_acc:0.900]
Epoch [28/120    avg_loss:0.079, val_acc:0.959]
Epoch [29/120    avg_loss:0.050, val_acc:0.974]
Epoch [30/120    avg_loss:0.069, val_acc:0.959]
Epoch [31/120    avg_loss:0.055, val_acc:0.972]
Epoch [32/120    avg_loss:0.054, val_acc:0.973]
Epoch [33/120    avg_loss:0.031, val_acc:0.973]
Epoch [34/120    avg_loss:0.043, val_acc:0.940]
Epoch [35/120    avg_loss:0.046, val_acc:0.978]
Epoch [36/120    avg_loss:0.056, val_acc:0.975]
Epoch [37/120    avg_loss:0.032, val_acc:0.956]
Epoch [38/120    avg_loss:0.044, val_acc:0.974]
Epoch [39/120    avg_loss:0.044, val_acc:0.977]
Epoch [40/120    avg_loss:0.035, val_acc:0.979]
Epoch [41/120    avg_loss:0.029, val_acc:0.979]
Epoch [42/120    avg_loss:0.023, val_acc:0.984]
Epoch [43/120    avg_loss:0.019, val_acc:0.977]
Epoch [44/120    avg_loss:0.021, val_acc:0.974]
Epoch [45/120    avg_loss:0.026, val_acc:0.985]
Epoch [46/120    avg_loss:0.024, val_acc:0.964]
Epoch [47/120    avg_loss:0.054, val_acc:0.979]
Epoch [48/120    avg_loss:0.019, val_acc:0.980]
Epoch [49/120    avg_loss:0.019, val_acc:0.984]
Epoch [50/120    avg_loss:0.035, val_acc:0.973]
Epoch [51/120    avg_loss:0.023, val_acc:0.970]
Epoch [52/120    avg_loss:0.018, val_acc:0.984]
Epoch [53/120    avg_loss:0.014, val_acc:0.981]
Epoch [54/120    avg_loss:0.017, val_acc:0.984]
Epoch [55/120    avg_loss:0.014, val_acc:0.960]
Epoch [56/120    avg_loss:0.015, val_acc:0.985]
Epoch [57/120    avg_loss:0.006, val_acc:0.990]
Epoch [58/120    avg_loss:0.016, val_acc:0.984]
Epoch [59/120    avg_loss:0.015, val_acc:0.986]
Epoch [60/120    avg_loss:0.012, val_acc:0.990]
Epoch [61/120    avg_loss:0.014, val_acc:0.983]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.013, val_acc:0.981]
Epoch [64/120    avg_loss:0.010, val_acc:0.984]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.017, val_acc:0.987]
Epoch [67/120    avg_loss:0.012, val_acc:0.985]
Epoch [68/120    avg_loss:0.010, val_acc:0.987]
Epoch [69/120    avg_loss:0.007, val_acc:0.989]
Epoch [70/120    avg_loss:0.015, val_acc:0.973]
Epoch [71/120    avg_loss:0.019, val_acc:0.989]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.005, val_acc:0.991]
Epoch [74/120    avg_loss:0.041, val_acc:0.975]
Epoch [75/120    avg_loss:0.034, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.983]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.991]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.014, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.017, val_acc:0.954]
Epoch [86/120    avg_loss:0.013, val_acc:0.990]
Epoch [87/120    avg_loss:0.016, val_acc:0.980]
Epoch [88/120    avg_loss:0.025, val_acc:0.955]
Epoch [89/120    avg_loss:0.095, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.970]
Epoch [91/120    avg_loss:0.056, val_acc:0.944]
Epoch [92/120    avg_loss:0.102, val_acc:0.962]
Epoch [93/120    avg_loss:0.051, val_acc:0.970]
Epoch [94/120    avg_loss:0.062, val_acc:0.954]
Epoch [95/120    avg_loss:0.028, val_acc:0.980]
Epoch [96/120    avg_loss:0.021, val_acc:0.984]
Epoch [97/120    avg_loss:0.018, val_acc:0.983]
Epoch [98/120    avg_loss:0.021, val_acc:0.982]
Epoch [99/120    avg_loss:0.020, val_acc:0.981]
Epoch [100/120    avg_loss:0.017, val_acc:0.985]
Epoch [101/120    avg_loss:0.020, val_acc:0.985]
Epoch [102/120    avg_loss:0.018, val_acc:0.985]
Epoch [103/120    avg_loss:0.022, val_acc:0.985]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.016, val_acc:0.985]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.014, val_acc:0.985]
Epoch [108/120    avg_loss:0.015, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.021, val_acc:0.985]
Epoch [111/120    avg_loss:0.016, val_acc:0.985]
Epoch [112/120    avg_loss:0.012, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.985]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.014, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.985]
Epoch [117/120    avg_loss:0.015, val_acc:0.985]
Epoch [118/120    avg_loss:0.015, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6231     0    36     0     0    28    18   110     9]
 [    0     3 17985     0    40     0    41     0    21     0]
 [    0     2     0  1956     0     0     0     0    63    15]
 [    0    17     4     0  2908     0    24     0    17     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     3     0     0  4873     0     0     0]
 [    0     5     0     0     0     0     1  1282     0     2]
 [    0    22     0    12    44     0     2     0  3491     0]
 [    0     0     0     0    13    48     0     0     0   858]]

Accuracy:
98.54433277902297

F1 scores:
[       nan 0.98033354 0.99692359 0.96759832 0.97306341 0.98194131
 0.98974307 0.98996139 0.959989   0.95069252]

Kappa:
0.9807378090958003
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9924679978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.942, val_acc:0.347]
Epoch [2/120    avg_loss:1.446, val_acc:0.666]
Epoch [3/120    avg_loss:1.158, val_acc:0.737]
Epoch [4/120    avg_loss:0.921, val_acc:0.774]
Epoch [5/120    avg_loss:0.690, val_acc:0.812]
Epoch [6/120    avg_loss:0.556, val_acc:0.814]
Epoch [7/120    avg_loss:0.446, val_acc:0.829]
Epoch [8/120    avg_loss:0.362, val_acc:0.836]
Epoch [9/120    avg_loss:0.380, val_acc:0.840]
Epoch [10/120    avg_loss:0.303, val_acc:0.787]
Epoch [11/120    avg_loss:0.308, val_acc:0.787]
Epoch [12/120    avg_loss:0.254, val_acc:0.862]
Epoch [13/120    avg_loss:0.205, val_acc:0.913]
Epoch [14/120    avg_loss:0.232, val_acc:0.907]
Epoch [15/120    avg_loss:0.176, val_acc:0.916]
Epoch [16/120    avg_loss:0.126, val_acc:0.897]
Epoch [17/120    avg_loss:0.134, val_acc:0.918]
Epoch [18/120    avg_loss:0.136, val_acc:0.951]
Epoch [19/120    avg_loss:0.144, val_acc:0.917]
Epoch [20/120    avg_loss:0.136, val_acc:0.910]
Epoch [21/120    avg_loss:0.095, val_acc:0.947]
Epoch [22/120    avg_loss:0.107, val_acc:0.935]
Epoch [23/120    avg_loss:0.110, val_acc:0.916]
Epoch [24/120    avg_loss:0.106, val_acc:0.923]
Epoch [25/120    avg_loss:0.108, val_acc:0.941]
Epoch [26/120    avg_loss:0.083, val_acc:0.949]
Epoch [27/120    avg_loss:0.064, val_acc:0.946]
Epoch [28/120    avg_loss:0.079, val_acc:0.933]
Epoch [29/120    avg_loss:0.063, val_acc:0.954]
Epoch [30/120    avg_loss:0.041, val_acc:0.963]
Epoch [31/120    avg_loss:0.390, val_acc:0.818]
Epoch [32/120    avg_loss:0.207, val_acc:0.932]
Epoch [33/120    avg_loss:0.149, val_acc:0.932]
Epoch [34/120    avg_loss:0.089, val_acc:0.960]
Epoch [35/120    avg_loss:0.112, val_acc:0.937]
Epoch [36/120    avg_loss:0.072, val_acc:0.958]
Epoch [37/120    avg_loss:0.079, val_acc:0.952]
Epoch [38/120    avg_loss:0.047, val_acc:0.969]
Epoch [39/120    avg_loss:0.107, val_acc:0.926]
Epoch [40/120    avg_loss:0.054, val_acc:0.966]
Epoch [41/120    avg_loss:0.065, val_acc:0.954]
Epoch [42/120    avg_loss:0.068, val_acc:0.958]
Epoch [43/120    avg_loss:0.038, val_acc:0.971]
Epoch [44/120    avg_loss:0.045, val_acc:0.933]
Epoch [45/120    avg_loss:0.044, val_acc:0.976]
Epoch [46/120    avg_loss:0.055, val_acc:0.955]
Epoch [47/120    avg_loss:0.043, val_acc:0.932]
Epoch [48/120    avg_loss:0.082, val_acc:0.751]
Epoch [49/120    avg_loss:0.450, val_acc:0.876]
Epoch [50/120    avg_loss:0.255, val_acc:0.918]
Epoch [51/120    avg_loss:0.207, val_acc:0.895]
Epoch [52/120    avg_loss:0.165, val_acc:0.916]
Epoch [53/120    avg_loss:0.101, val_acc:0.918]
Epoch [54/120    avg_loss:0.085, val_acc:0.932]
Epoch [55/120    avg_loss:0.051, val_acc:0.947]
Epoch [56/120    avg_loss:0.057, val_acc:0.944]
Epoch [57/120    avg_loss:0.052, val_acc:0.965]
Epoch [58/120    avg_loss:0.040, val_acc:0.957]
Epoch [59/120    avg_loss:0.027, val_acc:0.970]
Epoch [60/120    avg_loss:0.029, val_acc:0.959]
Epoch [61/120    avg_loss:0.023, val_acc:0.968]
Epoch [62/120    avg_loss:0.024, val_acc:0.973]
Epoch [63/120    avg_loss:0.026, val_acc:0.967]
Epoch [64/120    avg_loss:0.020, val_acc:0.971]
Epoch [65/120    avg_loss:0.018, val_acc:0.976]
Epoch [66/120    avg_loss:0.030, val_acc:0.973]
Epoch [67/120    avg_loss:0.021, val_acc:0.971]
Epoch [68/120    avg_loss:0.019, val_acc:0.970]
Epoch [69/120    avg_loss:0.023, val_acc:0.971]
Epoch [70/120    avg_loss:0.028, val_acc:0.959]
Epoch [71/120    avg_loss:0.021, val_acc:0.971]
Epoch [72/120    avg_loss:0.016, val_acc:0.973]
Epoch [73/120    avg_loss:0.017, val_acc:0.972]
Epoch [74/120    avg_loss:0.023, val_acc:0.971]
Epoch [75/120    avg_loss:0.021, val_acc:0.973]
Epoch [76/120    avg_loss:0.017, val_acc:0.974]
Epoch [77/120    avg_loss:0.020, val_acc:0.970]
Epoch [78/120    avg_loss:0.016, val_acc:0.973]
Epoch [79/120    avg_loss:0.022, val_acc:0.972]
Epoch [80/120    avg_loss:0.016, val_acc:0.972]
Epoch [81/120    avg_loss:0.022, val_acc:0.972]
Epoch [82/120    avg_loss:0.016, val_acc:0.973]
Epoch [83/120    avg_loss:0.018, val_acc:0.973]
Epoch [84/120    avg_loss:0.015, val_acc:0.972]
Epoch [85/120    avg_loss:0.014, val_acc:0.972]
Epoch [86/120    avg_loss:0.018, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.973]
Epoch [88/120    avg_loss:0.021, val_acc:0.973]
Epoch [89/120    avg_loss:0.017, val_acc:0.972]
Epoch [90/120    avg_loss:0.014, val_acc:0.973]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.025, val_acc:0.974]
Epoch [93/120    avg_loss:0.017, val_acc:0.974]
Epoch [94/120    avg_loss:0.018, val_acc:0.974]
Epoch [95/120    avg_loss:0.015, val_acc:0.974]
Epoch [96/120    avg_loss:0.024, val_acc:0.974]
Epoch [97/120    avg_loss:0.024, val_acc:0.974]
Epoch [98/120    avg_loss:0.018, val_acc:0.974]
Epoch [99/120    avg_loss:0.017, val_acc:0.974]
Epoch [100/120    avg_loss:0.016, val_acc:0.974]
Epoch [101/120    avg_loss:0.015, val_acc:0.974]
Epoch [102/120    avg_loss:0.019, val_acc:0.974]
Epoch [103/120    avg_loss:0.017, val_acc:0.974]
Epoch [104/120    avg_loss:0.014, val_acc:0.974]
Epoch [105/120    avg_loss:0.024, val_acc:0.974]
Epoch [106/120    avg_loss:0.016, val_acc:0.974]
Epoch [107/120    avg_loss:0.017, val_acc:0.974]
Epoch [108/120    avg_loss:0.017, val_acc:0.974]
Epoch [109/120    avg_loss:0.018, val_acc:0.974]
Epoch [110/120    avg_loss:0.014, val_acc:0.974]
Epoch [111/120    avg_loss:0.017, val_acc:0.974]
Epoch [112/120    avg_loss:0.017, val_acc:0.974]
Epoch [113/120    avg_loss:0.021, val_acc:0.974]
Epoch [114/120    avg_loss:0.019, val_acc:0.974]
Epoch [115/120    avg_loss:0.020, val_acc:0.974]
Epoch [116/120    avg_loss:0.015, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.974]
Epoch [118/120    avg_loss:0.019, val_acc:0.974]
Epoch [119/120    avg_loss:0.023, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6321     0     0     0     0    12     0    97     2]
 [    0     2 17899     0    53     0   136     0     0     0]
 [    0     3     0  1972     0     0     0     0    52     9]
 [    0    23     9     0  2901     0    15     0    24     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     0     0     0     0     0     3  1282     0     5]
 [    0    24     0    12    56     0     0     0  3474     5]
 [    0     0     0     0    21    49     0     0     0   849]]

Accuracy:
98.52023232834455

F1 scores:
[       nan 0.9872706  0.99438889 0.98109453 0.96651674 0.98157202
 0.98306452 0.99688958 0.96259352 0.94913359]

Kappa:
0.9804296323079046
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f178f33b9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.898, val_acc:0.372]
Epoch [2/120    avg_loss:1.427, val_acc:0.647]
Epoch [3/120    avg_loss:1.120, val_acc:0.728]
Epoch [4/120    avg_loss:0.870, val_acc:0.694]
Epoch [5/120    avg_loss:0.709, val_acc:0.757]
Epoch [6/120    avg_loss:0.601, val_acc:0.703]
Epoch [7/120    avg_loss:0.475, val_acc:0.791]
Epoch [8/120    avg_loss:0.417, val_acc:0.826]
Epoch [9/120    avg_loss:0.430, val_acc:0.835]
Epoch [10/120    avg_loss:0.390, val_acc:0.874]
Epoch [11/120    avg_loss:0.267, val_acc:0.908]
Epoch [12/120    avg_loss:0.254, val_acc:0.889]
Epoch [13/120    avg_loss:0.250, val_acc:0.896]
Epoch [14/120    avg_loss:0.204, val_acc:0.931]
Epoch [15/120    avg_loss:0.167, val_acc:0.937]
Epoch [16/120    avg_loss:0.182, val_acc:0.914]
Epoch [17/120    avg_loss:0.123, val_acc:0.917]
Epoch [18/120    avg_loss:0.122, val_acc:0.958]
Epoch [19/120    avg_loss:0.146, val_acc:0.949]
Epoch [20/120    avg_loss:0.120, val_acc:0.928]
Epoch [21/120    avg_loss:0.108, val_acc:0.953]
Epoch [22/120    avg_loss:0.118, val_acc:0.943]
Epoch [23/120    avg_loss:0.138, val_acc:0.959]
Epoch [24/120    avg_loss:0.109, val_acc:0.967]
Epoch [25/120    avg_loss:0.067, val_acc:0.964]
Epoch [26/120    avg_loss:0.079, val_acc:0.951]
Epoch [27/120    avg_loss:0.219, val_acc:0.962]
Epoch [28/120    avg_loss:0.104, val_acc:0.885]
Epoch [29/120    avg_loss:0.145, val_acc:0.958]
Epoch [30/120    avg_loss:0.085, val_acc:0.948]
Epoch [31/120    avg_loss:0.099, val_acc:0.966]
Epoch [32/120    avg_loss:0.054, val_acc:0.967]
Epoch [33/120    avg_loss:0.071, val_acc:0.961]
Epoch [34/120    avg_loss:0.233, val_acc:0.938]
Epoch [35/120    avg_loss:0.088, val_acc:0.915]
Epoch [36/120    avg_loss:0.080, val_acc:0.964]
Epoch [37/120    avg_loss:0.071, val_acc:0.953]
Epoch [38/120    avg_loss:0.074, val_acc:0.973]
Epoch [39/120    avg_loss:0.051, val_acc:0.975]
Epoch [40/120    avg_loss:0.044, val_acc:0.969]
Epoch [41/120    avg_loss:0.031, val_acc:0.973]
Epoch [42/120    avg_loss:0.032, val_acc:0.976]
Epoch [43/120    avg_loss:0.030, val_acc:0.973]
Epoch [44/120    avg_loss:0.024, val_acc:0.980]
Epoch [45/120    avg_loss:0.029, val_acc:0.979]
Epoch [46/120    avg_loss:0.019, val_acc:0.976]
Epoch [47/120    avg_loss:0.036, val_acc:0.978]
Epoch [48/120    avg_loss:0.043, val_acc:0.981]
Epoch [49/120    avg_loss:0.029, val_acc:0.979]
Epoch [50/120    avg_loss:0.022, val_acc:0.981]
Epoch [51/120    avg_loss:0.019, val_acc:0.981]
Epoch [52/120    avg_loss:0.014, val_acc:0.967]
Epoch [53/120    avg_loss:0.020, val_acc:0.976]
Epoch [54/120    avg_loss:0.018, val_acc:0.980]
Epoch [55/120    avg_loss:0.012, val_acc:0.976]
Epoch [56/120    avg_loss:0.026, val_acc:0.980]
Epoch [57/120    avg_loss:0.012, val_acc:0.979]
Epoch [58/120    avg_loss:0.012, val_acc:0.982]
Epoch [59/120    avg_loss:0.018, val_acc:0.985]
Epoch [60/120    avg_loss:0.013, val_acc:0.985]
Epoch [61/120    avg_loss:0.011, val_acc:0.981]
Epoch [62/120    avg_loss:0.011, val_acc:0.985]
Epoch [63/120    avg_loss:0.010, val_acc:0.986]
Epoch [64/120    avg_loss:0.011, val_acc:0.984]
Epoch [65/120    avg_loss:0.019, val_acc:0.985]
Epoch [66/120    avg_loss:0.011, val_acc:0.978]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.982]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.982]
Epoch [73/120    avg_loss:0.005, val_acc:0.985]
Epoch [74/120    avg_loss:0.004, val_acc:0.986]
Epoch [75/120    avg_loss:0.017, val_acc:0.983]
Epoch [76/120    avg_loss:0.021, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.991]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.004, val_acc:0.992]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.985]
Epoch [83/120    avg_loss:0.041, val_acc:0.971]
Epoch [84/120    avg_loss:0.189, val_acc:0.340]
Epoch [85/120    avg_loss:0.858, val_acc:0.640]
Epoch [86/120    avg_loss:0.570, val_acc:0.699]
Epoch [87/120    avg_loss:0.476, val_acc:0.783]
Epoch [88/120    avg_loss:0.401, val_acc:0.765]
Epoch [89/120    avg_loss:0.360, val_acc:0.813]
Epoch [90/120    avg_loss:0.312, val_acc:0.824]
Epoch [91/120    avg_loss:0.247, val_acc:0.819]
Epoch [92/120    avg_loss:0.248, val_acc:0.906]
Epoch [93/120    avg_loss:0.181, val_acc:0.902]
Epoch [94/120    avg_loss:0.154, val_acc:0.895]
Epoch [95/120    avg_loss:0.146, val_acc:0.917]
Epoch [96/120    avg_loss:0.142, val_acc:0.928]
Epoch [97/120    avg_loss:0.135, val_acc:0.933]
Epoch [98/120    avg_loss:0.152, val_acc:0.928]
Epoch [99/120    avg_loss:0.133, val_acc:0.931]
Epoch [100/120    avg_loss:0.138, val_acc:0.910]
Epoch [101/120    avg_loss:0.114, val_acc:0.950]
Epoch [102/120    avg_loss:0.122, val_acc:0.942]
Epoch [103/120    avg_loss:0.131, val_acc:0.943]
Epoch [104/120    avg_loss:0.120, val_acc:0.920]
Epoch [105/120    avg_loss:0.112, val_acc:0.946]
Epoch [106/120    avg_loss:0.111, val_acc:0.944]
Epoch [107/120    avg_loss:0.110, val_acc:0.943]
Epoch [108/120    avg_loss:0.105, val_acc:0.943]
Epoch [109/120    avg_loss:0.097, val_acc:0.943]
Epoch [110/120    avg_loss:0.107, val_acc:0.942]
Epoch [111/120    avg_loss:0.100, val_acc:0.943]
Epoch [112/120    avg_loss:0.109, val_acc:0.941]
Epoch [113/120    avg_loss:0.116, val_acc:0.943]
Epoch [114/120    avg_loss:0.097, val_acc:0.945]
Epoch [115/120    avg_loss:0.099, val_acc:0.943]
Epoch [116/120    avg_loss:0.123, val_acc:0.943]
Epoch [117/120    avg_loss:0.097, val_acc:0.942]
Epoch [118/120    avg_loss:0.097, val_acc:0.942]
Epoch [119/120    avg_loss:0.107, val_acc:0.942]
Epoch [120/120    avg_loss:0.106, val_acc:0.942]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5926     0    91    73     0    18    20   248    56]
 [    0     0 17499     0   334     0   254     0     3     0]
 [    0    13     0  1961     0     0     0     0    27    35]
 [    0    37    22     0  2848     0    19     0    45     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    15     0     0     0     0     0  1261     0    14]
 [    0    60     4    16    48     0     0     0  3443     0]
 [    0     9     9     0    16    87     0     1     0   797]]

Accuracy:
96.20417901814764

F1 scores:
[       nan 0.94876721 0.98242758 0.95565302 0.90542044 0.96774194
 0.97103613 0.98055988 0.93853073 0.87486279]

Kappa:
0.9500548004550203
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f060f0c4908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.952, val_acc:0.405]
Epoch [2/120    avg_loss:1.496, val_acc:0.548]
Epoch [3/120    avg_loss:1.158, val_acc:0.661]
Epoch [4/120    avg_loss:0.906, val_acc:0.650]
Epoch [5/120    avg_loss:0.700, val_acc:0.683]
Epoch [6/120    avg_loss:0.584, val_acc:0.749]
Epoch [7/120    avg_loss:0.462, val_acc:0.795]
Epoch [8/120    avg_loss:0.391, val_acc:0.843]
Epoch [9/120    avg_loss:0.378, val_acc:0.767]
Epoch [10/120    avg_loss:0.339, val_acc:0.816]
Epoch [11/120    avg_loss:0.289, val_acc:0.722]
Epoch [12/120    avg_loss:0.269, val_acc:0.858]
Epoch [13/120    avg_loss:0.240, val_acc:0.920]
Epoch [14/120    avg_loss:0.212, val_acc:0.909]
Epoch [15/120    avg_loss:0.193, val_acc:0.952]
Epoch [16/120    avg_loss:0.183, val_acc:0.925]
Epoch [17/120    avg_loss:0.181, val_acc:0.889]
Epoch [18/120    avg_loss:0.138, val_acc:0.884]
Epoch [19/120    avg_loss:0.143, val_acc:0.928]
Epoch [20/120    avg_loss:0.124, val_acc:0.928]
Epoch [21/120    avg_loss:0.145, val_acc:0.930]
Epoch [22/120    avg_loss:0.127, val_acc:0.972]
Epoch [23/120    avg_loss:0.112, val_acc:0.945]
Epoch [24/120    avg_loss:0.103, val_acc:0.961]
Epoch [25/120    avg_loss:0.083, val_acc:0.973]
Epoch [26/120    avg_loss:0.104, val_acc:0.963]
Epoch [27/120    avg_loss:0.071, val_acc:0.981]
Epoch [28/120    avg_loss:0.069, val_acc:0.953]
Epoch [29/120    avg_loss:0.097, val_acc:0.968]
Epoch [30/120    avg_loss:0.072, val_acc:0.964]
Epoch [31/120    avg_loss:0.100, val_acc:0.968]
Epoch [32/120    avg_loss:0.070, val_acc:0.962]
Epoch [33/120    avg_loss:0.045, val_acc:0.988]
Epoch [34/120    avg_loss:0.060, val_acc:0.922]
Epoch [35/120    avg_loss:0.067, val_acc:0.981]
Epoch [36/120    avg_loss:0.036, val_acc:0.979]
Epoch [37/120    avg_loss:0.049, val_acc:0.951]
Epoch [38/120    avg_loss:0.054, val_acc:0.987]
Epoch [39/120    avg_loss:0.067, val_acc:0.971]
Epoch [40/120    avg_loss:0.032, val_acc:0.907]
Epoch [41/120    avg_loss:0.029, val_acc:0.982]
Epoch [42/120    avg_loss:0.033, val_acc:0.984]
Epoch [43/120    avg_loss:0.051, val_acc:0.983]
Epoch [44/120    avg_loss:0.033, val_acc:0.958]
Epoch [45/120    avg_loss:0.022, val_acc:0.984]
Epoch [46/120    avg_loss:0.019, val_acc:0.990]
Epoch [47/120    avg_loss:0.015, val_acc:0.990]
Epoch [48/120    avg_loss:0.015, val_acc:0.991]
Epoch [49/120    avg_loss:0.018, val_acc:0.986]
Epoch [50/120    avg_loss:0.023, val_acc:0.986]
Epoch [51/120    avg_loss:0.012, val_acc:0.988]
Epoch [52/120    avg_loss:0.012, val_acc:0.993]
Epoch [53/120    avg_loss:0.091, val_acc:0.964]
Epoch [54/120    avg_loss:0.035, val_acc:0.989]
Epoch [55/120    avg_loss:0.017, val_acc:0.989]
Epoch [56/120    avg_loss:0.014, val_acc:0.988]
Epoch [57/120    avg_loss:0.012, val_acc:0.993]
Epoch [58/120    avg_loss:0.012, val_acc:0.993]
Epoch [59/120    avg_loss:0.011, val_acc:0.989]
Epoch [60/120    avg_loss:0.015, val_acc:0.988]
Epoch [61/120    avg_loss:0.015, val_acc:0.983]
Epoch [62/120    avg_loss:0.019, val_acc:0.988]
Epoch [63/120    avg_loss:0.012, val_acc:0.986]
Epoch [64/120    avg_loss:0.017, val_acc:0.983]
Epoch [65/120    avg_loss:0.013, val_acc:0.977]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.022, val_acc:0.987]
Epoch [68/120    avg_loss:0.015, val_acc:0.990]
Epoch [69/120    avg_loss:0.007, val_acc:0.988]
Epoch [70/120    avg_loss:0.009, val_acc:0.993]
Epoch [71/120    avg_loss:0.005, val_acc:0.993]
Epoch [72/120    avg_loss:0.007, val_acc:0.992]
Epoch [73/120    avg_loss:0.006, val_acc:0.993]
Epoch [74/120    avg_loss:0.005, val_acc:0.993]
Epoch [75/120    avg_loss:0.005, val_acc:0.993]
Epoch [76/120    avg_loss:0.005, val_acc:0.994]
Epoch [77/120    avg_loss:0.006, val_acc:0.991]
Epoch [78/120    avg_loss:0.006, val_acc:0.993]
Epoch [79/120    avg_loss:0.006, val_acc:0.992]
Epoch [80/120    avg_loss:0.006, val_acc:0.993]
Epoch [81/120    avg_loss:0.005, val_acc:0.993]
Epoch [82/120    avg_loss:0.004, val_acc:0.993]
Epoch [83/120    avg_loss:0.007, val_acc:0.993]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.005, val_acc:0.991]
Epoch [86/120    avg_loss:0.004, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.993]
Epoch [90/120    avg_loss:0.005, val_acc:0.993]
Epoch [91/120    avg_loss:0.005, val_acc:0.993]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.006, val_acc:0.993]
Epoch [95/120    avg_loss:0.004, val_acc:0.993]
Epoch [96/120    avg_loss:0.007, val_acc:0.993]
Epoch [97/120    avg_loss:0.004, val_acc:0.993]
Epoch [98/120    avg_loss:0.008, val_acc:0.992]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.004, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6322     0     0     0     0    10    42    57     1]
 [    0     0 18058     0    26     0     4     0     2     0]
 [    0     1     0  2006     0     0     0     0    23     6]
 [    0    31     2     0  2918     0     7     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4872     0     6     0]
 [    0     2     0     0     0     0     0  1285     0     3]
 [    0     7     0    42    47     0     0     0  3473     2]
 [    0     0     0     0    14    17     0     0     0   888]]

Accuracy:
99.11792350516954

F1 scores:
[       nan 0.98819852 0.99905947 0.98237023 0.97640957 0.99352874
 0.99723672 0.9820405  0.97201231 0.97636064]

Kappa:
0.9883184636135706
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff499c9e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.921, val_acc:0.253]
Epoch [2/120    avg_loss:1.458, val_acc:0.372]
Epoch [3/120    avg_loss:1.182, val_acc:0.585]
Epoch [4/120    avg_loss:0.931, val_acc:0.609]
Epoch [5/120    avg_loss:0.686, val_acc:0.679]
Epoch [6/120    avg_loss:0.584, val_acc:0.751]
Epoch [7/120    avg_loss:0.470, val_acc:0.842]
Epoch [8/120    avg_loss:0.402, val_acc:0.819]
Epoch [9/120    avg_loss:0.354, val_acc:0.896]
Epoch [10/120    avg_loss:0.285, val_acc:0.908]
Epoch [11/120    avg_loss:0.251, val_acc:0.895]
Epoch [12/120    avg_loss:0.230, val_acc:0.907]
Epoch [13/120    avg_loss:0.226, val_acc:0.851]
Epoch [14/120    avg_loss:0.182, val_acc:0.924]
Epoch [15/120    avg_loss:0.156, val_acc:0.949]
Epoch [16/120    avg_loss:0.160, val_acc:0.881]
Epoch [17/120    avg_loss:0.176, val_acc:0.937]
Epoch [18/120    avg_loss:0.135, val_acc:0.899]
Epoch [19/120    avg_loss:0.129, val_acc:0.902]
Epoch [20/120    avg_loss:0.119, val_acc:0.931]
Epoch [21/120    avg_loss:0.086, val_acc:0.966]
Epoch [22/120    avg_loss:0.061, val_acc:0.955]
Epoch [23/120    avg_loss:0.067, val_acc:0.962]
Epoch [24/120    avg_loss:0.083, val_acc:0.915]
Epoch [25/120    avg_loss:0.083, val_acc:0.961]
Epoch [26/120    avg_loss:0.086, val_acc:0.948]
Epoch [27/120    avg_loss:0.094, val_acc:0.968]
Epoch [28/120    avg_loss:0.067, val_acc:0.967]
Epoch [29/120    avg_loss:0.058, val_acc:0.955]
Epoch [30/120    avg_loss:0.049, val_acc:0.964]
Epoch [31/120    avg_loss:0.048, val_acc:0.943]
Epoch [32/120    avg_loss:0.065, val_acc:0.964]
Epoch [33/120    avg_loss:0.126, val_acc:0.945]
Epoch [34/120    avg_loss:0.063, val_acc:0.974]
Epoch [35/120    avg_loss:0.046, val_acc:0.974]
Epoch [36/120    avg_loss:0.037, val_acc:0.962]
Epoch [37/120    avg_loss:0.104, val_acc:0.954]
Epoch [38/120    avg_loss:0.054, val_acc:0.971]
Epoch [39/120    avg_loss:0.030, val_acc:0.966]
Epoch [40/120    avg_loss:0.045, val_acc:0.950]
Epoch [41/120    avg_loss:0.036, val_acc:0.973]
Epoch [42/120    avg_loss:0.040, val_acc:0.981]
Epoch [43/120    avg_loss:0.024, val_acc:0.975]
Epoch [44/120    avg_loss:0.027, val_acc:0.983]
Epoch [45/120    avg_loss:0.045, val_acc:0.965]
Epoch [46/120    avg_loss:0.029, val_acc:0.987]
Epoch [47/120    avg_loss:0.015, val_acc:0.985]
Epoch [48/120    avg_loss:0.018, val_acc:0.985]
Epoch [49/120    avg_loss:0.011, val_acc:0.985]
Epoch [50/120    avg_loss:0.013, val_acc:0.985]
Epoch [51/120    avg_loss:0.018, val_acc:0.977]
Epoch [52/120    avg_loss:0.016, val_acc:0.983]
Epoch [53/120    avg_loss:0.024, val_acc:0.986]
Epoch [54/120    avg_loss:0.016, val_acc:0.988]
Epoch [55/120    avg_loss:0.013, val_acc:0.946]
Epoch [56/120    avg_loss:0.015, val_acc:0.989]
Epoch [57/120    avg_loss:0.017, val_acc:0.980]
Epoch [58/120    avg_loss:0.044, val_acc:0.905]
Epoch [59/120    avg_loss:0.029, val_acc:0.985]
Epoch [60/120    avg_loss:0.014, val_acc:0.990]
Epoch [61/120    avg_loss:0.013, val_acc:0.987]
Epoch [62/120    avg_loss:0.022, val_acc:0.962]
Epoch [63/120    avg_loss:0.017, val_acc:0.982]
Epoch [64/120    avg_loss:0.017, val_acc:0.984]
Epoch [65/120    avg_loss:0.016, val_acc:0.987]
Epoch [66/120    avg_loss:0.034, val_acc:0.978]
Epoch [67/120    avg_loss:0.025, val_acc:0.984]
Epoch [68/120    avg_loss:0.015, val_acc:0.979]
Epoch [69/120    avg_loss:0.021, val_acc:0.955]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.019, val_acc:0.984]
Epoch [72/120    avg_loss:0.012, val_acc:0.990]
Epoch [73/120    avg_loss:0.013, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.991]
Epoch [75/120    avg_loss:0.012, val_acc:0.987]
Epoch [76/120    avg_loss:0.007, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.990]
Epoch [79/120    avg_loss:0.017, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.005, val_acc:0.991]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.006, val_acc:0.993]
Epoch [86/120    avg_loss:0.017, val_acc:0.989]
Epoch [87/120    avg_loss:0.010, val_acc:0.994]
Epoch [88/120    avg_loss:0.006, val_acc:0.993]
Epoch [89/120    avg_loss:0.010, val_acc:0.996]
Epoch [90/120    avg_loss:0.009, val_acc:0.993]
Epoch [91/120    avg_loss:0.021, val_acc:0.997]
Epoch [92/120    avg_loss:0.008, val_acc:0.994]
Epoch [93/120    avg_loss:0.007, val_acc:0.997]
Epoch [94/120    avg_loss:0.004, val_acc:0.993]
Epoch [95/120    avg_loss:0.007, val_acc:0.993]
Epoch [96/120    avg_loss:0.006, val_acc:0.996]
Epoch [97/120    avg_loss:0.010, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.993]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.979]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.994]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.993]
Epoch [106/120    avg_loss:0.005, val_acc:0.995]
Epoch [107/120    avg_loss:0.003, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.995]
Epoch [109/120    avg_loss:0.003, val_acc:0.996]
Epoch [110/120    avg_loss:0.005, val_acc:0.996]
Epoch [111/120    avg_loss:0.003, val_acc:0.996]
Epoch [112/120    avg_loss:0.003, val_acc:0.996]
Epoch [113/120    avg_loss:0.005, val_acc:0.996]
Epoch [114/120    avg_loss:0.003, val_acc:0.996]
Epoch [115/120    avg_loss:0.006, val_acc:0.996]
Epoch [116/120    avg_loss:0.005, val_acc:0.995]
Epoch [117/120    avg_loss:0.004, val_acc:0.997]
Epoch [118/120    avg_loss:0.003, val_acc:0.997]
Epoch [119/120    avg_loss:0.003, val_acc:0.997]
Epoch [120/120    avg_loss:0.004, val_acc:0.997]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     1     0    23     5     2     2]
 [    0     0 18022     0    50     0    15     0     3     0]
 [    0     8     0  1982     3     0     0     0    39     4]
 [    0    18    13     0  2911     0    11     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     4     0     0  4860     0     6     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     6     0     2    48     0     0     0  3512     3]
 [    0     0     0     0    14    33     0     0     0   872]]

Accuracy:
99.17817463186562

F1 scores:
[       nan 0.99494675 0.99753688 0.98508946 0.97049508 0.98751419
 0.99315418 0.99767802 0.98210291 0.96835092]

Kappa:
0.9891152582202541
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc55f49f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.867, val_acc:0.206]
Epoch [2/120    avg_loss:1.441, val_acc:0.443]
Epoch [3/120    avg_loss:1.089, val_acc:0.483]
Epoch [4/120    avg_loss:0.883, val_acc:0.670]
Epoch [5/120    avg_loss:0.660, val_acc:0.679]
Epoch [6/120    avg_loss:0.518, val_acc:0.774]
Epoch [7/120    avg_loss:0.424, val_acc:0.843]
Epoch [8/120    avg_loss:0.364, val_acc:0.815]
Epoch [9/120    avg_loss:0.329, val_acc:0.842]
Epoch [10/120    avg_loss:0.259, val_acc:0.857]
Epoch [11/120    avg_loss:0.287, val_acc:0.856]
Epoch [12/120    avg_loss:0.264, val_acc:0.849]
Epoch [13/120    avg_loss:0.261, val_acc:0.903]
Epoch [14/120    avg_loss:0.219, val_acc:0.906]
Epoch [15/120    avg_loss:0.178, val_acc:0.905]
Epoch [16/120    avg_loss:0.141, val_acc:0.950]
Epoch [17/120    avg_loss:0.144, val_acc:0.927]
Epoch [18/120    avg_loss:0.118, val_acc:0.932]
Epoch [19/120    avg_loss:0.146, val_acc:0.945]
Epoch [20/120    avg_loss:0.102, val_acc:0.932]
Epoch [21/120    avg_loss:0.121, val_acc:0.945]
Epoch [22/120    avg_loss:0.119, val_acc:0.959]
Epoch [23/120    avg_loss:0.091, val_acc:0.939]
Epoch [24/120    avg_loss:0.077, val_acc:0.971]
Epoch [25/120    avg_loss:0.078, val_acc:0.956]
Epoch [26/120    avg_loss:0.093, val_acc:0.963]
Epoch [27/120    avg_loss:0.088, val_acc:0.928]
Epoch [28/120    avg_loss:0.085, val_acc:0.942]
Epoch [29/120    avg_loss:0.073, val_acc:0.953]
Epoch [30/120    avg_loss:0.061, val_acc:0.963]
Epoch [31/120    avg_loss:0.048, val_acc:0.966]
Epoch [32/120    avg_loss:0.093, val_acc:0.954]
Epoch [33/120    avg_loss:0.072, val_acc:0.927]
Epoch [34/120    avg_loss:0.064, val_acc:0.958]
Epoch [35/120    avg_loss:0.062, val_acc:0.961]
Epoch [36/120    avg_loss:0.057, val_acc:0.943]
Epoch [37/120    avg_loss:0.113, val_acc:0.880]
Epoch [38/120    avg_loss:0.087, val_acc:0.945]
Epoch [39/120    avg_loss:0.071, val_acc:0.965]
Epoch [40/120    avg_loss:0.051, val_acc:0.970]
Epoch [41/120    avg_loss:0.044, val_acc:0.972]
Epoch [42/120    avg_loss:0.044, val_acc:0.975]
Epoch [43/120    avg_loss:0.038, val_acc:0.973]
Epoch [44/120    avg_loss:0.035, val_acc:0.974]
Epoch [45/120    avg_loss:0.032, val_acc:0.976]
Epoch [46/120    avg_loss:0.031, val_acc:0.975]
Epoch [47/120    avg_loss:0.034, val_acc:0.978]
Epoch [48/120    avg_loss:0.032, val_acc:0.976]
Epoch [49/120    avg_loss:0.025, val_acc:0.976]
Epoch [50/120    avg_loss:0.031, val_acc:0.975]
Epoch [51/120    avg_loss:0.031, val_acc:0.977]
Epoch [52/120    avg_loss:0.035, val_acc:0.975]
Epoch [53/120    avg_loss:0.025, val_acc:0.976]
Epoch [54/120    avg_loss:0.022, val_acc:0.976]
Epoch [55/120    avg_loss:0.020, val_acc:0.976]
Epoch [56/120    avg_loss:0.024, val_acc:0.975]
Epoch [57/120    avg_loss:0.025, val_acc:0.977]
Epoch [58/120    avg_loss:0.021, val_acc:0.976]
Epoch [59/120    avg_loss:0.023, val_acc:0.976]
Epoch [60/120    avg_loss:0.026, val_acc:0.975]
Epoch [61/120    avg_loss:0.020, val_acc:0.975]
Epoch [62/120    avg_loss:0.023, val_acc:0.975]
Epoch [63/120    avg_loss:0.030, val_acc:0.975]
Epoch [64/120    avg_loss:0.024, val_acc:0.975]
Epoch [65/120    avg_loss:0.029, val_acc:0.975]
Epoch [66/120    avg_loss:0.022, val_acc:0.975]
Epoch [67/120    avg_loss:0.024, val_acc:0.975]
Epoch [68/120    avg_loss:0.021, val_acc:0.975]
Epoch [69/120    avg_loss:0.024, val_acc:0.975]
Epoch [70/120    avg_loss:0.020, val_acc:0.975]
Epoch [71/120    avg_loss:0.024, val_acc:0.975]
Epoch [72/120    avg_loss:0.024, val_acc:0.976]
Epoch [73/120    avg_loss:0.017, val_acc:0.975]
Epoch [74/120    avg_loss:0.021, val_acc:0.975]
Epoch [75/120    avg_loss:0.022, val_acc:0.976]
Epoch [76/120    avg_loss:0.021, val_acc:0.975]
Epoch [77/120    avg_loss:0.018, val_acc:0.975]
Epoch [78/120    avg_loss:0.018, val_acc:0.975]
Epoch [79/120    avg_loss:0.019, val_acc:0.975]
Epoch [80/120    avg_loss:0.021, val_acc:0.975]
Epoch [81/120    avg_loss:0.019, val_acc:0.975]
Epoch [82/120    avg_loss:0.018, val_acc:0.975]
Epoch [83/120    avg_loss:0.022, val_acc:0.975]
Epoch [84/120    avg_loss:0.024, val_acc:0.975]
Epoch [85/120    avg_loss:0.019, val_acc:0.975]
Epoch [86/120    avg_loss:0.020, val_acc:0.975]
Epoch [87/120    avg_loss:0.020, val_acc:0.975]
Epoch [88/120    avg_loss:0.023, val_acc:0.975]
Epoch [89/120    avg_loss:0.023, val_acc:0.975]
Epoch [90/120    avg_loss:0.023, val_acc:0.975]
Epoch [91/120    avg_loss:0.028, val_acc:0.975]
Epoch [92/120    avg_loss:0.019, val_acc:0.975]
Epoch [93/120    avg_loss:0.024, val_acc:0.975]
Epoch [94/120    avg_loss:0.018, val_acc:0.975]
Epoch [95/120    avg_loss:0.018, val_acc:0.975]
Epoch [96/120    avg_loss:0.025, val_acc:0.975]
Epoch [97/120    avg_loss:0.022, val_acc:0.975]
Epoch [98/120    avg_loss:0.022, val_acc:0.975]
Epoch [99/120    avg_loss:0.022, val_acc:0.975]
Epoch [100/120    avg_loss:0.021, val_acc:0.975]
Epoch [101/120    avg_loss:0.022, val_acc:0.975]
Epoch [102/120    avg_loss:0.020, val_acc:0.975]
Epoch [103/120    avg_loss:0.022, val_acc:0.975]
Epoch [104/120    avg_loss:0.020, val_acc:0.975]
Epoch [105/120    avg_loss:0.026, val_acc:0.975]
Epoch [106/120    avg_loss:0.023, val_acc:0.975]
Epoch [107/120    avg_loss:0.021, val_acc:0.975]
Epoch [108/120    avg_loss:0.024, val_acc:0.975]
Epoch [109/120    avg_loss:0.022, val_acc:0.975]
Epoch [110/120    avg_loss:0.018, val_acc:0.975]
Epoch [111/120    avg_loss:0.022, val_acc:0.975]
Epoch [112/120    avg_loss:0.021, val_acc:0.975]
Epoch [113/120    avg_loss:0.023, val_acc:0.975]
Epoch [114/120    avg_loss:0.021, val_acc:0.975]
Epoch [115/120    avg_loss:0.019, val_acc:0.975]
Epoch [116/120    avg_loss:0.023, val_acc:0.975]
Epoch [117/120    avg_loss:0.026, val_acc:0.975]
Epoch [118/120    avg_loss:0.022, val_acc:0.975]
Epoch [119/120    avg_loss:0.020, val_acc:0.975]
Epoch [120/120    avg_loss:0.021, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6283     0     5     1     0    34     1    90    18]
 [    0     0 17944     0    87     0    43     0    16     0]
 [    0     6     0  1999     0     0     0     0    29     2]
 [    0    31    13     0  2906     0     8     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     1     0     0  4866     0     2     0]
 [    0     9     0     0     0     0     2  1276     0     3]
 [    0    10     0    32    62     0     1     0  3465     1]
 [    0     0     0     0    14    43     0     0     0   862]]

Accuracy:
98.5853035451763

F1 scores:
[       nan 0.98394801 0.99534058 0.98158605 0.96193313 0.98379193
 0.98982913 0.9941566  0.96424099 0.95512465]

Kappa:
0.9812833370366336
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20564339b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.906, val_acc:0.420]
Epoch [2/120    avg_loss:1.459, val_acc:0.639]
Epoch [3/120    avg_loss:1.135, val_acc:0.635]
Epoch [4/120    avg_loss:0.894, val_acc:0.688]
Epoch [5/120    avg_loss:0.712, val_acc:0.723]
Epoch [6/120    avg_loss:0.557, val_acc:0.769]
Epoch [7/120    avg_loss:0.480, val_acc:0.805]
Epoch [8/120    avg_loss:0.404, val_acc:0.826]
Epoch [9/120    avg_loss:0.320, val_acc:0.837]
Epoch [10/120    avg_loss:0.334, val_acc:0.807]
Epoch [11/120    avg_loss:0.270, val_acc:0.870]
Epoch [12/120    avg_loss:0.287, val_acc:0.891]
Epoch [13/120    avg_loss:0.206, val_acc:0.926]
Epoch [14/120    avg_loss:0.277, val_acc:0.858]
Epoch [15/120    avg_loss:0.215, val_acc:0.917]
Epoch [16/120    avg_loss:0.158, val_acc:0.943]
Epoch [17/120    avg_loss:0.133, val_acc:0.941]
Epoch [18/120    avg_loss:0.147, val_acc:0.943]
Epoch [19/120    avg_loss:0.172, val_acc:0.939]
Epoch [20/120    avg_loss:0.107, val_acc:0.965]
Epoch [21/120    avg_loss:0.123, val_acc:0.876]
Epoch [22/120    avg_loss:0.147, val_acc:0.938]
Epoch [23/120    avg_loss:0.149, val_acc:0.958]
Epoch [24/120    avg_loss:0.758, val_acc:0.471]
Epoch [25/120    avg_loss:0.874, val_acc:0.679]
Epoch [26/120    avg_loss:0.718, val_acc:0.708]
Epoch [27/120    avg_loss:0.605, val_acc:0.743]
Epoch [28/120    avg_loss:0.529, val_acc:0.733]
Epoch [29/120    avg_loss:0.500, val_acc:0.748]
Epoch [30/120    avg_loss:0.474, val_acc:0.806]
Epoch [31/120    avg_loss:0.458, val_acc:0.823]
Epoch [32/120    avg_loss:0.484, val_acc:0.770]
Epoch [33/120    avg_loss:0.415, val_acc:0.813]
Epoch [34/120    avg_loss:0.353, val_acc:0.841]
Epoch [35/120    avg_loss:0.318, val_acc:0.857]
Epoch [36/120    avg_loss:0.314, val_acc:0.860]
Epoch [37/120    avg_loss:0.309, val_acc:0.851]
Epoch [38/120    avg_loss:0.268, val_acc:0.860]
Epoch [39/120    avg_loss:0.277, val_acc:0.846]
Epoch [40/120    avg_loss:0.264, val_acc:0.861]
Epoch [41/120    avg_loss:0.276, val_acc:0.865]
Epoch [42/120    avg_loss:0.237, val_acc:0.864]
Epoch [43/120    avg_loss:0.256, val_acc:0.863]
Epoch [44/120    avg_loss:0.253, val_acc:0.873]
Epoch [45/120    avg_loss:0.242, val_acc:0.881]
Epoch [46/120    avg_loss:0.248, val_acc:0.868]
Epoch [47/120    avg_loss:0.236, val_acc:0.873]
Epoch [48/120    avg_loss:0.232, val_acc:0.868]
Epoch [49/120    avg_loss:0.229, val_acc:0.864]
Epoch [50/120    avg_loss:0.240, val_acc:0.873]
Epoch [51/120    avg_loss:0.222, val_acc:0.865]
Epoch [52/120    avg_loss:0.221, val_acc:0.874]
Epoch [53/120    avg_loss:0.223, val_acc:0.870]
Epoch [54/120    avg_loss:0.233, val_acc:0.879]
Epoch [55/120    avg_loss:0.227, val_acc:0.867]
Epoch [56/120    avg_loss:0.210, val_acc:0.867]
Epoch [57/120    avg_loss:0.235, val_acc:0.868]
Epoch [58/120    avg_loss:0.229, val_acc:0.878]
Epoch [59/120    avg_loss:0.231, val_acc:0.874]
Epoch [60/120    avg_loss:0.226, val_acc:0.876]
Epoch [61/120    avg_loss:0.222, val_acc:0.877]
Epoch [62/120    avg_loss:0.221, val_acc:0.876]
Epoch [63/120    avg_loss:0.218, val_acc:0.877]
Epoch [64/120    avg_loss:0.211, val_acc:0.878]
Epoch [65/120    avg_loss:0.222, val_acc:0.880]
Epoch [66/120    avg_loss:0.232, val_acc:0.879]
Epoch [67/120    avg_loss:0.232, val_acc:0.880]
Epoch [68/120    avg_loss:0.213, val_acc:0.880]
Epoch [69/120    avg_loss:0.242, val_acc:0.879]
Epoch [70/120    avg_loss:0.219, val_acc:0.879]
Epoch [71/120    avg_loss:0.216, val_acc:0.881]
Epoch [72/120    avg_loss:0.223, val_acc:0.881]
Epoch [73/120    avg_loss:0.225, val_acc:0.881]
Epoch [74/120    avg_loss:0.235, val_acc:0.881]
Epoch [75/120    avg_loss:0.239, val_acc:0.881]
Epoch [76/120    avg_loss:0.229, val_acc:0.881]
Epoch [77/120    avg_loss:0.222, val_acc:0.881]
Epoch [78/120    avg_loss:0.230, val_acc:0.881]
Epoch [79/120    avg_loss:0.221, val_acc:0.881]
Epoch [80/120    avg_loss:0.222, val_acc:0.880]
Epoch [81/120    avg_loss:0.230, val_acc:0.880]
Epoch [82/120    avg_loss:0.216, val_acc:0.880]
Epoch [83/120    avg_loss:0.211, val_acc:0.880]
Epoch [84/120    avg_loss:0.238, val_acc:0.880]
Epoch [85/120    avg_loss:0.221, val_acc:0.880]
Epoch [86/120    avg_loss:0.224, val_acc:0.880]
Epoch [87/120    avg_loss:0.232, val_acc:0.880]
Epoch [88/120    avg_loss:0.229, val_acc:0.880]
Epoch [89/120    avg_loss:0.210, val_acc:0.880]
Epoch [90/120    avg_loss:0.213, val_acc:0.880]
Epoch [91/120    avg_loss:0.213, val_acc:0.880]
Epoch [92/120    avg_loss:0.220, val_acc:0.880]
Epoch [93/120    avg_loss:0.211, val_acc:0.880]
Epoch [94/120    avg_loss:0.229, val_acc:0.880]
Epoch [95/120    avg_loss:0.225, val_acc:0.880]
Epoch [96/120    avg_loss:0.233, val_acc:0.880]
Epoch [97/120    avg_loss:0.212, val_acc:0.880]
Epoch [98/120    avg_loss:0.217, val_acc:0.880]
Epoch [99/120    avg_loss:0.229, val_acc:0.880]
Epoch [100/120    avg_loss:0.222, val_acc:0.880]
Epoch [101/120    avg_loss:0.216, val_acc:0.880]
Epoch [102/120    avg_loss:0.231, val_acc:0.880]
Epoch [103/120    avg_loss:0.223, val_acc:0.880]
Epoch [104/120    avg_loss:0.233, val_acc:0.880]
Epoch [105/120    avg_loss:0.214, val_acc:0.880]
Epoch [106/120    avg_loss:0.235, val_acc:0.880]
Epoch [107/120    avg_loss:0.223, val_acc:0.880]
Epoch [108/120    avg_loss:0.236, val_acc:0.880]
Epoch [109/120    avg_loss:0.209, val_acc:0.880]
Epoch [110/120    avg_loss:0.214, val_acc:0.880]
Epoch [111/120    avg_loss:0.222, val_acc:0.880]
Epoch [112/120    avg_loss:0.233, val_acc:0.880]
Epoch [113/120    avg_loss:0.217, val_acc:0.880]
Epoch [114/120    avg_loss:0.223, val_acc:0.880]
Epoch [115/120    avg_loss:0.224, val_acc:0.880]
Epoch [116/120    avg_loss:0.224, val_acc:0.880]
Epoch [117/120    avg_loss:0.208, val_acc:0.880]
Epoch [118/120    avg_loss:0.232, val_acc:0.880]
Epoch [119/120    avg_loss:0.225, val_acc:0.880]
Epoch [120/120    avg_loss:0.220, val_acc:0.880]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5176   134    31    84     0     3    15   622   367]
 [    0     0 18052     0    12     0    26     0     0     0]
 [    0     3     0  1853     0     0     0     0   131    49]
 [    0    24    75     0  2772     0    54     0    29    18]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0  1386     0     0     0  3457     0    35     0]
 [    0    59     0     0     0     0     1  1218    10     2]
 [    0    99    36    12    55     0    37     0  3319    13]
 [    0     6     7     3    13    44     0     0     1   845]]

Accuracy:
91.57448244282168

F1 scores:
[       nan 0.87736249 0.9556379  0.94180432 0.93838863 0.98342125
 0.81764428 0.96551724 0.86006737 0.76366923]

Kappa:
0.8868103375258128
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5312cd39b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.900, val_acc:0.176]
Epoch [2/120    avg_loss:1.511, val_acc:0.403]
Epoch [3/120    avg_loss:1.230, val_acc:0.583]
Epoch [4/120    avg_loss:1.002, val_acc:0.514]
Epoch [5/120    avg_loss:0.877, val_acc:0.662]
Epoch [6/120    avg_loss:0.696, val_acc:0.653]
Epoch [7/120    avg_loss:0.581, val_acc:0.696]
Epoch [8/120    avg_loss:0.505, val_acc:0.816]
Epoch [9/120    avg_loss:0.457, val_acc:0.790]
Epoch [10/120    avg_loss:0.387, val_acc:0.877]
Epoch [11/120    avg_loss:0.289, val_acc:0.902]
Epoch [12/120    avg_loss:0.273, val_acc:0.889]
Epoch [13/120    avg_loss:0.285, val_acc:0.895]
Epoch [14/120    avg_loss:0.410, val_acc:0.870]
Epoch [15/120    avg_loss:0.282, val_acc:0.914]
Epoch [16/120    avg_loss:0.192, val_acc:0.933]
Epoch [17/120    avg_loss:0.390, val_acc:0.694]
Epoch [18/120    avg_loss:0.542, val_acc:0.886]
Epoch [19/120    avg_loss:0.278, val_acc:0.890]
Epoch [20/120    avg_loss:0.220, val_acc:0.930]
Epoch [21/120    avg_loss:0.167, val_acc:0.949]
Epoch [22/120    avg_loss:0.190, val_acc:0.917]
Epoch [23/120    avg_loss:0.216, val_acc:0.946]
Epoch [24/120    avg_loss:0.154, val_acc:0.945]
Epoch [25/120    avg_loss:0.116, val_acc:0.947]
Epoch [26/120    avg_loss:0.114, val_acc:0.927]
Epoch [27/120    avg_loss:0.108, val_acc:0.954]
Epoch [28/120    avg_loss:0.096, val_acc:0.957]
Epoch [29/120    avg_loss:0.122, val_acc:0.966]
Epoch [30/120    avg_loss:0.097, val_acc:0.966]
Epoch [31/120    avg_loss:0.128, val_acc:0.955]
Epoch [32/120    avg_loss:0.082, val_acc:0.957]
Epoch [33/120    avg_loss:0.078, val_acc:0.967]
Epoch [34/120    avg_loss:0.072, val_acc:0.923]
Epoch [35/120    avg_loss:0.088, val_acc:0.968]
Epoch [36/120    avg_loss:0.098, val_acc:0.967]
Epoch [37/120    avg_loss:0.060, val_acc:0.960]
Epoch [38/120    avg_loss:0.058, val_acc:0.975]
Epoch [39/120    avg_loss:0.049, val_acc:0.973]
Epoch [40/120    avg_loss:0.051, val_acc:0.972]
Epoch [41/120    avg_loss:0.034, val_acc:0.979]
Epoch [42/120    avg_loss:0.034, val_acc:0.979]
Epoch [43/120    avg_loss:0.051, val_acc:0.956]
Epoch [44/120    avg_loss:0.035, val_acc:0.967]
Epoch [45/120    avg_loss:0.086, val_acc:0.969]
Epoch [46/120    avg_loss:0.033, val_acc:0.975]
Epoch [47/120    avg_loss:0.021, val_acc:0.979]
Epoch [48/120    avg_loss:0.034, val_acc:0.965]
Epoch [49/120    avg_loss:0.178, val_acc:0.877]
Epoch [50/120    avg_loss:0.177, val_acc:0.940]
Epoch [51/120    avg_loss:0.113, val_acc:0.926]
Epoch [52/120    avg_loss:0.106, val_acc:0.965]
Epoch [53/120    avg_loss:0.070, val_acc:0.979]
Epoch [54/120    avg_loss:0.036, val_acc:0.965]
Epoch [55/120    avg_loss:0.053, val_acc:0.967]
Epoch [56/120    avg_loss:0.028, val_acc:0.985]
Epoch [57/120    avg_loss:0.025, val_acc:0.986]
Epoch [58/120    avg_loss:0.020, val_acc:0.991]
Epoch [59/120    avg_loss:0.023, val_acc:0.991]
Epoch [60/120    avg_loss:0.018, val_acc:0.990]
Epoch [61/120    avg_loss:0.020, val_acc:0.990]
Epoch [62/120    avg_loss:0.018, val_acc:0.989]
Epoch [63/120    avg_loss:0.017, val_acc:0.989]
Epoch [64/120    avg_loss:0.016, val_acc:0.988]
Epoch [65/120    avg_loss:0.024, val_acc:0.989]
Epoch [66/120    avg_loss:0.018, val_acc:0.988]
Epoch [67/120    avg_loss:0.015, val_acc:0.987]
Epoch [68/120    avg_loss:0.016, val_acc:0.988]
Epoch [69/120    avg_loss:0.022, val_acc:0.987]
Epoch [70/120    avg_loss:0.014, val_acc:0.988]
Epoch [71/120    avg_loss:0.016, val_acc:0.990]
Epoch [72/120    avg_loss:0.017, val_acc:0.988]
Epoch [73/120    avg_loss:0.014, val_acc:0.989]
Epoch [74/120    avg_loss:0.015, val_acc:0.989]
Epoch [75/120    avg_loss:0.016, val_acc:0.989]
Epoch [76/120    avg_loss:0.017, val_acc:0.989]
Epoch [77/120    avg_loss:0.024, val_acc:0.989]
Epoch [78/120    avg_loss:0.013, val_acc:0.989]
Epoch [79/120    avg_loss:0.014, val_acc:0.989]
Epoch [80/120    avg_loss:0.018, val_acc:0.988]
Epoch [81/120    avg_loss:0.018, val_acc:0.988]
Epoch [82/120    avg_loss:0.017, val_acc:0.988]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.013, val_acc:0.988]
Epoch [85/120    avg_loss:0.018, val_acc:0.988]
Epoch [86/120    avg_loss:0.015, val_acc:0.988]
Epoch [87/120    avg_loss:0.014, val_acc:0.988]
Epoch [88/120    avg_loss:0.018, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.018, val_acc:0.988]
Epoch [92/120    avg_loss:0.019, val_acc:0.988]
Epoch [93/120    avg_loss:0.017, val_acc:0.988]
Epoch [94/120    avg_loss:0.014, val_acc:0.988]
Epoch [95/120    avg_loss:0.019, val_acc:0.988]
Epoch [96/120    avg_loss:0.018, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.016, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.988]
Epoch [101/120    avg_loss:0.016, val_acc:0.988]
Epoch [102/120    avg_loss:0.014, val_acc:0.988]
Epoch [103/120    avg_loss:0.023, val_acc:0.988]
Epoch [104/120    avg_loss:0.017, val_acc:0.988]
Epoch [105/120    avg_loss:0.012, val_acc:0.988]
Epoch [106/120    avg_loss:0.015, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.016, val_acc:0.988]
Epoch [111/120    avg_loss:0.018, val_acc:0.988]
Epoch [112/120    avg_loss:0.014, val_acc:0.988]
Epoch [113/120    avg_loss:0.017, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.014, val_acc:0.988]
Epoch [116/120    avg_loss:0.020, val_acc:0.988]
Epoch [117/120    avg_loss:0.016, val_acc:0.988]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.020, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0    10     6     0     6     0     8     0]
 [    0     0 18040     0    12     0    38     0     0     0]
 [    0     4     0  2013     2     0     0     0    14     3]
 [    0    41    17     3  2892     0     1     0    18     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     3     8     0     0  4863     0     4     0]
 [    0     0     0     0     0     0     0  1284     0     6]
 [    0     4     0     6    78     0     0     0  3474     9]
 [    0     0     0     0    17    32     0     0     0   870]]

Accuracy:
99.15407418118718

F1 scores:
[       nan 0.99386789 0.99806362 0.98773307 0.96738585 0.98750473
 0.99386879 0.997669   0.98011003 0.96238938]

Kappa:
0.9887939569475316
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd2b12d4908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.950, val_acc:0.271]
Epoch [2/120    avg_loss:1.473, val_acc:0.699]
Epoch [3/120    avg_loss:1.159, val_acc:0.703]
Epoch [4/120    avg_loss:0.926, val_acc:0.692]
Epoch [5/120    avg_loss:0.733, val_acc:0.766]
Epoch [6/120    avg_loss:0.595, val_acc:0.740]
Epoch [7/120    avg_loss:0.478, val_acc:0.818]
Epoch [8/120    avg_loss:0.432, val_acc:0.810]
Epoch [9/120    avg_loss:0.401, val_acc:0.816]
Epoch [10/120    avg_loss:0.321, val_acc:0.866]
Epoch [11/120    avg_loss:0.273, val_acc:0.868]
Epoch [12/120    avg_loss:0.235, val_acc:0.900]
Epoch [13/120    avg_loss:0.194, val_acc:0.912]
Epoch [14/120    avg_loss:0.219, val_acc:0.932]
Epoch [15/120    avg_loss:0.173, val_acc:0.878]
Epoch [16/120    avg_loss:0.200, val_acc:0.950]
Epoch [17/120    avg_loss:0.138, val_acc:0.947]
Epoch [18/120    avg_loss:0.146, val_acc:0.878]
Epoch [19/120    avg_loss:0.127, val_acc:0.905]
Epoch [20/120    avg_loss:0.123, val_acc:0.877]
Epoch [21/120    avg_loss:0.138, val_acc:0.961]
Epoch [22/120    avg_loss:0.089, val_acc:0.941]
Epoch [23/120    avg_loss:0.086, val_acc:0.955]
Epoch [24/120    avg_loss:0.072, val_acc:0.948]
Epoch [25/120    avg_loss:0.087, val_acc:0.965]
Epoch [26/120    avg_loss:0.073, val_acc:0.956]
Epoch [27/120    avg_loss:0.061, val_acc:0.961]
Epoch [28/120    avg_loss:0.057, val_acc:0.969]
Epoch [29/120    avg_loss:0.052, val_acc:0.947]
Epoch [30/120    avg_loss:0.043, val_acc:0.977]
Epoch [31/120    avg_loss:0.040, val_acc:0.968]
Epoch [32/120    avg_loss:0.031, val_acc:0.978]
Epoch [33/120    avg_loss:0.050, val_acc:0.971]
Epoch [34/120    avg_loss:0.030, val_acc:0.965]
Epoch [35/120    avg_loss:0.035, val_acc:0.967]
Epoch [36/120    avg_loss:0.029, val_acc:0.985]
Epoch [37/120    avg_loss:0.040, val_acc:0.974]
Epoch [38/120    avg_loss:0.016, val_acc:0.986]
Epoch [39/120    avg_loss:0.048, val_acc:0.971]
Epoch [40/120    avg_loss:0.050, val_acc:0.982]
Epoch [41/120    avg_loss:0.026, val_acc:0.983]
Epoch [42/120    avg_loss:0.026, val_acc:0.982]
Epoch [43/120    avg_loss:0.021, val_acc:0.973]
Epoch [44/120    avg_loss:0.025, val_acc:0.979]
Epoch [45/120    avg_loss:0.027, val_acc:0.979]
Epoch [46/120    avg_loss:0.017, val_acc:0.985]
Epoch [47/120    avg_loss:0.022, val_acc:0.960]
Epoch [48/120    avg_loss:0.012, val_acc:0.984]
Epoch [49/120    avg_loss:0.034, val_acc:0.978]
Epoch [50/120    avg_loss:0.025, val_acc:0.982]
Epoch [51/120    avg_loss:0.019, val_acc:0.988]
Epoch [52/120    avg_loss:0.014, val_acc:0.964]
Epoch [53/120    avg_loss:0.017, val_acc:0.979]
Epoch [54/120    avg_loss:0.012, val_acc:0.979]
Epoch [55/120    avg_loss:0.013, val_acc:0.986]
Epoch [56/120    avg_loss:0.010, val_acc:0.983]
Epoch [57/120    avg_loss:0.009, val_acc:0.984]
Epoch [58/120    avg_loss:0.026, val_acc:0.979]
Epoch [59/120    avg_loss:0.032, val_acc:0.991]
Epoch [60/120    avg_loss:0.009, val_acc:0.990]
Epoch [61/120    avg_loss:0.008, val_acc:0.984]
Epoch [62/120    avg_loss:0.008, val_acc:0.990]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.032, val_acc:0.956]
Epoch [65/120    avg_loss:0.021, val_acc:0.982]
Epoch [66/120    avg_loss:0.012, val_acc:0.986]
Epoch [67/120    avg_loss:0.007, val_acc:0.990]
Epoch [68/120    avg_loss:0.006, val_acc:0.990]
Epoch [69/120    avg_loss:0.011, val_acc:0.985]
Epoch [70/120    avg_loss:0.033, val_acc:0.969]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.016, val_acc:0.985]
Epoch [73/120    avg_loss:0.012, val_acc:0.983]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.987]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.989]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.989]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.006, val_acc:0.990]
Epoch [83/120    avg_loss:0.005, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.990]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.990]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.007, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0     8     2    28     0]
 [    0     7 18036     0    32     0    15     0     0     0]
 [    0     1     0  2029     1     0     0     0     2     3]
 [    0    27    18     0  2883     0     9     0    35     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     3     0     0  4875     0     0     0]
 [    0     2     0     0     0     0     0  1284     0     4]
 [    0     3     0     0    30     0     0     0  3526    12]
 [    0     0     0     0    14    63     0     0     0   842]]

Accuracy:
99.23119562335816

F1 scores:
[       nan 0.99393751 0.99800797 0.99754179 0.97201618 0.97643098
 0.9964231  0.99689441 0.98464116 0.94606742]

Kappa:
0.9898163696990014
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6a5d209b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.003, val_acc:0.259]
Epoch [2/120    avg_loss:1.582, val_acc:0.349]
Epoch [3/120    avg_loss:1.232, val_acc:0.449]
Epoch [4/120    avg_loss:0.995, val_acc:0.583]
Epoch [5/120    avg_loss:0.754, val_acc:0.641]
Epoch [6/120    avg_loss:0.633, val_acc:0.777]
Epoch [7/120    avg_loss:0.561, val_acc:0.737]
Epoch [8/120    avg_loss:0.525, val_acc:0.812]
Epoch [9/120    avg_loss:0.412, val_acc:0.844]
Epoch [10/120    avg_loss:0.370, val_acc:0.895]
Epoch [11/120    avg_loss:0.316, val_acc:0.882]
Epoch [12/120    avg_loss:0.266, val_acc:0.891]
Epoch [13/120    avg_loss:0.284, val_acc:0.886]
Epoch [14/120    avg_loss:0.218, val_acc:0.902]
Epoch [15/120    avg_loss:0.189, val_acc:0.930]
Epoch [16/120    avg_loss:0.183, val_acc:0.907]
Epoch [17/120    avg_loss:0.153, val_acc:0.895]
Epoch [18/120    avg_loss:0.157, val_acc:0.943]
Epoch [19/120    avg_loss:0.173, val_acc:0.939]
Epoch [20/120    avg_loss:0.110, val_acc:0.943]
Epoch [21/120    avg_loss:0.156, val_acc:0.920]
Epoch [22/120    avg_loss:0.152, val_acc:0.932]
Epoch [23/120    avg_loss:0.129, val_acc:0.942]
Epoch [24/120    avg_loss:0.104, val_acc:0.926]
Epoch [25/120    avg_loss:0.097, val_acc:0.949]
Epoch [26/120    avg_loss:0.121, val_acc:0.938]
Epoch [27/120    avg_loss:0.094, val_acc:0.943]
Epoch [28/120    avg_loss:0.077, val_acc:0.960]
Epoch [29/120    avg_loss:0.093, val_acc:0.945]
Epoch [30/120    avg_loss:0.095, val_acc:0.932]
Epoch [31/120    avg_loss:0.108, val_acc:0.914]
Epoch [32/120    avg_loss:0.121, val_acc:0.950]
Epoch [33/120    avg_loss:0.078, val_acc:0.935]
Epoch [34/120    avg_loss:0.064, val_acc:0.948]
Epoch [35/120    avg_loss:0.052, val_acc:0.970]
Epoch [36/120    avg_loss:0.043, val_acc:0.968]
Epoch [37/120    avg_loss:0.045, val_acc:0.959]
Epoch [38/120    avg_loss:0.043, val_acc:0.963]
Epoch [39/120    avg_loss:0.052, val_acc:0.968]
Epoch [40/120    avg_loss:0.037, val_acc:0.958]
Epoch [41/120    avg_loss:0.032, val_acc:0.962]
Epoch [42/120    avg_loss:0.037, val_acc:0.947]
Epoch [43/120    avg_loss:0.044, val_acc:0.974]
Epoch [44/120    avg_loss:0.034, val_acc:0.956]
Epoch [45/120    avg_loss:0.036, val_acc:0.952]
Epoch [46/120    avg_loss:0.064, val_acc:0.948]
Epoch [47/120    avg_loss:0.029, val_acc:0.970]
Epoch [48/120    avg_loss:0.028, val_acc:0.960]
Epoch [49/120    avg_loss:0.025, val_acc:0.968]
Epoch [50/120    avg_loss:0.033, val_acc:0.970]
Epoch [51/120    avg_loss:0.050, val_acc:0.961]
Epoch [52/120    avg_loss:0.036, val_acc:0.956]
Epoch [53/120    avg_loss:0.034, val_acc:0.962]
Epoch [54/120    avg_loss:0.022, val_acc:0.967]
Epoch [55/120    avg_loss:0.021, val_acc:0.972]
Epoch [56/120    avg_loss:0.014, val_acc:0.972]
Epoch [57/120    avg_loss:0.015, val_acc:0.973]
Epoch [58/120    avg_loss:0.014, val_acc:0.974]
Epoch [59/120    avg_loss:0.011, val_acc:0.976]
Epoch [60/120    avg_loss:0.010, val_acc:0.976]
Epoch [61/120    avg_loss:0.012, val_acc:0.975]
Epoch [62/120    avg_loss:0.010, val_acc:0.976]
Epoch [63/120    avg_loss:0.011, val_acc:0.976]
Epoch [64/120    avg_loss:0.010, val_acc:0.979]
Epoch [65/120    avg_loss:0.012, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.979]
Epoch [67/120    avg_loss:0.009, val_acc:0.979]
Epoch [68/120    avg_loss:0.012, val_acc:0.976]
Epoch [69/120    avg_loss:0.009, val_acc:0.977]
Epoch [70/120    avg_loss:0.010, val_acc:0.979]
Epoch [71/120    avg_loss:0.010, val_acc:0.979]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.010, val_acc:0.977]
Epoch [74/120    avg_loss:0.011, val_acc:0.977]
Epoch [75/120    avg_loss:0.011, val_acc:0.979]
Epoch [76/120    avg_loss:0.012, val_acc:0.979]
Epoch [77/120    avg_loss:0.008, val_acc:0.979]
Epoch [78/120    avg_loss:0.007, val_acc:0.979]
Epoch [79/120    avg_loss:0.010, val_acc:0.979]
Epoch [80/120    avg_loss:0.012, val_acc:0.980]
Epoch [81/120    avg_loss:0.008, val_acc:0.978]
Epoch [82/120    avg_loss:0.009, val_acc:0.979]
Epoch [83/120    avg_loss:0.011, val_acc:0.980]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.009, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.008, val_acc:0.977]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.974]
Epoch [91/120    avg_loss:0.009, val_acc:0.979]
Epoch [92/120    avg_loss:0.009, val_acc:0.979]
Epoch [93/120    avg_loss:0.007, val_acc:0.979]
Epoch [94/120    avg_loss:0.008, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.009, val_acc:0.979]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.979]
Epoch [102/120    avg_loss:0.008, val_acc:0.979]
Epoch [103/120    avg_loss:0.010, val_acc:0.979]
Epoch [104/120    avg_loss:0.036, val_acc:0.979]
Epoch [105/120    avg_loss:0.008, val_acc:0.979]
Epoch [106/120    avg_loss:0.008, val_acc:0.980]
Epoch [107/120    avg_loss:0.006, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.980]
Epoch [110/120    avg_loss:0.008, val_acc:0.979]
Epoch [111/120    avg_loss:0.008, val_acc:0.979]
Epoch [112/120    avg_loss:0.007, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.008, val_acc:0.979]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.008, val_acc:0.979]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6428     0     0     1     0     0     3     0     0]
 [    0     4 17993     0    60     0    33     0     0     0]
 [    0    16     0  2011     0     0     0     0     4     5]
 [    0    39    27     0  2884     0     2     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10    14     0     0  4853     0     1     0]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0    14     0    23    47     0     0     0  3480     7]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.08900296435543

F1 scores:
[       nan 0.99404624 0.99629014 0.98481881 0.96487119 0.98901099
 0.99385624 0.99689682 0.98360656 0.96688742]

Kappa:
0.9879351613187792
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc93187978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.925, val_acc:0.471]
Epoch [2/120    avg_loss:1.458, val_acc:0.506]
Epoch [3/120    avg_loss:1.121, val_acc:0.588]
Epoch [4/120    avg_loss:0.910, val_acc:0.741]
Epoch [5/120    avg_loss:0.728, val_acc:0.701]
Epoch [6/120    avg_loss:0.545, val_acc:0.766]
Epoch [7/120    avg_loss:0.492, val_acc:0.779]
Epoch [8/120    avg_loss:0.437, val_acc:0.842]
Epoch [9/120    avg_loss:0.402, val_acc:0.842]
Epoch [10/120    avg_loss:0.276, val_acc:0.781]
Epoch [11/120    avg_loss:0.279, val_acc:0.902]
Epoch [12/120    avg_loss:0.242, val_acc:0.863]
Epoch [13/120    avg_loss:0.221, val_acc:0.899]
Epoch [14/120    avg_loss:0.201, val_acc:0.940]
Epoch [15/120    avg_loss:0.173, val_acc:0.926]
Epoch [16/120    avg_loss:0.183, val_acc:0.906]
Epoch [17/120    avg_loss:0.201, val_acc:0.905]
Epoch [18/120    avg_loss:0.174, val_acc:0.928]
Epoch [19/120    avg_loss:0.178, val_acc:0.930]
Epoch [20/120    avg_loss:0.141, val_acc:0.820]
Epoch [21/120    avg_loss:0.130, val_acc:0.883]
Epoch [22/120    avg_loss:0.110, val_acc:0.899]
Epoch [23/120    avg_loss:0.114, val_acc:0.952]
Epoch [24/120    avg_loss:0.087, val_acc:0.958]
Epoch [25/120    avg_loss:0.106, val_acc:0.949]
Epoch [26/120    avg_loss:0.096, val_acc:0.949]
Epoch [27/120    avg_loss:0.106, val_acc:0.898]
Epoch [28/120    avg_loss:0.076, val_acc:0.941]
Epoch [29/120    avg_loss:0.072, val_acc:0.966]
Epoch [30/120    avg_loss:0.067, val_acc:0.960]
Epoch [31/120    avg_loss:0.099, val_acc:0.966]
Epoch [32/120    avg_loss:0.097, val_acc:0.943]
Epoch [33/120    avg_loss:0.086, val_acc:0.935]
Epoch [34/120    avg_loss:0.104, val_acc:0.948]
Epoch [35/120    avg_loss:0.073, val_acc:0.968]
Epoch [36/120    avg_loss:0.047, val_acc:0.969]
Epoch [37/120    avg_loss:0.059, val_acc:0.963]
Epoch [38/120    avg_loss:0.041, val_acc:0.968]
Epoch [39/120    avg_loss:0.042, val_acc:0.979]
Epoch [40/120    avg_loss:0.031, val_acc:0.973]
Epoch [41/120    avg_loss:0.028, val_acc:0.961]
Epoch [42/120    avg_loss:0.031, val_acc:0.971]
Epoch [43/120    avg_loss:0.025, val_acc:0.979]
Epoch [44/120    avg_loss:0.037, val_acc:0.941]
Epoch [45/120    avg_loss:0.053, val_acc:0.975]
Epoch [46/120    avg_loss:0.041, val_acc:0.977]
Epoch [47/120    avg_loss:0.040, val_acc:0.946]
Epoch [48/120    avg_loss:0.041, val_acc:0.984]
Epoch [49/120    avg_loss:0.029, val_acc:0.941]
Epoch [50/120    avg_loss:0.041, val_acc:0.978]
Epoch [51/120    avg_loss:0.037, val_acc:0.977]
Epoch [52/120    avg_loss:0.023, val_acc:0.976]
Epoch [53/120    avg_loss:0.028, val_acc:0.985]
Epoch [54/120    avg_loss:0.111, val_acc:0.940]
Epoch [55/120    avg_loss:0.083, val_acc:0.976]
Epoch [56/120    avg_loss:0.031, val_acc:0.967]
Epoch [57/120    avg_loss:0.019, val_acc:0.985]
Epoch [58/120    avg_loss:0.021, val_acc:0.988]
Epoch [59/120    avg_loss:0.026, val_acc:0.974]
Epoch [60/120    avg_loss:0.031, val_acc:0.983]
Epoch [61/120    avg_loss:0.116, val_acc:0.963]
Epoch [62/120    avg_loss:0.036, val_acc:0.975]
Epoch [63/120    avg_loss:0.017, val_acc:0.985]
Epoch [64/120    avg_loss:0.020, val_acc:0.986]
Epoch [65/120    avg_loss:0.031, val_acc:0.983]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.025, val_acc:0.983]
Epoch [68/120    avg_loss:0.015, val_acc:0.979]
Epoch [69/120    avg_loss:0.014, val_acc:0.986]
Epoch [70/120    avg_loss:0.016, val_acc:0.981]
Epoch [71/120    avg_loss:0.021, val_acc:0.979]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.014, val_acc:0.987]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.990]
Epoch [78/120    avg_loss:0.008, val_acc:0.990]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.013, val_acc:0.990]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.011, val_acc:0.991]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.008, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.008, val_acc:0.990]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.991]
Epoch [91/120    avg_loss:0.008, val_acc:0.991]
Epoch [92/120    avg_loss:0.009, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.011, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.009, val_acc:0.991]
Epoch [99/120    avg_loss:0.008, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.009, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.991]
Epoch [110/120    avg_loss:0.009, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.008, val_acc:0.991]
Epoch [114/120    avg_loss:0.014, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.991]
Epoch [117/120    avg_loss:0.010, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     0     0     2     0     0     0    21     1]
 [    0     4 18033     0    29     0    24     0     0     0]
 [    0     0     0  2010     3     0     0     0    18     5]
 [    0    44    14     0  2896     0     0     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     9     1     0  4860     0     3     0]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0     1     0     0    53     0     0     0  3496    21]
 [    0     0     0     0    17    59     0     0     0   843]]

Accuracy:
99.1372038657123

F1 scores:
[       nan 0.99433626 0.99789718 0.99136868 0.96969697 0.97789434
 0.99549365 0.997669   0.98105795 0.94032348]

Kappa:
0.9885714909470442
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e425fe9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.959, val_acc:0.249]
Epoch [2/120    avg_loss:1.448, val_acc:0.403]
Epoch [3/120    avg_loss:1.139, val_acc:0.668]
Epoch [4/120    avg_loss:0.846, val_acc:0.500]
Epoch [5/120    avg_loss:0.675, val_acc:0.650]
Epoch [6/120    avg_loss:0.553, val_acc:0.679]
Epoch [7/120    avg_loss:0.450, val_acc:0.740]
Epoch [8/120    avg_loss:0.451, val_acc:0.753]
Epoch [9/120    avg_loss:0.345, val_acc:0.781]
Epoch [10/120    avg_loss:0.340, val_acc:0.791]
Epoch [11/120    avg_loss:0.313, val_acc:0.778]
Epoch [12/120    avg_loss:0.306, val_acc:0.806]
Epoch [13/120    avg_loss:0.277, val_acc:0.786]
Epoch [14/120    avg_loss:0.242, val_acc:0.852]
Epoch [15/120    avg_loss:0.226, val_acc:0.899]
Epoch [16/120    avg_loss:0.203, val_acc:0.843]
Epoch [17/120    avg_loss:0.181, val_acc:0.917]
Epoch [18/120    avg_loss:0.206, val_acc:0.923]
Epoch [19/120    avg_loss:0.144, val_acc:0.868]
Epoch [20/120    avg_loss:0.150, val_acc:0.930]
Epoch [21/120    avg_loss:0.135, val_acc:0.905]
Epoch [22/120    avg_loss:0.133, val_acc:0.950]
Epoch [23/120    avg_loss:0.106, val_acc:0.944]
Epoch [24/120    avg_loss:0.092, val_acc:0.945]
Epoch [25/120    avg_loss:0.091, val_acc:0.948]
Epoch [26/120    avg_loss:0.099, val_acc:0.963]
Epoch [27/120    avg_loss:0.125, val_acc:0.968]
Epoch [28/120    avg_loss:0.071, val_acc:0.968]
Epoch [29/120    avg_loss:0.072, val_acc:0.968]
Epoch [30/120    avg_loss:0.054, val_acc:0.969]
Epoch [31/120    avg_loss:0.077, val_acc:0.960]
Epoch [32/120    avg_loss:0.384, val_acc:0.713]
Epoch [33/120    avg_loss:0.350, val_acc:0.836]
Epoch [34/120    avg_loss:0.155, val_acc:0.945]
Epoch [35/120    avg_loss:0.107, val_acc:0.937]
Epoch [36/120    avg_loss:0.088, val_acc:0.966]
Epoch [37/120    avg_loss:0.086, val_acc:0.967]
Epoch [38/120    avg_loss:0.057, val_acc:0.971]
Epoch [39/120    avg_loss:0.082, val_acc:0.970]
Epoch [40/120    avg_loss:0.104, val_acc:0.969]
Epoch [41/120    avg_loss:0.052, val_acc:0.979]
Epoch [42/120    avg_loss:0.043, val_acc:0.983]
Epoch [43/120    avg_loss:0.035, val_acc:0.976]
Epoch [44/120    avg_loss:0.041, val_acc:0.976]
Epoch [45/120    avg_loss:0.037, val_acc:0.981]
Epoch [46/120    avg_loss:0.034, val_acc:0.986]
Epoch [47/120    avg_loss:0.053, val_acc:0.971]
Epoch [48/120    avg_loss:0.057, val_acc:0.975]
Epoch [49/120    avg_loss:0.025, val_acc:0.976]
Epoch [50/120    avg_loss:0.034, val_acc:0.981]
Epoch [51/120    avg_loss:0.028, val_acc:0.960]
Epoch [52/120    avg_loss:0.026, val_acc:0.981]
Epoch [53/120    avg_loss:0.016, val_acc:0.986]
Epoch [54/120    avg_loss:0.023, val_acc:0.984]
Epoch [55/120    avg_loss:0.017, val_acc:0.983]
Epoch [56/120    avg_loss:0.089, val_acc:0.950]
Epoch [57/120    avg_loss:0.065, val_acc:0.969]
Epoch [58/120    avg_loss:0.065, val_acc:0.966]
Epoch [59/120    avg_loss:0.053, val_acc:0.978]
Epoch [60/120    avg_loss:0.049, val_acc:0.928]
Epoch [61/120    avg_loss:0.031, val_acc:0.980]
Epoch [62/120    avg_loss:0.037, val_acc:0.965]
Epoch [63/120    avg_loss:0.092, val_acc:0.976]
Epoch [64/120    avg_loss:0.037, val_acc:0.987]
Epoch [65/120    avg_loss:0.031, val_acc:0.981]
Epoch [66/120    avg_loss:0.025, val_acc:0.983]
Epoch [67/120    avg_loss:0.018, val_acc:0.990]
Epoch [68/120    avg_loss:0.021, val_acc:0.983]
Epoch [69/120    avg_loss:0.027, val_acc:0.988]
Epoch [70/120    avg_loss:0.025, val_acc:0.978]
Epoch [71/120    avg_loss:0.021, val_acc:0.977]
Epoch [72/120    avg_loss:0.016, val_acc:0.976]
Epoch [73/120    avg_loss:0.022, val_acc:0.985]
Epoch [74/120    avg_loss:0.016, val_acc:0.989]
Epoch [75/120    avg_loss:0.017, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.045, val_acc:0.965]
Epoch [78/120    avg_loss:0.026, val_acc:0.981]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.990]
Epoch [92/120    avg_loss:0.008, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     4     0     0    10    17     2]
 [    0     0 18034     0    21     0    26     0     9     0]
 [    0     0     0  2031     1     0     0     0     3     1]
 [    0    31    11     0  2905     0     0     0    23     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4875     0     0     2]
 [    0     0     0     0     0     0     3  1281     0     6]
 [    0     0     0    26    74     0     0     0  3448    23]
 [    0     0     0     2    14    33     0     0     0   870]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.9950241  0.99811822 0.99194139 0.96978802 0.98751419
 0.99672869 0.99263851 0.97525103 0.95342466]

Kappa:
0.9889886523879051
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7238e01940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.936, val_acc:0.201]
Epoch [2/120    avg_loss:1.467, val_acc:0.330]
Epoch [3/120    avg_loss:1.198, val_acc:0.462]
Epoch [4/120    avg_loss:0.963, val_acc:0.543]
Epoch [5/120    avg_loss:0.739, val_acc:0.729]
Epoch [6/120    avg_loss:0.619, val_acc:0.746]
Epoch [7/120    avg_loss:0.524, val_acc:0.843]
Epoch [8/120    avg_loss:0.485, val_acc:0.799]
Epoch [9/120    avg_loss:0.396, val_acc:0.873]
Epoch [10/120    avg_loss:0.333, val_acc:0.777]
Epoch [11/120    avg_loss:0.292, val_acc:0.841]
Epoch [12/120    avg_loss:0.316, val_acc:0.862]
Epoch [13/120    avg_loss:0.242, val_acc:0.914]
Epoch [14/120    avg_loss:0.200, val_acc:0.926]
Epoch [15/120    avg_loss:0.188, val_acc:0.926]
Epoch [16/120    avg_loss:0.194, val_acc:0.923]
Epoch [17/120    avg_loss:0.140, val_acc:0.941]
Epoch [18/120    avg_loss:0.138, val_acc:0.891]
Epoch [19/120    avg_loss:0.138, val_acc:0.956]
Epoch [20/120    avg_loss:0.123, val_acc:0.926]
Epoch [21/120    avg_loss:0.174, val_acc:0.921]
Epoch [22/120    avg_loss:0.125, val_acc:0.898]
Epoch [23/120    avg_loss:0.107, val_acc:0.878]
Epoch [24/120    avg_loss:0.100, val_acc:0.955]
Epoch [25/120    avg_loss:0.077, val_acc:0.955]
Epoch [26/120    avg_loss:0.114, val_acc:0.900]
Epoch [27/120    avg_loss:0.084, val_acc:0.928]
Epoch [28/120    avg_loss:0.064, val_acc:0.960]
Epoch [29/120    avg_loss:0.062, val_acc:0.965]
Epoch [30/120    avg_loss:0.058, val_acc:0.970]
Epoch [31/120    avg_loss:0.047, val_acc:0.973]
Epoch [32/120    avg_loss:0.049, val_acc:0.932]
Epoch [33/120    avg_loss:0.054, val_acc:0.949]
Epoch [34/120    avg_loss:0.043, val_acc:0.976]
Epoch [35/120    avg_loss:0.038, val_acc:0.960]
Epoch [36/120    avg_loss:0.032, val_acc:0.983]
Epoch [37/120    avg_loss:0.035, val_acc:0.959]
Epoch [38/120    avg_loss:0.042, val_acc:0.975]
Epoch [39/120    avg_loss:0.053, val_acc:0.967]
Epoch [40/120    avg_loss:0.040, val_acc:0.963]
Epoch [41/120    avg_loss:0.025, val_acc:0.982]
Epoch [42/120    avg_loss:0.027, val_acc:0.979]
Epoch [43/120    avg_loss:0.042, val_acc:0.973]
Epoch [44/120    avg_loss:0.040, val_acc:0.967]
Epoch [45/120    avg_loss:0.061, val_acc:0.938]
Epoch [46/120    avg_loss:0.104, val_acc:0.933]
Epoch [47/120    avg_loss:0.067, val_acc:0.972]
Epoch [48/120    avg_loss:0.038, val_acc:0.982]
Epoch [49/120    avg_loss:0.026, val_acc:0.979]
Epoch [50/120    avg_loss:0.023, val_acc:0.983]
Epoch [51/120    avg_loss:0.014, val_acc:0.982]
Epoch [52/120    avg_loss:0.018, val_acc:0.977]
Epoch [53/120    avg_loss:0.021, val_acc:0.984]
Epoch [54/120    avg_loss:0.014, val_acc:0.986]
Epoch [55/120    avg_loss:0.016, val_acc:0.987]
Epoch [56/120    avg_loss:0.022, val_acc:0.986]
Epoch [57/120    avg_loss:0.016, val_acc:0.987]
Epoch [58/120    avg_loss:0.013, val_acc:0.987]
Epoch [59/120    avg_loss:0.017, val_acc:0.985]
Epoch [60/120    avg_loss:0.016, val_acc:0.987]
Epoch [61/120    avg_loss:0.016, val_acc:0.990]
Epoch [62/120    avg_loss:0.013, val_acc:0.989]
Epoch [63/120    avg_loss:0.013, val_acc:0.989]
Epoch [64/120    avg_loss:0.013, val_acc:0.988]
Epoch [65/120    avg_loss:0.014, val_acc:0.989]
Epoch [66/120    avg_loss:0.011, val_acc:0.988]
Epoch [67/120    avg_loss:0.015, val_acc:0.988]
Epoch [68/120    avg_loss:0.012, val_acc:0.988]
Epoch [69/120    avg_loss:0.012, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.988]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.016, val_acc:0.985]
Epoch [73/120    avg_loss:0.011, val_acc:0.987]
Epoch [74/120    avg_loss:0.014, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.986]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.014, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.011, val_acc:0.987]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.987]
Epoch [83/120    avg_loss:0.014, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.011, val_acc:0.987]
Epoch [86/120    avg_loss:0.010, val_acc:0.987]
Epoch [87/120    avg_loss:0.012, val_acc:0.987]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.011, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.987]
Epoch [91/120    avg_loss:0.011, val_acc:0.987]
Epoch [92/120    avg_loss:0.011, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.987]
Epoch [94/120    avg_loss:0.011, val_acc:0.988]
Epoch [95/120    avg_loss:0.015, val_acc:0.987]
Epoch [96/120    avg_loss:0.011, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.012, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.013, val_acc:0.988]
Epoch [120/120    avg_loss:0.014, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6324     0     0     2     0    20     4    79     3]
 [    0     1 17909     0    71     0   106     0     3     0]
 [    0     0     0  2006     2     0     0    10    16     2]
 [    0    25    18     0  2894     0     8     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0     0     0     0     0     0     5  1285     0     0]
 [    0     3     0     1    56     0     0     0  3491    20]
 [    0     0     0     0    16    44     0     1     0   858]]

Accuracy:
98.68411539295785

F1 scores:
[       nan 0.98928432 0.99447483 0.99233243 0.96258107 0.98342125
 0.98564497 0.99227799 0.97107093 0.95227525]

Kappa:
0.982596399464477
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe2cb315978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.944, val_acc:0.307]
Epoch [2/120    avg_loss:1.468, val_acc:0.388]
Epoch [3/120    avg_loss:1.172, val_acc:0.647]
Epoch [4/120    avg_loss:0.961, val_acc:0.710]
Epoch [5/120    avg_loss:0.757, val_acc:0.705]
Epoch [6/120    avg_loss:0.611, val_acc:0.822]
Epoch [7/120    avg_loss:0.464, val_acc:0.900]
Epoch [8/120    avg_loss:0.358, val_acc:0.851]
Epoch [9/120    avg_loss:0.378, val_acc:0.878]
Epoch [10/120    avg_loss:0.287, val_acc:0.919]
Epoch [11/120    avg_loss:0.298, val_acc:0.867]
Epoch [12/120    avg_loss:0.231, val_acc:0.934]
Epoch [13/120    avg_loss:0.200, val_acc:0.946]
Epoch [14/120    avg_loss:0.156, val_acc:0.910]
Epoch [15/120    avg_loss:0.140, val_acc:0.942]
Epoch [16/120    avg_loss:0.125, val_acc:0.942]
Epoch [17/120    avg_loss:0.133, val_acc:0.958]
Epoch [18/120    avg_loss:0.109, val_acc:0.957]
Epoch [19/120    avg_loss:0.096, val_acc:0.943]
Epoch [20/120    avg_loss:0.076, val_acc:0.966]
Epoch [21/120    avg_loss:0.113, val_acc:0.925]
Epoch [22/120    avg_loss:0.099, val_acc:0.965]
Epoch [23/120    avg_loss:0.087, val_acc:0.954]
Epoch [24/120    avg_loss:0.128, val_acc:0.938]
Epoch [25/120    avg_loss:0.100, val_acc:0.971]
Epoch [26/120    avg_loss:0.053, val_acc:0.967]
Epoch [27/120    avg_loss:0.060, val_acc:0.970]
Epoch [28/120    avg_loss:0.062, val_acc:0.947]
Epoch [29/120    avg_loss:0.052, val_acc:0.956]
Epoch [30/120    avg_loss:0.058, val_acc:0.950]
Epoch [31/120    avg_loss:0.060, val_acc:0.970]
Epoch [32/120    avg_loss:0.039, val_acc:0.966]
Epoch [33/120    avg_loss:0.048, val_acc:0.969]
Epoch [34/120    avg_loss:0.034, val_acc:0.976]
Epoch [35/120    avg_loss:0.048, val_acc:0.963]
Epoch [36/120    avg_loss:0.028, val_acc:0.977]
Epoch [37/120    avg_loss:0.037, val_acc:0.959]
Epoch [38/120    avg_loss:0.027, val_acc:0.976]
Epoch [39/120    avg_loss:0.022, val_acc:0.978]
Epoch [40/120    avg_loss:0.039, val_acc:0.926]
Epoch [41/120    avg_loss:0.073, val_acc:0.953]
Epoch [42/120    avg_loss:0.053, val_acc:0.970]
Epoch [43/120    avg_loss:0.038, val_acc:0.968]
Epoch [44/120    avg_loss:0.053, val_acc:0.971]
Epoch [45/120    avg_loss:0.034, val_acc:0.966]
Epoch [46/120    avg_loss:0.024, val_acc:0.978]
Epoch [47/120    avg_loss:0.014, val_acc:0.976]
Epoch [48/120    avg_loss:0.021, val_acc:0.967]
Epoch [49/120    avg_loss:0.028, val_acc:0.979]
Epoch [50/120    avg_loss:0.025, val_acc:0.981]
Epoch [51/120    avg_loss:0.033, val_acc:0.976]
Epoch [52/120    avg_loss:0.018, val_acc:0.972]
Epoch [53/120    avg_loss:0.014, val_acc:0.977]
Epoch [54/120    avg_loss:0.011, val_acc:0.985]
Epoch [55/120    avg_loss:0.010, val_acc:0.983]
Epoch [56/120    avg_loss:0.020, val_acc:0.976]
Epoch [57/120    avg_loss:0.021, val_acc:0.979]
Epoch [58/120    avg_loss:0.016, val_acc:0.976]
Epoch [59/120    avg_loss:0.013, val_acc:0.982]
Epoch [60/120    avg_loss:0.027, val_acc:0.969]
Epoch [61/120    avg_loss:0.044, val_acc:0.976]
Epoch [62/120    avg_loss:0.021, val_acc:0.980]
Epoch [63/120    avg_loss:0.020, val_acc:0.981]
Epoch [64/120    avg_loss:0.024, val_acc:0.979]
Epoch [65/120    avg_loss:0.012, val_acc:0.982]
Epoch [66/120    avg_loss:0.007, val_acc:0.981]
Epoch [67/120    avg_loss:0.015, val_acc:0.983]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.010, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.006, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.005, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.982]
Epoch [89/120    avg_loss:0.005, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.982]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.005, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.983]
Epoch [97/120    avg_loss:0.005, val_acc:0.983]
Epoch [98/120    avg_loss:0.005, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.004, val_acc:0.983]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     0     0     4    30    21     0]
 [    0     3 17973     0    52     0    51     0    11     0]
 [    0     6     0  2016     0     0     0     0    12     2]
 [    0    41     9     1  2899     0     1     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4872     0     0     0]
 [    0     0     0     0     0     0     7  1280     0     3]
 [    0     7     0     3    51     0     0     0  3503     7]
 [    0     0     0     0    15    55     0     0     0   849]]

Accuracy:
98.99019111657388

F1 scores:
[       nan 0.99129489 0.99634126 0.99408284 0.9681082  0.9793621
 0.99296851 0.98461538 0.98136994 0.95393258]

Kappa:
0.9866340251272747
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f472832c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.987, val_acc:0.219]
Epoch [2/120    avg_loss:1.539, val_acc:0.457]
Epoch [3/120    avg_loss:1.269, val_acc:0.657]
Epoch [4/120    avg_loss:1.027, val_acc:0.649]
Epoch [5/120    avg_loss:0.839, val_acc:0.629]
Epoch [6/120    avg_loss:0.668, val_acc:0.750]
Epoch [7/120    avg_loss:0.536, val_acc:0.779]
Epoch [8/120    avg_loss:0.511, val_acc:0.791]
Epoch [9/120    avg_loss:0.435, val_acc:0.778]
Epoch [10/120    avg_loss:0.375, val_acc:0.843]
Epoch [11/120    avg_loss:0.613, val_acc:0.492]
Epoch [12/120    avg_loss:1.445, val_acc:0.661]
Epoch [13/120    avg_loss:1.236, val_acc:0.377]
Epoch [14/120    avg_loss:1.104, val_acc:0.701]
Epoch [15/120    avg_loss:1.015, val_acc:0.704]
Epoch [16/120    avg_loss:0.939, val_acc:0.562]
Epoch [17/120    avg_loss:0.926, val_acc:0.732]
Epoch [18/120    avg_loss:0.831, val_acc:0.745]
Epoch [19/120    avg_loss:0.791, val_acc:0.737]
Epoch [20/120    avg_loss:0.751, val_acc:0.724]
Epoch [21/120    avg_loss:0.728, val_acc:0.737]
Epoch [22/120    avg_loss:0.710, val_acc:0.756]
Epoch [23/120    avg_loss:0.672, val_acc:0.742]
Epoch [24/120    avg_loss:0.670, val_acc:0.753]
Epoch [25/120    avg_loss:0.641, val_acc:0.765]
Epoch [26/120    avg_loss:0.663, val_acc:0.771]
Epoch [27/120    avg_loss:0.600, val_acc:0.771]
Epoch [28/120    avg_loss:0.626, val_acc:0.781]
Epoch [29/120    avg_loss:0.647, val_acc:0.775]
Epoch [30/120    avg_loss:0.637, val_acc:0.777]
Epoch [31/120    avg_loss:0.637, val_acc:0.784]
Epoch [32/120    avg_loss:0.618, val_acc:0.774]
Epoch [33/120    avg_loss:0.630, val_acc:0.778]
Epoch [34/120    avg_loss:0.647, val_acc:0.776]
Epoch [35/120    avg_loss:0.637, val_acc:0.760]
Epoch [36/120    avg_loss:0.638, val_acc:0.764]
Epoch [37/120    avg_loss:0.596, val_acc:0.769]
Epoch [38/120    avg_loss:0.597, val_acc:0.770]
Epoch [39/120    avg_loss:0.598, val_acc:0.772]
Epoch [40/120    avg_loss:0.642, val_acc:0.771]
Epoch [41/120    avg_loss:0.609, val_acc:0.774]
Epoch [42/120    avg_loss:0.604, val_acc:0.772]
Epoch [43/120    avg_loss:0.656, val_acc:0.771]
Epoch [44/120    avg_loss:0.595, val_acc:0.770]
Epoch [45/120    avg_loss:0.615, val_acc:0.770]
Epoch [46/120    avg_loss:0.622, val_acc:0.770]
Epoch [47/120    avg_loss:0.611, val_acc:0.771]
Epoch [48/120    avg_loss:0.595, val_acc:0.772]
Epoch [49/120    avg_loss:0.631, val_acc:0.771]
Epoch [50/120    avg_loss:0.607, val_acc:0.771]
Epoch [51/120    avg_loss:0.599, val_acc:0.771]
Epoch [52/120    avg_loss:0.633, val_acc:0.771]
Epoch [53/120    avg_loss:0.594, val_acc:0.771]
Epoch [54/120    avg_loss:0.593, val_acc:0.771]
Epoch [55/120    avg_loss:0.591, val_acc:0.771]
Epoch [56/120    avg_loss:0.620, val_acc:0.771]
Epoch [57/120    avg_loss:0.590, val_acc:0.772]
Epoch [58/120    avg_loss:0.623, val_acc:0.771]
Epoch [59/120    avg_loss:0.618, val_acc:0.772]
Epoch [60/120    avg_loss:0.629, val_acc:0.772]
Epoch [61/120    avg_loss:0.620, val_acc:0.771]
Epoch [62/120    avg_loss:0.626, val_acc:0.771]
Epoch [63/120    avg_loss:0.628, val_acc:0.771]
Epoch [64/120    avg_loss:0.601, val_acc:0.771]
Epoch [65/120    avg_loss:0.613, val_acc:0.771]
Epoch [66/120    avg_loss:0.592, val_acc:0.771]
Epoch [67/120    avg_loss:0.572, val_acc:0.771]
Epoch [68/120    avg_loss:0.637, val_acc:0.771]
Epoch [69/120    avg_loss:0.668, val_acc:0.771]
Epoch [70/120    avg_loss:0.614, val_acc:0.771]
Epoch [71/120    avg_loss:0.602, val_acc:0.771]
Epoch [72/120    avg_loss:0.598, val_acc:0.771]
Epoch [73/120    avg_loss:0.631, val_acc:0.771]
Epoch [74/120    avg_loss:0.604, val_acc:0.771]
Epoch [75/120    avg_loss:0.591, val_acc:0.771]
Epoch [76/120    avg_loss:0.645, val_acc:0.771]
Epoch [77/120    avg_loss:0.592, val_acc:0.771]
Epoch [78/120    avg_loss:0.596, val_acc:0.771]
Epoch [79/120    avg_loss:0.615, val_acc:0.771]
Epoch [80/120    avg_loss:0.623, val_acc:0.771]
Epoch [81/120    avg_loss:0.619, val_acc:0.771]
Epoch [82/120    avg_loss:0.587, val_acc:0.771]
Epoch [83/120    avg_loss:0.623, val_acc:0.771]
Epoch [84/120    avg_loss:0.600, val_acc:0.771]
Epoch [85/120    avg_loss:0.633, val_acc:0.771]
Epoch [86/120    avg_loss:0.625, val_acc:0.771]
Epoch [87/120    avg_loss:0.608, val_acc:0.771]
Epoch [88/120    avg_loss:0.631, val_acc:0.771]
Epoch [89/120    avg_loss:0.616, val_acc:0.771]
Epoch [90/120    avg_loss:0.603, val_acc:0.771]
Epoch [91/120    avg_loss:0.598, val_acc:0.771]
Epoch [92/120    avg_loss:0.626, val_acc:0.771]
Epoch [93/120    avg_loss:0.641, val_acc:0.771]
Epoch [94/120    avg_loss:0.593, val_acc:0.771]
Epoch [95/120    avg_loss:0.608, val_acc:0.771]
Epoch [96/120    avg_loss:0.612, val_acc:0.771]
Epoch [97/120    avg_loss:0.610, val_acc:0.771]
Epoch [98/120    avg_loss:0.620, val_acc:0.771]
Epoch [99/120    avg_loss:0.585, val_acc:0.771]
Epoch [100/120    avg_loss:0.601, val_acc:0.771]
Epoch [101/120    avg_loss:0.608, val_acc:0.771]
Epoch [102/120    avg_loss:0.635, val_acc:0.771]
Epoch [103/120    avg_loss:0.597, val_acc:0.771]
Epoch [104/120    avg_loss:0.626, val_acc:0.771]
Epoch [105/120    avg_loss:0.647, val_acc:0.771]
Epoch [106/120    avg_loss:0.612, val_acc:0.771]
Epoch [107/120    avg_loss:0.630, val_acc:0.771]
Epoch [108/120    avg_loss:0.608, val_acc:0.771]
Epoch [109/120    avg_loss:0.594, val_acc:0.771]
Epoch [110/120    avg_loss:0.605, val_acc:0.771]
Epoch [111/120    avg_loss:0.620, val_acc:0.771]
Epoch [112/120    avg_loss:0.621, val_acc:0.771]
Epoch [113/120    avg_loss:0.601, val_acc:0.771]
Epoch [114/120    avg_loss:0.603, val_acc:0.771]
Epoch [115/120    avg_loss:0.581, val_acc:0.771]
Epoch [116/120    avg_loss:0.603, val_acc:0.771]
Epoch [117/120    avg_loss:0.589, val_acc:0.771]
Epoch [118/120    avg_loss:0.585, val_acc:0.771]
Epoch [119/120    avg_loss:0.595, val_acc:0.771]
Epoch [120/120    avg_loss:0.612, val_acc:0.771]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4916     0     8   624     0    92     0   454   338]
 [    0     0 16324     0     8     0  1756     0     2     0]
 [    0    34     0  1611    15     0     0     0   300    76]
 [    0    54   535     0  1991     0   215     8   155    14]
 [    0     0     0     0     0  1301     0     0     0     4]
 [    0     0  1468   116   137     0  2947     0   210     0]
 [    0   140     0     0     4     0     0  1125     0    21]
 [    0   151     0    34    80     0    72     0  3212    22]
 [    0    34     0     1    21   140     0     1     4   718]]

Accuracy:
82.29098884149134

F1 scores:
[       nan 0.83598333 0.89650438 0.84655807 0.68045113 0.94756009
 0.59176707 0.92821782 0.81234193 0.67992424]

Kappa:
0.7652841398171485
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa02cd088d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.926, val_acc:0.193]
Epoch [2/120    avg_loss:1.505, val_acc:0.341]
Epoch [3/120    avg_loss:1.225, val_acc:0.476]
Epoch [4/120    avg_loss:0.995, val_acc:0.506]
Epoch [5/120    avg_loss:0.797, val_acc:0.575]
Epoch [6/120    avg_loss:0.662, val_acc:0.662]
Epoch [7/120    avg_loss:0.577, val_acc:0.735]
Epoch [8/120    avg_loss:0.473, val_acc:0.759]
Epoch [9/120    avg_loss:0.459, val_acc:0.781]
Epoch [10/120    avg_loss:0.381, val_acc:0.841]
Epoch [11/120    avg_loss:0.298, val_acc:0.853]
Epoch [12/120    avg_loss:0.265, val_acc:0.913]
Epoch [13/120    avg_loss:0.244, val_acc:0.830]
Epoch [14/120    avg_loss:0.222, val_acc:0.882]
Epoch [15/120    avg_loss:0.244, val_acc:0.856]
Epoch [16/120    avg_loss:0.247, val_acc:0.928]
Epoch [17/120    avg_loss:0.207, val_acc:0.896]
Epoch [18/120    avg_loss:0.216, val_acc:0.908]
Epoch [19/120    avg_loss:0.152, val_acc:0.926]
Epoch [20/120    avg_loss:0.169, val_acc:0.932]
Epoch [21/120    avg_loss:0.153, val_acc:0.887]
Epoch [22/120    avg_loss:0.149, val_acc:0.874]
Epoch [23/120    avg_loss:0.136, val_acc:0.953]
Epoch [24/120    avg_loss:0.124, val_acc:0.912]
Epoch [25/120    avg_loss:0.084, val_acc:0.968]
Epoch [26/120    avg_loss:0.093, val_acc:0.926]
Epoch [27/120    avg_loss:0.089, val_acc:0.959]
Epoch [28/120    avg_loss:0.097, val_acc:0.970]
Epoch [29/120    avg_loss:0.089, val_acc:0.963]
Epoch [30/120    avg_loss:0.077, val_acc:0.959]
Epoch [31/120    avg_loss:0.071, val_acc:0.962]
Epoch [32/120    avg_loss:0.109, val_acc:0.973]
Epoch [33/120    avg_loss:0.098, val_acc:0.887]
Epoch [34/120    avg_loss:0.099, val_acc:0.945]
Epoch [35/120    avg_loss:0.055, val_acc:0.973]
Epoch [36/120    avg_loss:0.043, val_acc:0.984]
Epoch [37/120    avg_loss:0.074, val_acc:0.960]
Epoch [38/120    avg_loss:0.055, val_acc:0.967]
Epoch [39/120    avg_loss:0.033, val_acc:0.980]
Epoch [40/120    avg_loss:0.039, val_acc:0.975]
Epoch [41/120    avg_loss:0.036, val_acc:0.952]
Epoch [42/120    avg_loss:0.245, val_acc:0.916]
Epoch [43/120    avg_loss:0.123, val_acc:0.970]
Epoch [44/120    avg_loss:0.059, val_acc:0.960]
Epoch [45/120    avg_loss:0.056, val_acc:0.982]
Epoch [46/120    avg_loss:0.040, val_acc:0.957]
Epoch [47/120    avg_loss:0.028, val_acc:0.982]
Epoch [48/120    avg_loss:0.061, val_acc:0.943]
Epoch [49/120    avg_loss:0.040, val_acc:0.971]
Epoch [50/120    avg_loss:0.036, val_acc:0.985]
Epoch [51/120    avg_loss:0.028, val_acc:0.988]
Epoch [52/120    avg_loss:0.026, val_acc:0.987]
Epoch [53/120    avg_loss:0.025, val_acc:0.985]
Epoch [54/120    avg_loss:0.024, val_acc:0.986]
Epoch [55/120    avg_loss:0.018, val_acc:0.986]
Epoch [56/120    avg_loss:0.021, val_acc:0.988]
Epoch [57/120    avg_loss:0.023, val_acc:0.989]
Epoch [58/120    avg_loss:0.021, val_acc:0.987]
Epoch [59/120    avg_loss:0.018, val_acc:0.988]
Epoch [60/120    avg_loss:0.021, val_acc:0.991]
Epoch [61/120    avg_loss:0.020, val_acc:0.991]
Epoch [62/120    avg_loss:0.022, val_acc:0.990]
Epoch [63/120    avg_loss:0.018, val_acc:0.991]
Epoch [64/120    avg_loss:0.017, val_acc:0.990]
Epoch [65/120    avg_loss:0.016, val_acc:0.992]
Epoch [66/120    avg_loss:0.018, val_acc:0.990]
Epoch [67/120    avg_loss:0.013, val_acc:0.991]
Epoch [68/120    avg_loss:0.017, val_acc:0.991]
Epoch [69/120    avg_loss:0.018, val_acc:0.991]
Epoch [70/120    avg_loss:0.017, val_acc:0.987]
Epoch [71/120    avg_loss:0.015, val_acc:0.991]
Epoch [72/120    avg_loss:0.014, val_acc:0.991]
Epoch [73/120    avg_loss:0.017, val_acc:0.991]
Epoch [74/120    avg_loss:0.016, val_acc:0.991]
Epoch [75/120    avg_loss:0.014, val_acc:0.991]
Epoch [76/120    avg_loss:0.015, val_acc:0.993]
Epoch [77/120    avg_loss:0.015, val_acc:0.990]
Epoch [78/120    avg_loss:0.016, val_acc:0.991]
Epoch [79/120    avg_loss:0.015, val_acc:0.990]
Epoch [80/120    avg_loss:0.018, val_acc:0.991]
Epoch [81/120    avg_loss:0.021, val_acc:0.988]
Epoch [82/120    avg_loss:0.012, val_acc:0.990]
Epoch [83/120    avg_loss:0.014, val_acc:0.988]
Epoch [84/120    avg_loss:0.013, val_acc:0.991]
Epoch [85/120    avg_loss:0.021, val_acc:0.989]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.015, val_acc:0.991]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.012, val_acc:0.992]
Epoch [90/120    avg_loss:0.017, val_acc:0.992]
Epoch [91/120    avg_loss:0.012, val_acc:0.992]
Epoch [92/120    avg_loss:0.013, val_acc:0.992]
Epoch [93/120    avg_loss:0.018, val_acc:0.991]
Epoch [94/120    avg_loss:0.016, val_acc:0.991]
Epoch [95/120    avg_loss:0.014, val_acc:0.991]
Epoch [96/120    avg_loss:0.017, val_acc:0.991]
Epoch [97/120    avg_loss:0.011, val_acc:0.991]
Epoch [98/120    avg_loss:0.014, val_acc:0.991]
Epoch [99/120    avg_loss:0.012, val_acc:0.991]
Epoch [100/120    avg_loss:0.018, val_acc:0.991]
Epoch [101/120    avg_loss:0.012, val_acc:0.991]
Epoch [102/120    avg_loss:0.015, val_acc:0.991]
Epoch [103/120    avg_loss:0.012, val_acc:0.991]
Epoch [104/120    avg_loss:0.016, val_acc:0.991]
Epoch [105/120    avg_loss:0.013, val_acc:0.991]
Epoch [106/120    avg_loss:0.013, val_acc:0.992]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.991]
Epoch [109/120    avg_loss:0.011, val_acc:0.992]
Epoch [110/120    avg_loss:0.012, val_acc:0.991]
Epoch [111/120    avg_loss:0.014, val_acc:0.991]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.011, val_acc:0.992]
Epoch [114/120    avg_loss:0.021, val_acc:0.991]
Epoch [115/120    avg_loss:0.012, val_acc:0.991]
Epoch [116/120    avg_loss:0.012, val_acc:0.991]
Epoch [117/120    avg_loss:0.020, val_acc:0.991]
Epoch [118/120    avg_loss:0.019, val_acc:0.991]
Epoch [119/120    avg_loss:0.030, val_acc:0.991]
Epoch [120/120    avg_loss:0.017, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     5     2     0     0     0    19     0]
 [    0     0 18003     0    50     0    36     0     1     0]
 [    0    15     0  1994     0     0     0     0    26     1]
 [    0    24    16     0  2911     0     7     0    11     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     7     0     0  4868     0     3     0]
 [    0     2     0     0     0     0     1  1285     0     2]
 [    0     5     0     0    55     0     0     0  3508     3]
 [    0     0     0     1    14    18     0     0     0   886]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99441167 0.99714753 0.98639624 0.96968688 0.99315068
 0.99448417 0.99805825 0.9827707  0.97684675]

Kappa:
0.989564519953444
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b8cddd908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.950, val_acc:0.229]
Epoch [2/120    avg_loss:1.461, val_acc:0.373]
Epoch [3/120    avg_loss:1.240, val_acc:0.604]
Epoch [4/120    avg_loss:1.058, val_acc:0.740]
Epoch [5/120    avg_loss:0.901, val_acc:0.698]
Epoch [6/120    avg_loss:0.799, val_acc:0.702]
Epoch [7/120    avg_loss:0.658, val_acc:0.757]
Epoch [8/120    avg_loss:0.541, val_acc:0.751]
Epoch [9/120    avg_loss:0.482, val_acc:0.806]
Epoch [10/120    avg_loss:0.405, val_acc:0.769]
Epoch [11/120    avg_loss:0.349, val_acc:0.830]
Epoch [12/120    avg_loss:0.307, val_acc:0.848]
Epoch [13/120    avg_loss:0.277, val_acc:0.860]
Epoch [14/120    avg_loss:0.270, val_acc:0.888]
Epoch [15/120    avg_loss:0.233, val_acc:0.911]
Epoch [16/120    avg_loss:0.186, val_acc:0.911]
Epoch [17/120    avg_loss:0.189, val_acc:0.917]
Epoch [18/120    avg_loss:0.179, val_acc:0.862]
Epoch [19/120    avg_loss:0.154, val_acc:0.930]
Epoch [20/120    avg_loss:0.121, val_acc:0.951]
Epoch [21/120    avg_loss:0.115, val_acc:0.932]
Epoch [22/120    avg_loss:0.109, val_acc:0.948]
Epoch [23/120    avg_loss:0.091, val_acc:0.952]
Epoch [24/120    avg_loss:0.074, val_acc:0.958]
Epoch [25/120    avg_loss:0.110, val_acc:0.937]
Epoch [26/120    avg_loss:0.078, val_acc:0.939]
Epoch [27/120    avg_loss:0.437, val_acc:0.838]
Epoch [28/120    avg_loss:0.238, val_acc:0.924]
Epoch [29/120    avg_loss:0.187, val_acc:0.885]
Epoch [30/120    avg_loss:0.134, val_acc:0.957]
Epoch [31/120    avg_loss:0.105, val_acc:0.957]
Epoch [32/120    avg_loss:0.072, val_acc:0.939]
Epoch [33/120    avg_loss:0.077, val_acc:0.951]
Epoch [34/120    avg_loss:0.082, val_acc:0.944]
Epoch [35/120    avg_loss:0.066, val_acc:0.968]
Epoch [36/120    avg_loss:0.051, val_acc:0.961]
Epoch [37/120    avg_loss:0.089, val_acc:0.945]
Epoch [38/120    avg_loss:0.052, val_acc:0.940]
Epoch [39/120    avg_loss:0.058, val_acc:0.963]
Epoch [40/120    avg_loss:0.046, val_acc:0.966]
Epoch [41/120    avg_loss:0.042, val_acc:0.968]
Epoch [42/120    avg_loss:0.056, val_acc:0.955]
Epoch [43/120    avg_loss:0.050, val_acc:0.964]
Epoch [44/120    avg_loss:0.036, val_acc:0.970]
Epoch [45/120    avg_loss:0.026, val_acc:0.961]
Epoch [46/120    avg_loss:0.042, val_acc:0.964]
Epoch [47/120    avg_loss:0.020, val_acc:0.971]
Epoch [48/120    avg_loss:0.058, val_acc:0.960]
Epoch [49/120    avg_loss:0.043, val_acc:0.967]
Epoch [50/120    avg_loss:0.048, val_acc:0.965]
Epoch [51/120    avg_loss:0.027, val_acc:0.969]
Epoch [52/120    avg_loss:0.026, val_acc:0.957]
Epoch [53/120    avg_loss:0.029, val_acc:0.969]
Epoch [54/120    avg_loss:0.019, val_acc:0.969]
Epoch [55/120    avg_loss:0.018, val_acc:0.966]
Epoch [56/120    avg_loss:0.015, val_acc:0.971]
Epoch [57/120    avg_loss:0.013, val_acc:0.974]
Epoch [58/120    avg_loss:0.016, val_acc:0.972]
Epoch [59/120    avg_loss:0.020, val_acc:0.972]
Epoch [60/120    avg_loss:0.017, val_acc:0.966]
Epoch [61/120    avg_loss:0.019, val_acc:0.951]
Epoch [62/120    avg_loss:0.014, val_acc:0.975]
Epoch [63/120    avg_loss:0.025, val_acc:0.968]
Epoch [64/120    avg_loss:0.028, val_acc:0.962]
Epoch [65/120    avg_loss:0.041, val_acc:0.958]
Epoch [66/120    avg_loss:0.017, val_acc:0.977]
Epoch [67/120    avg_loss:0.015, val_acc:0.970]
Epoch [68/120    avg_loss:0.010, val_acc:0.975]
Epoch [69/120    avg_loss:0.030, val_acc:0.971]
Epoch [70/120    avg_loss:0.015, val_acc:0.976]
Epoch [71/120    avg_loss:0.019, val_acc:0.973]
Epoch [72/120    avg_loss:0.012, val_acc:0.951]
Epoch [73/120    avg_loss:0.023, val_acc:0.976]
Epoch [74/120    avg_loss:0.009, val_acc:0.970]
Epoch [75/120    avg_loss:0.010, val_acc:0.972]
Epoch [76/120    avg_loss:0.008, val_acc:0.974]
Epoch [77/120    avg_loss:0.009, val_acc:0.975]
Epoch [78/120    avg_loss:0.037, val_acc:0.956]
Epoch [79/120    avg_loss:0.016, val_acc:0.977]
Epoch [80/120    avg_loss:0.012, val_acc:0.979]
Epoch [81/120    avg_loss:0.011, val_acc:0.975]
Epoch [82/120    avg_loss:0.016, val_acc:0.966]
Epoch [83/120    avg_loss:0.027, val_acc:0.977]
Epoch [84/120    avg_loss:0.009, val_acc:0.977]
Epoch [85/120    avg_loss:0.013, val_acc:0.977]
Epoch [86/120    avg_loss:0.020, val_acc:0.974]
Epoch [87/120    avg_loss:0.007, val_acc:0.975]
Epoch [88/120    avg_loss:0.006, val_acc:0.978]
Epoch [89/120    avg_loss:0.006, val_acc:0.977]
Epoch [90/120    avg_loss:0.010, val_acc:0.964]
Epoch [91/120    avg_loss:0.009, val_acc:0.980]
Epoch [92/120    avg_loss:0.004, val_acc:0.979]
Epoch [93/120    avg_loss:0.005, val_acc:0.978]
Epoch [94/120    avg_loss:0.008, val_acc:0.977]
Epoch [95/120    avg_loss:0.005, val_acc:0.978]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.005, val_acc:0.970]
Epoch [98/120    avg_loss:0.007, val_acc:0.977]
Epoch [99/120    avg_loss:0.007, val_acc:0.976]
Epoch [100/120    avg_loss:0.004, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.980]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.008, val_acc:0.972]
Epoch [104/120    avg_loss:0.005, val_acc:0.975]
Epoch [105/120    avg_loss:0.004, val_acc:0.977]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.004, val_acc:0.977]
Epoch [108/120    avg_loss:0.005, val_acc:0.977]
Epoch [109/120    avg_loss:0.005, val_acc:0.976]
Epoch [110/120    avg_loss:0.004, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.003, val_acc:0.977]
Epoch [114/120    avg_loss:0.004, val_acc:0.979]
Epoch [115/120    avg_loss:0.004, val_acc:0.979]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.005, val_acc:0.979]
Epoch [118/120    avg_loss:0.003, val_acc:0.978]
Epoch [119/120    avg_loss:0.003, val_acc:0.978]
Epoch [120/120    avg_loss:0.005, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6415     0     0     0     0     0    10     7     0]
 [    0     0 18035     0    37     0    18     0     0     0]
 [    0     5     0  1992     3     0     0     0    36     0]
 [    0    31    19     0  2895     0     7     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4870     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    17     0    40    65     0     0     0  3439    10]
 [    0     0     0     2    14    64     0     0     1   838]]

Accuracy:
99.00224134191309

F1 scores:
[       nan 0.99457364 0.9977318  0.97886978 0.96725693 0.97606582
 0.99662335 0.996139   0.9722929  0.94850028]

Kappa:
0.98678197353614
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb0edc08d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.953, val_acc:0.189]
Epoch [2/120    avg_loss:1.555, val_acc:0.375]
Epoch [3/120    avg_loss:1.318, val_acc:0.431]
Epoch [4/120    avg_loss:1.066, val_acc:0.546]
Epoch [5/120    avg_loss:0.860, val_acc:0.610]
Epoch [6/120    avg_loss:0.746, val_acc:0.729]
Epoch [7/120    avg_loss:0.584, val_acc:0.788]
Epoch [8/120    avg_loss:0.513, val_acc:0.783]
Epoch [9/120    avg_loss:0.446, val_acc:0.822]
Epoch [10/120    avg_loss:0.430, val_acc:0.857]
Epoch [11/120    avg_loss:0.345, val_acc:0.867]
Epoch [12/120    avg_loss:0.314, val_acc:0.880]
Epoch [13/120    avg_loss:0.302, val_acc:0.894]
Epoch [14/120    avg_loss:0.232, val_acc:0.873]
Epoch [15/120    avg_loss:0.250, val_acc:0.923]
Epoch [16/120    avg_loss:0.233, val_acc:0.905]
Epoch [17/120    avg_loss:0.211, val_acc:0.930]
Epoch [18/120    avg_loss:0.185, val_acc:0.936]
Epoch [19/120    avg_loss:0.180, val_acc:0.943]
Epoch [20/120    avg_loss:0.159, val_acc:0.847]
Epoch [21/120    avg_loss:0.191, val_acc:0.945]
Epoch [22/120    avg_loss:0.110, val_acc:0.938]
Epoch [23/120    avg_loss:0.095, val_acc:0.935]
Epoch [24/120    avg_loss:0.132, val_acc:0.943]
Epoch [25/120    avg_loss:0.117, val_acc:0.932]
Epoch [26/120    avg_loss:0.129, val_acc:0.934]
Epoch [27/120    avg_loss:0.086, val_acc:0.937]
Epoch [28/120    avg_loss:0.099, val_acc:0.952]
Epoch [29/120    avg_loss:0.094, val_acc:0.951]
Epoch [30/120    avg_loss:0.065, val_acc:0.960]
Epoch [31/120    avg_loss:0.062, val_acc:0.949]
Epoch [32/120    avg_loss:0.078, val_acc:0.973]
Epoch [33/120    avg_loss:0.043, val_acc:0.966]
Epoch [34/120    avg_loss:0.045, val_acc:0.968]
Epoch [35/120    avg_loss:0.042, val_acc:0.951]
Epoch [36/120    avg_loss:0.043, val_acc:0.972]
Epoch [37/120    avg_loss:0.076, val_acc:0.947]
Epoch [38/120    avg_loss:0.060, val_acc:0.975]
Epoch [39/120    avg_loss:0.043, val_acc:0.961]
Epoch [40/120    avg_loss:0.279, val_acc:0.908]
Epoch [41/120    avg_loss:0.173, val_acc:0.905]
Epoch [42/120    avg_loss:0.113, val_acc:0.962]
Epoch [43/120    avg_loss:0.088, val_acc:0.958]
Epoch [44/120    avg_loss:0.061, val_acc:0.963]
Epoch [45/120    avg_loss:0.053, val_acc:0.908]
Epoch [46/120    avg_loss:0.058, val_acc:0.912]
Epoch [47/120    avg_loss:0.070, val_acc:0.959]
Epoch [48/120    avg_loss:0.064, val_acc:0.962]
Epoch [49/120    avg_loss:0.076, val_acc:0.951]
Epoch [50/120    avg_loss:0.122, val_acc:0.937]
Epoch [51/120    avg_loss:0.103, val_acc:0.952]
Epoch [52/120    avg_loss:0.047, val_acc:0.977]
Epoch [53/120    avg_loss:0.031, val_acc:0.977]
Epoch [54/120    avg_loss:0.032, val_acc:0.975]
Epoch [55/120    avg_loss:0.028, val_acc:0.976]
Epoch [56/120    avg_loss:0.029, val_acc:0.977]
Epoch [57/120    avg_loss:0.027, val_acc:0.977]
Epoch [58/120    avg_loss:0.030, val_acc:0.977]
Epoch [59/120    avg_loss:0.030, val_acc:0.974]
Epoch [60/120    avg_loss:0.032, val_acc:0.975]
Epoch [61/120    avg_loss:0.022, val_acc:0.975]
Epoch [62/120    avg_loss:0.028, val_acc:0.975]
Epoch [63/120    avg_loss:0.024, val_acc:0.976]
Epoch [64/120    avg_loss:0.022, val_acc:0.979]
Epoch [65/120    avg_loss:0.024, val_acc:0.974]
Epoch [66/120    avg_loss:0.024, val_acc:0.976]
Epoch [67/120    avg_loss:0.020, val_acc:0.978]
Epoch [68/120    avg_loss:0.020, val_acc:0.976]
Epoch [69/120    avg_loss:0.015, val_acc:0.977]
Epoch [70/120    avg_loss:0.018, val_acc:0.976]
Epoch [71/120    avg_loss:0.021, val_acc:0.975]
Epoch [72/120    avg_loss:0.019, val_acc:0.974]
Epoch [73/120    avg_loss:0.018, val_acc:0.978]
Epoch [74/120    avg_loss:0.024, val_acc:0.974]
Epoch [75/120    avg_loss:0.022, val_acc:0.975]
Epoch [76/120    avg_loss:0.019, val_acc:0.975]
Epoch [77/120    avg_loss:0.016, val_acc:0.974]
Epoch [78/120    avg_loss:0.015, val_acc:0.974]
Epoch [79/120    avg_loss:0.019, val_acc:0.975]
Epoch [80/120    avg_loss:0.016, val_acc:0.975]
Epoch [81/120    avg_loss:0.017, val_acc:0.975]
Epoch [82/120    avg_loss:0.018, val_acc:0.975]
Epoch [83/120    avg_loss:0.014, val_acc:0.976]
Epoch [84/120    avg_loss:0.018, val_acc:0.975]
Epoch [85/120    avg_loss:0.016, val_acc:0.975]
Epoch [86/120    avg_loss:0.017, val_acc:0.976]
Epoch [87/120    avg_loss:0.015, val_acc:0.975]
Epoch [88/120    avg_loss:0.015, val_acc:0.976]
Epoch [89/120    avg_loss:0.017, val_acc:0.976]
Epoch [90/120    avg_loss:0.017, val_acc:0.975]
Epoch [91/120    avg_loss:0.018, val_acc:0.975]
Epoch [92/120    avg_loss:0.017, val_acc:0.975]
Epoch [93/120    avg_loss:0.015, val_acc:0.975]
Epoch [94/120    avg_loss:0.016, val_acc:0.975]
Epoch [95/120    avg_loss:0.016, val_acc:0.975]
Epoch [96/120    avg_loss:0.017, val_acc:0.975]
Epoch [97/120    avg_loss:0.030, val_acc:0.975]
Epoch [98/120    avg_loss:0.019, val_acc:0.975]
Epoch [99/120    avg_loss:0.018, val_acc:0.975]
Epoch [100/120    avg_loss:0.017, val_acc:0.975]
Epoch [101/120    avg_loss:0.021, val_acc:0.975]
Epoch [102/120    avg_loss:0.016, val_acc:0.975]
Epoch [103/120    avg_loss:0.017, val_acc:0.975]
Epoch [104/120    avg_loss:0.019, val_acc:0.975]
Epoch [105/120    avg_loss:0.018, val_acc:0.975]
Epoch [106/120    avg_loss:0.018, val_acc:0.975]
Epoch [107/120    avg_loss:0.022, val_acc:0.975]
Epoch [108/120    avg_loss:0.019, val_acc:0.975]
Epoch [109/120    avg_loss:0.018, val_acc:0.975]
Epoch [110/120    avg_loss:0.018, val_acc:0.975]
Epoch [111/120    avg_loss:0.020, val_acc:0.975]
Epoch [112/120    avg_loss:0.019, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.975]
Epoch [114/120    avg_loss:0.017, val_acc:0.975]
Epoch [115/120    avg_loss:0.023, val_acc:0.975]
Epoch [116/120    avg_loss:0.017, val_acc:0.975]
Epoch [117/120    avg_loss:0.017, val_acc:0.975]
Epoch [118/120    avg_loss:0.017, val_acc:0.975]
Epoch [119/120    avg_loss:0.016, val_acc:0.975]
Epoch [120/120    avg_loss:0.016, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     0     1     0    20     0    41     0]
 [    0     0 17983     0    46     0    30     0    31     0]
 [    0     2     0  1991     3     0     0     0    36     4]
 [    0    30    18     0  2875     0    14     0    31     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     0     5]
 [    0     0     0     0     0     0     3  1286     0     1]
 [    0     8     0     1    45     0     0     0  3496    21]
 [    0     0     0     3    23    48     0     0     0   845]]

Accuracy:
98.86968886318175

F1 scores:
[       nan 0.99205731 0.99653653 0.98784421 0.96395641 0.98194131
 0.99266653 0.9984472  0.97030253 0.93941078]

Kappa:
0.9850367440681711
creating ./logs/logs-2022-01-17PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4ab2dd8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.969, val_acc:0.188]
Epoch [2/120    avg_loss:1.550, val_acc:0.453]
Epoch [3/120    avg_loss:1.212, val_acc:0.613]
Epoch [4/120    avg_loss:0.984, val_acc:0.704]
Epoch [5/120    avg_loss:0.814, val_acc:0.753]
Epoch [6/120    avg_loss:0.650, val_acc:0.702]
Epoch [7/120    avg_loss:0.538, val_acc:0.750]
Epoch [8/120    avg_loss:0.505, val_acc:0.771]
Epoch [9/120    avg_loss:0.493, val_acc:0.811]
Epoch [10/120    avg_loss:0.379, val_acc:0.780]
Epoch [11/120    avg_loss:0.357, val_acc:0.828]
Epoch [12/120    avg_loss:0.315, val_acc:0.829]
Epoch [13/120    avg_loss:0.294, val_acc:0.866]
Epoch [14/120    avg_loss:0.282, val_acc:0.871]
Epoch [15/120    avg_loss:0.237, val_acc:0.916]
Epoch [16/120    avg_loss:0.220, val_acc:0.901]
Epoch [17/120    avg_loss:0.190, val_acc:0.794]
Epoch [18/120    avg_loss:0.246, val_acc:0.872]
Epoch [19/120    avg_loss:0.205, val_acc:0.903]
Epoch [20/120    avg_loss:0.126, val_acc:0.937]
Epoch [21/120    avg_loss:0.125, val_acc:0.941]
Epoch [22/120    avg_loss:0.138, val_acc:0.948]
Epoch [23/120    avg_loss:0.125, val_acc:0.945]
Epoch [24/120    avg_loss:0.112, val_acc:0.948]
Epoch [25/120    avg_loss:0.123, val_acc:0.924]
Epoch [26/120    avg_loss:0.106, val_acc:0.931]
Epoch [27/120    avg_loss:0.133, val_acc:0.955]
Epoch [28/120    avg_loss:0.078, val_acc:0.951]
Epoch [29/120    avg_loss:0.076, val_acc:0.965]
Epoch [30/120    avg_loss:0.067, val_acc:0.947]
Epoch [31/120    avg_loss:0.067, val_acc:0.957]
Epoch [32/120    avg_loss:0.054, val_acc:0.949]
Epoch [33/120    avg_loss:0.063, val_acc:0.957]
Epoch [34/120    avg_loss:0.071, val_acc:0.948]
Epoch [35/120    avg_loss:0.079, val_acc:0.946]
Epoch [36/120    avg_loss:0.073, val_acc:0.923]
Epoch [37/120    avg_loss:0.049, val_acc:0.967]
Epoch [38/120    avg_loss:0.037, val_acc:0.971]
Epoch [39/120    avg_loss:0.056, val_acc:0.965]
Epoch [40/120    avg_loss:0.047, val_acc:0.974]
Epoch [41/120    avg_loss:0.047, val_acc:0.970]
Epoch [42/120    avg_loss:0.032, val_acc:0.954]
Epoch [43/120    avg_loss:0.049, val_acc:0.961]
Epoch [44/120    avg_loss:0.027, val_acc:0.976]
Epoch [45/120    avg_loss:0.033, val_acc:0.974]
Epoch [46/120    avg_loss:0.030, val_acc:0.951]
Epoch [47/120    avg_loss:0.036, val_acc:0.970]
Epoch [48/120    avg_loss:0.201, val_acc:0.824]
Epoch [49/120    avg_loss:0.217, val_acc:0.951]
Epoch [50/120    avg_loss:0.052, val_acc:0.971]
Epoch [51/120    avg_loss:0.040, val_acc:0.970]
Epoch [52/120    avg_loss:0.034, val_acc:0.976]
Epoch [53/120    avg_loss:0.019, val_acc:0.983]
Epoch [54/120    avg_loss:0.018, val_acc:0.983]
Epoch [55/120    avg_loss:0.015, val_acc:0.979]
Epoch [56/120    avg_loss:0.030, val_acc:0.980]
Epoch [57/120    avg_loss:0.031, val_acc:0.974]
Epoch [58/120    avg_loss:0.027, val_acc:0.982]
Epoch [59/120    avg_loss:0.029, val_acc:0.973]
Epoch [60/120    avg_loss:0.027, val_acc:0.982]
Epoch [61/120    avg_loss:0.023, val_acc:0.988]
Epoch [62/120    avg_loss:0.018, val_acc:0.982]
Epoch [63/120    avg_loss:0.016, val_acc:0.984]
Epoch [64/120    avg_loss:0.022, val_acc:0.984]
Epoch [65/120    avg_loss:0.012, val_acc:0.988]
Epoch [66/120    avg_loss:0.020, val_acc:0.983]
Epoch [67/120    avg_loss:0.012, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.979]
Epoch [70/120    avg_loss:0.007, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.978]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.010, val_acc:0.955]
Epoch [75/120    avg_loss:0.030, val_acc:0.976]
Epoch [76/120    avg_loss:0.024, val_acc:0.981]
Epoch [77/120    avg_loss:0.012, val_acc:0.989]
Epoch [78/120    avg_loss:0.013, val_acc:0.967]
Epoch [79/120    avg_loss:0.135, val_acc:0.972]
Epoch [80/120    avg_loss:0.037, val_acc:0.959]
Epoch [81/120    avg_loss:0.029, val_acc:0.980]
Epoch [82/120    avg_loss:0.018, val_acc:0.980]
Epoch [83/120    avg_loss:0.026, val_acc:0.978]
Epoch [84/120    avg_loss:0.044, val_acc:0.971]
Epoch [85/120    avg_loss:0.075, val_acc:0.964]
Epoch [86/120    avg_loss:0.020, val_acc:0.984]
Epoch [87/120    avg_loss:0.023, val_acc:0.944]
Epoch [88/120    avg_loss:0.013, val_acc:0.979]
Epoch [89/120    avg_loss:0.017, val_acc:0.984]
Epoch [90/120    avg_loss:0.015, val_acc:0.987]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.014, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     0     0    20     0     0     0]
 [    0     3 18032     0    51     0     4     0     0     0]
 [    0     0     0  2007     2     0     0     0    26     1]
 [    0    29    20     0  2893     0     8     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     5     0     0  4864     0     0     9]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0     5     0     0    49     0     0     0  3483    34]
 [    0     0     0     1    14    41     0     0     0   863]]

Accuracy:
99.15889427132288

F1 scores:
[       nan 0.99557488 0.99784185 0.99135589 0.96739676 0.98453414
 0.99529364 0.99805825 0.98085046 0.94265429]

Kappa:
0.9888593798376516
