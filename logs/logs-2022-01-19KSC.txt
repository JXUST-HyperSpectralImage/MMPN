creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4a6b20860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.258, val_acc:0.565]
Epoch [2/120    avg_loss:1.727, val_acc:0.610]
Epoch [3/120    avg_loss:1.344, val_acc:0.690]
Epoch [4/120    avg_loss:1.085, val_acc:0.725]
Epoch [5/120    avg_loss:0.925, val_acc:0.727]
Epoch [6/120    avg_loss:0.830, val_acc:0.727]
Epoch [7/120    avg_loss:0.674, val_acc:0.819]
Epoch [8/120    avg_loss:0.628, val_acc:0.769]
Epoch [9/120    avg_loss:0.559, val_acc:0.823]
Epoch [10/120    avg_loss:0.520, val_acc:0.840]
Epoch [11/120    avg_loss:0.515, val_acc:0.875]
Epoch [12/120    avg_loss:0.501, val_acc:0.869]
Epoch [13/120    avg_loss:0.407, val_acc:0.867]
Epoch [14/120    avg_loss:0.434, val_acc:0.908]
Epoch [15/120    avg_loss:0.317, val_acc:0.883]
Epoch [16/120    avg_loss:0.301, val_acc:0.850]
Epoch [17/120    avg_loss:0.290, val_acc:0.896]
Epoch [18/120    avg_loss:0.324, val_acc:0.940]
Epoch [19/120    avg_loss:0.245, val_acc:0.935]
Epoch [20/120    avg_loss:0.243, val_acc:0.906]
Epoch [21/120    avg_loss:0.293, val_acc:0.919]
Epoch [22/120    avg_loss:0.234, val_acc:0.948]
Epoch [23/120    avg_loss:0.226, val_acc:0.952]
Epoch [24/120    avg_loss:0.191, val_acc:0.971]
Epoch [25/120    avg_loss:0.182, val_acc:0.969]
Epoch [26/120    avg_loss:0.221, val_acc:0.952]
Epoch [27/120    avg_loss:0.223, val_acc:0.942]
Epoch [28/120    avg_loss:0.287, val_acc:0.952]
Epoch [29/120    avg_loss:0.175, val_acc:0.969]
Epoch [30/120    avg_loss:0.163, val_acc:0.971]
Epoch [31/120    avg_loss:0.158, val_acc:0.977]
Epoch [32/120    avg_loss:0.164, val_acc:0.946]
Epoch [33/120    avg_loss:0.162, val_acc:0.975]
Epoch [34/120    avg_loss:0.112, val_acc:0.983]
Epoch [35/120    avg_loss:0.139, val_acc:0.942]
Epoch [36/120    avg_loss:0.171, val_acc:0.965]
Epoch [37/120    avg_loss:0.141, val_acc:0.977]
Epoch [38/120    avg_loss:0.132, val_acc:0.967]
Epoch [39/120    avg_loss:0.149, val_acc:0.940]
Epoch [40/120    avg_loss:0.101, val_acc:0.990]
Epoch [41/120    avg_loss:0.081, val_acc:0.979]
Epoch [42/120    avg_loss:0.101, val_acc:0.979]
Epoch [43/120    avg_loss:0.143, val_acc:0.942]
Epoch [44/120    avg_loss:0.106, val_acc:0.973]
Epoch [45/120    avg_loss:0.083, val_acc:0.994]
Epoch [46/120    avg_loss:0.083, val_acc:0.990]
Epoch [47/120    avg_loss:0.055, val_acc:0.992]
Epoch [48/120    avg_loss:0.056, val_acc:0.990]
Epoch [49/120    avg_loss:0.079, val_acc:0.983]
Epoch [50/120    avg_loss:0.058, val_acc:0.979]
Epoch [51/120    avg_loss:0.036, val_acc:0.990]
Epoch [52/120    avg_loss:0.055, val_acc:0.979]
Epoch [53/120    avg_loss:0.035, val_acc:0.990]
Epoch [54/120    avg_loss:0.038, val_acc:0.988]
Epoch [55/120    avg_loss:0.046, val_acc:0.994]
Epoch [56/120    avg_loss:0.046, val_acc:0.981]
Epoch [57/120    avg_loss:0.063, val_acc:0.975]
Epoch [58/120    avg_loss:0.060, val_acc:0.988]
Epoch [59/120    avg_loss:0.047, val_acc:0.985]
Epoch [60/120    avg_loss:0.036, val_acc:0.990]
Epoch [61/120    avg_loss:0.058, val_acc:0.988]
Epoch [62/120    avg_loss:0.041, val_acc:0.996]
Epoch [63/120    avg_loss:0.027, val_acc:0.996]
Epoch [64/120    avg_loss:0.020, val_acc:0.994]
Epoch [65/120    avg_loss:0.019, val_acc:0.994]
Epoch [66/120    avg_loss:0.019, val_acc:0.994]
Epoch [67/120    avg_loss:0.026, val_acc:0.990]
Epoch [68/120    avg_loss:0.023, val_acc:0.990]
Epoch [69/120    avg_loss:0.025, val_acc:0.996]
Epoch [70/120    avg_loss:0.018, val_acc:0.996]
Epoch [71/120    avg_loss:0.018, val_acc:0.996]
Epoch [72/120    avg_loss:0.014, val_acc:0.994]
Epoch [73/120    avg_loss:0.020, val_acc:0.994]
Epoch [74/120    avg_loss:0.015, val_acc:0.994]
Epoch [75/120    avg_loss:0.014, val_acc:0.990]
Epoch [76/120    avg_loss:0.014, val_acc:0.990]
Epoch [77/120    avg_loss:0.045, val_acc:0.992]
Epoch [78/120    avg_loss:0.067, val_acc:0.979]
Epoch [79/120    avg_loss:0.111, val_acc:0.979]
Epoch [80/120    avg_loss:0.037, val_acc:0.990]
Epoch [81/120    avg_loss:0.021, val_acc:0.996]
Epoch [82/120    avg_loss:0.025, val_acc:0.994]
Epoch [83/120    avg_loss:0.016, val_acc:0.998]
Epoch [84/120    avg_loss:0.016, val_acc:0.992]
Epoch [85/120    avg_loss:0.011, val_acc:0.996]
Epoch [86/120    avg_loss:0.010, val_acc:0.994]
Epoch [87/120    avg_loss:0.015, val_acc:0.994]
Epoch [88/120    avg_loss:0.021, val_acc:0.996]
Epoch [89/120    avg_loss:0.010, val_acc:0.998]
Epoch [90/120    avg_loss:0.019, val_acc:0.998]
Epoch [91/120    avg_loss:0.008, val_acc:0.998]
Epoch [92/120    avg_loss:0.007, val_acc:0.998]
Epoch [93/120    avg_loss:0.008, val_acc:0.996]
Epoch [94/120    avg_loss:0.005, val_acc:0.996]
Epoch [95/120    avg_loss:0.007, val_acc:0.996]
Epoch [96/120    avg_loss:0.010, val_acc:0.998]
Epoch [97/120    avg_loss:0.007, val_acc:0.996]
Epoch [98/120    avg_loss:0.012, val_acc:0.996]
Epoch [99/120    avg_loss:0.006, val_acc:0.998]
Epoch [100/120    avg_loss:0.004, val_acc:0.998]
Epoch [101/120    avg_loss:0.008, val_acc:0.998]
Epoch [102/120    avg_loss:0.009, val_acc:0.996]
Epoch [103/120    avg_loss:0.008, val_acc:0.996]
Epoch [104/120    avg_loss:0.012, val_acc:0.992]
Epoch [105/120    avg_loss:0.018, val_acc:0.992]
Epoch [106/120    avg_loss:0.021, val_acc:0.990]
Epoch [107/120    avg_loss:0.018, val_acc:0.996]
Epoch [108/120    avg_loss:0.011, val_acc:0.994]
Epoch [109/120    avg_loss:0.007, val_acc:0.994]
Epoch [110/120    avg_loss:0.008, val_acc:0.994]
Epoch [111/120    avg_loss:0.006, val_acc:0.996]
Epoch [112/120    avg_loss:0.007, val_acc:0.996]
Epoch [113/120    avg_loss:0.008, val_acc:0.996]
Epoch [114/120    avg_loss:0.005, val_acc:0.996]
Epoch [115/120    avg_loss:0.005, val_acc:0.996]
Epoch [116/120    avg_loss:0.004, val_acc:0.996]
Epoch [117/120    avg_loss:0.005, val_acc:0.996]
Epoch [118/120    avg_loss:0.005, val_acc:0.996]
Epoch [119/120    avg_loss:0.004, val_acc:0.996]
Epoch [120/120    avg_loss:0.004, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   3   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.996337   1.         1.         0.95010846 0.92198582
 0.99516908 1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9933535789701814
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f057a78b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.311, val_acc:0.538]
Epoch [2/120    avg_loss:1.683, val_acc:0.665]
Epoch [3/120    avg_loss:1.279, val_acc:0.671]
Epoch [4/120    avg_loss:1.011, val_acc:0.718]
Epoch [5/120    avg_loss:0.904, val_acc:0.796]
Epoch [6/120    avg_loss:0.723, val_acc:0.847]
Epoch [7/120    avg_loss:0.569, val_acc:0.812]
Epoch [8/120    avg_loss:0.580, val_acc:0.859]
Epoch [9/120    avg_loss:0.597, val_acc:0.861]
Epoch [10/120    avg_loss:0.525, val_acc:0.889]
Epoch [11/120    avg_loss:0.432, val_acc:0.901]
Epoch [12/120    avg_loss:0.398, val_acc:0.883]
Epoch [13/120    avg_loss:0.394, val_acc:0.935]
Epoch [14/120    avg_loss:0.383, val_acc:0.911]
Epoch [15/120    avg_loss:0.431, val_acc:0.931]
Epoch [16/120    avg_loss:0.292, val_acc:0.946]
Epoch [17/120    avg_loss:0.258, val_acc:0.940]
Epoch [18/120    avg_loss:0.264, val_acc:0.970]
Epoch [19/120    avg_loss:0.309, val_acc:0.964]
Epoch [20/120    avg_loss:0.254, val_acc:0.905]
Epoch [21/120    avg_loss:0.266, val_acc:0.958]
Epoch [22/120    avg_loss:0.338, val_acc:0.950]
Epoch [23/120    avg_loss:0.236, val_acc:0.958]
Epoch [24/120    avg_loss:0.286, val_acc:0.942]
Epoch [25/120    avg_loss:0.222, val_acc:0.952]
Epoch [26/120    avg_loss:0.207, val_acc:0.946]
Epoch [27/120    avg_loss:0.168, val_acc:0.972]
Epoch [28/120    avg_loss:0.198, val_acc:0.978]
Epoch [29/120    avg_loss:0.149, val_acc:0.956]
Epoch [30/120    avg_loss:0.232, val_acc:0.960]
Epoch [31/120    avg_loss:0.134, val_acc:0.972]
Epoch [32/120    avg_loss:0.164, val_acc:0.970]
Epoch [33/120    avg_loss:0.104, val_acc:0.988]
Epoch [34/120    avg_loss:0.205, val_acc:0.873]
Epoch [35/120    avg_loss:0.172, val_acc:0.966]
Epoch [36/120    avg_loss:0.114, val_acc:0.980]
Epoch [37/120    avg_loss:0.094, val_acc:0.976]
Epoch [38/120    avg_loss:0.096, val_acc:0.972]
Epoch [39/120    avg_loss:0.148, val_acc:0.980]
Epoch [40/120    avg_loss:0.113, val_acc:0.960]
Epoch [41/120    avg_loss:0.075, val_acc:0.982]
Epoch [42/120    avg_loss:0.085, val_acc:0.978]
Epoch [43/120    avg_loss:0.110, val_acc:0.976]
Epoch [44/120    avg_loss:0.059, val_acc:0.980]
Epoch [45/120    avg_loss:0.062, val_acc:0.992]
Epoch [46/120    avg_loss:0.141, val_acc:0.966]
Epoch [47/120    avg_loss:0.118, val_acc:0.964]
Epoch [48/120    avg_loss:0.113, val_acc:0.990]
Epoch [49/120    avg_loss:0.057, val_acc:0.986]
Epoch [50/120    avg_loss:0.060, val_acc:0.982]
Epoch [51/120    avg_loss:0.037, val_acc:0.988]
Epoch [52/120    avg_loss:0.054, val_acc:0.986]
Epoch [53/120    avg_loss:0.056, val_acc:0.986]
Epoch [54/120    avg_loss:0.049, val_acc:0.982]
Epoch [55/120    avg_loss:0.063, val_acc:0.990]
Epoch [56/120    avg_loss:0.055, val_acc:0.996]
Epoch [57/120    avg_loss:0.040, val_acc:0.990]
Epoch [58/120    avg_loss:0.037, val_acc:0.994]
Epoch [59/120    avg_loss:0.025, val_acc:0.984]
Epoch [60/120    avg_loss:0.023, val_acc:0.990]
Epoch [61/120    avg_loss:0.016, val_acc:0.990]
Epoch [62/120    avg_loss:0.021, val_acc:0.996]
Epoch [63/120    avg_loss:0.019, val_acc:0.992]
Epoch [64/120    avg_loss:0.053, val_acc:0.980]
Epoch [65/120    avg_loss:0.093, val_acc:0.972]
Epoch [66/120    avg_loss:0.051, val_acc:0.992]
Epoch [67/120    avg_loss:0.035, val_acc:0.998]
Epoch [68/120    avg_loss:0.029, val_acc:0.994]
Epoch [69/120    avg_loss:0.032, val_acc:0.984]
Epoch [70/120    avg_loss:0.028, val_acc:0.970]
Epoch [71/120    avg_loss:0.072, val_acc:0.950]
Epoch [72/120    avg_loss:0.044, val_acc:0.984]
Epoch [73/120    avg_loss:0.040, val_acc:0.986]
Epoch [74/120    avg_loss:0.038, val_acc:0.984]
Epoch [75/120    avg_loss:0.032, val_acc:0.990]
Epoch [76/120    avg_loss:0.036, val_acc:0.990]
Epoch [77/120    avg_loss:0.051, val_acc:0.990]
Epoch [78/120    avg_loss:0.054, val_acc:0.990]
Epoch [79/120    avg_loss:0.020, val_acc:0.994]
Epoch [80/120    avg_loss:0.017, val_acc:0.996]
Epoch [81/120    avg_loss:0.019, val_acc:0.996]
Epoch [82/120    avg_loss:0.010, val_acc:0.996]
Epoch [83/120    avg_loss:0.014, val_acc:0.996]
Epoch [84/120    avg_loss:0.011, val_acc:0.996]
Epoch [85/120    avg_loss:0.010, val_acc:0.996]
Epoch [86/120    avg_loss:0.016, val_acc:0.996]
Epoch [87/120    avg_loss:0.011, val_acc:0.998]
Epoch [88/120    avg_loss:0.020, val_acc:0.998]
Epoch [89/120    avg_loss:0.013, val_acc:0.998]
Epoch [90/120    avg_loss:0.009, val_acc:0.998]
Epoch [91/120    avg_loss:0.012, val_acc:0.996]
Epoch [92/120    avg_loss:0.009, val_acc:0.996]
Epoch [93/120    avg_loss:0.013, val_acc:0.996]
Epoch [94/120    avg_loss:0.012, val_acc:0.996]
Epoch [95/120    avg_loss:0.012, val_acc:0.996]
Epoch [96/120    avg_loss:0.016, val_acc:0.996]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.008, val_acc:0.996]
Epoch [99/120    avg_loss:0.011, val_acc:0.996]
Epoch [100/120    avg_loss:0.013, val_acc:0.996]
Epoch [101/120    avg_loss:0.009, val_acc:0.996]
Epoch [102/120    avg_loss:0.011, val_acc:0.998]
Epoch [103/120    avg_loss:0.010, val_acc:0.998]
Epoch [104/120    avg_loss:0.010, val_acc:0.998]
Epoch [105/120    avg_loss:0.011, val_acc:0.998]
Epoch [106/120    avg_loss:0.012, val_acc:0.998]
Epoch [107/120    avg_loss:0.013, val_acc:0.996]
Epoch [108/120    avg_loss:0.009, val_acc:0.996]
Epoch [109/120    avg_loss:0.010, val_acc:0.996]
Epoch [110/120    avg_loss:0.008, val_acc:0.996]
Epoch [111/120    avg_loss:0.009, val_acc:0.996]
Epoch [112/120    avg_loss:0.010, val_acc:0.998]
Epoch [113/120    avg_loss:0.008, val_acc:0.998]
Epoch [114/120    avg_loss:0.010, val_acc:0.998]
Epoch [115/120    avg_loss:0.008, val_acc:0.996]
Epoch [116/120    avg_loss:0.008, val_acc:0.996]
Epoch [117/120    avg_loss:0.010, val_acc:0.996]
Epoch [118/120    avg_loss:0.008, val_acc:0.998]
Epoch [119/120    avg_loss:0.010, val_acc:0.996]
Epoch [120/120    avg_loss:0.007, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 0.99780541 0.99319728 1.         0.98230088 0.97260274
 0.99277108 0.98378378 1.         1.         1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.996202103176467
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6e90a707f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.290, val_acc:0.617]
Epoch [2/120    avg_loss:1.704, val_acc:0.667]
Epoch [3/120    avg_loss:1.259, val_acc:0.721]
Epoch [4/120    avg_loss:1.005, val_acc:0.760]
Epoch [5/120    avg_loss:0.858, val_acc:0.779]
Epoch [6/120    avg_loss:0.749, val_acc:0.777]
Epoch [7/120    avg_loss:0.681, val_acc:0.802]
Epoch [8/120    avg_loss:0.672, val_acc:0.840]
Epoch [9/120    avg_loss:0.587, val_acc:0.877]
Epoch [10/120    avg_loss:0.527, val_acc:0.894]
Epoch [11/120    avg_loss:0.455, val_acc:0.894]
Epoch [12/120    avg_loss:0.438, val_acc:0.838]
Epoch [13/120    avg_loss:0.422, val_acc:0.908]
Epoch [14/120    avg_loss:0.380, val_acc:0.917]
Epoch [15/120    avg_loss:0.372, val_acc:0.898]
Epoch [16/120    avg_loss:0.331, val_acc:0.954]
Epoch [17/120    avg_loss:0.280, val_acc:0.956]
Epoch [18/120    avg_loss:0.360, val_acc:0.871]
Epoch [19/120    avg_loss:0.278, val_acc:0.935]
Epoch [20/120    avg_loss:0.279, val_acc:0.940]
Epoch [21/120    avg_loss:0.270, val_acc:0.931]
Epoch [22/120    avg_loss:0.329, val_acc:0.923]
Epoch [23/120    avg_loss:0.309, val_acc:0.944]
Epoch [24/120    avg_loss:0.211, val_acc:0.952]
Epoch [25/120    avg_loss:0.250, val_acc:0.952]
Epoch [26/120    avg_loss:0.219, val_acc:0.965]
Epoch [27/120    avg_loss:0.170, val_acc:0.960]
Epoch [28/120    avg_loss:0.200, val_acc:0.952]
Epoch [29/120    avg_loss:0.198, val_acc:0.956]
Epoch [30/120    avg_loss:0.164, val_acc:0.973]
Epoch [31/120    avg_loss:0.175, val_acc:0.958]
Epoch [32/120    avg_loss:0.161, val_acc:0.952]
Epoch [33/120    avg_loss:0.156, val_acc:0.965]
Epoch [34/120    avg_loss:0.101, val_acc:0.960]
Epoch [35/120    avg_loss:0.123, val_acc:0.971]
Epoch [36/120    avg_loss:0.149, val_acc:0.971]
Epoch [37/120    avg_loss:0.106, val_acc:0.971]
Epoch [38/120    avg_loss:0.114, val_acc:0.938]
Epoch [39/120    avg_loss:0.154, val_acc:0.977]
Epoch [40/120    avg_loss:0.160, val_acc:0.977]
Epoch [41/120    avg_loss:0.084, val_acc:0.979]
Epoch [42/120    avg_loss:0.085, val_acc:0.979]
Epoch [43/120    avg_loss:0.102, val_acc:0.979]
Epoch [44/120    avg_loss:0.082, val_acc:0.983]
Epoch [45/120    avg_loss:0.057, val_acc:0.985]
Epoch [46/120    avg_loss:0.071, val_acc:0.992]
Epoch [47/120    avg_loss:0.087, val_acc:0.975]
Epoch [48/120    avg_loss:0.096, val_acc:0.965]
Epoch [49/120    avg_loss:0.075, val_acc:0.981]
Epoch [50/120    avg_loss:0.066, val_acc:0.988]
Epoch [51/120    avg_loss:0.072, val_acc:0.988]
Epoch [52/120    avg_loss:0.045, val_acc:0.969]
Epoch [53/120    avg_loss:0.088, val_acc:0.979]
Epoch [54/120    avg_loss:0.070, val_acc:0.983]
Epoch [55/120    avg_loss:0.047, val_acc:0.981]
Epoch [56/120    avg_loss:0.084, val_acc:0.985]
Epoch [57/120    avg_loss:0.077, val_acc:0.994]
Epoch [58/120    avg_loss:0.035, val_acc:0.992]
Epoch [59/120    avg_loss:0.034, val_acc:0.990]
Epoch [60/120    avg_loss:0.061, val_acc:0.938]
Epoch [61/120    avg_loss:0.093, val_acc:0.981]
Epoch [62/120    avg_loss:0.038, val_acc:0.990]
Epoch [63/120    avg_loss:0.052, val_acc:0.988]
Epoch [64/120    avg_loss:0.042, val_acc:0.975]
Epoch [65/120    avg_loss:0.079, val_acc:0.990]
Epoch [66/120    avg_loss:0.044, val_acc:0.990]
Epoch [67/120    avg_loss:0.053, val_acc:0.990]
Epoch [68/120    avg_loss:0.065, val_acc:0.979]
Epoch [69/120    avg_loss:0.040, val_acc:0.985]
Epoch [70/120    avg_loss:0.023, val_acc:0.992]
Epoch [71/120    avg_loss:0.030, val_acc:0.990]
Epoch [72/120    avg_loss:0.017, val_acc:0.988]
Epoch [73/120    avg_loss:0.016, val_acc:0.992]
Epoch [74/120    avg_loss:0.016, val_acc:0.992]
Epoch [75/120    avg_loss:0.015, val_acc:0.992]
Epoch [76/120    avg_loss:0.011, val_acc:0.992]
Epoch [77/120    avg_loss:0.015, val_acc:0.990]
Epoch [78/120    avg_loss:0.013, val_acc:0.992]
Epoch [79/120    avg_loss:0.022, val_acc:0.992]
Epoch [80/120    avg_loss:0.012, val_acc:0.992]
Epoch [81/120    avg_loss:0.013, val_acc:0.992]
Epoch [82/120    avg_loss:0.011, val_acc:0.992]
Epoch [83/120    avg_loss:0.017, val_acc:0.992]
Epoch [84/120    avg_loss:0.013, val_acc:0.992]
Epoch [85/120    avg_loss:0.010, val_acc:0.992]
Epoch [86/120    avg_loss:0.012, val_acc:0.992]
Epoch [87/120    avg_loss:0.019, val_acc:0.992]
Epoch [88/120    avg_loss:0.017, val_acc:0.992]
Epoch [89/120    avg_loss:0.011, val_acc:0.992]
Epoch [90/120    avg_loss:0.015, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.992]
Epoch [92/120    avg_loss:0.015, val_acc:0.992]
Epoch [93/120    avg_loss:0.014, val_acc:0.992]
Epoch [94/120    avg_loss:0.011, val_acc:0.992]
Epoch [95/120    avg_loss:0.011, val_acc:0.992]
Epoch [96/120    avg_loss:0.019, val_acc:0.992]
Epoch [97/120    avg_loss:0.014, val_acc:0.992]
Epoch [98/120    avg_loss:0.014, val_acc:0.992]
Epoch [99/120    avg_loss:0.013, val_acc:0.992]
Epoch [100/120    avg_loss:0.013, val_acc:0.992]
Epoch [101/120    avg_loss:0.016, val_acc:0.992]
Epoch [102/120    avg_loss:0.020, val_acc:0.992]
Epoch [103/120    avg_loss:0.012, val_acc:0.992]
Epoch [104/120    avg_loss:0.019, val_acc:0.992]
Epoch [105/120    avg_loss:0.014, val_acc:0.992]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.016, val_acc:0.992]
Epoch [108/120    avg_loss:0.010, val_acc:0.992]
Epoch [109/120    avg_loss:0.017, val_acc:0.992]
Epoch [110/120    avg_loss:0.016, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.014, val_acc:0.992]
Epoch [113/120    avg_loss:0.015, val_acc:0.992]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.013, val_acc:0.992]
Epoch [116/120    avg_loss:0.013, val_acc:0.992]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.016, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.992]
Epoch [120/120    avg_loss:0.013, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   4   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99926954 0.99545455 1.         0.96086957 0.94326241
 0.99757869 0.98924731 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9950147676562336
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7c39c47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.272, val_acc:0.606]
Epoch [2/120    avg_loss:1.673, val_acc:0.613]
Epoch [3/120    avg_loss:1.305, val_acc:0.744]
Epoch [4/120    avg_loss:0.987, val_acc:0.713]
Epoch [5/120    avg_loss:0.816, val_acc:0.775]
Epoch [6/120    avg_loss:0.645, val_acc:0.844]
Epoch [7/120    avg_loss:0.562, val_acc:0.904]
Epoch [8/120    avg_loss:0.499, val_acc:0.865]
Epoch [9/120    avg_loss:0.475, val_acc:0.877]
Epoch [10/120    avg_loss:0.442, val_acc:0.819]
Epoch [11/120    avg_loss:0.381, val_acc:0.908]
Epoch [12/120    avg_loss:0.436, val_acc:0.890]
Epoch [13/120    avg_loss:0.377, val_acc:0.904]
Epoch [14/120    avg_loss:0.381, val_acc:0.902]
Epoch [15/120    avg_loss:0.358, val_acc:0.912]
Epoch [16/120    avg_loss:0.306, val_acc:0.931]
Epoch [17/120    avg_loss:0.329, val_acc:0.910]
Epoch [18/120    avg_loss:0.458, val_acc:0.898]
Epoch [19/120    avg_loss:0.331, val_acc:0.940]
Epoch [20/120    avg_loss:0.241, val_acc:0.952]
Epoch [21/120    avg_loss:0.216, val_acc:0.958]
Epoch [22/120    avg_loss:0.208, val_acc:0.946]
Epoch [23/120    avg_loss:0.202, val_acc:0.965]
Epoch [24/120    avg_loss:0.208, val_acc:0.960]
Epoch [25/120    avg_loss:0.225, val_acc:0.921]
Epoch [26/120    avg_loss:0.209, val_acc:0.956]
Epoch [27/120    avg_loss:0.163, val_acc:0.958]
Epoch [28/120    avg_loss:0.215, val_acc:0.933]
Epoch [29/120    avg_loss:0.242, val_acc:0.946]
Epoch [30/120    avg_loss:0.165, val_acc:0.956]
Epoch [31/120    avg_loss:0.258, val_acc:0.910]
Epoch [32/120    avg_loss:0.197, val_acc:0.950]
Epoch [33/120    avg_loss:0.176, val_acc:0.967]
Epoch [34/120    avg_loss:0.149, val_acc:0.971]
Epoch [35/120    avg_loss:0.134, val_acc:0.975]
Epoch [36/120    avg_loss:0.109, val_acc:0.973]
Epoch [37/120    avg_loss:0.156, val_acc:0.981]
Epoch [38/120    avg_loss:0.109, val_acc:0.981]
Epoch [39/120    avg_loss:0.110, val_acc:0.981]
Epoch [40/120    avg_loss:0.096, val_acc:0.990]
Epoch [41/120    avg_loss:0.092, val_acc:0.979]
Epoch [42/120    avg_loss:0.090, val_acc:0.985]
Epoch [43/120    avg_loss:0.079, val_acc:0.983]
Epoch [44/120    avg_loss:0.094, val_acc:0.960]
Epoch [45/120    avg_loss:0.103, val_acc:0.981]
Epoch [46/120    avg_loss:0.091, val_acc:0.977]
Epoch [47/120    avg_loss:0.095, val_acc:0.971]
Epoch [48/120    avg_loss:0.077, val_acc:0.971]
Epoch [49/120    avg_loss:0.085, val_acc:0.983]
Epoch [50/120    avg_loss:0.073, val_acc:0.954]
Epoch [51/120    avg_loss:0.081, val_acc:0.990]
Epoch [52/120    avg_loss:0.075, val_acc:0.983]
Epoch [53/120    avg_loss:0.078, val_acc:0.992]
Epoch [54/120    avg_loss:0.047, val_acc:0.992]
Epoch [55/120    avg_loss:0.053, val_acc:0.990]
Epoch [56/120    avg_loss:0.054, val_acc:0.988]
Epoch [57/120    avg_loss:0.137, val_acc:0.975]
Epoch [58/120    avg_loss:0.125, val_acc:0.977]
Epoch [59/120    avg_loss:0.071, val_acc:0.958]
Epoch [60/120    avg_loss:0.070, val_acc:0.988]
Epoch [61/120    avg_loss:0.085, val_acc:0.967]
Epoch [62/120    avg_loss:0.122, val_acc:0.948]
Epoch [63/120    avg_loss:0.079, val_acc:0.973]
Epoch [64/120    avg_loss:0.059, val_acc:0.992]
Epoch [65/120    avg_loss:0.034, val_acc:0.992]
Epoch [66/120    avg_loss:0.040, val_acc:0.992]
Epoch [67/120    avg_loss:0.036, val_acc:0.983]
Epoch [68/120    avg_loss:0.044, val_acc:0.979]
Epoch [69/120    avg_loss:0.055, val_acc:0.990]
Epoch [70/120    avg_loss:0.029, val_acc:0.996]
Epoch [71/120    avg_loss:0.023, val_acc:0.988]
Epoch [72/120    avg_loss:0.039, val_acc:0.992]
Epoch [73/120    avg_loss:0.024, val_acc:0.998]
Epoch [74/120    avg_loss:0.022, val_acc:0.996]
Epoch [75/120    avg_loss:0.018, val_acc:0.996]
Epoch [76/120    avg_loss:0.031, val_acc:0.990]
Epoch [77/120    avg_loss:0.023, val_acc:0.994]
Epoch [78/120    avg_loss:0.023, val_acc:0.985]
Epoch [79/120    avg_loss:0.055, val_acc:0.988]
Epoch [80/120    avg_loss:0.063, val_acc:0.994]
Epoch [81/120    avg_loss:0.034, val_acc:0.994]
Epoch [82/120    avg_loss:0.027, val_acc:0.992]
Epoch [83/120    avg_loss:0.058, val_acc:0.975]
Epoch [84/120    avg_loss:0.184, val_acc:0.938]
Epoch [85/120    avg_loss:0.130, val_acc:0.969]
Epoch [86/120    avg_loss:0.078, val_acc:0.975]
Epoch [87/120    avg_loss:0.033, val_acc:0.985]
Epoch [88/120    avg_loss:0.036, val_acc:0.985]
Epoch [89/120    avg_loss:0.032, val_acc:0.990]
Epoch [90/120    avg_loss:0.023, val_acc:0.990]
Epoch [91/120    avg_loss:0.020, val_acc:0.992]
Epoch [92/120    avg_loss:0.024, val_acc:0.990]
Epoch [93/120    avg_loss:0.027, val_acc:0.994]
Epoch [94/120    avg_loss:0.023, val_acc:0.992]
Epoch [95/120    avg_loss:0.024, val_acc:0.992]
Epoch [96/120    avg_loss:0.027, val_acc:0.992]
Epoch [97/120    avg_loss:0.035, val_acc:0.992]
Epoch [98/120    avg_loss:0.019, val_acc:0.992]
Epoch [99/120    avg_loss:0.021, val_acc:0.994]
Epoch [100/120    avg_loss:0.022, val_acc:0.994]
Epoch [101/120    avg_loss:0.022, val_acc:0.994]
Epoch [102/120    avg_loss:0.025, val_acc:0.994]
Epoch [103/120    avg_loss:0.028, val_acc:0.994]
Epoch [104/120    avg_loss:0.022, val_acc:0.994]
Epoch [105/120    avg_loss:0.019, val_acc:0.994]
Epoch [106/120    avg_loss:0.024, val_acc:0.994]
Epoch [107/120    avg_loss:0.025, val_acc:0.994]
Epoch [108/120    avg_loss:0.017, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.013, val_acc:0.994]
Epoch [111/120    avg_loss:0.028, val_acc:0.994]
Epoch [112/120    avg_loss:0.019, val_acc:0.994]
Epoch [113/120    avg_loss:0.017, val_acc:0.994]
Epoch [114/120    avg_loss:0.020, val_acc:0.994]
Epoch [115/120    avg_loss:0.018, val_acc:0.994]
Epoch [116/120    avg_loss:0.022, val_acc:0.994]
Epoch [117/120    avg_loss:0.017, val_acc:0.994]
Epoch [118/120    avg_loss:0.022, val_acc:0.994]
Epoch [119/120    avg_loss:0.021, val_acc:0.994]
Epoch [120/120    avg_loss:0.017, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.996337   0.97986577 1.         0.95483871 0.92805755
 0.98800959 0.94972067 1.         1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.9914542607070005
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4e8367c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.255, val_acc:0.591]
Epoch [2/120    avg_loss:1.688, val_acc:0.633]
Epoch [3/120    avg_loss:1.294, val_acc:0.712]
Epoch [4/120    avg_loss:1.025, val_acc:0.726]
Epoch [5/120    avg_loss:0.892, val_acc:0.815]
Epoch [6/120    avg_loss:0.775, val_acc:0.808]
Epoch [7/120    avg_loss:0.742, val_acc:0.841]
Epoch [8/120    avg_loss:0.597, val_acc:0.851]
Epoch [9/120    avg_loss:0.614, val_acc:0.827]
Epoch [10/120    avg_loss:0.601, val_acc:0.821]
Epoch [11/120    avg_loss:0.515, val_acc:0.865]
Epoch [12/120    avg_loss:0.454, val_acc:0.841]
Epoch [13/120    avg_loss:0.484, val_acc:0.857]
Epoch [14/120    avg_loss:0.451, val_acc:0.871]
Epoch [15/120    avg_loss:0.421, val_acc:0.875]
Epoch [16/120    avg_loss:0.525, val_acc:0.837]
Epoch [17/120    avg_loss:0.482, val_acc:0.885]
Epoch [18/120    avg_loss:0.408, val_acc:0.881]
Epoch [19/120    avg_loss:0.371, val_acc:0.923]
Epoch [20/120    avg_loss:0.348, val_acc:0.919]
Epoch [21/120    avg_loss:0.375, val_acc:0.925]
Epoch [22/120    avg_loss:0.326, val_acc:0.940]
Epoch [23/120    avg_loss:0.336, val_acc:0.927]
Epoch [24/120    avg_loss:0.323, val_acc:0.913]
Epoch [25/120    avg_loss:0.293, val_acc:0.942]
Epoch [26/120    avg_loss:0.241, val_acc:0.942]
Epoch [27/120    avg_loss:0.225, val_acc:0.952]
Epoch [28/120    avg_loss:0.278, val_acc:0.929]
Epoch [29/120    avg_loss:0.268, val_acc:0.956]
Epoch [30/120    avg_loss:0.234, val_acc:0.962]
Epoch [31/120    avg_loss:0.192, val_acc:0.958]
Epoch [32/120    avg_loss:0.209, val_acc:0.925]
Epoch [33/120    avg_loss:0.177, val_acc:0.938]
Epoch [34/120    avg_loss:0.211, val_acc:0.948]
Epoch [35/120    avg_loss:0.222, val_acc:0.964]
Epoch [36/120    avg_loss:0.170, val_acc:0.958]
Epoch [37/120    avg_loss:0.167, val_acc:0.964]
Epoch [38/120    avg_loss:0.153, val_acc:0.954]
Epoch [39/120    avg_loss:0.179, val_acc:0.968]
Epoch [40/120    avg_loss:0.162, val_acc:0.929]
Epoch [41/120    avg_loss:0.193, val_acc:0.942]
Epoch [42/120    avg_loss:0.211, val_acc:0.964]
Epoch [43/120    avg_loss:0.119, val_acc:0.962]
Epoch [44/120    avg_loss:0.148, val_acc:0.952]
Epoch [45/120    avg_loss:0.200, val_acc:0.962]
Epoch [46/120    avg_loss:0.140, val_acc:0.958]
Epoch [47/120    avg_loss:0.146, val_acc:0.958]
Epoch [48/120    avg_loss:0.165, val_acc:0.972]
Epoch [49/120    avg_loss:0.108, val_acc:0.964]
Epoch [50/120    avg_loss:0.087, val_acc:0.974]
Epoch [51/120    avg_loss:0.106, val_acc:0.978]
Epoch [52/120    avg_loss:0.100, val_acc:0.964]
Epoch [53/120    avg_loss:0.083, val_acc:0.978]
Epoch [54/120    avg_loss:0.064, val_acc:0.982]
Epoch [55/120    avg_loss:0.063, val_acc:0.982]
Epoch [56/120    avg_loss:0.068, val_acc:0.974]
Epoch [57/120    avg_loss:0.059, val_acc:0.968]
Epoch [58/120    avg_loss:0.065, val_acc:0.988]
Epoch [59/120    avg_loss:0.104, val_acc:0.964]
Epoch [60/120    avg_loss:0.067, val_acc:0.978]
Epoch [61/120    avg_loss:0.032, val_acc:0.984]
Epoch [62/120    avg_loss:0.050, val_acc:0.988]
Epoch [63/120    avg_loss:0.054, val_acc:0.974]
Epoch [64/120    avg_loss:0.055, val_acc:0.982]
Epoch [65/120    avg_loss:0.041, val_acc:0.986]
Epoch [66/120    avg_loss:0.043, val_acc:0.982]
Epoch [67/120    avg_loss:0.080, val_acc:0.982]
Epoch [68/120    avg_loss:0.050, val_acc:0.978]
Epoch [69/120    avg_loss:0.050, val_acc:0.984]
Epoch [70/120    avg_loss:0.103, val_acc:0.988]
Epoch [71/120    avg_loss:0.067, val_acc:0.980]
Epoch [72/120    avg_loss:0.039, val_acc:0.982]
Epoch [73/120    avg_loss:0.054, val_acc:0.938]
Epoch [74/120    avg_loss:0.067, val_acc:0.986]
Epoch [75/120    avg_loss:0.054, val_acc:0.980]
Epoch [76/120    avg_loss:0.045, val_acc:0.990]
Epoch [77/120    avg_loss:0.038, val_acc:0.986]
Epoch [78/120    avg_loss:0.038, val_acc:0.988]
Epoch [79/120    avg_loss:0.022, val_acc:0.970]
Epoch [80/120    avg_loss:0.022, val_acc:0.986]
Epoch [81/120    avg_loss:0.018, val_acc:0.988]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.022, val_acc:0.986]
Epoch [84/120    avg_loss:0.049, val_acc:0.984]
Epoch [85/120    avg_loss:0.033, val_acc:0.982]
Epoch [86/120    avg_loss:0.079, val_acc:0.988]
Epoch [87/120    avg_loss:0.023, val_acc:0.990]
Epoch [88/120    avg_loss:0.063, val_acc:0.982]
Epoch [89/120    avg_loss:0.035, val_acc:0.982]
Epoch [90/120    avg_loss:0.082, val_acc:0.978]
Epoch [91/120    avg_loss:0.049, val_acc:0.982]
Epoch [92/120    avg_loss:0.052, val_acc:0.986]
Epoch [93/120    avg_loss:0.034, val_acc:0.992]
Epoch [94/120    avg_loss:0.015, val_acc:0.988]
Epoch [95/120    avg_loss:0.016, val_acc:0.988]
Epoch [96/120    avg_loss:0.026, val_acc:0.984]
Epoch [97/120    avg_loss:0.024, val_acc:0.978]
Epoch [98/120    avg_loss:0.039, val_acc:0.986]
Epoch [99/120    avg_loss:0.045, val_acc:0.972]
Epoch [100/120    avg_loss:0.043, val_acc:0.988]
Epoch [101/120    avg_loss:0.049, val_acc:0.982]
Epoch [102/120    avg_loss:0.031, val_acc:0.990]
Epoch [103/120    avg_loss:0.015, val_acc:0.990]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.009, val_acc:0.994]
Epoch [106/120    avg_loss:0.011, val_acc:0.994]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.994]
Epoch [109/120    avg_loss:0.012, val_acc:0.994]
Epoch [110/120    avg_loss:0.010, val_acc:0.992]
Epoch [111/120    avg_loss:0.010, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.988]
Epoch [113/120    avg_loss:0.015, val_acc:0.992]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.994]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.992]
Epoch [119/120    avg_loss:0.009, val_acc:0.994]
Epoch [120/120    avg_loss:0.008, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99486427 1.         1.         0.96846847 0.95652174
 0.98329356 1.         1.         1.         1.         0.9973545
 0.99668508 1.        ]

Kappa:
0.9945413363842879
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f73c02a67b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.241, val_acc:0.540]
Epoch [2/120    avg_loss:1.667, val_acc:0.644]
Epoch [3/120    avg_loss:1.326, val_acc:0.775]
Epoch [4/120    avg_loss:1.074, val_acc:0.771]
Epoch [5/120    avg_loss:0.894, val_acc:0.790]
Epoch [6/120    avg_loss:0.732, val_acc:0.827]
Epoch [7/120    avg_loss:0.707, val_acc:0.823]
Epoch [8/120    avg_loss:0.667, val_acc:0.850]
Epoch [9/120    avg_loss:0.610, val_acc:0.858]
Epoch [10/120    avg_loss:0.562, val_acc:0.825]
Epoch [11/120    avg_loss:0.511, val_acc:0.838]
Epoch [12/120    avg_loss:0.461, val_acc:0.863]
Epoch [13/120    avg_loss:0.535, val_acc:0.867]
Epoch [14/120    avg_loss:0.416, val_acc:0.875]
Epoch [15/120    avg_loss:0.403, val_acc:0.892]
Epoch [16/120    avg_loss:0.504, val_acc:0.873]
Epoch [17/120    avg_loss:0.382, val_acc:0.873]
Epoch [18/120    avg_loss:0.335, val_acc:0.912]
Epoch [19/120    avg_loss:0.340, val_acc:0.919]
Epoch [20/120    avg_loss:0.271, val_acc:0.927]
Epoch [21/120    avg_loss:0.289, val_acc:0.921]
Epoch [22/120    avg_loss:0.311, val_acc:0.927]
Epoch [23/120    avg_loss:0.218, val_acc:0.887]
Epoch [24/120    avg_loss:0.253, val_acc:0.906]
Epoch [25/120    avg_loss:0.282, val_acc:0.935]
Epoch [26/120    avg_loss:0.226, val_acc:0.963]
Epoch [27/120    avg_loss:0.216, val_acc:0.969]
Epoch [28/120    avg_loss:0.165, val_acc:0.942]
Epoch [29/120    avg_loss:0.156, val_acc:0.965]
Epoch [30/120    avg_loss:0.208, val_acc:0.971]
Epoch [31/120    avg_loss:0.167, val_acc:0.952]
Epoch [32/120    avg_loss:0.168, val_acc:0.965]
Epoch [33/120    avg_loss:0.093, val_acc:0.973]
Epoch [34/120    avg_loss:0.129, val_acc:0.940]
Epoch [35/120    avg_loss:0.166, val_acc:0.938]
Epoch [36/120    avg_loss:0.184, val_acc:0.977]
Epoch [37/120    avg_loss:0.128, val_acc:0.967]
Epoch [38/120    avg_loss:0.134, val_acc:0.969]
Epoch [39/120    avg_loss:0.128, val_acc:0.925]
Epoch [40/120    avg_loss:0.144, val_acc:0.977]
Epoch [41/120    avg_loss:0.109, val_acc:0.979]
Epoch [42/120    avg_loss:0.169, val_acc:0.979]
Epoch [43/120    avg_loss:0.131, val_acc:0.956]
Epoch [44/120    avg_loss:0.150, val_acc:0.967]
Epoch [45/120    avg_loss:0.070, val_acc:0.988]
Epoch [46/120    avg_loss:0.041, val_acc:0.992]
Epoch [47/120    avg_loss:0.063, val_acc:0.988]
Epoch [48/120    avg_loss:0.058, val_acc:0.992]
Epoch [49/120    avg_loss:0.074, val_acc:0.990]
Epoch [50/120    avg_loss:0.125, val_acc:0.975]
Epoch [51/120    avg_loss:0.082, val_acc:1.000]
Epoch [52/120    avg_loss:0.061, val_acc:0.990]
Epoch [53/120    avg_loss:0.049, val_acc:0.971]
Epoch [54/120    avg_loss:0.046, val_acc:0.992]
Epoch [55/120    avg_loss:0.027, val_acc:0.994]
Epoch [56/120    avg_loss:0.024, val_acc:0.998]
Epoch [57/120    avg_loss:0.022, val_acc:0.994]
Epoch [58/120    avg_loss:0.032, val_acc:0.996]
Epoch [59/120    avg_loss:0.029, val_acc:0.996]
Epoch [60/120    avg_loss:0.035, val_acc:0.992]
Epoch [61/120    avg_loss:0.035, val_acc:0.996]
Epoch [62/120    avg_loss:0.021, val_acc:0.998]
Epoch [63/120    avg_loss:0.089, val_acc:0.965]
Epoch [64/120    avg_loss:0.093, val_acc:0.979]
Epoch [65/120    avg_loss:0.056, val_acc:0.988]
Epoch [66/120    avg_loss:0.033, val_acc:0.992]
Epoch [67/120    avg_loss:0.026, val_acc:0.992]
Epoch [68/120    avg_loss:0.022, val_acc:0.994]
Epoch [69/120    avg_loss:0.023, val_acc:0.994]
Epoch [70/120    avg_loss:0.020, val_acc:0.996]
Epoch [71/120    avg_loss:0.033, val_acc:0.996]
Epoch [72/120    avg_loss:0.034, val_acc:0.998]
Epoch [73/120    avg_loss:0.024, val_acc:0.998]
Epoch [74/120    avg_loss:0.017, val_acc:0.998]
Epoch [75/120    avg_loss:0.029, val_acc:1.000]
Epoch [76/120    avg_loss:0.015, val_acc:1.000]
Epoch [77/120    avg_loss:0.020, val_acc:1.000]
Epoch [78/120    avg_loss:0.026, val_acc:0.998]
Epoch [79/120    avg_loss:0.019, val_acc:1.000]
Epoch [80/120    avg_loss:0.013, val_acc:1.000]
Epoch [81/120    avg_loss:0.019, val_acc:1.000]
Epoch [82/120    avg_loss:0.016, val_acc:1.000]
Epoch [83/120    avg_loss:0.024, val_acc:0.998]
Epoch [84/120    avg_loss:0.018, val_acc:1.000]
Epoch [85/120    avg_loss:0.024, val_acc:0.996]
Epoch [86/120    avg_loss:0.023, val_acc:1.000]
Epoch [87/120    avg_loss:0.018, val_acc:1.000]
Epoch [88/120    avg_loss:0.023, val_acc:0.998]
Epoch [89/120    avg_loss:0.015, val_acc:0.998]
Epoch [90/120    avg_loss:0.024, val_acc:0.998]
Epoch [91/120    avg_loss:0.014, val_acc:0.998]
Epoch [92/120    avg_loss:0.015, val_acc:0.998]
Epoch [93/120    avg_loss:0.016, val_acc:0.998]
Epoch [94/120    avg_loss:0.022, val_acc:0.998]
Epoch [95/120    avg_loss:0.017, val_acc:0.998]
Epoch [96/120    avg_loss:0.014, val_acc:1.000]
Epoch [97/120    avg_loss:0.016, val_acc:1.000]
Epoch [98/120    avg_loss:0.012, val_acc:1.000]
Epoch [99/120    avg_loss:0.019, val_acc:0.998]
Epoch [100/120    avg_loss:0.011, val_acc:1.000]
Epoch [101/120    avg_loss:0.020, val_acc:0.998]
Epoch [102/120    avg_loss:0.012, val_acc:1.000]
Epoch [103/120    avg_loss:0.012, val_acc:1.000]
Epoch [104/120    avg_loss:0.011, val_acc:1.000]
Epoch [105/120    avg_loss:0.019, val_acc:1.000]
Epoch [106/120    avg_loss:0.010, val_acc:1.000]
Epoch [107/120    avg_loss:0.013, val_acc:0.998]
Epoch [108/120    avg_loss:0.014, val_acc:1.000]
Epoch [109/120    avg_loss:0.016, val_acc:1.000]
Epoch [110/120    avg_loss:0.014, val_acc:1.000]
Epoch [111/120    avg_loss:0.017, val_acc:1.000]
Epoch [112/120    avg_loss:0.013, val_acc:1.000]
Epoch [113/120    avg_loss:0.007, val_acc:1.000]
Epoch [114/120    avg_loss:0.014, val_acc:1.000]
Epoch [115/120    avg_loss:0.010, val_acc:1.000]
Epoch [116/120    avg_loss:0.014, val_acc:1.000]
Epoch [117/120    avg_loss:0.011, val_acc:1.000]
Epoch [118/120    avg_loss:0.012, val_acc:1.000]
Epoch [119/120    avg_loss:0.010, val_acc:1.000]
Epoch [120/120    avg_loss:0.021, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.996337   1.         1.         0.95633188 0.94326241
 0.98800959 1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9940657144310437
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb87728d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.259, val_acc:0.562]
Epoch [2/120    avg_loss:1.691, val_acc:0.673]
Epoch [3/120    avg_loss:1.282, val_acc:0.688]
Epoch [4/120    avg_loss:1.017, val_acc:0.746]
Epoch [5/120    avg_loss:0.865, val_acc:0.796]
Epoch [6/120    avg_loss:0.744, val_acc:0.821]
Epoch [7/120    avg_loss:0.710, val_acc:0.873]
Epoch [8/120    avg_loss:0.656, val_acc:0.865]
Epoch [9/120    avg_loss:0.614, val_acc:0.800]
Epoch [10/120    avg_loss:0.506, val_acc:0.863]
Epoch [11/120    avg_loss:0.488, val_acc:0.871]
Epoch [12/120    avg_loss:0.479, val_acc:0.877]
Epoch [13/120    avg_loss:0.468, val_acc:0.908]
Epoch [14/120    avg_loss:0.403, val_acc:0.904]
Epoch [15/120    avg_loss:0.390, val_acc:0.890]
Epoch [16/120    avg_loss:0.343, val_acc:0.942]
Epoch [17/120    avg_loss:0.353, val_acc:0.900]
Epoch [18/120    avg_loss:0.343, val_acc:0.935]
Epoch [19/120    avg_loss:0.249, val_acc:0.933]
Epoch [20/120    avg_loss:0.240, val_acc:0.952]
Epoch [21/120    avg_loss:0.250, val_acc:0.910]
Epoch [22/120    avg_loss:0.349, val_acc:0.933]
Epoch [23/120    avg_loss:0.270, val_acc:0.927]
Epoch [24/120    avg_loss:0.294, val_acc:0.912]
Epoch [25/120    avg_loss:0.251, val_acc:0.958]
Epoch [26/120    avg_loss:0.216, val_acc:0.925]
Epoch [27/120    avg_loss:0.193, val_acc:0.942]
Epoch [28/120    avg_loss:0.171, val_acc:0.965]
Epoch [29/120    avg_loss:0.156, val_acc:0.942]
Epoch [30/120    avg_loss:0.152, val_acc:0.969]
Epoch [31/120    avg_loss:0.155, val_acc:0.896]
Epoch [32/120    avg_loss:0.207, val_acc:0.958]
Epoch [33/120    avg_loss:0.143, val_acc:0.954]
Epoch [34/120    avg_loss:0.159, val_acc:0.929]
Epoch [35/120    avg_loss:0.179, val_acc:0.977]
Epoch [36/120    avg_loss:0.133, val_acc:0.952]
Epoch [37/120    avg_loss:0.173, val_acc:0.963]
Epoch [38/120    avg_loss:0.126, val_acc:0.973]
Epoch [39/120    avg_loss:0.091, val_acc:0.969]
Epoch [40/120    avg_loss:0.061, val_acc:0.969]
Epoch [41/120    avg_loss:0.139, val_acc:0.960]
Epoch [42/120    avg_loss:0.088, val_acc:0.983]
Epoch [43/120    avg_loss:0.076, val_acc:0.969]
Epoch [44/120    avg_loss:0.083, val_acc:0.969]
Epoch [45/120    avg_loss:0.093, val_acc:0.971]
Epoch [46/120    avg_loss:0.128, val_acc:0.954]
Epoch [47/120    avg_loss:0.167, val_acc:0.971]
Epoch [48/120    avg_loss:0.117, val_acc:0.971]
Epoch [49/120    avg_loss:0.111, val_acc:0.971]
Epoch [50/120    avg_loss:0.089, val_acc:0.973]
Epoch [51/120    avg_loss:0.086, val_acc:0.977]
Epoch [52/120    avg_loss:0.073, val_acc:0.988]
Epoch [53/120    avg_loss:0.043, val_acc:0.975]
Epoch [54/120    avg_loss:0.051, val_acc:0.977]
Epoch [55/120    avg_loss:0.069, val_acc:0.988]
Epoch [56/120    avg_loss:0.086, val_acc:0.971]
Epoch [57/120    avg_loss:0.071, val_acc:0.985]
Epoch [58/120    avg_loss:0.099, val_acc:0.954]
Epoch [59/120    avg_loss:0.112, val_acc:0.985]
Epoch [60/120    avg_loss:0.107, val_acc:0.981]
Epoch [61/120    avg_loss:0.099, val_acc:0.990]
Epoch [62/120    avg_loss:0.058, val_acc:0.981]
Epoch [63/120    avg_loss:0.041, val_acc:0.969]
Epoch [64/120    avg_loss:0.031, val_acc:0.981]
Epoch [65/120    avg_loss:0.044, val_acc:0.988]
Epoch [66/120    avg_loss:0.031, val_acc:0.994]
Epoch [67/120    avg_loss:0.035, val_acc:0.983]
Epoch [68/120    avg_loss:0.027, val_acc:0.994]
Epoch [69/120    avg_loss:0.028, val_acc:0.996]
Epoch [70/120    avg_loss:0.022, val_acc:0.985]
Epoch [71/120    avg_loss:0.048, val_acc:0.985]
Epoch [72/120    avg_loss:0.024, val_acc:0.996]
Epoch [73/120    avg_loss:0.028, val_acc:0.985]
Epoch [74/120    avg_loss:0.027, val_acc:0.996]
Epoch [75/120    avg_loss:0.047, val_acc:0.983]
Epoch [76/120    avg_loss:0.031, val_acc:0.994]
Epoch [77/120    avg_loss:0.021, val_acc:0.994]
Epoch [78/120    avg_loss:0.036, val_acc:0.992]
Epoch [79/120    avg_loss:0.019, val_acc:0.996]
Epoch [80/120    avg_loss:0.073, val_acc:0.950]
Epoch [81/120    avg_loss:0.138, val_acc:0.915]
Epoch [82/120    avg_loss:0.155, val_acc:0.952]
Epoch [83/120    avg_loss:0.092, val_acc:0.981]
Epoch [84/120    avg_loss:0.129, val_acc:0.971]
Epoch [85/120    avg_loss:0.102, val_acc:0.992]
Epoch [86/120    avg_loss:0.055, val_acc:0.990]
Epoch [87/120    avg_loss:0.042, val_acc:0.985]
Epoch [88/120    avg_loss:0.024, val_acc:0.994]
Epoch [89/120    avg_loss:0.035, val_acc:0.988]
Epoch [90/120    avg_loss:0.030, val_acc:0.990]
Epoch [91/120    avg_loss:0.022, val_acc:0.990]
Epoch [92/120    avg_loss:0.013, val_acc:0.996]
Epoch [93/120    avg_loss:0.014, val_acc:0.985]
Epoch [94/120    avg_loss:0.015, val_acc:0.996]
Epoch [95/120    avg_loss:0.010, val_acc:0.996]
Epoch [96/120    avg_loss:0.017, val_acc:0.994]
Epoch [97/120    avg_loss:0.020, val_acc:0.996]
Epoch [98/120    avg_loss:0.016, val_acc:0.996]
Epoch [99/120    avg_loss:0.083, val_acc:0.988]
Epoch [100/120    avg_loss:0.062, val_acc:0.988]
Epoch [101/120    avg_loss:0.050, val_acc:0.985]
Epoch [102/120    avg_loss:0.032, val_acc:0.983]
Epoch [103/120    avg_loss:0.021, val_acc:0.994]
Epoch [104/120    avg_loss:0.026, val_acc:0.985]
Epoch [105/120    avg_loss:0.027, val_acc:0.979]
Epoch [106/120    avg_loss:0.012, val_acc:0.992]
Epoch [107/120    avg_loss:0.016, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.996]
Epoch [109/120    avg_loss:0.012, val_acc:0.994]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.016, val_acc:0.998]
Epoch [112/120    avg_loss:0.008, val_acc:0.996]
Epoch [113/120    avg_loss:0.005, val_acc:0.996]
Epoch [114/120    avg_loss:0.007, val_acc:0.996]
Epoch [115/120    avg_loss:0.007, val_acc:0.996]
Epoch [116/120    avg_loss:0.006, val_acc:0.996]
Epoch [117/120    avg_loss:0.007, val_acc:0.996]
Epoch [118/120    avg_loss:0.011, val_acc:0.996]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   0   0   0   0   0   0   0   1   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99780541 0.9977221  1.         0.96170213 0.93772894
 0.99277108 0.99465241 1.         1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.9945401204220552
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f990ca01860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.228, val_acc:0.542]
Epoch [2/120    avg_loss:1.669, val_acc:0.650]
Epoch [3/120    avg_loss:1.273, val_acc:0.721]
Epoch [4/120    avg_loss:1.032, val_acc:0.750]
Epoch [5/120    avg_loss:0.883, val_acc:0.719]
Epoch [6/120    avg_loss:0.756, val_acc:0.765]
Epoch [7/120    avg_loss:0.713, val_acc:0.771]
Epoch [8/120    avg_loss:0.674, val_acc:0.775]
Epoch [9/120    avg_loss:0.640, val_acc:0.798]
Epoch [10/120    avg_loss:0.594, val_acc:0.838]
Epoch [11/120    avg_loss:0.516, val_acc:0.850]
Epoch [12/120    avg_loss:0.498, val_acc:0.867]
Epoch [13/120    avg_loss:0.460, val_acc:0.877]
Epoch [14/120    avg_loss:0.413, val_acc:0.933]
Epoch [15/120    avg_loss:0.452, val_acc:0.879]
Epoch [16/120    avg_loss:0.373, val_acc:0.931]
Epoch [17/120    avg_loss:0.379, val_acc:0.919]
Epoch [18/120    avg_loss:0.327, val_acc:0.896]
Epoch [19/120    avg_loss:0.367, val_acc:0.906]
Epoch [20/120    avg_loss:0.282, val_acc:0.931]
Epoch [21/120    avg_loss:0.259, val_acc:0.929]
Epoch [22/120    avg_loss:0.237, val_acc:0.923]
Epoch [23/120    avg_loss:0.327, val_acc:0.904]
Epoch [24/120    avg_loss:0.270, val_acc:0.946]
Epoch [25/120    avg_loss:0.205, val_acc:0.944]
Epoch [26/120    avg_loss:0.217, val_acc:0.933]
Epoch [27/120    avg_loss:0.180, val_acc:0.942]
Epoch [28/120    avg_loss:0.196, val_acc:0.950]
Epoch [29/120    avg_loss:0.219, val_acc:0.948]
Epoch [30/120    avg_loss:0.183, val_acc:0.944]
Epoch [31/120    avg_loss:0.195, val_acc:0.948]
Epoch [32/120    avg_loss:0.210, val_acc:0.946]
Epoch [33/120    avg_loss:0.225, val_acc:0.952]
Epoch [34/120    avg_loss:0.130, val_acc:0.960]
Epoch [35/120    avg_loss:0.135, val_acc:0.977]
Epoch [36/120    avg_loss:0.091, val_acc:0.973]
Epoch [37/120    avg_loss:0.153, val_acc:0.929]
Epoch [38/120    avg_loss:0.145, val_acc:0.981]
Epoch [39/120    avg_loss:0.126, val_acc:0.956]
Epoch [40/120    avg_loss:0.137, val_acc:0.981]
Epoch [41/120    avg_loss:0.093, val_acc:0.963]
Epoch [42/120    avg_loss:0.165, val_acc:0.965]
Epoch [43/120    avg_loss:0.127, val_acc:0.971]
Epoch [44/120    avg_loss:0.124, val_acc:0.973]
Epoch [45/120    avg_loss:0.091, val_acc:0.975]
Epoch [46/120    avg_loss:0.084, val_acc:0.971]
Epoch [47/120    avg_loss:0.066, val_acc:0.983]
Epoch [48/120    avg_loss:0.068, val_acc:0.988]
Epoch [49/120    avg_loss:0.063, val_acc:0.971]
Epoch [50/120    avg_loss:0.063, val_acc:0.975]
Epoch [51/120    avg_loss:0.076, val_acc:0.973]
Epoch [52/120    avg_loss:0.095, val_acc:0.969]
Epoch [53/120    avg_loss:0.127, val_acc:0.960]
Epoch [54/120    avg_loss:0.121, val_acc:0.969]
Epoch [55/120    avg_loss:0.116, val_acc:0.973]
Epoch [56/120    avg_loss:0.080, val_acc:0.981]
Epoch [57/120    avg_loss:0.056, val_acc:0.983]
Epoch [58/120    avg_loss:0.064, val_acc:0.994]
Epoch [59/120    avg_loss:0.058, val_acc:0.988]
Epoch [60/120    avg_loss:0.074, val_acc:0.977]
Epoch [61/120    avg_loss:0.122, val_acc:0.981]
Epoch [62/120    avg_loss:0.096, val_acc:0.973]
Epoch [63/120    avg_loss:0.051, val_acc:0.990]
Epoch [64/120    avg_loss:0.069, val_acc:0.979]
Epoch [65/120    avg_loss:0.073, val_acc:0.988]
Epoch [66/120    avg_loss:0.034, val_acc:0.994]
Epoch [67/120    avg_loss:0.041, val_acc:0.992]
Epoch [68/120    avg_loss:0.036, val_acc:0.996]
Epoch [69/120    avg_loss:0.027, val_acc:0.994]
Epoch [70/120    avg_loss:0.019, val_acc:0.996]
Epoch [71/120    avg_loss:0.019, val_acc:0.992]
Epoch [72/120    avg_loss:0.027, val_acc:0.996]
Epoch [73/120    avg_loss:0.064, val_acc:0.927]
Epoch [74/120    avg_loss:0.050, val_acc:0.988]
Epoch [75/120    avg_loss:0.043, val_acc:0.992]
Epoch [76/120    avg_loss:0.029, val_acc:0.988]
Epoch [77/120    avg_loss:0.038, val_acc:0.985]
Epoch [78/120    avg_loss:0.022, val_acc:0.994]
Epoch [79/120    avg_loss:0.014, val_acc:0.996]
Epoch [80/120    avg_loss:0.017, val_acc:0.994]
Epoch [81/120    avg_loss:0.030, val_acc:0.963]
Epoch [82/120    avg_loss:0.060, val_acc:0.990]
Epoch [83/120    avg_loss:0.035, val_acc:0.992]
Epoch [84/120    avg_loss:0.027, val_acc:0.994]
Epoch [85/120    avg_loss:0.016, val_acc:0.990]
Epoch [86/120    avg_loss:0.018, val_acc:0.992]
Epoch [87/120    avg_loss:0.025, val_acc:0.990]
Epoch [88/120    avg_loss:0.023, val_acc:0.992]
Epoch [89/120    avg_loss:0.020, val_acc:0.990]
Epoch [90/120    avg_loss:0.011, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.994]
Epoch [92/120    avg_loss:0.017, val_acc:0.998]
Epoch [93/120    avg_loss:0.031, val_acc:0.985]
Epoch [94/120    avg_loss:0.023, val_acc:0.992]
Epoch [95/120    avg_loss:0.021, val_acc:0.994]
Epoch [96/120    avg_loss:0.018, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.994]
Epoch [98/120    avg_loss:0.012, val_acc:0.994]
Epoch [99/120    avg_loss:0.011, val_acc:0.994]
Epoch [100/120    avg_loss:0.012, val_acc:0.992]
Epoch [101/120    avg_loss:0.007, val_acc:0.994]
Epoch [102/120    avg_loss:0.006, val_acc:0.994]
Epoch [103/120    avg_loss:0.009, val_acc:0.994]
Epoch [104/120    avg_loss:0.009, val_acc:0.994]
Epoch [105/120    avg_loss:0.014, val_acc:0.996]
Epoch [106/120    avg_loss:0.005, val_acc:0.994]
Epoch [107/120    avg_loss:0.008, val_acc:0.994]
Epoch [108/120    avg_loss:0.007, val_acc:0.994]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.007, val_acc:0.994]
Epoch [112/120    avg_loss:0.010, val_acc:0.994]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.005, val_acc:0.994]
Epoch [115/120    avg_loss:0.004, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.994]
Epoch [117/120    avg_loss:0.005, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.004, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.7228144989339

F1 scores:
[       nan 0.99926954 1.         1.         0.97379913 0.95804196
 1.         1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9969140260930558
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f672ead27f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.285, val_acc:0.633]
Epoch [2/120    avg_loss:1.709, val_acc:0.671]
Epoch [3/120    avg_loss:1.256, val_acc:0.744]
Epoch [4/120    avg_loss:1.051, val_acc:0.706]
Epoch [5/120    avg_loss:0.854, val_acc:0.767]
Epoch [6/120    avg_loss:0.701, val_acc:0.844]
Epoch [7/120    avg_loss:0.665, val_acc:0.848]
Epoch [8/120    avg_loss:0.572, val_acc:0.823]
Epoch [9/120    avg_loss:0.556, val_acc:0.844]
Epoch [10/120    avg_loss:0.585, val_acc:0.867]
Epoch [11/120    avg_loss:0.502, val_acc:0.856]
Epoch [12/120    avg_loss:0.595, val_acc:0.850]
Epoch [13/120    avg_loss:0.503, val_acc:0.904]
Epoch [14/120    avg_loss:0.462, val_acc:0.902]
Epoch [15/120    avg_loss:0.425, val_acc:0.927]
Epoch [16/120    avg_loss:0.446, val_acc:0.919]
Epoch [17/120    avg_loss:0.399, val_acc:0.925]
Epoch [18/120    avg_loss:0.409, val_acc:0.915]
Epoch [19/120    avg_loss:0.340, val_acc:0.906]
Epoch [20/120    avg_loss:0.378, val_acc:0.946]
Epoch [21/120    avg_loss:0.270, val_acc:0.950]
Epoch [22/120    avg_loss:0.266, val_acc:0.940]
Epoch [23/120    avg_loss:0.302, val_acc:0.956]
Epoch [24/120    avg_loss:0.250, val_acc:0.921]
Epoch [25/120    avg_loss:0.301, val_acc:0.948]
Epoch [26/120    avg_loss:0.208, val_acc:0.956]
Epoch [27/120    avg_loss:0.220, val_acc:0.912]
Epoch [28/120    avg_loss:0.311, val_acc:0.925]
Epoch [29/120    avg_loss:0.221, val_acc:0.963]
Epoch [30/120    avg_loss:0.209, val_acc:0.935]
Epoch [31/120    avg_loss:0.197, val_acc:0.960]
Epoch [32/120    avg_loss:0.205, val_acc:0.954]
Epoch [33/120    avg_loss:0.160, val_acc:0.960]
Epoch [34/120    avg_loss:0.204, val_acc:0.956]
Epoch [35/120    avg_loss:0.176, val_acc:0.960]
Epoch [36/120    avg_loss:0.186, val_acc:0.963]
Epoch [37/120    avg_loss:0.123, val_acc:0.975]
Epoch [38/120    avg_loss:0.131, val_acc:0.967]
Epoch [39/120    avg_loss:0.099, val_acc:0.973]
Epoch [40/120    avg_loss:0.142, val_acc:0.971]
Epoch [41/120    avg_loss:0.130, val_acc:0.960]
Epoch [42/120    avg_loss:0.128, val_acc:0.973]
Epoch [43/120    avg_loss:0.143, val_acc:0.973]
Epoch [44/120    avg_loss:0.104, val_acc:0.988]
Epoch [45/120    avg_loss:0.151, val_acc:0.981]
Epoch [46/120    avg_loss:0.088, val_acc:0.981]
Epoch [47/120    avg_loss:0.065, val_acc:0.985]
Epoch [48/120    avg_loss:0.062, val_acc:0.985]
Epoch [49/120    avg_loss:0.063, val_acc:0.975]
Epoch [50/120    avg_loss:0.071, val_acc:0.983]
Epoch [51/120    avg_loss:0.063, val_acc:0.971]
Epoch [52/120    avg_loss:0.070, val_acc:0.977]
Epoch [53/120    avg_loss:0.079, val_acc:0.990]
Epoch [54/120    avg_loss:0.062, val_acc:0.971]
Epoch [55/120    avg_loss:0.084, val_acc:0.979]
Epoch [56/120    avg_loss:0.079, val_acc:0.985]
Epoch [57/120    avg_loss:0.045, val_acc:0.973]
Epoch [58/120    avg_loss:0.066, val_acc:0.979]
Epoch [59/120    avg_loss:0.066, val_acc:0.985]
Epoch [60/120    avg_loss:0.053, val_acc:0.977]
Epoch [61/120    avg_loss:0.046, val_acc:0.977]
Epoch [62/120    avg_loss:0.040, val_acc:0.990]
Epoch [63/120    avg_loss:0.049, val_acc:0.990]
Epoch [64/120    avg_loss:0.032, val_acc:0.988]
Epoch [65/120    avg_loss:0.034, val_acc:0.985]
Epoch [66/120    avg_loss:0.034, val_acc:0.988]
Epoch [67/120    avg_loss:0.034, val_acc:0.990]
Epoch [68/120    avg_loss:0.085, val_acc:0.983]
Epoch [69/120    avg_loss:0.072, val_acc:0.977]
Epoch [70/120    avg_loss:0.071, val_acc:0.946]
Epoch [71/120    avg_loss:0.068, val_acc:0.981]
Epoch [72/120    avg_loss:0.102, val_acc:0.965]
Epoch [73/120    avg_loss:0.053, val_acc:0.988]
Epoch [74/120    avg_loss:0.037, val_acc:0.990]
Epoch [75/120    avg_loss:0.037, val_acc:0.988]
Epoch [76/120    avg_loss:0.039, val_acc:0.990]
Epoch [77/120    avg_loss:0.027, val_acc:0.992]
Epoch [78/120    avg_loss:0.036, val_acc:0.988]
Epoch [79/120    avg_loss:0.036, val_acc:0.985]
Epoch [80/120    avg_loss:0.040, val_acc:0.992]
Epoch [81/120    avg_loss:0.018, val_acc:0.992]
Epoch [82/120    avg_loss:0.027, val_acc:0.992]
Epoch [83/120    avg_loss:0.059, val_acc:0.988]
Epoch [84/120    avg_loss:0.053, val_acc:0.983]
Epoch [85/120    avg_loss:0.053, val_acc:0.979]
Epoch [86/120    avg_loss:0.098, val_acc:0.971]
Epoch [87/120    avg_loss:0.035, val_acc:0.983]
Epoch [88/120    avg_loss:0.025, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.994]
Epoch [90/120    avg_loss:0.020, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.996]
Epoch [92/120    avg_loss:0.011, val_acc:0.990]
Epoch [93/120    avg_loss:0.028, val_acc:0.996]
Epoch [94/120    avg_loss:0.023, val_acc:0.994]
Epoch [95/120    avg_loss:0.015, val_acc:0.988]
Epoch [96/120    avg_loss:0.014, val_acc:0.994]
Epoch [97/120    avg_loss:0.009, val_acc:0.994]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.994]
Epoch [100/120    avg_loss:0.008, val_acc:0.992]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.035, val_acc:0.983]
Epoch [106/120    avg_loss:0.021, val_acc:0.992]
Epoch [107/120    avg_loss:0.031, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.992]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.010, val_acc:0.994]
Epoch [112/120    avg_loss:0.008, val_acc:0.994]
Epoch [113/120    avg_loss:0.007, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.994]
Epoch [116/120    avg_loss:0.010, val_acc:0.994]
Epoch [117/120    avg_loss:0.010, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  10   0   0   0   0   0   0   2   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99707174 1.         1.         0.95555556 0.93835616
 0.99038462 1.         1.         1.         1.         0.99867198
 0.99669967 1.        ]

Kappa:
0.9940659297723918
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b02de07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.367, val_acc:0.490]
Epoch [2/120    avg_loss:1.902, val_acc:0.552]
Epoch [3/120    avg_loss:1.609, val_acc:0.635]
Epoch [4/120    avg_loss:1.322, val_acc:0.728]
Epoch [5/120    avg_loss:1.102, val_acc:0.776]
Epoch [6/120    avg_loss:0.950, val_acc:0.833]
Epoch [7/120    avg_loss:0.870, val_acc:0.784]
Epoch [8/120    avg_loss:0.789, val_acc:0.849]
Epoch [9/120    avg_loss:0.652, val_acc:0.889]
Epoch [10/120    avg_loss:0.560, val_acc:0.905]
Epoch [11/120    avg_loss:0.578, val_acc:0.865]
Epoch [12/120    avg_loss:0.498, val_acc:0.897]
Epoch [13/120    avg_loss:0.429, val_acc:0.919]
Epoch [14/120    avg_loss:0.424, val_acc:0.913]
Epoch [15/120    avg_loss:0.408, val_acc:0.881]
Epoch [16/120    avg_loss:0.424, val_acc:0.925]
Epoch [17/120    avg_loss:0.364, val_acc:0.940]
Epoch [18/120    avg_loss:0.365, val_acc:0.935]
Epoch [19/120    avg_loss:0.326, val_acc:0.911]
Epoch [20/120    avg_loss:0.326, val_acc:0.917]
Epoch [21/120    avg_loss:0.300, val_acc:0.940]
Epoch [22/120    avg_loss:0.251, val_acc:0.956]
Epoch [23/120    avg_loss:0.233, val_acc:0.948]
Epoch [24/120    avg_loss:0.303, val_acc:0.891]
Epoch [25/120    avg_loss:0.300, val_acc:0.917]
Epoch [26/120    avg_loss:0.257, val_acc:0.937]
Epoch [27/120    avg_loss:0.242, val_acc:0.956]
Epoch [28/120    avg_loss:0.244, val_acc:0.944]
Epoch [29/120    avg_loss:0.197, val_acc:0.962]
Epoch [30/120    avg_loss:0.275, val_acc:0.933]
Epoch [31/120    avg_loss:0.377, val_acc:0.956]
Epoch [32/120    avg_loss:0.214, val_acc:0.958]
Epoch [33/120    avg_loss:0.247, val_acc:0.950]
Epoch [34/120    avg_loss:0.216, val_acc:0.948]
Epoch [35/120    avg_loss:0.191, val_acc:0.968]
Epoch [36/120    avg_loss:0.177, val_acc:0.962]
Epoch [37/120    avg_loss:0.221, val_acc:0.952]
Epoch [38/120    avg_loss:0.202, val_acc:0.954]
Epoch [39/120    avg_loss:0.132, val_acc:0.964]
Epoch [40/120    avg_loss:0.152, val_acc:0.966]
Epoch [41/120    avg_loss:0.134, val_acc:0.950]
Epoch [42/120    avg_loss:0.123, val_acc:0.946]
Epoch [43/120    avg_loss:0.146, val_acc:0.970]
Epoch [44/120    avg_loss:0.123, val_acc:0.960]
Epoch [45/120    avg_loss:0.110, val_acc:0.964]
Epoch [46/120    avg_loss:0.150, val_acc:0.964]
Epoch [47/120    avg_loss:0.137, val_acc:0.964]
Epoch [48/120    avg_loss:0.102, val_acc:0.964]
Epoch [49/120    avg_loss:0.136, val_acc:0.964]
Epoch [50/120    avg_loss:0.125, val_acc:0.964]
Epoch [51/120    avg_loss:0.114, val_acc:0.978]
Epoch [52/120    avg_loss:0.146, val_acc:0.968]
Epoch [53/120    avg_loss:0.178, val_acc:0.942]
Epoch [54/120    avg_loss:0.182, val_acc:0.956]
Epoch [55/120    avg_loss:0.139, val_acc:0.950]
Epoch [56/120    avg_loss:0.119, val_acc:0.970]
Epoch [57/120    avg_loss:0.100, val_acc:0.968]
Epoch [58/120    avg_loss:0.084, val_acc:0.970]
Epoch [59/120    avg_loss:0.175, val_acc:0.968]
Epoch [60/120    avg_loss:0.118, val_acc:0.948]
Epoch [61/120    avg_loss:0.128, val_acc:0.950]
Epoch [62/120    avg_loss:0.124, val_acc:0.966]
Epoch [63/120    avg_loss:0.095, val_acc:0.982]
Epoch [64/120    avg_loss:0.060, val_acc:0.980]
Epoch [65/120    avg_loss:0.082, val_acc:0.972]
Epoch [66/120    avg_loss:0.089, val_acc:0.970]
Epoch [67/120    avg_loss:0.070, val_acc:0.972]
Epoch [68/120    avg_loss:0.063, val_acc:0.970]
Epoch [69/120    avg_loss:0.088, val_acc:0.942]
Epoch [70/120    avg_loss:0.074, val_acc:0.984]
Epoch [71/120    avg_loss:0.054, val_acc:0.974]
Epoch [72/120    avg_loss:0.069, val_acc:0.982]
Epoch [73/120    avg_loss:0.064, val_acc:0.980]
Epoch [74/120    avg_loss:0.079, val_acc:0.974]
Epoch [75/120    avg_loss:0.083, val_acc:0.978]
Epoch [76/120    avg_loss:0.083, val_acc:0.966]
Epoch [77/120    avg_loss:0.105, val_acc:0.974]
Epoch [78/120    avg_loss:0.120, val_acc:0.958]
Epoch [79/120    avg_loss:0.123, val_acc:0.960]
Epoch [80/120    avg_loss:0.075, val_acc:0.978]
Epoch [81/120    avg_loss:0.080, val_acc:0.964]
Epoch [82/120    avg_loss:0.059, val_acc:0.976]
Epoch [83/120    avg_loss:0.085, val_acc:0.976]
Epoch [84/120    avg_loss:0.051, val_acc:0.976]
Epoch [85/120    avg_loss:0.032, val_acc:0.982]
Epoch [86/120    avg_loss:0.029, val_acc:0.980]
Epoch [87/120    avg_loss:0.044, val_acc:0.980]
Epoch [88/120    avg_loss:0.036, val_acc:0.980]
Epoch [89/120    avg_loss:0.049, val_acc:0.980]
Epoch [90/120    avg_loss:0.034, val_acc:0.982]
Epoch [91/120    avg_loss:0.034, val_acc:0.984]
Epoch [92/120    avg_loss:0.030, val_acc:0.984]
Epoch [93/120    avg_loss:0.041, val_acc:0.984]
Epoch [94/120    avg_loss:0.045, val_acc:0.982]
Epoch [95/120    avg_loss:0.036, val_acc:0.984]
Epoch [96/120    avg_loss:0.044, val_acc:0.984]
Epoch [97/120    avg_loss:0.034, val_acc:0.984]
Epoch [98/120    avg_loss:0.034, val_acc:0.984]
Epoch [99/120    avg_loss:0.024, val_acc:0.988]
Epoch [100/120    avg_loss:0.024, val_acc:0.986]
Epoch [101/120    avg_loss:0.022, val_acc:0.986]
Epoch [102/120    avg_loss:0.033, val_acc:0.984]
Epoch [103/120    avg_loss:0.031, val_acc:0.986]
Epoch [104/120    avg_loss:0.020, val_acc:0.986]
Epoch [105/120    avg_loss:0.033, val_acc:0.986]
Epoch [106/120    avg_loss:0.025, val_acc:0.988]
Epoch [107/120    avg_loss:0.029, val_acc:0.984]
Epoch [108/120    avg_loss:0.036, val_acc:0.986]
Epoch [109/120    avg_loss:0.034, val_acc:0.988]
Epoch [110/120    avg_loss:0.035, val_acc:0.988]
Epoch [111/120    avg_loss:0.027, val_acc:0.988]
Epoch [112/120    avg_loss:0.038, val_acc:0.986]
Epoch [113/120    avg_loss:0.033, val_acc:0.988]
Epoch [114/120    avg_loss:0.029, val_acc:0.988]
Epoch [115/120    avg_loss:0.031, val_acc:0.988]
Epoch [116/120    avg_loss:0.042, val_acc:0.984]
Epoch [117/120    avg_loss:0.027, val_acc:0.986]
Epoch [118/120    avg_loss:0.028, val_acc:0.990]
Epoch [119/120    avg_loss:0.040, val_acc:0.986]
Epoch [120/120    avg_loss:0.023, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.95730337 0.98678414 0.93886463 0.92465753
 1.         0.9010989  1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9888428830115824
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15f507b7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.396, val_acc:0.462]
Epoch [2/120    avg_loss:1.905, val_acc:0.615]
Epoch [3/120    avg_loss:1.532, val_acc:0.724]
Epoch [4/120    avg_loss:1.227, val_acc:0.736]
Epoch [5/120    avg_loss:1.039, val_acc:0.808]
Epoch [6/120    avg_loss:0.915, val_acc:0.806]
Epoch [7/120    avg_loss:0.860, val_acc:0.839]
Epoch [8/120    avg_loss:0.785, val_acc:0.869]
Epoch [9/120    avg_loss:0.671, val_acc:0.871]
Epoch [10/120    avg_loss:0.666, val_acc:0.802]
Epoch [11/120    avg_loss:0.545, val_acc:0.885]
Epoch [12/120    avg_loss:0.535, val_acc:0.881]
Epoch [13/120    avg_loss:0.536, val_acc:0.837]
Epoch [14/120    avg_loss:0.535, val_acc:0.867]
Epoch [15/120    avg_loss:0.530, val_acc:0.869]
Epoch [16/120    avg_loss:0.490, val_acc:0.885]
Epoch [17/120    avg_loss:0.463, val_acc:0.913]
Epoch [18/120    avg_loss:0.422, val_acc:0.921]
Epoch [19/120    avg_loss:0.399, val_acc:0.915]
Epoch [20/120    avg_loss:0.346, val_acc:0.935]
Epoch [21/120    avg_loss:0.362, val_acc:0.925]
Epoch [22/120    avg_loss:0.332, val_acc:0.917]
Epoch [23/120    avg_loss:0.393, val_acc:0.919]
Epoch [24/120    avg_loss:0.364, val_acc:0.927]
Epoch [25/120    avg_loss:0.332, val_acc:0.871]
Epoch [26/120    avg_loss:0.365, val_acc:0.897]
Epoch [27/120    avg_loss:0.322, val_acc:0.954]
Epoch [28/120    avg_loss:0.276, val_acc:0.933]
Epoch [29/120    avg_loss:0.294, val_acc:0.885]
Epoch [30/120    avg_loss:0.319, val_acc:0.917]
Epoch [31/120    avg_loss:0.254, val_acc:0.901]
Epoch [32/120    avg_loss:0.278, val_acc:0.938]
Epoch [33/120    avg_loss:0.285, val_acc:0.931]
Epoch [34/120    avg_loss:0.384, val_acc:0.921]
Epoch [35/120    avg_loss:0.248, val_acc:0.942]
Epoch [36/120    avg_loss:0.243, val_acc:0.942]
Epoch [37/120    avg_loss:0.266, val_acc:0.952]
Epoch [38/120    avg_loss:0.262, val_acc:0.946]
Epoch [39/120    avg_loss:0.230, val_acc:0.927]
Epoch [40/120    avg_loss:0.277, val_acc:0.942]
Epoch [41/120    avg_loss:0.190, val_acc:0.962]
Epoch [42/120    avg_loss:0.158, val_acc:0.968]
Epoch [43/120    avg_loss:0.134, val_acc:0.970]
Epoch [44/120    avg_loss:0.132, val_acc:0.972]
Epoch [45/120    avg_loss:0.116, val_acc:0.970]
Epoch [46/120    avg_loss:0.142, val_acc:0.970]
Epoch [47/120    avg_loss:0.140, val_acc:0.970]
Epoch [48/120    avg_loss:0.137, val_acc:0.970]
Epoch [49/120    avg_loss:0.126, val_acc:0.972]
Epoch [50/120    avg_loss:0.127, val_acc:0.974]
Epoch [51/120    avg_loss:0.120, val_acc:0.972]
Epoch [52/120    avg_loss:0.119, val_acc:0.974]
Epoch [53/120    avg_loss:0.101, val_acc:0.976]
Epoch [54/120    avg_loss:0.124, val_acc:0.974]
Epoch [55/120    avg_loss:0.100, val_acc:0.974]
Epoch [56/120    avg_loss:0.100, val_acc:0.978]
Epoch [57/120    avg_loss:0.111, val_acc:0.974]
Epoch [58/120    avg_loss:0.092, val_acc:0.974]
Epoch [59/120    avg_loss:0.098, val_acc:0.974]
Epoch [60/120    avg_loss:0.127, val_acc:0.980]
Epoch [61/120    avg_loss:0.105, val_acc:0.976]
Epoch [62/120    avg_loss:0.099, val_acc:0.974]
Epoch [63/120    avg_loss:0.107, val_acc:0.974]
Epoch [64/120    avg_loss:0.103, val_acc:0.978]
Epoch [65/120    avg_loss:0.093, val_acc:0.980]
Epoch [66/120    avg_loss:0.105, val_acc:0.978]
Epoch [67/120    avg_loss:0.111, val_acc:0.976]
Epoch [68/120    avg_loss:0.102, val_acc:0.978]
Epoch [69/120    avg_loss:0.113, val_acc:0.982]
Epoch [70/120    avg_loss:0.078, val_acc:0.982]
Epoch [71/120    avg_loss:0.103, val_acc:0.980]
Epoch [72/120    avg_loss:0.088, val_acc:0.978]
Epoch [73/120    avg_loss:0.089, val_acc:0.982]
Epoch [74/120    avg_loss:0.098, val_acc:0.980]
Epoch [75/120    avg_loss:0.079, val_acc:0.982]
Epoch [76/120    avg_loss:0.099, val_acc:0.980]
Epoch [77/120    avg_loss:0.100, val_acc:0.978]
Epoch [78/120    avg_loss:0.099, val_acc:0.978]
Epoch [79/120    avg_loss:0.083, val_acc:0.982]
Epoch [80/120    avg_loss:0.096, val_acc:0.982]
Epoch [81/120    avg_loss:0.096, val_acc:0.978]
Epoch [82/120    avg_loss:0.108, val_acc:0.982]
Epoch [83/120    avg_loss:0.088, val_acc:0.984]
Epoch [84/120    avg_loss:0.083, val_acc:0.982]
Epoch [85/120    avg_loss:0.077, val_acc:0.984]
Epoch [86/120    avg_loss:0.087, val_acc:0.986]
Epoch [87/120    avg_loss:0.079, val_acc:0.984]
Epoch [88/120    avg_loss:0.085, val_acc:0.980]
Epoch [89/120    avg_loss:0.095, val_acc:0.980]
Epoch [90/120    avg_loss:0.113, val_acc:0.984]
Epoch [91/120    avg_loss:0.099, val_acc:0.984]
Epoch [92/120    avg_loss:0.080, val_acc:0.982]
Epoch [93/120    avg_loss:0.075, val_acc:0.984]
Epoch [94/120    avg_loss:0.072, val_acc:0.986]
Epoch [95/120    avg_loss:0.091, val_acc:0.984]
Epoch [96/120    avg_loss:0.083, val_acc:0.984]
Epoch [97/120    avg_loss:0.066, val_acc:0.982]
Epoch [98/120    avg_loss:0.074, val_acc:0.984]
Epoch [99/120    avg_loss:0.082, val_acc:0.986]
Epoch [100/120    avg_loss:0.074, val_acc:0.988]
Epoch [101/120    avg_loss:0.096, val_acc:0.986]
Epoch [102/120    avg_loss:0.091, val_acc:0.984]
Epoch [103/120    avg_loss:0.083, val_acc:0.980]
Epoch [104/120    avg_loss:0.084, val_acc:0.984]
Epoch [105/120    avg_loss:0.077, val_acc:0.982]
Epoch [106/120    avg_loss:0.070, val_acc:0.982]
Epoch [107/120    avg_loss:0.074, val_acc:0.984]
Epoch [108/120    avg_loss:0.100, val_acc:0.984]
Epoch [109/120    avg_loss:0.062, val_acc:0.982]
Epoch [110/120    avg_loss:0.076, val_acc:0.982]
Epoch [111/120    avg_loss:0.069, val_acc:0.984]
Epoch [112/120    avg_loss:0.085, val_acc:0.978]
Epoch [113/120    avg_loss:0.068, val_acc:0.986]
Epoch [114/120    avg_loss:0.074, val_acc:0.984]
Epoch [115/120    avg_loss:0.064, val_acc:0.984]
Epoch [116/120    avg_loss:0.060, val_acc:0.984]
Epoch [117/120    avg_loss:0.065, val_acc:0.982]
Epoch [118/120    avg_loss:0.063, val_acc:0.982]
Epoch [119/120    avg_loss:0.071, val_acc:0.982]
Epoch [120/120    avg_loss:0.081, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 221   7   0   0   0   1   1   0   0   0   0]
 [  0   0   1   0 204  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0   0 383   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   4   1   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 0.997815   0.94347826 0.98004435 0.91891892 0.90849673
 0.99266504 0.88372093 0.99222798 0.9946865  0.99862826 0.99867198
 0.99334812 1.        ]

Kappa:
0.9829070696942872
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd37b5f07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.340, val_acc:0.488]
Epoch [2/120    avg_loss:1.846, val_acc:0.677]
Epoch [3/120    avg_loss:1.501, val_acc:0.722]
Epoch [4/120    avg_loss:1.181, val_acc:0.742]
Epoch [5/120    avg_loss:1.012, val_acc:0.823]
Epoch [6/120    avg_loss:0.883, val_acc:0.782]
Epoch [7/120    avg_loss:0.759, val_acc:0.829]
Epoch [8/120    avg_loss:0.685, val_acc:0.869]
Epoch [9/120    avg_loss:0.633, val_acc:0.853]
Epoch [10/120    avg_loss:0.625, val_acc:0.891]
Epoch [11/120    avg_loss:0.558, val_acc:0.891]
Epoch [12/120    avg_loss:0.585, val_acc:0.903]
Epoch [13/120    avg_loss:0.446, val_acc:0.911]
Epoch [14/120    avg_loss:0.459, val_acc:0.911]
Epoch [15/120    avg_loss:0.425, val_acc:0.938]
Epoch [16/120    avg_loss:0.379, val_acc:0.905]
Epoch [17/120    avg_loss:0.450, val_acc:0.835]
Epoch [18/120    avg_loss:0.404, val_acc:0.937]
Epoch [19/120    avg_loss:0.361, val_acc:0.942]
Epoch [20/120    avg_loss:0.357, val_acc:0.925]
Epoch [21/120    avg_loss:0.416, val_acc:0.907]
Epoch [22/120    avg_loss:0.359, val_acc:0.940]
Epoch [23/120    avg_loss:0.337, val_acc:0.935]
Epoch [24/120    avg_loss:0.294, val_acc:0.901]
Epoch [25/120    avg_loss:0.423, val_acc:0.937]
Epoch [26/120    avg_loss:0.329, val_acc:0.933]
Epoch [27/120    avg_loss:0.293, val_acc:0.929]
Epoch [28/120    avg_loss:0.317, val_acc:0.938]
Epoch [29/120    avg_loss:0.304, val_acc:0.950]
Epoch [30/120    avg_loss:0.221, val_acc:0.950]
Epoch [31/120    avg_loss:0.195, val_acc:0.952]
Epoch [32/120    avg_loss:0.217, val_acc:0.952]
Epoch [33/120    avg_loss:0.280, val_acc:0.950]
Epoch [34/120    avg_loss:0.222, val_acc:0.964]
Epoch [35/120    avg_loss:0.294, val_acc:0.937]
Epoch [36/120    avg_loss:0.296, val_acc:0.950]
Epoch [37/120    avg_loss:0.229, val_acc:0.952]
Epoch [38/120    avg_loss:0.198, val_acc:0.966]
Epoch [39/120    avg_loss:0.198, val_acc:0.950]
Epoch [40/120    avg_loss:0.245, val_acc:0.952]
Epoch [41/120    avg_loss:0.156, val_acc:0.948]
Epoch [42/120    avg_loss:0.263, val_acc:0.901]
Epoch [43/120    avg_loss:0.242, val_acc:0.960]
Epoch [44/120    avg_loss:0.254, val_acc:0.938]
Epoch [45/120    avg_loss:0.244, val_acc:0.923]
Epoch [46/120    avg_loss:0.247, val_acc:0.929]
Epoch [47/120    avg_loss:0.168, val_acc:0.972]
Epoch [48/120    avg_loss:0.148, val_acc:0.966]
Epoch [49/120    avg_loss:0.188, val_acc:0.937]
Epoch [50/120    avg_loss:0.237, val_acc:0.938]
Epoch [51/120    avg_loss:0.200, val_acc:0.956]
Epoch [52/120    avg_loss:0.227, val_acc:0.960]
Epoch [53/120    avg_loss:0.168, val_acc:0.962]
Epoch [54/120    avg_loss:0.139, val_acc:0.970]
Epoch [55/120    avg_loss:0.183, val_acc:0.946]
Epoch [56/120    avg_loss:0.221, val_acc:0.970]
Epoch [57/120    avg_loss:0.168, val_acc:0.970]
Epoch [58/120    avg_loss:0.104, val_acc:0.966]
Epoch [59/120    avg_loss:0.108, val_acc:0.974]
Epoch [60/120    avg_loss:0.144, val_acc:0.956]
Epoch [61/120    avg_loss:0.111, val_acc:0.962]
Epoch [62/120    avg_loss:0.088, val_acc:0.972]
Epoch [63/120    avg_loss:0.138, val_acc:0.921]
Epoch [64/120    avg_loss:0.138, val_acc:0.964]
Epoch [65/120    avg_loss:0.153, val_acc:0.946]
Epoch [66/120    avg_loss:0.124, val_acc:0.974]
Epoch [67/120    avg_loss:0.103, val_acc:0.972]
Epoch [68/120    avg_loss:0.125, val_acc:0.948]
Epoch [69/120    avg_loss:0.110, val_acc:0.978]
Epoch [70/120    avg_loss:0.109, val_acc:0.964]
Epoch [71/120    avg_loss:0.102, val_acc:0.970]
Epoch [72/120    avg_loss:0.098, val_acc:0.972]
Epoch [73/120    avg_loss:0.090, val_acc:0.972]
Epoch [74/120    avg_loss:0.106, val_acc:0.976]
Epoch [75/120    avg_loss:0.120, val_acc:0.909]
Epoch [76/120    avg_loss:0.091, val_acc:0.950]
Epoch [77/120    avg_loss:0.131, val_acc:0.978]
Epoch [78/120    avg_loss:0.071, val_acc:0.976]
Epoch [79/120    avg_loss:0.058, val_acc:0.976]
Epoch [80/120    avg_loss:0.064, val_acc:0.982]
Epoch [81/120    avg_loss:0.057, val_acc:0.976]
Epoch [82/120    avg_loss:0.101, val_acc:0.966]
Epoch [83/120    avg_loss:0.112, val_acc:0.968]
Epoch [84/120    avg_loss:0.080, val_acc:0.976]
Epoch [85/120    avg_loss:0.086, val_acc:0.970]
Epoch [86/120    avg_loss:0.086, val_acc:0.974]
Epoch [87/120    avg_loss:0.046, val_acc:0.980]
Epoch [88/120    avg_loss:0.069, val_acc:0.974]
Epoch [89/120    avg_loss:0.055, val_acc:0.978]
Epoch [90/120    avg_loss:0.119, val_acc:0.954]
Epoch [91/120    avg_loss:0.091, val_acc:0.966]
Epoch [92/120    avg_loss:0.094, val_acc:0.970]
Epoch [93/120    avg_loss:0.066, val_acc:0.976]
Epoch [94/120    avg_loss:0.064, val_acc:0.974]
Epoch [95/120    avg_loss:0.068, val_acc:0.976]
Epoch [96/120    avg_loss:0.038, val_acc:0.978]
Epoch [97/120    avg_loss:0.033, val_acc:0.976]
Epoch [98/120    avg_loss:0.039, val_acc:0.980]
Epoch [99/120    avg_loss:0.042, val_acc:0.980]
Epoch [100/120    avg_loss:0.041, val_acc:0.980]
Epoch [101/120    avg_loss:0.042, val_acc:0.980]
Epoch [102/120    avg_loss:0.041, val_acc:0.982]
Epoch [103/120    avg_loss:0.025, val_acc:0.982]
Epoch [104/120    avg_loss:0.042, val_acc:0.982]
Epoch [105/120    avg_loss:0.028, val_acc:0.980]
Epoch [106/120    avg_loss:0.034, val_acc:0.980]
Epoch [107/120    avg_loss:0.039, val_acc:0.980]
Epoch [108/120    avg_loss:0.043, val_acc:0.980]
Epoch [109/120    avg_loss:0.030, val_acc:0.980]
Epoch [110/120    avg_loss:0.035, val_acc:0.980]
Epoch [111/120    avg_loss:0.034, val_acc:0.980]
Epoch [112/120    avg_loss:0.040, val_acc:0.980]
Epoch [113/120    avg_loss:0.031, val_acc:0.980]
Epoch [114/120    avg_loss:0.028, val_acc:0.980]
Epoch [115/120    avg_loss:0.043, val_acc:0.980]
Epoch [116/120    avg_loss:0.046, val_acc:0.980]
Epoch [117/120    avg_loss:0.026, val_acc:0.980]
Epoch [118/120    avg_loss:0.032, val_acc:0.980]
Epoch [119/120    avg_loss:0.025, val_acc:0.980]
Epoch [120/120    avg_loss:0.030, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   6   1   0   0   0   0   0]
 [  0   0   0 214  14   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.61407249466951

F1 scores:
[       nan 1.         0.95927602 0.96396396 0.90644491 0.88808664
 1.         0.91304348 0.998713   0.9978678  1.         1.
 0.99889503 1.        ]

Kappa:
0.9845686941146665
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f7e7e2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.421, val_acc:0.458]
Epoch [2/120    avg_loss:1.838, val_acc:0.631]
Epoch [3/120    avg_loss:1.461, val_acc:0.665]
Epoch [4/120    avg_loss:1.193, val_acc:0.746]
Epoch [5/120    avg_loss:1.023, val_acc:0.760]
Epoch [6/120    avg_loss:0.937, val_acc:0.786]
Epoch [7/120    avg_loss:0.864, val_acc:0.819]
Epoch [8/120    avg_loss:0.781, val_acc:0.841]
Epoch [9/120    avg_loss:0.727, val_acc:0.819]
Epoch [10/120    avg_loss:0.664, val_acc:0.871]
Epoch [11/120    avg_loss:0.615, val_acc:0.863]
Epoch [12/120    avg_loss:0.531, val_acc:0.871]
Epoch [13/120    avg_loss:0.518, val_acc:0.883]
Epoch [14/120    avg_loss:0.505, val_acc:0.869]
Epoch [15/120    avg_loss:0.399, val_acc:0.885]
Epoch [16/120    avg_loss:0.397, val_acc:0.879]
Epoch [17/120    avg_loss:0.385, val_acc:0.913]
Epoch [18/120    avg_loss:0.419, val_acc:0.895]
Epoch [19/120    avg_loss:0.362, val_acc:0.901]
Epoch [20/120    avg_loss:0.425, val_acc:0.899]
Epoch [21/120    avg_loss:0.393, val_acc:0.923]
Epoch [22/120    avg_loss:0.354, val_acc:0.901]
Epoch [23/120    avg_loss:0.309, val_acc:0.921]
Epoch [24/120    avg_loss:0.291, val_acc:0.929]
Epoch [25/120    avg_loss:0.303, val_acc:0.843]
Epoch [26/120    avg_loss:0.333, val_acc:0.923]
Epoch [27/120    avg_loss:0.260, val_acc:0.925]
Epoch [28/120    avg_loss:0.330, val_acc:0.933]
Epoch [29/120    avg_loss:0.270, val_acc:0.929]
Epoch [30/120    avg_loss:0.218, val_acc:0.942]
Epoch [31/120    avg_loss:0.243, val_acc:0.923]
Epoch [32/120    avg_loss:0.330, val_acc:0.929]
Epoch [33/120    avg_loss:0.267, val_acc:0.940]
Epoch [34/120    avg_loss:0.277, val_acc:0.942]
Epoch [35/120    avg_loss:0.285, val_acc:0.940]
Epoch [36/120    avg_loss:0.215, val_acc:0.933]
Epoch [37/120    avg_loss:0.181, val_acc:0.948]
Epoch [38/120    avg_loss:0.159, val_acc:0.942]
Epoch [39/120    avg_loss:0.176, val_acc:0.954]
Epoch [40/120    avg_loss:0.220, val_acc:0.944]
Epoch [41/120    avg_loss:0.202, val_acc:0.935]
Epoch [42/120    avg_loss:0.185, val_acc:0.938]
Epoch [43/120    avg_loss:0.202, val_acc:0.948]
Epoch [44/120    avg_loss:0.302, val_acc:0.917]
Epoch [45/120    avg_loss:0.206, val_acc:0.946]
Epoch [46/120    avg_loss:0.196, val_acc:0.933]
Epoch [47/120    avg_loss:0.223, val_acc:0.929]
Epoch [48/120    avg_loss:0.181, val_acc:0.950]
Epoch [49/120    avg_loss:0.183, val_acc:0.948]
Epoch [50/120    avg_loss:0.131, val_acc:0.954]
Epoch [51/120    avg_loss:0.168, val_acc:0.954]
Epoch [52/120    avg_loss:0.237, val_acc:0.933]
Epoch [53/120    avg_loss:0.178, val_acc:0.954]
Epoch [54/120    avg_loss:0.203, val_acc:0.948]
Epoch [55/120    avg_loss:0.121, val_acc:0.962]
Epoch [56/120    avg_loss:0.104, val_acc:0.960]
Epoch [57/120    avg_loss:0.123, val_acc:0.935]
Epoch [58/120    avg_loss:0.147, val_acc:0.956]
Epoch [59/120    avg_loss:0.173, val_acc:0.960]
Epoch [60/120    avg_loss:0.120, val_acc:0.968]
Epoch [61/120    avg_loss:0.082, val_acc:0.935]
Epoch [62/120    avg_loss:0.130, val_acc:0.925]
Epoch [63/120    avg_loss:0.140, val_acc:0.956]
Epoch [64/120    avg_loss:0.133, val_acc:0.952]
Epoch [65/120    avg_loss:0.111, val_acc:0.958]
Epoch [66/120    avg_loss:0.084, val_acc:0.972]
Epoch [67/120    avg_loss:0.073, val_acc:0.964]
Epoch [68/120    avg_loss:0.115, val_acc:0.966]
Epoch [69/120    avg_loss:0.080, val_acc:0.962]
Epoch [70/120    avg_loss:0.070, val_acc:0.974]
Epoch [71/120    avg_loss:0.071, val_acc:0.968]
Epoch [72/120    avg_loss:0.076, val_acc:0.962]
Epoch [73/120    avg_loss:0.072, val_acc:0.950]
Epoch [74/120    avg_loss:0.081, val_acc:0.974]
Epoch [75/120    avg_loss:0.071, val_acc:0.972]
Epoch [76/120    avg_loss:0.056, val_acc:0.960]
Epoch [77/120    avg_loss:0.086, val_acc:0.970]
Epoch [78/120    avg_loss:0.056, val_acc:0.966]
Epoch [79/120    avg_loss:0.084, val_acc:0.976]
Epoch [80/120    avg_loss:0.080, val_acc:0.966]
Epoch [81/120    avg_loss:0.088, val_acc:0.970]
Epoch [82/120    avg_loss:0.113, val_acc:0.946]
Epoch [83/120    avg_loss:0.116, val_acc:0.972]
Epoch [84/120    avg_loss:0.074, val_acc:0.972]
Epoch [85/120    avg_loss:0.082, val_acc:0.946]
Epoch [86/120    avg_loss:0.091, val_acc:0.972]
Epoch [87/120    avg_loss:0.071, val_acc:0.970]
Epoch [88/120    avg_loss:0.075, val_acc:0.976]
Epoch [89/120    avg_loss:0.059, val_acc:0.970]
Epoch [90/120    avg_loss:0.047, val_acc:0.974]
Epoch [91/120    avg_loss:0.066, val_acc:0.978]
Epoch [92/120    avg_loss:0.067, val_acc:0.970]
Epoch [93/120    avg_loss:0.056, val_acc:0.964]
Epoch [94/120    avg_loss:0.105, val_acc:0.935]
Epoch [95/120    avg_loss:0.109, val_acc:0.962]
Epoch [96/120    avg_loss:0.070, val_acc:0.966]
Epoch [97/120    avg_loss:0.061, val_acc:0.960]
Epoch [98/120    avg_loss:0.078, val_acc:0.974]
Epoch [99/120    avg_loss:0.051, val_acc:0.978]
Epoch [100/120    avg_loss:0.037, val_acc:0.978]
Epoch [101/120    avg_loss:0.040, val_acc:0.972]
Epoch [102/120    avg_loss:0.048, val_acc:0.962]
Epoch [103/120    avg_loss:0.162, val_acc:0.970]
Epoch [104/120    avg_loss:0.075, val_acc:0.966]
Epoch [105/120    avg_loss:0.041, val_acc:0.970]
Epoch [106/120    avg_loss:0.040, val_acc:0.956]
Epoch [107/120    avg_loss:0.072, val_acc:0.970]
Epoch [108/120    avg_loss:0.063, val_acc:0.962]
Epoch [109/120    avg_loss:0.093, val_acc:0.966]
Epoch [110/120    avg_loss:0.050, val_acc:0.964]
Epoch [111/120    avg_loss:0.056, val_acc:0.962]
Epoch [112/120    avg_loss:0.061, val_acc:0.964]
Epoch [113/120    avg_loss:0.057, val_acc:0.980]
Epoch [114/120    avg_loss:0.040, val_acc:0.976]
Epoch [115/120    avg_loss:0.053, val_acc:0.960]
Epoch [116/120    avg_loss:0.048, val_acc:0.980]
Epoch [117/120    avg_loss:0.051, val_acc:0.968]
Epoch [118/120    avg_loss:0.056, val_acc:0.966]
Epoch [119/120    avg_loss:0.043, val_acc:0.980]
Epoch [120/120    avg_loss:0.040, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 215  10   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 226   0   0   0   0   0   0   0   1   0]
 [  0   0   0   0  34 111   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 1.         0.97471264 0.96629213 0.90945674 0.8671875
 1.         0.94791667 0.99487179 0.99893276 1.         1.
 0.99779249 1.        ]

Kappa:
0.9855174442179374
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b3143d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.361, val_acc:0.563]
Epoch [2/120    avg_loss:1.887, val_acc:0.611]
Epoch [3/120    avg_loss:1.530, val_acc:0.776]
Epoch [4/120    avg_loss:1.274, val_acc:0.802]
Epoch [5/120    avg_loss:1.058, val_acc:0.794]
Epoch [6/120    avg_loss:0.917, val_acc:0.873]
Epoch [7/120    avg_loss:0.792, val_acc:0.857]
Epoch [8/120    avg_loss:0.672, val_acc:0.871]
Epoch [9/120    avg_loss:0.677, val_acc:0.883]
Epoch [10/120    avg_loss:0.606, val_acc:0.865]
Epoch [11/120    avg_loss:0.600, val_acc:0.887]
Epoch [12/120    avg_loss:0.470, val_acc:0.903]
Epoch [13/120    avg_loss:0.489, val_acc:0.873]
Epoch [14/120    avg_loss:0.456, val_acc:0.929]
Epoch [15/120    avg_loss:0.459, val_acc:0.905]
Epoch [16/120    avg_loss:0.424, val_acc:0.917]
Epoch [17/120    avg_loss:0.420, val_acc:0.925]
Epoch [18/120    avg_loss:0.434, val_acc:0.913]
Epoch [19/120    avg_loss:0.401, val_acc:0.925]
Epoch [20/120    avg_loss:0.328, val_acc:0.942]
Epoch [21/120    avg_loss:0.327, val_acc:0.944]
Epoch [22/120    avg_loss:0.297, val_acc:0.948]
Epoch [23/120    avg_loss:0.322, val_acc:0.937]
Epoch [24/120    avg_loss:0.328, val_acc:0.919]
Epoch [25/120    avg_loss:0.337, val_acc:0.929]
Epoch [26/120    avg_loss:0.377, val_acc:0.948]
Epoch [27/120    avg_loss:0.348, val_acc:0.919]
Epoch [28/120    avg_loss:0.379, val_acc:0.952]
Epoch [29/120    avg_loss:0.344, val_acc:0.903]
Epoch [30/120    avg_loss:0.265, val_acc:0.937]
Epoch [31/120    avg_loss:0.254, val_acc:0.952]
Epoch [32/120    avg_loss:0.229, val_acc:0.937]
Epoch [33/120    avg_loss:0.250, val_acc:0.946]
Epoch [34/120    avg_loss:0.213, val_acc:0.919]
Epoch [35/120    avg_loss:0.202, val_acc:0.966]
Epoch [36/120    avg_loss:0.207, val_acc:0.960]
Epoch [37/120    avg_loss:0.254, val_acc:0.919]
Epoch [38/120    avg_loss:0.259, val_acc:0.942]
Epoch [39/120    avg_loss:0.226, val_acc:0.966]
Epoch [40/120    avg_loss:0.241, val_acc:0.950]
Epoch [41/120    avg_loss:0.214, val_acc:0.978]
Epoch [42/120    avg_loss:0.151, val_acc:0.968]
Epoch [43/120    avg_loss:0.171, val_acc:0.964]
Epoch [44/120    avg_loss:0.178, val_acc:0.938]
Epoch [45/120    avg_loss:0.218, val_acc:0.962]
Epoch [46/120    avg_loss:0.191, val_acc:0.954]
Epoch [47/120    avg_loss:0.141, val_acc:0.966]
Epoch [48/120    avg_loss:0.162, val_acc:0.976]
Epoch [49/120    avg_loss:0.114, val_acc:0.970]
Epoch [50/120    avg_loss:0.184, val_acc:0.976]
Epoch [51/120    avg_loss:0.190, val_acc:0.966]
Epoch [52/120    avg_loss:0.146, val_acc:0.962]
Epoch [53/120    avg_loss:0.117, val_acc:0.966]
Epoch [54/120    avg_loss:0.130, val_acc:0.978]
Epoch [55/120    avg_loss:0.130, val_acc:0.970]
Epoch [56/120    avg_loss:0.123, val_acc:0.976]
Epoch [57/120    avg_loss:0.102, val_acc:0.978]
Epoch [58/120    avg_loss:0.102, val_acc:0.964]
Epoch [59/120    avg_loss:0.108, val_acc:0.966]
Epoch [60/120    avg_loss:0.137, val_acc:0.952]
Epoch [61/120    avg_loss:0.211, val_acc:0.960]
Epoch [62/120    avg_loss:0.174, val_acc:0.982]
Epoch [63/120    avg_loss:0.131, val_acc:0.972]
Epoch [64/120    avg_loss:0.108, val_acc:0.974]
Epoch [65/120    avg_loss:0.085, val_acc:0.982]
Epoch [66/120    avg_loss:0.077, val_acc:0.986]
Epoch [67/120    avg_loss:0.086, val_acc:0.984]
Epoch [68/120    avg_loss:0.087, val_acc:0.972]
Epoch [69/120    avg_loss:0.091, val_acc:0.974]
Epoch [70/120    avg_loss:0.109, val_acc:0.978]
Epoch [71/120    avg_loss:0.095, val_acc:0.976]
Epoch [72/120    avg_loss:0.094, val_acc:0.972]
Epoch [73/120    avg_loss:0.076, val_acc:0.984]
Epoch [74/120    avg_loss:0.077, val_acc:0.980]
Epoch [75/120    avg_loss:0.057, val_acc:0.978]
Epoch [76/120    avg_loss:0.070, val_acc:0.970]
Epoch [77/120    avg_loss:0.088, val_acc:0.968]
Epoch [78/120    avg_loss:0.101, val_acc:0.982]
Epoch [79/120    avg_loss:0.103, val_acc:0.966]
Epoch [80/120    avg_loss:0.053, val_acc:0.976]
Epoch [81/120    avg_loss:0.047, val_acc:0.986]
Epoch [82/120    avg_loss:0.051, val_acc:0.986]
Epoch [83/120    avg_loss:0.044, val_acc:0.988]
Epoch [84/120    avg_loss:0.037, val_acc:0.992]
Epoch [85/120    avg_loss:0.041, val_acc:0.986]
Epoch [86/120    avg_loss:0.028, val_acc:0.988]
Epoch [87/120    avg_loss:0.050, val_acc:0.986]
Epoch [88/120    avg_loss:0.032, val_acc:0.988]
Epoch [89/120    avg_loss:0.031, val_acc:0.988]
Epoch [90/120    avg_loss:0.043, val_acc:0.988]
Epoch [91/120    avg_loss:0.036, val_acc:0.988]
Epoch [92/120    avg_loss:0.033, val_acc:0.988]
Epoch [93/120    avg_loss:0.033, val_acc:0.984]
Epoch [94/120    avg_loss:0.029, val_acc:0.988]
Epoch [95/120    avg_loss:0.029, val_acc:0.988]
Epoch [96/120    avg_loss:0.035, val_acc:0.988]
Epoch [97/120    avg_loss:0.034, val_acc:0.990]
Epoch [98/120    avg_loss:0.042, val_acc:0.988]
Epoch [99/120    avg_loss:0.030, val_acc:0.992]
Epoch [100/120    avg_loss:0.038, val_acc:0.992]
Epoch [101/120    avg_loss:0.036, val_acc:0.992]
Epoch [102/120    avg_loss:0.026, val_acc:0.992]
Epoch [103/120    avg_loss:0.036, val_acc:0.992]
Epoch [104/120    avg_loss:0.034, val_acc:0.992]
Epoch [105/120    avg_loss:0.030, val_acc:0.992]
Epoch [106/120    avg_loss:0.032, val_acc:0.992]
Epoch [107/120    avg_loss:0.025, val_acc:0.992]
Epoch [108/120    avg_loss:0.028, val_acc:0.992]
Epoch [109/120    avg_loss:0.034, val_acc:0.992]
Epoch [110/120    avg_loss:0.026, val_acc:0.992]
Epoch [111/120    avg_loss:0.035, val_acc:0.992]
Epoch [112/120    avg_loss:0.027, val_acc:0.992]
Epoch [113/120    avg_loss:0.036, val_acc:0.992]
Epoch [114/120    avg_loss:0.026, val_acc:0.992]
Epoch [115/120    avg_loss:0.024, val_acc:0.992]
Epoch [116/120    avg_loss:0.027, val_acc:0.992]
Epoch [117/120    avg_loss:0.037, val_acc:0.990]
Epoch [118/120    avg_loss:0.036, val_acc:0.990]
Epoch [119/120    avg_loss:0.029, val_acc:0.990]
Epoch [120/120    avg_loss:0.020, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 220   9   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   3   0   0   0   0   0   0   0   0   2 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.96444444 0.97777778 0.9044586  0.87234043
 1.         0.9273743  1.         0.99893276 1.         0.99469496
 0.99224806 1.        ]

Kappa:
0.9843318009672029
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea8af2c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.379, val_acc:0.498]
Epoch [2/120    avg_loss:1.910, val_acc:0.577]
Epoch [3/120    avg_loss:1.574, val_acc:0.732]
Epoch [4/120    avg_loss:1.305, val_acc:0.732]
Epoch [5/120    avg_loss:1.101, val_acc:0.815]
Epoch [6/120    avg_loss:0.933, val_acc:0.788]
Epoch [7/120    avg_loss:0.932, val_acc:0.821]
Epoch [8/120    avg_loss:0.848, val_acc:0.764]
Epoch [9/120    avg_loss:0.740, val_acc:0.855]
Epoch [10/120    avg_loss:0.695, val_acc:0.827]
Epoch [11/120    avg_loss:0.613, val_acc:0.871]
Epoch [12/120    avg_loss:0.623, val_acc:0.875]
Epoch [13/120    avg_loss:0.528, val_acc:0.859]
Epoch [14/120    avg_loss:0.609, val_acc:0.865]
Epoch [15/120    avg_loss:0.494, val_acc:0.893]
Epoch [16/120    avg_loss:0.491, val_acc:0.875]
Epoch [17/120    avg_loss:0.550, val_acc:0.841]
Epoch [18/120    avg_loss:0.453, val_acc:0.909]
Epoch [19/120    avg_loss:0.414, val_acc:0.925]
Epoch [20/120    avg_loss:0.430, val_acc:0.895]
Epoch [21/120    avg_loss:0.423, val_acc:0.917]
Epoch [22/120    avg_loss:0.381, val_acc:0.899]
Epoch [23/120    avg_loss:0.381, val_acc:0.925]
Epoch [24/120    avg_loss:0.342, val_acc:0.927]
Epoch [25/120    avg_loss:0.367, val_acc:0.929]
Epoch [26/120    avg_loss:0.296, val_acc:0.929]
Epoch [27/120    avg_loss:0.303, val_acc:0.927]
Epoch [28/120    avg_loss:0.287, val_acc:0.942]
Epoch [29/120    avg_loss:0.250, val_acc:0.937]
Epoch [30/120    avg_loss:0.252, val_acc:0.946]
Epoch [31/120    avg_loss:0.229, val_acc:0.954]
Epoch [32/120    avg_loss:0.231, val_acc:0.938]
Epoch [33/120    avg_loss:0.254, val_acc:0.931]
Epoch [34/120    avg_loss:0.251, val_acc:0.929]
Epoch [35/120    avg_loss:0.309, val_acc:0.952]
Epoch [36/120    avg_loss:0.338, val_acc:0.950]
Epoch [37/120    avg_loss:0.262, val_acc:0.931]
Epoch [38/120    avg_loss:0.211, val_acc:0.948]
Epoch [39/120    avg_loss:0.177, val_acc:0.960]
Epoch [40/120    avg_loss:0.179, val_acc:0.966]
Epoch [41/120    avg_loss:0.187, val_acc:0.927]
Epoch [42/120    avg_loss:0.248, val_acc:0.919]
Epoch [43/120    avg_loss:0.170, val_acc:0.940]
Epoch [44/120    avg_loss:0.167, val_acc:0.964]
Epoch [45/120    avg_loss:0.169, val_acc:0.962]
Epoch [46/120    avg_loss:0.132, val_acc:0.960]
Epoch [47/120    avg_loss:0.148, val_acc:0.968]
Epoch [48/120    avg_loss:0.129, val_acc:0.960]
Epoch [49/120    avg_loss:0.107, val_acc:0.958]
Epoch [50/120    avg_loss:0.195, val_acc:0.958]
Epoch [51/120    avg_loss:0.112, val_acc:0.980]
Epoch [52/120    avg_loss:0.121, val_acc:0.948]
Epoch [53/120    avg_loss:0.196, val_acc:0.935]
Epoch [54/120    avg_loss:0.185, val_acc:0.950]
Epoch [55/120    avg_loss:0.124, val_acc:0.968]
Epoch [56/120    avg_loss:0.117, val_acc:0.968]
Epoch [57/120    avg_loss:0.113, val_acc:0.950]
Epoch [58/120    avg_loss:0.098, val_acc:0.958]
Epoch [59/120    avg_loss:0.090, val_acc:0.970]
Epoch [60/120    avg_loss:0.100, val_acc:0.962]
Epoch [61/120    avg_loss:0.120, val_acc:0.972]
Epoch [62/120    avg_loss:0.076, val_acc:0.982]
Epoch [63/120    avg_loss:0.112, val_acc:0.970]
Epoch [64/120    avg_loss:0.086, val_acc:0.974]
Epoch [65/120    avg_loss:0.087, val_acc:0.974]
Epoch [66/120    avg_loss:0.087, val_acc:0.972]
Epoch [67/120    avg_loss:0.093, val_acc:0.966]
Epoch [68/120    avg_loss:0.100, val_acc:0.980]
Epoch [69/120    avg_loss:0.101, val_acc:0.962]
Epoch [70/120    avg_loss:0.129, val_acc:0.982]
Epoch [71/120    avg_loss:0.103, val_acc:0.978]
Epoch [72/120    avg_loss:0.065, val_acc:0.982]
Epoch [73/120    avg_loss:0.045, val_acc:0.980]
Epoch [74/120    avg_loss:0.059, val_acc:0.976]
Epoch [75/120    avg_loss:0.071, val_acc:0.982]
Epoch [76/120    avg_loss:0.049, val_acc:0.988]
Epoch [77/120    avg_loss:0.038, val_acc:0.984]
Epoch [78/120    avg_loss:0.041, val_acc:0.984]
Epoch [79/120    avg_loss:0.030, val_acc:0.990]
Epoch [80/120    avg_loss:0.033, val_acc:0.986]
Epoch [81/120    avg_loss:0.039, val_acc:0.982]
Epoch [82/120    avg_loss:0.089, val_acc:0.976]
Epoch [83/120    avg_loss:0.089, val_acc:0.986]
Epoch [84/120    avg_loss:0.062, val_acc:0.980]
Epoch [85/120    avg_loss:0.046, val_acc:0.972]
Epoch [86/120    avg_loss:0.054, val_acc:0.982]
Epoch [87/120    avg_loss:0.040, val_acc:0.986]
Epoch [88/120    avg_loss:0.114, val_acc:0.970]
Epoch [89/120    avg_loss:0.125, val_acc:0.970]
Epoch [90/120    avg_loss:0.132, val_acc:0.958]
Epoch [91/120    avg_loss:0.068, val_acc:0.982]
Epoch [92/120    avg_loss:0.054, val_acc:0.978]
Epoch [93/120    avg_loss:0.041, val_acc:0.978]
Epoch [94/120    avg_loss:0.030, val_acc:0.982]
Epoch [95/120    avg_loss:0.028, val_acc:0.982]
Epoch [96/120    avg_loss:0.027, val_acc:0.984]
Epoch [97/120    avg_loss:0.024, val_acc:0.988]
Epoch [98/120    avg_loss:0.028, val_acc:0.986]
Epoch [99/120    avg_loss:0.028, val_acc:0.986]
Epoch [100/120    avg_loss:0.032, val_acc:0.986]
Epoch [101/120    avg_loss:0.026, val_acc:0.986]
Epoch [102/120    avg_loss:0.031, val_acc:0.988]
Epoch [103/120    avg_loss:0.029, val_acc:0.988]
Epoch [104/120    avg_loss:0.025, val_acc:0.988]
Epoch [105/120    avg_loss:0.022, val_acc:0.988]
Epoch [106/120    avg_loss:0.032, val_acc:0.988]
Epoch [107/120    avg_loss:0.024, val_acc:0.988]
Epoch [108/120    avg_loss:0.030, val_acc:0.988]
Epoch [109/120    avg_loss:0.025, val_acc:0.988]
Epoch [110/120    avg_loss:0.022, val_acc:0.988]
Epoch [111/120    avg_loss:0.030, val_acc:0.988]
Epoch [112/120    avg_loss:0.021, val_acc:0.988]
Epoch [113/120    avg_loss:0.022, val_acc:0.988]
Epoch [114/120    avg_loss:0.027, val_acc:0.988]
Epoch [115/120    avg_loss:0.033, val_acc:0.988]
Epoch [116/120    avg_loss:0.024, val_acc:0.988]
Epoch [117/120    avg_loss:0.029, val_acc:0.988]
Epoch [118/120    avg_loss:0.026, val_acc:0.988]
Epoch [119/120    avg_loss:0.018, val_acc:0.988]
Epoch [120/120    avg_loss:0.025, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 220   6   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 212  13   0   0   0   0   0   0   2   0]
 [  0   0   0   0  17 127   0   0   1   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 0.99854227 0.96551724 0.97777778 0.91774892 0.89122807
 0.99512195 0.92146597 0.99487179 0.99893276 1.         1.
 0.99779736 1.        ]

Kappa:
0.9857554544612408
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fefa67667b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.414, val_acc:0.486]
Epoch [2/120    avg_loss:1.916, val_acc:0.645]
Epoch [3/120    avg_loss:1.558, val_acc:0.688]
Epoch [4/120    avg_loss:1.279, val_acc:0.643]
Epoch [5/120    avg_loss:1.137, val_acc:0.750]
Epoch [6/120    avg_loss:0.973, val_acc:0.786]
Epoch [7/120    avg_loss:0.828, val_acc:0.869]
Epoch [8/120    avg_loss:0.728, val_acc:0.859]
Epoch [9/120    avg_loss:0.740, val_acc:0.841]
Epoch [10/120    avg_loss:0.665, val_acc:0.897]
Epoch [11/120    avg_loss:0.526, val_acc:0.889]
Epoch [12/120    avg_loss:0.492, val_acc:0.889]
Epoch [13/120    avg_loss:0.506, val_acc:0.913]
Epoch [14/120    avg_loss:0.574, val_acc:0.893]
Epoch [15/120    avg_loss:0.498, val_acc:0.901]
Epoch [16/120    avg_loss:0.409, val_acc:0.871]
Epoch [17/120    avg_loss:0.416, val_acc:0.907]
Epoch [18/120    avg_loss:0.376, val_acc:0.903]
Epoch [19/120    avg_loss:0.484, val_acc:0.883]
Epoch [20/120    avg_loss:0.405, val_acc:0.923]
Epoch [21/120    avg_loss:0.328, val_acc:0.923]
Epoch [22/120    avg_loss:0.343, val_acc:0.917]
Epoch [23/120    avg_loss:0.305, val_acc:0.927]
Epoch [24/120    avg_loss:0.312, val_acc:0.937]
Epoch [25/120    avg_loss:0.391, val_acc:0.887]
Epoch [26/120    avg_loss:0.292, val_acc:0.921]
Epoch [27/120    avg_loss:0.305, val_acc:0.933]
Epoch [28/120    avg_loss:0.285, val_acc:0.937]
Epoch [29/120    avg_loss:0.254, val_acc:0.940]
Epoch [30/120    avg_loss:0.325, val_acc:0.956]
Epoch [31/120    avg_loss:0.229, val_acc:0.923]
Epoch [32/120    avg_loss:0.313, val_acc:0.887]
Epoch [33/120    avg_loss:0.284, val_acc:0.946]
Epoch [34/120    avg_loss:0.268, val_acc:0.901]
Epoch [35/120    avg_loss:0.302, val_acc:0.921]
Epoch [36/120    avg_loss:0.260, val_acc:0.919]
Epoch [37/120    avg_loss:0.244, val_acc:0.933]
Epoch [38/120    avg_loss:0.209, val_acc:0.958]
Epoch [39/120    avg_loss:0.197, val_acc:0.958]
Epoch [40/120    avg_loss:0.166, val_acc:0.954]
Epoch [41/120    avg_loss:0.169, val_acc:0.968]
Epoch [42/120    avg_loss:0.149, val_acc:0.964]
Epoch [43/120    avg_loss:0.133, val_acc:0.958]
Epoch [44/120    avg_loss:0.219, val_acc:0.952]
Epoch [45/120    avg_loss:0.164, val_acc:0.964]
Epoch [46/120    avg_loss:0.140, val_acc:0.958]
Epoch [47/120    avg_loss:0.136, val_acc:0.952]
Epoch [48/120    avg_loss:0.148, val_acc:0.933]
Epoch [49/120    avg_loss:0.184, val_acc:0.935]
Epoch [50/120    avg_loss:0.168, val_acc:0.978]
Epoch [51/120    avg_loss:0.153, val_acc:0.972]
Epoch [52/120    avg_loss:0.213, val_acc:0.952]
Epoch [53/120    avg_loss:0.137, val_acc:0.964]
Epoch [54/120    avg_loss:0.131, val_acc:0.952]
Epoch [55/120    avg_loss:0.180, val_acc:0.933]
Epoch [56/120    avg_loss:0.151, val_acc:0.954]
Epoch [57/120    avg_loss:0.111, val_acc:0.968]
Epoch [58/120    avg_loss:0.118, val_acc:0.956]
Epoch [59/120    avg_loss:0.144, val_acc:0.976]
Epoch [60/120    avg_loss:0.092, val_acc:0.982]
Epoch [61/120    avg_loss:0.060, val_acc:0.986]
Epoch [62/120    avg_loss:0.089, val_acc:0.972]
Epoch [63/120    avg_loss:0.089, val_acc:0.948]
Epoch [64/120    avg_loss:0.092, val_acc:0.972]
Epoch [65/120    avg_loss:0.090, val_acc:0.982]
Epoch [66/120    avg_loss:0.071, val_acc:0.982]
Epoch [67/120    avg_loss:0.084, val_acc:0.986]
Epoch [68/120    avg_loss:0.090, val_acc:0.970]
Epoch [69/120    avg_loss:0.135, val_acc:0.972]
Epoch [70/120    avg_loss:0.087, val_acc:0.982]
Epoch [71/120    avg_loss:0.063, val_acc:0.978]
Epoch [72/120    avg_loss:0.105, val_acc:0.950]
Epoch [73/120    avg_loss:0.097, val_acc:0.968]
Epoch [74/120    avg_loss:0.096, val_acc:0.960]
Epoch [75/120    avg_loss:0.088, val_acc:0.988]
Epoch [76/120    avg_loss:0.063, val_acc:0.990]
Epoch [77/120    avg_loss:0.084, val_acc:0.984]
Epoch [78/120    avg_loss:0.087, val_acc:0.976]
Epoch [79/120    avg_loss:0.073, val_acc:0.982]
Epoch [80/120    avg_loss:0.051, val_acc:0.980]
Epoch [81/120    avg_loss:0.127, val_acc:0.964]
Epoch [82/120    avg_loss:0.156, val_acc:0.972]
Epoch [83/120    avg_loss:0.068, val_acc:0.978]
Epoch [84/120    avg_loss:0.051, val_acc:0.988]
Epoch [85/120    avg_loss:0.046, val_acc:0.980]
Epoch [86/120    avg_loss:0.057, val_acc:0.984]
Epoch [87/120    avg_loss:0.088, val_acc:0.984]
Epoch [88/120    avg_loss:0.109, val_acc:0.966]
Epoch [89/120    avg_loss:0.235, val_acc:0.970]
Epoch [90/120    avg_loss:0.088, val_acc:0.974]
Epoch [91/120    avg_loss:0.064, val_acc:0.976]
Epoch [92/120    avg_loss:0.051, val_acc:0.976]
Epoch [93/120    avg_loss:0.046, val_acc:0.978]
Epoch [94/120    avg_loss:0.049, val_acc:0.980]
Epoch [95/120    avg_loss:0.051, val_acc:0.984]
Epoch [96/120    avg_loss:0.045, val_acc:0.982]
Epoch [97/120    avg_loss:0.045, val_acc:0.986]
Epoch [98/120    avg_loss:0.034, val_acc:0.988]
Epoch [99/120    avg_loss:0.039, val_acc:0.986]
Epoch [100/120    avg_loss:0.042, val_acc:0.986]
Epoch [101/120    avg_loss:0.036, val_acc:0.986]
Epoch [102/120    avg_loss:0.040, val_acc:0.986]
Epoch [103/120    avg_loss:0.045, val_acc:0.986]
Epoch [104/120    avg_loss:0.030, val_acc:0.986]
Epoch [105/120    avg_loss:0.037, val_acc:0.986]
Epoch [106/120    avg_loss:0.044, val_acc:0.986]
Epoch [107/120    avg_loss:0.037, val_acc:0.986]
Epoch [108/120    avg_loss:0.040, val_acc:0.986]
Epoch [109/120    avg_loss:0.033, val_acc:0.986]
Epoch [110/120    avg_loss:0.036, val_acc:0.986]
Epoch [111/120    avg_loss:0.033, val_acc:0.986]
Epoch [112/120    avg_loss:0.038, val_acc:0.986]
Epoch [113/120    avg_loss:0.032, val_acc:0.986]
Epoch [114/120    avg_loss:0.033, val_acc:0.986]
Epoch [115/120    avg_loss:0.041, val_acc:0.986]
Epoch [116/120    avg_loss:0.037, val_acc:0.986]
Epoch [117/120    avg_loss:0.040, val_acc:0.986]
Epoch [118/120    avg_loss:0.041, val_acc:0.986]
Epoch [119/120    avg_loss:0.034, val_acc:0.986]
Epoch [120/120    avg_loss:0.041, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 218   8   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   1   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0   0 832]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 1.         0.93693694 0.97321429 0.92920354 0.92307692
 1.         0.87096774 0.99232737 1.         0.99862826 0.99867198
 0.9944629  0.99879952]

Kappa:
0.9840966459121224
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1fd9e2828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.438]
Epoch [2/120    avg_loss:1.887, val_acc:0.591]
Epoch [3/120    avg_loss:1.507, val_acc:0.681]
Epoch [4/120    avg_loss:1.223, val_acc:0.710]
Epoch [5/120    avg_loss:1.011, val_acc:0.790]
Epoch [6/120    avg_loss:0.862, val_acc:0.744]
Epoch [7/120    avg_loss:0.837, val_acc:0.853]
Epoch [8/120    avg_loss:0.744, val_acc:0.869]
Epoch [9/120    avg_loss:0.625, val_acc:0.863]
Epoch [10/120    avg_loss:0.594, val_acc:0.851]
Epoch [11/120    avg_loss:0.558, val_acc:0.883]
Epoch [12/120    avg_loss:0.568, val_acc:0.873]
Epoch [13/120    avg_loss:0.528, val_acc:0.889]
Epoch [14/120    avg_loss:0.494, val_acc:0.861]
Epoch [15/120    avg_loss:0.493, val_acc:0.873]
Epoch [16/120    avg_loss:0.438, val_acc:0.907]
Epoch [17/120    avg_loss:0.464, val_acc:0.891]
Epoch [18/120    avg_loss:0.492, val_acc:0.895]
Epoch [19/120    avg_loss:0.407, val_acc:0.921]
Epoch [20/120    avg_loss:0.347, val_acc:0.893]
Epoch [21/120    avg_loss:0.400, val_acc:0.895]
Epoch [22/120    avg_loss:0.301, val_acc:0.925]
Epoch [23/120    avg_loss:0.324, val_acc:0.899]
Epoch [24/120    avg_loss:0.356, val_acc:0.915]
Epoch [25/120    avg_loss:0.353, val_acc:0.919]
Epoch [26/120    avg_loss:0.322, val_acc:0.909]
Epoch [27/120    avg_loss:0.336, val_acc:0.921]
Epoch [28/120    avg_loss:0.290, val_acc:0.905]
Epoch [29/120    avg_loss:0.260, val_acc:0.938]
Epoch [30/120    avg_loss:0.251, val_acc:0.901]
Epoch [31/120    avg_loss:0.260, val_acc:0.940]
Epoch [32/120    avg_loss:0.247, val_acc:0.950]
Epoch [33/120    avg_loss:0.232, val_acc:0.927]
Epoch [34/120    avg_loss:0.228, val_acc:0.929]
Epoch [35/120    avg_loss:0.271, val_acc:0.937]
Epoch [36/120    avg_loss:0.229, val_acc:0.948]
Epoch [37/120    avg_loss:0.184, val_acc:0.954]
Epoch [38/120    avg_loss:0.191, val_acc:0.948]
Epoch [39/120    avg_loss:0.227, val_acc:0.948]
Epoch [40/120    avg_loss:0.156, val_acc:0.950]
Epoch [41/120    avg_loss:0.145, val_acc:0.950]
Epoch [42/120    avg_loss:0.212, val_acc:0.937]
Epoch [43/120    avg_loss:0.248, val_acc:0.903]
Epoch [44/120    avg_loss:0.213, val_acc:0.942]
Epoch [45/120    avg_loss:0.163, val_acc:0.960]
Epoch [46/120    avg_loss:0.162, val_acc:0.942]
Epoch [47/120    avg_loss:0.163, val_acc:0.909]
Epoch [48/120    avg_loss:0.188, val_acc:0.942]
Epoch [49/120    avg_loss:0.144, val_acc:0.954]
Epoch [50/120    avg_loss:0.120, val_acc:0.958]
Epoch [51/120    avg_loss:0.124, val_acc:0.954]
Epoch [52/120    avg_loss:0.164, val_acc:0.942]
Epoch [53/120    avg_loss:0.112, val_acc:0.962]
Epoch [54/120    avg_loss:0.090, val_acc:0.958]
Epoch [55/120    avg_loss:0.128, val_acc:0.962]
Epoch [56/120    avg_loss:0.095, val_acc:0.980]
Epoch [57/120    avg_loss:0.106, val_acc:0.948]
Epoch [58/120    avg_loss:0.209, val_acc:0.937]
Epoch [59/120    avg_loss:0.110, val_acc:0.942]
Epoch [60/120    avg_loss:0.123, val_acc:0.960]
Epoch [61/120    avg_loss:0.104, val_acc:0.956]
Epoch [62/120    avg_loss:0.124, val_acc:0.976]
Epoch [63/120    avg_loss:0.130, val_acc:0.956]
Epoch [64/120    avg_loss:0.137, val_acc:0.950]
Epoch [65/120    avg_loss:0.141, val_acc:0.964]
Epoch [66/120    avg_loss:0.083, val_acc:0.974]
Epoch [67/120    avg_loss:0.081, val_acc:0.974]
Epoch [68/120    avg_loss:0.085, val_acc:0.978]
Epoch [69/120    avg_loss:0.084, val_acc:0.954]
Epoch [70/120    avg_loss:0.058, val_acc:0.976]
Epoch [71/120    avg_loss:0.046, val_acc:0.974]
Epoch [72/120    avg_loss:0.058, val_acc:0.974]
Epoch [73/120    avg_loss:0.038, val_acc:0.972]
Epoch [74/120    avg_loss:0.041, val_acc:0.972]
Epoch [75/120    avg_loss:0.044, val_acc:0.972]
Epoch [76/120    avg_loss:0.053, val_acc:0.972]
Epoch [77/120    avg_loss:0.040, val_acc:0.972]
Epoch [78/120    avg_loss:0.050, val_acc:0.972]
Epoch [79/120    avg_loss:0.040, val_acc:0.974]
Epoch [80/120    avg_loss:0.042, val_acc:0.972]
Epoch [81/120    avg_loss:0.037, val_acc:0.974]
Epoch [82/120    avg_loss:0.032, val_acc:0.974]
Epoch [83/120    avg_loss:0.034, val_acc:0.974]
Epoch [84/120    avg_loss:0.040, val_acc:0.974]
Epoch [85/120    avg_loss:0.040, val_acc:0.974]
Epoch [86/120    avg_loss:0.034, val_acc:0.974]
Epoch [87/120    avg_loss:0.033, val_acc:0.974]
Epoch [88/120    avg_loss:0.040, val_acc:0.974]
Epoch [89/120    avg_loss:0.040, val_acc:0.974]
Epoch [90/120    avg_loss:0.042, val_acc:0.974]
Epoch [91/120    avg_loss:0.039, val_acc:0.974]
Epoch [92/120    avg_loss:0.038, val_acc:0.974]
Epoch [93/120    avg_loss:0.041, val_acc:0.974]
Epoch [94/120    avg_loss:0.043, val_acc:0.974]
Epoch [95/120    avg_loss:0.038, val_acc:0.972]
Epoch [96/120    avg_loss:0.038, val_acc:0.972]
Epoch [97/120    avg_loss:0.034, val_acc:0.974]
Epoch [98/120    avg_loss:0.043, val_acc:0.974]
Epoch [99/120    avg_loss:0.039, val_acc:0.974]
Epoch [100/120    avg_loss:0.050, val_acc:0.974]
Epoch [101/120    avg_loss:0.032, val_acc:0.974]
Epoch [102/120    avg_loss:0.041, val_acc:0.974]
Epoch [103/120    avg_loss:0.040, val_acc:0.974]
Epoch [104/120    avg_loss:0.036, val_acc:0.974]
Epoch [105/120    avg_loss:0.044, val_acc:0.974]
Epoch [106/120    avg_loss:0.037, val_acc:0.974]
Epoch [107/120    avg_loss:0.042, val_acc:0.974]
Epoch [108/120    avg_loss:0.041, val_acc:0.974]
Epoch [109/120    avg_loss:0.045, val_acc:0.974]
Epoch [110/120    avg_loss:0.032, val_acc:0.974]
Epoch [111/120    avg_loss:0.033, val_acc:0.974]
Epoch [112/120    avg_loss:0.046, val_acc:0.974]
Epoch [113/120    avg_loss:0.047, val_acc:0.974]
Epoch [114/120    avg_loss:0.048, val_acc:0.974]
Epoch [115/120    avg_loss:0.037, val_acc:0.974]
Epoch [116/120    avg_loss:0.040, val_acc:0.974]
Epoch [117/120    avg_loss:0.041, val_acc:0.974]
Epoch [118/120    avg_loss:0.035, val_acc:0.974]
Epoch [119/120    avg_loss:0.048, val_acc:0.974]
Epoch [120/120    avg_loss:0.040, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 223   3   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.92650334 0.98454746 0.94117647 0.90774908
 1.         0.83333333 0.99614891 0.99893276 1.         0.99867198
 0.99557522 1.        ]

Kappa:
0.9843307341912099
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff421f4b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.369, val_acc:0.571]
Epoch [2/120    avg_loss:1.917, val_acc:0.611]
Epoch [3/120    avg_loss:1.516, val_acc:0.738]
Epoch [4/120    avg_loss:1.265, val_acc:0.657]
Epoch [5/120    avg_loss:1.060, val_acc:0.780]
Epoch [6/120    avg_loss:0.941, val_acc:0.780]
Epoch [7/120    avg_loss:0.850, val_acc:0.823]
Epoch [8/120    avg_loss:0.780, val_acc:0.748]
Epoch [9/120    avg_loss:0.764, val_acc:0.847]
Epoch [10/120    avg_loss:0.608, val_acc:0.861]
Epoch [11/120    avg_loss:0.586, val_acc:0.859]
Epoch [12/120    avg_loss:0.567, val_acc:0.881]
Epoch [13/120    avg_loss:0.538, val_acc:0.879]
Epoch [14/120    avg_loss:0.528, val_acc:0.899]
Epoch [15/120    avg_loss:0.463, val_acc:0.913]
Epoch [16/120    avg_loss:0.390, val_acc:0.901]
Epoch [17/120    avg_loss:0.403, val_acc:0.897]
Epoch [18/120    avg_loss:0.443, val_acc:0.915]
Epoch [19/120    avg_loss:0.369, val_acc:0.893]
Epoch [20/120    avg_loss:0.388, val_acc:0.925]
Epoch [21/120    avg_loss:0.318, val_acc:0.909]
Epoch [22/120    avg_loss:0.345, val_acc:0.891]
Epoch [23/120    avg_loss:0.364, val_acc:0.909]
Epoch [24/120    avg_loss:0.337, val_acc:0.935]
Epoch [25/120    avg_loss:0.295, val_acc:0.933]
Epoch [26/120    avg_loss:0.286, val_acc:0.931]
Epoch [27/120    avg_loss:0.264, val_acc:0.925]
Epoch [28/120    avg_loss:0.270, val_acc:0.921]
Epoch [29/120    avg_loss:0.281, val_acc:0.937]
Epoch [30/120    avg_loss:0.303, val_acc:0.933]
Epoch [31/120    avg_loss:0.279, val_acc:0.925]
Epoch [32/120    avg_loss:0.287, val_acc:0.915]
Epoch [33/120    avg_loss:0.263, val_acc:0.909]
Epoch [34/120    avg_loss:0.193, val_acc:0.942]
Epoch [35/120    avg_loss:0.216, val_acc:0.935]
Epoch [36/120    avg_loss:0.209, val_acc:0.942]
Epoch [37/120    avg_loss:0.244, val_acc:0.919]
Epoch [38/120    avg_loss:0.234, val_acc:0.935]
Epoch [39/120    avg_loss:0.204, val_acc:0.933]
Epoch [40/120    avg_loss:0.197, val_acc:0.933]
Epoch [41/120    avg_loss:0.200, val_acc:0.940]
Epoch [42/120    avg_loss:0.199, val_acc:0.948]
Epoch [43/120    avg_loss:0.160, val_acc:0.935]
Epoch [44/120    avg_loss:0.162, val_acc:0.938]
Epoch [45/120    avg_loss:0.176, val_acc:0.925]
Epoch [46/120    avg_loss:0.181, val_acc:0.940]
Epoch [47/120    avg_loss:0.121, val_acc:0.935]
Epoch [48/120    avg_loss:0.169, val_acc:0.946]
Epoch [49/120    avg_loss:0.129, val_acc:0.948]
Epoch [50/120    avg_loss:0.145, val_acc:0.958]
Epoch [51/120    avg_loss:0.129, val_acc:0.938]
Epoch [52/120    avg_loss:0.126, val_acc:0.962]
Epoch [53/120    avg_loss:0.147, val_acc:0.913]
Epoch [54/120    avg_loss:0.125, val_acc:0.954]
Epoch [55/120    avg_loss:0.117, val_acc:0.948]
Epoch [56/120    avg_loss:0.118, val_acc:0.933]
Epoch [57/120    avg_loss:0.125, val_acc:0.950]
Epoch [58/120    avg_loss:0.156, val_acc:0.929]
Epoch [59/120    avg_loss:0.133, val_acc:0.968]
Epoch [60/120    avg_loss:0.098, val_acc:0.964]
Epoch [61/120    avg_loss:0.104, val_acc:0.964]
Epoch [62/120    avg_loss:0.091, val_acc:0.968]
Epoch [63/120    avg_loss:0.137, val_acc:0.940]
Epoch [64/120    avg_loss:0.120, val_acc:0.964]
Epoch [65/120    avg_loss:0.122, val_acc:0.972]
Epoch [66/120    avg_loss:0.062, val_acc:0.972]
Epoch [67/120    avg_loss:0.069, val_acc:0.964]
Epoch [68/120    avg_loss:0.076, val_acc:0.968]
Epoch [69/120    avg_loss:0.084, val_acc:0.960]
Epoch [70/120    avg_loss:0.084, val_acc:0.964]
Epoch [71/120    avg_loss:0.079, val_acc:0.946]
Epoch [72/120    avg_loss:0.068, val_acc:0.966]
Epoch [73/120    avg_loss:0.070, val_acc:0.970]
Epoch [74/120    avg_loss:0.078, val_acc:0.980]
Epoch [75/120    avg_loss:0.070, val_acc:0.966]
Epoch [76/120    avg_loss:0.063, val_acc:0.950]
Epoch [77/120    avg_loss:0.078, val_acc:0.966]
Epoch [78/120    avg_loss:0.048, val_acc:0.966]
Epoch [79/120    avg_loss:0.050, val_acc:0.970]
Epoch [80/120    avg_loss:0.079, val_acc:0.964]
Epoch [81/120    avg_loss:0.108, val_acc:0.958]
Epoch [82/120    avg_loss:0.084, val_acc:0.946]
Epoch [83/120    avg_loss:0.094, val_acc:0.960]
Epoch [84/120    avg_loss:0.112, val_acc:0.968]
Epoch [85/120    avg_loss:0.073, val_acc:0.968]
Epoch [86/120    avg_loss:0.034, val_acc:0.980]
Epoch [87/120    avg_loss:0.036, val_acc:0.972]
Epoch [88/120    avg_loss:0.041, val_acc:0.972]
Epoch [89/120    avg_loss:0.030, val_acc:0.976]
Epoch [90/120    avg_loss:0.020, val_acc:0.980]
Epoch [91/120    avg_loss:0.019, val_acc:0.970]
Epoch [92/120    avg_loss:0.027, val_acc:0.980]
Epoch [93/120    avg_loss:0.045, val_acc:0.970]
Epoch [94/120    avg_loss:0.046, val_acc:0.984]
Epoch [95/120    avg_loss:0.031, val_acc:0.974]
Epoch [96/120    avg_loss:0.036, val_acc:0.968]
Epoch [97/120    avg_loss:0.057, val_acc:0.968]
Epoch [98/120    avg_loss:0.037, val_acc:0.970]
Epoch [99/120    avg_loss:0.075, val_acc:0.956]
Epoch [100/120    avg_loss:0.058, val_acc:0.956]
Epoch [101/120    avg_loss:0.037, val_acc:0.968]
Epoch [102/120    avg_loss:0.028, val_acc:0.976]
Epoch [103/120    avg_loss:0.019, val_acc:0.984]
Epoch [104/120    avg_loss:0.030, val_acc:0.974]
Epoch [105/120    avg_loss:0.045, val_acc:0.970]
Epoch [106/120    avg_loss:0.030, val_acc:0.976]
Epoch [107/120    avg_loss:0.033, val_acc:0.972]
Epoch [108/120    avg_loss:0.024, val_acc:0.982]
Epoch [109/120    avg_loss:0.027, val_acc:0.980]
Epoch [110/120    avg_loss:0.035, val_acc:0.982]
Epoch [111/120    avg_loss:0.018, val_acc:0.982]
Epoch [112/120    avg_loss:0.028, val_acc:0.970]
Epoch [113/120    avg_loss:0.038, val_acc:0.972]
Epoch [114/120    avg_loss:0.063, val_acc:0.962]
Epoch [115/120    avg_loss:0.029, val_acc:0.972]
Epoch [116/120    avg_loss:0.021, val_acc:0.968]
Epoch [117/120    avg_loss:0.017, val_acc:0.972]
Epoch [118/120    avg_loss:0.016, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.978]
Epoch [120/120    avg_loss:0.017, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 220   6   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  35 110   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 1.         0.98398169 0.97777778 0.89484536 0.83018868
 1.         0.96296296 0.99614891 0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9850429281175184
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97eb82c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.412, val_acc:0.601]
Epoch [2/120    avg_loss:1.889, val_acc:0.704]
Epoch [3/120    avg_loss:1.467, val_acc:0.683]
Epoch [4/120    avg_loss:1.173, val_acc:0.794]
Epoch [5/120    avg_loss:1.036, val_acc:0.812]
Epoch [6/120    avg_loss:0.882, val_acc:0.833]
Epoch [7/120    avg_loss:0.785, val_acc:0.853]
Epoch [8/120    avg_loss:0.647, val_acc:0.889]
Epoch [9/120    avg_loss:0.595, val_acc:0.915]
Epoch [10/120    avg_loss:0.538, val_acc:0.879]
Epoch [11/120    avg_loss:0.568, val_acc:0.903]
Epoch [12/120    avg_loss:0.521, val_acc:0.903]
Epoch [13/120    avg_loss:0.447, val_acc:0.915]
Epoch [14/120    avg_loss:0.463, val_acc:0.921]
Epoch [15/120    avg_loss:0.502, val_acc:0.865]
Epoch [16/120    avg_loss:0.391, val_acc:0.907]
Epoch [17/120    avg_loss:0.427, val_acc:0.897]
Epoch [18/120    avg_loss:0.423, val_acc:0.931]
Epoch [19/120    avg_loss:0.344, val_acc:0.911]
Epoch [20/120    avg_loss:0.423, val_acc:0.923]
Epoch [21/120    avg_loss:0.376, val_acc:0.925]
Epoch [22/120    avg_loss:0.342, val_acc:0.917]
Epoch [23/120    avg_loss:0.267, val_acc:0.937]
Epoch [24/120    avg_loss:0.302, val_acc:0.940]
Epoch [25/120    avg_loss:0.340, val_acc:0.911]
Epoch [26/120    avg_loss:0.378, val_acc:0.929]
Epoch [27/120    avg_loss:0.324, val_acc:0.940]
Epoch [28/120    avg_loss:0.266, val_acc:0.942]
Epoch [29/120    avg_loss:0.280, val_acc:0.915]
Epoch [30/120    avg_loss:0.309, val_acc:0.946]
Epoch [31/120    avg_loss:0.286, val_acc:0.925]
Epoch [32/120    avg_loss:0.312, val_acc:0.942]
Epoch [33/120    avg_loss:0.277, val_acc:0.913]
Epoch [34/120    avg_loss:0.272, val_acc:0.925]
Epoch [35/120    avg_loss:0.332, val_acc:0.933]
Epoch [36/120    avg_loss:0.219, val_acc:0.942]
Epoch [37/120    avg_loss:0.229, val_acc:0.954]
Epoch [38/120    avg_loss:0.231, val_acc:0.952]
Epoch [39/120    avg_loss:0.196, val_acc:0.954]
Epoch [40/120    avg_loss:0.186, val_acc:0.960]
Epoch [41/120    avg_loss:0.175, val_acc:0.942]
Epoch [42/120    avg_loss:0.181, val_acc:0.946]
Epoch [43/120    avg_loss:0.174, val_acc:0.956]
Epoch [44/120    avg_loss:0.138, val_acc:0.952]
Epoch [45/120    avg_loss:0.205, val_acc:0.960]
Epoch [46/120    avg_loss:0.167, val_acc:0.964]
Epoch [47/120    avg_loss:0.159, val_acc:0.948]
Epoch [48/120    avg_loss:0.181, val_acc:0.960]
Epoch [49/120    avg_loss:0.182, val_acc:0.958]
Epoch [50/120    avg_loss:0.147, val_acc:0.958]
Epoch [51/120    avg_loss:0.161, val_acc:0.954]
Epoch [52/120    avg_loss:0.132, val_acc:0.958]
Epoch [53/120    avg_loss:0.162, val_acc:0.954]
Epoch [54/120    avg_loss:0.119, val_acc:0.970]
Epoch [55/120    avg_loss:0.122, val_acc:0.960]
Epoch [56/120    avg_loss:0.110, val_acc:0.952]
Epoch [57/120    avg_loss:0.139, val_acc:0.964]
Epoch [58/120    avg_loss:0.157, val_acc:0.954]
Epoch [59/120    avg_loss:0.097, val_acc:0.976]
Epoch [60/120    avg_loss:0.079, val_acc:0.962]
Epoch [61/120    avg_loss:0.143, val_acc:0.966]
Epoch [62/120    avg_loss:0.142, val_acc:0.962]
Epoch [63/120    avg_loss:0.092, val_acc:0.970]
Epoch [64/120    avg_loss:0.092, val_acc:0.982]
Epoch [65/120    avg_loss:0.072, val_acc:0.974]
Epoch [66/120    avg_loss:0.079, val_acc:0.942]
Epoch [67/120    avg_loss:0.146, val_acc:0.974]
Epoch [68/120    avg_loss:0.113, val_acc:0.962]
Epoch [69/120    avg_loss:0.091, val_acc:0.974]
Epoch [70/120    avg_loss:0.072, val_acc:0.976]
Epoch [71/120    avg_loss:0.086, val_acc:0.974]
Epoch [72/120    avg_loss:0.072, val_acc:0.980]
Epoch [73/120    avg_loss:0.055, val_acc:0.970]
Epoch [74/120    avg_loss:0.079, val_acc:0.978]
Epoch [75/120    avg_loss:0.057, val_acc:0.968]
Epoch [76/120    avg_loss:0.057, val_acc:0.976]
Epoch [77/120    avg_loss:0.086, val_acc:0.966]
Epoch [78/120    avg_loss:0.080, val_acc:0.976]
Epoch [79/120    avg_loss:0.050, val_acc:0.982]
Epoch [80/120    avg_loss:0.033, val_acc:0.982]
Epoch [81/120    avg_loss:0.038, val_acc:0.980]
Epoch [82/120    avg_loss:0.034, val_acc:0.984]
Epoch [83/120    avg_loss:0.048, val_acc:0.982]
Epoch [84/120    avg_loss:0.037, val_acc:0.982]
Epoch [85/120    avg_loss:0.045, val_acc:0.984]
Epoch [86/120    avg_loss:0.034, val_acc:0.982]
Epoch [87/120    avg_loss:0.046, val_acc:0.984]
Epoch [88/120    avg_loss:0.035, val_acc:0.986]
Epoch [89/120    avg_loss:0.032, val_acc:0.986]
Epoch [90/120    avg_loss:0.036, val_acc:0.986]
Epoch [91/120    avg_loss:0.030, val_acc:0.984]
Epoch [92/120    avg_loss:0.034, val_acc:0.984]
Epoch [93/120    avg_loss:0.023, val_acc:0.984]
Epoch [94/120    avg_loss:0.036, val_acc:0.982]
Epoch [95/120    avg_loss:0.039, val_acc:0.982]
Epoch [96/120    avg_loss:0.024, val_acc:0.984]
Epoch [97/120    avg_loss:0.030, val_acc:0.984]
Epoch [98/120    avg_loss:0.036, val_acc:0.984]
Epoch [99/120    avg_loss:0.029, val_acc:0.986]
Epoch [100/120    avg_loss:0.043, val_acc:0.984]
Epoch [101/120    avg_loss:0.042, val_acc:0.980]
Epoch [102/120    avg_loss:0.025, val_acc:0.980]
Epoch [103/120    avg_loss:0.031, val_acc:0.980]
Epoch [104/120    avg_loss:0.035, val_acc:0.982]
Epoch [105/120    avg_loss:0.028, val_acc:0.986]
Epoch [106/120    avg_loss:0.036, val_acc:0.986]
Epoch [107/120    avg_loss:0.019, val_acc:0.986]
Epoch [108/120    avg_loss:0.026, val_acc:0.986]
Epoch [109/120    avg_loss:0.030, val_acc:0.984]
Epoch [110/120    avg_loss:0.030, val_acc:0.984]
Epoch [111/120    avg_loss:0.034, val_acc:0.984]
Epoch [112/120    avg_loss:0.023, val_acc:0.984]
Epoch [113/120    avg_loss:0.030, val_acc:0.984]
Epoch [114/120    avg_loss:0.032, val_acc:0.984]
Epoch [115/120    avg_loss:0.030, val_acc:0.984]
Epoch [116/120    avg_loss:0.030, val_acc:0.986]
Epoch [117/120    avg_loss:0.028, val_acc:0.988]
Epoch [118/120    avg_loss:0.023, val_acc:0.986]
Epoch [119/120    avg_loss:0.022, val_acc:0.984]
Epoch [120/120    avg_loss:0.028, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   3 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.9569161  0.98454746 0.90985325 0.86446886
 1.         0.90322581 0.998713   1.         1.         0.99603699
 0.99556541 1.        ]

Kappa:
0.9843317755883456
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1c05c72860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.387, val_acc:0.425]
Epoch [2/120    avg_loss:1.916, val_acc:0.656]
Epoch [3/120    avg_loss:1.570, val_acc:0.648]
Epoch [4/120    avg_loss:1.304, val_acc:0.675]
Epoch [5/120    avg_loss:1.092, val_acc:0.765]
Epoch [6/120    avg_loss:0.946, val_acc:0.750]
Epoch [7/120    avg_loss:0.824, val_acc:0.810]
Epoch [8/120    avg_loss:0.757, val_acc:0.852]
Epoch [9/120    avg_loss:0.633, val_acc:0.854]
Epoch [10/120    avg_loss:0.602, val_acc:0.842]
Epoch [11/120    avg_loss:0.521, val_acc:0.894]
Epoch [12/120    avg_loss:0.446, val_acc:0.894]
Epoch [13/120    avg_loss:0.567, val_acc:0.852]
Epoch [14/120    avg_loss:0.471, val_acc:0.912]
Epoch [15/120    avg_loss:0.436, val_acc:0.892]
Epoch [16/120    avg_loss:0.434, val_acc:0.885]
Epoch [17/120    avg_loss:0.419, val_acc:0.896]
Epoch [18/120    avg_loss:0.371, val_acc:0.898]
Epoch [19/120    avg_loss:0.347, val_acc:0.879]
Epoch [20/120    avg_loss:0.427, val_acc:0.900]
Epoch [21/120    avg_loss:0.383, val_acc:0.892]
Epoch [22/120    avg_loss:0.356, val_acc:0.912]
Epoch [23/120    avg_loss:0.366, val_acc:0.917]
Epoch [24/120    avg_loss:0.344, val_acc:0.925]
Epoch [25/120    avg_loss:0.340, val_acc:0.912]
Epoch [26/120    avg_loss:0.293, val_acc:0.925]
Epoch [27/120    avg_loss:0.279, val_acc:0.927]
Epoch [28/120    avg_loss:0.342, val_acc:0.933]
Epoch [29/120    avg_loss:0.235, val_acc:0.927]
Epoch [30/120    avg_loss:0.239, val_acc:0.927]
Epoch [31/120    avg_loss:0.213, val_acc:0.938]
Epoch [32/120    avg_loss:0.207, val_acc:0.935]
Epoch [33/120    avg_loss:0.259, val_acc:0.925]
Epoch [34/120    avg_loss:0.206, val_acc:0.946]
Epoch [35/120    avg_loss:0.176, val_acc:0.940]
Epoch [36/120    avg_loss:0.177, val_acc:0.940]
Epoch [37/120    avg_loss:0.282, val_acc:0.927]
Epoch [38/120    avg_loss:0.223, val_acc:0.938]
Epoch [39/120    avg_loss:0.254, val_acc:0.887]
Epoch [40/120    avg_loss:0.186, val_acc:0.956]
Epoch [41/120    avg_loss:0.152, val_acc:0.952]
Epoch [42/120    avg_loss:0.119, val_acc:0.963]
Epoch [43/120    avg_loss:0.125, val_acc:0.954]
Epoch [44/120    avg_loss:0.164, val_acc:0.956]
Epoch [45/120    avg_loss:0.159, val_acc:0.950]
Epoch [46/120    avg_loss:0.135, val_acc:0.958]
Epoch [47/120    avg_loss:0.128, val_acc:0.931]
Epoch [48/120    avg_loss:0.131, val_acc:0.960]
Epoch [49/120    avg_loss:0.177, val_acc:0.960]
Epoch [50/120    avg_loss:0.134, val_acc:0.958]
Epoch [51/120    avg_loss:0.120, val_acc:0.950]
Epoch [52/120    avg_loss:0.102, val_acc:0.971]
Epoch [53/120    avg_loss:0.063, val_acc:0.960]
Epoch [54/120    avg_loss:0.084, val_acc:0.981]
Epoch [55/120    avg_loss:0.097, val_acc:0.967]
Epoch [56/120    avg_loss:0.107, val_acc:0.969]
Epoch [57/120    avg_loss:0.063, val_acc:0.975]
Epoch [58/120    avg_loss:0.062, val_acc:0.952]
Epoch [59/120    avg_loss:0.130, val_acc:0.975]
Epoch [60/120    avg_loss:0.059, val_acc:0.967]
Epoch [61/120    avg_loss:0.069, val_acc:0.979]
Epoch [62/120    avg_loss:0.085, val_acc:0.965]
Epoch [63/120    avg_loss:0.171, val_acc:0.950]
Epoch [64/120    avg_loss:0.128, val_acc:0.973]
Epoch [65/120    avg_loss:0.160, val_acc:0.946]
Epoch [66/120    avg_loss:0.100, val_acc:0.958]
Epoch [67/120    avg_loss:0.122, val_acc:0.975]
Epoch [68/120    avg_loss:0.057, val_acc:0.973]
Epoch [69/120    avg_loss:0.054, val_acc:0.973]
Epoch [70/120    avg_loss:0.048, val_acc:0.977]
Epoch [71/120    avg_loss:0.044, val_acc:0.975]
Epoch [72/120    avg_loss:0.042, val_acc:0.977]
Epoch [73/120    avg_loss:0.042, val_acc:0.979]
Epoch [74/120    avg_loss:0.041, val_acc:0.977]
Epoch [75/120    avg_loss:0.037, val_acc:0.977]
Epoch [76/120    avg_loss:0.040, val_acc:0.977]
Epoch [77/120    avg_loss:0.043, val_acc:0.979]
Epoch [78/120    avg_loss:0.045, val_acc:0.979]
Epoch [79/120    avg_loss:0.041, val_acc:0.975]
Epoch [80/120    avg_loss:0.032, val_acc:0.977]
Epoch [81/120    avg_loss:0.039, val_acc:0.977]
Epoch [82/120    avg_loss:0.035, val_acc:0.977]
Epoch [83/120    avg_loss:0.040, val_acc:0.977]
Epoch [84/120    avg_loss:0.039, val_acc:0.975]
Epoch [85/120    avg_loss:0.040, val_acc:0.975]
Epoch [86/120    avg_loss:0.032, val_acc:0.975]
Epoch [87/120    avg_loss:0.037, val_acc:0.975]
Epoch [88/120    avg_loss:0.029, val_acc:0.975]
Epoch [89/120    avg_loss:0.038, val_acc:0.975]
Epoch [90/120    avg_loss:0.036, val_acc:0.975]
Epoch [91/120    avg_loss:0.033, val_acc:0.975]
Epoch [92/120    avg_loss:0.037, val_acc:0.975]
Epoch [93/120    avg_loss:0.040, val_acc:0.975]
Epoch [94/120    avg_loss:0.032, val_acc:0.975]
Epoch [95/120    avg_loss:0.032, val_acc:0.975]
Epoch [96/120    avg_loss:0.030, val_acc:0.975]
Epoch [97/120    avg_loss:0.039, val_acc:0.975]
Epoch [98/120    avg_loss:0.043, val_acc:0.975]
Epoch [99/120    avg_loss:0.035, val_acc:0.975]
Epoch [100/120    avg_loss:0.038, val_acc:0.975]
Epoch [101/120    avg_loss:0.038, val_acc:0.975]
Epoch [102/120    avg_loss:0.038, val_acc:0.975]
Epoch [103/120    avg_loss:0.048, val_acc:0.975]
Epoch [104/120    avg_loss:0.034, val_acc:0.975]
Epoch [105/120    avg_loss:0.030, val_acc:0.975]
Epoch [106/120    avg_loss:0.036, val_acc:0.975]
Epoch [107/120    avg_loss:0.029, val_acc:0.975]
Epoch [108/120    avg_loss:0.039, val_acc:0.975]
Epoch [109/120    avg_loss:0.038, val_acc:0.975]
Epoch [110/120    avg_loss:0.032, val_acc:0.975]
Epoch [111/120    avg_loss:0.038, val_acc:0.975]
Epoch [112/120    avg_loss:0.036, val_acc:0.975]
Epoch [113/120    avg_loss:0.031, val_acc:0.975]
Epoch [114/120    avg_loss:0.030, val_acc:0.975]
Epoch [115/120    avg_loss:0.035, val_acc:0.975]
Epoch [116/120    avg_loss:0.031, val_acc:0.975]
Epoch [117/120    avg_loss:0.031, val_acc:0.975]
Epoch [118/120    avg_loss:0.036, val_acc:0.975]
Epoch [119/120    avg_loss:0.034, val_acc:0.975]
Epoch [120/120    avg_loss:0.031, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   8   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.97117517 0.98004435 0.9122807  0.89189189
 1.         0.92571429 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9871805274408642
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f507ff537f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.375, val_acc:0.411]
Epoch [2/120    avg_loss:1.871, val_acc:0.609]
Epoch [3/120    avg_loss:1.488, val_acc:0.665]
Epoch [4/120    avg_loss:1.166, val_acc:0.744]
Epoch [5/120    avg_loss:0.945, val_acc:0.802]
Epoch [6/120    avg_loss:0.896, val_acc:0.837]
Epoch [7/120    avg_loss:0.818, val_acc:0.819]
Epoch [8/120    avg_loss:0.744, val_acc:0.855]
Epoch [9/120    avg_loss:0.653, val_acc:0.825]
Epoch [10/120    avg_loss:0.559, val_acc:0.887]
Epoch [11/120    avg_loss:0.560, val_acc:0.853]
Epoch [12/120    avg_loss:0.456, val_acc:0.915]
Epoch [13/120    avg_loss:0.496, val_acc:0.901]
Epoch [14/120    avg_loss:0.474, val_acc:0.911]
Epoch [15/120    avg_loss:0.352, val_acc:0.903]
Epoch [16/120    avg_loss:0.409, val_acc:0.849]
Epoch [17/120    avg_loss:0.385, val_acc:0.865]
Epoch [18/120    avg_loss:0.371, val_acc:0.871]
Epoch [19/120    avg_loss:0.400, val_acc:0.889]
Epoch [20/120    avg_loss:0.415, val_acc:0.905]
Epoch [21/120    avg_loss:0.326, val_acc:0.891]
Epoch [22/120    avg_loss:0.275, val_acc:0.929]
Epoch [23/120    avg_loss:0.317, val_acc:0.923]
Epoch [24/120    avg_loss:0.348, val_acc:0.913]
Epoch [25/120    avg_loss:0.314, val_acc:0.917]
Epoch [26/120    avg_loss:0.264, val_acc:0.929]
Epoch [27/120    avg_loss:0.307, val_acc:0.946]
Epoch [28/120    avg_loss:0.218, val_acc:0.954]
Epoch [29/120    avg_loss:0.265, val_acc:0.940]
Epoch [30/120    avg_loss:0.245, val_acc:0.948]
Epoch [31/120    avg_loss:0.245, val_acc:0.952]
Epoch [32/120    avg_loss:0.264, val_acc:0.966]
Epoch [33/120    avg_loss:0.287, val_acc:0.962]
Epoch [34/120    avg_loss:0.260, val_acc:0.946]
Epoch [35/120    avg_loss:0.218, val_acc:0.956]
Epoch [36/120    avg_loss:0.190, val_acc:0.942]
Epoch [37/120    avg_loss:0.186, val_acc:0.956]
Epoch [38/120    avg_loss:0.175, val_acc:0.972]
Epoch [39/120    avg_loss:0.131, val_acc:0.968]
Epoch [40/120    avg_loss:0.134, val_acc:0.964]
Epoch [41/120    avg_loss:0.222, val_acc:0.958]
Epoch [42/120    avg_loss:0.175, val_acc:0.948]
Epoch [43/120    avg_loss:0.193, val_acc:0.946]
Epoch [44/120    avg_loss:0.157, val_acc:0.976]
Epoch [45/120    avg_loss:0.134, val_acc:0.980]
Epoch [46/120    avg_loss:0.118, val_acc:0.962]
Epoch [47/120    avg_loss:0.182, val_acc:0.966]
Epoch [48/120    avg_loss:0.100, val_acc:0.980]
Epoch [49/120    avg_loss:0.114, val_acc:0.952]
Epoch [50/120    avg_loss:0.129, val_acc:0.958]
Epoch [51/120    avg_loss:0.133, val_acc:0.972]
Epoch [52/120    avg_loss:0.117, val_acc:0.968]
Epoch [53/120    avg_loss:0.122, val_acc:0.944]
Epoch [54/120    avg_loss:0.092, val_acc:0.976]
Epoch [55/120    avg_loss:0.146, val_acc:0.972]
Epoch [56/120    avg_loss:0.129, val_acc:0.972]
Epoch [57/120    avg_loss:0.118, val_acc:0.958]
Epoch [58/120    avg_loss:0.100, val_acc:0.968]
Epoch [59/120    avg_loss:0.078, val_acc:0.986]
Epoch [60/120    avg_loss:0.098, val_acc:0.986]
Epoch [61/120    avg_loss:0.061, val_acc:0.974]
Epoch [62/120    avg_loss:0.257, val_acc:0.960]
Epoch [63/120    avg_loss:0.157, val_acc:0.978]
Epoch [64/120    avg_loss:0.095, val_acc:0.978]
Epoch [65/120    avg_loss:0.054, val_acc:0.980]
Epoch [66/120    avg_loss:0.081, val_acc:0.972]
Epoch [67/120    avg_loss:0.213, val_acc:0.948]
Epoch [68/120    avg_loss:0.122, val_acc:0.980]
Epoch [69/120    avg_loss:0.081, val_acc:0.962]
Epoch [70/120    avg_loss:0.084, val_acc:0.980]
Epoch [71/120    avg_loss:0.077, val_acc:0.986]
Epoch [72/120    avg_loss:0.056, val_acc:0.988]
Epoch [73/120    avg_loss:0.050, val_acc:0.984]
Epoch [74/120    avg_loss:0.053, val_acc:0.976]
Epoch [75/120    avg_loss:0.051, val_acc:0.986]
Epoch [76/120    avg_loss:0.049, val_acc:0.994]
Epoch [77/120    avg_loss:0.047, val_acc:0.992]
Epoch [78/120    avg_loss:0.057, val_acc:0.984]
Epoch [79/120    avg_loss:0.082, val_acc:0.970]
Epoch [80/120    avg_loss:0.070, val_acc:0.986]
Epoch [81/120    avg_loss:0.087, val_acc:0.976]
Epoch [82/120    avg_loss:0.081, val_acc:0.982]
Epoch [83/120    avg_loss:0.039, val_acc:0.990]
Epoch [84/120    avg_loss:0.030, val_acc:0.990]
Epoch [85/120    avg_loss:0.039, val_acc:0.996]
Epoch [86/120    avg_loss:0.021, val_acc:0.998]
Epoch [87/120    avg_loss:0.028, val_acc:0.992]
Epoch [88/120    avg_loss:0.023, val_acc:0.966]
Epoch [89/120    avg_loss:0.047, val_acc:0.994]
Epoch [90/120    avg_loss:0.085, val_acc:0.982]
Epoch [91/120    avg_loss:0.107, val_acc:0.986]
Epoch [92/120    avg_loss:0.050, val_acc:0.988]
Epoch [93/120    avg_loss:0.038, val_acc:0.986]
Epoch [94/120    avg_loss:0.103, val_acc:0.982]
Epoch [95/120    avg_loss:0.110, val_acc:0.972]
Epoch [96/120    avg_loss:0.052, val_acc:0.986]
Epoch [97/120    avg_loss:0.026, val_acc:0.992]
Epoch [98/120    avg_loss:0.026, val_acc:0.992]
Epoch [99/120    avg_loss:0.020, val_acc:0.992]
Epoch [100/120    avg_loss:0.019, val_acc:0.996]
Epoch [101/120    avg_loss:0.015, val_acc:0.998]
Epoch [102/120    avg_loss:0.014, val_acc:0.998]
Epoch [103/120    avg_loss:0.020, val_acc:0.998]
Epoch [104/120    avg_loss:0.028, val_acc:0.994]
Epoch [105/120    avg_loss:0.016, val_acc:0.994]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.012, val_acc:0.994]
Epoch [108/120    avg_loss:0.016, val_acc:0.992]
Epoch [109/120    avg_loss:0.023, val_acc:0.994]
Epoch [110/120    avg_loss:0.013, val_acc:0.998]
Epoch [111/120    avg_loss:0.015, val_acc:0.998]
Epoch [112/120    avg_loss:0.025, val_acc:0.996]
Epoch [113/120    avg_loss:0.015, val_acc:0.996]
Epoch [114/120    avg_loss:0.014, val_acc:0.996]
Epoch [115/120    avg_loss:0.010, val_acc:0.996]
Epoch [116/120    avg_loss:0.013, val_acc:0.996]
Epoch [117/120    avg_loss:0.017, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.998]
Epoch [120/120    avg_loss:0.012, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   5   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 132   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.9977221  0.98230088 0.9321663  0.90721649
 1.         0.99465241 0.99614891 1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.9912165287385482
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d450de7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.359, val_acc:0.492]
Epoch [2/120    avg_loss:1.832, val_acc:0.623]
Epoch [3/120    avg_loss:1.488, val_acc:0.730]
Epoch [4/120    avg_loss:1.202, val_acc:0.740]
Epoch [5/120    avg_loss:1.048, val_acc:0.764]
Epoch [6/120    avg_loss:0.898, val_acc:0.752]
Epoch [7/120    avg_loss:0.843, val_acc:0.788]
Epoch [8/120    avg_loss:0.747, val_acc:0.869]
Epoch [9/120    avg_loss:0.663, val_acc:0.812]
Epoch [10/120    avg_loss:0.602, val_acc:0.867]
Epoch [11/120    avg_loss:0.517, val_acc:0.899]
Epoch [12/120    avg_loss:0.473, val_acc:0.895]
Epoch [13/120    avg_loss:0.568, val_acc:0.913]
Epoch [14/120    avg_loss:0.440, val_acc:0.917]
Epoch [15/120    avg_loss:0.414, val_acc:0.919]
Epoch [16/120    avg_loss:0.407, val_acc:0.823]
Epoch [17/120    avg_loss:0.437, val_acc:0.940]
Epoch [18/120    avg_loss:0.356, val_acc:0.935]
Epoch [19/120    avg_loss:0.324, val_acc:0.952]
Epoch [20/120    avg_loss:0.302, val_acc:0.952]
Epoch [21/120    avg_loss:0.289, val_acc:0.937]
Epoch [22/120    avg_loss:0.285, val_acc:0.935]
Epoch [23/120    avg_loss:0.285, val_acc:0.946]
Epoch [24/120    avg_loss:0.317, val_acc:0.933]
Epoch [25/120    avg_loss:0.346, val_acc:0.942]
Epoch [26/120    avg_loss:0.272, val_acc:0.940]
Epoch [27/120    avg_loss:0.230, val_acc:0.933]
Epoch [28/120    avg_loss:0.301, val_acc:0.917]
Epoch [29/120    avg_loss:0.278, val_acc:0.950]
Epoch [30/120    avg_loss:0.264, val_acc:0.915]
Epoch [31/120    avg_loss:0.271, val_acc:0.954]
Epoch [32/120    avg_loss:0.181, val_acc:0.948]
Epoch [33/120    avg_loss:0.225, val_acc:0.946]
Epoch [34/120    avg_loss:0.221, val_acc:0.931]
Epoch [35/120    avg_loss:0.243, val_acc:0.964]
Epoch [36/120    avg_loss:0.147, val_acc:0.958]
Epoch [37/120    avg_loss:0.159, val_acc:0.952]
Epoch [38/120    avg_loss:0.198, val_acc:0.952]
Epoch [39/120    avg_loss:0.141, val_acc:0.950]
Epoch [40/120    avg_loss:0.147, val_acc:0.946]
Epoch [41/120    avg_loss:0.194, val_acc:0.938]
Epoch [42/120    avg_loss:0.239, val_acc:0.923]
Epoch [43/120    avg_loss:0.246, val_acc:0.962]
Epoch [44/120    avg_loss:0.146, val_acc:0.964]
Epoch [45/120    avg_loss:0.123, val_acc:0.960]
Epoch [46/120    avg_loss:0.155, val_acc:0.956]
Epoch [47/120    avg_loss:0.176, val_acc:0.962]
Epoch [48/120    avg_loss:0.152, val_acc:0.966]
Epoch [49/120    avg_loss:0.132, val_acc:0.970]
Epoch [50/120    avg_loss:0.113, val_acc:0.976]
Epoch [51/120    avg_loss:0.116, val_acc:0.952]
Epoch [52/120    avg_loss:0.126, val_acc:0.970]
Epoch [53/120    avg_loss:0.132, val_acc:0.958]
Epoch [54/120    avg_loss:0.128, val_acc:0.976]
Epoch [55/120    avg_loss:0.122, val_acc:0.952]
Epoch [56/120    avg_loss:0.093, val_acc:0.976]
Epoch [57/120    avg_loss:0.098, val_acc:0.958]
Epoch [58/120    avg_loss:0.094, val_acc:0.976]
Epoch [59/120    avg_loss:0.073, val_acc:0.972]
Epoch [60/120    avg_loss:0.092, val_acc:0.976]
Epoch [61/120    avg_loss:0.091, val_acc:0.972]
Epoch [62/120    avg_loss:0.090, val_acc:0.968]
Epoch [63/120    avg_loss:0.061, val_acc:0.978]
Epoch [64/120    avg_loss:0.057, val_acc:0.958]
Epoch [65/120    avg_loss:0.152, val_acc:0.952]
Epoch [66/120    avg_loss:0.082, val_acc:0.972]
Epoch [67/120    avg_loss:0.060, val_acc:0.970]
Epoch [68/120    avg_loss:0.049, val_acc:0.976]
Epoch [69/120    avg_loss:0.127, val_acc:0.966]
Epoch [70/120    avg_loss:0.069, val_acc:0.978]
Epoch [71/120    avg_loss:0.056, val_acc:0.972]
Epoch [72/120    avg_loss:0.037, val_acc:0.980]
Epoch [73/120    avg_loss:0.056, val_acc:0.978]
Epoch [74/120    avg_loss:0.060, val_acc:0.976]
Epoch [75/120    avg_loss:0.061, val_acc:0.972]
Epoch [76/120    avg_loss:0.091, val_acc:0.974]
Epoch [77/120    avg_loss:0.050, val_acc:0.988]
Epoch [78/120    avg_loss:0.042, val_acc:0.984]
Epoch [79/120    avg_loss:0.055, val_acc:0.982]
Epoch [80/120    avg_loss:0.038, val_acc:0.982]
Epoch [81/120    avg_loss:0.041, val_acc:0.986]
Epoch [82/120    avg_loss:0.084, val_acc:0.968]
Epoch [83/120    avg_loss:0.113, val_acc:0.968]
Epoch [84/120    avg_loss:0.076, val_acc:0.940]
Epoch [85/120    avg_loss:0.054, val_acc:0.978]
Epoch [86/120    avg_loss:0.056, val_acc:0.974]
Epoch [87/120    avg_loss:0.061, val_acc:0.982]
Epoch [88/120    avg_loss:0.037, val_acc:0.970]
Epoch [89/120    avg_loss:0.144, val_acc:0.956]
Epoch [90/120    avg_loss:0.067, val_acc:0.968]
Epoch [91/120    avg_loss:0.039, val_acc:0.968]
Epoch [92/120    avg_loss:0.035, val_acc:0.978]
Epoch [93/120    avg_loss:0.029, val_acc:0.976]
Epoch [94/120    avg_loss:0.027, val_acc:0.978]
Epoch [95/120    avg_loss:0.029, val_acc:0.974]
Epoch [96/120    avg_loss:0.025, val_acc:0.974]
Epoch [97/120    avg_loss:0.018, val_acc:0.972]
Epoch [98/120    avg_loss:0.028, val_acc:0.980]
Epoch [99/120    avg_loss:0.022, val_acc:0.978]
Epoch [100/120    avg_loss:0.029, val_acc:0.976]
Epoch [101/120    avg_loss:0.027, val_acc:0.978]
Epoch [102/120    avg_loss:0.020, val_acc:0.976]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.032, val_acc:0.978]
Epoch [105/120    avg_loss:0.020, val_acc:0.978]
Epoch [106/120    avg_loss:0.026, val_acc:0.978]
Epoch [107/120    avg_loss:0.028, val_acc:0.978]
Epoch [108/120    avg_loss:0.020, val_acc:0.978]
Epoch [109/120    avg_loss:0.023, val_acc:0.978]
Epoch [110/120    avg_loss:0.019, val_acc:0.978]
Epoch [111/120    avg_loss:0.023, val_acc:0.978]
Epoch [112/120    avg_loss:0.022, val_acc:0.978]
Epoch [113/120    avg_loss:0.013, val_acc:0.978]
Epoch [114/120    avg_loss:0.021, val_acc:0.978]
Epoch [115/120    avg_loss:0.025, val_acc:0.978]
Epoch [116/120    avg_loss:0.021, val_acc:0.978]
Epoch [117/120    avg_loss:0.023, val_acc:0.978]
Epoch [118/120    avg_loss:0.018, val_acc:0.978]
Epoch [119/120    avg_loss:0.029, val_acc:0.978]
Epoch [120/120    avg_loss:0.025, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 219   8   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   2   2 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.97968397 0.97117517 0.9375     0.93069307
 0.99756691 0.95081967 1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9900298396200143
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f740d2cb7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.332, val_acc:0.468]
Epoch [2/120    avg_loss:1.863, val_acc:0.629]
Epoch [3/120    avg_loss:1.487, val_acc:0.671]
Epoch [4/120    avg_loss:1.221, val_acc:0.712]
Epoch [5/120    avg_loss:1.002, val_acc:0.782]
Epoch [6/120    avg_loss:0.947, val_acc:0.748]
Epoch [7/120    avg_loss:0.833, val_acc:0.810]
Epoch [8/120    avg_loss:0.779, val_acc:0.841]
Epoch [9/120    avg_loss:0.757, val_acc:0.825]
Epoch [10/120    avg_loss:0.683, val_acc:0.889]
Epoch [11/120    avg_loss:0.598, val_acc:0.885]
Epoch [12/120    avg_loss:0.516, val_acc:0.903]
Epoch [13/120    avg_loss:0.456, val_acc:0.897]
Epoch [14/120    avg_loss:0.541, val_acc:0.889]
Epoch [15/120    avg_loss:0.470, val_acc:0.895]
Epoch [16/120    avg_loss:0.380, val_acc:0.885]
Epoch [17/120    avg_loss:0.362, val_acc:0.921]
Epoch [18/120    avg_loss:0.344, val_acc:0.937]
Epoch [19/120    avg_loss:0.357, val_acc:0.899]
Epoch [20/120    avg_loss:0.324, val_acc:0.927]
Epoch [21/120    avg_loss:0.364, val_acc:0.911]
Epoch [22/120    avg_loss:0.275, val_acc:0.919]
Epoch [23/120    avg_loss:0.285, val_acc:0.931]
Epoch [24/120    avg_loss:0.267, val_acc:0.907]
Epoch [25/120    avg_loss:0.236, val_acc:0.944]
Epoch [26/120    avg_loss:0.288, val_acc:0.938]
Epoch [27/120    avg_loss:0.248, val_acc:0.937]
Epoch [28/120    avg_loss:0.320, val_acc:0.935]
Epoch [29/120    avg_loss:0.269, val_acc:0.942]
Epoch [30/120    avg_loss:0.208, val_acc:0.925]
Epoch [31/120    avg_loss:0.336, val_acc:0.923]
Epoch [32/120    avg_loss:0.257, val_acc:0.929]
Epoch [33/120    avg_loss:0.254, val_acc:0.929]
Epoch [34/120    avg_loss:0.271, val_acc:0.927]
Epoch [35/120    avg_loss:0.302, val_acc:0.927]
Epoch [36/120    avg_loss:0.213, val_acc:0.954]
Epoch [37/120    avg_loss:0.192, val_acc:0.927]
Epoch [38/120    avg_loss:0.229, val_acc:0.940]
Epoch [39/120    avg_loss:0.234, val_acc:0.940]
Epoch [40/120    avg_loss:0.219, val_acc:0.944]
Epoch [41/120    avg_loss:0.163, val_acc:0.960]
Epoch [42/120    avg_loss:0.134, val_acc:0.954]
Epoch [43/120    avg_loss:0.165, val_acc:0.946]
Epoch [44/120    avg_loss:0.128, val_acc:0.976]
Epoch [45/120    avg_loss:0.119, val_acc:0.952]
Epoch [46/120    avg_loss:0.146, val_acc:0.962]
Epoch [47/120    avg_loss:0.142, val_acc:0.964]
Epoch [48/120    avg_loss:0.150, val_acc:0.937]
Epoch [49/120    avg_loss:0.231, val_acc:0.960]
Epoch [50/120    avg_loss:0.126, val_acc:0.933]
Epoch [51/120    avg_loss:0.108, val_acc:0.970]
Epoch [52/120    avg_loss:0.109, val_acc:0.946]
Epoch [53/120    avg_loss:0.137, val_acc:0.958]
Epoch [54/120    avg_loss:0.096, val_acc:0.966]
Epoch [55/120    avg_loss:0.155, val_acc:0.942]
Epoch [56/120    avg_loss:0.128, val_acc:0.974]
Epoch [57/120    avg_loss:0.127, val_acc:0.958]
Epoch [58/120    avg_loss:0.092, val_acc:0.960]
Epoch [59/120    avg_loss:0.066, val_acc:0.970]
Epoch [60/120    avg_loss:0.068, val_acc:0.976]
Epoch [61/120    avg_loss:0.060, val_acc:0.976]
Epoch [62/120    avg_loss:0.058, val_acc:0.980]
Epoch [63/120    avg_loss:0.054, val_acc:0.978]
Epoch [64/120    avg_loss:0.052, val_acc:0.978]
Epoch [65/120    avg_loss:0.057, val_acc:0.978]
Epoch [66/120    avg_loss:0.072, val_acc:0.978]
Epoch [67/120    avg_loss:0.051, val_acc:0.980]
Epoch [68/120    avg_loss:0.062, val_acc:0.976]
Epoch [69/120    avg_loss:0.044, val_acc:0.980]
Epoch [70/120    avg_loss:0.039, val_acc:0.978]
Epoch [71/120    avg_loss:0.050, val_acc:0.980]
Epoch [72/120    avg_loss:0.045, val_acc:0.978]
Epoch [73/120    avg_loss:0.058, val_acc:0.978]
Epoch [74/120    avg_loss:0.048, val_acc:0.976]
Epoch [75/120    avg_loss:0.053, val_acc:0.980]
Epoch [76/120    avg_loss:0.042, val_acc:0.980]
Epoch [77/120    avg_loss:0.047, val_acc:0.978]
Epoch [78/120    avg_loss:0.037, val_acc:0.978]
Epoch [79/120    avg_loss:0.037, val_acc:0.982]
Epoch [80/120    avg_loss:0.042, val_acc:0.980]
Epoch [81/120    avg_loss:0.049, val_acc:0.980]
Epoch [82/120    avg_loss:0.032, val_acc:0.980]
Epoch [83/120    avg_loss:0.038, val_acc:0.980]
Epoch [84/120    avg_loss:0.043, val_acc:0.980]
Epoch [85/120    avg_loss:0.042, val_acc:0.978]
Epoch [86/120    avg_loss:0.035, val_acc:0.982]
Epoch [87/120    avg_loss:0.030, val_acc:0.980]
Epoch [88/120    avg_loss:0.040, val_acc:0.980]
Epoch [89/120    avg_loss:0.041, val_acc:0.984]
Epoch [90/120    avg_loss:0.041, val_acc:0.982]
Epoch [91/120    avg_loss:0.033, val_acc:0.978]
Epoch [92/120    avg_loss:0.037, val_acc:0.978]
Epoch [93/120    avg_loss:0.039, val_acc:0.976]
Epoch [94/120    avg_loss:0.045, val_acc:0.978]
Epoch [95/120    avg_loss:0.056, val_acc:0.976]
Epoch [96/120    avg_loss:0.040, val_acc:0.980]
Epoch [97/120    avg_loss:0.034, val_acc:0.978]
Epoch [98/120    avg_loss:0.044, val_acc:0.978]
Epoch [99/120    avg_loss:0.038, val_acc:0.980]
Epoch [100/120    avg_loss:0.038, val_acc:0.982]
Epoch [101/120    avg_loss:0.029, val_acc:0.982]
Epoch [102/120    avg_loss:0.042, val_acc:0.982]
Epoch [103/120    avg_loss:0.034, val_acc:0.982]
Epoch [104/120    avg_loss:0.035, val_acc:0.982]
Epoch [105/120    avg_loss:0.030, val_acc:0.982]
Epoch [106/120    avg_loss:0.030, val_acc:0.982]
Epoch [107/120    avg_loss:0.030, val_acc:0.982]
Epoch [108/120    avg_loss:0.037, val_acc:0.982]
Epoch [109/120    avg_loss:0.042, val_acc:0.982]
Epoch [110/120    avg_loss:0.031, val_acc:0.982]
Epoch [111/120    avg_loss:0.042, val_acc:0.982]
Epoch [112/120    avg_loss:0.036, val_acc:0.982]
Epoch [113/120    avg_loss:0.030, val_acc:0.982]
Epoch [114/120    avg_loss:0.036, val_acc:0.982]
Epoch [115/120    avg_loss:0.030, val_acc:0.982]
Epoch [116/120    avg_loss:0.031, val_acc:0.982]
Epoch [117/120    avg_loss:0.036, val_acc:0.982]
Epoch [118/120    avg_loss:0.027, val_acc:0.982]
Epoch [119/120    avg_loss:0.031, val_acc:0.982]
Epoch [120/120    avg_loss:0.036, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   1   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 132   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   6 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.97986577 0.98678414 0.96995708 0.94964029
 0.99757869 0.96132597 0.99487179 0.99893276 1.         0.99210526
 0.99109131 1.        ]

Kappa:
0.9916909000650909
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feea32c37f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.306, val_acc:0.526]
Epoch [2/120    avg_loss:1.871, val_acc:0.617]
Epoch [3/120    avg_loss:1.553, val_acc:0.633]
Epoch [4/120    avg_loss:1.311, val_acc:0.782]
Epoch [5/120    avg_loss:1.118, val_acc:0.722]
Epoch [6/120    avg_loss:0.935, val_acc:0.760]
Epoch [7/120    avg_loss:0.877, val_acc:0.865]
Epoch [8/120    avg_loss:0.754, val_acc:0.849]
Epoch [9/120    avg_loss:0.677, val_acc:0.849]
Epoch [10/120    avg_loss:0.654, val_acc:0.802]
Epoch [11/120    avg_loss:0.572, val_acc:0.887]
Epoch [12/120    avg_loss:0.529, val_acc:0.877]
Epoch [13/120    avg_loss:0.501, val_acc:0.897]
Epoch [14/120    avg_loss:0.537, val_acc:0.881]
Epoch [15/120    avg_loss:0.535, val_acc:0.873]
Epoch [16/120    avg_loss:0.452, val_acc:0.897]
Epoch [17/120    avg_loss:0.437, val_acc:0.895]
Epoch [18/120    avg_loss:0.442, val_acc:0.831]
Epoch [19/120    avg_loss:0.386, val_acc:0.885]
Epoch [20/120    avg_loss:0.409, val_acc:0.919]
Epoch [21/120    avg_loss:0.435, val_acc:0.907]
Epoch [22/120    avg_loss:0.301, val_acc:0.899]
Epoch [23/120    avg_loss:0.339, val_acc:0.931]
Epoch [24/120    avg_loss:0.326, val_acc:0.794]
Epoch [25/120    avg_loss:0.323, val_acc:0.917]
Epoch [26/120    avg_loss:0.341, val_acc:0.909]
Epoch [27/120    avg_loss:0.281, val_acc:0.925]
Epoch [28/120    avg_loss:0.288, val_acc:0.909]
Epoch [29/120    avg_loss:0.271, val_acc:0.948]
Epoch [30/120    avg_loss:0.241, val_acc:0.948]
Epoch [31/120    avg_loss:0.212, val_acc:0.952]
Epoch [32/120    avg_loss:0.223, val_acc:0.960]
Epoch [33/120    avg_loss:0.272, val_acc:0.923]
Epoch [34/120    avg_loss:0.215, val_acc:0.954]
Epoch [35/120    avg_loss:0.289, val_acc:0.935]
Epoch [36/120    avg_loss:0.226, val_acc:0.923]
Epoch [37/120    avg_loss:0.204, val_acc:0.950]
Epoch [38/120    avg_loss:0.164, val_acc:0.944]
Epoch [39/120    avg_loss:0.199, val_acc:0.935]
Epoch [40/120    avg_loss:0.202, val_acc:0.927]
Epoch [41/120    avg_loss:0.172, val_acc:0.958]
Epoch [42/120    avg_loss:0.161, val_acc:0.958]
Epoch [43/120    avg_loss:0.134, val_acc:0.948]
Epoch [44/120    avg_loss:0.140, val_acc:0.891]
Epoch [45/120    avg_loss:0.256, val_acc:0.935]
Epoch [46/120    avg_loss:0.160, val_acc:0.938]
Epoch [47/120    avg_loss:0.105, val_acc:0.960]
Epoch [48/120    avg_loss:0.101, val_acc:0.966]
Epoch [49/120    avg_loss:0.083, val_acc:0.964]
Epoch [50/120    avg_loss:0.099, val_acc:0.964]
Epoch [51/120    avg_loss:0.092, val_acc:0.966]
Epoch [52/120    avg_loss:0.099, val_acc:0.964]
Epoch [53/120    avg_loss:0.089, val_acc:0.968]
Epoch [54/120    avg_loss:0.088, val_acc:0.966]
Epoch [55/120    avg_loss:0.078, val_acc:0.970]
Epoch [56/120    avg_loss:0.067, val_acc:0.972]
Epoch [57/120    avg_loss:0.092, val_acc:0.972]
Epoch [58/120    avg_loss:0.079, val_acc:0.970]
Epoch [59/120    avg_loss:0.084, val_acc:0.972]
Epoch [60/120    avg_loss:0.070, val_acc:0.974]
Epoch [61/120    avg_loss:0.079, val_acc:0.974]
Epoch [62/120    avg_loss:0.082, val_acc:0.974]
Epoch [63/120    avg_loss:0.078, val_acc:0.972]
Epoch [64/120    avg_loss:0.062, val_acc:0.972]
Epoch [65/120    avg_loss:0.080, val_acc:0.974]
Epoch [66/120    avg_loss:0.064, val_acc:0.974]
Epoch [67/120    avg_loss:0.068, val_acc:0.974]
Epoch [68/120    avg_loss:0.068, val_acc:0.974]
Epoch [69/120    avg_loss:0.063, val_acc:0.974]
Epoch [70/120    avg_loss:0.058, val_acc:0.974]
Epoch [71/120    avg_loss:0.061, val_acc:0.974]
Epoch [72/120    avg_loss:0.054, val_acc:0.976]
Epoch [73/120    avg_loss:0.067, val_acc:0.974]
Epoch [74/120    avg_loss:0.083, val_acc:0.970]
Epoch [75/120    avg_loss:0.088, val_acc:0.970]
Epoch [76/120    avg_loss:0.064, val_acc:0.972]
Epoch [77/120    avg_loss:0.060, val_acc:0.974]
Epoch [78/120    avg_loss:0.057, val_acc:0.974]
Epoch [79/120    avg_loss:0.058, val_acc:0.976]
Epoch [80/120    avg_loss:0.073, val_acc:0.972]
Epoch [81/120    avg_loss:0.064, val_acc:0.974]
Epoch [82/120    avg_loss:0.070, val_acc:0.974]
Epoch [83/120    avg_loss:0.061, val_acc:0.972]
Epoch [84/120    avg_loss:0.049, val_acc:0.976]
Epoch [85/120    avg_loss:0.052, val_acc:0.974]
Epoch [86/120    avg_loss:0.063, val_acc:0.974]
Epoch [87/120    avg_loss:0.057, val_acc:0.974]
Epoch [88/120    avg_loss:0.054, val_acc:0.978]
Epoch [89/120    avg_loss:0.065, val_acc:0.978]
Epoch [90/120    avg_loss:0.052, val_acc:0.982]
Epoch [91/120    avg_loss:0.058, val_acc:0.978]
Epoch [92/120    avg_loss:0.047, val_acc:0.980]
Epoch [93/120    avg_loss:0.054, val_acc:0.974]
Epoch [94/120    avg_loss:0.050, val_acc:0.978]
Epoch [95/120    avg_loss:0.053, val_acc:0.972]
Epoch [96/120    avg_loss:0.053, val_acc:0.976]
Epoch [97/120    avg_loss:0.045, val_acc:0.978]
Epoch [98/120    avg_loss:0.069, val_acc:0.976]
Epoch [99/120    avg_loss:0.055, val_acc:0.978]
Epoch [100/120    avg_loss:0.054, val_acc:0.976]
Epoch [101/120    avg_loss:0.068, val_acc:0.974]
Epoch [102/120    avg_loss:0.049, val_acc:0.982]
Epoch [103/120    avg_loss:0.053, val_acc:0.980]
Epoch [104/120    avg_loss:0.046, val_acc:0.980]
Epoch [105/120    avg_loss:0.056, val_acc:0.982]
Epoch [106/120    avg_loss:0.043, val_acc:0.980]
Epoch [107/120    avg_loss:0.052, val_acc:0.976]
Epoch [108/120    avg_loss:0.034, val_acc:0.978]
Epoch [109/120    avg_loss:0.044, val_acc:0.976]
Epoch [110/120    avg_loss:0.040, val_acc:0.976]
Epoch [111/120    avg_loss:0.042, val_acc:0.978]
Epoch [112/120    avg_loss:0.063, val_acc:0.980]
Epoch [113/120    avg_loss:0.061, val_acc:0.980]
Epoch [114/120    avg_loss:0.043, val_acc:0.974]
Epoch [115/120    avg_loss:0.037, val_acc:0.976]
Epoch [116/120    avg_loss:0.038, val_acc:0.974]
Epoch [117/120    avg_loss:0.044, val_acc:0.974]
Epoch [118/120    avg_loss:0.036, val_acc:0.974]
Epoch [119/120    avg_loss:0.058, val_acc:0.974]
Epoch [120/120    avg_loss:0.042, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   1 221   5   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   2  10 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.98181818 0.97571744 0.92682927 0.89864865
 1.         0.96256684 0.99742931 0.99893276 1.         0.99210526
 0.99333333 1.        ]

Kappa:
0.9876563449084877
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f674df30748>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.361, val_acc:0.516]
Epoch [2/120    avg_loss:1.844, val_acc:0.623]
Epoch [3/120    avg_loss:1.533, val_acc:0.692]
Epoch [4/120    avg_loss:1.278, val_acc:0.806]
Epoch [5/120    avg_loss:1.109, val_acc:0.831]
Epoch [6/120    avg_loss:0.964, val_acc:0.794]
Epoch [7/120    avg_loss:0.848, val_acc:0.845]
Epoch [8/120    avg_loss:0.741, val_acc:0.845]
Epoch [9/120    avg_loss:0.704, val_acc:0.863]
Epoch [10/120    avg_loss:0.630, val_acc:0.865]
Epoch [11/120    avg_loss:0.554, val_acc:0.913]
Epoch [12/120    avg_loss:0.531, val_acc:0.905]
Epoch [13/120    avg_loss:0.470, val_acc:0.913]
Epoch [14/120    avg_loss:0.476, val_acc:0.893]
Epoch [15/120    avg_loss:0.403, val_acc:0.919]
Epoch [16/120    avg_loss:0.406, val_acc:0.875]
Epoch [17/120    avg_loss:0.398, val_acc:0.909]
Epoch [18/120    avg_loss:0.473, val_acc:0.887]
Epoch [19/120    avg_loss:0.426, val_acc:0.923]
Epoch [20/120    avg_loss:0.370, val_acc:0.895]
Epoch [21/120    avg_loss:0.334, val_acc:0.933]
Epoch [22/120    avg_loss:0.367, val_acc:0.887]
Epoch [23/120    avg_loss:0.420, val_acc:0.925]
Epoch [24/120    avg_loss:0.346, val_acc:0.923]
Epoch [25/120    avg_loss:0.309, val_acc:0.925]
Epoch [26/120    avg_loss:0.263, val_acc:0.931]
Epoch [27/120    avg_loss:0.229, val_acc:0.933]
Epoch [28/120    avg_loss:0.223, val_acc:0.950]
Epoch [29/120    avg_loss:0.234, val_acc:0.929]
Epoch [30/120    avg_loss:0.285, val_acc:0.935]
Epoch [31/120    avg_loss:0.240, val_acc:0.938]
Epoch [32/120    avg_loss:0.218, val_acc:0.933]
Epoch [33/120    avg_loss:0.210, val_acc:0.950]
Epoch [34/120    avg_loss:0.233, val_acc:0.938]
Epoch [35/120    avg_loss:0.383, val_acc:0.927]
Epoch [36/120    avg_loss:0.261, val_acc:0.927]
Epoch [37/120    avg_loss:0.227, val_acc:0.942]
Epoch [38/120    avg_loss:0.214, val_acc:0.938]
Epoch [39/120    avg_loss:0.187, val_acc:0.938]
Epoch [40/120    avg_loss:0.177, val_acc:0.944]
Epoch [41/120    avg_loss:0.142, val_acc:0.942]
Epoch [42/120    avg_loss:0.195, val_acc:0.962]
Epoch [43/120    avg_loss:0.153, val_acc:0.956]
Epoch [44/120    avg_loss:0.143, val_acc:0.940]
Epoch [45/120    avg_loss:0.169, val_acc:0.940]
Epoch [46/120    avg_loss:0.148, val_acc:0.948]
Epoch [47/120    avg_loss:0.127, val_acc:0.964]
Epoch [48/120    avg_loss:0.159, val_acc:0.950]
Epoch [49/120    avg_loss:0.116, val_acc:0.964]
Epoch [50/120    avg_loss:0.114, val_acc:0.960]
Epoch [51/120    avg_loss:0.081, val_acc:0.938]
Epoch [52/120    avg_loss:0.127, val_acc:0.966]
Epoch [53/120    avg_loss:0.067, val_acc:0.956]
Epoch [54/120    avg_loss:0.083, val_acc:0.972]
Epoch [55/120    avg_loss:0.090, val_acc:0.954]
Epoch [56/120    avg_loss:0.153, val_acc:0.931]
Epoch [57/120    avg_loss:0.101, val_acc:0.946]
Epoch [58/120    avg_loss:0.131, val_acc:0.927]
Epoch [59/120    avg_loss:0.111, val_acc:0.933]
Epoch [60/120    avg_loss:0.140, val_acc:0.962]
Epoch [61/120    avg_loss:0.097, val_acc:0.964]
Epoch [62/120    avg_loss:0.125, val_acc:0.954]
Epoch [63/120    avg_loss:0.082, val_acc:0.966]
Epoch [64/120    avg_loss:0.072, val_acc:0.964]
Epoch [65/120    avg_loss:0.064, val_acc:0.976]
Epoch [66/120    avg_loss:0.057, val_acc:0.974]
Epoch [67/120    avg_loss:0.059, val_acc:0.970]
Epoch [68/120    avg_loss:0.057, val_acc:0.968]
Epoch [69/120    avg_loss:0.060, val_acc:0.966]
Epoch [70/120    avg_loss:0.070, val_acc:0.970]
Epoch [71/120    avg_loss:0.076, val_acc:0.944]
Epoch [72/120    avg_loss:0.096, val_acc:0.974]
Epoch [73/120    avg_loss:0.229, val_acc:0.948]
Epoch [74/120    avg_loss:0.118, val_acc:0.966]
Epoch [75/120    avg_loss:0.081, val_acc:0.952]
Epoch [76/120    avg_loss:0.062, val_acc:0.972]
Epoch [77/120    avg_loss:0.046, val_acc:0.970]
Epoch [78/120    avg_loss:0.066, val_acc:0.974]
Epoch [79/120    avg_loss:0.031, val_acc:0.976]
Epoch [80/120    avg_loss:0.031, val_acc:0.980]
Epoch [81/120    avg_loss:0.030, val_acc:0.982]
Epoch [82/120    avg_loss:0.033, val_acc:0.980]
Epoch [83/120    avg_loss:0.022, val_acc:0.980]
Epoch [84/120    avg_loss:0.027, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.980]
Epoch [86/120    avg_loss:0.020, val_acc:0.980]
Epoch [87/120    avg_loss:0.026, val_acc:0.974]
Epoch [88/120    avg_loss:0.024, val_acc:0.976]
Epoch [89/120    avg_loss:0.023, val_acc:0.978]
Epoch [90/120    avg_loss:0.026, val_acc:0.978]
Epoch [91/120    avg_loss:0.030, val_acc:0.980]
Epoch [92/120    avg_loss:0.023, val_acc:0.980]
Epoch [93/120    avg_loss:0.022, val_acc:0.978]
Epoch [94/120    avg_loss:0.023, val_acc:0.978]
Epoch [95/120    avg_loss:0.026, val_acc:0.978]
Epoch [96/120    avg_loss:0.027, val_acc:0.978]
Epoch [97/120    avg_loss:0.019, val_acc:0.978]
Epoch [98/120    avg_loss:0.015, val_acc:0.978]
Epoch [99/120    avg_loss:0.019, val_acc:0.978]
Epoch [100/120    avg_loss:0.021, val_acc:0.978]
Epoch [101/120    avg_loss:0.022, val_acc:0.978]
Epoch [102/120    avg_loss:0.019, val_acc:0.978]
Epoch [103/120    avg_loss:0.022, val_acc:0.978]
Epoch [104/120    avg_loss:0.017, val_acc:0.978]
Epoch [105/120    avg_loss:0.024, val_acc:0.980]
Epoch [106/120    avg_loss:0.021, val_acc:0.978]
Epoch [107/120    avg_loss:0.022, val_acc:0.978]
Epoch [108/120    avg_loss:0.021, val_acc:0.978]
Epoch [109/120    avg_loss:0.024, val_acc:0.978]
Epoch [110/120    avg_loss:0.020, val_acc:0.978]
Epoch [111/120    avg_loss:0.030, val_acc:0.978]
Epoch [112/120    avg_loss:0.016, val_acc:0.978]
Epoch [113/120    avg_loss:0.020, val_acc:0.978]
Epoch [114/120    avg_loss:0.036, val_acc:0.978]
Epoch [115/120    avg_loss:0.029, val_acc:0.978]
Epoch [116/120    avg_loss:0.028, val_acc:0.978]
Epoch [117/120    avg_loss:0.028, val_acc:0.978]
Epoch [118/120    avg_loss:0.026, val_acc:0.978]
Epoch [119/120    avg_loss:0.020, val_acc:0.978]
Epoch [120/120    avg_loss:0.025, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 206  21   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.52878464818762

F1 scores:
[       nan 1.         0.98642534 0.94495413 0.88517745 0.90784983
 0.98271605 0.9673913  1.         0.99680511 1.         0.99339498
 0.9944629  1.        ]

Kappa:
0.983619996471018
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf1f8a67b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.338, val_acc:0.524]
Epoch [2/120    avg_loss:1.842, val_acc:0.599]
Epoch [3/120    avg_loss:1.475, val_acc:0.692]
Epoch [4/120    avg_loss:1.223, val_acc:0.720]
Epoch [5/120    avg_loss:1.047, val_acc:0.768]
Epoch [6/120    avg_loss:0.919, val_acc:0.841]
Epoch [7/120    avg_loss:0.830, val_acc:0.843]
Epoch [8/120    avg_loss:0.759, val_acc:0.796]
Epoch [9/120    avg_loss:0.625, val_acc:0.851]
Epoch [10/120    avg_loss:0.559, val_acc:0.821]
Epoch [11/120    avg_loss:0.583, val_acc:0.871]
Epoch [12/120    avg_loss:0.490, val_acc:0.847]
Epoch [13/120    avg_loss:0.492, val_acc:0.903]
Epoch [14/120    avg_loss:0.443, val_acc:0.925]
Epoch [15/120    avg_loss:0.396, val_acc:0.889]
Epoch [16/120    avg_loss:0.453, val_acc:0.919]
Epoch [17/120    avg_loss:0.380, val_acc:0.919]
Epoch [18/120    avg_loss:0.375, val_acc:0.897]
Epoch [19/120    avg_loss:0.387, val_acc:0.913]
Epoch [20/120    avg_loss:0.372, val_acc:0.921]
Epoch [21/120    avg_loss:0.314, val_acc:0.942]
Epoch [22/120    avg_loss:0.317, val_acc:0.937]
Epoch [23/120    avg_loss:0.313, val_acc:0.935]
Epoch [24/120    avg_loss:0.306, val_acc:0.921]
Epoch [25/120    avg_loss:0.361, val_acc:0.927]
Epoch [26/120    avg_loss:0.368, val_acc:0.909]
Epoch [27/120    avg_loss:0.335, val_acc:0.942]
Epoch [28/120    avg_loss:0.311, val_acc:0.897]
Epoch [29/120    avg_loss:0.267, val_acc:0.935]
Epoch [30/120    avg_loss:0.283, val_acc:0.925]
Epoch [31/120    avg_loss:0.296, val_acc:0.942]
Epoch [32/120    avg_loss:0.278, val_acc:0.944]
Epoch [33/120    avg_loss:0.183, val_acc:0.974]
Epoch [34/120    avg_loss:0.175, val_acc:0.960]
Epoch [35/120    avg_loss:0.157, val_acc:0.964]
Epoch [36/120    avg_loss:0.214, val_acc:0.935]
Epoch [37/120    avg_loss:0.349, val_acc:0.903]
Epoch [38/120    avg_loss:0.341, val_acc:0.946]
Epoch [39/120    avg_loss:0.234, val_acc:0.966]
Epoch [40/120    avg_loss:0.264, val_acc:0.899]
Epoch [41/120    avg_loss:0.245, val_acc:0.958]
Epoch [42/120    avg_loss:0.191, val_acc:0.946]
Epoch [43/120    avg_loss:0.157, val_acc:0.960]
Epoch [44/120    avg_loss:0.204, val_acc:0.956]
Epoch [45/120    avg_loss:0.142, val_acc:0.958]
Epoch [46/120    avg_loss:0.147, val_acc:0.966]
Epoch [47/120    avg_loss:0.101, val_acc:0.972]
Epoch [48/120    avg_loss:0.124, val_acc:0.976]
Epoch [49/120    avg_loss:0.087, val_acc:0.974]
Epoch [50/120    avg_loss:0.086, val_acc:0.976]
Epoch [51/120    avg_loss:0.084, val_acc:0.980]
Epoch [52/120    avg_loss:0.071, val_acc:0.980]
Epoch [53/120    avg_loss:0.063, val_acc:0.980]
Epoch [54/120    avg_loss:0.078, val_acc:0.976]
Epoch [55/120    avg_loss:0.074, val_acc:0.978]
Epoch [56/120    avg_loss:0.065, val_acc:0.982]
Epoch [57/120    avg_loss:0.062, val_acc:0.980]
Epoch [58/120    avg_loss:0.068, val_acc:0.980]
Epoch [59/120    avg_loss:0.073, val_acc:0.978]
Epoch [60/120    avg_loss:0.070, val_acc:0.980]
Epoch [61/120    avg_loss:0.079, val_acc:0.986]
Epoch [62/120    avg_loss:0.063, val_acc:0.982]
Epoch [63/120    avg_loss:0.058, val_acc:0.984]
Epoch [64/120    avg_loss:0.054, val_acc:0.976]
Epoch [65/120    avg_loss:0.070, val_acc:0.980]
Epoch [66/120    avg_loss:0.075, val_acc:0.986]
Epoch [67/120    avg_loss:0.067, val_acc:0.982]
Epoch [68/120    avg_loss:0.073, val_acc:0.978]
Epoch [69/120    avg_loss:0.061, val_acc:0.978]
Epoch [70/120    avg_loss:0.053, val_acc:0.986]
Epoch [71/120    avg_loss:0.059, val_acc:0.988]
Epoch [72/120    avg_loss:0.065, val_acc:0.986]
Epoch [73/120    avg_loss:0.059, val_acc:0.986]
Epoch [74/120    avg_loss:0.057, val_acc:0.988]
Epoch [75/120    avg_loss:0.064, val_acc:0.986]
Epoch [76/120    avg_loss:0.047, val_acc:0.984]
Epoch [77/120    avg_loss:0.049, val_acc:0.984]
Epoch [78/120    avg_loss:0.069, val_acc:0.984]
Epoch [79/120    avg_loss:0.061, val_acc:0.986]
Epoch [80/120    avg_loss:0.052, val_acc:0.986]
Epoch [81/120    avg_loss:0.073, val_acc:0.988]
Epoch [82/120    avg_loss:0.044, val_acc:0.984]
Epoch [83/120    avg_loss:0.047, val_acc:0.986]
Epoch [84/120    avg_loss:0.055, val_acc:0.988]
Epoch [85/120    avg_loss:0.058, val_acc:0.986]
Epoch [86/120    avg_loss:0.055, val_acc:0.988]
Epoch [87/120    avg_loss:0.057, val_acc:0.990]
Epoch [88/120    avg_loss:0.041, val_acc:0.984]
Epoch [89/120    avg_loss:0.052, val_acc:0.982]
Epoch [90/120    avg_loss:0.061, val_acc:0.990]
Epoch [91/120    avg_loss:0.050, val_acc:0.990]
Epoch [92/120    avg_loss:0.053, val_acc:0.986]
Epoch [93/120    avg_loss:0.047, val_acc:0.988]
Epoch [94/120    avg_loss:0.049, val_acc:0.988]
Epoch [95/120    avg_loss:0.044, val_acc:0.986]
Epoch [96/120    avg_loss:0.051, val_acc:0.984]
Epoch [97/120    avg_loss:0.043, val_acc:0.984]
Epoch [98/120    avg_loss:0.064, val_acc:0.982]
Epoch [99/120    avg_loss:0.048, val_acc:0.988]
Epoch [100/120    avg_loss:0.049, val_acc:0.988]
Epoch [101/120    avg_loss:0.044, val_acc:0.984]
Epoch [102/120    avg_loss:0.045, val_acc:0.986]
Epoch [103/120    avg_loss:0.050, val_acc:0.986]
Epoch [104/120    avg_loss:0.054, val_acc:0.988]
Epoch [105/120    avg_loss:0.040, val_acc:0.988]
Epoch [106/120    avg_loss:0.054, val_acc:0.990]
Epoch [107/120    avg_loss:0.056, val_acc:0.990]
Epoch [108/120    avg_loss:0.048, val_acc:0.990]
Epoch [109/120    avg_loss:0.050, val_acc:0.990]
Epoch [110/120    avg_loss:0.049, val_acc:0.990]
Epoch [111/120    avg_loss:0.042, val_acc:0.990]
Epoch [112/120    avg_loss:0.045, val_acc:0.990]
Epoch [113/120    avg_loss:0.050, val_acc:0.990]
Epoch [114/120    avg_loss:0.045, val_acc:0.988]
Epoch [115/120    avg_loss:0.041, val_acc:0.988]
Epoch [116/120    avg_loss:0.053, val_acc:0.988]
Epoch [117/120    avg_loss:0.042, val_acc:0.988]
Epoch [118/120    avg_loss:0.050, val_acc:0.988]
Epoch [119/120    avg_loss:0.060, val_acc:0.988]
Epoch [120/120    avg_loss:0.040, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.96583144 0.98901099 0.93693694 0.92459016
 1.         0.9197861  1.         1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9881319196096215
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b90f7f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.332, val_acc:0.474]
Epoch [2/120    avg_loss:1.879, val_acc:0.538]
Epoch [3/120    avg_loss:1.528, val_acc:0.681]
Epoch [4/120    avg_loss:1.292, val_acc:0.724]
Epoch [5/120    avg_loss:1.034, val_acc:0.764]
Epoch [6/120    avg_loss:0.871, val_acc:0.812]
Epoch [7/120    avg_loss:0.747, val_acc:0.891]
Epoch [8/120    avg_loss:0.676, val_acc:0.887]
Epoch [9/120    avg_loss:0.600, val_acc:0.907]
Epoch [10/120    avg_loss:0.653, val_acc:0.885]
Epoch [11/120    avg_loss:0.522, val_acc:0.915]
Epoch [12/120    avg_loss:0.495, val_acc:0.885]
Epoch [13/120    avg_loss:0.484, val_acc:0.905]
Epoch [14/120    avg_loss:0.449, val_acc:0.919]
Epoch [15/120    avg_loss:0.417, val_acc:0.911]
Epoch [16/120    avg_loss:0.391, val_acc:0.911]
Epoch [17/120    avg_loss:0.378, val_acc:0.927]
Epoch [18/120    avg_loss:0.348, val_acc:0.929]
Epoch [19/120    avg_loss:0.344, val_acc:0.925]
Epoch [20/120    avg_loss:0.383, val_acc:0.921]
Epoch [21/120    avg_loss:0.372, val_acc:0.913]
Epoch [22/120    avg_loss:0.306, val_acc:0.931]
Epoch [23/120    avg_loss:0.376, val_acc:0.913]
Epoch [24/120    avg_loss:0.292, val_acc:0.929]
Epoch [25/120    avg_loss:0.369, val_acc:0.917]
Epoch [26/120    avg_loss:0.394, val_acc:0.915]
Epoch [27/120    avg_loss:0.308, val_acc:0.901]
Epoch [28/120    avg_loss:0.259, val_acc:0.940]
Epoch [29/120    avg_loss:0.232, val_acc:0.938]
Epoch [30/120    avg_loss:0.264, val_acc:0.942]
Epoch [31/120    avg_loss:0.225, val_acc:0.931]
Epoch [32/120    avg_loss:0.341, val_acc:0.907]
Epoch [33/120    avg_loss:0.255, val_acc:0.915]
Epoch [34/120    avg_loss:0.273, val_acc:0.935]
Epoch [35/120    avg_loss:0.224, val_acc:0.946]
Epoch [36/120    avg_loss:0.178, val_acc:0.956]
Epoch [37/120    avg_loss:0.188, val_acc:0.954]
Epoch [38/120    avg_loss:0.216, val_acc:0.942]
Epoch [39/120    avg_loss:0.184, val_acc:0.956]
Epoch [40/120    avg_loss:0.189, val_acc:0.929]
Epoch [41/120    avg_loss:0.226, val_acc:0.958]
Epoch [42/120    avg_loss:0.160, val_acc:0.954]
Epoch [43/120    avg_loss:0.158, val_acc:0.966]
Epoch [44/120    avg_loss:0.193, val_acc:0.940]
Epoch [45/120    avg_loss:0.161, val_acc:0.962]
Epoch [46/120    avg_loss:0.143, val_acc:0.954]
Epoch [47/120    avg_loss:0.170, val_acc:0.956]
Epoch [48/120    avg_loss:0.111, val_acc:0.962]
Epoch [49/120    avg_loss:0.137, val_acc:0.964]
Epoch [50/120    avg_loss:0.172, val_acc:0.966]
Epoch [51/120    avg_loss:0.077, val_acc:0.970]
Epoch [52/120    avg_loss:0.093, val_acc:0.962]
Epoch [53/120    avg_loss:0.102, val_acc:0.962]
Epoch [54/120    avg_loss:0.159, val_acc:0.942]
Epoch [55/120    avg_loss:0.127, val_acc:0.958]
Epoch [56/120    avg_loss:0.142, val_acc:0.935]
Epoch [57/120    avg_loss:0.167, val_acc:0.960]
Epoch [58/120    avg_loss:0.096, val_acc:0.942]
Epoch [59/120    avg_loss:0.094, val_acc:0.984]
Epoch [60/120    avg_loss:0.081, val_acc:0.976]
Epoch [61/120    avg_loss:0.102, val_acc:0.974]
Epoch [62/120    avg_loss:0.064, val_acc:0.974]
Epoch [63/120    avg_loss:0.105, val_acc:0.978]
Epoch [64/120    avg_loss:0.053, val_acc:0.980]
Epoch [65/120    avg_loss:0.058, val_acc:0.970]
Epoch [66/120    avg_loss:0.050, val_acc:0.976]
Epoch [67/120    avg_loss:0.051, val_acc:0.974]
Epoch [68/120    avg_loss:0.042, val_acc:0.980]
Epoch [69/120    avg_loss:0.049, val_acc:0.978]
Epoch [70/120    avg_loss:0.078, val_acc:0.968]
Epoch [71/120    avg_loss:0.081, val_acc:0.978]
Epoch [72/120    avg_loss:0.030, val_acc:0.976]
Epoch [73/120    avg_loss:0.042, val_acc:0.980]
Epoch [74/120    avg_loss:0.020, val_acc:0.982]
Epoch [75/120    avg_loss:0.032, val_acc:0.984]
Epoch [76/120    avg_loss:0.029, val_acc:0.982]
Epoch [77/120    avg_loss:0.026, val_acc:0.982]
Epoch [78/120    avg_loss:0.024, val_acc:0.984]
Epoch [79/120    avg_loss:0.023, val_acc:0.986]
Epoch [80/120    avg_loss:0.022, val_acc:0.986]
Epoch [81/120    avg_loss:0.024, val_acc:0.988]
Epoch [82/120    avg_loss:0.021, val_acc:0.986]
Epoch [83/120    avg_loss:0.028, val_acc:0.986]
Epoch [84/120    avg_loss:0.020, val_acc:0.986]
Epoch [85/120    avg_loss:0.020, val_acc:0.986]
Epoch [86/120    avg_loss:0.027, val_acc:0.986]
Epoch [87/120    avg_loss:0.043, val_acc:0.988]
Epoch [88/120    avg_loss:0.030, val_acc:0.986]
Epoch [89/120    avg_loss:0.023, val_acc:0.988]
Epoch [90/120    avg_loss:0.027, val_acc:0.986]
Epoch [91/120    avg_loss:0.018, val_acc:0.988]
Epoch [92/120    avg_loss:0.028, val_acc:0.988]
Epoch [93/120    avg_loss:0.018, val_acc:0.988]
Epoch [94/120    avg_loss:0.027, val_acc:0.990]
Epoch [95/120    avg_loss:0.027, val_acc:0.990]
Epoch [96/120    avg_loss:0.017, val_acc:0.990]
Epoch [97/120    avg_loss:0.028, val_acc:0.990]
Epoch [98/120    avg_loss:0.023, val_acc:0.988]
Epoch [99/120    avg_loss:0.025, val_acc:0.986]
Epoch [100/120    avg_loss:0.033, val_acc:0.990]
Epoch [101/120    avg_loss:0.014, val_acc:0.990]
Epoch [102/120    avg_loss:0.019, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.990]
Epoch [104/120    avg_loss:0.021, val_acc:0.988]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.022, val_acc:0.982]
Epoch [107/120    avg_loss:0.020, val_acc:0.986]
Epoch [108/120    avg_loss:0.020, val_acc:0.988]
Epoch [109/120    avg_loss:0.017, val_acc:0.986]
Epoch [110/120    avg_loss:0.020, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.986]
Epoch [112/120    avg_loss:0.016, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.988]
Epoch [114/120    avg_loss:0.019, val_acc:0.988]
Epoch [115/120    avg_loss:0.022, val_acc:0.986]
Epoch [116/120    avg_loss:0.031, val_acc:0.986]
Epoch [117/120    avg_loss:0.019, val_acc:0.986]
Epoch [118/120    avg_loss:0.021, val_acc:0.986]
Epoch [119/120    avg_loss:0.024, val_acc:0.986]
Epoch [120/120    avg_loss:0.016, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 206  23   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 1.         0.99086758 0.94495413 0.8907563  0.90034364
 1.         0.9787234  1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9864691974947998
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f318ed39860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.498]
Epoch [2/120    avg_loss:1.905, val_acc:0.563]
Epoch [3/120    avg_loss:1.566, val_acc:0.708]
Epoch [4/120    avg_loss:1.271, val_acc:0.726]
Epoch [5/120    avg_loss:1.031, val_acc:0.794]
Epoch [6/120    avg_loss:0.854, val_acc:0.829]
Epoch [7/120    avg_loss:0.714, val_acc:0.903]
Epoch [8/120    avg_loss:0.659, val_acc:0.907]
Epoch [9/120    avg_loss:0.586, val_acc:0.863]
Epoch [10/120    avg_loss:0.532, val_acc:0.891]
Epoch [11/120    avg_loss:0.513, val_acc:0.911]
Epoch [12/120    avg_loss:0.509, val_acc:0.861]
Epoch [13/120    avg_loss:0.501, val_acc:0.875]
Epoch [14/120    avg_loss:0.503, val_acc:0.909]
Epoch [15/120    avg_loss:0.441, val_acc:0.917]
Epoch [16/120    avg_loss:0.388, val_acc:0.921]
Epoch [17/120    avg_loss:0.411, val_acc:0.925]
Epoch [18/120    avg_loss:0.385, val_acc:0.919]
Epoch [19/120    avg_loss:0.366, val_acc:0.931]
Epoch [20/120    avg_loss:0.333, val_acc:0.883]
Epoch [21/120    avg_loss:0.343, val_acc:0.883]
Epoch [22/120    avg_loss:0.316, val_acc:0.948]
Epoch [23/120    avg_loss:0.271, val_acc:0.960]
Epoch [24/120    avg_loss:0.278, val_acc:0.919]
Epoch [25/120    avg_loss:0.277, val_acc:0.954]
Epoch [26/120    avg_loss:0.256, val_acc:0.960]
Epoch [27/120    avg_loss:0.231, val_acc:0.942]
Epoch [28/120    avg_loss:0.204, val_acc:0.976]
Epoch [29/120    avg_loss:0.188, val_acc:0.970]
Epoch [30/120    avg_loss:0.213, val_acc:0.950]
Epoch [31/120    avg_loss:0.157, val_acc:0.968]
Epoch [32/120    avg_loss:0.266, val_acc:0.919]
Epoch [33/120    avg_loss:0.297, val_acc:0.944]
Epoch [34/120    avg_loss:0.209, val_acc:0.944]
Epoch [35/120    avg_loss:0.176, val_acc:0.968]
Epoch [36/120    avg_loss:0.150, val_acc:0.952]
Epoch [37/120    avg_loss:0.175, val_acc:0.974]
Epoch [38/120    avg_loss:0.137, val_acc:0.982]
Epoch [39/120    avg_loss:0.144, val_acc:0.964]
Epoch [40/120    avg_loss:0.116, val_acc:0.982]
Epoch [41/120    avg_loss:0.119, val_acc:0.970]
Epoch [42/120    avg_loss:0.099, val_acc:0.978]
Epoch [43/120    avg_loss:0.119, val_acc:0.974]
Epoch [44/120    avg_loss:0.098, val_acc:0.970]
Epoch [45/120    avg_loss:0.086, val_acc:0.980]
Epoch [46/120    avg_loss:0.143, val_acc:0.976]
Epoch [47/120    avg_loss:0.154, val_acc:0.970]
Epoch [48/120    avg_loss:0.092, val_acc:0.962]
Epoch [49/120    avg_loss:0.099, val_acc:0.968]
Epoch [50/120    avg_loss:0.104, val_acc:0.978]
Epoch [51/120    avg_loss:0.097, val_acc:0.968]
Epoch [52/120    avg_loss:0.068, val_acc:0.960]
Epoch [53/120    avg_loss:0.125, val_acc:0.980]
Epoch [54/120    avg_loss:0.062, val_acc:0.978]
Epoch [55/120    avg_loss:0.055, val_acc:0.980]
Epoch [56/120    avg_loss:0.050, val_acc:0.984]
Epoch [57/120    avg_loss:0.048, val_acc:0.982]
Epoch [58/120    avg_loss:0.053, val_acc:0.986]
Epoch [59/120    avg_loss:0.046, val_acc:0.986]
Epoch [60/120    avg_loss:0.044, val_acc:0.986]
Epoch [61/120    avg_loss:0.034, val_acc:0.984]
Epoch [62/120    avg_loss:0.043, val_acc:0.986]
Epoch [63/120    avg_loss:0.043, val_acc:0.984]
Epoch [64/120    avg_loss:0.045, val_acc:0.984]
Epoch [65/120    avg_loss:0.035, val_acc:0.984]
Epoch [66/120    avg_loss:0.041, val_acc:0.984]
Epoch [67/120    avg_loss:0.037, val_acc:0.982]
Epoch [68/120    avg_loss:0.032, val_acc:0.984]
Epoch [69/120    avg_loss:0.038, val_acc:0.986]
Epoch [70/120    avg_loss:0.041, val_acc:0.988]
Epoch [71/120    avg_loss:0.035, val_acc:0.988]
Epoch [72/120    avg_loss:0.042, val_acc:0.988]
Epoch [73/120    avg_loss:0.042, val_acc:0.986]
Epoch [74/120    avg_loss:0.054, val_acc:0.986]
Epoch [75/120    avg_loss:0.039, val_acc:0.982]
Epoch [76/120    avg_loss:0.044, val_acc:0.984]
Epoch [77/120    avg_loss:0.035, val_acc:0.988]
Epoch [78/120    avg_loss:0.034, val_acc:0.986]
Epoch [79/120    avg_loss:0.039, val_acc:0.986]
Epoch [80/120    avg_loss:0.038, val_acc:0.986]
Epoch [81/120    avg_loss:0.034, val_acc:0.984]
Epoch [82/120    avg_loss:0.032, val_acc:0.986]
Epoch [83/120    avg_loss:0.036, val_acc:0.986]
Epoch [84/120    avg_loss:0.031, val_acc:0.988]
Epoch [85/120    avg_loss:0.037, val_acc:0.986]
Epoch [86/120    avg_loss:0.035, val_acc:0.988]
Epoch [87/120    avg_loss:0.034, val_acc:0.986]
Epoch [88/120    avg_loss:0.026, val_acc:0.986]
Epoch [89/120    avg_loss:0.026, val_acc:0.988]
Epoch [90/120    avg_loss:0.034, val_acc:0.986]
Epoch [91/120    avg_loss:0.036, val_acc:0.988]
Epoch [92/120    avg_loss:0.035, val_acc:0.990]
Epoch [93/120    avg_loss:0.034, val_acc:0.988]
Epoch [94/120    avg_loss:0.033, val_acc:0.988]
Epoch [95/120    avg_loss:0.034, val_acc:0.986]
Epoch [96/120    avg_loss:0.029, val_acc:0.984]
Epoch [97/120    avg_loss:0.030, val_acc:0.984]
Epoch [98/120    avg_loss:0.036, val_acc:0.984]
Epoch [99/120    avg_loss:0.034, val_acc:0.984]
Epoch [100/120    avg_loss:0.033, val_acc:0.986]
Epoch [101/120    avg_loss:0.029, val_acc:0.986]
Epoch [102/120    avg_loss:0.023, val_acc:0.986]
Epoch [103/120    avg_loss:0.025, val_acc:0.988]
Epoch [104/120    avg_loss:0.033, val_acc:0.986]
Epoch [105/120    avg_loss:0.030, val_acc:0.988]
Epoch [106/120    avg_loss:0.032, val_acc:0.988]
Epoch [107/120    avg_loss:0.040, val_acc:0.988]
Epoch [108/120    avg_loss:0.029, val_acc:0.988]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.029, val_acc:0.988]
Epoch [111/120    avg_loss:0.029, val_acc:0.988]
Epoch [112/120    avg_loss:0.029, val_acc:0.988]
Epoch [113/120    avg_loss:0.028, val_acc:0.988]
Epoch [114/120    avg_loss:0.030, val_acc:0.988]
Epoch [115/120    avg_loss:0.027, val_acc:0.988]
Epoch [116/120    avg_loss:0.027, val_acc:0.988]
Epoch [117/120    avg_loss:0.026, val_acc:0.988]
Epoch [118/120    avg_loss:0.025, val_acc:0.988]
Epoch [119/120    avg_loss:0.033, val_acc:0.988]
Epoch [120/120    avg_loss:0.021, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.98871332 0.98901099 0.94196429 0.94078947
 0.99266504 0.9726776  0.998713   1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9924040010588239
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f57ec32e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.370, val_acc:0.562]
Epoch [2/120    avg_loss:1.840, val_acc:0.710]
Epoch [3/120    avg_loss:1.464, val_acc:0.698]
Epoch [4/120    avg_loss:1.171, val_acc:0.740]
Epoch [5/120    avg_loss:0.988, val_acc:0.782]
Epoch [6/120    avg_loss:0.813, val_acc:0.800]
Epoch [7/120    avg_loss:0.753, val_acc:0.863]
Epoch [8/120    avg_loss:0.716, val_acc:0.798]
Epoch [9/120    avg_loss:0.702, val_acc:0.883]
Epoch [10/120    avg_loss:0.611, val_acc:0.883]
Epoch [11/120    avg_loss:0.545, val_acc:0.887]
Epoch [12/120    avg_loss:0.467, val_acc:0.915]
Epoch [13/120    avg_loss:0.453, val_acc:0.915]
Epoch [14/120    avg_loss:0.415, val_acc:0.917]
Epoch [15/120    avg_loss:0.460, val_acc:0.831]
Epoch [16/120    avg_loss:0.468, val_acc:0.931]
Epoch [17/120    avg_loss:0.385, val_acc:0.925]
Epoch [18/120    avg_loss:0.372, val_acc:0.933]
Epoch [19/120    avg_loss:0.438, val_acc:0.889]
Epoch [20/120    avg_loss:0.394, val_acc:0.935]
Epoch [21/120    avg_loss:0.359, val_acc:0.942]
Epoch [22/120    avg_loss:0.273, val_acc:0.946]
Epoch [23/120    avg_loss:0.295, val_acc:0.944]
Epoch [24/120    avg_loss:0.238, val_acc:0.964]
Epoch [25/120    avg_loss:0.241, val_acc:0.956]
Epoch [26/120    avg_loss:0.254, val_acc:0.940]
Epoch [27/120    avg_loss:0.238, val_acc:0.970]
Epoch [28/120    avg_loss:0.265, val_acc:0.958]
Epoch [29/120    avg_loss:0.235, val_acc:0.946]
Epoch [30/120    avg_loss:0.245, val_acc:0.933]
Epoch [31/120    avg_loss:0.200, val_acc:0.952]
Epoch [32/120    avg_loss:0.161, val_acc:0.972]
Epoch [33/120    avg_loss:0.200, val_acc:0.952]
Epoch [34/120    avg_loss:0.293, val_acc:0.915]
Epoch [35/120    avg_loss:0.300, val_acc:0.917]
Epoch [36/120    avg_loss:0.236, val_acc:0.942]
Epoch [37/120    avg_loss:0.309, val_acc:0.948]
Epoch [38/120    avg_loss:0.261, val_acc:0.978]
Epoch [39/120    avg_loss:0.206, val_acc:0.958]
Epoch [40/120    avg_loss:0.184, val_acc:0.968]
Epoch [41/120    avg_loss:0.138, val_acc:0.972]
Epoch [42/120    avg_loss:0.152, val_acc:0.960]
Epoch [43/120    avg_loss:0.131, val_acc:0.986]
Epoch [44/120    avg_loss:0.172, val_acc:0.974]
Epoch [45/120    avg_loss:0.160, val_acc:0.972]
Epoch [46/120    avg_loss:0.114, val_acc:0.980]
Epoch [47/120    avg_loss:0.098, val_acc:0.980]
Epoch [48/120    avg_loss:0.109, val_acc:0.974]
Epoch [49/120    avg_loss:0.141, val_acc:0.974]
Epoch [50/120    avg_loss:0.130, val_acc:0.978]
Epoch [51/120    avg_loss:0.115, val_acc:0.970]
Epoch [52/120    avg_loss:0.122, val_acc:0.986]
Epoch [53/120    avg_loss:0.081, val_acc:0.988]
Epoch [54/120    avg_loss:0.099, val_acc:0.984]
Epoch [55/120    avg_loss:0.059, val_acc:0.984]
Epoch [56/120    avg_loss:0.081, val_acc:0.982]
Epoch [57/120    avg_loss:0.091, val_acc:0.974]
Epoch [58/120    avg_loss:0.103, val_acc:0.966]
Epoch [59/120    avg_loss:0.113, val_acc:0.966]
Epoch [60/120    avg_loss:0.142, val_acc:0.964]
Epoch [61/120    avg_loss:0.111, val_acc:0.988]
Epoch [62/120    avg_loss:0.074, val_acc:0.978]
Epoch [63/120    avg_loss:0.078, val_acc:0.978]
Epoch [64/120    avg_loss:0.069, val_acc:0.958]
Epoch [65/120    avg_loss:0.085, val_acc:0.990]
Epoch [66/120    avg_loss:0.077, val_acc:0.984]
Epoch [67/120    avg_loss:0.113, val_acc:0.986]
Epoch [68/120    avg_loss:0.064, val_acc:0.986]
Epoch [69/120    avg_loss:0.054, val_acc:0.982]
Epoch [70/120    avg_loss:0.039, val_acc:0.986]
Epoch [71/120    avg_loss:0.051, val_acc:0.980]
Epoch [72/120    avg_loss:0.061, val_acc:0.990]
Epoch [73/120    avg_loss:0.064, val_acc:0.984]
Epoch [74/120    avg_loss:0.086, val_acc:0.986]
Epoch [75/120    avg_loss:0.054, val_acc:0.992]
Epoch [76/120    avg_loss:0.044, val_acc:0.986]
Epoch [77/120    avg_loss:0.052, val_acc:0.980]
Epoch [78/120    avg_loss:0.036, val_acc:0.984]
Epoch [79/120    avg_loss:0.026, val_acc:0.988]
Epoch [80/120    avg_loss:0.031, val_acc:0.988]
Epoch [81/120    avg_loss:0.028, val_acc:0.988]
Epoch [82/120    avg_loss:0.028, val_acc:0.992]
Epoch [83/120    avg_loss:0.041, val_acc:0.978]
Epoch [84/120    avg_loss:0.139, val_acc:0.974]
Epoch [85/120    avg_loss:0.119, val_acc:0.978]
Epoch [86/120    avg_loss:0.093, val_acc:0.976]
Epoch [87/120    avg_loss:0.048, val_acc:0.990]
Epoch [88/120    avg_loss:0.056, val_acc:0.988]
Epoch [89/120    avg_loss:0.035, val_acc:0.992]
Epoch [90/120    avg_loss:0.028, val_acc:0.990]
Epoch [91/120    avg_loss:0.052, val_acc:0.976]
Epoch [92/120    avg_loss:0.118, val_acc:0.927]
Epoch [93/120    avg_loss:0.086, val_acc:0.978]
Epoch [94/120    avg_loss:0.074, val_acc:0.984]
Epoch [95/120    avg_loss:0.039, val_acc:0.992]
Epoch [96/120    avg_loss:0.025, val_acc:0.992]
Epoch [97/120    avg_loss:0.020, val_acc:0.992]
Epoch [98/120    avg_loss:0.018, val_acc:0.992]
Epoch [99/120    avg_loss:0.027, val_acc:0.992]
Epoch [100/120    avg_loss:0.025, val_acc:0.994]
Epoch [101/120    avg_loss:0.017, val_acc:0.994]
Epoch [102/120    avg_loss:0.022, val_acc:0.992]
Epoch [103/120    avg_loss:0.022, val_acc:0.992]
Epoch [104/120    avg_loss:0.042, val_acc:0.990]
Epoch [105/120    avg_loss:0.027, val_acc:0.992]
Epoch [106/120    avg_loss:0.022, val_acc:0.984]
Epoch [107/120    avg_loss:0.025, val_acc:0.994]
Epoch [108/120    avg_loss:0.057, val_acc:0.984]
Epoch [109/120    avg_loss:0.049, val_acc:0.982]
Epoch [110/120    avg_loss:0.080, val_acc:0.980]
Epoch [111/120    avg_loss:0.059, val_acc:0.984]
Epoch [112/120    avg_loss:0.023, val_acc:0.994]
Epoch [113/120    avg_loss:0.031, val_acc:0.990]
Epoch [114/120    avg_loss:0.029, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.986]
Epoch [116/120    avg_loss:0.037, val_acc:0.974]
Epoch [117/120    avg_loss:0.047, val_acc:0.992]
Epoch [118/120    avg_loss:0.029, val_acc:0.994]
Epoch [119/120    avg_loss:0.020, val_acc:0.992]
Epoch [120/120    avg_loss:0.019, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   3  11 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97333333 0.98915401 0.95154185 0.91289199
 1.         0.94382022 1.         1.         1.         0.98305085
 0.98544233 1.        ]

Kappa:
0.9881306298839357
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71024ff860>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.370, val_acc:0.480]
Epoch [2/120    avg_loss:1.910, val_acc:0.613]
Epoch [3/120    avg_loss:1.528, val_acc:0.651]
Epoch [4/120    avg_loss:1.297, val_acc:0.732]
Epoch [5/120    avg_loss:1.075, val_acc:0.720]
Epoch [6/120    avg_loss:0.979, val_acc:0.806]
Epoch [7/120    avg_loss:0.873, val_acc:0.750]
Epoch [8/120    avg_loss:0.826, val_acc:0.871]
Epoch [9/120    avg_loss:0.750, val_acc:0.825]
Epoch [10/120    avg_loss:0.641, val_acc:0.841]
Epoch [11/120    avg_loss:0.661, val_acc:0.817]
Epoch [12/120    avg_loss:0.619, val_acc:0.772]
Epoch [13/120    avg_loss:0.565, val_acc:0.873]
Epoch [14/120    avg_loss:0.535, val_acc:0.867]
Epoch [15/120    avg_loss:0.462, val_acc:0.889]
Epoch [16/120    avg_loss:0.534, val_acc:0.905]
Epoch [17/120    avg_loss:0.417, val_acc:0.901]
Epoch [18/120    avg_loss:0.366, val_acc:0.931]
Epoch [19/120    avg_loss:0.370, val_acc:0.921]
Epoch [20/120    avg_loss:0.332, val_acc:0.923]
Epoch [21/120    avg_loss:0.369, val_acc:0.905]
Epoch [22/120    avg_loss:0.328, val_acc:0.921]
Epoch [23/120    avg_loss:0.317, val_acc:0.931]
Epoch [24/120    avg_loss:0.294, val_acc:0.913]
Epoch [25/120    avg_loss:0.236, val_acc:0.946]
Epoch [26/120    avg_loss:0.217, val_acc:0.940]
Epoch [27/120    avg_loss:0.234, val_acc:0.950]
Epoch [28/120    avg_loss:0.258, val_acc:0.958]
Epoch [29/120    avg_loss:0.219, val_acc:0.946]
Epoch [30/120    avg_loss:0.262, val_acc:0.960]
Epoch [31/120    avg_loss:0.224, val_acc:0.937]
Epoch [32/120    avg_loss:0.218, val_acc:0.948]
Epoch [33/120    avg_loss:0.149, val_acc:0.966]
Epoch [34/120    avg_loss:0.167, val_acc:0.950]
Epoch [35/120    avg_loss:0.123, val_acc:0.952]
Epoch [36/120    avg_loss:0.159, val_acc:0.931]
Epoch [37/120    avg_loss:0.157, val_acc:0.956]
Epoch [38/120    avg_loss:0.131, val_acc:0.966]
Epoch [39/120    avg_loss:0.129, val_acc:0.968]
Epoch [40/120    avg_loss:0.159, val_acc:0.972]
Epoch [41/120    avg_loss:0.146, val_acc:0.954]
Epoch [42/120    avg_loss:0.155, val_acc:0.974]
Epoch [43/120    avg_loss:0.169, val_acc:0.968]
Epoch [44/120    avg_loss:0.121, val_acc:0.966]
Epoch [45/120    avg_loss:0.116, val_acc:0.970]
Epoch [46/120    avg_loss:0.170, val_acc:0.986]
Epoch [47/120    avg_loss:0.093, val_acc:0.970]
Epoch [48/120    avg_loss:0.068, val_acc:0.968]
Epoch [49/120    avg_loss:0.067, val_acc:0.978]
Epoch [50/120    avg_loss:0.075, val_acc:0.982]
Epoch [51/120    avg_loss:0.071, val_acc:0.978]
Epoch [52/120    avg_loss:0.078, val_acc:0.974]
Epoch [53/120    avg_loss:0.134, val_acc:0.976]
Epoch [54/120    avg_loss:0.139, val_acc:0.921]
Epoch [55/120    avg_loss:0.166, val_acc:0.954]
Epoch [56/120    avg_loss:0.098, val_acc:0.976]
Epoch [57/120    avg_loss:0.070, val_acc:0.982]
Epoch [58/120    avg_loss:0.072, val_acc:0.958]
Epoch [59/120    avg_loss:0.119, val_acc:0.972]
Epoch [60/120    avg_loss:0.096, val_acc:0.978]
Epoch [61/120    avg_loss:0.062, val_acc:0.978]
Epoch [62/120    avg_loss:0.050, val_acc:0.980]
Epoch [63/120    avg_loss:0.042, val_acc:0.982]
Epoch [64/120    avg_loss:0.042, val_acc:0.982]
Epoch [65/120    avg_loss:0.041, val_acc:0.984]
Epoch [66/120    avg_loss:0.039, val_acc:0.986]
Epoch [67/120    avg_loss:0.042, val_acc:0.986]
Epoch [68/120    avg_loss:0.042, val_acc:0.984]
Epoch [69/120    avg_loss:0.035, val_acc:0.982]
Epoch [70/120    avg_loss:0.040, val_acc:0.982]
Epoch [71/120    avg_loss:0.038, val_acc:0.984]
Epoch [72/120    avg_loss:0.032, val_acc:0.986]
Epoch [73/120    avg_loss:0.040, val_acc:0.986]
Epoch [74/120    avg_loss:0.039, val_acc:0.986]
Epoch [75/120    avg_loss:0.035, val_acc:0.988]
Epoch [76/120    avg_loss:0.031, val_acc:0.986]
Epoch [77/120    avg_loss:0.032, val_acc:0.986]
Epoch [78/120    avg_loss:0.039, val_acc:0.988]
Epoch [79/120    avg_loss:0.029, val_acc:0.986]
Epoch [80/120    avg_loss:0.037, val_acc:0.982]
Epoch [81/120    avg_loss:0.034, val_acc:0.986]
Epoch [82/120    avg_loss:0.023, val_acc:0.986]
Epoch [83/120    avg_loss:0.027, val_acc:0.986]
Epoch [84/120    avg_loss:0.037, val_acc:0.988]
Epoch [85/120    avg_loss:0.031, val_acc:0.986]
Epoch [86/120    avg_loss:0.033, val_acc:0.986]
Epoch [87/120    avg_loss:0.032, val_acc:0.986]
Epoch [88/120    avg_loss:0.029, val_acc:0.988]
Epoch [89/120    avg_loss:0.034, val_acc:0.986]
Epoch [90/120    avg_loss:0.032, val_acc:0.986]
Epoch [91/120    avg_loss:0.038, val_acc:0.986]
Epoch [92/120    avg_loss:0.027, val_acc:0.988]
Epoch [93/120    avg_loss:0.025, val_acc:0.988]
Epoch [94/120    avg_loss:0.031, val_acc:0.988]
Epoch [95/120    avg_loss:0.034, val_acc:0.988]
Epoch [96/120    avg_loss:0.033, val_acc:0.986]
Epoch [97/120    avg_loss:0.028, val_acc:0.988]
Epoch [98/120    avg_loss:0.026, val_acc:0.986]
Epoch [99/120    avg_loss:0.023, val_acc:0.988]
Epoch [100/120    avg_loss:0.026, val_acc:0.986]
Epoch [101/120    avg_loss:0.025, val_acc:0.990]
Epoch [102/120    avg_loss:0.028, val_acc:0.988]
Epoch [103/120    avg_loss:0.030, val_acc:0.986]
Epoch [104/120    avg_loss:0.031, val_acc:0.988]
Epoch [105/120    avg_loss:0.036, val_acc:0.988]
Epoch [106/120    avg_loss:0.022, val_acc:0.984]
Epoch [107/120    avg_loss:0.029, val_acc:0.984]
Epoch [108/120    avg_loss:0.022, val_acc:0.988]
Epoch [109/120    avg_loss:0.024, val_acc:0.988]
Epoch [110/120    avg_loss:0.023, val_acc:0.988]
Epoch [111/120    avg_loss:0.021, val_acc:0.986]
Epoch [112/120    avg_loss:0.028, val_acc:0.986]
Epoch [113/120    avg_loss:0.020, val_acc:0.988]
Epoch [114/120    avg_loss:0.021, val_acc:0.986]
Epoch [115/120    avg_loss:0.028, val_acc:0.986]
Epoch [116/120    avg_loss:0.027, val_acc:0.986]
Epoch [117/120    avg_loss:0.028, val_acc:0.986]
Epoch [118/120    avg_loss:0.025, val_acc:0.986]
Epoch [119/120    avg_loss:0.021, val_acc:0.986]
Epoch [120/120    avg_loss:0.032, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.98426966 0.99782135 0.95927602 0.94389439
 1.         0.96132597 1.         1.         1.         0.98558322
 0.9877369  1.        ]

Kappa:
0.9914546473781801
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04bd658898>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.380, val_acc:0.520]
Epoch [2/120    avg_loss:1.868, val_acc:0.665]
Epoch [3/120    avg_loss:1.456, val_acc:0.760]
Epoch [4/120    avg_loss:1.148, val_acc:0.796]
Epoch [5/120    avg_loss:0.943, val_acc:0.798]
Epoch [6/120    avg_loss:0.888, val_acc:0.792]
Epoch [7/120    avg_loss:0.774, val_acc:0.847]
Epoch [8/120    avg_loss:0.683, val_acc:0.885]
Epoch [9/120    avg_loss:0.611, val_acc:0.897]
Epoch [10/120    avg_loss:0.543, val_acc:0.798]
Epoch [11/120    avg_loss:0.546, val_acc:0.911]
Epoch [12/120    avg_loss:0.504, val_acc:0.909]
Epoch [13/120    avg_loss:0.469, val_acc:0.913]
Epoch [14/120    avg_loss:0.406, val_acc:0.905]
Epoch [15/120    avg_loss:0.412, val_acc:0.889]
Epoch [16/120    avg_loss:0.380, val_acc:0.919]
Epoch [17/120    avg_loss:0.417, val_acc:0.929]
Epoch [18/120    avg_loss:0.343, val_acc:0.903]
Epoch [19/120    avg_loss:0.383, val_acc:0.869]
Epoch [20/120    avg_loss:0.330, val_acc:0.933]
Epoch [21/120    avg_loss:0.287, val_acc:0.946]
Epoch [22/120    avg_loss:0.257, val_acc:0.954]
Epoch [23/120    avg_loss:0.291, val_acc:0.937]
Epoch [24/120    avg_loss:0.296, val_acc:0.954]
Epoch [25/120    avg_loss:0.288, val_acc:0.931]
Epoch [26/120    avg_loss:0.253, val_acc:0.929]
Epoch [27/120    avg_loss:0.227, val_acc:0.925]
Epoch [28/120    avg_loss:0.225, val_acc:0.954]
Epoch [29/120    avg_loss:0.217, val_acc:0.940]
Epoch [30/120    avg_loss:0.198, val_acc:0.972]
Epoch [31/120    avg_loss:0.195, val_acc:0.960]
Epoch [32/120    avg_loss:0.260, val_acc:0.966]
Epoch [33/120    avg_loss:0.239, val_acc:0.962]
Epoch [34/120    avg_loss:0.175, val_acc:0.944]
Epoch [35/120    avg_loss:0.150, val_acc:0.946]
Epoch [36/120    avg_loss:0.156, val_acc:0.956]
Epoch [37/120    avg_loss:0.175, val_acc:0.964]
Epoch [38/120    avg_loss:0.142, val_acc:0.966]
Epoch [39/120    avg_loss:0.156, val_acc:0.960]
Epoch [40/120    avg_loss:0.134, val_acc:0.938]
Epoch [41/120    avg_loss:0.141, val_acc:0.954]
Epoch [42/120    avg_loss:0.157, val_acc:0.962]
Epoch [43/120    avg_loss:0.158, val_acc:0.970]
Epoch [44/120    avg_loss:0.129, val_acc:0.968]
Epoch [45/120    avg_loss:0.097, val_acc:0.976]
Epoch [46/120    avg_loss:0.089, val_acc:0.974]
Epoch [47/120    avg_loss:0.067, val_acc:0.976]
Epoch [48/120    avg_loss:0.066, val_acc:0.978]
Epoch [49/120    avg_loss:0.076, val_acc:0.980]
Epoch [50/120    avg_loss:0.080, val_acc:0.984]
Epoch [51/120    avg_loss:0.086, val_acc:0.980]
Epoch [52/120    avg_loss:0.074, val_acc:0.982]
Epoch [53/120    avg_loss:0.090, val_acc:0.982]
Epoch [54/120    avg_loss:0.059, val_acc:0.982]
Epoch [55/120    avg_loss:0.059, val_acc:0.984]
Epoch [56/120    avg_loss:0.066, val_acc:0.984]
Epoch [57/120    avg_loss:0.084, val_acc:0.982]
Epoch [58/120    avg_loss:0.066, val_acc:0.984]
Epoch [59/120    avg_loss:0.063, val_acc:0.986]
Epoch [60/120    avg_loss:0.070, val_acc:0.982]
Epoch [61/120    avg_loss:0.062, val_acc:0.984]
Epoch [62/120    avg_loss:0.051, val_acc:0.982]
Epoch [63/120    avg_loss:0.057, val_acc:0.982]
Epoch [64/120    avg_loss:0.048, val_acc:0.986]
Epoch [65/120    avg_loss:0.047, val_acc:0.982]
Epoch [66/120    avg_loss:0.065, val_acc:0.984]
Epoch [67/120    avg_loss:0.049, val_acc:0.984]
Epoch [68/120    avg_loss:0.042, val_acc:0.984]
Epoch [69/120    avg_loss:0.058, val_acc:0.986]
Epoch [70/120    avg_loss:0.056, val_acc:0.984]
Epoch [71/120    avg_loss:0.045, val_acc:0.986]
Epoch [72/120    avg_loss:0.046, val_acc:0.986]
Epoch [73/120    avg_loss:0.050, val_acc:0.982]
Epoch [74/120    avg_loss:0.053, val_acc:0.984]
Epoch [75/120    avg_loss:0.045, val_acc:0.986]
Epoch [76/120    avg_loss:0.059, val_acc:0.986]
Epoch [77/120    avg_loss:0.047, val_acc:0.986]
Epoch [78/120    avg_loss:0.062, val_acc:0.986]
Epoch [79/120    avg_loss:0.052, val_acc:0.986]
Epoch [80/120    avg_loss:0.041, val_acc:0.986]
Epoch [81/120    avg_loss:0.042, val_acc:0.984]
Epoch [82/120    avg_loss:0.039, val_acc:0.986]
Epoch [83/120    avg_loss:0.056, val_acc:0.984]
Epoch [84/120    avg_loss:0.044, val_acc:0.984]
Epoch [85/120    avg_loss:0.045, val_acc:0.986]
Epoch [86/120    avg_loss:0.050, val_acc:0.986]
Epoch [87/120    avg_loss:0.041, val_acc:0.984]
Epoch [88/120    avg_loss:0.042, val_acc:0.984]
Epoch [89/120    avg_loss:0.044, val_acc:0.984]
Epoch [90/120    avg_loss:0.048, val_acc:0.986]
Epoch [91/120    avg_loss:0.041, val_acc:0.986]
Epoch [92/120    avg_loss:0.035, val_acc:0.986]
Epoch [93/120    avg_loss:0.056, val_acc:0.984]
Epoch [94/120    avg_loss:0.041, val_acc:0.986]
Epoch [95/120    avg_loss:0.052, val_acc:0.986]
Epoch [96/120    avg_loss:0.046, val_acc:0.986]
Epoch [97/120    avg_loss:0.044, val_acc:0.986]
Epoch [98/120    avg_loss:0.043, val_acc:0.986]
Epoch [99/120    avg_loss:0.037, val_acc:0.986]
Epoch [100/120    avg_loss:0.047, val_acc:0.986]
Epoch [101/120    avg_loss:0.040, val_acc:0.986]
Epoch [102/120    avg_loss:0.039, val_acc:0.984]
Epoch [103/120    avg_loss:0.036, val_acc:0.986]
Epoch [104/120    avg_loss:0.031, val_acc:0.986]
Epoch [105/120    avg_loss:0.049, val_acc:0.986]
Epoch [106/120    avg_loss:0.037, val_acc:0.986]
Epoch [107/120    avg_loss:0.029, val_acc:0.984]
Epoch [108/120    avg_loss:0.038, val_acc:0.986]
Epoch [109/120    avg_loss:0.040, val_acc:0.984]
Epoch [110/120    avg_loss:0.036, val_acc:0.986]
Epoch [111/120    avg_loss:0.037, val_acc:0.986]
Epoch [112/120    avg_loss:0.033, val_acc:0.986]
Epoch [113/120    avg_loss:0.031, val_acc:0.986]
Epoch [114/120    avg_loss:0.042, val_acc:0.984]
Epoch [115/120    avg_loss:0.034, val_acc:0.984]
Epoch [116/120    avg_loss:0.030, val_acc:0.988]
Epoch [117/120    avg_loss:0.042, val_acc:0.986]
Epoch [118/120    avg_loss:0.027, val_acc:0.986]
Epoch [119/120    avg_loss:0.040, val_acc:0.986]
Epoch [120/120    avg_loss:0.035, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   8   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98871332 0.98004435 0.93986637 0.93729373
 1.         0.9726776  1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9921665019946646
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f611d862780>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.339, val_acc:0.429]
Epoch [2/120    avg_loss:1.841, val_acc:0.610]
Epoch [3/120    avg_loss:1.569, val_acc:0.677]
Epoch [4/120    avg_loss:1.280, val_acc:0.700]
Epoch [5/120    avg_loss:1.067, val_acc:0.700]
Epoch [6/120    avg_loss:0.897, val_acc:0.767]
Epoch [7/120    avg_loss:0.862, val_acc:0.765]
Epoch [8/120    avg_loss:0.817, val_acc:0.798]
Epoch [9/120    avg_loss:0.706, val_acc:0.838]
Epoch [10/120    avg_loss:0.648, val_acc:0.873]
Epoch [11/120    avg_loss:0.643, val_acc:0.867]
Epoch [12/120    avg_loss:0.580, val_acc:0.827]
Epoch [13/120    avg_loss:0.534, val_acc:0.915]
Epoch [14/120    avg_loss:0.550, val_acc:0.883]
Epoch [15/120    avg_loss:0.484, val_acc:0.875]
Epoch [16/120    avg_loss:0.396, val_acc:0.904]
Epoch [17/120    avg_loss:0.506, val_acc:0.881]
Epoch [18/120    avg_loss:0.445, val_acc:0.931]
Epoch [19/120    avg_loss:0.369, val_acc:0.929]
Epoch [20/120    avg_loss:0.377, val_acc:0.927]
Epoch [21/120    avg_loss:0.331, val_acc:0.933]
Epoch [22/120    avg_loss:0.367, val_acc:0.931]
Epoch [23/120    avg_loss:0.296, val_acc:0.935]
Epoch [24/120    avg_loss:0.331, val_acc:0.923]
Epoch [25/120    avg_loss:0.277, val_acc:0.912]
Epoch [26/120    avg_loss:0.269, val_acc:0.940]
Epoch [27/120    avg_loss:0.283, val_acc:0.950]
Epoch [28/120    avg_loss:0.241, val_acc:0.940]
Epoch [29/120    avg_loss:0.245, val_acc:0.946]
Epoch [30/120    avg_loss:0.212, val_acc:0.958]
Epoch [31/120    avg_loss:0.268, val_acc:0.940]
Epoch [32/120    avg_loss:0.370, val_acc:0.923]
Epoch [33/120    avg_loss:0.249, val_acc:0.929]
Epoch [34/120    avg_loss:0.409, val_acc:0.940]
Epoch [35/120    avg_loss:0.236, val_acc:0.965]
Epoch [36/120    avg_loss:0.212, val_acc:0.965]
Epoch [37/120    avg_loss:0.162, val_acc:0.963]
Epoch [38/120    avg_loss:0.187, val_acc:0.967]
Epoch [39/120    avg_loss:0.186, val_acc:0.956]
Epoch [40/120    avg_loss:0.131, val_acc:0.973]
Epoch [41/120    avg_loss:0.213, val_acc:0.965]
Epoch [42/120    avg_loss:0.154, val_acc:0.973]
Epoch [43/120    avg_loss:0.110, val_acc:0.971]
Epoch [44/120    avg_loss:0.139, val_acc:0.971]
Epoch [45/120    avg_loss:0.157, val_acc:0.956]
Epoch [46/120    avg_loss:0.174, val_acc:0.958]
Epoch [47/120    avg_loss:0.127, val_acc:0.971]
Epoch [48/120    avg_loss:0.102, val_acc:0.979]
Epoch [49/120    avg_loss:0.165, val_acc:0.975]
Epoch [50/120    avg_loss:0.209, val_acc:0.979]
Epoch [51/120    avg_loss:0.172, val_acc:0.973]
Epoch [52/120    avg_loss:0.130, val_acc:0.960]
Epoch [53/120    avg_loss:0.090, val_acc:0.977]
Epoch [54/120    avg_loss:0.102, val_acc:0.985]
Epoch [55/120    avg_loss:0.069, val_acc:0.973]
Epoch [56/120    avg_loss:0.084, val_acc:0.988]
Epoch [57/120    avg_loss:0.081, val_acc:0.979]
Epoch [58/120    avg_loss:0.043, val_acc:0.981]
Epoch [59/120    avg_loss:0.084, val_acc:0.983]
Epoch [60/120    avg_loss:0.083, val_acc:0.985]
Epoch [61/120    avg_loss:0.045, val_acc:0.977]
Epoch [62/120    avg_loss:0.103, val_acc:0.981]
Epoch [63/120    avg_loss:0.118, val_acc:0.981]
Epoch [64/120    avg_loss:0.056, val_acc:0.977]
Epoch [65/120    avg_loss:0.056, val_acc:0.981]
Epoch [66/120    avg_loss:0.047, val_acc:0.990]
Epoch [67/120    avg_loss:0.061, val_acc:0.958]
Epoch [68/120    avg_loss:0.092, val_acc:0.988]
Epoch [69/120    avg_loss:0.065, val_acc:0.979]
Epoch [70/120    avg_loss:0.073, val_acc:0.988]
Epoch [71/120    avg_loss:0.057, val_acc:0.983]
Epoch [72/120    avg_loss:0.035, val_acc:0.985]
Epoch [73/120    avg_loss:0.040, val_acc:0.988]
Epoch [74/120    avg_loss:0.048, val_acc:0.979]
Epoch [75/120    avg_loss:0.053, val_acc:0.971]
Epoch [76/120    avg_loss:0.053, val_acc:0.979]
Epoch [77/120    avg_loss:0.066, val_acc:0.975]
Epoch [78/120    avg_loss:0.087, val_acc:0.969]
Epoch [79/120    avg_loss:0.049, val_acc:0.990]
Epoch [80/120    avg_loss:0.045, val_acc:0.988]
Epoch [81/120    avg_loss:0.032, val_acc:0.985]
Epoch [82/120    avg_loss:0.027, val_acc:0.994]
Epoch [83/120    avg_loss:0.021, val_acc:0.988]
Epoch [84/120    avg_loss:0.056, val_acc:0.992]
Epoch [85/120    avg_loss:0.079, val_acc:0.985]
Epoch [86/120    avg_loss:0.163, val_acc:0.942]
Epoch [87/120    avg_loss:0.149, val_acc:0.981]
Epoch [88/120    avg_loss:0.079, val_acc:0.988]
Epoch [89/120    avg_loss:0.044, val_acc:0.988]
Epoch [90/120    avg_loss:0.032, val_acc:0.981]
Epoch [91/120    avg_loss:0.033, val_acc:0.983]
Epoch [92/120    avg_loss:0.036, val_acc:0.977]
Epoch [93/120    avg_loss:0.037, val_acc:0.985]
Epoch [94/120    avg_loss:0.022, val_acc:0.990]
Epoch [95/120    avg_loss:0.030, val_acc:0.990]
Epoch [96/120    avg_loss:0.024, val_acc:0.990]
Epoch [97/120    avg_loss:0.019, val_acc:0.998]
Epoch [98/120    avg_loss:0.014, val_acc:0.996]
Epoch [99/120    avg_loss:0.017, val_acc:0.996]
Epoch [100/120    avg_loss:0.014, val_acc:0.996]
Epoch [101/120    avg_loss:0.011, val_acc:0.996]
Epoch [102/120    avg_loss:0.014, val_acc:0.996]
Epoch [103/120    avg_loss:0.024, val_acc:0.996]
Epoch [104/120    avg_loss:0.015, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.016, val_acc:0.996]
Epoch [107/120    avg_loss:0.013, val_acc:0.996]
Epoch [108/120    avg_loss:0.014, val_acc:0.996]
Epoch [109/120    avg_loss:0.016, val_acc:0.996]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.010, val_acc:0.994]
Epoch [112/120    avg_loss:0.017, val_acc:0.996]
Epoch [113/120    avg_loss:0.017, val_acc:0.996]
Epoch [114/120    avg_loss:0.013, val_acc:0.996]
Epoch [115/120    avg_loss:0.010, val_acc:0.996]
Epoch [116/120    avg_loss:0.012, val_acc:0.996]
Epoch [117/120    avg_loss:0.010, val_acc:0.996]
Epoch [118/120    avg_loss:0.013, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.021, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  14 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.9977221  0.99343545 0.96460177 0.9559322
 1.         0.99465241 1.         1.         1.         0.98177083
 0.98430493 1.        ]

Kappa:
0.9926416806707038
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f696b6407f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.341, val_acc:0.502]
Epoch [2/120    avg_loss:1.916, val_acc:0.577]
Epoch [3/120    avg_loss:1.579, val_acc:0.647]
Epoch [4/120    avg_loss:1.286, val_acc:0.746]
Epoch [5/120    avg_loss:1.045, val_acc:0.790]
Epoch [6/120    avg_loss:0.886, val_acc:0.825]
Epoch [7/120    avg_loss:0.765, val_acc:0.823]
Epoch [8/120    avg_loss:0.670, val_acc:0.780]
Epoch [9/120    avg_loss:0.621, val_acc:0.881]
Epoch [10/120    avg_loss:0.598, val_acc:0.859]
Epoch [11/120    avg_loss:0.540, val_acc:0.907]
Epoch [12/120    avg_loss:0.475, val_acc:0.883]
Epoch [13/120    avg_loss:0.478, val_acc:0.895]
Epoch [14/120    avg_loss:0.475, val_acc:0.915]
Epoch [15/120    avg_loss:0.396, val_acc:0.919]
Epoch [16/120    avg_loss:0.369, val_acc:0.907]
Epoch [17/120    avg_loss:0.359, val_acc:0.919]
Epoch [18/120    avg_loss:0.388, val_acc:0.913]
Epoch [19/120    avg_loss:0.372, val_acc:0.925]
Epoch [20/120    avg_loss:0.319, val_acc:0.933]
Epoch [21/120    avg_loss:0.321, val_acc:0.907]
Epoch [22/120    avg_loss:0.291, val_acc:0.917]
Epoch [23/120    avg_loss:0.295, val_acc:0.915]
Epoch [24/120    avg_loss:0.276, val_acc:0.952]
Epoch [25/120    avg_loss:0.234, val_acc:0.937]
Epoch [26/120    avg_loss:0.264, val_acc:0.917]
Epoch [27/120    avg_loss:0.281, val_acc:0.883]
Epoch [28/120    avg_loss:0.213, val_acc:0.968]
Epoch [29/120    avg_loss:0.203, val_acc:0.954]
Epoch [30/120    avg_loss:0.236, val_acc:0.923]
Epoch [31/120    avg_loss:0.198, val_acc:0.935]
Epoch [32/120    avg_loss:0.217, val_acc:0.952]
Epoch [33/120    avg_loss:0.186, val_acc:0.962]
Epoch [34/120    avg_loss:0.178, val_acc:0.948]
Epoch [35/120    avg_loss:0.163, val_acc:0.972]
Epoch [36/120    avg_loss:0.213, val_acc:0.933]
Epoch [37/120    avg_loss:0.196, val_acc:0.937]
Epoch [38/120    avg_loss:0.201, val_acc:0.946]
Epoch [39/120    avg_loss:0.161, val_acc:0.966]
Epoch [40/120    avg_loss:0.109, val_acc:0.972]
Epoch [41/120    avg_loss:0.128, val_acc:0.978]
Epoch [42/120    avg_loss:0.094, val_acc:0.976]
Epoch [43/120    avg_loss:0.085, val_acc:0.986]
Epoch [44/120    avg_loss:0.070, val_acc:0.964]
Epoch [45/120    avg_loss:0.082, val_acc:0.976]
Epoch [46/120    avg_loss:0.096, val_acc:0.960]
Epoch [47/120    avg_loss:0.089, val_acc:0.978]
Epoch [48/120    avg_loss:0.062, val_acc:0.980]
Epoch [49/120    avg_loss:0.065, val_acc:0.976]
Epoch [50/120    avg_loss:0.059, val_acc:0.978]
Epoch [51/120    avg_loss:0.071, val_acc:0.974]
Epoch [52/120    avg_loss:0.062, val_acc:0.974]
Epoch [53/120    avg_loss:0.067, val_acc:0.976]
Epoch [54/120    avg_loss:0.049, val_acc:0.972]
Epoch [55/120    avg_loss:0.063, val_acc:0.968]
Epoch [56/120    avg_loss:0.066, val_acc:0.982]
Epoch [57/120    avg_loss:0.048, val_acc:0.992]
Epoch [58/120    avg_loss:0.037, val_acc:0.994]
Epoch [59/120    avg_loss:0.049, val_acc:0.994]
Epoch [60/120    avg_loss:0.036, val_acc:0.994]
Epoch [61/120    avg_loss:0.032, val_acc:0.994]
Epoch [62/120    avg_loss:0.032, val_acc:0.994]
Epoch [63/120    avg_loss:0.030, val_acc:0.994]
Epoch [64/120    avg_loss:0.033, val_acc:0.994]
Epoch [65/120    avg_loss:0.033, val_acc:0.994]
Epoch [66/120    avg_loss:0.029, val_acc:0.994]
Epoch [67/120    avg_loss:0.029, val_acc:0.994]
Epoch [68/120    avg_loss:0.027, val_acc:0.994]
Epoch [69/120    avg_loss:0.038, val_acc:0.994]
Epoch [70/120    avg_loss:0.027, val_acc:0.994]
Epoch [71/120    avg_loss:0.026, val_acc:0.994]
Epoch [72/120    avg_loss:0.023, val_acc:0.994]
Epoch [73/120    avg_loss:0.030, val_acc:0.994]
Epoch [74/120    avg_loss:0.020, val_acc:0.994]
Epoch [75/120    avg_loss:0.028, val_acc:0.994]
Epoch [76/120    avg_loss:0.023, val_acc:0.996]
Epoch [77/120    avg_loss:0.027, val_acc:0.994]
Epoch [78/120    avg_loss:0.025, val_acc:0.994]
Epoch [79/120    avg_loss:0.020, val_acc:0.994]
Epoch [80/120    avg_loss:0.022, val_acc:0.994]
Epoch [81/120    avg_loss:0.019, val_acc:0.994]
Epoch [82/120    avg_loss:0.022, val_acc:0.994]
Epoch [83/120    avg_loss:0.018, val_acc:0.994]
Epoch [84/120    avg_loss:0.026, val_acc:0.994]
Epoch [85/120    avg_loss:0.045, val_acc:0.996]
Epoch [86/120    avg_loss:0.026, val_acc:0.996]
Epoch [87/120    avg_loss:0.022, val_acc:0.994]
Epoch [88/120    avg_loss:0.019, val_acc:0.994]
Epoch [89/120    avg_loss:0.028, val_acc:0.994]
Epoch [90/120    avg_loss:0.021, val_acc:0.994]
Epoch [91/120    avg_loss:0.025, val_acc:0.994]
Epoch [92/120    avg_loss:0.031, val_acc:0.994]
Epoch [93/120    avg_loss:0.025, val_acc:0.996]
Epoch [94/120    avg_loss:0.023, val_acc:0.996]
Epoch [95/120    avg_loss:0.019, val_acc:0.996]
Epoch [96/120    avg_loss:0.021, val_acc:0.994]
Epoch [97/120    avg_loss:0.022, val_acc:0.994]
Epoch [98/120    avg_loss:0.020, val_acc:0.994]
Epoch [99/120    avg_loss:0.031, val_acc:0.996]
Epoch [100/120    avg_loss:0.021, val_acc:0.996]
Epoch [101/120    avg_loss:0.022, val_acc:0.996]
Epoch [102/120    avg_loss:0.017, val_acc:0.996]
Epoch [103/120    avg_loss:0.018, val_acc:0.996]
Epoch [104/120    avg_loss:0.017, val_acc:0.996]
Epoch [105/120    avg_loss:0.018, val_acc:0.994]
Epoch [106/120    avg_loss:0.021, val_acc:0.994]
Epoch [107/120    avg_loss:0.026, val_acc:0.994]
Epoch [108/120    avg_loss:0.017, val_acc:0.994]
Epoch [109/120    avg_loss:0.023, val_acc:0.992]
Epoch [110/120    avg_loss:0.020, val_acc:0.992]
Epoch [111/120    avg_loss:0.019, val_acc:0.996]
Epoch [112/120    avg_loss:0.019, val_acc:0.996]
Epoch [113/120    avg_loss:0.017, val_acc:0.994]
Epoch [114/120    avg_loss:0.020, val_acc:0.994]
Epoch [115/120    avg_loss:0.015, val_acc:0.994]
Epoch [116/120    avg_loss:0.023, val_acc:0.994]
Epoch [117/120    avg_loss:0.025, val_acc:0.994]
Epoch [118/120    avg_loss:0.015, val_acc:0.994]
Epoch [119/120    avg_loss:0.017, val_acc:0.994]
Epoch [120/120    avg_loss:0.019, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99095023 0.99563319 0.95089286 0.92567568
 1.         0.98378378 0.998713   1.         1.         0.99075297
 0.99224806 1.        ]

Kappa:
0.991929111420778
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4597e4b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.346, val_acc:0.583]
Epoch [2/120    avg_loss:1.886, val_acc:0.629]
Epoch [3/120    avg_loss:1.486, val_acc:0.683]
Epoch [4/120    avg_loss:1.164, val_acc:0.775]
Epoch [5/120    avg_loss:0.969, val_acc:0.842]
Epoch [6/120    avg_loss:0.844, val_acc:0.819]
Epoch [7/120    avg_loss:0.750, val_acc:0.887]
Epoch [8/120    avg_loss:0.622, val_acc:0.881]
Epoch [9/120    avg_loss:0.548, val_acc:0.881]
Epoch [10/120    avg_loss:0.514, val_acc:0.896]
Epoch [11/120    avg_loss:0.495, val_acc:0.881]
Epoch [12/120    avg_loss:0.459, val_acc:0.852]
Epoch [13/120    avg_loss:0.412, val_acc:0.873]
Epoch [14/120    avg_loss:0.328, val_acc:0.938]
Epoch [15/120    avg_loss:0.381, val_acc:0.869]
Epoch [16/120    avg_loss:0.535, val_acc:0.898]
Epoch [17/120    avg_loss:0.427, val_acc:0.917]
Epoch [18/120    avg_loss:0.361, val_acc:0.933]
Epoch [19/120    avg_loss:0.290, val_acc:0.927]
Epoch [20/120    avg_loss:0.318, val_acc:0.931]
Epoch [21/120    avg_loss:0.312, val_acc:0.923]
Epoch [22/120    avg_loss:0.322, val_acc:0.925]
Epoch [23/120    avg_loss:0.266, val_acc:0.938]
Epoch [24/120    avg_loss:0.368, val_acc:0.904]
Epoch [25/120    avg_loss:0.355, val_acc:0.925]
Epoch [26/120    avg_loss:0.252, val_acc:0.908]
Epoch [27/120    avg_loss:0.258, val_acc:0.919]
Epoch [28/120    avg_loss:0.263, val_acc:0.950]
Epoch [29/120    avg_loss:0.203, val_acc:0.927]
Epoch [30/120    avg_loss:0.212, val_acc:0.956]
Epoch [31/120    avg_loss:0.202, val_acc:0.948]
Epoch [32/120    avg_loss:0.204, val_acc:0.933]
Epoch [33/120    avg_loss:0.195, val_acc:0.952]
Epoch [34/120    avg_loss:0.180, val_acc:0.954]
Epoch [35/120    avg_loss:0.138, val_acc:0.950]
Epoch [36/120    avg_loss:0.175, val_acc:0.938]
Epoch [37/120    avg_loss:0.135, val_acc:0.965]
Epoch [38/120    avg_loss:0.134, val_acc:0.940]
Epoch [39/120    avg_loss:0.132, val_acc:0.952]
Epoch [40/120    avg_loss:0.134, val_acc:0.954]
Epoch [41/120    avg_loss:0.101, val_acc:0.969]
Epoch [42/120    avg_loss:0.141, val_acc:0.971]
Epoch [43/120    avg_loss:0.145, val_acc:0.942]
Epoch [44/120    avg_loss:0.101, val_acc:0.963]
Epoch [45/120    avg_loss:0.100, val_acc:0.975]
Epoch [46/120    avg_loss:0.115, val_acc:0.975]
Epoch [47/120    avg_loss:0.125, val_acc:0.942]
Epoch [48/120    avg_loss:0.152, val_acc:0.973]
Epoch [49/120    avg_loss:0.080, val_acc:0.967]
Epoch [50/120    avg_loss:0.074, val_acc:0.973]
Epoch [51/120    avg_loss:0.072, val_acc:0.977]
Epoch [52/120    avg_loss:0.089, val_acc:0.979]
Epoch [53/120    avg_loss:0.094, val_acc:0.969]
Epoch [54/120    avg_loss:0.077, val_acc:0.975]
Epoch [55/120    avg_loss:0.070, val_acc:0.977]
Epoch [56/120    avg_loss:0.104, val_acc:0.944]
Epoch [57/120    avg_loss:0.115, val_acc:0.963]
Epoch [58/120    avg_loss:0.105, val_acc:0.963]
Epoch [59/120    avg_loss:0.122, val_acc:0.973]
Epoch [60/120    avg_loss:0.052, val_acc:0.983]
Epoch [61/120    avg_loss:0.043, val_acc:0.985]
Epoch [62/120    avg_loss:0.039, val_acc:0.981]
Epoch [63/120    avg_loss:0.054, val_acc:0.981]
Epoch [64/120    avg_loss:0.074, val_acc:0.969]
Epoch [65/120    avg_loss:0.066, val_acc:0.969]
Epoch [66/120    avg_loss:0.058, val_acc:0.977]
Epoch [67/120    avg_loss:0.087, val_acc:0.967]
Epoch [68/120    avg_loss:0.087, val_acc:0.960]
Epoch [69/120    avg_loss:0.097, val_acc:0.971]
Epoch [70/120    avg_loss:0.071, val_acc:0.963]
Epoch [71/120    avg_loss:0.060, val_acc:0.956]
Epoch [72/120    avg_loss:0.086, val_acc:0.958]
Epoch [73/120    avg_loss:0.083, val_acc:0.967]
Epoch [74/120    avg_loss:0.052, val_acc:0.979]
Epoch [75/120    avg_loss:0.030, val_acc:0.979]
Epoch [76/120    avg_loss:0.019, val_acc:0.981]
Epoch [77/120    avg_loss:0.025, val_acc:0.983]
Epoch [78/120    avg_loss:0.031, val_acc:0.983]
Epoch [79/120    avg_loss:0.023, val_acc:0.981]
Epoch [80/120    avg_loss:0.021, val_acc:0.983]
Epoch [81/120    avg_loss:0.024, val_acc:0.983]
Epoch [82/120    avg_loss:0.026, val_acc:0.985]
Epoch [83/120    avg_loss:0.021, val_acc:0.983]
Epoch [84/120    avg_loss:0.022, val_acc:0.983]
Epoch [85/120    avg_loss:0.021, val_acc:0.983]
Epoch [86/120    avg_loss:0.018, val_acc:0.983]
Epoch [87/120    avg_loss:0.019, val_acc:0.983]
Epoch [88/120    avg_loss:0.022, val_acc:0.983]
Epoch [89/120    avg_loss:0.021, val_acc:0.983]
Epoch [90/120    avg_loss:0.017, val_acc:0.983]
Epoch [91/120    avg_loss:0.020, val_acc:0.985]
Epoch [92/120    avg_loss:0.018, val_acc:0.988]
Epoch [93/120    avg_loss:0.017, val_acc:0.988]
Epoch [94/120    avg_loss:0.017, val_acc:0.988]
Epoch [95/120    avg_loss:0.024, val_acc:0.988]
Epoch [96/120    avg_loss:0.025, val_acc:0.988]
Epoch [97/120    avg_loss:0.020, val_acc:0.990]
Epoch [98/120    avg_loss:0.017, val_acc:0.985]
Epoch [99/120    avg_loss:0.018, val_acc:0.988]
Epoch [100/120    avg_loss:0.019, val_acc:0.990]
Epoch [101/120    avg_loss:0.022, val_acc:0.988]
Epoch [102/120    avg_loss:0.018, val_acc:0.988]
Epoch [103/120    avg_loss:0.015, val_acc:0.988]
Epoch [104/120    avg_loss:0.021, val_acc:0.985]
Epoch [105/120    avg_loss:0.020, val_acc:0.985]
Epoch [106/120    avg_loss:0.016, val_acc:0.988]
Epoch [107/120    avg_loss:0.020, val_acc:0.988]
Epoch [108/120    avg_loss:0.015, val_acc:0.988]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.014, val_acc:0.990]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.014, val_acc:0.985]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.022, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.016, val_acc:0.988]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.018, val_acc:0.988]
Epoch [120/120    avg_loss:0.017, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 224   4   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.97767857 0.98678414 0.95259594 0.94736842
 1.         0.94972067 1.         0.99893276 1.         1.
 0.99889746 1.        ]

Kappa:
0.9924036327318395
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f914545b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.344, val_acc:0.460]
Epoch [2/120    avg_loss:1.928, val_acc:0.571]
Epoch [3/120    avg_loss:1.631, val_acc:0.646]
Epoch [4/120    avg_loss:1.330, val_acc:0.700]
Epoch [5/120    avg_loss:1.062, val_acc:0.744]
Epoch [6/120    avg_loss:0.875, val_acc:0.835]
Epoch [7/120    avg_loss:0.780, val_acc:0.810]
Epoch [8/120    avg_loss:0.709, val_acc:0.842]
Epoch [9/120    avg_loss:0.675, val_acc:0.856]
Epoch [10/120    avg_loss:0.585, val_acc:0.883]
Epoch [11/120    avg_loss:0.554, val_acc:0.877]
Epoch [12/120    avg_loss:0.580, val_acc:0.881]
Epoch [13/120    avg_loss:0.513, val_acc:0.887]
Epoch [14/120    avg_loss:0.456, val_acc:0.890]
Epoch [15/120    avg_loss:0.441, val_acc:0.900]
Epoch [16/120    avg_loss:0.418, val_acc:0.929]
Epoch [17/120    avg_loss:0.386, val_acc:0.923]
Epoch [18/120    avg_loss:0.366, val_acc:0.921]
Epoch [19/120    avg_loss:0.349, val_acc:0.933]
Epoch [20/120    avg_loss:0.312, val_acc:0.927]
Epoch [21/120    avg_loss:0.307, val_acc:0.935]
Epoch [22/120    avg_loss:0.284, val_acc:0.904]
Epoch [23/120    avg_loss:0.323, val_acc:0.923]
Epoch [24/120    avg_loss:0.274, val_acc:0.933]
Epoch [25/120    avg_loss:0.321, val_acc:0.929]
Epoch [26/120    avg_loss:0.238, val_acc:0.952]
Epoch [27/120    avg_loss:0.232, val_acc:0.935]
Epoch [28/120    avg_loss:0.217, val_acc:0.946]
Epoch [29/120    avg_loss:0.220, val_acc:0.931]
Epoch [30/120    avg_loss:0.213, val_acc:0.954]
Epoch [31/120    avg_loss:0.160, val_acc:0.954]
Epoch [32/120    avg_loss:0.147, val_acc:0.935]
Epoch [33/120    avg_loss:0.274, val_acc:0.956]
Epoch [34/120    avg_loss:0.234, val_acc:0.950]
Epoch [35/120    avg_loss:0.245, val_acc:0.925]
Epoch [36/120    avg_loss:0.228, val_acc:0.946]
Epoch [37/120    avg_loss:0.199, val_acc:0.958]
Epoch [38/120    avg_loss:0.180, val_acc:0.940]
Epoch [39/120    avg_loss:0.181, val_acc:0.954]
Epoch [40/120    avg_loss:0.155, val_acc:0.971]
Epoch [41/120    avg_loss:0.155, val_acc:0.967]
Epoch [42/120    avg_loss:0.149, val_acc:0.956]
Epoch [43/120    avg_loss:0.151, val_acc:0.921]
Epoch [44/120    avg_loss:0.166, val_acc:0.948]
Epoch [45/120    avg_loss:0.205, val_acc:0.894]
Epoch [46/120    avg_loss:0.186, val_acc:0.954]
Epoch [47/120    avg_loss:0.245, val_acc:0.952]
Epoch [48/120    avg_loss:0.147, val_acc:0.946]
Epoch [49/120    avg_loss:0.161, val_acc:0.969]
Epoch [50/120    avg_loss:0.114, val_acc:0.967]
Epoch [51/120    avg_loss:0.082, val_acc:0.977]
Epoch [52/120    avg_loss:0.090, val_acc:0.975]
Epoch [53/120    avg_loss:0.104, val_acc:0.956]
Epoch [54/120    avg_loss:0.114, val_acc:0.952]
Epoch [55/120    avg_loss:0.107, val_acc:0.969]
Epoch [56/120    avg_loss:0.089, val_acc:0.975]
Epoch [57/120    avg_loss:0.058, val_acc:0.979]
Epoch [58/120    avg_loss:0.066, val_acc:0.981]
Epoch [59/120    avg_loss:0.049, val_acc:0.979]
Epoch [60/120    avg_loss:0.051, val_acc:0.975]
Epoch [61/120    avg_loss:0.086, val_acc:0.960]
Epoch [62/120    avg_loss:0.105, val_acc:0.977]
Epoch [63/120    avg_loss:0.066, val_acc:0.979]
Epoch [64/120    avg_loss:0.059, val_acc:0.952]
Epoch [65/120    avg_loss:0.065, val_acc:0.983]
Epoch [66/120    avg_loss:0.085, val_acc:0.979]
Epoch [67/120    avg_loss:0.045, val_acc:0.971]
Epoch [68/120    avg_loss:0.053, val_acc:0.973]
Epoch [69/120    avg_loss:0.040, val_acc:0.983]
Epoch [70/120    avg_loss:0.045, val_acc:0.979]
Epoch [71/120    avg_loss:0.032, val_acc:0.985]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.040, val_acc:0.979]
Epoch [74/120    avg_loss:0.043, val_acc:0.983]
Epoch [75/120    avg_loss:0.043, val_acc:0.973]
Epoch [76/120    avg_loss:0.029, val_acc:0.988]
Epoch [77/120    avg_loss:0.032, val_acc:0.977]
Epoch [78/120    avg_loss:0.045, val_acc:0.958]
Epoch [79/120    avg_loss:0.040, val_acc:0.985]
Epoch [80/120    avg_loss:0.028, val_acc:0.985]
Epoch [81/120    avg_loss:0.034, val_acc:0.975]
Epoch [82/120    avg_loss:0.030, val_acc:0.983]
Epoch [83/120    avg_loss:0.045, val_acc:0.973]
Epoch [84/120    avg_loss:0.107, val_acc:0.979]
Epoch [85/120    avg_loss:0.051, val_acc:0.979]
Epoch [86/120    avg_loss:0.032, val_acc:0.979]
Epoch [87/120    avg_loss:0.023, val_acc:0.981]
Epoch [88/120    avg_loss:0.023, val_acc:0.981]
Epoch [89/120    avg_loss:0.017, val_acc:0.983]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.014, val_acc:0.985]
Epoch [92/120    avg_loss:0.013, val_acc:0.985]
Epoch [93/120    avg_loss:0.013, val_acc:0.985]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.013, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.985]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.013, val_acc:0.985]
Epoch [106/120    avg_loss:0.010, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.985]
Epoch [114/120    avg_loss:0.012, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  15   0   0   0   0   0   0   4   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99319728 0.98678414 0.93064877 0.92976589
 1.         0.98378378 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9919288168822871
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbd812c27f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.334, val_acc:0.506]
Epoch [2/120    avg_loss:1.886, val_acc:0.613]
Epoch [3/120    avg_loss:1.562, val_acc:0.702]
Epoch [4/120    avg_loss:1.261, val_acc:0.748]
Epoch [5/120    avg_loss:1.051, val_acc:0.823]
Epoch [6/120    avg_loss:0.888, val_acc:0.847]
Epoch [7/120    avg_loss:0.802, val_acc:0.788]
Epoch [8/120    avg_loss:0.699, val_acc:0.839]
Epoch [9/120    avg_loss:0.661, val_acc:0.829]
Epoch [10/120    avg_loss:0.669, val_acc:0.869]
Epoch [11/120    avg_loss:0.584, val_acc:0.893]
Epoch [12/120    avg_loss:0.490, val_acc:0.873]
Epoch [13/120    avg_loss:0.538, val_acc:0.905]
Epoch [14/120    avg_loss:0.408, val_acc:0.925]
Epoch [15/120    avg_loss:0.377, val_acc:0.919]
Epoch [16/120    avg_loss:0.398, val_acc:0.940]
Epoch [17/120    avg_loss:0.395, val_acc:0.933]
Epoch [18/120    avg_loss:0.360, val_acc:0.921]
Epoch [19/120    avg_loss:0.324, val_acc:0.923]
Epoch [20/120    avg_loss:0.283, val_acc:0.913]
Epoch [21/120    avg_loss:0.330, val_acc:0.933]
Epoch [22/120    avg_loss:0.302, val_acc:0.940]
Epoch [23/120    avg_loss:0.306, val_acc:0.937]
Epoch [24/120    avg_loss:0.289, val_acc:0.942]
Epoch [25/120    avg_loss:0.228, val_acc:0.940]
Epoch [26/120    avg_loss:0.258, val_acc:0.948]
Epoch [27/120    avg_loss:0.273, val_acc:0.927]
Epoch [28/120    avg_loss:0.240, val_acc:0.954]
Epoch [29/120    avg_loss:0.189, val_acc:0.962]
Epoch [30/120    avg_loss:0.199, val_acc:0.942]
Epoch [31/120    avg_loss:0.223, val_acc:0.950]
Epoch [32/120    avg_loss:0.212, val_acc:0.948]
Epoch [33/120    avg_loss:0.179, val_acc:0.915]
Epoch [34/120    avg_loss:0.225, val_acc:0.954]
Epoch [35/120    avg_loss:0.261, val_acc:0.923]
Epoch [36/120    avg_loss:0.213, val_acc:0.946]
Epoch [37/120    avg_loss:0.177, val_acc:0.966]
Epoch [38/120    avg_loss:0.166, val_acc:0.964]
Epoch [39/120    avg_loss:0.158, val_acc:0.944]
Epoch [40/120    avg_loss:0.156, val_acc:0.942]
Epoch [41/120    avg_loss:0.192, val_acc:0.956]
Epoch [42/120    avg_loss:0.118, val_acc:0.972]
Epoch [43/120    avg_loss:0.104, val_acc:0.982]
Epoch [44/120    avg_loss:0.088, val_acc:0.978]
Epoch [45/120    avg_loss:0.095, val_acc:0.974]
Epoch [46/120    avg_loss:0.076, val_acc:0.968]
Epoch [47/120    avg_loss:0.111, val_acc:0.974]
Epoch [48/120    avg_loss:0.086, val_acc:0.972]
Epoch [49/120    avg_loss:0.116, val_acc:0.974]
Epoch [50/120    avg_loss:0.106, val_acc:0.938]
Epoch [51/120    avg_loss:0.144, val_acc:0.962]
Epoch [52/120    avg_loss:0.132, val_acc:0.972]
Epoch [53/120    avg_loss:0.122, val_acc:0.960]
Epoch [54/120    avg_loss:0.115, val_acc:0.962]
Epoch [55/120    avg_loss:0.114, val_acc:0.966]
Epoch [56/120    avg_loss:0.122, val_acc:0.980]
Epoch [57/120    avg_loss:0.070, val_acc:0.980]
Epoch [58/120    avg_loss:0.060, val_acc:0.980]
Epoch [59/120    avg_loss:0.057, val_acc:0.986]
Epoch [60/120    avg_loss:0.051, val_acc:0.982]
Epoch [61/120    avg_loss:0.048, val_acc:0.986]
Epoch [62/120    avg_loss:0.059, val_acc:0.986]
Epoch [63/120    avg_loss:0.046, val_acc:0.988]
Epoch [64/120    avg_loss:0.041, val_acc:0.988]
Epoch [65/120    avg_loss:0.038, val_acc:0.988]
Epoch [66/120    avg_loss:0.045, val_acc:0.984]
Epoch [67/120    avg_loss:0.044, val_acc:0.984]
Epoch [68/120    avg_loss:0.048, val_acc:0.986]
Epoch [69/120    avg_loss:0.042, val_acc:0.986]
Epoch [70/120    avg_loss:0.064, val_acc:0.988]
Epoch [71/120    avg_loss:0.045, val_acc:0.988]
Epoch [72/120    avg_loss:0.047, val_acc:0.988]
Epoch [73/120    avg_loss:0.036, val_acc:0.988]
Epoch [74/120    avg_loss:0.047, val_acc:0.988]
Epoch [75/120    avg_loss:0.041, val_acc:0.990]
Epoch [76/120    avg_loss:0.043, val_acc:0.988]
Epoch [77/120    avg_loss:0.038, val_acc:0.988]
Epoch [78/120    avg_loss:0.032, val_acc:0.988]
Epoch [79/120    avg_loss:0.040, val_acc:0.986]
Epoch [80/120    avg_loss:0.040, val_acc:0.986]
Epoch [81/120    avg_loss:0.043, val_acc:0.988]
Epoch [82/120    avg_loss:0.038, val_acc:0.992]
Epoch [83/120    avg_loss:0.041, val_acc:0.988]
Epoch [84/120    avg_loss:0.041, val_acc:0.988]
Epoch [85/120    avg_loss:0.032, val_acc:0.992]
Epoch [86/120    avg_loss:0.031, val_acc:0.988]
Epoch [87/120    avg_loss:0.032, val_acc:0.986]
Epoch [88/120    avg_loss:0.038, val_acc:0.990]
Epoch [89/120    avg_loss:0.031, val_acc:0.992]
Epoch [90/120    avg_loss:0.036, val_acc:0.990]
Epoch [91/120    avg_loss:0.030, val_acc:0.992]
Epoch [92/120    avg_loss:0.036, val_acc:0.992]
Epoch [93/120    avg_loss:0.036, val_acc:0.990]
Epoch [94/120    avg_loss:0.040, val_acc:0.990]
Epoch [95/120    avg_loss:0.030, val_acc:0.990]
Epoch [96/120    avg_loss:0.032, val_acc:0.992]
Epoch [97/120    avg_loss:0.029, val_acc:0.990]
Epoch [98/120    avg_loss:0.025, val_acc:0.990]
Epoch [99/120    avg_loss:0.026, val_acc:0.990]
Epoch [100/120    avg_loss:0.025, val_acc:0.992]
Epoch [101/120    avg_loss:0.031, val_acc:0.992]
Epoch [102/120    avg_loss:0.038, val_acc:0.992]
Epoch [103/120    avg_loss:0.031, val_acc:0.992]
Epoch [104/120    avg_loss:0.025, val_acc:0.992]
Epoch [105/120    avg_loss:0.045, val_acc:0.992]
Epoch [106/120    avg_loss:0.029, val_acc:0.990]
Epoch [107/120    avg_loss:0.031, val_acc:0.990]
Epoch [108/120    avg_loss:0.033, val_acc:0.994]
Epoch [109/120    avg_loss:0.027, val_acc:0.996]
Epoch [110/120    avg_loss:0.028, val_acc:0.992]
Epoch [111/120    avg_loss:0.036, val_acc:0.992]
Epoch [112/120    avg_loss:0.034, val_acc:0.996]
Epoch [113/120    avg_loss:0.033, val_acc:0.994]
Epoch [114/120    avg_loss:0.030, val_acc:0.994]
Epoch [115/120    avg_loss:0.039, val_acc:0.994]
Epoch [116/120    avg_loss:0.028, val_acc:0.994]
Epoch [117/120    avg_loss:0.027, val_acc:0.994]
Epoch [118/120    avg_loss:0.033, val_acc:0.994]
Epoch [119/120    avg_loss:0.033, val_acc:0.994]
Epoch [120/120    avg_loss:0.028, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98206278 1.         0.94170404 0.91275168
 1.         0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919288900082736
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b23a87780>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.363, val_acc:0.458]
Epoch [2/120    avg_loss:1.863, val_acc:0.649]
Epoch [3/120    avg_loss:1.508, val_acc:0.681]
Epoch [4/120    avg_loss:1.243, val_acc:0.712]
Epoch [5/120    avg_loss:1.023, val_acc:0.742]
Epoch [6/120    avg_loss:0.834, val_acc:0.796]
Epoch [7/120    avg_loss:0.733, val_acc:0.819]
Epoch [8/120    avg_loss:0.653, val_acc:0.823]
Epoch [9/120    avg_loss:0.573, val_acc:0.796]
Epoch [10/120    avg_loss:0.535, val_acc:0.825]
Epoch [11/120    avg_loss:0.631, val_acc:0.849]
Epoch [12/120    avg_loss:0.577, val_acc:0.885]
Epoch [13/120    avg_loss:0.446, val_acc:0.871]
Epoch [14/120    avg_loss:0.484, val_acc:0.899]
Epoch [15/120    avg_loss:0.456, val_acc:0.911]
Epoch [16/120    avg_loss:0.390, val_acc:0.907]
Epoch [17/120    avg_loss:0.403, val_acc:0.925]
Epoch [18/120    avg_loss:0.344, val_acc:0.935]
Epoch [19/120    avg_loss:0.317, val_acc:0.907]
Epoch [20/120    avg_loss:0.302, val_acc:0.915]
Epoch [21/120    avg_loss:0.310, val_acc:0.911]
Epoch [22/120    avg_loss:0.361, val_acc:0.933]
Epoch [23/120    avg_loss:0.240, val_acc:0.927]
Epoch [24/120    avg_loss:0.216, val_acc:0.946]
Epoch [25/120    avg_loss:0.227, val_acc:0.923]
Epoch [26/120    avg_loss:0.280, val_acc:0.950]
Epoch [27/120    avg_loss:0.262, val_acc:0.935]
Epoch [28/120    avg_loss:0.245, val_acc:0.938]
Epoch [29/120    avg_loss:0.223, val_acc:0.958]
Epoch [30/120    avg_loss:0.188, val_acc:0.964]
Epoch [31/120    avg_loss:0.196, val_acc:0.966]
Epoch [32/120    avg_loss:0.177, val_acc:0.956]
Epoch [33/120    avg_loss:0.245, val_acc:0.956]
Epoch [34/120    avg_loss:0.164, val_acc:0.970]
Epoch [35/120    avg_loss:0.210, val_acc:0.917]
Epoch [36/120    avg_loss:0.221, val_acc:0.944]
Epoch [37/120    avg_loss:0.197, val_acc:0.962]
Epoch [38/120    avg_loss:0.158, val_acc:0.946]
Epoch [39/120    avg_loss:0.155, val_acc:0.966]
Epoch [40/120    avg_loss:0.158, val_acc:0.964]
Epoch [41/120    avg_loss:0.166, val_acc:0.974]
Epoch [42/120    avg_loss:0.152, val_acc:0.948]
Epoch [43/120    avg_loss:0.147, val_acc:0.966]
Epoch [44/120    avg_loss:0.167, val_acc:0.954]
Epoch [45/120    avg_loss:0.141, val_acc:0.962]
Epoch [46/120    avg_loss:0.113, val_acc:0.966]
Epoch [47/120    avg_loss:0.105, val_acc:0.978]
Epoch [48/120    avg_loss:0.113, val_acc:0.952]
Epoch [49/120    avg_loss:0.154, val_acc:0.956]
Epoch [50/120    avg_loss:0.146, val_acc:0.970]
Epoch [51/120    avg_loss:0.094, val_acc:0.978]
Epoch [52/120    avg_loss:0.095, val_acc:0.974]
Epoch [53/120    avg_loss:0.103, val_acc:0.962]
Epoch [54/120    avg_loss:0.117, val_acc:0.972]
Epoch [55/120    avg_loss:0.096, val_acc:0.974]
Epoch [56/120    avg_loss:0.100, val_acc:0.978]
Epoch [57/120    avg_loss:0.067, val_acc:0.980]
Epoch [58/120    avg_loss:0.095, val_acc:0.984]
Epoch [59/120    avg_loss:0.061, val_acc:0.982]
Epoch [60/120    avg_loss:0.058, val_acc:0.976]
Epoch [61/120    avg_loss:0.073, val_acc:0.960]
Epoch [62/120    avg_loss:0.074, val_acc:0.984]
Epoch [63/120    avg_loss:0.057, val_acc:0.986]
Epoch [64/120    avg_loss:0.076, val_acc:0.970]
Epoch [65/120    avg_loss:0.058, val_acc:0.986]
Epoch [66/120    avg_loss:0.053, val_acc:0.984]
Epoch [67/120    avg_loss:0.037, val_acc:0.982]
Epoch [68/120    avg_loss:0.044, val_acc:0.992]
Epoch [69/120    avg_loss:0.056, val_acc:0.976]
Epoch [70/120    avg_loss:0.077, val_acc:0.966]
Epoch [71/120    avg_loss:0.072, val_acc:0.978]
Epoch [72/120    avg_loss:0.096, val_acc:0.996]
Epoch [73/120    avg_loss:0.051, val_acc:0.984]
Epoch [74/120    avg_loss:0.061, val_acc:0.982]
Epoch [75/120    avg_loss:0.066, val_acc:0.990]
Epoch [76/120    avg_loss:0.045, val_acc:0.990]
Epoch [77/120    avg_loss:0.036, val_acc:0.990]
Epoch [78/120    avg_loss:0.032, val_acc:0.994]
Epoch [79/120    avg_loss:0.032, val_acc:0.994]
Epoch [80/120    avg_loss:0.032, val_acc:0.990]
Epoch [81/120    avg_loss:0.038, val_acc:0.986]
Epoch [82/120    avg_loss:0.036, val_acc:0.992]
Epoch [83/120    avg_loss:0.047, val_acc:0.992]
Epoch [84/120    avg_loss:0.029, val_acc:0.990]
Epoch [85/120    avg_loss:0.062, val_acc:0.970]
Epoch [86/120    avg_loss:0.098, val_acc:0.976]
Epoch [87/120    avg_loss:0.049, val_acc:0.982]
Epoch [88/120    avg_loss:0.031, val_acc:0.988]
Epoch [89/120    avg_loss:0.036, val_acc:0.988]
Epoch [90/120    avg_loss:0.024, val_acc:0.988]
Epoch [91/120    avg_loss:0.027, val_acc:0.988]
Epoch [92/120    avg_loss:0.023, val_acc:0.988]
Epoch [93/120    avg_loss:0.026, val_acc:0.990]
Epoch [94/120    avg_loss:0.022, val_acc:0.990]
Epoch [95/120    avg_loss:0.022, val_acc:0.990]
Epoch [96/120    avg_loss:0.023, val_acc:0.990]
Epoch [97/120    avg_loss:0.024, val_acc:0.990]
Epoch [98/120    avg_loss:0.024, val_acc:0.992]
Epoch [99/120    avg_loss:0.026, val_acc:0.992]
Epoch [100/120    avg_loss:0.018, val_acc:0.992]
Epoch [101/120    avg_loss:0.020, val_acc:0.992]
Epoch [102/120    avg_loss:0.020, val_acc:0.992]
Epoch [103/120    avg_loss:0.028, val_acc:0.992]
Epoch [104/120    avg_loss:0.023, val_acc:0.992]
Epoch [105/120    avg_loss:0.018, val_acc:0.992]
Epoch [106/120    avg_loss:0.021, val_acc:0.992]
Epoch [107/120    avg_loss:0.019, val_acc:0.992]
Epoch [108/120    avg_loss:0.021, val_acc:0.992]
Epoch [109/120    avg_loss:0.017, val_acc:0.992]
Epoch [110/120    avg_loss:0.026, val_acc:0.992]
Epoch [111/120    avg_loss:0.021, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.992]
Epoch [113/120    avg_loss:0.019, val_acc:0.992]
Epoch [114/120    avg_loss:0.022, val_acc:0.992]
Epoch [115/120    avg_loss:0.020, val_acc:0.992]
Epoch [116/120    avg_loss:0.018, val_acc:0.992]
Epoch [117/120    avg_loss:0.025, val_acc:0.992]
Epoch [118/120    avg_loss:0.018, val_acc:0.992]
Epoch [119/120    avg_loss:0.024, val_acc:0.992]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.99095023 0.99782135 0.94273128 0.91034483
 1.         0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9928784251510335
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc50340860>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.382, val_acc:0.523]
Epoch [2/120    avg_loss:1.877, val_acc:0.667]
Epoch [3/120    avg_loss:1.490, val_acc:0.727]
Epoch [4/120    avg_loss:1.216, val_acc:0.781]
Epoch [5/120    avg_loss:0.992, val_acc:0.829]
Epoch [6/120    avg_loss:0.920, val_acc:0.810]
Epoch [7/120    avg_loss:0.818, val_acc:0.885]
Epoch [8/120    avg_loss:0.662, val_acc:0.902]
Epoch [9/120    avg_loss:0.537, val_acc:0.906]
Epoch [10/120    avg_loss:0.497, val_acc:0.910]
Epoch [11/120    avg_loss:0.463, val_acc:0.912]
Epoch [12/120    avg_loss:0.478, val_acc:0.923]
Epoch [13/120    avg_loss:0.436, val_acc:0.912]
Epoch [14/120    avg_loss:0.397, val_acc:0.915]
Epoch [15/120    avg_loss:0.355, val_acc:0.935]
Epoch [16/120    avg_loss:0.352, val_acc:0.917]
Epoch [17/120    avg_loss:0.332, val_acc:0.950]
Epoch [18/120    avg_loss:0.327, val_acc:0.923]
Epoch [19/120    avg_loss:0.314, val_acc:0.956]
Epoch [20/120    avg_loss:0.329, val_acc:0.956]
Epoch [21/120    avg_loss:0.282, val_acc:0.942]
Epoch [22/120    avg_loss:0.333, val_acc:0.898]
Epoch [23/120    avg_loss:0.335, val_acc:0.954]
Epoch [24/120    avg_loss:0.267, val_acc:0.983]
Epoch [25/120    avg_loss:0.208, val_acc:0.935]
Epoch [26/120    avg_loss:0.231, val_acc:0.975]
Epoch [27/120    avg_loss:0.194, val_acc:0.971]
Epoch [28/120    avg_loss:0.166, val_acc:0.969]
Epoch [29/120    avg_loss:0.173, val_acc:0.956]
Epoch [30/120    avg_loss:0.193, val_acc:0.973]
Epoch [31/120    avg_loss:0.172, val_acc:0.979]
Epoch [32/120    avg_loss:0.176, val_acc:0.973]
Epoch [33/120    avg_loss:0.196, val_acc:0.971]
Epoch [34/120    avg_loss:0.159, val_acc:0.975]
Epoch [35/120    avg_loss:0.136, val_acc:0.977]
Epoch [36/120    avg_loss:0.125, val_acc:0.967]
Epoch [37/120    avg_loss:0.155, val_acc:0.948]
Epoch [38/120    avg_loss:0.160, val_acc:0.969]
Epoch [39/120    avg_loss:0.129, val_acc:0.979]
Epoch [40/120    avg_loss:0.087, val_acc:0.985]
Epoch [41/120    avg_loss:0.097, val_acc:0.983]
Epoch [42/120    avg_loss:0.083, val_acc:0.990]
Epoch [43/120    avg_loss:0.091, val_acc:0.988]
Epoch [44/120    avg_loss:0.074, val_acc:0.990]
Epoch [45/120    avg_loss:0.073, val_acc:0.992]
Epoch [46/120    avg_loss:0.075, val_acc:0.992]
Epoch [47/120    avg_loss:0.083, val_acc:0.992]
Epoch [48/120    avg_loss:0.087, val_acc:0.992]
Epoch [49/120    avg_loss:0.075, val_acc:0.990]
Epoch [50/120    avg_loss:0.061, val_acc:0.992]
Epoch [51/120    avg_loss:0.075, val_acc:0.992]
Epoch [52/120    avg_loss:0.069, val_acc:0.992]
Epoch [53/120    avg_loss:0.078, val_acc:0.990]
Epoch [54/120    avg_loss:0.069, val_acc:0.990]
Epoch [55/120    avg_loss:0.081, val_acc:0.992]
Epoch [56/120    avg_loss:0.075, val_acc:0.990]
Epoch [57/120    avg_loss:0.074, val_acc:0.994]
Epoch [58/120    avg_loss:0.064, val_acc:0.992]
Epoch [59/120    avg_loss:0.073, val_acc:0.992]
Epoch [60/120    avg_loss:0.065, val_acc:0.992]
Epoch [61/120    avg_loss:0.075, val_acc:0.992]
Epoch [62/120    avg_loss:0.067, val_acc:0.992]
Epoch [63/120    avg_loss:0.059, val_acc:0.992]
Epoch [64/120    avg_loss:0.062, val_acc:0.994]
Epoch [65/120    avg_loss:0.073, val_acc:0.992]
Epoch [66/120    avg_loss:0.062, val_acc:0.990]
Epoch [67/120    avg_loss:0.066, val_acc:0.990]
Epoch [68/120    avg_loss:0.057, val_acc:0.988]
Epoch [69/120    avg_loss:0.064, val_acc:0.992]
Epoch [70/120    avg_loss:0.052, val_acc:0.994]
Epoch [71/120    avg_loss:0.065, val_acc:0.990]
Epoch [72/120    avg_loss:0.069, val_acc:0.990]
Epoch [73/120    avg_loss:0.076, val_acc:0.992]
Epoch [74/120    avg_loss:0.077, val_acc:0.994]
Epoch [75/120    avg_loss:0.064, val_acc:0.990]
Epoch [76/120    avg_loss:0.063, val_acc:0.992]
Epoch [77/120    avg_loss:0.059, val_acc:0.994]
Epoch [78/120    avg_loss:0.049, val_acc:0.994]
Epoch [79/120    avg_loss:0.056, val_acc:0.990]
Epoch [80/120    avg_loss:0.049, val_acc:0.990]
Epoch [81/120    avg_loss:0.052, val_acc:0.994]
Epoch [82/120    avg_loss:0.045, val_acc:0.994]
Epoch [83/120    avg_loss:0.045, val_acc:0.988]
Epoch [84/120    avg_loss:0.044, val_acc:0.990]
Epoch [85/120    avg_loss:0.043, val_acc:0.988]
Epoch [86/120    avg_loss:0.054, val_acc:0.994]
Epoch [87/120    avg_loss:0.052, val_acc:0.994]
Epoch [88/120    avg_loss:0.054, val_acc:0.992]
Epoch [89/120    avg_loss:0.050, val_acc:0.994]
Epoch [90/120    avg_loss:0.056, val_acc:0.988]
Epoch [91/120    avg_loss:0.039, val_acc:0.992]
Epoch [92/120    avg_loss:0.045, val_acc:0.992]
Epoch [93/120    avg_loss:0.052, val_acc:0.994]
Epoch [94/120    avg_loss:0.038, val_acc:0.990]
Epoch [95/120    avg_loss:0.052, val_acc:0.994]
Epoch [96/120    avg_loss:0.049, val_acc:0.992]
Epoch [97/120    avg_loss:0.044, val_acc:0.992]
Epoch [98/120    avg_loss:0.044, val_acc:0.996]
Epoch [99/120    avg_loss:0.053, val_acc:0.992]
Epoch [100/120    avg_loss:0.046, val_acc:0.990]
Epoch [101/120    avg_loss:0.043, val_acc:0.992]
Epoch [102/120    avg_loss:0.045, val_acc:0.994]
Epoch [103/120    avg_loss:0.037, val_acc:0.992]
Epoch [104/120    avg_loss:0.045, val_acc:0.992]
Epoch [105/120    avg_loss:0.044, val_acc:0.990]
Epoch [106/120    avg_loss:0.046, val_acc:0.990]
Epoch [107/120    avg_loss:0.041, val_acc:0.992]
Epoch [108/120    avg_loss:0.043, val_acc:0.994]
Epoch [109/120    avg_loss:0.031, val_acc:0.992]
Epoch [110/120    avg_loss:0.037, val_acc:0.992]
Epoch [111/120    avg_loss:0.049, val_acc:0.990]
Epoch [112/120    avg_loss:0.034, val_acc:0.990]
Epoch [113/120    avg_loss:0.044, val_acc:0.990]
Epoch [114/120    avg_loss:0.043, val_acc:0.990]
Epoch [115/120    avg_loss:0.038, val_acc:0.990]
Epoch [116/120    avg_loss:0.037, val_acc:0.990]
Epoch [117/120    avg_loss:0.037, val_acc:0.990]
Epoch [118/120    avg_loss:0.036, val_acc:0.990]
Epoch [119/120    avg_loss:0.031, val_acc:0.990]
Epoch [120/120    avg_loss:0.033, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.98426966 0.99122807 0.93243243 0.91447368
 1.         0.96132597 1.         1.         1.         0.98550725
 0.98779134 1.        ]

Kappa:
0.9886060736733342
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa4a4da7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.346, val_acc:0.512]
Epoch [2/120    avg_loss:1.869, val_acc:0.611]
Epoch [3/120    avg_loss:1.518, val_acc:0.716]
Epoch [4/120    avg_loss:1.218, val_acc:0.710]
Epoch [5/120    avg_loss:1.017, val_acc:0.748]
Epoch [6/120    avg_loss:0.855, val_acc:0.823]
Epoch [7/120    avg_loss:0.725, val_acc:0.859]
Epoch [8/120    avg_loss:0.737, val_acc:0.849]
Epoch [9/120    avg_loss:0.677, val_acc:0.855]
Epoch [10/120    avg_loss:0.568, val_acc:0.905]
Epoch [11/120    avg_loss:0.472, val_acc:0.905]
Epoch [12/120    avg_loss:0.415, val_acc:0.921]
Epoch [13/120    avg_loss:0.444, val_acc:0.933]
Epoch [14/120    avg_loss:0.393, val_acc:0.931]
Epoch [15/120    avg_loss:0.334, val_acc:0.935]
Epoch [16/120    avg_loss:0.386, val_acc:0.944]
Epoch [17/120    avg_loss:0.339, val_acc:0.933]
Epoch [18/120    avg_loss:0.338, val_acc:0.946]
Epoch [19/120    avg_loss:0.382, val_acc:0.913]
Epoch [20/120    avg_loss:0.336, val_acc:0.960]
Epoch [21/120    avg_loss:0.254, val_acc:0.929]
Epoch [22/120    avg_loss:0.248, val_acc:0.950]
Epoch [23/120    avg_loss:0.209, val_acc:0.956]
Epoch [24/120    avg_loss:0.206, val_acc:0.921]
Epoch [25/120    avg_loss:0.216, val_acc:0.956]
Epoch [26/120    avg_loss:0.214, val_acc:0.966]
Epoch [27/120    avg_loss:0.171, val_acc:0.958]
Epoch [28/120    avg_loss:0.272, val_acc:0.954]
Epoch [29/120    avg_loss:0.202, val_acc:0.976]
Epoch [30/120    avg_loss:0.171, val_acc:0.956]
Epoch [31/120    avg_loss:0.195, val_acc:0.960]
Epoch [32/120    avg_loss:0.168, val_acc:0.956]
Epoch [33/120    avg_loss:0.210, val_acc:0.933]
Epoch [34/120    avg_loss:0.171, val_acc:0.982]
Epoch [35/120    avg_loss:0.133, val_acc:0.964]
Epoch [36/120    avg_loss:0.140, val_acc:0.970]
Epoch [37/120    avg_loss:0.161, val_acc:0.937]
Epoch [38/120    avg_loss:0.210, val_acc:0.960]
Epoch [39/120    avg_loss:0.135, val_acc:0.980]
Epoch [40/120    avg_loss:0.209, val_acc:0.946]
Epoch [41/120    avg_loss:0.187, val_acc:0.956]
Epoch [42/120    avg_loss:0.125, val_acc:0.972]
Epoch [43/120    avg_loss:0.108, val_acc:0.972]
Epoch [44/120    avg_loss:0.130, val_acc:0.976]
Epoch [45/120    avg_loss:0.170, val_acc:0.966]
Epoch [46/120    avg_loss:0.135, val_acc:0.958]
Epoch [47/120    avg_loss:0.113, val_acc:0.984]
Epoch [48/120    avg_loss:0.118, val_acc:0.980]
Epoch [49/120    avg_loss:0.093, val_acc:0.990]
Epoch [50/120    avg_loss:0.095, val_acc:0.992]
Epoch [51/120    avg_loss:0.100, val_acc:0.978]
Epoch [52/120    avg_loss:0.099, val_acc:0.978]
Epoch [53/120    avg_loss:0.075, val_acc:0.980]
Epoch [54/120    avg_loss:0.189, val_acc:0.972]
Epoch [55/120    avg_loss:0.143, val_acc:0.980]
Epoch [56/120    avg_loss:0.155, val_acc:0.988]
Epoch [57/120    avg_loss:0.105, val_acc:0.984]
Epoch [58/120    avg_loss:0.132, val_acc:0.982]
Epoch [59/120    avg_loss:0.147, val_acc:0.960]
Epoch [60/120    avg_loss:0.134, val_acc:0.988]
Epoch [61/120    avg_loss:0.094, val_acc:0.992]
Epoch [62/120    avg_loss:0.078, val_acc:0.994]
Epoch [63/120    avg_loss:0.066, val_acc:0.988]
Epoch [64/120    avg_loss:0.093, val_acc:0.970]
Epoch [65/120    avg_loss:0.135, val_acc:0.974]
Epoch [66/120    avg_loss:0.088, val_acc:0.992]
Epoch [67/120    avg_loss:0.054, val_acc:0.988]
Epoch [68/120    avg_loss:0.045, val_acc:0.994]
Epoch [69/120    avg_loss:0.032, val_acc:0.996]
Epoch [70/120    avg_loss:0.031, val_acc:0.996]
Epoch [71/120    avg_loss:0.043, val_acc:0.990]
Epoch [72/120    avg_loss:0.073, val_acc:0.996]
Epoch [73/120    avg_loss:0.035, val_acc:0.992]
Epoch [74/120    avg_loss:0.043, val_acc:0.994]
Epoch [75/120    avg_loss:0.051, val_acc:0.988]
Epoch [76/120    avg_loss:0.038, val_acc:0.994]
Epoch [77/120    avg_loss:0.027, val_acc:0.988]
Epoch [78/120    avg_loss:0.071, val_acc:0.992]
Epoch [79/120    avg_loss:0.094, val_acc:0.966]
Epoch [80/120    avg_loss:0.073, val_acc:0.988]
Epoch [81/120    avg_loss:0.032, val_acc:0.994]
Epoch [82/120    avg_loss:0.025, val_acc:0.994]
Epoch [83/120    avg_loss:0.018, val_acc:0.992]
Epoch [84/120    avg_loss:0.019, val_acc:0.994]
Epoch [85/120    avg_loss:0.042, val_acc:0.996]
Epoch [86/120    avg_loss:0.017, val_acc:0.996]
Epoch [87/120    avg_loss:0.017, val_acc:0.994]
Epoch [88/120    avg_loss:0.012, val_acc:0.996]
Epoch [89/120    avg_loss:0.014, val_acc:0.996]
Epoch [90/120    avg_loss:0.011, val_acc:0.998]
Epoch [91/120    avg_loss:0.011, val_acc:0.998]
Epoch [92/120    avg_loss:0.010, val_acc:0.996]
Epoch [93/120    avg_loss:0.009, val_acc:0.996]
Epoch [94/120    avg_loss:0.012, val_acc:0.996]
Epoch [95/120    avg_loss:0.022, val_acc:0.994]
Epoch [96/120    avg_loss:0.015, val_acc:0.998]
Epoch [97/120    avg_loss:0.024, val_acc:0.992]
Epoch [98/120    avg_loss:0.027, val_acc:0.994]
Epoch [99/120    avg_loss:0.027, val_acc:0.990]
Epoch [100/120    avg_loss:0.023, val_acc:0.996]
Epoch [101/120    avg_loss:0.021, val_acc:0.988]
Epoch [102/120    avg_loss:0.015, val_acc:0.994]
Epoch [103/120    avg_loss:0.010, val_acc:0.994]
Epoch [104/120    avg_loss:0.020, val_acc:0.990]
Epoch [105/120    avg_loss:0.016, val_acc:0.992]
Epoch [106/120    avg_loss:0.015, val_acc:0.992]
Epoch [107/120    avg_loss:0.009, val_acc:0.996]
Epoch [108/120    avg_loss:0.035, val_acc:0.982]
Epoch [109/120    avg_loss:0.044, val_acc:0.986]
Epoch [110/120    avg_loss:0.019, val_acc:0.988]
Epoch [111/120    avg_loss:0.015, val_acc:0.990]
Epoch [112/120    avg_loss:0.023, val_acc:0.994]
Epoch [113/120    avg_loss:0.014, val_acc:0.998]
Epoch [114/120    avg_loss:0.009, val_acc:0.998]
Epoch [115/120    avg_loss:0.010, val_acc:0.998]
Epoch [116/120    avg_loss:0.010, val_acc:0.998]
Epoch [117/120    avg_loss:0.010, val_acc:0.998]
Epoch [118/120    avg_loss:0.011, val_acc:0.998]
Epoch [119/120    avg_loss:0.009, val_acc:0.998]
Epoch [120/120    avg_loss:0.007, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         1.         0.99782135 0.92608696 0.88028169
 1.         1.         1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9916913420899449
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f1b0ab828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.419, val_acc:0.502]
Epoch [2/120    avg_loss:1.952, val_acc:0.619]
Epoch [3/120    avg_loss:1.621, val_acc:0.677]
Epoch [4/120    avg_loss:1.361, val_acc:0.708]
Epoch [5/120    avg_loss:1.124, val_acc:0.738]
Epoch [6/120    avg_loss:0.994, val_acc:0.783]
Epoch [7/120    avg_loss:0.862, val_acc:0.804]
Epoch [8/120    avg_loss:0.745, val_acc:0.831]
Epoch [9/120    avg_loss:0.706, val_acc:0.863]
Epoch [10/120    avg_loss:0.615, val_acc:0.900]
Epoch [11/120    avg_loss:0.556, val_acc:0.900]
Epoch [12/120    avg_loss:0.539, val_acc:0.875]
Epoch [13/120    avg_loss:0.474, val_acc:0.931]
Epoch [14/120    avg_loss:0.399, val_acc:0.898]
Epoch [15/120    avg_loss:0.414, val_acc:0.919]
Epoch [16/120    avg_loss:0.417, val_acc:0.938]
Epoch [17/120    avg_loss:0.364, val_acc:0.935]
Epoch [18/120    avg_loss:0.349, val_acc:0.923]
Epoch [19/120    avg_loss:0.377, val_acc:0.908]
Epoch [20/120    avg_loss:0.315, val_acc:0.912]
Epoch [21/120    avg_loss:0.350, val_acc:0.935]
Epoch [22/120    avg_loss:0.283, val_acc:0.952]
Epoch [23/120    avg_loss:0.288, val_acc:0.940]
Epoch [24/120    avg_loss:0.233, val_acc:0.935]
Epoch [25/120    avg_loss:0.260, val_acc:0.946]
Epoch [26/120    avg_loss:0.318, val_acc:0.927]
Epoch [27/120    avg_loss:0.234, val_acc:0.950]
Epoch [28/120    avg_loss:0.230, val_acc:0.940]
Epoch [29/120    avg_loss:0.177, val_acc:0.954]
Epoch [30/120    avg_loss:0.201, val_acc:0.940]
Epoch [31/120    avg_loss:0.158, val_acc:0.952]
Epoch [32/120    avg_loss:0.164, val_acc:0.969]
Epoch [33/120    avg_loss:0.154, val_acc:0.956]
Epoch [34/120    avg_loss:0.137, val_acc:0.958]
Epoch [35/120    avg_loss:0.145, val_acc:0.944]
Epoch [36/120    avg_loss:0.123, val_acc:0.969]
Epoch [37/120    avg_loss:0.257, val_acc:0.840]
Epoch [38/120    avg_loss:0.207, val_acc:0.942]
Epoch [39/120    avg_loss:0.202, val_acc:0.929]
Epoch [40/120    avg_loss:0.228, val_acc:0.944]
Epoch [41/120    avg_loss:0.169, val_acc:0.960]
Epoch [42/120    avg_loss:0.096, val_acc:0.973]
Epoch [43/120    avg_loss:0.090, val_acc:0.981]
Epoch [44/120    avg_loss:0.073, val_acc:0.971]
Epoch [45/120    avg_loss:0.105, val_acc:0.971]
Epoch [46/120    avg_loss:0.087, val_acc:0.969]
Epoch [47/120    avg_loss:0.093, val_acc:0.975]
Epoch [48/120    avg_loss:0.104, val_acc:0.975]
Epoch [49/120    avg_loss:0.073, val_acc:0.942]
Epoch [50/120    avg_loss:0.114, val_acc:0.965]
Epoch [51/120    avg_loss:0.112, val_acc:0.977]
Epoch [52/120    avg_loss:0.072, val_acc:0.983]
Epoch [53/120    avg_loss:0.080, val_acc:0.977]
Epoch [54/120    avg_loss:0.101, val_acc:0.979]
Epoch [55/120    avg_loss:0.102, val_acc:0.971]
Epoch [56/120    avg_loss:0.090, val_acc:0.988]
Epoch [57/120    avg_loss:0.052, val_acc:0.975]
Epoch [58/120    avg_loss:0.052, val_acc:0.965]
Epoch [59/120    avg_loss:0.082, val_acc:0.977]
Epoch [60/120    avg_loss:0.098, val_acc:0.992]
Epoch [61/120    avg_loss:0.060, val_acc:0.983]
Epoch [62/120    avg_loss:0.051, val_acc:0.977]
Epoch [63/120    avg_loss:0.061, val_acc:0.983]
Epoch [64/120    avg_loss:0.072, val_acc:0.988]
Epoch [65/120    avg_loss:0.060, val_acc:0.973]
Epoch [66/120    avg_loss:0.037, val_acc:0.979]
Epoch [67/120    avg_loss:0.038, val_acc:0.983]
Epoch [68/120    avg_loss:0.033, val_acc:0.981]
Epoch [69/120    avg_loss:0.045, val_acc:0.988]
Epoch [70/120    avg_loss:0.062, val_acc:0.981]
Epoch [71/120    avg_loss:0.068, val_acc:0.981]
Epoch [72/120    avg_loss:0.050, val_acc:0.971]
Epoch [73/120    avg_loss:0.038, val_acc:0.988]
Epoch [74/120    avg_loss:0.030, val_acc:0.988]
Epoch [75/120    avg_loss:0.032, val_acc:0.990]
Epoch [76/120    avg_loss:0.019, val_acc:0.990]
Epoch [77/120    avg_loss:0.027, val_acc:0.990]
Epoch [78/120    avg_loss:0.017, val_acc:0.992]
Epoch [79/120    avg_loss:0.018, val_acc:0.992]
Epoch [80/120    avg_loss:0.016, val_acc:0.990]
Epoch [81/120    avg_loss:0.015, val_acc:0.990]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.015, val_acc:0.990]
Epoch [84/120    avg_loss:0.024, val_acc:0.992]
Epoch [85/120    avg_loss:0.014, val_acc:0.992]
Epoch [86/120    avg_loss:0.016, val_acc:0.992]
Epoch [87/120    avg_loss:0.017, val_acc:0.992]
Epoch [88/120    avg_loss:0.017, val_acc:0.992]
Epoch [89/120    avg_loss:0.019, val_acc:0.992]
Epoch [90/120    avg_loss:0.017, val_acc:0.992]
Epoch [91/120    avg_loss:0.018, val_acc:0.992]
Epoch [92/120    avg_loss:0.013, val_acc:0.992]
Epoch [93/120    avg_loss:0.015, val_acc:0.992]
Epoch [94/120    avg_loss:0.013, val_acc:0.992]
Epoch [95/120    avg_loss:0.016, val_acc:0.990]
Epoch [96/120    avg_loss:0.019, val_acc:0.990]
Epoch [97/120    avg_loss:0.016, val_acc:0.990]
Epoch [98/120    avg_loss:0.018, val_acc:0.992]
Epoch [99/120    avg_loss:0.014, val_acc:0.990]
Epoch [100/120    avg_loss:0.017, val_acc:0.992]
Epoch [101/120    avg_loss:0.015, val_acc:0.992]
Epoch [102/120    avg_loss:0.019, val_acc:0.992]
Epoch [103/120    avg_loss:0.010, val_acc:0.992]
Epoch [104/120    avg_loss:0.023, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.017, val_acc:0.992]
Epoch [107/120    avg_loss:0.019, val_acc:0.990]
Epoch [108/120    avg_loss:0.016, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.990]
Epoch [110/120    avg_loss:0.016, val_acc:0.990]
Epoch [111/120    avg_loss:0.015, val_acc:0.992]
Epoch [112/120    avg_loss:0.011, val_acc:0.992]
Epoch [113/120    avg_loss:0.009, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.990]
Epoch [115/120    avg_loss:0.013, val_acc:0.992]
Epoch [116/120    avg_loss:0.014, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  11   0   0   0   0   0   0   5   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 0.99853801 0.99095023 1.         0.9254386  0.8975265
 0.99516908 0.97826087 1.         1.         1.         0.99077734
 0.98675497 1.        ]

Kappa:
0.9888425825723802
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9fb46b7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.392, val_acc:0.483]
Epoch [2/120    avg_loss:1.918, val_acc:0.621]
Epoch [3/120    avg_loss:1.552, val_acc:0.671]
Epoch [4/120    avg_loss:1.240, val_acc:0.690]
Epoch [5/120    avg_loss:1.020, val_acc:0.729]
Epoch [6/120    avg_loss:0.869, val_acc:0.713]
Epoch [7/120    avg_loss:0.839, val_acc:0.808]
Epoch [8/120    avg_loss:0.751, val_acc:0.760]
Epoch [9/120    avg_loss:0.616, val_acc:0.840]
Epoch [10/120    avg_loss:0.580, val_acc:0.848]
Epoch [11/120    avg_loss:0.557, val_acc:0.781]
Epoch [12/120    avg_loss:0.487, val_acc:0.892]
Epoch [13/120    avg_loss:0.417, val_acc:0.890]
Epoch [14/120    avg_loss:0.484, val_acc:0.871]
Epoch [15/120    avg_loss:0.547, val_acc:0.900]
Epoch [16/120    avg_loss:0.419, val_acc:0.904]
Epoch [17/120    avg_loss:0.387, val_acc:0.935]
Epoch [18/120    avg_loss:0.371, val_acc:0.944]
Epoch [19/120    avg_loss:0.308, val_acc:0.925]
Epoch [20/120    avg_loss:0.326, val_acc:0.927]
Epoch [21/120    avg_loss:0.269, val_acc:0.967]
Epoch [22/120    avg_loss:0.232, val_acc:0.948]
Epoch [23/120    avg_loss:0.254, val_acc:0.919]
Epoch [24/120    avg_loss:0.234, val_acc:0.960]
Epoch [25/120    avg_loss:0.219, val_acc:0.950]
Epoch [26/120    avg_loss:0.222, val_acc:0.956]
Epoch [27/120    avg_loss:0.182, val_acc:0.952]
Epoch [28/120    avg_loss:0.170, val_acc:0.950]
Epoch [29/120    avg_loss:0.207, val_acc:0.948]
Epoch [30/120    avg_loss:0.271, val_acc:0.942]
Epoch [31/120    avg_loss:0.212, val_acc:0.977]
Epoch [32/120    avg_loss:0.175, val_acc:0.965]
Epoch [33/120    avg_loss:0.146, val_acc:0.977]
Epoch [34/120    avg_loss:0.159, val_acc:0.960]
Epoch [35/120    avg_loss:0.166, val_acc:0.965]
Epoch [36/120    avg_loss:0.154, val_acc:0.954]
Epoch [37/120    avg_loss:0.121, val_acc:0.954]
Epoch [38/120    avg_loss:0.145, val_acc:0.967]
Epoch [39/120    avg_loss:0.175, val_acc:0.971]
Epoch [40/120    avg_loss:0.126, val_acc:0.965]
Epoch [41/120    avg_loss:0.100, val_acc:0.965]
Epoch [42/120    avg_loss:0.140, val_acc:0.927]
Epoch [43/120    avg_loss:0.141, val_acc:0.958]
Epoch [44/120    avg_loss:0.101, val_acc:0.967]
Epoch [45/120    avg_loss:0.089, val_acc:0.973]
Epoch [46/120    avg_loss:0.120, val_acc:0.969]
Epoch [47/120    avg_loss:0.076, val_acc:0.981]
Epoch [48/120    avg_loss:0.061, val_acc:0.981]
Epoch [49/120    avg_loss:0.048, val_acc:0.983]
Epoch [50/120    avg_loss:0.057, val_acc:0.985]
Epoch [51/120    avg_loss:0.049, val_acc:0.985]
Epoch [52/120    avg_loss:0.067, val_acc:0.983]
Epoch [53/120    avg_loss:0.057, val_acc:0.985]
Epoch [54/120    avg_loss:0.048, val_acc:0.985]
Epoch [55/120    avg_loss:0.047, val_acc:0.983]
Epoch [56/120    avg_loss:0.056, val_acc:0.983]
Epoch [57/120    avg_loss:0.047, val_acc:0.990]
Epoch [58/120    avg_loss:0.050, val_acc:0.990]
Epoch [59/120    avg_loss:0.040, val_acc:0.990]
Epoch [60/120    avg_loss:0.051, val_acc:0.985]
Epoch [61/120    avg_loss:0.040, val_acc:0.985]
Epoch [62/120    avg_loss:0.043, val_acc:0.985]
Epoch [63/120    avg_loss:0.034, val_acc:0.988]
Epoch [64/120    avg_loss:0.047, val_acc:0.990]
Epoch [65/120    avg_loss:0.045, val_acc:0.990]
Epoch [66/120    avg_loss:0.056, val_acc:0.990]
Epoch [67/120    avg_loss:0.053, val_acc:0.988]
Epoch [68/120    avg_loss:0.046, val_acc:0.992]
Epoch [69/120    avg_loss:0.043, val_acc:0.990]
Epoch [70/120    avg_loss:0.041, val_acc:0.988]
Epoch [71/120    avg_loss:0.033, val_acc:0.990]
Epoch [72/120    avg_loss:0.043, val_acc:0.996]
Epoch [73/120    avg_loss:0.031, val_acc:0.988]
Epoch [74/120    avg_loss:0.039, val_acc:0.990]
Epoch [75/120    avg_loss:0.042, val_acc:0.983]
Epoch [76/120    avg_loss:0.042, val_acc:0.996]
Epoch [77/120    avg_loss:0.033, val_acc:0.996]
Epoch [78/120    avg_loss:0.044, val_acc:0.996]
Epoch [79/120    avg_loss:0.047, val_acc:0.985]
Epoch [80/120    avg_loss:0.047, val_acc:0.994]
Epoch [81/120    avg_loss:0.038, val_acc:0.992]
Epoch [82/120    avg_loss:0.032, val_acc:0.998]
Epoch [83/120    avg_loss:0.038, val_acc:0.996]
Epoch [84/120    avg_loss:0.044, val_acc:0.985]
Epoch [85/120    avg_loss:0.057, val_acc:0.990]
Epoch [86/120    avg_loss:0.058, val_acc:0.996]
Epoch [87/120    avg_loss:0.027, val_acc:0.994]
Epoch [88/120    avg_loss:0.033, val_acc:0.992]
Epoch [89/120    avg_loss:0.029, val_acc:0.988]
Epoch [90/120    avg_loss:0.039, val_acc:0.998]
Epoch [91/120    avg_loss:0.029, val_acc:0.996]
Epoch [92/120    avg_loss:0.031, val_acc:0.996]
Epoch [93/120    avg_loss:0.036, val_acc:0.992]
Epoch [94/120    avg_loss:0.048, val_acc:0.985]
Epoch [95/120    avg_loss:0.034, val_acc:0.992]
Epoch [96/120    avg_loss:0.030, val_acc:0.996]
Epoch [97/120    avg_loss:0.033, val_acc:0.996]
Epoch [98/120    avg_loss:0.037, val_acc:0.988]
Epoch [99/120    avg_loss:0.034, val_acc:0.992]
Epoch [100/120    avg_loss:0.027, val_acc:0.990]
Epoch [101/120    avg_loss:0.030, val_acc:0.998]
Epoch [102/120    avg_loss:0.028, val_acc:0.996]
Epoch [103/120    avg_loss:0.034, val_acc:0.992]
Epoch [104/120    avg_loss:0.040, val_acc:0.994]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.027, val_acc:0.994]
Epoch [107/120    avg_loss:0.029, val_acc:0.996]
Epoch [108/120    avg_loss:0.027, val_acc:0.990]
Epoch [109/120    avg_loss:0.034, val_acc:0.998]
Epoch [110/120    avg_loss:0.029, val_acc:0.992]
Epoch [111/120    avg_loss:0.028, val_acc:0.992]
Epoch [112/120    avg_loss:0.031, val_acc:0.990]
Epoch [113/120    avg_loss:0.024, val_acc:0.988]
Epoch [114/120    avg_loss:0.032, val_acc:0.992]
Epoch [115/120    avg_loss:0.031, val_acc:0.988]
Epoch [116/120    avg_loss:0.042, val_acc:0.990]
Epoch [117/120    avg_loss:0.024, val_acc:0.994]
Epoch [118/120    avg_loss:0.028, val_acc:0.990]
Epoch [119/120    avg_loss:0.025, val_acc:0.990]
Epoch [120/120    avg_loss:0.026, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  11   0   0   0   0   0   0   3   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.96598639 0.96
 0.99516908 0.98924731 1.         1.         1.         0.9973545
 0.99448732 1.        ]

Kappa:
0.9950152422544689
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe7d25c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.404, val_acc:0.456]
Epoch [2/120    avg_loss:1.889, val_acc:0.583]
Epoch [3/120    avg_loss:1.565, val_acc:0.679]
Epoch [4/120    avg_loss:1.279, val_acc:0.735]
Epoch [5/120    avg_loss:1.100, val_acc:0.769]
Epoch [6/120    avg_loss:0.921, val_acc:0.815]
Epoch [7/120    avg_loss:0.761, val_acc:0.804]
Epoch [8/120    avg_loss:0.725, val_acc:0.815]
Epoch [9/120    avg_loss:0.632, val_acc:0.842]
Epoch [10/120    avg_loss:0.546, val_acc:0.865]
Epoch [11/120    avg_loss:0.570, val_acc:0.798]
Epoch [12/120    avg_loss:0.510, val_acc:0.871]
Epoch [13/120    avg_loss:0.465, val_acc:0.865]
Epoch [14/120    avg_loss:0.484, val_acc:0.875]
Epoch [15/120    avg_loss:0.428, val_acc:0.927]
Epoch [16/120    avg_loss:0.348, val_acc:0.885]
Epoch [17/120    avg_loss:0.424, val_acc:0.877]
Epoch [18/120    avg_loss:0.411, val_acc:0.915]
Epoch [19/120    avg_loss:0.327, val_acc:0.902]
Epoch [20/120    avg_loss:0.353, val_acc:0.902]
Epoch [21/120    avg_loss:0.337, val_acc:0.894]
Epoch [22/120    avg_loss:0.310, val_acc:0.929]
Epoch [23/120    avg_loss:0.290, val_acc:0.915]
Epoch [24/120    avg_loss:0.238, val_acc:0.938]
Epoch [25/120    avg_loss:0.313, val_acc:0.927]
Epoch [26/120    avg_loss:0.302, val_acc:0.915]
Epoch [27/120    avg_loss:0.239, val_acc:0.948]
Epoch [28/120    avg_loss:0.203, val_acc:0.950]
Epoch [29/120    avg_loss:0.168, val_acc:0.956]
Epoch [30/120    avg_loss:0.164, val_acc:0.967]
Epoch [31/120    avg_loss:0.159, val_acc:0.931]
Epoch [32/120    avg_loss:0.158, val_acc:0.954]
Epoch [33/120    avg_loss:0.234, val_acc:0.912]
Epoch [34/120    avg_loss:0.236, val_acc:0.942]
Epoch [35/120    avg_loss:0.206, val_acc:0.965]
Epoch [36/120    avg_loss:0.348, val_acc:0.898]
Epoch [37/120    avg_loss:0.277, val_acc:0.929]
Epoch [38/120    avg_loss:0.319, val_acc:0.923]
Epoch [39/120    avg_loss:0.257, val_acc:0.950]
Epoch [40/120    avg_loss:0.170, val_acc:0.975]
Epoch [41/120    avg_loss:0.150, val_acc:0.963]
Epoch [42/120    avg_loss:0.143, val_acc:0.958]
Epoch [43/120    avg_loss:0.124, val_acc:0.963]
Epoch [44/120    avg_loss:0.127, val_acc:0.965]
Epoch [45/120    avg_loss:0.119, val_acc:0.971]
Epoch [46/120    avg_loss:0.098, val_acc:0.973]
Epoch [47/120    avg_loss:0.094, val_acc:0.975]
Epoch [48/120    avg_loss:0.070, val_acc:0.977]
Epoch [49/120    avg_loss:0.092, val_acc:0.973]
Epoch [50/120    avg_loss:0.110, val_acc:0.981]
Epoch [51/120    avg_loss:0.090, val_acc:0.975]
Epoch [52/120    avg_loss:0.101, val_acc:0.944]
Epoch [53/120    avg_loss:0.094, val_acc:0.988]
Epoch [54/120    avg_loss:0.087, val_acc:0.975]
Epoch [55/120    avg_loss:0.064, val_acc:0.985]
Epoch [56/120    avg_loss:0.057, val_acc:0.981]
Epoch [57/120    avg_loss:0.069, val_acc:0.983]
Epoch [58/120    avg_loss:0.088, val_acc:0.983]
Epoch [59/120    avg_loss:0.066, val_acc:0.985]
Epoch [60/120    avg_loss:0.068, val_acc:0.983]
Epoch [61/120    avg_loss:0.098, val_acc:0.977]
Epoch [62/120    avg_loss:0.066, val_acc:0.981]
Epoch [63/120    avg_loss:0.097, val_acc:0.988]
Epoch [64/120    avg_loss:0.060, val_acc:0.975]
Epoch [65/120    avg_loss:0.074, val_acc:0.977]
Epoch [66/120    avg_loss:0.050, val_acc:0.981]
Epoch [67/120    avg_loss:0.053, val_acc:0.983]
Epoch [68/120    avg_loss:0.031, val_acc:0.988]
Epoch [69/120    avg_loss:0.026, val_acc:0.990]
Epoch [70/120    avg_loss:0.040, val_acc:0.983]
Epoch [71/120    avg_loss:0.050, val_acc:0.975]
Epoch [72/120    avg_loss:0.048, val_acc:0.985]
Epoch [73/120    avg_loss:0.041, val_acc:0.975]
Epoch [74/120    avg_loss:0.038, val_acc:0.983]
Epoch [75/120    avg_loss:0.029, val_acc:0.985]
Epoch [76/120    avg_loss:0.035, val_acc:0.985]
Epoch [77/120    avg_loss:0.028, val_acc:0.988]
Epoch [78/120    avg_loss:0.022, val_acc:0.985]
Epoch [79/120    avg_loss:0.024, val_acc:0.994]
Epoch [80/120    avg_loss:0.029, val_acc:0.990]
Epoch [81/120    avg_loss:0.024, val_acc:0.985]
Epoch [82/120    avg_loss:0.030, val_acc:0.985]
Epoch [83/120    avg_loss:0.048, val_acc:0.981]
Epoch [84/120    avg_loss:0.038, val_acc:0.983]
Epoch [85/120    avg_loss:0.039, val_acc:0.990]
Epoch [86/120    avg_loss:0.022, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.994]
Epoch [88/120    avg_loss:0.014, val_acc:0.992]
Epoch [89/120    avg_loss:0.021, val_acc:0.992]
Epoch [90/120    avg_loss:0.021, val_acc:0.992]
Epoch [91/120    avg_loss:0.024, val_acc:0.992]
Epoch [92/120    avg_loss:0.047, val_acc:0.948]
Epoch [93/120    avg_loss:0.186, val_acc:0.935]
Epoch [94/120    avg_loss:0.148, val_acc:0.979]
Epoch [95/120    avg_loss:0.108, val_acc:0.985]
Epoch [96/120    avg_loss:0.061, val_acc:0.990]
Epoch [97/120    avg_loss:0.050, val_acc:0.988]
Epoch [98/120    avg_loss:0.033, val_acc:0.988]
Epoch [99/120    avg_loss:0.019, val_acc:0.994]
Epoch [100/120    avg_loss:0.019, val_acc:0.998]
Epoch [101/120    avg_loss:0.016, val_acc:0.994]
Epoch [102/120    avg_loss:0.015, val_acc:0.994]
Epoch [103/120    avg_loss:0.021, val_acc:0.988]
Epoch [104/120    avg_loss:0.022, val_acc:0.983]
Epoch [105/120    avg_loss:0.031, val_acc:0.983]
Epoch [106/120    avg_loss:0.016, val_acc:0.992]
Epoch [107/120    avg_loss:0.024, val_acc:0.992]
Epoch [108/120    avg_loss:0.030, val_acc:0.992]
Epoch [109/120    avg_loss:0.028, val_acc:0.990]
Epoch [110/120    avg_loss:0.016, val_acc:0.996]
Epoch [111/120    avg_loss:0.028, val_acc:0.981]
Epoch [112/120    avg_loss:0.021, val_acc:0.990]
Epoch [113/120    avg_loss:0.025, val_acc:0.992]
Epoch [114/120    avg_loss:0.014, val_acc:0.994]
Epoch [115/120    avg_loss:0.010, val_acc:0.994]
Epoch [116/120    avg_loss:0.018, val_acc:0.994]
Epoch [117/120    avg_loss:0.010, val_acc:0.994]
Epoch [118/120    avg_loss:0.013, val_acc:0.996]
Epoch [119/120    avg_loss:0.008, val_acc:0.996]
Epoch [120/120    avg_loss:0.011, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 1.         1.         1.         0.97571744 0.96219931
 1.         1.         1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9971514234076062
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48672ab780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.376, val_acc:0.523]
Epoch [2/120    avg_loss:1.944, val_acc:0.604]
Epoch [3/120    avg_loss:1.613, val_acc:0.623]
Epoch [4/120    avg_loss:1.354, val_acc:0.717]
Epoch [5/120    avg_loss:1.154, val_acc:0.721]
Epoch [6/120    avg_loss:0.976, val_acc:0.779]
Epoch [7/120    avg_loss:0.938, val_acc:0.775]
Epoch [8/120    avg_loss:0.791, val_acc:0.817]
Epoch [9/120    avg_loss:0.683, val_acc:0.846]
Epoch [10/120    avg_loss:0.576, val_acc:0.881]
Epoch [11/120    avg_loss:0.515, val_acc:0.900]
Epoch [12/120    avg_loss:0.482, val_acc:0.881]
Epoch [13/120    avg_loss:0.488, val_acc:0.856]
Epoch [14/120    avg_loss:0.466, val_acc:0.927]
Epoch [15/120    avg_loss:0.420, val_acc:0.912]
Epoch [16/120    avg_loss:0.357, val_acc:0.908]
Epoch [17/120    avg_loss:0.372, val_acc:0.898]
Epoch [18/120    avg_loss:0.313, val_acc:0.938]
Epoch [19/120    avg_loss:0.331, val_acc:0.921]
Epoch [20/120    avg_loss:0.356, val_acc:0.896]
Epoch [21/120    avg_loss:0.398, val_acc:0.927]
Epoch [22/120    avg_loss:0.270, val_acc:0.940]
Epoch [23/120    avg_loss:0.259, val_acc:0.956]
Epoch [24/120    avg_loss:0.298, val_acc:0.944]
Epoch [25/120    avg_loss:0.274, val_acc:0.952]
Epoch [26/120    avg_loss:0.229, val_acc:0.933]
Epoch [27/120    avg_loss:0.233, val_acc:0.931]
Epoch [28/120    avg_loss:0.207, val_acc:0.946]
Epoch [29/120    avg_loss:0.235, val_acc:0.946]
Epoch [30/120    avg_loss:0.229, val_acc:0.960]
Epoch [31/120    avg_loss:0.134, val_acc:0.967]
Epoch [32/120    avg_loss:0.165, val_acc:0.956]
Epoch [33/120    avg_loss:0.127, val_acc:0.967]
Epoch [34/120    avg_loss:0.136, val_acc:0.967]
Epoch [35/120    avg_loss:0.140, val_acc:0.948]
Epoch [36/120    avg_loss:0.188, val_acc:0.958]
Epoch [37/120    avg_loss:0.134, val_acc:0.946]
Epoch [38/120    avg_loss:0.130, val_acc:0.975]
Epoch [39/120    avg_loss:0.118, val_acc:0.979]
Epoch [40/120    avg_loss:0.153, val_acc:0.975]
Epoch [41/120    avg_loss:0.177, val_acc:0.965]
Epoch [42/120    avg_loss:0.114, val_acc:0.967]
Epoch [43/120    avg_loss:0.138, val_acc:0.965]
Epoch [44/120    avg_loss:0.099, val_acc:0.977]
Epoch [45/120    avg_loss:0.133, val_acc:0.915]
Epoch [46/120    avg_loss:0.170, val_acc:0.967]
Epoch [47/120    avg_loss:0.109, val_acc:0.975]
Epoch [48/120    avg_loss:0.113, val_acc:0.969]
Epoch [49/120    avg_loss:0.084, val_acc:0.983]
Epoch [50/120    avg_loss:0.082, val_acc:0.969]
Epoch [51/120    avg_loss:0.082, val_acc:0.981]
Epoch [52/120    avg_loss:0.056, val_acc:0.985]
Epoch [53/120    avg_loss:0.048, val_acc:0.983]
Epoch [54/120    avg_loss:0.053, val_acc:0.979]
Epoch [55/120    avg_loss:0.139, val_acc:0.960]
Epoch [56/120    avg_loss:0.184, val_acc:0.954]
Epoch [57/120    avg_loss:0.119, val_acc:0.954]
Epoch [58/120    avg_loss:0.099, val_acc:0.988]
Epoch [59/120    avg_loss:0.051, val_acc:0.965]
Epoch [60/120    avg_loss:0.084, val_acc:0.983]
Epoch [61/120    avg_loss:0.070, val_acc:0.992]
Epoch [62/120    avg_loss:0.063, val_acc:0.981]
Epoch [63/120    avg_loss:0.050, val_acc:0.988]
Epoch [64/120    avg_loss:0.042, val_acc:0.990]
Epoch [65/120    avg_loss:0.048, val_acc:0.988]
Epoch [66/120    avg_loss:0.058, val_acc:0.990]
Epoch [67/120    avg_loss:0.047, val_acc:0.990]
Epoch [68/120    avg_loss:0.031, val_acc:0.992]
Epoch [69/120    avg_loss:0.023, val_acc:0.985]
Epoch [70/120    avg_loss:0.026, val_acc:0.992]
Epoch [71/120    avg_loss:0.021, val_acc:0.988]
Epoch [72/120    avg_loss:0.071, val_acc:0.969]
Epoch [73/120    avg_loss:0.086, val_acc:0.979]
Epoch [74/120    avg_loss:0.096, val_acc:0.969]
Epoch [75/120    avg_loss:0.043, val_acc:0.985]
Epoch [76/120    avg_loss:0.046, val_acc:0.988]
Epoch [77/120    avg_loss:0.043, val_acc:0.979]
Epoch [78/120    avg_loss:0.037, val_acc:0.985]
Epoch [79/120    avg_loss:0.035, val_acc:0.981]
Epoch [80/120    avg_loss:0.048, val_acc:0.981]
Epoch [81/120    avg_loss:0.036, val_acc:0.988]
Epoch [82/120    avg_loss:0.030, val_acc:0.990]
Epoch [83/120    avg_loss:0.046, val_acc:0.992]
Epoch [84/120    avg_loss:0.020, val_acc:0.992]
Epoch [85/120    avg_loss:0.020, val_acc:0.992]
Epoch [86/120    avg_loss:0.014, val_acc:0.990]
Epoch [87/120    avg_loss:0.019, val_acc:0.992]
Epoch [88/120    avg_loss:0.047, val_acc:0.988]
Epoch [89/120    avg_loss:0.037, val_acc:0.990]
Epoch [90/120    avg_loss:0.026, val_acc:0.994]
Epoch [91/120    avg_loss:0.016, val_acc:0.996]
Epoch [92/120    avg_loss:0.012, val_acc:0.992]
Epoch [93/120    avg_loss:0.013, val_acc:0.996]
Epoch [94/120    avg_loss:0.013, val_acc:0.996]
Epoch [95/120    avg_loss:0.010, val_acc:0.990]
Epoch [96/120    avg_loss:0.016, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.996]
Epoch [100/120    avg_loss:0.014, val_acc:0.996]
Epoch [101/120    avg_loss:0.014, val_acc:0.992]
Epoch [102/120    avg_loss:0.011, val_acc:0.996]
Epoch [103/120    avg_loss:0.009, val_acc:0.996]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.018, val_acc:0.996]
Epoch [106/120    avg_loss:0.020, val_acc:0.985]
Epoch [107/120    avg_loss:0.031, val_acc:0.979]
Epoch [108/120    avg_loss:0.055, val_acc:0.988]
Epoch [109/120    avg_loss:0.056, val_acc:0.990]
Epoch [110/120    avg_loss:0.062, val_acc:0.988]
Epoch [111/120    avg_loss:0.031, val_acc:0.979]
Epoch [112/120    avg_loss:0.051, val_acc:0.992]
Epoch [113/120    avg_loss:0.041, val_acc:0.990]
Epoch [114/120    avg_loss:0.029, val_acc:0.988]
Epoch [115/120    avg_loss:0.034, val_acc:0.992]
Epoch [116/120    avg_loss:0.025, val_acc:0.988]
Epoch [117/120    avg_loss:0.039, val_acc:0.985]
Epoch [118/120    avg_loss:0.038, val_acc:0.994]
Epoch [119/120    avg_loss:0.024, val_acc:0.994]
Epoch [120/120    avg_loss:0.019, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  18   0   0   0   0   0   0   2   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.99545455 1.         0.94520548 0.92763158
 1.         0.98924731 1.         1.         1.         0.99341238
 0.99224806 1.        ]

Kappa:
0.9926414825426144
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f87933ac860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.397, val_acc:0.567]
Epoch [2/120    avg_loss:1.863, val_acc:0.617]
Epoch [3/120    avg_loss:1.488, val_acc:0.794]
Epoch [4/120    avg_loss:1.205, val_acc:0.744]
Epoch [5/120    avg_loss:0.969, val_acc:0.800]
Epoch [6/120    avg_loss:0.799, val_acc:0.852]
Epoch [7/120    avg_loss:0.745, val_acc:0.854]
Epoch [8/120    avg_loss:0.645, val_acc:0.842]
Epoch [9/120    avg_loss:0.575, val_acc:0.904]
Epoch [10/120    avg_loss:0.602, val_acc:0.879]
Epoch [11/120    avg_loss:0.543, val_acc:0.885]
Epoch [12/120    avg_loss:0.554, val_acc:0.842]
Epoch [13/120    avg_loss:0.489, val_acc:0.912]
Epoch [14/120    avg_loss:0.404, val_acc:0.940]
Epoch [15/120    avg_loss:0.371, val_acc:0.935]
Epoch [16/120    avg_loss:0.391, val_acc:0.921]
Epoch [17/120    avg_loss:0.402, val_acc:0.923]
Epoch [18/120    avg_loss:0.363, val_acc:0.931]
Epoch [19/120    avg_loss:0.323, val_acc:0.946]
Epoch [20/120    avg_loss:0.256, val_acc:0.894]
Epoch [21/120    avg_loss:0.313, val_acc:0.929]
Epoch [22/120    avg_loss:0.297, val_acc:0.923]
Epoch [23/120    avg_loss:0.237, val_acc:0.935]
Epoch [24/120    avg_loss:0.239, val_acc:0.956]
Epoch [25/120    avg_loss:0.176, val_acc:0.954]
Epoch [26/120    avg_loss:0.278, val_acc:0.933]
Epoch [27/120    avg_loss:0.205, val_acc:0.963]
Epoch [28/120    avg_loss:0.187, val_acc:0.948]
Epoch [29/120    avg_loss:0.239, val_acc:0.963]
Epoch [30/120    avg_loss:0.165, val_acc:0.965]
Epoch [31/120    avg_loss:0.207, val_acc:0.942]
Epoch [32/120    avg_loss:0.244, val_acc:0.967]
Epoch [33/120    avg_loss:0.190, val_acc:0.948]
Epoch [34/120    avg_loss:0.204, val_acc:0.954]
Epoch [35/120    avg_loss:0.160, val_acc:0.971]
Epoch [36/120    avg_loss:0.103, val_acc:0.971]
Epoch [37/120    avg_loss:0.162, val_acc:0.940]
Epoch [38/120    avg_loss:0.177, val_acc:0.971]
Epoch [39/120    avg_loss:0.138, val_acc:0.969]
Epoch [40/120    avg_loss:0.138, val_acc:0.960]
Epoch [41/120    avg_loss:0.175, val_acc:0.969]
Epoch [42/120    avg_loss:0.120, val_acc:0.971]
Epoch [43/120    avg_loss:0.112, val_acc:0.985]
Epoch [44/120    avg_loss:0.104, val_acc:0.958]
Epoch [45/120    avg_loss:0.158, val_acc:0.975]
Epoch [46/120    avg_loss:0.098, val_acc:0.985]
Epoch [47/120    avg_loss:0.067, val_acc:0.979]
Epoch [48/120    avg_loss:0.085, val_acc:0.973]
Epoch [49/120    avg_loss:0.097, val_acc:0.975]
Epoch [50/120    avg_loss:0.089, val_acc:0.985]
Epoch [51/120    avg_loss:0.083, val_acc:0.967]
Epoch [52/120    avg_loss:0.070, val_acc:0.979]
Epoch [53/120    avg_loss:0.052, val_acc:0.985]
Epoch [54/120    avg_loss:0.050, val_acc:0.981]
Epoch [55/120    avg_loss:0.047, val_acc:0.977]
Epoch [56/120    avg_loss:0.042, val_acc:0.983]
Epoch [57/120    avg_loss:0.034, val_acc:0.985]
Epoch [58/120    avg_loss:0.060, val_acc:0.960]
Epoch [59/120    avg_loss:0.096, val_acc:0.979]
Epoch [60/120    avg_loss:0.125, val_acc:0.971]
Epoch [61/120    avg_loss:0.076, val_acc:0.973]
Epoch [62/120    avg_loss:0.063, val_acc:0.973]
Epoch [63/120    avg_loss:0.075, val_acc:0.981]
Epoch [64/120    avg_loss:0.081, val_acc:0.981]
Epoch [65/120    avg_loss:0.035, val_acc:0.983]
Epoch [66/120    avg_loss:0.068, val_acc:0.971]
Epoch [67/120    avg_loss:0.065, val_acc:0.981]
Epoch [68/120    avg_loss:0.099, val_acc:0.979]
Epoch [69/120    avg_loss:0.045, val_acc:0.981]
Epoch [70/120    avg_loss:0.046, val_acc:0.988]
Epoch [71/120    avg_loss:0.057, val_acc:0.990]
Epoch [72/120    avg_loss:0.060, val_acc:0.981]
Epoch [73/120    avg_loss:0.041, val_acc:0.983]
Epoch [74/120    avg_loss:0.029, val_acc:0.977]
Epoch [75/120    avg_loss:0.029, val_acc:0.985]
Epoch [76/120    avg_loss:0.022, val_acc:0.985]
Epoch [77/120    avg_loss:0.030, val_acc:0.983]
Epoch [78/120    avg_loss:0.034, val_acc:0.967]
Epoch [79/120    avg_loss:0.048, val_acc:0.973]
Epoch [80/120    avg_loss:0.043, val_acc:0.979]
Epoch [81/120    avg_loss:0.024, val_acc:0.988]
Epoch [82/120    avg_loss:0.019, val_acc:0.983]
Epoch [83/120    avg_loss:0.022, val_acc:0.983]
Epoch [84/120    avg_loss:0.019, val_acc:0.988]
Epoch [85/120    avg_loss:0.017, val_acc:0.988]
Epoch [86/120    avg_loss:0.016, val_acc:0.994]
Epoch [87/120    avg_loss:0.016, val_acc:0.992]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.016, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.010, val_acc:0.990]
Epoch [94/120    avg_loss:0.009, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.990]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.020, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.017, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 1.         0.99545455 1.         0.9753915  0.96621622
 1.         0.98924731 1.         1.         1.         0.99077734
 0.99113082 1.        ]

Kappa:
0.9952524660604467
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb7e90b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.345, val_acc:0.427]
Epoch [2/120    avg_loss:1.946, val_acc:0.581]
Epoch [3/120    avg_loss:1.621, val_acc:0.671]
Epoch [4/120    avg_loss:1.325, val_acc:0.720]
Epoch [5/120    avg_loss:1.156, val_acc:0.696]
Epoch [6/120    avg_loss:0.997, val_acc:0.788]
Epoch [7/120    avg_loss:0.849, val_acc:0.754]
Epoch [8/120    avg_loss:0.764, val_acc:0.821]
Epoch [9/120    avg_loss:0.717, val_acc:0.817]
Epoch [10/120    avg_loss:0.694, val_acc:0.776]
Epoch [11/120    avg_loss:0.563, val_acc:0.875]
Epoch [12/120    avg_loss:0.493, val_acc:0.841]
Epoch [13/120    avg_loss:0.508, val_acc:0.780]
Epoch [14/120    avg_loss:0.465, val_acc:0.907]
Epoch [15/120    avg_loss:0.469, val_acc:0.929]
Epoch [16/120    avg_loss:0.379, val_acc:0.927]
Epoch [17/120    avg_loss:0.432, val_acc:0.881]
Epoch [18/120    avg_loss:0.389, val_acc:0.861]
Epoch [19/120    avg_loss:0.341, val_acc:0.925]
Epoch [20/120    avg_loss:0.355, val_acc:0.927]
Epoch [21/120    avg_loss:0.307, val_acc:0.933]
Epoch [22/120    avg_loss:0.284, val_acc:0.929]
Epoch [23/120    avg_loss:0.265, val_acc:0.940]
Epoch [24/120    avg_loss:0.263, val_acc:0.940]
Epoch [25/120    avg_loss:0.229, val_acc:0.982]
Epoch [26/120    avg_loss:0.205, val_acc:0.958]
Epoch [27/120    avg_loss:0.177, val_acc:0.958]
Epoch [28/120    avg_loss:0.172, val_acc:0.964]
Epoch [29/120    avg_loss:0.191, val_acc:0.946]
Epoch [30/120    avg_loss:0.157, val_acc:0.976]
Epoch [31/120    avg_loss:0.158, val_acc:0.968]
Epoch [32/120    avg_loss:0.156, val_acc:0.966]
Epoch [33/120    avg_loss:0.246, val_acc:0.968]
Epoch [34/120    avg_loss:0.271, val_acc:0.919]
Epoch [35/120    avg_loss:0.193, val_acc:0.970]
Epoch [36/120    avg_loss:0.143, val_acc:0.966]
Epoch [37/120    avg_loss:0.117, val_acc:0.968]
Epoch [38/120    avg_loss:0.153, val_acc:0.976]
Epoch [39/120    avg_loss:0.103, val_acc:0.980]
Epoch [40/120    avg_loss:0.102, val_acc:0.982]
Epoch [41/120    avg_loss:0.072, val_acc:0.984]
Epoch [42/120    avg_loss:0.074, val_acc:0.982]
Epoch [43/120    avg_loss:0.083, val_acc:0.986]
Epoch [44/120    avg_loss:0.061, val_acc:0.982]
Epoch [45/120    avg_loss:0.075, val_acc:0.986]
Epoch [46/120    avg_loss:0.079, val_acc:0.984]
Epoch [47/120    avg_loss:0.072, val_acc:0.986]
Epoch [48/120    avg_loss:0.061, val_acc:0.988]
Epoch [49/120    avg_loss:0.055, val_acc:0.984]
Epoch [50/120    avg_loss:0.057, val_acc:0.986]
Epoch [51/120    avg_loss:0.054, val_acc:0.988]
Epoch [52/120    avg_loss:0.063, val_acc:0.986]
Epoch [53/120    avg_loss:0.064, val_acc:0.984]
Epoch [54/120    avg_loss:0.063, val_acc:0.984]
Epoch [55/120    avg_loss:0.069, val_acc:0.984]
Epoch [56/120    avg_loss:0.063, val_acc:0.988]
Epoch [57/120    avg_loss:0.059, val_acc:0.984]
Epoch [58/120    avg_loss:0.059, val_acc:0.986]
Epoch [59/120    avg_loss:0.049, val_acc:0.986]
Epoch [60/120    avg_loss:0.063, val_acc:0.990]
Epoch [61/120    avg_loss:0.052, val_acc:0.986]
Epoch [62/120    avg_loss:0.053, val_acc:0.986]
Epoch [63/120    avg_loss:0.058, val_acc:0.984]
Epoch [64/120    avg_loss:0.060, val_acc:0.986]
Epoch [65/120    avg_loss:0.060, val_acc:0.988]
Epoch [66/120    avg_loss:0.048, val_acc:0.984]
Epoch [67/120    avg_loss:0.042, val_acc:0.990]
Epoch [68/120    avg_loss:0.047, val_acc:0.986]
Epoch [69/120    avg_loss:0.061, val_acc:0.986]
Epoch [70/120    avg_loss:0.047, val_acc:0.990]
Epoch [71/120    avg_loss:0.050, val_acc:0.988]
Epoch [72/120    avg_loss:0.052, val_acc:0.988]
Epoch [73/120    avg_loss:0.050, val_acc:0.984]
Epoch [74/120    avg_loss:0.044, val_acc:0.988]
Epoch [75/120    avg_loss:0.045, val_acc:0.988]
Epoch [76/120    avg_loss:0.040, val_acc:0.986]
Epoch [77/120    avg_loss:0.052, val_acc:0.988]
Epoch [78/120    avg_loss:0.047, val_acc:0.986]
Epoch [79/120    avg_loss:0.042, val_acc:0.986]
Epoch [80/120    avg_loss:0.043, val_acc:0.990]
Epoch [81/120    avg_loss:0.041, val_acc:0.990]
Epoch [82/120    avg_loss:0.042, val_acc:0.988]
Epoch [83/120    avg_loss:0.043, val_acc:0.988]
Epoch [84/120    avg_loss:0.057, val_acc:0.990]
Epoch [85/120    avg_loss:0.049, val_acc:0.992]
Epoch [86/120    avg_loss:0.039, val_acc:0.990]
Epoch [87/120    avg_loss:0.041, val_acc:0.992]
Epoch [88/120    avg_loss:0.044, val_acc:0.992]
Epoch [89/120    avg_loss:0.045, val_acc:0.990]
Epoch [90/120    avg_loss:0.040, val_acc:0.990]
Epoch [91/120    avg_loss:0.041, val_acc:0.992]
Epoch [92/120    avg_loss:0.033, val_acc:0.988]
Epoch [93/120    avg_loss:0.038, val_acc:0.992]
Epoch [94/120    avg_loss:0.038, val_acc:0.992]
Epoch [95/120    avg_loss:0.042, val_acc:0.992]
Epoch [96/120    avg_loss:0.037, val_acc:0.992]
Epoch [97/120    avg_loss:0.041, val_acc:0.992]
Epoch [98/120    avg_loss:0.040, val_acc:0.990]
Epoch [99/120    avg_loss:0.037, val_acc:0.990]
Epoch [100/120    avg_loss:0.041, val_acc:0.992]
Epoch [101/120    avg_loss:0.045, val_acc:0.992]
Epoch [102/120    avg_loss:0.033, val_acc:0.992]
Epoch [103/120    avg_loss:0.038, val_acc:0.994]
Epoch [104/120    avg_loss:0.039, val_acc:0.992]
Epoch [105/120    avg_loss:0.033, val_acc:0.990]
Epoch [106/120    avg_loss:0.035, val_acc:0.990]
Epoch [107/120    avg_loss:0.032, val_acc:0.990]
Epoch [108/120    avg_loss:0.034, val_acc:0.990]
Epoch [109/120    avg_loss:0.041, val_acc:0.990]
Epoch [110/120    avg_loss:0.041, val_acc:0.994]
Epoch [111/120    avg_loss:0.034, val_acc:0.992]
Epoch [112/120    avg_loss:0.036, val_acc:0.992]
Epoch [113/120    avg_loss:0.036, val_acc:0.992]
Epoch [114/120    avg_loss:0.036, val_acc:0.992]
Epoch [115/120    avg_loss:0.037, val_acc:0.994]
Epoch [116/120    avg_loss:0.036, val_acc:0.992]
Epoch [117/120    avg_loss:0.040, val_acc:0.992]
Epoch [118/120    avg_loss:0.044, val_acc:0.992]
Epoch [119/120    avg_loss:0.037, val_acc:0.992]
Epoch [120/120    avg_loss:0.039, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99780541 0.98648649 1.         0.95045045 0.93288591
 0.99277108 0.96703297 1.         1.         1.         0.99867198
 0.99669967 1.        ]

Kappa:
0.992404106782832
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5dab3fd7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.385, val_acc:0.481]
Epoch [2/120    avg_loss:1.895, val_acc:0.577]
Epoch [3/120    avg_loss:1.545, val_acc:0.713]
Epoch [4/120    avg_loss:1.272, val_acc:0.735]
Epoch [5/120    avg_loss:1.062, val_acc:0.815]
Epoch [6/120    avg_loss:0.918, val_acc:0.869]
Epoch [7/120    avg_loss:0.849, val_acc:0.842]
Epoch [8/120    avg_loss:0.775, val_acc:0.892]
Epoch [9/120    avg_loss:0.631, val_acc:0.869]
Epoch [10/120    avg_loss:0.610, val_acc:0.894]
Epoch [11/120    avg_loss:0.508, val_acc:0.931]
Epoch [12/120    avg_loss:0.476, val_acc:0.919]
Epoch [13/120    avg_loss:0.484, val_acc:0.931]
Epoch [14/120    avg_loss:0.367, val_acc:0.925]
Epoch [15/120    avg_loss:0.445, val_acc:0.919]
Epoch [16/120    avg_loss:0.372, val_acc:0.946]
Epoch [17/120    avg_loss:0.362, val_acc:0.940]
Epoch [18/120    avg_loss:0.329, val_acc:0.942]
Epoch [19/120    avg_loss:0.326, val_acc:0.931]
Epoch [20/120    avg_loss:0.358, val_acc:0.946]
Epoch [21/120    avg_loss:0.317, val_acc:0.958]
Epoch [22/120    avg_loss:0.299, val_acc:0.952]
Epoch [23/120    avg_loss:0.246, val_acc:0.965]
Epoch [24/120    avg_loss:0.279, val_acc:0.958]
Epoch [25/120    avg_loss:0.231, val_acc:0.956]
Epoch [26/120    avg_loss:0.219, val_acc:0.967]
Epoch [27/120    avg_loss:0.250, val_acc:0.952]
Epoch [28/120    avg_loss:0.247, val_acc:0.967]
Epoch [29/120    avg_loss:0.257, val_acc:0.952]
Epoch [30/120    avg_loss:0.212, val_acc:0.967]
Epoch [31/120    avg_loss:0.185, val_acc:0.963]
Epoch [32/120    avg_loss:0.155, val_acc:0.963]
Epoch [33/120    avg_loss:0.139, val_acc:0.977]
Epoch [34/120    avg_loss:0.131, val_acc:0.977]
Epoch [35/120    avg_loss:0.151, val_acc:0.971]
Epoch [36/120    avg_loss:0.176, val_acc:0.975]
Epoch [37/120    avg_loss:0.141, val_acc:0.969]
Epoch [38/120    avg_loss:0.144, val_acc:0.940]
Epoch [39/120    avg_loss:0.139, val_acc:0.975]
Epoch [40/120    avg_loss:0.099, val_acc:0.977]
Epoch [41/120    avg_loss:0.126, val_acc:0.965]
Epoch [42/120    avg_loss:0.128, val_acc:0.960]
Epoch [43/120    avg_loss:0.149, val_acc:0.967]
Epoch [44/120    avg_loss:0.133, val_acc:0.981]
Epoch [45/120    avg_loss:0.118, val_acc:0.981]
Epoch [46/120    avg_loss:0.077, val_acc:0.981]
Epoch [47/120    avg_loss:0.079, val_acc:0.971]
Epoch [48/120    avg_loss:0.075, val_acc:0.969]
Epoch [49/120    avg_loss:0.069, val_acc:0.977]
Epoch [50/120    avg_loss:0.076, val_acc:0.985]
Epoch [51/120    avg_loss:0.086, val_acc:0.952]
Epoch [52/120    avg_loss:0.123, val_acc:0.985]
Epoch [53/120    avg_loss:0.106, val_acc:0.985]
Epoch [54/120    avg_loss:0.080, val_acc:0.988]
Epoch [55/120    avg_loss:0.068, val_acc:0.985]
Epoch [56/120    avg_loss:0.068, val_acc:0.983]
Epoch [57/120    avg_loss:0.053, val_acc:0.988]
Epoch [58/120    avg_loss:0.075, val_acc:0.983]
Epoch [59/120    avg_loss:0.075, val_acc:0.985]
Epoch [60/120    avg_loss:0.075, val_acc:0.973]
Epoch [61/120    avg_loss:0.052, val_acc:0.983]
Epoch [62/120    avg_loss:0.046, val_acc:0.990]
Epoch [63/120    avg_loss:0.047, val_acc:0.981]
Epoch [64/120    avg_loss:0.054, val_acc:0.981]
Epoch [65/120    avg_loss:0.042, val_acc:0.983]
Epoch [66/120    avg_loss:0.052, val_acc:0.983]
Epoch [67/120    avg_loss:0.054, val_acc:0.983]
Epoch [68/120    avg_loss:0.038, val_acc:0.983]
Epoch [69/120    avg_loss:0.044, val_acc:0.985]
Epoch [70/120    avg_loss:0.065, val_acc:0.983]
Epoch [71/120    avg_loss:0.049, val_acc:0.988]
Epoch [72/120    avg_loss:0.030, val_acc:0.981]
Epoch [73/120    avg_loss:0.027, val_acc:0.990]
Epoch [74/120    avg_loss:0.016, val_acc:0.985]
Epoch [75/120    avg_loss:0.020, val_acc:0.992]
Epoch [76/120    avg_loss:0.027, val_acc:0.992]
Epoch [77/120    avg_loss:0.027, val_acc:0.992]
Epoch [78/120    avg_loss:0.017, val_acc:0.990]
Epoch [79/120    avg_loss:0.019, val_acc:0.988]
Epoch [80/120    avg_loss:0.016, val_acc:0.988]
Epoch [81/120    avg_loss:0.018, val_acc:0.985]
Epoch [82/120    avg_loss:0.057, val_acc:0.985]
Epoch [83/120    avg_loss:0.039, val_acc:0.985]
Epoch [84/120    avg_loss:0.020, val_acc:0.981]
Epoch [85/120    avg_loss:0.024, val_acc:0.988]
Epoch [86/120    avg_loss:0.023, val_acc:0.983]
Epoch [87/120    avg_loss:0.038, val_acc:0.990]
Epoch [88/120    avg_loss:0.054, val_acc:0.988]
Epoch [89/120    avg_loss:0.030, val_acc:0.983]
Epoch [90/120    avg_loss:0.044, val_acc:0.988]
Epoch [91/120    avg_loss:0.038, val_acc:0.988]
Epoch [92/120    avg_loss:0.015, val_acc:0.985]
Epoch [93/120    avg_loss:0.028, val_acc:0.985]
Epoch [94/120    avg_loss:0.020, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.014, val_acc:0.988]
Epoch [98/120    avg_loss:0.017, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.013, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.013, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.011, val_acc:0.985]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.014, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.014, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  13   0   0   0   0   0   0   2   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         0.99319728 1.         0.95067265 0.93243243
 1.         0.98378378 1.         1.         1.         0.9973545
 0.99558499 1.        ]

Kappa:
0.9935906413544494
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f312a20e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.408, val_acc:0.350]
Epoch [2/120    avg_loss:1.996, val_acc:0.565]
Epoch [3/120    avg_loss:1.655, val_acc:0.596]
Epoch [4/120    avg_loss:1.440, val_acc:0.660]
Epoch [5/120    avg_loss:1.245, val_acc:0.723]
Epoch [6/120    avg_loss:1.055, val_acc:0.729]
Epoch [7/120    avg_loss:0.866, val_acc:0.748]
Epoch [8/120    avg_loss:0.791, val_acc:0.798]
Epoch [9/120    avg_loss:0.660, val_acc:0.840]
Epoch [10/120    avg_loss:0.605, val_acc:0.800]
Epoch [11/120    avg_loss:0.627, val_acc:0.827]
Epoch [12/120    avg_loss:0.526, val_acc:0.835]
Epoch [13/120    avg_loss:0.547, val_acc:0.883]
Epoch [14/120    avg_loss:0.453, val_acc:0.898]
Epoch [15/120    avg_loss:0.394, val_acc:0.917]
Epoch [16/120    avg_loss:0.383, val_acc:0.929]
Epoch [17/120    avg_loss:0.404, val_acc:0.860]
Epoch [18/120    avg_loss:0.432, val_acc:0.944]
Epoch [19/120    avg_loss:0.378, val_acc:0.935]
Epoch [20/120    avg_loss:0.371, val_acc:0.948]
Epoch [21/120    avg_loss:0.338, val_acc:0.931]
Epoch [22/120    avg_loss:0.330, val_acc:0.935]
Epoch [23/120    avg_loss:0.297, val_acc:0.944]
Epoch [24/120    avg_loss:0.271, val_acc:0.946]
Epoch [25/120    avg_loss:0.241, val_acc:0.917]
Epoch [26/120    avg_loss:0.314, val_acc:0.917]
Epoch [27/120    avg_loss:0.250, val_acc:0.952]
Epoch [28/120    avg_loss:0.228, val_acc:0.935]
Epoch [29/120    avg_loss:0.221, val_acc:0.938]
Epoch [30/120    avg_loss:0.215, val_acc:0.967]
Epoch [31/120    avg_loss:0.212, val_acc:0.942]
Epoch [32/120    avg_loss:0.182, val_acc:0.933]
Epoch [33/120    avg_loss:0.213, val_acc:0.944]
Epoch [34/120    avg_loss:0.160, val_acc:0.950]
Epoch [35/120    avg_loss:0.171, val_acc:0.948]
Epoch [36/120    avg_loss:0.138, val_acc:0.963]
Epoch [37/120    avg_loss:0.168, val_acc:0.923]
Epoch [38/120    avg_loss:0.155, val_acc:0.977]
Epoch [39/120    avg_loss:0.132, val_acc:0.960]
Epoch [40/120    avg_loss:0.140, val_acc:0.967]
Epoch [41/120    avg_loss:0.163, val_acc:0.965]
Epoch [42/120    avg_loss:0.176, val_acc:0.942]
Epoch [43/120    avg_loss:0.142, val_acc:0.952]
Epoch [44/120    avg_loss:0.093, val_acc:0.977]
Epoch [45/120    avg_loss:0.105, val_acc:0.983]
Epoch [46/120    avg_loss:0.072, val_acc:0.975]
Epoch [47/120    avg_loss:0.081, val_acc:0.977]
Epoch [48/120    avg_loss:0.179, val_acc:0.963]
Epoch [49/120    avg_loss:0.108, val_acc:0.971]
Epoch [50/120    avg_loss:0.099, val_acc:0.985]
Epoch [51/120    avg_loss:0.091, val_acc:0.979]
Epoch [52/120    avg_loss:0.101, val_acc:0.969]
Epoch [53/120    avg_loss:0.099, val_acc:0.975]
Epoch [54/120    avg_loss:0.090, val_acc:0.979]
Epoch [55/120    avg_loss:0.062, val_acc:0.985]
Epoch [56/120    avg_loss:0.057, val_acc:0.971]
Epoch [57/120    avg_loss:0.066, val_acc:0.994]
Epoch [58/120    avg_loss:0.044, val_acc:0.990]
Epoch [59/120    avg_loss:0.043, val_acc:0.988]
Epoch [60/120    avg_loss:0.066, val_acc:0.983]
Epoch [61/120    avg_loss:0.034, val_acc:0.990]
Epoch [62/120    avg_loss:0.062, val_acc:0.952]
Epoch [63/120    avg_loss:0.059, val_acc:0.992]
Epoch [64/120    avg_loss:0.044, val_acc:0.990]
Epoch [65/120    avg_loss:0.041, val_acc:0.990]
Epoch [66/120    avg_loss:0.036, val_acc:0.992]
Epoch [67/120    avg_loss:0.069, val_acc:0.975]
Epoch [68/120    avg_loss:0.067, val_acc:0.975]
Epoch [69/120    avg_loss:0.087, val_acc:0.983]
Epoch [70/120    avg_loss:0.123, val_acc:0.994]
Epoch [71/120    avg_loss:0.099, val_acc:0.973]
Epoch [72/120    avg_loss:0.061, val_acc:0.988]
Epoch [73/120    avg_loss:0.045, val_acc:0.990]
Epoch [74/120    avg_loss:0.019, val_acc:0.983]
Epoch [75/120    avg_loss:0.047, val_acc:0.992]
Epoch [76/120    avg_loss:0.026, val_acc:0.996]
Epoch [77/120    avg_loss:0.029, val_acc:0.996]
Epoch [78/120    avg_loss:0.023, val_acc:0.992]
Epoch [79/120    avg_loss:0.030, val_acc:0.985]
Epoch [80/120    avg_loss:0.027, val_acc:0.996]
Epoch [81/120    avg_loss:0.017, val_acc:0.994]
Epoch [82/120    avg_loss:0.025, val_acc:0.983]
Epoch [83/120    avg_loss:0.034, val_acc:0.990]
Epoch [84/120    avg_loss:0.028, val_acc:0.992]
Epoch [85/120    avg_loss:0.032, val_acc:0.996]
Epoch [86/120    avg_loss:0.017, val_acc:0.996]
Epoch [87/120    avg_loss:0.017, val_acc:0.996]
Epoch [88/120    avg_loss:0.032, val_acc:0.992]
Epoch [89/120    avg_loss:0.016, val_acc:0.996]
Epoch [90/120    avg_loss:0.023, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.994]
Epoch [92/120    avg_loss:0.014, val_acc:0.996]
Epoch [93/120    avg_loss:0.008, val_acc:0.996]
Epoch [94/120    avg_loss:0.022, val_acc:0.996]
Epoch [95/120    avg_loss:0.055, val_acc:0.992]
Epoch [96/120    avg_loss:0.022, val_acc:0.994]
Epoch [97/120    avg_loss:0.016, val_acc:0.994]
Epoch [98/120    avg_loss:0.032, val_acc:0.992]
Epoch [99/120    avg_loss:0.053, val_acc:0.985]
Epoch [100/120    avg_loss:0.114, val_acc:0.973]
Epoch [101/120    avg_loss:0.100, val_acc:0.981]
Epoch [102/120    avg_loss:0.053, val_acc:0.990]
Epoch [103/120    avg_loss:0.032, val_acc:0.990]
Epoch [104/120    avg_loss:0.034, val_acc:0.994]
Epoch [105/120    avg_loss:0.023, val_acc:0.994]
Epoch [106/120    avg_loss:0.013, val_acc:0.994]
Epoch [107/120    avg_loss:0.016, val_acc:0.996]
Epoch [108/120    avg_loss:0.015, val_acc:0.996]
Epoch [109/120    avg_loss:0.012, val_acc:0.996]
Epoch [110/120    avg_loss:0.017, val_acc:0.998]
Epoch [111/120    avg_loss:0.015, val_acc:0.996]
Epoch [112/120    avg_loss:0.013, val_acc:0.996]
Epoch [113/120    avg_loss:0.010, val_acc:0.996]
Epoch [114/120    avg_loss:0.007, val_acc:0.996]
Epoch [115/120    avg_loss:0.012, val_acc:0.996]
Epoch [116/120    avg_loss:0.029, val_acc:0.992]
Epoch [117/120    avg_loss:0.047, val_acc:0.990]
Epoch [118/120    avg_loss:0.042, val_acc:0.994]
Epoch [119/120    avg_loss:0.027, val_acc:0.998]
Epoch [120/120    avg_loss:0.025, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215   1   9   0   0   0   0   0   2   0]
 [  0   0   0   0   2 130  13   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   1   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.98206278 0.99563319 0.96629213 0.94202899
 0.94930876 0.97826087 0.99870968 1.         1.         0.99867198
 0.99448732 1.        ]

Kappa:
0.991216413588729
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5f35e4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.380, val_acc:0.508]
Epoch [2/120    avg_loss:1.871, val_acc:0.600]
Epoch [3/120    avg_loss:1.514, val_acc:0.675]
Epoch [4/120    avg_loss:1.253, val_acc:0.713]
Epoch [5/120    avg_loss:1.024, val_acc:0.719]
Epoch [6/120    avg_loss:0.878, val_acc:0.731]
Epoch [7/120    avg_loss:0.793, val_acc:0.783]
Epoch [8/120    avg_loss:0.741, val_acc:0.838]
Epoch [9/120    avg_loss:0.640, val_acc:0.883]
Epoch [10/120    avg_loss:0.643, val_acc:0.842]
Epoch [11/120    avg_loss:0.588, val_acc:0.875]
Epoch [12/120    avg_loss:0.594, val_acc:0.856]
Epoch [13/120    avg_loss:0.532, val_acc:0.890]
Epoch [14/120    avg_loss:0.509, val_acc:0.873]
Epoch [15/120    avg_loss:0.433, val_acc:0.863]
Epoch [16/120    avg_loss:0.395, val_acc:0.879]
Epoch [17/120    avg_loss:0.396, val_acc:0.885]
Epoch [18/120    avg_loss:0.370, val_acc:0.938]
Epoch [19/120    avg_loss:0.305, val_acc:0.896]
Epoch [20/120    avg_loss:0.350, val_acc:0.854]
Epoch [21/120    avg_loss:0.387, val_acc:0.908]
Epoch [22/120    avg_loss:0.377, val_acc:0.940]
Epoch [23/120    avg_loss:0.266, val_acc:0.931]
Epoch [24/120    avg_loss:0.257, val_acc:0.940]
Epoch [25/120    avg_loss:0.292, val_acc:0.948]
Epoch [26/120    avg_loss:0.267, val_acc:0.973]
Epoch [27/120    avg_loss:0.332, val_acc:0.898]
Epoch [28/120    avg_loss:0.416, val_acc:0.952]
Epoch [29/120    avg_loss:0.294, val_acc:0.969]
Epoch [30/120    avg_loss:0.267, val_acc:0.963]
Epoch [31/120    avg_loss:0.283, val_acc:0.967]
Epoch [32/120    avg_loss:0.184, val_acc:0.967]
Epoch [33/120    avg_loss:0.206, val_acc:0.925]
Epoch [34/120    avg_loss:0.214, val_acc:0.965]
Epoch [35/120    avg_loss:0.226, val_acc:0.944]
Epoch [36/120    avg_loss:0.227, val_acc:0.967]
Epoch [37/120    avg_loss:0.236, val_acc:0.952]
Epoch [38/120    avg_loss:0.187, val_acc:0.967]
Epoch [39/120    avg_loss:0.195, val_acc:0.940]
Epoch [40/120    avg_loss:0.195, val_acc:0.965]
Epoch [41/120    avg_loss:0.118, val_acc:0.983]
Epoch [42/120    avg_loss:0.110, val_acc:0.979]
Epoch [43/120    avg_loss:0.120, val_acc:0.979]
Epoch [44/120    avg_loss:0.094, val_acc:0.981]
Epoch [45/120    avg_loss:0.088, val_acc:0.983]
Epoch [46/120    avg_loss:0.096, val_acc:0.981]
Epoch [47/120    avg_loss:0.082, val_acc:0.981]
Epoch [48/120    avg_loss:0.074, val_acc:0.983]
Epoch [49/120    avg_loss:0.081, val_acc:0.985]
Epoch [50/120    avg_loss:0.086, val_acc:0.981]
Epoch [51/120    avg_loss:0.102, val_acc:0.981]
Epoch [52/120    avg_loss:0.079, val_acc:0.983]
Epoch [53/120    avg_loss:0.082, val_acc:0.985]
Epoch [54/120    avg_loss:0.081, val_acc:0.985]
Epoch [55/120    avg_loss:0.072, val_acc:0.983]
Epoch [56/120    avg_loss:0.075, val_acc:0.983]
Epoch [57/120    avg_loss:0.074, val_acc:0.985]
Epoch [58/120    avg_loss:0.063, val_acc:0.985]
Epoch [59/120    avg_loss:0.064, val_acc:0.983]
Epoch [60/120    avg_loss:0.070, val_acc:0.988]
Epoch [61/120    avg_loss:0.070, val_acc:0.985]
Epoch [62/120    avg_loss:0.063, val_acc:0.988]
Epoch [63/120    avg_loss:0.067, val_acc:0.985]
Epoch [64/120    avg_loss:0.066, val_acc:0.985]
Epoch [65/120    avg_loss:0.061, val_acc:0.985]
Epoch [66/120    avg_loss:0.074, val_acc:0.985]
Epoch [67/120    avg_loss:0.054, val_acc:0.985]
Epoch [68/120    avg_loss:0.060, val_acc:0.985]
Epoch [69/120    avg_loss:0.071, val_acc:0.985]
Epoch [70/120    avg_loss:0.070, val_acc:0.985]
Epoch [71/120    avg_loss:0.049, val_acc:0.985]
Epoch [72/120    avg_loss:0.066, val_acc:0.990]
Epoch [73/120    avg_loss:0.056, val_acc:0.985]
Epoch [74/120    avg_loss:0.055, val_acc:0.985]
Epoch [75/120    avg_loss:0.062, val_acc:0.988]
Epoch [76/120    avg_loss:0.058, val_acc:0.985]
Epoch [77/120    avg_loss:0.063, val_acc:0.988]
Epoch [78/120    avg_loss:0.073, val_acc:0.988]
Epoch [79/120    avg_loss:0.071, val_acc:0.988]
Epoch [80/120    avg_loss:0.065, val_acc:0.990]
Epoch [81/120    avg_loss:0.052, val_acc:0.985]
Epoch [82/120    avg_loss:0.048, val_acc:0.985]
Epoch [83/120    avg_loss:0.054, val_acc:0.990]
Epoch [84/120    avg_loss:0.053, val_acc:0.990]
Epoch [85/120    avg_loss:0.052, val_acc:0.988]
Epoch [86/120    avg_loss:0.060, val_acc:0.988]
Epoch [87/120    avg_loss:0.046, val_acc:0.988]
Epoch [88/120    avg_loss:0.054, val_acc:0.988]
Epoch [89/120    avg_loss:0.059, val_acc:0.990]
Epoch [90/120    avg_loss:0.067, val_acc:0.988]
Epoch [91/120    avg_loss:0.057, val_acc:0.990]
Epoch [92/120    avg_loss:0.060, val_acc:0.990]
Epoch [93/120    avg_loss:0.042, val_acc:0.988]
Epoch [94/120    avg_loss:0.054, val_acc:0.992]
Epoch [95/120    avg_loss:0.054, val_acc:0.990]
Epoch [96/120    avg_loss:0.064, val_acc:0.988]
Epoch [97/120    avg_loss:0.060, val_acc:0.985]
Epoch [98/120    avg_loss:0.047, val_acc:0.990]
Epoch [99/120    avg_loss:0.058, val_acc:0.988]
Epoch [100/120    avg_loss:0.053, val_acc:0.988]
Epoch [101/120    avg_loss:0.045, val_acc:0.988]
Epoch [102/120    avg_loss:0.051, val_acc:0.988]
Epoch [103/120    avg_loss:0.047, val_acc:0.988]
Epoch [104/120    avg_loss:0.041, val_acc:0.988]
Epoch [105/120    avg_loss:0.050, val_acc:0.988]
Epoch [106/120    avg_loss:0.051, val_acc:0.988]
Epoch [107/120    avg_loss:0.049, val_acc:0.990]
Epoch [108/120    avg_loss:0.049, val_acc:0.990]
Epoch [109/120    avg_loss:0.046, val_acc:0.988]
Epoch [110/120    avg_loss:0.041, val_acc:0.990]
Epoch [111/120    avg_loss:0.048, val_acc:0.990]
Epoch [112/120    avg_loss:0.037, val_acc:0.990]
Epoch [113/120    avg_loss:0.038, val_acc:0.990]
Epoch [114/120    avg_loss:0.044, val_acc:0.990]
Epoch [115/120    avg_loss:0.046, val_acc:0.990]
Epoch [116/120    avg_loss:0.061, val_acc:0.988]
Epoch [117/120    avg_loss:0.036, val_acc:0.990]
Epoch [118/120    avg_loss:0.049, val_acc:0.990]
Epoch [119/120    avg_loss:0.043, val_acc:0.990]
Epoch [120/120    avg_loss:0.045, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   1   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99926954 0.9977221  1.         0.91938998 0.87719298
 1.         0.99465241 1.         1.         1.         0.99206349
 0.99226519 1.        ]

Kappa:
0.9895551361615753
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e9fcc5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.408, val_acc:0.481]
Epoch [2/120    avg_loss:1.970, val_acc:0.667]
Epoch [3/120    avg_loss:1.624, val_acc:0.669]
Epoch [4/120    avg_loss:1.291, val_acc:0.775]
Epoch [5/120    avg_loss:1.047, val_acc:0.796]
Epoch [6/120    avg_loss:0.852, val_acc:0.810]
Epoch [7/120    avg_loss:0.805, val_acc:0.856]
Epoch [8/120    avg_loss:0.704, val_acc:0.852]
Epoch [9/120    avg_loss:0.658, val_acc:0.887]
Epoch [10/120    avg_loss:0.666, val_acc:0.860]
Epoch [11/120    avg_loss:0.511, val_acc:0.892]
Epoch [12/120    avg_loss:0.473, val_acc:0.896]
Epoch [13/120    avg_loss:0.558, val_acc:0.921]
Epoch [14/120    avg_loss:0.437, val_acc:0.921]
Epoch [15/120    avg_loss:0.377, val_acc:0.931]
Epoch [16/120    avg_loss:0.353, val_acc:0.919]
Epoch [17/120    avg_loss:0.359, val_acc:0.910]
Epoch [18/120    avg_loss:0.402, val_acc:0.925]
Epoch [19/120    avg_loss:0.403, val_acc:0.894]
Epoch [20/120    avg_loss:0.341, val_acc:0.948]
Epoch [21/120    avg_loss:0.363, val_acc:0.954]
Epoch [22/120    avg_loss:0.293, val_acc:0.954]
Epoch [23/120    avg_loss:0.270, val_acc:0.971]
Epoch [24/120    avg_loss:0.225, val_acc:0.948]
Epoch [25/120    avg_loss:0.288, val_acc:0.935]
Epoch [26/120    avg_loss:0.236, val_acc:0.958]
Epoch [27/120    avg_loss:0.185, val_acc:0.942]
Epoch [28/120    avg_loss:0.263, val_acc:0.954]
Epoch [29/120    avg_loss:0.234, val_acc:0.963]
Epoch [30/120    avg_loss:0.207, val_acc:0.971]
Epoch [31/120    avg_loss:0.198, val_acc:0.977]
Epoch [32/120    avg_loss:0.180, val_acc:0.975]
Epoch [33/120    avg_loss:0.149, val_acc:0.971]
Epoch [34/120    avg_loss:0.138, val_acc:0.977]
Epoch [35/120    avg_loss:0.153, val_acc:0.965]
Epoch [36/120    avg_loss:0.164, val_acc:0.950]
Epoch [37/120    avg_loss:0.136, val_acc:0.979]
Epoch [38/120    avg_loss:0.145, val_acc:0.963]
Epoch [39/120    avg_loss:0.127, val_acc:0.977]
Epoch [40/120    avg_loss:0.134, val_acc:0.975]
Epoch [41/120    avg_loss:0.132, val_acc:0.971]
Epoch [42/120    avg_loss:0.085, val_acc:0.988]
Epoch [43/120    avg_loss:0.095, val_acc:0.994]
Epoch [44/120    avg_loss:0.110, val_acc:0.946]
Epoch [45/120    avg_loss:0.139, val_acc:0.965]
Epoch [46/120    avg_loss:0.113, val_acc:0.985]
Epoch [47/120    avg_loss:0.106, val_acc:0.981]
Epoch [48/120    avg_loss:0.090, val_acc:0.971]
Epoch [49/120    avg_loss:0.065, val_acc:0.988]
Epoch [50/120    avg_loss:0.113, val_acc:0.979]
Epoch [51/120    avg_loss:0.109, val_acc:0.973]
Epoch [52/120    avg_loss:0.104, val_acc:0.985]
Epoch [53/120    avg_loss:0.120, val_acc:0.992]
Epoch [54/120    avg_loss:0.050, val_acc:0.985]
Epoch [55/120    avg_loss:0.067, val_acc:0.992]
Epoch [56/120    avg_loss:0.062, val_acc:0.990]
Epoch [57/120    avg_loss:0.047, val_acc:0.992]
Epoch [58/120    avg_loss:0.032, val_acc:0.992]
Epoch [59/120    avg_loss:0.034, val_acc:0.990]
Epoch [60/120    avg_loss:0.047, val_acc:0.992]
Epoch [61/120    avg_loss:0.036, val_acc:0.992]
Epoch [62/120    avg_loss:0.031, val_acc:0.992]
Epoch [63/120    avg_loss:0.033, val_acc:0.992]
Epoch [64/120    avg_loss:0.030, val_acc:0.992]
Epoch [65/120    avg_loss:0.030, val_acc:0.994]
Epoch [66/120    avg_loss:0.031, val_acc:0.992]
Epoch [67/120    avg_loss:0.027, val_acc:0.992]
Epoch [68/120    avg_loss:0.035, val_acc:0.994]
Epoch [69/120    avg_loss:0.027, val_acc:0.994]
Epoch [70/120    avg_loss:0.038, val_acc:0.994]
Epoch [71/120    avg_loss:0.035, val_acc:0.992]
Epoch [72/120    avg_loss:0.029, val_acc:0.994]
Epoch [73/120    avg_loss:0.038, val_acc:0.994]
Epoch [74/120    avg_loss:0.032, val_acc:0.994]
Epoch [75/120    avg_loss:0.034, val_acc:0.994]
Epoch [76/120    avg_loss:0.026, val_acc:0.994]
Epoch [77/120    avg_loss:0.029, val_acc:0.996]
Epoch [78/120    avg_loss:0.025, val_acc:0.994]
Epoch [79/120    avg_loss:0.026, val_acc:0.994]
Epoch [80/120    avg_loss:0.026, val_acc:0.994]
Epoch [81/120    avg_loss:0.028, val_acc:0.996]
Epoch [82/120    avg_loss:0.029, val_acc:0.996]
Epoch [83/120    avg_loss:0.024, val_acc:0.996]
Epoch [84/120    avg_loss:0.024, val_acc:0.996]
Epoch [85/120    avg_loss:0.031, val_acc:0.996]
Epoch [86/120    avg_loss:0.026, val_acc:0.996]
Epoch [87/120    avg_loss:0.032, val_acc:0.996]
Epoch [88/120    avg_loss:0.018, val_acc:0.996]
Epoch [89/120    avg_loss:0.022, val_acc:0.996]
Epoch [90/120    avg_loss:0.023, val_acc:0.996]
Epoch [91/120    avg_loss:0.025, val_acc:0.994]
Epoch [92/120    avg_loss:0.024, val_acc:0.996]
Epoch [93/120    avg_loss:0.021, val_acc:0.996]
Epoch [94/120    avg_loss:0.022, val_acc:0.996]
Epoch [95/120    avg_loss:0.029, val_acc:0.996]
Epoch [96/120    avg_loss:0.019, val_acc:0.996]
Epoch [97/120    avg_loss:0.019, val_acc:0.994]
Epoch [98/120    avg_loss:0.025, val_acc:0.996]
Epoch [99/120    avg_loss:0.024, val_acc:0.996]
Epoch [100/120    avg_loss:0.029, val_acc:0.996]
Epoch [101/120    avg_loss:0.028, val_acc:0.996]
Epoch [102/120    avg_loss:0.023, val_acc:0.996]
Epoch [103/120    avg_loss:0.028, val_acc:0.994]
Epoch [104/120    avg_loss:0.029, val_acc:0.996]
Epoch [105/120    avg_loss:0.020, val_acc:0.996]
Epoch [106/120    avg_loss:0.025, val_acc:0.996]
Epoch [107/120    avg_loss:0.024, val_acc:0.996]
Epoch [108/120    avg_loss:0.018, val_acc:0.996]
Epoch [109/120    avg_loss:0.020, val_acc:0.996]
Epoch [110/120    avg_loss:0.024, val_acc:0.996]
Epoch [111/120    avg_loss:0.018, val_acc:0.996]
Epoch [112/120    avg_loss:0.020, val_acc:0.996]
Epoch [113/120    avg_loss:0.020, val_acc:0.996]
Epoch [114/120    avg_loss:0.022, val_acc:0.996]
Epoch [115/120    avg_loss:0.023, val_acc:0.996]
Epoch [116/120    avg_loss:0.021, val_acc:0.996]
Epoch [117/120    avg_loss:0.018, val_acc:0.996]
Epoch [118/120    avg_loss:0.022, val_acc:0.996]
Epoch [119/120    avg_loss:0.027, val_acc:0.996]
Epoch [120/120    avg_loss:0.034, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  17   0   0   0   0   0   0   4   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99926954 0.98426966 1.         0.94930876 0.94117647
 0.99757869 0.96132597 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9928785606816983
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac84d5f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.391, val_acc:0.429]
Epoch [2/120    avg_loss:1.919, val_acc:0.588]
Epoch [3/120    avg_loss:1.600, val_acc:0.683]
Epoch [4/120    avg_loss:1.370, val_acc:0.700]
Epoch [5/120    avg_loss:1.120, val_acc:0.727]
Epoch [6/120    avg_loss:0.988, val_acc:0.812]
Epoch [7/120    avg_loss:0.836, val_acc:0.823]
Epoch [8/120    avg_loss:0.767, val_acc:0.831]
Epoch [9/120    avg_loss:0.684, val_acc:0.867]
Epoch [10/120    avg_loss:0.620, val_acc:0.883]
Epoch [11/120    avg_loss:0.690, val_acc:0.902]
Epoch [12/120    avg_loss:0.576, val_acc:0.892]
Epoch [13/120    avg_loss:0.484, val_acc:0.908]
Epoch [14/120    avg_loss:0.412, val_acc:0.912]
Epoch [15/120    avg_loss:0.416, val_acc:0.917]
Epoch [16/120    avg_loss:0.410, val_acc:0.927]
Epoch [17/120    avg_loss:0.404, val_acc:0.925]
Epoch [18/120    avg_loss:0.373, val_acc:0.931]
Epoch [19/120    avg_loss:0.301, val_acc:0.931]
Epoch [20/120    avg_loss:0.294, val_acc:0.940]
Epoch [21/120    avg_loss:0.302, val_acc:0.938]
Epoch [22/120    avg_loss:0.212, val_acc:0.952]
Epoch [23/120    avg_loss:0.227, val_acc:0.931]
Epoch [24/120    avg_loss:0.262, val_acc:0.927]
Epoch [25/120    avg_loss:0.363, val_acc:0.963]
Epoch [26/120    avg_loss:0.223, val_acc:0.967]
Epoch [27/120    avg_loss:0.215, val_acc:0.933]
Epoch [28/120    avg_loss:0.182, val_acc:0.958]
Epoch [29/120    avg_loss:0.219, val_acc:0.887]
Epoch [30/120    avg_loss:0.209, val_acc:0.946]
Epoch [31/120    avg_loss:0.185, val_acc:0.925]
Epoch [32/120    avg_loss:0.248, val_acc:0.946]
Epoch [33/120    avg_loss:0.173, val_acc:0.973]
Epoch [34/120    avg_loss:0.176, val_acc:0.946]
Epoch [35/120    avg_loss:0.182, val_acc:0.952]
Epoch [36/120    avg_loss:0.138, val_acc:0.981]
Epoch [37/120    avg_loss:0.159, val_acc:0.948]
Epoch [38/120    avg_loss:0.122, val_acc:0.979]
Epoch [39/120    avg_loss:0.114, val_acc:0.990]
Epoch [40/120    avg_loss:0.097, val_acc:0.969]
Epoch [41/120    avg_loss:0.162, val_acc:0.954]
Epoch [42/120    avg_loss:0.136, val_acc:0.940]
Epoch [43/120    avg_loss:0.163, val_acc:0.985]
Epoch [44/120    avg_loss:0.113, val_acc:0.965]
Epoch [45/120    avg_loss:0.107, val_acc:0.942]
Epoch [46/120    avg_loss:0.125, val_acc:0.981]
Epoch [47/120    avg_loss:0.067, val_acc:0.994]
Epoch [48/120    avg_loss:0.072, val_acc:0.967]
Epoch [49/120    avg_loss:0.115, val_acc:0.990]
Epoch [50/120    avg_loss:0.126, val_acc:0.967]
Epoch [51/120    avg_loss:0.163, val_acc:0.985]
Epoch [52/120    avg_loss:0.120, val_acc:0.975]
Epoch [53/120    avg_loss:0.086, val_acc:0.988]
Epoch [54/120    avg_loss:0.069, val_acc:0.983]
Epoch [55/120    avg_loss:0.058, val_acc:0.985]
Epoch [56/120    avg_loss:0.107, val_acc:0.979]
Epoch [57/120    avg_loss:0.076, val_acc:0.992]
Epoch [58/120    avg_loss:0.047, val_acc:0.994]
Epoch [59/120    avg_loss:0.055, val_acc:0.994]
Epoch [60/120    avg_loss:0.061, val_acc:0.994]
Epoch [61/120    avg_loss:0.098, val_acc:0.977]
Epoch [62/120    avg_loss:0.150, val_acc:0.988]
Epoch [63/120    avg_loss:0.083, val_acc:0.979]
Epoch [64/120    avg_loss:0.050, val_acc:0.973]
Epoch [65/120    avg_loss:0.086, val_acc:0.990]
Epoch [66/120    avg_loss:0.054, val_acc:0.983]
Epoch [67/120    avg_loss:0.042, val_acc:0.994]
Epoch [68/120    avg_loss:0.040, val_acc:0.992]
Epoch [69/120    avg_loss:0.037, val_acc:0.985]
Epoch [70/120    avg_loss:0.039, val_acc:0.996]
Epoch [71/120    avg_loss:0.039, val_acc:0.983]
Epoch [72/120    avg_loss:0.047, val_acc:0.988]
Epoch [73/120    avg_loss:0.049, val_acc:0.971]
Epoch [74/120    avg_loss:0.057, val_acc:0.990]
Epoch [75/120    avg_loss:0.038, val_acc:0.981]
Epoch [76/120    avg_loss:0.091, val_acc:0.985]
Epoch [77/120    avg_loss:0.071, val_acc:0.979]
Epoch [78/120    avg_loss:0.067, val_acc:0.994]
Epoch [79/120    avg_loss:0.068, val_acc:0.988]
Epoch [80/120    avg_loss:0.080, val_acc:0.975]
Epoch [81/120    avg_loss:0.047, val_acc:0.990]
Epoch [82/120    avg_loss:0.030, val_acc:0.990]
Epoch [83/120    avg_loss:0.032, val_acc:0.992]
Epoch [84/120    avg_loss:0.029, val_acc:0.994]
Epoch [85/120    avg_loss:0.024, val_acc:0.996]
Epoch [86/120    avg_loss:0.015, val_acc:0.998]
Epoch [87/120    avg_loss:0.022, val_acc:0.998]
Epoch [88/120    avg_loss:0.020, val_acc:0.998]
Epoch [89/120    avg_loss:0.020, val_acc:0.998]
Epoch [90/120    avg_loss:0.012, val_acc:0.998]
Epoch [91/120    avg_loss:0.026, val_acc:0.998]
Epoch [92/120    avg_loss:0.022, val_acc:0.998]
Epoch [93/120    avg_loss:0.018, val_acc:0.998]
Epoch [94/120    avg_loss:0.019, val_acc:0.998]
Epoch [95/120    avg_loss:0.017, val_acc:0.998]
Epoch [96/120    avg_loss:0.018, val_acc:0.998]
Epoch [97/120    avg_loss:0.013, val_acc:0.998]
Epoch [98/120    avg_loss:0.019, val_acc:0.998]
Epoch [99/120    avg_loss:0.015, val_acc:0.998]
Epoch [100/120    avg_loss:0.021, val_acc:0.998]
Epoch [101/120    avg_loss:0.018, val_acc:0.998]
Epoch [102/120    avg_loss:0.018, val_acc:0.998]
Epoch [103/120    avg_loss:0.016, val_acc:0.998]
Epoch [104/120    avg_loss:0.020, val_acc:0.998]
Epoch [105/120    avg_loss:0.018, val_acc:0.998]
Epoch [106/120    avg_loss:0.015, val_acc:0.998]
Epoch [107/120    avg_loss:0.017, val_acc:0.998]
Epoch [108/120    avg_loss:0.016, val_acc:0.998]
Epoch [109/120    avg_loss:0.014, val_acc:0.998]
Epoch [110/120    avg_loss:0.013, val_acc:0.998]
Epoch [111/120    avg_loss:0.016, val_acc:0.998]
Epoch [112/120    avg_loss:0.016, val_acc:0.998]
Epoch [113/120    avg_loss:0.013, val_acc:0.998]
Epoch [114/120    avg_loss:0.015, val_acc:0.998]
Epoch [115/120    avg_loss:0.013, val_acc:0.998]
Epoch [116/120    avg_loss:0.012, val_acc:0.998]
Epoch [117/120    avg_loss:0.018, val_acc:0.998]
Epoch [118/120    avg_loss:0.014, val_acc:0.998]
Epoch [119/120    avg_loss:0.023, val_acc:0.998]
Epoch [120/120    avg_loss:0.014, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   2   1 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 1.         0.99545455 0.995671   0.98214286 0.96928328
 1.         0.98924731 1.         1.         1.         0.99470899
 0.99447514 1.        ]

Kappa:
0.996201880575261
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0b8de86828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.370, val_acc:0.542]
Epoch [2/120    avg_loss:1.870, val_acc:0.646]
Epoch [3/120    avg_loss:1.521, val_acc:0.635]
Epoch [4/120    avg_loss:1.257, val_acc:0.717]
Epoch [5/120    avg_loss:1.038, val_acc:0.733]
Epoch [6/120    avg_loss:0.848, val_acc:0.775]
Epoch [7/120    avg_loss:0.758, val_acc:0.777]
Epoch [8/120    avg_loss:0.661, val_acc:0.777]
Epoch [9/120    avg_loss:0.642, val_acc:0.792]
Epoch [10/120    avg_loss:0.564, val_acc:0.848]
Epoch [11/120    avg_loss:0.552, val_acc:0.802]
Epoch [12/120    avg_loss:0.479, val_acc:0.887]
Epoch [13/120    avg_loss:0.409, val_acc:0.900]
Epoch [14/120    avg_loss:0.385, val_acc:0.929]
Epoch [15/120    avg_loss:0.378, val_acc:0.915]
Epoch [16/120    avg_loss:0.406, val_acc:0.929]
Epoch [17/120    avg_loss:0.312, val_acc:0.933]
Epoch [18/120    avg_loss:0.260, val_acc:0.946]
Epoch [19/120    avg_loss:0.268, val_acc:0.933]
Epoch [20/120    avg_loss:0.275, val_acc:0.917]
Epoch [21/120    avg_loss:0.258, val_acc:0.931]
Epoch [22/120    avg_loss:0.301, val_acc:0.942]
Epoch [23/120    avg_loss:0.247, val_acc:0.954]
Epoch [24/120    avg_loss:0.199, val_acc:0.908]
Epoch [25/120    avg_loss:0.183, val_acc:0.933]
Epoch [26/120    avg_loss:0.294, val_acc:0.929]
Epoch [27/120    avg_loss:0.239, val_acc:0.954]
Epoch [28/120    avg_loss:0.211, val_acc:0.904]
Epoch [29/120    avg_loss:0.253, val_acc:0.919]
Epoch [30/120    avg_loss:0.209, val_acc:0.944]
Epoch [31/120    avg_loss:0.136, val_acc:0.977]
Epoch [32/120    avg_loss:0.162, val_acc:0.933]
Epoch [33/120    avg_loss:0.126, val_acc:0.983]
Epoch [34/120    avg_loss:0.118, val_acc:0.977]
Epoch [35/120    avg_loss:0.093, val_acc:0.927]
Epoch [36/120    avg_loss:0.138, val_acc:0.960]
Epoch [37/120    avg_loss:0.116, val_acc:0.967]
Epoch [38/120    avg_loss:0.080, val_acc:0.973]
Epoch [39/120    avg_loss:0.103, val_acc:0.977]
Epoch [40/120    avg_loss:0.139, val_acc:0.954]
Epoch [41/120    avg_loss:0.112, val_acc:0.954]
Epoch [42/120    avg_loss:0.138, val_acc:0.971]
Epoch [43/120    avg_loss:0.069, val_acc:0.979]
Epoch [44/120    avg_loss:0.061, val_acc:0.971]
Epoch [45/120    avg_loss:0.065, val_acc:0.967]
Epoch [46/120    avg_loss:0.077, val_acc:0.983]
Epoch [47/120    avg_loss:0.059, val_acc:0.981]
Epoch [48/120    avg_loss:0.071, val_acc:0.977]
Epoch [49/120    avg_loss:0.070, val_acc:0.969]
Epoch [50/120    avg_loss:0.119, val_acc:0.952]
Epoch [51/120    avg_loss:0.174, val_acc:0.973]
Epoch [52/120    avg_loss:0.100, val_acc:0.965]
Epoch [53/120    avg_loss:0.085, val_acc:0.983]
Epoch [54/120    avg_loss:0.056, val_acc:0.979]
Epoch [55/120    avg_loss:0.094, val_acc:0.944]
Epoch [56/120    avg_loss:0.133, val_acc:0.985]
Epoch [57/120    avg_loss:0.051, val_acc:0.990]
Epoch [58/120    avg_loss:0.035, val_acc:0.988]
Epoch [59/120    avg_loss:0.044, val_acc:0.965]
Epoch [60/120    avg_loss:0.038, val_acc:0.988]
Epoch [61/120    avg_loss:0.033, val_acc:0.990]
Epoch [62/120    avg_loss:0.018, val_acc:0.988]
Epoch [63/120    avg_loss:0.015, val_acc:0.992]
Epoch [64/120    avg_loss:0.034, val_acc:0.988]
Epoch [65/120    avg_loss:0.058, val_acc:0.973]
Epoch [66/120    avg_loss:0.074, val_acc:0.983]
Epoch [67/120    avg_loss:0.032, val_acc:0.985]
Epoch [68/120    avg_loss:0.026, val_acc:0.983]
Epoch [69/120    avg_loss:0.022, val_acc:0.988]
Epoch [70/120    avg_loss:0.018, val_acc:0.985]
Epoch [71/120    avg_loss:0.031, val_acc:0.975]
Epoch [72/120    avg_loss:0.025, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.990]
Epoch [74/120    avg_loss:0.014, val_acc:0.992]
Epoch [75/120    avg_loss:0.013, val_acc:0.988]
Epoch [76/120    avg_loss:0.019, val_acc:0.985]
Epoch [77/120    avg_loss:0.023, val_acc:0.990]
Epoch [78/120    avg_loss:0.019, val_acc:0.990]
Epoch [79/120    avg_loss:0.018, val_acc:0.988]
Epoch [80/120    avg_loss:0.014, val_acc:0.990]
Epoch [81/120    avg_loss:0.022, val_acc:0.990]
Epoch [82/120    avg_loss:0.016, val_acc:0.985]
Epoch [83/120    avg_loss:0.027, val_acc:0.985]
Epoch [84/120    avg_loss:0.093, val_acc:0.971]
Epoch [85/120    avg_loss:0.111, val_acc:0.971]
Epoch [86/120    avg_loss:0.055, val_acc:0.963]
Epoch [87/120    avg_loss:0.056, val_acc:0.985]
Epoch [88/120    avg_loss:0.041, val_acc:0.988]
Epoch [89/120    avg_loss:0.023, val_acc:0.988]
Epoch [90/120    avg_loss:0.022, val_acc:0.988]
Epoch [91/120    avg_loss:0.015, val_acc:0.988]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.028, val_acc:0.990]
Epoch [94/120    avg_loss:0.014, val_acc:0.990]
Epoch [95/120    avg_loss:0.014, val_acc:0.990]
Epoch [96/120    avg_loss:0.021, val_acc:0.990]
Epoch [97/120    avg_loss:0.018, val_acc:0.990]
Epoch [98/120    avg_loss:0.014, val_acc:0.992]
Epoch [99/120    avg_loss:0.012, val_acc:0.992]
Epoch [100/120    avg_loss:0.014, val_acc:0.992]
Epoch [101/120    avg_loss:0.013, val_acc:0.992]
Epoch [102/120    avg_loss:0.013, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.019, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.015, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.015, val_acc:0.990]
Epoch [117/120    avg_loss:0.014, val_acc:0.990]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   0   0   1   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99412628 1.         1.         0.96137339 0.93862816
 0.98095238 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.993828875538531
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40107df7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.360, val_acc:0.435]
Epoch [2/120    avg_loss:1.910, val_acc:0.608]
Epoch [3/120    avg_loss:1.586, val_acc:0.694]
Epoch [4/120    avg_loss:1.307, val_acc:0.708]
Epoch [5/120    avg_loss:1.102, val_acc:0.717]
Epoch [6/120    avg_loss:0.946, val_acc:0.748]
Epoch [7/120    avg_loss:0.820, val_acc:0.744]
Epoch [8/120    avg_loss:0.718, val_acc:0.775]
Epoch [9/120    avg_loss:0.619, val_acc:0.840]
Epoch [10/120    avg_loss:0.605, val_acc:0.785]
Epoch [11/120    avg_loss:0.506, val_acc:0.902]
Epoch [12/120    avg_loss:0.519, val_acc:0.879]
Epoch [13/120    avg_loss:0.455, val_acc:0.842]
Epoch [14/120    avg_loss:0.514, val_acc:0.898]
Epoch [15/120    avg_loss:0.407, val_acc:0.892]
Epoch [16/120    avg_loss:0.376, val_acc:0.915]
Epoch [17/120    avg_loss:0.362, val_acc:0.879]
Epoch [18/120    avg_loss:0.392, val_acc:0.912]
Epoch [19/120    avg_loss:0.371, val_acc:0.908]
Epoch [20/120    avg_loss:0.324, val_acc:0.938]
Epoch [21/120    avg_loss:0.374, val_acc:0.821]
Epoch [22/120    avg_loss:0.323, val_acc:0.933]
Epoch [23/120    avg_loss:0.277, val_acc:0.963]
Epoch [24/120    avg_loss:0.294, val_acc:0.963]
Epoch [25/120    avg_loss:0.241, val_acc:0.931]
Epoch [26/120    avg_loss:0.228, val_acc:0.933]
Epoch [27/120    avg_loss:0.264, val_acc:0.958]
Epoch [28/120    avg_loss:0.248, val_acc:0.952]
Epoch [29/120    avg_loss:0.208, val_acc:0.933]
Epoch [30/120    avg_loss:0.202, val_acc:0.925]
Epoch [31/120    avg_loss:0.201, val_acc:0.981]
Epoch [32/120    avg_loss:0.186, val_acc:0.977]
Epoch [33/120    avg_loss:0.162, val_acc:0.963]
Epoch [34/120    avg_loss:0.193, val_acc:0.929]
Epoch [35/120    avg_loss:0.212, val_acc:0.952]
Epoch [36/120    avg_loss:0.191, val_acc:0.977]
Epoch [37/120    avg_loss:0.141, val_acc:0.969]
Epoch [38/120    avg_loss:0.141, val_acc:0.973]
Epoch [39/120    avg_loss:0.184, val_acc:0.950]
Epoch [40/120    avg_loss:0.170, val_acc:0.965]
Epoch [41/120    avg_loss:0.119, val_acc:0.973]
Epoch [42/120    avg_loss:0.135, val_acc:0.967]
Epoch [43/120    avg_loss:0.139, val_acc:0.979]
Epoch [44/120    avg_loss:0.099, val_acc:0.983]
Epoch [45/120    avg_loss:0.134, val_acc:0.977]
Epoch [46/120    avg_loss:0.145, val_acc:0.958]
Epoch [47/120    avg_loss:0.105, val_acc:0.973]
Epoch [48/120    avg_loss:0.103, val_acc:0.981]
Epoch [49/120    avg_loss:0.094, val_acc:0.983]
Epoch [50/120    avg_loss:0.181, val_acc:0.848]
Epoch [51/120    avg_loss:0.243, val_acc:0.958]
Epoch [52/120    avg_loss:0.189, val_acc:0.967]
Epoch [53/120    avg_loss:0.115, val_acc:0.977]
Epoch [54/120    avg_loss:0.095, val_acc:0.969]
Epoch [55/120    avg_loss:0.101, val_acc:0.973]
Epoch [56/120    avg_loss:0.090, val_acc:0.985]
Epoch [57/120    avg_loss:0.062, val_acc:0.983]
Epoch [58/120    avg_loss:0.055, val_acc:0.990]
Epoch [59/120    avg_loss:0.111, val_acc:0.981]
Epoch [60/120    avg_loss:0.092, val_acc:0.983]
Epoch [61/120    avg_loss:0.094, val_acc:0.973]
Epoch [62/120    avg_loss:0.075, val_acc:0.990]
Epoch [63/120    avg_loss:0.075, val_acc:0.988]
Epoch [64/120    avg_loss:0.053, val_acc:0.992]
Epoch [65/120    avg_loss:0.062, val_acc:0.990]
Epoch [66/120    avg_loss:0.050, val_acc:0.981]
Epoch [67/120    avg_loss:0.070, val_acc:0.977]
Epoch [68/120    avg_loss:0.081, val_acc:0.963]
Epoch [69/120    avg_loss:0.053, val_acc:0.985]
Epoch [70/120    avg_loss:0.068, val_acc:0.992]
Epoch [71/120    avg_loss:0.041, val_acc:0.990]
Epoch [72/120    avg_loss:0.042, val_acc:0.990]
Epoch [73/120    avg_loss:0.041, val_acc:0.983]
Epoch [74/120    avg_loss:0.034, val_acc:0.985]
Epoch [75/120    avg_loss:0.046, val_acc:0.988]
Epoch [76/120    avg_loss:0.030, val_acc:0.988]
Epoch [77/120    avg_loss:0.030, val_acc:0.988]
Epoch [78/120    avg_loss:0.029, val_acc:0.988]
Epoch [79/120    avg_loss:0.036, val_acc:0.988]
Epoch [80/120    avg_loss:0.037, val_acc:0.988]
Epoch [81/120    avg_loss:0.041, val_acc:0.992]
Epoch [82/120    avg_loss:0.026, val_acc:0.985]
Epoch [83/120    avg_loss:0.044, val_acc:0.983]
Epoch [84/120    avg_loss:0.057, val_acc:0.973]
Epoch [85/120    avg_loss:0.059, val_acc:0.965]
Epoch [86/120    avg_loss:0.047, val_acc:0.990]
Epoch [87/120    avg_loss:0.048, val_acc:0.996]
Epoch [88/120    avg_loss:0.024, val_acc:0.990]
Epoch [89/120    avg_loss:0.025, val_acc:0.998]
Epoch [90/120    avg_loss:0.024, val_acc:0.992]
Epoch [91/120    avg_loss:0.028, val_acc:0.990]
Epoch [92/120    avg_loss:0.018, val_acc:0.996]
Epoch [93/120    avg_loss:0.036, val_acc:0.992]
Epoch [94/120    avg_loss:0.033, val_acc:0.988]
Epoch [95/120    avg_loss:0.026, val_acc:0.994]
Epoch [96/120    avg_loss:0.023, val_acc:0.996]
Epoch [97/120    avg_loss:0.015, val_acc:0.994]
Epoch [98/120    avg_loss:0.018, val_acc:0.994]
Epoch [99/120    avg_loss:0.041, val_acc:0.985]
Epoch [100/120    avg_loss:0.059, val_acc:0.977]
Epoch [101/120    avg_loss:0.111, val_acc:0.988]
Epoch [102/120    avg_loss:0.042, val_acc:0.998]
Epoch [103/120    avg_loss:0.040, val_acc:0.992]
Epoch [104/120    avg_loss:0.016, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.996]
Epoch [106/120    avg_loss:0.011, val_acc:0.996]
Epoch [107/120    avg_loss:0.010, val_acc:0.992]
Epoch [108/120    avg_loss:0.012, val_acc:0.996]
Epoch [109/120    avg_loss:0.012, val_acc:0.998]
Epoch [110/120    avg_loss:0.011, val_acc:0.998]
Epoch [111/120    avg_loss:0.011, val_acc:0.998]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.046, val_acc:0.967]
Epoch [114/120    avg_loss:0.062, val_acc:0.985]
Epoch [115/120    avg_loss:0.026, val_acc:0.983]
Epoch [116/120    avg_loss:0.022, val_acc:0.996]
Epoch [117/120    avg_loss:0.018, val_acc:0.994]
Epoch [118/120    avg_loss:0.013, val_acc:0.996]
Epoch [119/120    avg_loss:0.009, val_acc:0.994]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99780541 0.98871332 1.         0.95671982 0.93770492
 0.99277108 0.9726776  1.         1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9919300818975735
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb25eb667f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.378, val_acc:0.438]
Epoch [2/120    avg_loss:1.903, val_acc:0.571]
Epoch [3/120    avg_loss:1.558, val_acc:0.696]
Epoch [4/120    avg_loss:1.221, val_acc:0.733]
Epoch [5/120    avg_loss:1.058, val_acc:0.735]
Epoch [6/120    avg_loss:0.919, val_acc:0.829]
Epoch [7/120    avg_loss:0.789, val_acc:0.769]
Epoch [8/120    avg_loss:0.718, val_acc:0.833]
Epoch [9/120    avg_loss:0.697, val_acc:0.785]
Epoch [10/120    avg_loss:0.587, val_acc:0.802]
Epoch [11/120    avg_loss:0.593, val_acc:0.879]
Epoch [12/120    avg_loss:0.548, val_acc:0.833]
Epoch [13/120    avg_loss:0.509, val_acc:0.896]
Epoch [14/120    avg_loss:0.450, val_acc:0.900]
Epoch [15/120    avg_loss:0.442, val_acc:0.904]
Epoch [16/120    avg_loss:0.471, val_acc:0.910]
Epoch [17/120    avg_loss:0.386, val_acc:0.935]
Epoch [18/120    avg_loss:0.326, val_acc:0.940]
Epoch [19/120    avg_loss:0.356, val_acc:0.950]
Epoch [20/120    avg_loss:0.266, val_acc:0.912]
Epoch [21/120    avg_loss:0.370, val_acc:0.829]
Epoch [22/120    avg_loss:0.374, val_acc:0.887]
Epoch [23/120    avg_loss:0.305, val_acc:0.940]
Epoch [24/120    avg_loss:0.238, val_acc:0.927]
Epoch [25/120    avg_loss:0.240, val_acc:0.898]
Epoch [26/120    avg_loss:0.225, val_acc:0.948]
Epoch [27/120    avg_loss:0.234, val_acc:0.940]
Epoch [28/120    avg_loss:0.289, val_acc:0.944]
Epoch [29/120    avg_loss:0.185, val_acc:0.960]
Epoch [30/120    avg_loss:0.183, val_acc:0.960]
Epoch [31/120    avg_loss:0.204, val_acc:0.950]
Epoch [32/120    avg_loss:0.177, val_acc:0.956]
Epoch [33/120    avg_loss:0.154, val_acc:0.965]
Epoch [34/120    avg_loss:0.152, val_acc:0.967]
Epoch [35/120    avg_loss:0.123, val_acc:0.967]
Epoch [36/120    avg_loss:0.088, val_acc:0.963]
Epoch [37/120    avg_loss:0.116, val_acc:0.952]
Epoch [38/120    avg_loss:0.102, val_acc:0.969]
Epoch [39/120    avg_loss:0.104, val_acc:0.967]
Epoch [40/120    avg_loss:0.098, val_acc:0.977]
Epoch [41/120    avg_loss:0.117, val_acc:0.969]
Epoch [42/120    avg_loss:0.120, val_acc:0.971]
Epoch [43/120    avg_loss:0.090, val_acc:0.956]
Epoch [44/120    avg_loss:0.123, val_acc:0.969]
Epoch [45/120    avg_loss:0.080, val_acc:0.975]
Epoch [46/120    avg_loss:0.072, val_acc:0.975]
Epoch [47/120    avg_loss:0.061, val_acc:0.969]
Epoch [48/120    avg_loss:0.059, val_acc:0.990]
Epoch [49/120    avg_loss:0.091, val_acc:0.956]
Epoch [50/120    avg_loss:0.133, val_acc:0.952]
Epoch [51/120    avg_loss:0.114, val_acc:0.977]
Epoch [52/120    avg_loss:0.088, val_acc:0.988]
Epoch [53/120    avg_loss:0.057, val_acc:0.979]
Epoch [54/120    avg_loss:0.084, val_acc:0.988]
Epoch [55/120    avg_loss:0.065, val_acc:0.973]
Epoch [56/120    avg_loss:0.073, val_acc:0.977]
Epoch [57/120    avg_loss:0.071, val_acc:0.979]
Epoch [58/120    avg_loss:0.076, val_acc:0.975]
Epoch [59/120    avg_loss:0.081, val_acc:0.988]
Epoch [60/120    avg_loss:0.064, val_acc:0.988]
Epoch [61/120    avg_loss:0.036, val_acc:0.988]
Epoch [62/120    avg_loss:0.034, val_acc:0.988]
Epoch [63/120    avg_loss:0.032, val_acc:0.988]
Epoch [64/120    avg_loss:0.025, val_acc:0.990]
Epoch [65/120    avg_loss:0.031, val_acc:0.990]
Epoch [66/120    avg_loss:0.024, val_acc:0.990]
Epoch [67/120    avg_loss:0.031, val_acc:0.990]
Epoch [68/120    avg_loss:0.026, val_acc:0.992]
Epoch [69/120    avg_loss:0.030, val_acc:0.996]
Epoch [70/120    avg_loss:0.028, val_acc:0.992]
Epoch [71/120    avg_loss:0.026, val_acc:0.996]
Epoch [72/120    avg_loss:0.022, val_acc:0.996]
Epoch [73/120    avg_loss:0.032, val_acc:0.996]
Epoch [74/120    avg_loss:0.020, val_acc:0.996]
Epoch [75/120    avg_loss:0.028, val_acc:0.996]
Epoch [76/120    avg_loss:0.021, val_acc:0.998]
Epoch [77/120    avg_loss:0.026, val_acc:0.996]
Epoch [78/120    avg_loss:0.019, val_acc:0.994]
Epoch [79/120    avg_loss:0.019, val_acc:0.994]
Epoch [80/120    avg_loss:0.021, val_acc:0.996]
Epoch [81/120    avg_loss:0.019, val_acc:0.996]
Epoch [82/120    avg_loss:0.027, val_acc:0.996]
Epoch [83/120    avg_loss:0.016, val_acc:0.996]
Epoch [84/120    avg_loss:0.015, val_acc:0.994]
Epoch [85/120    avg_loss:0.020, val_acc:0.994]
Epoch [86/120    avg_loss:0.017, val_acc:0.996]
Epoch [87/120    avg_loss:0.018, val_acc:0.996]
Epoch [88/120    avg_loss:0.019, val_acc:0.996]
Epoch [89/120    avg_loss:0.019, val_acc:0.994]
Epoch [90/120    avg_loss:0.018, val_acc:0.994]
Epoch [91/120    avg_loss:0.029, val_acc:0.996]
Epoch [92/120    avg_loss:0.032, val_acc:0.996]
Epoch [93/120    avg_loss:0.016, val_acc:0.996]
Epoch [94/120    avg_loss:0.020, val_acc:0.996]
Epoch [95/120    avg_loss:0.023, val_acc:0.996]
Epoch [96/120    avg_loss:0.017, val_acc:0.996]
Epoch [97/120    avg_loss:0.025, val_acc:0.996]
Epoch [98/120    avg_loss:0.015, val_acc:0.996]
Epoch [99/120    avg_loss:0.024, val_acc:0.996]
Epoch [100/120    avg_loss:0.018, val_acc:0.996]
Epoch [101/120    avg_loss:0.026, val_acc:0.996]
Epoch [102/120    avg_loss:0.021, val_acc:0.996]
Epoch [103/120    avg_loss:0.029, val_acc:0.996]
Epoch [104/120    avg_loss:0.018, val_acc:0.996]
Epoch [105/120    avg_loss:0.017, val_acc:0.996]
Epoch [106/120    avg_loss:0.017, val_acc:0.996]
Epoch [107/120    avg_loss:0.015, val_acc:0.996]
Epoch [108/120    avg_loss:0.020, val_acc:0.996]
Epoch [109/120    avg_loss:0.022, val_acc:0.996]
Epoch [110/120    avg_loss:0.018, val_acc:0.996]
Epoch [111/120    avg_loss:0.018, val_acc:0.996]
Epoch [112/120    avg_loss:0.016, val_acc:0.996]
Epoch [113/120    avg_loss:0.019, val_acc:0.996]
Epoch [114/120    avg_loss:0.020, val_acc:0.996]
Epoch [115/120    avg_loss:0.018, val_acc:0.996]
Epoch [116/120    avg_loss:0.022, val_acc:0.996]
Epoch [117/120    avg_loss:0.020, val_acc:0.996]
Epoch [118/120    avg_loss:0.018, val_acc:0.996]
Epoch [119/120    avg_loss:0.017, val_acc:0.996]
Epoch [120/120    avg_loss:0.024, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   0   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 0.99338722 1.         1.         0.97747748 0.96666667
 0.97862233 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9954909233770196
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f497418a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.362, val_acc:0.479]
Epoch [2/120    avg_loss:1.907, val_acc:0.613]
Epoch [3/120    avg_loss:1.564, val_acc:0.658]
Epoch [4/120    avg_loss:1.291, val_acc:0.696]
Epoch [5/120    avg_loss:1.124, val_acc:0.740]
Epoch [6/120    avg_loss:0.968, val_acc:0.769]
Epoch [7/120    avg_loss:0.815, val_acc:0.794]
Epoch [8/120    avg_loss:0.678, val_acc:0.875]
Epoch [9/120    avg_loss:0.608, val_acc:0.875]
Epoch [10/120    avg_loss:0.527, val_acc:0.892]
Epoch [11/120    avg_loss:0.503, val_acc:0.904]
Epoch [12/120    avg_loss:0.452, val_acc:0.919]
Epoch [13/120    avg_loss:0.388, val_acc:0.912]
Epoch [14/120    avg_loss:0.338, val_acc:0.925]
Epoch [15/120    avg_loss:0.329, val_acc:0.915]
Epoch [16/120    avg_loss:0.373, val_acc:0.915]
Epoch [17/120    avg_loss:0.330, val_acc:0.923]
Epoch [18/120    avg_loss:0.324, val_acc:0.912]
Epoch [19/120    avg_loss:0.316, val_acc:0.919]
Epoch [20/120    avg_loss:0.327, val_acc:0.900]
Epoch [21/120    avg_loss:0.307, val_acc:0.933]
Epoch [22/120    avg_loss:0.265, val_acc:0.933]
Epoch [23/120    avg_loss:0.217, val_acc:0.927]
Epoch [24/120    avg_loss:0.265, val_acc:0.896]
Epoch [25/120    avg_loss:0.250, val_acc:0.946]
Epoch [26/120    avg_loss:0.256, val_acc:0.912]
Epoch [27/120    avg_loss:0.266, val_acc:0.946]
Epoch [28/120    avg_loss:0.160, val_acc:0.940]
Epoch [29/120    avg_loss:0.173, val_acc:0.938]
Epoch [30/120    avg_loss:0.215, val_acc:0.948]
Epoch [31/120    avg_loss:0.178, val_acc:0.967]
Epoch [32/120    avg_loss:0.135, val_acc:0.958]
Epoch [33/120    avg_loss:0.152, val_acc:0.963]
Epoch [34/120    avg_loss:0.214, val_acc:0.965]
Epoch [35/120    avg_loss:0.148, val_acc:0.956]
Epoch [36/120    avg_loss:0.142, val_acc:0.954]
Epoch [37/120    avg_loss:0.121, val_acc:0.956]
Epoch [38/120    avg_loss:0.216, val_acc:0.954]
Epoch [39/120    avg_loss:0.092, val_acc:0.969]
Epoch [40/120    avg_loss:0.080, val_acc:0.963]
Epoch [41/120    avg_loss:0.117, val_acc:0.965]
Epoch [42/120    avg_loss:0.213, val_acc:0.948]
Epoch [43/120    avg_loss:0.173, val_acc:0.967]
Epoch [44/120    avg_loss:0.137, val_acc:0.963]
Epoch [45/120    avg_loss:0.118, val_acc:0.969]
Epoch [46/120    avg_loss:0.094, val_acc:0.967]
Epoch [47/120    avg_loss:0.080, val_acc:0.969]
Epoch [48/120    avg_loss:0.122, val_acc:0.963]
Epoch [49/120    avg_loss:0.104, val_acc:0.977]
Epoch [50/120    avg_loss:0.115, val_acc:0.971]
Epoch [51/120    avg_loss:0.089, val_acc:0.971]
Epoch [52/120    avg_loss:0.113, val_acc:0.967]
Epoch [53/120    avg_loss:0.082, val_acc:0.983]
Epoch [54/120    avg_loss:0.138, val_acc:0.950]
Epoch [55/120    avg_loss:0.111, val_acc:0.965]
Epoch [56/120    avg_loss:0.069, val_acc:0.979]
Epoch [57/120    avg_loss:0.056, val_acc:0.983]
Epoch [58/120    avg_loss:0.065, val_acc:0.983]
Epoch [59/120    avg_loss:0.077, val_acc:0.979]
Epoch [60/120    avg_loss:0.045, val_acc:0.985]
Epoch [61/120    avg_loss:0.035, val_acc:0.983]
Epoch [62/120    avg_loss:0.031, val_acc:0.981]
Epoch [63/120    avg_loss:0.042, val_acc:0.988]
Epoch [64/120    avg_loss:0.048, val_acc:0.985]
Epoch [65/120    avg_loss:0.058, val_acc:0.981]
Epoch [66/120    avg_loss:0.051, val_acc:0.985]
Epoch [67/120    avg_loss:0.070, val_acc:0.981]
Epoch [68/120    avg_loss:0.110, val_acc:0.973]
Epoch [69/120    avg_loss:0.092, val_acc:0.973]
Epoch [70/120    avg_loss:0.074, val_acc:0.977]
Epoch [71/120    avg_loss:0.042, val_acc:0.985]
Epoch [72/120    avg_loss:0.050, val_acc:0.979]
Epoch [73/120    avg_loss:0.037, val_acc:0.985]
Epoch [74/120    avg_loss:0.031, val_acc:0.990]
Epoch [75/120    avg_loss:0.026, val_acc:0.988]
Epoch [76/120    avg_loss:0.021, val_acc:0.992]
Epoch [77/120    avg_loss:0.027, val_acc:0.988]
Epoch [78/120    avg_loss:0.022, val_acc:0.992]
Epoch [79/120    avg_loss:0.019, val_acc:0.988]
Epoch [80/120    avg_loss:0.021, val_acc:0.990]
Epoch [81/120    avg_loss:0.029, val_acc:0.988]
Epoch [82/120    avg_loss:0.018, val_acc:0.988]
Epoch [83/120    avg_loss:0.014, val_acc:0.990]
Epoch [84/120    avg_loss:0.021, val_acc:0.988]
Epoch [85/120    avg_loss:0.025, val_acc:0.988]
Epoch [86/120    avg_loss:0.051, val_acc:0.990]
Epoch [87/120    avg_loss:0.083, val_acc:0.988]
Epoch [88/120    avg_loss:0.044, val_acc:0.985]
Epoch [89/120    avg_loss:0.037, val_acc:0.990]
Epoch [90/120    avg_loss:0.039, val_acc:0.988]
Epoch [91/120    avg_loss:0.021, val_acc:0.990]
Epoch [92/120    avg_loss:0.023, val_acc:0.992]
Epoch [93/120    avg_loss:0.022, val_acc:0.992]
Epoch [94/120    avg_loss:0.015, val_acc:0.992]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.013, val_acc:0.992]
Epoch [97/120    avg_loss:0.016, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.016, val_acc:0.992]
Epoch [100/120    avg_loss:0.012, val_acc:0.992]
Epoch [101/120    avg_loss:0.012, val_acc:0.992]
Epoch [102/120    avg_loss:0.011, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.992]
Epoch [104/120    avg_loss:0.013, val_acc:0.992]
Epoch [105/120    avg_loss:0.012, val_acc:0.992]
Epoch [106/120    avg_loss:0.011, val_acc:0.992]
Epoch [107/120    avg_loss:0.009, val_acc:0.992]
Epoch [108/120    avg_loss:0.013, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.014, val_acc:0.992]
Epoch [111/120    avg_loss:0.009, val_acc:0.992]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.011, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.014, val_acc:0.992]
Epoch [117/120    avg_loss:0.010, val_acc:0.992]
Epoch [118/120    avg_loss:0.009, val_acc:0.992]
Epoch [119/120    avg_loss:0.012, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.68017057569296

F1 scores:
[       nan 0.996337   1.         1.         0.98013245 0.97241379
 0.98800959 1.         1.         1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.9964396553464914
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea435527f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.382, val_acc:0.512]
Epoch [2/120    avg_loss:1.925, val_acc:0.585]
Epoch [3/120    avg_loss:1.561, val_acc:0.688]
Epoch [4/120    avg_loss:1.290, val_acc:0.746]
Epoch [5/120    avg_loss:1.078, val_acc:0.769]
Epoch [6/120    avg_loss:0.900, val_acc:0.800]
Epoch [7/120    avg_loss:0.882, val_acc:0.808]
Epoch [8/120    avg_loss:0.745, val_acc:0.831]
Epoch [9/120    avg_loss:0.703, val_acc:0.875]
Epoch [10/120    avg_loss:0.599, val_acc:0.802]
Epoch [11/120    avg_loss:0.568, val_acc:0.827]
Epoch [12/120    avg_loss:0.531, val_acc:0.896]
Epoch [13/120    avg_loss:0.476, val_acc:0.869]
Epoch [14/120    avg_loss:0.462, val_acc:0.881]
Epoch [15/120    avg_loss:0.426, val_acc:0.921]
Epoch [16/120    avg_loss:0.393, val_acc:0.925]
Epoch [17/120    avg_loss:0.375, val_acc:0.917]
Epoch [18/120    avg_loss:0.363, val_acc:0.944]
Epoch [19/120    avg_loss:0.316, val_acc:0.940]
Epoch [20/120    avg_loss:0.355, val_acc:0.956]
Epoch [21/120    avg_loss:0.260, val_acc:0.940]
Epoch [22/120    avg_loss:0.240, val_acc:0.952]
Epoch [23/120    avg_loss:0.250, val_acc:0.963]
Epoch [24/120    avg_loss:0.201, val_acc:0.967]
Epoch [25/120    avg_loss:0.193, val_acc:0.958]
Epoch [26/120    avg_loss:0.185, val_acc:0.973]
Epoch [27/120    avg_loss:0.200, val_acc:0.952]
Epoch [28/120    avg_loss:0.182, val_acc:0.956]
Epoch [29/120    avg_loss:0.254, val_acc:0.952]
Epoch [30/120    avg_loss:0.217, val_acc:0.969]
Epoch [31/120    avg_loss:0.218, val_acc:0.946]
Epoch [32/120    avg_loss:0.205, val_acc:0.963]
Epoch [33/120    avg_loss:0.162, val_acc:0.963]
Epoch [34/120    avg_loss:0.190, val_acc:0.960]
Epoch [35/120    avg_loss:0.186, val_acc:0.973]
Epoch [36/120    avg_loss:0.135, val_acc:0.971]
Epoch [37/120    avg_loss:0.154, val_acc:0.969]
Epoch [38/120    avg_loss:0.189, val_acc:0.940]
Epoch [39/120    avg_loss:0.182, val_acc:0.965]
Epoch [40/120    avg_loss:0.101, val_acc:0.979]
Epoch [41/120    avg_loss:0.117, val_acc:0.973]
Epoch [42/120    avg_loss:0.121, val_acc:0.975]
Epoch [43/120    avg_loss:0.089, val_acc:0.975]
Epoch [44/120    avg_loss:0.120, val_acc:0.977]
Epoch [45/120    avg_loss:0.112, val_acc:0.969]
Epoch [46/120    avg_loss:0.075, val_acc:0.992]
Epoch [47/120    avg_loss:0.059, val_acc:0.983]
Epoch [48/120    avg_loss:0.105, val_acc:0.979]
Epoch [49/120    avg_loss:0.086, val_acc:0.971]
Epoch [50/120    avg_loss:0.123, val_acc:0.975]
Epoch [51/120    avg_loss:0.076, val_acc:0.979]
Epoch [52/120    avg_loss:0.067, val_acc:0.973]
Epoch [53/120    avg_loss:0.062, val_acc:0.975]
Epoch [54/120    avg_loss:0.046, val_acc:0.969]
Epoch [55/120    avg_loss:0.073, val_acc:0.979]
Epoch [56/120    avg_loss:0.102, val_acc:0.979]
Epoch [57/120    avg_loss:0.063, val_acc:0.963]
Epoch [58/120    avg_loss:0.089, val_acc:0.969]
Epoch [59/120    avg_loss:0.063, val_acc:0.975]
Epoch [60/120    avg_loss:0.050, val_acc:0.981]
Epoch [61/120    avg_loss:0.047, val_acc:0.979]
Epoch [62/120    avg_loss:0.041, val_acc:0.981]
Epoch [63/120    avg_loss:0.030, val_acc:0.983]
Epoch [64/120    avg_loss:0.033, val_acc:0.983]
Epoch [65/120    avg_loss:0.031, val_acc:0.981]
Epoch [66/120    avg_loss:0.041, val_acc:0.985]
Epoch [67/120    avg_loss:0.038, val_acc:0.983]
Epoch [68/120    avg_loss:0.029, val_acc:0.983]
Epoch [69/120    avg_loss:0.041, val_acc:0.981]
Epoch [70/120    avg_loss:0.027, val_acc:0.988]
Epoch [71/120    avg_loss:0.025, val_acc:0.988]
Epoch [72/120    avg_loss:0.035, val_acc:0.988]
Epoch [73/120    avg_loss:0.036, val_acc:0.985]
Epoch [74/120    avg_loss:0.033, val_acc:0.988]
Epoch [75/120    avg_loss:0.027, val_acc:0.983]
Epoch [76/120    avg_loss:0.026, val_acc:0.983]
Epoch [77/120    avg_loss:0.028, val_acc:0.983]
Epoch [78/120    avg_loss:0.028, val_acc:0.983]
Epoch [79/120    avg_loss:0.034, val_acc:0.983]
Epoch [80/120    avg_loss:0.022, val_acc:0.988]
Epoch [81/120    avg_loss:0.034, val_acc:0.983]
Epoch [82/120    avg_loss:0.028, val_acc:0.983]
Epoch [83/120    avg_loss:0.035, val_acc:0.983]
Epoch [84/120    avg_loss:0.024, val_acc:0.983]
Epoch [85/120    avg_loss:0.026, val_acc:0.983]
Epoch [86/120    avg_loss:0.050, val_acc:0.983]
Epoch [87/120    avg_loss:0.032, val_acc:0.983]
Epoch [88/120    avg_loss:0.044, val_acc:0.983]
Epoch [89/120    avg_loss:0.031, val_acc:0.983]
Epoch [90/120    avg_loss:0.032, val_acc:0.983]
Epoch [91/120    avg_loss:0.029, val_acc:0.983]
Epoch [92/120    avg_loss:0.021, val_acc:0.983]
Epoch [93/120    avg_loss:0.026, val_acc:0.983]
Epoch [94/120    avg_loss:0.028, val_acc:0.983]
Epoch [95/120    avg_loss:0.024, val_acc:0.983]
Epoch [96/120    avg_loss:0.032, val_acc:0.983]
Epoch [97/120    avg_loss:0.026, val_acc:0.983]
Epoch [98/120    avg_loss:0.037, val_acc:0.983]
Epoch [99/120    avg_loss:0.025, val_acc:0.983]
Epoch [100/120    avg_loss:0.030, val_acc:0.983]
Epoch [101/120    avg_loss:0.029, val_acc:0.983]
Epoch [102/120    avg_loss:0.026, val_acc:0.983]
Epoch [103/120    avg_loss:0.031, val_acc:0.983]
Epoch [104/120    avg_loss:0.028, val_acc:0.983]
Epoch [105/120    avg_loss:0.035, val_acc:0.983]
Epoch [106/120    avg_loss:0.030, val_acc:0.983]
Epoch [107/120    avg_loss:0.036, val_acc:0.983]
Epoch [108/120    avg_loss:0.026, val_acc:0.983]
Epoch [109/120    avg_loss:0.031, val_acc:0.983]
Epoch [110/120    avg_loss:0.024, val_acc:0.983]
Epoch [111/120    avg_loss:0.029, val_acc:0.983]
Epoch [112/120    avg_loss:0.042, val_acc:0.983]
Epoch [113/120    avg_loss:0.027, val_acc:0.983]
Epoch [114/120    avg_loss:0.025, val_acc:0.983]
Epoch [115/120    avg_loss:0.036, val_acc:0.983]
Epoch [116/120    avg_loss:0.029, val_acc:0.983]
Epoch [117/120    avg_loss:0.031, val_acc:0.983]
Epoch [118/120    avg_loss:0.026, val_acc:0.983]
Epoch [119/120    avg_loss:0.025, val_acc:0.983]
Epoch [120/120    avg_loss:0.032, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   1   0   0   0   0   0   0   4   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.9977221  1.         0.95689655 0.94202899
 1.         0.99465241 1.         1.         1.         0.98820446
 0.98557159 1.        ]

Kappa:
0.9928780181675834
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde70e97860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.350, val_acc:0.546]
Epoch [2/120    avg_loss:1.887, val_acc:0.567]
Epoch [3/120    avg_loss:1.563, val_acc:0.696]
Epoch [4/120    avg_loss:1.277, val_acc:0.787]
Epoch [5/120    avg_loss:1.048, val_acc:0.827]
Epoch [6/120    avg_loss:0.879, val_acc:0.838]
Epoch [7/120    avg_loss:0.768, val_acc:0.871]
Epoch [8/120    avg_loss:0.708, val_acc:0.846]
Epoch [9/120    avg_loss:0.612, val_acc:0.875]
Epoch [10/120    avg_loss:0.619, val_acc:0.890]
Epoch [11/120    avg_loss:0.553, val_acc:0.908]
Epoch [12/120    avg_loss:0.504, val_acc:0.912]
Epoch [13/120    avg_loss:0.461, val_acc:0.906]
Epoch [14/120    avg_loss:0.428, val_acc:0.890]
Epoch [15/120    avg_loss:0.437, val_acc:0.929]
Epoch [16/120    avg_loss:0.390, val_acc:0.925]
Epoch [17/120    avg_loss:0.346, val_acc:0.933]
Epoch [18/120    avg_loss:0.338, val_acc:0.931]
Epoch [19/120    avg_loss:0.335, val_acc:0.935]
Epoch [20/120    avg_loss:0.328, val_acc:0.944]
Epoch [21/120    avg_loss:0.268, val_acc:0.942]
Epoch [22/120    avg_loss:0.259, val_acc:0.935]
Epoch [23/120    avg_loss:0.255, val_acc:0.946]
Epoch [24/120    avg_loss:0.230, val_acc:0.960]
Epoch [25/120    avg_loss:0.230, val_acc:0.942]
Epoch [26/120    avg_loss:0.232, val_acc:0.925]
Epoch [27/120    avg_loss:0.201, val_acc:0.960]
Epoch [28/120    avg_loss:0.215, val_acc:0.917]
Epoch [29/120    avg_loss:0.223, val_acc:0.919]
Epoch [30/120    avg_loss:0.204, val_acc:0.898]
Epoch [31/120    avg_loss:0.232, val_acc:0.952]
Epoch [32/120    avg_loss:0.201, val_acc:0.933]
Epoch [33/120    avg_loss:0.262, val_acc:0.946]
Epoch [34/120    avg_loss:0.225, val_acc:0.950]
Epoch [35/120    avg_loss:0.167, val_acc:0.969]
Epoch [36/120    avg_loss:0.190, val_acc:0.960]
Epoch [37/120    avg_loss:0.176, val_acc:0.952]
Epoch [38/120    avg_loss:0.194, val_acc:0.967]
Epoch [39/120    avg_loss:0.168, val_acc:0.965]
Epoch [40/120    avg_loss:0.143, val_acc:0.944]
Epoch [41/120    avg_loss:0.160, val_acc:0.977]
Epoch [42/120    avg_loss:0.169, val_acc:0.925]
Epoch [43/120    avg_loss:0.226, val_acc:0.940]
Epoch [44/120    avg_loss:0.202, val_acc:0.971]
Epoch [45/120    avg_loss:0.171, val_acc:0.965]
Epoch [46/120    avg_loss:0.125, val_acc:0.977]
Epoch [47/120    avg_loss:0.101, val_acc:0.973]
Epoch [48/120    avg_loss:0.098, val_acc:0.975]
Epoch [49/120    avg_loss:0.083, val_acc:0.975]
Epoch [50/120    avg_loss:0.101, val_acc:0.983]
Epoch [51/120    avg_loss:0.087, val_acc:0.973]
Epoch [52/120    avg_loss:0.074, val_acc:0.985]
Epoch [53/120    avg_loss:0.063, val_acc:0.981]
Epoch [54/120    avg_loss:0.105, val_acc:0.977]
Epoch [55/120    avg_loss:0.090, val_acc:0.967]
Epoch [56/120    avg_loss:0.108, val_acc:0.954]
Epoch [57/120    avg_loss:0.185, val_acc:0.950]
Epoch [58/120    avg_loss:0.139, val_acc:0.971]
Epoch [59/120    avg_loss:0.065, val_acc:0.977]
Epoch [60/120    avg_loss:0.089, val_acc:0.988]
Epoch [61/120    avg_loss:0.069, val_acc:0.977]
Epoch [62/120    avg_loss:0.112, val_acc:0.977]
Epoch [63/120    avg_loss:0.120, val_acc:0.975]
Epoch [64/120    avg_loss:0.072, val_acc:0.988]
Epoch [65/120    avg_loss:0.060, val_acc:0.977]
Epoch [66/120    avg_loss:0.079, val_acc:0.967]
Epoch [67/120    avg_loss:0.052, val_acc:0.983]
Epoch [68/120    avg_loss:0.056, val_acc:0.985]
Epoch [69/120    avg_loss:0.042, val_acc:0.975]
Epoch [70/120    avg_loss:0.071, val_acc:0.985]
Epoch [71/120    avg_loss:0.046, val_acc:0.988]
Epoch [72/120    avg_loss:0.030, val_acc:0.994]
Epoch [73/120    avg_loss:0.048, val_acc:0.973]
Epoch [74/120    avg_loss:0.030, val_acc:0.992]
Epoch [75/120    avg_loss:0.022, val_acc:0.994]
Epoch [76/120    avg_loss:0.023, val_acc:0.975]
Epoch [77/120    avg_loss:0.033, val_acc:0.988]
Epoch [78/120    avg_loss:0.047, val_acc:0.983]
Epoch [79/120    avg_loss:0.045, val_acc:0.988]
Epoch [80/120    avg_loss:0.029, val_acc:0.990]
Epoch [81/120    avg_loss:0.039, val_acc:0.990]
Epoch [82/120    avg_loss:0.031, val_acc:0.981]
Epoch [83/120    avg_loss:0.033, val_acc:0.979]
Epoch [84/120    avg_loss:0.047, val_acc:0.990]
Epoch [85/120    avg_loss:0.029, val_acc:0.981]
Epoch [86/120    avg_loss:0.022, val_acc:0.994]
Epoch [87/120    avg_loss:0.015, val_acc:0.990]
Epoch [88/120    avg_loss:0.030, val_acc:0.988]
Epoch [89/120    avg_loss:0.062, val_acc:0.981]
Epoch [90/120    avg_loss:0.017, val_acc:0.990]
Epoch [91/120    avg_loss:0.017, val_acc:0.990]
Epoch [92/120    avg_loss:0.022, val_acc:0.996]
Epoch [93/120    avg_loss:0.013, val_acc:0.992]
Epoch [94/120    avg_loss:0.018, val_acc:0.996]
Epoch [95/120    avg_loss:0.014, val_acc:0.992]
Epoch [96/120    avg_loss:0.021, val_acc:0.996]
Epoch [97/120    avg_loss:0.026, val_acc:0.994]
Epoch [98/120    avg_loss:0.031, val_acc:0.990]
Epoch [99/120    avg_loss:0.077, val_acc:0.965]
Epoch [100/120    avg_loss:0.066, val_acc:0.985]
Epoch [101/120    avg_loss:0.064, val_acc:0.990]
Epoch [102/120    avg_loss:0.024, val_acc:0.996]
Epoch [103/120    avg_loss:0.028, val_acc:0.998]
Epoch [104/120    avg_loss:0.014, val_acc:0.988]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.049, val_acc:0.998]
Epoch [107/120    avg_loss:0.045, val_acc:0.990]
Epoch [108/120    avg_loss:0.031, val_acc:0.994]
Epoch [109/120    avg_loss:0.018, val_acc:0.994]
Epoch [110/120    avg_loss:0.029, val_acc:0.990]
Epoch [111/120    avg_loss:0.026, val_acc:0.996]
Epoch [112/120    avg_loss:0.040, val_acc:0.977]
Epoch [113/120    avg_loss:0.025, val_acc:0.983]
Epoch [114/120    avg_loss:0.028, val_acc:0.992]
Epoch [115/120    avg_loss:0.012, val_acc:0.996]
Epoch [116/120    avg_loss:0.009, val_acc:0.996]
Epoch [117/120    avg_loss:0.008, val_acc:0.998]
Epoch [118/120    avg_loss:0.009, val_acc:1.000]
Epoch [119/120    avg_loss:0.014, val_acc:0.998]
Epoch [120/120    avg_loss:0.018, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 641   0   0   0   0  44   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   0   0   1   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 0.9668175  1.         1.         0.97391304 0.96113074
 0.90350877 1.         1.         1.         1.         0.98691099
 0.9877369  1.        ]

Kappa:
0.9843494874626522
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb54e66d7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.389, val_acc:0.521]
Epoch [2/120    avg_loss:1.869, val_acc:0.642]
Epoch [3/120    avg_loss:1.541, val_acc:0.715]
Epoch [4/120    avg_loss:1.268, val_acc:0.723]
Epoch [5/120    avg_loss:1.039, val_acc:0.748]
Epoch [6/120    avg_loss:0.849, val_acc:0.812]
Epoch [7/120    avg_loss:0.782, val_acc:0.825]
Epoch [8/120    avg_loss:0.683, val_acc:0.902]
Epoch [9/120    avg_loss:0.540, val_acc:0.877]
Epoch [10/120    avg_loss:0.506, val_acc:0.910]
Epoch [11/120    avg_loss:0.460, val_acc:0.881]
Epoch [12/120    avg_loss:0.437, val_acc:0.931]
Epoch [13/120    avg_loss:0.397, val_acc:0.902]
Epoch [14/120    avg_loss:0.407, val_acc:0.929]
Epoch [15/120    avg_loss:0.328, val_acc:0.917]
Epoch [16/120    avg_loss:0.295, val_acc:0.935]
Epoch [17/120    avg_loss:0.278, val_acc:0.950]
Epoch [18/120    avg_loss:0.294, val_acc:0.919]
Epoch [19/120    avg_loss:0.264, val_acc:0.863]
Epoch [20/120    avg_loss:0.311, val_acc:0.917]
Epoch [21/120    avg_loss:0.334, val_acc:0.944]
Epoch [22/120    avg_loss:0.300, val_acc:0.946]
Epoch [23/120    avg_loss:0.327, val_acc:0.940]
Epoch [24/120    avg_loss:0.305, val_acc:0.967]
Epoch [25/120    avg_loss:0.308, val_acc:0.923]
Epoch [26/120    avg_loss:0.247, val_acc:0.954]
Epoch [27/120    avg_loss:0.180, val_acc:0.933]
Epoch [28/120    avg_loss:0.227, val_acc:0.915]
Epoch [29/120    avg_loss:0.195, val_acc:0.954]
Epoch [30/120    avg_loss:0.179, val_acc:0.956]
Epoch [31/120    avg_loss:0.160, val_acc:0.954]
Epoch [32/120    avg_loss:0.138, val_acc:0.971]
Epoch [33/120    avg_loss:0.119, val_acc:0.975]
Epoch [34/120    avg_loss:0.177, val_acc:0.973]
Epoch [35/120    avg_loss:0.203, val_acc:0.963]
Epoch [36/120    avg_loss:0.166, val_acc:0.960]
Epoch [37/120    avg_loss:0.189, val_acc:0.958]
Epoch [38/120    avg_loss:0.150, val_acc:0.975]
Epoch [39/120    avg_loss:0.146, val_acc:0.969]
Epoch [40/120    avg_loss:0.124, val_acc:0.973]
Epoch [41/120    avg_loss:0.101, val_acc:0.981]
Epoch [42/120    avg_loss:0.078, val_acc:0.981]
Epoch [43/120    avg_loss:0.082, val_acc:0.975]
Epoch [44/120    avg_loss:0.104, val_acc:0.975]
Epoch [45/120    avg_loss:0.071, val_acc:0.973]
Epoch [46/120    avg_loss:0.078, val_acc:0.979]
Epoch [47/120    avg_loss:0.106, val_acc:0.977]
Epoch [48/120    avg_loss:0.071, val_acc:0.973]
Epoch [49/120    avg_loss:0.050, val_acc:0.988]
Epoch [50/120    avg_loss:0.062, val_acc:0.983]
Epoch [51/120    avg_loss:0.050, val_acc:0.979]
Epoch [52/120    avg_loss:0.081, val_acc:0.971]
Epoch [53/120    avg_loss:0.085, val_acc:0.981]
Epoch [54/120    avg_loss:0.083, val_acc:0.979]
Epoch [55/120    avg_loss:0.043, val_acc:0.979]
Epoch [56/120    avg_loss:0.035, val_acc:0.977]
Epoch [57/120    avg_loss:0.039, val_acc:0.981]
Epoch [58/120    avg_loss:0.084, val_acc:0.988]
Epoch [59/120    avg_loss:0.063, val_acc:0.973]
Epoch [60/120    avg_loss:0.097, val_acc:0.975]
Epoch [61/120    avg_loss:0.077, val_acc:0.952]
Epoch [62/120    avg_loss:0.078, val_acc:0.988]
Epoch [63/120    avg_loss:0.090, val_acc:0.983]
Epoch [64/120    avg_loss:0.077, val_acc:0.967]
Epoch [65/120    avg_loss:0.087, val_acc:0.973]
Epoch [66/120    avg_loss:0.043, val_acc:0.983]
Epoch [67/120    avg_loss:0.054, val_acc:0.971]
Epoch [68/120    avg_loss:0.039, val_acc:0.990]
Epoch [69/120    avg_loss:0.042, val_acc:0.973]
Epoch [70/120    avg_loss:0.035, val_acc:0.985]
Epoch [71/120    avg_loss:0.027, val_acc:0.975]
Epoch [72/120    avg_loss:0.033, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.985]
Epoch [74/120    avg_loss:0.027, val_acc:0.985]
Epoch [75/120    avg_loss:0.028, val_acc:0.994]
Epoch [76/120    avg_loss:0.017, val_acc:0.979]
Epoch [77/120    avg_loss:0.016, val_acc:0.990]
Epoch [78/120    avg_loss:0.023, val_acc:0.981]
Epoch [79/120    avg_loss:0.078, val_acc:0.975]
Epoch [80/120    avg_loss:0.121, val_acc:0.979]
Epoch [81/120    avg_loss:0.057, val_acc:0.979]
Epoch [82/120    avg_loss:0.053, val_acc:0.979]
Epoch [83/120    avg_loss:0.096, val_acc:0.965]
Epoch [84/120    avg_loss:0.058, val_acc:0.977]
Epoch [85/120    avg_loss:0.062, val_acc:0.988]
Epoch [86/120    avg_loss:0.025, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.988]
Epoch [88/120    avg_loss:0.016, val_acc:0.990]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.014, val_acc:0.990]
Epoch [91/120    avg_loss:0.013, val_acc:0.990]
Epoch [92/120    avg_loss:0.015, val_acc:0.990]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.990]
Epoch [96/120    avg_loss:0.017, val_acc:0.990]
Epoch [97/120    avg_loss:0.009, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.014, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.012, val_acc:0.988]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.013, val_acc:0.988]
Epoch [105/120    avg_loss:0.014, val_acc:0.988]
Epoch [106/120    avg_loss:0.017, val_acc:0.990]
Epoch [107/120    avg_loss:0.017, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.009, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.016, val_acc:0.990]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.70149253731343

F1 scores:
[       nan 0.99853801 0.9977221  1.         0.97747748 0.96666667
 0.99516908 0.99465241 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.996676950440557
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fafb05ff748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.352, val_acc:0.523]
Epoch [2/120    avg_loss:1.868, val_acc:0.627]
Epoch [3/120    avg_loss:1.525, val_acc:0.685]
Epoch [4/120    avg_loss:1.257, val_acc:0.750]
Epoch [5/120    avg_loss:1.047, val_acc:0.760]
Epoch [6/120    avg_loss:0.851, val_acc:0.810]
Epoch [7/120    avg_loss:0.749, val_acc:0.798]
Epoch [8/120    avg_loss:0.722, val_acc:0.810]
Epoch [9/120    avg_loss:0.664, val_acc:0.852]
Epoch [10/120    avg_loss:0.614, val_acc:0.890]
Epoch [11/120    avg_loss:0.472, val_acc:0.890]
Epoch [12/120    avg_loss:0.474, val_acc:0.904]
Epoch [13/120    avg_loss:0.530, val_acc:0.906]
Epoch [14/120    avg_loss:0.492, val_acc:0.904]
Epoch [15/120    avg_loss:0.494, val_acc:0.900]
Epoch [16/120    avg_loss:0.373, val_acc:0.929]
Epoch [17/120    avg_loss:0.390, val_acc:0.917]
Epoch [18/120    avg_loss:0.357, val_acc:0.942]
Epoch [19/120    avg_loss:0.339, val_acc:0.906]
Epoch [20/120    avg_loss:0.398, val_acc:0.958]
Epoch [21/120    avg_loss:0.295, val_acc:0.935]
Epoch [22/120    avg_loss:0.295, val_acc:0.933]
Epoch [23/120    avg_loss:0.236, val_acc:0.958]
Epoch [24/120    avg_loss:0.202, val_acc:0.958]
Epoch [25/120    avg_loss:0.229, val_acc:0.981]
Epoch [26/120    avg_loss:0.236, val_acc:0.935]
Epoch [27/120    avg_loss:0.228, val_acc:0.954]
Epoch [28/120    avg_loss:0.210, val_acc:0.981]
Epoch [29/120    avg_loss:0.188, val_acc:0.973]
Epoch [30/120    avg_loss:0.205, val_acc:0.971]
Epoch [31/120    avg_loss:0.199, val_acc:0.975]
Epoch [32/120    avg_loss:0.163, val_acc:0.977]
Epoch [33/120    avg_loss:0.176, val_acc:0.954]
Epoch [34/120    avg_loss:0.200, val_acc:0.979]
Epoch [35/120    avg_loss:0.119, val_acc:0.967]
Epoch [36/120    avg_loss:0.185, val_acc:0.983]
Epoch [37/120    avg_loss:0.152, val_acc:0.975]
Epoch [38/120    avg_loss:0.125, val_acc:0.983]
Epoch [39/120    avg_loss:0.096, val_acc:0.988]
Epoch [40/120    avg_loss:0.105, val_acc:0.975]
Epoch [41/120    avg_loss:0.108, val_acc:0.983]
Epoch [42/120    avg_loss:0.087, val_acc:0.981]
Epoch [43/120    avg_loss:0.116, val_acc:0.992]
Epoch [44/120    avg_loss:0.089, val_acc:0.981]
Epoch [45/120    avg_loss:0.081, val_acc:0.990]
Epoch [46/120    avg_loss:0.123, val_acc:0.988]
Epoch [47/120    avg_loss:0.079, val_acc:0.988]
Epoch [48/120    avg_loss:0.098, val_acc:0.960]
Epoch [49/120    avg_loss:0.142, val_acc:0.948]
Epoch [50/120    avg_loss:0.146, val_acc:0.973]
Epoch [51/120    avg_loss:0.065, val_acc:0.981]
Epoch [52/120    avg_loss:0.080, val_acc:0.983]
Epoch [53/120    avg_loss:0.099, val_acc:0.973]
Epoch [54/120    avg_loss:0.089, val_acc:0.977]
Epoch [55/120    avg_loss:0.149, val_acc:0.971]
Epoch [56/120    avg_loss:0.084, val_acc:0.988]
Epoch [57/120    avg_loss:0.071, val_acc:0.992]
Epoch [58/120    avg_loss:0.047, val_acc:0.990]
Epoch [59/120    avg_loss:0.043, val_acc:0.992]
Epoch [60/120    avg_loss:0.044, val_acc:0.992]
Epoch [61/120    avg_loss:0.045, val_acc:0.994]
Epoch [62/120    avg_loss:0.030, val_acc:0.994]
Epoch [63/120    avg_loss:0.037, val_acc:0.994]
Epoch [64/120    avg_loss:0.032, val_acc:0.994]
Epoch [65/120    avg_loss:0.027, val_acc:0.994]
Epoch [66/120    avg_loss:0.033, val_acc:0.994]
Epoch [67/120    avg_loss:0.034, val_acc:0.994]
Epoch [68/120    avg_loss:0.032, val_acc:0.996]
Epoch [69/120    avg_loss:0.035, val_acc:0.994]
Epoch [70/120    avg_loss:0.032, val_acc:0.996]
Epoch [71/120    avg_loss:0.035, val_acc:0.996]
Epoch [72/120    avg_loss:0.029, val_acc:0.994]
Epoch [73/120    avg_loss:0.032, val_acc:0.996]
Epoch [74/120    avg_loss:0.038, val_acc:0.994]
Epoch [75/120    avg_loss:0.025, val_acc:0.994]
Epoch [76/120    avg_loss:0.030, val_acc:0.996]
Epoch [77/120    avg_loss:0.024, val_acc:0.998]
Epoch [78/120    avg_loss:0.031, val_acc:0.998]
Epoch [79/120    avg_loss:0.034, val_acc:0.996]
Epoch [80/120    avg_loss:0.025, val_acc:0.996]
Epoch [81/120    avg_loss:0.032, val_acc:0.996]
Epoch [82/120    avg_loss:0.042, val_acc:0.994]
Epoch [83/120    avg_loss:0.029, val_acc:0.996]
Epoch [84/120    avg_loss:0.024, val_acc:0.996]
Epoch [85/120    avg_loss:0.024, val_acc:0.996]
Epoch [86/120    avg_loss:0.030, val_acc:0.996]
Epoch [87/120    avg_loss:0.024, val_acc:0.996]
Epoch [88/120    avg_loss:0.033, val_acc:0.996]
Epoch [89/120    avg_loss:0.027, val_acc:0.998]
Epoch [90/120    avg_loss:0.023, val_acc:0.998]
Epoch [91/120    avg_loss:0.024, val_acc:0.996]
Epoch [92/120    avg_loss:0.031, val_acc:0.996]
Epoch [93/120    avg_loss:0.023, val_acc:0.998]
Epoch [94/120    avg_loss:0.028, val_acc:0.998]
Epoch [95/120    avg_loss:0.024, val_acc:0.998]
Epoch [96/120    avg_loss:0.019, val_acc:0.998]
Epoch [97/120    avg_loss:0.018, val_acc:0.998]
Epoch [98/120    avg_loss:0.025, val_acc:0.998]
Epoch [99/120    avg_loss:0.018, val_acc:0.998]
Epoch [100/120    avg_loss:0.020, val_acc:0.998]
Epoch [101/120    avg_loss:0.030, val_acc:0.998]
Epoch [102/120    avg_loss:0.023, val_acc:0.998]
Epoch [103/120    avg_loss:0.020, val_acc:0.998]
Epoch [104/120    avg_loss:0.018, val_acc:0.998]
Epoch [105/120    avg_loss:0.022, val_acc:0.998]
Epoch [106/120    avg_loss:0.029, val_acc:0.998]
Epoch [107/120    avg_loss:0.021, val_acc:0.996]
Epoch [108/120    avg_loss:0.028, val_acc:0.996]
Epoch [109/120    avg_loss:0.021, val_acc:0.996]
Epoch [110/120    avg_loss:0.023, val_acc:0.996]
Epoch [111/120    avg_loss:0.023, val_acc:0.998]
Epoch [112/120    avg_loss:0.022, val_acc:0.996]
Epoch [113/120    avg_loss:0.018, val_acc:0.996]
Epoch [114/120    avg_loss:0.018, val_acc:0.998]
Epoch [115/120    avg_loss:0.018, val_acc:0.998]
Epoch [116/120    avg_loss:0.022, val_acc:0.998]
Epoch [117/120    avg_loss:0.020, val_acc:0.998]
Epoch [118/120    avg_loss:0.020, val_acc:0.998]
Epoch [119/120    avg_loss:0.022, val_acc:0.998]
Epoch [120/120    avg_loss:0.017, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99486427 0.99095023 1.         0.97285068 0.9602649
 0.98329356 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9945412866623042
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4073f0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.333, val_acc:0.494]
Epoch [2/120    avg_loss:1.927, val_acc:0.652]
Epoch [3/120    avg_loss:1.595, val_acc:0.654]
Epoch [4/120    avg_loss:1.271, val_acc:0.706]
Epoch [5/120    avg_loss:1.061, val_acc:0.781]
Epoch [6/120    avg_loss:0.897, val_acc:0.765]
Epoch [7/120    avg_loss:0.809, val_acc:0.758]
Epoch [8/120    avg_loss:0.752, val_acc:0.783]
Epoch [9/120    avg_loss:0.655, val_acc:0.819]
Epoch [10/120    avg_loss:0.584, val_acc:0.871]
Epoch [11/120    avg_loss:0.551, val_acc:0.842]
Epoch [12/120    avg_loss:0.525, val_acc:0.825]
Epoch [13/120    avg_loss:0.480, val_acc:0.877]
Epoch [14/120    avg_loss:0.418, val_acc:0.917]
Epoch [15/120    avg_loss:0.364, val_acc:0.944]
Epoch [16/120    avg_loss:0.380, val_acc:0.887]
Epoch [17/120    avg_loss:0.350, val_acc:0.938]
Epoch [18/120    avg_loss:0.347, val_acc:0.948]
Epoch [19/120    avg_loss:0.310, val_acc:0.935]
Epoch [20/120    avg_loss:0.302, val_acc:0.938]
Epoch [21/120    avg_loss:0.261, val_acc:0.967]
Epoch [22/120    avg_loss:0.266, val_acc:0.950]
Epoch [23/120    avg_loss:0.253, val_acc:0.960]
Epoch [24/120    avg_loss:0.210, val_acc:0.952]
Epoch [25/120    avg_loss:0.160, val_acc:0.967]
Epoch [26/120    avg_loss:0.212, val_acc:0.977]
Epoch [27/120    avg_loss:0.268, val_acc:0.906]
Epoch [28/120    avg_loss:0.260, val_acc:0.971]
Epoch [29/120    avg_loss:0.196, val_acc:0.963]
Epoch [30/120    avg_loss:0.185, val_acc:0.921]
Epoch [31/120    avg_loss:0.197, val_acc:0.967]
Epoch [32/120    avg_loss:0.172, val_acc:0.971]
Epoch [33/120    avg_loss:0.174, val_acc:0.956]
Epoch [34/120    avg_loss:0.159, val_acc:0.973]
Epoch [35/120    avg_loss:0.121, val_acc:0.983]
Epoch [36/120    avg_loss:0.137, val_acc:0.979]
Epoch [37/120    avg_loss:0.149, val_acc:0.975]
Epoch [38/120    avg_loss:0.167, val_acc:0.956]
Epoch [39/120    avg_loss:0.140, val_acc:0.977]
Epoch [40/120    avg_loss:0.110, val_acc:0.921]
Epoch [41/120    avg_loss:0.150, val_acc:0.983]
Epoch [42/120    avg_loss:0.113, val_acc:0.981]
Epoch [43/120    avg_loss:0.097, val_acc:0.979]
Epoch [44/120    avg_loss:0.103, val_acc:0.975]
Epoch [45/120    avg_loss:0.187, val_acc:0.975]
Epoch [46/120    avg_loss:0.118, val_acc:0.992]
Epoch [47/120    avg_loss:0.081, val_acc:0.975]
Epoch [48/120    avg_loss:0.104, val_acc:0.956]
Epoch [49/120    avg_loss:0.078, val_acc:0.979]
Epoch [50/120    avg_loss:0.091, val_acc:0.983]
Epoch [51/120    avg_loss:0.075, val_acc:0.994]
Epoch [52/120    avg_loss:0.083, val_acc:0.983]
Epoch [53/120    avg_loss:0.089, val_acc:0.973]
Epoch [54/120    avg_loss:0.071, val_acc:0.983]
Epoch [55/120    avg_loss:0.066, val_acc:0.994]
Epoch [56/120    avg_loss:0.089, val_acc:0.988]
Epoch [57/120    avg_loss:0.093, val_acc:0.973]
Epoch [58/120    avg_loss:0.071, val_acc:0.996]
Epoch [59/120    avg_loss:0.048, val_acc:0.983]
Epoch [60/120    avg_loss:0.046, val_acc:0.992]
Epoch [61/120    avg_loss:0.048, val_acc:0.988]
Epoch [62/120    avg_loss:0.081, val_acc:0.973]
Epoch [63/120    avg_loss:0.077, val_acc:0.983]
Epoch [64/120    avg_loss:0.042, val_acc:0.996]
Epoch [65/120    avg_loss:0.037, val_acc:0.990]
Epoch [66/120    avg_loss:0.038, val_acc:0.994]
Epoch [67/120    avg_loss:0.034, val_acc:0.990]
Epoch [68/120    avg_loss:0.036, val_acc:0.988]
Epoch [69/120    avg_loss:0.059, val_acc:0.988]
Epoch [70/120    avg_loss:0.057, val_acc:0.992]
Epoch [71/120    avg_loss:0.030, val_acc:0.992]
Epoch [72/120    avg_loss:0.032, val_acc:0.994]
Epoch [73/120    avg_loss:0.035, val_acc:0.994]
Epoch [74/120    avg_loss:0.040, val_acc:0.996]
Epoch [75/120    avg_loss:0.046, val_acc:0.996]
Epoch [76/120    avg_loss:0.054, val_acc:1.000]
Epoch [77/120    avg_loss:0.076, val_acc:0.992]
Epoch [78/120    avg_loss:0.042, val_acc:0.990]
Epoch [79/120    avg_loss:0.055, val_acc:0.994]
Epoch [80/120    avg_loss:0.058, val_acc:0.994]
Epoch [81/120    avg_loss:0.033, val_acc:0.994]
Epoch [82/120    avg_loss:0.017, val_acc:0.994]
Epoch [83/120    avg_loss:0.036, val_acc:0.988]
Epoch [84/120    avg_loss:0.036, val_acc:0.992]
Epoch [85/120    avg_loss:0.028, val_acc:0.992]
Epoch [86/120    avg_loss:0.029, val_acc:0.994]
Epoch [87/120    avg_loss:0.018, val_acc:0.990]
Epoch [88/120    avg_loss:0.023, val_acc:0.994]
Epoch [89/120    avg_loss:0.018, val_acc:0.998]
Epoch [90/120    avg_loss:0.019, val_acc:0.998]
Epoch [91/120    avg_loss:0.013, val_acc:0.996]
Epoch [92/120    avg_loss:0.020, val_acc:0.996]
Epoch [93/120    avg_loss:0.020, val_acc:0.996]
Epoch [94/120    avg_loss:0.011, val_acc:0.998]
Epoch [95/120    avg_loss:0.014, val_acc:0.996]
Epoch [96/120    avg_loss:0.013, val_acc:0.996]
Epoch [97/120    avg_loss:0.012, val_acc:0.996]
Epoch [98/120    avg_loss:0.015, val_acc:0.996]
Epoch [99/120    avg_loss:0.016, val_acc:0.996]
Epoch [100/120    avg_loss:0.013, val_acc:0.996]
Epoch [101/120    avg_loss:0.010, val_acc:0.996]
Epoch [102/120    avg_loss:0.010, val_acc:0.996]
Epoch [103/120    avg_loss:0.010, val_acc:0.996]
Epoch [104/120    avg_loss:0.009, val_acc:0.996]
Epoch [105/120    avg_loss:0.012, val_acc:0.996]
Epoch [106/120    avg_loss:0.009, val_acc:0.996]
Epoch [107/120    avg_loss:0.017, val_acc:0.996]
Epoch [108/120    avg_loss:0.015, val_acc:0.996]
Epoch [109/120    avg_loss:0.016, val_acc:0.996]
Epoch [110/120    avg_loss:0.010, val_acc:0.996]
Epoch [111/120    avg_loss:0.009, val_acc:0.996]
Epoch [112/120    avg_loss:0.011, val_acc:0.996]
Epoch [113/120    avg_loss:0.008, val_acc:0.996]
Epoch [114/120    avg_loss:0.012, val_acc:0.996]
Epoch [115/120    avg_loss:0.011, val_acc:0.996]
Epoch [116/120    avg_loss:0.010, val_acc:0.996]
Epoch [117/120    avg_loss:0.008, val_acc:0.996]
Epoch [118/120    avg_loss:0.011, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.014, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  11   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99707174 0.99319728 1.         0.96583144 0.96345515
 0.99038462 0.98378378 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9947780147639764
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde4c5e2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.414, val_acc:0.434]
Epoch [2/120    avg_loss:1.986, val_acc:0.533]
Epoch [3/120    avg_loss:1.708, val_acc:0.699]
Epoch [4/120    avg_loss:1.493, val_acc:0.654]
Epoch [5/120    avg_loss:1.264, val_acc:0.754]
Epoch [6/120    avg_loss:1.146, val_acc:0.779]
Epoch [7/120    avg_loss:1.011, val_acc:0.779]
Epoch [8/120    avg_loss:0.894, val_acc:0.818]
Epoch [9/120    avg_loss:0.860, val_acc:0.803]
Epoch [10/120    avg_loss:0.789, val_acc:0.793]
Epoch [11/120    avg_loss:0.691, val_acc:0.869]
Epoch [12/120    avg_loss:0.661, val_acc:0.873]
Epoch [13/120    avg_loss:0.644, val_acc:0.848]
Epoch [14/120    avg_loss:0.621, val_acc:0.873]
Epoch [15/120    avg_loss:0.530, val_acc:0.885]
Epoch [16/120    avg_loss:0.466, val_acc:0.871]
Epoch [17/120    avg_loss:0.429, val_acc:0.855]
Epoch [18/120    avg_loss:0.428, val_acc:0.887]
Epoch [19/120    avg_loss:0.427, val_acc:0.898]
Epoch [20/120    avg_loss:0.370, val_acc:0.889]
Epoch [21/120    avg_loss:0.418, val_acc:0.883]
Epoch [22/120    avg_loss:0.421, val_acc:0.904]
Epoch [23/120    avg_loss:0.362, val_acc:0.873]
Epoch [24/120    avg_loss:0.325, val_acc:0.924]
Epoch [25/120    avg_loss:0.312, val_acc:0.920]
Epoch [26/120    avg_loss:0.436, val_acc:0.881]
Epoch [27/120    avg_loss:0.356, val_acc:0.912]
Epoch [28/120    avg_loss:0.296, val_acc:0.938]
Epoch [29/120    avg_loss:0.301, val_acc:0.910]
Epoch [30/120    avg_loss:0.369, val_acc:0.922]
Epoch [31/120    avg_loss:0.316, val_acc:0.926]
Epoch [32/120    avg_loss:0.318, val_acc:0.904]
Epoch [33/120    avg_loss:0.250, val_acc:0.930]
Epoch [34/120    avg_loss:0.256, val_acc:0.936]
Epoch [35/120    avg_loss:0.266, val_acc:0.943]
Epoch [36/120    avg_loss:0.203, val_acc:0.928]
Epoch [37/120    avg_loss:0.285, val_acc:0.953]
Epoch [38/120    avg_loss:0.277, val_acc:0.926]
Epoch [39/120    avg_loss:0.242, val_acc:0.943]
Epoch [40/120    avg_loss:0.283, val_acc:0.947]
Epoch [41/120    avg_loss:0.251, val_acc:0.930]
Epoch [42/120    avg_loss:0.236, val_acc:0.951]
Epoch [43/120    avg_loss:0.201, val_acc:0.953]
Epoch [44/120    avg_loss:0.185, val_acc:0.961]
Epoch [45/120    avg_loss:0.181, val_acc:0.957]
Epoch [46/120    avg_loss:0.168, val_acc:0.975]
Epoch [47/120    avg_loss:0.185, val_acc:0.955]
Epoch [48/120    avg_loss:0.236, val_acc:0.939]
Epoch [49/120    avg_loss:0.173, val_acc:0.967]
Epoch [50/120    avg_loss:0.171, val_acc:0.961]
Epoch [51/120    avg_loss:0.191, val_acc:0.963]
Epoch [52/120    avg_loss:0.124, val_acc:0.971]
Epoch [53/120    avg_loss:0.141, val_acc:0.963]
Epoch [54/120    avg_loss:0.189, val_acc:0.963]
Epoch [55/120    avg_loss:0.138, val_acc:0.945]
Epoch [56/120    avg_loss:0.205, val_acc:0.961]
Epoch [57/120    avg_loss:0.138, val_acc:0.973]
Epoch [58/120    avg_loss:0.101, val_acc:0.973]
Epoch [59/120    avg_loss:0.119, val_acc:0.902]
Epoch [60/120    avg_loss:0.189, val_acc:0.961]
Epoch [61/120    avg_loss:0.073, val_acc:0.969]
Epoch [62/120    avg_loss:0.076, val_acc:0.969]
Epoch [63/120    avg_loss:0.070, val_acc:0.971]
Epoch [64/120    avg_loss:0.064, val_acc:0.971]
Epoch [65/120    avg_loss:0.057, val_acc:0.971]
Epoch [66/120    avg_loss:0.067, val_acc:0.971]
Epoch [67/120    avg_loss:0.068, val_acc:0.971]
Epoch [68/120    avg_loss:0.070, val_acc:0.973]
Epoch [69/120    avg_loss:0.061, val_acc:0.971]
Epoch [70/120    avg_loss:0.070, val_acc:0.977]
Epoch [71/120    avg_loss:0.060, val_acc:0.975]
Epoch [72/120    avg_loss:0.074, val_acc:0.975]
Epoch [73/120    avg_loss:0.053, val_acc:0.977]
Epoch [74/120    avg_loss:0.067, val_acc:0.977]
Epoch [75/120    avg_loss:0.061, val_acc:0.975]
Epoch [76/120    avg_loss:0.059, val_acc:0.973]
Epoch [77/120    avg_loss:0.082, val_acc:0.975]
Epoch [78/120    avg_loss:0.074, val_acc:0.973]
Epoch [79/120    avg_loss:0.070, val_acc:0.973]
Epoch [80/120    avg_loss:0.054, val_acc:0.973]
Epoch [81/120    avg_loss:0.047, val_acc:0.971]
Epoch [82/120    avg_loss:0.059, val_acc:0.975]
Epoch [83/120    avg_loss:0.054, val_acc:0.977]
Epoch [84/120    avg_loss:0.064, val_acc:0.977]
Epoch [85/120    avg_loss:0.061, val_acc:0.979]
Epoch [86/120    avg_loss:0.051, val_acc:0.979]
Epoch [87/120    avg_loss:0.052, val_acc:0.975]
Epoch [88/120    avg_loss:0.055, val_acc:0.977]
Epoch [89/120    avg_loss:0.058, val_acc:0.977]
Epoch [90/120    avg_loss:0.054, val_acc:0.973]
Epoch [91/120    avg_loss:0.049, val_acc:0.977]
Epoch [92/120    avg_loss:0.048, val_acc:0.973]
Epoch [93/120    avg_loss:0.062, val_acc:0.971]
Epoch [94/120    avg_loss:0.065, val_acc:0.975]
Epoch [95/120    avg_loss:0.056, val_acc:0.977]
Epoch [96/120    avg_loss:0.066, val_acc:0.975]
Epoch [97/120    avg_loss:0.051, val_acc:0.975]
Epoch [98/120    avg_loss:0.047, val_acc:0.977]
Epoch [99/120    avg_loss:0.066, val_acc:0.975]
Epoch [100/120    avg_loss:0.044, val_acc:0.975]
Epoch [101/120    avg_loss:0.044, val_acc:0.975]
Epoch [102/120    avg_loss:0.056, val_acc:0.975]
Epoch [103/120    avg_loss:0.048, val_acc:0.975]
Epoch [104/120    avg_loss:0.054, val_acc:0.975]
Epoch [105/120    avg_loss:0.052, val_acc:0.975]
Epoch [106/120    avg_loss:0.049, val_acc:0.975]
Epoch [107/120    avg_loss:0.051, val_acc:0.975]
Epoch [108/120    avg_loss:0.049, val_acc:0.975]
Epoch [109/120    avg_loss:0.041, val_acc:0.975]
Epoch [110/120    avg_loss:0.048, val_acc:0.975]
Epoch [111/120    avg_loss:0.056, val_acc:0.975]
Epoch [112/120    avg_loss:0.059, val_acc:0.975]
Epoch [113/120    avg_loss:0.043, val_acc:0.977]
Epoch [114/120    avg_loss:0.055, val_acc:0.977]
Epoch [115/120    avg_loss:0.047, val_acc:0.977]
Epoch [116/120    avg_loss:0.048, val_acc:0.975]
Epoch [117/120    avg_loss:0.059, val_acc:0.975]
Epoch [118/120    avg_loss:0.051, val_acc:0.975]
Epoch [119/120    avg_loss:0.055, val_acc:0.975]
Epoch [120/120    avg_loss:0.052, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   0 223   3   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.52878464818762

F1 scores:
[       nan 1.         0.93424036 0.98454746 0.92473118 0.88652482
 1.         0.84324324 0.99614891 0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.983619178939248
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab18ea1780>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.418, val_acc:0.346]
Epoch [2/120    avg_loss:2.003, val_acc:0.557]
Epoch [3/120    avg_loss:1.705, val_acc:0.697]
Epoch [4/120    avg_loss:1.436, val_acc:0.775]
Epoch [5/120    avg_loss:1.229, val_acc:0.762]
Epoch [6/120    avg_loss:1.051, val_acc:0.836]
Epoch [7/120    avg_loss:0.943, val_acc:0.830]
Epoch [8/120    avg_loss:0.856, val_acc:0.857]
Epoch [9/120    avg_loss:0.734, val_acc:0.887]
Epoch [10/120    avg_loss:0.652, val_acc:0.885]
Epoch [11/120    avg_loss:0.612, val_acc:0.902]
Epoch [12/120    avg_loss:0.592, val_acc:0.902]
Epoch [13/120    avg_loss:0.564, val_acc:0.885]
Epoch [14/120    avg_loss:0.530, val_acc:0.908]
Epoch [15/120    avg_loss:0.470, val_acc:0.895]
Epoch [16/120    avg_loss:0.464, val_acc:0.887]
Epoch [17/120    avg_loss:0.450, val_acc:0.885]
Epoch [18/120    avg_loss:0.458, val_acc:0.891]
Epoch [19/120    avg_loss:0.448, val_acc:0.906]
Epoch [20/120    avg_loss:0.447, val_acc:0.900]
Epoch [21/120    avg_loss:0.378, val_acc:0.914]
Epoch [22/120    avg_loss:0.373, val_acc:0.893]
Epoch [23/120    avg_loss:0.386, val_acc:0.932]
Epoch [24/120    avg_loss:0.371, val_acc:0.902]
Epoch [25/120    avg_loss:0.338, val_acc:0.906]
Epoch [26/120    avg_loss:0.316, val_acc:0.936]
Epoch [27/120    avg_loss:0.308, val_acc:0.938]
Epoch [28/120    avg_loss:0.332, val_acc:0.922]
Epoch [29/120    avg_loss:0.293, val_acc:0.924]
Epoch [30/120    avg_loss:0.266, val_acc:0.906]
Epoch [31/120    avg_loss:0.274, val_acc:0.939]
Epoch [32/120    avg_loss:0.275, val_acc:0.902]
Epoch [33/120    avg_loss:0.337, val_acc:0.916]
Epoch [34/120    avg_loss:0.284, val_acc:0.922]
Epoch [35/120    avg_loss:0.287, val_acc:0.943]
Epoch [36/120    avg_loss:0.275, val_acc:0.936]
Epoch [37/120    avg_loss:0.213, val_acc:0.934]
Epoch [38/120    avg_loss:0.246, val_acc:0.947]
Epoch [39/120    avg_loss:0.227, val_acc:0.939]
Epoch [40/120    avg_loss:0.206, val_acc:0.945]
Epoch [41/120    avg_loss:0.197, val_acc:0.951]
Epoch [42/120    avg_loss:0.138, val_acc:0.963]
Epoch [43/120    avg_loss:0.174, val_acc:0.947]
Epoch [44/120    avg_loss:0.200, val_acc:0.961]
Epoch [45/120    avg_loss:0.150, val_acc:0.953]
Epoch [46/120    avg_loss:0.160, val_acc:0.961]
Epoch [47/120    avg_loss:0.134, val_acc:0.961]
Epoch [48/120    avg_loss:0.131, val_acc:0.959]
Epoch [49/120    avg_loss:0.174, val_acc:0.936]
Epoch [50/120    avg_loss:0.155, val_acc:0.965]
Epoch [51/120    avg_loss:0.149, val_acc:0.947]
Epoch [52/120    avg_loss:0.117, val_acc:0.959]
Epoch [53/120    avg_loss:0.132, val_acc:0.971]
Epoch [54/120    avg_loss:0.123, val_acc:0.973]
Epoch [55/120    avg_loss:0.095, val_acc:0.967]
Epoch [56/120    avg_loss:0.109, val_acc:0.967]
Epoch [57/120    avg_loss:0.116, val_acc:0.963]
Epoch [58/120    avg_loss:0.115, val_acc:0.963]
Epoch [59/120    avg_loss:0.110, val_acc:0.971]
Epoch [60/120    avg_loss:0.102, val_acc:0.967]
Epoch [61/120    avg_loss:0.092, val_acc:0.961]
Epoch [62/120    avg_loss:0.125, val_acc:0.975]
Epoch [63/120    avg_loss:0.133, val_acc:0.967]
Epoch [64/120    avg_loss:0.105, val_acc:0.975]
Epoch [65/120    avg_loss:0.099, val_acc:0.971]
Epoch [66/120    avg_loss:0.086, val_acc:0.965]
Epoch [67/120    avg_loss:0.067, val_acc:0.977]
Epoch [68/120    avg_loss:0.073, val_acc:0.973]
Epoch [69/120    avg_loss:0.079, val_acc:0.971]
Epoch [70/120    avg_loss:0.112, val_acc:0.959]
Epoch [71/120    avg_loss:0.157, val_acc:0.965]
Epoch [72/120    avg_loss:0.137, val_acc:0.963]
Epoch [73/120    avg_loss:0.157, val_acc:0.959]
Epoch [74/120    avg_loss:0.125, val_acc:0.971]
Epoch [75/120    avg_loss:0.087, val_acc:0.975]
Epoch [76/120    avg_loss:0.073, val_acc:0.965]
Epoch [77/120    avg_loss:0.067, val_acc:0.969]
Epoch [78/120    avg_loss:0.082, val_acc:0.975]
Epoch [79/120    avg_loss:0.083, val_acc:0.967]
Epoch [80/120    avg_loss:0.065, val_acc:0.975]
Epoch [81/120    avg_loss:0.056, val_acc:0.979]
Epoch [82/120    avg_loss:0.049, val_acc:0.980]
Epoch [83/120    avg_loss:0.034, val_acc:0.982]
Epoch [84/120    avg_loss:0.042, val_acc:0.982]
Epoch [85/120    avg_loss:0.038, val_acc:0.980]
Epoch [86/120    avg_loss:0.038, val_acc:0.980]
Epoch [87/120    avg_loss:0.034, val_acc:0.984]
Epoch [88/120    avg_loss:0.038, val_acc:0.982]
Epoch [89/120    avg_loss:0.027, val_acc:0.979]
Epoch [90/120    avg_loss:0.031, val_acc:0.980]
Epoch [91/120    avg_loss:0.033, val_acc:0.984]
Epoch [92/120    avg_loss:0.032, val_acc:0.980]
Epoch [93/120    avg_loss:0.039, val_acc:0.979]
Epoch [94/120    avg_loss:0.032, val_acc:0.984]
Epoch [95/120    avg_loss:0.027, val_acc:0.982]
Epoch [96/120    avg_loss:0.035, val_acc:0.982]
Epoch [97/120    avg_loss:0.029, val_acc:0.980]
Epoch [98/120    avg_loss:0.031, val_acc:0.979]
Epoch [99/120    avg_loss:0.031, val_acc:0.980]
Epoch [100/120    avg_loss:0.038, val_acc:0.982]
Epoch [101/120    avg_loss:0.041, val_acc:0.982]
Epoch [102/120    avg_loss:0.028, val_acc:0.980]
Epoch [103/120    avg_loss:0.026, val_acc:0.980]
Epoch [104/120    avg_loss:0.032, val_acc:0.980]
Epoch [105/120    avg_loss:0.034, val_acc:0.984]
Epoch [106/120    avg_loss:0.032, val_acc:0.984]
Epoch [107/120    avg_loss:0.025, val_acc:0.984]
Epoch [108/120    avg_loss:0.023, val_acc:0.984]
Epoch [109/120    avg_loss:0.022, val_acc:0.986]
Epoch [110/120    avg_loss:0.024, val_acc:0.980]
Epoch [111/120    avg_loss:0.037, val_acc:0.984]
Epoch [112/120    avg_loss:0.028, val_acc:0.988]
Epoch [113/120    avg_loss:0.027, val_acc:0.982]
Epoch [114/120    avg_loss:0.034, val_acc:0.982]
Epoch [115/120    avg_loss:0.023, val_acc:0.982]
Epoch [116/120    avg_loss:0.021, val_acc:0.982]
Epoch [117/120    avg_loss:0.025, val_acc:0.982]
Epoch [118/120    avg_loss:0.018, val_acc:0.982]
Epoch [119/120    avg_loss:0.029, val_acc:0.984]
Epoch [120/120    avg_loss:0.020, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 221   4   2   0   0   2   1   0   0   0   0]
 [  0   0   0   0 223   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 1.         0.95515695 0.98004435 0.92723493 0.88059701
 1.         0.91891892 0.99742931 0.99893276 1.         1.
 0.99334812 1.        ]

Kappa:
0.9857558358932587
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ea4ca47b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.452, val_acc:0.551]
Epoch [2/120    avg_loss:2.032, val_acc:0.600]
Epoch [3/120    avg_loss:1.714, val_acc:0.703]
Epoch [4/120    avg_loss:1.432, val_acc:0.666]
Epoch [5/120    avg_loss:1.167, val_acc:0.795]
Epoch [6/120    avg_loss:0.990, val_acc:0.840]
Epoch [7/120    avg_loss:0.979, val_acc:0.846]
Epoch [8/120    avg_loss:0.813, val_acc:0.861]
Epoch [9/120    avg_loss:0.694, val_acc:0.889]
Epoch [10/120    avg_loss:0.600, val_acc:0.875]
Epoch [11/120    avg_loss:0.601, val_acc:0.830]
Epoch [12/120    avg_loss:0.538, val_acc:0.900]
Epoch [13/120    avg_loss:0.579, val_acc:0.904]
Epoch [14/120    avg_loss:0.520, val_acc:0.861]
Epoch [15/120    avg_loss:0.473, val_acc:0.900]
Epoch [16/120    avg_loss:0.447, val_acc:0.902]
Epoch [17/120    avg_loss:0.438, val_acc:0.924]
Epoch [18/120    avg_loss:0.445, val_acc:0.881]
Epoch [19/120    avg_loss:0.476, val_acc:0.891]
Epoch [20/120    avg_loss:0.397, val_acc:0.914]
Epoch [21/120    avg_loss:0.395, val_acc:0.922]
Epoch [22/120    avg_loss:0.428, val_acc:0.898]
Epoch [23/120    avg_loss:0.367, val_acc:0.885]
Epoch [24/120    avg_loss:0.368, val_acc:0.922]
Epoch [25/120    avg_loss:0.342, val_acc:0.920]
Epoch [26/120    avg_loss:0.303, val_acc:0.926]
Epoch [27/120    avg_loss:0.334, val_acc:0.928]
Epoch [28/120    avg_loss:0.347, val_acc:0.902]
Epoch [29/120    avg_loss:0.313, val_acc:0.926]
Epoch [30/120    avg_loss:0.308, val_acc:0.920]
Epoch [31/120    avg_loss:0.299, val_acc:0.953]
Epoch [32/120    avg_loss:0.253, val_acc:0.947]
Epoch [33/120    avg_loss:0.250, val_acc:0.938]
Epoch [34/120    avg_loss:0.240, val_acc:0.943]
Epoch [35/120    avg_loss:0.192, val_acc:0.934]
Epoch [36/120    avg_loss:0.210, val_acc:0.951]
Epoch [37/120    avg_loss:0.260, val_acc:0.939]
Epoch [38/120    avg_loss:0.281, val_acc:0.926]
Epoch [39/120    avg_loss:0.219, val_acc:0.955]
Epoch [40/120    avg_loss:0.188, val_acc:0.957]
Epoch [41/120    avg_loss:0.241, val_acc:0.943]
Epoch [42/120    avg_loss:0.225, val_acc:0.953]
Epoch [43/120    avg_loss:0.192, val_acc:0.957]
Epoch [44/120    avg_loss:0.193, val_acc:0.959]
Epoch [45/120    avg_loss:0.170, val_acc:0.955]
Epoch [46/120    avg_loss:0.140, val_acc:0.953]
Epoch [47/120    avg_loss:0.120, val_acc:0.955]
Epoch [48/120    avg_loss:0.133, val_acc:0.955]
Epoch [49/120    avg_loss:0.159, val_acc:0.957]
Epoch [50/120    avg_loss:0.195, val_acc:0.943]
Epoch [51/120    avg_loss:0.210, val_acc:0.932]
Epoch [52/120    avg_loss:0.176, val_acc:0.947]
Epoch [53/120    avg_loss:0.150, val_acc:0.959]
Epoch [54/120    avg_loss:0.202, val_acc:0.953]
Epoch [55/120    avg_loss:0.158, val_acc:0.961]
Epoch [56/120    avg_loss:0.160, val_acc:0.961]
Epoch [57/120    avg_loss:0.176, val_acc:0.959]
Epoch [58/120    avg_loss:0.148, val_acc:0.963]
Epoch [59/120    avg_loss:0.137, val_acc:0.969]
Epoch [60/120    avg_loss:0.098, val_acc:0.967]
Epoch [61/120    avg_loss:0.101, val_acc:0.977]
Epoch [62/120    avg_loss:0.092, val_acc:0.980]
Epoch [63/120    avg_loss:0.080, val_acc:0.961]
Epoch [64/120    avg_loss:0.102, val_acc:0.959]
Epoch [65/120    avg_loss:0.123, val_acc:0.965]
Epoch [66/120    avg_loss:0.104, val_acc:0.971]
Epoch [67/120    avg_loss:0.087, val_acc:0.969]
Epoch [68/120    avg_loss:0.134, val_acc:0.977]
Epoch [69/120    avg_loss:0.090, val_acc:0.975]
Epoch [70/120    avg_loss:0.092, val_acc:0.971]
Epoch [71/120    avg_loss:0.081, val_acc:0.969]
Epoch [72/120    avg_loss:0.065, val_acc:0.969]
Epoch [73/120    avg_loss:0.093, val_acc:0.977]
Epoch [74/120    avg_loss:0.068, val_acc:0.982]
Epoch [75/120    avg_loss:0.093, val_acc:0.967]
Epoch [76/120    avg_loss:0.054, val_acc:0.973]
Epoch [77/120    avg_loss:0.047, val_acc:0.979]
Epoch [78/120    avg_loss:0.064, val_acc:0.980]
Epoch [79/120    avg_loss:0.116, val_acc:0.965]
Epoch [80/120    avg_loss:0.123, val_acc:0.980]
Epoch [81/120    avg_loss:0.071, val_acc:0.973]
Epoch [82/120    avg_loss:0.077, val_acc:0.977]
Epoch [83/120    avg_loss:0.082, val_acc:0.977]
Epoch [84/120    avg_loss:0.060, val_acc:0.977]
Epoch [85/120    avg_loss:0.092, val_acc:0.975]
Epoch [86/120    avg_loss:0.115, val_acc:0.971]
Epoch [87/120    avg_loss:0.096, val_acc:0.969]
Epoch [88/120    avg_loss:0.073, val_acc:0.973]
Epoch [89/120    avg_loss:0.059, val_acc:0.975]
Epoch [90/120    avg_loss:0.050, val_acc:0.977]
Epoch [91/120    avg_loss:0.050, val_acc:0.977]
Epoch [92/120    avg_loss:0.043, val_acc:0.977]
Epoch [93/120    avg_loss:0.036, val_acc:0.975]
Epoch [94/120    avg_loss:0.045, val_acc:0.975]
Epoch [95/120    avg_loss:0.042, val_acc:0.979]
Epoch [96/120    avg_loss:0.051, val_acc:0.982]
Epoch [97/120    avg_loss:0.040, val_acc:0.982]
Epoch [98/120    avg_loss:0.044, val_acc:0.982]
Epoch [99/120    avg_loss:0.040, val_acc:0.984]
Epoch [100/120    avg_loss:0.043, val_acc:0.984]
Epoch [101/120    avg_loss:0.039, val_acc:0.984]
Epoch [102/120    avg_loss:0.033, val_acc:0.984]
Epoch [103/120    avg_loss:0.046, val_acc:0.982]
Epoch [104/120    avg_loss:0.051, val_acc:0.982]
Epoch [105/120    avg_loss:0.035, val_acc:0.984]
Epoch [106/120    avg_loss:0.036, val_acc:0.982]
Epoch [107/120    avg_loss:0.032, val_acc:0.982]
Epoch [108/120    avg_loss:0.037, val_acc:0.980]
Epoch [109/120    avg_loss:0.033, val_acc:0.982]
Epoch [110/120    avg_loss:0.038, val_acc:0.984]
Epoch [111/120    avg_loss:0.033, val_acc:0.982]
Epoch [112/120    avg_loss:0.026, val_acc:0.980]
Epoch [113/120    avg_loss:0.040, val_acc:0.982]
Epoch [114/120    avg_loss:0.036, val_acc:0.982]
Epoch [115/120    avg_loss:0.030, val_acc:0.982]
Epoch [116/120    avg_loss:0.035, val_acc:0.982]
Epoch [117/120    avg_loss:0.034, val_acc:0.980]
Epoch [118/120    avg_loss:0.032, val_acc:0.984]
Epoch [119/120    avg_loss:0.035, val_acc:0.984]
Epoch [120/120    avg_loss:0.030, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   4   0   0   0   3   1   0   0   0   0]
 [  0   0   0   1 213  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0   0 831]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.96263736 0.98013245 0.95089286 0.94314381
 1.         0.92571429 0.99232737 0.99893276 1.         1.
 0.99556541 0.9981982 ]

Kappa:
0.9890808311039314
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1dcd2da7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.410, val_acc:0.391]
Epoch [2/120    avg_loss:1.996, val_acc:0.535]
Epoch [3/120    avg_loss:1.697, val_acc:0.691]
Epoch [4/120    avg_loss:1.406, val_acc:0.703]
Epoch [5/120    avg_loss:1.204, val_acc:0.775]
Epoch [6/120    avg_loss:1.103, val_acc:0.732]
Epoch [7/120    avg_loss:0.931, val_acc:0.811]
Epoch [8/120    avg_loss:0.861, val_acc:0.857]
Epoch [9/120    avg_loss:0.769, val_acc:0.889]
Epoch [10/120    avg_loss:0.706, val_acc:0.852]
Epoch [11/120    avg_loss:0.698, val_acc:0.883]
Epoch [12/120    avg_loss:0.638, val_acc:0.906]
Epoch [13/120    avg_loss:0.586, val_acc:0.887]
Epoch [14/120    avg_loss:0.526, val_acc:0.893]
Epoch [15/120    avg_loss:0.483, val_acc:0.895]
Epoch [16/120    avg_loss:0.456, val_acc:0.891]
Epoch [17/120    avg_loss:0.427, val_acc:0.934]
Epoch [18/120    avg_loss:0.442, val_acc:0.926]
Epoch [19/120    avg_loss:0.506, val_acc:0.898]
Epoch [20/120    avg_loss:0.371, val_acc:0.914]
Epoch [21/120    avg_loss:0.394, val_acc:0.898]
Epoch [22/120    avg_loss:0.391, val_acc:0.924]
Epoch [23/120    avg_loss:0.367, val_acc:0.926]
Epoch [24/120    avg_loss:0.323, val_acc:0.930]
Epoch [25/120    avg_loss:0.327, val_acc:0.924]
Epoch [26/120    avg_loss:0.384, val_acc:0.895]
Epoch [27/120    avg_loss:0.373, val_acc:0.896]
Epoch [28/120    avg_loss:0.310, val_acc:0.908]
Epoch [29/120    avg_loss:0.257, val_acc:0.906]
Epoch [30/120    avg_loss:0.281, val_acc:0.920]
Epoch [31/120    avg_loss:0.245, val_acc:0.938]
Epoch [32/120    avg_loss:0.227, val_acc:0.939]
Epoch [33/120    avg_loss:0.203, val_acc:0.953]
Epoch [34/120    avg_loss:0.195, val_acc:0.945]
Epoch [35/120    avg_loss:0.191, val_acc:0.939]
Epoch [36/120    avg_loss:0.195, val_acc:0.951]
Epoch [37/120    avg_loss:0.177, val_acc:0.951]
Epoch [38/120    avg_loss:0.187, val_acc:0.951]
Epoch [39/120    avg_loss:0.169, val_acc:0.953]
Epoch [40/120    avg_loss:0.188, val_acc:0.951]
Epoch [41/120    avg_loss:0.176, val_acc:0.957]
Epoch [42/120    avg_loss:0.174, val_acc:0.955]
Epoch [43/120    avg_loss:0.175, val_acc:0.963]
Epoch [44/120    avg_loss:0.172, val_acc:0.967]
Epoch [45/120    avg_loss:0.152, val_acc:0.961]
Epoch [46/120    avg_loss:0.179, val_acc:0.957]
Epoch [47/120    avg_loss:0.176, val_acc:0.959]
Epoch [48/120    avg_loss:0.186, val_acc:0.957]
Epoch [49/120    avg_loss:0.190, val_acc:0.959]
Epoch [50/120    avg_loss:0.153, val_acc:0.955]
Epoch [51/120    avg_loss:0.169, val_acc:0.957]
Epoch [52/120    avg_loss:0.170, val_acc:0.965]
Epoch [53/120    avg_loss:0.159, val_acc:0.963]
Epoch [54/120    avg_loss:0.153, val_acc:0.963]
Epoch [55/120    avg_loss:0.145, val_acc:0.955]
Epoch [56/120    avg_loss:0.173, val_acc:0.963]
Epoch [57/120    avg_loss:0.142, val_acc:0.957]
Epoch [58/120    avg_loss:0.160, val_acc:0.957]
Epoch [59/120    avg_loss:0.150, val_acc:0.959]
Epoch [60/120    avg_loss:0.137, val_acc:0.959]
Epoch [61/120    avg_loss:0.142, val_acc:0.959]
Epoch [62/120    avg_loss:0.160, val_acc:0.959]
Epoch [63/120    avg_loss:0.167, val_acc:0.959]
Epoch [64/120    avg_loss:0.149, val_acc:0.959]
Epoch [65/120    avg_loss:0.133, val_acc:0.959]
Epoch [66/120    avg_loss:0.147, val_acc:0.961]
Epoch [67/120    avg_loss:0.134, val_acc:0.961]
Epoch [68/120    avg_loss:0.134, val_acc:0.961]
Epoch [69/120    avg_loss:0.153, val_acc:0.959]
Epoch [70/120    avg_loss:0.163, val_acc:0.963]
Epoch [71/120    avg_loss:0.150, val_acc:0.963]
Epoch [72/120    avg_loss:0.136, val_acc:0.963]
Epoch [73/120    avg_loss:0.141, val_acc:0.963]
Epoch [74/120    avg_loss:0.140, val_acc:0.963]
Epoch [75/120    avg_loss:0.138, val_acc:0.963]
Epoch [76/120    avg_loss:0.134, val_acc:0.963]
Epoch [77/120    avg_loss:0.138, val_acc:0.963]
Epoch [78/120    avg_loss:0.150, val_acc:0.963]
Epoch [79/120    avg_loss:0.144, val_acc:0.963]
Epoch [80/120    avg_loss:0.151, val_acc:0.963]
Epoch [81/120    avg_loss:0.143, val_acc:0.963]
Epoch [82/120    avg_loss:0.137, val_acc:0.963]
Epoch [83/120    avg_loss:0.149, val_acc:0.963]
Epoch [84/120    avg_loss:0.142, val_acc:0.963]
Epoch [85/120    avg_loss:0.133, val_acc:0.963]
Epoch [86/120    avg_loss:0.175, val_acc:0.963]
Epoch [87/120    avg_loss:0.153, val_acc:0.963]
Epoch [88/120    avg_loss:0.139, val_acc:0.963]
Epoch [89/120    avg_loss:0.147, val_acc:0.963]
Epoch [90/120    avg_loss:0.132, val_acc:0.963]
Epoch [91/120    avg_loss:0.166, val_acc:0.963]
Epoch [92/120    avg_loss:0.160, val_acc:0.963]
Epoch [93/120    avg_loss:0.151, val_acc:0.963]
Epoch [94/120    avg_loss:0.138, val_acc:0.963]
Epoch [95/120    avg_loss:0.135, val_acc:0.963]
Epoch [96/120    avg_loss:0.156, val_acc:0.963]
Epoch [97/120    avg_loss:0.146, val_acc:0.963]
Epoch [98/120    avg_loss:0.141, val_acc:0.963]
Epoch [99/120    avg_loss:0.143, val_acc:0.963]
Epoch [100/120    avg_loss:0.143, val_acc:0.963]
Epoch [101/120    avg_loss:0.156, val_acc:0.963]
Epoch [102/120    avg_loss:0.147, val_acc:0.963]
Epoch [103/120    avg_loss:0.134, val_acc:0.963]
Epoch [104/120    avg_loss:0.160, val_acc:0.963]
Epoch [105/120    avg_loss:0.155, val_acc:0.963]
Epoch [106/120    avg_loss:0.151, val_acc:0.963]
Epoch [107/120    avg_loss:0.134, val_acc:0.963]
Epoch [108/120    avg_loss:0.149, val_acc:0.963]
Epoch [109/120    avg_loss:0.139, val_acc:0.963]
Epoch [110/120    avg_loss:0.143, val_acc:0.963]
Epoch [111/120    avg_loss:0.153, val_acc:0.963]
Epoch [112/120    avg_loss:0.148, val_acc:0.963]
Epoch [113/120    avg_loss:0.151, val_acc:0.963]
Epoch [114/120    avg_loss:0.142, val_acc:0.963]
Epoch [115/120    avg_loss:0.152, val_acc:0.963]
Epoch [116/120    avg_loss:0.157, val_acc:0.963]
Epoch [117/120    avg_loss:0.143, val_acc:0.963]
Epoch [118/120    avg_loss:0.148, val_acc:0.963]
Epoch [119/120    avg_loss:0.169, val_acc:0.963]
Epoch [120/120    avg_loss:0.136, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 196   0   0   0   0  23   0   0   0   0   0   0]
 [  0   0   0 216   8   1   0   0   5   0   0   0   0   0]
 [  0   0   0   3 196  28   0   0   0   0   0   0   0   0]
 [  0   0   0   4  29 112   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.37739872068231

F1 scores:
[       nan 1.         0.90322581 0.95364238 0.85217391 0.78321678
 1.         0.79381443 0.99230769 1.         1.         0.99600533
 0.99559471 1.        ]

Kappa:
0.9708018047032646
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1d3293828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.403, val_acc:0.346]
Epoch [2/120    avg_loss:2.004, val_acc:0.561]
Epoch [3/120    avg_loss:1.723, val_acc:0.590]
Epoch [4/120    avg_loss:1.446, val_acc:0.664]
Epoch [5/120    avg_loss:1.200, val_acc:0.752]
Epoch [6/120    avg_loss:1.018, val_acc:0.771]
Epoch [7/120    avg_loss:0.939, val_acc:0.766]
Epoch [8/120    avg_loss:0.873, val_acc:0.781]
Epoch [9/120    avg_loss:0.822, val_acc:0.812]
Epoch [10/120    avg_loss:0.724, val_acc:0.879]
Epoch [11/120    avg_loss:0.686, val_acc:0.893]
Epoch [12/120    avg_loss:0.625, val_acc:0.930]
Epoch [13/120    avg_loss:0.564, val_acc:0.926]
Epoch [14/120    avg_loss:0.526, val_acc:0.926]
Epoch [15/120    avg_loss:0.501, val_acc:0.924]
Epoch [16/120    avg_loss:0.462, val_acc:0.941]
Epoch [17/120    avg_loss:0.460, val_acc:0.947]
Epoch [18/120    avg_loss:0.496, val_acc:0.914]
Epoch [19/120    avg_loss:0.404, val_acc:0.918]
Epoch [20/120    avg_loss:0.416, val_acc:0.930]
Epoch [21/120    avg_loss:0.410, val_acc:0.918]
Epoch [22/120    avg_loss:0.414, val_acc:0.938]
Epoch [23/120    avg_loss:0.336, val_acc:0.955]
Epoch [24/120    avg_loss:0.330, val_acc:0.898]
Epoch [25/120    avg_loss:0.443, val_acc:0.930]
Epoch [26/120    avg_loss:0.341, val_acc:0.928]
Epoch [27/120    avg_loss:0.300, val_acc:0.955]
Epoch [28/120    avg_loss:0.280, val_acc:0.926]
Epoch [29/120    avg_loss:0.371, val_acc:0.912]
Epoch [30/120    avg_loss:0.289, val_acc:0.949]
Epoch [31/120    avg_loss:0.371, val_acc:0.916]
Epoch [32/120    avg_loss:0.303, val_acc:0.934]
Epoch [33/120    avg_loss:0.310, val_acc:0.953]
Epoch [34/120    avg_loss:0.301, val_acc:0.932]
Epoch [35/120    avg_loss:0.273, val_acc:0.967]
Epoch [36/120    avg_loss:0.247, val_acc:0.943]
Epoch [37/120    avg_loss:0.348, val_acc:0.924]
Epoch [38/120    avg_loss:0.300, val_acc:0.949]
Epoch [39/120    avg_loss:0.241, val_acc:0.973]
Epoch [40/120    avg_loss:0.202, val_acc:0.965]
Epoch [41/120    avg_loss:0.191, val_acc:0.969]
Epoch [42/120    avg_loss:0.182, val_acc:0.965]
Epoch [43/120    avg_loss:0.204, val_acc:0.961]
Epoch [44/120    avg_loss:0.188, val_acc:0.951]
Epoch [45/120    avg_loss:0.234, val_acc:0.961]
Epoch [46/120    avg_loss:0.164, val_acc:0.969]
Epoch [47/120    avg_loss:0.145, val_acc:0.936]
Epoch [48/120    avg_loss:0.161, val_acc:0.977]
Epoch [49/120    avg_loss:0.167, val_acc:0.973]
Epoch [50/120    avg_loss:0.170, val_acc:0.975]
Epoch [51/120    avg_loss:0.121, val_acc:0.975]
Epoch [52/120    avg_loss:0.116, val_acc:0.979]
Epoch [53/120    avg_loss:0.153, val_acc:0.973]
Epoch [54/120    avg_loss:0.186, val_acc:0.973]
Epoch [55/120    avg_loss:0.201, val_acc:0.971]
Epoch [56/120    avg_loss:0.119, val_acc:0.979]
Epoch [57/120    avg_loss:0.145, val_acc:0.949]
Epoch [58/120    avg_loss:0.131, val_acc:0.975]
Epoch [59/120    avg_loss:0.147, val_acc:0.953]
Epoch [60/120    avg_loss:0.186, val_acc:0.963]
Epoch [61/120    avg_loss:0.163, val_acc:0.975]
Epoch [62/120    avg_loss:0.099, val_acc:0.963]
Epoch [63/120    avg_loss:0.108, val_acc:0.973]
Epoch [64/120    avg_loss:0.092, val_acc:0.988]
Epoch [65/120    avg_loss:0.103, val_acc:0.986]
Epoch [66/120    avg_loss:0.126, val_acc:0.969]
Epoch [67/120    avg_loss:0.096, val_acc:0.973]
Epoch [68/120    avg_loss:0.097, val_acc:0.979]
Epoch [69/120    avg_loss:0.095, val_acc:0.982]
Epoch [70/120    avg_loss:0.093, val_acc:0.975]
Epoch [71/120    avg_loss:0.090, val_acc:0.969]
Epoch [72/120    avg_loss:0.090, val_acc:0.977]
Epoch [73/120    avg_loss:0.077, val_acc:0.973]
Epoch [74/120    avg_loss:0.095, val_acc:0.967]
Epoch [75/120    avg_loss:0.121, val_acc:0.959]
Epoch [76/120    avg_loss:0.110, val_acc:0.979]
Epoch [77/120    avg_loss:0.091, val_acc:0.979]
Epoch [78/120    avg_loss:0.060, val_acc:0.980]
Epoch [79/120    avg_loss:0.057, val_acc:0.980]
Epoch [80/120    avg_loss:0.061, val_acc:0.980]
Epoch [81/120    avg_loss:0.048, val_acc:0.980]
Epoch [82/120    avg_loss:0.058, val_acc:0.982]
Epoch [83/120    avg_loss:0.050, val_acc:0.980]
Epoch [84/120    avg_loss:0.054, val_acc:0.980]
Epoch [85/120    avg_loss:0.060, val_acc:0.982]
Epoch [86/120    avg_loss:0.040, val_acc:0.984]
Epoch [87/120    avg_loss:0.045, val_acc:0.984]
Epoch [88/120    avg_loss:0.046, val_acc:0.980]
Epoch [89/120    avg_loss:0.042, val_acc:0.984]
Epoch [90/120    avg_loss:0.050, val_acc:0.980]
Epoch [91/120    avg_loss:0.048, val_acc:0.980]
Epoch [92/120    avg_loss:0.047, val_acc:0.980]
Epoch [93/120    avg_loss:0.050, val_acc:0.980]
Epoch [94/120    avg_loss:0.047, val_acc:0.980]
Epoch [95/120    avg_loss:0.042, val_acc:0.980]
Epoch [96/120    avg_loss:0.051, val_acc:0.980]
Epoch [97/120    avg_loss:0.042, val_acc:0.982]
Epoch [98/120    avg_loss:0.047, val_acc:0.980]
Epoch [99/120    avg_loss:0.045, val_acc:0.980]
Epoch [100/120    avg_loss:0.054, val_acc:0.980]
Epoch [101/120    avg_loss:0.034, val_acc:0.980]
Epoch [102/120    avg_loss:0.043, val_acc:0.980]
Epoch [103/120    avg_loss:0.034, val_acc:0.980]
Epoch [104/120    avg_loss:0.035, val_acc:0.980]
Epoch [105/120    avg_loss:0.034, val_acc:0.980]
Epoch [106/120    avg_loss:0.048, val_acc:0.980]
Epoch [107/120    avg_loss:0.050, val_acc:0.980]
Epoch [108/120    avg_loss:0.045, val_acc:0.980]
Epoch [109/120    avg_loss:0.038, val_acc:0.980]
Epoch [110/120    avg_loss:0.036, val_acc:0.980]
Epoch [111/120    avg_loss:0.047, val_acc:0.980]
Epoch [112/120    avg_loss:0.037, val_acc:0.980]
Epoch [113/120    avg_loss:0.039, val_acc:0.980]
Epoch [114/120    avg_loss:0.036, val_acc:0.980]
Epoch [115/120    avg_loss:0.041, val_acc:0.980]
Epoch [116/120    avg_loss:0.040, val_acc:0.980]
Epoch [117/120    avg_loss:0.051, val_acc:0.980]
Epoch [118/120    avg_loss:0.039, val_acc:0.980]
Epoch [119/120    avg_loss:0.045, val_acc:0.980]
Epoch [120/120    avg_loss:0.035, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 218   6   0   0   0   6   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   1   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 1.         0.93303571 0.97321429 0.93418259 0.91039427
 1.         0.85714286 0.99106003 1.         1.         1.
 0.99445061 1.        ]

Kappa:
0.9838567692714119
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0489a287f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.360, val_acc:0.383]
Epoch [2/120    avg_loss:2.017, val_acc:0.570]
Epoch [3/120    avg_loss:1.724, val_acc:0.590]
Epoch [4/120    avg_loss:1.455, val_acc:0.660]
Epoch [5/120    avg_loss:1.269, val_acc:0.643]
Epoch [6/120    avg_loss:1.124, val_acc:0.750]
Epoch [7/120    avg_loss:1.008, val_acc:0.834]
Epoch [8/120    avg_loss:0.957, val_acc:0.764]
Epoch [9/120    avg_loss:0.871, val_acc:0.820]
Epoch [10/120    avg_loss:0.773, val_acc:0.918]
Epoch [11/120    avg_loss:0.726, val_acc:0.893]
Epoch [12/120    avg_loss:0.648, val_acc:0.900]
Epoch [13/120    avg_loss:0.579, val_acc:0.904]
Epoch [14/120    avg_loss:0.533, val_acc:0.928]
Epoch [15/120    avg_loss:0.472, val_acc:0.920]
Epoch [16/120    avg_loss:0.571, val_acc:0.887]
Epoch [17/120    avg_loss:0.436, val_acc:0.920]
Epoch [18/120    avg_loss:0.406, val_acc:0.828]
Epoch [19/120    avg_loss:0.422, val_acc:0.922]
Epoch [20/120    avg_loss:0.446, val_acc:0.922]
Epoch [21/120    avg_loss:0.464, val_acc:0.896]
Epoch [22/120    avg_loss:0.458, val_acc:0.885]
Epoch [23/120    avg_loss:0.393, val_acc:0.930]
Epoch [24/120    avg_loss:0.332, val_acc:0.938]
Epoch [25/120    avg_loss:0.302, val_acc:0.920]
Epoch [26/120    avg_loss:0.372, val_acc:0.936]
Epoch [27/120    avg_loss:0.358, val_acc:0.943]
Epoch [28/120    avg_loss:0.289, val_acc:0.943]
Epoch [29/120    avg_loss:0.308, val_acc:0.949]
Epoch [30/120    avg_loss:0.249, val_acc:0.930]
Epoch [31/120    avg_loss:0.330, val_acc:0.939]
Epoch [32/120    avg_loss:0.267, val_acc:0.938]
Epoch [33/120    avg_loss:0.255, val_acc:0.934]
Epoch [34/120    avg_loss:0.355, val_acc:0.926]
Epoch [35/120    avg_loss:0.236, val_acc:0.943]
Epoch [36/120    avg_loss:0.218, val_acc:0.957]
Epoch [37/120    avg_loss:0.326, val_acc:0.955]
Epoch [38/120    avg_loss:0.225, val_acc:0.951]
Epoch [39/120    avg_loss:0.184, val_acc:0.945]
Epoch [40/120    avg_loss:0.224, val_acc:0.963]
Epoch [41/120    avg_loss:0.185, val_acc:0.955]
Epoch [42/120    avg_loss:0.186, val_acc:0.961]
Epoch [43/120    avg_loss:0.204, val_acc:0.939]
Epoch [44/120    avg_loss:0.201, val_acc:0.951]
Epoch [45/120    avg_loss:0.191, val_acc:0.943]
Epoch [46/120    avg_loss:0.226, val_acc:0.943]
Epoch [47/120    avg_loss:0.238, val_acc:0.947]
Epoch [48/120    avg_loss:0.285, val_acc:0.951]
Epoch [49/120    avg_loss:0.234, val_acc:0.955]
Epoch [50/120    avg_loss:0.183, val_acc:0.965]
Epoch [51/120    avg_loss:0.142, val_acc:0.963]
Epoch [52/120    avg_loss:0.186, val_acc:0.951]
Epoch [53/120    avg_loss:0.169, val_acc:0.965]
Epoch [54/120    avg_loss:0.123, val_acc:0.963]
Epoch [55/120    avg_loss:0.141, val_acc:0.957]
Epoch [56/120    avg_loss:0.156, val_acc:0.967]
Epoch [57/120    avg_loss:0.127, val_acc:0.965]
Epoch [58/120    avg_loss:0.124, val_acc:0.967]
Epoch [59/120    avg_loss:0.138, val_acc:0.963]
Epoch [60/120    avg_loss:0.121, val_acc:0.965]
Epoch [61/120    avg_loss:0.125, val_acc:0.953]
Epoch [62/120    avg_loss:0.137, val_acc:0.971]
Epoch [63/120    avg_loss:0.150, val_acc:0.961]
Epoch [64/120    avg_loss:0.142, val_acc:0.967]
Epoch [65/120    avg_loss:0.112, val_acc:0.965]
Epoch [66/120    avg_loss:0.115, val_acc:0.963]
Epoch [67/120    avg_loss:0.083, val_acc:0.977]
Epoch [68/120    avg_loss:0.094, val_acc:0.975]
Epoch [69/120    avg_loss:0.104, val_acc:0.975]
Epoch [70/120    avg_loss:0.102, val_acc:0.977]
Epoch [71/120    avg_loss:0.123, val_acc:0.965]
Epoch [72/120    avg_loss:0.162, val_acc:0.975]
Epoch [73/120    avg_loss:0.109, val_acc:0.967]
Epoch [74/120    avg_loss:0.084, val_acc:0.973]
Epoch [75/120    avg_loss:0.075, val_acc:0.979]
Epoch [76/120    avg_loss:0.060, val_acc:0.977]
Epoch [77/120    avg_loss:0.088, val_acc:0.969]
Epoch [78/120    avg_loss:0.143, val_acc:0.967]
Epoch [79/120    avg_loss:0.116, val_acc:0.963]
Epoch [80/120    avg_loss:0.110, val_acc:0.973]
Epoch [81/120    avg_loss:0.092, val_acc:0.980]
Epoch [82/120    avg_loss:0.077, val_acc:0.973]
Epoch [83/120    avg_loss:0.109, val_acc:0.980]
Epoch [84/120    avg_loss:0.097, val_acc:0.975]
Epoch [85/120    avg_loss:0.082, val_acc:0.959]
Epoch [86/120    avg_loss:0.095, val_acc:0.973]
Epoch [87/120    avg_loss:0.096, val_acc:0.973]
Epoch [88/120    avg_loss:0.097, val_acc:0.971]
Epoch [89/120    avg_loss:0.116, val_acc:0.977]
Epoch [90/120    avg_loss:0.080, val_acc:0.969]
Epoch [91/120    avg_loss:0.101, val_acc:0.975]
Epoch [92/120    avg_loss:0.090, val_acc:0.977]
Epoch [93/120    avg_loss:0.101, val_acc:0.982]
Epoch [94/120    avg_loss:0.074, val_acc:0.979]
Epoch [95/120    avg_loss:0.084, val_acc:0.977]
Epoch [96/120    avg_loss:0.042, val_acc:0.979]
Epoch [97/120    avg_loss:0.056, val_acc:0.986]
Epoch [98/120    avg_loss:0.059, val_acc:0.980]
Epoch [99/120    avg_loss:0.061, val_acc:0.988]
Epoch [100/120    avg_loss:0.081, val_acc:0.979]
Epoch [101/120    avg_loss:0.064, val_acc:0.971]
Epoch [102/120    avg_loss:0.177, val_acc:0.951]
Epoch [103/120    avg_loss:0.101, val_acc:0.977]
Epoch [104/120    avg_loss:0.137, val_acc:0.971]
Epoch [105/120    avg_loss:0.050, val_acc:0.975]
Epoch [106/120    avg_loss:0.060, val_acc:0.982]
Epoch [107/120    avg_loss:0.061, val_acc:0.977]
Epoch [108/120    avg_loss:0.052, val_acc:0.973]
Epoch [109/120    avg_loss:0.030, val_acc:0.979]
Epoch [110/120    avg_loss:0.040, val_acc:0.982]
Epoch [111/120    avg_loss:0.024, val_acc:0.982]
Epoch [112/120    avg_loss:0.050, val_acc:0.986]
Epoch [113/120    avg_loss:0.037, val_acc:0.984]
Epoch [114/120    avg_loss:0.023, val_acc:0.986]
Epoch [115/120    avg_loss:0.023, val_acc:0.986]
Epoch [116/120    avg_loss:0.019, val_acc:0.984]
Epoch [117/120    avg_loss:0.023, val_acc:0.984]
Epoch [118/120    avg_loss:0.024, val_acc:0.986]
Epoch [119/120    avg_loss:0.016, val_acc:0.984]
Epoch [120/120    avg_loss:0.020, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   0   2   0   0   0   0   0]
 [  0   0   0   2 208  16   0   0   1   0   0   0   0   0]
 [  0   0   0   5  13 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.61407249466951

F1 scores:
[       nan 1.         0.96363636 0.96732026 0.90829694 0.88194444
 0.99019608 0.9197861  0.99614891 1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9845697220034347
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd3215c77f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.410, val_acc:0.473]
Epoch [2/120    avg_loss:1.972, val_acc:0.557]
Epoch [3/120    avg_loss:1.669, val_acc:0.627]
Epoch [4/120    avg_loss:1.410, val_acc:0.729]
Epoch [5/120    avg_loss:1.220, val_acc:0.738]
Epoch [6/120    avg_loss:1.070, val_acc:0.764]
Epoch [7/120    avg_loss:0.953, val_acc:0.811]
Epoch [8/120    avg_loss:0.868, val_acc:0.818]
Epoch [9/120    avg_loss:0.770, val_acc:0.834]
Epoch [10/120    avg_loss:0.668, val_acc:0.869]
Epoch [11/120    avg_loss:0.669, val_acc:0.875]
Epoch [12/120    avg_loss:0.573, val_acc:0.873]
Epoch [13/120    avg_loss:0.534, val_acc:0.863]
Epoch [14/120    avg_loss:0.547, val_acc:0.879]
Epoch [15/120    avg_loss:0.476, val_acc:0.877]
Epoch [16/120    avg_loss:0.460, val_acc:0.904]
Epoch [17/120    avg_loss:0.436, val_acc:0.846]
Epoch [18/120    avg_loss:0.491, val_acc:0.879]
Epoch [19/120    avg_loss:0.441, val_acc:0.904]
Epoch [20/120    avg_loss:0.378, val_acc:0.926]
Epoch [21/120    avg_loss:0.345, val_acc:0.904]
Epoch [22/120    avg_loss:0.330, val_acc:0.910]
Epoch [23/120    avg_loss:0.380, val_acc:0.898]
Epoch [24/120    avg_loss:0.361, val_acc:0.932]
Epoch [25/120    avg_loss:0.346, val_acc:0.912]
Epoch [26/120    avg_loss:0.319, val_acc:0.910]
Epoch [27/120    avg_loss:0.279, val_acc:0.930]
Epoch [28/120    avg_loss:0.298, val_acc:0.945]
Epoch [29/120    avg_loss:0.228, val_acc:0.941]
Epoch [30/120    avg_loss:0.251, val_acc:0.943]
Epoch [31/120    avg_loss:0.210, val_acc:0.943]
Epoch [32/120    avg_loss:0.218, val_acc:0.936]
Epoch [33/120    avg_loss:0.235, val_acc:0.959]
Epoch [34/120    avg_loss:0.209, val_acc:0.951]
Epoch [35/120    avg_loss:0.204, val_acc:0.955]
Epoch [36/120    avg_loss:0.173, val_acc:0.938]
Epoch [37/120    avg_loss:0.186, val_acc:0.957]
Epoch [38/120    avg_loss:0.161, val_acc:0.934]
Epoch [39/120    avg_loss:0.160, val_acc:0.951]
Epoch [40/120    avg_loss:0.152, val_acc:0.947]
Epoch [41/120    avg_loss:0.164, val_acc:0.949]
Epoch [42/120    avg_loss:0.155, val_acc:0.947]
Epoch [43/120    avg_loss:0.161, val_acc:0.941]
Epoch [44/120    avg_loss:0.150, val_acc:0.928]
Epoch [45/120    avg_loss:0.156, val_acc:0.945]
Epoch [46/120    avg_loss:0.189, val_acc:0.953]
Epoch [47/120    avg_loss:0.160, val_acc:0.969]
Epoch [48/120    avg_loss:0.119, val_acc:0.963]
Epoch [49/120    avg_loss:0.099, val_acc:0.967]
Epoch [50/120    avg_loss:0.105, val_acc:0.965]
Epoch [51/120    avg_loss:0.090, val_acc:0.967]
Epoch [52/120    avg_loss:0.103, val_acc:0.967]
Epoch [53/120    avg_loss:0.102, val_acc:0.967]
Epoch [54/120    avg_loss:0.102, val_acc:0.967]
Epoch [55/120    avg_loss:0.092, val_acc:0.967]
Epoch [56/120    avg_loss:0.091, val_acc:0.969]
Epoch [57/120    avg_loss:0.098, val_acc:0.969]
Epoch [58/120    avg_loss:0.086, val_acc:0.971]
Epoch [59/120    avg_loss:0.090, val_acc:0.973]
Epoch [60/120    avg_loss:0.088, val_acc:0.973]
Epoch [61/120    avg_loss:0.081, val_acc:0.971]
Epoch [62/120    avg_loss:0.079, val_acc:0.971]
Epoch [63/120    avg_loss:0.079, val_acc:0.971]
Epoch [64/120    avg_loss:0.076, val_acc:0.969]
Epoch [65/120    avg_loss:0.074, val_acc:0.973]
Epoch [66/120    avg_loss:0.080, val_acc:0.973]
Epoch [67/120    avg_loss:0.063, val_acc:0.975]
Epoch [68/120    avg_loss:0.081, val_acc:0.977]
Epoch [69/120    avg_loss:0.075, val_acc:0.973]
Epoch [70/120    avg_loss:0.094, val_acc:0.971]
Epoch [71/120    avg_loss:0.074, val_acc:0.973]
Epoch [72/120    avg_loss:0.083, val_acc:0.975]
Epoch [73/120    avg_loss:0.087, val_acc:0.973]
Epoch [74/120    avg_loss:0.081, val_acc:0.977]
Epoch [75/120    avg_loss:0.078, val_acc:0.975]
Epoch [76/120    avg_loss:0.077, val_acc:0.977]
Epoch [77/120    avg_loss:0.063, val_acc:0.979]
Epoch [78/120    avg_loss:0.084, val_acc:0.980]
Epoch [79/120    avg_loss:0.075, val_acc:0.982]
Epoch [80/120    avg_loss:0.072, val_acc:0.979]
Epoch [81/120    avg_loss:0.066, val_acc:0.982]
Epoch [82/120    avg_loss:0.078, val_acc:0.980]
Epoch [83/120    avg_loss:0.070, val_acc:0.979]
Epoch [84/120    avg_loss:0.054, val_acc:0.980]
Epoch [85/120    avg_loss:0.064, val_acc:0.979]
Epoch [86/120    avg_loss:0.062, val_acc:0.977]
Epoch [87/120    avg_loss:0.064, val_acc:0.979]
Epoch [88/120    avg_loss:0.070, val_acc:0.980]
Epoch [89/120    avg_loss:0.059, val_acc:0.979]
Epoch [90/120    avg_loss:0.071, val_acc:0.977]
Epoch [91/120    avg_loss:0.062, val_acc:0.980]
Epoch [92/120    avg_loss:0.063, val_acc:0.982]
Epoch [93/120    avg_loss:0.057, val_acc:0.980]
Epoch [94/120    avg_loss:0.050, val_acc:0.980]
Epoch [95/120    avg_loss:0.065, val_acc:0.982]
Epoch [96/120    avg_loss:0.069, val_acc:0.982]
Epoch [97/120    avg_loss:0.067, val_acc:0.980]
Epoch [98/120    avg_loss:0.054, val_acc:0.980]
Epoch [99/120    avg_loss:0.069, val_acc:0.982]
Epoch [100/120    avg_loss:0.050, val_acc:0.982]
Epoch [101/120    avg_loss:0.054, val_acc:0.980]
Epoch [102/120    avg_loss:0.075, val_acc:0.980]
Epoch [103/120    avg_loss:0.058, val_acc:0.982]
Epoch [104/120    avg_loss:0.054, val_acc:0.979]
Epoch [105/120    avg_loss:0.048, val_acc:0.979]
Epoch [106/120    avg_loss:0.053, val_acc:0.984]
Epoch [107/120    avg_loss:0.055, val_acc:0.977]
Epoch [108/120    avg_loss:0.060, val_acc:0.984]
Epoch [109/120    avg_loss:0.058, val_acc:0.982]
Epoch [110/120    avg_loss:0.063, val_acc:0.982]
Epoch [111/120    avg_loss:0.064, val_acc:0.982]
Epoch [112/120    avg_loss:0.070, val_acc:0.977]
Epoch [113/120    avg_loss:0.061, val_acc:0.979]
Epoch [114/120    avg_loss:0.057, val_acc:0.980]
Epoch [115/120    avg_loss:0.055, val_acc:0.980]
Epoch [116/120    avg_loss:0.059, val_acc:0.979]
Epoch [117/120    avg_loss:0.052, val_acc:0.980]
Epoch [118/120    avg_loss:0.042, val_acc:0.979]
Epoch [119/120    avg_loss:0.063, val_acc:0.982]
Epoch [120/120    avg_loss:0.057, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 210  15   0   0   1   0   0   0   1   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  24   0   0   0   0  70   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 1.         0.94805195 0.98230088 0.90322581 0.86925795
 1.         0.85365854 0.99614891 1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.9829047843366757
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e5ab81828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.436, val_acc:0.496]
Epoch [2/120    avg_loss:2.018, val_acc:0.674]
Epoch [3/120    avg_loss:1.726, val_acc:0.639]
Epoch [4/120    avg_loss:1.427, val_acc:0.709]
Epoch [5/120    avg_loss:1.203, val_acc:0.707]
Epoch [6/120    avg_loss:1.033, val_acc:0.834]
Epoch [7/120    avg_loss:0.911, val_acc:0.842]
Epoch [8/120    avg_loss:0.796, val_acc:0.859]
Epoch [9/120    avg_loss:0.732, val_acc:0.869]
Epoch [10/120    avg_loss:0.621, val_acc:0.902]
Epoch [11/120    avg_loss:0.592, val_acc:0.879]
Epoch [12/120    avg_loss:0.573, val_acc:0.889]
Epoch [13/120    avg_loss:0.504, val_acc:0.900]
Epoch [14/120    avg_loss:0.505, val_acc:0.883]
Epoch [15/120    avg_loss:0.432, val_acc:0.896]
Epoch [16/120    avg_loss:0.488, val_acc:0.906]
Epoch [17/120    avg_loss:0.524, val_acc:0.883]
Epoch [18/120    avg_loss:0.433, val_acc:0.918]
Epoch [19/120    avg_loss:0.472, val_acc:0.904]
Epoch [20/120    avg_loss:0.477, val_acc:0.896]
Epoch [21/120    avg_loss:0.411, val_acc:0.920]
Epoch [22/120    avg_loss:0.346, val_acc:0.891]
Epoch [23/120    avg_loss:0.352, val_acc:0.912]
Epoch [24/120    avg_loss:0.395, val_acc:0.922]
Epoch [25/120    avg_loss:0.308, val_acc:0.928]
Epoch [26/120    avg_loss:0.305, val_acc:0.908]
Epoch [27/120    avg_loss:0.384, val_acc:0.920]
Epoch [28/120    avg_loss:0.351, val_acc:0.883]
Epoch [29/120    avg_loss:0.369, val_acc:0.910]
Epoch [30/120    avg_loss:0.323, val_acc:0.916]
Epoch [31/120    avg_loss:0.277, val_acc:0.924]
Epoch [32/120    avg_loss:0.316, val_acc:0.889]
Epoch [33/120    avg_loss:0.309, val_acc:0.936]
Epoch [34/120    avg_loss:0.223, val_acc:0.945]
Epoch [35/120    avg_loss:0.278, val_acc:0.906]
Epoch [36/120    avg_loss:0.273, val_acc:0.961]
Epoch [37/120    avg_loss:0.237, val_acc:0.945]
Epoch [38/120    avg_loss:0.205, val_acc:0.951]
Epoch [39/120    avg_loss:0.220, val_acc:0.953]
Epoch [40/120    avg_loss:0.199, val_acc:0.957]
Epoch [41/120    avg_loss:0.229, val_acc:0.949]
Epoch [42/120    avg_loss:0.228, val_acc:0.922]
Epoch [43/120    avg_loss:0.235, val_acc:0.945]
Epoch [44/120    avg_loss:0.192, val_acc:0.928]
Epoch [45/120    avg_loss:0.269, val_acc:0.912]
Epoch [46/120    avg_loss:0.197, val_acc:0.959]
Epoch [47/120    avg_loss:0.178, val_acc:0.957]
Epoch [48/120    avg_loss:0.112, val_acc:0.963]
Epoch [49/120    avg_loss:0.169, val_acc:0.955]
Epoch [50/120    avg_loss:0.173, val_acc:0.961]
Epoch [51/120    avg_loss:0.157, val_acc:0.957]
Epoch [52/120    avg_loss:0.174, val_acc:0.977]
Epoch [53/120    avg_loss:0.158, val_acc:0.949]
Epoch [54/120    avg_loss:0.184, val_acc:0.949]
Epoch [55/120    avg_loss:0.170, val_acc:0.959]
Epoch [56/120    avg_loss:0.127, val_acc:0.951]
Epoch [57/120    avg_loss:0.144, val_acc:0.961]
Epoch [58/120    avg_loss:0.131, val_acc:0.949]
Epoch [59/120    avg_loss:0.104, val_acc:0.975]
Epoch [60/120    avg_loss:0.104, val_acc:0.957]
Epoch [61/120    avg_loss:0.099, val_acc:0.965]
Epoch [62/120    avg_loss:0.110, val_acc:0.969]
Epoch [63/120    avg_loss:0.100, val_acc:0.959]
Epoch [64/120    avg_loss:0.090, val_acc:0.963]
Epoch [65/120    avg_loss:0.077, val_acc:0.975]
Epoch [66/120    avg_loss:0.068, val_acc:0.980]
Epoch [67/120    avg_loss:0.072, val_acc:0.980]
Epoch [68/120    avg_loss:0.054, val_acc:0.982]
Epoch [69/120    avg_loss:0.058, val_acc:0.986]
Epoch [70/120    avg_loss:0.061, val_acc:0.982]
Epoch [71/120    avg_loss:0.059, val_acc:0.980]
Epoch [72/120    avg_loss:0.058, val_acc:0.982]
Epoch [73/120    avg_loss:0.058, val_acc:0.986]
Epoch [74/120    avg_loss:0.059, val_acc:0.980]
Epoch [75/120    avg_loss:0.055, val_acc:0.982]
Epoch [76/120    avg_loss:0.053, val_acc:0.986]
Epoch [77/120    avg_loss:0.045, val_acc:0.986]
Epoch [78/120    avg_loss:0.059, val_acc:0.982]
Epoch [79/120    avg_loss:0.044, val_acc:0.982]
Epoch [80/120    avg_loss:0.043, val_acc:0.982]
Epoch [81/120    avg_loss:0.051, val_acc:0.986]
Epoch [82/120    avg_loss:0.056, val_acc:0.979]
Epoch [83/120    avg_loss:0.055, val_acc:0.980]
Epoch [84/120    avg_loss:0.047, val_acc:0.986]
Epoch [85/120    avg_loss:0.043, val_acc:0.984]
Epoch [86/120    avg_loss:0.045, val_acc:0.982]
Epoch [87/120    avg_loss:0.046, val_acc:0.986]
Epoch [88/120    avg_loss:0.052, val_acc:0.986]
Epoch [89/120    avg_loss:0.051, val_acc:0.982]
Epoch [90/120    avg_loss:0.059, val_acc:0.979]
Epoch [91/120    avg_loss:0.053, val_acc:0.988]
Epoch [92/120    avg_loss:0.041, val_acc:0.982]
Epoch [93/120    avg_loss:0.051, val_acc:0.986]
Epoch [94/120    avg_loss:0.041, val_acc:0.986]
Epoch [95/120    avg_loss:0.048, val_acc:0.982]
Epoch [96/120    avg_loss:0.038, val_acc:0.988]
Epoch [97/120    avg_loss:0.046, val_acc:0.984]
Epoch [98/120    avg_loss:0.033, val_acc:0.988]
Epoch [99/120    avg_loss:0.036, val_acc:0.984]
Epoch [100/120    avg_loss:0.038, val_acc:0.982]
Epoch [101/120    avg_loss:0.039, val_acc:0.984]
Epoch [102/120    avg_loss:0.036, val_acc:0.988]
Epoch [103/120    avg_loss:0.038, val_acc:0.986]
Epoch [104/120    avg_loss:0.044, val_acc:0.984]
Epoch [105/120    avg_loss:0.041, val_acc:0.986]
Epoch [106/120    avg_loss:0.032, val_acc:0.986]
Epoch [107/120    avg_loss:0.036, val_acc:0.984]
Epoch [108/120    avg_loss:0.045, val_acc:0.980]
Epoch [109/120    avg_loss:0.047, val_acc:0.984]
Epoch [110/120    avg_loss:0.038, val_acc:0.984]
Epoch [111/120    avg_loss:0.036, val_acc:0.984]
Epoch [112/120    avg_loss:0.052, val_acc:0.986]
Epoch [113/120    avg_loss:0.045, val_acc:0.982]
Epoch [114/120    avg_loss:0.037, val_acc:0.988]
Epoch [115/120    avg_loss:0.041, val_acc:0.986]
Epoch [116/120    avg_loss:0.044, val_acc:0.982]
Epoch [117/120    avg_loss:0.038, val_acc:0.984]
Epoch [118/120    avg_loss:0.045, val_acc:0.986]
Epoch [119/120    avg_loss:0.043, val_acc:0.984]
Epoch [120/120    avg_loss:0.042, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  14   0   0   0   0   1   0]
 [  0   0   0 206  24   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   2 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 1.         0.95774648 0.94495413 0.91025641 0.94
 1.         0.92       1.         1.         1.         0.9973545
 0.99557522 1.        ]

Kappa:
0.9852842446477373
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f779a30e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.468, val_acc:0.416]
Epoch [2/120    avg_loss:2.048, val_acc:0.600]
Epoch [3/120    avg_loss:1.740, val_acc:0.715]
Epoch [4/120    avg_loss:1.438, val_acc:0.732]
Epoch [5/120    avg_loss:1.289, val_acc:0.678]
Epoch [6/120    avg_loss:1.125, val_acc:0.793]
Epoch [7/120    avg_loss:0.987, val_acc:0.855]
Epoch [8/120    avg_loss:0.894, val_acc:0.863]
Epoch [9/120    avg_loss:0.764, val_acc:0.869]
Epoch [10/120    avg_loss:0.649, val_acc:0.910]
Epoch [11/120    avg_loss:0.599, val_acc:0.900]
Epoch [12/120    avg_loss:0.545, val_acc:0.910]
Epoch [13/120    avg_loss:0.527, val_acc:0.900]
Epoch [14/120    avg_loss:0.477, val_acc:0.922]
Epoch [15/120    avg_loss:0.480, val_acc:0.918]
Epoch [16/120    avg_loss:0.452, val_acc:0.893]
Epoch [17/120    avg_loss:0.403, val_acc:0.918]
Epoch [18/120    avg_loss:0.376, val_acc:0.836]
Epoch [19/120    avg_loss:0.373, val_acc:0.938]
Epoch [20/120    avg_loss:0.377, val_acc:0.928]
Epoch [21/120    avg_loss:0.416, val_acc:0.932]
Epoch [22/120    avg_loss:0.368, val_acc:0.883]
Epoch [23/120    avg_loss:0.371, val_acc:0.941]
Epoch [24/120    avg_loss:0.297, val_acc:0.943]
Epoch [25/120    avg_loss:0.299, val_acc:0.900]
Epoch [26/120    avg_loss:0.328, val_acc:0.906]
Epoch [27/120    avg_loss:0.295, val_acc:0.945]
Epoch [28/120    avg_loss:0.316, val_acc:0.930]
Epoch [29/120    avg_loss:0.346, val_acc:0.953]
Epoch [30/120    avg_loss:0.246, val_acc:0.949]
Epoch [31/120    avg_loss:0.226, val_acc:0.951]
Epoch [32/120    avg_loss:0.231, val_acc:0.957]
Epoch [33/120    avg_loss:0.244, val_acc:0.939]
Epoch [34/120    avg_loss:0.238, val_acc:0.941]
Epoch [35/120    avg_loss:0.251, val_acc:0.936]
Epoch [36/120    avg_loss:0.285, val_acc:0.938]
Epoch [37/120    avg_loss:0.231, val_acc:0.953]
Epoch [38/120    avg_loss:0.224, val_acc:0.939]
Epoch [39/120    avg_loss:0.218, val_acc:0.926]
Epoch [40/120    avg_loss:0.272, val_acc:0.926]
Epoch [41/120    avg_loss:0.231, val_acc:0.957]
Epoch [42/120    avg_loss:0.229, val_acc:0.951]
Epoch [43/120    avg_loss:0.199, val_acc:0.963]
Epoch [44/120    avg_loss:0.245, val_acc:0.910]
Epoch [45/120    avg_loss:0.291, val_acc:0.953]
Epoch [46/120    avg_loss:0.181, val_acc:0.949]
Epoch [47/120    avg_loss:0.143, val_acc:0.969]
Epoch [48/120    avg_loss:0.181, val_acc:0.957]
Epoch [49/120    avg_loss:0.204, val_acc:0.963]
Epoch [50/120    avg_loss:0.169, val_acc:0.947]
Epoch [51/120    avg_loss:0.197, val_acc:0.977]
Epoch [52/120    avg_loss:0.132, val_acc:0.977]
Epoch [53/120    avg_loss:0.115, val_acc:0.949]
Epoch [54/120    avg_loss:0.135, val_acc:0.969]
Epoch [55/120    avg_loss:0.166, val_acc:0.934]
Epoch [56/120    avg_loss:0.209, val_acc:0.967]
Epoch [57/120    avg_loss:0.137, val_acc:0.963]
Epoch [58/120    avg_loss:0.142, val_acc:0.953]
Epoch [59/120    avg_loss:0.125, val_acc:0.957]
Epoch [60/120    avg_loss:0.150, val_acc:0.951]
Epoch [61/120    avg_loss:0.162, val_acc:0.949]
Epoch [62/120    avg_loss:0.157, val_acc:0.951]
Epoch [63/120    avg_loss:0.129, val_acc:0.967]
Epoch [64/120    avg_loss:0.178, val_acc:0.955]
Epoch [65/120    avg_loss:0.092, val_acc:0.980]
Epoch [66/120    avg_loss:0.089, val_acc:0.975]
Epoch [67/120    avg_loss:0.074, val_acc:0.973]
Epoch [68/120    avg_loss:0.096, val_acc:0.967]
Epoch [69/120    avg_loss:0.103, val_acc:0.967]
Epoch [70/120    avg_loss:0.078, val_acc:0.975]
Epoch [71/120    avg_loss:0.083, val_acc:0.969]
Epoch [72/120    avg_loss:0.096, val_acc:0.963]
Epoch [73/120    avg_loss:0.076, val_acc:0.955]
Epoch [74/120    avg_loss:0.096, val_acc:0.967]
Epoch [75/120    avg_loss:0.109, val_acc:0.975]
Epoch [76/120    avg_loss:0.050, val_acc:0.977]
Epoch [77/120    avg_loss:0.068, val_acc:0.973]
Epoch [78/120    avg_loss:0.052, val_acc:0.979]
Epoch [79/120    avg_loss:0.042, val_acc:0.977]
Epoch [80/120    avg_loss:0.042, val_acc:0.980]
Epoch [81/120    avg_loss:0.045, val_acc:0.980]
Epoch [82/120    avg_loss:0.039, val_acc:0.980]
Epoch [83/120    avg_loss:0.041, val_acc:0.980]
Epoch [84/120    avg_loss:0.037, val_acc:0.980]
Epoch [85/120    avg_loss:0.034, val_acc:0.982]
Epoch [86/120    avg_loss:0.042, val_acc:0.982]
Epoch [87/120    avg_loss:0.040, val_acc:0.982]
Epoch [88/120    avg_loss:0.039, val_acc:0.982]
Epoch [89/120    avg_loss:0.037, val_acc:0.982]
Epoch [90/120    avg_loss:0.031, val_acc:0.982]
Epoch [91/120    avg_loss:0.036, val_acc:0.982]
Epoch [92/120    avg_loss:0.035, val_acc:0.982]
Epoch [93/120    avg_loss:0.032, val_acc:0.982]
Epoch [94/120    avg_loss:0.038, val_acc:0.982]
Epoch [95/120    avg_loss:0.041, val_acc:0.982]
Epoch [96/120    avg_loss:0.036, val_acc:0.982]
Epoch [97/120    avg_loss:0.036, val_acc:0.980]
Epoch [98/120    avg_loss:0.033, val_acc:0.980]
Epoch [99/120    avg_loss:0.035, val_acc:0.980]
Epoch [100/120    avg_loss:0.033, val_acc:0.982]
Epoch [101/120    avg_loss:0.043, val_acc:0.982]
Epoch [102/120    avg_loss:0.029, val_acc:0.982]
Epoch [103/120    avg_loss:0.032, val_acc:0.982]
Epoch [104/120    avg_loss:0.039, val_acc:0.982]
Epoch [105/120    avg_loss:0.030, val_acc:0.982]
Epoch [106/120    avg_loss:0.037, val_acc:0.982]
Epoch [107/120    avg_loss:0.046, val_acc:0.982]
Epoch [108/120    avg_loss:0.036, val_acc:0.982]
Epoch [109/120    avg_loss:0.042, val_acc:0.982]
Epoch [110/120    avg_loss:0.031, val_acc:0.982]
Epoch [111/120    avg_loss:0.027, val_acc:0.982]
Epoch [112/120    avg_loss:0.039, val_acc:0.980]
Epoch [113/120    avg_loss:0.036, val_acc:0.980]
Epoch [114/120    avg_loss:0.035, val_acc:0.982]
Epoch [115/120    avg_loss:0.031, val_acc:0.982]
Epoch [116/120    avg_loss:0.041, val_acc:0.982]
Epoch [117/120    avg_loss:0.042, val_acc:0.979]
Epoch [118/120    avg_loss:0.040, val_acc:0.982]
Epoch [119/120    avg_loss:0.028, val_acc:0.982]
Epoch [120/120    avg_loss:0.040, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 220   6   0   0   0   2   2   0   0   0   0]
 [  0   0   0   0 216   9   0   0   0   0   2   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   1   0 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 0.99926954 0.94854586 0.97777778 0.91914894 0.89208633
 0.99757869 0.90217391 0.99742931 0.9978678  0.99589603 1.
 0.99333333 1.        ]

Kappa:
0.9840948033339617
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fefaffeb898>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.423, val_acc:0.420]
Epoch [2/120    avg_loss:2.038, val_acc:0.604]
Epoch [3/120    avg_loss:1.755, val_acc:0.582]
Epoch [4/120    avg_loss:1.470, val_acc:0.736]
Epoch [5/120    avg_loss:1.256, val_acc:0.781]
Epoch [6/120    avg_loss:1.098, val_acc:0.793]
Epoch [7/120    avg_loss:0.973, val_acc:0.824]
Epoch [8/120    avg_loss:0.881, val_acc:0.846]
Epoch [9/120    avg_loss:0.793, val_acc:0.850]
Epoch [10/120    avg_loss:0.749, val_acc:0.863]
Epoch [11/120    avg_loss:0.658, val_acc:0.869]
Epoch [12/120    avg_loss:0.682, val_acc:0.830]
Epoch [13/120    avg_loss:0.662, val_acc:0.850]
Epoch [14/120    avg_loss:0.595, val_acc:0.896]
Epoch [15/120    avg_loss:0.521, val_acc:0.904]
Epoch [16/120    avg_loss:0.522, val_acc:0.881]
Epoch [17/120    avg_loss:0.464, val_acc:0.900]
Epoch [18/120    avg_loss:0.456, val_acc:0.914]
Epoch [19/120    avg_loss:0.388, val_acc:0.910]
Epoch [20/120    avg_loss:0.410, val_acc:0.916]
Epoch [21/120    avg_loss:0.423, val_acc:0.918]
Epoch [22/120    avg_loss:0.379, val_acc:0.922]
Epoch [23/120    avg_loss:0.358, val_acc:0.924]
Epoch [24/120    avg_loss:0.366, val_acc:0.918]
Epoch [25/120    avg_loss:0.338, val_acc:0.922]
Epoch [26/120    avg_loss:0.312, val_acc:0.934]
Epoch [27/120    avg_loss:0.313, val_acc:0.941]
Epoch [28/120    avg_loss:0.284, val_acc:0.938]
Epoch [29/120    avg_loss:0.236, val_acc:0.939]
Epoch [30/120    avg_loss:0.294, val_acc:0.912]
Epoch [31/120    avg_loss:0.263, val_acc:0.939]
Epoch [32/120    avg_loss:0.292, val_acc:0.926]
Epoch [33/120    avg_loss:0.264, val_acc:0.924]
Epoch [34/120    avg_loss:0.369, val_acc:0.908]
Epoch [35/120    avg_loss:0.279, val_acc:0.939]
Epoch [36/120    avg_loss:0.292, val_acc:0.951]
Epoch [37/120    avg_loss:0.221, val_acc:0.943]
Epoch [38/120    avg_loss:0.217, val_acc:0.961]
Epoch [39/120    avg_loss:0.204, val_acc:0.961]
Epoch [40/120    avg_loss:0.189, val_acc:0.949]
Epoch [41/120    avg_loss:0.177, val_acc:0.961]
Epoch [42/120    avg_loss:0.224, val_acc:0.932]
Epoch [43/120    avg_loss:0.217, val_acc:0.963]
Epoch [44/120    avg_loss:0.189, val_acc:0.961]
Epoch [45/120    avg_loss:0.196, val_acc:0.957]
Epoch [46/120    avg_loss:0.238, val_acc:0.912]
Epoch [47/120    avg_loss:0.250, val_acc:0.932]
Epoch [48/120    avg_loss:0.257, val_acc:0.920]
Epoch [49/120    avg_loss:0.285, val_acc:0.945]
Epoch [50/120    avg_loss:0.175, val_acc:0.953]
Epoch [51/120    avg_loss:0.171, val_acc:0.971]
Epoch [52/120    avg_loss:0.140, val_acc:0.971]
Epoch [53/120    avg_loss:0.092, val_acc:0.975]
Epoch [54/120    avg_loss:0.134, val_acc:0.955]
Epoch [55/120    avg_loss:0.220, val_acc:0.939]
Epoch [56/120    avg_loss:0.199, val_acc:0.971]
Epoch [57/120    avg_loss:0.098, val_acc:0.965]
Epoch [58/120    avg_loss:0.123, val_acc:0.969]
Epoch [59/120    avg_loss:0.088, val_acc:0.967]
Epoch [60/120    avg_loss:0.121, val_acc:0.971]
Epoch [61/120    avg_loss:0.133, val_acc:0.963]
Epoch [62/120    avg_loss:0.105, val_acc:0.975]
Epoch [63/120    avg_loss:0.072, val_acc:0.975]
Epoch [64/120    avg_loss:0.086, val_acc:0.961]
Epoch [65/120    avg_loss:0.092, val_acc:0.969]
Epoch [66/120    avg_loss:0.110, val_acc:0.959]
Epoch [67/120    avg_loss:0.112, val_acc:0.951]
Epoch [68/120    avg_loss:0.083, val_acc:0.963]
Epoch [69/120    avg_loss:0.062, val_acc:0.979]
Epoch [70/120    avg_loss:0.067, val_acc:0.965]
Epoch [71/120    avg_loss:0.126, val_acc:0.977]
Epoch [72/120    avg_loss:0.073, val_acc:0.975]
Epoch [73/120    avg_loss:0.060, val_acc:0.971]
Epoch [74/120    avg_loss:0.098, val_acc:0.957]
Epoch [75/120    avg_loss:0.130, val_acc:0.977]
Epoch [76/120    avg_loss:0.082, val_acc:0.977]
Epoch [77/120    avg_loss:0.059, val_acc:0.975]
Epoch [78/120    avg_loss:0.066, val_acc:0.971]
Epoch [79/120    avg_loss:0.041, val_acc:0.977]
Epoch [80/120    avg_loss:0.097, val_acc:0.957]
Epoch [81/120    avg_loss:0.121, val_acc:0.959]
Epoch [82/120    avg_loss:0.139, val_acc:0.965]
Epoch [83/120    avg_loss:0.074, val_acc:0.973]
Epoch [84/120    avg_loss:0.068, val_acc:0.975]
Epoch [85/120    avg_loss:0.057, val_acc:0.977]
Epoch [86/120    avg_loss:0.056, val_acc:0.977]
Epoch [87/120    avg_loss:0.054, val_acc:0.977]
Epoch [88/120    avg_loss:0.047, val_acc:0.979]
Epoch [89/120    avg_loss:0.039, val_acc:0.977]
Epoch [90/120    avg_loss:0.040, val_acc:0.975]
Epoch [91/120    avg_loss:0.035, val_acc:0.977]
Epoch [92/120    avg_loss:0.055, val_acc:0.980]
Epoch [93/120    avg_loss:0.051, val_acc:0.977]
Epoch [94/120    avg_loss:0.040, val_acc:0.979]
Epoch [95/120    avg_loss:0.045, val_acc:0.980]
Epoch [96/120    avg_loss:0.032, val_acc:0.979]
Epoch [97/120    avg_loss:0.039, val_acc:0.979]
Epoch [98/120    avg_loss:0.039, val_acc:0.977]
Epoch [99/120    avg_loss:0.039, val_acc:0.977]
Epoch [100/120    avg_loss:0.041, val_acc:0.980]
Epoch [101/120    avg_loss:0.032, val_acc:0.980]
Epoch [102/120    avg_loss:0.044, val_acc:0.982]
Epoch [103/120    avg_loss:0.038, val_acc:0.979]
Epoch [104/120    avg_loss:0.036, val_acc:0.979]
Epoch [105/120    avg_loss:0.030, val_acc:0.977]
Epoch [106/120    avg_loss:0.043, val_acc:0.979]
Epoch [107/120    avg_loss:0.039, val_acc:0.979]
Epoch [108/120    avg_loss:0.039, val_acc:0.979]
Epoch [109/120    avg_loss:0.039, val_acc:0.979]
Epoch [110/120    avg_loss:0.040, val_acc:0.979]
Epoch [111/120    avg_loss:0.034, val_acc:0.980]
Epoch [112/120    avg_loss:0.034, val_acc:0.980]
Epoch [113/120    avg_loss:0.037, val_acc:0.980]
Epoch [114/120    avg_loss:0.039, val_acc:0.980]
Epoch [115/120    avg_loss:0.033, val_acc:0.980]
Epoch [116/120    avg_loss:0.032, val_acc:0.980]
Epoch [117/120    avg_loss:0.030, val_acc:0.980]
Epoch [118/120    avg_loss:0.055, val_acc:0.980]
Epoch [119/120    avg_loss:0.034, val_acc:0.980]
Epoch [120/120    avg_loss:0.036, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   0 214  13   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   1  10 134   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 0.99927061 0.93002257 0.96179775 0.91774892 0.91156463
 0.99756691 0.86170213 0.99614891 1.         1.         1.
 0.99445061 1.        ]

Kappa:
0.9824342888997166
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa11e767f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.487, val_acc:0.481]
Epoch [2/120    avg_loss:2.052, val_acc:0.594]
Epoch [3/120    avg_loss:1.716, val_acc:0.669]
Epoch [4/120    avg_loss:1.438, val_acc:0.694]
Epoch [5/120    avg_loss:1.186, val_acc:0.750]
Epoch [6/120    avg_loss:1.032, val_acc:0.762]
Epoch [7/120    avg_loss:0.858, val_acc:0.856]
Epoch [8/120    avg_loss:0.790, val_acc:0.808]
Epoch [9/120    avg_loss:0.760, val_acc:0.831]
Epoch [10/120    avg_loss:0.706, val_acc:0.873]
Epoch [11/120    avg_loss:0.619, val_acc:0.890]
Epoch [12/120    avg_loss:0.631, val_acc:0.885]
Epoch [13/120    avg_loss:0.533, val_acc:0.887]
Epoch [14/120    avg_loss:0.472, val_acc:0.898]
Epoch [15/120    avg_loss:0.511, val_acc:0.860]
Epoch [16/120    avg_loss:0.487, val_acc:0.904]
Epoch [17/120    avg_loss:0.424, val_acc:0.885]
Epoch [18/120    avg_loss:0.398, val_acc:0.900]
Epoch [19/120    avg_loss:0.346, val_acc:0.919]
Epoch [20/120    avg_loss:0.369, val_acc:0.915]
Epoch [21/120    avg_loss:0.362, val_acc:0.938]
Epoch [22/120    avg_loss:0.323, val_acc:0.885]
Epoch [23/120    avg_loss:0.300, val_acc:0.927]
Epoch [24/120    avg_loss:0.288, val_acc:0.931]
Epoch [25/120    avg_loss:0.314, val_acc:0.873]
Epoch [26/120    avg_loss:0.347, val_acc:0.942]
Epoch [27/120    avg_loss:0.278, val_acc:0.940]
Epoch [28/120    avg_loss:0.284, val_acc:0.904]
Epoch [29/120    avg_loss:0.375, val_acc:0.948]
Epoch [30/120    avg_loss:0.276, val_acc:0.915]
Epoch [31/120    avg_loss:0.229, val_acc:0.950]
Epoch [32/120    avg_loss:0.250, val_acc:0.956]
Epoch [33/120    avg_loss:0.166, val_acc:0.944]
Epoch [34/120    avg_loss:0.252, val_acc:0.942]
Epoch [35/120    avg_loss:0.293, val_acc:0.954]
Epoch [36/120    avg_loss:0.208, val_acc:0.952]
Epoch [37/120    avg_loss:0.196, val_acc:0.931]
Epoch [38/120    avg_loss:0.202, val_acc:0.954]
Epoch [39/120    avg_loss:0.163, val_acc:0.965]
Epoch [40/120    avg_loss:0.190, val_acc:0.950]
Epoch [41/120    avg_loss:0.181, val_acc:0.950]
Epoch [42/120    avg_loss:0.167, val_acc:0.956]
Epoch [43/120    avg_loss:0.136, val_acc:0.965]
Epoch [44/120    avg_loss:0.149, val_acc:0.963]
Epoch [45/120    avg_loss:0.173, val_acc:0.927]
Epoch [46/120    avg_loss:0.186, val_acc:0.938]
Epoch [47/120    avg_loss:0.175, val_acc:0.963]
Epoch [48/120    avg_loss:0.144, val_acc:0.960]
Epoch [49/120    avg_loss:0.200, val_acc:0.923]
Epoch [50/120    avg_loss:0.174, val_acc:0.958]
Epoch [51/120    avg_loss:0.116, val_acc:0.958]
Epoch [52/120    avg_loss:0.137, val_acc:0.950]
Epoch [53/120    avg_loss:0.189, val_acc:0.927]
Epoch [54/120    avg_loss:0.205, val_acc:0.967]
Epoch [55/120    avg_loss:0.165, val_acc:0.946]
Epoch [56/120    avg_loss:0.135, val_acc:0.973]
Epoch [57/120    avg_loss:0.100, val_acc:0.969]
Epoch [58/120    avg_loss:0.107, val_acc:0.956]
Epoch [59/120    avg_loss:0.095, val_acc:0.975]
Epoch [60/120    avg_loss:0.088, val_acc:0.979]
Epoch [61/120    avg_loss:0.088, val_acc:0.931]
Epoch [62/120    avg_loss:0.194, val_acc:0.969]
Epoch [63/120    avg_loss:0.134, val_acc:0.958]
Epoch [64/120    avg_loss:0.102, val_acc:0.983]
Epoch [65/120    avg_loss:0.080, val_acc:0.971]
Epoch [66/120    avg_loss:0.084, val_acc:0.985]
Epoch [67/120    avg_loss:0.068, val_acc:0.985]
Epoch [68/120    avg_loss:0.059, val_acc:0.983]
Epoch [69/120    avg_loss:0.047, val_acc:0.985]
Epoch [70/120    avg_loss:0.059, val_acc:0.975]
Epoch [71/120    avg_loss:0.059, val_acc:0.971]
Epoch [72/120    avg_loss:0.091, val_acc:0.979]
Epoch [73/120    avg_loss:0.060, val_acc:0.988]
Epoch [74/120    avg_loss:0.056, val_acc:0.988]
Epoch [75/120    avg_loss:0.067, val_acc:0.983]
Epoch [76/120    avg_loss:0.050, val_acc:0.977]
Epoch [77/120    avg_loss:0.070, val_acc:0.958]
Epoch [78/120    avg_loss:0.089, val_acc:0.969]
Epoch [79/120    avg_loss:0.089, val_acc:0.971]
Epoch [80/120    avg_loss:0.113, val_acc:0.985]
Epoch [81/120    avg_loss:0.066, val_acc:0.983]
Epoch [82/120    avg_loss:0.064, val_acc:0.979]
Epoch [83/120    avg_loss:0.052, val_acc:0.981]
Epoch [84/120    avg_loss:0.104, val_acc:0.967]
Epoch [85/120    avg_loss:0.090, val_acc:0.990]
Epoch [86/120    avg_loss:0.051, val_acc:0.983]
Epoch [87/120    avg_loss:0.046, val_acc:0.979]
Epoch [88/120    avg_loss:0.043, val_acc:0.979]
Epoch [89/120    avg_loss:0.043, val_acc:0.983]
Epoch [90/120    avg_loss:0.021, val_acc:0.983]
Epoch [91/120    avg_loss:0.026, val_acc:0.992]
Epoch [92/120    avg_loss:0.025, val_acc:0.990]
Epoch [93/120    avg_loss:0.021, val_acc:0.992]
Epoch [94/120    avg_loss:0.016, val_acc:0.988]
Epoch [95/120    avg_loss:0.019, val_acc:0.990]
Epoch [96/120    avg_loss:0.015, val_acc:0.988]
Epoch [97/120    avg_loss:0.021, val_acc:0.992]
Epoch [98/120    avg_loss:0.025, val_acc:0.975]
Epoch [99/120    avg_loss:0.029, val_acc:0.983]
Epoch [100/120    avg_loss:0.031, val_acc:0.992]
Epoch [101/120    avg_loss:0.016, val_acc:0.994]
Epoch [102/120    avg_loss:0.045, val_acc:0.983]
Epoch [103/120    avg_loss:0.038, val_acc:0.990]
Epoch [104/120    avg_loss:0.040, val_acc:0.988]
Epoch [105/120    avg_loss:0.034, val_acc:0.985]
Epoch [106/120    avg_loss:0.024, val_acc:0.990]
Epoch [107/120    avg_loss:0.102, val_acc:0.935]
Epoch [108/120    avg_loss:0.144, val_acc:0.965]
Epoch [109/120    avg_loss:0.129, val_acc:0.983]
Epoch [110/120    avg_loss:0.119, val_acc:0.988]
Epoch [111/120    avg_loss:0.050, val_acc:0.988]
Epoch [112/120    avg_loss:0.072, val_acc:0.990]
Epoch [113/120    avg_loss:0.037, val_acc:0.994]
Epoch [114/120    avg_loss:0.054, val_acc:0.983]
Epoch [115/120    avg_loss:0.043, val_acc:0.994]
Epoch [116/120    avg_loss:0.025, val_acc:0.990]
Epoch [117/120    avg_loss:0.026, val_acc:0.990]
Epoch [118/120    avg_loss:0.021, val_acc:0.994]
Epoch [119/120    avg_loss:0.014, val_acc:0.994]
Epoch [120/120    avg_loss:0.015, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  16   0   0   0   0   0   0   3   0]
 [  0   0   0   0  14 130   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.98636364 1.         0.92650334 0.89347079
 1.         0.96774194 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.990504226580897
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4655425780>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.416, val_acc:0.455]
Epoch [2/120    avg_loss:2.026, val_acc:0.586]
Epoch [3/120    avg_loss:1.733, val_acc:0.664]
Epoch [4/120    avg_loss:1.473, val_acc:0.662]
Epoch [5/120    avg_loss:1.238, val_acc:0.691]
Epoch [6/120    avg_loss:1.011, val_acc:0.744]
Epoch [7/120    avg_loss:0.899, val_acc:0.855]
Epoch [8/120    avg_loss:0.800, val_acc:0.832]
Epoch [9/120    avg_loss:0.708, val_acc:0.834]
Epoch [10/120    avg_loss:0.614, val_acc:0.887]
Epoch [11/120    avg_loss:0.596, val_acc:0.891]
Epoch [12/120    avg_loss:0.551, val_acc:0.873]
Epoch [13/120    avg_loss:0.572, val_acc:0.855]
Epoch [14/120    avg_loss:0.453, val_acc:0.883]
Epoch [15/120    avg_loss:0.460, val_acc:0.914]
Epoch [16/120    avg_loss:0.455, val_acc:0.891]
Epoch [17/120    avg_loss:0.453, val_acc:0.889]
Epoch [18/120    avg_loss:0.394, val_acc:0.924]
Epoch [19/120    avg_loss:0.431, val_acc:0.910]
Epoch [20/120    avg_loss:0.344, val_acc:0.920]
Epoch [21/120    avg_loss:0.444, val_acc:0.900]
Epoch [22/120    avg_loss:0.374, val_acc:0.914]
Epoch [23/120    avg_loss:0.315, val_acc:0.926]
Epoch [24/120    avg_loss:0.346, val_acc:0.912]
Epoch [25/120    avg_loss:0.346, val_acc:0.936]
Epoch [26/120    avg_loss:0.276, val_acc:0.930]
Epoch [27/120    avg_loss:0.257, val_acc:0.945]
Epoch [28/120    avg_loss:0.242, val_acc:0.918]
Epoch [29/120    avg_loss:0.268, val_acc:0.945]
Epoch [30/120    avg_loss:0.297, val_acc:0.893]
Epoch [31/120    avg_loss:0.290, val_acc:0.930]
Epoch [32/120    avg_loss:0.314, val_acc:0.924]
Epoch [33/120    avg_loss:0.273, val_acc:0.928]
Epoch [34/120    avg_loss:0.243, val_acc:0.912]
Epoch [35/120    avg_loss:0.228, val_acc:0.953]
Epoch [36/120    avg_loss:0.205, val_acc:0.959]
Epoch [37/120    avg_loss:0.183, val_acc:0.943]
Epoch [38/120    avg_loss:0.204, val_acc:0.949]
Epoch [39/120    avg_loss:0.201, val_acc:0.963]
Epoch [40/120    avg_loss:0.187, val_acc:0.965]
Epoch [41/120    avg_loss:0.212, val_acc:0.908]
Epoch [42/120    avg_loss:0.200, val_acc:0.953]
Epoch [43/120    avg_loss:0.140, val_acc:0.947]
Epoch [44/120    avg_loss:0.185, val_acc:0.967]
Epoch [45/120    avg_loss:0.165, val_acc:0.955]
Epoch [46/120    avg_loss:0.134, val_acc:0.973]
Epoch [47/120    avg_loss:0.125, val_acc:0.975]
Epoch [48/120    avg_loss:0.154, val_acc:0.967]
Epoch [49/120    avg_loss:0.133, val_acc:0.959]
Epoch [50/120    avg_loss:0.136, val_acc:0.980]
Epoch [51/120    avg_loss:0.131, val_acc:0.979]
Epoch [52/120    avg_loss:0.081, val_acc:0.979]
Epoch [53/120    avg_loss:0.130, val_acc:0.975]
Epoch [54/120    avg_loss:0.108, val_acc:0.955]
Epoch [55/120    avg_loss:0.088, val_acc:0.977]
Epoch [56/120    avg_loss:0.076, val_acc:0.979]
Epoch [57/120    avg_loss:0.103, val_acc:0.973]
Epoch [58/120    avg_loss:0.086, val_acc:0.979]
Epoch [59/120    avg_loss:0.134, val_acc:0.939]
Epoch [60/120    avg_loss:0.117, val_acc:0.977]
Epoch [61/120    avg_loss:0.075, val_acc:0.986]
Epoch [62/120    avg_loss:0.081, val_acc:0.980]
Epoch [63/120    avg_loss:0.057, val_acc:0.982]
Epoch [64/120    avg_loss:0.067, val_acc:0.984]
Epoch [65/120    avg_loss:0.066, val_acc:0.988]
Epoch [66/120    avg_loss:0.070, val_acc:0.961]
Epoch [67/120    avg_loss:0.057, val_acc:0.977]
Epoch [68/120    avg_loss:0.052, val_acc:0.984]
Epoch [69/120    avg_loss:0.047, val_acc:0.986]
Epoch [70/120    avg_loss:0.075, val_acc:0.973]
Epoch [71/120    avg_loss:0.100, val_acc:0.959]
Epoch [72/120    avg_loss:0.105, val_acc:0.967]
Epoch [73/120    avg_loss:0.098, val_acc:0.980]
Epoch [74/120    avg_loss:0.082, val_acc:0.971]
Epoch [75/120    avg_loss:0.078, val_acc:0.969]
Epoch [76/120    avg_loss:0.040, val_acc:0.990]
Epoch [77/120    avg_loss:0.046, val_acc:0.977]
Epoch [78/120    avg_loss:0.041, val_acc:0.988]
Epoch [79/120    avg_loss:0.057, val_acc:0.990]
Epoch [80/120    avg_loss:0.039, val_acc:0.988]
Epoch [81/120    avg_loss:0.042, val_acc:0.984]
Epoch [82/120    avg_loss:0.030, val_acc:0.990]
Epoch [83/120    avg_loss:0.029, val_acc:0.980]
Epoch [84/120    avg_loss:0.033, val_acc:0.992]
Epoch [85/120    avg_loss:0.038, val_acc:0.980]
Epoch [86/120    avg_loss:0.028, val_acc:0.990]
Epoch [87/120    avg_loss:0.043, val_acc:0.971]
Epoch [88/120    avg_loss:0.070, val_acc:0.971]
Epoch [89/120    avg_loss:0.065, val_acc:0.965]
Epoch [90/120    avg_loss:0.117, val_acc:0.926]
Epoch [91/120    avg_loss:0.069, val_acc:0.986]
Epoch [92/120    avg_loss:0.036, val_acc:0.984]
Epoch [93/120    avg_loss:0.037, val_acc:0.992]
Epoch [94/120    avg_loss:0.022, val_acc:0.986]
Epoch [95/120    avg_loss:0.028, val_acc:0.988]
Epoch [96/120    avg_loss:0.022, val_acc:0.988]
Epoch [97/120    avg_loss:0.022, val_acc:0.986]
Epoch [98/120    avg_loss:0.022, val_acc:0.988]
Epoch [99/120    avg_loss:0.020, val_acc:0.992]
Epoch [100/120    avg_loss:0.029, val_acc:0.984]
Epoch [101/120    avg_loss:0.070, val_acc:0.984]
Epoch [102/120    avg_loss:0.036, val_acc:0.986]
Epoch [103/120    avg_loss:0.023, val_acc:0.988]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.994]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.012, val_acc:0.980]
Epoch [111/120    avg_loss:0.027, val_acc:0.953]
Epoch [112/120    avg_loss:0.089, val_acc:0.947]
Epoch [113/120    avg_loss:0.080, val_acc:0.957]
Epoch [114/120    avg_loss:0.049, val_acc:0.979]
Epoch [115/120    avg_loss:0.017, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.018, val_acc:0.986]
Epoch [118/120    avg_loss:0.014, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.023, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 210  20   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11   0 195   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.99543379 0.95454545 0.91803279 0.96864111
 0.97256858 0.9893617  1.         1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9876563786452921
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fed8d1d4828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.460, val_acc:0.415]
Epoch [2/120    avg_loss:2.012, val_acc:0.600]
Epoch [3/120    avg_loss:1.689, val_acc:0.692]
Epoch [4/120    avg_loss:1.493, val_acc:0.654]
Epoch [5/120    avg_loss:1.250, val_acc:0.738]
Epoch [6/120    avg_loss:1.075, val_acc:0.735]
Epoch [7/120    avg_loss:0.920, val_acc:0.775]
Epoch [8/120    avg_loss:0.856, val_acc:0.869]
Epoch [9/120    avg_loss:0.745, val_acc:0.869]
Epoch [10/120    avg_loss:0.685, val_acc:0.887]
Epoch [11/120    avg_loss:0.743, val_acc:0.838]
Epoch [12/120    avg_loss:0.700, val_acc:0.910]
Epoch [13/120    avg_loss:0.575, val_acc:0.896]
Epoch [14/120    avg_loss:0.517, val_acc:0.900]
Epoch [15/120    avg_loss:0.489, val_acc:0.887]
Epoch [16/120    avg_loss:0.446, val_acc:0.933]
Epoch [17/120    avg_loss:0.431, val_acc:0.910]
Epoch [18/120    avg_loss:0.386, val_acc:0.935]
Epoch [19/120    avg_loss:0.385, val_acc:0.902]
Epoch [20/120    avg_loss:0.409, val_acc:0.942]
Epoch [21/120    avg_loss:0.388, val_acc:0.940]
Epoch [22/120    avg_loss:0.368, val_acc:0.931]
Epoch [23/120    avg_loss:0.341, val_acc:0.915]
Epoch [24/120    avg_loss:0.307, val_acc:0.931]
Epoch [25/120    avg_loss:0.255, val_acc:0.940]
Epoch [26/120    avg_loss:0.284, val_acc:0.952]
Epoch [27/120    avg_loss:0.242, val_acc:0.944]
Epoch [28/120    avg_loss:0.266, val_acc:0.935]
Epoch [29/120    avg_loss:0.234, val_acc:0.948]
Epoch [30/120    avg_loss:0.277, val_acc:0.925]
Epoch [31/120    avg_loss:0.258, val_acc:0.940]
Epoch [32/120    avg_loss:0.258, val_acc:0.938]
Epoch [33/120    avg_loss:0.231, val_acc:0.960]
Epoch [34/120    avg_loss:0.229, val_acc:0.956]
Epoch [35/120    avg_loss:0.228, val_acc:0.958]
Epoch [36/120    avg_loss:0.183, val_acc:0.954]
Epoch [37/120    avg_loss:0.179, val_acc:0.956]
Epoch [38/120    avg_loss:0.174, val_acc:0.933]
Epoch [39/120    avg_loss:0.228, val_acc:0.963]
Epoch [40/120    avg_loss:0.141, val_acc:0.977]
Epoch [41/120    avg_loss:0.152, val_acc:0.958]
Epoch [42/120    avg_loss:0.159, val_acc:0.967]
Epoch [43/120    avg_loss:0.183, val_acc:0.956]
Epoch [44/120    avg_loss:0.147, val_acc:0.975]
Epoch [45/120    avg_loss:0.151, val_acc:0.969]
Epoch [46/120    avg_loss:0.139, val_acc:0.971]
Epoch [47/120    avg_loss:0.114, val_acc:0.944]
Epoch [48/120    avg_loss:0.108, val_acc:0.956]
Epoch [49/120    avg_loss:0.103, val_acc:0.969]
Epoch [50/120    avg_loss:0.095, val_acc:0.971]
Epoch [51/120    avg_loss:0.118, val_acc:0.967]
Epoch [52/120    avg_loss:0.095, val_acc:0.975]
Epoch [53/120    avg_loss:0.086, val_acc:0.981]
Epoch [54/120    avg_loss:0.089, val_acc:0.954]
Epoch [55/120    avg_loss:0.111, val_acc:0.979]
Epoch [56/120    avg_loss:0.075, val_acc:0.967]
Epoch [57/120    avg_loss:0.126, val_acc:0.973]
Epoch [58/120    avg_loss:0.098, val_acc:0.973]
Epoch [59/120    avg_loss:0.091, val_acc:0.981]
Epoch [60/120    avg_loss:0.152, val_acc:0.973]
Epoch [61/120    avg_loss:0.108, val_acc:0.969]
Epoch [62/120    avg_loss:0.164, val_acc:0.975]
Epoch [63/120    avg_loss:0.097, val_acc:0.960]
Epoch [64/120    avg_loss:0.077, val_acc:0.977]
Epoch [65/120    avg_loss:0.061, val_acc:0.975]
Epoch [66/120    avg_loss:0.093, val_acc:0.971]
Epoch [67/120    avg_loss:0.071, val_acc:0.981]
Epoch [68/120    avg_loss:0.081, val_acc:0.979]
Epoch [69/120    avg_loss:0.123, val_acc:0.967]
Epoch [70/120    avg_loss:0.100, val_acc:0.969]
Epoch [71/120    avg_loss:0.068, val_acc:0.979]
Epoch [72/120    avg_loss:0.069, val_acc:0.965]
Epoch [73/120    avg_loss:0.043, val_acc:0.990]
Epoch [74/120    avg_loss:0.048, val_acc:0.988]
Epoch [75/120    avg_loss:0.068, val_acc:0.983]
Epoch [76/120    avg_loss:0.105, val_acc:0.971]
Epoch [77/120    avg_loss:0.124, val_acc:0.967]
Epoch [78/120    avg_loss:0.126, val_acc:0.967]
Epoch [79/120    avg_loss:0.080, val_acc:0.983]
Epoch [80/120    avg_loss:0.066, val_acc:0.975]
Epoch [81/120    avg_loss:0.055, val_acc:0.981]
Epoch [82/120    avg_loss:0.066, val_acc:0.971]
Epoch [83/120    avg_loss:0.066, val_acc:0.981]
Epoch [84/120    avg_loss:0.056, val_acc:0.983]
Epoch [85/120    avg_loss:0.061, val_acc:0.985]
Epoch [86/120    avg_loss:0.044, val_acc:0.979]
Epoch [87/120    avg_loss:0.038, val_acc:0.981]
Epoch [88/120    avg_loss:0.034, val_acc:0.985]
Epoch [89/120    avg_loss:0.028, val_acc:0.985]
Epoch [90/120    avg_loss:0.023, val_acc:0.985]
Epoch [91/120    avg_loss:0.021, val_acc:0.988]
Epoch [92/120    avg_loss:0.024, val_acc:0.985]
Epoch [93/120    avg_loss:0.020, val_acc:0.985]
Epoch [94/120    avg_loss:0.030, val_acc:0.985]
Epoch [95/120    avg_loss:0.027, val_acc:0.985]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.029, val_acc:0.983]
Epoch [98/120    avg_loss:0.025, val_acc:0.985]
Epoch [99/120    avg_loss:0.030, val_acc:0.985]
Epoch [100/120    avg_loss:0.020, val_acc:0.985]
Epoch [101/120    avg_loss:0.024, val_acc:0.985]
Epoch [102/120    avg_loss:0.024, val_acc:0.985]
Epoch [103/120    avg_loss:0.021, val_acc:0.985]
Epoch [104/120    avg_loss:0.022, val_acc:0.985]
Epoch [105/120    avg_loss:0.018, val_acc:0.985]
Epoch [106/120    avg_loss:0.017, val_acc:0.985]
Epoch [107/120    avg_loss:0.039, val_acc:0.985]
Epoch [108/120    avg_loss:0.030, val_acc:0.985]
Epoch [109/120    avg_loss:0.023, val_acc:0.985]
Epoch [110/120    avg_loss:0.022, val_acc:0.985]
Epoch [111/120    avg_loss:0.028, val_acc:0.985]
Epoch [112/120    avg_loss:0.029, val_acc:0.985]
Epoch [113/120    avg_loss:0.019, val_acc:0.985]
Epoch [114/120    avg_loss:0.025, val_acc:0.985]
Epoch [115/120    avg_loss:0.030, val_acc:0.985]
Epoch [116/120    avg_loss:0.031, val_acc:0.985]
Epoch [117/120    avg_loss:0.017, val_acc:0.985]
Epoch [118/120    avg_loss:0.025, val_acc:0.985]
Epoch [119/120    avg_loss:0.031, val_acc:0.985]
Epoch [120/120    avg_loss:0.025, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.98206278 0.98454746 0.9254386  0.9047619
 1.         0.96132597 1.         0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.9897923220667908
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f841bc53860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.401, val_acc:0.404]
Epoch [2/120    avg_loss:2.033, val_acc:0.487]
Epoch [3/120    avg_loss:1.724, val_acc:0.744]
Epoch [4/120    avg_loss:1.424, val_acc:0.738]
Epoch [5/120    avg_loss:1.208, val_acc:0.785]
Epoch [6/120    avg_loss:1.061, val_acc:0.804]
Epoch [7/120    avg_loss:0.900, val_acc:0.844]
Epoch [8/120    avg_loss:0.776, val_acc:0.875]
Epoch [9/120    avg_loss:0.756, val_acc:0.846]
Epoch [10/120    avg_loss:0.660, val_acc:0.879]
Epoch [11/120    avg_loss:0.597, val_acc:0.892]
Epoch [12/120    avg_loss:0.505, val_acc:0.892]
Epoch [13/120    avg_loss:0.526, val_acc:0.894]
Epoch [14/120    avg_loss:0.479, val_acc:0.867]
Epoch [15/120    avg_loss:0.539, val_acc:0.898]
Epoch [16/120    avg_loss:0.437, val_acc:0.900]
Epoch [17/120    avg_loss:0.415, val_acc:0.917]
Epoch [18/120    avg_loss:0.460, val_acc:0.890]
Epoch [19/120    avg_loss:0.488, val_acc:0.902]
Epoch [20/120    avg_loss:0.370, val_acc:0.910]
Epoch [21/120    avg_loss:0.345, val_acc:0.927]
Epoch [22/120    avg_loss:0.323, val_acc:0.910]
Epoch [23/120    avg_loss:0.297, val_acc:0.935]
Epoch [24/120    avg_loss:0.305, val_acc:0.938]
Epoch [25/120    avg_loss:0.301, val_acc:0.942]
Epoch [26/120    avg_loss:0.308, val_acc:0.931]
Epoch [27/120    avg_loss:0.311, val_acc:0.954]
Epoch [28/120    avg_loss:0.283, val_acc:0.931]
Epoch [29/120    avg_loss:0.284, val_acc:0.935]
Epoch [30/120    avg_loss:0.256, val_acc:0.906]
Epoch [31/120    avg_loss:0.242, val_acc:0.954]
Epoch [32/120    avg_loss:0.267, val_acc:0.952]
Epoch [33/120    avg_loss:0.212, val_acc:0.908]
Epoch [34/120    avg_loss:0.290, val_acc:0.935]
Epoch [35/120    avg_loss:0.232, val_acc:0.948]
Epoch [36/120    avg_loss:0.218, val_acc:0.940]
Epoch [37/120    avg_loss:0.198, val_acc:0.929]
Epoch [38/120    avg_loss:0.251, val_acc:0.954]
Epoch [39/120    avg_loss:0.214, val_acc:0.963]
Epoch [40/120    avg_loss:0.200, val_acc:0.946]
Epoch [41/120    avg_loss:0.237, val_acc:0.919]
Epoch [42/120    avg_loss:0.238, val_acc:0.956]
Epoch [43/120    avg_loss:0.196, val_acc:0.952]
Epoch [44/120    avg_loss:0.164, val_acc:0.956]
Epoch [45/120    avg_loss:0.173, val_acc:0.967]
Epoch [46/120    avg_loss:0.142, val_acc:0.960]
Epoch [47/120    avg_loss:0.123, val_acc:0.944]
Epoch [48/120    avg_loss:0.125, val_acc:0.977]
Epoch [49/120    avg_loss:0.098, val_acc:0.981]
Epoch [50/120    avg_loss:0.139, val_acc:0.942]
Epoch [51/120    avg_loss:0.146, val_acc:0.965]
Epoch [52/120    avg_loss:0.183, val_acc:0.908]
Epoch [53/120    avg_loss:0.199, val_acc:0.963]
Epoch [54/120    avg_loss:0.182, val_acc:0.952]
Epoch [55/120    avg_loss:0.121, val_acc:0.967]
Epoch [56/120    avg_loss:0.083, val_acc:0.985]
Epoch [57/120    avg_loss:0.104, val_acc:0.973]
Epoch [58/120    avg_loss:0.100, val_acc:0.967]
Epoch [59/120    avg_loss:0.127, val_acc:0.977]
Epoch [60/120    avg_loss:0.117, val_acc:0.975]
Epoch [61/120    avg_loss:0.223, val_acc:0.921]
Epoch [62/120    avg_loss:0.127, val_acc:0.958]
Epoch [63/120    avg_loss:0.125, val_acc:0.965]
Epoch [64/120    avg_loss:0.071, val_acc:0.977]
Epoch [65/120    avg_loss:0.101, val_acc:0.979]
Epoch [66/120    avg_loss:0.064, val_acc:0.983]
Epoch [67/120    avg_loss:0.060, val_acc:0.985]
Epoch [68/120    avg_loss:0.056, val_acc:0.981]
Epoch [69/120    avg_loss:0.105, val_acc:0.950]
Epoch [70/120    avg_loss:0.149, val_acc:0.975]
Epoch [71/120    avg_loss:0.115, val_acc:0.983]
Epoch [72/120    avg_loss:0.078, val_acc:0.990]
Epoch [73/120    avg_loss:0.063, val_acc:0.990]
Epoch [74/120    avg_loss:0.068, val_acc:0.981]
Epoch [75/120    avg_loss:0.095, val_acc:0.983]
Epoch [76/120    avg_loss:0.089, val_acc:0.967]
Epoch [77/120    avg_loss:0.056, val_acc:0.985]
Epoch [78/120    avg_loss:0.067, val_acc:0.985]
Epoch [79/120    avg_loss:0.046, val_acc:0.992]
Epoch [80/120    avg_loss:0.025, val_acc:0.990]
Epoch [81/120    avg_loss:0.050, val_acc:0.994]
Epoch [82/120    avg_loss:0.046, val_acc:0.992]
Epoch [83/120    avg_loss:0.037, val_acc:0.994]
Epoch [84/120    avg_loss:0.039, val_acc:0.988]
Epoch [85/120    avg_loss:0.052, val_acc:0.994]
Epoch [86/120    avg_loss:0.055, val_acc:0.940]
Epoch [87/120    avg_loss:0.094, val_acc:0.985]
Epoch [88/120    avg_loss:0.074, val_acc:0.990]
Epoch [89/120    avg_loss:0.051, val_acc:0.973]
Epoch [90/120    avg_loss:0.065, val_acc:0.985]
Epoch [91/120    avg_loss:0.089, val_acc:0.983]
Epoch [92/120    avg_loss:0.044, val_acc:0.996]
Epoch [93/120    avg_loss:0.040, val_acc:0.992]
Epoch [94/120    avg_loss:0.034, val_acc:0.994]
Epoch [95/120    avg_loss:0.054, val_acc:0.977]
Epoch [96/120    avg_loss:0.055, val_acc:0.992]
Epoch [97/120    avg_loss:0.049, val_acc:0.992]
Epoch [98/120    avg_loss:0.038, val_acc:0.992]
Epoch [99/120    avg_loss:0.056, val_acc:0.998]
Epoch [100/120    avg_loss:0.032, val_acc:0.994]
Epoch [101/120    avg_loss:0.021, val_acc:0.998]
Epoch [102/120    avg_loss:0.022, val_acc:1.000]
Epoch [103/120    avg_loss:0.017, val_acc:0.998]
Epoch [104/120    avg_loss:0.021, val_acc:0.998]
Epoch [105/120    avg_loss:0.013, val_acc:0.996]
Epoch [106/120    avg_loss:0.013, val_acc:0.998]
Epoch [107/120    avg_loss:0.025, val_acc:0.998]
Epoch [108/120    avg_loss:0.015, val_acc:0.998]
Epoch [109/120    avg_loss:0.012, val_acc:0.998]
Epoch [110/120    avg_loss:0.014, val_acc:0.990]
Epoch [111/120    avg_loss:0.017, val_acc:0.998]
Epoch [112/120    avg_loss:0.021, val_acc:1.000]
Epoch [113/120    avg_loss:0.010, val_acc:0.998]
Epoch [114/120    avg_loss:0.013, val_acc:0.998]
Epoch [115/120    avg_loss:0.016, val_acc:0.996]
Epoch [116/120    avg_loss:0.073, val_acc:0.977]
Epoch [117/120    avg_loss:0.090, val_acc:0.992]
Epoch [118/120    avg_loss:0.052, val_acc:0.996]
Epoch [119/120    avg_loss:0.021, val_acc:0.996]
Epoch [120/120    avg_loss:0.078, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 641   0   0   0   0  44   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 220   9   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.33688699360341

F1 scores:
[       nan 0.9668175  0.99086758 0.97777778 0.93970894 0.92647059
 0.90350877 0.9787234  1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9815023993737251
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac53b63828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.413, val_acc:0.352]
Epoch [2/120    avg_loss:2.000, val_acc:0.579]
Epoch [3/120    avg_loss:1.673, val_acc:0.631]
Epoch [4/120    avg_loss:1.403, val_acc:0.746]
Epoch [5/120    avg_loss:1.225, val_acc:0.700]
Epoch [6/120    avg_loss:1.031, val_acc:0.733]
Epoch [7/120    avg_loss:0.902, val_acc:0.808]
Epoch [8/120    avg_loss:0.827, val_acc:0.812]
Epoch [9/120    avg_loss:0.745, val_acc:0.910]
Epoch [10/120    avg_loss:0.624, val_acc:0.917]
Epoch [11/120    avg_loss:0.601, val_acc:0.896]
Epoch [12/120    avg_loss:0.620, val_acc:0.825]
Epoch [13/120    avg_loss:0.515, val_acc:0.942]
Epoch [14/120    avg_loss:0.503, val_acc:0.921]
Epoch [15/120    avg_loss:0.482, val_acc:0.942]
Epoch [16/120    avg_loss:0.388, val_acc:0.952]
Epoch [17/120    avg_loss:0.346, val_acc:0.935]
Epoch [18/120    avg_loss:0.346, val_acc:0.933]
Epoch [19/120    avg_loss:0.361, val_acc:0.927]
Epoch [20/120    avg_loss:0.322, val_acc:0.925]
Epoch [21/120    avg_loss:0.335, val_acc:0.944]
Epoch [22/120    avg_loss:0.336, val_acc:0.942]
Epoch [23/120    avg_loss:0.336, val_acc:0.952]
Epoch [24/120    avg_loss:0.270, val_acc:0.960]
Epoch [25/120    avg_loss:0.261, val_acc:0.929]
Epoch [26/120    avg_loss:0.268, val_acc:0.960]
Epoch [27/120    avg_loss:0.242, val_acc:0.969]
Epoch [28/120    avg_loss:0.277, val_acc:0.921]
Epoch [29/120    avg_loss:0.357, val_acc:0.950]
Epoch [30/120    avg_loss:0.274, val_acc:0.927]
Epoch [31/120    avg_loss:0.254, val_acc:0.927]
Epoch [32/120    avg_loss:0.306, val_acc:0.940]
Epoch [33/120    avg_loss:0.228, val_acc:0.944]
Epoch [34/120    avg_loss:0.228, val_acc:0.931]
Epoch [35/120    avg_loss:0.196, val_acc:0.910]
Epoch [36/120    avg_loss:0.184, val_acc:0.971]
Epoch [37/120    avg_loss:0.168, val_acc:0.954]
Epoch [38/120    avg_loss:0.172, val_acc:0.942]
Epoch [39/120    avg_loss:0.204, val_acc:0.960]
Epoch [40/120    avg_loss:0.161, val_acc:0.975]
Epoch [41/120    avg_loss:0.175, val_acc:0.956]
Epoch [42/120    avg_loss:0.180, val_acc:0.967]
Epoch [43/120    avg_loss:0.128, val_acc:0.979]
Epoch [44/120    avg_loss:0.132, val_acc:0.969]
Epoch [45/120    avg_loss:0.127, val_acc:0.977]
Epoch [46/120    avg_loss:0.124, val_acc:0.967]
Epoch [47/120    avg_loss:0.109, val_acc:0.975]
Epoch [48/120    avg_loss:0.133, val_acc:0.963]
Epoch [49/120    avg_loss:0.156, val_acc:0.938]
Epoch [50/120    avg_loss:0.135, val_acc:0.971]
Epoch [51/120    avg_loss:0.111, val_acc:0.977]
Epoch [52/120    avg_loss:0.100, val_acc:0.969]
Epoch [53/120    avg_loss:0.132, val_acc:0.965]
Epoch [54/120    avg_loss:0.124, val_acc:0.965]
Epoch [55/120    avg_loss:0.108, val_acc:0.971]
Epoch [56/120    avg_loss:0.139, val_acc:0.981]
Epoch [57/120    avg_loss:0.098, val_acc:0.975]
Epoch [58/120    avg_loss:0.099, val_acc:0.977]
Epoch [59/120    avg_loss:0.096, val_acc:0.979]
Epoch [60/120    avg_loss:0.078, val_acc:0.977]
Epoch [61/120    avg_loss:0.070, val_acc:0.981]
Epoch [62/120    avg_loss:0.096, val_acc:0.971]
Epoch [63/120    avg_loss:0.136, val_acc:0.967]
Epoch [64/120    avg_loss:0.089, val_acc:0.981]
Epoch [65/120    avg_loss:0.098, val_acc:0.967]
Epoch [66/120    avg_loss:0.087, val_acc:0.979]
Epoch [67/120    avg_loss:0.082, val_acc:0.981]
Epoch [68/120    avg_loss:0.079, val_acc:0.985]
Epoch [69/120    avg_loss:0.080, val_acc:0.973]
Epoch [70/120    avg_loss:0.098, val_acc:0.977]
Epoch [71/120    avg_loss:0.050, val_acc:0.975]
Epoch [72/120    avg_loss:0.053, val_acc:0.988]
Epoch [73/120    avg_loss:0.055, val_acc:0.977]
Epoch [74/120    avg_loss:0.074, val_acc:0.990]
Epoch [75/120    avg_loss:0.053, val_acc:0.981]
Epoch [76/120    avg_loss:0.066, val_acc:0.988]
Epoch [77/120    avg_loss:0.071, val_acc:0.985]
Epoch [78/120    avg_loss:0.071, val_acc:0.977]
Epoch [79/120    avg_loss:0.054, val_acc:0.988]
Epoch [80/120    avg_loss:0.044, val_acc:0.985]
Epoch [81/120    avg_loss:0.035, val_acc:0.990]
Epoch [82/120    avg_loss:0.027, val_acc:0.983]
Epoch [83/120    avg_loss:0.037, val_acc:0.983]
Epoch [84/120    avg_loss:0.022, val_acc:0.994]
Epoch [85/120    avg_loss:0.035, val_acc:0.981]
Epoch [86/120    avg_loss:0.037, val_acc:0.983]
Epoch [87/120    avg_loss:0.047, val_acc:0.979]
Epoch [88/120    avg_loss:0.033, val_acc:0.981]
Epoch [89/120    avg_loss:0.080, val_acc:0.971]
Epoch [90/120    avg_loss:0.091, val_acc:0.985]
Epoch [91/120    avg_loss:0.088, val_acc:0.985]
Epoch [92/120    avg_loss:0.049, val_acc:0.988]
Epoch [93/120    avg_loss:0.031, val_acc:0.988]
Epoch [94/120    avg_loss:0.037, val_acc:0.981]
Epoch [95/120    avg_loss:0.043, val_acc:0.988]
Epoch [96/120    avg_loss:0.036, val_acc:0.992]
Epoch [97/120    avg_loss:0.049, val_acc:0.996]
Epoch [98/120    avg_loss:0.100, val_acc:0.931]
Epoch [99/120    avg_loss:0.117, val_acc:0.985]
Epoch [100/120    avg_loss:0.065, val_acc:0.973]
Epoch [101/120    avg_loss:0.040, val_acc:0.990]
Epoch [102/120    avg_loss:0.033, val_acc:0.992]
Epoch [103/120    avg_loss:0.037, val_acc:0.992]
Epoch [104/120    avg_loss:0.036, val_acc:0.988]
Epoch [105/120    avg_loss:0.041, val_acc:0.988]
Epoch [106/120    avg_loss:0.048, val_acc:0.979]
Epoch [107/120    avg_loss:0.041, val_acc:0.990]
Epoch [108/120    avg_loss:0.061, val_acc:0.983]
Epoch [109/120    avg_loss:0.047, val_acc:0.988]
Epoch [110/120    avg_loss:0.040, val_acc:0.988]
Epoch [111/120    avg_loss:0.028, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.992]
Epoch [113/120    avg_loss:0.016, val_acc:0.994]
Epoch [114/120    avg_loss:0.014, val_acc:0.996]
Epoch [115/120    avg_loss:0.017, val_acc:0.996]
Epoch [116/120    avg_loss:0.022, val_acc:0.996]
Epoch [117/120    avg_loss:0.021, val_acc:0.994]
Epoch [118/120    avg_loss:0.017, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.015, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 224   4   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 1.         0.99543379 0.98678414 0.96153846 0.95
 1.         0.99470899 0.998713   0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.9947773846278052
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a822887b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.435, val_acc:0.430]
Epoch [2/120    avg_loss:2.012, val_acc:0.613]
Epoch [3/120    avg_loss:1.716, val_acc:0.633]
Epoch [4/120    avg_loss:1.465, val_acc:0.654]
Epoch [5/120    avg_loss:1.234, val_acc:0.777]
Epoch [6/120    avg_loss:1.091, val_acc:0.840]
Epoch [7/120    avg_loss:0.946, val_acc:0.848]
Epoch [8/120    avg_loss:0.845, val_acc:0.883]
Epoch [9/120    avg_loss:0.791, val_acc:0.809]
Epoch [10/120    avg_loss:0.767, val_acc:0.871]
Epoch [11/120    avg_loss:0.646, val_acc:0.914]
Epoch [12/120    avg_loss:0.564, val_acc:0.902]
Epoch [13/120    avg_loss:0.555, val_acc:0.898]
Epoch [14/120    avg_loss:0.511, val_acc:0.865]
Epoch [15/120    avg_loss:0.491, val_acc:0.898]
Epoch [16/120    avg_loss:0.464, val_acc:0.918]
Epoch [17/120    avg_loss:0.428, val_acc:0.906]
Epoch [18/120    avg_loss:0.406, val_acc:0.943]
Epoch [19/120    avg_loss:0.350, val_acc:0.928]
Epoch [20/120    avg_loss:0.319, val_acc:0.957]
Epoch [21/120    avg_loss:0.313, val_acc:0.943]
Epoch [22/120    avg_loss:0.325, val_acc:0.932]
Epoch [23/120    avg_loss:0.313, val_acc:0.955]
Epoch [24/120    avg_loss:0.291, val_acc:0.945]
Epoch [25/120    avg_loss:0.289, val_acc:0.947]
Epoch [26/120    avg_loss:0.265, val_acc:0.949]
Epoch [27/120    avg_loss:0.293, val_acc:0.951]
Epoch [28/120    avg_loss:0.275, val_acc:0.934]
Epoch [29/120    avg_loss:0.288, val_acc:0.932]
Epoch [30/120    avg_loss:0.245, val_acc:0.947]
Epoch [31/120    avg_loss:0.305, val_acc:0.951]
Epoch [32/120    avg_loss:0.239, val_acc:0.961]
Epoch [33/120    avg_loss:0.248, val_acc:0.938]
Epoch [34/120    avg_loss:0.321, val_acc:0.936]
Epoch [35/120    avg_loss:0.251, val_acc:0.939]
Epoch [36/120    avg_loss:0.202, val_acc:0.951]
Epoch [37/120    avg_loss:0.225, val_acc:0.955]
Epoch [38/120    avg_loss:0.168, val_acc:0.980]
Epoch [39/120    avg_loss:0.194, val_acc:0.914]
Epoch [40/120    avg_loss:0.219, val_acc:0.973]
Epoch [41/120    avg_loss:0.139, val_acc:0.977]
Epoch [42/120    avg_loss:0.176, val_acc:0.973]
Epoch [43/120    avg_loss:0.183, val_acc:0.973]
Epoch [44/120    avg_loss:0.141, val_acc:0.934]
Epoch [45/120    avg_loss:0.158, val_acc:0.969]
Epoch [46/120    avg_loss:0.109, val_acc:0.971]
Epoch [47/120    avg_loss:0.152, val_acc:0.971]
Epoch [48/120    avg_loss:0.138, val_acc:0.984]
Epoch [49/120    avg_loss:0.136, val_acc:0.973]
Epoch [50/120    avg_loss:0.122, val_acc:0.969]
Epoch [51/120    avg_loss:0.121, val_acc:0.977]
Epoch [52/120    avg_loss:0.085, val_acc:0.984]
Epoch [53/120    avg_loss:0.093, val_acc:0.963]
Epoch [54/120    avg_loss:0.160, val_acc:0.971]
Epoch [55/120    avg_loss:0.138, val_acc:0.947]
Epoch [56/120    avg_loss:0.117, val_acc:0.973]
Epoch [57/120    avg_loss:0.113, val_acc:0.977]
Epoch [58/120    avg_loss:0.100, val_acc:0.965]
Epoch [59/120    avg_loss:0.102, val_acc:0.967]
Epoch [60/120    avg_loss:0.122, val_acc:0.979]
Epoch [61/120    avg_loss:0.085, val_acc:0.973]
Epoch [62/120    avg_loss:0.104, val_acc:0.969]
Epoch [63/120    avg_loss:0.072, val_acc:0.971]
Epoch [64/120    avg_loss:0.136, val_acc:0.975]
Epoch [65/120    avg_loss:0.100, val_acc:0.963]
Epoch [66/120    avg_loss:0.102, val_acc:0.969]
Epoch [67/120    avg_loss:0.075, val_acc:0.979]
Epoch [68/120    avg_loss:0.101, val_acc:0.979]
Epoch [69/120    avg_loss:0.061, val_acc:0.980]
Epoch [70/120    avg_loss:0.066, val_acc:0.980]
Epoch [71/120    avg_loss:0.072, val_acc:0.982]
Epoch [72/120    avg_loss:0.060, val_acc:0.982]
Epoch [73/120    avg_loss:0.044, val_acc:0.982]
Epoch [74/120    avg_loss:0.046, val_acc:0.982]
Epoch [75/120    avg_loss:0.035, val_acc:0.982]
Epoch [76/120    avg_loss:0.059, val_acc:0.982]
Epoch [77/120    avg_loss:0.042, val_acc:0.984]
Epoch [78/120    avg_loss:0.043, val_acc:0.988]
Epoch [79/120    avg_loss:0.036, val_acc:0.986]
Epoch [80/120    avg_loss:0.037, val_acc:0.984]
Epoch [81/120    avg_loss:0.035, val_acc:0.986]
Epoch [82/120    avg_loss:0.059, val_acc:0.986]
Epoch [83/120    avg_loss:0.036, val_acc:0.986]
Epoch [84/120    avg_loss:0.042, val_acc:0.986]
Epoch [85/120    avg_loss:0.031, val_acc:0.986]
Epoch [86/120    avg_loss:0.035, val_acc:0.984]
Epoch [87/120    avg_loss:0.045, val_acc:0.986]
Epoch [88/120    avg_loss:0.036, val_acc:0.984]
Epoch [89/120    avg_loss:0.043, val_acc:0.984]
Epoch [90/120    avg_loss:0.046, val_acc:0.988]
Epoch [91/120    avg_loss:0.040, val_acc:0.990]
Epoch [92/120    avg_loss:0.033, val_acc:0.988]
Epoch [93/120    avg_loss:0.033, val_acc:0.986]
Epoch [94/120    avg_loss:0.029, val_acc:0.988]
Epoch [95/120    avg_loss:0.033, val_acc:0.986]
Epoch [96/120    avg_loss:0.033, val_acc:0.986]
Epoch [97/120    avg_loss:0.031, val_acc:0.986]
Epoch [98/120    avg_loss:0.042, val_acc:0.986]
Epoch [99/120    avg_loss:0.033, val_acc:0.986]
Epoch [100/120    avg_loss:0.033, val_acc:0.984]
Epoch [101/120    avg_loss:0.030, val_acc:0.986]
Epoch [102/120    avg_loss:0.040, val_acc:0.988]
Epoch [103/120    avg_loss:0.029, val_acc:0.988]
Epoch [104/120    avg_loss:0.029, val_acc:0.990]
Epoch [105/120    avg_loss:0.046, val_acc:0.984]
Epoch [106/120    avg_loss:0.028, val_acc:0.984]
Epoch [107/120    avg_loss:0.030, val_acc:0.988]
Epoch [108/120    avg_loss:0.029, val_acc:0.990]
Epoch [109/120    avg_loss:0.034, val_acc:0.988]
Epoch [110/120    avg_loss:0.028, val_acc:0.990]
Epoch [111/120    avg_loss:0.027, val_acc:0.990]
Epoch [112/120    avg_loss:0.039, val_acc:0.984]
Epoch [113/120    avg_loss:0.031, val_acc:0.984]
Epoch [114/120    avg_loss:0.038, val_acc:0.986]
Epoch [115/120    avg_loss:0.037, val_acc:0.986]
Epoch [116/120    avg_loss:0.032, val_acc:0.988]
Epoch [117/120    avg_loss:0.038, val_acc:0.990]
Epoch [118/120    avg_loss:0.037, val_acc:0.984]
Epoch [119/120    avg_loss:0.019, val_acc:0.982]
Epoch [120/120    avg_loss:0.025, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 202  27   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.97737557 0.93518519 0.90987124 0.95081967
 1.         0.94565217 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9874191477665811
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4663f8d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.476, val_acc:0.465]
Epoch [2/120    avg_loss:2.096, val_acc:0.650]
Epoch [3/120    avg_loss:1.801, val_acc:0.708]
Epoch [4/120    avg_loss:1.527, val_acc:0.677]
Epoch [5/120    avg_loss:1.336, val_acc:0.675]
Epoch [6/120    avg_loss:1.103, val_acc:0.733]
Epoch [7/120    avg_loss:0.962, val_acc:0.735]
Epoch [8/120    avg_loss:0.836, val_acc:0.781]
Epoch [9/120    avg_loss:0.741, val_acc:0.738]
Epoch [10/120    avg_loss:0.677, val_acc:0.887]
Epoch [11/120    avg_loss:0.664, val_acc:0.835]
Epoch [12/120    avg_loss:0.583, val_acc:0.875]
Epoch [13/120    avg_loss:0.586, val_acc:0.894]
Epoch [14/120    avg_loss:0.507, val_acc:0.902]
Epoch [15/120    avg_loss:0.478, val_acc:0.900]
Epoch [16/120    avg_loss:0.471, val_acc:0.883]
Epoch [17/120    avg_loss:0.436, val_acc:0.915]
Epoch [18/120    avg_loss:0.432, val_acc:0.892]
Epoch [19/120    avg_loss:0.379, val_acc:0.898]
Epoch [20/120    avg_loss:0.428, val_acc:0.877]
Epoch [21/120    avg_loss:0.398, val_acc:0.925]
Epoch [22/120    avg_loss:0.337, val_acc:0.925]
Epoch [23/120    avg_loss:0.333, val_acc:0.906]
Epoch [24/120    avg_loss:0.330, val_acc:0.892]
Epoch [25/120    avg_loss:0.273, val_acc:0.948]
Epoch [26/120    avg_loss:0.261, val_acc:0.950]
Epoch [27/120    avg_loss:0.252, val_acc:0.917]
Epoch [28/120    avg_loss:0.285, val_acc:0.927]
Epoch [29/120    avg_loss:0.294, val_acc:0.942]
Epoch [30/120    avg_loss:0.281, val_acc:0.927]
Epoch [31/120    avg_loss:0.253, val_acc:0.894]
Epoch [32/120    avg_loss:0.331, val_acc:0.940]
Epoch [33/120    avg_loss:0.249, val_acc:0.946]
Epoch [34/120    avg_loss:0.248, val_acc:0.954]
Epoch [35/120    avg_loss:0.217, val_acc:0.944]
Epoch [36/120    avg_loss:0.259, val_acc:0.960]
Epoch [37/120    avg_loss:0.218, val_acc:0.948]
Epoch [38/120    avg_loss:0.198, val_acc:0.960]
Epoch [39/120    avg_loss:0.166, val_acc:0.960]
Epoch [40/120    avg_loss:0.173, val_acc:0.940]
Epoch [41/120    avg_loss:0.164, val_acc:0.946]
Epoch [42/120    avg_loss:0.156, val_acc:0.948]
Epoch [43/120    avg_loss:0.156, val_acc:0.960]
Epoch [44/120    avg_loss:0.140, val_acc:0.975]
Epoch [45/120    avg_loss:0.172, val_acc:0.956]
Epoch [46/120    avg_loss:0.142, val_acc:0.969]
Epoch [47/120    avg_loss:0.128, val_acc:0.977]
Epoch [48/120    avg_loss:0.154, val_acc:0.950]
Epoch [49/120    avg_loss:0.158, val_acc:0.944]
Epoch [50/120    avg_loss:0.168, val_acc:0.973]
Epoch [51/120    avg_loss:0.145, val_acc:0.969]
Epoch [52/120    avg_loss:0.115, val_acc:0.979]
Epoch [53/120    avg_loss:0.103, val_acc:0.981]
Epoch [54/120    avg_loss:0.120, val_acc:0.969]
Epoch [55/120    avg_loss:0.093, val_acc:0.963]
Epoch [56/120    avg_loss:0.128, val_acc:0.975]
Epoch [57/120    avg_loss:0.141, val_acc:0.965]
Epoch [58/120    avg_loss:0.117, val_acc:0.975]
Epoch [59/120    avg_loss:0.096, val_acc:0.971]
Epoch [60/120    avg_loss:0.096, val_acc:0.977]
Epoch [61/120    avg_loss:0.078, val_acc:0.983]
Epoch [62/120    avg_loss:0.080, val_acc:0.975]
Epoch [63/120    avg_loss:0.080, val_acc:0.985]
Epoch [64/120    avg_loss:0.073, val_acc:0.979]
Epoch [65/120    avg_loss:0.070, val_acc:0.985]
Epoch [66/120    avg_loss:0.087, val_acc:0.977]
Epoch [67/120    avg_loss:0.079, val_acc:0.965]
Epoch [68/120    avg_loss:0.099, val_acc:0.973]
Epoch [69/120    avg_loss:0.080, val_acc:0.979]
Epoch [70/120    avg_loss:0.053, val_acc:0.975]
Epoch [71/120    avg_loss:0.061, val_acc:0.992]
Epoch [72/120    avg_loss:0.071, val_acc:0.981]
Epoch [73/120    avg_loss:0.043, val_acc:0.985]
Epoch [74/120    avg_loss:0.051, val_acc:0.985]
Epoch [75/120    avg_loss:0.058, val_acc:0.990]
Epoch [76/120    avg_loss:0.092, val_acc:0.969]
Epoch [77/120    avg_loss:0.102, val_acc:0.973]
Epoch [78/120    avg_loss:0.075, val_acc:0.973]
Epoch [79/120    avg_loss:0.059, val_acc:0.983]
Epoch [80/120    avg_loss:0.055, val_acc:0.975]
Epoch [81/120    avg_loss:0.052, val_acc:0.985]
Epoch [82/120    avg_loss:0.034, val_acc:0.990]
Epoch [83/120    avg_loss:0.036, val_acc:0.988]
Epoch [84/120    avg_loss:0.048, val_acc:0.990]
Epoch [85/120    avg_loss:0.032, val_acc:0.990]
Epoch [86/120    avg_loss:0.025, val_acc:0.990]
Epoch [87/120    avg_loss:0.023, val_acc:0.992]
Epoch [88/120    avg_loss:0.023, val_acc:0.992]
Epoch [89/120    avg_loss:0.024, val_acc:0.992]
Epoch [90/120    avg_loss:0.023, val_acc:0.992]
Epoch [91/120    avg_loss:0.022, val_acc:0.992]
Epoch [92/120    avg_loss:0.029, val_acc:0.992]
Epoch [93/120    avg_loss:0.021, val_acc:0.994]
Epoch [94/120    avg_loss:0.023, val_acc:0.992]
Epoch [95/120    avg_loss:0.023, val_acc:0.992]
Epoch [96/120    avg_loss:0.022, val_acc:0.992]
Epoch [97/120    avg_loss:0.019, val_acc:0.994]
Epoch [98/120    avg_loss:0.021, val_acc:0.994]
Epoch [99/120    avg_loss:0.023, val_acc:0.994]
Epoch [100/120    avg_loss:0.030, val_acc:0.994]
Epoch [101/120    avg_loss:0.025, val_acc:0.994]
Epoch [102/120    avg_loss:0.023, val_acc:0.994]
Epoch [103/120    avg_loss:0.020, val_acc:0.994]
Epoch [104/120    avg_loss:0.023, val_acc:0.994]
Epoch [105/120    avg_loss:0.022, val_acc:0.992]
Epoch [106/120    avg_loss:0.020, val_acc:0.994]
Epoch [107/120    avg_loss:0.025, val_acc:0.994]
Epoch [108/120    avg_loss:0.021, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.018, val_acc:0.994]
Epoch [111/120    avg_loss:0.023, val_acc:0.994]
Epoch [112/120    avg_loss:0.016, val_acc:0.994]
Epoch [113/120    avg_loss:0.017, val_acc:0.994]
Epoch [114/120    avg_loss:0.024, val_acc:0.994]
Epoch [115/120    avg_loss:0.025, val_acc:0.994]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.019, val_acc:0.994]
Epoch [118/120    avg_loss:0.022, val_acc:0.994]
Epoch [119/120    avg_loss:0.016, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   6   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.97550111 0.98004435 0.91796009 0.89632107
 1.         0.93785311 0.99614891 1.         1.         1.
 1.         1.        ]

Kappa:
0.9878928760212585
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72ad4997b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.450, val_acc:0.415]
Epoch [2/120    avg_loss:2.014, val_acc:0.502]
Epoch [3/120    avg_loss:1.684, val_acc:0.646]
Epoch [4/120    avg_loss:1.410, val_acc:0.698]
Epoch [5/120    avg_loss:1.211, val_acc:0.708]
Epoch [6/120    avg_loss:1.116, val_acc:0.748]
Epoch [7/120    avg_loss:0.945, val_acc:0.760]
Epoch [8/120    avg_loss:0.871, val_acc:0.808]
Epoch [9/120    avg_loss:0.800, val_acc:0.833]
Epoch [10/120    avg_loss:0.704, val_acc:0.863]
Epoch [11/120    avg_loss:0.629, val_acc:0.881]
Epoch [12/120    avg_loss:0.617, val_acc:0.885]
Epoch [13/120    avg_loss:0.609, val_acc:0.898]
Epoch [14/120    avg_loss:0.594, val_acc:0.915]
Epoch [15/120    avg_loss:0.525, val_acc:0.904]
Epoch [16/120    avg_loss:0.475, val_acc:0.917]
Epoch [17/120    avg_loss:0.455, val_acc:0.885]
Epoch [18/120    avg_loss:0.451, val_acc:0.881]
Epoch [19/120    avg_loss:0.428, val_acc:0.912]
Epoch [20/120    avg_loss:0.368, val_acc:0.883]
Epoch [21/120    avg_loss:0.441, val_acc:0.923]
Epoch [22/120    avg_loss:0.403, val_acc:0.906]
Epoch [23/120    avg_loss:0.372, val_acc:0.940]
Epoch [24/120    avg_loss:0.328, val_acc:0.927]
Epoch [25/120    avg_loss:0.294, val_acc:0.925]
Epoch [26/120    avg_loss:0.312, val_acc:0.910]
Epoch [27/120    avg_loss:0.324, val_acc:0.900]
Epoch [28/120    avg_loss:0.326, val_acc:0.946]
Epoch [29/120    avg_loss:0.271, val_acc:0.935]
Epoch [30/120    avg_loss:0.266, val_acc:0.950]
Epoch [31/120    avg_loss:0.226, val_acc:0.956]
Epoch [32/120    avg_loss:0.256, val_acc:0.933]
Epoch [33/120    avg_loss:0.228, val_acc:0.946]
Epoch [34/120    avg_loss:0.225, val_acc:0.938]
Epoch [35/120    avg_loss:0.187, val_acc:0.954]
Epoch [36/120    avg_loss:0.246, val_acc:0.958]
Epoch [37/120    avg_loss:0.227, val_acc:0.919]
Epoch [38/120    avg_loss:0.237, val_acc:0.967]
Epoch [39/120    avg_loss:0.153, val_acc:0.971]
Epoch [40/120    avg_loss:0.177, val_acc:0.967]
Epoch [41/120    avg_loss:0.172, val_acc:0.944]
Epoch [42/120    avg_loss:0.247, val_acc:0.971]
Epoch [43/120    avg_loss:0.181, val_acc:0.973]
Epoch [44/120    avg_loss:0.158, val_acc:0.973]
Epoch [45/120    avg_loss:0.104, val_acc:0.975]
Epoch [46/120    avg_loss:0.103, val_acc:0.969]
Epoch [47/120    avg_loss:0.133, val_acc:0.950]
Epoch [48/120    avg_loss:0.091, val_acc:0.973]
Epoch [49/120    avg_loss:0.161, val_acc:0.975]
Epoch [50/120    avg_loss:0.167, val_acc:0.960]
Epoch [51/120    avg_loss:0.175, val_acc:0.967]
Epoch [52/120    avg_loss:0.151, val_acc:0.967]
Epoch [53/120    avg_loss:0.161, val_acc:0.971]
Epoch [54/120    avg_loss:0.107, val_acc:0.967]
Epoch [55/120    avg_loss:0.094, val_acc:0.975]
Epoch [56/120    avg_loss:0.102, val_acc:0.969]
Epoch [57/120    avg_loss:0.114, val_acc:0.965]
Epoch [58/120    avg_loss:0.135, val_acc:0.960]
Epoch [59/120    avg_loss:0.109, val_acc:0.973]
Epoch [60/120    avg_loss:0.092, val_acc:0.977]
Epoch [61/120    avg_loss:0.062, val_acc:0.981]
Epoch [62/120    avg_loss:0.080, val_acc:0.969]
Epoch [63/120    avg_loss:0.122, val_acc:0.967]
Epoch [64/120    avg_loss:0.171, val_acc:0.956]
Epoch [65/120    avg_loss:0.171, val_acc:0.977]
Epoch [66/120    avg_loss:0.097, val_acc:0.979]
Epoch [67/120    avg_loss:0.080, val_acc:0.983]
Epoch [68/120    avg_loss:0.085, val_acc:0.979]
Epoch [69/120    avg_loss:0.094, val_acc:0.979]
Epoch [70/120    avg_loss:0.084, val_acc:0.985]
Epoch [71/120    avg_loss:0.061, val_acc:0.988]
Epoch [72/120    avg_loss:0.059, val_acc:0.983]
Epoch [73/120    avg_loss:0.054, val_acc:0.981]
Epoch [74/120    avg_loss:0.074, val_acc:0.973]
Epoch [75/120    avg_loss:0.075, val_acc:0.988]
Epoch [76/120    avg_loss:0.075, val_acc:0.967]
Epoch [77/120    avg_loss:0.097, val_acc:0.981]
Epoch [78/120    avg_loss:0.139, val_acc:0.981]
Epoch [79/120    avg_loss:0.091, val_acc:0.979]
Epoch [80/120    avg_loss:0.073, val_acc:0.990]
Epoch [81/120    avg_loss:0.060, val_acc:0.977]
Epoch [82/120    avg_loss:0.062, val_acc:0.988]
Epoch [83/120    avg_loss:0.087, val_acc:0.985]
Epoch [84/120    avg_loss:0.080, val_acc:0.983]
Epoch [85/120    avg_loss:0.072, val_acc:0.983]
Epoch [86/120    avg_loss:0.060, val_acc:0.979]
Epoch [87/120    avg_loss:0.071, val_acc:0.985]
Epoch [88/120    avg_loss:0.086, val_acc:0.988]
Epoch [89/120    avg_loss:0.041, val_acc:0.992]
Epoch [90/120    avg_loss:0.057, val_acc:0.979]
Epoch [91/120    avg_loss:0.101, val_acc:0.967]
Epoch [92/120    avg_loss:0.112, val_acc:0.975]
Epoch [93/120    avg_loss:0.078, val_acc:0.981]
Epoch [94/120    avg_loss:0.056, val_acc:0.981]
Epoch [95/120    avg_loss:0.040, val_acc:0.988]
Epoch [96/120    avg_loss:0.059, val_acc:0.983]
Epoch [97/120    avg_loss:0.072, val_acc:0.983]
Epoch [98/120    avg_loss:0.059, val_acc:0.985]
Epoch [99/120    avg_loss:0.045, val_acc:0.981]
Epoch [100/120    avg_loss:0.063, val_acc:0.985]
Epoch [101/120    avg_loss:0.059, val_acc:0.983]
Epoch [102/120    avg_loss:0.034, val_acc:0.983]
Epoch [103/120    avg_loss:0.046, val_acc:0.983]
Epoch [104/120    avg_loss:0.034, val_acc:0.988]
Epoch [105/120    avg_loss:0.029, val_acc:0.988]
Epoch [106/120    avg_loss:0.034, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.988]
Epoch [108/120    avg_loss:0.031, val_acc:0.988]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.026, val_acc:0.990]
Epoch [111/120    avg_loss:0.026, val_acc:0.990]
Epoch [112/120    avg_loss:0.017, val_acc:0.990]
Epoch [113/120    avg_loss:0.022, val_acc:0.992]
Epoch [114/120    avg_loss:0.033, val_acc:0.990]
Epoch [115/120    avg_loss:0.025, val_acc:0.990]
Epoch [116/120    avg_loss:0.027, val_acc:0.988]
Epoch [117/120    avg_loss:0.021, val_acc:0.988]
Epoch [118/120    avg_loss:0.019, val_acc:0.985]
Epoch [119/120    avg_loss:0.033, val_acc:0.988]
Epoch [120/120    avg_loss:0.022, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  12   0   0   0   0   0   0   4   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.97117517 0.98678414 0.93986637 0.95333333
 0.99266504 0.92571429 1.         1.         1.         0.99867198
 0.99451153 1.        ]

Kappa:
0.9902664587910142
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb484f817f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.420, val_acc:0.482]
Epoch [2/120    avg_loss:1.995, val_acc:0.523]
Epoch [3/120    avg_loss:1.711, val_acc:0.582]
Epoch [4/120    avg_loss:1.474, val_acc:0.648]
Epoch [5/120    avg_loss:1.281, val_acc:0.725]
Epoch [6/120    avg_loss:1.066, val_acc:0.770]
Epoch [7/120    avg_loss:0.966, val_acc:0.783]
Epoch [8/120    avg_loss:0.864, val_acc:0.830]
Epoch [9/120    avg_loss:0.777, val_acc:0.863]
Epoch [10/120    avg_loss:0.730, val_acc:0.857]
Epoch [11/120    avg_loss:0.643, val_acc:0.875]
Epoch [12/120    avg_loss:0.558, val_acc:0.893]
Epoch [13/120    avg_loss:0.575, val_acc:0.879]
Epoch [14/120    avg_loss:0.558, val_acc:0.881]
Epoch [15/120    avg_loss:0.528, val_acc:0.885]
Epoch [16/120    avg_loss:0.441, val_acc:0.887]
Epoch [17/120    avg_loss:0.422, val_acc:0.900]
Epoch [18/120    avg_loss:0.445, val_acc:0.898]
Epoch [19/120    avg_loss:0.414, val_acc:0.865]
Epoch [20/120    avg_loss:0.433, val_acc:0.895]
Epoch [21/120    avg_loss:0.414, val_acc:0.891]
Epoch [22/120    avg_loss:0.385, val_acc:0.893]
Epoch [23/120    avg_loss:0.313, val_acc:0.914]
Epoch [24/120    avg_loss:0.309, val_acc:0.922]
Epoch [25/120    avg_loss:0.302, val_acc:0.916]
Epoch [26/120    avg_loss:0.320, val_acc:0.889]
Epoch [27/120    avg_loss:0.340, val_acc:0.916]
Epoch [28/120    avg_loss:0.292, val_acc:0.934]
Epoch [29/120    avg_loss:0.332, val_acc:0.908]
Epoch [30/120    avg_loss:0.327, val_acc:0.922]
Epoch [31/120    avg_loss:0.238, val_acc:0.938]
Epoch [32/120    avg_loss:0.225, val_acc:0.932]
Epoch [33/120    avg_loss:0.230, val_acc:0.916]
Epoch [34/120    avg_loss:0.203, val_acc:0.949]
Epoch [35/120    avg_loss:0.196, val_acc:0.959]
Epoch [36/120    avg_loss:0.164, val_acc:0.947]
Epoch [37/120    avg_loss:0.201, val_acc:0.916]
Epoch [38/120    avg_loss:0.182, val_acc:0.936]
Epoch [39/120    avg_loss:0.182, val_acc:0.920]
Epoch [40/120    avg_loss:0.269, val_acc:0.938]
Epoch [41/120    avg_loss:0.200, val_acc:0.936]
Epoch [42/120    avg_loss:0.174, val_acc:0.939]
Epoch [43/120    avg_loss:0.177, val_acc:0.926]
Epoch [44/120    avg_loss:0.139, val_acc:0.963]
Epoch [45/120    avg_loss:0.217, val_acc:0.939]
Epoch [46/120    avg_loss:0.121, val_acc:0.957]
Epoch [47/120    avg_loss:0.114, val_acc:0.943]
Epoch [48/120    avg_loss:0.114, val_acc:0.959]
Epoch [49/120    avg_loss:0.106, val_acc:0.963]
Epoch [50/120    avg_loss:0.089, val_acc:0.967]
Epoch [51/120    avg_loss:0.080, val_acc:0.963]
Epoch [52/120    avg_loss:0.097, val_acc:0.967]
Epoch [53/120    avg_loss:0.106, val_acc:0.961]
Epoch [54/120    avg_loss:0.089, val_acc:0.939]
Epoch [55/120    avg_loss:0.091, val_acc:0.949]
Epoch [56/120    avg_loss:0.109, val_acc:0.957]
Epoch [57/120    avg_loss:0.152, val_acc:0.953]
Epoch [58/120    avg_loss:0.094, val_acc:0.957]
Epoch [59/120    avg_loss:0.101, val_acc:0.957]
Epoch [60/120    avg_loss:0.064, val_acc:0.957]
Epoch [61/120    avg_loss:0.079, val_acc:0.971]
Epoch [62/120    avg_loss:0.075, val_acc:0.967]
Epoch [63/120    avg_loss:0.067, val_acc:0.969]
Epoch [64/120    avg_loss:0.057, val_acc:0.980]
Epoch [65/120    avg_loss:0.047, val_acc:0.982]
Epoch [66/120    avg_loss:0.039, val_acc:0.975]
Epoch [67/120    avg_loss:0.047, val_acc:0.971]
Epoch [68/120    avg_loss:0.066, val_acc:0.973]
Epoch [69/120    avg_loss:0.070, val_acc:0.957]
Epoch [70/120    avg_loss:0.050, val_acc:0.969]
Epoch [71/120    avg_loss:0.049, val_acc:0.973]
Epoch [72/120    avg_loss:0.056, val_acc:0.973]
Epoch [73/120    avg_loss:0.048, val_acc:0.967]
Epoch [74/120    avg_loss:0.058, val_acc:0.959]
Epoch [75/120    avg_loss:0.052, val_acc:0.971]
Epoch [76/120    avg_loss:0.040, val_acc:0.980]
Epoch [77/120    avg_loss:0.046, val_acc:0.967]
Epoch [78/120    avg_loss:0.075, val_acc:0.955]
Epoch [79/120    avg_loss:0.033, val_acc:0.973]
Epoch [80/120    avg_loss:0.040, val_acc:0.979]
Epoch [81/120    avg_loss:0.025, val_acc:0.980]
Epoch [82/120    avg_loss:0.023, val_acc:0.979]
Epoch [83/120    avg_loss:0.032, val_acc:0.980]
Epoch [84/120    avg_loss:0.025, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.980]
Epoch [86/120    avg_loss:0.025, val_acc:0.984]
Epoch [87/120    avg_loss:0.021, val_acc:0.984]
Epoch [88/120    avg_loss:0.018, val_acc:0.984]
Epoch [89/120    avg_loss:0.025, val_acc:0.986]
Epoch [90/120    avg_loss:0.030, val_acc:0.977]
Epoch [91/120    avg_loss:0.020, val_acc:0.979]
Epoch [92/120    avg_loss:0.022, val_acc:0.980]
Epoch [93/120    avg_loss:0.018, val_acc:0.982]
Epoch [94/120    avg_loss:0.018, val_acc:0.982]
Epoch [95/120    avg_loss:0.024, val_acc:0.982]
Epoch [96/120    avg_loss:0.017, val_acc:0.986]
Epoch [97/120    avg_loss:0.025, val_acc:0.984]
Epoch [98/120    avg_loss:0.031, val_acc:0.986]
Epoch [99/120    avg_loss:0.024, val_acc:0.982]
Epoch [100/120    avg_loss:0.019, val_acc:0.986]
Epoch [101/120    avg_loss:0.031, val_acc:0.990]
Epoch [102/120    avg_loss:0.023, val_acc:0.986]
Epoch [103/120    avg_loss:0.020, val_acc:0.986]
Epoch [104/120    avg_loss:0.020, val_acc:0.986]
Epoch [105/120    avg_loss:0.028, val_acc:0.986]
Epoch [106/120    avg_loss:0.026, val_acc:0.986]
Epoch [107/120    avg_loss:0.025, val_acc:0.986]
Epoch [108/120    avg_loss:0.030, val_acc:0.986]
Epoch [109/120    avg_loss:0.023, val_acc:0.986]
Epoch [110/120    avg_loss:0.017, val_acc:0.986]
Epoch [111/120    avg_loss:0.023, val_acc:0.986]
Epoch [112/120    avg_loss:0.016, val_acc:0.988]
Epoch [113/120    avg_loss:0.022, val_acc:0.986]
Epoch [114/120    avg_loss:0.021, val_acc:0.984]
Epoch [115/120    avg_loss:0.025, val_acc:0.984]
Epoch [116/120    avg_loss:0.020, val_acc:0.984]
Epoch [117/120    avg_loss:0.027, val_acc:0.984]
Epoch [118/120    avg_loss:0.025, val_acc:0.984]
Epoch [119/120    avg_loss:0.020, val_acc:0.984]
Epoch [120/120    avg_loss:0.025, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 209  21   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.99319728 0.95216401 0.91182796 0.93333333
 1.         0.98378378 1.         1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9878941830433534
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79156b3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.482, val_acc:0.418]
Epoch [2/120    avg_loss:2.034, val_acc:0.600]
Epoch [3/120    avg_loss:1.710, val_acc:0.619]
Epoch [4/120    avg_loss:1.438, val_acc:0.695]
Epoch [5/120    avg_loss:1.225, val_acc:0.752]
Epoch [6/120    avg_loss:1.063, val_acc:0.826]
Epoch [7/120    avg_loss:0.964, val_acc:0.775]
Epoch [8/120    avg_loss:0.843, val_acc:0.846]
Epoch [9/120    avg_loss:0.763, val_acc:0.775]
Epoch [10/120    avg_loss:0.716, val_acc:0.855]
Epoch [11/120    avg_loss:0.674, val_acc:0.838]
Epoch [12/120    avg_loss:0.683, val_acc:0.877]
Epoch [13/120    avg_loss:0.624, val_acc:0.871]
Epoch [14/120    avg_loss:0.570, val_acc:0.898]
Epoch [15/120    avg_loss:0.508, val_acc:0.793]
Epoch [16/120    avg_loss:0.512, val_acc:0.893]
Epoch [17/120    avg_loss:0.477, val_acc:0.898]
Epoch [18/120    avg_loss:0.417, val_acc:0.910]
Epoch [19/120    avg_loss:0.391, val_acc:0.924]
Epoch [20/120    avg_loss:0.382, val_acc:0.926]
Epoch [21/120    avg_loss:0.384, val_acc:0.924]
Epoch [22/120    avg_loss:0.365, val_acc:0.934]
Epoch [23/120    avg_loss:0.312, val_acc:0.912]
Epoch [24/120    avg_loss:0.337, val_acc:0.920]
Epoch [25/120    avg_loss:0.321, val_acc:0.922]
Epoch [26/120    avg_loss:0.299, val_acc:0.930]
Epoch [27/120    avg_loss:0.324, val_acc:0.926]
Epoch [28/120    avg_loss:0.392, val_acc:0.922]
Epoch [29/120    avg_loss:0.352, val_acc:0.918]
Epoch [30/120    avg_loss:0.304, val_acc:0.928]
Epoch [31/120    avg_loss:0.321, val_acc:0.857]
Epoch [32/120    avg_loss:0.333, val_acc:0.939]
Epoch [33/120    avg_loss:0.244, val_acc:0.945]
Epoch [34/120    avg_loss:0.187, val_acc:0.957]
Epoch [35/120    avg_loss:0.194, val_acc:0.934]
Epoch [36/120    avg_loss:0.185, val_acc:0.947]
Epoch [37/120    avg_loss:0.185, val_acc:0.961]
Epoch [38/120    avg_loss:0.190, val_acc:0.957]
Epoch [39/120    avg_loss:0.167, val_acc:0.975]
Epoch [40/120    avg_loss:0.142, val_acc:0.959]
Epoch [41/120    avg_loss:0.168, val_acc:0.896]
Epoch [42/120    avg_loss:0.164, val_acc:0.947]
Epoch [43/120    avg_loss:0.160, val_acc:0.965]
Epoch [44/120    avg_loss:0.160, val_acc:0.975]
Epoch [45/120    avg_loss:0.169, val_acc:0.926]
Epoch [46/120    avg_loss:0.139, val_acc:0.953]
Epoch [47/120    avg_loss:0.122, val_acc:0.969]
Epoch [48/120    avg_loss:0.134, val_acc:0.965]
Epoch [49/120    avg_loss:0.109, val_acc:0.973]
Epoch [50/120    avg_loss:0.096, val_acc:0.977]
Epoch [51/120    avg_loss:0.118, val_acc:0.975]
Epoch [52/120    avg_loss:0.101, val_acc:0.973]
Epoch [53/120    avg_loss:0.066, val_acc:0.973]
Epoch [54/120    avg_loss:0.078, val_acc:0.969]
Epoch [55/120    avg_loss:0.121, val_acc:0.971]
Epoch [56/120    avg_loss:0.084, val_acc:0.984]
Epoch [57/120    avg_loss:0.107, val_acc:0.977]
Epoch [58/120    avg_loss:0.069, val_acc:0.979]
Epoch [59/120    avg_loss:0.091, val_acc:0.975]
Epoch [60/120    avg_loss:0.096, val_acc:0.980]
Epoch [61/120    avg_loss:0.120, val_acc:0.922]
Epoch [62/120    avg_loss:0.105, val_acc:0.977]
Epoch [63/120    avg_loss:0.106, val_acc:0.984]
Epoch [64/120    avg_loss:0.076, val_acc:0.982]
Epoch [65/120    avg_loss:0.080, val_acc:0.971]
Epoch [66/120    avg_loss:0.109, val_acc:0.969]
Epoch [67/120    avg_loss:0.070, val_acc:0.977]
Epoch [68/120    avg_loss:0.101, val_acc:0.979]
Epoch [69/120    avg_loss:0.147, val_acc:0.969]
Epoch [70/120    avg_loss:0.151, val_acc:0.980]
Epoch [71/120    avg_loss:0.144, val_acc:0.967]
Epoch [72/120    avg_loss:0.142, val_acc:0.973]
Epoch [73/120    avg_loss:0.066, val_acc:0.988]
Epoch [74/120    avg_loss:0.075, val_acc:0.986]
Epoch [75/120    avg_loss:0.052, val_acc:0.969]
Epoch [76/120    avg_loss:0.095, val_acc:0.980]
Epoch [77/120    avg_loss:0.058, val_acc:0.986]
Epoch [78/120    avg_loss:0.048, val_acc:0.979]
Epoch [79/120    avg_loss:0.057, val_acc:0.982]
Epoch [80/120    avg_loss:0.048, val_acc:0.982]
Epoch [81/120    avg_loss:0.027, val_acc:0.990]
Epoch [82/120    avg_loss:0.035, val_acc:0.986]
Epoch [83/120    avg_loss:0.045, val_acc:0.979]
Epoch [84/120    avg_loss:0.061, val_acc:0.986]
Epoch [85/120    avg_loss:0.056, val_acc:0.973]
Epoch [86/120    avg_loss:0.041, val_acc:0.980]
Epoch [87/120    avg_loss:0.049, val_acc:0.982]
Epoch [88/120    avg_loss:0.067, val_acc:0.986]
Epoch [89/120    avg_loss:0.048, val_acc:0.982]
Epoch [90/120    avg_loss:0.029, val_acc:0.982]
Epoch [91/120    avg_loss:0.036, val_acc:0.980]
Epoch [92/120    avg_loss:0.044, val_acc:0.979]
Epoch [93/120    avg_loss:0.035, val_acc:0.992]
Epoch [94/120    avg_loss:0.015, val_acc:0.986]
Epoch [95/120    avg_loss:0.015, val_acc:0.992]
Epoch [96/120    avg_loss:0.017, val_acc:0.988]
Epoch [97/120    avg_loss:0.032, val_acc:0.984]
Epoch [98/120    avg_loss:0.059, val_acc:0.967]
Epoch [99/120    avg_loss:0.124, val_acc:0.984]
Epoch [100/120    avg_loss:0.062, val_acc:0.986]
Epoch [101/120    avg_loss:0.031, val_acc:0.990]
Epoch [102/120    avg_loss:0.032, val_acc:0.990]
Epoch [103/120    avg_loss:0.030, val_acc:0.994]
Epoch [104/120    avg_loss:0.020, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.988]
Epoch [106/120    avg_loss:0.021, val_acc:0.988]
Epoch [107/120    avg_loss:0.024, val_acc:0.992]
Epoch [108/120    avg_loss:0.023, val_acc:0.990]
Epoch [109/120    avg_loss:0.028, val_acc:0.988]
Epoch [110/120    avg_loss:0.029, val_acc:0.994]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.016, val_acc:0.992]
Epoch [113/120    avg_loss:0.032, val_acc:0.990]
Epoch [114/120    avg_loss:0.052, val_acc:0.973]
Epoch [115/120    avg_loss:0.058, val_acc:0.984]
Epoch [116/120    avg_loss:0.054, val_acc:0.975]
Epoch [117/120    avg_loss:0.028, val_acc:0.988]
Epoch [118/120    avg_loss:0.028, val_acc:0.980]
Epoch [119/120    avg_loss:0.018, val_acc:0.986]
Epoch [120/120    avg_loss:0.018, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.99541284 0.98230088 0.93303571 0.94155844
 0.99019608 0.98947368 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924044339319362
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f64b6ea17b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.438, val_acc:0.362]
Epoch [2/120    avg_loss:2.034, val_acc:0.602]
Epoch [3/120    avg_loss:1.726, val_acc:0.706]
Epoch [4/120    avg_loss:1.450, val_acc:0.681]
Epoch [5/120    avg_loss:1.219, val_acc:0.658]
Epoch [6/120    avg_loss:1.149, val_acc:0.769]
Epoch [7/120    avg_loss:0.959, val_acc:0.752]
Epoch [8/120    avg_loss:0.855, val_acc:0.846]
Epoch [9/120    avg_loss:0.732, val_acc:0.869]
Epoch [10/120    avg_loss:0.664, val_acc:0.881]
Epoch [11/120    avg_loss:0.592, val_acc:0.887]
Epoch [12/120    avg_loss:0.548, val_acc:0.923]
Epoch [13/120    avg_loss:0.509, val_acc:0.902]
Epoch [14/120    avg_loss:0.524, val_acc:0.935]
Epoch [15/120    avg_loss:0.445, val_acc:0.940]
Epoch [16/120    avg_loss:0.439, val_acc:0.904]
Epoch [17/120    avg_loss:0.467, val_acc:0.908]
Epoch [18/120    avg_loss:0.364, val_acc:0.948]
Epoch [19/120    avg_loss:0.334, val_acc:0.940]
Epoch [20/120    avg_loss:0.363, val_acc:0.942]
Epoch [21/120    avg_loss:0.343, val_acc:0.956]
Epoch [22/120    avg_loss:0.312, val_acc:0.915]
Epoch [23/120    avg_loss:0.313, val_acc:0.944]
Epoch [24/120    avg_loss:0.256, val_acc:0.956]
Epoch [25/120    avg_loss:0.308, val_acc:0.935]
Epoch [26/120    avg_loss:0.284, val_acc:0.938]
Epoch [27/120    avg_loss:0.286, val_acc:0.958]
Epoch [28/120    avg_loss:0.251, val_acc:0.940]
Epoch [29/120    avg_loss:0.253, val_acc:0.963]
Epoch [30/120    avg_loss:0.199, val_acc:0.969]
Epoch [31/120    avg_loss:0.184, val_acc:0.967]
Epoch [32/120    avg_loss:0.183, val_acc:0.958]
Epoch [33/120    avg_loss:0.181, val_acc:0.960]
Epoch [34/120    avg_loss:0.197, val_acc:0.960]
Epoch [35/120    avg_loss:0.223, val_acc:0.948]
Epoch [36/120    avg_loss:0.178, val_acc:0.973]
Epoch [37/120    avg_loss:0.169, val_acc:0.967]
Epoch [38/120    avg_loss:0.156, val_acc:0.977]
Epoch [39/120    avg_loss:0.140, val_acc:0.971]
Epoch [40/120    avg_loss:0.149, val_acc:0.977]
Epoch [41/120    avg_loss:0.123, val_acc:0.971]
Epoch [42/120    avg_loss:0.129, val_acc:0.973]
Epoch [43/120    avg_loss:0.136, val_acc:0.988]
Epoch [44/120    avg_loss:0.131, val_acc:0.969]
Epoch [45/120    avg_loss:0.194, val_acc:0.960]
Epoch [46/120    avg_loss:0.208, val_acc:0.946]
Epoch [47/120    avg_loss:0.160, val_acc:0.927]
Epoch [48/120    avg_loss:0.118, val_acc:0.967]
Epoch [49/120    avg_loss:0.105, val_acc:0.983]
Epoch [50/120    avg_loss:0.128, val_acc:0.979]
Epoch [51/120    avg_loss:0.109, val_acc:0.979]
Epoch [52/120    avg_loss:0.087, val_acc:0.981]
Epoch [53/120    avg_loss:0.119, val_acc:0.979]
Epoch [54/120    avg_loss:0.114, val_acc:0.981]
Epoch [55/120    avg_loss:0.100, val_acc:0.979]
Epoch [56/120    avg_loss:0.084, val_acc:0.985]
Epoch [57/120    avg_loss:0.064, val_acc:0.988]
Epoch [58/120    avg_loss:0.061, val_acc:0.988]
Epoch [59/120    avg_loss:0.065, val_acc:0.988]
Epoch [60/120    avg_loss:0.051, val_acc:0.985]
Epoch [61/120    avg_loss:0.060, val_acc:0.985]
Epoch [62/120    avg_loss:0.041, val_acc:0.985]
Epoch [63/120    avg_loss:0.049, val_acc:0.983]
Epoch [64/120    avg_loss:0.050, val_acc:0.983]
Epoch [65/120    avg_loss:0.046, val_acc:0.985]
Epoch [66/120    avg_loss:0.063, val_acc:0.985]
Epoch [67/120    avg_loss:0.049, val_acc:0.985]
Epoch [68/120    avg_loss:0.049, val_acc:0.985]
Epoch [69/120    avg_loss:0.040, val_acc:0.985]
Epoch [70/120    avg_loss:0.046, val_acc:0.985]
Epoch [71/120    avg_loss:0.053, val_acc:0.985]
Epoch [72/120    avg_loss:0.044, val_acc:0.990]
Epoch [73/120    avg_loss:0.046, val_acc:0.985]
Epoch [74/120    avg_loss:0.057, val_acc:0.985]
Epoch [75/120    avg_loss:0.042, val_acc:0.985]
Epoch [76/120    avg_loss:0.049, val_acc:0.985]
Epoch [77/120    avg_loss:0.042, val_acc:0.985]
Epoch [78/120    avg_loss:0.049, val_acc:0.985]
Epoch [79/120    avg_loss:0.047, val_acc:0.985]
Epoch [80/120    avg_loss:0.037, val_acc:0.985]
Epoch [81/120    avg_loss:0.043, val_acc:0.988]
Epoch [82/120    avg_loss:0.048, val_acc:0.988]
Epoch [83/120    avg_loss:0.035, val_acc:0.985]
Epoch [84/120    avg_loss:0.049, val_acc:0.988]
Epoch [85/120    avg_loss:0.048, val_acc:0.985]
Epoch [86/120    avg_loss:0.041, val_acc:0.985]
Epoch [87/120    avg_loss:0.037, val_acc:0.985]
Epoch [88/120    avg_loss:0.049, val_acc:0.985]
Epoch [89/120    avg_loss:0.054, val_acc:0.985]
Epoch [90/120    avg_loss:0.036, val_acc:0.985]
Epoch [91/120    avg_loss:0.045, val_acc:0.985]
Epoch [92/120    avg_loss:0.040, val_acc:0.985]
Epoch [93/120    avg_loss:0.037, val_acc:0.985]
Epoch [94/120    avg_loss:0.038, val_acc:0.985]
Epoch [95/120    avg_loss:0.044, val_acc:0.985]
Epoch [96/120    avg_loss:0.036, val_acc:0.985]
Epoch [97/120    avg_loss:0.038, val_acc:0.985]
Epoch [98/120    avg_loss:0.038, val_acc:0.985]
Epoch [99/120    avg_loss:0.036, val_acc:0.985]
Epoch [100/120    avg_loss:0.031, val_acc:0.985]
Epoch [101/120    avg_loss:0.043, val_acc:0.985]
Epoch [102/120    avg_loss:0.036, val_acc:0.985]
Epoch [103/120    avg_loss:0.048, val_acc:0.985]
Epoch [104/120    avg_loss:0.032, val_acc:0.985]
Epoch [105/120    avg_loss:0.040, val_acc:0.985]
Epoch [106/120    avg_loss:0.048, val_acc:0.985]
Epoch [107/120    avg_loss:0.034, val_acc:0.985]
Epoch [108/120    avg_loss:0.038, val_acc:0.985]
Epoch [109/120    avg_loss:0.044, val_acc:0.985]
Epoch [110/120    avg_loss:0.038, val_acc:0.985]
Epoch [111/120    avg_loss:0.033, val_acc:0.985]
Epoch [112/120    avg_loss:0.040, val_acc:0.985]
Epoch [113/120    avg_loss:0.039, val_acc:0.985]
Epoch [114/120    avg_loss:0.036, val_acc:0.985]
Epoch [115/120    avg_loss:0.040, val_acc:0.985]
Epoch [116/120    avg_loss:0.048, val_acc:0.985]
Epoch [117/120    avg_loss:0.036, val_acc:0.985]
Epoch [118/120    avg_loss:0.049, val_acc:0.985]
Epoch [119/120    avg_loss:0.035, val_acc:0.985]
Epoch [120/120    avg_loss:0.043, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.97767857 1.         0.95172414 0.93203883
 1.         0.94382022 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9924038826444408
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdababfc898>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.459, val_acc:0.352]
Epoch [2/120    avg_loss:2.075, val_acc:0.579]
Epoch [3/120    avg_loss:1.805, val_acc:0.552]
Epoch [4/120    avg_loss:1.541, val_acc:0.729]
Epoch [5/120    avg_loss:1.297, val_acc:0.738]
Epoch [6/120    avg_loss:1.154, val_acc:0.740]
Epoch [7/120    avg_loss:1.003, val_acc:0.831]
Epoch [8/120    avg_loss:0.853, val_acc:0.817]
Epoch [9/120    avg_loss:0.769, val_acc:0.831]
Epoch [10/120    avg_loss:0.715, val_acc:0.863]
Epoch [11/120    avg_loss:0.679, val_acc:0.846]
Epoch [12/120    avg_loss:0.593, val_acc:0.875]
Epoch [13/120    avg_loss:0.595, val_acc:0.875]
Epoch [14/120    avg_loss:0.550, val_acc:0.894]
Epoch [15/120    avg_loss:0.554, val_acc:0.896]
Epoch [16/120    avg_loss:0.485, val_acc:0.898]
Epoch [17/120    avg_loss:0.430, val_acc:0.900]
Epoch [18/120    avg_loss:0.459, val_acc:0.879]
Epoch [19/120    avg_loss:0.438, val_acc:0.900]
Epoch [20/120    avg_loss:0.393, val_acc:0.896]
Epoch [21/120    avg_loss:0.354, val_acc:0.933]
Epoch [22/120    avg_loss:0.335, val_acc:0.938]
Epoch [23/120    avg_loss:0.407, val_acc:0.896]
Epoch [24/120    avg_loss:0.292, val_acc:0.940]
Epoch [25/120    avg_loss:0.288, val_acc:0.938]
Epoch [26/120    avg_loss:0.319, val_acc:0.931]
Epoch [27/120    avg_loss:0.283, val_acc:0.944]
Epoch [28/120    avg_loss:0.235, val_acc:0.952]
Epoch [29/120    avg_loss:0.225, val_acc:0.925]
Epoch [30/120    avg_loss:0.223, val_acc:0.950]
Epoch [31/120    avg_loss:0.220, val_acc:0.950]
Epoch [32/120    avg_loss:0.202, val_acc:0.967]
Epoch [33/120    avg_loss:0.221, val_acc:0.950]
Epoch [34/120    avg_loss:0.271, val_acc:0.929]
Epoch [35/120    avg_loss:0.278, val_acc:0.929]
Epoch [36/120    avg_loss:0.250, val_acc:0.960]
Epoch [37/120    avg_loss:0.240, val_acc:0.923]
Epoch [38/120    avg_loss:0.224, val_acc:0.942]
Epoch [39/120    avg_loss:0.203, val_acc:0.950]
Epoch [40/120    avg_loss:0.210, val_acc:0.960]
Epoch [41/120    avg_loss:0.209, val_acc:0.952]
Epoch [42/120    avg_loss:0.144, val_acc:0.973]
Epoch [43/120    avg_loss:0.114, val_acc:0.971]
Epoch [44/120    avg_loss:0.154, val_acc:0.973]
Epoch [45/120    avg_loss:0.135, val_acc:0.954]
Epoch [46/120    avg_loss:0.143, val_acc:0.969]
Epoch [47/120    avg_loss:0.102, val_acc:0.985]
Epoch [48/120    avg_loss:0.104, val_acc:0.979]
Epoch [49/120    avg_loss:0.103, val_acc:0.977]
Epoch [50/120    avg_loss:0.113, val_acc:0.969]
Epoch [51/120    avg_loss:0.112, val_acc:0.927]
Epoch [52/120    avg_loss:0.100, val_acc:0.973]
Epoch [53/120    avg_loss:0.121, val_acc:0.985]
Epoch [54/120    avg_loss:0.095, val_acc:0.963]
Epoch [55/120    avg_loss:0.096, val_acc:0.973]
Epoch [56/120    avg_loss:0.115, val_acc:0.981]
Epoch [57/120    avg_loss:0.085, val_acc:0.990]
Epoch [58/120    avg_loss:0.065, val_acc:0.981]
Epoch [59/120    avg_loss:0.057, val_acc:0.988]
Epoch [60/120    avg_loss:0.067, val_acc:0.973]
Epoch [61/120    avg_loss:0.116, val_acc:0.915]
Epoch [62/120    avg_loss:0.240, val_acc:0.967]
Epoch [63/120    avg_loss:0.224, val_acc:0.946]
Epoch [64/120    avg_loss:0.127, val_acc:0.963]
Epoch [65/120    avg_loss:0.123, val_acc:0.967]
Epoch [66/120    avg_loss:0.082, val_acc:0.975]
Epoch [67/120    avg_loss:0.078, val_acc:0.979]
Epoch [68/120    avg_loss:0.072, val_acc:0.981]
Epoch [69/120    avg_loss:0.058, val_acc:0.985]
Epoch [70/120    avg_loss:0.045, val_acc:0.979]
Epoch [71/120    avg_loss:0.057, val_acc:0.985]
Epoch [72/120    avg_loss:0.051, val_acc:0.985]
Epoch [73/120    avg_loss:0.040, val_acc:0.985]
Epoch [74/120    avg_loss:0.036, val_acc:0.985]
Epoch [75/120    avg_loss:0.045, val_acc:0.988]
Epoch [76/120    avg_loss:0.027, val_acc:0.988]
Epoch [77/120    avg_loss:0.035, val_acc:0.988]
Epoch [78/120    avg_loss:0.043, val_acc:0.988]
Epoch [79/120    avg_loss:0.039, val_acc:0.985]
Epoch [80/120    avg_loss:0.041, val_acc:0.985]
Epoch [81/120    avg_loss:0.033, val_acc:0.985]
Epoch [82/120    avg_loss:0.034, val_acc:0.988]
Epoch [83/120    avg_loss:0.032, val_acc:0.990]
Epoch [84/120    avg_loss:0.033, val_acc:0.990]
Epoch [85/120    avg_loss:0.035, val_acc:0.988]
Epoch [86/120    avg_loss:0.026, val_acc:0.990]
Epoch [87/120    avg_loss:0.035, val_acc:0.990]
Epoch [88/120    avg_loss:0.037, val_acc:0.988]
Epoch [89/120    avg_loss:0.037, val_acc:0.988]
Epoch [90/120    avg_loss:0.028, val_acc:0.985]
Epoch [91/120    avg_loss:0.024, val_acc:0.985]
Epoch [92/120    avg_loss:0.030, val_acc:0.985]
Epoch [93/120    avg_loss:0.046, val_acc:0.988]
Epoch [94/120    avg_loss:0.037, val_acc:0.985]
Epoch [95/120    avg_loss:0.033, val_acc:0.985]
Epoch [96/120    avg_loss:0.034, val_acc:0.985]
Epoch [97/120    avg_loss:0.035, val_acc:0.985]
Epoch [98/120    avg_loss:0.040, val_acc:0.985]
Epoch [99/120    avg_loss:0.028, val_acc:0.990]
Epoch [100/120    avg_loss:0.032, val_acc:0.985]
Epoch [101/120    avg_loss:0.034, val_acc:0.985]
Epoch [102/120    avg_loss:0.025, val_acc:0.988]
Epoch [103/120    avg_loss:0.027, val_acc:0.988]
Epoch [104/120    avg_loss:0.053, val_acc:0.985]
Epoch [105/120    avg_loss:0.033, val_acc:0.988]
Epoch [106/120    avg_loss:0.030, val_acc:0.988]
Epoch [107/120    avg_loss:0.038, val_acc:0.985]
Epoch [108/120    avg_loss:0.037, val_acc:0.988]
Epoch [109/120    avg_loss:0.031, val_acc:0.988]
Epoch [110/120    avg_loss:0.030, val_acc:0.988]
Epoch [111/120    avg_loss:0.026, val_acc:0.985]
Epoch [112/120    avg_loss:0.027, val_acc:0.988]
Epoch [113/120    avg_loss:0.034, val_acc:0.988]
Epoch [114/120    avg_loss:0.027, val_acc:0.988]
Epoch [115/120    avg_loss:0.025, val_acc:0.988]
Epoch [116/120    avg_loss:0.026, val_acc:0.988]
Epoch [117/120    avg_loss:0.035, val_acc:0.988]
Epoch [118/120    avg_loss:0.027, val_acc:0.988]
Epoch [119/120    avg_loss:0.034, val_acc:0.988]
Epoch [120/120    avg_loss:0.028, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.98648649 0.99782135 0.94570136 0.92409241
 1.         0.96703297 1.         1.         1.         0.99208443
 0.99334812 1.        ]

Kappa:
0.9914545370896322
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a74f4c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.392, val_acc:0.425]
Epoch [2/120    avg_loss:2.008, val_acc:0.560]
Epoch [3/120    avg_loss:1.688, val_acc:0.637]
Epoch [4/120    avg_loss:1.380, val_acc:0.752]
Epoch [5/120    avg_loss:1.162, val_acc:0.754]
Epoch [6/120    avg_loss:1.010, val_acc:0.783]
Epoch [7/120    avg_loss:0.882, val_acc:0.738]
Epoch [8/120    avg_loss:0.773, val_acc:0.848]
Epoch [9/120    avg_loss:0.693, val_acc:0.802]
Epoch [10/120    avg_loss:0.666, val_acc:0.850]
Epoch [11/120    avg_loss:0.618, val_acc:0.806]
Epoch [12/120    avg_loss:0.546, val_acc:0.873]
Epoch [13/120    avg_loss:0.524, val_acc:0.804]
Epoch [14/120    avg_loss:0.560, val_acc:0.881]
Epoch [15/120    avg_loss:0.488, val_acc:0.892]
Epoch [16/120    avg_loss:0.473, val_acc:0.892]
Epoch [17/120    avg_loss:0.461, val_acc:0.877]
Epoch [18/120    avg_loss:0.427, val_acc:0.896]
Epoch [19/120    avg_loss:0.370, val_acc:0.904]
Epoch [20/120    avg_loss:0.359, val_acc:0.935]
Epoch [21/120    avg_loss:0.354, val_acc:0.890]
Epoch [22/120    avg_loss:0.446, val_acc:0.898]
Epoch [23/120    avg_loss:0.376, val_acc:0.917]
Epoch [24/120    avg_loss:0.308, val_acc:0.923]
Epoch [25/120    avg_loss:0.358, val_acc:0.902]
Epoch [26/120    avg_loss:0.357, val_acc:0.940]
Epoch [27/120    avg_loss:0.267, val_acc:0.960]
Epoch [28/120    avg_loss:0.254, val_acc:0.929]
Epoch [29/120    avg_loss:0.229, val_acc:0.950]
Epoch [30/120    avg_loss:0.288, val_acc:0.944]
Epoch [31/120    avg_loss:0.242, val_acc:0.963]
Epoch [32/120    avg_loss:0.181, val_acc:0.958]
Epoch [33/120    avg_loss:0.207, val_acc:0.940]
Epoch [34/120    avg_loss:0.210, val_acc:0.973]
Epoch [35/120    avg_loss:0.185, val_acc:0.965]
Epoch [36/120    avg_loss:0.182, val_acc:0.975]
Epoch [37/120    avg_loss:0.168, val_acc:0.975]
Epoch [38/120    avg_loss:0.209, val_acc:0.963]
Epoch [39/120    avg_loss:0.196, val_acc:0.958]
Epoch [40/120    avg_loss:0.185, val_acc:0.958]
Epoch [41/120    avg_loss:0.168, val_acc:0.967]
Epoch [42/120    avg_loss:0.138, val_acc:0.981]
Epoch [43/120    avg_loss:0.135, val_acc:0.965]
Epoch [44/120    avg_loss:0.138, val_acc:0.981]
Epoch [45/120    avg_loss:0.158, val_acc:0.938]
Epoch [46/120    avg_loss:0.150, val_acc:0.977]
Epoch [47/120    avg_loss:0.138, val_acc:0.956]
Epoch [48/120    avg_loss:0.113, val_acc:0.973]
Epoch [49/120    avg_loss:0.165, val_acc:0.981]
Epoch [50/120    avg_loss:0.150, val_acc:0.975]
Epoch [51/120    avg_loss:0.123, val_acc:0.979]
Epoch [52/120    avg_loss:0.101, val_acc:0.979]
Epoch [53/120    avg_loss:0.101, val_acc:0.985]
Epoch [54/120    avg_loss:0.163, val_acc:0.973]
Epoch [55/120    avg_loss:0.096, val_acc:0.973]
Epoch [56/120    avg_loss:0.111, val_acc:0.963]
Epoch [57/120    avg_loss:0.084, val_acc:0.990]
Epoch [58/120    avg_loss:0.076, val_acc:0.992]
Epoch [59/120    avg_loss:0.063, val_acc:0.971]
Epoch [60/120    avg_loss:0.066, val_acc:0.979]
Epoch [61/120    avg_loss:0.048, val_acc:0.988]
Epoch [62/120    avg_loss:0.041, val_acc:0.988]
Epoch [63/120    avg_loss:0.045, val_acc:0.992]
Epoch [64/120    avg_loss:0.068, val_acc:0.983]
Epoch [65/120    avg_loss:0.088, val_acc:0.983]
Epoch [66/120    avg_loss:0.055, val_acc:0.992]
Epoch [67/120    avg_loss:0.064, val_acc:0.983]
Epoch [68/120    avg_loss:0.094, val_acc:0.977]
Epoch [69/120    avg_loss:0.077, val_acc:0.967]
Epoch [70/120    avg_loss:0.086, val_acc:0.990]
Epoch [71/120    avg_loss:0.071, val_acc:0.981]
Epoch [72/120    avg_loss:0.081, val_acc:0.983]
Epoch [73/120    avg_loss:0.083, val_acc:0.990]
Epoch [74/120    avg_loss:0.101, val_acc:0.969]
Epoch [75/120    avg_loss:0.151, val_acc:0.977]
Epoch [76/120    avg_loss:0.108, val_acc:0.965]
Epoch [77/120    avg_loss:0.084, val_acc:0.983]
Epoch [78/120    avg_loss:0.053, val_acc:0.981]
Epoch [79/120    avg_loss:0.067, val_acc:0.990]
Epoch [80/120    avg_loss:0.044, val_acc:0.990]
Epoch [81/120    avg_loss:0.034, val_acc:0.992]
Epoch [82/120    avg_loss:0.028, val_acc:0.992]
Epoch [83/120    avg_loss:0.030, val_acc:0.992]
Epoch [84/120    avg_loss:0.034, val_acc:0.992]
Epoch [85/120    avg_loss:0.037, val_acc:0.992]
Epoch [86/120    avg_loss:0.026, val_acc:0.994]
Epoch [87/120    avg_loss:0.030, val_acc:0.992]
Epoch [88/120    avg_loss:0.025, val_acc:0.992]
Epoch [89/120    avg_loss:0.026, val_acc:0.994]
Epoch [90/120    avg_loss:0.022, val_acc:0.994]
Epoch [91/120    avg_loss:0.035, val_acc:0.994]
Epoch [92/120    avg_loss:0.030, val_acc:0.994]
Epoch [93/120    avg_loss:0.021, val_acc:0.992]
Epoch [94/120    avg_loss:0.025, val_acc:0.992]
Epoch [95/120    avg_loss:0.025, val_acc:0.992]
Epoch [96/120    avg_loss:0.023, val_acc:0.992]
Epoch [97/120    avg_loss:0.027, val_acc:0.992]
Epoch [98/120    avg_loss:0.024, val_acc:0.994]
Epoch [99/120    avg_loss:0.021, val_acc:0.992]
Epoch [100/120    avg_loss:0.027, val_acc:0.990]
Epoch [101/120    avg_loss:0.036, val_acc:0.992]
Epoch [102/120    avg_loss:0.026, val_acc:0.992]
Epoch [103/120    avg_loss:0.023, val_acc:0.992]
Epoch [104/120    avg_loss:0.024, val_acc:0.992]
Epoch [105/120    avg_loss:0.022, val_acc:0.992]
Epoch [106/120    avg_loss:0.027, val_acc:0.992]
Epoch [107/120    avg_loss:0.021, val_acc:0.994]
Epoch [108/120    avg_loss:0.024, val_acc:0.994]
Epoch [109/120    avg_loss:0.030, val_acc:0.992]
Epoch [110/120    avg_loss:0.017, val_acc:0.992]
Epoch [111/120    avg_loss:0.020, val_acc:0.992]
Epoch [112/120    avg_loss:0.023, val_acc:0.992]
Epoch [113/120    avg_loss:0.020, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.021, val_acc:0.992]
Epoch [116/120    avg_loss:0.020, val_acc:0.992]
Epoch [117/120    avg_loss:0.021, val_acc:0.990]
Epoch [118/120    avg_loss:0.020, val_acc:0.992]
Epoch [119/120    avg_loss:0.024, val_acc:0.992]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         0.98426966 0.99563319 0.96179775 0.95016611
 1.         0.96132597 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9940654507142136
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a7f3df7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.457, val_acc:0.459]
Epoch [2/120    avg_loss:2.027, val_acc:0.523]
Epoch [3/120    avg_loss:1.727, val_acc:0.543]
Epoch [4/120    avg_loss:1.479, val_acc:0.631]
Epoch [5/120    avg_loss:1.278, val_acc:0.707]
Epoch [6/120    avg_loss:1.133, val_acc:0.723]
Epoch [7/120    avg_loss:1.020, val_acc:0.797]
Epoch [8/120    avg_loss:0.912, val_acc:0.854]
Epoch [9/120    avg_loss:0.815, val_acc:0.854]
Epoch [10/120    avg_loss:0.664, val_acc:0.869]
Epoch [11/120    avg_loss:0.600, val_acc:0.879]
Epoch [12/120    avg_loss:0.550, val_acc:0.883]
Epoch [13/120    avg_loss:0.528, val_acc:0.875]
Epoch [14/120    avg_loss:0.572, val_acc:0.900]
Epoch [15/120    avg_loss:0.570, val_acc:0.924]
Epoch [16/120    avg_loss:0.442, val_acc:0.938]
Epoch [17/120    avg_loss:0.455, val_acc:0.898]
Epoch [18/120    avg_loss:0.400, val_acc:0.910]
Epoch [19/120    avg_loss:0.382, val_acc:0.871]
Epoch [20/120    avg_loss:0.419, val_acc:0.916]
Epoch [21/120    avg_loss:0.385, val_acc:0.926]
Epoch [22/120    avg_loss:0.391, val_acc:0.910]
Epoch [23/120    avg_loss:0.354, val_acc:0.928]
Epoch [24/120    avg_loss:0.385, val_acc:0.918]
Epoch [25/120    avg_loss:0.297, val_acc:0.926]
Epoch [26/120    avg_loss:0.320, val_acc:0.941]
Epoch [27/120    avg_loss:0.333, val_acc:0.949]
Epoch [28/120    avg_loss:0.384, val_acc:0.910]
Epoch [29/120    avg_loss:0.301, val_acc:0.953]
Epoch [30/120    avg_loss:0.258, val_acc:0.943]
Epoch [31/120    avg_loss:0.238, val_acc:0.936]
Epoch [32/120    avg_loss:0.261, val_acc:0.928]
Epoch [33/120    avg_loss:0.278, val_acc:0.918]
Epoch [34/120    avg_loss:0.274, val_acc:0.914]
Epoch [35/120    avg_loss:0.231, val_acc:0.971]
Epoch [36/120    avg_loss:0.188, val_acc:0.936]
Epoch [37/120    avg_loss:0.286, val_acc:0.953]
Epoch [38/120    avg_loss:0.229, val_acc:0.967]
Epoch [39/120    avg_loss:0.190, val_acc:0.943]
Epoch [40/120    avg_loss:0.210, val_acc:0.967]
Epoch [41/120    avg_loss:0.211, val_acc:0.957]
Epoch [42/120    avg_loss:0.195, val_acc:0.949]
Epoch [43/120    avg_loss:0.179, val_acc:0.955]
Epoch [44/120    avg_loss:0.143, val_acc:0.957]
Epoch [45/120    avg_loss:0.205, val_acc:0.938]
Epoch [46/120    avg_loss:0.220, val_acc:0.938]
Epoch [47/120    avg_loss:0.193, val_acc:0.967]
Epoch [48/120    avg_loss:0.162, val_acc:0.979]
Epoch [49/120    avg_loss:0.212, val_acc:0.961]
Epoch [50/120    avg_loss:0.137, val_acc:0.953]
Epoch [51/120    avg_loss:0.139, val_acc:0.982]
Epoch [52/120    avg_loss:0.111, val_acc:0.975]
Epoch [53/120    avg_loss:0.172, val_acc:0.953]
Epoch [54/120    avg_loss:0.144, val_acc:0.963]
Epoch [55/120    avg_loss:0.140, val_acc:0.949]
Epoch [56/120    avg_loss:0.118, val_acc:0.971]
Epoch [57/120    avg_loss:0.097, val_acc:0.979]
Epoch [58/120    avg_loss:0.145, val_acc:0.936]
Epoch [59/120    avg_loss:0.150, val_acc:0.953]
Epoch [60/120    avg_loss:0.122, val_acc:0.965]
Epoch [61/120    avg_loss:0.117, val_acc:0.961]
Epoch [62/120    avg_loss:0.107, val_acc:0.967]
Epoch [63/120    avg_loss:0.100, val_acc:0.969]
Epoch [64/120    avg_loss:0.098, val_acc:0.963]
Epoch [65/120    avg_loss:0.092, val_acc:0.971]
Epoch [66/120    avg_loss:0.053, val_acc:0.979]
Epoch [67/120    avg_loss:0.054, val_acc:0.982]
Epoch [68/120    avg_loss:0.053, val_acc:0.982]
Epoch [69/120    avg_loss:0.057, val_acc:0.979]
Epoch [70/120    avg_loss:0.048, val_acc:0.984]
Epoch [71/120    avg_loss:0.070, val_acc:0.979]
Epoch [72/120    avg_loss:0.058, val_acc:0.980]
Epoch [73/120    avg_loss:0.055, val_acc:0.980]
Epoch [74/120    avg_loss:0.060, val_acc:0.980]
Epoch [75/120    avg_loss:0.052, val_acc:0.980]
Epoch [76/120    avg_loss:0.081, val_acc:0.979]
Epoch [77/120    avg_loss:0.055, val_acc:0.982]
Epoch [78/120    avg_loss:0.050, val_acc:0.984]
Epoch [79/120    avg_loss:0.047, val_acc:0.986]
Epoch [80/120    avg_loss:0.057, val_acc:0.986]
Epoch [81/120    avg_loss:0.048, val_acc:0.984]
Epoch [82/120    avg_loss:0.048, val_acc:0.984]
Epoch [83/120    avg_loss:0.050, val_acc:0.986]
Epoch [84/120    avg_loss:0.060, val_acc:0.986]
Epoch [85/120    avg_loss:0.057, val_acc:0.984]
Epoch [86/120    avg_loss:0.062, val_acc:0.986]
Epoch [87/120    avg_loss:0.043, val_acc:0.986]
Epoch [88/120    avg_loss:0.043, val_acc:0.986]
Epoch [89/120    avg_loss:0.053, val_acc:0.986]
Epoch [90/120    avg_loss:0.052, val_acc:0.984]
Epoch [91/120    avg_loss:0.056, val_acc:0.984]
Epoch [92/120    avg_loss:0.053, val_acc:0.982]
Epoch [93/120    avg_loss:0.072, val_acc:0.984]
Epoch [94/120    avg_loss:0.045, val_acc:0.982]
Epoch [95/120    avg_loss:0.038, val_acc:0.984]
Epoch [96/120    avg_loss:0.047, val_acc:0.984]
Epoch [97/120    avg_loss:0.041, val_acc:0.986]
Epoch [98/120    avg_loss:0.051, val_acc:0.986]
Epoch [99/120    avg_loss:0.046, val_acc:0.984]
Epoch [100/120    avg_loss:0.048, val_acc:0.986]
Epoch [101/120    avg_loss:0.052, val_acc:0.984]
Epoch [102/120    avg_loss:0.050, val_acc:0.986]
Epoch [103/120    avg_loss:0.045, val_acc:0.980]
Epoch [104/120    avg_loss:0.045, val_acc:0.986]
Epoch [105/120    avg_loss:0.043, val_acc:0.984]
Epoch [106/120    avg_loss:0.042, val_acc:0.988]
Epoch [107/120    avg_loss:0.042, val_acc:0.988]
Epoch [108/120    avg_loss:0.044, val_acc:0.986]
Epoch [109/120    avg_loss:0.043, val_acc:0.986]
Epoch [110/120    avg_loss:0.057, val_acc:0.986]
Epoch [111/120    avg_loss:0.038, val_acc:0.986]
Epoch [112/120    avg_loss:0.036, val_acc:0.986]
Epoch [113/120    avg_loss:0.039, val_acc:0.988]
Epoch [114/120    avg_loss:0.058, val_acc:0.984]
Epoch [115/120    avg_loss:0.045, val_acc:0.986]
Epoch [116/120    avg_loss:0.037, val_acc:0.986]
Epoch [117/120    avg_loss:0.031, val_acc:0.986]
Epoch [118/120    avg_loss:0.045, val_acc:0.986]
Epoch [119/120    avg_loss:0.044, val_acc:0.988]
Epoch [120/120    avg_loss:0.045, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98426966 0.98230088 0.93095768 0.92409241
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9909796028788135
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93a9ee0710>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.453, val_acc:0.481]
Epoch [2/120    avg_loss:1.988, val_acc:0.569]
Epoch [3/120    avg_loss:1.710, val_acc:0.583]
Epoch [4/120    avg_loss:1.480, val_acc:0.627]
Epoch [5/120    avg_loss:1.316, val_acc:0.656]
Epoch [6/120    avg_loss:1.130, val_acc:0.696]
Epoch [7/120    avg_loss:0.957, val_acc:0.825]
Epoch [8/120    avg_loss:0.813, val_acc:0.833]
Epoch [9/120    avg_loss:0.744, val_acc:0.883]
Epoch [10/120    avg_loss:0.663, val_acc:0.890]
Epoch [11/120    avg_loss:0.548, val_acc:0.904]
Epoch [12/120    avg_loss:0.519, val_acc:0.881]
Epoch [13/120    avg_loss:0.518, val_acc:0.917]
Epoch [14/120    avg_loss:0.466, val_acc:0.896]
Epoch [15/120    avg_loss:0.450, val_acc:0.898]
Epoch [16/120    avg_loss:0.501, val_acc:0.917]
Epoch [17/120    avg_loss:0.408, val_acc:0.931]
Epoch [18/120    avg_loss:0.452, val_acc:0.925]
Epoch [19/120    avg_loss:0.446, val_acc:0.910]
Epoch [20/120    avg_loss:0.366, val_acc:0.954]
Epoch [21/120    avg_loss:0.351, val_acc:0.944]
Epoch [22/120    avg_loss:0.336, val_acc:0.952]
Epoch [23/120    avg_loss:0.301, val_acc:0.944]
Epoch [24/120    avg_loss:0.258, val_acc:0.940]
Epoch [25/120    avg_loss:0.279, val_acc:0.935]
Epoch [26/120    avg_loss:0.263, val_acc:0.940]
Epoch [27/120    avg_loss:0.297, val_acc:0.938]
Epoch [28/120    avg_loss:0.256, val_acc:0.925]
Epoch [29/120    avg_loss:0.243, val_acc:0.960]
Epoch [30/120    avg_loss:0.219, val_acc:0.954]
Epoch [31/120    avg_loss:0.183, val_acc:0.973]
Epoch [32/120    avg_loss:0.241, val_acc:0.954]
Epoch [33/120    avg_loss:0.204, val_acc:0.946]
Epoch [34/120    avg_loss:0.177, val_acc:0.977]
Epoch [35/120    avg_loss:0.180, val_acc:0.971]
Epoch [36/120    avg_loss:0.197, val_acc:0.979]
Epoch [37/120    avg_loss:0.164, val_acc:0.973]
Epoch [38/120    avg_loss:0.159, val_acc:0.965]
Epoch [39/120    avg_loss:0.154, val_acc:0.940]
Epoch [40/120    avg_loss:0.218, val_acc:0.958]
Epoch [41/120    avg_loss:0.193, val_acc:0.942]
Epoch [42/120    avg_loss:0.156, val_acc:0.958]
Epoch [43/120    avg_loss:0.182, val_acc:0.973]
Epoch [44/120    avg_loss:0.178, val_acc:0.975]
Epoch [45/120    avg_loss:0.138, val_acc:0.965]
Epoch [46/120    avg_loss:0.125, val_acc:0.979]
Epoch [47/120    avg_loss:0.119, val_acc:0.983]
Epoch [48/120    avg_loss:0.105, val_acc:0.988]
Epoch [49/120    avg_loss:0.144, val_acc:0.973]
Epoch [50/120    avg_loss:0.103, val_acc:0.985]
Epoch [51/120    avg_loss:0.080, val_acc:0.983]
Epoch [52/120    avg_loss:0.101, val_acc:0.990]
Epoch [53/120    avg_loss:0.091, val_acc:0.983]
Epoch [54/120    avg_loss:0.071, val_acc:0.971]
Epoch [55/120    avg_loss:0.115, val_acc:0.983]
Epoch [56/120    avg_loss:0.128, val_acc:0.983]
Epoch [57/120    avg_loss:0.078, val_acc:0.979]
Epoch [58/120    avg_loss:0.106, val_acc:0.979]
Epoch [59/120    avg_loss:0.116, val_acc:0.981]
Epoch [60/120    avg_loss:0.091, val_acc:0.985]
Epoch [61/120    avg_loss:0.278, val_acc:0.963]
Epoch [62/120    avg_loss:0.158, val_acc:0.973]
Epoch [63/120    avg_loss:0.085, val_acc:0.981]
Epoch [64/120    avg_loss:0.082, val_acc:0.977]
Epoch [65/120    avg_loss:0.076, val_acc:0.973]
Epoch [66/120    avg_loss:0.077, val_acc:0.990]
Epoch [67/120    avg_loss:0.051, val_acc:0.990]
Epoch [68/120    avg_loss:0.044, val_acc:0.990]
Epoch [69/120    avg_loss:0.049, val_acc:0.988]
Epoch [70/120    avg_loss:0.043, val_acc:0.988]
Epoch [71/120    avg_loss:0.043, val_acc:0.988]
Epoch [72/120    avg_loss:0.047, val_acc:0.988]
Epoch [73/120    avg_loss:0.047, val_acc:0.985]
Epoch [74/120    avg_loss:0.059, val_acc:0.988]
Epoch [75/120    avg_loss:0.037, val_acc:0.988]
Epoch [76/120    avg_loss:0.051, val_acc:0.988]
Epoch [77/120    avg_loss:0.033, val_acc:0.990]
Epoch [78/120    avg_loss:0.036, val_acc:0.990]
Epoch [79/120    avg_loss:0.029, val_acc:0.990]
Epoch [80/120    avg_loss:0.039, val_acc:0.990]
Epoch [81/120    avg_loss:0.040, val_acc:0.990]
Epoch [82/120    avg_loss:0.036, val_acc:0.988]
Epoch [83/120    avg_loss:0.035, val_acc:0.985]
Epoch [84/120    avg_loss:0.034, val_acc:0.988]
Epoch [85/120    avg_loss:0.031, val_acc:0.988]
Epoch [86/120    avg_loss:0.030, val_acc:0.990]
Epoch [87/120    avg_loss:0.033, val_acc:0.988]
Epoch [88/120    avg_loss:0.029, val_acc:0.988]
Epoch [89/120    avg_loss:0.030, val_acc:0.988]
Epoch [90/120    avg_loss:0.031, val_acc:0.988]
Epoch [91/120    avg_loss:0.036, val_acc:0.988]
Epoch [92/120    avg_loss:0.030, val_acc:0.988]
Epoch [93/120    avg_loss:0.030, val_acc:0.988]
Epoch [94/120    avg_loss:0.037, val_acc:0.990]
Epoch [95/120    avg_loss:0.029, val_acc:0.990]
Epoch [96/120    avg_loss:0.031, val_acc:0.988]
Epoch [97/120    avg_loss:0.029, val_acc:0.988]
Epoch [98/120    avg_loss:0.024, val_acc:0.988]
Epoch [99/120    avg_loss:0.023, val_acc:0.990]
Epoch [100/120    avg_loss:0.027, val_acc:0.990]
Epoch [101/120    avg_loss:0.032, val_acc:0.990]
Epoch [102/120    avg_loss:0.038, val_acc:0.985]
Epoch [103/120    avg_loss:0.031, val_acc:0.988]
Epoch [104/120    avg_loss:0.035, val_acc:0.990]
Epoch [105/120    avg_loss:0.039, val_acc:0.988]
Epoch [106/120    avg_loss:0.027, val_acc:0.990]
Epoch [107/120    avg_loss:0.032, val_acc:0.990]
Epoch [108/120    avg_loss:0.031, val_acc:0.990]
Epoch [109/120    avg_loss:0.025, val_acc:0.988]
Epoch [110/120    avg_loss:0.035, val_acc:0.988]
Epoch [111/120    avg_loss:0.023, val_acc:0.988]
Epoch [112/120    avg_loss:0.026, val_acc:0.988]
Epoch [113/120    avg_loss:0.032, val_acc:0.988]
Epoch [114/120    avg_loss:0.022, val_acc:0.988]
Epoch [115/120    avg_loss:0.033, val_acc:0.988]
Epoch [116/120    avg_loss:0.028, val_acc:0.990]
Epoch [117/120    avg_loss:0.028, val_acc:0.990]
Epoch [118/120    avg_loss:0.021, val_acc:0.988]
Epoch [119/120    avg_loss:0.025, val_acc:0.988]
Epoch [120/120    avg_loss:0.025, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.98206278 0.99563319 0.95909091 0.94771242
 1.         0.95555556 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9926414348711059
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa99c6777b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.480, val_acc:0.346]
Epoch [2/120    avg_loss:2.079, val_acc:0.637]
Epoch [3/120    avg_loss:1.762, val_acc:0.623]
Epoch [4/120    avg_loss:1.492, val_acc:0.619]
Epoch [5/120    avg_loss:1.271, val_acc:0.700]
Epoch [6/120    avg_loss:1.072, val_acc:0.731]
Epoch [7/120    avg_loss:0.976, val_acc:0.750]
Epoch [8/120    avg_loss:0.876, val_acc:0.769]
Epoch [9/120    avg_loss:0.812, val_acc:0.823]
Epoch [10/120    avg_loss:0.717, val_acc:0.800]
Epoch [11/120    avg_loss:0.715, val_acc:0.827]
Epoch [12/120    avg_loss:0.647, val_acc:0.848]
Epoch [13/120    avg_loss:0.584, val_acc:0.887]
Epoch [14/120    avg_loss:0.561, val_acc:0.887]
Epoch [15/120    avg_loss:0.560, val_acc:0.871]
Epoch [16/120    avg_loss:0.520, val_acc:0.894]
Epoch [17/120    avg_loss:0.536, val_acc:0.908]
Epoch [18/120    avg_loss:0.430, val_acc:0.933]
Epoch [19/120    avg_loss:0.377, val_acc:0.919]
Epoch [20/120    avg_loss:0.393, val_acc:0.906]
Epoch [21/120    avg_loss:0.401, val_acc:0.912]
Epoch [22/120    avg_loss:0.407, val_acc:0.912]
Epoch [23/120    avg_loss:0.453, val_acc:0.940]
Epoch [24/120    avg_loss:0.373, val_acc:0.925]
Epoch [25/120    avg_loss:0.327, val_acc:0.940]
Epoch [26/120    avg_loss:0.246, val_acc:0.946]
Epoch [27/120    avg_loss:0.250, val_acc:0.946]
Epoch [28/120    avg_loss:0.271, val_acc:0.894]
Epoch [29/120    avg_loss:0.315, val_acc:0.950]
Epoch [30/120    avg_loss:0.251, val_acc:0.944]
Epoch [31/120    avg_loss:0.308, val_acc:0.948]
Epoch [32/120    avg_loss:0.231, val_acc:0.942]
Epoch [33/120    avg_loss:0.215, val_acc:0.956]
Epoch [34/120    avg_loss:0.163, val_acc:0.958]
Epoch [35/120    avg_loss:0.176, val_acc:0.969]
Epoch [36/120    avg_loss:0.200, val_acc:0.912]
Epoch [37/120    avg_loss:0.195, val_acc:0.952]
Epoch [38/120    avg_loss:0.228, val_acc:0.929]
Epoch [39/120    avg_loss:0.231, val_acc:0.960]
Epoch [40/120    avg_loss:0.240, val_acc:0.952]
Epoch [41/120    avg_loss:0.179, val_acc:0.963]
Epoch [42/120    avg_loss:0.179, val_acc:0.948]
Epoch [43/120    avg_loss:0.142, val_acc:0.965]
Epoch [44/120    avg_loss:0.141, val_acc:0.967]
Epoch [45/120    avg_loss:0.182, val_acc:0.965]
Epoch [46/120    avg_loss:0.146, val_acc:0.969]
Epoch [47/120    avg_loss:0.141, val_acc:0.973]
Epoch [48/120    avg_loss:0.103, val_acc:0.975]
Epoch [49/120    avg_loss:0.102, val_acc:0.973]
Epoch [50/120    avg_loss:0.110, val_acc:0.973]
Epoch [51/120    avg_loss:0.138, val_acc:0.979]
Epoch [52/120    avg_loss:0.081, val_acc:0.992]
Epoch [53/120    avg_loss:0.071, val_acc:0.988]
Epoch [54/120    avg_loss:0.072, val_acc:0.990]
Epoch [55/120    avg_loss:0.081, val_acc:0.983]
Epoch [56/120    avg_loss:0.072, val_acc:0.983]
Epoch [57/120    avg_loss:0.165, val_acc:0.950]
Epoch [58/120    avg_loss:0.118, val_acc:0.960]
Epoch [59/120    avg_loss:0.096, val_acc:0.973]
Epoch [60/120    avg_loss:0.093, val_acc:0.975]
Epoch [61/120    avg_loss:0.073, val_acc:0.981]
Epoch [62/120    avg_loss:0.069, val_acc:0.988]
Epoch [63/120    avg_loss:0.152, val_acc:0.973]
Epoch [64/120    avg_loss:0.106, val_acc:0.981]
Epoch [65/120    avg_loss:0.088, val_acc:0.979]
Epoch [66/120    avg_loss:0.065, val_acc:0.983]
Epoch [67/120    avg_loss:0.063, val_acc:0.985]
Epoch [68/120    avg_loss:0.049, val_acc:0.985]
Epoch [69/120    avg_loss:0.044, val_acc:0.983]
Epoch [70/120    avg_loss:0.044, val_acc:0.983]
Epoch [71/120    avg_loss:0.038, val_acc:0.983]
Epoch [72/120    avg_loss:0.034, val_acc:0.983]
Epoch [73/120    avg_loss:0.043, val_acc:0.985]
Epoch [74/120    avg_loss:0.034, val_acc:0.983]
Epoch [75/120    avg_loss:0.034, val_acc:0.983]
Epoch [76/120    avg_loss:0.043, val_acc:0.983]
Epoch [77/120    avg_loss:0.038, val_acc:0.983]
Epoch [78/120    avg_loss:0.036, val_acc:0.983]
Epoch [79/120    avg_loss:0.030, val_acc:0.983]
Epoch [80/120    avg_loss:0.035, val_acc:0.983]
Epoch [81/120    avg_loss:0.043, val_acc:0.983]
Epoch [82/120    avg_loss:0.037, val_acc:0.983]
Epoch [83/120    avg_loss:0.033, val_acc:0.983]
Epoch [84/120    avg_loss:0.039, val_acc:0.983]
Epoch [85/120    avg_loss:0.032, val_acc:0.983]
Epoch [86/120    avg_loss:0.039, val_acc:0.983]
Epoch [87/120    avg_loss:0.033, val_acc:0.983]
Epoch [88/120    avg_loss:0.031, val_acc:0.983]
Epoch [89/120    avg_loss:0.038, val_acc:0.983]
Epoch [90/120    avg_loss:0.033, val_acc:0.983]
Epoch [91/120    avg_loss:0.033, val_acc:0.983]
Epoch [92/120    avg_loss:0.046, val_acc:0.983]
Epoch [93/120    avg_loss:0.040, val_acc:0.983]
Epoch [94/120    avg_loss:0.030, val_acc:0.983]
Epoch [95/120    avg_loss:0.041, val_acc:0.983]
Epoch [96/120    avg_loss:0.031, val_acc:0.983]
Epoch [97/120    avg_loss:0.032, val_acc:0.983]
Epoch [98/120    avg_loss:0.040, val_acc:0.983]
Epoch [99/120    avg_loss:0.038, val_acc:0.983]
Epoch [100/120    avg_loss:0.038, val_acc:0.983]
Epoch [101/120    avg_loss:0.033, val_acc:0.983]
Epoch [102/120    avg_loss:0.029, val_acc:0.983]
Epoch [103/120    avg_loss:0.031, val_acc:0.983]
Epoch [104/120    avg_loss:0.035, val_acc:0.983]
Epoch [105/120    avg_loss:0.039, val_acc:0.983]
Epoch [106/120    avg_loss:0.032, val_acc:0.983]
Epoch [107/120    avg_loss:0.033, val_acc:0.983]
Epoch [108/120    avg_loss:0.035, val_acc:0.983]
Epoch [109/120    avg_loss:0.032, val_acc:0.983]
Epoch [110/120    avg_loss:0.037, val_acc:0.983]
Epoch [111/120    avg_loss:0.036, val_acc:0.983]
Epoch [112/120    avg_loss:0.025, val_acc:0.983]
Epoch [113/120    avg_loss:0.032, val_acc:0.983]
Epoch [114/120    avg_loss:0.033, val_acc:0.983]
Epoch [115/120    avg_loss:0.034, val_acc:0.983]
Epoch [116/120    avg_loss:0.030, val_acc:0.983]
Epoch [117/120    avg_loss:0.032, val_acc:0.983]
Epoch [118/120    avg_loss:0.036, val_acc:0.983]
Epoch [119/120    avg_loss:0.032, val_acc:0.983]
Epoch [120/120    avg_loss:0.041, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.97767857 0.99122807 0.94356659 0.93114754
 1.         0.94382022 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9916916457182088
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ed6982828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.449, val_acc:0.458]
Epoch [2/120    avg_loss:2.029, val_acc:0.594]
Epoch [3/120    avg_loss:1.732, val_acc:0.662]
Epoch [4/120    avg_loss:1.446, val_acc:0.698]
Epoch [5/120    avg_loss:1.234, val_acc:0.704]
Epoch [6/120    avg_loss:1.026, val_acc:0.831]
Epoch [7/120    avg_loss:0.892, val_acc:0.833]
Epoch [8/120    avg_loss:0.736, val_acc:0.904]
Epoch [9/120    avg_loss:0.704, val_acc:0.896]
Epoch [10/120    avg_loss:0.668, val_acc:0.910]
Epoch [11/120    avg_loss:0.541, val_acc:0.927]
Epoch [12/120    avg_loss:0.495, val_acc:0.929]
Epoch [13/120    avg_loss:0.470, val_acc:0.938]
Epoch [14/120    avg_loss:0.475, val_acc:0.910]
Epoch [15/120    avg_loss:0.430, val_acc:0.931]
Epoch [16/120    avg_loss:0.392, val_acc:0.890]
Epoch [17/120    avg_loss:0.386, val_acc:0.906]
Epoch [18/120    avg_loss:0.378, val_acc:0.938]
Epoch [19/120    avg_loss:0.365, val_acc:0.917]
Epoch [20/120    avg_loss:0.318, val_acc:0.935]
Epoch [21/120    avg_loss:0.322, val_acc:0.927]
Epoch [22/120    avg_loss:0.336, val_acc:0.944]
Epoch [23/120    avg_loss:0.316, val_acc:0.948]
Epoch [24/120    avg_loss:0.290, val_acc:0.946]
Epoch [25/120    avg_loss:0.253, val_acc:0.971]
Epoch [26/120    avg_loss:0.262, val_acc:0.950]
Epoch [27/120    avg_loss:0.244, val_acc:0.931]
Epoch [28/120    avg_loss:0.285, val_acc:0.952]
Epoch [29/120    avg_loss:0.261, val_acc:0.950]
Epoch [30/120    avg_loss:0.270, val_acc:0.979]
Epoch [31/120    avg_loss:0.208, val_acc:0.950]
Epoch [32/120    avg_loss:0.184, val_acc:0.983]
Epoch [33/120    avg_loss:0.162, val_acc:0.960]
Epoch [34/120    avg_loss:0.218, val_acc:0.963]
Epoch [35/120    avg_loss:0.181, val_acc:0.956]
Epoch [36/120    avg_loss:0.223, val_acc:0.979]
Epoch [37/120    avg_loss:0.177, val_acc:0.969]
Epoch [38/120    avg_loss:0.176, val_acc:0.969]
Epoch [39/120    avg_loss:0.151, val_acc:0.981]
Epoch [40/120    avg_loss:0.171, val_acc:0.973]
Epoch [41/120    avg_loss:0.253, val_acc:0.971]
Epoch [42/120    avg_loss:0.153, val_acc:0.965]
Epoch [43/120    avg_loss:0.136, val_acc:0.969]
Epoch [44/120    avg_loss:0.136, val_acc:0.979]
Epoch [45/120    avg_loss:0.102, val_acc:0.981]
Epoch [46/120    avg_loss:0.095, val_acc:0.988]
Epoch [47/120    avg_loss:0.079, val_acc:0.990]
Epoch [48/120    avg_loss:0.095, val_acc:0.985]
Epoch [49/120    avg_loss:0.093, val_acc:0.988]
Epoch [50/120    avg_loss:0.095, val_acc:0.985]
Epoch [51/120    avg_loss:0.093, val_acc:0.985]
Epoch [52/120    avg_loss:0.079, val_acc:0.988]
Epoch [53/120    avg_loss:0.085, val_acc:0.990]
Epoch [54/120    avg_loss:0.077, val_acc:0.985]
Epoch [55/120    avg_loss:0.068, val_acc:0.983]
Epoch [56/120    avg_loss:0.079, val_acc:0.985]
Epoch [57/120    avg_loss:0.063, val_acc:0.990]
Epoch [58/120    avg_loss:0.074, val_acc:0.992]
Epoch [59/120    avg_loss:0.081, val_acc:0.985]
Epoch [60/120    avg_loss:0.064, val_acc:0.992]
Epoch [61/120    avg_loss:0.071, val_acc:0.988]
Epoch [62/120    avg_loss:0.061, val_acc:0.988]
Epoch [63/120    avg_loss:0.082, val_acc:0.985]
Epoch [64/120    avg_loss:0.060, val_acc:0.990]
Epoch [65/120    avg_loss:0.055, val_acc:0.992]
Epoch [66/120    avg_loss:0.068, val_acc:0.994]
Epoch [67/120    avg_loss:0.078, val_acc:0.994]
Epoch [68/120    avg_loss:0.077, val_acc:0.994]
Epoch [69/120    avg_loss:0.059, val_acc:0.990]
Epoch [70/120    avg_loss:0.062, val_acc:0.992]
Epoch [71/120    avg_loss:0.069, val_acc:0.990]
Epoch [72/120    avg_loss:0.064, val_acc:0.994]
Epoch [73/120    avg_loss:0.067, val_acc:0.994]
Epoch [74/120    avg_loss:0.060, val_acc:0.992]
Epoch [75/120    avg_loss:0.062, val_acc:0.994]
Epoch [76/120    avg_loss:0.060, val_acc:0.994]
Epoch [77/120    avg_loss:0.061, val_acc:0.990]
Epoch [78/120    avg_loss:0.071, val_acc:0.990]
Epoch [79/120    avg_loss:0.072, val_acc:0.992]
Epoch [80/120    avg_loss:0.060, val_acc:0.990]
Epoch [81/120    avg_loss:0.052, val_acc:0.994]
Epoch [82/120    avg_loss:0.053, val_acc:0.994]
Epoch [83/120    avg_loss:0.071, val_acc:0.992]
Epoch [84/120    avg_loss:0.049, val_acc:0.992]
Epoch [85/120    avg_loss:0.056, val_acc:0.992]
Epoch [86/120    avg_loss:0.061, val_acc:0.988]
Epoch [87/120    avg_loss:0.065, val_acc:0.990]
Epoch [88/120    avg_loss:0.055, val_acc:0.994]
Epoch [89/120    avg_loss:0.062, val_acc:0.996]
Epoch [90/120    avg_loss:0.048, val_acc:0.996]
Epoch [91/120    avg_loss:0.058, val_acc:0.990]
Epoch [92/120    avg_loss:0.066, val_acc:0.992]
Epoch [93/120    avg_loss:0.051, val_acc:0.992]
Epoch [94/120    avg_loss:0.055, val_acc:0.994]
Epoch [95/120    avg_loss:0.061, val_acc:0.994]
Epoch [96/120    avg_loss:0.059, val_acc:0.994]
Epoch [97/120    avg_loss:0.056, val_acc:0.994]
Epoch [98/120    avg_loss:0.051, val_acc:0.996]
Epoch [99/120    avg_loss:0.044, val_acc:0.994]
Epoch [100/120    avg_loss:0.047, val_acc:0.994]
Epoch [101/120    avg_loss:0.047, val_acc:0.994]
Epoch [102/120    avg_loss:0.049, val_acc:0.994]
Epoch [103/120    avg_loss:0.049, val_acc:0.992]
Epoch [104/120    avg_loss:0.050, val_acc:0.990]
Epoch [105/120    avg_loss:0.048, val_acc:0.994]
Epoch [106/120    avg_loss:0.041, val_acc:0.996]
Epoch [107/120    avg_loss:0.044, val_acc:0.994]
Epoch [108/120    avg_loss:0.043, val_acc:0.994]
Epoch [109/120    avg_loss:0.037, val_acc:0.994]
Epoch [110/120    avg_loss:0.035, val_acc:0.994]
Epoch [111/120    avg_loss:0.046, val_acc:0.992]
Epoch [112/120    avg_loss:0.056, val_acc:0.992]
Epoch [113/120    avg_loss:0.039, val_acc:0.992]
Epoch [114/120    avg_loss:0.043, val_acc:0.992]
Epoch [115/120    avg_loss:0.075, val_acc:0.992]
Epoch [116/120    avg_loss:0.037, val_acc:0.994]
Epoch [117/120    avg_loss:0.054, val_acc:0.994]
Epoch [118/120    avg_loss:0.056, val_acc:0.994]
Epoch [119/120    avg_loss:0.052, val_acc:0.992]
Epoch [120/120    avg_loss:0.045, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.97767857 0.98901099 0.95515695 0.95049505
 1.         0.94382022 1.         1.         1.         0.99337748
 0.99447514 1.        ]

Kappa:
0.9916916099736516
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1159a897f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.466, val_acc:0.356]
Epoch [2/120    avg_loss:2.067, val_acc:0.502]
Epoch [3/120    avg_loss:1.765, val_acc:0.640]
Epoch [4/120    avg_loss:1.481, val_acc:0.646]
Epoch [5/120    avg_loss:1.272, val_acc:0.713]
Epoch [6/120    avg_loss:1.090, val_acc:0.756]
Epoch [7/120    avg_loss:0.965, val_acc:0.754]
Epoch [8/120    avg_loss:0.851, val_acc:0.752]
Epoch [9/120    avg_loss:0.802, val_acc:0.800]
Epoch [10/120    avg_loss:0.752, val_acc:0.833]
Epoch [11/120    avg_loss:0.632, val_acc:0.850]
Epoch [12/120    avg_loss:0.571, val_acc:0.906]
Epoch [13/120    avg_loss:0.563, val_acc:0.887]
Epoch [14/120    avg_loss:0.505, val_acc:0.919]
Epoch [15/120    avg_loss:0.437, val_acc:0.912]
Epoch [16/120    avg_loss:0.462, val_acc:0.873]
Epoch [17/120    avg_loss:0.459, val_acc:0.925]
Epoch [18/120    avg_loss:0.360, val_acc:0.927]
Epoch [19/120    avg_loss:0.355, val_acc:0.944]
Epoch [20/120    avg_loss:0.316, val_acc:0.944]
Epoch [21/120    avg_loss:0.304, val_acc:0.942]
Epoch [22/120    avg_loss:0.303, val_acc:0.952]
Epoch [23/120    avg_loss:0.243, val_acc:0.917]
Epoch [24/120    avg_loss:0.230, val_acc:0.944]
Epoch [25/120    avg_loss:0.227, val_acc:0.946]
Epoch [26/120    avg_loss:0.242, val_acc:0.956]
Epoch [27/120    avg_loss:0.277, val_acc:0.946]
Epoch [28/120    avg_loss:0.235, val_acc:0.935]
Epoch [29/120    avg_loss:0.226, val_acc:0.967]
Epoch [30/120    avg_loss:0.214, val_acc:0.940]
Epoch [31/120    avg_loss:0.174, val_acc:0.950]
Epoch [32/120    avg_loss:0.206, val_acc:0.940]
Epoch [33/120    avg_loss:0.203, val_acc:0.952]
Epoch [34/120    avg_loss:0.158, val_acc:0.954]
Epoch [35/120    avg_loss:0.143, val_acc:0.973]
Epoch [36/120    avg_loss:0.178, val_acc:0.954]
Epoch [37/120    avg_loss:0.147, val_acc:0.967]
Epoch [38/120    avg_loss:0.180, val_acc:0.946]
Epoch [39/120    avg_loss:0.205, val_acc:0.965]
Epoch [40/120    avg_loss:0.169, val_acc:0.975]
Epoch [41/120    avg_loss:0.139, val_acc:0.969]
Epoch [42/120    avg_loss:0.107, val_acc:0.969]
Epoch [43/120    avg_loss:0.088, val_acc:0.977]
Epoch [44/120    avg_loss:0.095, val_acc:0.956]
Epoch [45/120    avg_loss:0.094, val_acc:0.965]
Epoch [46/120    avg_loss:0.124, val_acc:0.969]
Epoch [47/120    avg_loss:0.115, val_acc:0.965]
Epoch [48/120    avg_loss:0.114, val_acc:0.977]
Epoch [49/120    avg_loss:0.097, val_acc:0.969]
Epoch [50/120    avg_loss:0.143, val_acc:0.956]
Epoch [51/120    avg_loss:0.154, val_acc:0.977]
Epoch [52/120    avg_loss:0.093, val_acc:0.979]
Epoch [53/120    avg_loss:0.076, val_acc:0.971]
Epoch [54/120    avg_loss:0.076, val_acc:0.973]
Epoch [55/120    avg_loss:0.067, val_acc:0.979]
Epoch [56/120    avg_loss:0.079, val_acc:0.977]
Epoch [57/120    avg_loss:0.060, val_acc:0.965]
Epoch [58/120    avg_loss:0.085, val_acc:0.969]
Epoch [59/120    avg_loss:0.065, val_acc:0.971]
Epoch [60/120    avg_loss:0.062, val_acc:0.965]
Epoch [61/120    avg_loss:0.123, val_acc:0.971]
Epoch [62/120    avg_loss:0.104, val_acc:0.975]
Epoch [63/120    avg_loss:0.077, val_acc:0.971]
Epoch [64/120    avg_loss:0.054, val_acc:0.971]
Epoch [65/120    avg_loss:0.108, val_acc:0.985]
Epoch [66/120    avg_loss:0.058, val_acc:0.973]
Epoch [67/120    avg_loss:0.038, val_acc:0.977]
Epoch [68/120    avg_loss:0.084, val_acc:0.969]
Epoch [69/120    avg_loss:0.083, val_acc:0.988]
Epoch [70/120    avg_loss:0.065, val_acc:0.981]
Epoch [71/120    avg_loss:0.042, val_acc:0.985]
Epoch [72/120    avg_loss:0.036, val_acc:0.990]
Epoch [73/120    avg_loss:0.047, val_acc:0.981]
Epoch [74/120    avg_loss:0.041, val_acc:0.990]
Epoch [75/120    avg_loss:0.054, val_acc:0.973]
Epoch [76/120    avg_loss:0.055, val_acc:0.979]
Epoch [77/120    avg_loss:0.037, val_acc:0.979]
Epoch [78/120    avg_loss:0.035, val_acc:0.985]
Epoch [79/120    avg_loss:0.034, val_acc:0.990]
Epoch [80/120    avg_loss:0.024, val_acc:0.983]
Epoch [81/120    avg_loss:0.027, val_acc:0.983]
Epoch [82/120    avg_loss:0.026, val_acc:0.983]
Epoch [83/120    avg_loss:0.027, val_acc:0.985]
Epoch [84/120    avg_loss:0.046, val_acc:0.977]
Epoch [85/120    avg_loss:0.047, val_acc:0.981]
Epoch [86/120    avg_loss:0.052, val_acc:0.988]
Epoch [87/120    avg_loss:0.045, val_acc:0.981]
Epoch [88/120    avg_loss:0.032, val_acc:0.985]
Epoch [89/120    avg_loss:0.022, val_acc:0.988]
Epoch [90/120    avg_loss:0.030, val_acc:0.981]
Epoch [91/120    avg_loss:0.037, val_acc:0.979]
Epoch [92/120    avg_loss:0.044, val_acc:0.971]
Epoch [93/120    avg_loss:0.030, val_acc:0.973]
Epoch [94/120    avg_loss:0.027, val_acc:0.975]
Epoch [95/120    avg_loss:0.025, val_acc:0.979]
Epoch [96/120    avg_loss:0.026, val_acc:0.981]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.988]
Epoch [99/120    avg_loss:0.027, val_acc:0.990]
Epoch [100/120    avg_loss:0.016, val_acc:0.990]
Epoch [101/120    avg_loss:0.017, val_acc:0.990]
Epoch [102/120    avg_loss:0.019, val_acc:0.990]
Epoch [103/120    avg_loss:0.014, val_acc:0.990]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.015, val_acc:0.990]
Epoch [106/120    avg_loss:0.013, val_acc:0.990]
Epoch [107/120    avg_loss:0.018, val_acc:0.990]
Epoch [108/120    avg_loss:0.014, val_acc:0.990]
Epoch [109/120    avg_loss:0.017, val_acc:0.988]
Epoch [110/120    avg_loss:0.015, val_acc:0.990]
Epoch [111/120    avg_loss:0.014, val_acc:0.990]
Epoch [112/120    avg_loss:0.019, val_acc:0.988]
Epoch [113/120    avg_loss:0.018, val_acc:0.990]
Epoch [114/120    avg_loss:0.018, val_acc:0.990]
Epoch [115/120    avg_loss:0.015, val_acc:0.990]
Epoch [116/120    avg_loss:0.011, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.020, val_acc:0.992]
Epoch [119/120    avg_loss:0.015, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.97767857 1.         0.95475113 0.93687708
 1.         0.94382022 1.         1.         1.         0.99470899
 0.99447514 1.        ]

Kappa:
0.9919288581434814
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff7ea701828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.398, val_acc:0.504]
Epoch [2/120    avg_loss:2.033, val_acc:0.552]
Epoch [3/120    avg_loss:1.715, val_acc:0.558]
Epoch [4/120    avg_loss:1.467, val_acc:0.658]
Epoch [5/120    avg_loss:1.216, val_acc:0.717]
Epoch [6/120    avg_loss:1.061, val_acc:0.798]
Epoch [7/120    avg_loss:0.990, val_acc:0.840]
Epoch [8/120    avg_loss:0.858, val_acc:0.902]
Epoch [9/120    avg_loss:0.711, val_acc:0.921]
Epoch [10/120    avg_loss:0.603, val_acc:0.908]
Epoch [11/120    avg_loss:0.542, val_acc:0.931]
Epoch [12/120    avg_loss:0.552, val_acc:0.921]
Epoch [13/120    avg_loss:0.487, val_acc:0.925]
Epoch [14/120    avg_loss:0.440, val_acc:0.942]
Epoch [15/120    avg_loss:0.444, val_acc:0.944]
Epoch [16/120    avg_loss:0.420, val_acc:0.942]
Epoch [17/120    avg_loss:0.353, val_acc:0.940]
Epoch [18/120    avg_loss:0.369, val_acc:0.933]
Epoch [19/120    avg_loss:0.323, val_acc:0.946]
Epoch [20/120    avg_loss:0.310, val_acc:0.906]
Epoch [21/120    avg_loss:0.283, val_acc:0.952]
Epoch [22/120    avg_loss:0.256, val_acc:0.952]
Epoch [23/120    avg_loss:0.223, val_acc:0.956]
Epoch [24/120    avg_loss:0.252, val_acc:0.960]
Epoch [25/120    avg_loss:0.317, val_acc:0.894]
Epoch [26/120    avg_loss:0.337, val_acc:0.917]
Epoch [27/120    avg_loss:0.316, val_acc:0.921]
Epoch [28/120    avg_loss:0.321, val_acc:0.956]
Epoch [29/120    avg_loss:0.231, val_acc:0.973]
Epoch [30/120    avg_loss:0.212, val_acc:0.948]
Epoch [31/120    avg_loss:0.197, val_acc:0.956]
Epoch [32/120    avg_loss:0.167, val_acc:0.973]
Epoch [33/120    avg_loss:0.151, val_acc:0.967]
Epoch [34/120    avg_loss:0.153, val_acc:0.965]
Epoch [35/120    avg_loss:0.136, val_acc:0.975]
Epoch [36/120    avg_loss:0.118, val_acc:0.975]
Epoch [37/120    avg_loss:0.143, val_acc:0.965]
Epoch [38/120    avg_loss:0.168, val_acc:0.958]
Epoch [39/120    avg_loss:0.144, val_acc:0.956]
Epoch [40/120    avg_loss:0.138, val_acc:0.967]
Epoch [41/120    avg_loss:0.102, val_acc:0.969]
Epoch [42/120    avg_loss:0.129, val_acc:0.958]
Epoch [43/120    avg_loss:0.170, val_acc:0.973]
Epoch [44/120    avg_loss:0.151, val_acc:0.958]
Epoch [45/120    avg_loss:0.142, val_acc:0.967]
Epoch [46/120    avg_loss:0.135, val_acc:0.967]
Epoch [47/120    avg_loss:0.117, val_acc:0.979]
Epoch [48/120    avg_loss:0.112, val_acc:0.979]
Epoch [49/120    avg_loss:0.124, val_acc:0.927]
Epoch [50/120    avg_loss:0.249, val_acc:0.988]
Epoch [51/120    avg_loss:0.142, val_acc:0.977]
Epoch [52/120    avg_loss:0.102, val_acc:0.985]
Epoch [53/120    avg_loss:0.090, val_acc:0.985]
Epoch [54/120    avg_loss:0.124, val_acc:0.975]
Epoch [55/120    avg_loss:0.116, val_acc:0.960]
Epoch [56/120    avg_loss:0.143, val_acc:0.975]
Epoch [57/120    avg_loss:0.119, val_acc:0.971]
Epoch [58/120    avg_loss:0.204, val_acc:0.967]
Epoch [59/120    avg_loss:0.159, val_acc:0.969]
Epoch [60/120    avg_loss:0.091, val_acc:0.985]
Epoch [61/120    avg_loss:0.109, val_acc:0.983]
Epoch [62/120    avg_loss:0.076, val_acc:0.985]
Epoch [63/120    avg_loss:0.096, val_acc:0.990]
Epoch [64/120    avg_loss:0.100, val_acc:0.988]
Epoch [65/120    avg_loss:0.066, val_acc:0.985]
Epoch [66/120    avg_loss:0.065, val_acc:0.992]
Epoch [67/120    avg_loss:0.083, val_acc:0.979]
Epoch [68/120    avg_loss:0.075, val_acc:0.981]
Epoch [69/120    avg_loss:0.046, val_acc:0.981]
Epoch [70/120    avg_loss:0.087, val_acc:0.990]
Epoch [71/120    avg_loss:0.058, val_acc:0.983]
Epoch [72/120    avg_loss:0.060, val_acc:0.992]
Epoch [73/120    avg_loss:0.035, val_acc:0.985]
Epoch [74/120    avg_loss:0.050, val_acc:0.985]
Epoch [75/120    avg_loss:0.051, val_acc:0.985]
Epoch [76/120    avg_loss:0.028, val_acc:0.988]
Epoch [77/120    avg_loss:0.057, val_acc:0.983]
Epoch [78/120    avg_loss:0.060, val_acc:0.983]
Epoch [79/120    avg_loss:0.041, val_acc:0.990]
Epoch [80/120    avg_loss:0.039, val_acc:0.985]
Epoch [81/120    avg_loss:0.041, val_acc:0.965]
Epoch [82/120    avg_loss:0.039, val_acc:0.985]
Epoch [83/120    avg_loss:0.034, val_acc:0.988]
Epoch [84/120    avg_loss:0.031, val_acc:0.990]
Epoch [85/120    avg_loss:0.043, val_acc:0.992]
Epoch [86/120    avg_loss:0.034, val_acc:0.988]
Epoch [87/120    avg_loss:0.026, val_acc:0.985]
Epoch [88/120    avg_loss:0.030, val_acc:0.988]
Epoch [89/120    avg_loss:0.030, val_acc:0.990]
Epoch [90/120    avg_loss:0.065, val_acc:0.981]
Epoch [91/120    avg_loss:0.035, val_acc:0.996]
Epoch [92/120    avg_loss:0.027, val_acc:0.992]
Epoch [93/120    avg_loss:0.018, val_acc:0.994]
Epoch [94/120    avg_loss:0.016, val_acc:0.996]
Epoch [95/120    avg_loss:0.014, val_acc:0.998]
Epoch [96/120    avg_loss:0.018, val_acc:0.996]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.024, val_acc:0.998]
Epoch [99/120    avg_loss:0.030, val_acc:0.994]
Epoch [100/120    avg_loss:0.059, val_acc:0.988]
Epoch [101/120    avg_loss:0.022, val_acc:0.992]
Epoch [102/120    avg_loss:0.021, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.998]
Epoch [104/120    avg_loss:0.019, val_acc:0.996]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.994]
Epoch [107/120    avg_loss:0.017, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.998]
Epoch [109/120    avg_loss:0.017, val_acc:0.985]
Epoch [110/120    avg_loss:0.115, val_acc:0.990]
Epoch [111/120    avg_loss:0.059, val_acc:0.988]
Epoch [112/120    avg_loss:0.031, val_acc:0.988]
Epoch [113/120    avg_loss:0.060, val_acc:0.988]
Epoch [114/120    avg_loss:0.027, val_acc:0.994]
Epoch [115/120    avg_loss:0.021, val_acc:0.988]
Epoch [116/120    avg_loss:0.017, val_acc:0.998]
Epoch [117/120    avg_loss:0.014, val_acc:0.994]
Epoch [118/120    avg_loss:0.014, val_acc:0.994]
Epoch [119/120    avg_loss:0.010, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 1.         0.99319728 1.         0.96196868 0.94276094
 1.         0.98378378 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9950150588346048
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fed061b37b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.418, val_acc:0.569]
Epoch [2/120    avg_loss:2.017, val_acc:0.562]
Epoch [3/120    avg_loss:1.691, val_acc:0.619]
Epoch [4/120    avg_loss:1.416, val_acc:0.690]
Epoch [5/120    avg_loss:1.196, val_acc:0.729]
Epoch [6/120    avg_loss:1.043, val_acc:0.723]
Epoch [7/120    avg_loss:0.901, val_acc:0.790]
Epoch [8/120    avg_loss:0.780, val_acc:0.848]
Epoch [9/120    avg_loss:0.744, val_acc:0.808]
Epoch [10/120    avg_loss:0.630, val_acc:0.865]
Epoch [11/120    avg_loss:0.557, val_acc:0.885]
Epoch [12/120    avg_loss:0.549, val_acc:0.894]
Epoch [13/120    avg_loss:0.508, val_acc:0.894]
Epoch [14/120    avg_loss:0.452, val_acc:0.902]
Epoch [15/120    avg_loss:0.404, val_acc:0.912]
Epoch [16/120    avg_loss:0.419, val_acc:0.900]
Epoch [17/120    avg_loss:0.337, val_acc:0.925]
Epoch [18/120    avg_loss:0.328, val_acc:0.935]
Epoch [19/120    avg_loss:0.337, val_acc:0.900]
Epoch [20/120    avg_loss:0.345, val_acc:0.925]
Epoch [21/120    avg_loss:0.356, val_acc:0.925]
Epoch [22/120    avg_loss:0.265, val_acc:0.894]
Epoch [23/120    avg_loss:0.347, val_acc:0.950]
Epoch [24/120    avg_loss:0.249, val_acc:0.940]
Epoch [25/120    avg_loss:0.224, val_acc:0.946]
Epoch [26/120    avg_loss:0.236, val_acc:0.958]
Epoch [27/120    avg_loss:0.253, val_acc:0.931]
Epoch [28/120    avg_loss:0.270, val_acc:0.948]
Epoch [29/120    avg_loss:0.188, val_acc:0.950]
Epoch [30/120    avg_loss:0.181, val_acc:0.944]
Epoch [31/120    avg_loss:0.155, val_acc:0.969]
Epoch [32/120    avg_loss:0.235, val_acc:0.944]
Epoch [33/120    avg_loss:0.172, val_acc:0.946]
Epoch [34/120    avg_loss:0.221, val_acc:0.940]
Epoch [35/120    avg_loss:0.218, val_acc:0.942]
Epoch [36/120    avg_loss:0.179, val_acc:0.971]
Epoch [37/120    avg_loss:0.167, val_acc:0.954]
Epoch [38/120    avg_loss:0.127, val_acc:0.973]
Epoch [39/120    avg_loss:0.167, val_acc:0.952]
Epoch [40/120    avg_loss:0.150, val_acc:0.958]
Epoch [41/120    avg_loss:0.234, val_acc:0.900]
Epoch [42/120    avg_loss:0.309, val_acc:0.938]
Epoch [43/120    avg_loss:0.215, val_acc:0.931]
Epoch [44/120    avg_loss:0.239, val_acc:0.965]
Epoch [45/120    avg_loss:0.128, val_acc:0.965]
Epoch [46/120    avg_loss:0.134, val_acc:0.944]
Epoch [47/120    avg_loss:0.137, val_acc:0.967]
Epoch [48/120    avg_loss:0.094, val_acc:0.971]
Epoch [49/120    avg_loss:0.078, val_acc:0.973]
Epoch [50/120    avg_loss:0.114, val_acc:0.971]
Epoch [51/120    avg_loss:0.112, val_acc:0.971]
Epoch [52/120    avg_loss:0.103, val_acc:0.983]
Epoch [53/120    avg_loss:0.080, val_acc:0.985]
Epoch [54/120    avg_loss:0.079, val_acc:0.975]
Epoch [55/120    avg_loss:0.124, val_acc:0.967]
Epoch [56/120    avg_loss:0.065, val_acc:0.983]
Epoch [57/120    avg_loss:0.061, val_acc:0.985]
Epoch [58/120    avg_loss:0.070, val_acc:0.979]
Epoch [59/120    avg_loss:0.074, val_acc:0.973]
Epoch [60/120    avg_loss:0.073, val_acc:0.969]
Epoch [61/120    avg_loss:0.098, val_acc:0.973]
Epoch [62/120    avg_loss:0.082, val_acc:0.965]
Epoch [63/120    avg_loss:0.097, val_acc:0.971]
Epoch [64/120    avg_loss:0.071, val_acc:0.971]
Epoch [65/120    avg_loss:0.077, val_acc:0.985]
Epoch [66/120    avg_loss:0.089, val_acc:0.975]
Epoch [67/120    avg_loss:0.064, val_acc:0.979]
Epoch [68/120    avg_loss:0.079, val_acc:0.979]
Epoch [69/120    avg_loss:0.050, val_acc:0.983]
Epoch [70/120    avg_loss:0.057, val_acc:0.963]
Epoch [71/120    avg_loss:0.099, val_acc:0.967]
Epoch [72/120    avg_loss:0.103, val_acc:0.971]
Epoch [73/120    avg_loss:0.074, val_acc:0.988]
Epoch [74/120    avg_loss:0.066, val_acc:0.977]
Epoch [75/120    avg_loss:0.067, val_acc:0.981]
Epoch [76/120    avg_loss:0.048, val_acc:0.983]
Epoch [77/120    avg_loss:0.062, val_acc:0.990]
Epoch [78/120    avg_loss:0.058, val_acc:0.983]
Epoch [79/120    avg_loss:0.053, val_acc:0.981]
Epoch [80/120    avg_loss:0.058, val_acc:0.977]
Epoch [81/120    avg_loss:0.045, val_acc:0.983]
Epoch [82/120    avg_loss:0.058, val_acc:0.977]
Epoch [83/120    avg_loss:0.058, val_acc:0.988]
Epoch [84/120    avg_loss:0.045, val_acc:0.988]
Epoch [85/120    avg_loss:0.036, val_acc:0.988]
Epoch [86/120    avg_loss:0.053, val_acc:0.981]
Epoch [87/120    avg_loss:0.047, val_acc:0.983]
Epoch [88/120    avg_loss:0.065, val_acc:0.990]
Epoch [89/120    avg_loss:0.073, val_acc:0.983]
Epoch [90/120    avg_loss:0.058, val_acc:0.981]
Epoch [91/120    avg_loss:0.042, val_acc:0.979]
Epoch [92/120    avg_loss:0.048, val_acc:0.971]
Epoch [93/120    avg_loss:0.049, val_acc:0.981]
Epoch [94/120    avg_loss:0.053, val_acc:0.985]
Epoch [95/120    avg_loss:0.045, val_acc:0.979]
Epoch [96/120    avg_loss:0.031, val_acc:0.983]
Epoch [97/120    avg_loss:0.035, val_acc:0.994]
Epoch [98/120    avg_loss:0.031, val_acc:0.985]
Epoch [99/120    avg_loss:0.022, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.990]
Epoch [101/120    avg_loss:0.015, val_acc:0.990]
Epoch [102/120    avg_loss:0.034, val_acc:0.983]
Epoch [103/120    avg_loss:0.022, val_acc:0.979]
Epoch [104/120    avg_loss:0.080, val_acc:0.985]
Epoch [105/120    avg_loss:0.044, val_acc:0.971]
Epoch [106/120    avg_loss:0.065, val_acc:0.963]
Epoch [107/120    avg_loss:0.058, val_acc:0.979]
Epoch [108/120    avg_loss:0.055, val_acc:0.983]
Epoch [109/120    avg_loss:0.058, val_acc:0.992]
Epoch [110/120    avg_loss:0.025, val_acc:0.990]
Epoch [111/120    avg_loss:0.037, val_acc:0.990]
Epoch [112/120    avg_loss:0.026, val_acc:0.990]
Epoch [113/120    avg_loss:0.027, val_acc:0.992]
Epoch [114/120    avg_loss:0.016, val_acc:0.996]
Epoch [115/120    avg_loss:0.016, val_acc:0.994]
Epoch [116/120    avg_loss:0.020, val_acc:0.994]
Epoch [117/120    avg_loss:0.015, val_acc:0.994]
Epoch [118/120    avg_loss:0.018, val_acc:0.996]
Epoch [119/120    avg_loss:0.013, val_acc:0.996]
Epoch [120/120    avg_loss:0.029, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99853801 0.98206278 1.         0.94298246 0.90972222
 0.99516908 0.95555556 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9912167608073879
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ca3946828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.430, val_acc:0.417]
Epoch [2/120    avg_loss:2.034, val_acc:0.590]
Epoch [3/120    avg_loss:1.745, val_acc:0.646]
Epoch [4/120    avg_loss:1.477, val_acc:0.685]
Epoch [5/120    avg_loss:1.251, val_acc:0.696]
Epoch [6/120    avg_loss:1.063, val_acc:0.723]
Epoch [7/120    avg_loss:0.900, val_acc:0.800]
Epoch [8/120    avg_loss:0.851, val_acc:0.754]
Epoch [9/120    avg_loss:0.730, val_acc:0.906]
Epoch [10/120    avg_loss:0.596, val_acc:0.890]
Epoch [11/120    avg_loss:0.603, val_acc:0.890]
Epoch [12/120    avg_loss:0.519, val_acc:0.919]
Epoch [13/120    avg_loss:0.420, val_acc:0.927]
Epoch [14/120    avg_loss:0.465, val_acc:0.935]
Epoch [15/120    avg_loss:0.411, val_acc:0.933]
Epoch [16/120    avg_loss:0.396, val_acc:0.940]
Epoch [17/120    avg_loss:0.380, val_acc:0.929]
Epoch [18/120    avg_loss:0.340, val_acc:0.931]
Epoch [19/120    avg_loss:0.376, val_acc:0.933]
Epoch [20/120    avg_loss:0.308, val_acc:0.958]
Epoch [21/120    avg_loss:0.282, val_acc:0.950]
Epoch [22/120    avg_loss:0.287, val_acc:0.946]
Epoch [23/120    avg_loss:0.282, val_acc:0.954]
Epoch [24/120    avg_loss:0.281, val_acc:0.956]
Epoch [25/120    avg_loss:0.273, val_acc:0.944]
Epoch [26/120    avg_loss:0.274, val_acc:0.935]
Epoch [27/120    avg_loss:0.266, val_acc:0.950]
Epoch [28/120    avg_loss:0.254, val_acc:0.965]
Epoch [29/120    avg_loss:0.218, val_acc:0.971]
Epoch [30/120    avg_loss:0.245, val_acc:0.960]
Epoch [31/120    avg_loss:0.237, val_acc:0.958]
Epoch [32/120    avg_loss:0.210, val_acc:0.963]
Epoch [33/120    avg_loss:0.198, val_acc:0.973]
Epoch [34/120    avg_loss:0.144, val_acc:0.973]
Epoch [35/120    avg_loss:0.159, val_acc:0.969]
Epoch [36/120    avg_loss:0.162, val_acc:0.971]
Epoch [37/120    avg_loss:0.139, val_acc:0.990]
Epoch [38/120    avg_loss:0.124, val_acc:0.975]
Epoch [39/120    avg_loss:0.129, val_acc:0.971]
Epoch [40/120    avg_loss:0.152, val_acc:0.977]
Epoch [41/120    avg_loss:0.150, val_acc:0.981]
Epoch [42/120    avg_loss:0.134, val_acc:0.971]
Epoch [43/120    avg_loss:0.109, val_acc:0.988]
Epoch [44/120    avg_loss:0.097, val_acc:0.977]
Epoch [45/120    avg_loss:0.150, val_acc:0.992]
Epoch [46/120    avg_loss:0.107, val_acc:0.985]
Epoch [47/120    avg_loss:0.102, val_acc:0.983]
Epoch [48/120    avg_loss:0.103, val_acc:0.973]
Epoch [49/120    avg_loss:0.109, val_acc:0.990]
Epoch [50/120    avg_loss:0.070, val_acc:0.992]
Epoch [51/120    avg_loss:0.100, val_acc:0.988]
Epoch [52/120    avg_loss:0.079, val_acc:0.998]
Epoch [53/120    avg_loss:0.080, val_acc:0.990]
Epoch [54/120    avg_loss:0.091, val_acc:0.990]
Epoch [55/120    avg_loss:0.105, val_acc:0.973]
Epoch [56/120    avg_loss:0.111, val_acc:0.985]
Epoch [57/120    avg_loss:0.084, val_acc:0.988]
Epoch [58/120    avg_loss:0.078, val_acc:0.994]
Epoch [59/120    avg_loss:0.058, val_acc:0.990]
Epoch [60/120    avg_loss:0.069, val_acc:0.994]
Epoch [61/120    avg_loss:0.036, val_acc:0.992]
Epoch [62/120    avg_loss:0.087, val_acc:0.988]
Epoch [63/120    avg_loss:0.090, val_acc:0.994]
Epoch [64/120    avg_loss:0.057, val_acc:0.994]
Epoch [65/120    avg_loss:0.074, val_acc:1.000]
Epoch [66/120    avg_loss:0.045, val_acc:0.998]
Epoch [67/120    avg_loss:0.063, val_acc:0.996]
Epoch [68/120    avg_loss:0.062, val_acc:0.994]
Epoch [69/120    avg_loss:0.053, val_acc:0.992]
Epoch [70/120    avg_loss:0.079, val_acc:0.992]
Epoch [71/120    avg_loss:0.035, val_acc:1.000]
Epoch [72/120    avg_loss:0.032, val_acc:0.998]
Epoch [73/120    avg_loss:0.040, val_acc:0.990]
Epoch [74/120    avg_loss:0.036, val_acc:0.998]
Epoch [75/120    avg_loss:0.031, val_acc:0.992]
Epoch [76/120    avg_loss:0.055, val_acc:0.979]
Epoch [77/120    avg_loss:0.069, val_acc:0.990]
Epoch [78/120    avg_loss:0.045, val_acc:0.996]
Epoch [79/120    avg_loss:0.029, val_acc:0.992]
Epoch [80/120    avg_loss:0.053, val_acc:0.998]
Epoch [81/120    avg_loss:0.050, val_acc:0.998]
Epoch [82/120    avg_loss:0.050, val_acc:0.985]
Epoch [83/120    avg_loss:0.045, val_acc:0.985]
Epoch [84/120    avg_loss:0.028, val_acc:0.992]
Epoch [85/120    avg_loss:0.038, val_acc:0.996]
Epoch [86/120    avg_loss:0.019, val_acc:0.998]
Epoch [87/120    avg_loss:0.028, val_acc:0.998]
Epoch [88/120    avg_loss:0.023, val_acc:0.998]
Epoch [89/120    avg_loss:0.020, val_acc:0.998]
Epoch [90/120    avg_loss:0.017, val_acc:0.998]
Epoch [91/120    avg_loss:0.019, val_acc:0.998]
Epoch [92/120    avg_loss:0.016, val_acc:0.998]
Epoch [93/120    avg_loss:0.016, val_acc:1.000]
Epoch [94/120    avg_loss:0.015, val_acc:0.998]
Epoch [95/120    avg_loss:0.017, val_acc:1.000]
Epoch [96/120    avg_loss:0.015, val_acc:1.000]
Epoch [97/120    avg_loss:0.012, val_acc:0.998]
Epoch [98/120    avg_loss:0.016, val_acc:1.000]
Epoch [99/120    avg_loss:0.011, val_acc:1.000]
Epoch [100/120    avg_loss:0.014, val_acc:1.000]
Epoch [101/120    avg_loss:0.016, val_acc:1.000]
Epoch [102/120    avg_loss:0.013, val_acc:1.000]
Epoch [103/120    avg_loss:0.014, val_acc:1.000]
Epoch [104/120    avg_loss:0.018, val_acc:1.000]
Epoch [105/120    avg_loss:0.012, val_acc:1.000]
Epoch [106/120    avg_loss:0.012, val_acc:1.000]
Epoch [107/120    avg_loss:0.012, val_acc:1.000]
Epoch [108/120    avg_loss:0.018, val_acc:1.000]
Epoch [109/120    avg_loss:0.012, val_acc:1.000]
Epoch [110/120    avg_loss:0.014, val_acc:1.000]
Epoch [111/120    avg_loss:0.013, val_acc:1.000]
Epoch [112/120    avg_loss:0.013, val_acc:1.000]
Epoch [113/120    avg_loss:0.014, val_acc:1.000]
Epoch [114/120    avg_loss:0.013, val_acc:1.000]
Epoch [115/120    avg_loss:0.013, val_acc:1.000]
Epoch [116/120    avg_loss:0.013, val_acc:1.000]
Epoch [117/120    avg_loss:0.016, val_acc:1.000]
Epoch [118/120    avg_loss:0.012, val_acc:1.000]
Epoch [119/120    avg_loss:0.010, val_acc:1.000]
Epoch [120/120    avg_loss:0.013, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 1.         0.98871332 1.         0.9751693  0.96345515
 1.         0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9962019499727369
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f98898cd860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.373, val_acc:0.577]
Epoch [2/120    avg_loss:2.010, val_acc:0.619]
Epoch [3/120    avg_loss:1.740, val_acc:0.644]
Epoch [4/120    avg_loss:1.467, val_acc:0.667]
Epoch [5/120    avg_loss:1.235, val_acc:0.725]
Epoch [6/120    avg_loss:1.077, val_acc:0.723]
Epoch [7/120    avg_loss:0.946, val_acc:0.733]
Epoch [8/120    avg_loss:0.786, val_acc:0.752]
Epoch [9/120    avg_loss:0.763, val_acc:0.794]
Epoch [10/120    avg_loss:0.680, val_acc:0.850]
Epoch [11/120    avg_loss:0.639, val_acc:0.798]
Epoch [12/120    avg_loss:0.587, val_acc:0.883]
Epoch [13/120    avg_loss:0.508, val_acc:0.838]
Epoch [14/120    avg_loss:0.505, val_acc:0.894]
Epoch [15/120    avg_loss:0.492, val_acc:0.931]
Epoch [16/120    avg_loss:0.402, val_acc:0.881]
Epoch [17/120    avg_loss:0.421, val_acc:0.944]
Epoch [18/120    avg_loss:0.339, val_acc:0.933]
Epoch [19/120    avg_loss:0.301, val_acc:0.906]
Epoch [20/120    avg_loss:0.314, val_acc:0.950]
Epoch [21/120    avg_loss:0.311, val_acc:0.917]
Epoch [22/120    avg_loss:0.314, val_acc:0.923]
Epoch [23/120    avg_loss:0.302, val_acc:0.923]
Epoch [24/120    avg_loss:0.253, val_acc:0.944]
Epoch [25/120    avg_loss:0.239, val_acc:0.967]
Epoch [26/120    avg_loss:0.207, val_acc:0.942]
Epoch [27/120    avg_loss:0.262, val_acc:0.938]
Epoch [28/120    avg_loss:0.239, val_acc:0.946]
Epoch [29/120    avg_loss:0.208, val_acc:0.952]
Epoch [30/120    avg_loss:0.159, val_acc:0.946]
Epoch [31/120    avg_loss:0.201, val_acc:0.960]
Epoch [32/120    avg_loss:0.149, val_acc:0.958]
Epoch [33/120    avg_loss:0.154, val_acc:0.925]
Epoch [34/120    avg_loss:0.173, val_acc:0.971]
Epoch [35/120    avg_loss:0.164, val_acc:0.975]
Epoch [36/120    avg_loss:0.176, val_acc:0.977]
Epoch [37/120    avg_loss:0.132, val_acc:0.963]
Epoch [38/120    avg_loss:0.138, val_acc:0.967]
Epoch [39/120    avg_loss:0.125, val_acc:0.965]
Epoch [40/120    avg_loss:0.135, val_acc:0.963]
Epoch [41/120    avg_loss:0.120, val_acc:0.969]
Epoch [42/120    avg_loss:0.156, val_acc:0.940]
Epoch [43/120    avg_loss:0.150, val_acc:0.952]
Epoch [44/120    avg_loss:0.143, val_acc:0.958]
Epoch [45/120    avg_loss:0.115, val_acc:0.969]
Epoch [46/120    avg_loss:0.087, val_acc:0.963]
Epoch [47/120    avg_loss:0.074, val_acc:0.977]
Epoch [48/120    avg_loss:0.061, val_acc:0.975]
Epoch [49/120    avg_loss:0.076, val_acc:0.967]
Epoch [50/120    avg_loss:0.080, val_acc:0.979]
Epoch [51/120    avg_loss:0.091, val_acc:0.979]
Epoch [52/120    avg_loss:0.105, val_acc:0.967]
Epoch [53/120    avg_loss:0.088, val_acc:0.981]
Epoch [54/120    avg_loss:0.072, val_acc:0.973]
Epoch [55/120    avg_loss:0.070, val_acc:0.969]
Epoch [56/120    avg_loss:0.074, val_acc:0.969]
Epoch [57/120    avg_loss:0.092, val_acc:0.967]
Epoch [58/120    avg_loss:0.082, val_acc:0.977]
Epoch [59/120    avg_loss:0.055, val_acc:0.973]
Epoch [60/120    avg_loss:0.046, val_acc:0.981]
Epoch [61/120    avg_loss:0.046, val_acc:0.973]
Epoch [62/120    avg_loss:0.059, val_acc:0.979]
Epoch [63/120    avg_loss:0.047, val_acc:0.967]
Epoch [64/120    avg_loss:0.076, val_acc:0.975]
Epoch [65/120    avg_loss:0.037, val_acc:0.983]
Epoch [66/120    avg_loss:0.038, val_acc:0.983]
Epoch [67/120    avg_loss:0.034, val_acc:0.983]
Epoch [68/120    avg_loss:0.051, val_acc:0.981]
Epoch [69/120    avg_loss:0.046, val_acc:0.981]
Epoch [70/120    avg_loss:0.036, val_acc:0.981]
Epoch [71/120    avg_loss:0.030, val_acc:0.977]
Epoch [72/120    avg_loss:0.033, val_acc:0.990]
Epoch [73/120    avg_loss:0.027, val_acc:0.988]
Epoch [74/120    avg_loss:0.015, val_acc:0.985]
Epoch [75/120    avg_loss:0.043, val_acc:0.985]
Epoch [76/120    avg_loss:0.053, val_acc:0.975]
Epoch [77/120    avg_loss:0.058, val_acc:0.973]
Epoch [78/120    avg_loss:0.038, val_acc:0.983]
Epoch [79/120    avg_loss:0.047, val_acc:0.981]
Epoch [80/120    avg_loss:0.047, val_acc:0.977]
Epoch [81/120    avg_loss:0.066, val_acc:0.977]
Epoch [82/120    avg_loss:0.042, val_acc:0.985]
Epoch [83/120    avg_loss:0.053, val_acc:0.988]
Epoch [84/120    avg_loss:0.027, val_acc:0.985]
Epoch [85/120    avg_loss:0.017, val_acc:0.988]
Epoch [86/120    avg_loss:0.022, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.988]
Epoch [88/120    avg_loss:0.015, val_acc:0.988]
Epoch [89/120    avg_loss:0.020, val_acc:0.988]
Epoch [90/120    avg_loss:0.021, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.016, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.021, val_acc:0.988]
Epoch [95/120    avg_loss:0.019, val_acc:0.988]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.017, val_acc:0.988]
Epoch [98/120    avg_loss:0.022, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.988]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.018, val_acc:0.988]
Epoch [105/120    avg_loss:0.014, val_acc:0.988]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.019, val_acc:0.988]
Epoch [113/120    avg_loss:0.015, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.012, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.988]
Epoch [118/120    avg_loss:0.013, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.988]
Epoch [120/120    avg_loss:0.018, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99926954 0.99095023 1.         0.96536797 0.94326241
 0.99757869 0.97826087 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9947774089479617
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d92fc5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.480, val_acc:0.510]
Epoch [2/120    avg_loss:2.061, val_acc:0.579]
Epoch [3/120    avg_loss:1.758, val_acc:0.665]
Epoch [4/120    avg_loss:1.449, val_acc:0.673]
Epoch [5/120    avg_loss:1.235, val_acc:0.700]
Epoch [6/120    avg_loss:1.035, val_acc:0.740]
Epoch [7/120    avg_loss:0.915, val_acc:0.729]
Epoch [8/120    avg_loss:0.858, val_acc:0.767]
Epoch [9/120    avg_loss:0.720, val_acc:0.867]
Epoch [10/120    avg_loss:0.667, val_acc:0.892]
Epoch [11/120    avg_loss:0.547, val_acc:0.900]
Epoch [12/120    avg_loss:0.564, val_acc:0.881]
Epoch [13/120    avg_loss:0.474, val_acc:0.875]
Epoch [14/120    avg_loss:0.496, val_acc:0.912]
Epoch [15/120    avg_loss:0.464, val_acc:0.896]
Epoch [16/120    avg_loss:0.395, val_acc:0.925]
Epoch [17/120    avg_loss:0.448, val_acc:0.887]
Epoch [18/120    avg_loss:0.460, val_acc:0.946]
Epoch [19/120    avg_loss:0.392, val_acc:0.944]
Epoch [20/120    avg_loss:0.366, val_acc:0.931]
Epoch [21/120    avg_loss:0.322, val_acc:0.952]
Epoch [22/120    avg_loss:0.310, val_acc:0.948]
Epoch [23/120    avg_loss:0.344, val_acc:0.948]
Epoch [24/120    avg_loss:0.268, val_acc:0.948]
Epoch [25/120    avg_loss:0.274, val_acc:0.946]
Epoch [26/120    avg_loss:0.281, val_acc:0.944]
Epoch [27/120    avg_loss:0.231, val_acc:0.963]
Epoch [28/120    avg_loss:0.266, val_acc:0.942]
Epoch [29/120    avg_loss:0.275, val_acc:0.956]
Epoch [30/120    avg_loss:0.223, val_acc:0.971]
Epoch [31/120    avg_loss:0.286, val_acc:0.969]
Epoch [32/120    avg_loss:0.222, val_acc:0.969]
Epoch [33/120    avg_loss:0.190, val_acc:0.969]
Epoch [34/120    avg_loss:0.208, val_acc:0.981]
Epoch [35/120    avg_loss:0.225, val_acc:0.973]
Epoch [36/120    avg_loss:0.179, val_acc:0.979]
Epoch [37/120    avg_loss:0.186, val_acc:0.960]
Epoch [38/120    avg_loss:0.183, val_acc:0.981]
Epoch [39/120    avg_loss:0.210, val_acc:0.977]
Epoch [40/120    avg_loss:0.159, val_acc:0.981]
Epoch [41/120    avg_loss:0.161, val_acc:0.983]
Epoch [42/120    avg_loss:0.143, val_acc:0.979]
Epoch [43/120    avg_loss:0.156, val_acc:0.965]
Epoch [44/120    avg_loss:0.212, val_acc:0.971]
Epoch [45/120    avg_loss:0.204, val_acc:0.971]
Epoch [46/120    avg_loss:0.181, val_acc:0.973]
Epoch [47/120    avg_loss:0.133, val_acc:0.983]
Epoch [48/120    avg_loss:0.128, val_acc:0.981]
Epoch [49/120    avg_loss:0.136, val_acc:0.983]
Epoch [50/120    avg_loss:0.118, val_acc:0.981]
Epoch [51/120    avg_loss:0.091, val_acc:0.992]
Epoch [52/120    avg_loss:0.072, val_acc:0.992]
Epoch [53/120    avg_loss:0.078, val_acc:0.994]
Epoch [54/120    avg_loss:0.117, val_acc:0.906]
Epoch [55/120    avg_loss:0.151, val_acc:0.983]
Epoch [56/120    avg_loss:0.103, val_acc:0.983]
Epoch [57/120    avg_loss:0.159, val_acc:0.990]
Epoch [58/120    avg_loss:0.074, val_acc:0.990]
Epoch [59/120    avg_loss:0.085, val_acc:0.992]
Epoch [60/120    avg_loss:0.065, val_acc:0.994]
Epoch [61/120    avg_loss:0.079, val_acc:0.983]
Epoch [62/120    avg_loss:0.085, val_acc:0.994]
Epoch [63/120    avg_loss:0.072, val_acc:0.998]
Epoch [64/120    avg_loss:0.072, val_acc:0.994]
Epoch [65/120    avg_loss:0.080, val_acc:0.990]
Epoch [66/120    avg_loss:0.064, val_acc:0.988]
Epoch [67/120    avg_loss:0.047, val_acc:1.000]
Epoch [68/120    avg_loss:0.053, val_acc:0.994]
Epoch [69/120    avg_loss:0.060, val_acc:0.994]
Epoch [70/120    avg_loss:0.042, val_acc:0.996]
Epoch [71/120    avg_loss:0.054, val_acc:0.994]
Epoch [72/120    avg_loss:0.034, val_acc:0.994]
Epoch [73/120    avg_loss:0.048, val_acc:0.996]
Epoch [74/120    avg_loss:0.055, val_acc:0.996]
Epoch [75/120    avg_loss:0.029, val_acc:0.998]
Epoch [76/120    avg_loss:0.030, val_acc:1.000]
Epoch [77/120    avg_loss:0.026, val_acc:0.992]
Epoch [78/120    avg_loss:0.039, val_acc:0.998]
Epoch [79/120    avg_loss:0.038, val_acc:0.998]
Epoch [80/120    avg_loss:0.043, val_acc:0.973]
Epoch [81/120    avg_loss:0.056, val_acc:0.994]
Epoch [82/120    avg_loss:0.039, val_acc:0.996]
Epoch [83/120    avg_loss:0.019, val_acc:1.000]
Epoch [84/120    avg_loss:0.028, val_acc:1.000]
Epoch [85/120    avg_loss:0.032, val_acc:0.994]
Epoch [86/120    avg_loss:0.024, val_acc:0.998]
Epoch [87/120    avg_loss:0.033, val_acc:0.994]
Epoch [88/120    avg_loss:0.061, val_acc:0.992]
Epoch [89/120    avg_loss:0.027, val_acc:0.996]
Epoch [90/120    avg_loss:0.048, val_acc:0.992]
Epoch [91/120    avg_loss:0.100, val_acc:0.988]
Epoch [92/120    avg_loss:0.122, val_acc:0.994]
Epoch [93/120    avg_loss:0.052, val_acc:0.998]
Epoch [94/120    avg_loss:0.037, val_acc:0.996]
Epoch [95/120    avg_loss:0.037, val_acc:1.000]
Epoch [96/120    avg_loss:0.043, val_acc:0.998]
Epoch [97/120    avg_loss:0.038, val_acc:0.998]
Epoch [98/120    avg_loss:0.029, val_acc:0.998]
Epoch [99/120    avg_loss:0.045, val_acc:0.998]
Epoch [100/120    avg_loss:0.021, val_acc:1.000]
Epoch [101/120    avg_loss:0.019, val_acc:0.998]
Epoch [102/120    avg_loss:0.023, val_acc:1.000]
Epoch [103/120    avg_loss:0.032, val_acc:1.000]
Epoch [104/120    avg_loss:0.026, val_acc:0.998]
Epoch [105/120    avg_loss:0.033, val_acc:0.994]
Epoch [106/120    avg_loss:0.028, val_acc:0.994]
Epoch [107/120    avg_loss:0.027, val_acc:0.998]
Epoch [108/120    avg_loss:0.016, val_acc:1.000]
Epoch [109/120    avg_loss:0.014, val_acc:1.000]
Epoch [110/120    avg_loss:0.010, val_acc:1.000]
Epoch [111/120    avg_loss:0.014, val_acc:0.998]
Epoch [112/120    avg_loss:0.015, val_acc:1.000]
Epoch [113/120    avg_loss:0.015, val_acc:1.000]
Epoch [114/120    avg_loss:0.010, val_acc:1.000]
Epoch [115/120    avg_loss:0.022, val_acc:0.992]
Epoch [116/120    avg_loss:0.049, val_acc:0.990]
Epoch [117/120    avg_loss:0.039, val_acc:1.000]
Epoch [118/120    avg_loss:0.020, val_acc:0.998]
Epoch [119/120    avg_loss:0.041, val_acc:0.998]
Epoch [120/120    avg_loss:0.035, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         0.99545455 1.         0.96875    0.9527027
 1.         0.98924731 1.         1.         1.         0.99734748
 0.99779249 1.        ]

Kappa:
0.9957271861501309
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a8b8b4828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.467, val_acc:0.371]
Epoch [2/120    avg_loss:2.050, val_acc:0.613]
Epoch [3/120    avg_loss:1.782, val_acc:0.635]
Epoch [4/120    avg_loss:1.419, val_acc:0.713]
Epoch [5/120    avg_loss:1.186, val_acc:0.698]
Epoch [6/120    avg_loss:1.060, val_acc:0.750]
Epoch [7/120    avg_loss:0.920, val_acc:0.740]
Epoch [8/120    avg_loss:0.785, val_acc:0.750]
Epoch [9/120    avg_loss:0.733, val_acc:0.775]
Epoch [10/120    avg_loss:0.716, val_acc:0.804]
Epoch [11/120    avg_loss:0.641, val_acc:0.865]
Epoch [12/120    avg_loss:0.573, val_acc:0.883]
Epoch [13/120    avg_loss:0.547, val_acc:0.885]
Epoch [14/120    avg_loss:0.498, val_acc:0.900]
Epoch [15/120    avg_loss:0.450, val_acc:0.900]
Epoch [16/120    avg_loss:0.398, val_acc:0.927]
Epoch [17/120    avg_loss:0.403, val_acc:0.910]
Epoch [18/120    avg_loss:0.333, val_acc:0.942]
Epoch [19/120    avg_loss:0.340, val_acc:0.915]
Epoch [20/120    avg_loss:0.356, val_acc:0.931]
Epoch [21/120    avg_loss:0.310, val_acc:0.933]
Epoch [22/120    avg_loss:0.275, val_acc:0.921]
Epoch [23/120    avg_loss:0.354, val_acc:0.900]
Epoch [24/120    avg_loss:0.303, val_acc:0.942]
Epoch [25/120    avg_loss:0.268, val_acc:0.944]
Epoch [26/120    avg_loss:0.224, val_acc:0.952]
Epoch [27/120    avg_loss:0.204, val_acc:0.944]
Epoch [28/120    avg_loss:0.216, val_acc:0.960]
Epoch [29/120    avg_loss:0.202, val_acc:0.960]
Epoch [30/120    avg_loss:0.197, val_acc:0.958]
Epoch [31/120    avg_loss:0.256, val_acc:0.967]
Epoch [32/120    avg_loss:0.211, val_acc:0.958]
Epoch [33/120    avg_loss:0.257, val_acc:0.919]
Epoch [34/120    avg_loss:0.273, val_acc:0.967]
Epoch [35/120    avg_loss:0.168, val_acc:0.958]
Epoch [36/120    avg_loss:0.156, val_acc:0.950]
Epoch [37/120    avg_loss:0.179, val_acc:0.969]
Epoch [38/120    avg_loss:0.163, val_acc:0.963]
Epoch [39/120    avg_loss:0.116, val_acc:0.969]
Epoch [40/120    avg_loss:0.153, val_acc:0.967]
Epoch [41/120    avg_loss:0.152, val_acc:0.954]
Epoch [42/120    avg_loss:0.115, val_acc:0.967]
Epoch [43/120    avg_loss:0.158, val_acc:0.979]
Epoch [44/120    avg_loss:0.112, val_acc:0.975]
Epoch [45/120    avg_loss:0.130, val_acc:0.971]
Epoch [46/120    avg_loss:0.118, val_acc:0.975]
Epoch [47/120    avg_loss:0.197, val_acc:0.933]
Epoch [48/120    avg_loss:0.154, val_acc:0.965]
Epoch [49/120    avg_loss:0.129, val_acc:0.956]
Epoch [50/120    avg_loss:0.095, val_acc:0.973]
Epoch [51/120    avg_loss:0.079, val_acc:0.975]
Epoch [52/120    avg_loss:0.093, val_acc:0.969]
Epoch [53/120    avg_loss:0.084, val_acc:0.971]
Epoch [54/120    avg_loss:0.077, val_acc:0.979]
Epoch [55/120    avg_loss:0.083, val_acc:0.985]
Epoch [56/120    avg_loss:0.081, val_acc:0.979]
Epoch [57/120    avg_loss:0.103, val_acc:0.973]
Epoch [58/120    avg_loss:0.074, val_acc:0.985]
Epoch [59/120    avg_loss:0.083, val_acc:0.985]
Epoch [60/120    avg_loss:0.069, val_acc:0.981]
Epoch [61/120    avg_loss:0.055, val_acc:0.985]
Epoch [62/120    avg_loss:0.057, val_acc:0.975]
Epoch [63/120    avg_loss:0.065, val_acc:0.977]
Epoch [64/120    avg_loss:0.077, val_acc:0.983]
Epoch [65/120    avg_loss:0.063, val_acc:0.983]
Epoch [66/120    avg_loss:0.046, val_acc:0.988]
Epoch [67/120    avg_loss:0.095, val_acc:0.988]
Epoch [68/120    avg_loss:0.109, val_acc:0.985]
Epoch [69/120    avg_loss:0.099, val_acc:0.985]
Epoch [70/120    avg_loss:0.057, val_acc:0.981]
Epoch [71/120    avg_loss:0.055, val_acc:0.988]
Epoch [72/120    avg_loss:0.048, val_acc:0.988]
Epoch [73/120    avg_loss:0.039, val_acc:0.990]
Epoch [74/120    avg_loss:0.044, val_acc:0.981]
Epoch [75/120    avg_loss:0.029, val_acc:0.985]
Epoch [76/120    avg_loss:0.037, val_acc:0.977]
Epoch [77/120    avg_loss:0.124, val_acc:0.963]
Epoch [78/120    avg_loss:0.149, val_acc:0.971]
Epoch [79/120    avg_loss:0.133, val_acc:0.983]
Epoch [80/120    avg_loss:0.062, val_acc:0.983]
Epoch [81/120    avg_loss:0.061, val_acc:0.990]
Epoch [82/120    avg_loss:0.043, val_acc:0.988]
Epoch [83/120    avg_loss:0.045, val_acc:0.990]
Epoch [84/120    avg_loss:0.044, val_acc:0.988]
Epoch [85/120    avg_loss:0.028, val_acc:0.988]
Epoch [86/120    avg_loss:0.030, val_acc:0.985]
Epoch [87/120    avg_loss:0.026, val_acc:0.988]
Epoch [88/120    avg_loss:0.030, val_acc:0.983]
Epoch [89/120    avg_loss:0.059, val_acc:0.977]
Epoch [90/120    avg_loss:0.032, val_acc:0.992]
Epoch [91/120    avg_loss:0.023, val_acc:0.990]
Epoch [92/120    avg_loss:0.018, val_acc:0.992]
Epoch [93/120    avg_loss:0.026, val_acc:0.990]
Epoch [94/120    avg_loss:0.017, val_acc:0.990]
Epoch [95/120    avg_loss:0.020, val_acc:0.992]
Epoch [96/120    avg_loss:0.016, val_acc:0.988]
Epoch [97/120    avg_loss:0.018, val_acc:0.994]
Epoch [98/120    avg_loss:0.021, val_acc:0.994]
Epoch [99/120    avg_loss:0.031, val_acc:0.994]
Epoch [100/120    avg_loss:0.017, val_acc:0.992]
Epoch [101/120    avg_loss:0.016, val_acc:0.992]
Epoch [102/120    avg_loss:0.013, val_acc:0.994]
Epoch [103/120    avg_loss:0.012, val_acc:0.996]
Epoch [104/120    avg_loss:0.015, val_acc:0.990]
Epoch [105/120    avg_loss:0.013, val_acc:0.996]
Epoch [106/120    avg_loss:0.012, val_acc:0.994]
Epoch [107/120    avg_loss:0.026, val_acc:0.990]
Epoch [108/120    avg_loss:0.019, val_acc:0.992]
Epoch [109/120    avg_loss:0.012, val_acc:0.996]
Epoch [110/120    avg_loss:0.010, val_acc:0.994]
Epoch [111/120    avg_loss:0.009, val_acc:0.994]
Epoch [112/120    avg_loss:0.013, val_acc:0.988]
Epoch [113/120    avg_loss:0.016, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.014, val_acc:0.996]
Epoch [116/120    avg_loss:0.015, val_acc:0.992]
Epoch [117/120    avg_loss:0.015, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.996]
Epoch [119/120    avg_loss:0.013, val_acc:0.994]
Epoch [120/120    avg_loss:0.013, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.98871332 0.97777778 0.95384615 0.9632107
 1.         0.9726776  1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9933534185076318
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f25d7c5b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.447, val_acc:0.523]
Epoch [2/120    avg_loss:2.055, val_acc:0.575]
Epoch [3/120    avg_loss:1.779, val_acc:0.671]
Epoch [4/120    avg_loss:1.492, val_acc:0.713]
Epoch [5/120    avg_loss:1.248, val_acc:0.738]
Epoch [6/120    avg_loss:1.056, val_acc:0.781]
Epoch [7/120    avg_loss:0.965, val_acc:0.852]
Epoch [8/120    avg_loss:0.815, val_acc:0.810]
Epoch [9/120    avg_loss:0.723, val_acc:0.887]
Epoch [10/120    avg_loss:0.663, val_acc:0.885]
Epoch [11/120    avg_loss:0.607, val_acc:0.910]
Epoch [12/120    avg_loss:0.521, val_acc:0.919]
Epoch [13/120    avg_loss:0.474, val_acc:0.910]
Epoch [14/120    avg_loss:0.443, val_acc:0.906]
Epoch [15/120    avg_loss:0.436, val_acc:0.887]
Epoch [16/120    avg_loss:0.445, val_acc:0.933]
Epoch [17/120    avg_loss:0.397, val_acc:0.898]
Epoch [18/120    avg_loss:0.389, val_acc:0.929]
Epoch [19/120    avg_loss:0.379, val_acc:0.885]
Epoch [20/120    avg_loss:0.320, val_acc:0.965]
Epoch [21/120    avg_loss:0.278, val_acc:0.971]
Epoch [22/120    avg_loss:0.257, val_acc:0.963]
Epoch [23/120    avg_loss:0.256, val_acc:0.950]
Epoch [24/120    avg_loss:0.287, val_acc:0.950]
Epoch [25/120    avg_loss:0.263, val_acc:0.973]
Epoch [26/120    avg_loss:0.205, val_acc:0.933]
Epoch [27/120    avg_loss:0.181, val_acc:0.929]
Epoch [28/120    avg_loss:0.232, val_acc:0.971]
Epoch [29/120    avg_loss:0.209, val_acc:0.975]
Epoch [30/120    avg_loss:0.189, val_acc:0.967]
Epoch [31/120    avg_loss:0.180, val_acc:0.977]
Epoch [32/120    avg_loss:0.181, val_acc:0.963]
Epoch [33/120    avg_loss:0.235, val_acc:0.983]
Epoch [34/120    avg_loss:0.146, val_acc:0.981]
Epoch [35/120    avg_loss:0.170, val_acc:0.983]
Epoch [36/120    avg_loss:0.169, val_acc:0.973]
Epoch [37/120    avg_loss:0.154, val_acc:0.969]
Epoch [38/120    avg_loss:0.148, val_acc:0.977]
Epoch [39/120    avg_loss:0.141, val_acc:0.981]
Epoch [40/120    avg_loss:0.154, val_acc:0.983]
Epoch [41/120    avg_loss:0.152, val_acc:0.977]
Epoch [42/120    avg_loss:0.195, val_acc:0.960]
Epoch [43/120    avg_loss:0.174, val_acc:0.985]
Epoch [44/120    avg_loss:0.156, val_acc:0.973]
Epoch [45/120    avg_loss:0.183, val_acc:0.929]
Epoch [46/120    avg_loss:0.125, val_acc:0.990]
Epoch [47/120    avg_loss:0.086, val_acc:0.985]
Epoch [48/120    avg_loss:0.113, val_acc:0.975]
Epoch [49/120    avg_loss:0.112, val_acc:0.977]
Epoch [50/120    avg_loss:0.113, val_acc:0.988]
Epoch [51/120    avg_loss:0.086, val_acc:0.992]
Epoch [52/120    avg_loss:0.077, val_acc:0.992]
Epoch [53/120    avg_loss:0.067, val_acc:0.988]
Epoch [54/120    avg_loss:0.067, val_acc:0.988]
Epoch [55/120    avg_loss:0.049, val_acc:0.992]
Epoch [56/120    avg_loss:0.048, val_acc:0.992]
Epoch [57/120    avg_loss:0.045, val_acc:0.992]
Epoch [58/120    avg_loss:0.056, val_acc:0.990]
Epoch [59/120    avg_loss:0.056, val_acc:0.988]
Epoch [60/120    avg_loss:0.060, val_acc:0.992]
Epoch [61/120    avg_loss:0.036, val_acc:0.990]
Epoch [62/120    avg_loss:0.044, val_acc:0.996]
Epoch [63/120    avg_loss:0.057, val_acc:0.950]
Epoch [64/120    avg_loss:0.098, val_acc:0.983]
Epoch [65/120    avg_loss:0.083, val_acc:0.990]
Epoch [66/120    avg_loss:0.085, val_acc:0.960]
Epoch [67/120    avg_loss:0.064, val_acc:0.988]
Epoch [68/120    avg_loss:0.039, val_acc:0.985]
Epoch [69/120    avg_loss:0.056, val_acc:0.985]
Epoch [70/120    avg_loss:0.056, val_acc:0.988]
Epoch [71/120    avg_loss:0.043, val_acc:0.994]
Epoch [72/120    avg_loss:0.029, val_acc:0.992]
Epoch [73/120    avg_loss:0.049, val_acc:0.988]
Epoch [74/120    avg_loss:0.068, val_acc:0.981]
Epoch [75/120    avg_loss:0.053, val_acc:0.988]
Epoch [76/120    avg_loss:0.043, val_acc:0.990]
Epoch [77/120    avg_loss:0.038, val_acc:0.994]
Epoch [78/120    avg_loss:0.032, val_acc:0.998]
Epoch [79/120    avg_loss:0.029, val_acc:0.998]
Epoch [80/120    avg_loss:0.028, val_acc:0.996]
Epoch [81/120    avg_loss:0.024, val_acc:0.996]
Epoch [82/120    avg_loss:0.029, val_acc:0.996]
Epoch [83/120    avg_loss:0.024, val_acc:0.996]
Epoch [84/120    avg_loss:0.027, val_acc:0.996]
Epoch [85/120    avg_loss:0.025, val_acc:0.996]
Epoch [86/120    avg_loss:0.026, val_acc:0.996]
Epoch [87/120    avg_loss:0.020, val_acc:0.996]
Epoch [88/120    avg_loss:0.026, val_acc:0.996]
Epoch [89/120    avg_loss:0.026, val_acc:0.998]
Epoch [90/120    avg_loss:0.025, val_acc:0.998]
Epoch [91/120    avg_loss:0.023, val_acc:0.998]
Epoch [92/120    avg_loss:0.024, val_acc:0.996]
Epoch [93/120    avg_loss:0.026, val_acc:0.996]
Epoch [94/120    avg_loss:0.026, val_acc:0.996]
Epoch [95/120    avg_loss:0.027, val_acc:0.996]
Epoch [96/120    avg_loss:0.040, val_acc:0.996]
Epoch [97/120    avg_loss:0.027, val_acc:0.996]
Epoch [98/120    avg_loss:0.020, val_acc:0.996]
Epoch [99/120    avg_loss:0.026, val_acc:0.996]
Epoch [100/120    avg_loss:0.024, val_acc:0.996]
Epoch [101/120    avg_loss:0.020, val_acc:0.996]
Epoch [102/120    avg_loss:0.020, val_acc:0.994]
Epoch [103/120    avg_loss:0.018, val_acc:0.996]
Epoch [104/120    avg_loss:0.022, val_acc:0.996]
Epoch [105/120    avg_loss:0.026, val_acc:0.996]
Epoch [106/120    avg_loss:0.024, val_acc:0.998]
Epoch [107/120    avg_loss:0.022, val_acc:0.998]
Epoch [108/120    avg_loss:0.022, val_acc:0.998]
Epoch [109/120    avg_loss:0.025, val_acc:0.998]
Epoch [110/120    avg_loss:0.018, val_acc:0.998]
Epoch [111/120    avg_loss:0.023, val_acc:0.998]
Epoch [112/120    avg_loss:0.021, val_acc:0.998]
Epoch [113/120    avg_loss:0.019, val_acc:0.998]
Epoch [114/120    avg_loss:0.016, val_acc:0.998]
Epoch [115/120    avg_loss:0.018, val_acc:0.998]
Epoch [116/120    avg_loss:0.017, val_acc:0.998]
Epoch [117/120    avg_loss:0.016, val_acc:0.998]
Epoch [118/120    avg_loss:0.022, val_acc:0.998]
Epoch [119/120    avg_loss:0.021, val_acc:0.998]
Epoch [120/120    avg_loss:0.025, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 1.         0.99545455 1.         0.96888889 0.95238095
 1.         0.98924731 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9959645156770949
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5be6faa7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.406, val_acc:0.371]
Epoch [2/120    avg_loss:1.984, val_acc:0.613]
Epoch [3/120    avg_loss:1.680, val_acc:0.637]
Epoch [4/120    avg_loss:1.451, val_acc:0.717]
Epoch [5/120    avg_loss:1.286, val_acc:0.715]
Epoch [6/120    avg_loss:1.124, val_acc:0.790]
Epoch [7/120    avg_loss:0.991, val_acc:0.821]
Epoch [8/120    avg_loss:0.867, val_acc:0.754]
Epoch [9/120    avg_loss:0.736, val_acc:0.798]
Epoch [10/120    avg_loss:0.698, val_acc:0.846]
Epoch [11/120    avg_loss:0.632, val_acc:0.877]
Epoch [12/120    avg_loss:0.572, val_acc:0.827]
Epoch [13/120    avg_loss:0.603, val_acc:0.892]
Epoch [14/120    avg_loss:0.537, val_acc:0.917]
Epoch [15/120    avg_loss:0.515, val_acc:0.925]
Epoch [16/120    avg_loss:0.412, val_acc:0.875]
Epoch [17/120    avg_loss:0.482, val_acc:0.946]
Epoch [18/120    avg_loss:0.354, val_acc:0.908]
Epoch [19/120    avg_loss:0.415, val_acc:0.869]
Epoch [20/120    avg_loss:0.355, val_acc:0.887]
Epoch [21/120    avg_loss:0.372, val_acc:0.917]
Epoch [22/120    avg_loss:0.375, val_acc:0.954]
Epoch [23/120    avg_loss:0.310, val_acc:0.954]
Epoch [24/120    avg_loss:0.303, val_acc:0.973]
Epoch [25/120    avg_loss:0.332, val_acc:0.944]
Epoch [26/120    avg_loss:0.265, val_acc:0.933]
Epoch [27/120    avg_loss:0.270, val_acc:0.946]
Epoch [28/120    avg_loss:0.237, val_acc:0.965]
Epoch [29/120    avg_loss:0.237, val_acc:0.958]
Epoch [30/120    avg_loss:0.194, val_acc:0.965]
Epoch [31/120    avg_loss:0.189, val_acc:0.938]
Epoch [32/120    avg_loss:0.231, val_acc:0.925]
Epoch [33/120    avg_loss:0.223, val_acc:0.969]
Epoch [34/120    avg_loss:0.151, val_acc:0.971]
Epoch [35/120    avg_loss:0.138, val_acc:0.971]
Epoch [36/120    avg_loss:0.145, val_acc:0.969]
Epoch [37/120    avg_loss:0.134, val_acc:0.954]
Epoch [38/120    avg_loss:0.168, val_acc:0.975]
Epoch [39/120    avg_loss:0.097, val_acc:0.983]
Epoch [40/120    avg_loss:0.102, val_acc:0.985]
Epoch [41/120    avg_loss:0.102, val_acc:0.983]
Epoch [42/120    avg_loss:0.089, val_acc:0.990]
Epoch [43/120    avg_loss:0.092, val_acc:0.983]
Epoch [44/120    avg_loss:0.079, val_acc:0.985]
Epoch [45/120    avg_loss:0.090, val_acc:0.990]
Epoch [46/120    avg_loss:0.069, val_acc:0.985]
Epoch [47/120    avg_loss:0.099, val_acc:0.985]
Epoch [48/120    avg_loss:0.082, val_acc:0.992]
Epoch [49/120    avg_loss:0.078, val_acc:0.990]
Epoch [50/120    avg_loss:0.088, val_acc:0.985]
Epoch [51/120    avg_loss:0.083, val_acc:0.985]
Epoch [52/120    avg_loss:0.096, val_acc:0.985]
Epoch [53/120    avg_loss:0.079, val_acc:0.992]
Epoch [54/120    avg_loss:0.084, val_acc:0.992]
Epoch [55/120    avg_loss:0.087, val_acc:0.990]
Epoch [56/120    avg_loss:0.078, val_acc:0.992]
Epoch [57/120    avg_loss:0.087, val_acc:0.988]
Epoch [58/120    avg_loss:0.097, val_acc:0.985]
Epoch [59/120    avg_loss:0.069, val_acc:0.994]
Epoch [60/120    avg_loss:0.073, val_acc:0.994]
Epoch [61/120    avg_loss:0.069, val_acc:0.992]
Epoch [62/120    avg_loss:0.071, val_acc:0.990]
Epoch [63/120    avg_loss:0.080, val_acc:0.988]
Epoch [64/120    avg_loss:0.080, val_acc:0.990]
Epoch [65/120    avg_loss:0.069, val_acc:0.988]
Epoch [66/120    avg_loss:0.065, val_acc:0.988]
Epoch [67/120    avg_loss:0.077, val_acc:0.992]
Epoch [68/120    avg_loss:0.078, val_acc:0.994]
Epoch [69/120    avg_loss:0.073, val_acc:0.988]
Epoch [70/120    avg_loss:0.059, val_acc:0.988]
Epoch [71/120    avg_loss:0.064, val_acc:0.988]
Epoch [72/120    avg_loss:0.058, val_acc:0.990]
Epoch [73/120    avg_loss:0.079, val_acc:0.985]
Epoch [74/120    avg_loss:0.066, val_acc:0.985]
Epoch [75/120    avg_loss:0.063, val_acc:0.992]
Epoch [76/120    avg_loss:0.064, val_acc:0.990]
Epoch [77/120    avg_loss:0.073, val_acc:0.990]
Epoch [78/120    avg_loss:0.065, val_acc:0.990]
Epoch [79/120    avg_loss:0.064, val_acc:0.990]
Epoch [80/120    avg_loss:0.065, val_acc:0.996]
Epoch [81/120    avg_loss:0.075, val_acc:0.990]
Epoch [82/120    avg_loss:0.059, val_acc:0.985]
Epoch [83/120    avg_loss:0.056, val_acc:0.985]
Epoch [84/120    avg_loss:0.069, val_acc:0.992]
Epoch [85/120    avg_loss:0.064, val_acc:0.992]
Epoch [86/120    avg_loss:0.057, val_acc:0.988]
Epoch [87/120    avg_loss:0.062, val_acc:0.985]
Epoch [88/120    avg_loss:0.060, val_acc:0.990]
Epoch [89/120    avg_loss:0.057, val_acc:0.990]
Epoch [90/120    avg_loss:0.060, val_acc:0.988]
Epoch [91/120    avg_loss:0.054, val_acc:0.985]
Epoch [92/120    avg_loss:0.064, val_acc:0.985]
Epoch [93/120    avg_loss:0.056, val_acc:0.988]
Epoch [94/120    avg_loss:0.056, val_acc:0.988]
Epoch [95/120    avg_loss:0.054, val_acc:0.988]
Epoch [96/120    avg_loss:0.063, val_acc:0.988]
Epoch [97/120    avg_loss:0.062, val_acc:0.988]
Epoch [98/120    avg_loss:0.075, val_acc:0.988]
Epoch [99/120    avg_loss:0.060, val_acc:0.988]
Epoch [100/120    avg_loss:0.059, val_acc:0.988]
Epoch [101/120    avg_loss:0.060, val_acc:0.988]
Epoch [102/120    avg_loss:0.051, val_acc:0.988]
Epoch [103/120    avg_loss:0.050, val_acc:0.988]
Epoch [104/120    avg_loss:0.063, val_acc:0.988]
Epoch [105/120    avg_loss:0.060, val_acc:0.988]
Epoch [106/120    avg_loss:0.052, val_acc:0.988]
Epoch [107/120    avg_loss:0.069, val_acc:0.988]
Epoch [108/120    avg_loss:0.045, val_acc:0.988]
Epoch [109/120    avg_loss:0.049, val_acc:0.988]
Epoch [110/120    avg_loss:0.048, val_acc:0.988]
Epoch [111/120    avg_loss:0.053, val_acc:0.988]
Epoch [112/120    avg_loss:0.051, val_acc:0.988]
Epoch [113/120    avg_loss:0.047, val_acc:0.988]
Epoch [114/120    avg_loss:0.049, val_acc:0.988]
Epoch [115/120    avg_loss:0.059, val_acc:0.988]
Epoch [116/120    avg_loss:0.062, val_acc:0.988]
Epoch [117/120    avg_loss:0.044, val_acc:0.988]
Epoch [118/120    avg_loss:0.062, val_acc:0.988]
Epoch [119/120    avg_loss:0.049, val_acc:0.988]
Epoch [120/120    avg_loss:0.053, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 0.99780541 0.98426966 1.         0.92341357 0.87804878
 0.99277108 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.989318005383867
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6144efa7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.496, val_acc:0.394]
Epoch [2/120    avg_loss:2.065, val_acc:0.623]
Epoch [3/120    avg_loss:1.785, val_acc:0.681]
Epoch [4/120    avg_loss:1.481, val_acc:0.706]
Epoch [5/120    avg_loss:1.199, val_acc:0.758]
Epoch [6/120    avg_loss:0.999, val_acc:0.792]
Epoch [7/120    avg_loss:0.854, val_acc:0.790]
Epoch [8/120    avg_loss:0.749, val_acc:0.810]
Epoch [9/120    avg_loss:0.708, val_acc:0.823]
Epoch [10/120    avg_loss:0.620, val_acc:0.821]
Epoch [11/120    avg_loss:0.557, val_acc:0.829]
Epoch [12/120    avg_loss:0.569, val_acc:0.887]
Epoch [13/120    avg_loss:0.487, val_acc:0.898]
Epoch [14/120    avg_loss:0.411, val_acc:0.894]
Epoch [15/120    avg_loss:0.427, val_acc:0.900]
Epoch [16/120    avg_loss:0.410, val_acc:0.896]
Epoch [17/120    avg_loss:0.425, val_acc:0.881]
Epoch [18/120    avg_loss:0.386, val_acc:0.844]
Epoch [19/120    avg_loss:0.337, val_acc:0.915]
Epoch [20/120    avg_loss:0.364, val_acc:0.887]
Epoch [21/120    avg_loss:0.269, val_acc:0.933]
Epoch [22/120    avg_loss:0.295, val_acc:0.938]
Epoch [23/120    avg_loss:0.238, val_acc:0.927]
Epoch [24/120    avg_loss:0.275, val_acc:0.946]
Epoch [25/120    avg_loss:0.254, val_acc:0.925]
Epoch [26/120    avg_loss:0.257, val_acc:0.931]
Epoch [27/120    avg_loss:0.246, val_acc:0.929]
Epoch [28/120    avg_loss:0.232, val_acc:0.942]
Epoch [29/120    avg_loss:0.248, val_acc:0.940]
Epoch [30/120    avg_loss:0.255, val_acc:0.927]
Epoch [31/120    avg_loss:0.324, val_acc:0.944]
Epoch [32/120    avg_loss:0.253, val_acc:0.963]
Epoch [33/120    avg_loss:0.217, val_acc:0.948]
Epoch [34/120    avg_loss:0.213, val_acc:0.912]
Epoch [35/120    avg_loss:0.204, val_acc:0.944]
Epoch [36/120    avg_loss:0.223, val_acc:0.958]
Epoch [37/120    avg_loss:0.168, val_acc:0.954]
Epoch [38/120    avg_loss:0.141, val_acc:0.954]
Epoch [39/120    avg_loss:0.138, val_acc:0.956]
Epoch [40/120    avg_loss:0.145, val_acc:0.958]
Epoch [41/120    avg_loss:0.149, val_acc:0.952]
Epoch [42/120    avg_loss:0.182, val_acc:0.963]
Epoch [43/120    avg_loss:0.176, val_acc:0.958]
Epoch [44/120    avg_loss:0.201, val_acc:0.967]
Epoch [45/120    avg_loss:0.121, val_acc:0.969]
Epoch [46/120    avg_loss:0.109, val_acc:0.967]
Epoch [47/120    avg_loss:0.104, val_acc:0.967]
Epoch [48/120    avg_loss:0.106, val_acc:0.967]
Epoch [49/120    avg_loss:0.117, val_acc:0.967]
Epoch [50/120    avg_loss:0.091, val_acc:0.973]
Epoch [51/120    avg_loss:0.080, val_acc:0.973]
Epoch [52/120    avg_loss:0.080, val_acc:0.963]
Epoch [53/120    avg_loss:0.106, val_acc:0.981]
Epoch [54/120    avg_loss:0.089, val_acc:0.954]
Epoch [55/120    avg_loss:0.127, val_acc:0.971]
Epoch [56/120    avg_loss:0.087, val_acc:0.958]
Epoch [57/120    avg_loss:0.107, val_acc:0.969]
Epoch [58/120    avg_loss:0.104, val_acc:0.973]
Epoch [59/120    avg_loss:0.116, val_acc:0.971]
Epoch [60/120    avg_loss:0.094, val_acc:0.979]
Epoch [61/120    avg_loss:0.077, val_acc:0.981]
Epoch [62/120    avg_loss:0.100, val_acc:0.971]
Epoch [63/120    avg_loss:0.080, val_acc:0.965]
Epoch [64/120    avg_loss:0.105, val_acc:0.979]
Epoch [65/120    avg_loss:0.086, val_acc:0.956]
Epoch [66/120    avg_loss:0.068, val_acc:0.979]
Epoch [67/120    avg_loss:0.066, val_acc:0.971]
Epoch [68/120    avg_loss:0.042, val_acc:0.979]
Epoch [69/120    avg_loss:0.125, val_acc:0.940]
Epoch [70/120    avg_loss:0.120, val_acc:0.919]
Epoch [71/120    avg_loss:0.122, val_acc:0.975]
Epoch [72/120    avg_loss:0.094, val_acc:0.985]
Epoch [73/120    avg_loss:0.081, val_acc:0.977]
Epoch [74/120    avg_loss:0.090, val_acc:0.985]
Epoch [75/120    avg_loss:0.075, val_acc:0.975]
Epoch [76/120    avg_loss:0.077, val_acc:0.981]
Epoch [77/120    avg_loss:0.062, val_acc:0.981]
Epoch [78/120    avg_loss:0.050, val_acc:0.983]
Epoch [79/120    avg_loss:0.040, val_acc:0.985]
Epoch [80/120    avg_loss:0.052, val_acc:0.983]
Epoch [81/120    avg_loss:0.063, val_acc:0.979]
Epoch [82/120    avg_loss:0.050, val_acc:0.979]
Epoch [83/120    avg_loss:0.043, val_acc:0.983]
Epoch [84/120    avg_loss:0.032, val_acc:0.983]
Epoch [85/120    avg_loss:0.029, val_acc:0.985]
Epoch [86/120    avg_loss:0.035, val_acc:0.981]
Epoch [87/120    avg_loss:0.041, val_acc:0.979]
Epoch [88/120    avg_loss:0.039, val_acc:0.983]
Epoch [89/120    avg_loss:0.039, val_acc:0.977]
Epoch [90/120    avg_loss:0.034, val_acc:0.985]
Epoch [91/120    avg_loss:0.050, val_acc:0.967]
Epoch [92/120    avg_loss:0.115, val_acc:0.985]
Epoch [93/120    avg_loss:0.076, val_acc:0.977]
Epoch [94/120    avg_loss:0.049, val_acc:0.979]
Epoch [95/120    avg_loss:0.051, val_acc:0.975]
Epoch [96/120    avg_loss:0.056, val_acc:0.979]
Epoch [97/120    avg_loss:0.032, val_acc:0.985]
Epoch [98/120    avg_loss:0.033, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.985]
Epoch [100/120    avg_loss:0.026, val_acc:0.983]
Epoch [101/120    avg_loss:0.022, val_acc:0.983]
Epoch [102/120    avg_loss:0.025, val_acc:0.983]
Epoch [103/120    avg_loss:0.019, val_acc:0.985]
Epoch [104/120    avg_loss:0.019, val_acc:0.983]
Epoch [105/120    avg_loss:0.019, val_acc:0.985]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.019, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.016, val_acc:0.981]
Epoch [114/120    avg_loss:0.020, val_acc:0.985]
Epoch [115/120    avg_loss:0.013, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.013, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.018, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   1   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  10   0   0   0   0   0   0   3   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99780541 0.99545455 1.         0.94690265 0.93103448
 0.99516908 0.98924731 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9933534834336638
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9eb8ec4710>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.488, val_acc:0.396]
Epoch [2/120    avg_loss:2.056, val_acc:0.550]
Epoch [3/120    avg_loss:1.784, val_acc:0.579]
Epoch [4/120    avg_loss:1.548, val_acc:0.642]
Epoch [5/120    avg_loss:1.324, val_acc:0.692]
Epoch [6/120    avg_loss:1.145, val_acc:0.721]
Epoch [7/120    avg_loss:1.012, val_acc:0.735]
Epoch [8/120    avg_loss:0.898, val_acc:0.756]
Epoch [9/120    avg_loss:0.807, val_acc:0.727]
Epoch [10/120    avg_loss:0.778, val_acc:0.781]
Epoch [11/120    avg_loss:0.673, val_acc:0.802]
Epoch [12/120    avg_loss:0.647, val_acc:0.840]
Epoch [13/120    avg_loss:0.550, val_acc:0.817]
Epoch [14/120    avg_loss:0.512, val_acc:0.900]
Epoch [15/120    avg_loss:0.444, val_acc:0.844]
Epoch [16/120    avg_loss:0.412, val_acc:0.906]
Epoch [17/120    avg_loss:0.438, val_acc:0.885]
Epoch [18/120    avg_loss:0.414, val_acc:0.935]
Epoch [19/120    avg_loss:0.381, val_acc:0.915]
Epoch [20/120    avg_loss:0.359, val_acc:0.819]
Epoch [21/120    avg_loss:0.381, val_acc:0.881]
Epoch [22/120    avg_loss:0.351, val_acc:0.915]
Epoch [23/120    avg_loss:0.270, val_acc:0.956]
Epoch [24/120    avg_loss:0.280, val_acc:0.952]
Epoch [25/120    avg_loss:0.234, val_acc:0.946]
Epoch [26/120    avg_loss:0.217, val_acc:0.952]
Epoch [27/120    avg_loss:0.270, val_acc:0.975]
Epoch [28/120    avg_loss:0.226, val_acc:0.935]
Epoch [29/120    avg_loss:0.213, val_acc:0.950]
Epoch [30/120    avg_loss:0.276, val_acc:0.963]
Epoch [31/120    avg_loss:0.215, val_acc:0.981]
Epoch [32/120    avg_loss:0.183, val_acc:0.958]
Epoch [33/120    avg_loss:0.144, val_acc:0.975]
Epoch [34/120    avg_loss:0.157, val_acc:0.985]
Epoch [35/120    avg_loss:0.128, val_acc:0.969]
Epoch [36/120    avg_loss:0.166, val_acc:0.977]
Epoch [37/120    avg_loss:0.163, val_acc:0.967]
Epoch [38/120    avg_loss:0.143, val_acc:0.965]
Epoch [39/120    avg_loss:0.138, val_acc:0.979]
Epoch [40/120    avg_loss:0.097, val_acc:0.973]
Epoch [41/120    avg_loss:0.131, val_acc:0.977]
Epoch [42/120    avg_loss:0.151, val_acc:0.927]
Epoch [43/120    avg_loss:0.164, val_acc:0.983]
Epoch [44/120    avg_loss:0.133, val_acc:0.960]
Epoch [45/120    avg_loss:0.096, val_acc:0.985]
Epoch [46/120    avg_loss:0.076, val_acc:0.977]
Epoch [47/120    avg_loss:0.094, val_acc:0.967]
Epoch [48/120    avg_loss:0.114, val_acc:0.975]
Epoch [49/120    avg_loss:0.097, val_acc:0.977]
Epoch [50/120    avg_loss:0.082, val_acc:0.979]
Epoch [51/120    avg_loss:0.082, val_acc:0.979]
Epoch [52/120    avg_loss:0.074, val_acc:0.985]
Epoch [53/120    avg_loss:0.073, val_acc:0.988]
Epoch [54/120    avg_loss:0.074, val_acc:0.981]
Epoch [55/120    avg_loss:0.067, val_acc:0.981]
Epoch [56/120    avg_loss:0.096, val_acc:0.979]
Epoch [57/120    avg_loss:0.057, val_acc:0.988]
Epoch [58/120    avg_loss:0.065, val_acc:0.985]
Epoch [59/120    avg_loss:0.068, val_acc:0.979]
Epoch [60/120    avg_loss:0.078, val_acc:0.990]
Epoch [61/120    avg_loss:0.064, val_acc:0.985]
Epoch [62/120    avg_loss:0.052, val_acc:0.985]
Epoch [63/120    avg_loss:0.063, val_acc:0.985]
Epoch [64/120    avg_loss:0.053, val_acc:0.990]
Epoch [65/120    avg_loss:0.067, val_acc:0.988]
Epoch [66/120    avg_loss:0.066, val_acc:0.990]
Epoch [67/120    avg_loss:0.056, val_acc:0.985]
Epoch [68/120    avg_loss:0.060, val_acc:0.981]
Epoch [69/120    avg_loss:0.048, val_acc:0.988]
Epoch [70/120    avg_loss:0.036, val_acc:0.996]
Epoch [71/120    avg_loss:0.047, val_acc:0.985]
Epoch [72/120    avg_loss:0.060, val_acc:0.983]
Epoch [73/120    avg_loss:0.047, val_acc:0.996]
Epoch [74/120    avg_loss:0.111, val_acc:0.985]
Epoch [75/120    avg_loss:0.066, val_acc:0.985]
Epoch [76/120    avg_loss:0.049, val_acc:0.990]
Epoch [77/120    avg_loss:0.045, val_acc:0.992]
Epoch [78/120    avg_loss:0.046, val_acc:0.990]
Epoch [79/120    avg_loss:0.030, val_acc:0.992]
Epoch [80/120    avg_loss:0.028, val_acc:0.990]
Epoch [81/120    avg_loss:0.030, val_acc:0.990]
Epoch [82/120    avg_loss:0.032, val_acc:0.990]
Epoch [83/120    avg_loss:0.017, val_acc:0.992]
Epoch [84/120    avg_loss:0.024, val_acc:0.988]
Epoch [85/120    avg_loss:0.055, val_acc:0.992]
Epoch [86/120    avg_loss:0.052, val_acc:0.985]
Epoch [87/120    avg_loss:0.030, val_acc:0.990]
Epoch [88/120    avg_loss:0.026, val_acc:0.992]
Epoch [89/120    avg_loss:0.024, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.992]
Epoch [91/120    avg_loss:0.021, val_acc:0.992]
Epoch [92/120    avg_loss:0.019, val_acc:0.992]
Epoch [93/120    avg_loss:0.021, val_acc:0.992]
Epoch [94/120    avg_loss:0.014, val_acc:0.992]
Epoch [95/120    avg_loss:0.022, val_acc:0.992]
Epoch [96/120    avg_loss:0.024, val_acc:0.992]
Epoch [97/120    avg_loss:0.019, val_acc:0.992]
Epoch [98/120    avg_loss:0.024, val_acc:0.992]
Epoch [99/120    avg_loss:0.019, val_acc:0.992]
Epoch [100/120    avg_loss:0.019, val_acc:0.992]
Epoch [101/120    avg_loss:0.016, val_acc:0.992]
Epoch [102/120    avg_loss:0.019, val_acc:0.992]
Epoch [103/120    avg_loss:0.017, val_acc:0.992]
Epoch [104/120    avg_loss:0.014, val_acc:0.992]
Epoch [105/120    avg_loss:0.019, val_acc:0.992]
Epoch [106/120    avg_loss:0.021, val_acc:0.992]
Epoch [107/120    avg_loss:0.015, val_acc:0.992]
Epoch [108/120    avg_loss:0.014, val_acc:0.992]
Epoch [109/120    avg_loss:0.016, val_acc:0.992]
Epoch [110/120    avg_loss:0.015, val_acc:0.992]
Epoch [111/120    avg_loss:0.013, val_acc:0.992]
Epoch [112/120    avg_loss:0.014, val_acc:0.992]
Epoch [113/120    avg_loss:0.015, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.015, val_acc:0.992]
Epoch [116/120    avg_loss:0.018, val_acc:0.992]
Epoch [117/120    avg_loss:0.022, val_acc:0.992]
Epoch [118/120    avg_loss:0.021, val_acc:0.992]
Epoch [119/120    avg_loss:0.015, val_acc:0.992]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         0.99095023 1.         0.9751693  0.96345515
 1.         0.97826087 1.         1.         1.         0.99472296
 0.99556541 1.        ]

Kappa:
0.9954899135213314
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f963544e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.405, val_acc:0.494]
Epoch [2/120    avg_loss:2.045, val_acc:0.600]
Epoch [3/120    avg_loss:1.779, val_acc:0.625]
Epoch [4/120    avg_loss:1.525, val_acc:0.692]
Epoch [5/120    avg_loss:1.270, val_acc:0.690]
Epoch [6/120    avg_loss:1.057, val_acc:0.779]
Epoch [7/120    avg_loss:0.939, val_acc:0.877]
Epoch [8/120    avg_loss:0.782, val_acc:0.796]
Epoch [9/120    avg_loss:0.726, val_acc:0.869]
Epoch [10/120    avg_loss:0.630, val_acc:0.923]
Epoch [11/120    avg_loss:0.520, val_acc:0.890]
Epoch [12/120    avg_loss:0.480, val_acc:0.921]
Epoch [13/120    avg_loss:0.435, val_acc:0.940]
Epoch [14/120    avg_loss:0.387, val_acc:0.908]
Epoch [15/120    avg_loss:0.380, val_acc:0.921]
Epoch [16/120    avg_loss:0.397, val_acc:0.910]
Epoch [17/120    avg_loss:0.350, val_acc:0.925]
Epoch [18/120    avg_loss:0.307, val_acc:0.927]
Epoch [19/120    avg_loss:0.355, val_acc:0.938]
Epoch [20/120    avg_loss:0.329, val_acc:0.931]
Epoch [21/120    avg_loss:0.268, val_acc:0.948]
Epoch [22/120    avg_loss:0.239, val_acc:0.958]
Epoch [23/120    avg_loss:0.270, val_acc:0.927]
Epoch [24/120    avg_loss:0.303, val_acc:0.940]
Epoch [25/120    avg_loss:0.290, val_acc:0.935]
Epoch [26/120    avg_loss:0.232, val_acc:0.925]
Epoch [27/120    avg_loss:0.220, val_acc:0.956]
Epoch [28/120    avg_loss:0.192, val_acc:0.958]
Epoch [29/120    avg_loss:0.182, val_acc:0.979]
Epoch [30/120    avg_loss:0.189, val_acc:0.960]
Epoch [31/120    avg_loss:0.174, val_acc:0.965]
Epoch [32/120    avg_loss:0.177, val_acc:0.969]
Epoch [33/120    avg_loss:0.154, val_acc:0.958]
Epoch [34/120    avg_loss:0.205, val_acc:0.967]
Epoch [35/120    avg_loss:0.203, val_acc:0.969]
Epoch [36/120    avg_loss:0.183, val_acc:0.944]
Epoch [37/120    avg_loss:0.227, val_acc:0.958]
Epoch [38/120    avg_loss:0.161, val_acc:0.963]
Epoch [39/120    avg_loss:0.137, val_acc:0.975]
Epoch [40/120    avg_loss:0.122, val_acc:0.952]
Epoch [41/120    avg_loss:0.197, val_acc:0.946]
Epoch [42/120    avg_loss:0.122, val_acc:0.975]
Epoch [43/120    avg_loss:0.100, val_acc:0.977]
Epoch [44/120    avg_loss:0.102, val_acc:0.983]
Epoch [45/120    avg_loss:0.111, val_acc:0.981]
Epoch [46/120    avg_loss:0.094, val_acc:0.983]
Epoch [47/120    avg_loss:0.074, val_acc:0.981]
Epoch [48/120    avg_loss:0.075, val_acc:0.983]
Epoch [49/120    avg_loss:0.104, val_acc:0.983]
Epoch [50/120    avg_loss:0.088, val_acc:0.983]
Epoch [51/120    avg_loss:0.074, val_acc:0.988]
Epoch [52/120    avg_loss:0.079, val_acc:0.985]
Epoch [53/120    avg_loss:0.075, val_acc:0.983]
Epoch [54/120    avg_loss:0.073, val_acc:0.983]
Epoch [55/120    avg_loss:0.071, val_acc:0.983]
Epoch [56/120    avg_loss:0.067, val_acc:0.988]
Epoch [57/120    avg_loss:0.064, val_acc:0.983]
Epoch [58/120    avg_loss:0.062, val_acc:0.981]
Epoch [59/120    avg_loss:0.053, val_acc:0.981]
Epoch [60/120    avg_loss:0.069, val_acc:0.983]
Epoch [61/120    avg_loss:0.065, val_acc:0.983]
Epoch [62/120    avg_loss:0.072, val_acc:0.985]
Epoch [63/120    avg_loss:0.059, val_acc:0.985]
Epoch [64/120    avg_loss:0.086, val_acc:0.981]
Epoch [65/120    avg_loss:0.079, val_acc:0.988]
Epoch [66/120    avg_loss:0.073, val_acc:0.990]
Epoch [67/120    avg_loss:0.061, val_acc:0.983]
Epoch [68/120    avg_loss:0.072, val_acc:0.988]
Epoch [69/120    avg_loss:0.063, val_acc:0.985]
Epoch [70/120    avg_loss:0.072, val_acc:0.985]
Epoch [71/120    avg_loss:0.064, val_acc:0.985]
Epoch [72/120    avg_loss:0.072, val_acc:0.985]
Epoch [73/120    avg_loss:0.060, val_acc:0.983]
Epoch [74/120    avg_loss:0.061, val_acc:0.983]
Epoch [75/120    avg_loss:0.057, val_acc:0.985]
Epoch [76/120    avg_loss:0.064, val_acc:0.990]
Epoch [77/120    avg_loss:0.059, val_acc:0.988]
Epoch [78/120    avg_loss:0.052, val_acc:0.990]
Epoch [79/120    avg_loss:0.062, val_acc:0.990]
Epoch [80/120    avg_loss:0.055, val_acc:0.990]
Epoch [81/120    avg_loss:0.064, val_acc:0.988]
Epoch [82/120    avg_loss:0.054, val_acc:0.985]
Epoch [83/120    avg_loss:0.056, val_acc:0.985]
Epoch [84/120    avg_loss:0.050, val_acc:0.988]
Epoch [85/120    avg_loss:0.049, val_acc:0.988]
Epoch [86/120    avg_loss:0.066, val_acc:0.988]
Epoch [87/120    avg_loss:0.040, val_acc:0.988]
Epoch [88/120    avg_loss:0.066, val_acc:0.988]
Epoch [89/120    avg_loss:0.045, val_acc:0.988]
Epoch [90/120    avg_loss:0.070, val_acc:0.988]
Epoch [91/120    avg_loss:0.050, val_acc:0.985]
Epoch [92/120    avg_loss:0.060, val_acc:0.988]
Epoch [93/120    avg_loss:0.048, val_acc:0.990]
Epoch [94/120    avg_loss:0.052, val_acc:0.988]
Epoch [95/120    avg_loss:0.045, val_acc:0.985]
Epoch [96/120    avg_loss:0.052, val_acc:0.988]
Epoch [97/120    avg_loss:0.044, val_acc:0.985]
Epoch [98/120    avg_loss:0.047, val_acc:0.990]
Epoch [99/120    avg_loss:0.037, val_acc:0.990]
Epoch [100/120    avg_loss:0.047, val_acc:0.988]
Epoch [101/120    avg_loss:0.039, val_acc:0.988]
Epoch [102/120    avg_loss:0.038, val_acc:0.988]
Epoch [103/120    avg_loss:0.051, val_acc:0.988]
Epoch [104/120    avg_loss:0.037, val_acc:0.988]
Epoch [105/120    avg_loss:0.041, val_acc:0.990]
Epoch [106/120    avg_loss:0.053, val_acc:0.990]
Epoch [107/120    avg_loss:0.044, val_acc:0.985]
Epoch [108/120    avg_loss:0.057, val_acc:0.985]
Epoch [109/120    avg_loss:0.043, val_acc:0.985]
Epoch [110/120    avg_loss:0.032, val_acc:0.988]
Epoch [111/120    avg_loss:0.042, val_acc:0.988]
Epoch [112/120    avg_loss:0.048, val_acc:0.990]
Epoch [113/120    avg_loss:0.040, val_acc:0.985]
Epoch [114/120    avg_loss:0.044, val_acc:0.983]
Epoch [115/120    avg_loss:0.046, val_acc:0.985]
Epoch [116/120    avg_loss:0.035, val_acc:0.990]
Epoch [117/120    avg_loss:0.041, val_acc:0.990]
Epoch [118/120    avg_loss:0.034, val_acc:0.988]
Epoch [119/120    avg_loss:0.033, val_acc:0.988]
Epoch [120/120    avg_loss:0.037, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99780541 0.97986577 1.         0.95067265 0.9261745
 0.99277108 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919294259527108
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19e2f097f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.426, val_acc:0.346]
Epoch [2/120    avg_loss:2.080, val_acc:0.529]
Epoch [3/120    avg_loss:1.852, val_acc:0.552]
Epoch [4/120    avg_loss:1.573, val_acc:0.648]
Epoch [5/120    avg_loss:1.363, val_acc:0.650]
Epoch [6/120    avg_loss:1.161, val_acc:0.683]
Epoch [7/120    avg_loss:1.032, val_acc:0.727]
Epoch [8/120    avg_loss:0.866, val_acc:0.740]
Epoch [9/120    avg_loss:0.862, val_acc:0.821]
Epoch [10/120    avg_loss:0.718, val_acc:0.760]
Epoch [11/120    avg_loss:0.647, val_acc:0.804]
Epoch [12/120    avg_loss:0.605, val_acc:0.887]
Epoch [13/120    avg_loss:0.531, val_acc:0.844]
Epoch [14/120    avg_loss:0.503, val_acc:0.912]
Epoch [15/120    avg_loss:0.435, val_acc:0.931]
Epoch [16/120    avg_loss:0.418, val_acc:0.902]
Epoch [17/120    avg_loss:0.406, val_acc:0.890]
Epoch [18/120    avg_loss:0.372, val_acc:0.917]
Epoch [19/120    avg_loss:0.334, val_acc:0.923]
Epoch [20/120    avg_loss:0.359, val_acc:0.904]
Epoch [21/120    avg_loss:0.359, val_acc:0.881]
Epoch [22/120    avg_loss:0.350, val_acc:0.935]
Epoch [23/120    avg_loss:0.330, val_acc:0.933]
Epoch [24/120    avg_loss:0.265, val_acc:0.923]
Epoch [25/120    avg_loss:0.268, val_acc:0.942]
Epoch [26/120    avg_loss:0.261, val_acc:0.938]
Epoch [27/120    avg_loss:0.237, val_acc:0.933]
Epoch [28/120    avg_loss:0.270, val_acc:0.927]
Epoch [29/120    avg_loss:0.245, val_acc:0.952]
Epoch [30/120    avg_loss:0.199, val_acc:0.919]
Epoch [31/120    avg_loss:0.263, val_acc:0.900]
Epoch [32/120    avg_loss:0.323, val_acc:0.929]
Epoch [33/120    avg_loss:0.245, val_acc:0.915]
Epoch [34/120    avg_loss:0.222, val_acc:0.948]
Epoch [35/120    avg_loss:0.227, val_acc:0.940]
Epoch [36/120    avg_loss:0.195, val_acc:0.948]
Epoch [37/120    avg_loss:0.222, val_acc:0.908]
Epoch [38/120    avg_loss:0.209, val_acc:0.912]
Epoch [39/120    avg_loss:0.199, val_acc:0.948]
Epoch [40/120    avg_loss:0.148, val_acc:0.963]
Epoch [41/120    avg_loss:0.134, val_acc:0.963]
Epoch [42/120    avg_loss:0.142, val_acc:0.954]
Epoch [43/120    avg_loss:0.132, val_acc:0.954]
Epoch [44/120    avg_loss:0.130, val_acc:0.958]
Epoch [45/120    avg_loss:0.128, val_acc:0.938]
Epoch [46/120    avg_loss:0.201, val_acc:0.927]
Epoch [47/120    avg_loss:0.161, val_acc:0.942]
Epoch [48/120    avg_loss:0.171, val_acc:0.958]
Epoch [49/120    avg_loss:0.132, val_acc:0.965]
Epoch [50/120    avg_loss:0.090, val_acc:0.950]
Epoch [51/120    avg_loss:0.110, val_acc:0.971]
Epoch [52/120    avg_loss:0.098, val_acc:0.948]
Epoch [53/120    avg_loss:0.117, val_acc:0.977]
Epoch [54/120    avg_loss:0.087, val_acc:0.967]
Epoch [55/120    avg_loss:0.079, val_acc:0.971]
Epoch [56/120    avg_loss:0.094, val_acc:0.973]
Epoch [57/120    avg_loss:0.108, val_acc:0.963]
Epoch [58/120    avg_loss:0.116, val_acc:0.958]
Epoch [59/120    avg_loss:0.106, val_acc:0.971]
Epoch [60/120    avg_loss:0.106, val_acc:0.965]
Epoch [61/120    avg_loss:0.071, val_acc:0.971]
Epoch [62/120    avg_loss:0.057, val_acc:0.965]
Epoch [63/120    avg_loss:0.066, val_acc:0.958]
Epoch [64/120    avg_loss:0.067, val_acc:0.975]
Epoch [65/120    avg_loss:0.116, val_acc:0.977]
Epoch [66/120    avg_loss:0.082, val_acc:0.967]
Epoch [67/120    avg_loss:0.078, val_acc:0.965]
Epoch [68/120    avg_loss:0.060, val_acc:0.963]
Epoch [69/120    avg_loss:0.073, val_acc:0.973]
Epoch [70/120    avg_loss:0.065, val_acc:0.967]
Epoch [71/120    avg_loss:0.044, val_acc:0.977]
Epoch [72/120    avg_loss:0.053, val_acc:0.963]
Epoch [73/120    avg_loss:0.081, val_acc:0.971]
Epoch [74/120    avg_loss:0.046, val_acc:0.973]
Epoch [75/120    avg_loss:0.083, val_acc:0.975]
Epoch [76/120    avg_loss:0.062, val_acc:0.981]
Epoch [77/120    avg_loss:0.047, val_acc:0.979]
Epoch [78/120    avg_loss:0.040, val_acc:0.975]
Epoch [79/120    avg_loss:0.043, val_acc:0.975]
Epoch [80/120    avg_loss:0.037, val_acc:0.973]
Epoch [81/120    avg_loss:0.051, val_acc:0.981]
Epoch [82/120    avg_loss:0.025, val_acc:0.979]
Epoch [83/120    avg_loss:0.027, val_acc:0.979]
Epoch [84/120    avg_loss:0.021, val_acc:0.979]
Epoch [85/120    avg_loss:0.019, val_acc:0.979]
Epoch [86/120    avg_loss:0.017, val_acc:0.990]
Epoch [87/120    avg_loss:0.031, val_acc:0.981]
Epoch [88/120    avg_loss:0.046, val_acc:0.979]
Epoch [89/120    avg_loss:0.045, val_acc:0.975]
Epoch [90/120    avg_loss:0.038, val_acc:0.963]
Epoch [91/120    avg_loss:0.031, val_acc:0.973]
Epoch [92/120    avg_loss:0.041, val_acc:0.977]
Epoch [93/120    avg_loss:0.041, val_acc:0.983]
Epoch [94/120    avg_loss:0.032, val_acc:0.975]
Epoch [95/120    avg_loss:0.025, val_acc:0.979]
Epoch [96/120    avg_loss:0.016, val_acc:0.985]
Epoch [97/120    avg_loss:0.028, val_acc:0.979]
Epoch [98/120    avg_loss:0.048, val_acc:0.944]
Epoch [99/120    avg_loss:0.040, val_acc:0.988]
Epoch [100/120    avg_loss:0.026, val_acc:0.990]
Epoch [101/120    avg_loss:0.018, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.990]
Epoch [103/120    avg_loss:0.012, val_acc:0.990]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.013, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.990]
Epoch [107/120    avg_loss:0.018, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.019, val_acc:0.990]
Epoch [110/120    avg_loss:0.012, val_acc:0.988]
Epoch [111/120    avg_loss:0.016, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.988]
Epoch [113/120    avg_loss:0.013, val_acc:0.985]
Epoch [114/120    avg_loss:0.014, val_acc:0.985]
Epoch [115/120    avg_loss:0.018, val_acc:0.983]
Epoch [116/120    avg_loss:0.018, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 1.         0.99545455 1.         0.9844098  0.97627119
 1.         0.98924731 1.         1.         1.         0.99602649
 0.99668508 1.        ]

Kappa:
0.9971514565683676
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2a5d667f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.425, val_acc:0.390]
Epoch [2/120    avg_loss:2.088, val_acc:0.600]
Epoch [3/120    avg_loss:1.782, val_acc:0.654]
Epoch [4/120    avg_loss:1.532, val_acc:0.688]
Epoch [5/120    avg_loss:1.261, val_acc:0.731]
Epoch [6/120    avg_loss:1.026, val_acc:0.762]
Epoch [7/120    avg_loss:0.893, val_acc:0.760]
Epoch [8/120    avg_loss:0.789, val_acc:0.800]
Epoch [9/120    avg_loss:0.696, val_acc:0.835]
Epoch [10/120    avg_loss:0.597, val_acc:0.852]
Epoch [11/120    avg_loss:0.511, val_acc:0.869]
Epoch [12/120    avg_loss:0.523, val_acc:0.796]
Epoch [13/120    avg_loss:0.578, val_acc:0.898]
Epoch [14/120    avg_loss:0.488, val_acc:0.917]
Epoch [15/120    avg_loss:0.395, val_acc:0.912]
Epoch [16/120    avg_loss:0.366, val_acc:0.950]
Epoch [17/120    avg_loss:0.361, val_acc:0.908]
Epoch [18/120    avg_loss:0.474, val_acc:0.921]
Epoch [19/120    avg_loss:0.378, val_acc:0.931]
Epoch [20/120    avg_loss:0.298, val_acc:0.950]
Epoch [21/120    avg_loss:0.283, val_acc:0.963]
Epoch [22/120    avg_loss:0.245, val_acc:0.963]
Epoch [23/120    avg_loss:0.234, val_acc:0.956]
Epoch [24/120    avg_loss:0.210, val_acc:0.946]
Epoch [25/120    avg_loss:0.224, val_acc:0.965]
Epoch [26/120    avg_loss:0.242, val_acc:0.967]
Epoch [27/120    avg_loss:0.180, val_acc:0.956]
Epoch [28/120    avg_loss:0.206, val_acc:0.977]
Epoch [29/120    avg_loss:0.182, val_acc:0.965]
Epoch [30/120    avg_loss:0.246, val_acc:0.975]
Epoch [31/120    avg_loss:0.193, val_acc:0.954]
Epoch [32/120    avg_loss:0.288, val_acc:0.967]
Epoch [33/120    avg_loss:0.232, val_acc:0.958]
Epoch [34/120    avg_loss:0.197, val_acc:0.969]
Epoch [35/120    avg_loss:0.170, val_acc:0.967]
Epoch [36/120    avg_loss:0.207, val_acc:0.944]
Epoch [37/120    avg_loss:0.181, val_acc:0.965]
Epoch [38/120    avg_loss:0.117, val_acc:0.981]
Epoch [39/120    avg_loss:0.141, val_acc:0.963]
Epoch [40/120    avg_loss:0.144, val_acc:0.967]
Epoch [41/120    avg_loss:0.113, val_acc:0.971]
Epoch [42/120    avg_loss:0.109, val_acc:0.967]
Epoch [43/120    avg_loss:0.155, val_acc:0.973]
Epoch [44/120    avg_loss:0.155, val_acc:0.965]
Epoch [45/120    avg_loss:0.110, val_acc:0.958]
Epoch [46/120    avg_loss:0.167, val_acc:0.983]
Epoch [47/120    avg_loss:0.116, val_acc:0.979]
Epoch [48/120    avg_loss:0.106, val_acc:0.992]
Epoch [49/120    avg_loss:0.071, val_acc:0.988]
Epoch [50/120    avg_loss:0.081, val_acc:0.981]
Epoch [51/120    avg_loss:0.057, val_acc:0.988]
Epoch [52/120    avg_loss:0.065, val_acc:0.985]
Epoch [53/120    avg_loss:0.065, val_acc:0.985]
Epoch [54/120    avg_loss:0.129, val_acc:0.983]
Epoch [55/120    avg_loss:0.087, val_acc:0.977]
Epoch [56/120    avg_loss:0.078, val_acc:0.979]
Epoch [57/120    avg_loss:0.072, val_acc:0.990]
Epoch [58/120    avg_loss:0.066, val_acc:0.992]
Epoch [59/120    avg_loss:0.081, val_acc:0.992]
Epoch [60/120    avg_loss:0.067, val_acc:0.990]
Epoch [61/120    avg_loss:0.041, val_acc:0.994]
Epoch [62/120    avg_loss:0.055, val_acc:0.990]
Epoch [63/120    avg_loss:0.131, val_acc:0.963]
Epoch [64/120    avg_loss:0.145, val_acc:0.969]
Epoch [65/120    avg_loss:0.105, val_acc:0.981]
Epoch [66/120    avg_loss:0.054, val_acc:0.990]
Epoch [67/120    avg_loss:0.057, val_acc:0.994]
Epoch [68/120    avg_loss:0.041, val_acc:0.992]
Epoch [69/120    avg_loss:0.035, val_acc:0.983]
Epoch [70/120    avg_loss:0.070, val_acc:0.979]
Epoch [71/120    avg_loss:0.062, val_acc:0.990]
Epoch [72/120    avg_loss:0.053, val_acc:0.977]
Epoch [73/120    avg_loss:0.047, val_acc:0.983]
Epoch [74/120    avg_loss:0.040, val_acc:0.990]
Epoch [75/120    avg_loss:0.038, val_acc:0.990]
Epoch [76/120    avg_loss:0.037, val_acc:0.992]
Epoch [77/120    avg_loss:0.038, val_acc:0.990]
Epoch [78/120    avg_loss:0.034, val_acc:0.988]
Epoch [79/120    avg_loss:0.037, val_acc:0.985]
Epoch [80/120    avg_loss:0.039, val_acc:0.981]
Epoch [81/120    avg_loss:0.028, val_acc:0.985]
Epoch [82/120    avg_loss:0.033, val_acc:0.990]
Epoch [83/120    avg_loss:0.021, val_acc:0.992]
Epoch [84/120    avg_loss:0.016, val_acc:0.992]
Epoch [85/120    avg_loss:0.022, val_acc:0.990]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.018, val_acc:0.992]
Epoch [88/120    avg_loss:0.016, val_acc:0.992]
Epoch [89/120    avg_loss:0.029, val_acc:0.992]
Epoch [90/120    avg_loss:0.022, val_acc:0.992]
Epoch [91/120    avg_loss:0.017, val_acc:0.994]
Epoch [92/120    avg_loss:0.023, val_acc:0.994]
Epoch [93/120    avg_loss:0.016, val_acc:0.994]
Epoch [94/120    avg_loss:0.023, val_acc:0.992]
Epoch [95/120    avg_loss:0.018, val_acc:0.992]
Epoch [96/120    avg_loss:0.027, val_acc:0.994]
Epoch [97/120    avg_loss:0.034, val_acc:0.994]
Epoch [98/120    avg_loss:0.016, val_acc:0.994]
Epoch [99/120    avg_loss:0.024, val_acc:0.994]
Epoch [100/120    avg_loss:0.021, val_acc:0.994]
Epoch [101/120    avg_loss:0.018, val_acc:0.994]
Epoch [102/120    avg_loss:0.015, val_acc:0.994]
Epoch [103/120    avg_loss:0.015, val_acc:0.994]
Epoch [104/120    avg_loss:0.023, val_acc:0.994]
Epoch [105/120    avg_loss:0.017, val_acc:0.994]
Epoch [106/120    avg_loss:0.015, val_acc:0.994]
Epoch [107/120    avg_loss:0.017, val_acc:0.994]
Epoch [108/120    avg_loss:0.018, val_acc:0.994]
Epoch [109/120    avg_loss:0.015, val_acc:0.994]
Epoch [110/120    avg_loss:0.015, val_acc:0.994]
Epoch [111/120    avg_loss:0.016, val_acc:0.994]
Epoch [112/120    avg_loss:0.016, val_acc:0.994]
Epoch [113/120    avg_loss:0.019, val_acc:0.994]
Epoch [114/120    avg_loss:0.013, val_acc:0.994]
Epoch [115/120    avg_loss:0.020, val_acc:0.994]
Epoch [116/120    avg_loss:0.014, val_acc:0.994]
Epoch [117/120    avg_loss:0.015, val_acc:0.994]
Epoch [118/120    avg_loss:0.020, val_acc:0.994]
Epoch [119/120    avg_loss:0.017, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   4   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99560117 1.         1.         0.96760259 0.93333333
 0.99516908 1.         1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9947782169356828
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20934be860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.416, val_acc:0.404]
Epoch [2/120    avg_loss:2.039, val_acc:0.677]
Epoch [3/120    avg_loss:1.711, val_acc:0.658]
Epoch [4/120    avg_loss:1.453, val_acc:0.725]
Epoch [5/120    avg_loss:1.216, val_acc:0.746]
Epoch [6/120    avg_loss:1.022, val_acc:0.781]
Epoch [7/120    avg_loss:0.920, val_acc:0.787]
Epoch [8/120    avg_loss:0.784, val_acc:0.821]
Epoch [9/120    avg_loss:0.735, val_acc:0.838]
Epoch [10/120    avg_loss:0.611, val_acc:0.898]
Epoch [11/120    avg_loss:0.558, val_acc:0.904]
Epoch [12/120    avg_loss:0.494, val_acc:0.906]
Epoch [13/120    avg_loss:0.483, val_acc:0.881]
Epoch [14/120    avg_loss:0.467, val_acc:0.908]
Epoch [15/120    avg_loss:0.443, val_acc:0.898]
Epoch [16/120    avg_loss:0.397, val_acc:0.892]
Epoch [17/120    avg_loss:0.328, val_acc:0.915]
Epoch [18/120    avg_loss:0.324, val_acc:0.935]
Epoch [19/120    avg_loss:0.351, val_acc:0.948]
Epoch [20/120    avg_loss:0.302, val_acc:0.940]
Epoch [21/120    avg_loss:0.270, val_acc:0.938]
Epoch [22/120    avg_loss:0.287, val_acc:0.925]
Epoch [23/120    avg_loss:0.261, val_acc:0.956]
Epoch [24/120    avg_loss:0.239, val_acc:0.958]
Epoch [25/120    avg_loss:0.245, val_acc:0.960]
Epoch [26/120    avg_loss:0.237, val_acc:0.965]
Epoch [27/120    avg_loss:0.186, val_acc:0.975]
Epoch [28/120    avg_loss:0.179, val_acc:0.956]
Epoch [29/120    avg_loss:0.200, val_acc:0.950]
Epoch [30/120    avg_loss:0.197, val_acc:0.948]
Epoch [31/120    avg_loss:0.180, val_acc:0.960]
Epoch [32/120    avg_loss:0.210, val_acc:0.933]
Epoch [33/120    avg_loss:0.199, val_acc:0.975]
Epoch [34/120    avg_loss:0.162, val_acc:0.965]
Epoch [35/120    avg_loss:0.146, val_acc:0.958]
Epoch [36/120    avg_loss:0.130, val_acc:0.965]
Epoch [37/120    avg_loss:0.107, val_acc:0.956]
Epoch [38/120    avg_loss:0.159, val_acc:0.927]
Epoch [39/120    avg_loss:0.162, val_acc:0.956]
Epoch [40/120    avg_loss:0.165, val_acc:0.963]
Epoch [41/120    avg_loss:0.163, val_acc:0.915]
Epoch [42/120    avg_loss:0.133, val_acc:0.960]
Epoch [43/120    avg_loss:0.082, val_acc:0.969]
Epoch [44/120    avg_loss:0.097, val_acc:0.969]
Epoch [45/120    avg_loss:0.099, val_acc:0.969]
Epoch [46/120    avg_loss:0.085, val_acc:0.963]
Epoch [47/120    avg_loss:0.082, val_acc:0.971]
Epoch [48/120    avg_loss:0.062, val_acc:0.975]
Epoch [49/120    avg_loss:0.059, val_acc:0.979]
Epoch [50/120    avg_loss:0.064, val_acc:0.975]
Epoch [51/120    avg_loss:0.076, val_acc:0.979]
Epoch [52/120    avg_loss:0.051, val_acc:0.979]
Epoch [53/120    avg_loss:0.051, val_acc:0.979]
Epoch [54/120    avg_loss:0.044, val_acc:0.979]
Epoch [55/120    avg_loss:0.064, val_acc:0.977]
Epoch [56/120    avg_loss:0.045, val_acc:0.979]
Epoch [57/120    avg_loss:0.053, val_acc:0.975]
Epoch [58/120    avg_loss:0.058, val_acc:0.981]
Epoch [59/120    avg_loss:0.047, val_acc:0.981]
Epoch [60/120    avg_loss:0.054, val_acc:0.979]
Epoch [61/120    avg_loss:0.045, val_acc:0.981]
Epoch [62/120    avg_loss:0.047, val_acc:0.981]
Epoch [63/120    avg_loss:0.044, val_acc:0.979]
Epoch [64/120    avg_loss:0.044, val_acc:0.979]
Epoch [65/120    avg_loss:0.056, val_acc:0.985]
Epoch [66/120    avg_loss:0.043, val_acc:0.985]
Epoch [67/120    avg_loss:0.041, val_acc:0.985]
Epoch [68/120    avg_loss:0.042, val_acc:0.988]
Epoch [69/120    avg_loss:0.043, val_acc:0.988]
Epoch [70/120    avg_loss:0.044, val_acc:0.988]
Epoch [71/120    avg_loss:0.038, val_acc:0.985]
Epoch [72/120    avg_loss:0.047, val_acc:0.983]
Epoch [73/120    avg_loss:0.047, val_acc:0.985]
Epoch [74/120    avg_loss:0.034, val_acc:0.990]
Epoch [75/120    avg_loss:0.037, val_acc:0.988]
Epoch [76/120    avg_loss:0.034, val_acc:0.985]
Epoch [77/120    avg_loss:0.039, val_acc:0.985]
Epoch [78/120    avg_loss:0.048, val_acc:0.985]
Epoch [79/120    avg_loss:0.039, val_acc:0.990]
Epoch [80/120    avg_loss:0.037, val_acc:0.990]
Epoch [81/120    avg_loss:0.035, val_acc:0.988]
Epoch [82/120    avg_loss:0.038, val_acc:0.990]
Epoch [83/120    avg_loss:0.038, val_acc:0.990]
Epoch [84/120    avg_loss:0.031, val_acc:0.990]
Epoch [85/120    avg_loss:0.042, val_acc:0.990]
Epoch [86/120    avg_loss:0.038, val_acc:0.990]
Epoch [87/120    avg_loss:0.039, val_acc:0.988]
Epoch [88/120    avg_loss:0.036, val_acc:0.988]
Epoch [89/120    avg_loss:0.034, val_acc:0.990]
Epoch [90/120    avg_loss:0.029, val_acc:0.988]
Epoch [91/120    avg_loss:0.032, val_acc:0.990]
Epoch [92/120    avg_loss:0.036, val_acc:0.988]
Epoch [93/120    avg_loss:0.037, val_acc:0.990]
Epoch [94/120    avg_loss:0.030, val_acc:0.988]
Epoch [95/120    avg_loss:0.044, val_acc:0.985]
Epoch [96/120    avg_loss:0.033, val_acc:0.988]
Epoch [97/120    avg_loss:0.030, val_acc:0.988]
Epoch [98/120    avg_loss:0.030, val_acc:0.990]
Epoch [99/120    avg_loss:0.035, val_acc:0.990]
Epoch [100/120    avg_loss:0.034, val_acc:0.990]
Epoch [101/120    avg_loss:0.031, val_acc:0.990]
Epoch [102/120    avg_loss:0.033, val_acc:0.990]
Epoch [103/120    avg_loss:0.030, val_acc:0.988]
Epoch [104/120    avg_loss:0.033, val_acc:0.990]
Epoch [105/120    avg_loss:0.034, val_acc:0.988]
Epoch [106/120    avg_loss:0.029, val_acc:0.988]
Epoch [107/120    avg_loss:0.039, val_acc:0.988]
Epoch [108/120    avg_loss:0.033, val_acc:0.990]
Epoch [109/120    avg_loss:0.035, val_acc:0.990]
Epoch [110/120    avg_loss:0.031, val_acc:0.985]
Epoch [111/120    avg_loss:0.041, val_acc:0.985]
Epoch [112/120    avg_loss:0.031, val_acc:0.983]
Epoch [113/120    avg_loss:0.031, val_acc:0.988]
Epoch [114/120    avg_loss:0.033, val_acc:0.990]
Epoch [115/120    avg_loss:0.031, val_acc:0.990]
Epoch [116/120    avg_loss:0.027, val_acc:0.988]
Epoch [117/120    avg_loss:0.033, val_acc:0.985]
Epoch [118/120    avg_loss:0.027, val_acc:0.988]
Epoch [119/120    avg_loss:0.029, val_acc:0.988]
Epoch [120/120    avg_loss:0.034, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 0.99560117 0.99095023 1.         0.98660714 0.97972973
 0.98564593 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9962024475994449
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa76bc0c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.426, val_acc:0.552]
Epoch [2/120    avg_loss:2.012, val_acc:0.598]
Epoch [3/120    avg_loss:1.712, val_acc:0.642]
Epoch [4/120    avg_loss:1.442, val_acc:0.692]
Epoch [5/120    avg_loss:1.249, val_acc:0.715]
Epoch [6/120    avg_loss:1.024, val_acc:0.750]
Epoch [7/120    avg_loss:0.879, val_acc:0.779]
Epoch [8/120    avg_loss:0.823, val_acc:0.792]
Epoch [9/120    avg_loss:0.712, val_acc:0.854]
Epoch [10/120    avg_loss:0.651, val_acc:0.887]
Epoch [11/120    avg_loss:0.583, val_acc:0.896]
Epoch [12/120    avg_loss:0.564, val_acc:0.869]
Epoch [13/120    avg_loss:0.481, val_acc:0.940]
Epoch [14/120    avg_loss:0.482, val_acc:0.881]
Epoch [15/120    avg_loss:0.478, val_acc:0.904]
Epoch [16/120    avg_loss:0.430, val_acc:0.948]
Epoch [17/120    avg_loss:0.458, val_acc:0.925]
Epoch [18/120    avg_loss:0.389, val_acc:0.929]
Epoch [19/120    avg_loss:0.335, val_acc:0.896]
Epoch [20/120    avg_loss:0.446, val_acc:0.938]
Epoch [21/120    avg_loss:0.360, val_acc:0.919]
Epoch [22/120    avg_loss:0.290, val_acc:0.956]
Epoch [23/120    avg_loss:0.265, val_acc:0.952]
Epoch [24/120    avg_loss:0.221, val_acc:0.965]
Epoch [25/120    avg_loss:0.199, val_acc:0.952]
Epoch [26/120    avg_loss:0.225, val_acc:0.954]
Epoch [27/120    avg_loss:0.269, val_acc:0.973]
Epoch [28/120    avg_loss:0.246, val_acc:0.958]
Epoch [29/120    avg_loss:0.255, val_acc:0.971]
Epoch [30/120    avg_loss:0.176, val_acc:0.960]
Epoch [31/120    avg_loss:0.166, val_acc:0.956]
Epoch [32/120    avg_loss:0.191, val_acc:0.973]
Epoch [33/120    avg_loss:0.180, val_acc:0.967]
Epoch [34/120    avg_loss:0.207, val_acc:0.942]
Epoch [35/120    avg_loss:0.171, val_acc:0.965]
Epoch [36/120    avg_loss:0.171, val_acc:0.956]
Epoch [37/120    avg_loss:0.186, val_acc:0.963]
Epoch [38/120    avg_loss:0.187, val_acc:0.977]
Epoch [39/120    avg_loss:0.140, val_acc:0.971]
Epoch [40/120    avg_loss:0.227, val_acc:0.965]
Epoch [41/120    avg_loss:0.186, val_acc:0.975]
Epoch [42/120    avg_loss:0.176, val_acc:0.971]
Epoch [43/120    avg_loss:0.127, val_acc:0.983]
Epoch [44/120    avg_loss:0.122, val_acc:0.954]
Epoch [45/120    avg_loss:0.111, val_acc:0.981]
Epoch [46/120    avg_loss:0.099, val_acc:0.973]
Epoch [47/120    avg_loss:0.105, val_acc:0.971]
Epoch [48/120    avg_loss:0.135, val_acc:0.971]
Epoch [49/120    avg_loss:0.087, val_acc:0.983]
Epoch [50/120    avg_loss:0.090, val_acc:0.977]
Epoch [51/120    avg_loss:0.095, val_acc:0.975]
Epoch [52/120    avg_loss:0.185, val_acc:0.973]
Epoch [53/120    avg_loss:0.110, val_acc:0.971]
Epoch [54/120    avg_loss:0.129, val_acc:0.948]
Epoch [55/120    avg_loss:0.091, val_acc:0.979]
Epoch [56/120    avg_loss:0.172, val_acc:0.973]
Epoch [57/120    avg_loss:0.087, val_acc:0.990]
Epoch [58/120    avg_loss:0.074, val_acc:0.985]
Epoch [59/120    avg_loss:0.085, val_acc:0.960]
Epoch [60/120    avg_loss:0.081, val_acc:0.977]
Epoch [61/120    avg_loss:0.061, val_acc:0.996]
Epoch [62/120    avg_loss:0.052, val_acc:0.994]
Epoch [63/120    avg_loss:0.043, val_acc:0.992]
Epoch [64/120    avg_loss:0.049, val_acc:0.990]
Epoch [65/120    avg_loss:0.038, val_acc:0.992]
Epoch [66/120    avg_loss:0.082, val_acc:0.988]
Epoch [67/120    avg_loss:0.078, val_acc:0.981]
Epoch [68/120    avg_loss:0.053, val_acc:0.992]
Epoch [69/120    avg_loss:0.049, val_acc:0.994]
Epoch [70/120    avg_loss:0.058, val_acc:0.988]
Epoch [71/120    avg_loss:0.071, val_acc:0.971]
Epoch [72/120    avg_loss:0.187, val_acc:0.975]
Epoch [73/120    avg_loss:0.129, val_acc:0.979]
Epoch [74/120    avg_loss:0.103, val_acc:0.971]
Epoch [75/120    avg_loss:0.079, val_acc:0.985]
Epoch [76/120    avg_loss:0.055, val_acc:0.994]
Epoch [77/120    avg_loss:0.058, val_acc:0.994]
Epoch [78/120    avg_loss:0.044, val_acc:0.994]
Epoch [79/120    avg_loss:0.035, val_acc:0.992]
Epoch [80/120    avg_loss:0.043, val_acc:0.990]
Epoch [81/120    avg_loss:0.034, val_acc:0.990]
Epoch [82/120    avg_loss:0.029, val_acc:0.990]
Epoch [83/120    avg_loss:0.032, val_acc:0.990]
Epoch [84/120    avg_loss:0.036, val_acc:0.990]
Epoch [85/120    avg_loss:0.052, val_acc:0.990]
Epoch [86/120    avg_loss:0.042, val_acc:0.990]
Epoch [87/120    avg_loss:0.028, val_acc:0.992]
Epoch [88/120    avg_loss:0.031, val_acc:0.992]
Epoch [89/120    avg_loss:0.029, val_acc:0.992]
Epoch [90/120    avg_loss:0.062, val_acc:0.994]
Epoch [91/120    avg_loss:0.034, val_acc:0.996]
Epoch [92/120    avg_loss:0.036, val_acc:0.996]
Epoch [93/120    avg_loss:0.039, val_acc:0.996]
Epoch [94/120    avg_loss:0.033, val_acc:0.996]
Epoch [95/120    avg_loss:0.037, val_acc:0.996]
Epoch [96/120    avg_loss:0.043, val_acc:0.996]
Epoch [97/120    avg_loss:0.034, val_acc:0.996]
Epoch [98/120    avg_loss:0.043, val_acc:0.996]
Epoch [99/120    avg_loss:0.037, val_acc:0.996]
Epoch [100/120    avg_loss:0.044, val_acc:0.996]
Epoch [101/120    avg_loss:0.031, val_acc:0.998]
Epoch [102/120    avg_loss:0.071, val_acc:0.998]
Epoch [103/120    avg_loss:0.030, val_acc:0.998]
Epoch [104/120    avg_loss:0.057, val_acc:0.998]
Epoch [105/120    avg_loss:0.033, val_acc:0.996]
Epoch [106/120    avg_loss:0.034, val_acc:0.996]
Epoch [107/120    avg_loss:0.025, val_acc:0.996]
Epoch [108/120    avg_loss:0.042, val_acc:0.998]
Epoch [109/120    avg_loss:0.039, val_acc:0.998]
Epoch [110/120    avg_loss:0.033, val_acc:0.996]
Epoch [111/120    avg_loss:0.034, val_acc:0.996]
Epoch [112/120    avg_loss:0.035, val_acc:0.998]
Epoch [113/120    avg_loss:0.035, val_acc:0.998]
Epoch [114/120    avg_loss:0.028, val_acc:0.998]
Epoch [115/120    avg_loss:0.027, val_acc:0.998]
Epoch [116/120    avg_loss:0.033, val_acc:0.998]
Epoch [117/120    avg_loss:0.036, val_acc:0.998]
Epoch [118/120    avg_loss:0.041, val_acc:0.998]
Epoch [119/120    avg_loss:0.034, val_acc:0.998]
Epoch [120/120    avg_loss:0.030, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215   8   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.7228144989339

F1 scores:
[       nan 1.         0.9977221  1.         0.97285068 0.97315436
 1.         0.99465241 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9969139828263169
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a21be2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.502, val_acc:0.417]
Epoch [2/120    avg_loss:2.016, val_acc:0.573]
Epoch [3/120    avg_loss:1.648, val_acc:0.637]
Epoch [4/120    avg_loss:1.384, val_acc:0.690]
Epoch [5/120    avg_loss:1.174, val_acc:0.733]
Epoch [6/120    avg_loss:1.013, val_acc:0.748]
Epoch [7/120    avg_loss:0.909, val_acc:0.771]
Epoch [8/120    avg_loss:0.785, val_acc:0.781]
Epoch [9/120    avg_loss:0.753, val_acc:0.806]
Epoch [10/120    avg_loss:0.646, val_acc:0.798]
Epoch [11/120    avg_loss:0.599, val_acc:0.852]
Epoch [12/120    avg_loss:0.533, val_acc:0.819]
Epoch [13/120    avg_loss:0.507, val_acc:0.902]
Epoch [14/120    avg_loss:0.443, val_acc:0.921]
Epoch [15/120    avg_loss:0.415, val_acc:0.894]
Epoch [16/120    avg_loss:0.382, val_acc:0.933]
Epoch [17/120    avg_loss:0.414, val_acc:0.871]
Epoch [18/120    avg_loss:0.333, val_acc:0.915]
Epoch [19/120    avg_loss:0.354, val_acc:0.940]
Epoch [20/120    avg_loss:0.364, val_acc:0.929]
Epoch [21/120    avg_loss:0.358, val_acc:0.931]
Epoch [22/120    avg_loss:0.311, val_acc:0.935]
Epoch [23/120    avg_loss:0.342, val_acc:0.927]
Epoch [24/120    avg_loss:0.332, val_acc:0.875]
Epoch [25/120    avg_loss:0.259, val_acc:0.952]
Epoch [26/120    avg_loss:0.245, val_acc:0.931]
Epoch [27/120    avg_loss:0.263, val_acc:0.954]
Epoch [28/120    avg_loss:0.225, val_acc:0.929]
Epoch [29/120    avg_loss:0.215, val_acc:0.935]
Epoch [30/120    avg_loss:0.223, val_acc:0.938]
Epoch [31/120    avg_loss:0.178, val_acc:0.960]
Epoch [32/120    avg_loss:0.190, val_acc:0.971]
Epoch [33/120    avg_loss:0.166, val_acc:0.960]
Epoch [34/120    avg_loss:0.168, val_acc:0.975]
Epoch [35/120    avg_loss:0.149, val_acc:0.977]
Epoch [36/120    avg_loss:0.108, val_acc:0.971]
Epoch [37/120    avg_loss:0.127, val_acc:0.902]
Epoch [38/120    avg_loss:0.170, val_acc:0.981]
Epoch [39/120    avg_loss:0.136, val_acc:0.979]
Epoch [40/120    avg_loss:0.140, val_acc:0.975]
Epoch [41/120    avg_loss:0.127, val_acc:0.981]
Epoch [42/120    avg_loss:0.115, val_acc:0.967]
Epoch [43/120    avg_loss:0.109, val_acc:0.975]
Epoch [44/120    avg_loss:0.136, val_acc:0.977]
Epoch [45/120    avg_loss:0.132, val_acc:0.979]
Epoch [46/120    avg_loss:0.133, val_acc:0.952]
Epoch [47/120    avg_loss:0.091, val_acc:0.975]
Epoch [48/120    avg_loss:0.085, val_acc:0.977]
Epoch [49/120    avg_loss:0.103, val_acc:0.977]
Epoch [50/120    avg_loss:0.108, val_acc:0.985]
Epoch [51/120    avg_loss:0.103, val_acc:0.979]
Epoch [52/120    avg_loss:0.101, val_acc:0.985]
Epoch [53/120    avg_loss:0.088, val_acc:0.983]
Epoch [54/120    avg_loss:0.096, val_acc:0.983]
Epoch [55/120    avg_loss:0.064, val_acc:0.979]
Epoch [56/120    avg_loss:0.074, val_acc:0.981]
Epoch [57/120    avg_loss:0.063, val_acc:0.977]
Epoch [58/120    avg_loss:0.058, val_acc:0.992]
Epoch [59/120    avg_loss:0.046, val_acc:0.988]
Epoch [60/120    avg_loss:0.043, val_acc:0.994]
Epoch [61/120    avg_loss:0.050, val_acc:0.990]
Epoch [62/120    avg_loss:0.046, val_acc:0.996]
Epoch [63/120    avg_loss:0.039, val_acc:0.992]
Epoch [64/120    avg_loss:0.054, val_acc:0.988]
Epoch [65/120    avg_loss:0.055, val_acc:0.988]
Epoch [66/120    avg_loss:0.047, val_acc:0.992]
Epoch [67/120    avg_loss:0.028, val_acc:0.994]
Epoch [68/120    avg_loss:0.032, val_acc:0.994]
Epoch [69/120    avg_loss:0.039, val_acc:0.992]
Epoch [70/120    avg_loss:0.047, val_acc:0.967]
Epoch [71/120    avg_loss:0.034, val_acc:0.994]
Epoch [72/120    avg_loss:0.030, val_acc:0.994]
Epoch [73/120    avg_loss:0.031, val_acc:0.990]
Epoch [74/120    avg_loss:0.035, val_acc:0.990]
Epoch [75/120    avg_loss:0.086, val_acc:0.960]
Epoch [76/120    avg_loss:0.070, val_acc:0.988]
Epoch [77/120    avg_loss:0.037, val_acc:0.994]
Epoch [78/120    avg_loss:0.022, val_acc:0.994]
Epoch [79/120    avg_loss:0.026, val_acc:0.994]
Epoch [80/120    avg_loss:0.019, val_acc:0.994]
Epoch [81/120    avg_loss:0.020, val_acc:0.994]
Epoch [82/120    avg_loss:0.017, val_acc:0.994]
Epoch [83/120    avg_loss:0.025, val_acc:0.994]
Epoch [84/120    avg_loss:0.023, val_acc:0.994]
Epoch [85/120    avg_loss:0.019, val_acc:0.994]
Epoch [86/120    avg_loss:0.018, val_acc:0.994]
Epoch [87/120    avg_loss:0.016, val_acc:0.994]
Epoch [88/120    avg_loss:0.024, val_acc:0.994]
Epoch [89/120    avg_loss:0.028, val_acc:0.994]
Epoch [90/120    avg_loss:0.020, val_acc:0.994]
Epoch [91/120    avg_loss:0.016, val_acc:0.994]
Epoch [92/120    avg_loss:0.019, val_acc:0.994]
Epoch [93/120    avg_loss:0.017, val_acc:0.994]
Epoch [94/120    avg_loss:0.018, val_acc:0.994]
Epoch [95/120    avg_loss:0.018, val_acc:0.994]
Epoch [96/120    avg_loss:0.018, val_acc:0.994]
Epoch [97/120    avg_loss:0.022, val_acc:0.994]
Epoch [98/120    avg_loss:0.016, val_acc:0.994]
Epoch [99/120    avg_loss:0.021, val_acc:0.994]
Epoch [100/120    avg_loss:0.019, val_acc:0.994]
Epoch [101/120    avg_loss:0.013, val_acc:0.994]
Epoch [102/120    avg_loss:0.021, val_acc:0.994]
Epoch [103/120    avg_loss:0.022, val_acc:0.994]
Epoch [104/120    avg_loss:0.019, val_acc:0.994]
Epoch [105/120    avg_loss:0.014, val_acc:0.994]
Epoch [106/120    avg_loss:0.021, val_acc:0.994]
Epoch [107/120    avg_loss:0.018, val_acc:0.994]
Epoch [108/120    avg_loss:0.014, val_acc:0.994]
Epoch [109/120    avg_loss:0.020, val_acc:0.994]
Epoch [110/120    avg_loss:0.021, val_acc:0.994]
Epoch [111/120    avg_loss:0.015, val_acc:0.994]
Epoch [112/120    avg_loss:0.018, val_acc:0.994]
Epoch [113/120    avg_loss:0.020, val_acc:0.994]
Epoch [114/120    avg_loss:0.014, val_acc:0.994]
Epoch [115/120    avg_loss:0.018, val_acc:0.994]
Epoch [116/120    avg_loss:0.016, val_acc:0.994]
Epoch [117/120    avg_loss:0.014, val_acc:0.994]
Epoch [118/120    avg_loss:0.017, val_acc:0.994]
Epoch [119/120    avg_loss:0.015, val_acc:0.994]
Epoch [120/120    avg_loss:0.015, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   1   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99560117 0.98426966 1.         0.97977528 0.96666667
 0.98800959 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9947783474800744
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa62657e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.467, val_acc:0.463]
Epoch [2/120    avg_loss:2.034, val_acc:0.658]
Epoch [3/120    avg_loss:1.692, val_acc:0.625]
Epoch [4/120    avg_loss:1.429, val_acc:0.702]
Epoch [5/120    avg_loss:1.175, val_acc:0.719]
Epoch [6/120    avg_loss:0.990, val_acc:0.744]
Epoch [7/120    avg_loss:0.874, val_acc:0.735]
Epoch [8/120    avg_loss:0.777, val_acc:0.754]
Epoch [9/120    avg_loss:0.716, val_acc:0.804]
Epoch [10/120    avg_loss:0.615, val_acc:0.833]
Epoch [11/120    avg_loss:0.577, val_acc:0.850]
Epoch [12/120    avg_loss:0.518, val_acc:0.867]
Epoch [13/120    avg_loss:0.484, val_acc:0.915]
Epoch [14/120    avg_loss:0.386, val_acc:0.896]
Epoch [15/120    avg_loss:0.363, val_acc:0.908]
Epoch [16/120    avg_loss:0.385, val_acc:0.925]
Epoch [17/120    avg_loss:0.339, val_acc:0.921]
Epoch [18/120    avg_loss:0.312, val_acc:0.896]
Epoch [19/120    avg_loss:0.302, val_acc:0.929]
Epoch [20/120    avg_loss:0.278, val_acc:0.915]
Epoch [21/120    avg_loss:0.301, val_acc:0.925]
Epoch [22/120    avg_loss:0.239, val_acc:0.946]
Epoch [23/120    avg_loss:0.221, val_acc:0.917]
Epoch [24/120    avg_loss:0.275, val_acc:0.850]
Epoch [25/120    avg_loss:0.343, val_acc:0.921]
Epoch [26/120    avg_loss:0.240, val_acc:0.942]
Epoch [27/120    avg_loss:0.226, val_acc:0.946]
Epoch [28/120    avg_loss:0.201, val_acc:0.948]
Epoch [29/120    avg_loss:0.213, val_acc:0.950]
Epoch [30/120    avg_loss:0.188, val_acc:0.952]
Epoch [31/120    avg_loss:0.241, val_acc:0.958]
Epoch [32/120    avg_loss:0.199, val_acc:0.958]
Epoch [33/120    avg_loss:0.182, val_acc:0.960]
Epoch [34/120    avg_loss:0.155, val_acc:0.956]
Epoch [35/120    avg_loss:0.155, val_acc:0.963]
Epoch [36/120    avg_loss:0.160, val_acc:0.985]
Epoch [37/120    avg_loss:0.113, val_acc:0.973]
Epoch [38/120    avg_loss:0.160, val_acc:0.965]
Epoch [39/120    avg_loss:0.138, val_acc:0.977]
Epoch [40/120    avg_loss:0.090, val_acc:0.983]
Epoch [41/120    avg_loss:0.095, val_acc:0.977]
Epoch [42/120    avg_loss:0.139, val_acc:0.950]
Epoch [43/120    avg_loss:0.154, val_acc:0.985]
Epoch [44/120    avg_loss:0.144, val_acc:0.969]
Epoch [45/120    avg_loss:0.136, val_acc:0.975]
Epoch [46/120    avg_loss:0.102, val_acc:0.977]
Epoch [47/120    avg_loss:0.102, val_acc:0.973]
Epoch [48/120    avg_loss:0.165, val_acc:0.958]
Epoch [49/120    avg_loss:0.150, val_acc:0.973]
Epoch [50/120    avg_loss:0.149, val_acc:0.981]
Epoch [51/120    avg_loss:0.075, val_acc:0.983]
Epoch [52/120    avg_loss:0.081, val_acc:0.981]
Epoch [53/120    avg_loss:0.059, val_acc:0.983]
Epoch [54/120    avg_loss:0.067, val_acc:0.992]
Epoch [55/120    avg_loss:0.074, val_acc:0.992]
Epoch [56/120    avg_loss:0.067, val_acc:0.979]
Epoch [57/120    avg_loss:0.118, val_acc:0.979]
Epoch [58/120    avg_loss:0.133, val_acc:0.946]
Epoch [59/120    avg_loss:0.080, val_acc:0.985]
Epoch [60/120    avg_loss:0.052, val_acc:0.992]
Epoch [61/120    avg_loss:0.052, val_acc:0.990]
Epoch [62/120    avg_loss:0.035, val_acc:0.988]
Epoch [63/120    avg_loss:0.034, val_acc:0.990]
Epoch [64/120    avg_loss:0.059, val_acc:0.990]
Epoch [65/120    avg_loss:0.030, val_acc:0.985]
Epoch [66/120    avg_loss:0.030, val_acc:0.988]
Epoch [67/120    avg_loss:0.036, val_acc:0.983]
Epoch [68/120    avg_loss:0.046, val_acc:0.988]
Epoch [69/120    avg_loss:0.027, val_acc:0.988]
Epoch [70/120    avg_loss:0.024, val_acc:0.985]
Epoch [71/120    avg_loss:0.022, val_acc:0.990]
Epoch [72/120    avg_loss:0.026, val_acc:0.981]
Epoch [73/120    avg_loss:0.026, val_acc:0.990]
Epoch [74/120    avg_loss:0.022, val_acc:0.994]
Epoch [75/120    avg_loss:0.016, val_acc:0.990]
Epoch [76/120    avg_loss:0.015, val_acc:0.992]
Epoch [77/120    avg_loss:0.015, val_acc:0.992]
Epoch [78/120    avg_loss:0.016, val_acc:0.992]
Epoch [79/120    avg_loss:0.019, val_acc:0.992]
Epoch [80/120    avg_loss:0.013, val_acc:0.992]
Epoch [81/120    avg_loss:0.017, val_acc:0.992]
Epoch [82/120    avg_loss:0.017, val_acc:0.988]
Epoch [83/120    avg_loss:0.012, val_acc:0.988]
Epoch [84/120    avg_loss:0.016, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.990]
Epoch [87/120    avg_loss:0.016, val_acc:0.990]
Epoch [88/120    avg_loss:0.014, val_acc:0.990]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.990]
Epoch [91/120    avg_loss:0.014, val_acc:0.990]
Epoch [92/120    avg_loss:0.012, val_acc:0.990]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.016, val_acc:0.990]
Epoch [95/120    avg_loss:0.015, val_acc:0.990]
Epoch [96/120    avg_loss:0.012, val_acc:0.990]
Epoch [97/120    avg_loss:0.017, val_acc:0.990]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.015, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.990]
Epoch [102/120    avg_loss:0.014, val_acc:0.990]
Epoch [103/120    avg_loss:0.014, val_acc:0.990]
Epoch [104/120    avg_loss:0.012, val_acc:0.990]
Epoch [105/120    avg_loss:0.023, val_acc:0.990]
Epoch [106/120    avg_loss:0.013, val_acc:0.990]
Epoch [107/120    avg_loss:0.017, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.990]
Epoch [109/120    avg_loss:0.016, val_acc:0.990]
Epoch [110/120    avg_loss:0.013, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.017, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.014, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.015, val_acc:0.990]
Epoch [117/120    avg_loss:0.015, val_acc:0.990]
Epoch [118/120    avg_loss:0.013, val_acc:0.990]
Epoch [119/120    avg_loss:0.014, val_acc:0.990]
Epoch [120/120    avg_loss:0.020, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.82942430703625

F1 scores:
[       nan 0.99926954 1.         1.         0.98454746 0.97260274
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9981010081460273
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f78832db860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.402, val_acc:0.417]
Epoch [2/120    avg_loss:1.994, val_acc:0.550]
Epoch [3/120    avg_loss:1.671, val_acc:0.610]
Epoch [4/120    avg_loss:1.382, val_acc:0.690]
Epoch [5/120    avg_loss:1.225, val_acc:0.733]
Epoch [6/120    avg_loss:1.061, val_acc:0.715]
Epoch [7/120    avg_loss:0.928, val_acc:0.762]
Epoch [8/120    avg_loss:0.788, val_acc:0.823]
Epoch [9/120    avg_loss:0.736, val_acc:0.835]
Epoch [10/120    avg_loss:0.732, val_acc:0.806]
Epoch [11/120    avg_loss:0.619, val_acc:0.871]
Epoch [12/120    avg_loss:0.513, val_acc:0.852]
Epoch [13/120    avg_loss:0.505, val_acc:0.917]
Epoch [14/120    avg_loss:0.456, val_acc:0.892]
Epoch [15/120    avg_loss:0.441, val_acc:0.935]
Epoch [16/120    avg_loss:0.389, val_acc:0.940]
Epoch [17/120    avg_loss:0.373, val_acc:0.867]
Epoch [18/120    avg_loss:0.345, val_acc:0.944]
Epoch [19/120    avg_loss:0.336, val_acc:0.906]
Epoch [20/120    avg_loss:0.314, val_acc:0.948]
Epoch [21/120    avg_loss:0.336, val_acc:0.942]
Epoch [22/120    avg_loss:0.308, val_acc:0.915]
Epoch [23/120    avg_loss:0.337, val_acc:0.965]
Epoch [24/120    avg_loss:0.278, val_acc:0.954]
Epoch [25/120    avg_loss:0.306, val_acc:0.954]
Epoch [26/120    avg_loss:0.329, val_acc:0.956]
Epoch [27/120    avg_loss:0.225, val_acc:0.948]
Epoch [28/120    avg_loss:0.252, val_acc:0.971]
Epoch [29/120    avg_loss:0.209, val_acc:0.979]
Epoch [30/120    avg_loss:0.209, val_acc:0.969]
Epoch [31/120    avg_loss:0.175, val_acc:0.960]
Epoch [32/120    avg_loss:0.198, val_acc:0.981]
Epoch [33/120    avg_loss:0.213, val_acc:0.971]
Epoch [34/120    avg_loss:0.197, val_acc:0.950]
Epoch [35/120    avg_loss:0.210, val_acc:0.977]
Epoch [36/120    avg_loss:0.194, val_acc:0.983]
Epoch [37/120    avg_loss:0.150, val_acc:0.983]
Epoch [38/120    avg_loss:0.122, val_acc:0.979]
Epoch [39/120    avg_loss:0.135, val_acc:0.979]
Epoch [40/120    avg_loss:0.103, val_acc:0.981]
Epoch [41/120    avg_loss:0.100, val_acc:0.981]
Epoch [42/120    avg_loss:0.093, val_acc:0.985]
Epoch [43/120    avg_loss:0.079, val_acc:0.973]
Epoch [44/120    avg_loss:0.089, val_acc:0.979]
Epoch [45/120    avg_loss:0.109, val_acc:0.965]
Epoch [46/120    avg_loss:0.144, val_acc:0.973]
Epoch [47/120    avg_loss:0.122, val_acc:0.956]
Epoch [48/120    avg_loss:0.111, val_acc:0.985]
Epoch [49/120    avg_loss:0.077, val_acc:0.992]
Epoch [50/120    avg_loss:0.076, val_acc:0.990]
Epoch [51/120    avg_loss:0.077, val_acc:0.985]
Epoch [52/120    avg_loss:0.061, val_acc:0.985]
Epoch [53/120    avg_loss:0.110, val_acc:0.967]
Epoch [54/120    avg_loss:0.085, val_acc:0.985]
Epoch [55/120    avg_loss:0.093, val_acc:0.985]
Epoch [56/120    avg_loss:0.070, val_acc:0.985]
Epoch [57/120    avg_loss:0.066, val_acc:0.990]
Epoch [58/120    avg_loss:0.059, val_acc:0.992]
Epoch [59/120    avg_loss:0.046, val_acc:0.985]
Epoch [60/120    avg_loss:0.082, val_acc:0.985]
Epoch [61/120    avg_loss:0.088, val_acc:0.994]
Epoch [62/120    avg_loss:0.073, val_acc:0.990]
Epoch [63/120    avg_loss:0.105, val_acc:0.981]
Epoch [64/120    avg_loss:0.105, val_acc:0.965]
Epoch [65/120    avg_loss:0.077, val_acc:0.985]
Epoch [66/120    avg_loss:0.060, val_acc:0.994]
Epoch [67/120    avg_loss:0.033, val_acc:0.994]
Epoch [68/120    avg_loss:0.033, val_acc:0.992]
Epoch [69/120    avg_loss:0.044, val_acc:0.994]
Epoch [70/120    avg_loss:0.044, val_acc:0.990]
Epoch [71/120    avg_loss:0.056, val_acc:0.990]
Epoch [72/120    avg_loss:0.045, val_acc:0.990]
Epoch [73/120    avg_loss:0.056, val_acc:0.981]
Epoch [74/120    avg_loss:0.060, val_acc:0.992]
Epoch [75/120    avg_loss:0.040, val_acc:1.000]
Epoch [76/120    avg_loss:0.029, val_acc:0.996]
Epoch [77/120    avg_loss:0.047, val_acc:0.996]
Epoch [78/120    avg_loss:0.025, val_acc:0.996]
Epoch [79/120    avg_loss:0.024, val_acc:0.996]
Epoch [80/120    avg_loss:0.034, val_acc:0.992]
Epoch [81/120    avg_loss:0.038, val_acc:0.996]
Epoch [82/120    avg_loss:0.030, val_acc:0.998]
Epoch [83/120    avg_loss:0.029, val_acc:0.990]
Epoch [84/120    avg_loss:0.029, val_acc:0.994]
Epoch [85/120    avg_loss:0.018, val_acc:0.994]
Epoch [86/120    avg_loss:0.023, val_acc:0.996]
Epoch [87/120    avg_loss:0.059, val_acc:0.990]
Epoch [88/120    avg_loss:0.027, val_acc:0.996]
Epoch [89/120    avg_loss:0.019, val_acc:0.996]
Epoch [90/120    avg_loss:0.019, val_acc:0.996]
Epoch [91/120    avg_loss:0.017, val_acc:0.994]
Epoch [92/120    avg_loss:0.015, val_acc:0.996]
Epoch [93/120    avg_loss:0.025, val_acc:0.996]
Epoch [94/120    avg_loss:0.016, val_acc:0.996]
Epoch [95/120    avg_loss:0.018, val_acc:0.996]
Epoch [96/120    avg_loss:0.017, val_acc:0.996]
Epoch [97/120    avg_loss:0.020, val_acc:0.994]
Epoch [98/120    avg_loss:0.019, val_acc:0.996]
Epoch [99/120    avg_loss:0.015, val_acc:0.996]
Epoch [100/120    avg_loss:0.020, val_acc:0.996]
Epoch [101/120    avg_loss:0.016, val_acc:0.998]
Epoch [102/120    avg_loss:0.013, val_acc:0.998]
Epoch [103/120    avg_loss:0.013, val_acc:0.998]
Epoch [104/120    avg_loss:0.012, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.021, val_acc:0.996]
Epoch [108/120    avg_loss:0.013, val_acc:0.996]
Epoch [109/120    avg_loss:0.016, val_acc:0.996]
Epoch [110/120    avg_loss:0.013, val_acc:0.996]
Epoch [111/120    avg_loss:0.014, val_acc:0.996]
Epoch [112/120    avg_loss:0.015, val_acc:0.996]
Epoch [113/120    avg_loss:0.015, val_acc:0.996]
Epoch [114/120    avg_loss:0.016, val_acc:0.996]
Epoch [115/120    avg_loss:0.015, val_acc:0.996]
Epoch [116/120    avg_loss:0.014, val_acc:0.996]
Epoch [117/120    avg_loss:0.014, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.017, val_acc:0.996]
Epoch [120/120    avg_loss:0.015, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   5   0   0   0   0   0   0   2   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.98434004 0.98305085
 0.99038462 0.99465241 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.997151674689264
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc89467d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.464, val_acc:0.406]
Epoch [2/120    avg_loss:2.015, val_acc:0.602]
Epoch [3/120    avg_loss:1.692, val_acc:0.640]
Epoch [4/120    avg_loss:1.446, val_acc:0.669]
Epoch [5/120    avg_loss:1.220, val_acc:0.725]
Epoch [6/120    avg_loss:1.033, val_acc:0.754]
Epoch [7/120    avg_loss:0.854, val_acc:0.798]
Epoch [8/120    avg_loss:0.789, val_acc:0.781]
Epoch [9/120    avg_loss:0.704, val_acc:0.804]
Epoch [10/120    avg_loss:0.631, val_acc:0.865]
Epoch [11/120    avg_loss:0.577, val_acc:0.877]
Epoch [12/120    avg_loss:0.564, val_acc:0.892]
Epoch [13/120    avg_loss:0.528, val_acc:0.879]
Epoch [14/120    avg_loss:0.431, val_acc:0.917]
Epoch [15/120    avg_loss:0.393, val_acc:0.898]
Epoch [16/120    avg_loss:0.378, val_acc:0.885]
Epoch [17/120    avg_loss:0.394, val_acc:0.929]
Epoch [18/120    avg_loss:0.370, val_acc:0.910]
Epoch [19/120    avg_loss:0.404, val_acc:0.915]
Epoch [20/120    avg_loss:0.328, val_acc:0.946]
Epoch [21/120    avg_loss:0.305, val_acc:0.946]
Epoch [22/120    avg_loss:0.302, val_acc:0.948]
Epoch [23/120    avg_loss:0.308, val_acc:0.938]
Epoch [24/120    avg_loss:0.251, val_acc:0.954]
Epoch [25/120    avg_loss:0.190, val_acc:0.950]
Epoch [26/120    avg_loss:0.238, val_acc:0.942]
Epoch [27/120    avg_loss:0.258, val_acc:0.938]
Epoch [28/120    avg_loss:0.212, val_acc:0.954]
Epoch [29/120    avg_loss:0.192, val_acc:0.948]
Epoch [30/120    avg_loss:0.194, val_acc:0.956]
Epoch [31/120    avg_loss:0.164, val_acc:0.969]
Epoch [32/120    avg_loss:0.193, val_acc:0.894]
Epoch [33/120    avg_loss:0.192, val_acc:0.946]
Epoch [34/120    avg_loss:0.178, val_acc:0.969]
Epoch [35/120    avg_loss:0.143, val_acc:0.973]
Epoch [36/120    avg_loss:0.132, val_acc:0.975]
Epoch [37/120    avg_loss:0.105, val_acc:0.981]
Epoch [38/120    avg_loss:0.097, val_acc:0.960]
Epoch [39/120    avg_loss:0.119, val_acc:0.981]
Epoch [40/120    avg_loss:0.159, val_acc:0.963]
Epoch [41/120    avg_loss:0.136, val_acc:0.977]
Epoch [42/120    avg_loss:0.143, val_acc:0.973]
Epoch [43/120    avg_loss:0.102, val_acc:0.985]
Epoch [44/120    avg_loss:0.052, val_acc:0.992]
Epoch [45/120    avg_loss:0.068, val_acc:0.990]
Epoch [46/120    avg_loss:0.069, val_acc:0.977]
Epoch [47/120    avg_loss:0.089, val_acc:0.990]
Epoch [48/120    avg_loss:0.115, val_acc:0.983]
Epoch [49/120    avg_loss:0.146, val_acc:0.975]
Epoch [50/120    avg_loss:0.212, val_acc:0.967]
Epoch [51/120    avg_loss:0.157, val_acc:0.979]
Epoch [52/120    avg_loss:0.113, val_acc:0.983]
Epoch [53/120    avg_loss:0.092, val_acc:0.981]
Epoch [54/120    avg_loss:0.060, val_acc:0.988]
Epoch [55/120    avg_loss:0.054, val_acc:0.992]
Epoch [56/120    avg_loss:0.048, val_acc:0.983]
Epoch [57/120    avg_loss:0.043, val_acc:0.985]
Epoch [58/120    avg_loss:0.031, val_acc:0.992]
Epoch [59/120    avg_loss:0.038, val_acc:0.990]
Epoch [60/120    avg_loss:0.044, val_acc:0.992]
Epoch [61/120    avg_loss:0.039, val_acc:0.992]
Epoch [62/120    avg_loss:0.062, val_acc:0.983]
Epoch [63/120    avg_loss:0.078, val_acc:0.990]
Epoch [64/120    avg_loss:0.048, val_acc:0.990]
Epoch [65/120    avg_loss:0.047, val_acc:0.988]
Epoch [66/120    avg_loss:0.047, val_acc:0.990]
Epoch [67/120    avg_loss:0.045, val_acc:0.985]
Epoch [68/120    avg_loss:0.056, val_acc:0.988]
Epoch [69/120    avg_loss:0.048, val_acc:0.996]
Epoch [70/120    avg_loss:0.027, val_acc:0.996]
Epoch [71/120    avg_loss:0.047, val_acc:0.988]
Epoch [72/120    avg_loss:0.051, val_acc:0.960]
Epoch [73/120    avg_loss:0.092, val_acc:0.981]
Epoch [74/120    avg_loss:0.039, val_acc:0.990]
Epoch [75/120    avg_loss:0.029, val_acc:0.988]
Epoch [76/120    avg_loss:0.024, val_acc:0.992]
Epoch [77/120    avg_loss:0.019, val_acc:0.992]
Epoch [78/120    avg_loss:0.031, val_acc:0.994]
Epoch [79/120    avg_loss:0.022, val_acc:0.994]
Epoch [80/120    avg_loss:0.022, val_acc:0.996]
Epoch [81/120    avg_loss:0.027, val_acc:0.996]
Epoch [82/120    avg_loss:0.016, val_acc:0.998]
Epoch [83/120    avg_loss:0.019, val_acc:0.996]
Epoch [84/120    avg_loss:0.015, val_acc:0.996]
Epoch [85/120    avg_loss:0.014, val_acc:0.994]
Epoch [86/120    avg_loss:0.010, val_acc:0.996]
Epoch [87/120    avg_loss:0.014, val_acc:0.992]
Epoch [88/120    avg_loss:0.011, val_acc:1.000]
Epoch [89/120    avg_loss:0.010, val_acc:0.994]
Epoch [90/120    avg_loss:0.013, val_acc:0.992]
Epoch [91/120    avg_loss:0.018, val_acc:0.994]
Epoch [92/120    avg_loss:0.015, val_acc:0.994]
Epoch [93/120    avg_loss:0.020, val_acc:0.990]
Epoch [94/120    avg_loss:0.017, val_acc:0.994]
Epoch [95/120    avg_loss:0.009, val_acc:0.996]
Epoch [96/120    avg_loss:0.009, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.996]
Epoch [98/120    avg_loss:0.008, val_acc:0.996]
Epoch [99/120    avg_loss:0.009, val_acc:0.998]
Epoch [100/120    avg_loss:0.014, val_acc:0.994]
Epoch [101/120    avg_loss:0.015, val_acc:0.998]
Epoch [102/120    avg_loss:0.008, val_acc:0.998]
Epoch [103/120    avg_loss:0.008, val_acc:0.998]
Epoch [104/120    avg_loss:0.008, val_acc:0.998]
Epoch [105/120    avg_loss:0.007, val_acc:0.998]
Epoch [106/120    avg_loss:0.009, val_acc:0.998]
Epoch [107/120    avg_loss:0.006, val_acc:0.998]
Epoch [108/120    avg_loss:0.008, val_acc:0.998]
Epoch [109/120    avg_loss:0.006, val_acc:0.998]
Epoch [110/120    avg_loss:0.006, val_acc:0.998]
Epoch [111/120    avg_loss:0.008, val_acc:0.998]
Epoch [112/120    avg_loss:0.011, val_acc:0.998]
Epoch [113/120    avg_loss:0.007, val_acc:0.998]
Epoch [114/120    avg_loss:0.008, val_acc:0.998]
Epoch [115/120    avg_loss:0.007, val_acc:0.998]
Epoch [116/120    avg_loss:0.009, val_acc:0.998]
Epoch [117/120    avg_loss:0.007, val_acc:0.998]
Epoch [118/120    avg_loss:0.009, val_acc:0.998]
Epoch [119/120    avg_loss:0.005, val_acc:0.998]
Epoch [120/120    avg_loss:0.010, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   1   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.68017057569296

F1 scores:
[       nan 0.99264706 1.         1.         0.98886414 0.98305085
 0.97862233 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.99644017060261
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f287f6837b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.433, val_acc:0.340]
Epoch [2/120    avg_loss:2.043, val_acc:0.523]
Epoch [3/120    avg_loss:1.774, val_acc:0.621]
Epoch [4/120    avg_loss:1.474, val_acc:0.648]
Epoch [5/120    avg_loss:1.271, val_acc:0.690]
Epoch [6/120    avg_loss:1.109, val_acc:0.746]
Epoch [7/120    avg_loss:0.984, val_acc:0.760]
Epoch [8/120    avg_loss:0.872, val_acc:0.783]
Epoch [9/120    avg_loss:0.770, val_acc:0.819]
Epoch [10/120    avg_loss:0.723, val_acc:0.810]
Epoch [11/120    avg_loss:0.671, val_acc:0.856]
Epoch [12/120    avg_loss:0.595, val_acc:0.904]
Epoch [13/120    avg_loss:0.527, val_acc:0.817]
Epoch [14/120    avg_loss:0.465, val_acc:0.863]
Epoch [15/120    avg_loss:0.446, val_acc:0.927]
Epoch [16/120    avg_loss:0.382, val_acc:0.944]
Epoch [17/120    avg_loss:0.374, val_acc:0.948]
Epoch [18/120    avg_loss:0.343, val_acc:0.956]
Epoch [19/120    avg_loss:0.304, val_acc:0.950]
Epoch [20/120    avg_loss:0.283, val_acc:0.969]
Epoch [21/120    avg_loss:0.278, val_acc:0.954]
Epoch [22/120    avg_loss:0.238, val_acc:0.950]
Epoch [23/120    avg_loss:0.215, val_acc:0.948]
Epoch [24/120    avg_loss:0.263, val_acc:0.973]
Epoch [25/120    avg_loss:0.247, val_acc:0.956]
Epoch [26/120    avg_loss:0.192, val_acc:0.977]
Epoch [27/120    avg_loss:0.214, val_acc:0.967]
Epoch [28/120    avg_loss:0.195, val_acc:0.956]
Epoch [29/120    avg_loss:0.200, val_acc:0.963]
Epoch [30/120    avg_loss:0.158, val_acc:0.952]
Epoch [31/120    avg_loss:0.173, val_acc:0.975]
Epoch [32/120    avg_loss:0.183, val_acc:0.973]
Epoch [33/120    avg_loss:0.282, val_acc:0.906]
Epoch [34/120    avg_loss:0.222, val_acc:0.944]
Epoch [35/120    avg_loss:0.203, val_acc:0.977]
Epoch [36/120    avg_loss:0.173, val_acc:0.977]
Epoch [37/120    avg_loss:0.162, val_acc:0.960]
Epoch [38/120    avg_loss:0.130, val_acc:0.988]
Epoch [39/120    avg_loss:0.113, val_acc:0.956]
Epoch [40/120    avg_loss:0.098, val_acc:0.983]
Epoch [41/120    avg_loss:0.134, val_acc:0.979]
Epoch [42/120    avg_loss:0.153, val_acc:0.960]
Epoch [43/120    avg_loss:0.178, val_acc:0.960]
Epoch [44/120    avg_loss:0.103, val_acc:0.994]
Epoch [45/120    avg_loss:0.102, val_acc:0.988]
Epoch [46/120    avg_loss:0.141, val_acc:0.983]
Epoch [47/120    avg_loss:0.105, val_acc:0.933]
Epoch [48/120    avg_loss:0.111, val_acc:0.973]
Epoch [49/120    avg_loss:0.068, val_acc:0.983]
Epoch [50/120    avg_loss:0.056, val_acc:0.994]
Epoch [51/120    avg_loss:0.054, val_acc:0.990]
Epoch [52/120    avg_loss:0.064, val_acc:0.979]
Epoch [53/120    avg_loss:0.107, val_acc:0.990]
Epoch [54/120    avg_loss:0.079, val_acc:0.992]
Epoch [55/120    avg_loss:0.081, val_acc:0.944]
Epoch [56/120    avg_loss:0.058, val_acc:0.994]
Epoch [57/120    avg_loss:0.063, val_acc:0.990]
Epoch [58/120    avg_loss:0.063, val_acc:0.981]
Epoch [59/120    avg_loss:0.101, val_acc:0.988]
Epoch [60/120    avg_loss:0.070, val_acc:0.983]
Epoch [61/120    avg_loss:0.052, val_acc:0.985]
Epoch [62/120    avg_loss:0.044, val_acc:0.983]
Epoch [63/120    avg_loss:0.046, val_acc:0.990]
Epoch [64/120    avg_loss:0.091, val_acc:0.990]
Epoch [65/120    avg_loss:0.080, val_acc:0.967]
Epoch [66/120    avg_loss:0.075, val_acc:0.958]
Epoch [67/120    avg_loss:0.089, val_acc:0.992]
Epoch [68/120    avg_loss:0.084, val_acc:0.954]
Epoch [69/120    avg_loss:0.109, val_acc:0.977]
Epoch [70/120    avg_loss:0.067, val_acc:0.985]
Epoch [71/120    avg_loss:0.064, val_acc:0.994]
Epoch [72/120    avg_loss:0.056, val_acc:0.994]
Epoch [73/120    avg_loss:0.039, val_acc:0.992]
Epoch [74/120    avg_loss:0.036, val_acc:0.994]
Epoch [75/120    avg_loss:0.047, val_acc:0.992]
Epoch [76/120    avg_loss:0.030, val_acc:0.996]
Epoch [77/120    avg_loss:0.031, val_acc:0.996]
Epoch [78/120    avg_loss:0.035, val_acc:0.996]
Epoch [79/120    avg_loss:0.041, val_acc:0.996]
Epoch [80/120    avg_loss:0.035, val_acc:0.996]
Epoch [81/120    avg_loss:0.028, val_acc:0.994]
Epoch [82/120    avg_loss:0.029, val_acc:0.994]
Epoch [83/120    avg_loss:0.027, val_acc:0.996]
Epoch [84/120    avg_loss:0.024, val_acc:0.994]
Epoch [85/120    avg_loss:0.020, val_acc:0.994]
Epoch [86/120    avg_loss:0.035, val_acc:0.994]
Epoch [87/120    avg_loss:0.027, val_acc:0.996]
Epoch [88/120    avg_loss:0.027, val_acc:0.996]
Epoch [89/120    avg_loss:0.031, val_acc:0.996]
Epoch [90/120    avg_loss:0.023, val_acc:0.996]
Epoch [91/120    avg_loss:0.026, val_acc:0.994]
Epoch [92/120    avg_loss:0.027, val_acc:0.996]
Epoch [93/120    avg_loss:0.029, val_acc:0.994]
Epoch [94/120    avg_loss:0.023, val_acc:0.996]
Epoch [95/120    avg_loss:0.037, val_acc:0.994]
Epoch [96/120    avg_loss:0.024, val_acc:0.994]
Epoch [97/120    avg_loss:0.026, val_acc:0.994]
Epoch [98/120    avg_loss:0.043, val_acc:0.996]
Epoch [99/120    avg_loss:0.028, val_acc:0.996]
Epoch [100/120    avg_loss:0.023, val_acc:0.996]
Epoch [101/120    avg_loss:0.024, val_acc:0.996]
Epoch [102/120    avg_loss:0.027, val_acc:0.996]
Epoch [103/120    avg_loss:0.018, val_acc:0.996]
Epoch [104/120    avg_loss:0.021, val_acc:0.994]
Epoch [105/120    avg_loss:0.026, val_acc:0.994]
Epoch [106/120    avg_loss:0.025, val_acc:0.994]
Epoch [107/120    avg_loss:0.019, val_acc:0.994]
Epoch [108/120    avg_loss:0.037, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.994]
Epoch [110/120    avg_loss:0.028, val_acc:0.994]
Epoch [111/120    avg_loss:0.023, val_acc:0.994]
Epoch [112/120    avg_loss:0.030, val_acc:0.994]
Epoch [113/120    avg_loss:0.026, val_acc:0.994]
Epoch [114/120    avg_loss:0.032, val_acc:0.994]
Epoch [115/120    avg_loss:0.021, val_acc:0.996]
Epoch [116/120    avg_loss:0.020, val_acc:0.994]
Epoch [117/120    avg_loss:0.018, val_acc:0.994]
Epoch [118/120    avg_loss:0.024, val_acc:0.994]
Epoch [119/120    avg_loss:0.023, val_acc:0.994]
Epoch [120/120    avg_loss:0.022, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   3   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.95196507 0.91349481
 0.99757869 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9935912346431043
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04687f4780>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.462, val_acc:0.510]
Epoch [2/120    avg_loss:2.020, val_acc:0.621]
Epoch [3/120    avg_loss:1.743, val_acc:0.621]
Epoch [4/120    avg_loss:1.471, val_acc:0.713]
Epoch [5/120    avg_loss:1.340, val_acc:0.713]
Epoch [6/120    avg_loss:1.156, val_acc:0.733]
Epoch [7/120    avg_loss:1.021, val_acc:0.777]
Epoch [8/120    avg_loss:0.884, val_acc:0.787]
Epoch [9/120    avg_loss:0.777, val_acc:0.846]
Epoch [10/120    avg_loss:0.698, val_acc:0.896]
Epoch [11/120    avg_loss:0.623, val_acc:0.875]
Epoch [12/120    avg_loss:0.575, val_acc:0.894]
Epoch [13/120    avg_loss:0.553, val_acc:0.904]
Epoch [14/120    avg_loss:0.500, val_acc:0.900]
Epoch [15/120    avg_loss:0.462, val_acc:0.917]
Epoch [16/120    avg_loss:0.434, val_acc:0.883]
Epoch [17/120    avg_loss:0.365, val_acc:0.954]
Epoch [18/120    avg_loss:0.412, val_acc:0.944]
Epoch [19/120    avg_loss:0.373, val_acc:0.940]
Epoch [20/120    avg_loss:0.341, val_acc:0.958]
Epoch [21/120    avg_loss:0.339, val_acc:0.963]
Epoch [22/120    avg_loss:0.291, val_acc:0.933]
Epoch [23/120    avg_loss:0.261, val_acc:0.925]
Epoch [24/120    avg_loss:0.298, val_acc:0.873]
Epoch [25/120    avg_loss:0.282, val_acc:0.950]
Epoch [26/120    avg_loss:0.222, val_acc:0.969]
Epoch [27/120    avg_loss:0.218, val_acc:0.956]
Epoch [28/120    avg_loss:0.222, val_acc:0.958]
Epoch [29/120    avg_loss:0.258, val_acc:0.958]
Epoch [30/120    avg_loss:0.216, val_acc:0.965]
Epoch [31/120    avg_loss:0.198, val_acc:0.965]
Epoch [32/120    avg_loss:0.203, val_acc:0.965]
Epoch [33/120    avg_loss:0.189, val_acc:0.963]
Epoch [34/120    avg_loss:0.220, val_acc:0.952]
Epoch [35/120    avg_loss:0.195, val_acc:0.956]
Epoch [36/120    avg_loss:0.188, val_acc:0.963]
Epoch [37/120    avg_loss:0.139, val_acc:0.979]
Epoch [38/120    avg_loss:0.151, val_acc:0.975]
Epoch [39/120    avg_loss:0.228, val_acc:0.969]
Epoch [40/120    avg_loss:0.163, val_acc:0.979]
Epoch [41/120    avg_loss:0.136, val_acc:0.983]
Epoch [42/120    avg_loss:0.141, val_acc:0.948]
Epoch [43/120    avg_loss:0.176, val_acc:0.960]
Epoch [44/120    avg_loss:0.150, val_acc:0.965]
Epoch [45/120    avg_loss:0.114, val_acc:0.963]
Epoch [46/120    avg_loss:0.115, val_acc:0.973]
Epoch [47/120    avg_loss:0.110, val_acc:0.965]
Epoch [48/120    avg_loss:0.103, val_acc:0.975]
Epoch [49/120    avg_loss:0.140, val_acc:0.971]
Epoch [50/120    avg_loss:0.136, val_acc:0.963]
Epoch [51/120    avg_loss:0.143, val_acc:0.948]
Epoch [52/120    avg_loss:0.216, val_acc:0.969]
Epoch [53/120    avg_loss:0.149, val_acc:0.981]
Epoch [54/120    avg_loss:0.124, val_acc:0.979]
Epoch [55/120    avg_loss:0.083, val_acc:0.979]
Epoch [56/120    avg_loss:0.082, val_acc:0.981]
Epoch [57/120    avg_loss:0.082, val_acc:0.983]
Epoch [58/120    avg_loss:0.084, val_acc:0.985]
Epoch [59/120    avg_loss:0.073, val_acc:0.985]
Epoch [60/120    avg_loss:0.060, val_acc:0.985]
Epoch [61/120    avg_loss:0.059, val_acc:0.985]
Epoch [62/120    avg_loss:0.061, val_acc:0.985]
Epoch [63/120    avg_loss:0.054, val_acc:0.985]
Epoch [64/120    avg_loss:0.063, val_acc:0.988]
Epoch [65/120    avg_loss:0.086, val_acc:0.988]
Epoch [66/120    avg_loss:0.066, val_acc:0.988]
Epoch [67/120    avg_loss:0.054, val_acc:0.988]
Epoch [68/120    avg_loss:0.056, val_acc:0.988]
Epoch [69/120    avg_loss:0.061, val_acc:0.988]
Epoch [70/120    avg_loss:0.070, val_acc:0.988]
Epoch [71/120    avg_loss:0.053, val_acc:0.988]
Epoch [72/120    avg_loss:0.050, val_acc:0.988]
Epoch [73/120    avg_loss:0.061, val_acc:0.988]
Epoch [74/120    avg_loss:0.062, val_acc:0.988]
Epoch [75/120    avg_loss:0.065, val_acc:0.988]
Epoch [76/120    avg_loss:0.050, val_acc:0.988]
Epoch [77/120    avg_loss:0.046, val_acc:0.988]
Epoch [78/120    avg_loss:0.051, val_acc:0.988]
Epoch [79/120    avg_loss:0.047, val_acc:0.990]
Epoch [80/120    avg_loss:0.048, val_acc:0.988]
Epoch [81/120    avg_loss:0.049, val_acc:0.990]
Epoch [82/120    avg_loss:0.048, val_acc:0.990]
Epoch [83/120    avg_loss:0.050, val_acc:0.992]
Epoch [84/120    avg_loss:0.051, val_acc:0.992]
Epoch [85/120    avg_loss:0.051, val_acc:0.992]
Epoch [86/120    avg_loss:0.045, val_acc:0.992]
Epoch [87/120    avg_loss:0.041, val_acc:0.990]
Epoch [88/120    avg_loss:0.044, val_acc:0.992]
Epoch [89/120    avg_loss:0.045, val_acc:0.992]
Epoch [90/120    avg_loss:0.037, val_acc:0.992]
Epoch [91/120    avg_loss:0.037, val_acc:0.992]
Epoch [92/120    avg_loss:0.046, val_acc:0.992]
Epoch [93/120    avg_loss:0.054, val_acc:0.992]
Epoch [94/120    avg_loss:0.067, val_acc:0.992]
Epoch [95/120    avg_loss:0.049, val_acc:0.992]
Epoch [96/120    avg_loss:0.045, val_acc:0.990]
Epoch [97/120    avg_loss:0.035, val_acc:0.992]
Epoch [98/120    avg_loss:0.045, val_acc:0.992]
Epoch [99/120    avg_loss:0.038, val_acc:0.992]
Epoch [100/120    avg_loss:0.033, val_acc:0.992]
Epoch [101/120    avg_loss:0.036, val_acc:0.992]
Epoch [102/120    avg_loss:0.031, val_acc:0.992]
Epoch [103/120    avg_loss:0.051, val_acc:0.992]
Epoch [104/120    avg_loss:0.038, val_acc:0.992]
Epoch [105/120    avg_loss:0.039, val_acc:0.992]
Epoch [106/120    avg_loss:0.032, val_acc:0.992]
Epoch [107/120    avg_loss:0.065, val_acc:0.992]
Epoch [108/120    avg_loss:0.033, val_acc:0.992]
Epoch [109/120    avg_loss:0.033, val_acc:0.992]
Epoch [110/120    avg_loss:0.037, val_acc:0.992]
Epoch [111/120    avg_loss:0.030, val_acc:0.994]
Epoch [112/120    avg_loss:0.045, val_acc:0.992]
Epoch [113/120    avg_loss:0.040, val_acc:0.992]
Epoch [114/120    avg_loss:0.047, val_acc:0.992]
Epoch [115/120    avg_loss:0.031, val_acc:0.992]
Epoch [116/120    avg_loss:0.034, val_acc:0.992]
Epoch [117/120    avg_loss:0.047, val_acc:0.992]
Epoch [118/120    avg_loss:0.031, val_acc:0.992]
Epoch [119/120    avg_loss:0.036, val_acc:0.992]
Epoch [120/120    avg_loss:0.037, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 227   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.996337   0.99545455 1.         0.97216274 0.95306859
 0.98800959 0.98924731 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9950152425067565
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b6a8b97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.468, val_acc:0.483]
Epoch [2/120    avg_loss:2.047, val_acc:0.637]
Epoch [3/120    avg_loss:1.777, val_acc:0.610]
Epoch [4/120    avg_loss:1.499, val_acc:0.667]
Epoch [5/120    avg_loss:1.265, val_acc:0.723]
Epoch [6/120    avg_loss:1.081, val_acc:0.738]
Epoch [7/120    avg_loss:0.909, val_acc:0.783]
Epoch [8/120    avg_loss:0.755, val_acc:0.808]
Epoch [9/120    avg_loss:0.721, val_acc:0.840]
Epoch [10/120    avg_loss:0.592, val_acc:0.835]
Epoch [11/120    avg_loss:0.577, val_acc:0.904]
Epoch [12/120    avg_loss:0.477, val_acc:0.887]
Epoch [13/120    avg_loss:0.458, val_acc:0.894]
Epoch [14/120    avg_loss:0.514, val_acc:0.856]
Epoch [15/120    avg_loss:0.471, val_acc:0.810]
Epoch [16/120    avg_loss:0.397, val_acc:0.917]
Epoch [17/120    avg_loss:0.367, val_acc:0.906]
Epoch [18/120    avg_loss:0.327, val_acc:0.921]
Epoch [19/120    avg_loss:0.338, val_acc:0.881]
Epoch [20/120    avg_loss:0.316, val_acc:0.885]
Epoch [21/120    avg_loss:0.298, val_acc:0.938]
Epoch [22/120    avg_loss:0.264, val_acc:0.938]
Epoch [23/120    avg_loss:0.281, val_acc:0.935]
Epoch [24/120    avg_loss:0.272, val_acc:0.948]
Epoch [25/120    avg_loss:0.254, val_acc:0.915]
Epoch [26/120    avg_loss:0.316, val_acc:0.890]
Epoch [27/120    avg_loss:0.265, val_acc:0.944]
Epoch [28/120    avg_loss:0.209, val_acc:0.946]
Epoch [29/120    avg_loss:0.172, val_acc:0.969]
Epoch [30/120    avg_loss:0.189, val_acc:0.942]
Epoch [31/120    avg_loss:0.152, val_acc:0.971]
Epoch [32/120    avg_loss:0.169, val_acc:0.969]
Epoch [33/120    avg_loss:0.138, val_acc:0.956]
Epoch [34/120    avg_loss:0.202, val_acc:0.950]
Epoch [35/120    avg_loss:0.174, val_acc:0.971]
Epoch [36/120    avg_loss:0.215, val_acc:0.912]
Epoch [37/120    avg_loss:0.222, val_acc:0.954]
Epoch [38/120    avg_loss:0.152, val_acc:0.960]
Epoch [39/120    avg_loss:0.147, val_acc:0.952]
Epoch [40/120    avg_loss:0.121, val_acc:0.975]
Epoch [41/120    avg_loss:0.137, val_acc:0.948]
Epoch [42/120    avg_loss:0.152, val_acc:0.960]
Epoch [43/120    avg_loss:0.132, val_acc:0.960]
Epoch [44/120    avg_loss:0.133, val_acc:0.960]
Epoch [45/120    avg_loss:0.154, val_acc:0.950]
Epoch [46/120    avg_loss:0.164, val_acc:0.965]
Epoch [47/120    avg_loss:0.175, val_acc:0.921]
Epoch [48/120    avg_loss:0.113, val_acc:0.973]
Epoch [49/120    avg_loss:0.099, val_acc:0.963]
Epoch [50/120    avg_loss:0.112, val_acc:0.977]
Epoch [51/120    avg_loss:0.106, val_acc:0.971]
Epoch [52/120    avg_loss:0.085, val_acc:0.977]
Epoch [53/120    avg_loss:0.079, val_acc:0.983]
Epoch [54/120    avg_loss:0.069, val_acc:0.981]
Epoch [55/120    avg_loss:0.073, val_acc:0.975]
Epoch [56/120    avg_loss:0.047, val_acc:0.977]
Epoch [57/120    avg_loss:0.053, val_acc:0.988]
Epoch [58/120    avg_loss:0.071, val_acc:0.975]
Epoch [59/120    avg_loss:0.095, val_acc:0.977]
Epoch [60/120    avg_loss:0.102, val_acc:0.975]
Epoch [61/120    avg_loss:0.078, val_acc:0.963]
Epoch [62/120    avg_loss:0.076, val_acc:0.975]
Epoch [63/120    avg_loss:0.050, val_acc:0.981]
Epoch [64/120    avg_loss:0.051, val_acc:0.983]
Epoch [65/120    avg_loss:0.041, val_acc:0.979]
Epoch [66/120    avg_loss:0.063, val_acc:0.975]
Epoch [67/120    avg_loss:0.059, val_acc:0.985]
Epoch [68/120    avg_loss:0.071, val_acc:0.977]
Epoch [69/120    avg_loss:0.062, val_acc:0.990]
Epoch [70/120    avg_loss:0.072, val_acc:0.981]
Epoch [71/120    avg_loss:0.063, val_acc:0.977]
Epoch [72/120    avg_loss:0.053, val_acc:0.990]
Epoch [73/120    avg_loss:0.023, val_acc:0.977]
Epoch [74/120    avg_loss:0.037, val_acc:0.990]
Epoch [75/120    avg_loss:0.030, val_acc:0.983]
Epoch [76/120    avg_loss:0.063, val_acc:0.985]
Epoch [77/120    avg_loss:0.043, val_acc:0.983]
Epoch [78/120    avg_loss:0.036, val_acc:0.985]
Epoch [79/120    avg_loss:0.040, val_acc:0.981]
Epoch [80/120    avg_loss:0.074, val_acc:0.956]
Epoch [81/120    avg_loss:0.062, val_acc:0.985]
Epoch [82/120    avg_loss:0.037, val_acc:0.979]
Epoch [83/120    avg_loss:0.045, val_acc:0.973]
Epoch [84/120    avg_loss:0.021, val_acc:0.990]
Epoch [85/120    avg_loss:0.029, val_acc:0.985]
Epoch [86/120    avg_loss:0.028, val_acc:0.985]
Epoch [87/120    avg_loss:0.019, val_acc:0.990]
Epoch [88/120    avg_loss:0.020, val_acc:0.994]
Epoch [89/120    avg_loss:0.029, val_acc:0.992]
Epoch [90/120    avg_loss:0.020, val_acc:0.988]
Epoch [91/120    avg_loss:0.018, val_acc:0.992]
Epoch [92/120    avg_loss:0.022, val_acc:0.990]
Epoch [93/120    avg_loss:0.028, val_acc:0.990]
Epoch [94/120    avg_loss:0.053, val_acc:0.950]
Epoch [95/120    avg_loss:0.034, val_acc:0.992]
Epoch [96/120    avg_loss:0.069, val_acc:0.988]
Epoch [97/120    avg_loss:0.068, val_acc:0.983]
Epoch [98/120    avg_loss:0.039, val_acc:0.985]
Epoch [99/120    avg_loss:0.037, val_acc:0.988]
Epoch [100/120    avg_loss:0.026, val_acc:0.988]
Epoch [101/120    avg_loss:0.028, val_acc:0.990]
Epoch [102/120    avg_loss:0.019, val_acc:0.990]
Epoch [103/120    avg_loss:0.026, val_acc:0.990]
Epoch [104/120    avg_loss:0.018, val_acc:0.992]
Epoch [105/120    avg_loss:0.012, val_acc:0.992]
Epoch [106/120    avg_loss:0.013, val_acc:0.990]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.011, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.012, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.014, val_acc:0.990]
Epoch [115/120    avg_loss:0.014, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.014, val_acc:0.990]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.82942430703625

F1 scores:
[       nan 0.996337   1.         1.         0.99557522 0.99315068
 0.98800959 1.         1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9981011869959853
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4aa72457f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.549, val_acc:0.404]
Epoch [2/120    avg_loss:2.155, val_acc:0.590]
Epoch [3/120    avg_loss:1.907, val_acc:0.656]
Epoch [4/120    avg_loss:1.695, val_acc:0.629]
Epoch [5/120    avg_loss:1.474, val_acc:0.721]
Epoch [6/120    avg_loss:1.326, val_acc:0.665]
Epoch [7/120    avg_loss:1.163, val_acc:0.729]
Epoch [8/120    avg_loss:1.059, val_acc:0.794]
Epoch [9/120    avg_loss:0.984, val_acc:0.815]
Epoch [10/120    avg_loss:0.914, val_acc:0.748]
Epoch [11/120    avg_loss:0.842, val_acc:0.819]
Epoch [12/120    avg_loss:0.774, val_acc:0.863]
Epoch [13/120    avg_loss:0.690, val_acc:0.850]
Epoch [14/120    avg_loss:0.618, val_acc:0.904]
Epoch [15/120    avg_loss:0.576, val_acc:0.908]
Epoch [16/120    avg_loss:0.511, val_acc:0.927]
Epoch [17/120    avg_loss:0.470, val_acc:0.917]
Epoch [18/120    avg_loss:0.523, val_acc:0.921]
Epoch [19/120    avg_loss:0.458, val_acc:0.940]
Epoch [20/120    avg_loss:0.432, val_acc:0.935]
Epoch [21/120    avg_loss:0.420, val_acc:0.940]
Epoch [22/120    avg_loss:0.380, val_acc:0.921]
Epoch [23/120    avg_loss:0.395, val_acc:0.950]
Epoch [24/120    avg_loss:0.352, val_acc:0.925]
Epoch [25/120    avg_loss:0.308, val_acc:0.894]
Epoch [26/120    avg_loss:0.327, val_acc:0.933]
Epoch [27/120    avg_loss:0.353, val_acc:0.952]
Epoch [28/120    avg_loss:0.265, val_acc:0.950]
Epoch [29/120    avg_loss:0.294, val_acc:0.931]
Epoch [30/120    avg_loss:0.279, val_acc:0.958]
Epoch [31/120    avg_loss:0.242, val_acc:0.929]
Epoch [32/120    avg_loss:0.235, val_acc:0.954]
Epoch [33/120    avg_loss:0.224, val_acc:0.954]
Epoch [34/120    avg_loss:0.223, val_acc:0.950]
Epoch [35/120    avg_loss:0.250, val_acc:0.942]
Epoch [36/120    avg_loss:0.213, val_acc:0.965]
Epoch [37/120    avg_loss:0.204, val_acc:0.965]
Epoch [38/120    avg_loss:0.171, val_acc:0.960]
Epoch [39/120    avg_loss:0.198, val_acc:0.960]
Epoch [40/120    avg_loss:0.216, val_acc:0.952]
Epoch [41/120    avg_loss:0.213, val_acc:0.919]
Epoch [42/120    avg_loss:0.263, val_acc:0.954]
Epoch [43/120    avg_loss:0.238, val_acc:0.952]
Epoch [44/120    avg_loss:0.182, val_acc:0.965]
Epoch [45/120    avg_loss:0.179, val_acc:0.946]
Epoch [46/120    avg_loss:0.166, val_acc:0.969]
Epoch [47/120    avg_loss:0.175, val_acc:0.960]
Epoch [48/120    avg_loss:0.146, val_acc:0.963]
Epoch [49/120    avg_loss:0.173, val_acc:0.971]
Epoch [50/120    avg_loss:0.127, val_acc:0.967]
Epoch [51/120    avg_loss:0.119, val_acc:0.977]
Epoch [52/120    avg_loss:0.116, val_acc:0.933]
Epoch [53/120    avg_loss:0.198, val_acc:0.963]
Epoch [54/120    avg_loss:0.133, val_acc:0.985]
Epoch [55/120    avg_loss:0.153, val_acc:0.975]
Epoch [56/120    avg_loss:0.142, val_acc:0.975]
Epoch [57/120    avg_loss:0.131, val_acc:0.973]
Epoch [58/120    avg_loss:0.123, val_acc:0.967]
Epoch [59/120    avg_loss:0.097, val_acc:0.983]
Epoch [60/120    avg_loss:0.096, val_acc:0.952]
Epoch [61/120    avg_loss:0.187, val_acc:0.967]
Epoch [62/120    avg_loss:0.263, val_acc:0.935]
Epoch [63/120    avg_loss:0.194, val_acc:0.960]
Epoch [64/120    avg_loss:0.180, val_acc:0.969]
Epoch [65/120    avg_loss:0.135, val_acc:0.988]
Epoch [66/120    avg_loss:0.185, val_acc:0.975]
Epoch [67/120    avg_loss:0.149, val_acc:0.979]
Epoch [68/120    avg_loss:0.140, val_acc:0.981]
Epoch [69/120    avg_loss:0.095, val_acc:0.975]
Epoch [70/120    avg_loss:0.088, val_acc:0.979]
Epoch [71/120    avg_loss:0.070, val_acc:0.985]
Epoch [72/120    avg_loss:0.089, val_acc:0.979]
Epoch [73/120    avg_loss:0.102, val_acc:0.977]
Epoch [74/120    avg_loss:0.105, val_acc:0.973]
Epoch [75/120    avg_loss:0.126, val_acc:0.973]
Epoch [76/120    avg_loss:0.125, val_acc:0.965]
Epoch [77/120    avg_loss:0.142, val_acc:0.979]
Epoch [78/120    avg_loss:0.180, val_acc:0.944]
Epoch [79/120    avg_loss:0.206, val_acc:0.969]
Epoch [80/120    avg_loss:0.087, val_acc:0.975]
Epoch [81/120    avg_loss:0.063, val_acc:0.988]
Epoch [82/120    avg_loss:0.059, val_acc:0.988]
Epoch [83/120    avg_loss:0.065, val_acc:0.988]
Epoch [84/120    avg_loss:0.056, val_acc:0.990]
Epoch [85/120    avg_loss:0.062, val_acc:0.992]
Epoch [86/120    avg_loss:0.045, val_acc:0.994]
Epoch [87/120    avg_loss:0.049, val_acc:0.994]
Epoch [88/120    avg_loss:0.040, val_acc:0.990]
Epoch [89/120    avg_loss:0.057, val_acc:0.992]
Epoch [90/120    avg_loss:0.044, val_acc:0.994]
Epoch [91/120    avg_loss:0.053, val_acc:0.994]
Epoch [92/120    avg_loss:0.050, val_acc:0.994]
Epoch [93/120    avg_loss:0.040, val_acc:0.990]
Epoch [94/120    avg_loss:0.053, val_acc:0.990]
Epoch [95/120    avg_loss:0.051, val_acc:0.992]
Epoch [96/120    avg_loss:0.044, val_acc:0.990]
Epoch [97/120    avg_loss:0.051, val_acc:0.992]
Epoch [98/120    avg_loss:0.038, val_acc:0.990]
Epoch [99/120    avg_loss:0.041, val_acc:0.992]
Epoch [100/120    avg_loss:0.039, val_acc:0.992]
Epoch [101/120    avg_loss:0.074, val_acc:0.992]
Epoch [102/120    avg_loss:0.053, val_acc:0.992]
Epoch [103/120    avg_loss:0.045, val_acc:0.994]
Epoch [104/120    avg_loss:0.053, val_acc:0.992]
Epoch [105/120    avg_loss:0.046, val_acc:0.994]
Epoch [106/120    avg_loss:0.039, val_acc:0.988]
Epoch [107/120    avg_loss:0.036, val_acc:0.990]
Epoch [108/120    avg_loss:0.042, val_acc:0.990]
Epoch [109/120    avg_loss:0.048, val_acc:0.992]
Epoch [110/120    avg_loss:0.044, val_acc:0.992]
Epoch [111/120    avg_loss:0.030, val_acc:0.992]
Epoch [112/120    avg_loss:0.042, val_acc:0.992]
Epoch [113/120    avg_loss:0.043, val_acc:0.992]
Epoch [114/120    avg_loss:0.033, val_acc:0.990]
Epoch [115/120    avg_loss:0.033, val_acc:0.992]
Epoch [116/120    avg_loss:0.045, val_acc:0.992]
Epoch [117/120    avg_loss:0.044, val_acc:0.994]
Epoch [118/120    avg_loss:0.051, val_acc:0.992]
Epoch [119/120    avg_loss:0.040, val_acc:0.996]
Epoch [120/120    avg_loss:0.035, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   5   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99708879 0.97117517 0.98678414 0.96312364 0.96167247
 0.99019608 0.94972067 1.         0.99893276 1.         0.99867198
 0.99336283 1.        ]

Kappa:
0.9914530234701676
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2dd0eec780>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.534, val_acc:0.312]
Epoch [2/120    avg_loss:2.183, val_acc:0.535]
Epoch [3/120    avg_loss:1.957, val_acc:0.625]
Epoch [4/120    avg_loss:1.756, val_acc:0.629]
Epoch [5/120    avg_loss:1.545, val_acc:0.642]
Epoch [6/120    avg_loss:1.352, val_acc:0.662]
Epoch [7/120    avg_loss:1.236, val_acc:0.685]
Epoch [8/120    avg_loss:1.090, val_acc:0.708]
Epoch [9/120    avg_loss:1.005, val_acc:0.706]
Epoch [10/120    avg_loss:0.929, val_acc:0.775]
Epoch [11/120    avg_loss:0.848, val_acc:0.804]
Epoch [12/120    avg_loss:0.795, val_acc:0.819]
Epoch [13/120    avg_loss:0.754, val_acc:0.827]
Epoch [14/120    avg_loss:0.673, val_acc:0.867]
Epoch [15/120    avg_loss:0.659, val_acc:0.894]
Epoch [16/120    avg_loss:0.593, val_acc:0.902]
Epoch [17/120    avg_loss:0.537, val_acc:0.904]
Epoch [18/120    avg_loss:0.482, val_acc:0.912]
Epoch [19/120    avg_loss:0.447, val_acc:0.896]
Epoch [20/120    avg_loss:0.415, val_acc:0.915]
Epoch [21/120    avg_loss:0.413, val_acc:0.908]
Epoch [22/120    avg_loss:0.374, val_acc:0.919]
Epoch [23/120    avg_loss:0.375, val_acc:0.938]
Epoch [24/120    avg_loss:0.371, val_acc:0.923]
Epoch [25/120    avg_loss:0.356, val_acc:0.929]
Epoch [26/120    avg_loss:0.348, val_acc:0.931]
Epoch [27/120    avg_loss:0.406, val_acc:0.896]
Epoch [28/120    avg_loss:0.353, val_acc:0.933]
Epoch [29/120    avg_loss:0.323, val_acc:0.938]
Epoch [30/120    avg_loss:0.305, val_acc:0.881]
Epoch [31/120    avg_loss:0.315, val_acc:0.944]
Epoch [32/120    avg_loss:0.327, val_acc:0.927]
Epoch [33/120    avg_loss:0.322, val_acc:0.931]
Epoch [34/120    avg_loss:0.345, val_acc:0.952]
Epoch [35/120    avg_loss:0.254, val_acc:0.944]
Epoch [36/120    avg_loss:0.256, val_acc:0.925]
Epoch [37/120    avg_loss:0.310, val_acc:0.863]
Epoch [38/120    avg_loss:0.300, val_acc:0.954]
Epoch [39/120    avg_loss:0.261, val_acc:0.946]
Epoch [40/120    avg_loss:0.216, val_acc:0.942]
Epoch [41/120    avg_loss:0.222, val_acc:0.952]
Epoch [42/120    avg_loss:0.229, val_acc:0.944]
Epoch [43/120    avg_loss:0.251, val_acc:0.946]
Epoch [44/120    avg_loss:0.210, val_acc:0.963]
Epoch [45/120    avg_loss:0.194, val_acc:0.950]
Epoch [46/120    avg_loss:0.187, val_acc:0.948]
Epoch [47/120    avg_loss:0.209, val_acc:0.969]
Epoch [48/120    avg_loss:0.151, val_acc:0.975]
Epoch [49/120    avg_loss:0.155, val_acc:0.963]
Epoch [50/120    avg_loss:0.205, val_acc:0.950]
Epoch [51/120    avg_loss:0.209, val_acc:0.948]
Epoch [52/120    avg_loss:0.240, val_acc:0.960]
Epoch [53/120    avg_loss:0.176, val_acc:0.946]
Epoch [54/120    avg_loss:0.151, val_acc:0.967]
Epoch [55/120    avg_loss:0.141, val_acc:0.971]
Epoch [56/120    avg_loss:0.207, val_acc:0.942]
Epoch [57/120    avg_loss:0.149, val_acc:0.946]
Epoch [58/120    avg_loss:0.185, val_acc:0.956]
Epoch [59/120    avg_loss:0.180, val_acc:0.967]
Epoch [60/120    avg_loss:0.138, val_acc:0.958]
Epoch [61/120    avg_loss:0.120, val_acc:0.969]
Epoch [62/120    avg_loss:0.109, val_acc:0.971]
Epoch [63/120    avg_loss:0.104, val_acc:0.969]
Epoch [64/120    avg_loss:0.092, val_acc:0.969]
Epoch [65/120    avg_loss:0.112, val_acc:0.973]
Epoch [66/120    avg_loss:0.086, val_acc:0.973]
Epoch [67/120    avg_loss:0.072, val_acc:0.975]
Epoch [68/120    avg_loss:0.077, val_acc:0.975]
Epoch [69/120    avg_loss:0.092, val_acc:0.977]
Epoch [70/120    avg_loss:0.090, val_acc:0.979]
Epoch [71/120    avg_loss:0.065, val_acc:0.975]
Epoch [72/120    avg_loss:0.066, val_acc:0.975]
Epoch [73/120    avg_loss:0.086, val_acc:0.977]
Epoch [74/120    avg_loss:0.084, val_acc:0.977]
Epoch [75/120    avg_loss:0.080, val_acc:0.977]
Epoch [76/120    avg_loss:0.083, val_acc:0.983]
Epoch [77/120    avg_loss:0.081, val_acc:0.981]
Epoch [78/120    avg_loss:0.077, val_acc:0.977]
Epoch [79/120    avg_loss:0.084, val_acc:0.979]
Epoch [80/120    avg_loss:0.078, val_acc:0.981]
Epoch [81/120    avg_loss:0.069, val_acc:0.981]
Epoch [82/120    avg_loss:0.070, val_acc:0.983]
Epoch [83/120    avg_loss:0.070, val_acc:0.983]
Epoch [84/120    avg_loss:0.069, val_acc:0.983]
Epoch [85/120    avg_loss:0.067, val_acc:0.981]
Epoch [86/120    avg_loss:0.066, val_acc:0.983]
Epoch [87/120    avg_loss:0.056, val_acc:0.979]
Epoch [88/120    avg_loss:0.061, val_acc:0.981]
Epoch [89/120    avg_loss:0.076, val_acc:0.983]
Epoch [90/120    avg_loss:0.061, val_acc:0.983]
Epoch [91/120    avg_loss:0.064, val_acc:0.983]
Epoch [92/120    avg_loss:0.079, val_acc:0.983]
Epoch [93/120    avg_loss:0.063, val_acc:0.981]
Epoch [94/120    avg_loss:0.062, val_acc:0.981]
Epoch [95/120    avg_loss:0.056, val_acc:0.981]
Epoch [96/120    avg_loss:0.058, val_acc:0.983]
Epoch [97/120    avg_loss:0.061, val_acc:0.983]
Epoch [98/120    avg_loss:0.054, val_acc:0.979]
Epoch [99/120    avg_loss:0.069, val_acc:0.979]
Epoch [100/120    avg_loss:0.059, val_acc:0.979]
Epoch [101/120    avg_loss:0.057, val_acc:0.981]
Epoch [102/120    avg_loss:0.061, val_acc:0.983]
Epoch [103/120    avg_loss:0.058, val_acc:0.985]
Epoch [104/120    avg_loss:0.062, val_acc:0.983]
Epoch [105/120    avg_loss:0.062, val_acc:0.983]
Epoch [106/120    avg_loss:0.079, val_acc:0.983]
Epoch [107/120    avg_loss:0.056, val_acc:0.985]
Epoch [108/120    avg_loss:0.065, val_acc:0.983]
Epoch [109/120    avg_loss:0.071, val_acc:0.985]
Epoch [110/120    avg_loss:0.048, val_acc:0.985]
Epoch [111/120    avg_loss:0.054, val_acc:0.985]
Epoch [112/120    avg_loss:0.056, val_acc:0.983]
Epoch [113/120    avg_loss:0.055, val_acc:0.988]
Epoch [114/120    avg_loss:0.059, val_acc:0.983]
Epoch [115/120    avg_loss:0.057, val_acc:0.985]
Epoch [116/120    avg_loss:0.057, val_acc:0.981]
Epoch [117/120    avg_loss:0.076, val_acc:0.985]
Epoch [118/120    avg_loss:0.063, val_acc:0.985]
Epoch [119/120    avg_loss:0.055, val_acc:0.985]
Epoch [120/120    avg_loss:0.050, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 212   7   0   0   0   8   3   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.96629213 0.95927602 0.89956332 0.8668942
 1.         0.91712707 0.98979592 0.99680511 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9826688186752552
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ffafcb780>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.511, val_acc:0.435]
Epoch [2/120    avg_loss:2.092, val_acc:0.567]
Epoch [3/120    avg_loss:1.806, val_acc:0.575]
Epoch [4/120    avg_loss:1.569, val_acc:0.700]
Epoch [5/120    avg_loss:1.362, val_acc:0.658]
Epoch [6/120    avg_loss:1.188, val_acc:0.717]
Epoch [7/120    avg_loss:1.059, val_acc:0.738]
Epoch [8/120    avg_loss:0.920, val_acc:0.706]
Epoch [9/120    avg_loss:0.911, val_acc:0.760]
Epoch [10/120    avg_loss:0.805, val_acc:0.829]
Epoch [11/120    avg_loss:0.765, val_acc:0.781]
Epoch [12/120    avg_loss:0.680, val_acc:0.783]
Epoch [13/120    avg_loss:0.709, val_acc:0.856]
Epoch [14/120    avg_loss:0.627, val_acc:0.831]
Epoch [15/120    avg_loss:0.581, val_acc:0.883]
Epoch [16/120    avg_loss:0.581, val_acc:0.821]
Epoch [17/120    avg_loss:0.545, val_acc:0.912]
Epoch [18/120    avg_loss:0.546, val_acc:0.873]
Epoch [19/120    avg_loss:0.596, val_acc:0.898]
Epoch [20/120    avg_loss:0.524, val_acc:0.858]
Epoch [21/120    avg_loss:0.464, val_acc:0.902]
Epoch [22/120    avg_loss:0.411, val_acc:0.902]
Epoch [23/120    avg_loss:0.388, val_acc:0.919]
Epoch [24/120    avg_loss:0.403, val_acc:0.923]
Epoch [25/120    avg_loss:0.377, val_acc:0.912]
Epoch [26/120    avg_loss:0.382, val_acc:0.927]
Epoch [27/120    avg_loss:0.343, val_acc:0.915]
Epoch [28/120    avg_loss:0.374, val_acc:0.931]
Epoch [29/120    avg_loss:0.375, val_acc:0.929]
Epoch [30/120    avg_loss:0.375, val_acc:0.917]
Epoch [31/120    avg_loss:0.344, val_acc:0.910]
Epoch [32/120    avg_loss:0.525, val_acc:0.835]
Epoch [33/120    avg_loss:0.549, val_acc:0.887]
Epoch [34/120    avg_loss:0.450, val_acc:0.919]
Epoch [35/120    avg_loss:0.348, val_acc:0.921]
Epoch [36/120    avg_loss:0.279, val_acc:0.935]
Epoch [37/120    avg_loss:0.250, val_acc:0.927]
Epoch [38/120    avg_loss:0.231, val_acc:0.948]
Epoch [39/120    avg_loss:0.246, val_acc:0.942]
Epoch [40/120    avg_loss:0.249, val_acc:0.944]
Epoch [41/120    avg_loss:0.272, val_acc:0.942]
Epoch [42/120    avg_loss:0.211, val_acc:0.948]
Epoch [43/120    avg_loss:0.241, val_acc:0.952]
Epoch [44/120    avg_loss:0.268, val_acc:0.942]
Epoch [45/120    avg_loss:0.290, val_acc:0.938]
Epoch [46/120    avg_loss:0.245, val_acc:0.944]
Epoch [47/120    avg_loss:0.226, val_acc:0.940]
Epoch [48/120    avg_loss:0.190, val_acc:0.950]
Epoch [49/120    avg_loss:0.225, val_acc:0.960]
Epoch [50/120    avg_loss:0.174, val_acc:0.948]
Epoch [51/120    avg_loss:0.160, val_acc:0.952]
Epoch [52/120    avg_loss:0.193, val_acc:0.938]
Epoch [53/120    avg_loss:0.179, val_acc:0.950]
Epoch [54/120    avg_loss:0.191, val_acc:0.954]
Epoch [55/120    avg_loss:0.189, val_acc:0.933]
Epoch [56/120    avg_loss:0.189, val_acc:0.942]
Epoch [57/120    avg_loss:0.162, val_acc:0.940]
Epoch [58/120    avg_loss:0.154, val_acc:0.948]
Epoch [59/120    avg_loss:0.132, val_acc:0.956]
Epoch [60/120    avg_loss:0.165, val_acc:0.954]
Epoch [61/120    avg_loss:0.145, val_acc:0.944]
Epoch [62/120    avg_loss:0.141, val_acc:0.958]
Epoch [63/120    avg_loss:0.142, val_acc:0.952]
Epoch [64/120    avg_loss:0.096, val_acc:0.950]
Epoch [65/120    avg_loss:0.101, val_acc:0.944]
Epoch [66/120    avg_loss:0.087, val_acc:0.948]
Epoch [67/120    avg_loss:0.099, val_acc:0.952]
Epoch [68/120    avg_loss:0.093, val_acc:0.950]
Epoch [69/120    avg_loss:0.095, val_acc:0.950]
Epoch [70/120    avg_loss:0.086, val_acc:0.950]
Epoch [71/120    avg_loss:0.089, val_acc:0.952]
Epoch [72/120    avg_loss:0.089, val_acc:0.952]
Epoch [73/120    avg_loss:0.086, val_acc:0.950]
Epoch [74/120    avg_loss:0.090, val_acc:0.954]
Epoch [75/120    avg_loss:0.091, val_acc:0.952]
Epoch [76/120    avg_loss:0.078, val_acc:0.952]
Epoch [77/120    avg_loss:0.078, val_acc:0.952]
Epoch [78/120    avg_loss:0.092, val_acc:0.952]
Epoch [79/120    avg_loss:0.076, val_acc:0.954]
Epoch [80/120    avg_loss:0.096, val_acc:0.952]
Epoch [81/120    avg_loss:0.099, val_acc:0.952]
Epoch [82/120    avg_loss:0.077, val_acc:0.950]
Epoch [83/120    avg_loss:0.069, val_acc:0.952]
Epoch [84/120    avg_loss:0.076, val_acc:0.952]
Epoch [85/120    avg_loss:0.090, val_acc:0.952]
Epoch [86/120    avg_loss:0.082, val_acc:0.952]
Epoch [87/120    avg_loss:0.086, val_acc:0.952]
Epoch [88/120    avg_loss:0.086, val_acc:0.952]
Epoch [89/120    avg_loss:0.079, val_acc:0.952]
Epoch [90/120    avg_loss:0.085, val_acc:0.952]
Epoch [91/120    avg_loss:0.072, val_acc:0.952]
Epoch [92/120    avg_loss:0.080, val_acc:0.952]
Epoch [93/120    avg_loss:0.091, val_acc:0.952]
Epoch [94/120    avg_loss:0.063, val_acc:0.952]
Epoch [95/120    avg_loss:0.072, val_acc:0.952]
Epoch [96/120    avg_loss:0.080, val_acc:0.952]
Epoch [97/120    avg_loss:0.098, val_acc:0.952]
Epoch [98/120    avg_loss:0.089, val_acc:0.952]
Epoch [99/120    avg_loss:0.099, val_acc:0.952]
Epoch [100/120    avg_loss:0.082, val_acc:0.952]
Epoch [101/120    avg_loss:0.079, val_acc:0.952]
Epoch [102/120    avg_loss:0.080, val_acc:0.952]
Epoch [103/120    avg_loss:0.105, val_acc:0.952]
Epoch [104/120    avg_loss:0.069, val_acc:0.952]
Epoch [105/120    avg_loss:0.086, val_acc:0.952]
Epoch [106/120    avg_loss:0.080, val_acc:0.952]
Epoch [107/120    avg_loss:0.094, val_acc:0.952]
Epoch [108/120    avg_loss:0.072, val_acc:0.952]
Epoch [109/120    avg_loss:0.084, val_acc:0.952]
Epoch [110/120    avg_loss:0.086, val_acc:0.952]
Epoch [111/120    avg_loss:0.075, val_acc:0.952]
Epoch [112/120    avg_loss:0.080, val_acc:0.952]
Epoch [113/120    avg_loss:0.097, val_acc:0.952]
Epoch [114/120    avg_loss:0.080, val_acc:0.952]
Epoch [115/120    avg_loss:0.080, val_acc:0.952]
Epoch [116/120    avg_loss:0.084, val_acc:0.952]
Epoch [117/120    avg_loss:0.093, val_acc:0.952]
Epoch [118/120    avg_loss:0.071, val_acc:0.952]
Epoch [119/120    avg_loss:0.080, val_acc:0.952]
Epoch [120/120    avg_loss:0.071, val_acc:0.952]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 192   0   0   0   0  27   0   0   0   0   0   0]
 [  0   0   0 204  24   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  28 117   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.52665245202559

F1 scores:
[       nan 1.         0.90995261 0.94009217 0.84710744 0.82394366
 1.         0.81372549 0.998713   0.99893276 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9724651894989549
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd759f767f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.519, val_acc:0.331]
Epoch [2/120    avg_loss:2.125, val_acc:0.592]
Epoch [3/120    avg_loss:1.813, val_acc:0.604]
Epoch [4/120    avg_loss:1.561, val_acc:0.685]
Epoch [5/120    avg_loss:1.379, val_acc:0.650]
Epoch [6/120    avg_loss:1.226, val_acc:0.715]
Epoch [7/120    avg_loss:1.094, val_acc:0.783]
Epoch [8/120    avg_loss:1.011, val_acc:0.817]
Epoch [9/120    avg_loss:0.892, val_acc:0.840]
Epoch [10/120    avg_loss:0.824, val_acc:0.873]
Epoch [11/120    avg_loss:0.746, val_acc:0.865]
Epoch [12/120    avg_loss:0.678, val_acc:0.875]
Epoch [13/120    avg_loss:0.632, val_acc:0.885]
Epoch [14/120    avg_loss:0.643, val_acc:0.885]
Epoch [15/120    avg_loss:0.630, val_acc:0.894]
Epoch [16/120    avg_loss:0.536, val_acc:0.890]
Epoch [17/120    avg_loss:0.565, val_acc:0.890]
Epoch [18/120    avg_loss:0.500, val_acc:0.900]
Epoch [19/120    avg_loss:0.443, val_acc:0.900]
Epoch [20/120    avg_loss:0.433, val_acc:0.904]
Epoch [21/120    avg_loss:0.457, val_acc:0.844]
Epoch [22/120    avg_loss:0.419, val_acc:0.921]
Epoch [23/120    avg_loss:0.403, val_acc:0.910]
Epoch [24/120    avg_loss:0.390, val_acc:0.908]
Epoch [25/120    avg_loss:0.354, val_acc:0.906]
Epoch [26/120    avg_loss:0.320, val_acc:0.923]
Epoch [27/120    avg_loss:0.349, val_acc:0.910]
Epoch [28/120    avg_loss:0.336, val_acc:0.904]
Epoch [29/120    avg_loss:0.319, val_acc:0.923]
Epoch [30/120    avg_loss:0.272, val_acc:0.921]
Epoch [31/120    avg_loss:0.233, val_acc:0.929]
Epoch [32/120    avg_loss:0.284, val_acc:0.933]
Epoch [33/120    avg_loss:0.262, val_acc:0.944]
Epoch [34/120    avg_loss:0.246, val_acc:0.933]
Epoch [35/120    avg_loss:0.234, val_acc:0.927]
Epoch [36/120    avg_loss:0.283, val_acc:0.929]
Epoch [37/120    avg_loss:0.319, val_acc:0.931]
Epoch [38/120    avg_loss:0.235, val_acc:0.931]
Epoch [39/120    avg_loss:0.238, val_acc:0.950]
Epoch [40/120    avg_loss:0.221, val_acc:0.948]
Epoch [41/120    avg_loss:0.212, val_acc:0.944]
Epoch [42/120    avg_loss:0.219, val_acc:0.950]
Epoch [43/120    avg_loss:0.186, val_acc:0.952]
Epoch [44/120    avg_loss:0.199, val_acc:0.938]
Epoch [45/120    avg_loss:0.208, val_acc:0.963]
Epoch [46/120    avg_loss:0.224, val_acc:0.919]
Epoch [47/120    avg_loss:0.187, val_acc:0.956]
Epoch [48/120    avg_loss:0.167, val_acc:0.960]
Epoch [49/120    avg_loss:0.154, val_acc:0.963]
Epoch [50/120    avg_loss:0.153, val_acc:0.946]
Epoch [51/120    avg_loss:0.169, val_acc:0.956]
Epoch [52/120    avg_loss:0.145, val_acc:0.948]
Epoch [53/120    avg_loss:0.130, val_acc:0.956]
Epoch [54/120    avg_loss:0.183, val_acc:0.956]
Epoch [55/120    avg_loss:0.147, val_acc:0.965]
Epoch [56/120    avg_loss:0.131, val_acc:0.960]
Epoch [57/120    avg_loss:0.120, val_acc:0.944]
Epoch [58/120    avg_loss:0.135, val_acc:0.960]
Epoch [59/120    avg_loss:0.129, val_acc:0.960]
Epoch [60/120    avg_loss:0.113, val_acc:0.950]
Epoch [61/120    avg_loss:0.133, val_acc:0.952]
Epoch [62/120    avg_loss:0.131, val_acc:0.958]
Epoch [63/120    avg_loss:0.129, val_acc:0.958]
Epoch [64/120    avg_loss:0.130, val_acc:0.956]
Epoch [65/120    avg_loss:0.099, val_acc:0.946]
Epoch [66/120    avg_loss:0.103, val_acc:0.963]
Epoch [67/120    avg_loss:0.124, val_acc:0.960]
Epoch [68/120    avg_loss:0.121, val_acc:0.958]
Epoch [69/120    avg_loss:0.102, val_acc:0.969]
Epoch [70/120    avg_loss:0.080, val_acc:0.971]
Epoch [71/120    avg_loss:0.075, val_acc:0.969]
Epoch [72/120    avg_loss:0.080, val_acc:0.967]
Epoch [73/120    avg_loss:0.071, val_acc:0.969]
Epoch [74/120    avg_loss:0.069, val_acc:0.971]
Epoch [75/120    avg_loss:0.056, val_acc:0.975]
Epoch [76/120    avg_loss:0.062, val_acc:0.971]
Epoch [77/120    avg_loss:0.067, val_acc:0.971]
Epoch [78/120    avg_loss:0.067, val_acc:0.969]
Epoch [79/120    avg_loss:0.068, val_acc:0.969]
Epoch [80/120    avg_loss:0.080, val_acc:0.971]
Epoch [81/120    avg_loss:0.060, val_acc:0.971]
Epoch [82/120    avg_loss:0.071, val_acc:0.969]
Epoch [83/120    avg_loss:0.067, val_acc:0.973]
Epoch [84/120    avg_loss:0.063, val_acc:0.975]
Epoch [85/120    avg_loss:0.060, val_acc:0.973]
Epoch [86/120    avg_loss:0.061, val_acc:0.971]
Epoch [87/120    avg_loss:0.063, val_acc:0.973]
Epoch [88/120    avg_loss:0.047, val_acc:0.973]
Epoch [89/120    avg_loss:0.067, val_acc:0.971]
Epoch [90/120    avg_loss:0.054, val_acc:0.971]
Epoch [91/120    avg_loss:0.060, val_acc:0.973]
Epoch [92/120    avg_loss:0.049, val_acc:0.975]
Epoch [93/120    avg_loss:0.063, val_acc:0.973]
Epoch [94/120    avg_loss:0.058, val_acc:0.971]
Epoch [95/120    avg_loss:0.059, val_acc:0.973]
Epoch [96/120    avg_loss:0.061, val_acc:0.975]
Epoch [97/120    avg_loss:0.063, val_acc:0.973]
Epoch [98/120    avg_loss:0.057, val_acc:0.973]
Epoch [99/120    avg_loss:0.051, val_acc:0.977]
Epoch [100/120    avg_loss:0.053, val_acc:0.975]
Epoch [101/120    avg_loss:0.048, val_acc:0.975]
Epoch [102/120    avg_loss:0.055, val_acc:0.977]
Epoch [103/120    avg_loss:0.048, val_acc:0.975]
Epoch [104/120    avg_loss:0.047, val_acc:0.973]
Epoch [105/120    avg_loss:0.052, val_acc:0.975]
Epoch [106/120    avg_loss:0.047, val_acc:0.971]
Epoch [107/120    avg_loss:0.053, val_acc:0.975]
Epoch [108/120    avg_loss:0.058, val_acc:0.977]
Epoch [109/120    avg_loss:0.048, val_acc:0.975]
Epoch [110/120    avg_loss:0.046, val_acc:0.975]
Epoch [111/120    avg_loss:0.045, val_acc:0.977]
Epoch [112/120    avg_loss:0.057, val_acc:0.975]
Epoch [113/120    avg_loss:0.045, val_acc:0.975]
Epoch [114/120    avg_loss:0.046, val_acc:0.973]
Epoch [115/120    avg_loss:0.064, val_acc:0.973]
Epoch [116/120    avg_loss:0.044, val_acc:0.975]
Epoch [117/120    avg_loss:0.048, val_acc:0.977]
Epoch [118/120    avg_loss:0.052, val_acc:0.975]
Epoch [119/120    avg_loss:0.062, val_acc:0.981]
Epoch [120/120    avg_loss:0.045, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 0.99854227 0.9261745  0.98230088 0.93424036 0.92903226
 0.99512195 0.84153005 1.         0.99893276 1.         0.99867198
 0.9944629  1.        ]

Kappa:
0.9843333790533726
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa2054e4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.559, val_acc:0.385]
Epoch [2/120    avg_loss:2.120, val_acc:0.444]
Epoch [3/120    avg_loss:1.894, val_acc:0.606]
Epoch [4/120    avg_loss:1.680, val_acc:0.615]
Epoch [5/120    avg_loss:1.485, val_acc:0.629]
Epoch [6/120    avg_loss:1.324, val_acc:0.715]
Epoch [7/120    avg_loss:1.164, val_acc:0.744]
Epoch [8/120    avg_loss:1.075, val_acc:0.721]
Epoch [9/120    avg_loss:0.998, val_acc:0.792]
Epoch [10/120    avg_loss:0.903, val_acc:0.771]
Epoch [11/120    avg_loss:0.854, val_acc:0.840]
Epoch [12/120    avg_loss:0.734, val_acc:0.871]
Epoch [13/120    avg_loss:0.700, val_acc:0.865]
Epoch [14/120    avg_loss:0.665, val_acc:0.885]
Epoch [15/120    avg_loss:0.597, val_acc:0.863]
Epoch [16/120    avg_loss:0.517, val_acc:0.890]
Epoch [17/120    avg_loss:0.502, val_acc:0.898]
Epoch [18/120    avg_loss:0.456, val_acc:0.896]
Epoch [19/120    avg_loss:0.437, val_acc:0.910]
Epoch [20/120    avg_loss:0.460, val_acc:0.906]
Epoch [21/120    avg_loss:0.417, val_acc:0.894]
Epoch [22/120    avg_loss:0.414, val_acc:0.906]
Epoch [23/120    avg_loss:0.409, val_acc:0.867]
Epoch [24/120    avg_loss:0.433, val_acc:0.912]
Epoch [25/120    avg_loss:0.402, val_acc:0.910]
Epoch [26/120    avg_loss:0.397, val_acc:0.904]
Epoch [27/120    avg_loss:0.387, val_acc:0.912]
Epoch [28/120    avg_loss:0.317, val_acc:0.910]
Epoch [29/120    avg_loss:0.324, val_acc:0.923]
Epoch [30/120    avg_loss:0.276, val_acc:0.929]
Epoch [31/120    avg_loss:0.280, val_acc:0.915]
Epoch [32/120    avg_loss:0.297, val_acc:0.910]
Epoch [33/120    avg_loss:0.260, val_acc:0.935]
Epoch [34/120    avg_loss:0.286, val_acc:0.923]
Epoch [35/120    avg_loss:0.288, val_acc:0.935]
Epoch [36/120    avg_loss:0.284, val_acc:0.927]
Epoch [37/120    avg_loss:0.304, val_acc:0.921]
Epoch [38/120    avg_loss:0.295, val_acc:0.927]
Epoch [39/120    avg_loss:0.243, val_acc:0.935]
Epoch [40/120    avg_loss:0.234, val_acc:0.944]
Epoch [41/120    avg_loss:0.225, val_acc:0.894]
Epoch [42/120    avg_loss:0.275, val_acc:0.912]
Epoch [43/120    avg_loss:0.239, val_acc:0.946]
Epoch [44/120    avg_loss:0.232, val_acc:0.942]
Epoch [45/120    avg_loss:0.246, val_acc:0.944]
Epoch [46/120    avg_loss:0.205, val_acc:0.931]
Epoch [47/120    avg_loss:0.215, val_acc:0.929]
Epoch [48/120    avg_loss:0.217, val_acc:0.940]
Epoch [49/120    avg_loss:0.258, val_acc:0.940]
Epoch [50/120    avg_loss:0.217, val_acc:0.887]
Epoch [51/120    avg_loss:0.206, val_acc:0.944]
Epoch [52/120    avg_loss:0.204, val_acc:0.950]
Epoch [53/120    avg_loss:0.146, val_acc:0.956]
Epoch [54/120    avg_loss:0.168, val_acc:0.956]
Epoch [55/120    avg_loss:0.158, val_acc:0.960]
Epoch [56/120    avg_loss:0.198, val_acc:0.935]
Epoch [57/120    avg_loss:0.161, val_acc:0.929]
Epoch [58/120    avg_loss:0.210, val_acc:0.921]
Epoch [59/120    avg_loss:0.185, val_acc:0.944]
Epoch [60/120    avg_loss:0.191, val_acc:0.904]
Epoch [61/120    avg_loss:0.157, val_acc:0.946]
Epoch [62/120    avg_loss:0.152, val_acc:0.954]
Epoch [63/120    avg_loss:0.120, val_acc:0.952]
Epoch [64/120    avg_loss:0.112, val_acc:0.963]
Epoch [65/120    avg_loss:0.122, val_acc:0.967]
Epoch [66/120    avg_loss:0.129, val_acc:0.946]
Epoch [67/120    avg_loss:0.205, val_acc:0.952]
Epoch [68/120    avg_loss:0.115, val_acc:0.952]
Epoch [69/120    avg_loss:0.131, val_acc:0.954]
Epoch [70/120    avg_loss:0.121, val_acc:0.958]
Epoch [71/120    avg_loss:0.106, val_acc:0.950]
Epoch [72/120    avg_loss:0.104, val_acc:0.956]
Epoch [73/120    avg_loss:0.112, val_acc:0.956]
Epoch [74/120    avg_loss:0.075, val_acc:0.946]
Epoch [75/120    avg_loss:0.147, val_acc:0.960]
Epoch [76/120    avg_loss:0.103, val_acc:0.969]
Epoch [77/120    avg_loss:0.092, val_acc:0.958]
Epoch [78/120    avg_loss:0.120, val_acc:0.952]
Epoch [79/120    avg_loss:0.101, val_acc:0.960]
Epoch [80/120    avg_loss:0.088, val_acc:0.952]
Epoch [81/120    avg_loss:0.097, val_acc:0.952]
Epoch [82/120    avg_loss:0.086, val_acc:0.963]
Epoch [83/120    avg_loss:0.099, val_acc:0.963]
Epoch [84/120    avg_loss:0.094, val_acc:0.963]
Epoch [85/120    avg_loss:0.095, val_acc:0.960]
Epoch [86/120    avg_loss:0.073, val_acc:0.965]
Epoch [87/120    avg_loss:0.078, val_acc:0.963]
Epoch [88/120    avg_loss:0.095, val_acc:0.963]
Epoch [89/120    avg_loss:0.065, val_acc:0.960]
Epoch [90/120    avg_loss:0.066, val_acc:0.969]
Epoch [91/120    avg_loss:0.058, val_acc:0.975]
Epoch [92/120    avg_loss:0.047, val_acc:0.971]
Epoch [93/120    avg_loss:0.063, val_acc:0.971]
Epoch [94/120    avg_loss:0.039, val_acc:0.971]
Epoch [95/120    avg_loss:0.041, val_acc:0.973]
Epoch [96/120    avg_loss:0.036, val_acc:0.969]
Epoch [97/120    avg_loss:0.039, val_acc:0.969]
Epoch [98/120    avg_loss:0.053, val_acc:0.973]
Epoch [99/120    avg_loss:0.037, val_acc:0.973]
Epoch [100/120    avg_loss:0.047, val_acc:0.971]
Epoch [101/120    avg_loss:0.037, val_acc:0.971]
Epoch [102/120    avg_loss:0.041, val_acc:0.973]
Epoch [103/120    avg_loss:0.048, val_acc:0.971]
Epoch [104/120    avg_loss:0.047, val_acc:0.971]
Epoch [105/120    avg_loss:0.044, val_acc:0.971]
Epoch [106/120    avg_loss:0.041, val_acc:0.971]
Epoch [107/120    avg_loss:0.040, val_acc:0.971]
Epoch [108/120    avg_loss:0.046, val_acc:0.969]
Epoch [109/120    avg_loss:0.043, val_acc:0.969]
Epoch [110/120    avg_loss:0.028, val_acc:0.969]
Epoch [111/120    avg_loss:0.046, val_acc:0.969]
Epoch [112/120    avg_loss:0.031, val_acc:0.969]
Epoch [113/120    avg_loss:0.041, val_acc:0.971]
Epoch [114/120    avg_loss:0.033, val_acc:0.971]
Epoch [115/120    avg_loss:0.029, val_acc:0.971]
Epoch [116/120    avg_loss:0.042, val_acc:0.971]
Epoch [117/120    avg_loss:0.040, val_acc:0.971]
Epoch [118/120    avg_loss:0.031, val_acc:0.971]
Epoch [119/120    avg_loss:0.041, val_acc:0.971]
Epoch [120/120    avg_loss:0.058, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 211   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 201  24   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 207  19   0   0   0   0   1   0   0   0]
 [  0   0   0  10   4 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.18763326226012

F1 scores:
[       nan 0.99927061 0.95475113 0.91156463 0.8961039  0.88813559
 1.         0.89617486 0.99487179 0.99893276 0.99862826 0.99734043
 0.99779736 1.        ]

Kappa:
0.9798207399362363
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2abfdf7828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.447, val_acc:0.444]
Epoch [2/120    avg_loss:2.102, val_acc:0.496]
Epoch [3/120    avg_loss:1.858, val_acc:0.615]
Epoch [4/120    avg_loss:1.586, val_acc:0.646]
Epoch [5/120    avg_loss:1.381, val_acc:0.662]
Epoch [6/120    avg_loss:1.199, val_acc:0.775]
Epoch [7/120    avg_loss:1.062, val_acc:0.796]
Epoch [8/120    avg_loss:0.939, val_acc:0.842]
Epoch [9/120    avg_loss:0.874, val_acc:0.852]
Epoch [10/120    avg_loss:0.783, val_acc:0.869]
Epoch [11/120    avg_loss:0.700, val_acc:0.867]
Epoch [12/120    avg_loss:0.657, val_acc:0.896]
Epoch [13/120    avg_loss:0.649, val_acc:0.894]
Epoch [14/120    avg_loss:0.589, val_acc:0.896]
Epoch [15/120    avg_loss:0.531, val_acc:0.879]
Epoch [16/120    avg_loss:0.521, val_acc:0.894]
Epoch [17/120    avg_loss:0.529, val_acc:0.892]
Epoch [18/120    avg_loss:0.545, val_acc:0.904]
Epoch [19/120    avg_loss:0.429, val_acc:0.923]
Epoch [20/120    avg_loss:0.452, val_acc:0.917]
Epoch [21/120    avg_loss:0.446, val_acc:0.929]
Epoch [22/120    avg_loss:0.383, val_acc:0.906]
Epoch [23/120    avg_loss:0.435, val_acc:0.869]
Epoch [24/120    avg_loss:0.436, val_acc:0.915]
Epoch [25/120    avg_loss:0.383, val_acc:0.938]
Epoch [26/120    avg_loss:0.312, val_acc:0.925]
Epoch [27/120    avg_loss:0.329, val_acc:0.935]
Epoch [28/120    avg_loss:0.296, val_acc:0.942]
Epoch [29/120    avg_loss:0.275, val_acc:0.938]
Epoch [30/120    avg_loss:0.261, val_acc:0.948]
Epoch [31/120    avg_loss:0.242, val_acc:0.948]
Epoch [32/120    avg_loss:0.323, val_acc:0.931]
Epoch [33/120    avg_loss:0.273, val_acc:0.925]
Epoch [34/120    avg_loss:0.300, val_acc:0.944]
Epoch [35/120    avg_loss:0.264, val_acc:0.942]
Epoch [36/120    avg_loss:0.299, val_acc:0.954]
Epoch [37/120    avg_loss:0.241, val_acc:0.942]
Epoch [38/120    avg_loss:0.249, val_acc:0.958]
Epoch [39/120    avg_loss:0.235, val_acc:0.952]
Epoch [40/120    avg_loss:0.241, val_acc:0.956]
Epoch [41/120    avg_loss:0.227, val_acc:0.950]
Epoch [42/120    avg_loss:0.212, val_acc:0.940]
Epoch [43/120    avg_loss:0.217, val_acc:0.946]
Epoch [44/120    avg_loss:0.227, val_acc:0.956]
Epoch [45/120    avg_loss:0.183, val_acc:0.948]
Epoch [46/120    avg_loss:0.194, val_acc:0.944]
Epoch [47/120    avg_loss:0.178, val_acc:0.956]
Epoch [48/120    avg_loss:0.198, val_acc:0.956]
Epoch [49/120    avg_loss:0.181, val_acc:0.944]
Epoch [50/120    avg_loss:0.202, val_acc:0.950]
Epoch [51/120    avg_loss:0.204, val_acc:0.933]
Epoch [52/120    avg_loss:0.222, val_acc:0.952]
Epoch [53/120    avg_loss:0.160, val_acc:0.963]
Epoch [54/120    avg_loss:0.142, val_acc:0.965]
Epoch [55/120    avg_loss:0.130, val_acc:0.965]
Epoch [56/120    avg_loss:0.134, val_acc:0.960]
Epoch [57/120    avg_loss:0.117, val_acc:0.963]
Epoch [58/120    avg_loss:0.122, val_acc:0.967]
Epoch [59/120    avg_loss:0.140, val_acc:0.965]
Epoch [60/120    avg_loss:0.114, val_acc:0.967]
Epoch [61/120    avg_loss:0.119, val_acc:0.967]
Epoch [62/120    avg_loss:0.113, val_acc:0.965]
Epoch [63/120    avg_loss:0.116, val_acc:0.971]
Epoch [64/120    avg_loss:0.130, val_acc:0.969]
Epoch [65/120    avg_loss:0.123, val_acc:0.969]
Epoch [66/120    avg_loss:0.118, val_acc:0.969]
Epoch [67/120    avg_loss:0.101, val_acc:0.969]
Epoch [68/120    avg_loss:0.115, val_acc:0.969]
Epoch [69/120    avg_loss:0.112, val_acc:0.971]
Epoch [70/120    avg_loss:0.109, val_acc:0.971]
Epoch [71/120    avg_loss:0.088, val_acc:0.971]
Epoch [72/120    avg_loss:0.109, val_acc:0.971]
Epoch [73/120    avg_loss:0.107, val_acc:0.967]
Epoch [74/120    avg_loss:0.101, val_acc:0.969]
Epoch [75/120    avg_loss:0.099, val_acc:0.967]
Epoch [76/120    avg_loss:0.088, val_acc:0.967]
Epoch [77/120    avg_loss:0.098, val_acc:0.971]
Epoch [78/120    avg_loss:0.101, val_acc:0.969]
Epoch [79/120    avg_loss:0.091, val_acc:0.969]
Epoch [80/120    avg_loss:0.107, val_acc:0.967]
Epoch [81/120    avg_loss:0.114, val_acc:0.967]
Epoch [82/120    avg_loss:0.096, val_acc:0.969]
Epoch [83/120    avg_loss:0.091, val_acc:0.971]
Epoch [84/120    avg_loss:0.113, val_acc:0.969]
Epoch [85/120    avg_loss:0.104, val_acc:0.969]
Epoch [86/120    avg_loss:0.106, val_acc:0.963]
Epoch [87/120    avg_loss:0.107, val_acc:0.969]
Epoch [88/120    avg_loss:0.089, val_acc:0.971]
Epoch [89/120    avg_loss:0.080, val_acc:0.969]
Epoch [90/120    avg_loss:0.088, val_acc:0.971]
Epoch [91/120    avg_loss:0.088, val_acc:0.973]
Epoch [92/120    avg_loss:0.084, val_acc:0.971]
Epoch [93/120    avg_loss:0.085, val_acc:0.971]
Epoch [94/120    avg_loss:0.107, val_acc:0.973]
Epoch [95/120    avg_loss:0.095, val_acc:0.969]
Epoch [96/120    avg_loss:0.104, val_acc:0.969]
Epoch [97/120    avg_loss:0.083, val_acc:0.969]
Epoch [98/120    avg_loss:0.079, val_acc:0.971]
Epoch [99/120    avg_loss:0.087, val_acc:0.967]
Epoch [100/120    avg_loss:0.093, val_acc:0.967]
Epoch [101/120    avg_loss:0.099, val_acc:0.969]
Epoch [102/120    avg_loss:0.080, val_acc:0.971]
Epoch [103/120    avg_loss:0.090, val_acc:0.971]
Epoch [104/120    avg_loss:0.091, val_acc:0.967]
Epoch [105/120    avg_loss:0.093, val_acc:0.969]
Epoch [106/120    avg_loss:0.078, val_acc:0.971]
Epoch [107/120    avg_loss:0.090, val_acc:0.971]
Epoch [108/120    avg_loss:0.080, val_acc:0.971]
Epoch [109/120    avg_loss:0.099, val_acc:0.971]
Epoch [110/120    avg_loss:0.084, val_acc:0.971]
Epoch [111/120    avg_loss:0.087, val_acc:0.971]
Epoch [112/120    avg_loss:0.085, val_acc:0.971]
Epoch [113/120    avg_loss:0.082, val_acc:0.971]
Epoch [114/120    avg_loss:0.094, val_acc:0.971]
Epoch [115/120    avg_loss:0.077, val_acc:0.971]
Epoch [116/120    avg_loss:0.087, val_acc:0.971]
Epoch [117/120    avg_loss:0.080, val_acc:0.971]
Epoch [118/120    avg_loss:0.080, val_acc:0.971]
Epoch [119/120    avg_loss:0.107, val_acc:0.971]
Epoch [120/120    avg_loss:0.082, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 216   6   0   0   0   8   0   0   0   0   0]
 [  0   0   0   0 200  26   0   0   1   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   4   0   1   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.18763326226012

F1 scores:
[       nan 1.         0.95302013 0.96860987 0.87912088 0.84067797
 1.         0.90710383 0.98853503 1.         1.         0.99867198
 0.99334812 1.        ]

Kappa:
0.9798220289285242
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab02f91860>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.479, val_acc:0.427]
Epoch [2/120    avg_loss:2.137, val_acc:0.454]
Epoch [3/120    avg_loss:1.928, val_acc:0.573]
Epoch [4/120    avg_loss:1.704, val_acc:0.617]
Epoch [5/120    avg_loss:1.476, val_acc:0.648]
Epoch [6/120    avg_loss:1.297, val_acc:0.669]
Epoch [7/120    avg_loss:1.155, val_acc:0.702]
Epoch [8/120    avg_loss:1.041, val_acc:0.767]
Epoch [9/120    avg_loss:0.963, val_acc:0.808]
Epoch [10/120    avg_loss:0.871, val_acc:0.823]
Epoch [11/120    avg_loss:0.860, val_acc:0.842]
Epoch [12/120    avg_loss:0.758, val_acc:0.879]
Epoch [13/120    avg_loss:0.662, val_acc:0.904]
Epoch [14/120    avg_loss:0.582, val_acc:0.894]
Epoch [15/120    avg_loss:0.544, val_acc:0.915]
Epoch [16/120    avg_loss:0.511, val_acc:0.908]
Epoch [17/120    avg_loss:0.479, val_acc:0.898]
Epoch [18/120    avg_loss:0.496, val_acc:0.915]
Epoch [19/120    avg_loss:0.494, val_acc:0.912]
Epoch [20/120    avg_loss:0.446, val_acc:0.917]
Epoch [21/120    avg_loss:0.457, val_acc:0.927]
Epoch [22/120    avg_loss:0.478, val_acc:0.906]
Epoch [23/120    avg_loss:0.411, val_acc:0.908]
Epoch [24/120    avg_loss:0.442, val_acc:0.938]
Epoch [25/120    avg_loss:0.398, val_acc:0.900]
Epoch [26/120    avg_loss:0.373, val_acc:0.906]
Epoch [27/120    avg_loss:0.397, val_acc:0.908]
Epoch [28/120    avg_loss:0.412, val_acc:0.938]
Epoch [29/120    avg_loss:0.382, val_acc:0.942]
Epoch [30/120    avg_loss:0.325, val_acc:0.938]
Epoch [31/120    avg_loss:0.336, val_acc:0.917]
Epoch [32/120    avg_loss:0.324, val_acc:0.950]
Epoch [33/120    avg_loss:0.262, val_acc:0.956]
Epoch [34/120    avg_loss:0.249, val_acc:0.906]
Epoch [35/120    avg_loss:0.351, val_acc:0.881]
Epoch [36/120    avg_loss:0.348, val_acc:0.879]
Epoch [37/120    avg_loss:0.280, val_acc:0.952]
Epoch [38/120    avg_loss:0.237, val_acc:0.940]
Epoch [39/120    avg_loss:0.219, val_acc:0.967]
Epoch [40/120    avg_loss:0.236, val_acc:0.940]
Epoch [41/120    avg_loss:0.299, val_acc:0.960]
Epoch [42/120    avg_loss:0.220, val_acc:0.956]
Epoch [43/120    avg_loss:0.225, val_acc:0.942]
Epoch [44/120    avg_loss:0.232, val_acc:0.967]
Epoch [45/120    avg_loss:0.190, val_acc:0.963]
Epoch [46/120    avg_loss:0.295, val_acc:0.938]
Epoch [47/120    avg_loss:0.224, val_acc:0.954]
Epoch [48/120    avg_loss:0.212, val_acc:0.973]
Epoch [49/120    avg_loss:0.178, val_acc:0.971]
Epoch [50/120    avg_loss:0.259, val_acc:0.977]
Epoch [51/120    avg_loss:0.244, val_acc:0.967]
Epoch [52/120    avg_loss:0.181, val_acc:0.977]
Epoch [53/120    avg_loss:0.137, val_acc:0.971]
Epoch [54/120    avg_loss:0.143, val_acc:0.963]
Epoch [55/120    avg_loss:0.172, val_acc:0.977]
Epoch [56/120    avg_loss:0.142, val_acc:0.977]
Epoch [57/120    avg_loss:0.154, val_acc:0.977]
Epoch [58/120    avg_loss:0.117, val_acc:0.977]
Epoch [59/120    avg_loss:0.122, val_acc:0.952]
Epoch [60/120    avg_loss:0.103, val_acc:0.983]
Epoch [61/120    avg_loss:0.158, val_acc:0.981]
Epoch [62/120    avg_loss:0.159, val_acc:0.973]
Epoch [63/120    avg_loss:0.144, val_acc:0.960]
Epoch [64/120    avg_loss:0.113, val_acc:0.983]
Epoch [65/120    avg_loss:0.137, val_acc:0.956]
Epoch [66/120    avg_loss:0.140, val_acc:0.979]
Epoch [67/120    avg_loss:0.124, val_acc:0.946]
Epoch [68/120    avg_loss:0.117, val_acc:0.979]
Epoch [69/120    avg_loss:0.104, val_acc:0.971]
Epoch [70/120    avg_loss:0.119, val_acc:0.985]
Epoch [71/120    avg_loss:0.087, val_acc:0.981]
Epoch [72/120    avg_loss:0.126, val_acc:0.979]
Epoch [73/120    avg_loss:0.096, val_acc:0.975]
Epoch [74/120    avg_loss:0.097, val_acc:0.979]
Epoch [75/120    avg_loss:0.089, val_acc:0.981]
Epoch [76/120    avg_loss:0.082, val_acc:0.983]
Epoch [77/120    avg_loss:0.092, val_acc:0.977]
Epoch [78/120    avg_loss:0.071, val_acc:0.981]
Epoch [79/120    avg_loss:0.085, val_acc:0.973]
Epoch [80/120    avg_loss:0.057, val_acc:0.975]
Epoch [81/120    avg_loss:0.074, val_acc:0.975]
Epoch [82/120    avg_loss:0.081, val_acc:0.981]
Epoch [83/120    avg_loss:0.070, val_acc:0.977]
Epoch [84/120    avg_loss:0.040, val_acc:0.981]
Epoch [85/120    avg_loss:0.069, val_acc:0.981]
Epoch [86/120    avg_loss:0.061, val_acc:0.981]
Epoch [87/120    avg_loss:0.050, val_acc:0.983]
Epoch [88/120    avg_loss:0.034, val_acc:0.988]
Epoch [89/120    avg_loss:0.052, val_acc:0.985]
Epoch [90/120    avg_loss:0.049, val_acc:0.985]
Epoch [91/120    avg_loss:0.052, val_acc:0.985]
Epoch [92/120    avg_loss:0.044, val_acc:0.985]
Epoch [93/120    avg_loss:0.042, val_acc:0.985]
Epoch [94/120    avg_loss:0.043, val_acc:0.985]
Epoch [95/120    avg_loss:0.045, val_acc:0.981]
Epoch [96/120    avg_loss:0.051, val_acc:0.983]
Epoch [97/120    avg_loss:0.037, val_acc:0.983]
Epoch [98/120    avg_loss:0.054, val_acc:0.983]
Epoch [99/120    avg_loss:0.047, val_acc:0.985]
Epoch [100/120    avg_loss:0.046, val_acc:0.985]
Epoch [101/120    avg_loss:0.036, val_acc:0.981]
Epoch [102/120    avg_loss:0.032, val_acc:0.981]
Epoch [103/120    avg_loss:0.061, val_acc:0.981]
Epoch [104/120    avg_loss:0.045, val_acc:0.983]
Epoch [105/120    avg_loss:0.043, val_acc:0.983]
Epoch [106/120    avg_loss:0.032, val_acc:0.981]
Epoch [107/120    avg_loss:0.040, val_acc:0.981]
Epoch [108/120    avg_loss:0.039, val_acc:0.981]
Epoch [109/120    avg_loss:0.044, val_acc:0.981]
Epoch [110/120    avg_loss:0.043, val_acc:0.983]
Epoch [111/120    avg_loss:0.038, val_acc:0.983]
Epoch [112/120    avg_loss:0.038, val_acc:0.983]
Epoch [113/120    avg_loss:0.040, val_acc:0.983]
Epoch [114/120    avg_loss:0.043, val_acc:0.983]
Epoch [115/120    avg_loss:0.040, val_acc:0.983]
Epoch [116/120    avg_loss:0.042, val_acc:0.983]
Epoch [117/120    avg_loss:0.039, val_acc:0.983]
Epoch [118/120    avg_loss:0.060, val_acc:0.983]
Epoch [119/120    avg_loss:0.048, val_acc:0.983]
Epoch [120/120    avg_loss:0.045, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 224   4   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 198  27   0   0   0   0   2   0   0   0]
 [  0   0   0   2   5 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   1   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.61407249466951

F1 scores:
[       nan 1.         0.95132743 0.98245614 0.9124424  0.89032258
 1.         0.88       0.99742931 1.         0.99589603 1.
 0.99778761 1.        ]

Kappa:
0.9845700133178222
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d84fe07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.531, val_acc:0.415]
Epoch [2/120    avg_loss:2.089, val_acc:0.542]
Epoch [3/120    avg_loss:1.776, val_acc:0.608]
Epoch [4/120    avg_loss:1.567, val_acc:0.719]
Epoch [5/120    avg_loss:1.363, val_acc:0.740]
Epoch [6/120    avg_loss:1.207, val_acc:0.808]
Epoch [7/120    avg_loss:1.077, val_acc:0.773]
Epoch [8/120    avg_loss:1.002, val_acc:0.794]
Epoch [9/120    avg_loss:0.945, val_acc:0.842]
Epoch [10/120    avg_loss:0.863, val_acc:0.825]
Epoch [11/120    avg_loss:0.764, val_acc:0.819]
Epoch [12/120    avg_loss:0.736, val_acc:0.850]
Epoch [13/120    avg_loss:0.692, val_acc:0.858]
Epoch [14/120    avg_loss:0.635, val_acc:0.856]
Epoch [15/120    avg_loss:0.564, val_acc:0.890]
Epoch [16/120    avg_loss:0.569, val_acc:0.898]
Epoch [17/120    avg_loss:0.557, val_acc:0.881]
Epoch [18/120    avg_loss:0.516, val_acc:0.887]
Epoch [19/120    avg_loss:0.507, val_acc:0.915]
Epoch [20/120    avg_loss:0.485, val_acc:0.817]
Epoch [21/120    avg_loss:0.545, val_acc:0.898]
Epoch [22/120    avg_loss:0.441, val_acc:0.906]
Epoch [23/120    avg_loss:0.404, val_acc:0.923]
Epoch [24/120    avg_loss:0.379, val_acc:0.923]
Epoch [25/120    avg_loss:0.428, val_acc:0.929]
Epoch [26/120    avg_loss:0.404, val_acc:0.896]
Epoch [27/120    avg_loss:0.431, val_acc:0.892]
Epoch [28/120    avg_loss:0.323, val_acc:0.946]
Epoch [29/120    avg_loss:0.292, val_acc:0.950]
Epoch [30/120    avg_loss:0.327, val_acc:0.935]
Epoch [31/120    avg_loss:0.289, val_acc:0.940]
Epoch [32/120    avg_loss:0.332, val_acc:0.894]
Epoch [33/120    avg_loss:0.291, val_acc:0.944]
Epoch [34/120    avg_loss:0.265, val_acc:0.950]
Epoch [35/120    avg_loss:0.241, val_acc:0.944]
Epoch [36/120    avg_loss:0.263, val_acc:0.950]
Epoch [37/120    avg_loss:0.276, val_acc:0.960]
Epoch [38/120    avg_loss:0.229, val_acc:0.933]
Epoch [39/120    avg_loss:0.223, val_acc:0.958]
Epoch [40/120    avg_loss:0.212, val_acc:0.956]
Epoch [41/120    avg_loss:0.259, val_acc:0.948]
Epoch [42/120    avg_loss:0.221, val_acc:0.915]
Epoch [43/120    avg_loss:0.251, val_acc:0.952]
Epoch [44/120    avg_loss:0.244, val_acc:0.952]
Epoch [45/120    avg_loss:0.205, val_acc:0.969]
Epoch [46/120    avg_loss:0.171, val_acc:0.950]
Epoch [47/120    avg_loss:0.192, val_acc:0.967]
Epoch [48/120    avg_loss:0.179, val_acc:0.925]
Epoch [49/120    avg_loss:0.155, val_acc:0.950]
Epoch [50/120    avg_loss:0.149, val_acc:0.948]
Epoch [51/120    avg_loss:0.139, val_acc:0.969]
Epoch [52/120    avg_loss:0.120, val_acc:0.958]
Epoch [53/120    avg_loss:0.125, val_acc:0.952]
Epoch [54/120    avg_loss:0.203, val_acc:0.954]
Epoch [55/120    avg_loss:0.127, val_acc:0.965]
Epoch [56/120    avg_loss:0.135, val_acc:0.967]
Epoch [57/120    avg_loss:0.142, val_acc:0.965]
Epoch [58/120    avg_loss:0.132, val_acc:0.944]
Epoch [59/120    avg_loss:0.134, val_acc:0.935]
Epoch [60/120    avg_loss:0.172, val_acc:0.954]
Epoch [61/120    avg_loss:0.228, val_acc:0.950]
Epoch [62/120    avg_loss:0.183, val_acc:0.963]
Epoch [63/120    avg_loss:0.133, val_acc:0.969]
Epoch [64/120    avg_loss:0.113, val_acc:0.973]
Epoch [65/120    avg_loss:0.111, val_acc:0.960]
Epoch [66/120    avg_loss:0.109, val_acc:0.967]
Epoch [67/120    avg_loss:0.129, val_acc:0.956]
Epoch [68/120    avg_loss:0.092, val_acc:0.960]
Epoch [69/120    avg_loss:0.087, val_acc:0.973]
Epoch [70/120    avg_loss:0.054, val_acc:0.971]
Epoch [71/120    avg_loss:0.062, val_acc:0.963]
Epoch [72/120    avg_loss:0.081, val_acc:0.971]
Epoch [73/120    avg_loss:0.074, val_acc:0.954]
Epoch [74/120    avg_loss:0.103, val_acc:0.965]
Epoch [75/120    avg_loss:0.113, val_acc:0.958]
Epoch [76/120    avg_loss:0.060, val_acc:0.973]
Epoch [77/120    avg_loss:0.073, val_acc:0.969]
Epoch [78/120    avg_loss:0.101, val_acc:0.967]
Epoch [79/120    avg_loss:0.073, val_acc:0.967]
Epoch [80/120    avg_loss:0.094, val_acc:0.963]
Epoch [81/120    avg_loss:0.073, val_acc:0.956]
Epoch [82/120    avg_loss:0.095, val_acc:0.979]
Epoch [83/120    avg_loss:0.065, val_acc:0.975]
Epoch [84/120    avg_loss:0.065, val_acc:0.971]
Epoch [85/120    avg_loss:0.041, val_acc:0.977]
Epoch [86/120    avg_loss:0.050, val_acc:0.983]
Epoch [87/120    avg_loss:0.045, val_acc:0.977]
Epoch [88/120    avg_loss:0.068, val_acc:0.977]
Epoch [89/120    avg_loss:0.049, val_acc:0.973]
Epoch [90/120    avg_loss:0.060, val_acc:0.979]
Epoch [91/120    avg_loss:0.066, val_acc:0.973]
Epoch [92/120    avg_loss:0.121, val_acc:0.969]
Epoch [93/120    avg_loss:0.103, val_acc:0.973]
Epoch [94/120    avg_loss:0.066, val_acc:0.973]
Epoch [95/120    avg_loss:0.069, val_acc:0.975]
Epoch [96/120    avg_loss:0.067, val_acc:0.977]
Epoch [97/120    avg_loss:0.060, val_acc:0.973]
Epoch [98/120    avg_loss:0.106, val_acc:0.977]
Epoch [99/120    avg_loss:0.083, val_acc:0.952]
Epoch [100/120    avg_loss:0.080, val_acc:0.963]
Epoch [101/120    avg_loss:0.049, val_acc:0.973]
Epoch [102/120    avg_loss:0.037, val_acc:0.977]
Epoch [103/120    avg_loss:0.034, val_acc:0.977]
Epoch [104/120    avg_loss:0.026, val_acc:0.975]
Epoch [105/120    avg_loss:0.028, val_acc:0.975]
Epoch [106/120    avg_loss:0.023, val_acc:0.975]
Epoch [107/120    avg_loss:0.032, val_acc:0.977]
Epoch [108/120    avg_loss:0.031, val_acc:0.977]
Epoch [109/120    avg_loss:0.029, val_acc:0.977]
Epoch [110/120    avg_loss:0.026, val_acc:0.977]
Epoch [111/120    avg_loss:0.039, val_acc:0.977]
Epoch [112/120    avg_loss:0.033, val_acc:0.979]
Epoch [113/120    avg_loss:0.023, val_acc:0.979]
Epoch [114/120    avg_loss:0.027, val_acc:0.977]
Epoch [115/120    avg_loss:0.025, val_acc:0.977]
Epoch [116/120    avg_loss:0.019, val_acc:0.977]
Epoch [117/120    avg_loss:0.026, val_acc:0.977]
Epoch [118/120    avg_loss:0.026, val_acc:0.977]
Epoch [119/120    avg_loss:0.024, val_acc:0.977]
Epoch [120/120    avg_loss:0.033, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   5   0   0   5   0   0   0   0   1   0]
 [  0   0   0 209   9   6   1   0   3   2   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   2  25 117   0   0   1   0   0   0   0   0]
 [  0   1   0   0   1   0 204   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.25159914712154

F1 scores:
[       nan 0.99927061 0.94977169 0.9478458  0.89896907 0.84476534
 0.99270073 0.91208791 0.99487179 0.9978678  1.         1.
 0.99889746 1.        ]

Kappa:
0.980531135805579
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbec6ac4710>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.482, val_acc:0.369]
Epoch [2/120    avg_loss:2.111, val_acc:0.452]
Epoch [3/120    avg_loss:1.837, val_acc:0.748]
Epoch [4/120    avg_loss:1.629, val_acc:0.683]
Epoch [5/120    avg_loss:1.413, val_acc:0.694]
Epoch [6/120    avg_loss:1.251, val_acc:0.733]
Epoch [7/120    avg_loss:1.109, val_acc:0.854]
Epoch [8/120    avg_loss:0.995, val_acc:0.840]
Epoch [9/120    avg_loss:0.903, val_acc:0.863]
Epoch [10/120    avg_loss:0.783, val_acc:0.890]
Epoch [11/120    avg_loss:0.724, val_acc:0.898]
Epoch [12/120    avg_loss:0.633, val_acc:0.879]
Epoch [13/120    avg_loss:0.672, val_acc:0.904]
Epoch [14/120    avg_loss:0.578, val_acc:0.921]
Epoch [15/120    avg_loss:0.508, val_acc:0.946]
Epoch [16/120    avg_loss:0.518, val_acc:0.933]
Epoch [17/120    avg_loss:0.464, val_acc:0.931]
Epoch [18/120    avg_loss:0.457, val_acc:0.925]
Epoch [19/120    avg_loss:0.425, val_acc:0.938]
Epoch [20/120    avg_loss:0.399, val_acc:0.940]
Epoch [21/120    avg_loss:0.391, val_acc:0.931]
Epoch [22/120    avg_loss:0.360, val_acc:0.935]
Epoch [23/120    avg_loss:0.357, val_acc:0.950]
Epoch [24/120    avg_loss:0.326, val_acc:0.927]
Epoch [25/120    avg_loss:0.358, val_acc:0.927]
Epoch [26/120    avg_loss:0.281, val_acc:0.958]
Epoch [27/120    avg_loss:0.307, val_acc:0.946]
Epoch [28/120    avg_loss:0.309, val_acc:0.965]
Epoch [29/120    avg_loss:0.283, val_acc:0.969]
Epoch [30/120    avg_loss:0.274, val_acc:0.973]
Epoch [31/120    avg_loss:0.268, val_acc:0.942]
Epoch [32/120    avg_loss:0.230, val_acc:0.969]
Epoch [33/120    avg_loss:0.204, val_acc:0.977]
Epoch [34/120    avg_loss:0.188, val_acc:0.960]
Epoch [35/120    avg_loss:0.241, val_acc:0.948]
Epoch [36/120    avg_loss:0.254, val_acc:0.965]
Epoch [37/120    avg_loss:0.210, val_acc:0.973]
Epoch [38/120    avg_loss:0.194, val_acc:0.965]
Epoch [39/120    avg_loss:0.260, val_acc:0.948]
Epoch [40/120    avg_loss:0.200, val_acc:0.958]
Epoch [41/120    avg_loss:0.193, val_acc:0.975]
Epoch [42/120    avg_loss:0.186, val_acc:0.975]
Epoch [43/120    avg_loss:0.220, val_acc:0.973]
Epoch [44/120    avg_loss:0.159, val_acc:0.969]
Epoch [45/120    avg_loss:0.196, val_acc:0.958]
Epoch [46/120    avg_loss:0.196, val_acc:0.944]
Epoch [47/120    avg_loss:0.189, val_acc:0.965]
Epoch [48/120    avg_loss:0.166, val_acc:0.973]
Epoch [49/120    avg_loss:0.157, val_acc:0.985]
Epoch [50/120    avg_loss:0.118, val_acc:0.983]
Epoch [51/120    avg_loss:0.125, val_acc:0.981]
Epoch [52/120    avg_loss:0.116, val_acc:0.983]
Epoch [53/120    avg_loss:0.131, val_acc:0.981]
Epoch [54/120    avg_loss:0.111, val_acc:0.985]
Epoch [55/120    avg_loss:0.120, val_acc:0.983]
Epoch [56/120    avg_loss:0.132, val_acc:0.988]
Epoch [57/120    avg_loss:0.107, val_acc:0.985]
Epoch [58/120    avg_loss:0.116, val_acc:0.983]
Epoch [59/120    avg_loss:0.130, val_acc:0.985]
Epoch [60/120    avg_loss:0.112, val_acc:0.981]
Epoch [61/120    avg_loss:0.104, val_acc:0.983]
Epoch [62/120    avg_loss:0.125, val_acc:0.983]
Epoch [63/120    avg_loss:0.122, val_acc:0.985]
Epoch [64/120    avg_loss:0.120, val_acc:0.981]
Epoch [65/120    avg_loss:0.100, val_acc:0.979]
Epoch [66/120    avg_loss:0.097, val_acc:0.979]
Epoch [67/120    avg_loss:0.103, val_acc:0.979]
Epoch [68/120    avg_loss:0.105, val_acc:0.979]
Epoch [69/120    avg_loss:0.111, val_acc:0.981]
Epoch [70/120    avg_loss:0.109, val_acc:0.979]
Epoch [71/120    avg_loss:0.086, val_acc:0.977]
Epoch [72/120    avg_loss:0.110, val_acc:0.977]
Epoch [73/120    avg_loss:0.112, val_acc:0.977]
Epoch [74/120    avg_loss:0.104, val_acc:0.979]
Epoch [75/120    avg_loss:0.092, val_acc:0.979]
Epoch [76/120    avg_loss:0.089, val_acc:0.979]
Epoch [77/120    avg_loss:0.093, val_acc:0.977]
Epoch [78/120    avg_loss:0.093, val_acc:0.977]
Epoch [79/120    avg_loss:0.117, val_acc:0.977]
Epoch [80/120    avg_loss:0.099, val_acc:0.979]
Epoch [81/120    avg_loss:0.094, val_acc:0.981]
Epoch [82/120    avg_loss:0.089, val_acc:0.983]
Epoch [83/120    avg_loss:0.108, val_acc:0.983]
Epoch [84/120    avg_loss:0.078, val_acc:0.983]
Epoch [85/120    avg_loss:0.098, val_acc:0.983]
Epoch [86/120    avg_loss:0.097, val_acc:0.983]
Epoch [87/120    avg_loss:0.105, val_acc:0.983]
Epoch [88/120    avg_loss:0.105, val_acc:0.983]
Epoch [89/120    avg_loss:0.108, val_acc:0.983]
Epoch [90/120    avg_loss:0.103, val_acc:0.983]
Epoch [91/120    avg_loss:0.096, val_acc:0.983]
Epoch [92/120    avg_loss:0.100, val_acc:0.983]
Epoch [93/120    avg_loss:0.088, val_acc:0.983]
Epoch [94/120    avg_loss:0.099, val_acc:0.983]
Epoch [95/120    avg_loss:0.105, val_acc:0.983]
Epoch [96/120    avg_loss:0.105, val_acc:0.983]
Epoch [97/120    avg_loss:0.102, val_acc:0.983]
Epoch [98/120    avg_loss:0.105, val_acc:0.983]
Epoch [99/120    avg_loss:0.106, val_acc:0.983]
Epoch [100/120    avg_loss:0.100, val_acc:0.983]
Epoch [101/120    avg_loss:0.093, val_acc:0.983]
Epoch [102/120    avg_loss:0.092, val_acc:0.983]
Epoch [103/120    avg_loss:0.090, val_acc:0.983]
Epoch [104/120    avg_loss:0.095, val_acc:0.983]
Epoch [105/120    avg_loss:0.093, val_acc:0.983]
Epoch [106/120    avg_loss:0.099, val_acc:0.983]
Epoch [107/120    avg_loss:0.089, val_acc:0.983]
Epoch [108/120    avg_loss:0.104, val_acc:0.983]
Epoch [109/120    avg_loss:0.091, val_acc:0.983]
Epoch [110/120    avg_loss:0.091, val_acc:0.983]
Epoch [111/120    avg_loss:0.089, val_acc:0.983]
Epoch [112/120    avg_loss:0.095, val_acc:0.983]
Epoch [113/120    avg_loss:0.087, val_acc:0.983]
Epoch [114/120    avg_loss:0.093, val_acc:0.983]
Epoch [115/120    avg_loss:0.094, val_acc:0.983]
Epoch [116/120    avg_loss:0.094, val_acc:0.983]
Epoch [117/120    avg_loss:0.087, val_acc:0.983]
Epoch [118/120    avg_loss:0.093, val_acc:0.983]
Epoch [119/120    avg_loss:0.104, val_acc:0.983]
Epoch [120/120    avg_loss:0.111, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 200   0   0   0   0  19   0   0   0   0   0   0]
 [  0   0   1 224   4   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0  35 110   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.10234541577825

F1 scores:
[       nan 1.         0.92807425 0.98678414 0.88469602 0.81481481
 1.         0.84693878 1.         0.99893276 1.         0.99734748
 0.99669239 1.        ]

Kappa:
0.9788718945174993
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc43cbcd828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.306]
Epoch [2/120    avg_loss:2.178, val_acc:0.481]
Epoch [3/120    avg_loss:1.912, val_acc:0.579]
Epoch [4/120    avg_loss:1.675, val_acc:0.610]
Epoch [5/120    avg_loss:1.485, val_acc:0.642]
Epoch [6/120    avg_loss:1.254, val_acc:0.733]
Epoch [7/120    avg_loss:1.208, val_acc:0.787]
Epoch [8/120    avg_loss:1.031, val_acc:0.835]
Epoch [9/120    avg_loss:0.945, val_acc:0.854]
Epoch [10/120    avg_loss:0.864, val_acc:0.885]
Epoch [11/120    avg_loss:0.783, val_acc:0.873]
Epoch [12/120    avg_loss:0.787, val_acc:0.883]
Epoch [13/120    avg_loss:0.698, val_acc:0.894]
Epoch [14/120    avg_loss:0.587, val_acc:0.890]
Epoch [15/120    avg_loss:0.548, val_acc:0.917]
Epoch [16/120    avg_loss:0.483, val_acc:0.910]
Epoch [17/120    avg_loss:0.460, val_acc:0.910]
Epoch [18/120    avg_loss:0.561, val_acc:0.883]
Epoch [19/120    avg_loss:0.552, val_acc:0.842]
Epoch [20/120    avg_loss:0.504, val_acc:0.921]
Epoch [21/120    avg_loss:0.442, val_acc:0.917]
Epoch [22/120    avg_loss:0.429, val_acc:0.881]
Epoch [23/120    avg_loss:0.463, val_acc:0.898]
Epoch [24/120    avg_loss:0.426, val_acc:0.919]
Epoch [25/120    avg_loss:0.357, val_acc:0.923]
Epoch [26/120    avg_loss:0.414, val_acc:0.892]
Epoch [27/120    avg_loss:0.399, val_acc:0.929]
Epoch [28/120    avg_loss:0.409, val_acc:0.908]
Epoch [29/120    avg_loss:0.319, val_acc:0.898]
Epoch [30/120    avg_loss:0.348, val_acc:0.938]
Epoch [31/120    avg_loss:0.321, val_acc:0.938]
Epoch [32/120    avg_loss:0.261, val_acc:0.933]
Epoch [33/120    avg_loss:0.310, val_acc:0.958]
Epoch [34/120    avg_loss:0.300, val_acc:0.929]
Epoch [35/120    avg_loss:0.266, val_acc:0.954]
Epoch [36/120    avg_loss:0.298, val_acc:0.940]
Epoch [37/120    avg_loss:0.302, val_acc:0.956]
Epoch [38/120    avg_loss:0.244, val_acc:0.948]
Epoch [39/120    avg_loss:0.210, val_acc:0.950]
Epoch [40/120    avg_loss:0.230, val_acc:0.919]
Epoch [41/120    avg_loss:0.216, val_acc:0.925]
Epoch [42/120    avg_loss:0.244, val_acc:0.933]
Epoch [43/120    avg_loss:0.207, val_acc:0.963]
Epoch [44/120    avg_loss:0.221, val_acc:0.960]
Epoch [45/120    avg_loss:0.197, val_acc:0.958]
Epoch [46/120    avg_loss:0.200, val_acc:0.963]
Epoch [47/120    avg_loss:0.177, val_acc:0.965]
Epoch [48/120    avg_loss:0.183, val_acc:0.969]
Epoch [49/120    avg_loss:0.153, val_acc:0.963]
Epoch [50/120    avg_loss:0.164, val_acc:0.967]
Epoch [51/120    avg_loss:0.169, val_acc:0.952]
Epoch [52/120    avg_loss:0.197, val_acc:0.948]
Epoch [53/120    avg_loss:0.179, val_acc:0.956]
Epoch [54/120    avg_loss:0.216, val_acc:0.965]
Epoch [55/120    avg_loss:0.183, val_acc:0.971]
Epoch [56/120    avg_loss:0.168, val_acc:0.940]
Epoch [57/120    avg_loss:0.177, val_acc:0.969]
Epoch [58/120    avg_loss:0.141, val_acc:0.965]
Epoch [59/120    avg_loss:0.153, val_acc:0.975]
Epoch [60/120    avg_loss:0.124, val_acc:0.983]
Epoch [61/120    avg_loss:0.098, val_acc:0.971]
Epoch [62/120    avg_loss:0.113, val_acc:0.981]
Epoch [63/120    avg_loss:0.124, val_acc:0.973]
Epoch [64/120    avg_loss:0.160, val_acc:0.985]
Epoch [65/120    avg_loss:0.137, val_acc:0.967]
Epoch [66/120    avg_loss:0.106, val_acc:0.971]
Epoch [67/120    avg_loss:0.114, val_acc:0.981]
Epoch [68/120    avg_loss:0.096, val_acc:0.975]
Epoch [69/120    avg_loss:0.121, val_acc:0.971]
Epoch [70/120    avg_loss:0.146, val_acc:0.973]
Epoch [71/120    avg_loss:0.119, val_acc:0.969]
Epoch [72/120    avg_loss:0.126, val_acc:0.979]
Epoch [73/120    avg_loss:0.100, val_acc:0.981]
Epoch [74/120    avg_loss:0.099, val_acc:0.967]
Epoch [75/120    avg_loss:0.080, val_acc:0.975]
Epoch [76/120    avg_loss:0.132, val_acc:0.981]
Epoch [77/120    avg_loss:0.080, val_acc:0.983]
Epoch [78/120    avg_loss:0.055, val_acc:0.981]
Epoch [79/120    avg_loss:0.077, val_acc:0.985]
Epoch [80/120    avg_loss:0.067, val_acc:0.988]
Epoch [81/120    avg_loss:0.053, val_acc:0.988]
Epoch [82/120    avg_loss:0.049, val_acc:0.981]
Epoch [83/120    avg_loss:0.053, val_acc:0.985]
Epoch [84/120    avg_loss:0.051, val_acc:0.985]
Epoch [85/120    avg_loss:0.055, val_acc:0.983]
Epoch [86/120    avg_loss:0.051, val_acc:0.985]
Epoch [87/120    avg_loss:0.051, val_acc:0.985]
Epoch [88/120    avg_loss:0.047, val_acc:0.988]
Epoch [89/120    avg_loss:0.049, val_acc:0.985]
Epoch [90/120    avg_loss:0.049, val_acc:0.981]
Epoch [91/120    avg_loss:0.051, val_acc:0.983]
Epoch [92/120    avg_loss:0.043, val_acc:0.988]
Epoch [93/120    avg_loss:0.051, val_acc:0.988]
Epoch [94/120    avg_loss:0.053, val_acc:0.985]
Epoch [95/120    avg_loss:0.060, val_acc:0.983]
Epoch [96/120    avg_loss:0.041, val_acc:0.983]
Epoch [97/120    avg_loss:0.054, val_acc:0.992]
Epoch [98/120    avg_loss:0.041, val_acc:0.992]
Epoch [99/120    avg_loss:0.043, val_acc:0.990]
Epoch [100/120    avg_loss:0.036, val_acc:0.988]
Epoch [101/120    avg_loss:0.053, val_acc:0.990]
Epoch [102/120    avg_loss:0.052, val_acc:0.990]
Epoch [103/120    avg_loss:0.044, val_acc:0.988]
Epoch [104/120    avg_loss:0.038, val_acc:0.988]
Epoch [105/120    avg_loss:0.046, val_acc:0.988]
Epoch [106/120    avg_loss:0.047, val_acc:0.985]
Epoch [107/120    avg_loss:0.035, val_acc:0.988]
Epoch [108/120    avg_loss:0.047, val_acc:0.988]
Epoch [109/120    avg_loss:0.041, val_acc:0.988]
Epoch [110/120    avg_loss:0.029, val_acc:0.988]
Epoch [111/120    avg_loss:0.036, val_acc:0.988]
Epoch [112/120    avg_loss:0.043, val_acc:0.988]
Epoch [113/120    avg_loss:0.038, val_acc:0.988]
Epoch [114/120    avg_loss:0.041, val_acc:0.988]
Epoch [115/120    avg_loss:0.033, val_acc:0.988]
Epoch [116/120    avg_loss:0.035, val_acc:0.988]
Epoch [117/120    avg_loss:0.049, val_acc:0.988]
Epoch [118/120    avg_loss:0.042, val_acc:0.988]
Epoch [119/120    avg_loss:0.040, val_acc:0.988]
Epoch [120/120    avg_loss:0.034, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   0 226   3   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 204  20   0   0   0   0   1   0   2   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 1.         0.96127563 0.99122807 0.89082969 0.84615385
 1.         0.90909091 1.         0.99893276 0.99862826 1.
 0.99779736 1.        ]

Kappa:
0.9838570225824015
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f30249ba7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.522, val_acc:0.417]
Epoch [2/120    avg_loss:2.126, val_acc:0.515]
Epoch [3/120    avg_loss:1.868, val_acc:0.565]
Epoch [4/120    avg_loss:1.681, val_acc:0.631]
Epoch [5/120    avg_loss:1.460, val_acc:0.635]
Epoch [6/120    avg_loss:1.304, val_acc:0.696]
Epoch [7/120    avg_loss:1.156, val_acc:0.738]
Epoch [8/120    avg_loss:1.012, val_acc:0.762]
Epoch [9/120    avg_loss:0.919, val_acc:0.785]
Epoch [10/120    avg_loss:0.838, val_acc:0.794]
Epoch [11/120    avg_loss:0.726, val_acc:0.838]
Epoch [12/120    avg_loss:0.693, val_acc:0.856]
Epoch [13/120    avg_loss:0.590, val_acc:0.912]
Epoch [14/120    avg_loss:0.585, val_acc:0.906]
Epoch [15/120    avg_loss:0.545, val_acc:0.910]
Epoch [16/120    avg_loss:0.564, val_acc:0.915]
Epoch [17/120    avg_loss:0.514, val_acc:0.925]
Epoch [18/120    avg_loss:0.456, val_acc:0.935]
Epoch [19/120    avg_loss:0.439, val_acc:0.919]
Epoch [20/120    avg_loss:0.417, val_acc:0.935]
Epoch [21/120    avg_loss:0.402, val_acc:0.948]
Epoch [22/120    avg_loss:0.383, val_acc:0.956]
Epoch [23/120    avg_loss:0.332, val_acc:0.956]
Epoch [24/120    avg_loss:0.316, val_acc:0.948]
Epoch [25/120    avg_loss:0.357, val_acc:0.923]
Epoch [26/120    avg_loss:0.341, val_acc:0.965]
Epoch [27/120    avg_loss:0.285, val_acc:0.969]
Epoch [28/120    avg_loss:0.267, val_acc:0.921]
Epoch [29/120    avg_loss:0.352, val_acc:0.952]
Epoch [30/120    avg_loss:0.317, val_acc:0.960]
Epoch [31/120    avg_loss:0.279, val_acc:0.960]
Epoch [32/120    avg_loss:0.232, val_acc:0.965]
Epoch [33/120    avg_loss:0.183, val_acc:0.956]
Epoch [34/120    avg_loss:0.236, val_acc:0.952]
Epoch [35/120    avg_loss:0.225, val_acc:0.933]
Epoch [36/120    avg_loss:0.298, val_acc:0.965]
Epoch [37/120    avg_loss:0.255, val_acc:0.971]
Epoch [38/120    avg_loss:0.255, val_acc:0.944]
Epoch [39/120    avg_loss:0.230, val_acc:0.963]
Epoch [40/120    avg_loss:0.209, val_acc:0.958]
Epoch [41/120    avg_loss:0.194, val_acc:0.950]
Epoch [42/120    avg_loss:0.189, val_acc:0.950]
Epoch [43/120    avg_loss:0.234, val_acc:0.967]
Epoch [44/120    avg_loss:0.159, val_acc:0.967]
Epoch [45/120    avg_loss:0.179, val_acc:0.983]
Epoch [46/120    avg_loss:0.177, val_acc:0.977]
Epoch [47/120    avg_loss:0.139, val_acc:0.983]
Epoch [48/120    avg_loss:0.122, val_acc:0.967]
Epoch [49/120    avg_loss:0.111, val_acc:0.969]
Epoch [50/120    avg_loss:0.122, val_acc:0.981]
Epoch [51/120    avg_loss:0.129, val_acc:0.967]
Epoch [52/120    avg_loss:0.117, val_acc:0.977]
Epoch [53/120    avg_loss:0.101, val_acc:0.988]
Epoch [54/120    avg_loss:0.119, val_acc:0.971]
Epoch [55/120    avg_loss:0.113, val_acc:0.985]
Epoch [56/120    avg_loss:0.101, val_acc:0.983]
Epoch [57/120    avg_loss:0.113, val_acc:0.973]
Epoch [58/120    avg_loss:0.095, val_acc:0.983]
Epoch [59/120    avg_loss:0.083, val_acc:0.975]
Epoch [60/120    avg_loss:0.100, val_acc:0.985]
Epoch [61/120    avg_loss:0.153, val_acc:0.963]
Epoch [62/120    avg_loss:0.091, val_acc:0.985]
Epoch [63/120    avg_loss:0.123, val_acc:0.985]
Epoch [64/120    avg_loss:0.087, val_acc:0.979]
Epoch [65/120    avg_loss:0.077, val_acc:0.988]
Epoch [66/120    avg_loss:0.058, val_acc:0.977]
Epoch [67/120    avg_loss:0.103, val_acc:0.988]
Epoch [68/120    avg_loss:0.056, val_acc:0.983]
Epoch [69/120    avg_loss:0.074, val_acc:0.973]
Epoch [70/120    avg_loss:0.073, val_acc:0.979]
Epoch [71/120    avg_loss:0.051, val_acc:0.988]
Epoch [72/120    avg_loss:0.062, val_acc:0.983]
Epoch [73/120    avg_loss:0.053, val_acc:0.983]
Epoch [74/120    avg_loss:0.073, val_acc:0.988]
Epoch [75/120    avg_loss:0.133, val_acc:0.971]
Epoch [76/120    avg_loss:0.203, val_acc:0.979]
Epoch [77/120    avg_loss:0.106, val_acc:0.988]
Epoch [78/120    avg_loss:0.061, val_acc:0.985]
Epoch [79/120    avg_loss:0.084, val_acc:0.988]
Epoch [80/120    avg_loss:0.087, val_acc:0.988]
Epoch [81/120    avg_loss:0.111, val_acc:0.985]
Epoch [82/120    avg_loss:0.084, val_acc:0.992]
Epoch [83/120    avg_loss:0.066, val_acc:0.979]
Epoch [84/120    avg_loss:0.071, val_acc:0.985]
Epoch [85/120    avg_loss:0.072, val_acc:0.979]
Epoch [86/120    avg_loss:0.075, val_acc:0.992]
Epoch [87/120    avg_loss:0.056, val_acc:0.992]
Epoch [88/120    avg_loss:0.048, val_acc:0.992]
Epoch [89/120    avg_loss:0.048, val_acc:0.994]
Epoch [90/120    avg_loss:0.039, val_acc:0.983]
Epoch [91/120    avg_loss:0.033, val_acc:0.990]
Epoch [92/120    avg_loss:0.032, val_acc:0.985]
Epoch [93/120    avg_loss:0.049, val_acc:0.994]
Epoch [94/120    avg_loss:0.039, val_acc:0.985]
Epoch [95/120    avg_loss:0.046, val_acc:0.985]
Epoch [96/120    avg_loss:0.042, val_acc:0.992]
Epoch [97/120    avg_loss:0.029, val_acc:0.992]
Epoch [98/120    avg_loss:0.037, val_acc:0.988]
Epoch [99/120    avg_loss:0.031, val_acc:0.992]
Epoch [100/120    avg_loss:0.039, val_acc:0.988]
Epoch [101/120    avg_loss:0.065, val_acc:0.994]
Epoch [102/120    avg_loss:0.048, val_acc:0.994]
Epoch [103/120    avg_loss:0.032, val_acc:0.992]
Epoch [104/120    avg_loss:0.034, val_acc:0.985]
Epoch [105/120    avg_loss:0.023, val_acc:0.996]
Epoch [106/120    avg_loss:0.025, val_acc:0.996]
Epoch [107/120    avg_loss:0.030, val_acc:0.973]
Epoch [108/120    avg_loss:0.038, val_acc:0.992]
Epoch [109/120    avg_loss:0.022, val_acc:0.990]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.039, val_acc:0.975]
Epoch [112/120    avg_loss:0.026, val_acc:0.992]
Epoch [113/120    avg_loss:0.070, val_acc:0.990]
Epoch [114/120    avg_loss:0.067, val_acc:0.933]
Epoch [115/120    avg_loss:0.069, val_acc:0.992]
Epoch [116/120    avg_loss:0.060, val_acc:0.971]
Epoch [117/120    avg_loss:0.113, val_acc:0.981]
Epoch [118/120    avg_loss:0.040, val_acc:0.981]
Epoch [119/120    avg_loss:0.045, val_acc:0.979]
Epoch [120/120    avg_loss:0.042, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 201  29   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 204  19   0   0   0   0   3   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.35820895522387

F1 scores:
[       nan 1.         0.96162528 0.93055556 0.87179487 0.910299
 1.         0.93048128 0.99481865 1.         0.99589603 1.
 1.         1.        ]

Kappa:
0.9817226696391137
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0e85dc1828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.452, val_acc:0.327]
Epoch [2/120    avg_loss:2.102, val_acc:0.504]
Epoch [3/120    avg_loss:1.888, val_acc:0.554]
Epoch [4/120    avg_loss:1.678, val_acc:0.640]
Epoch [5/120    avg_loss:1.468, val_acc:0.646]
Epoch [6/120    avg_loss:1.316, val_acc:0.740]
Epoch [7/120    avg_loss:1.173, val_acc:0.746]
Epoch [8/120    avg_loss:1.048, val_acc:0.758]
Epoch [9/120    avg_loss:0.941, val_acc:0.821]
Epoch [10/120    avg_loss:0.899, val_acc:0.821]
Epoch [11/120    avg_loss:0.802, val_acc:0.842]
Epoch [12/120    avg_loss:0.733, val_acc:0.879]
Epoch [13/120    avg_loss:0.663, val_acc:0.867]
Epoch [14/120    avg_loss:0.630, val_acc:0.890]
Epoch [15/120    avg_loss:0.538, val_acc:0.898]
Epoch [16/120    avg_loss:0.508, val_acc:0.892]
Epoch [17/120    avg_loss:0.481, val_acc:0.906]
Epoch [18/120    avg_loss:0.507, val_acc:0.898]
Epoch [19/120    avg_loss:0.530, val_acc:0.887]
Epoch [20/120    avg_loss:0.444, val_acc:0.919]
Epoch [21/120    avg_loss:0.464, val_acc:0.921]
Epoch [22/120    avg_loss:0.428, val_acc:0.915]
Epoch [23/120    avg_loss:0.400, val_acc:0.915]
Epoch [24/120    avg_loss:0.353, val_acc:0.931]
Epoch [25/120    avg_loss:0.315, val_acc:0.935]
Epoch [26/120    avg_loss:0.317, val_acc:0.923]
Epoch [27/120    avg_loss:0.330, val_acc:0.931]
Epoch [28/120    avg_loss:0.281, val_acc:0.948]
Epoch [29/120    avg_loss:0.280, val_acc:0.954]
Epoch [30/120    avg_loss:0.305, val_acc:0.925]
Epoch [31/120    avg_loss:0.369, val_acc:0.915]
Epoch [32/120    avg_loss:0.310, val_acc:0.921]
Epoch [33/120    avg_loss:0.379, val_acc:0.940]
Epoch [34/120    avg_loss:0.294, val_acc:0.938]
Epoch [35/120    avg_loss:0.215, val_acc:0.956]
Epoch [36/120    avg_loss:0.197, val_acc:0.958]
Epoch [37/120    avg_loss:0.186, val_acc:0.958]
Epoch [38/120    avg_loss:0.189, val_acc:0.963]
Epoch [39/120    avg_loss:0.239, val_acc:0.965]
Epoch [40/120    avg_loss:0.194, val_acc:0.960]
Epoch [41/120    avg_loss:0.228, val_acc:0.931]
Epoch [42/120    avg_loss:0.282, val_acc:0.950]
Epoch [43/120    avg_loss:0.217, val_acc:0.946]
Epoch [44/120    avg_loss:0.183, val_acc:0.965]
Epoch [45/120    avg_loss:0.205, val_acc:0.967]
Epoch [46/120    avg_loss:0.204, val_acc:0.971]
Epoch [47/120    avg_loss:0.145, val_acc:0.958]
Epoch [48/120    avg_loss:0.151, val_acc:0.977]
Epoch [49/120    avg_loss:0.154, val_acc:0.942]
Epoch [50/120    avg_loss:0.146, val_acc:0.973]
Epoch [51/120    avg_loss:0.116, val_acc:0.979]
Epoch [52/120    avg_loss:0.125, val_acc:0.958]
Epoch [53/120    avg_loss:0.123, val_acc:0.952]
Epoch [54/120    avg_loss:0.151, val_acc:0.950]
Epoch [55/120    avg_loss:0.128, val_acc:0.990]
Epoch [56/120    avg_loss:0.084, val_acc:0.981]
Epoch [57/120    avg_loss:0.108, val_acc:0.979]
Epoch [58/120    avg_loss:0.097, val_acc:0.965]
Epoch [59/120    avg_loss:0.075, val_acc:0.983]
Epoch [60/120    avg_loss:0.088, val_acc:0.960]
Epoch [61/120    avg_loss:0.127, val_acc:0.979]
Epoch [62/120    avg_loss:0.108, val_acc:0.963]
Epoch [63/120    avg_loss:0.123, val_acc:0.981]
Epoch [64/120    avg_loss:0.079, val_acc:0.973]
Epoch [65/120    avg_loss:0.085, val_acc:0.975]
Epoch [66/120    avg_loss:0.088, val_acc:0.981]
Epoch [67/120    avg_loss:0.062, val_acc:0.992]
Epoch [68/120    avg_loss:0.056, val_acc:0.990]
Epoch [69/120    avg_loss:0.057, val_acc:0.990]
Epoch [70/120    avg_loss:0.087, val_acc:0.971]
Epoch [71/120    avg_loss:0.080, val_acc:0.965]
Epoch [72/120    avg_loss:0.102, val_acc:0.975]
Epoch [73/120    avg_loss:0.089, val_acc:0.988]
Epoch [74/120    avg_loss:0.075, val_acc:0.992]
Epoch [75/120    avg_loss:0.093, val_acc:0.977]
Epoch [76/120    avg_loss:0.090, val_acc:0.979]
Epoch [77/120    avg_loss:0.048, val_acc:0.983]
Epoch [78/120    avg_loss:0.065, val_acc:0.985]
Epoch [79/120    avg_loss:0.065, val_acc:0.971]
Epoch [80/120    avg_loss:0.116, val_acc:0.958]
Epoch [81/120    avg_loss:0.105, val_acc:0.975]
Epoch [82/120    avg_loss:0.074, val_acc:0.988]
Epoch [83/120    avg_loss:0.096, val_acc:0.979]
Epoch [84/120    avg_loss:0.055, val_acc:0.985]
Epoch [85/120    avg_loss:0.056, val_acc:0.977]
Epoch [86/120    avg_loss:0.049, val_acc:0.981]
Epoch [87/120    avg_loss:0.041, val_acc:0.990]
Epoch [88/120    avg_loss:0.032, val_acc:0.990]
Epoch [89/120    avg_loss:0.047, val_acc:0.992]
Epoch [90/120    avg_loss:0.034, val_acc:0.988]
Epoch [91/120    avg_loss:0.035, val_acc:0.988]
Epoch [92/120    avg_loss:0.030, val_acc:0.990]
Epoch [93/120    avg_loss:0.040, val_acc:0.990]
Epoch [94/120    avg_loss:0.022, val_acc:0.990]
Epoch [95/120    avg_loss:0.031, val_acc:0.990]
Epoch [96/120    avg_loss:0.024, val_acc:0.990]
Epoch [97/120    avg_loss:0.038, val_acc:0.990]
Epoch [98/120    avg_loss:0.029, val_acc:0.990]
Epoch [99/120    avg_loss:0.026, val_acc:0.990]
Epoch [100/120    avg_loss:0.021, val_acc:0.990]
Epoch [101/120    avg_loss:0.023, val_acc:0.990]
Epoch [102/120    avg_loss:0.031, val_acc:0.990]
Epoch [103/120    avg_loss:0.024, val_acc:0.990]
Epoch [104/120    avg_loss:0.032, val_acc:0.990]
Epoch [105/120    avg_loss:0.026, val_acc:0.990]
Epoch [106/120    avg_loss:0.026, val_acc:0.990]
Epoch [107/120    avg_loss:0.020, val_acc:0.990]
Epoch [108/120    avg_loss:0.022, val_acc:0.990]
Epoch [109/120    avg_loss:0.028, val_acc:0.990]
Epoch [110/120    avg_loss:0.029, val_acc:0.990]
Epoch [111/120    avg_loss:0.027, val_acc:0.990]
Epoch [112/120    avg_loss:0.028, val_acc:0.990]
Epoch [113/120    avg_loss:0.025, val_acc:0.990]
Epoch [114/120    avg_loss:0.024, val_acc:0.990]
Epoch [115/120    avg_loss:0.022, val_acc:0.990]
Epoch [116/120    avg_loss:0.024, val_acc:0.990]
Epoch [117/120    avg_loss:0.031, val_acc:0.990]
Epoch [118/120    avg_loss:0.021, val_acc:0.990]
Epoch [119/120    avg_loss:0.024, val_acc:0.990]
Epoch [120/120    avg_loss:0.024, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217  12   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   1   0 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   5 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.99095023 0.96875    0.93777778 0.9442623
 1.         0.97826087 1.         0.99893276 0.99862826 0.99341238
 0.99333333 1.        ]

Kappa:
0.990505219497461
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff74e5b3860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.512, val_acc:0.323]
Epoch [2/120    avg_loss:2.145, val_acc:0.523]
Epoch [3/120    avg_loss:1.908, val_acc:0.596]
Epoch [4/120    avg_loss:1.681, val_acc:0.590]
Epoch [5/120    avg_loss:1.440, val_acc:0.650]
Epoch [6/120    avg_loss:1.279, val_acc:0.669]
Epoch [7/120    avg_loss:1.123, val_acc:0.725]
Epoch [8/120    avg_loss:1.009, val_acc:0.727]
Epoch [9/120    avg_loss:0.889, val_acc:0.746]
Epoch [10/120    avg_loss:0.867, val_acc:0.860]
Epoch [11/120    avg_loss:0.827, val_acc:0.800]
Epoch [12/120    avg_loss:0.731, val_acc:0.806]
Epoch [13/120    avg_loss:0.648, val_acc:0.887]
Epoch [14/120    avg_loss:0.632, val_acc:0.812]
Epoch [15/120    avg_loss:0.592, val_acc:0.885]
Epoch [16/120    avg_loss:0.568, val_acc:0.871]
Epoch [17/120    avg_loss:0.544, val_acc:0.919]
Epoch [18/120    avg_loss:0.466, val_acc:0.917]
Epoch [19/120    avg_loss:0.457, val_acc:0.942]
Epoch [20/120    avg_loss:0.405, val_acc:0.879]
Epoch [21/120    avg_loss:0.400, val_acc:0.933]
Epoch [22/120    avg_loss:0.353, val_acc:0.915]
Epoch [23/120    avg_loss:0.350, val_acc:0.938]
Epoch [24/120    avg_loss:0.337, val_acc:0.927]
Epoch [25/120    avg_loss:0.346, val_acc:0.927]
Epoch [26/120    avg_loss:0.360, val_acc:0.887]
Epoch [27/120    avg_loss:0.304, val_acc:0.933]
Epoch [28/120    avg_loss:0.339, val_acc:0.929]
Epoch [29/120    avg_loss:0.318, val_acc:0.944]
Epoch [30/120    avg_loss:0.253, val_acc:0.969]
Epoch [31/120    avg_loss:0.280, val_acc:0.942]
Epoch [32/120    avg_loss:0.264, val_acc:0.940]
Epoch [33/120    avg_loss:0.272, val_acc:0.948]
Epoch [34/120    avg_loss:0.305, val_acc:0.946]
Epoch [35/120    avg_loss:0.209, val_acc:0.960]
Epoch [36/120    avg_loss:0.230, val_acc:0.963]
Epoch [37/120    avg_loss:0.212, val_acc:0.963]
Epoch [38/120    avg_loss:0.236, val_acc:0.958]
Epoch [39/120    avg_loss:0.345, val_acc:0.940]
Epoch [40/120    avg_loss:0.304, val_acc:0.890]
Epoch [41/120    avg_loss:0.286, val_acc:0.927]
Epoch [42/120    avg_loss:0.280, val_acc:0.910]
Epoch [43/120    avg_loss:0.256, val_acc:0.954]
Epoch [44/120    avg_loss:0.198, val_acc:0.967]
Epoch [45/120    avg_loss:0.182, val_acc:0.977]
Epoch [46/120    avg_loss:0.173, val_acc:0.977]
Epoch [47/120    avg_loss:0.171, val_acc:0.977]
Epoch [48/120    avg_loss:0.169, val_acc:0.975]
Epoch [49/120    avg_loss:0.146, val_acc:0.973]
Epoch [50/120    avg_loss:0.139, val_acc:0.977]
Epoch [51/120    avg_loss:0.142, val_acc:0.977]
Epoch [52/120    avg_loss:0.163, val_acc:0.979]
Epoch [53/120    avg_loss:0.136, val_acc:0.979]
Epoch [54/120    avg_loss:0.134, val_acc:0.979]
Epoch [55/120    avg_loss:0.159, val_acc:0.977]
Epoch [56/120    avg_loss:0.148, val_acc:0.977]
Epoch [57/120    avg_loss:0.135, val_acc:0.975]
Epoch [58/120    avg_loss:0.150, val_acc:0.979]
Epoch [59/120    avg_loss:0.123, val_acc:0.979]
Epoch [60/120    avg_loss:0.133, val_acc:0.979]
Epoch [61/120    avg_loss:0.127, val_acc:0.981]
Epoch [62/120    avg_loss:0.134, val_acc:0.975]
Epoch [63/120    avg_loss:0.124, val_acc:0.981]
Epoch [64/120    avg_loss:0.126, val_acc:0.981]
Epoch [65/120    avg_loss:0.131, val_acc:0.981]
Epoch [66/120    avg_loss:0.121, val_acc:0.981]
Epoch [67/120    avg_loss:0.128, val_acc:0.983]
Epoch [68/120    avg_loss:0.148, val_acc:0.983]
Epoch [69/120    avg_loss:0.123, val_acc:0.983]
Epoch [70/120    avg_loss:0.128, val_acc:0.983]
Epoch [71/120    avg_loss:0.119, val_acc:0.981]
Epoch [72/120    avg_loss:0.113, val_acc:0.981]
Epoch [73/120    avg_loss:0.115, val_acc:0.983]
Epoch [74/120    avg_loss:0.112, val_acc:0.983]
Epoch [75/120    avg_loss:0.117, val_acc:0.979]
Epoch [76/120    avg_loss:0.108, val_acc:0.981]
Epoch [77/120    avg_loss:0.111, val_acc:0.983]
Epoch [78/120    avg_loss:0.124, val_acc:0.983]
Epoch [79/120    avg_loss:0.114, val_acc:0.979]
Epoch [80/120    avg_loss:0.102, val_acc:0.983]
Epoch [81/120    avg_loss:0.108, val_acc:0.981]
Epoch [82/120    avg_loss:0.108, val_acc:0.983]
Epoch [83/120    avg_loss:0.113, val_acc:0.983]
Epoch [84/120    avg_loss:0.127, val_acc:0.983]
Epoch [85/120    avg_loss:0.108, val_acc:0.983]
Epoch [86/120    avg_loss:0.116, val_acc:0.977]
Epoch [87/120    avg_loss:0.107, val_acc:0.983]
Epoch [88/120    avg_loss:0.096, val_acc:0.983]
Epoch [89/120    avg_loss:0.092, val_acc:0.985]
Epoch [90/120    avg_loss:0.110, val_acc:0.985]
Epoch [91/120    avg_loss:0.091, val_acc:0.985]
Epoch [92/120    avg_loss:0.100, val_acc:0.985]
Epoch [93/120    avg_loss:0.111, val_acc:0.985]
Epoch [94/120    avg_loss:0.088, val_acc:0.985]
Epoch [95/120    avg_loss:0.095, val_acc:0.983]
Epoch [96/120    avg_loss:0.089, val_acc:0.985]
Epoch [97/120    avg_loss:0.110, val_acc:0.985]
Epoch [98/120    avg_loss:0.089, val_acc:0.985]
Epoch [99/120    avg_loss:0.096, val_acc:0.985]
Epoch [100/120    avg_loss:0.093, val_acc:0.985]
Epoch [101/120    avg_loss:0.095, val_acc:0.985]
Epoch [102/120    avg_loss:0.089, val_acc:0.985]
Epoch [103/120    avg_loss:0.088, val_acc:0.985]
Epoch [104/120    avg_loss:0.092, val_acc:0.985]
Epoch [105/120    avg_loss:0.107, val_acc:0.985]
Epoch [106/120    avg_loss:0.089, val_acc:0.988]
Epoch [107/120    avg_loss:0.084, val_acc:0.985]
Epoch [108/120    avg_loss:0.079, val_acc:0.988]
Epoch [109/120    avg_loss:0.097, val_acc:0.985]
Epoch [110/120    avg_loss:0.085, val_acc:0.988]
Epoch [111/120    avg_loss:0.101, val_acc:0.988]
Epoch [112/120    avg_loss:0.107, val_acc:0.988]
Epoch [113/120    avg_loss:0.094, val_acc:0.990]
Epoch [114/120    avg_loss:0.083, val_acc:0.990]
Epoch [115/120    avg_loss:0.099, val_acc:0.985]
Epoch [116/120    avg_loss:0.088, val_acc:0.985]
Epoch [117/120    avg_loss:0.094, val_acc:0.985]
Epoch [118/120    avg_loss:0.077, val_acc:0.988]
Epoch [119/120    avg_loss:0.086, val_acc:0.988]
Epoch [120/120    avg_loss:0.089, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.95280899 0.99563319 0.91990847 0.89320388
 1.         0.8839779  1.         1.         1.         0.99600533
 0.99669967 1.        ]

Kappa:
0.9859948209535213
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8508c9f860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.558, val_acc:0.344]
Epoch [2/120    avg_loss:2.163, val_acc:0.592]
Epoch [3/120    avg_loss:1.950, val_acc:0.619]
Epoch [4/120    avg_loss:1.750, val_acc:0.637]
Epoch [5/120    avg_loss:1.553, val_acc:0.652]
Epoch [6/120    avg_loss:1.355, val_acc:0.677]
Epoch [7/120    avg_loss:1.218, val_acc:0.675]
Epoch [8/120    avg_loss:1.071, val_acc:0.692]
Epoch [9/120    avg_loss:0.965, val_acc:0.710]
Epoch [10/120    avg_loss:0.923, val_acc:0.773]
Epoch [11/120    avg_loss:0.845, val_acc:0.819]
Epoch [12/120    avg_loss:0.748, val_acc:0.840]
Epoch [13/120    avg_loss:0.661, val_acc:0.856]
Epoch [14/120    avg_loss:0.653, val_acc:0.865]
Epoch [15/120    avg_loss:0.580, val_acc:0.890]
Epoch [16/120    avg_loss:0.574, val_acc:0.875]
Epoch [17/120    avg_loss:0.490, val_acc:0.896]
Epoch [18/120    avg_loss:0.527, val_acc:0.904]
Epoch [19/120    avg_loss:0.428, val_acc:0.881]
Epoch [20/120    avg_loss:0.459, val_acc:0.900]
Epoch [21/120    avg_loss:0.421, val_acc:0.885]
Epoch [22/120    avg_loss:0.428, val_acc:0.908]
Epoch [23/120    avg_loss:0.412, val_acc:0.919]
Epoch [24/120    avg_loss:0.451, val_acc:0.904]
Epoch [25/120    avg_loss:0.406, val_acc:0.908]
Epoch [26/120    avg_loss:0.364, val_acc:0.923]
Epoch [27/120    avg_loss:0.323, val_acc:0.929]
Epoch [28/120    avg_loss:0.319, val_acc:0.917]
Epoch [29/120    avg_loss:0.304, val_acc:0.904]
Epoch [30/120    avg_loss:0.302, val_acc:0.912]
Epoch [31/120    avg_loss:0.277, val_acc:0.904]
Epoch [32/120    avg_loss:0.302, val_acc:0.931]
Epoch [33/120    avg_loss:0.300, val_acc:0.944]
Epoch [34/120    avg_loss:0.300, val_acc:0.944]
Epoch [35/120    avg_loss:0.299, val_acc:0.946]
Epoch [36/120    avg_loss:0.260, val_acc:0.933]
Epoch [37/120    avg_loss:0.246, val_acc:0.933]
Epoch [38/120    avg_loss:0.200, val_acc:0.938]
Epoch [39/120    avg_loss:0.214, val_acc:0.944]
Epoch [40/120    avg_loss:0.179, val_acc:0.952]
Epoch [41/120    avg_loss:0.180, val_acc:0.929]
Epoch [42/120    avg_loss:0.197, val_acc:0.944]
Epoch [43/120    avg_loss:0.202, val_acc:0.952]
Epoch [44/120    avg_loss:0.177, val_acc:0.942]
Epoch [45/120    avg_loss:0.136, val_acc:0.956]
Epoch [46/120    avg_loss:0.219, val_acc:0.915]
Epoch [47/120    avg_loss:0.215, val_acc:0.927]
Epoch [48/120    avg_loss:0.203, val_acc:0.950]
Epoch [49/120    avg_loss:0.181, val_acc:0.950]
Epoch [50/120    avg_loss:0.176, val_acc:0.960]
Epoch [51/120    avg_loss:0.205, val_acc:0.942]
Epoch [52/120    avg_loss:0.188, val_acc:0.952]
Epoch [53/120    avg_loss:0.169, val_acc:0.950]
Epoch [54/120    avg_loss:0.121, val_acc:0.954]
Epoch [55/120    avg_loss:0.117, val_acc:0.954]
Epoch [56/120    avg_loss:0.158, val_acc:0.952]
Epoch [57/120    avg_loss:0.165, val_acc:0.946]
Epoch [58/120    avg_loss:0.120, val_acc:0.960]
Epoch [59/120    avg_loss:0.120, val_acc:0.948]
Epoch [60/120    avg_loss:0.173, val_acc:0.952]
Epoch [61/120    avg_loss:0.121, val_acc:0.960]
Epoch [62/120    avg_loss:0.104, val_acc:0.971]
Epoch [63/120    avg_loss:0.082, val_acc:0.971]
Epoch [64/120    avg_loss:0.080, val_acc:0.960]
Epoch [65/120    avg_loss:0.066, val_acc:0.969]
Epoch [66/120    avg_loss:0.080, val_acc:0.973]
Epoch [67/120    avg_loss:0.102, val_acc:0.942]
Epoch [68/120    avg_loss:0.122, val_acc:0.967]
Epoch [69/120    avg_loss:0.090, val_acc:0.973]
Epoch [70/120    avg_loss:0.062, val_acc:0.967]
Epoch [71/120    avg_loss:0.078, val_acc:0.971]
Epoch [72/120    avg_loss:0.075, val_acc:0.963]
Epoch [73/120    avg_loss:0.086, val_acc:0.973]
Epoch [74/120    avg_loss:0.096, val_acc:0.967]
Epoch [75/120    avg_loss:0.082, val_acc:0.975]
Epoch [76/120    avg_loss:0.077, val_acc:0.977]
Epoch [77/120    avg_loss:0.096, val_acc:0.948]
Epoch [78/120    avg_loss:0.075, val_acc:0.958]
Epoch [79/120    avg_loss:0.081, val_acc:0.956]
Epoch [80/120    avg_loss:0.080, val_acc:0.973]
Epoch [81/120    avg_loss:0.056, val_acc:0.975]
Epoch [82/120    avg_loss:0.062, val_acc:0.975]
Epoch [83/120    avg_loss:0.051, val_acc:0.971]
Epoch [84/120    avg_loss:0.060, val_acc:0.977]
Epoch [85/120    avg_loss:0.064, val_acc:0.971]
Epoch [86/120    avg_loss:0.076, val_acc:0.977]
Epoch [87/120    avg_loss:0.049, val_acc:0.977]
Epoch [88/120    avg_loss:0.058, val_acc:0.973]
Epoch [89/120    avg_loss:0.047, val_acc:0.977]
Epoch [90/120    avg_loss:0.055, val_acc:0.985]
Epoch [91/120    avg_loss:0.050, val_acc:0.975]
Epoch [92/120    avg_loss:0.041, val_acc:0.979]
Epoch [93/120    avg_loss:0.038, val_acc:0.981]
Epoch [94/120    avg_loss:0.029, val_acc:0.979]
Epoch [95/120    avg_loss:0.040, val_acc:0.981]
Epoch [96/120    avg_loss:0.031, val_acc:0.983]
Epoch [97/120    avg_loss:0.030, val_acc:0.981]
Epoch [98/120    avg_loss:0.023, val_acc:0.977]
Epoch [99/120    avg_loss:0.035, val_acc:0.985]
Epoch [100/120    avg_loss:0.029, val_acc:0.973]
Epoch [101/120    avg_loss:0.027, val_acc:0.975]
Epoch [102/120    avg_loss:0.028, val_acc:0.977]
Epoch [103/120    avg_loss:0.023, val_acc:0.983]
Epoch [104/120    avg_loss:0.026, val_acc:0.983]
Epoch [105/120    avg_loss:0.022, val_acc:0.979]
Epoch [106/120    avg_loss:0.022, val_acc:0.981]
Epoch [107/120    avg_loss:0.071, val_acc:0.973]
Epoch [108/120    avg_loss:0.050, val_acc:0.983]
Epoch [109/120    avg_loss:0.040, val_acc:0.975]
Epoch [110/120    avg_loss:0.037, val_acc:0.969]
Epoch [111/120    avg_loss:0.043, val_acc:0.973]
Epoch [112/120    avg_loss:0.057, val_acc:0.977]
Epoch [113/120    avg_loss:0.044, val_acc:0.979]
Epoch [114/120    avg_loss:0.033, val_acc:0.975]
Epoch [115/120    avg_loss:0.023, val_acc:0.979]
Epoch [116/120    avg_loss:0.019, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.981]
Epoch [118/120    avg_loss:0.030, val_acc:0.981]
Epoch [119/120    avg_loss:0.021, val_acc:0.983]
Epoch [120/120    avg_loss:0.017, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.9977221  0.98678414 0.9254386  0.9047619
 1.         1.         1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9916918929780173
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0cd469d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.534, val_acc:0.375]
Epoch [2/120    avg_loss:2.147, val_acc:0.521]
Epoch [3/120    avg_loss:1.891, val_acc:0.617]
Epoch [4/120    avg_loss:1.679, val_acc:0.660]
Epoch [5/120    avg_loss:1.466, val_acc:0.733]
Epoch [6/120    avg_loss:1.266, val_acc:0.690]
Epoch [7/120    avg_loss:1.099, val_acc:0.808]
Epoch [8/120    avg_loss:0.972, val_acc:0.817]
Epoch [9/120    avg_loss:0.837, val_acc:0.781]
Epoch [10/120    avg_loss:0.785, val_acc:0.810]
Epoch [11/120    avg_loss:0.694, val_acc:0.869]
Epoch [12/120    avg_loss:0.670, val_acc:0.879]
Epoch [13/120    avg_loss:0.576, val_acc:0.769]
Epoch [14/120    avg_loss:0.557, val_acc:0.875]
Epoch [15/120    avg_loss:0.512, val_acc:0.908]
Epoch [16/120    avg_loss:0.514, val_acc:0.881]
Epoch [17/120    avg_loss:0.457, val_acc:0.840]
Epoch [18/120    avg_loss:0.460, val_acc:0.883]
Epoch [19/120    avg_loss:0.440, val_acc:0.890]
Epoch [20/120    avg_loss:0.405, val_acc:0.894]
Epoch [21/120    avg_loss:0.382, val_acc:0.896]
Epoch [22/120    avg_loss:0.392, val_acc:0.940]
Epoch [23/120    avg_loss:0.347, val_acc:0.904]
Epoch [24/120    avg_loss:0.353, val_acc:0.925]
Epoch [25/120    avg_loss:0.371, val_acc:0.942]
Epoch [26/120    avg_loss:0.336, val_acc:0.933]
Epoch [27/120    avg_loss:0.327, val_acc:0.923]
Epoch [28/120    avg_loss:0.264, val_acc:0.946]
Epoch [29/120    avg_loss:0.279, val_acc:0.950]
Epoch [30/120    avg_loss:0.247, val_acc:0.935]
Epoch [31/120    avg_loss:0.264, val_acc:0.919]
Epoch [32/120    avg_loss:0.283, val_acc:0.938]
Epoch [33/120    avg_loss:0.265, val_acc:0.927]
Epoch [34/120    avg_loss:0.287, val_acc:0.950]
Epoch [35/120    avg_loss:0.227, val_acc:0.967]
Epoch [36/120    avg_loss:0.225, val_acc:0.944]
Epoch [37/120    avg_loss:0.237, val_acc:0.965]
Epoch [38/120    avg_loss:0.244, val_acc:0.946]
Epoch [39/120    avg_loss:0.188, val_acc:0.958]
Epoch [40/120    avg_loss:0.228, val_acc:0.975]
Epoch [41/120    avg_loss:0.189, val_acc:0.956]
Epoch [42/120    avg_loss:0.249, val_acc:0.954]
Epoch [43/120    avg_loss:0.219, val_acc:0.952]
Epoch [44/120    avg_loss:0.202, val_acc:0.944]
Epoch [45/120    avg_loss:0.178, val_acc:0.975]
Epoch [46/120    avg_loss:0.203, val_acc:0.948]
Epoch [47/120    avg_loss:0.154, val_acc:0.965]
Epoch [48/120    avg_loss:0.178, val_acc:0.960]
Epoch [49/120    avg_loss:0.136, val_acc:0.965]
Epoch [50/120    avg_loss:0.149, val_acc:0.967]
Epoch [51/120    avg_loss:0.139, val_acc:0.975]
Epoch [52/120    avg_loss:0.143, val_acc:0.977]
Epoch [53/120    avg_loss:0.135, val_acc:0.973]
Epoch [54/120    avg_loss:0.108, val_acc:0.985]
Epoch [55/120    avg_loss:0.118, val_acc:0.963]
Epoch [56/120    avg_loss:0.172, val_acc:0.948]
Epoch [57/120    avg_loss:0.163, val_acc:0.971]
Epoch [58/120    avg_loss:0.127, val_acc:0.967]
Epoch [59/120    avg_loss:0.128, val_acc:0.963]
Epoch [60/120    avg_loss:0.115, val_acc:0.983]
Epoch [61/120    avg_loss:0.144, val_acc:0.963]
Epoch [62/120    avg_loss:0.120, val_acc:0.967]
Epoch [63/120    avg_loss:0.106, val_acc:0.971]
Epoch [64/120    avg_loss:0.098, val_acc:0.979]
Epoch [65/120    avg_loss:0.082, val_acc:0.988]
Epoch [66/120    avg_loss:0.093, val_acc:0.985]
Epoch [67/120    avg_loss:0.067, val_acc:0.983]
Epoch [68/120    avg_loss:0.078, val_acc:0.985]
Epoch [69/120    avg_loss:0.078, val_acc:0.975]
Epoch [70/120    avg_loss:0.067, val_acc:0.985]
Epoch [71/120    avg_loss:0.087, val_acc:0.979]
Epoch [72/120    avg_loss:0.060, val_acc:0.981]
Epoch [73/120    avg_loss:0.073, val_acc:0.958]
Epoch [74/120    avg_loss:0.155, val_acc:0.965]
Epoch [75/120    avg_loss:0.104, val_acc:0.967]
Epoch [76/120    avg_loss:0.092, val_acc:0.977]
Epoch [77/120    avg_loss:0.129, val_acc:0.981]
Epoch [78/120    avg_loss:0.076, val_acc:0.977]
Epoch [79/120    avg_loss:0.060, val_acc:0.981]
Epoch [80/120    avg_loss:0.056, val_acc:0.985]
Epoch [81/120    avg_loss:0.042, val_acc:0.985]
Epoch [82/120    avg_loss:0.054, val_acc:0.988]
Epoch [83/120    avg_loss:0.047, val_acc:0.985]
Epoch [84/120    avg_loss:0.035, val_acc:0.988]
Epoch [85/120    avg_loss:0.031, val_acc:0.985]
Epoch [86/120    avg_loss:0.047, val_acc:0.985]
Epoch [87/120    avg_loss:0.045, val_acc:0.985]
Epoch [88/120    avg_loss:0.044, val_acc:0.983]
Epoch [89/120    avg_loss:0.051, val_acc:0.983]
Epoch [90/120    avg_loss:0.037, val_acc:0.981]
Epoch [91/120    avg_loss:0.044, val_acc:0.985]
Epoch [92/120    avg_loss:0.033, val_acc:0.985]
Epoch [93/120    avg_loss:0.033, val_acc:0.985]
Epoch [94/120    avg_loss:0.039, val_acc:0.985]
Epoch [95/120    avg_loss:0.034, val_acc:0.985]
Epoch [96/120    avg_loss:0.050, val_acc:0.985]
Epoch [97/120    avg_loss:0.045, val_acc:0.981]
Epoch [98/120    avg_loss:0.039, val_acc:0.981]
Epoch [99/120    avg_loss:0.038, val_acc:0.983]
Epoch [100/120    avg_loss:0.054, val_acc:0.983]
Epoch [101/120    avg_loss:0.039, val_acc:0.985]
Epoch [102/120    avg_loss:0.036, val_acc:0.985]
Epoch [103/120    avg_loss:0.035, val_acc:0.985]
Epoch [104/120    avg_loss:0.032, val_acc:0.985]
Epoch [105/120    avg_loss:0.037, val_acc:0.985]
Epoch [106/120    avg_loss:0.033, val_acc:0.985]
Epoch [107/120    avg_loss:0.033, val_acc:0.985]
Epoch [108/120    avg_loss:0.044, val_acc:0.985]
Epoch [109/120    avg_loss:0.042, val_acc:0.985]
Epoch [110/120    avg_loss:0.042, val_acc:0.985]
Epoch [111/120    avg_loss:0.048, val_acc:0.985]
Epoch [112/120    avg_loss:0.031, val_acc:0.985]
Epoch [113/120    avg_loss:0.036, val_acc:0.985]
Epoch [114/120    avg_loss:0.034, val_acc:0.985]
Epoch [115/120    avg_loss:0.036, val_acc:0.985]
Epoch [116/120    avg_loss:0.045, val_acc:0.985]
Epoch [117/120    avg_loss:0.030, val_acc:0.985]
Epoch [118/120    avg_loss:0.035, val_acc:0.985]
Epoch [119/120    avg_loss:0.032, val_acc:0.985]
Epoch [120/120    avg_loss:0.046, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.97767857 0.98454746 0.91703057 0.89041096
 1.         0.95555556 1.         0.99893276 1.         1.
 0.99778761 1.        ]

Kappa:
0.9883679625312843
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3f8cb87f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.494, val_acc:0.404]
Epoch [2/120    avg_loss:2.165, val_acc:0.508]
Epoch [3/120    avg_loss:1.973, val_acc:0.590]
Epoch [4/120    avg_loss:1.800, val_acc:0.629]
Epoch [5/120    avg_loss:1.569, val_acc:0.646]
Epoch [6/120    avg_loss:1.389, val_acc:0.729]
Epoch [7/120    avg_loss:1.219, val_acc:0.779]
Epoch [8/120    avg_loss:1.076, val_acc:0.779]
Epoch [9/120    avg_loss:0.964, val_acc:0.856]
Epoch [10/120    avg_loss:0.898, val_acc:0.835]
Epoch [11/120    avg_loss:0.788, val_acc:0.858]
Epoch [12/120    avg_loss:0.705, val_acc:0.869]
Epoch [13/120    avg_loss:0.693, val_acc:0.879]
Epoch [14/120    avg_loss:0.614, val_acc:0.854]
Epoch [15/120    avg_loss:0.581, val_acc:0.896]
Epoch [16/120    avg_loss:0.577, val_acc:0.917]
Epoch [17/120    avg_loss:0.630, val_acc:0.875]
Epoch [18/120    avg_loss:0.553, val_acc:0.902]
Epoch [19/120    avg_loss:0.490, val_acc:0.921]
Epoch [20/120    avg_loss:0.466, val_acc:0.921]
Epoch [21/120    avg_loss:0.469, val_acc:0.915]
Epoch [22/120    avg_loss:0.387, val_acc:0.933]
Epoch [23/120    avg_loss:0.383, val_acc:0.923]
Epoch [24/120    avg_loss:0.354, val_acc:0.944]
Epoch [25/120    avg_loss:0.354, val_acc:0.923]
Epoch [26/120    avg_loss:0.326, val_acc:0.931]
Epoch [27/120    avg_loss:0.315, val_acc:0.935]
Epoch [28/120    avg_loss:0.394, val_acc:0.844]
Epoch [29/120    avg_loss:0.341, val_acc:0.921]
Epoch [30/120    avg_loss:0.335, val_acc:0.940]
Epoch [31/120    avg_loss:0.246, val_acc:0.948]
Epoch [32/120    avg_loss:0.240, val_acc:0.942]
Epoch [33/120    avg_loss:0.301, val_acc:0.942]
Epoch [34/120    avg_loss:0.269, val_acc:0.917]
Epoch [35/120    avg_loss:0.268, val_acc:0.952]
Epoch [36/120    avg_loss:0.249, val_acc:0.956]
Epoch [37/120    avg_loss:0.244, val_acc:0.933]
Epoch [38/120    avg_loss:0.214, val_acc:0.952]
Epoch [39/120    avg_loss:0.229, val_acc:0.952]
Epoch [40/120    avg_loss:0.231, val_acc:0.910]
Epoch [41/120    avg_loss:0.282, val_acc:0.931]
Epoch [42/120    avg_loss:0.239, val_acc:0.950]
Epoch [43/120    avg_loss:0.173, val_acc:0.960]
Epoch [44/120    avg_loss:0.150, val_acc:0.965]
Epoch [45/120    avg_loss:0.189, val_acc:0.960]
Epoch [46/120    avg_loss:0.147, val_acc:0.954]
Epoch [47/120    avg_loss:0.132, val_acc:0.960]
Epoch [48/120    avg_loss:0.119, val_acc:0.973]
Epoch [49/120    avg_loss:0.129, val_acc:0.969]
Epoch [50/120    avg_loss:0.142, val_acc:0.938]
Epoch [51/120    avg_loss:0.132, val_acc:0.952]
Epoch [52/120    avg_loss:0.113, val_acc:0.969]
Epoch [53/120    avg_loss:0.117, val_acc:0.975]
Epoch [54/120    avg_loss:0.117, val_acc:0.963]
Epoch [55/120    avg_loss:0.131, val_acc:0.981]
Epoch [56/120    avg_loss:0.129, val_acc:0.950]
Epoch [57/120    avg_loss:0.134, val_acc:0.958]
Epoch [58/120    avg_loss:0.117, val_acc:0.975]
Epoch [59/120    avg_loss:0.125, val_acc:0.969]
Epoch [60/120    avg_loss:0.114, val_acc:0.973]
Epoch [61/120    avg_loss:0.098, val_acc:0.983]
Epoch [62/120    avg_loss:0.130, val_acc:0.965]
Epoch [63/120    avg_loss:0.115, val_acc:0.956]
Epoch [64/120    avg_loss:0.146, val_acc:0.967]
Epoch [65/120    avg_loss:0.098, val_acc:0.973]
Epoch [66/120    avg_loss:0.110, val_acc:0.973]
Epoch [67/120    avg_loss:0.080, val_acc:0.983]
Epoch [68/120    avg_loss:0.108, val_acc:0.967]
Epoch [69/120    avg_loss:0.057, val_acc:0.969]
Epoch [70/120    avg_loss:0.125, val_acc:0.967]
Epoch [71/120    avg_loss:0.128, val_acc:0.967]
Epoch [72/120    avg_loss:0.103, val_acc:0.981]
Epoch [73/120    avg_loss:0.075, val_acc:0.983]
Epoch [74/120    avg_loss:0.064, val_acc:0.981]
Epoch [75/120    avg_loss:0.055, val_acc:0.977]
Epoch [76/120    avg_loss:0.066, val_acc:0.975]
Epoch [77/120    avg_loss:0.064, val_acc:0.977]
Epoch [78/120    avg_loss:0.050, val_acc:0.983]
Epoch [79/120    avg_loss:0.054, val_acc:0.983]
Epoch [80/120    avg_loss:0.036, val_acc:0.981]
Epoch [81/120    avg_loss:0.059, val_acc:0.979]
Epoch [82/120    avg_loss:0.049, val_acc:0.981]
Epoch [83/120    avg_loss:0.069, val_acc:0.975]
Epoch [84/120    avg_loss:0.042, val_acc:0.983]
Epoch [85/120    avg_loss:0.067, val_acc:0.975]
Epoch [86/120    avg_loss:0.079, val_acc:0.973]
Epoch [87/120    avg_loss:0.107, val_acc:0.977]
Epoch [88/120    avg_loss:0.116, val_acc:0.981]
Epoch [89/120    avg_loss:0.085, val_acc:0.979]
Epoch [90/120    avg_loss:0.136, val_acc:0.965]
Epoch [91/120    avg_loss:0.072, val_acc:0.983]
Epoch [92/120    avg_loss:0.081, val_acc:0.960]
Epoch [93/120    avg_loss:0.198, val_acc:0.944]
Epoch [94/120    avg_loss:0.115, val_acc:0.975]
Epoch [95/120    avg_loss:0.091, val_acc:0.975]
Epoch [96/120    avg_loss:0.082, val_acc:0.960]
Epoch [97/120    avg_loss:0.107, val_acc:0.975]
Epoch [98/120    avg_loss:0.062, val_acc:0.981]
Epoch [99/120    avg_loss:0.051, val_acc:0.983]
Epoch [100/120    avg_loss:0.032, val_acc:0.977]
Epoch [101/120    avg_loss:0.049, val_acc:0.985]
Epoch [102/120    avg_loss:0.032, val_acc:0.985]
Epoch [103/120    avg_loss:0.033, val_acc:0.988]
Epoch [104/120    avg_loss:0.027, val_acc:0.973]
Epoch [105/120    avg_loss:0.052, val_acc:0.981]
Epoch [106/120    avg_loss:0.054, val_acc:0.979]
Epoch [107/120    avg_loss:0.045, val_acc:0.981]
Epoch [108/120    avg_loss:0.035, val_acc:0.983]
Epoch [109/120    avg_loss:0.035, val_acc:0.983]
Epoch [110/120    avg_loss:0.030, val_acc:0.975]
Epoch [111/120    avg_loss:0.021, val_acc:0.992]
Epoch [112/120    avg_loss:0.029, val_acc:0.977]
Epoch [113/120    avg_loss:0.023, val_acc:0.983]
Epoch [114/120    avg_loss:0.037, val_acc:0.985]
Epoch [115/120    avg_loss:0.041, val_acc:0.981]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.032, val_acc:0.981]
Epoch [118/120    avg_loss:0.027, val_acc:0.981]
Epoch [119/120    avg_loss:0.039, val_acc:0.975]
Epoch [120/120    avg_loss:0.046, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  21   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99853801 0.97550111 0.98004435 0.92970522 0.93247588
 0.99516908 0.93785311 1.         1.         1.         0.99208443
 0.99224806 1.        ]

Kappa:
0.9881317742516691
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0af3026828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.547, val_acc:0.308]
Epoch [2/120    avg_loss:2.153, val_acc:0.477]
Epoch [3/120    avg_loss:1.892, val_acc:0.556]
Epoch [4/120    avg_loss:1.695, val_acc:0.594]
Epoch [5/120    avg_loss:1.506, val_acc:0.660]
Epoch [6/120    avg_loss:1.334, val_acc:0.662]
Epoch [7/120    avg_loss:1.191, val_acc:0.729]
Epoch [8/120    avg_loss:1.072, val_acc:0.708]
Epoch [9/120    avg_loss:0.954, val_acc:0.840]
Epoch [10/120    avg_loss:0.847, val_acc:0.848]
Epoch [11/120    avg_loss:0.785, val_acc:0.869]
Epoch [12/120    avg_loss:0.730, val_acc:0.875]
Epoch [13/120    avg_loss:0.644, val_acc:0.898]
Epoch [14/120    avg_loss:0.568, val_acc:0.854]
Epoch [15/120    avg_loss:0.578, val_acc:0.819]
Epoch [16/120    avg_loss:0.557, val_acc:0.869]
Epoch [17/120    avg_loss:0.502, val_acc:0.879]
Epoch [18/120    avg_loss:0.448, val_acc:0.929]
Epoch [19/120    avg_loss:0.466, val_acc:0.931]
Epoch [20/120    avg_loss:0.461, val_acc:0.931]
Epoch [21/120    avg_loss:0.435, val_acc:0.915]
Epoch [22/120    avg_loss:0.417, val_acc:0.925]
Epoch [23/120    avg_loss:0.362, val_acc:0.938]
Epoch [24/120    avg_loss:0.384, val_acc:0.873]
Epoch [25/120    avg_loss:0.368, val_acc:0.915]
Epoch [26/120    avg_loss:0.347, val_acc:0.917]
Epoch [27/120    avg_loss:0.366, val_acc:0.933]
Epoch [28/120    avg_loss:0.321, val_acc:0.956]
Epoch [29/120    avg_loss:0.287, val_acc:0.925]
Epoch [30/120    avg_loss:0.280, val_acc:0.935]
Epoch [31/120    avg_loss:0.240, val_acc:0.938]
Epoch [32/120    avg_loss:0.309, val_acc:0.923]
Epoch [33/120    avg_loss:0.284, val_acc:0.960]
Epoch [34/120    avg_loss:0.225, val_acc:0.917]
Epoch [35/120    avg_loss:0.270, val_acc:0.960]
Epoch [36/120    avg_loss:0.221, val_acc:0.929]
Epoch [37/120    avg_loss:0.286, val_acc:0.946]
Epoch [38/120    avg_loss:0.346, val_acc:0.956]
Epoch [39/120    avg_loss:0.256, val_acc:0.958]
Epoch [40/120    avg_loss:0.224, val_acc:0.960]
Epoch [41/120    avg_loss:0.176, val_acc:0.960]
Epoch [42/120    avg_loss:0.173, val_acc:0.967]
Epoch [43/120    avg_loss:0.141, val_acc:0.973]
Epoch [44/120    avg_loss:0.168, val_acc:0.954]
Epoch [45/120    avg_loss:0.183, val_acc:0.971]
Epoch [46/120    avg_loss:0.158, val_acc:0.956]
Epoch [47/120    avg_loss:0.193, val_acc:0.971]
Epoch [48/120    avg_loss:0.151, val_acc:0.956]
Epoch [49/120    avg_loss:0.203, val_acc:0.963]
Epoch [50/120    avg_loss:0.177, val_acc:0.954]
Epoch [51/120    avg_loss:0.152, val_acc:0.963]
Epoch [52/120    avg_loss:0.106, val_acc:0.971]
Epoch [53/120    avg_loss:0.112, val_acc:0.979]
Epoch [54/120    avg_loss:0.109, val_acc:0.969]
Epoch [55/120    avg_loss:0.104, val_acc:0.960]
Epoch [56/120    avg_loss:0.102, val_acc:0.981]
Epoch [57/120    avg_loss:0.108, val_acc:0.977]
Epoch [58/120    avg_loss:0.125, val_acc:0.965]
Epoch [59/120    avg_loss:0.119, val_acc:0.958]
Epoch [60/120    avg_loss:0.099, val_acc:0.967]
Epoch [61/120    avg_loss:0.094, val_acc:0.971]
Epoch [62/120    avg_loss:0.107, val_acc:0.977]
Epoch [63/120    avg_loss:0.099, val_acc:0.956]
Epoch [64/120    avg_loss:0.140, val_acc:0.981]
Epoch [65/120    avg_loss:0.088, val_acc:0.967]
Epoch [66/120    avg_loss:0.081, val_acc:0.975]
Epoch [67/120    avg_loss:0.085, val_acc:0.979]
Epoch [68/120    avg_loss:0.053, val_acc:0.981]
Epoch [69/120    avg_loss:0.061, val_acc:0.979]
Epoch [70/120    avg_loss:0.080, val_acc:0.965]
Epoch [71/120    avg_loss:0.097, val_acc:0.979]
Epoch [72/120    avg_loss:0.048, val_acc:0.983]
Epoch [73/120    avg_loss:0.065, val_acc:0.973]
Epoch [74/120    avg_loss:0.058, val_acc:0.981]
Epoch [75/120    avg_loss:0.040, val_acc:0.985]
Epoch [76/120    avg_loss:0.046, val_acc:0.981]
Epoch [77/120    avg_loss:0.076, val_acc:0.981]
Epoch [78/120    avg_loss:0.042, val_acc:0.983]
Epoch [79/120    avg_loss:0.064, val_acc:0.981]
Epoch [80/120    avg_loss:0.104, val_acc:0.977]
Epoch [81/120    avg_loss:0.081, val_acc:0.975]
Epoch [82/120    avg_loss:0.124, val_acc:0.969]
Epoch [83/120    avg_loss:0.103, val_acc:0.979]
Epoch [84/120    avg_loss:0.071, val_acc:0.977]
Epoch [85/120    avg_loss:0.053, val_acc:0.983]
Epoch [86/120    avg_loss:0.099, val_acc:0.981]
Epoch [87/120    avg_loss:0.070, val_acc:0.977]
Epoch [88/120    avg_loss:0.112, val_acc:0.981]
Epoch [89/120    avg_loss:0.041, val_acc:0.988]
Epoch [90/120    avg_loss:0.034, val_acc:0.990]
Epoch [91/120    avg_loss:0.034, val_acc:0.988]
Epoch [92/120    avg_loss:0.038, val_acc:0.988]
Epoch [93/120    avg_loss:0.031, val_acc:0.988]
Epoch [94/120    avg_loss:0.031, val_acc:0.988]
Epoch [95/120    avg_loss:0.029, val_acc:0.988]
Epoch [96/120    avg_loss:0.027, val_acc:0.988]
Epoch [97/120    avg_loss:0.033, val_acc:0.985]
Epoch [98/120    avg_loss:0.029, val_acc:0.985]
Epoch [99/120    avg_loss:0.026, val_acc:0.985]
Epoch [100/120    avg_loss:0.025, val_acc:0.985]
Epoch [101/120    avg_loss:0.031, val_acc:0.985]
Epoch [102/120    avg_loss:0.031, val_acc:0.985]
Epoch [103/120    avg_loss:0.031, val_acc:0.985]
Epoch [104/120    avg_loss:0.023, val_acc:0.988]
Epoch [105/120    avg_loss:0.023, val_acc:0.988]
Epoch [106/120    avg_loss:0.025, val_acc:0.988]
Epoch [107/120    avg_loss:0.023, val_acc:0.988]
Epoch [108/120    avg_loss:0.021, val_acc:0.988]
Epoch [109/120    avg_loss:0.025, val_acc:0.988]
Epoch [110/120    avg_loss:0.030, val_acc:0.988]
Epoch [111/120    avg_loss:0.021, val_acc:0.988]
Epoch [112/120    avg_loss:0.028, val_acc:0.988]
Epoch [113/120    avg_loss:0.024, val_acc:0.988]
Epoch [114/120    avg_loss:0.022, val_acc:0.988]
Epoch [115/120    avg_loss:0.028, val_acc:0.988]
Epoch [116/120    avg_loss:0.023, val_acc:0.988]
Epoch [117/120    avg_loss:0.032, val_acc:0.988]
Epoch [118/120    avg_loss:0.028, val_acc:0.988]
Epoch [119/120    avg_loss:0.031, val_acc:0.988]
Epoch [120/120    avg_loss:0.024, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.98871332 0.98454746 0.94407159 0.94078947
 0.99756691 0.9726776  1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9926412799344371
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15b26a7c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.296]
Epoch [2/120    avg_loss:2.146, val_acc:0.448]
Epoch [3/120    avg_loss:1.906, val_acc:0.577]
Epoch [4/120    avg_loss:1.683, val_acc:0.625]
Epoch [5/120    avg_loss:1.487, val_acc:0.625]
Epoch [6/120    avg_loss:1.336, val_acc:0.756]
Epoch [7/120    avg_loss:1.170, val_acc:0.787]
Epoch [8/120    avg_loss:1.037, val_acc:0.746]
Epoch [9/120    avg_loss:0.973, val_acc:0.790]
Epoch [10/120    avg_loss:0.892, val_acc:0.840]
Epoch [11/120    avg_loss:0.818, val_acc:0.860]
Epoch [12/120    avg_loss:0.702, val_acc:0.842]
Epoch [13/120    avg_loss:0.689, val_acc:0.923]
Epoch [14/120    avg_loss:0.625, val_acc:0.875]
Epoch [15/120    avg_loss:0.542, val_acc:0.908]
Epoch [16/120    avg_loss:0.539, val_acc:0.919]
Epoch [17/120    avg_loss:0.540, val_acc:0.892]
Epoch [18/120    avg_loss:0.474, val_acc:0.921]
Epoch [19/120    avg_loss:0.492, val_acc:0.925]
Epoch [20/120    avg_loss:0.437, val_acc:0.921]
Epoch [21/120    avg_loss:0.379, val_acc:0.917]
Epoch [22/120    avg_loss:0.398, val_acc:0.931]
Epoch [23/120    avg_loss:0.346, val_acc:0.925]
Epoch [24/120    avg_loss:0.350, val_acc:0.946]
Epoch [25/120    avg_loss:0.303, val_acc:0.938]
Epoch [26/120    avg_loss:0.352, val_acc:0.885]
Epoch [27/120    avg_loss:0.346, val_acc:0.950]
Epoch [28/120    avg_loss:0.349, val_acc:0.954]
Epoch [29/120    avg_loss:0.315, val_acc:0.946]
Epoch [30/120    avg_loss:0.292, val_acc:0.933]
Epoch [31/120    avg_loss:0.299, val_acc:0.935]
Epoch [32/120    avg_loss:0.297, val_acc:0.940]
Epoch [33/120    avg_loss:0.244, val_acc:0.956]
Epoch [34/120    avg_loss:0.230, val_acc:0.917]
Epoch [35/120    avg_loss:0.237, val_acc:0.952]
Epoch [36/120    avg_loss:0.232, val_acc:0.938]
Epoch [37/120    avg_loss:0.216, val_acc:0.898]
Epoch [38/120    avg_loss:0.230, val_acc:0.969]
Epoch [39/120    avg_loss:0.193, val_acc:0.975]
Epoch [40/120    avg_loss:0.198, val_acc:0.940]
Epoch [41/120    avg_loss:0.226, val_acc:0.931]
Epoch [42/120    avg_loss:0.253, val_acc:0.954]
Epoch [43/120    avg_loss:0.234, val_acc:0.946]
Epoch [44/120    avg_loss:0.225, val_acc:0.935]
Epoch [45/120    avg_loss:0.191, val_acc:0.958]
Epoch [46/120    avg_loss:0.202, val_acc:0.950]
Epoch [47/120    avg_loss:0.165, val_acc:0.960]
Epoch [48/120    avg_loss:0.144, val_acc:0.946]
Epoch [49/120    avg_loss:0.145, val_acc:0.969]
Epoch [50/120    avg_loss:0.114, val_acc:0.971]
Epoch [51/120    avg_loss:0.107, val_acc:0.973]
Epoch [52/120    avg_loss:0.130, val_acc:0.983]
Epoch [53/120    avg_loss:0.088, val_acc:0.977]
Epoch [54/120    avg_loss:0.102, val_acc:0.977]
Epoch [55/120    avg_loss:0.102, val_acc:0.979]
Epoch [56/120    avg_loss:0.088, val_acc:0.983]
Epoch [57/120    avg_loss:0.103, val_acc:0.971]
Epoch [58/120    avg_loss:0.131, val_acc:0.979]
Epoch [59/120    avg_loss:0.105, val_acc:0.975]
Epoch [60/120    avg_loss:0.095, val_acc:0.973]
Epoch [61/120    avg_loss:0.114, val_acc:0.975]
Epoch [62/120    avg_loss:0.089, val_acc:0.981]
Epoch [63/120    avg_loss:0.068, val_acc:0.973]
Epoch [64/120    avg_loss:0.066, val_acc:0.983]
Epoch [65/120    avg_loss:0.066, val_acc:0.975]
Epoch [66/120    avg_loss:0.080, val_acc:0.981]
Epoch [67/120    avg_loss:0.097, val_acc:0.952]
Epoch [68/120    avg_loss:0.061, val_acc:0.983]
Epoch [69/120    avg_loss:0.059, val_acc:0.983]
Epoch [70/120    avg_loss:0.065, val_acc:0.985]
Epoch [71/120    avg_loss:0.066, val_acc:0.977]
Epoch [72/120    avg_loss:0.078, val_acc:0.992]
Epoch [73/120    avg_loss:0.064, val_acc:0.979]
Epoch [74/120    avg_loss:0.086, val_acc:0.983]
Epoch [75/120    avg_loss:0.078, val_acc:0.990]
Epoch [76/120    avg_loss:0.059, val_acc:0.990]
Epoch [77/120    avg_loss:0.054, val_acc:0.975]
Epoch [78/120    avg_loss:0.122, val_acc:0.983]
Epoch [79/120    avg_loss:0.135, val_acc:0.971]
Epoch [80/120    avg_loss:0.075, val_acc:0.983]
Epoch [81/120    avg_loss:0.065, val_acc:0.985]
Epoch [82/120    avg_loss:0.067, val_acc:0.975]
Epoch [83/120    avg_loss:0.091, val_acc:0.981]
Epoch [84/120    avg_loss:0.065, val_acc:0.988]
Epoch [85/120    avg_loss:0.068, val_acc:0.988]
Epoch [86/120    avg_loss:0.041, val_acc:0.985]
Epoch [87/120    avg_loss:0.041, val_acc:0.988]
Epoch [88/120    avg_loss:0.040, val_acc:0.990]
Epoch [89/120    avg_loss:0.039, val_acc:0.992]
Epoch [90/120    avg_loss:0.029, val_acc:0.992]
Epoch [91/120    avg_loss:0.049, val_acc:0.988]
Epoch [92/120    avg_loss:0.034, val_acc:0.990]
Epoch [93/120    avg_loss:0.029, val_acc:0.988]
Epoch [94/120    avg_loss:0.036, val_acc:0.988]
Epoch [95/120    avg_loss:0.029, val_acc:0.988]
Epoch [96/120    avg_loss:0.026, val_acc:0.990]
Epoch [97/120    avg_loss:0.027, val_acc:0.990]
Epoch [98/120    avg_loss:0.023, val_acc:0.990]
Epoch [99/120    avg_loss:0.027, val_acc:0.990]
Epoch [100/120    avg_loss:0.035, val_acc:0.988]
Epoch [101/120    avg_loss:0.027, val_acc:0.990]
Epoch [102/120    avg_loss:0.024, val_acc:0.992]
Epoch [103/120    avg_loss:0.027, val_acc:0.990]
Epoch [104/120    avg_loss:0.031, val_acc:0.990]
Epoch [105/120    avg_loss:0.039, val_acc:0.992]
Epoch [106/120    avg_loss:0.039, val_acc:0.990]
Epoch [107/120    avg_loss:0.026, val_acc:0.990]
Epoch [108/120    avg_loss:0.030, val_acc:0.992]
Epoch [109/120    avg_loss:0.022, val_acc:0.990]
Epoch [110/120    avg_loss:0.029, val_acc:0.992]
Epoch [111/120    avg_loss:0.024, val_acc:0.990]
Epoch [112/120    avg_loss:0.029, val_acc:0.990]
Epoch [113/120    avg_loss:0.026, val_acc:0.992]
Epoch [114/120    avg_loss:0.027, val_acc:0.988]
Epoch [115/120    avg_loss:0.027, val_acc:0.988]
Epoch [116/120    avg_loss:0.030, val_acc:0.990]
Epoch [117/120    avg_loss:0.028, val_acc:0.992]
Epoch [118/120    avg_loss:0.026, val_acc:0.988]
Epoch [119/120    avg_loss:0.030, val_acc:0.992]
Epoch [120/120    avg_loss:0.029, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.98866213 0.98678414 0.93569845 0.93687708
 0.99266504 0.97297297 1.         1.         1.         0.98562092
 0.98660714 1.        ]

Kappa:
0.9893184249148284
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9446009860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.531, val_acc:0.338]
Epoch [2/120    avg_loss:2.158, val_acc:0.531]
Epoch [3/120    avg_loss:1.961, val_acc:0.683]
Epoch [4/120    avg_loss:1.766, val_acc:0.690]
Epoch [5/120    avg_loss:1.594, val_acc:0.673]
Epoch [6/120    avg_loss:1.408, val_acc:0.704]
Epoch [7/120    avg_loss:1.204, val_acc:0.694]
Epoch [8/120    avg_loss:1.042, val_acc:0.812]
Epoch [9/120    avg_loss:0.916, val_acc:0.771]
Epoch [10/120    avg_loss:0.797, val_acc:0.806]
Epoch [11/120    avg_loss:0.702, val_acc:0.896]
Epoch [12/120    avg_loss:0.695, val_acc:0.838]
Epoch [13/120    avg_loss:0.591, val_acc:0.898]
Epoch [14/120    avg_loss:0.531, val_acc:0.898]
Epoch [15/120    avg_loss:0.496, val_acc:0.902]
Epoch [16/120    avg_loss:0.438, val_acc:0.900]
Epoch [17/120    avg_loss:0.468, val_acc:0.844]
Epoch [18/120    avg_loss:0.442, val_acc:0.929]
Epoch [19/120    avg_loss:0.421, val_acc:0.908]
Epoch [20/120    avg_loss:0.400, val_acc:0.923]
Epoch [21/120    avg_loss:0.339, val_acc:0.921]
Epoch [22/120    avg_loss:0.320, val_acc:0.942]
Epoch [23/120    avg_loss:0.347, val_acc:0.944]
Epoch [24/120    avg_loss:0.309, val_acc:0.933]
Epoch [25/120    avg_loss:0.318, val_acc:0.935]
Epoch [26/120    avg_loss:0.256, val_acc:0.931]
Epoch [27/120    avg_loss:0.298, val_acc:0.958]
Epoch [28/120    avg_loss:0.336, val_acc:0.904]
Epoch [29/120    avg_loss:0.280, val_acc:0.950]
Epoch [30/120    avg_loss:0.336, val_acc:0.935]
Epoch [31/120    avg_loss:0.258, val_acc:0.965]
Epoch [32/120    avg_loss:0.243, val_acc:0.948]
Epoch [33/120    avg_loss:0.231, val_acc:0.952]
Epoch [34/120    avg_loss:0.216, val_acc:0.971]
Epoch [35/120    avg_loss:0.201, val_acc:0.942]
Epoch [36/120    avg_loss:0.258, val_acc:0.931]
Epoch [37/120    avg_loss:0.269, val_acc:0.940]
Epoch [38/120    avg_loss:0.227, val_acc:0.958]
Epoch [39/120    avg_loss:0.238, val_acc:0.944]
Epoch [40/120    avg_loss:0.235, val_acc:0.969]
Epoch [41/120    avg_loss:0.188, val_acc:0.950]
Epoch [42/120    avg_loss:0.181, val_acc:0.946]
Epoch [43/120    avg_loss:0.211, val_acc:0.969]
Epoch [44/120    avg_loss:0.206, val_acc:0.967]
Epoch [45/120    avg_loss:0.203, val_acc:0.960]
Epoch [46/120    avg_loss:0.165, val_acc:0.963]
Epoch [47/120    avg_loss:0.175, val_acc:0.967]
Epoch [48/120    avg_loss:0.140, val_acc:0.975]
Epoch [49/120    avg_loss:0.118, val_acc:0.977]
Epoch [50/120    avg_loss:0.101, val_acc:0.975]
Epoch [51/120    avg_loss:0.112, val_acc:0.975]
Epoch [52/120    avg_loss:0.095, val_acc:0.975]
Epoch [53/120    avg_loss:0.105, val_acc:0.975]
Epoch [54/120    avg_loss:0.099, val_acc:0.979]
Epoch [55/120    avg_loss:0.095, val_acc:0.979]
Epoch [56/120    avg_loss:0.108, val_acc:0.975]
Epoch [57/120    avg_loss:0.082, val_acc:0.973]
Epoch [58/120    avg_loss:0.086, val_acc:0.977]
Epoch [59/120    avg_loss:0.108, val_acc:0.977]
Epoch [60/120    avg_loss:0.097, val_acc:0.977]
Epoch [61/120    avg_loss:0.094, val_acc:0.973]
Epoch [62/120    avg_loss:0.092, val_acc:0.975]
Epoch [63/120    avg_loss:0.089, val_acc:0.977]
Epoch [64/120    avg_loss:0.082, val_acc:0.975]
Epoch [65/120    avg_loss:0.094, val_acc:0.975]
Epoch [66/120    avg_loss:0.085, val_acc:0.975]
Epoch [67/120    avg_loss:0.084, val_acc:0.975]
Epoch [68/120    avg_loss:0.077, val_acc:0.973]
Epoch [69/120    avg_loss:0.083, val_acc:0.973]
Epoch [70/120    avg_loss:0.083, val_acc:0.975]
Epoch [71/120    avg_loss:0.079, val_acc:0.975]
Epoch [72/120    avg_loss:0.079, val_acc:0.977]
Epoch [73/120    avg_loss:0.080, val_acc:0.977]
Epoch [74/120    avg_loss:0.075, val_acc:0.977]
Epoch [75/120    avg_loss:0.073, val_acc:0.977]
Epoch [76/120    avg_loss:0.091, val_acc:0.977]
Epoch [77/120    avg_loss:0.083, val_acc:0.979]
Epoch [78/120    avg_loss:0.089, val_acc:0.979]
Epoch [79/120    avg_loss:0.088, val_acc:0.979]
Epoch [80/120    avg_loss:0.091, val_acc:0.979]
Epoch [81/120    avg_loss:0.090, val_acc:0.979]
Epoch [82/120    avg_loss:0.080, val_acc:0.979]
Epoch [83/120    avg_loss:0.075, val_acc:0.979]
Epoch [84/120    avg_loss:0.098, val_acc:0.979]
Epoch [85/120    avg_loss:0.082, val_acc:0.979]
Epoch [86/120    avg_loss:0.073, val_acc:0.979]
Epoch [87/120    avg_loss:0.089, val_acc:0.979]
Epoch [88/120    avg_loss:0.081, val_acc:0.979]
Epoch [89/120    avg_loss:0.081, val_acc:0.977]
Epoch [90/120    avg_loss:0.079, val_acc:0.977]
Epoch [91/120    avg_loss:0.068, val_acc:0.979]
Epoch [92/120    avg_loss:0.075, val_acc:0.979]
Epoch [93/120    avg_loss:0.093, val_acc:0.979]
Epoch [94/120    avg_loss:0.076, val_acc:0.979]
Epoch [95/120    avg_loss:0.077, val_acc:0.977]
Epoch [96/120    avg_loss:0.087, val_acc:0.977]
Epoch [97/120    avg_loss:0.064, val_acc:0.977]
Epoch [98/120    avg_loss:0.070, val_acc:0.977]
Epoch [99/120    avg_loss:0.077, val_acc:0.979]
Epoch [100/120    avg_loss:0.083, val_acc:0.979]
Epoch [101/120    avg_loss:0.088, val_acc:0.977]
Epoch [102/120    avg_loss:0.074, val_acc:0.979]
Epoch [103/120    avg_loss:0.082, val_acc:0.979]
Epoch [104/120    avg_loss:0.108, val_acc:0.979]
Epoch [105/120    avg_loss:0.073, val_acc:0.979]
Epoch [106/120    avg_loss:0.095, val_acc:0.979]
Epoch [107/120    avg_loss:0.072, val_acc:0.979]
Epoch [108/120    avg_loss:0.082, val_acc:0.979]
Epoch [109/120    avg_loss:0.082, val_acc:0.979]
Epoch [110/120    avg_loss:0.076, val_acc:0.977]
Epoch [111/120    avg_loss:0.092, val_acc:0.977]
Epoch [112/120    avg_loss:0.071, val_acc:0.977]
Epoch [113/120    avg_loss:0.068, val_acc:0.977]
Epoch [114/120    avg_loss:0.084, val_acc:0.977]
Epoch [115/120    avg_loss:0.083, val_acc:0.977]
Epoch [116/120    avg_loss:0.084, val_acc:0.977]
Epoch [117/120    avg_loss:0.066, val_acc:0.977]
Epoch [118/120    avg_loss:0.072, val_acc:0.977]
Epoch [119/120    avg_loss:0.073, val_acc:0.977]
Epoch [120/120    avg_loss:0.094, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   2 219   4   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   3  12 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.2089552238806

F1 scores:
[       nan 1.         0.94144144 0.96902655 0.90540541 0.86378738
 1.         0.86956522 0.99487179 0.99893276 1.         0.98944591
 0.99113082 1.        ]

Kappa:
0.9800598175105839
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffb0eba6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.443, val_acc:0.338]
Epoch [2/120    avg_loss:2.104, val_acc:0.525]
Epoch [3/120    avg_loss:1.770, val_acc:0.690]
Epoch [4/120    avg_loss:1.498, val_acc:0.640]
Epoch [5/120    avg_loss:1.263, val_acc:0.656]
Epoch [6/120    avg_loss:1.100, val_acc:0.735]
Epoch [7/120    avg_loss:0.946, val_acc:0.842]
Epoch [8/120    avg_loss:0.842, val_acc:0.863]
Epoch [9/120    avg_loss:0.732, val_acc:0.871]
Epoch [10/120    avg_loss:0.679, val_acc:0.871]
Epoch [11/120    avg_loss:0.635, val_acc:0.894]
Epoch [12/120    avg_loss:0.607, val_acc:0.875]
Epoch [13/120    avg_loss:0.554, val_acc:0.879]
Epoch [14/120    avg_loss:0.522, val_acc:0.894]
Epoch [15/120    avg_loss:0.553, val_acc:0.887]
Epoch [16/120    avg_loss:0.556, val_acc:0.887]
Epoch [17/120    avg_loss:0.432, val_acc:0.912]
Epoch [18/120    avg_loss:0.409, val_acc:0.935]
Epoch [19/120    avg_loss:0.439, val_acc:0.929]
Epoch [20/120    avg_loss:0.391, val_acc:0.929]
Epoch [21/120    avg_loss:0.409, val_acc:0.904]
Epoch [22/120    avg_loss:0.334, val_acc:0.935]
Epoch [23/120    avg_loss:0.324, val_acc:0.933]
Epoch [24/120    avg_loss:0.361, val_acc:0.940]
Epoch [25/120    avg_loss:0.280, val_acc:0.931]
Epoch [26/120    avg_loss:0.306, val_acc:0.954]
Epoch [27/120    avg_loss:0.274, val_acc:0.942]
Epoch [28/120    avg_loss:0.272, val_acc:0.931]
Epoch [29/120    avg_loss:0.297, val_acc:0.942]
Epoch [30/120    avg_loss:0.264, val_acc:0.942]
Epoch [31/120    avg_loss:0.240, val_acc:0.915]
Epoch [32/120    avg_loss:0.270, val_acc:0.946]
Epoch [33/120    avg_loss:0.222, val_acc:0.965]
Epoch [34/120    avg_loss:0.189, val_acc:0.965]
Epoch [35/120    avg_loss:0.200, val_acc:0.952]
Epoch [36/120    avg_loss:0.189, val_acc:0.963]
Epoch [37/120    avg_loss:0.267, val_acc:0.958]
Epoch [38/120    avg_loss:0.172, val_acc:0.912]
Epoch [39/120    avg_loss:0.194, val_acc:0.956]
Epoch [40/120    avg_loss:0.208, val_acc:0.946]
Epoch [41/120    avg_loss:0.159, val_acc:0.969]
Epoch [42/120    avg_loss:0.167, val_acc:0.981]
Epoch [43/120    avg_loss:0.124, val_acc:0.960]
Epoch [44/120    avg_loss:0.145, val_acc:0.960]
Epoch [45/120    avg_loss:0.210, val_acc:0.971]
Epoch [46/120    avg_loss:0.182, val_acc:0.956]
Epoch [47/120    avg_loss:0.145, val_acc:0.973]
Epoch [48/120    avg_loss:0.126, val_acc:0.975]
Epoch [49/120    avg_loss:0.162, val_acc:0.983]
Epoch [50/120    avg_loss:0.121, val_acc:0.960]
Epoch [51/120    avg_loss:0.120, val_acc:0.985]
Epoch [52/120    avg_loss:0.143, val_acc:0.944]
Epoch [53/120    avg_loss:0.197, val_acc:0.967]
Epoch [54/120    avg_loss:0.162, val_acc:0.979]
Epoch [55/120    avg_loss:0.083, val_acc:0.977]
Epoch [56/120    avg_loss:0.101, val_acc:0.990]
Epoch [57/120    avg_loss:0.093, val_acc:0.981]
Epoch [58/120    avg_loss:0.152, val_acc:0.950]
Epoch [59/120    avg_loss:0.188, val_acc:0.971]
Epoch [60/120    avg_loss:0.159, val_acc:0.967]
Epoch [61/120    avg_loss:0.096, val_acc:0.981]
Epoch [62/120    avg_loss:0.088, val_acc:0.981]
Epoch [63/120    avg_loss:0.084, val_acc:0.979]
Epoch [64/120    avg_loss:0.061, val_acc:0.981]
Epoch [65/120    avg_loss:0.073, val_acc:0.988]
Epoch [66/120    avg_loss:0.069, val_acc:0.979]
Epoch [67/120    avg_loss:0.077, val_acc:0.985]
Epoch [68/120    avg_loss:0.090, val_acc:0.975]
Epoch [69/120    avg_loss:0.062, val_acc:0.990]
Epoch [70/120    avg_loss:0.051, val_acc:0.971]
Epoch [71/120    avg_loss:0.054, val_acc:0.988]
Epoch [72/120    avg_loss:0.079, val_acc:0.985]
Epoch [73/120    avg_loss:0.134, val_acc:0.963]
Epoch [74/120    avg_loss:0.096, val_acc:0.973]
Epoch [75/120    avg_loss:0.075, val_acc:0.992]
Epoch [76/120    avg_loss:0.063, val_acc:0.971]
Epoch [77/120    avg_loss:0.099, val_acc:0.963]
Epoch [78/120    avg_loss:0.123, val_acc:0.965]
Epoch [79/120    avg_loss:0.102, val_acc:0.975]
Epoch [80/120    avg_loss:0.096, val_acc:0.983]
Epoch [81/120    avg_loss:0.090, val_acc:0.977]
Epoch [82/120    avg_loss:0.074, val_acc:0.985]
Epoch [83/120    avg_loss:0.055, val_acc:0.992]
Epoch [84/120    avg_loss:0.048, val_acc:0.990]
Epoch [85/120    avg_loss:0.033, val_acc:0.988]
Epoch [86/120    avg_loss:0.034, val_acc:0.994]
Epoch [87/120    avg_loss:0.031, val_acc:0.990]
Epoch [88/120    avg_loss:0.027, val_acc:0.996]
Epoch [89/120    avg_loss:0.045, val_acc:0.969]
Epoch [90/120    avg_loss:0.045, val_acc:0.996]
Epoch [91/120    avg_loss:0.048, val_acc:0.988]
Epoch [92/120    avg_loss:0.030, val_acc:0.992]
Epoch [93/120    avg_loss:0.035, val_acc:0.985]
Epoch [94/120    avg_loss:0.026, val_acc:0.988]
Epoch [95/120    avg_loss:0.027, val_acc:0.985]
Epoch [96/120    avg_loss:0.036, val_acc:0.992]
Epoch [97/120    avg_loss:0.032, val_acc:0.990]
Epoch [98/120    avg_loss:0.028, val_acc:0.992]
Epoch [99/120    avg_loss:0.021, val_acc:0.996]
Epoch [100/120    avg_loss:0.029, val_acc:0.994]
Epoch [101/120    avg_loss:0.027, val_acc:0.990]
Epoch [102/120    avg_loss:0.025, val_acc:0.994]
Epoch [103/120    avg_loss:0.058, val_acc:0.992]
Epoch [104/120    avg_loss:0.092, val_acc:0.975]
Epoch [105/120    avg_loss:0.069, val_acc:0.990]
Epoch [106/120    avg_loss:0.042, val_acc:0.985]
Epoch [107/120    avg_loss:0.030, val_acc:0.992]
Epoch [108/120    avg_loss:0.020, val_acc:0.992]
Epoch [109/120    avg_loss:0.028, val_acc:0.990]
Epoch [110/120    avg_loss:0.031, val_acc:0.979]
Epoch [111/120    avg_loss:0.023, val_acc:1.000]
Epoch [112/120    avg_loss:0.026, val_acc:0.990]
Epoch [113/120    avg_loss:0.022, val_acc:0.998]
Epoch [114/120    avg_loss:0.025, val_acc:0.998]
Epoch [115/120    avg_loss:0.023, val_acc:0.994]
Epoch [116/120    avg_loss:0.014, val_acc:0.994]
Epoch [117/120    avg_loss:0.010, val_acc:0.996]
Epoch [118/120    avg_loss:0.011, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.994]
Epoch [120/120    avg_loss:0.011, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98850575 0.98004435 0.92857143 0.92459016
 1.         0.97916667 1.         1.         1.         0.99867198
 0.99779249 1.        ]

Kappa:
0.9909803789587049
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4614e7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.475, val_acc:0.356]
Epoch [2/120    avg_loss:2.103, val_acc:0.540]
Epoch [3/120    avg_loss:1.844, val_acc:0.588]
Epoch [4/120    avg_loss:1.651, val_acc:0.652]
Epoch [5/120    avg_loss:1.459, val_acc:0.633]
Epoch [6/120    avg_loss:1.290, val_acc:0.665]
Epoch [7/120    avg_loss:1.123, val_acc:0.721]
Epoch [8/120    avg_loss:0.985, val_acc:0.750]
Epoch [9/120    avg_loss:0.894, val_acc:0.765]
Epoch [10/120    avg_loss:0.797, val_acc:0.804]
Epoch [11/120    avg_loss:0.790, val_acc:0.760]
Epoch [12/120    avg_loss:0.666, val_acc:0.858]
Epoch [13/120    avg_loss:0.640, val_acc:0.790]
Epoch [14/120    avg_loss:0.574, val_acc:0.762]
Epoch [15/120    avg_loss:0.522, val_acc:0.898]
Epoch [16/120    avg_loss:0.501, val_acc:0.902]
Epoch [17/120    avg_loss:0.520, val_acc:0.819]
Epoch [18/120    avg_loss:0.486, val_acc:0.912]
Epoch [19/120    avg_loss:0.465, val_acc:0.923]
Epoch [20/120    avg_loss:0.482, val_acc:0.921]
Epoch [21/120    avg_loss:0.413, val_acc:0.823]
Epoch [22/120    avg_loss:0.348, val_acc:0.935]
Epoch [23/120    avg_loss:0.339, val_acc:0.942]
Epoch [24/120    avg_loss:0.363, val_acc:0.948]
Epoch [25/120    avg_loss:0.365, val_acc:0.940]
Epoch [26/120    avg_loss:0.355, val_acc:0.948]
Epoch [27/120    avg_loss:0.290, val_acc:0.929]
Epoch [28/120    avg_loss:0.292, val_acc:0.950]
Epoch [29/120    avg_loss:0.240, val_acc:0.946]
Epoch [30/120    avg_loss:0.293, val_acc:0.877]
Epoch [31/120    avg_loss:0.362, val_acc:0.940]
Epoch [32/120    avg_loss:0.286, val_acc:0.948]
Epoch [33/120    avg_loss:0.274, val_acc:0.958]
Epoch [34/120    avg_loss:0.227, val_acc:0.975]
Epoch [35/120    avg_loss:0.172, val_acc:0.971]
Epoch [36/120    avg_loss:0.162, val_acc:0.971]
Epoch [37/120    avg_loss:0.162, val_acc:0.975]
Epoch [38/120    avg_loss:0.170, val_acc:0.975]
Epoch [39/120    avg_loss:0.140, val_acc:0.975]
Epoch [40/120    avg_loss:0.136, val_acc:0.977]
Epoch [41/120    avg_loss:0.146, val_acc:0.967]
Epoch [42/120    avg_loss:0.144, val_acc:0.969]
Epoch [43/120    avg_loss:0.161, val_acc:0.940]
Epoch [44/120    avg_loss:0.148, val_acc:0.979]
Epoch [45/120    avg_loss:0.138, val_acc:0.985]
Epoch [46/120    avg_loss:0.156, val_acc:0.981]
Epoch [47/120    avg_loss:0.133, val_acc:0.975]
Epoch [48/120    avg_loss:0.133, val_acc:0.963]
Epoch [49/120    avg_loss:0.107, val_acc:0.981]
Epoch [50/120    avg_loss:0.130, val_acc:0.975]
Epoch [51/120    avg_loss:0.134, val_acc:0.967]
Epoch [52/120    avg_loss:0.102, val_acc:0.979]
Epoch [53/120    avg_loss:0.123, val_acc:0.983]
Epoch [54/120    avg_loss:0.086, val_acc:0.988]
Epoch [55/120    avg_loss:0.101, val_acc:0.988]
Epoch [56/120    avg_loss:0.087, val_acc:0.985]
Epoch [57/120    avg_loss:0.087, val_acc:0.983]
Epoch [58/120    avg_loss:0.088, val_acc:0.985]
Epoch [59/120    avg_loss:0.078, val_acc:0.981]
Epoch [60/120    avg_loss:0.109, val_acc:0.990]
Epoch [61/120    avg_loss:0.063, val_acc:0.985]
Epoch [62/120    avg_loss:0.056, val_acc:0.988]
Epoch [63/120    avg_loss:0.051, val_acc:0.985]
Epoch [64/120    avg_loss:0.042, val_acc:0.990]
Epoch [65/120    avg_loss:0.059, val_acc:0.988]
Epoch [66/120    avg_loss:0.051, val_acc:0.985]
Epoch [67/120    avg_loss:0.074, val_acc:0.990]
Epoch [68/120    avg_loss:0.070, val_acc:0.990]
Epoch [69/120    avg_loss:0.097, val_acc:0.985]
Epoch [70/120    avg_loss:0.063, val_acc:0.988]
Epoch [71/120    avg_loss:0.079, val_acc:0.981]
Epoch [72/120    avg_loss:0.067, val_acc:0.988]
Epoch [73/120    avg_loss:0.063, val_acc:0.992]
Epoch [74/120    avg_loss:0.048, val_acc:0.981]
Epoch [75/120    avg_loss:0.058, val_acc:0.983]
Epoch [76/120    avg_loss:0.052, val_acc:0.973]
Epoch [77/120    avg_loss:0.076, val_acc:0.985]
Epoch [78/120    avg_loss:0.057, val_acc:0.990]
Epoch [79/120    avg_loss:0.043, val_acc:0.992]
Epoch [80/120    avg_loss:0.057, val_acc:0.988]
Epoch [81/120    avg_loss:0.043, val_acc:0.996]
Epoch [82/120    avg_loss:0.039, val_acc:0.994]
Epoch [83/120    avg_loss:0.041, val_acc:0.988]
Epoch [84/120    avg_loss:0.035, val_acc:0.992]
Epoch [85/120    avg_loss:0.036, val_acc:0.990]
Epoch [86/120    avg_loss:0.020, val_acc:0.992]
Epoch [87/120    avg_loss:0.039, val_acc:0.990]
Epoch [88/120    avg_loss:0.027, val_acc:0.990]
Epoch [89/120    avg_loss:0.032, val_acc:0.990]
Epoch [90/120    avg_loss:0.030, val_acc:0.992]
Epoch [91/120    avg_loss:0.039, val_acc:0.990]
Epoch [92/120    avg_loss:0.034, val_acc:0.992]
Epoch [93/120    avg_loss:0.025, val_acc:0.985]
Epoch [94/120    avg_loss:0.059, val_acc:0.983]
Epoch [95/120    avg_loss:0.047, val_acc:0.990]
Epoch [96/120    avg_loss:0.039, val_acc:0.990]
Epoch [97/120    avg_loss:0.019, val_acc:0.990]
Epoch [98/120    avg_loss:0.024, val_acc:0.990]
Epoch [99/120    avg_loss:0.031, val_acc:0.990]
Epoch [100/120    avg_loss:0.025, val_acc:0.990]
Epoch [101/120    avg_loss:0.017, val_acc:0.990]
Epoch [102/120    avg_loss:0.020, val_acc:0.990]
Epoch [103/120    avg_loss:0.019, val_acc:0.990]
Epoch [104/120    avg_loss:0.017, val_acc:0.990]
Epoch [105/120    avg_loss:0.020, val_acc:0.990]
Epoch [106/120    avg_loss:0.018, val_acc:0.990]
Epoch [107/120    avg_loss:0.016, val_acc:0.990]
Epoch [108/120    avg_loss:0.016, val_acc:0.990]
Epoch [109/120    avg_loss:0.023, val_acc:0.990]
Epoch [110/120    avg_loss:0.018, val_acc:0.990]
Epoch [111/120    avg_loss:0.015, val_acc:0.990]
Epoch [112/120    avg_loss:0.021, val_acc:0.990]
Epoch [113/120    avg_loss:0.027, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.990]
Epoch [115/120    avg_loss:0.017, val_acc:0.990]
Epoch [116/120    avg_loss:0.014, val_acc:0.990]
Epoch [117/120    avg_loss:0.017, val_acc:0.990]
Epoch [118/120    avg_loss:0.017, val_acc:0.990]
Epoch [119/120    avg_loss:0.020, val_acc:0.990]
Epoch [120/120    avg_loss:0.017, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   3   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         0.99095023 0.98901099 0.95927602 0.95081967
 1.         0.97826087 0.99742931 1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9940656135125652
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3493723898>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.469, val_acc:0.435]
Epoch [2/120    avg_loss:2.111, val_acc:0.544]
Epoch [3/120    avg_loss:1.855, val_acc:0.602]
Epoch [4/120    avg_loss:1.616, val_acc:0.610]
Epoch [5/120    avg_loss:1.393, val_acc:0.746]
Epoch [6/120    avg_loss:1.231, val_acc:0.754]
Epoch [7/120    avg_loss:1.084, val_acc:0.733]
Epoch [8/120    avg_loss:0.949, val_acc:0.794]
Epoch [9/120    avg_loss:0.808, val_acc:0.838]
Epoch [10/120    avg_loss:0.823, val_acc:0.850]
Epoch [11/120    avg_loss:0.723, val_acc:0.869]
Epoch [12/120    avg_loss:0.751, val_acc:0.848]
Epoch [13/120    avg_loss:0.627, val_acc:0.881]
Epoch [14/120    avg_loss:0.544, val_acc:0.869]
Epoch [15/120    avg_loss:0.533, val_acc:0.919]
Epoch [16/120    avg_loss:0.470, val_acc:0.923]
Epoch [17/120    avg_loss:0.423, val_acc:0.912]
Epoch [18/120    avg_loss:0.436, val_acc:0.925]
Epoch [19/120    avg_loss:0.449, val_acc:0.931]
Epoch [20/120    avg_loss:0.411, val_acc:0.929]
Epoch [21/120    avg_loss:0.309, val_acc:0.956]
Epoch [22/120    avg_loss:0.347, val_acc:0.948]
Epoch [23/120    avg_loss:0.360, val_acc:0.906]
Epoch [24/120    avg_loss:0.402, val_acc:0.938]
Epoch [25/120    avg_loss:0.364, val_acc:0.935]
Epoch [26/120    avg_loss:0.328, val_acc:0.940]
Epoch [27/120    avg_loss:0.300, val_acc:0.915]
Epoch [28/120    avg_loss:0.281, val_acc:0.921]
Epoch [29/120    avg_loss:0.242, val_acc:0.944]
Epoch [30/120    avg_loss:0.287, val_acc:0.942]
Epoch [31/120    avg_loss:0.255, val_acc:0.958]
Epoch [32/120    avg_loss:0.262, val_acc:0.927]
Epoch [33/120    avg_loss:0.305, val_acc:0.946]
Epoch [34/120    avg_loss:0.253, val_acc:0.960]
Epoch [35/120    avg_loss:0.236, val_acc:0.956]
Epoch [36/120    avg_loss:0.224, val_acc:0.960]
Epoch [37/120    avg_loss:0.162, val_acc:0.960]
Epoch [38/120    avg_loss:0.200, val_acc:0.929]
Epoch [39/120    avg_loss:0.172, val_acc:0.958]
Epoch [40/120    avg_loss:0.191, val_acc:0.948]
Epoch [41/120    avg_loss:0.180, val_acc:0.965]
Epoch [42/120    avg_loss:0.206, val_acc:0.946]
Epoch [43/120    avg_loss:0.229, val_acc:0.954]
Epoch [44/120    avg_loss:0.185, val_acc:0.935]
Epoch [45/120    avg_loss:0.188, val_acc:0.971]
Epoch [46/120    avg_loss:0.155, val_acc:0.952]
Epoch [47/120    avg_loss:0.146, val_acc:0.975]
Epoch [48/120    avg_loss:0.152, val_acc:0.948]
Epoch [49/120    avg_loss:0.212, val_acc:0.929]
Epoch [50/120    avg_loss:0.195, val_acc:0.952]
Epoch [51/120    avg_loss:0.151, val_acc:0.967]
Epoch [52/120    avg_loss:0.116, val_acc:0.965]
Epoch [53/120    avg_loss:0.115, val_acc:0.963]
Epoch [54/120    avg_loss:0.127, val_acc:0.960]
Epoch [55/120    avg_loss:0.182, val_acc:0.950]
Epoch [56/120    avg_loss:0.184, val_acc:0.956]
Epoch [57/120    avg_loss:0.143, val_acc:0.952]
Epoch [58/120    avg_loss:0.153, val_acc:0.975]
Epoch [59/120    avg_loss:0.098, val_acc:0.983]
Epoch [60/120    avg_loss:0.119, val_acc:0.971]
Epoch [61/120    avg_loss:0.107, val_acc:0.983]
Epoch [62/120    avg_loss:0.108, val_acc:0.967]
Epoch [63/120    avg_loss:0.114, val_acc:0.971]
Epoch [64/120    avg_loss:0.088, val_acc:0.973]
Epoch [65/120    avg_loss:0.095, val_acc:0.983]
Epoch [66/120    avg_loss:0.088, val_acc:0.975]
Epoch [67/120    avg_loss:0.073, val_acc:0.973]
Epoch [68/120    avg_loss:0.096, val_acc:0.954]
Epoch [69/120    avg_loss:0.087, val_acc:0.967]
Epoch [70/120    avg_loss:0.134, val_acc:0.969]
Epoch [71/120    avg_loss:0.079, val_acc:0.973]
Epoch [72/120    avg_loss:0.089, val_acc:0.975]
Epoch [73/120    avg_loss:0.091, val_acc:0.956]
Epoch [74/120    avg_loss:0.101, val_acc:0.969]
Epoch [75/120    avg_loss:0.120, val_acc:0.973]
Epoch [76/120    avg_loss:0.115, val_acc:0.965]
Epoch [77/120    avg_loss:0.104, val_acc:0.975]
Epoch [78/120    avg_loss:0.122, val_acc:0.965]
Epoch [79/120    avg_loss:0.084, val_acc:0.963]
Epoch [80/120    avg_loss:0.061, val_acc:0.973]
Epoch [81/120    avg_loss:0.055, val_acc:0.979]
Epoch [82/120    avg_loss:0.041, val_acc:0.983]
Epoch [83/120    avg_loss:0.046, val_acc:0.983]
Epoch [84/120    avg_loss:0.042, val_acc:0.983]
Epoch [85/120    avg_loss:0.038, val_acc:0.981]
Epoch [86/120    avg_loss:0.047, val_acc:0.985]
Epoch [87/120    avg_loss:0.047, val_acc:0.985]
Epoch [88/120    avg_loss:0.042, val_acc:0.985]
Epoch [89/120    avg_loss:0.036, val_acc:0.985]
Epoch [90/120    avg_loss:0.041, val_acc:0.985]
Epoch [91/120    avg_loss:0.039, val_acc:0.985]
Epoch [92/120    avg_loss:0.043, val_acc:0.985]
Epoch [93/120    avg_loss:0.038, val_acc:0.985]
Epoch [94/120    avg_loss:0.048, val_acc:0.985]
Epoch [95/120    avg_loss:0.040, val_acc:0.985]
Epoch [96/120    avg_loss:0.043, val_acc:0.990]
Epoch [97/120    avg_loss:0.044, val_acc:0.990]
Epoch [98/120    avg_loss:0.043, val_acc:0.985]
Epoch [99/120    avg_loss:0.041, val_acc:0.985]
Epoch [100/120    avg_loss:0.040, val_acc:0.985]
Epoch [101/120    avg_loss:0.043, val_acc:0.985]
Epoch [102/120    avg_loss:0.035, val_acc:0.985]
Epoch [103/120    avg_loss:0.034, val_acc:0.985]
Epoch [104/120    avg_loss:0.034, val_acc:0.985]
Epoch [105/120    avg_loss:0.046, val_acc:0.983]
Epoch [106/120    avg_loss:0.039, val_acc:0.981]
Epoch [107/120    avg_loss:0.037, val_acc:0.985]
Epoch [108/120    avg_loss:0.041, val_acc:0.985]
Epoch [109/120    avg_loss:0.040, val_acc:0.988]
Epoch [110/120    avg_loss:0.034, val_acc:0.988]
Epoch [111/120    avg_loss:0.031, val_acc:0.988]
Epoch [112/120    avg_loss:0.026, val_acc:0.988]
Epoch [113/120    avg_loss:0.033, val_acc:0.985]
Epoch [114/120    avg_loss:0.036, val_acc:0.985]
Epoch [115/120    avg_loss:0.036, val_acc:0.985]
Epoch [116/120    avg_loss:0.040, val_acc:0.985]
Epoch [117/120    avg_loss:0.030, val_acc:0.985]
Epoch [118/120    avg_loss:0.029, val_acc:0.985]
Epoch [119/120    avg_loss:0.042, val_acc:0.985]
Epoch [120/120    avg_loss:0.036, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   6 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.98871332 1.         0.94247788 0.9109589
 1.         0.97826087 1.         1.         1.         0.99210526
 0.99221357 1.        ]

Kappa:
0.9912170435413561
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6bcce7f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.538, val_acc:0.375]
Epoch [2/120    avg_loss:2.108, val_acc:0.546]
Epoch [3/120    avg_loss:1.851, val_acc:0.588]
Epoch [4/120    avg_loss:1.626, val_acc:0.675]
Epoch [5/120    avg_loss:1.430, val_acc:0.690]
Epoch [6/120    avg_loss:1.291, val_acc:0.715]
Epoch [7/120    avg_loss:1.165, val_acc:0.740]
Epoch [8/120    avg_loss:1.051, val_acc:0.740]
Epoch [9/120    avg_loss:0.922, val_acc:0.756]
Epoch [10/120    avg_loss:0.869, val_acc:0.785]
Epoch [11/120    avg_loss:0.805, val_acc:0.796]
Epoch [12/120    avg_loss:0.718, val_acc:0.840]
Epoch [13/120    avg_loss:0.666, val_acc:0.819]
Epoch [14/120    avg_loss:0.621, val_acc:0.873]
Epoch [15/120    avg_loss:0.637, val_acc:0.860]
Epoch [16/120    avg_loss:0.562, val_acc:0.910]
Epoch [17/120    avg_loss:0.541, val_acc:0.915]
Epoch [18/120    avg_loss:0.450, val_acc:0.917]
Epoch [19/120    avg_loss:0.446, val_acc:0.912]
Epoch [20/120    avg_loss:0.521, val_acc:0.887]
Epoch [21/120    avg_loss:0.493, val_acc:0.942]
Epoch [22/120    avg_loss:0.419, val_acc:0.904]
Epoch [23/120    avg_loss:0.379, val_acc:0.935]
Epoch [24/120    avg_loss:0.367, val_acc:0.915]
Epoch [25/120    avg_loss:0.375, val_acc:0.935]
Epoch [26/120    avg_loss:0.334, val_acc:0.919]
Epoch [27/120    avg_loss:0.316, val_acc:0.948]
Epoch [28/120    avg_loss:0.285, val_acc:0.971]
Epoch [29/120    avg_loss:0.289, val_acc:0.938]
Epoch [30/120    avg_loss:0.256, val_acc:0.958]
Epoch [31/120    avg_loss:0.237, val_acc:0.958]
Epoch [32/120    avg_loss:0.215, val_acc:0.975]
Epoch [33/120    avg_loss:0.252, val_acc:0.952]
Epoch [34/120    avg_loss:0.235, val_acc:0.942]
Epoch [35/120    avg_loss:0.311, val_acc:0.925]
Epoch [36/120    avg_loss:0.425, val_acc:0.929]
Epoch [37/120    avg_loss:0.354, val_acc:0.946]
Epoch [38/120    avg_loss:0.292, val_acc:0.925]
Epoch [39/120    avg_loss:0.286, val_acc:0.950]
Epoch [40/120    avg_loss:0.230, val_acc:0.954]
Epoch [41/120    avg_loss:0.257, val_acc:0.952]
Epoch [42/120    avg_loss:0.234, val_acc:0.963]
Epoch [43/120    avg_loss:0.228, val_acc:0.967]
Epoch [44/120    avg_loss:0.193, val_acc:0.975]
Epoch [45/120    avg_loss:0.166, val_acc:0.979]
Epoch [46/120    avg_loss:0.137, val_acc:0.975]
Epoch [47/120    avg_loss:0.116, val_acc:0.985]
Epoch [48/120    avg_loss:0.146, val_acc:0.967]
Epoch [49/120    avg_loss:0.125, val_acc:0.981]
Epoch [50/120    avg_loss:0.158, val_acc:0.983]
Epoch [51/120    avg_loss:0.104, val_acc:0.990]
Epoch [52/120    avg_loss:0.120, val_acc:0.985]
Epoch [53/120    avg_loss:0.102, val_acc:0.988]
Epoch [54/120    avg_loss:0.119, val_acc:0.985]
Epoch [55/120    avg_loss:0.100, val_acc:0.979]
Epoch [56/120    avg_loss:0.134, val_acc:0.977]
Epoch [57/120    avg_loss:0.108, val_acc:0.975]
Epoch [58/120    avg_loss:0.083, val_acc:0.988]
Epoch [59/120    avg_loss:0.082, val_acc:0.979]
Epoch [60/120    avg_loss:0.073, val_acc:0.992]
Epoch [61/120    avg_loss:0.071, val_acc:0.985]
Epoch [62/120    avg_loss:0.067, val_acc:0.971]
Epoch [63/120    avg_loss:0.089, val_acc:0.975]
Epoch [64/120    avg_loss:0.117, val_acc:0.965]
Epoch [65/120    avg_loss:0.112, val_acc:0.960]
Epoch [66/120    avg_loss:0.099, val_acc:0.992]
Epoch [67/120    avg_loss:0.079, val_acc:0.990]
Epoch [68/120    avg_loss:0.088, val_acc:0.975]
Epoch [69/120    avg_loss:0.100, val_acc:0.988]
Epoch [70/120    avg_loss:0.070, val_acc:0.988]
Epoch [71/120    avg_loss:0.072, val_acc:0.983]
Epoch [72/120    avg_loss:0.089, val_acc:0.973]
Epoch [73/120    avg_loss:0.075, val_acc:0.985]
Epoch [74/120    avg_loss:0.068, val_acc:0.988]
Epoch [75/120    avg_loss:0.060, val_acc:0.985]
Epoch [76/120    avg_loss:0.091, val_acc:0.992]
Epoch [77/120    avg_loss:0.069, val_acc:0.990]
Epoch [78/120    avg_loss:0.044, val_acc:0.990]
Epoch [79/120    avg_loss:0.055, val_acc:0.992]
Epoch [80/120    avg_loss:0.053, val_acc:0.988]
Epoch [81/120    avg_loss:0.057, val_acc:0.988]
Epoch [82/120    avg_loss:0.040, val_acc:0.990]
Epoch [83/120    avg_loss:0.027, val_acc:0.990]
Epoch [84/120    avg_loss:0.034, val_acc:0.994]
Epoch [85/120    avg_loss:0.025, val_acc:0.996]
Epoch [86/120    avg_loss:0.044, val_acc:0.996]
Epoch [87/120    avg_loss:0.060, val_acc:0.988]
Epoch [88/120    avg_loss:0.185, val_acc:0.977]
Epoch [89/120    avg_loss:0.087, val_acc:0.979]
Epoch [90/120    avg_loss:0.060, val_acc:0.990]
Epoch [91/120    avg_loss:0.069, val_acc:0.990]
Epoch [92/120    avg_loss:0.078, val_acc:0.996]
Epoch [93/120    avg_loss:0.079, val_acc:0.988]
Epoch [94/120    avg_loss:0.044, val_acc:0.994]
Epoch [95/120    avg_loss:0.048, val_acc:1.000]
Epoch [96/120    avg_loss:0.030, val_acc:1.000]
Epoch [97/120    avg_loss:0.024, val_acc:0.996]
Epoch [98/120    avg_loss:0.040, val_acc:0.992]
Epoch [99/120    avg_loss:0.024, val_acc:0.992]
Epoch [100/120    avg_loss:0.046, val_acc:0.990]
Epoch [101/120    avg_loss:0.053, val_acc:0.981]
Epoch [102/120    avg_loss:0.043, val_acc:0.992]
Epoch [103/120    avg_loss:0.020, val_acc:0.996]
Epoch [104/120    avg_loss:0.026, val_acc:0.994]
Epoch [105/120    avg_loss:0.032, val_acc:0.994]
Epoch [106/120    avg_loss:0.040, val_acc:0.994]
Epoch [107/120    avg_loss:0.039, val_acc:0.994]
Epoch [108/120    avg_loss:0.041, val_acc:0.992]
Epoch [109/120    avg_loss:0.043, val_acc:0.988]
Epoch [110/120    avg_loss:0.023, val_acc:0.992]
Epoch [111/120    avg_loss:0.016, val_acc:0.992]
Epoch [112/120    avg_loss:0.018, val_acc:0.996]
Epoch [113/120    avg_loss:0.017, val_acc:0.996]
Epoch [114/120    avg_loss:0.024, val_acc:0.996]
Epoch [115/120    avg_loss:0.017, val_acc:0.996]
Epoch [116/120    avg_loss:0.018, val_acc:0.996]
Epoch [117/120    avg_loss:0.020, val_acc:0.996]
Epoch [118/120    avg_loss:0.017, val_acc:0.996]
Epoch [119/120    avg_loss:0.018, val_acc:0.996]
Epoch [120/120    avg_loss:0.017, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  10   0   0   0   0   0   0   1   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   2   0   1 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         0.99319728 0.98901099 0.95364238 0.94915254
 1.         0.98378378 1.         0.9978678  1.         0.9986755
 0.99557522 1.        ]

Kappa:
0.9935906585480663
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f26ff4907f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.469, val_acc:0.371]
Epoch [2/120    avg_loss:2.100, val_acc:0.504]
Epoch [3/120    avg_loss:1.836, val_acc:0.527]
Epoch [4/120    avg_loss:1.614, val_acc:0.619]
Epoch [5/120    avg_loss:1.426, val_acc:0.713]
Epoch [6/120    avg_loss:1.264, val_acc:0.715]
Epoch [7/120    avg_loss:1.135, val_acc:0.771]
Epoch [8/120    avg_loss:1.002, val_acc:0.808]
Epoch [9/120    avg_loss:0.880, val_acc:0.808]
Epoch [10/120    avg_loss:0.797, val_acc:0.854]
Epoch [11/120    avg_loss:0.738, val_acc:0.860]
Epoch [12/120    avg_loss:0.666, val_acc:0.879]
Epoch [13/120    avg_loss:0.592, val_acc:0.900]
Epoch [14/120    avg_loss:0.568, val_acc:0.923]
Epoch [15/120    avg_loss:0.571, val_acc:0.906]
Epoch [16/120    avg_loss:0.480, val_acc:0.931]
Epoch [17/120    avg_loss:0.459, val_acc:0.919]
Epoch [18/120    avg_loss:0.417, val_acc:0.950]
Epoch [19/120    avg_loss:0.401, val_acc:0.917]
Epoch [20/120    avg_loss:0.370, val_acc:0.952]
Epoch [21/120    avg_loss:0.281, val_acc:0.946]
Epoch [22/120    avg_loss:0.321, val_acc:0.946]
Epoch [23/120    avg_loss:0.298, val_acc:0.952]
Epoch [24/120    avg_loss:0.286, val_acc:0.948]
Epoch [25/120    avg_loss:0.375, val_acc:0.963]
Epoch [26/120    avg_loss:0.371, val_acc:0.944]
Epoch [27/120    avg_loss:0.337, val_acc:0.956]
Epoch [28/120    avg_loss:0.260, val_acc:0.965]
Epoch [29/120    avg_loss:0.234, val_acc:0.952]
Epoch [30/120    avg_loss:0.247, val_acc:0.975]
Epoch [31/120    avg_loss:0.213, val_acc:0.971]
Epoch [32/120    avg_loss:0.273, val_acc:0.944]
Epoch [33/120    avg_loss:0.199, val_acc:0.975]
Epoch [34/120    avg_loss:0.200, val_acc:0.958]
Epoch [35/120    avg_loss:0.217, val_acc:0.965]
Epoch [36/120    avg_loss:0.213, val_acc:0.952]
Epoch [37/120    avg_loss:0.195, val_acc:0.973]
Epoch [38/120    avg_loss:0.177, val_acc:0.975]
Epoch [39/120    avg_loss:0.155, val_acc:0.975]
Epoch [40/120    avg_loss:0.138, val_acc:0.975]
Epoch [41/120    avg_loss:0.149, val_acc:0.960]
Epoch [42/120    avg_loss:0.164, val_acc:0.963]
Epoch [43/120    avg_loss:0.184, val_acc:0.958]
Epoch [44/120    avg_loss:0.208, val_acc:0.973]
Epoch [45/120    avg_loss:0.167, val_acc:0.969]
Epoch [46/120    avg_loss:0.194, val_acc:0.979]
Epoch [47/120    avg_loss:0.106, val_acc:0.971]
Epoch [48/120    avg_loss:0.114, val_acc:0.952]
Epoch [49/120    avg_loss:0.195, val_acc:0.981]
Epoch [50/120    avg_loss:0.170, val_acc:0.956]
Epoch [51/120    avg_loss:0.176, val_acc:0.973]
Epoch [52/120    avg_loss:0.128, val_acc:0.973]
Epoch [53/120    avg_loss:0.122, val_acc:0.963]
Epoch [54/120    avg_loss:0.144, val_acc:0.971]
Epoch [55/120    avg_loss:0.103, val_acc:0.990]
Epoch [56/120    avg_loss:0.103, val_acc:0.975]
Epoch [57/120    avg_loss:0.075, val_acc:0.990]
Epoch [58/120    avg_loss:0.075, val_acc:0.985]
Epoch [59/120    avg_loss:0.079, val_acc:0.990]
Epoch [60/120    avg_loss:0.072, val_acc:0.979]
Epoch [61/120    avg_loss:0.069, val_acc:0.979]
Epoch [62/120    avg_loss:0.103, val_acc:0.988]
Epoch [63/120    avg_loss:0.092, val_acc:0.992]
Epoch [64/120    avg_loss:0.087, val_acc:0.988]
Epoch [65/120    avg_loss:0.077, val_acc:0.981]
Epoch [66/120    avg_loss:0.084, val_acc:0.990]
Epoch [67/120    avg_loss:0.122, val_acc:0.887]
Epoch [68/120    avg_loss:0.100, val_acc:0.977]
Epoch [69/120    avg_loss:0.091, val_acc:0.973]
Epoch [70/120    avg_loss:0.060, val_acc:0.981]
Epoch [71/120    avg_loss:0.059, val_acc:0.985]
Epoch [72/120    avg_loss:0.053, val_acc:0.988]
Epoch [73/120    avg_loss:0.049, val_acc:0.992]
Epoch [74/120    avg_loss:0.035, val_acc:0.988]
Epoch [75/120    avg_loss:0.041, val_acc:0.985]
Epoch [76/120    avg_loss:0.044, val_acc:0.990]
Epoch [77/120    avg_loss:0.036, val_acc:0.990]
Epoch [78/120    avg_loss:0.035, val_acc:0.990]
Epoch [79/120    avg_loss:0.038, val_acc:0.996]
Epoch [80/120    avg_loss:0.033, val_acc:0.996]
Epoch [81/120    avg_loss:0.035, val_acc:0.994]
Epoch [82/120    avg_loss:0.031, val_acc:0.988]
Epoch [83/120    avg_loss:0.036, val_acc:0.990]
Epoch [84/120    avg_loss:0.020, val_acc:0.990]
Epoch [85/120    avg_loss:0.043, val_acc:0.956]
Epoch [86/120    avg_loss:0.085, val_acc:0.983]
Epoch [87/120    avg_loss:0.097, val_acc:0.979]
Epoch [88/120    avg_loss:0.071, val_acc:0.985]
Epoch [89/120    avg_loss:0.061, val_acc:0.988]
Epoch [90/120    avg_loss:0.054, val_acc:0.985]
Epoch [91/120    avg_loss:0.046, val_acc:0.988]
Epoch [92/120    avg_loss:0.093, val_acc:0.965]
Epoch [93/120    avg_loss:0.147, val_acc:0.979]
Epoch [94/120    avg_loss:0.087, val_acc:0.985]
Epoch [95/120    avg_loss:0.044, val_acc:0.990]
Epoch [96/120    avg_loss:0.048, val_acc:0.990]
Epoch [97/120    avg_loss:0.037, val_acc:0.992]
Epoch [98/120    avg_loss:0.044, val_acc:0.992]
Epoch [99/120    avg_loss:0.036, val_acc:0.994]
Epoch [100/120    avg_loss:0.025, val_acc:0.994]
Epoch [101/120    avg_loss:0.030, val_acc:0.994]
Epoch [102/120    avg_loss:0.036, val_acc:0.994]
Epoch [103/120    avg_loss:0.029, val_acc:0.994]
Epoch [104/120    avg_loss:0.025, val_acc:0.994]
Epoch [105/120    avg_loss:0.024, val_acc:0.994]
Epoch [106/120    avg_loss:0.025, val_acc:0.996]
Epoch [107/120    avg_loss:0.027, val_acc:0.996]
Epoch [108/120    avg_loss:0.023, val_acc:0.996]
Epoch [109/120    avg_loss:0.022, val_acc:0.996]
Epoch [110/120    avg_loss:0.028, val_acc:0.996]
Epoch [111/120    avg_loss:0.033, val_acc:0.996]
Epoch [112/120    avg_loss:0.020, val_acc:0.994]
Epoch [113/120    avg_loss:0.022, val_acc:0.994]
Epoch [114/120    avg_loss:0.026, val_acc:0.996]
Epoch [115/120    avg_loss:0.027, val_acc:0.996]
Epoch [116/120    avg_loss:0.030, val_acc:0.996]
Epoch [117/120    avg_loss:0.018, val_acc:0.994]
Epoch [118/120    avg_loss:0.023, val_acc:0.994]
Epoch [119/120    avg_loss:0.027, val_acc:0.994]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.99319728 0.99122807 0.9532294  0.94314381
 1.         0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9943029532172548
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14a41b9860>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.599, val_acc:0.338]
Epoch [2/120    avg_loss:2.205, val_acc:0.485]
Epoch [3/120    avg_loss:2.000, val_acc:0.535]
Epoch [4/120    avg_loss:1.798, val_acc:0.631]
Epoch [5/120    avg_loss:1.577, val_acc:0.637]
Epoch [6/120    avg_loss:1.378, val_acc:0.694]
Epoch [7/120    avg_loss:1.210, val_acc:0.710]
Epoch [8/120    avg_loss:1.075, val_acc:0.744]
Epoch [9/120    avg_loss:0.954, val_acc:0.769]
Epoch [10/120    avg_loss:0.874, val_acc:0.877]
Epoch [11/120    avg_loss:0.766, val_acc:0.856]
Epoch [12/120    avg_loss:0.729, val_acc:0.887]
Epoch [13/120    avg_loss:0.656, val_acc:0.900]
Epoch [14/120    avg_loss:0.595, val_acc:0.898]
Epoch [15/120    avg_loss:0.578, val_acc:0.892]
Epoch [16/120    avg_loss:0.541, val_acc:0.917]
Epoch [17/120    avg_loss:0.517, val_acc:0.890]
Epoch [18/120    avg_loss:0.500, val_acc:0.915]
Epoch [19/120    avg_loss:0.427, val_acc:0.915]
Epoch [20/120    avg_loss:0.418, val_acc:0.917]
Epoch [21/120    avg_loss:0.395, val_acc:0.940]
Epoch [22/120    avg_loss:0.369, val_acc:0.923]
Epoch [23/120    avg_loss:0.377, val_acc:0.940]
Epoch [24/120    avg_loss:0.334, val_acc:0.942]
Epoch [25/120    avg_loss:0.305, val_acc:0.940]
Epoch [26/120    avg_loss:0.302, val_acc:0.960]
Epoch [27/120    avg_loss:0.262, val_acc:0.950]
Epoch [28/120    avg_loss:0.323, val_acc:0.944]
Epoch [29/120    avg_loss:0.258, val_acc:0.954]
Epoch [30/120    avg_loss:0.289, val_acc:0.973]
Epoch [31/120    avg_loss:0.237, val_acc:0.960]
Epoch [32/120    avg_loss:0.250, val_acc:0.967]
Epoch [33/120    avg_loss:0.247, val_acc:0.965]
Epoch [34/120    avg_loss:0.224, val_acc:0.977]
Epoch [35/120    avg_loss:0.182, val_acc:0.975]
Epoch [36/120    avg_loss:0.175, val_acc:0.977]
Epoch [37/120    avg_loss:0.206, val_acc:0.967]
Epoch [38/120    avg_loss:0.171, val_acc:0.954]
Epoch [39/120    avg_loss:0.187, val_acc:0.942]
Epoch [40/120    avg_loss:0.182, val_acc:0.954]
Epoch [41/120    avg_loss:0.180, val_acc:0.973]
Epoch [42/120    avg_loss:0.188, val_acc:0.935]
Epoch [43/120    avg_loss:0.198, val_acc:0.965]
Epoch [44/120    avg_loss:0.167, val_acc:0.975]
Epoch [45/120    avg_loss:0.145, val_acc:0.988]
Epoch [46/120    avg_loss:0.120, val_acc:0.981]
Epoch [47/120    avg_loss:0.115, val_acc:0.979]
Epoch [48/120    avg_loss:0.117, val_acc:0.965]
Epoch [49/120    avg_loss:0.150, val_acc:0.967]
Epoch [50/120    avg_loss:0.143, val_acc:0.988]
Epoch [51/120    avg_loss:0.100, val_acc:0.988]
Epoch [52/120    avg_loss:0.107, val_acc:0.983]
Epoch [53/120    avg_loss:0.128, val_acc:0.971]
Epoch [54/120    avg_loss:0.127, val_acc:0.975]
Epoch [55/120    avg_loss:0.117, val_acc:0.983]
Epoch [56/120    avg_loss:0.083, val_acc:0.977]
Epoch [57/120    avg_loss:0.095, val_acc:0.985]
Epoch [58/120    avg_loss:0.109, val_acc:0.981]
Epoch [59/120    avg_loss:0.127, val_acc:0.963]
Epoch [60/120    avg_loss:0.121, val_acc:0.981]
Epoch [61/120    avg_loss:0.096, val_acc:0.979]
Epoch [62/120    avg_loss:0.092, val_acc:0.983]
Epoch [63/120    avg_loss:0.100, val_acc:0.985]
Epoch [64/120    avg_loss:0.114, val_acc:0.990]
Epoch [65/120    avg_loss:0.091, val_acc:0.979]
Epoch [66/120    avg_loss:0.084, val_acc:0.979]
Epoch [67/120    avg_loss:0.066, val_acc:0.990]
Epoch [68/120    avg_loss:0.066, val_acc:0.992]
Epoch [69/120    avg_loss:0.076, val_acc:0.971]
Epoch [70/120    avg_loss:0.079, val_acc:0.979]
Epoch [71/120    avg_loss:0.061, val_acc:0.988]
Epoch [72/120    avg_loss:0.082, val_acc:0.994]
Epoch [73/120    avg_loss:0.043, val_acc:0.985]
Epoch [74/120    avg_loss:0.039, val_acc:0.988]
Epoch [75/120    avg_loss:0.054, val_acc:0.994]
Epoch [76/120    avg_loss:0.057, val_acc:0.992]
Epoch [77/120    avg_loss:0.036, val_acc:0.992]
Epoch [78/120    avg_loss:0.039, val_acc:0.990]
Epoch [79/120    avg_loss:0.049, val_acc:0.994]
Epoch [80/120    avg_loss:0.049, val_acc:0.994]
Epoch [81/120    avg_loss:0.059, val_acc:0.992]
Epoch [82/120    avg_loss:0.061, val_acc:0.985]
Epoch [83/120    avg_loss:0.049, val_acc:0.988]
Epoch [84/120    avg_loss:0.043, val_acc:0.994]
Epoch [85/120    avg_loss:0.022, val_acc:0.992]
Epoch [86/120    avg_loss:0.041, val_acc:0.975]
Epoch [87/120    avg_loss:0.046, val_acc:0.990]
Epoch [88/120    avg_loss:0.033, val_acc:0.981]
Epoch [89/120    avg_loss:0.041, val_acc:0.985]
Epoch [90/120    avg_loss:0.037, val_acc:0.990]
Epoch [91/120    avg_loss:0.031, val_acc:0.990]
Epoch [92/120    avg_loss:0.045, val_acc:0.990]
Epoch [93/120    avg_loss:0.050, val_acc:0.990]
Epoch [94/120    avg_loss:0.049, val_acc:0.983]
Epoch [95/120    avg_loss:0.038, val_acc:0.988]
Epoch [96/120    avg_loss:0.041, val_acc:0.981]
Epoch [97/120    avg_loss:0.043, val_acc:0.990]
Epoch [98/120    avg_loss:0.028, val_acc:0.988]
Epoch [99/120    avg_loss:0.028, val_acc:0.988]
Epoch [100/120    avg_loss:0.016, val_acc:0.988]
Epoch [101/120    avg_loss:0.020, val_acc:0.988]
Epoch [102/120    avg_loss:0.020, val_acc:0.988]
Epoch [103/120    avg_loss:0.019, val_acc:0.988]
Epoch [104/120    avg_loss:0.017, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.016, val_acc:0.990]
Epoch [107/120    avg_loss:0.013, val_acc:0.992]
Epoch [108/120    avg_loss:0.020, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.016, val_acc:0.992]
Epoch [111/120    avg_loss:0.017, val_acc:0.992]
Epoch [112/120    avg_loss:0.021, val_acc:0.992]
Epoch [113/120    avg_loss:0.017, val_acc:0.992]
Epoch [114/120    avg_loss:0.018, val_acc:0.992]
Epoch [115/120    avg_loss:0.017, val_acc:0.992]
Epoch [116/120    avg_loss:0.015, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.011, val_acc:0.992]
Epoch [119/120    avg_loss:0.024, val_acc:0.992]
Epoch [120/120    avg_loss:0.016, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.97550111 1.         0.95768374 0.93559322
 1.         0.94382022 1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9928783379184621
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59c8d5f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.547, val_acc:0.379]
Epoch [2/120    avg_loss:2.121, val_acc:0.581]
Epoch [3/120    avg_loss:1.841, val_acc:0.658]
Epoch [4/120    avg_loss:1.617, val_acc:0.671]
Epoch [5/120    avg_loss:1.391, val_acc:0.671]
Epoch [6/120    avg_loss:1.226, val_acc:0.671]
Epoch [7/120    avg_loss:1.105, val_acc:0.754]
Epoch [8/120    avg_loss:0.947, val_acc:0.821]
Epoch [9/120    avg_loss:0.831, val_acc:0.831]
Epoch [10/120    avg_loss:0.753, val_acc:0.873]
Epoch [11/120    avg_loss:0.675, val_acc:0.860]
Epoch [12/120    avg_loss:0.581, val_acc:0.890]
Epoch [13/120    avg_loss:0.547, val_acc:0.904]
Epoch [14/120    avg_loss:0.535, val_acc:0.887]
Epoch [15/120    avg_loss:0.544, val_acc:0.900]
Epoch [16/120    avg_loss:0.535, val_acc:0.912]
Epoch [17/120    avg_loss:0.502, val_acc:0.892]
Epoch [18/120    avg_loss:0.382, val_acc:0.933]
Epoch [19/120    avg_loss:0.342, val_acc:0.921]
Epoch [20/120    avg_loss:0.409, val_acc:0.931]
Epoch [21/120    avg_loss:0.337, val_acc:0.938]
Epoch [22/120    avg_loss:0.368, val_acc:0.935]
Epoch [23/120    avg_loss:0.348, val_acc:0.927]
Epoch [24/120    avg_loss:0.358, val_acc:0.946]
Epoch [25/120    avg_loss:0.380, val_acc:0.940]
Epoch [26/120    avg_loss:0.326, val_acc:0.963]
Epoch [27/120    avg_loss:0.302, val_acc:0.950]
Epoch [28/120    avg_loss:0.263, val_acc:0.960]
Epoch [29/120    avg_loss:0.285, val_acc:0.938]
Epoch [30/120    avg_loss:0.220, val_acc:0.950]
Epoch [31/120    avg_loss:0.290, val_acc:0.921]
Epoch [32/120    avg_loss:0.252, val_acc:0.952]
Epoch [33/120    avg_loss:0.260, val_acc:0.973]
Epoch [34/120    avg_loss:0.308, val_acc:0.917]
Epoch [35/120    avg_loss:0.277, val_acc:0.963]
Epoch [36/120    avg_loss:0.234, val_acc:0.958]
Epoch [37/120    avg_loss:0.223, val_acc:0.952]
Epoch [38/120    avg_loss:0.194, val_acc:0.967]
Epoch [39/120    avg_loss:0.172, val_acc:0.977]
Epoch [40/120    avg_loss:0.147, val_acc:0.973]
Epoch [41/120    avg_loss:0.177, val_acc:0.981]
Epoch [42/120    avg_loss:0.180, val_acc:0.958]
Epoch [43/120    avg_loss:0.178, val_acc:0.977]
Epoch [44/120    avg_loss:0.133, val_acc:0.983]
Epoch [45/120    avg_loss:0.152, val_acc:0.965]
Epoch [46/120    avg_loss:0.213, val_acc:0.965]
Epoch [47/120    avg_loss:0.141, val_acc:0.973]
Epoch [48/120    avg_loss:0.111, val_acc:0.973]
Epoch [49/120    avg_loss:0.131, val_acc:0.969]
Epoch [50/120    avg_loss:0.115, val_acc:0.979]
Epoch [51/120    avg_loss:0.125, val_acc:0.960]
Epoch [52/120    avg_loss:0.174, val_acc:0.992]
Epoch [53/120    avg_loss:0.095, val_acc:0.985]
Epoch [54/120    avg_loss:0.091, val_acc:0.988]
Epoch [55/120    avg_loss:0.094, val_acc:0.988]
Epoch [56/120    avg_loss:0.098, val_acc:0.979]
Epoch [57/120    avg_loss:0.098, val_acc:0.981]
Epoch [58/120    avg_loss:0.089, val_acc:0.992]
Epoch [59/120    avg_loss:0.096, val_acc:0.985]
Epoch [60/120    avg_loss:0.103, val_acc:0.977]
Epoch [61/120    avg_loss:0.123, val_acc:0.948]
Epoch [62/120    avg_loss:0.117, val_acc:0.990]
Epoch [63/120    avg_loss:0.110, val_acc:0.975]
Epoch [64/120    avg_loss:0.079, val_acc:0.994]
Epoch [65/120    avg_loss:0.068, val_acc:0.994]
Epoch [66/120    avg_loss:0.056, val_acc:0.988]
Epoch [67/120    avg_loss:0.065, val_acc:0.990]
Epoch [68/120    avg_loss:0.056, val_acc:0.985]
Epoch [69/120    avg_loss:0.065, val_acc:0.990]
Epoch [70/120    avg_loss:0.053, val_acc:0.992]
Epoch [71/120    avg_loss:0.074, val_acc:0.994]
Epoch [72/120    avg_loss:0.071, val_acc:0.994]
Epoch [73/120    avg_loss:0.080, val_acc:0.983]
Epoch [74/120    avg_loss:0.068, val_acc:0.994]
Epoch [75/120    avg_loss:0.071, val_acc:0.988]
Epoch [76/120    avg_loss:0.078, val_acc:0.983]
Epoch [77/120    avg_loss:0.079, val_acc:0.992]
Epoch [78/120    avg_loss:0.064, val_acc:0.990]
Epoch [79/120    avg_loss:0.051, val_acc:0.990]
Epoch [80/120    avg_loss:0.035, val_acc:0.992]
Epoch [81/120    avg_loss:0.036, val_acc:0.994]
Epoch [82/120    avg_loss:0.051, val_acc:0.994]
Epoch [83/120    avg_loss:0.044, val_acc:0.996]
Epoch [84/120    avg_loss:0.026, val_acc:0.994]
Epoch [85/120    avg_loss:0.025, val_acc:0.994]
Epoch [86/120    avg_loss:0.028, val_acc:0.996]
Epoch [87/120    avg_loss:0.020, val_acc:0.992]
Epoch [88/120    avg_loss:0.016, val_acc:0.996]
Epoch [89/120    avg_loss:0.019, val_acc:0.994]
Epoch [90/120    avg_loss:0.020, val_acc:0.998]
Epoch [91/120    avg_loss:0.014, val_acc:0.998]
Epoch [92/120    avg_loss:0.012, val_acc:0.996]
Epoch [93/120    avg_loss:0.014, val_acc:0.996]
Epoch [94/120    avg_loss:0.025, val_acc:0.994]
Epoch [95/120    avg_loss:0.017, val_acc:0.996]
Epoch [96/120    avg_loss:0.018, val_acc:0.996]
Epoch [97/120    avg_loss:0.021, val_acc:0.994]
Epoch [98/120    avg_loss:0.023, val_acc:0.994]
Epoch [99/120    avg_loss:0.020, val_acc:0.992]
Epoch [100/120    avg_loss:0.046, val_acc:0.933]
Epoch [101/120    avg_loss:0.037, val_acc:0.988]
Epoch [102/120    avg_loss:0.023, val_acc:0.996]
Epoch [103/120    avg_loss:0.016, val_acc:0.994]
Epoch [104/120    avg_loss:0.013, val_acc:0.994]
Epoch [105/120    avg_loss:0.014, val_acc:0.994]
Epoch [106/120    avg_loss:0.013, val_acc:0.994]
Epoch [107/120    avg_loss:0.015, val_acc:0.994]
Epoch [108/120    avg_loss:0.011, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.996]
Epoch [110/120    avg_loss:0.011, val_acc:0.996]
Epoch [111/120    avg_loss:0.015, val_acc:0.996]
Epoch [112/120    avg_loss:0.022, val_acc:0.994]
Epoch [113/120    avg_loss:0.018, val_acc:0.996]
Epoch [114/120    avg_loss:0.012, val_acc:0.996]
Epoch [115/120    avg_loss:0.017, val_acc:0.996]
Epoch [116/120    avg_loss:0.013, val_acc:0.996]
Epoch [117/120    avg_loss:0.008, val_acc:0.994]
Epoch [118/120    avg_loss:0.012, val_acc:0.994]
Epoch [119/120    avg_loss:0.008, val_acc:0.994]
Epoch [120/120    avg_loss:0.012, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.99543379 0.98004435 0.93449782 0.93243243
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.992641318669216
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb284e2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.362]
Epoch [2/120    avg_loss:2.162, val_acc:0.533]
Epoch [3/120    avg_loss:1.926, val_acc:0.598]
Epoch [4/120    avg_loss:1.662, val_acc:0.658]
Epoch [5/120    avg_loss:1.439, val_acc:0.675]
Epoch [6/120    avg_loss:1.257, val_acc:0.683]
Epoch [7/120    avg_loss:1.090, val_acc:0.758]
Epoch [8/120    avg_loss:0.972, val_acc:0.725]
Epoch [9/120    avg_loss:0.889, val_acc:0.750]
Epoch [10/120    avg_loss:0.784, val_acc:0.769]
Epoch [11/120    avg_loss:0.770, val_acc:0.825]
Epoch [12/120    avg_loss:0.745, val_acc:0.812]
Epoch [13/120    avg_loss:0.707, val_acc:0.802]
Epoch [14/120    avg_loss:0.595, val_acc:0.881]
Epoch [15/120    avg_loss:0.602, val_acc:0.890]
Epoch [16/120    avg_loss:0.508, val_acc:0.906]
Epoch [17/120    avg_loss:0.432, val_acc:0.908]
Epoch [18/120    avg_loss:0.408, val_acc:0.917]
Epoch [19/120    avg_loss:0.409, val_acc:0.885]
Epoch [20/120    avg_loss:0.429, val_acc:0.923]
Epoch [21/120    avg_loss:0.432, val_acc:0.917]
Epoch [22/120    avg_loss:0.483, val_acc:0.925]
Epoch [23/120    avg_loss:0.368, val_acc:0.933]
Epoch [24/120    avg_loss:0.353, val_acc:0.929]
Epoch [25/120    avg_loss:0.330, val_acc:0.873]
Epoch [26/120    avg_loss:0.376, val_acc:0.925]
Epoch [27/120    avg_loss:0.319, val_acc:0.944]
Epoch [28/120    avg_loss:0.301, val_acc:0.946]
Epoch [29/120    avg_loss:0.251, val_acc:0.946]
Epoch [30/120    avg_loss:0.252, val_acc:0.954]
Epoch [31/120    avg_loss:0.275, val_acc:0.940]
Epoch [32/120    avg_loss:0.202, val_acc:0.956]
Epoch [33/120    avg_loss:0.179, val_acc:0.952]
Epoch [34/120    avg_loss:0.215, val_acc:0.944]
Epoch [35/120    avg_loss:0.268, val_acc:0.921]
Epoch [36/120    avg_loss:0.311, val_acc:0.954]
Epoch [37/120    avg_loss:0.203, val_acc:0.954]
Epoch [38/120    avg_loss:0.200, val_acc:0.952]
Epoch [39/120    avg_loss:0.169, val_acc:0.952]
Epoch [40/120    avg_loss:0.178, val_acc:0.942]
Epoch [41/120    avg_loss:0.199, val_acc:0.938]
Epoch [42/120    avg_loss:0.194, val_acc:0.940]
Epoch [43/120    avg_loss:0.193, val_acc:0.954]
Epoch [44/120    avg_loss:0.175, val_acc:0.956]
Epoch [45/120    avg_loss:0.154, val_acc:0.952]
Epoch [46/120    avg_loss:0.141, val_acc:0.954]
Epoch [47/120    avg_loss:0.156, val_acc:0.967]
Epoch [48/120    avg_loss:0.113, val_acc:0.958]
Epoch [49/120    avg_loss:0.131, val_acc:0.935]
Epoch [50/120    avg_loss:0.123, val_acc:0.950]
Epoch [51/120    avg_loss:0.135, val_acc:0.965]
Epoch [52/120    avg_loss:0.139, val_acc:0.960]
Epoch [53/120    avg_loss:0.114, val_acc:0.956]
Epoch [54/120    avg_loss:0.114, val_acc:0.956]
Epoch [55/120    avg_loss:0.140, val_acc:0.967]
Epoch [56/120    avg_loss:0.122, val_acc:0.969]
Epoch [57/120    avg_loss:0.122, val_acc:0.954]
Epoch [58/120    avg_loss:0.109, val_acc:0.971]
Epoch [59/120    avg_loss:0.112, val_acc:0.967]
Epoch [60/120    avg_loss:0.109, val_acc:0.975]
Epoch [61/120    avg_loss:0.074, val_acc:0.971]
Epoch [62/120    avg_loss:0.076, val_acc:0.977]
Epoch [63/120    avg_loss:0.114, val_acc:0.977]
Epoch [64/120    avg_loss:0.071, val_acc:0.965]
Epoch [65/120    avg_loss:0.101, val_acc:0.973]
Epoch [66/120    avg_loss:0.070, val_acc:0.960]
Epoch [67/120    avg_loss:0.059, val_acc:0.979]
Epoch [68/120    avg_loss:0.071, val_acc:0.975]
Epoch [69/120    avg_loss:0.084, val_acc:0.981]
Epoch [70/120    avg_loss:0.069, val_acc:0.973]
Epoch [71/120    avg_loss:0.050, val_acc:0.981]
Epoch [72/120    avg_loss:0.058, val_acc:0.975]
Epoch [73/120    avg_loss:0.060, val_acc:0.981]
Epoch [74/120    avg_loss:0.049, val_acc:0.981]
Epoch [75/120    avg_loss:0.107, val_acc:0.985]
Epoch [76/120    avg_loss:0.079, val_acc:0.963]
Epoch [77/120    avg_loss:0.086, val_acc:0.977]
Epoch [78/120    avg_loss:0.074, val_acc:0.983]
Epoch [79/120    avg_loss:0.054, val_acc:0.983]
Epoch [80/120    avg_loss:0.045, val_acc:0.979]
Epoch [81/120    avg_loss:0.042, val_acc:0.985]
Epoch [82/120    avg_loss:0.043, val_acc:0.990]
Epoch [83/120    avg_loss:0.036, val_acc:0.983]
Epoch [84/120    avg_loss:0.029, val_acc:0.981]
Epoch [85/120    avg_loss:0.054, val_acc:0.988]
Epoch [86/120    avg_loss:0.036, val_acc:0.981]
Epoch [87/120    avg_loss:0.050, val_acc:0.985]
Epoch [88/120    avg_loss:0.050, val_acc:0.979]
Epoch [89/120    avg_loss:0.042, val_acc:0.983]
Epoch [90/120    avg_loss:0.035, val_acc:0.983]
Epoch [91/120    avg_loss:0.031, val_acc:0.985]
Epoch [92/120    avg_loss:0.056, val_acc:0.988]
Epoch [93/120    avg_loss:0.050, val_acc:0.969]
Epoch [94/120    avg_loss:0.088, val_acc:0.988]
Epoch [95/120    avg_loss:0.062, val_acc:0.981]
Epoch [96/120    avg_loss:0.040, val_acc:0.981]
Epoch [97/120    avg_loss:0.035, val_acc:0.985]
Epoch [98/120    avg_loss:0.030, val_acc:0.985]
Epoch [99/120    avg_loss:0.023, val_acc:0.985]
Epoch [100/120    avg_loss:0.026, val_acc:0.985]
Epoch [101/120    avg_loss:0.027, val_acc:0.985]
Epoch [102/120    avg_loss:0.026, val_acc:0.985]
Epoch [103/120    avg_loss:0.028, val_acc:0.988]
Epoch [104/120    avg_loss:0.025, val_acc:0.985]
Epoch [105/120    avg_loss:0.022, val_acc:0.985]
Epoch [106/120    avg_loss:0.023, val_acc:0.985]
Epoch [107/120    avg_loss:0.025, val_acc:0.985]
Epoch [108/120    avg_loss:0.026, val_acc:0.985]
Epoch [109/120    avg_loss:0.021, val_acc:0.985]
Epoch [110/120    avg_loss:0.018, val_acc:0.985]
Epoch [111/120    avg_loss:0.024, val_acc:0.985]
Epoch [112/120    avg_loss:0.017, val_acc:0.985]
Epoch [113/120    avg_loss:0.022, val_acc:0.985]
Epoch [114/120    avg_loss:0.017, val_acc:0.985]
Epoch [115/120    avg_loss:0.031, val_acc:0.985]
Epoch [116/120    avg_loss:0.019, val_acc:0.985]
Epoch [117/120    avg_loss:0.025, val_acc:0.985]
Epoch [118/120    avg_loss:0.023, val_acc:0.985]
Epoch [119/120    avg_loss:0.020, val_acc:0.985]
Epoch [120/120    avg_loss:0.027, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.98426966 0.99563319 0.96363636 0.95081967
 1.         0.96132597 1.         0.99893276 1.         0.9843342
 0.98657718 1.        ]

Kappa:
0.9914547139824342
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efcae9e07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.465, val_acc:0.415]
Epoch [2/120    avg_loss:2.064, val_acc:0.521]
Epoch [3/120    avg_loss:1.833, val_acc:0.575]
Epoch [4/120    avg_loss:1.618, val_acc:0.633]
Epoch [5/120    avg_loss:1.390, val_acc:0.642]
Epoch [6/120    avg_loss:1.198, val_acc:0.723]
Epoch [7/120    avg_loss:1.076, val_acc:0.710]
Epoch [8/120    avg_loss:0.956, val_acc:0.831]
Epoch [9/120    avg_loss:0.819, val_acc:0.798]
Epoch [10/120    avg_loss:0.755, val_acc:0.800]
Epoch [11/120    avg_loss:0.771, val_acc:0.833]
Epoch [12/120    avg_loss:0.777, val_acc:0.860]
Epoch [13/120    avg_loss:0.628, val_acc:0.900]
Epoch [14/120    avg_loss:0.613, val_acc:0.810]
Epoch [15/120    avg_loss:0.621, val_acc:0.881]
Epoch [16/120    avg_loss:0.557, val_acc:0.817]
Epoch [17/120    avg_loss:0.561, val_acc:0.904]
Epoch [18/120    avg_loss:0.496, val_acc:0.887]
Epoch [19/120    avg_loss:0.446, val_acc:0.931]
Epoch [20/120    avg_loss:0.374, val_acc:0.912]
Epoch [21/120    avg_loss:0.415, val_acc:0.871]
Epoch [22/120    avg_loss:0.415, val_acc:0.921]
Epoch [23/120    avg_loss:0.404, val_acc:0.890]
Epoch [24/120    avg_loss:0.405, val_acc:0.908]
Epoch [25/120    avg_loss:0.397, val_acc:0.938]
Epoch [26/120    avg_loss:0.333, val_acc:0.917]
Epoch [27/120    avg_loss:0.318, val_acc:0.927]
Epoch [28/120    avg_loss:0.277, val_acc:0.927]
Epoch [29/120    avg_loss:0.327, val_acc:0.927]
Epoch [30/120    avg_loss:0.259, val_acc:0.933]
Epoch [31/120    avg_loss:0.291, val_acc:0.929]
Epoch [32/120    avg_loss:0.245, val_acc:0.938]
Epoch [33/120    avg_loss:0.230, val_acc:0.946]
Epoch [34/120    avg_loss:0.206, val_acc:0.944]
Epoch [35/120    avg_loss:0.206, val_acc:0.965]
Epoch [36/120    avg_loss:0.181, val_acc:0.944]
Epoch [37/120    avg_loss:0.244, val_acc:0.942]
Epoch [38/120    avg_loss:0.224, val_acc:0.940]
Epoch [39/120    avg_loss:0.300, val_acc:0.931]
Epoch [40/120    avg_loss:0.271, val_acc:0.942]
Epoch [41/120    avg_loss:0.245, val_acc:0.950]
Epoch [42/120    avg_loss:0.185, val_acc:0.958]
Epoch [43/120    avg_loss:0.174, val_acc:0.954]
Epoch [44/120    avg_loss:0.278, val_acc:0.946]
Epoch [45/120    avg_loss:0.192, val_acc:0.960]
Epoch [46/120    avg_loss:0.143, val_acc:0.963]
Epoch [47/120    avg_loss:0.140, val_acc:0.960]
Epoch [48/120    avg_loss:0.153, val_acc:0.967]
Epoch [49/120    avg_loss:0.146, val_acc:0.967]
Epoch [50/120    avg_loss:0.131, val_acc:0.967]
Epoch [51/120    avg_loss:0.099, val_acc:0.965]
Epoch [52/120    avg_loss:0.135, val_acc:0.975]
Epoch [53/120    avg_loss:0.116, val_acc:0.977]
Epoch [54/120    avg_loss:0.122, val_acc:0.967]
Epoch [55/120    avg_loss:0.124, val_acc:0.971]
Epoch [56/120    avg_loss:0.149, val_acc:0.958]
Epoch [57/120    avg_loss:0.197, val_acc:0.973]
Epoch [58/120    avg_loss:0.134, val_acc:0.971]
Epoch [59/120    avg_loss:0.111, val_acc:0.967]
Epoch [60/120    avg_loss:0.090, val_acc:0.958]
Epoch [61/120    avg_loss:0.124, val_acc:0.954]
Epoch [62/120    avg_loss:0.090, val_acc:0.975]
Epoch [63/120    avg_loss:0.093, val_acc:0.977]
Epoch [64/120    avg_loss:0.091, val_acc:0.963]
Epoch [65/120    avg_loss:0.081, val_acc:0.979]
Epoch [66/120    avg_loss:0.089, val_acc:0.969]
Epoch [67/120    avg_loss:0.091, val_acc:0.969]
Epoch [68/120    avg_loss:0.100, val_acc:0.977]
Epoch [69/120    avg_loss:0.067, val_acc:0.975]
Epoch [70/120    avg_loss:0.098, val_acc:0.967]
Epoch [71/120    avg_loss:0.122, val_acc:0.950]
Epoch [72/120    avg_loss:0.085, val_acc:0.979]
Epoch [73/120    avg_loss:0.062, val_acc:0.971]
Epoch [74/120    avg_loss:0.062, val_acc:0.979]
Epoch [75/120    avg_loss:0.076, val_acc:0.981]
Epoch [76/120    avg_loss:0.054, val_acc:0.973]
Epoch [77/120    avg_loss:0.084, val_acc:0.979]
Epoch [78/120    avg_loss:0.038, val_acc:0.981]
Epoch [79/120    avg_loss:0.058, val_acc:0.985]
Epoch [80/120    avg_loss:0.069, val_acc:0.985]
Epoch [81/120    avg_loss:0.064, val_acc:0.988]
Epoch [82/120    avg_loss:0.048, val_acc:0.973]
Epoch [83/120    avg_loss:0.084, val_acc:0.981]
Epoch [84/120    avg_loss:0.071, val_acc:0.969]
Epoch [85/120    avg_loss:0.041, val_acc:0.985]
Epoch [86/120    avg_loss:0.070, val_acc:0.979]
Epoch [87/120    avg_loss:0.092, val_acc:0.979]
Epoch [88/120    avg_loss:0.045, val_acc:0.977]
Epoch [89/120    avg_loss:0.044, val_acc:0.983]
Epoch [90/120    avg_loss:0.032, val_acc:0.985]
Epoch [91/120    avg_loss:0.041, val_acc:0.971]
Epoch [92/120    avg_loss:0.035, val_acc:0.981]
Epoch [93/120    avg_loss:0.041, val_acc:0.983]
Epoch [94/120    avg_loss:0.037, val_acc:0.981]
Epoch [95/120    avg_loss:0.030, val_acc:0.983]
Epoch [96/120    avg_loss:0.027, val_acc:0.985]
Epoch [97/120    avg_loss:0.025, val_acc:0.988]
Epoch [98/120    avg_loss:0.024, val_acc:0.985]
Epoch [99/120    avg_loss:0.024, val_acc:0.988]
Epoch [100/120    avg_loss:0.027, val_acc:0.985]
Epoch [101/120    avg_loss:0.021, val_acc:0.985]
Epoch [102/120    avg_loss:0.028, val_acc:0.988]
Epoch [103/120    avg_loss:0.017, val_acc:0.990]
Epoch [104/120    avg_loss:0.020, val_acc:0.990]
Epoch [105/120    avg_loss:0.024, val_acc:0.990]
Epoch [106/120    avg_loss:0.017, val_acc:0.988]
Epoch [107/120    avg_loss:0.025, val_acc:0.985]
Epoch [108/120    avg_loss:0.018, val_acc:0.985]
Epoch [109/120    avg_loss:0.020, val_acc:0.988]
Epoch [110/120    avg_loss:0.020, val_acc:0.988]
Epoch [111/120    avg_loss:0.020, val_acc:0.988]
Epoch [112/120    avg_loss:0.019, val_acc:0.990]
Epoch [113/120    avg_loss:0.034, val_acc:0.985]
Epoch [114/120    avg_loss:0.024, val_acc:0.985]
Epoch [115/120    avg_loss:0.020, val_acc:0.985]
Epoch [116/120    avg_loss:0.022, val_acc:0.988]
Epoch [117/120    avg_loss:0.017, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.985]
Epoch [119/120    avg_loss:0.015, val_acc:0.985]
Epoch [120/120    avg_loss:0.018, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   5   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   9 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.98648649 0.98230088 0.94854586 0.94736842
 0.99512195 0.9726776  0.99742931 0.99893276 1.         0.98820446
 0.98883929 1.        ]

Kappa:
0.9902677839811439
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7e0951828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.509, val_acc:0.340]
Epoch [2/120    avg_loss:2.130, val_acc:0.504]
Epoch [3/120    avg_loss:1.894, val_acc:0.546]
Epoch [4/120    avg_loss:1.705, val_acc:0.642]
Epoch [5/120    avg_loss:1.458, val_acc:0.650]
Epoch [6/120    avg_loss:1.283, val_acc:0.694]
Epoch [7/120    avg_loss:1.097, val_acc:0.731]
Epoch [8/120    avg_loss:0.955, val_acc:0.754]
Epoch [9/120    avg_loss:0.876, val_acc:0.756]
Epoch [10/120    avg_loss:0.841, val_acc:0.792]
Epoch [11/120    avg_loss:0.749, val_acc:0.794]
Epoch [12/120    avg_loss:0.713, val_acc:0.802]
Epoch [13/120    avg_loss:0.755, val_acc:0.800]
Epoch [14/120    avg_loss:0.692, val_acc:0.896]
Epoch [15/120    avg_loss:0.609, val_acc:0.910]
Epoch [16/120    avg_loss:0.509, val_acc:0.900]
Epoch [17/120    avg_loss:0.486, val_acc:0.915]
Epoch [18/120    avg_loss:0.452, val_acc:0.923]
Epoch [19/120    avg_loss:0.383, val_acc:0.948]
Epoch [20/120    avg_loss:0.379, val_acc:0.910]
Epoch [21/120    avg_loss:0.375, val_acc:0.925]
Epoch [22/120    avg_loss:0.348, val_acc:0.938]
Epoch [23/120    avg_loss:0.323, val_acc:0.938]
Epoch [24/120    avg_loss:0.306, val_acc:0.927]
Epoch [25/120    avg_loss:0.337, val_acc:0.942]
Epoch [26/120    avg_loss:0.309, val_acc:0.940]
Epoch [27/120    avg_loss:0.295, val_acc:0.954]
Epoch [28/120    avg_loss:0.296, val_acc:0.958]
Epoch [29/120    avg_loss:0.260, val_acc:0.950]
Epoch [30/120    avg_loss:0.240, val_acc:0.942]
Epoch [31/120    avg_loss:0.243, val_acc:0.956]
Epoch [32/120    avg_loss:0.298, val_acc:0.948]
Epoch [33/120    avg_loss:0.271, val_acc:0.931]
Epoch [34/120    avg_loss:0.248, val_acc:0.950]
Epoch [35/120    avg_loss:0.289, val_acc:0.975]
Epoch [36/120    avg_loss:0.221, val_acc:0.965]
Epoch [37/120    avg_loss:0.181, val_acc:0.973]
Epoch [38/120    avg_loss:0.187, val_acc:0.967]
Epoch [39/120    avg_loss:0.184, val_acc:0.973]
Epoch [40/120    avg_loss:0.187, val_acc:0.969]
Epoch [41/120    avg_loss:0.148, val_acc:0.973]
Epoch [42/120    avg_loss:0.176, val_acc:0.963]
Epoch [43/120    avg_loss:0.168, val_acc:0.971]
Epoch [44/120    avg_loss:0.191, val_acc:0.960]
Epoch [45/120    avg_loss:0.197, val_acc:0.971]
Epoch [46/120    avg_loss:0.171, val_acc:0.969]
Epoch [47/120    avg_loss:0.208, val_acc:0.929]
Epoch [48/120    avg_loss:0.170, val_acc:0.965]
Epoch [49/120    avg_loss:0.131, val_acc:0.973]
Epoch [50/120    avg_loss:0.108, val_acc:0.979]
Epoch [51/120    avg_loss:0.124, val_acc:0.979]
Epoch [52/120    avg_loss:0.126, val_acc:0.977]
Epoch [53/120    avg_loss:0.093, val_acc:0.979]
Epoch [54/120    avg_loss:0.119, val_acc:0.977]
Epoch [55/120    avg_loss:0.112, val_acc:0.981]
Epoch [56/120    avg_loss:0.097, val_acc:0.979]
Epoch [57/120    avg_loss:0.103, val_acc:0.981]
Epoch [58/120    avg_loss:0.096, val_acc:0.979]
Epoch [59/120    avg_loss:0.096, val_acc:0.981]
Epoch [60/120    avg_loss:0.091, val_acc:0.981]
Epoch [61/120    avg_loss:0.093, val_acc:0.981]
Epoch [62/120    avg_loss:0.099, val_acc:0.981]
Epoch [63/120    avg_loss:0.094, val_acc:0.981]
Epoch [64/120    avg_loss:0.088, val_acc:0.977]
Epoch [65/120    avg_loss:0.094, val_acc:0.981]
Epoch [66/120    avg_loss:0.092, val_acc:0.981]
Epoch [67/120    avg_loss:0.092, val_acc:0.973]
Epoch [68/120    avg_loss:0.094, val_acc:0.983]
Epoch [69/120    avg_loss:0.108, val_acc:0.983]
Epoch [70/120    avg_loss:0.093, val_acc:0.988]
Epoch [71/120    avg_loss:0.087, val_acc:0.983]
Epoch [72/120    avg_loss:0.075, val_acc:0.983]
Epoch [73/120    avg_loss:0.093, val_acc:0.981]
Epoch [74/120    avg_loss:0.077, val_acc:0.983]
Epoch [75/120    avg_loss:0.083, val_acc:0.981]
Epoch [76/120    avg_loss:0.082, val_acc:0.985]
Epoch [77/120    avg_loss:0.078, val_acc:0.983]
Epoch [78/120    avg_loss:0.083, val_acc:0.985]
Epoch [79/120    avg_loss:0.072, val_acc:0.988]
Epoch [80/120    avg_loss:0.093, val_acc:0.983]
Epoch [81/120    avg_loss:0.079, val_acc:0.983]
Epoch [82/120    avg_loss:0.088, val_acc:0.988]
Epoch [83/120    avg_loss:0.066, val_acc:0.985]
Epoch [84/120    avg_loss:0.070, val_acc:0.985]
Epoch [85/120    avg_loss:0.071, val_acc:0.985]
Epoch [86/120    avg_loss:0.066, val_acc:0.981]
Epoch [87/120    avg_loss:0.072, val_acc:0.985]
Epoch [88/120    avg_loss:0.074, val_acc:0.988]
Epoch [89/120    avg_loss:0.058, val_acc:0.988]
Epoch [90/120    avg_loss:0.080, val_acc:0.988]
Epoch [91/120    avg_loss:0.068, val_acc:0.988]
Epoch [92/120    avg_loss:0.073, val_acc:0.985]
Epoch [93/120    avg_loss:0.063, val_acc:0.981]
Epoch [94/120    avg_loss:0.068, val_acc:0.990]
Epoch [95/120    avg_loss:0.072, val_acc:0.988]
Epoch [96/120    avg_loss:0.064, val_acc:0.985]
Epoch [97/120    avg_loss:0.088, val_acc:0.988]
Epoch [98/120    avg_loss:0.070, val_acc:0.988]
Epoch [99/120    avg_loss:0.082, val_acc:0.990]
Epoch [100/120    avg_loss:0.083, val_acc:0.988]
Epoch [101/120    avg_loss:0.068, val_acc:0.992]
Epoch [102/120    avg_loss:0.057, val_acc:0.992]
Epoch [103/120    avg_loss:0.069, val_acc:0.990]
Epoch [104/120    avg_loss:0.072, val_acc:0.990]
Epoch [105/120    avg_loss:0.064, val_acc:0.992]
Epoch [106/120    avg_loss:0.068, val_acc:0.990]
Epoch [107/120    avg_loss:0.071, val_acc:0.992]
Epoch [108/120    avg_loss:0.069, val_acc:0.985]
Epoch [109/120    avg_loss:0.073, val_acc:0.990]
Epoch [110/120    avg_loss:0.064, val_acc:0.992]
Epoch [111/120    avg_loss:0.064, val_acc:0.990]
Epoch [112/120    avg_loss:0.062, val_acc:0.990]
Epoch [113/120    avg_loss:0.071, val_acc:0.990]
Epoch [114/120    avg_loss:0.063, val_acc:0.988]
Epoch [115/120    avg_loss:0.057, val_acc:0.990]
Epoch [116/120    avg_loss:0.066, val_acc:0.990]
Epoch [117/120    avg_loss:0.075, val_acc:0.988]
Epoch [118/120    avg_loss:0.049, val_acc:0.994]
Epoch [119/120    avg_loss:0.050, val_acc:0.994]
Epoch [120/120    avg_loss:0.054, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97550111 0.99782135 0.94036697 0.91558442
 1.         0.94382022 1.         1.         1.         0.98300654
 0.98547486 1.        ]

Kappa:
0.9881315718257787
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fae3afd5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.523, val_acc:0.296]
Epoch [2/120    avg_loss:2.174, val_acc:0.485]
Epoch [3/120    avg_loss:1.948, val_acc:0.556]
Epoch [4/120    avg_loss:1.731, val_acc:0.671]
Epoch [5/120    avg_loss:1.482, val_acc:0.675]
Epoch [6/120    avg_loss:1.259, val_acc:0.696]
Epoch [7/120    avg_loss:1.115, val_acc:0.721]
Epoch [8/120    avg_loss:1.001, val_acc:0.754]
Epoch [9/120    avg_loss:0.873, val_acc:0.787]
Epoch [10/120    avg_loss:0.820, val_acc:0.779]
Epoch [11/120    avg_loss:0.737, val_acc:0.796]
Epoch [12/120    avg_loss:0.697, val_acc:0.890]
Epoch [13/120    avg_loss:0.574, val_acc:0.850]
Epoch [14/120    avg_loss:0.534, val_acc:0.900]
Epoch [15/120    avg_loss:0.535, val_acc:0.902]
Epoch [16/120    avg_loss:0.490, val_acc:0.917]
Epoch [17/120    avg_loss:0.435, val_acc:0.923]
Epoch [18/120    avg_loss:0.448, val_acc:0.923]
Epoch [19/120    avg_loss:0.416, val_acc:0.935]
Epoch [20/120    avg_loss:0.384, val_acc:0.938]
Epoch [21/120    avg_loss:0.377, val_acc:0.929]
Epoch [22/120    avg_loss:0.397, val_acc:0.948]
Epoch [23/120    avg_loss:0.348, val_acc:0.946]
Epoch [24/120    avg_loss:0.340, val_acc:0.942]
Epoch [25/120    avg_loss:0.322, val_acc:0.917]
Epoch [26/120    avg_loss:0.317, val_acc:0.956]
Epoch [27/120    avg_loss:0.313, val_acc:0.965]
Epoch [28/120    avg_loss:0.308, val_acc:0.965]
Epoch [29/120    avg_loss:0.276, val_acc:0.952]
Epoch [30/120    avg_loss:0.259, val_acc:0.960]
Epoch [31/120    avg_loss:0.260, val_acc:0.915]
Epoch [32/120    avg_loss:0.221, val_acc:0.965]
Epoch [33/120    avg_loss:0.247, val_acc:0.933]
Epoch [34/120    avg_loss:0.207, val_acc:0.950]
Epoch [35/120    avg_loss:0.279, val_acc:0.963]
Epoch [36/120    avg_loss:0.194, val_acc:0.960]
Epoch [37/120    avg_loss:0.187, val_acc:0.956]
Epoch [38/120    avg_loss:0.183, val_acc:0.967]
Epoch [39/120    avg_loss:0.159, val_acc:0.960]
Epoch [40/120    avg_loss:0.171, val_acc:0.965]
Epoch [41/120    avg_loss:0.139, val_acc:0.960]
Epoch [42/120    avg_loss:0.185, val_acc:0.965]
Epoch [43/120    avg_loss:0.157, val_acc:0.969]
Epoch [44/120    avg_loss:0.163, val_acc:0.969]
Epoch [45/120    avg_loss:0.137, val_acc:0.958]
Epoch [46/120    avg_loss:0.120, val_acc:0.975]
Epoch [47/120    avg_loss:0.124, val_acc:0.952]
Epoch [48/120    avg_loss:0.141, val_acc:0.965]
Epoch [49/120    avg_loss:0.123, val_acc:0.963]
Epoch [50/120    avg_loss:0.143, val_acc:0.973]
Epoch [51/120    avg_loss:0.109, val_acc:0.969]
Epoch [52/120    avg_loss:0.098, val_acc:0.971]
Epoch [53/120    avg_loss:0.109, val_acc:0.967]
Epoch [54/120    avg_loss:0.167, val_acc:0.975]
Epoch [55/120    avg_loss:0.141, val_acc:0.965]
Epoch [56/120    avg_loss:0.098, val_acc:0.975]
Epoch [57/120    avg_loss:0.090, val_acc:0.975]
Epoch [58/120    avg_loss:0.115, val_acc:0.965]
Epoch [59/120    avg_loss:0.079, val_acc:0.967]
Epoch [60/120    avg_loss:0.108, val_acc:0.981]
Epoch [61/120    avg_loss:0.092, val_acc:0.971]
Epoch [62/120    avg_loss:0.099, val_acc:0.971]
Epoch [63/120    avg_loss:0.088, val_acc:0.981]
Epoch [64/120    avg_loss:0.086, val_acc:0.975]
Epoch [65/120    avg_loss:0.111, val_acc:0.975]
Epoch [66/120    avg_loss:0.079, val_acc:0.973]
Epoch [67/120    avg_loss:0.059, val_acc:0.983]
Epoch [68/120    avg_loss:0.075, val_acc:0.973]
Epoch [69/120    avg_loss:0.065, val_acc:0.977]
Epoch [70/120    avg_loss:0.080, val_acc:0.977]
Epoch [71/120    avg_loss:0.054, val_acc:0.975]
Epoch [72/120    avg_loss:0.065, val_acc:0.979]
Epoch [73/120    avg_loss:0.091, val_acc:0.977]
Epoch [74/120    avg_loss:0.064, val_acc:0.979]
Epoch [75/120    avg_loss:0.042, val_acc:0.971]
Epoch [76/120    avg_loss:0.064, val_acc:0.975]
Epoch [77/120    avg_loss:0.111, val_acc:0.979]
Epoch [78/120    avg_loss:0.065, val_acc:0.973]
Epoch [79/120    avg_loss:0.059, val_acc:0.977]
Epoch [80/120    avg_loss:0.059, val_acc:0.975]
Epoch [81/120    avg_loss:0.058, val_acc:0.979]
Epoch [82/120    avg_loss:0.033, val_acc:0.981]
Epoch [83/120    avg_loss:0.032, val_acc:0.981]
Epoch [84/120    avg_loss:0.033, val_acc:0.981]
Epoch [85/120    avg_loss:0.031, val_acc:0.981]
Epoch [86/120    avg_loss:0.028, val_acc:0.981]
Epoch [87/120    avg_loss:0.026, val_acc:0.981]
Epoch [88/120    avg_loss:0.033, val_acc:0.981]
Epoch [89/120    avg_loss:0.024, val_acc:0.981]
Epoch [90/120    avg_loss:0.028, val_acc:0.981]
Epoch [91/120    avg_loss:0.031, val_acc:0.981]
Epoch [92/120    avg_loss:0.027, val_acc:0.985]
Epoch [93/120    avg_loss:0.031, val_acc:0.985]
Epoch [94/120    avg_loss:0.028, val_acc:0.985]
Epoch [95/120    avg_loss:0.026, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.985]
Epoch [97/120    avg_loss:0.031, val_acc:0.985]
Epoch [98/120    avg_loss:0.039, val_acc:0.985]
Epoch [99/120    avg_loss:0.026, val_acc:0.985]
Epoch [100/120    avg_loss:0.027, val_acc:0.985]
Epoch [101/120    avg_loss:0.027, val_acc:0.985]
Epoch [102/120    avg_loss:0.026, val_acc:0.985]
Epoch [103/120    avg_loss:0.031, val_acc:0.985]
Epoch [104/120    avg_loss:0.024, val_acc:0.985]
Epoch [105/120    avg_loss:0.025, val_acc:0.985]
Epoch [106/120    avg_loss:0.024, val_acc:0.983]
Epoch [107/120    avg_loss:0.029, val_acc:0.983]
Epoch [108/120    avg_loss:0.027, val_acc:0.983]
Epoch [109/120    avg_loss:0.026, val_acc:0.983]
Epoch [110/120    avg_loss:0.026, val_acc:0.985]
Epoch [111/120    avg_loss:0.033, val_acc:0.985]
Epoch [112/120    avg_loss:0.023, val_acc:0.985]
Epoch [113/120    avg_loss:0.024, val_acc:0.985]
Epoch [114/120    avg_loss:0.019, val_acc:0.983]
Epoch [115/120    avg_loss:0.029, val_acc:0.983]
Epoch [116/120    avg_loss:0.017, val_acc:0.983]
Epoch [117/120    avg_loss:0.027, val_acc:0.981]
Epoch [118/120    avg_loss:0.035, val_acc:0.983]
Epoch [119/120    avg_loss:0.024, val_acc:0.983]
Epoch [120/120    avg_loss:0.027, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         0.98866213 0.98901099 0.95259594 0.94771242
 1.         0.97297297 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9935909848848873
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0f2fae780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.485, val_acc:0.404]
Epoch [2/120    avg_loss:2.077, val_acc:0.519]
Epoch [3/120    avg_loss:1.790, val_acc:0.637]
Epoch [4/120    avg_loss:1.574, val_acc:0.673]
Epoch [5/120    avg_loss:1.385, val_acc:0.685]
Epoch [6/120    avg_loss:1.202, val_acc:0.742]
Epoch [7/120    avg_loss:1.042, val_acc:0.765]
Epoch [8/120    avg_loss:0.950, val_acc:0.787]
Epoch [9/120    avg_loss:0.857, val_acc:0.777]
Epoch [10/120    avg_loss:0.785, val_acc:0.844]
Epoch [11/120    avg_loss:0.660, val_acc:0.885]
Epoch [12/120    avg_loss:0.606, val_acc:0.887]
Epoch [13/120    avg_loss:0.556, val_acc:0.875]
Epoch [14/120    avg_loss:0.495, val_acc:0.915]
Epoch [15/120    avg_loss:0.534, val_acc:0.921]
Epoch [16/120    avg_loss:0.512, val_acc:0.917]
Epoch [17/120    avg_loss:0.414, val_acc:0.950]
Epoch [18/120    avg_loss:0.350, val_acc:0.938]
Epoch [19/120    avg_loss:0.319, val_acc:0.921]
Epoch [20/120    avg_loss:0.306, val_acc:0.935]
Epoch [21/120    avg_loss:0.344, val_acc:0.933]
Epoch [22/120    avg_loss:0.333, val_acc:0.965]
Epoch [23/120    avg_loss:0.286, val_acc:0.931]
Epoch [24/120    avg_loss:0.275, val_acc:0.960]
Epoch [25/120    avg_loss:0.282, val_acc:0.890]
Epoch [26/120    avg_loss:0.247, val_acc:0.960]
Epoch [27/120    avg_loss:0.203, val_acc:0.958]
Epoch [28/120    avg_loss:0.225, val_acc:0.952]
Epoch [29/120    avg_loss:0.195, val_acc:0.965]
Epoch [30/120    avg_loss:0.154, val_acc:0.969]
Epoch [31/120    avg_loss:0.154, val_acc:0.971]
Epoch [32/120    avg_loss:0.200, val_acc:0.967]
Epoch [33/120    avg_loss:0.263, val_acc:0.946]
Epoch [34/120    avg_loss:0.236, val_acc:0.950]
Epoch [35/120    avg_loss:0.199, val_acc:0.923]
Epoch [36/120    avg_loss:0.190, val_acc:0.971]
Epoch [37/120    avg_loss:0.138, val_acc:0.971]
Epoch [38/120    avg_loss:0.162, val_acc:0.954]
Epoch [39/120    avg_loss:0.169, val_acc:0.963]
Epoch [40/120    avg_loss:0.153, val_acc:0.963]
Epoch [41/120    avg_loss:0.107, val_acc:0.967]
Epoch [42/120    avg_loss:0.094, val_acc:0.967]
Epoch [43/120    avg_loss:0.123, val_acc:0.979]
Epoch [44/120    avg_loss:0.145, val_acc:0.969]
Epoch [45/120    avg_loss:0.172, val_acc:0.963]
Epoch [46/120    avg_loss:0.135, val_acc:0.971]
Epoch [47/120    avg_loss:0.107, val_acc:0.963]
Epoch [48/120    avg_loss:0.099, val_acc:0.975]
Epoch [49/120    avg_loss:0.091, val_acc:0.969]
Epoch [50/120    avg_loss:0.070, val_acc:0.973]
Epoch [51/120    avg_loss:0.120, val_acc:0.975]
Epoch [52/120    avg_loss:0.111, val_acc:0.979]
Epoch [53/120    avg_loss:0.100, val_acc:0.967]
Epoch [54/120    avg_loss:0.155, val_acc:0.975]
Epoch [55/120    avg_loss:0.099, val_acc:0.967]
Epoch [56/120    avg_loss:0.114, val_acc:0.969]
Epoch [57/120    avg_loss:0.149, val_acc:0.965]
Epoch [58/120    avg_loss:0.112, val_acc:0.969]
Epoch [59/120    avg_loss:0.087, val_acc:0.975]
Epoch [60/120    avg_loss:0.113, val_acc:0.954]
Epoch [61/120    avg_loss:0.090, val_acc:0.975]
Epoch [62/120    avg_loss:0.046, val_acc:0.981]
Epoch [63/120    avg_loss:0.059, val_acc:0.981]
Epoch [64/120    avg_loss:0.076, val_acc:0.983]
Epoch [65/120    avg_loss:0.072, val_acc:0.979]
Epoch [66/120    avg_loss:0.055, val_acc:0.979]
Epoch [67/120    avg_loss:0.065, val_acc:0.977]
Epoch [68/120    avg_loss:0.056, val_acc:0.975]
Epoch [69/120    avg_loss:0.053, val_acc:0.977]
Epoch [70/120    avg_loss:0.042, val_acc:0.985]
Epoch [71/120    avg_loss:0.043, val_acc:0.983]
Epoch [72/120    avg_loss:0.042, val_acc:0.983]
Epoch [73/120    avg_loss:0.054, val_acc:0.979]
Epoch [74/120    avg_loss:0.085, val_acc:0.985]
Epoch [75/120    avg_loss:0.047, val_acc:0.985]
Epoch [76/120    avg_loss:0.036, val_acc:0.988]
Epoch [77/120    avg_loss:0.038, val_acc:0.988]
Epoch [78/120    avg_loss:0.041, val_acc:0.988]
Epoch [79/120    avg_loss:0.025, val_acc:0.983]
Epoch [80/120    avg_loss:0.039, val_acc:0.985]
Epoch [81/120    avg_loss:0.041, val_acc:0.981]
Epoch [82/120    avg_loss:0.038, val_acc:0.983]
Epoch [83/120    avg_loss:0.046, val_acc:0.981]
Epoch [84/120    avg_loss:0.027, val_acc:0.983]
Epoch [85/120    avg_loss:0.027, val_acc:0.990]
Epoch [86/120    avg_loss:0.025, val_acc:0.988]
Epoch [87/120    avg_loss:0.038, val_acc:0.983]
Epoch [88/120    avg_loss:0.051, val_acc:0.983]
Epoch [89/120    avg_loss:0.030, val_acc:0.985]
Epoch [90/120    avg_loss:0.024, val_acc:0.988]
Epoch [91/120    avg_loss:0.019, val_acc:0.983]
Epoch [92/120    avg_loss:0.030, val_acc:0.985]
Epoch [93/120    avg_loss:0.043, val_acc:0.981]
Epoch [94/120    avg_loss:0.039, val_acc:0.992]
Epoch [95/120    avg_loss:0.031, val_acc:0.985]
Epoch [96/120    avg_loss:0.023, val_acc:0.983]
Epoch [97/120    avg_loss:0.020, val_acc:0.990]
Epoch [98/120    avg_loss:0.020, val_acc:0.988]
Epoch [99/120    avg_loss:0.017, val_acc:0.983]
Epoch [100/120    avg_loss:0.023, val_acc:0.988]
Epoch [101/120    avg_loss:0.019, val_acc:0.983]
Epoch [102/120    avg_loss:0.038, val_acc:0.990]
Epoch [103/120    avg_loss:0.035, val_acc:0.985]
Epoch [104/120    avg_loss:0.032, val_acc:0.985]
Epoch [105/120    avg_loss:0.039, val_acc:0.979]
Epoch [106/120    avg_loss:0.061, val_acc:0.969]
Epoch [107/120    avg_loss:0.075, val_acc:0.981]
Epoch [108/120    avg_loss:0.047, val_acc:0.988]
Epoch [109/120    avg_loss:0.021, val_acc:0.990]
Epoch [110/120    avg_loss:0.026, val_acc:0.990]
Epoch [111/120    avg_loss:0.019, val_acc:0.990]
Epoch [112/120    avg_loss:0.019, val_acc:0.990]
Epoch [113/120    avg_loss:0.025, val_acc:0.990]
Epoch [114/120    avg_loss:0.018, val_acc:0.990]
Epoch [115/120    avg_loss:0.014, val_acc:0.990]
Epoch [116/120    avg_loss:0.017, val_acc:0.990]
Epoch [117/120    avg_loss:0.017, val_acc:0.990]
Epoch [118/120    avg_loss:0.019, val_acc:0.990]
Epoch [119/120    avg_loss:0.020, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         0.99095023 1.         0.96846847 0.95652174
 1.         0.97826087 1.         1.         1.         0.9986755
 0.99779249 1.        ]

Kappa:
0.9954897724478595
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa56d747f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.498, val_acc:0.373]
Epoch [2/120    avg_loss:2.153, val_acc:0.558]
Epoch [3/120    avg_loss:1.935, val_acc:0.652]
Epoch [4/120    avg_loss:1.745, val_acc:0.623]
Epoch [5/120    avg_loss:1.512, val_acc:0.681]
Epoch [6/120    avg_loss:1.322, val_acc:0.692]
Epoch [7/120    avg_loss:1.173, val_acc:0.721]
Epoch [8/120    avg_loss:1.017, val_acc:0.744]
Epoch [9/120    avg_loss:0.923, val_acc:0.808]
Epoch [10/120    avg_loss:0.821, val_acc:0.783]
Epoch [11/120    avg_loss:0.758, val_acc:0.823]
Epoch [12/120    avg_loss:0.721, val_acc:0.867]
Epoch [13/120    avg_loss:0.617, val_acc:0.892]
Epoch [14/120    avg_loss:0.563, val_acc:0.906]
Epoch [15/120    avg_loss:0.523, val_acc:0.917]
Epoch [16/120    avg_loss:0.471, val_acc:0.925]
Epoch [17/120    avg_loss:0.452, val_acc:0.871]
Epoch [18/120    avg_loss:0.413, val_acc:0.935]
Epoch [19/120    avg_loss:0.355, val_acc:0.927]
Epoch [20/120    avg_loss:0.349, val_acc:0.938]
Epoch [21/120    avg_loss:0.313, val_acc:0.946]
Epoch [22/120    avg_loss:0.349, val_acc:0.954]
Epoch [23/120    avg_loss:0.338, val_acc:0.935]
Epoch [24/120    avg_loss:0.358, val_acc:0.938]
Epoch [25/120    avg_loss:0.300, val_acc:0.971]
Epoch [26/120    avg_loss:0.283, val_acc:0.969]
Epoch [27/120    avg_loss:0.234, val_acc:0.967]
Epoch [28/120    avg_loss:0.283, val_acc:0.956]
Epoch [29/120    avg_loss:0.244, val_acc:0.973]
Epoch [30/120    avg_loss:0.242, val_acc:0.973]
Epoch [31/120    avg_loss:0.199, val_acc:0.963]
Epoch [32/120    avg_loss:0.208, val_acc:0.969]
Epoch [33/120    avg_loss:0.187, val_acc:0.927]
Epoch [34/120    avg_loss:0.306, val_acc:0.910]
Epoch [35/120    avg_loss:0.219, val_acc:0.956]
Epoch [36/120    avg_loss:0.248, val_acc:0.971]
Epoch [37/120    avg_loss:0.160, val_acc:0.971]
Epoch [38/120    avg_loss:0.141, val_acc:0.979]
Epoch [39/120    avg_loss:0.148, val_acc:0.952]
Epoch [40/120    avg_loss:0.176, val_acc:0.979]
Epoch [41/120    avg_loss:0.124, val_acc:0.983]
Epoch [42/120    avg_loss:0.117, val_acc:0.983]
Epoch [43/120    avg_loss:0.122, val_acc:0.963]
Epoch [44/120    avg_loss:0.151, val_acc:0.973]
Epoch [45/120    avg_loss:0.184, val_acc:0.973]
Epoch [46/120    avg_loss:0.151, val_acc:0.985]
Epoch [47/120    avg_loss:0.100, val_acc:0.985]
Epoch [48/120    avg_loss:0.103, val_acc:0.977]
Epoch [49/120    avg_loss:0.110, val_acc:0.979]
Epoch [50/120    avg_loss:0.107, val_acc:0.969]
Epoch [51/120    avg_loss:0.151, val_acc:0.971]
Epoch [52/120    avg_loss:0.125, val_acc:0.990]
Epoch [53/120    avg_loss:0.084, val_acc:0.981]
Epoch [54/120    avg_loss:0.073, val_acc:0.983]
Epoch [55/120    avg_loss:0.085, val_acc:0.990]
Epoch [56/120    avg_loss:0.108, val_acc:0.996]
Epoch [57/120    avg_loss:0.112, val_acc:0.988]
Epoch [58/120    avg_loss:0.079, val_acc:0.992]
Epoch [59/120    avg_loss:0.068, val_acc:0.988]
Epoch [60/120    avg_loss:0.071, val_acc:0.994]
Epoch [61/120    avg_loss:0.057, val_acc:0.990]
Epoch [62/120    avg_loss:0.106, val_acc:0.977]
Epoch [63/120    avg_loss:0.105, val_acc:0.990]
Epoch [64/120    avg_loss:0.057, val_acc:0.990]
Epoch [65/120    avg_loss:0.058, val_acc:0.996]
Epoch [66/120    avg_loss:0.058, val_acc:0.988]
Epoch [67/120    avg_loss:0.055, val_acc:0.996]
Epoch [68/120    avg_loss:0.065, val_acc:0.990]
Epoch [69/120    avg_loss:0.058, val_acc:0.994]
Epoch [70/120    avg_loss:0.038, val_acc:0.992]
Epoch [71/120    avg_loss:0.035, val_acc:0.996]
Epoch [72/120    avg_loss:0.035, val_acc:0.990]
Epoch [73/120    avg_loss:0.059, val_acc:0.996]
Epoch [74/120    avg_loss:0.066, val_acc:0.983]
Epoch [75/120    avg_loss:0.056, val_acc:0.969]
Epoch [76/120    avg_loss:0.047, val_acc:0.992]
Epoch [77/120    avg_loss:0.077, val_acc:0.994]
Epoch [78/120    avg_loss:0.070, val_acc:0.992]
Epoch [79/120    avg_loss:0.070, val_acc:0.983]
Epoch [80/120    avg_loss:0.080, val_acc:0.983]
Epoch [81/120    avg_loss:0.067, val_acc:0.996]
Epoch [82/120    avg_loss:0.036, val_acc:0.992]
Epoch [83/120    avg_loss:0.026, val_acc:0.994]
Epoch [84/120    avg_loss:0.027, val_acc:0.994]
Epoch [85/120    avg_loss:0.029, val_acc:0.996]
Epoch [86/120    avg_loss:0.029, val_acc:0.994]
Epoch [87/120    avg_loss:0.031, val_acc:0.994]
Epoch [88/120    avg_loss:0.022, val_acc:0.992]
Epoch [89/120    avg_loss:0.020, val_acc:0.994]
Epoch [90/120    avg_loss:0.023, val_acc:0.996]
Epoch [91/120    avg_loss:0.015, val_acc:0.996]
Epoch [92/120    avg_loss:0.019, val_acc:0.996]
Epoch [93/120    avg_loss:0.018, val_acc:0.996]
Epoch [94/120    avg_loss:0.014, val_acc:0.996]
Epoch [95/120    avg_loss:0.025, val_acc:0.994]
Epoch [96/120    avg_loss:0.032, val_acc:0.994]
Epoch [97/120    avg_loss:0.041, val_acc:0.992]
Epoch [98/120    avg_loss:0.026, val_acc:0.998]
Epoch [99/120    avg_loss:0.036, val_acc:0.992]
Epoch [100/120    avg_loss:0.037, val_acc:0.998]
Epoch [101/120    avg_loss:0.022, val_acc:0.996]
Epoch [102/120    avg_loss:0.012, val_acc:0.994]
Epoch [103/120    avg_loss:0.015, val_acc:0.998]
Epoch [104/120    avg_loss:0.014, val_acc:0.996]
Epoch [105/120    avg_loss:0.011, val_acc:0.994]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.010, val_acc:0.996]
Epoch [108/120    avg_loss:0.009, val_acc:0.998]
Epoch [109/120    avg_loss:0.013, val_acc:0.996]
Epoch [110/120    avg_loss:0.009, val_acc:0.996]
Epoch [111/120    avg_loss:0.015, val_acc:0.998]
Epoch [112/120    avg_loss:0.013, val_acc:0.994]
Epoch [113/120    avg_loss:0.021, val_acc:0.998]
Epoch [114/120    avg_loss:0.028, val_acc:0.994]
Epoch [115/120    avg_loss:0.033, val_acc:0.998]
Epoch [116/120    avg_loss:0.019, val_acc:0.996]
Epoch [117/120    avg_loss:0.017, val_acc:0.996]
Epoch [118/120    avg_loss:0.023, val_acc:0.990]
Epoch [119/120    avg_loss:0.037, val_acc:0.998]
Epoch [120/120    avg_loss:0.014, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218  12   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.99926954 0.99545455 0.97321429 0.92473118 0.9209622
 0.99757869 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9909796631439168
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37533777f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.567, val_acc:0.354]
Epoch [2/120    avg_loss:2.115, val_acc:0.525]
Epoch [3/120    avg_loss:1.846, val_acc:0.617]
Epoch [4/120    avg_loss:1.590, val_acc:0.665]
Epoch [5/120    avg_loss:1.396, val_acc:0.723]
Epoch [6/120    avg_loss:1.188, val_acc:0.800]
Epoch [7/120    avg_loss:1.064, val_acc:0.769]
Epoch [8/120    avg_loss:0.920, val_acc:0.767]
Epoch [9/120    avg_loss:0.792, val_acc:0.783]
Epoch [10/120    avg_loss:0.751, val_acc:0.750]
Epoch [11/120    avg_loss:0.622, val_acc:0.852]
Epoch [12/120    avg_loss:0.597, val_acc:0.892]
Epoch [13/120    avg_loss:0.539, val_acc:0.804]
Epoch [14/120    avg_loss:0.549, val_acc:0.885]
Epoch [15/120    avg_loss:0.457, val_acc:0.908]
Epoch [16/120    avg_loss:0.493, val_acc:0.917]
Epoch [17/120    avg_loss:0.453, val_acc:0.892]
Epoch [18/120    avg_loss:0.426, val_acc:0.946]
Epoch [19/120    avg_loss:0.345, val_acc:0.931]
Epoch [20/120    avg_loss:0.335, val_acc:0.942]
Epoch [21/120    avg_loss:0.309, val_acc:0.942]
Epoch [22/120    avg_loss:0.288, val_acc:0.954]
Epoch [23/120    avg_loss:0.361, val_acc:0.925]
Epoch [24/120    avg_loss:0.350, val_acc:0.908]
Epoch [25/120    avg_loss:0.343, val_acc:0.896]
Epoch [26/120    avg_loss:0.325, val_acc:0.946]
Epoch [27/120    avg_loss:0.275, val_acc:0.963]
Epoch [28/120    avg_loss:0.251, val_acc:0.944]
Epoch [29/120    avg_loss:0.292, val_acc:0.960]
Epoch [30/120    avg_loss:0.233, val_acc:0.963]
Epoch [31/120    avg_loss:0.239, val_acc:0.956]
Epoch [32/120    avg_loss:0.243, val_acc:0.944]
Epoch [33/120    avg_loss:0.216, val_acc:0.967]
Epoch [34/120    avg_loss:0.203, val_acc:0.977]
Epoch [35/120    avg_loss:0.199, val_acc:0.971]
Epoch [36/120    avg_loss:0.179, val_acc:0.969]
Epoch [37/120    avg_loss:0.171, val_acc:0.975]
Epoch [38/120    avg_loss:0.175, val_acc:0.973]
Epoch [39/120    avg_loss:0.182, val_acc:0.969]
Epoch [40/120    avg_loss:0.182, val_acc:0.971]
Epoch [41/120    avg_loss:0.191, val_acc:0.969]
Epoch [42/120    avg_loss:0.236, val_acc:0.958]
Epoch [43/120    avg_loss:0.217, val_acc:0.963]
Epoch [44/120    avg_loss:0.262, val_acc:0.948]
Epoch [45/120    avg_loss:0.185, val_acc:0.975]
Epoch [46/120    avg_loss:0.176, val_acc:0.971]
Epoch [47/120    avg_loss:0.154, val_acc:0.985]
Epoch [48/120    avg_loss:0.110, val_acc:0.985]
Epoch [49/120    avg_loss:0.118, val_acc:0.985]
Epoch [50/120    avg_loss:0.158, val_acc:0.971]
Epoch [51/120    avg_loss:0.126, val_acc:0.983]
Epoch [52/120    avg_loss:0.121, val_acc:0.981]
Epoch [53/120    avg_loss:0.126, val_acc:0.979]
Epoch [54/120    avg_loss:0.131, val_acc:0.979]
Epoch [55/120    avg_loss:0.141, val_acc:0.965]
Epoch [56/120    avg_loss:0.109, val_acc:0.977]
Epoch [57/120    avg_loss:0.100, val_acc:0.977]
Epoch [58/120    avg_loss:0.175, val_acc:0.983]
Epoch [59/120    avg_loss:0.118, val_acc:0.983]
Epoch [60/120    avg_loss:0.099, val_acc:0.985]
Epoch [61/120    avg_loss:0.077, val_acc:0.988]
Epoch [62/120    avg_loss:0.070, val_acc:0.985]
Epoch [63/120    avg_loss:0.066, val_acc:0.988]
Epoch [64/120    avg_loss:0.059, val_acc:0.990]
Epoch [65/120    avg_loss:0.077, val_acc:0.971]
Epoch [66/120    avg_loss:0.095, val_acc:0.973]
Epoch [67/120    avg_loss:0.130, val_acc:0.981]
Epoch [68/120    avg_loss:0.081, val_acc:0.983]
Epoch [69/120    avg_loss:0.063, val_acc:0.985]
Epoch [70/120    avg_loss:0.053, val_acc:0.990]
Epoch [71/120    avg_loss:0.057, val_acc:0.992]
Epoch [72/120    avg_loss:0.049, val_acc:0.988]
Epoch [73/120    avg_loss:0.056, val_acc:0.931]
Epoch [74/120    avg_loss:0.076, val_acc:0.992]
Epoch [75/120    avg_loss:0.045, val_acc:0.992]
Epoch [76/120    avg_loss:0.045, val_acc:0.990]
Epoch [77/120    avg_loss:0.055, val_acc:0.992]
Epoch [78/120    avg_loss:0.091, val_acc:0.979]
Epoch [79/120    avg_loss:0.089, val_acc:0.985]
Epoch [80/120    avg_loss:0.085, val_acc:0.988]
Epoch [81/120    avg_loss:0.069, val_acc:0.983]
Epoch [82/120    avg_loss:0.056, val_acc:0.983]
Epoch [83/120    avg_loss:0.065, val_acc:0.996]
Epoch [84/120    avg_loss:0.058, val_acc:0.981]
Epoch [85/120    avg_loss:0.050, val_acc:0.990]
Epoch [86/120    avg_loss:0.053, val_acc:0.994]
Epoch [87/120    avg_loss:0.033, val_acc:0.994]
Epoch [88/120    avg_loss:0.030, val_acc:0.994]
Epoch [89/120    avg_loss:0.025, val_acc:0.992]
Epoch [90/120    avg_loss:0.026, val_acc:0.992]
Epoch [91/120    avg_loss:0.026, val_acc:0.994]
Epoch [92/120    avg_loss:0.033, val_acc:0.992]
Epoch [93/120    avg_loss:0.027, val_acc:0.992]
Epoch [94/120    avg_loss:0.030, val_acc:0.994]
Epoch [95/120    avg_loss:0.043, val_acc:0.983]
Epoch [96/120    avg_loss:0.027, val_acc:0.988]
Epoch [97/120    avg_loss:0.026, val_acc:0.988]
Epoch [98/120    avg_loss:0.021, val_acc:0.990]
Epoch [99/120    avg_loss:0.020, val_acc:0.992]
Epoch [100/120    avg_loss:0.014, val_acc:0.992]
Epoch [101/120    avg_loss:0.018, val_acc:0.992]
Epoch [102/120    avg_loss:0.015, val_acc:0.992]
Epoch [103/120    avg_loss:0.014, val_acc:0.992]
Epoch [104/120    avg_loss:0.014, val_acc:0.992]
Epoch [105/120    avg_loss:0.019, val_acc:0.992]
Epoch [106/120    avg_loss:0.012, val_acc:0.994]
Epoch [107/120    avg_loss:0.020, val_acc:0.994]
Epoch [108/120    avg_loss:0.015, val_acc:0.994]
Epoch [109/120    avg_loss:0.015, val_acc:0.996]
Epoch [110/120    avg_loss:0.013, val_acc:0.996]
Epoch [111/120    avg_loss:0.015, val_acc:0.996]
Epoch [112/120    avg_loss:0.016, val_acc:0.996]
Epoch [113/120    avg_loss:0.015, val_acc:0.996]
Epoch [114/120    avg_loss:0.013, val_acc:0.996]
Epoch [115/120    avg_loss:0.015, val_acc:0.996]
Epoch [116/120    avg_loss:0.015, val_acc:0.996]
Epoch [117/120    avg_loss:0.013, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.016, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 0.99926954 0.99319728 1.         0.96846847 0.95333333
 0.99757869 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.995727333636533
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff32e6fd828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.501, val_acc:0.367]
Epoch [2/120    avg_loss:2.159, val_acc:0.504]
Epoch [3/120    avg_loss:1.919, val_acc:0.583]
Epoch [4/120    avg_loss:1.693, val_acc:0.644]
Epoch [5/120    avg_loss:1.491, val_acc:0.669]
Epoch [6/120    avg_loss:1.294, val_acc:0.708]
Epoch [7/120    avg_loss:1.143, val_acc:0.729]
Epoch [8/120    avg_loss:0.986, val_acc:0.812]
Epoch [9/120    avg_loss:0.867, val_acc:0.762]
Epoch [10/120    avg_loss:0.787, val_acc:0.800]
Epoch [11/120    avg_loss:0.702, val_acc:0.840]
Epoch [12/120    avg_loss:0.637, val_acc:0.852]
Epoch [13/120    avg_loss:0.643, val_acc:0.894]
Epoch [14/120    avg_loss:0.555, val_acc:0.879]
Epoch [15/120    avg_loss:0.516, val_acc:0.877]
Epoch [16/120    avg_loss:0.501, val_acc:0.912]
Epoch [17/120    avg_loss:0.438, val_acc:0.869]
Epoch [18/120    avg_loss:0.474, val_acc:0.923]
Epoch [19/120    avg_loss:0.478, val_acc:0.900]
Epoch [20/120    avg_loss:0.482, val_acc:0.875]
Epoch [21/120    avg_loss:0.422, val_acc:0.938]
Epoch [22/120    avg_loss:0.371, val_acc:0.915]
Epoch [23/120    avg_loss:0.323, val_acc:0.958]
Epoch [24/120    avg_loss:0.342, val_acc:0.900]
Epoch [25/120    avg_loss:0.412, val_acc:0.944]
Epoch [26/120    avg_loss:0.359, val_acc:0.921]
Epoch [27/120    avg_loss:0.307, val_acc:0.917]
Epoch [28/120    avg_loss:0.339, val_acc:0.952]
Epoch [29/120    avg_loss:0.374, val_acc:0.958]
Epoch [30/120    avg_loss:0.266, val_acc:0.946]
Epoch [31/120    avg_loss:0.258, val_acc:0.950]
Epoch [32/120    avg_loss:0.239, val_acc:0.960]
Epoch [33/120    avg_loss:0.277, val_acc:0.965]
Epoch [34/120    avg_loss:0.208, val_acc:0.975]
Epoch [35/120    avg_loss:0.177, val_acc:0.965]
Epoch [36/120    avg_loss:0.196, val_acc:0.973]
Epoch [37/120    avg_loss:0.170, val_acc:0.979]
Epoch [38/120    avg_loss:0.200, val_acc:0.965]
Epoch [39/120    avg_loss:0.184, val_acc:0.973]
Epoch [40/120    avg_loss:0.214, val_acc:0.965]
Epoch [41/120    avg_loss:0.149, val_acc:0.971]
Epoch [42/120    avg_loss:0.125, val_acc:0.985]
Epoch [43/120    avg_loss:0.164, val_acc:0.967]
Epoch [44/120    avg_loss:0.132, val_acc:0.967]
Epoch [45/120    avg_loss:0.141, val_acc:0.981]
Epoch [46/120    avg_loss:0.142, val_acc:0.973]
Epoch [47/120    avg_loss:0.103, val_acc:0.988]
Epoch [48/120    avg_loss:0.079, val_acc:0.983]
Epoch [49/120    avg_loss:0.098, val_acc:0.985]
Epoch [50/120    avg_loss:0.106, val_acc:0.973]
Epoch [51/120    avg_loss:0.115, val_acc:0.983]
Epoch [52/120    avg_loss:0.103, val_acc:0.981]
Epoch [53/120    avg_loss:0.084, val_acc:0.975]
Epoch [54/120    avg_loss:0.089, val_acc:0.985]
Epoch [55/120    avg_loss:0.065, val_acc:0.985]
Epoch [56/120    avg_loss:0.064, val_acc:0.985]
Epoch [57/120    avg_loss:0.076, val_acc:0.985]
Epoch [58/120    avg_loss:0.084, val_acc:0.977]
Epoch [59/120    avg_loss:0.072, val_acc:0.990]
Epoch [60/120    avg_loss:0.057, val_acc:0.988]
Epoch [61/120    avg_loss:0.067, val_acc:0.985]
Epoch [62/120    avg_loss:0.065, val_acc:0.992]
Epoch [63/120    avg_loss:0.057, val_acc:0.992]
Epoch [64/120    avg_loss:0.070, val_acc:0.992]
Epoch [65/120    avg_loss:0.046, val_acc:0.990]
Epoch [66/120    avg_loss:0.050, val_acc:0.992]
Epoch [67/120    avg_loss:0.047, val_acc:0.992]
Epoch [68/120    avg_loss:0.044, val_acc:0.960]
Epoch [69/120    avg_loss:0.064, val_acc:0.985]
Epoch [70/120    avg_loss:0.066, val_acc:0.990]
Epoch [71/120    avg_loss:0.073, val_acc:0.979]
Epoch [72/120    avg_loss:0.091, val_acc:0.981]
Epoch [73/120    avg_loss:0.056, val_acc:0.994]
Epoch [74/120    avg_loss:0.042, val_acc:0.992]
Epoch [75/120    avg_loss:0.032, val_acc:0.983]
Epoch [76/120    avg_loss:0.032, val_acc:0.996]
Epoch [77/120    avg_loss:0.032, val_acc:0.996]
Epoch [78/120    avg_loss:0.029, val_acc:0.994]
Epoch [79/120    avg_loss:0.031, val_acc:0.996]
Epoch [80/120    avg_loss:0.100, val_acc:0.992]
Epoch [81/120    avg_loss:0.039, val_acc:0.990]
Epoch [82/120    avg_loss:0.040, val_acc:0.994]
Epoch [83/120    avg_loss:0.037, val_acc:0.990]
Epoch [84/120    avg_loss:0.035, val_acc:0.990]
Epoch [85/120    avg_loss:0.109, val_acc:0.948]
Epoch [86/120    avg_loss:0.151, val_acc:0.975]
Epoch [87/120    avg_loss:0.087, val_acc:0.977]
Epoch [88/120    avg_loss:0.075, val_acc:0.988]
Epoch [89/120    avg_loss:0.071, val_acc:0.975]
Epoch [90/120    avg_loss:0.064, val_acc:0.990]
Epoch [91/120    avg_loss:0.037, val_acc:0.990]
Epoch [92/120    avg_loss:0.040, val_acc:0.996]
Epoch [93/120    avg_loss:0.028, val_acc:0.990]
Epoch [94/120    avg_loss:0.037, val_acc:0.990]
Epoch [95/120    avg_loss:0.052, val_acc:0.985]
Epoch [96/120    avg_loss:0.045, val_acc:0.992]
Epoch [97/120    avg_loss:0.046, val_acc:0.998]
Epoch [98/120    avg_loss:0.023, val_acc:0.992]
Epoch [99/120    avg_loss:0.019, val_acc:1.000]
Epoch [100/120    avg_loss:0.016, val_acc:0.996]
Epoch [101/120    avg_loss:0.015, val_acc:1.000]
Epoch [102/120    avg_loss:0.012, val_acc:0.998]
Epoch [103/120    avg_loss:0.016, val_acc:0.996]
Epoch [104/120    avg_loss:0.028, val_acc:0.996]
Epoch [105/120    avg_loss:0.035, val_acc:1.000]
Epoch [106/120    avg_loss:0.019, val_acc:0.998]
Epoch [107/120    avg_loss:0.017, val_acc:0.998]
Epoch [108/120    avg_loss:0.021, val_acc:0.992]
Epoch [109/120    avg_loss:0.022, val_acc:0.998]
Epoch [110/120    avg_loss:0.020, val_acc:0.994]
Epoch [111/120    avg_loss:0.012, val_acc:0.996]
Epoch [112/120    avg_loss:0.015, val_acc:0.998]
Epoch [113/120    avg_loss:0.012, val_acc:0.998]
Epoch [114/120    avg_loss:0.010, val_acc:0.998]
Epoch [115/120    avg_loss:0.011, val_acc:0.998]
Epoch [116/120    avg_loss:0.011, val_acc:0.998]
Epoch [117/120    avg_loss:0.010, val_acc:1.000]
Epoch [118/120    avg_loss:0.007, val_acc:1.000]
Epoch [119/120    avg_loss:0.015, val_acc:1.000]
Epoch [120/120    avg_loss:0.007, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 1.         1.         1.         0.96396396 0.94666667
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9962020543525297
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e66642860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.557, val_acc:0.310]
Epoch [2/120    avg_loss:2.157, val_acc:0.500]
Epoch [3/120    avg_loss:1.899, val_acc:0.596]
Epoch [4/120    avg_loss:1.687, val_acc:0.669]
Epoch [5/120    avg_loss:1.450, val_acc:0.662]
Epoch [6/120    avg_loss:1.268, val_acc:0.713]
Epoch [7/120    avg_loss:1.106, val_acc:0.708]
Epoch [8/120    avg_loss:0.979, val_acc:0.760]
Epoch [9/120    avg_loss:0.973, val_acc:0.875]
Epoch [10/120    avg_loss:0.851, val_acc:0.815]
Epoch [11/120    avg_loss:0.743, val_acc:0.865]
Epoch [12/120    avg_loss:0.649, val_acc:0.896]
Epoch [13/120    avg_loss:0.596, val_acc:0.925]
Epoch [14/120    avg_loss:0.573, val_acc:0.823]
Epoch [15/120    avg_loss:0.586, val_acc:0.850]
Epoch [16/120    avg_loss:0.514, val_acc:0.912]
Epoch [17/120    avg_loss:0.475, val_acc:0.858]
Epoch [18/120    avg_loss:0.450, val_acc:0.929]
Epoch [19/120    avg_loss:0.402, val_acc:0.952]
Epoch [20/120    avg_loss:0.367, val_acc:0.948]
Epoch [21/120    avg_loss:0.376, val_acc:0.950]
Epoch [22/120    avg_loss:0.318, val_acc:0.948]
Epoch [23/120    avg_loss:0.299, val_acc:0.948]
Epoch [24/120    avg_loss:0.270, val_acc:0.956]
Epoch [25/120    avg_loss:0.246, val_acc:0.956]
Epoch [26/120    avg_loss:0.245, val_acc:0.971]
Epoch [27/120    avg_loss:0.260, val_acc:0.929]
Epoch [28/120    avg_loss:0.237, val_acc:0.967]
Epoch [29/120    avg_loss:0.250, val_acc:0.971]
Epoch [30/120    avg_loss:0.195, val_acc:0.971]
Epoch [31/120    avg_loss:0.241, val_acc:0.954]
Epoch [32/120    avg_loss:0.236, val_acc:0.948]
Epoch [33/120    avg_loss:0.223, val_acc:0.960]
Epoch [34/120    avg_loss:0.183, val_acc:0.975]
Epoch [35/120    avg_loss:0.226, val_acc:0.965]
Epoch [36/120    avg_loss:0.189, val_acc:0.965]
Epoch [37/120    avg_loss:0.176, val_acc:0.969]
Epoch [38/120    avg_loss:0.164, val_acc:0.981]
Epoch [39/120    avg_loss:0.131, val_acc:0.979]
Epoch [40/120    avg_loss:0.141, val_acc:0.985]
Epoch [41/120    avg_loss:0.117, val_acc:0.963]
Epoch [42/120    avg_loss:0.192, val_acc:0.977]
Epoch [43/120    avg_loss:0.108, val_acc:0.981]
Epoch [44/120    avg_loss:0.116, val_acc:0.985]
Epoch [45/120    avg_loss:0.116, val_acc:0.973]
Epoch [46/120    avg_loss:0.138, val_acc:0.965]
Epoch [47/120    avg_loss:0.108, val_acc:0.958]
Epoch [48/120    avg_loss:0.122, val_acc:0.990]
Epoch [49/120    avg_loss:0.086, val_acc:0.983]
Epoch [50/120    avg_loss:0.113, val_acc:0.977]
Epoch [51/120    avg_loss:0.096, val_acc:0.975]
Epoch [52/120    avg_loss:0.078, val_acc:0.988]
Epoch [53/120    avg_loss:0.065, val_acc:0.985]
Epoch [54/120    avg_loss:0.090, val_acc:0.985]
Epoch [55/120    avg_loss:0.107, val_acc:0.979]
Epoch [56/120    avg_loss:0.105, val_acc:0.977]
Epoch [57/120    avg_loss:0.074, val_acc:0.990]
Epoch [58/120    avg_loss:0.080, val_acc:0.979]
Epoch [59/120    avg_loss:0.084, val_acc:0.965]
Epoch [60/120    avg_loss:0.090, val_acc:0.979]
Epoch [61/120    avg_loss:0.088, val_acc:0.988]
Epoch [62/120    avg_loss:0.082, val_acc:0.973]
Epoch [63/120    avg_loss:0.099, val_acc:0.988]
Epoch [64/120    avg_loss:0.115, val_acc:0.979]
Epoch [65/120    avg_loss:0.121, val_acc:0.988]
Epoch [66/120    avg_loss:0.087, val_acc:0.981]
Epoch [67/120    avg_loss:0.059, val_acc:0.990]
Epoch [68/120    avg_loss:0.056, val_acc:0.983]
Epoch [69/120    avg_loss:0.081, val_acc:0.977]
Epoch [70/120    avg_loss:0.059, val_acc:0.988]
Epoch [71/120    avg_loss:0.065, val_acc:0.990]
Epoch [72/120    avg_loss:0.042, val_acc:0.996]
Epoch [73/120    avg_loss:0.067, val_acc:0.927]
Epoch [74/120    avg_loss:0.064, val_acc:0.992]
Epoch [75/120    avg_loss:0.083, val_acc:0.983]
Epoch [76/120    avg_loss:0.088, val_acc:0.992]
Epoch [77/120    avg_loss:0.079, val_acc:0.985]
Epoch [78/120    avg_loss:0.048, val_acc:0.988]
Epoch [79/120    avg_loss:0.045, val_acc:0.983]
Epoch [80/120    avg_loss:0.089, val_acc:0.973]
Epoch [81/120    avg_loss:0.052, val_acc:0.979]
Epoch [82/120    avg_loss:0.066, val_acc:0.985]
Epoch [83/120    avg_loss:0.065, val_acc:0.988]
Epoch [84/120    avg_loss:0.057, val_acc:0.979]
Epoch [85/120    avg_loss:0.049, val_acc:0.990]
Epoch [86/120    avg_loss:0.035, val_acc:0.992]
Epoch [87/120    avg_loss:0.038, val_acc:0.994]
Epoch [88/120    avg_loss:0.025, val_acc:0.994]
Epoch [89/120    avg_loss:0.022, val_acc:0.994]
Epoch [90/120    avg_loss:0.026, val_acc:0.994]
Epoch [91/120    avg_loss:0.023, val_acc:0.994]
Epoch [92/120    avg_loss:0.026, val_acc:0.996]
Epoch [93/120    avg_loss:0.022, val_acc:0.996]
Epoch [94/120    avg_loss:0.019, val_acc:0.996]
Epoch [95/120    avg_loss:0.023, val_acc:0.996]
Epoch [96/120    avg_loss:0.023, val_acc:0.996]
Epoch [97/120    avg_loss:0.023, val_acc:0.996]
Epoch [98/120    avg_loss:0.028, val_acc:0.996]
Epoch [99/120    avg_loss:0.019, val_acc:0.996]
Epoch [100/120    avg_loss:0.019, val_acc:0.996]
Epoch [101/120    avg_loss:0.026, val_acc:0.996]
Epoch [102/120    avg_loss:0.029, val_acc:0.996]
Epoch [103/120    avg_loss:0.022, val_acc:0.996]
Epoch [104/120    avg_loss:0.015, val_acc:0.996]
Epoch [105/120    avg_loss:0.026, val_acc:0.996]
Epoch [106/120    avg_loss:0.021, val_acc:0.996]
Epoch [107/120    avg_loss:0.015, val_acc:0.996]
Epoch [108/120    avg_loss:0.018, val_acc:0.996]
Epoch [109/120    avg_loss:0.017, val_acc:0.994]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.021, val_acc:0.994]
Epoch [112/120    avg_loss:0.018, val_acc:0.996]
Epoch [113/120    avg_loss:0.015, val_acc:0.996]
Epoch [114/120    avg_loss:0.023, val_acc:0.994]
Epoch [115/120    avg_loss:0.016, val_acc:0.996]
Epoch [116/120    avg_loss:0.014, val_acc:0.996]
Epoch [117/120    avg_loss:0.019, val_acc:0.996]
Epoch [118/120    avg_loss:0.021, val_acc:0.996]
Epoch [119/120    avg_loss:0.020, val_acc:0.994]
Epoch [120/120    avg_loss:0.024, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 1.         0.98206278 1.         0.97747748 0.96666667
 1.         0.95555556 1.         1.         1.         0.99734748
 0.99779249 1.        ]

Kappa:
0.9952523276501299
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2af25e4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.498, val_acc:0.490]
Epoch [2/120    avg_loss:2.124, val_acc:0.567]
Epoch [3/120    avg_loss:1.893, val_acc:0.592]
Epoch [4/120    avg_loss:1.656, val_acc:0.660]
Epoch [5/120    avg_loss:1.460, val_acc:0.692]
Epoch [6/120    avg_loss:1.263, val_acc:0.740]
Epoch [7/120    avg_loss:1.149, val_acc:0.756]
Epoch [8/120    avg_loss:1.018, val_acc:0.790]
Epoch [9/120    avg_loss:0.910, val_acc:0.765]
Epoch [10/120    avg_loss:0.807, val_acc:0.906]
Epoch [11/120    avg_loss:0.749, val_acc:0.877]
Epoch [12/120    avg_loss:0.733, val_acc:0.877]
Epoch [13/120    avg_loss:0.633, val_acc:0.908]
Epoch [14/120    avg_loss:0.535, val_acc:0.925]
Epoch [15/120    avg_loss:0.501, val_acc:0.902]
Epoch [16/120    avg_loss:0.473, val_acc:0.935]
Epoch [17/120    avg_loss:0.419, val_acc:0.927]
Epoch [18/120    avg_loss:0.389, val_acc:0.944]
Epoch [19/120    avg_loss:0.392, val_acc:0.919]
Epoch [20/120    avg_loss:0.343, val_acc:0.940]
Epoch [21/120    avg_loss:0.351, val_acc:0.946]
Epoch [22/120    avg_loss:0.346, val_acc:0.927]
Epoch [23/120    avg_loss:0.298, val_acc:0.956]
Epoch [24/120    avg_loss:0.283, val_acc:0.952]
Epoch [25/120    avg_loss:0.298, val_acc:0.944]
Epoch [26/120    avg_loss:0.314, val_acc:0.927]
Epoch [27/120    avg_loss:0.316, val_acc:0.938]
Epoch [28/120    avg_loss:0.247, val_acc:0.952]
Epoch [29/120    avg_loss:0.264, val_acc:0.944]
Epoch [30/120    avg_loss:0.233, val_acc:0.954]
Epoch [31/120    avg_loss:0.236, val_acc:0.965]
Epoch [32/120    avg_loss:0.220, val_acc:0.956]
Epoch [33/120    avg_loss:0.230, val_acc:0.954]
Epoch [34/120    avg_loss:0.212, val_acc:0.942]
Epoch [35/120    avg_loss:0.185, val_acc:0.977]
Epoch [36/120    avg_loss:0.181, val_acc:0.969]
Epoch [37/120    avg_loss:0.170, val_acc:0.956]
Epoch [38/120    avg_loss:0.189, val_acc:0.965]
Epoch [39/120    avg_loss:0.178, val_acc:0.952]
Epoch [40/120    avg_loss:0.152, val_acc:0.950]
Epoch [41/120    avg_loss:0.151, val_acc:0.977]
Epoch [42/120    avg_loss:0.159, val_acc:0.975]
Epoch [43/120    avg_loss:0.129, val_acc:0.979]
Epoch [44/120    avg_loss:0.148, val_acc:0.973]
Epoch [45/120    avg_loss:0.156, val_acc:0.973]
Epoch [46/120    avg_loss:0.158, val_acc:0.969]
Epoch [47/120    avg_loss:0.142, val_acc:0.977]
Epoch [48/120    avg_loss:0.136, val_acc:0.981]
Epoch [49/120    avg_loss:0.137, val_acc:0.975]
Epoch [50/120    avg_loss:0.153, val_acc:0.971]
Epoch [51/120    avg_loss:0.141, val_acc:0.983]
Epoch [52/120    avg_loss:0.126, val_acc:0.973]
Epoch [53/120    avg_loss:0.109, val_acc:0.971]
Epoch [54/120    avg_loss:0.103, val_acc:0.979]
Epoch [55/120    avg_loss:0.084, val_acc:0.981]
Epoch [56/120    avg_loss:0.131, val_acc:0.973]
Epoch [57/120    avg_loss:0.111, val_acc:0.977]
Epoch [58/120    avg_loss:0.104, val_acc:0.985]
Epoch [59/120    avg_loss:0.100, val_acc:0.973]
Epoch [60/120    avg_loss:0.081, val_acc:0.979]
Epoch [61/120    avg_loss:0.093, val_acc:0.975]
Epoch [62/120    avg_loss:0.087, val_acc:0.983]
Epoch [63/120    avg_loss:0.107, val_acc:0.988]
Epoch [64/120    avg_loss:0.091, val_acc:0.988]
Epoch [65/120    avg_loss:0.076, val_acc:0.988]
Epoch [66/120    avg_loss:0.104, val_acc:0.977]
Epoch [67/120    avg_loss:0.105, val_acc:0.985]
Epoch [68/120    avg_loss:0.098, val_acc:0.988]
Epoch [69/120    avg_loss:0.070, val_acc:0.988]
Epoch [70/120    avg_loss:0.066, val_acc:0.988]
Epoch [71/120    avg_loss:0.064, val_acc:0.992]
Epoch [72/120    avg_loss:0.047, val_acc:0.981]
Epoch [73/120    avg_loss:0.067, val_acc:0.985]
Epoch [74/120    avg_loss:0.063, val_acc:0.985]
Epoch [75/120    avg_loss:0.049, val_acc:0.979]
Epoch [76/120    avg_loss:0.049, val_acc:0.992]
Epoch [77/120    avg_loss:0.040, val_acc:0.992]
Epoch [78/120    avg_loss:0.040, val_acc:0.992]
Epoch [79/120    avg_loss:0.045, val_acc:0.992]
Epoch [80/120    avg_loss:0.029, val_acc:0.992]
Epoch [81/120    avg_loss:0.042, val_acc:0.992]
Epoch [82/120    avg_loss:0.035, val_acc:0.990]
Epoch [83/120    avg_loss:0.034, val_acc:0.992]
Epoch [84/120    avg_loss:0.047, val_acc:0.992]
Epoch [85/120    avg_loss:0.040, val_acc:0.971]
Epoch [86/120    avg_loss:0.027, val_acc:0.979]
Epoch [87/120    avg_loss:0.037, val_acc:0.990]
Epoch [88/120    avg_loss:0.037, val_acc:0.990]
Epoch [89/120    avg_loss:0.029, val_acc:0.990]
Epoch [90/120    avg_loss:0.026, val_acc:0.992]
Epoch [91/120    avg_loss:0.023, val_acc:0.990]
Epoch [92/120    avg_loss:0.028, val_acc:0.992]
Epoch [93/120    avg_loss:0.021, val_acc:0.994]
Epoch [94/120    avg_loss:0.020, val_acc:0.994]
Epoch [95/120    avg_loss:0.021, val_acc:0.992]
Epoch [96/120    avg_loss:0.019, val_acc:0.994]
Epoch [97/120    avg_loss:0.017, val_acc:0.992]
Epoch [98/120    avg_loss:0.016, val_acc:0.990]
Epoch [99/120    avg_loss:0.018, val_acc:0.994]
Epoch [100/120    avg_loss:0.027, val_acc:0.992]
Epoch [101/120    avg_loss:0.021, val_acc:0.994]
Epoch [102/120    avg_loss:0.021, val_acc:0.990]
Epoch [103/120    avg_loss:0.021, val_acc:0.992]
Epoch [104/120    avg_loss:0.029, val_acc:0.992]
Epoch [105/120    avg_loss:0.024, val_acc:0.990]
Epoch [106/120    avg_loss:0.032, val_acc:0.977]
Epoch [107/120    avg_loss:0.040, val_acc:0.992]
Epoch [108/120    avg_loss:0.041, val_acc:0.992]
Epoch [109/120    avg_loss:0.062, val_acc:0.992]
Epoch [110/120    avg_loss:0.038, val_acc:0.992]
Epoch [111/120    avg_loss:0.026, val_acc:0.990]
Epoch [112/120    avg_loss:0.018, val_acc:0.990]
Epoch [113/120    avg_loss:0.018, val_acc:0.992]
Epoch [114/120    avg_loss:0.015, val_acc:0.992]
Epoch [115/120    avg_loss:0.013, val_acc:0.992]
Epoch [116/120    avg_loss:0.013, val_acc:0.992]
Epoch [117/120    avg_loss:0.012, val_acc:0.992]
Epoch [118/120    avg_loss:0.016, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         0.99545455 1.         0.96659243 0.94915254
 1.         0.98924731 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9957271519802743
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ca7343780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.479, val_acc:0.492]
Epoch [2/120    avg_loss:2.091, val_acc:0.585]
Epoch [3/120    avg_loss:1.827, val_acc:0.577]
Epoch [4/120    avg_loss:1.600, val_acc:0.658]
Epoch [5/120    avg_loss:1.404, val_acc:0.694]
Epoch [6/120    avg_loss:1.263, val_acc:0.677]
Epoch [7/120    avg_loss:1.100, val_acc:0.748]
Epoch [8/120    avg_loss:0.987, val_acc:0.840]
Epoch [9/120    avg_loss:0.887, val_acc:0.771]
Epoch [10/120    avg_loss:0.799, val_acc:0.846]
Epoch [11/120    avg_loss:0.765, val_acc:0.806]
Epoch [12/120    avg_loss:0.707, val_acc:0.879]
Epoch [13/120    avg_loss:0.587, val_acc:0.917]
Epoch [14/120    avg_loss:0.571, val_acc:0.869]
Epoch [15/120    avg_loss:0.499, val_acc:0.900]
Epoch [16/120    avg_loss:0.461, val_acc:0.919]
Epoch [17/120    avg_loss:0.432, val_acc:0.927]
Epoch [18/120    avg_loss:0.400, val_acc:0.933]
Epoch [19/120    avg_loss:0.421, val_acc:0.938]
Epoch [20/120    avg_loss:0.353, val_acc:0.925]
Epoch [21/120    avg_loss:0.375, val_acc:0.946]
Epoch [22/120    avg_loss:0.311, val_acc:0.917]
Epoch [23/120    avg_loss:0.299, val_acc:0.931]
Epoch [24/120    avg_loss:0.302, val_acc:0.952]
Epoch [25/120    avg_loss:0.286, val_acc:0.956]
Epoch [26/120    avg_loss:0.308, val_acc:0.923]
Epoch [27/120    avg_loss:0.268, val_acc:0.942]
Epoch [28/120    avg_loss:0.236, val_acc:0.946]
Epoch [29/120    avg_loss:0.218, val_acc:0.942]
Epoch [30/120    avg_loss:0.230, val_acc:0.929]
Epoch [31/120    avg_loss:0.255, val_acc:0.956]
Epoch [32/120    avg_loss:0.223, val_acc:0.935]
Epoch [33/120    avg_loss:0.213, val_acc:0.960]
Epoch [34/120    avg_loss:0.212, val_acc:0.965]
Epoch [35/120    avg_loss:0.160, val_acc:0.963]
Epoch [36/120    avg_loss:0.154, val_acc:0.954]
Epoch [37/120    avg_loss:0.185, val_acc:0.956]
Epoch [38/120    avg_loss:0.174, val_acc:0.971]
Epoch [39/120    avg_loss:0.170, val_acc:0.973]
Epoch [40/120    avg_loss:0.140, val_acc:0.960]
Epoch [41/120    avg_loss:0.157, val_acc:0.958]
Epoch [42/120    avg_loss:0.161, val_acc:0.956]
Epoch [43/120    avg_loss:0.156, val_acc:0.967]
Epoch [44/120    avg_loss:0.145, val_acc:0.969]
Epoch [45/120    avg_loss:0.132, val_acc:0.952]
Epoch [46/120    avg_loss:0.104, val_acc:0.971]
Epoch [47/120    avg_loss:0.122, val_acc:0.971]
Epoch [48/120    avg_loss:0.118, val_acc:0.963]
Epoch [49/120    avg_loss:0.140, val_acc:0.954]
Epoch [50/120    avg_loss:0.130, val_acc:0.963]
Epoch [51/120    avg_loss:0.146, val_acc:0.965]
Epoch [52/120    avg_loss:0.123, val_acc:0.965]
Epoch [53/120    avg_loss:0.100, val_acc:0.971]
Epoch [54/120    avg_loss:0.093, val_acc:0.975]
Epoch [55/120    avg_loss:0.090, val_acc:0.973]
Epoch [56/120    avg_loss:0.073, val_acc:0.973]
Epoch [57/120    avg_loss:0.074, val_acc:0.973]
Epoch [58/120    avg_loss:0.081, val_acc:0.975]
Epoch [59/120    avg_loss:0.096, val_acc:0.975]
Epoch [60/120    avg_loss:0.084, val_acc:0.973]
Epoch [61/120    avg_loss:0.072, val_acc:0.973]
Epoch [62/120    avg_loss:0.073, val_acc:0.971]
Epoch [63/120    avg_loss:0.067, val_acc:0.971]
Epoch [64/120    avg_loss:0.073, val_acc:0.975]
Epoch [65/120    avg_loss:0.065, val_acc:0.975]
Epoch [66/120    avg_loss:0.065, val_acc:0.975]
Epoch [67/120    avg_loss:0.079, val_acc:0.975]
Epoch [68/120    avg_loss:0.056, val_acc:0.981]
Epoch [69/120    avg_loss:0.062, val_acc:0.979]
Epoch [70/120    avg_loss:0.059, val_acc:0.977]
Epoch [71/120    avg_loss:0.053, val_acc:0.977]
Epoch [72/120    avg_loss:0.057, val_acc:0.973]
Epoch [73/120    avg_loss:0.057, val_acc:0.975]
Epoch [74/120    avg_loss:0.060, val_acc:0.981]
Epoch [75/120    avg_loss:0.084, val_acc:0.983]
Epoch [76/120    avg_loss:0.076, val_acc:0.981]
Epoch [77/120    avg_loss:0.059, val_acc:0.981]
Epoch [78/120    avg_loss:0.057, val_acc:0.981]
Epoch [79/120    avg_loss:0.056, val_acc:0.981]
Epoch [80/120    avg_loss:0.056, val_acc:0.981]
Epoch [81/120    avg_loss:0.080, val_acc:0.981]
Epoch [82/120    avg_loss:0.050, val_acc:0.979]
Epoch [83/120    avg_loss:0.059, val_acc:0.983]
Epoch [84/120    avg_loss:0.048, val_acc:0.983]
Epoch [85/120    avg_loss:0.058, val_acc:0.983]
Epoch [86/120    avg_loss:0.059, val_acc:0.981]
Epoch [87/120    avg_loss:0.053, val_acc:0.983]
Epoch [88/120    avg_loss:0.056, val_acc:0.981]
Epoch [89/120    avg_loss:0.057, val_acc:0.985]
Epoch [90/120    avg_loss:0.046, val_acc:0.985]
Epoch [91/120    avg_loss:0.058, val_acc:0.983]
Epoch [92/120    avg_loss:0.051, val_acc:0.983]
Epoch [93/120    avg_loss:0.060, val_acc:0.983]
Epoch [94/120    avg_loss:0.048, val_acc:0.988]
Epoch [95/120    avg_loss:0.069, val_acc:0.985]
Epoch [96/120    avg_loss:0.067, val_acc:0.983]
Epoch [97/120    avg_loss:0.061, val_acc:0.985]
Epoch [98/120    avg_loss:0.049, val_acc:0.981]
Epoch [99/120    avg_loss:0.058, val_acc:0.983]
Epoch [100/120    avg_loss:0.043, val_acc:0.983]
Epoch [101/120    avg_loss:0.049, val_acc:0.983]
Epoch [102/120    avg_loss:0.057, val_acc:0.985]
Epoch [103/120    avg_loss:0.050, val_acc:0.983]
Epoch [104/120    avg_loss:0.043, val_acc:0.981]
Epoch [105/120    avg_loss:0.039, val_acc:0.981]
Epoch [106/120    avg_loss:0.044, val_acc:0.981]
Epoch [107/120    avg_loss:0.051, val_acc:0.983]
Epoch [108/120    avg_loss:0.059, val_acc:0.983]
Epoch [109/120    avg_loss:0.047, val_acc:0.983]
Epoch [110/120    avg_loss:0.043, val_acc:0.983]
Epoch [111/120    avg_loss:0.052, val_acc:0.983]
Epoch [112/120    avg_loss:0.047, val_acc:0.983]
Epoch [113/120    avg_loss:0.057, val_acc:0.981]
Epoch [114/120    avg_loss:0.059, val_acc:0.981]
Epoch [115/120    avg_loss:0.038, val_acc:0.981]
Epoch [116/120    avg_loss:0.041, val_acc:0.981]
Epoch [117/120    avg_loss:0.043, val_acc:0.981]
Epoch [118/120    avg_loss:0.049, val_acc:0.983]
Epoch [119/120    avg_loss:0.051, val_acc:0.981]
Epoch [120/120    avg_loss:0.043, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98648649 1.         0.93304536 0.88967972
 1.         0.96703297 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9909787905910132
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ba71207f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.497, val_acc:0.369]
Epoch [2/120    avg_loss:2.159, val_acc:0.525]
Epoch [3/120    avg_loss:1.910, val_acc:0.581]
Epoch [4/120    avg_loss:1.711, val_acc:0.688]
Epoch [5/120    avg_loss:1.484, val_acc:0.669]
Epoch [6/120    avg_loss:1.296, val_acc:0.704]
Epoch [7/120    avg_loss:1.109, val_acc:0.735]
Epoch [8/120    avg_loss:0.981, val_acc:0.769]
Epoch [9/120    avg_loss:0.895, val_acc:0.790]
Epoch [10/120    avg_loss:0.787, val_acc:0.819]
Epoch [11/120    avg_loss:0.707, val_acc:0.860]
Epoch [12/120    avg_loss:0.616, val_acc:0.871]
Epoch [13/120    avg_loss:0.576, val_acc:0.881]
Epoch [14/120    avg_loss:0.551, val_acc:0.912]
Epoch [15/120    avg_loss:0.530, val_acc:0.933]
Epoch [16/120    avg_loss:0.439, val_acc:0.900]
Epoch [17/120    avg_loss:0.468, val_acc:0.923]
Epoch [18/120    avg_loss:0.435, val_acc:0.917]
Epoch [19/120    avg_loss:0.403, val_acc:0.912]
Epoch [20/120    avg_loss:0.353, val_acc:0.948]
Epoch [21/120    avg_loss:0.334, val_acc:0.948]
Epoch [22/120    avg_loss:0.306, val_acc:0.952]
Epoch [23/120    avg_loss:0.347, val_acc:0.940]
Epoch [24/120    avg_loss:0.285, val_acc:0.956]
Epoch [25/120    avg_loss:0.270, val_acc:0.940]
Epoch [26/120    avg_loss:0.297, val_acc:0.946]
Epoch [27/120    avg_loss:0.267, val_acc:0.963]
Epoch [28/120    avg_loss:0.240, val_acc:0.946]
Epoch [29/120    avg_loss:0.265, val_acc:0.960]
Epoch [30/120    avg_loss:0.233, val_acc:0.946]
Epoch [31/120    avg_loss:0.287, val_acc:0.935]
Epoch [32/120    avg_loss:0.243, val_acc:0.948]
Epoch [33/120    avg_loss:0.249, val_acc:0.967]
Epoch [34/120    avg_loss:0.278, val_acc:0.940]
Epoch [35/120    avg_loss:0.295, val_acc:0.931]
Epoch [36/120    avg_loss:0.225, val_acc:0.950]
Epoch [37/120    avg_loss:0.215, val_acc:0.946]
Epoch [38/120    avg_loss:0.206, val_acc:0.956]
Epoch [39/120    avg_loss:0.185, val_acc:0.977]
Epoch [40/120    avg_loss:0.150, val_acc:0.967]
Epoch [41/120    avg_loss:0.148, val_acc:0.946]
Epoch [42/120    avg_loss:0.164, val_acc:0.971]
Epoch [43/120    avg_loss:0.169, val_acc:0.973]
Epoch [44/120    avg_loss:0.126, val_acc:0.969]
Epoch [45/120    avg_loss:0.148, val_acc:0.971]
Epoch [46/120    avg_loss:0.143, val_acc:0.977]
Epoch [47/120    avg_loss:0.117, val_acc:0.969]
Epoch [48/120    avg_loss:0.102, val_acc:0.977]
Epoch [49/120    avg_loss:0.131, val_acc:0.965]
Epoch [50/120    avg_loss:0.123, val_acc:0.971]
Epoch [51/120    avg_loss:0.179, val_acc:0.969]
Epoch [52/120    avg_loss:0.156, val_acc:0.975]
Epoch [53/120    avg_loss:0.109, val_acc:0.954]
Epoch [54/120    avg_loss:0.114, val_acc:0.969]
Epoch [55/120    avg_loss:0.133, val_acc:0.969]
Epoch [56/120    avg_loss:0.124, val_acc:0.969]
Epoch [57/120    avg_loss:0.091, val_acc:0.979]
Epoch [58/120    avg_loss:0.114, val_acc:0.969]
Epoch [59/120    avg_loss:0.107, val_acc:0.981]
Epoch [60/120    avg_loss:0.085, val_acc:0.973]
Epoch [61/120    avg_loss:0.075, val_acc:0.985]
Epoch [62/120    avg_loss:0.090, val_acc:0.967]
Epoch [63/120    avg_loss:0.076, val_acc:0.975]
Epoch [64/120    avg_loss:0.069, val_acc:0.971]
Epoch [65/120    avg_loss:0.062, val_acc:0.981]
Epoch [66/120    avg_loss:0.097, val_acc:0.988]
Epoch [67/120    avg_loss:0.071, val_acc:0.981]
Epoch [68/120    avg_loss:0.081, val_acc:0.979]
Epoch [69/120    avg_loss:0.074, val_acc:0.981]
Epoch [70/120    avg_loss:0.060, val_acc:0.979]
Epoch [71/120    avg_loss:0.056, val_acc:0.981]
Epoch [72/120    avg_loss:0.050, val_acc:0.985]
Epoch [73/120    avg_loss:0.070, val_acc:0.988]
Epoch [74/120    avg_loss:0.096, val_acc:0.979]
Epoch [75/120    avg_loss:0.116, val_acc:0.969]
Epoch [76/120    avg_loss:0.104, val_acc:0.983]
Epoch [77/120    avg_loss:0.051, val_acc:0.981]
Epoch [78/120    avg_loss:0.055, val_acc:0.992]
Epoch [79/120    avg_loss:0.046, val_acc:0.981]
Epoch [80/120    avg_loss:0.056, val_acc:0.988]
Epoch [81/120    avg_loss:0.042, val_acc:0.983]
Epoch [82/120    avg_loss:0.043, val_acc:0.983]
Epoch [83/120    avg_loss:0.034, val_acc:0.983]
Epoch [84/120    avg_loss:0.055, val_acc:0.979]
Epoch [85/120    avg_loss:0.060, val_acc:0.973]
Epoch [86/120    avg_loss:0.129, val_acc:0.983]
Epoch [87/120    avg_loss:0.069, val_acc:0.985]
Epoch [88/120    avg_loss:0.066, val_acc:0.975]
Epoch [89/120    avg_loss:0.056, val_acc:0.988]
Epoch [90/120    avg_loss:0.044, val_acc:0.996]
Epoch [91/120    avg_loss:0.036, val_acc:0.992]
Epoch [92/120    avg_loss:0.028, val_acc:0.992]
Epoch [93/120    avg_loss:0.048, val_acc:0.988]
Epoch [94/120    avg_loss:0.047, val_acc:0.983]
Epoch [95/120    avg_loss:0.040, val_acc:0.981]
Epoch [96/120    avg_loss:0.052, val_acc:0.990]
Epoch [97/120    avg_loss:0.031, val_acc:0.996]
Epoch [98/120    avg_loss:0.023, val_acc:0.992]
Epoch [99/120    avg_loss:0.020, val_acc:0.992]
Epoch [100/120    avg_loss:0.053, val_acc:0.990]
Epoch [101/120    avg_loss:0.065, val_acc:0.990]
Epoch [102/120    avg_loss:0.049, val_acc:0.988]
Epoch [103/120    avg_loss:0.024, val_acc:0.992]
Epoch [104/120    avg_loss:0.025, val_acc:0.981]
Epoch [105/120    avg_loss:0.033, val_acc:0.990]
Epoch [106/120    avg_loss:0.148, val_acc:0.965]
Epoch [107/120    avg_loss:0.136, val_acc:0.992]
Epoch [108/120    avg_loss:0.085, val_acc:0.988]
Epoch [109/120    avg_loss:0.060, val_acc:0.981]
Epoch [110/120    avg_loss:0.037, val_acc:0.994]
Epoch [111/120    avg_loss:0.027, val_acc:0.994]
Epoch [112/120    avg_loss:0.020, val_acc:0.994]
Epoch [113/120    avg_loss:0.026, val_acc:0.992]
Epoch [114/120    avg_loss:0.019, val_acc:0.992]
Epoch [115/120    avg_loss:0.017, val_acc:0.992]
Epoch [116/120    avg_loss:0.024, val_acc:0.994]
Epoch [117/120    avg_loss:0.021, val_acc:0.994]
Epoch [118/120    avg_loss:0.021, val_acc:0.994]
Epoch [119/120    avg_loss:0.019, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  10   0   0   0   0   0   0   1   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 1.         0.98426966 1.         0.96860987 0.95622896
 1.         0.96132597 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.995014856476812
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c1ac4f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.534, val_acc:0.477]
Epoch [2/120    avg_loss:2.111, val_acc:0.500]
Epoch [3/120    avg_loss:1.847, val_acc:0.596]
Epoch [4/120    avg_loss:1.616, val_acc:0.631]
Epoch [5/120    avg_loss:1.385, val_acc:0.675]
Epoch [6/120    avg_loss:1.201, val_acc:0.742]
Epoch [7/120    avg_loss:1.040, val_acc:0.758]
Epoch [8/120    avg_loss:0.913, val_acc:0.810]
Epoch [9/120    avg_loss:0.835, val_acc:0.802]
Epoch [10/120    avg_loss:0.748, val_acc:0.867]
Epoch [11/120    avg_loss:0.724, val_acc:0.896]
Epoch [12/120    avg_loss:0.628, val_acc:0.910]
Epoch [13/120    avg_loss:0.595, val_acc:0.938]
Epoch [14/120    avg_loss:0.514, val_acc:0.944]
Epoch [15/120    avg_loss:0.464, val_acc:0.929]
Epoch [16/120    avg_loss:0.439, val_acc:0.954]
Epoch [17/120    avg_loss:0.362, val_acc:0.919]
Epoch [18/120    avg_loss:0.423, val_acc:0.958]
Epoch [19/120    avg_loss:0.396, val_acc:0.958]
Epoch [20/120    avg_loss:0.403, val_acc:0.931]
Epoch [21/120    avg_loss:0.327, val_acc:0.938]
Epoch [22/120    avg_loss:0.284, val_acc:0.963]
Epoch [23/120    avg_loss:0.259, val_acc:0.965]
Epoch [24/120    avg_loss:0.273, val_acc:0.963]
Epoch [25/120    avg_loss:0.242, val_acc:0.948]
Epoch [26/120    avg_loss:0.227, val_acc:0.967]
Epoch [27/120    avg_loss:0.237, val_acc:0.971]
Epoch [28/120    avg_loss:0.189, val_acc:0.965]
Epoch [29/120    avg_loss:0.192, val_acc:0.969]
Epoch [30/120    avg_loss:0.198, val_acc:0.965]
Epoch [31/120    avg_loss:0.159, val_acc:0.956]
Epoch [32/120    avg_loss:0.215, val_acc:0.958]
Epoch [33/120    avg_loss:0.181, val_acc:0.977]
Epoch [34/120    avg_loss:0.145, val_acc:0.985]
Epoch [35/120    avg_loss:0.128, val_acc:0.983]
Epoch [36/120    avg_loss:0.105, val_acc:0.983]
Epoch [37/120    avg_loss:0.111, val_acc:0.981]
Epoch [38/120    avg_loss:0.146, val_acc:0.973]
Epoch [39/120    avg_loss:0.135, val_acc:0.973]
Epoch [40/120    avg_loss:0.140, val_acc:0.979]
Epoch [41/120    avg_loss:0.160, val_acc:0.979]
Epoch [42/120    avg_loss:0.126, val_acc:0.975]
Epoch [43/120    avg_loss:0.095, val_acc:0.981]
Epoch [44/120    avg_loss:0.092, val_acc:0.979]
Epoch [45/120    avg_loss:0.087, val_acc:0.977]
Epoch [46/120    avg_loss:0.100, val_acc:0.977]
Epoch [47/120    avg_loss:0.073, val_acc:0.988]
Epoch [48/120    avg_loss:0.083, val_acc:0.983]
Epoch [49/120    avg_loss:0.080, val_acc:0.988]
Epoch [50/120    avg_loss:0.069, val_acc:0.981]
Epoch [51/120    avg_loss:0.084, val_acc:0.990]
Epoch [52/120    avg_loss:0.053, val_acc:0.985]
Epoch [53/120    avg_loss:0.054, val_acc:0.988]
Epoch [54/120    avg_loss:0.058, val_acc:0.988]
Epoch [55/120    avg_loss:0.103, val_acc:0.981]
Epoch [56/120    avg_loss:0.082, val_acc:0.973]
Epoch [57/120    avg_loss:0.105, val_acc:0.975]
Epoch [58/120    avg_loss:0.119, val_acc:0.981]
Epoch [59/120    avg_loss:0.099, val_acc:0.979]
Epoch [60/120    avg_loss:0.074, val_acc:0.990]
Epoch [61/120    avg_loss:0.064, val_acc:0.988]
Epoch [62/120    avg_loss:0.082, val_acc:0.979]
Epoch [63/120    avg_loss:0.104, val_acc:0.967]
Epoch [64/120    avg_loss:0.102, val_acc:0.979]
Epoch [65/120    avg_loss:0.073, val_acc:0.983]
Epoch [66/120    avg_loss:0.063, val_acc:0.985]
Epoch [67/120    avg_loss:0.044, val_acc:0.985]
Epoch [68/120    avg_loss:0.057, val_acc:0.983]
Epoch [69/120    avg_loss:0.060, val_acc:0.990]
Epoch [70/120    avg_loss:0.054, val_acc:0.983]
Epoch [71/120    avg_loss:0.049, val_acc:0.983]
Epoch [72/120    avg_loss:0.046, val_acc:0.988]
Epoch [73/120    avg_loss:0.047, val_acc:0.975]
Epoch [74/120    avg_loss:0.051, val_acc:0.990]
Epoch [75/120    avg_loss:0.039, val_acc:0.992]
Epoch [76/120    avg_loss:0.035, val_acc:0.990]
Epoch [77/120    avg_loss:0.035, val_acc:0.990]
Epoch [78/120    avg_loss:0.066, val_acc:0.983]
Epoch [79/120    avg_loss:0.046, val_acc:0.988]
Epoch [80/120    avg_loss:0.050, val_acc:0.996]
Epoch [81/120    avg_loss:0.047, val_acc:0.985]
Epoch [82/120    avg_loss:0.025, val_acc:0.992]
Epoch [83/120    avg_loss:0.023, val_acc:0.992]
Epoch [84/120    avg_loss:0.025, val_acc:0.992]
Epoch [85/120    avg_loss:0.026, val_acc:0.996]
Epoch [86/120    avg_loss:0.031, val_acc:0.994]
Epoch [87/120    avg_loss:0.038, val_acc:0.981]
Epoch [88/120    avg_loss:0.027, val_acc:0.985]
Epoch [89/120    avg_loss:0.037, val_acc:0.992]
Epoch [90/120    avg_loss:0.105, val_acc:0.921]
Epoch [91/120    avg_loss:0.201, val_acc:0.940]
Epoch [92/120    avg_loss:0.148, val_acc:0.981]
Epoch [93/120    avg_loss:0.093, val_acc:0.979]
Epoch [94/120    avg_loss:0.070, val_acc:0.988]
Epoch [95/120    avg_loss:0.052, val_acc:0.979]
Epoch [96/120    avg_loss:0.053, val_acc:0.981]
Epoch [97/120    avg_loss:0.052, val_acc:0.981]
Epoch [98/120    avg_loss:0.035, val_acc:0.990]
Epoch [99/120    avg_loss:0.017, val_acc:0.990]
Epoch [100/120    avg_loss:0.029, val_acc:0.990]
Epoch [101/120    avg_loss:0.018, val_acc:0.994]
Epoch [102/120    avg_loss:0.027, val_acc:0.996]
Epoch [103/120    avg_loss:0.019, val_acc:0.996]
Epoch [104/120    avg_loss:0.019, val_acc:0.992]
Epoch [105/120    avg_loss:0.020, val_acc:0.992]
Epoch [106/120    avg_loss:0.023, val_acc:0.994]
Epoch [107/120    avg_loss:0.018, val_acc:0.994]
Epoch [108/120    avg_loss:0.019, val_acc:0.994]
Epoch [109/120    avg_loss:0.016, val_acc:0.996]
Epoch [110/120    avg_loss:0.019, val_acc:0.996]
Epoch [111/120    avg_loss:0.016, val_acc:0.996]
Epoch [112/120    avg_loss:0.017, val_acc:0.996]
Epoch [113/120    avg_loss:0.017, val_acc:0.996]
Epoch [114/120    avg_loss:0.020, val_acc:0.996]
Epoch [115/120    avg_loss:0.026, val_acc:0.994]
Epoch [116/120    avg_loss:0.017, val_acc:0.996]
Epoch [117/120    avg_loss:0.016, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.020, val_acc:0.996]
Epoch [120/120    avg_loss:0.016, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.68017057569296

F1 scores:
[       nan 1.         0.99319728 1.         0.97321429 0.95945946
 1.         0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.996439299264218
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc972c2e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.489, val_acc:0.375]
Epoch [2/120    avg_loss:2.111, val_acc:0.529]
Epoch [3/120    avg_loss:1.837, val_acc:0.656]
Epoch [4/120    avg_loss:1.641, val_acc:0.685]
Epoch [5/120    avg_loss:1.439, val_acc:0.696]
Epoch [6/120    avg_loss:1.283, val_acc:0.748]
Epoch [7/120    avg_loss:1.109, val_acc:0.771]
Epoch [8/120    avg_loss:1.001, val_acc:0.787]
Epoch [9/120    avg_loss:0.898, val_acc:0.796]
Epoch [10/120    avg_loss:0.793, val_acc:0.802]
Epoch [11/120    avg_loss:0.709, val_acc:0.842]
Epoch [12/120    avg_loss:0.624, val_acc:0.846]
Epoch [13/120    avg_loss:0.635, val_acc:0.915]
Epoch [14/120    avg_loss:0.565, val_acc:0.912]
Epoch [15/120    avg_loss:0.519, val_acc:0.944]
Epoch [16/120    avg_loss:0.441, val_acc:0.940]
Epoch [17/120    avg_loss:0.439, val_acc:0.931]
Epoch [18/120    avg_loss:0.401, val_acc:0.956]
Epoch [19/120    avg_loss:0.406, val_acc:0.956]
Epoch [20/120    avg_loss:0.336, val_acc:0.944]
Epoch [21/120    avg_loss:0.306, val_acc:0.967]
Epoch [22/120    avg_loss:0.303, val_acc:0.969]
Epoch [23/120    avg_loss:0.292, val_acc:0.940]
Epoch [24/120    avg_loss:0.316, val_acc:0.969]
Epoch [25/120    avg_loss:0.313, val_acc:0.931]
Epoch [26/120    avg_loss:0.332, val_acc:0.910]
Epoch [27/120    avg_loss:0.300, val_acc:0.954]
Epoch [28/120    avg_loss:0.364, val_acc:0.915]
Epoch [29/120    avg_loss:0.313, val_acc:0.967]
Epoch [30/120    avg_loss:0.306, val_acc:0.963]
Epoch [31/120    avg_loss:0.223, val_acc:0.954]
Epoch [32/120    avg_loss:0.226, val_acc:0.975]
Epoch [33/120    avg_loss:0.208, val_acc:0.960]
Epoch [34/120    avg_loss:0.189, val_acc:0.971]
Epoch [35/120    avg_loss:0.169, val_acc:0.969]
Epoch [36/120    avg_loss:0.174, val_acc:0.969]
Epoch [37/120    avg_loss:0.158, val_acc:0.981]
Epoch [38/120    avg_loss:0.204, val_acc:0.956]
Epoch [39/120    avg_loss:0.213, val_acc:0.960]
Epoch [40/120    avg_loss:0.189, val_acc:0.954]
Epoch [41/120    avg_loss:0.192, val_acc:0.958]
Epoch [42/120    avg_loss:0.244, val_acc:0.969]
Epoch [43/120    avg_loss:0.181, val_acc:0.954]
Epoch [44/120    avg_loss:0.164, val_acc:0.977]
Epoch [45/120    avg_loss:0.121, val_acc:0.973]
Epoch [46/120    avg_loss:0.116, val_acc:0.977]
Epoch [47/120    avg_loss:0.126, val_acc:0.944]
Epoch [48/120    avg_loss:0.254, val_acc:0.963]
Epoch [49/120    avg_loss:0.210, val_acc:0.977]
Epoch [50/120    avg_loss:0.152, val_acc:0.981]
Epoch [51/120    avg_loss:0.156, val_acc:0.971]
Epoch [52/120    avg_loss:0.136, val_acc:0.965]
Epoch [53/120    avg_loss:0.146, val_acc:0.950]
Epoch [54/120    avg_loss:0.155, val_acc:0.988]
Epoch [55/120    avg_loss:0.130, val_acc:0.973]
Epoch [56/120    avg_loss:0.092, val_acc:0.983]
Epoch [57/120    avg_loss:0.095, val_acc:0.988]
Epoch [58/120    avg_loss:0.102, val_acc:0.992]
Epoch [59/120    avg_loss:0.082, val_acc:0.990]
Epoch [60/120    avg_loss:0.069, val_acc:0.990]
Epoch [61/120    avg_loss:0.077, val_acc:0.994]
Epoch [62/120    avg_loss:0.170, val_acc:0.929]
Epoch [63/120    avg_loss:0.203, val_acc:0.985]
Epoch [64/120    avg_loss:0.138, val_acc:0.979]
Epoch [65/120    avg_loss:0.081, val_acc:0.988]
Epoch [66/120    avg_loss:0.071, val_acc:0.983]
Epoch [67/120    avg_loss:0.098, val_acc:0.992]
Epoch [68/120    avg_loss:0.086, val_acc:0.983]
Epoch [69/120    avg_loss:0.068, val_acc:0.971]
Epoch [70/120    avg_loss:0.074, val_acc:0.983]
Epoch [71/120    avg_loss:0.126, val_acc:0.965]
Epoch [72/120    avg_loss:0.104, val_acc:0.981]
Epoch [73/120    avg_loss:0.073, val_acc:0.990]
Epoch [74/120    avg_loss:0.057, val_acc:0.994]
Epoch [75/120    avg_loss:0.055, val_acc:0.996]
Epoch [76/120    avg_loss:0.045, val_acc:0.992]
Epoch [77/120    avg_loss:0.045, val_acc:0.996]
Epoch [78/120    avg_loss:0.036, val_acc:0.992]
Epoch [79/120    avg_loss:0.031, val_acc:0.998]
Epoch [80/120    avg_loss:0.019, val_acc:0.994]
Epoch [81/120    avg_loss:0.052, val_acc:0.983]
Epoch [82/120    avg_loss:0.072, val_acc:0.990]
Epoch [83/120    avg_loss:0.055, val_acc:0.996]
Epoch [84/120    avg_loss:0.045, val_acc:0.996]
Epoch [85/120    avg_loss:0.051, val_acc:0.990]
Epoch [86/120    avg_loss:0.058, val_acc:0.985]
Epoch [87/120    avg_loss:0.056, val_acc:0.981]
Epoch [88/120    avg_loss:0.042, val_acc:0.992]
Epoch [89/120    avg_loss:0.033, val_acc:0.988]
Epoch [90/120    avg_loss:0.045, val_acc:0.988]
Epoch [91/120    avg_loss:0.048, val_acc:0.996]
Epoch [92/120    avg_loss:0.038, val_acc:0.994]
Epoch [93/120    avg_loss:0.031, val_acc:0.992]
Epoch [94/120    avg_loss:0.027, val_acc:0.994]
Epoch [95/120    avg_loss:0.020, val_acc:0.994]
Epoch [96/120    avg_loss:0.023, val_acc:0.994]
Epoch [97/120    avg_loss:0.022, val_acc:0.994]
Epoch [98/120    avg_loss:0.020, val_acc:0.994]
Epoch [99/120    avg_loss:0.019, val_acc:0.994]
Epoch [100/120    avg_loss:0.018, val_acc:0.996]
Epoch [101/120    avg_loss:0.022, val_acc:0.996]
Epoch [102/120    avg_loss:0.017, val_acc:0.996]
Epoch [103/120    avg_loss:0.017, val_acc:0.996]
Epoch [104/120    avg_loss:0.016, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.016, val_acc:0.996]
Epoch [107/120    avg_loss:0.020, val_acc:0.996]
Epoch [108/120    avg_loss:0.022, val_acc:0.996]
Epoch [109/120    avg_loss:0.025, val_acc:0.996]
Epoch [110/120    avg_loss:0.016, val_acc:0.996]
Epoch [111/120    avg_loss:0.019, val_acc:0.996]
Epoch [112/120    avg_loss:0.014, val_acc:0.996]
Epoch [113/120    avg_loss:0.018, val_acc:0.996]
Epoch [114/120    avg_loss:0.020, val_acc:0.996]
Epoch [115/120    avg_loss:0.017, val_acc:0.996]
Epoch [116/120    avg_loss:0.022, val_acc:0.996]
Epoch [117/120    avg_loss:0.015, val_acc:0.996]
Epoch [118/120    avg_loss:0.017, val_acc:0.996]
Epoch [119/120    avg_loss:0.016, val_acc:0.996]
Epoch [120/120    avg_loss:0.022, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         0.98871332 1.         0.97777778 0.96928328
 1.         0.9726776  1.         1.         1.         0.98950131
 0.98998888 1.        ]

Kappa:
0.9945402273688831
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c388067f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.542, val_acc:0.402]
Epoch [2/120    avg_loss:2.087, val_acc:0.577]
Epoch [3/120    avg_loss:1.815, val_acc:0.633]
Epoch [4/120    avg_loss:1.552, val_acc:0.669]
Epoch [5/120    avg_loss:1.344, val_acc:0.690]
Epoch [6/120    avg_loss:1.180, val_acc:0.704]
Epoch [7/120    avg_loss:1.050, val_acc:0.717]
Epoch [8/120    avg_loss:0.978, val_acc:0.719]
Epoch [9/120    avg_loss:0.882, val_acc:0.748]
Epoch [10/120    avg_loss:0.827, val_acc:0.746]
Epoch [11/120    avg_loss:0.755, val_acc:0.783]
Epoch [12/120    avg_loss:0.681, val_acc:0.785]
Epoch [13/120    avg_loss:0.629, val_acc:0.848]
Epoch [14/120    avg_loss:0.556, val_acc:0.829]
Epoch [15/120    avg_loss:0.564, val_acc:0.896]
Epoch [16/120    avg_loss:0.475, val_acc:0.915]
Epoch [17/120    avg_loss:0.448, val_acc:0.942]
Epoch [18/120    avg_loss:0.444, val_acc:0.919]
Epoch [19/120    avg_loss:0.418, val_acc:0.910]
Epoch [20/120    avg_loss:0.398, val_acc:0.935]
Epoch [21/120    avg_loss:0.397, val_acc:0.894]
Epoch [22/120    avg_loss:0.389, val_acc:0.879]
Epoch [23/120    avg_loss:0.362, val_acc:0.952]
Epoch [24/120    avg_loss:0.319, val_acc:0.950]
Epoch [25/120    avg_loss:0.253, val_acc:0.944]
Epoch [26/120    avg_loss:0.300, val_acc:0.948]
Epoch [27/120    avg_loss:0.269, val_acc:0.944]
Epoch [28/120    avg_loss:0.208, val_acc:0.958]
Epoch [29/120    avg_loss:0.244, val_acc:0.965]
Epoch [30/120    avg_loss:0.269, val_acc:0.948]
Epoch [31/120    avg_loss:0.259, val_acc:0.952]
Epoch [32/120    avg_loss:0.216, val_acc:0.952]
Epoch [33/120    avg_loss:0.206, val_acc:0.942]
Epoch [34/120    avg_loss:0.205, val_acc:0.958]
Epoch [35/120    avg_loss:0.190, val_acc:0.958]
Epoch [36/120    avg_loss:0.179, val_acc:0.950]
Epoch [37/120    avg_loss:0.209, val_acc:0.950]
Epoch [38/120    avg_loss:0.158, val_acc:0.969]
Epoch [39/120    avg_loss:0.152, val_acc:0.963]
Epoch [40/120    avg_loss:0.219, val_acc:0.912]
Epoch [41/120    avg_loss:0.239, val_acc:0.967]
Epoch [42/120    avg_loss:0.168, val_acc:0.963]
Epoch [43/120    avg_loss:0.166, val_acc:0.954]
Epoch [44/120    avg_loss:0.119, val_acc:0.975]
Epoch [45/120    avg_loss:0.187, val_acc:0.935]
Epoch [46/120    avg_loss:0.186, val_acc:0.958]
Epoch [47/120    avg_loss:0.164, val_acc:0.965]
Epoch [48/120    avg_loss:0.142, val_acc:0.975]
Epoch [49/120    avg_loss:0.108, val_acc:0.975]
Epoch [50/120    avg_loss:0.101, val_acc:0.979]
Epoch [51/120    avg_loss:0.097, val_acc:0.975]
Epoch [52/120    avg_loss:0.127, val_acc:0.965]
Epoch [53/120    avg_loss:0.145, val_acc:0.973]
Epoch [54/120    avg_loss:0.086, val_acc:0.981]
Epoch [55/120    avg_loss:0.088, val_acc:0.967]
Epoch [56/120    avg_loss:0.104, val_acc:0.973]
Epoch [57/120    avg_loss:0.089, val_acc:0.981]
Epoch [58/120    avg_loss:0.076, val_acc:0.981]
Epoch [59/120    avg_loss:0.074, val_acc:0.973]
Epoch [60/120    avg_loss:0.073, val_acc:0.977]
Epoch [61/120    avg_loss:0.075, val_acc:0.979]
Epoch [62/120    avg_loss:0.061, val_acc:0.985]
Epoch [63/120    avg_loss:0.086, val_acc:0.979]
Epoch [64/120    avg_loss:0.076, val_acc:0.981]
Epoch [65/120    avg_loss:0.095, val_acc:0.933]
Epoch [66/120    avg_loss:0.128, val_acc:0.975]
Epoch [67/120    avg_loss:0.114, val_acc:0.956]
Epoch [68/120    avg_loss:0.137, val_acc:0.979]
Epoch [69/120    avg_loss:0.082, val_acc:0.979]
Epoch [70/120    avg_loss:0.071, val_acc:0.979]
Epoch [71/120    avg_loss:0.065, val_acc:0.975]
Epoch [72/120    avg_loss:0.058, val_acc:0.952]
Epoch [73/120    avg_loss:0.073, val_acc:0.977]
Epoch [74/120    avg_loss:0.054, val_acc:0.971]
Epoch [75/120    avg_loss:0.089, val_acc:0.985]
Epoch [76/120    avg_loss:0.070, val_acc:0.983]
Epoch [77/120    avg_loss:0.047, val_acc:0.983]
Epoch [78/120    avg_loss:0.056, val_acc:0.985]
Epoch [79/120    avg_loss:0.037, val_acc:0.981]
Epoch [80/120    avg_loss:0.031, val_acc:0.983]
Epoch [81/120    avg_loss:0.035, val_acc:0.983]
Epoch [82/120    avg_loss:0.041, val_acc:0.985]
Epoch [83/120    avg_loss:0.036, val_acc:0.979]
Epoch [84/120    avg_loss:0.035, val_acc:0.983]
Epoch [85/120    avg_loss:0.030, val_acc:0.985]
Epoch [86/120    avg_loss:0.026, val_acc:0.988]
Epoch [87/120    avg_loss:0.047, val_acc:0.971]
Epoch [88/120    avg_loss:0.041, val_acc:0.992]
Epoch [89/120    avg_loss:0.075, val_acc:0.985]
Epoch [90/120    avg_loss:0.025, val_acc:0.985]
Epoch [91/120    avg_loss:0.027, val_acc:0.975]
Epoch [92/120    avg_loss:0.035, val_acc:0.985]
Epoch [93/120    avg_loss:0.030, val_acc:0.990]
Epoch [94/120    avg_loss:0.023, val_acc:0.992]
Epoch [95/120    avg_loss:0.027, val_acc:0.988]
Epoch [96/120    avg_loss:0.024, val_acc:0.992]
Epoch [97/120    avg_loss:0.031, val_acc:0.988]
Epoch [98/120    avg_loss:0.031, val_acc:0.983]
Epoch [99/120    avg_loss:0.026, val_acc:0.988]
Epoch [100/120    avg_loss:0.028, val_acc:0.988]
Epoch [101/120    avg_loss:0.019, val_acc:0.990]
Epoch [102/120    avg_loss:0.018, val_acc:0.988]
Epoch [103/120    avg_loss:0.023, val_acc:0.985]
Epoch [104/120    avg_loss:0.028, val_acc:0.990]
Epoch [105/120    avg_loss:0.030, val_acc:0.988]
Epoch [106/120    avg_loss:0.018, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.992]
Epoch [108/120    avg_loss:0.016, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.985]
Epoch [110/120    avg_loss:0.020, val_acc:0.994]
Epoch [111/120    avg_loss:0.017, val_acc:0.985]
Epoch [112/120    avg_loss:0.016, val_acc:0.985]
Epoch [113/120    avg_loss:0.016, val_acc:0.992]
Epoch [114/120    avg_loss:0.016, val_acc:0.985]
Epoch [115/120    avg_loss:0.019, val_acc:0.990]
Epoch [116/120    avg_loss:0.044, val_acc:0.973]
Epoch [117/120    avg_loss:0.066, val_acc:0.992]
Epoch [118/120    avg_loss:0.030, val_acc:0.994]
Epoch [119/120    avg_loss:0.022, val_acc:0.988]
Epoch [120/120    avg_loss:0.019, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99780541 1.         1.         0.94736842 0.91666667
 0.99277108 1.         1.         1.         1.         0.98687664
 0.98886414 1.        ]

Kappa:
0.9912177222952995
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ecd0bb828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.489, val_acc:0.342]
Epoch [2/120    avg_loss:2.152, val_acc:0.573]
Epoch [3/120    avg_loss:1.951, val_acc:0.633]
Epoch [4/120    avg_loss:1.771, val_acc:0.627]
Epoch [5/120    avg_loss:1.588, val_acc:0.673]
Epoch [6/120    avg_loss:1.406, val_acc:0.727]
Epoch [7/120    avg_loss:1.244, val_acc:0.729]
Epoch [8/120    avg_loss:1.094, val_acc:0.750]
Epoch [9/120    avg_loss:0.950, val_acc:0.779]
Epoch [10/120    avg_loss:0.858, val_acc:0.806]
Epoch [11/120    avg_loss:0.747, val_acc:0.835]
Epoch [12/120    avg_loss:0.698, val_acc:0.865]
Epoch [13/120    avg_loss:0.654, val_acc:0.846]
Epoch [14/120    avg_loss:0.669, val_acc:0.919]
Epoch [15/120    avg_loss:0.600, val_acc:0.887]
Epoch [16/120    avg_loss:0.519, val_acc:0.904]
Epoch [17/120    avg_loss:0.451, val_acc:0.931]
Epoch [18/120    avg_loss:0.382, val_acc:0.929]
Epoch [19/120    avg_loss:0.391, val_acc:0.881]
Epoch [20/120    avg_loss:0.402, val_acc:0.877]
Epoch [21/120    avg_loss:0.330, val_acc:0.925]
Epoch [22/120    avg_loss:0.327, val_acc:0.929]
Epoch [23/120    avg_loss:0.289, val_acc:0.938]
Epoch [24/120    avg_loss:0.287, val_acc:0.944]
Epoch [25/120    avg_loss:0.259, val_acc:0.942]
Epoch [26/120    avg_loss:0.237, val_acc:0.942]
Epoch [27/120    avg_loss:0.251, val_acc:0.965]
Epoch [28/120    avg_loss:0.274, val_acc:0.940]
Epoch [29/120    avg_loss:0.237, val_acc:0.960]
Epoch [30/120    avg_loss:0.210, val_acc:0.946]
Epoch [31/120    avg_loss:0.247, val_acc:0.950]
Epoch [32/120    avg_loss:0.231, val_acc:0.963]
Epoch [33/120    avg_loss:0.224, val_acc:0.950]
Epoch [34/120    avg_loss:0.220, val_acc:0.967]
Epoch [35/120    avg_loss:0.202, val_acc:0.963]
Epoch [36/120    avg_loss:0.148, val_acc:0.969]
Epoch [37/120    avg_loss:0.167, val_acc:0.969]
Epoch [38/120    avg_loss:0.153, val_acc:0.950]
Epoch [39/120    avg_loss:0.142, val_acc:0.975]
Epoch [40/120    avg_loss:0.111, val_acc:0.977]
Epoch [41/120    avg_loss:0.130, val_acc:0.985]
Epoch [42/120    avg_loss:0.127, val_acc:0.965]
Epoch [43/120    avg_loss:0.149, val_acc:0.975]
Epoch [44/120    avg_loss:0.120, val_acc:0.979]
Epoch [45/120    avg_loss:0.146, val_acc:0.971]
Epoch [46/120    avg_loss:0.183, val_acc:0.967]
Epoch [47/120    avg_loss:0.179, val_acc:0.975]
Epoch [48/120    avg_loss:0.170, val_acc:0.979]
Epoch [49/120    avg_loss:0.147, val_acc:0.967]
Epoch [50/120    avg_loss:0.170, val_acc:0.975]
Epoch [51/120    avg_loss:0.114, val_acc:0.981]
Epoch [52/120    avg_loss:0.130, val_acc:0.975]
Epoch [53/120    avg_loss:0.106, val_acc:0.990]
Epoch [54/120    avg_loss:0.096, val_acc:0.988]
Epoch [55/120    avg_loss:0.085, val_acc:0.983]
Epoch [56/120    avg_loss:0.064, val_acc:0.977]
Epoch [57/120    avg_loss:0.048, val_acc:0.994]
Epoch [58/120    avg_loss:0.057, val_acc:0.985]
Epoch [59/120    avg_loss:0.049, val_acc:0.990]
Epoch [60/120    avg_loss:0.054, val_acc:0.988]
Epoch [61/120    avg_loss:0.047, val_acc:0.985]
Epoch [62/120    avg_loss:0.053, val_acc:0.990]
Epoch [63/120    avg_loss:0.057, val_acc:0.973]
Epoch [64/120    avg_loss:0.061, val_acc:0.996]
Epoch [65/120    avg_loss:0.050, val_acc:0.988]
Epoch [66/120    avg_loss:0.064, val_acc:0.977]
Epoch [67/120    avg_loss:0.053, val_acc:0.990]
Epoch [68/120    avg_loss:0.043, val_acc:0.992]
Epoch [69/120    avg_loss:0.064, val_acc:0.996]
Epoch [70/120    avg_loss:0.051, val_acc:0.988]
Epoch [71/120    avg_loss:0.041, val_acc:0.994]
Epoch [72/120    avg_loss:0.030, val_acc:0.992]
Epoch [73/120    avg_loss:0.047, val_acc:0.988]
Epoch [74/120    avg_loss:0.057, val_acc:0.977]
Epoch [75/120    avg_loss:0.108, val_acc:0.977]
Epoch [76/120    avg_loss:0.100, val_acc:0.981]
Epoch [77/120    avg_loss:0.059, val_acc:0.990]
Epoch [78/120    avg_loss:0.038, val_acc:0.988]
Epoch [79/120    avg_loss:0.076, val_acc:0.990]
Epoch [80/120    avg_loss:0.044, val_acc:0.990]
Epoch [81/120    avg_loss:0.050, val_acc:0.990]
Epoch [82/120    avg_loss:0.035, val_acc:0.988]
Epoch [83/120    avg_loss:0.028, val_acc:0.992]
Epoch [84/120    avg_loss:0.025, val_acc:0.994]
Epoch [85/120    avg_loss:0.027, val_acc:0.994]
Epoch [86/120    avg_loss:0.028, val_acc:0.994]
Epoch [87/120    avg_loss:0.020, val_acc:0.994]
Epoch [88/120    avg_loss:0.022, val_acc:0.994]
Epoch [89/120    avg_loss:0.020, val_acc:0.994]
Epoch [90/120    avg_loss:0.018, val_acc:0.992]
Epoch [91/120    avg_loss:0.020, val_acc:0.992]
Epoch [92/120    avg_loss:0.016, val_acc:0.992]
Epoch [93/120    avg_loss:0.018, val_acc:0.992]
Epoch [94/120    avg_loss:0.021, val_acc:0.992]
Epoch [95/120    avg_loss:0.021, val_acc:0.992]
Epoch [96/120    avg_loss:0.028, val_acc:0.992]
Epoch [97/120    avg_loss:0.017, val_acc:0.994]
Epoch [98/120    avg_loss:0.021, val_acc:0.994]
Epoch [99/120    avg_loss:0.020, val_acc:0.994]
Epoch [100/120    avg_loss:0.030, val_acc:0.994]
Epoch [101/120    avg_loss:0.020, val_acc:0.994]
Epoch [102/120    avg_loss:0.031, val_acc:0.994]
Epoch [103/120    avg_loss:0.026, val_acc:0.994]
Epoch [104/120    avg_loss:0.020, val_acc:0.994]
Epoch [105/120    avg_loss:0.018, val_acc:0.994]
Epoch [106/120    avg_loss:0.015, val_acc:0.994]
Epoch [107/120    avg_loss:0.022, val_acc:0.994]
Epoch [108/120    avg_loss:0.021, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.015, val_acc:0.994]
Epoch [111/120    avg_loss:0.019, val_acc:0.994]
Epoch [112/120    avg_loss:0.017, val_acc:0.994]
Epoch [113/120    avg_loss:0.015, val_acc:0.994]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.017, val_acc:0.992]
Epoch [116/120    avg_loss:0.015, val_acc:0.992]
Epoch [117/120    avg_loss:0.016, val_acc:0.994]
Epoch [118/120    avg_loss:0.016, val_acc:0.994]
Epoch [119/120    avg_loss:0.022, val_acc:0.994]
Epoch [120/120    avg_loss:0.015, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0  11   0   0 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         0.99545455 1.         0.99111111 0.98639456
 1.         0.98924731 1.         0.98838437 1.         0.99867198
 0.98660714 1.        ]

Kappa:
0.995727098561745
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2447e287b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.482, val_acc:0.342]
Epoch [2/120    avg_loss:2.142, val_acc:0.565]
Epoch [3/120    avg_loss:1.942, val_acc:0.633]
Epoch [4/120    avg_loss:1.736, val_acc:0.646]
Epoch [5/120    avg_loss:1.541, val_acc:0.685]
Epoch [6/120    avg_loss:1.339, val_acc:0.700]
Epoch [7/120    avg_loss:1.188, val_acc:0.748]
Epoch [8/120    avg_loss:1.045, val_acc:0.746]
Epoch [9/120    avg_loss:0.934, val_acc:0.775]
Epoch [10/120    avg_loss:0.832, val_acc:0.819]
Epoch [11/120    avg_loss:0.752, val_acc:0.825]
Epoch [12/120    avg_loss:0.678, val_acc:0.856]
Epoch [13/120    avg_loss:0.636, val_acc:0.827]
Epoch [14/120    avg_loss:0.606, val_acc:0.910]
Epoch [15/120    avg_loss:0.538, val_acc:0.908]
Epoch [16/120    avg_loss:0.531, val_acc:0.831]
Epoch [17/120    avg_loss:0.527, val_acc:0.929]
Epoch [18/120    avg_loss:0.487, val_acc:0.910]
Epoch [19/120    avg_loss:0.416, val_acc:0.935]
Epoch [20/120    avg_loss:0.379, val_acc:0.956]
Epoch [21/120    avg_loss:0.383, val_acc:0.925]
Epoch [22/120    avg_loss:0.383, val_acc:0.931]
Epoch [23/120    avg_loss:0.358, val_acc:0.940]
Epoch [24/120    avg_loss:0.340, val_acc:0.929]
Epoch [25/120    avg_loss:0.297, val_acc:0.942]
Epoch [26/120    avg_loss:0.275, val_acc:0.977]
Epoch [27/120    avg_loss:0.287, val_acc:0.946]
Epoch [28/120    avg_loss:0.320, val_acc:0.954]
Epoch [29/120    avg_loss:0.253, val_acc:0.954]
Epoch [30/120    avg_loss:0.325, val_acc:0.912]
Epoch [31/120    avg_loss:0.251, val_acc:0.950]
Epoch [32/120    avg_loss:0.219, val_acc:0.950]
Epoch [33/120    avg_loss:0.251, val_acc:0.948]
Epoch [34/120    avg_loss:0.220, val_acc:0.975]
Epoch [35/120    avg_loss:0.205, val_acc:0.971]
Epoch [36/120    avg_loss:0.191, val_acc:0.960]
Epoch [37/120    avg_loss:0.165, val_acc:0.963]
Epoch [38/120    avg_loss:0.182, val_acc:0.985]
Epoch [39/120    avg_loss:0.151, val_acc:0.967]
Epoch [40/120    avg_loss:0.180, val_acc:0.981]
Epoch [41/120    avg_loss:0.141, val_acc:0.988]
Epoch [42/120    avg_loss:0.139, val_acc:0.965]
Epoch [43/120    avg_loss:0.150, val_acc:0.975]
Epoch [44/120    avg_loss:0.136, val_acc:0.985]
Epoch [45/120    avg_loss:0.171, val_acc:0.983]
Epoch [46/120    avg_loss:0.146, val_acc:0.960]
Epoch [47/120    avg_loss:0.125, val_acc:0.973]
Epoch [48/120    avg_loss:0.150, val_acc:0.977]
Epoch [49/120    avg_loss:0.139, val_acc:0.977]
Epoch [50/120    avg_loss:0.157, val_acc:0.975]
Epoch [51/120    avg_loss:0.203, val_acc:0.983]
Epoch [52/120    avg_loss:0.113, val_acc:0.967]
Epoch [53/120    avg_loss:0.121, val_acc:0.954]
Epoch [54/120    avg_loss:0.131, val_acc:0.983]
Epoch [55/120    avg_loss:0.091, val_acc:0.985]
Epoch [56/120    avg_loss:0.090, val_acc:0.988]
Epoch [57/120    avg_loss:0.077, val_acc:0.992]
Epoch [58/120    avg_loss:0.084, val_acc:0.990]
Epoch [59/120    avg_loss:0.065, val_acc:0.990]
Epoch [60/120    avg_loss:0.072, val_acc:0.992]
Epoch [61/120    avg_loss:0.065, val_acc:0.992]
Epoch [62/120    avg_loss:0.064, val_acc:0.992]
Epoch [63/120    avg_loss:0.062, val_acc:0.992]
Epoch [64/120    avg_loss:0.069, val_acc:0.988]
Epoch [65/120    avg_loss:0.074, val_acc:0.990]
Epoch [66/120    avg_loss:0.069, val_acc:0.992]
Epoch [67/120    avg_loss:0.070, val_acc:0.992]
Epoch [68/120    avg_loss:0.074, val_acc:0.990]
Epoch [69/120    avg_loss:0.055, val_acc:0.990]
Epoch [70/120    avg_loss:0.055, val_acc:0.992]
Epoch [71/120    avg_loss:0.063, val_acc:0.990]
Epoch [72/120    avg_loss:0.058, val_acc:0.988]
Epoch [73/120    avg_loss:0.067, val_acc:0.988]
Epoch [74/120    avg_loss:0.060, val_acc:0.988]
Epoch [75/120    avg_loss:0.061, val_acc:0.988]
Epoch [76/120    avg_loss:0.056, val_acc:0.988]
Epoch [77/120    avg_loss:0.051, val_acc:0.990]
Epoch [78/120    avg_loss:0.055, val_acc:0.992]
Epoch [79/120    avg_loss:0.049, val_acc:0.992]
Epoch [80/120    avg_loss:0.062, val_acc:0.994]
Epoch [81/120    avg_loss:0.055, val_acc:0.990]
Epoch [82/120    avg_loss:0.056, val_acc:0.992]
Epoch [83/120    avg_loss:0.057, val_acc:0.988]
Epoch [84/120    avg_loss:0.048, val_acc:0.988]
Epoch [85/120    avg_loss:0.048, val_acc:0.988]
Epoch [86/120    avg_loss:0.047, val_acc:0.988]
Epoch [87/120    avg_loss:0.052, val_acc:0.988]
Epoch [88/120    avg_loss:0.047, val_acc:0.988]
Epoch [89/120    avg_loss:0.057, val_acc:0.988]
Epoch [90/120    avg_loss:0.046, val_acc:0.988]
Epoch [91/120    avg_loss:0.046, val_acc:0.988]
Epoch [92/120    avg_loss:0.045, val_acc:0.988]
Epoch [93/120    avg_loss:0.051, val_acc:0.988]
Epoch [94/120    avg_loss:0.047, val_acc:0.988]
Epoch [95/120    avg_loss:0.042, val_acc:0.988]
Epoch [96/120    avg_loss:0.052, val_acc:0.988]
Epoch [97/120    avg_loss:0.050, val_acc:0.988]
Epoch [98/120    avg_loss:0.050, val_acc:0.988]
Epoch [99/120    avg_loss:0.058, val_acc:0.988]
Epoch [100/120    avg_loss:0.050, val_acc:0.988]
Epoch [101/120    avg_loss:0.044, val_acc:0.988]
Epoch [102/120    avg_loss:0.049, val_acc:0.988]
Epoch [103/120    avg_loss:0.044, val_acc:0.988]
Epoch [104/120    avg_loss:0.055, val_acc:0.988]
Epoch [105/120    avg_loss:0.041, val_acc:0.988]
Epoch [106/120    avg_loss:0.048, val_acc:0.988]
Epoch [107/120    avg_loss:0.044, val_acc:0.988]
Epoch [108/120    avg_loss:0.045, val_acc:0.988]
Epoch [109/120    avg_loss:0.039, val_acc:0.988]
Epoch [110/120    avg_loss:0.043, val_acc:0.988]
Epoch [111/120    avg_loss:0.052, val_acc:0.988]
Epoch [112/120    avg_loss:0.042, val_acc:0.988]
Epoch [113/120    avg_loss:0.041, val_acc:0.988]
Epoch [114/120    avg_loss:0.045, val_acc:0.988]
Epoch [115/120    avg_loss:0.044, val_acc:0.988]
Epoch [116/120    avg_loss:0.042, val_acc:0.988]
Epoch [117/120    avg_loss:0.057, val_acc:0.988]
Epoch [118/120    avg_loss:0.038, val_acc:0.988]
Epoch [119/120    avg_loss:0.043, val_acc:0.988]
Epoch [120/120    avg_loss:0.046, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 674   0   0   0   0  11   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99190581 1.         1.         0.94618834 0.91946309
 0.97399527 1.         1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9914567967875856
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f30cfb528d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.497, val_acc:0.335]
Epoch [2/120    avg_loss:2.121, val_acc:0.492]
Epoch [3/120    avg_loss:1.891, val_acc:0.552]
Epoch [4/120    avg_loss:1.697, val_acc:0.633]
Epoch [5/120    avg_loss:1.531, val_acc:0.683]
Epoch [6/120    avg_loss:1.337, val_acc:0.735]
Epoch [7/120    avg_loss:1.169, val_acc:0.733]
Epoch [8/120    avg_loss:1.027, val_acc:0.750]
Epoch [9/120    avg_loss:0.897, val_acc:0.773]
Epoch [10/120    avg_loss:0.789, val_acc:0.785]
Epoch [11/120    avg_loss:0.691, val_acc:0.890]
Epoch [12/120    avg_loss:0.657, val_acc:0.896]
Epoch [13/120    avg_loss:0.554, val_acc:0.871]
Epoch [14/120    avg_loss:0.512, val_acc:0.900]
Epoch [15/120    avg_loss:0.550, val_acc:0.871]
Epoch [16/120    avg_loss:0.531, val_acc:0.921]
Epoch [17/120    avg_loss:0.417, val_acc:0.925]
Epoch [18/120    avg_loss:0.424, val_acc:0.942]
Epoch [19/120    avg_loss:0.379, val_acc:0.933]
Epoch [20/120    avg_loss:0.295, val_acc:0.938]
Epoch [21/120    avg_loss:0.318, val_acc:0.925]
Epoch [22/120    avg_loss:0.298, val_acc:0.929]
Epoch [23/120    avg_loss:0.325, val_acc:0.944]
Epoch [24/120    avg_loss:0.271, val_acc:0.965]
Epoch [25/120    avg_loss:0.230, val_acc:0.954]
Epoch [26/120    avg_loss:0.218, val_acc:0.912]
Epoch [27/120    avg_loss:0.239, val_acc:0.958]
Epoch [28/120    avg_loss:0.220, val_acc:0.958]
Epoch [29/120    avg_loss:0.218, val_acc:0.967]
Epoch [30/120    avg_loss:0.196, val_acc:0.969]
Epoch [31/120    avg_loss:0.163, val_acc:0.971]
Epoch [32/120    avg_loss:0.195, val_acc:0.960]
Epoch [33/120    avg_loss:0.161, val_acc:0.967]
Epoch [34/120    avg_loss:0.137, val_acc:0.971]
Epoch [35/120    avg_loss:0.121, val_acc:0.979]
Epoch [36/120    avg_loss:0.121, val_acc:0.969]
Epoch [37/120    avg_loss:0.145, val_acc:0.975]
Epoch [38/120    avg_loss:0.119, val_acc:0.973]
Epoch [39/120    avg_loss:0.087, val_acc:0.971]
Epoch [40/120    avg_loss:0.130, val_acc:0.983]
Epoch [41/120    avg_loss:0.119, val_acc:0.971]
Epoch [42/120    avg_loss:0.114, val_acc:0.946]
Epoch [43/120    avg_loss:0.147, val_acc:0.988]
Epoch [44/120    avg_loss:0.107, val_acc:0.981]
Epoch [45/120    avg_loss:0.085, val_acc:0.985]
Epoch [46/120    avg_loss:0.081, val_acc:0.988]
Epoch [47/120    avg_loss:0.071, val_acc:0.983]
Epoch [48/120    avg_loss:0.090, val_acc:0.983]
Epoch [49/120    avg_loss:0.071, val_acc:0.988]
Epoch [50/120    avg_loss:0.066, val_acc:0.975]
Epoch [51/120    avg_loss:0.087, val_acc:0.975]
Epoch [52/120    avg_loss:0.086, val_acc:0.994]
Epoch [53/120    avg_loss:0.069, val_acc:0.981]
Epoch [54/120    avg_loss:0.065, val_acc:0.983]
Epoch [55/120    avg_loss:0.073, val_acc:0.981]
Epoch [56/120    avg_loss:0.045, val_acc:0.992]
Epoch [57/120    avg_loss:0.040, val_acc:0.994]
Epoch [58/120    avg_loss:0.053, val_acc:0.973]
Epoch [59/120    avg_loss:0.058, val_acc:0.985]
Epoch [60/120    avg_loss:0.041, val_acc:0.985]
Epoch [61/120    avg_loss:0.042, val_acc:0.990]
Epoch [62/120    avg_loss:0.044, val_acc:0.990]
Epoch [63/120    avg_loss:0.045, val_acc:0.985]
Epoch [64/120    avg_loss:0.038, val_acc:0.990]
Epoch [65/120    avg_loss:0.033, val_acc:0.992]
Epoch [66/120    avg_loss:0.022, val_acc:0.996]
Epoch [67/120    avg_loss:0.032, val_acc:0.992]
Epoch [68/120    avg_loss:0.043, val_acc:0.983]
Epoch [69/120    avg_loss:0.049, val_acc:0.983]
Epoch [70/120    avg_loss:0.049, val_acc:0.990]
Epoch [71/120    avg_loss:0.042, val_acc:0.994]
Epoch [72/120    avg_loss:0.026, val_acc:0.990]
Epoch [73/120    avg_loss:0.021, val_acc:0.994]
Epoch [74/120    avg_loss:0.027, val_acc:0.996]
Epoch [75/120    avg_loss:0.029, val_acc:0.994]
Epoch [76/120    avg_loss:0.038, val_acc:0.994]
Epoch [77/120    avg_loss:0.069, val_acc:0.994]
Epoch [78/120    avg_loss:0.059, val_acc:0.894]
Epoch [79/120    avg_loss:0.077, val_acc:0.992]
Epoch [80/120    avg_loss:0.039, val_acc:0.985]
Epoch [81/120    avg_loss:0.022, val_acc:0.994]
Epoch [82/120    avg_loss:0.019, val_acc:0.996]
Epoch [83/120    avg_loss:0.028, val_acc:0.994]
Epoch [84/120    avg_loss:0.032, val_acc:0.992]
Epoch [85/120    avg_loss:0.053, val_acc:0.990]
Epoch [86/120    avg_loss:0.030, val_acc:0.994]
Epoch [87/120    avg_loss:0.017, val_acc:0.996]
Epoch [88/120    avg_loss:0.013, val_acc:0.994]
Epoch [89/120    avg_loss:0.012, val_acc:0.994]
Epoch [90/120    avg_loss:0.016, val_acc:0.994]
Epoch [91/120    avg_loss:0.024, val_acc:0.992]
Epoch [92/120    avg_loss:0.015, val_acc:0.996]
Epoch [93/120    avg_loss:0.014, val_acc:0.998]
Epoch [94/120    avg_loss:0.011, val_acc:0.996]
Epoch [95/120    avg_loss:0.016, val_acc:0.994]
Epoch [96/120    avg_loss:0.013, val_acc:0.994]
Epoch [97/120    avg_loss:0.014, val_acc:0.996]
Epoch [98/120    avg_loss:0.023, val_acc:0.983]
Epoch [99/120    avg_loss:0.030, val_acc:0.994]
Epoch [100/120    avg_loss:0.019, val_acc:0.996]
Epoch [101/120    avg_loss:0.027, val_acc:0.998]
Epoch [102/120    avg_loss:0.024, val_acc:0.994]
Epoch [103/120    avg_loss:0.044, val_acc:0.992]
Epoch [104/120    avg_loss:0.019, val_acc:0.992]
Epoch [105/120    avg_loss:0.090, val_acc:0.981]
Epoch [106/120    avg_loss:0.054, val_acc:0.985]
Epoch [107/120    avg_loss:0.050, val_acc:0.996]
Epoch [108/120    avg_loss:0.030, val_acc:0.996]
Epoch [109/120    avg_loss:0.027, val_acc:0.996]
Epoch [110/120    avg_loss:0.018, val_acc:0.994]
Epoch [111/120    avg_loss:0.029, val_acc:0.998]
Epoch [112/120    avg_loss:0.030, val_acc:1.000]
Epoch [113/120    avg_loss:0.023, val_acc:0.994]
Epoch [114/120    avg_loss:0.020, val_acc:0.998]
Epoch [115/120    avg_loss:0.012, val_acc:0.996]
Epoch [116/120    avg_loss:0.011, val_acc:0.996]
Epoch [117/120    avg_loss:0.011, val_acc:1.000]
Epoch [118/120    avg_loss:0.008, val_acc:0.998]
Epoch [119/120    avg_loss:0.010, val_acc:0.996]
Epoch [120/120    avg_loss:0.012, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.76545842217485

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.98434004 0.97972973
 0.99516908 0.98924731 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9973889327187835
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2952a22710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.399, val_acc:0.471]
Epoch [2/120    avg_loss:2.054, val_acc:0.581]
Epoch [3/120    avg_loss:1.811, val_acc:0.633]
Epoch [4/120    avg_loss:1.550, val_acc:0.679]
Epoch [5/120    avg_loss:1.339, val_acc:0.750]
Epoch [6/120    avg_loss:1.134, val_acc:0.777]
Epoch [7/120    avg_loss:0.979, val_acc:0.783]
Epoch [8/120    avg_loss:0.856, val_acc:0.810]
Epoch [9/120    avg_loss:0.747, val_acc:0.835]
Epoch [10/120    avg_loss:0.666, val_acc:0.875]
Epoch [11/120    avg_loss:0.652, val_acc:0.910]
Epoch [12/120    avg_loss:0.582, val_acc:0.898]
Epoch [13/120    avg_loss:0.520, val_acc:0.931]
Epoch [14/120    avg_loss:0.505, val_acc:0.917]
Epoch [15/120    avg_loss:0.513, val_acc:0.925]
Epoch [16/120    avg_loss:0.394, val_acc:0.931]
Epoch [17/120    avg_loss:0.424, val_acc:0.952]
Epoch [18/120    avg_loss:0.364, val_acc:0.950]
Epoch [19/120    avg_loss:0.321, val_acc:0.967]
Epoch [20/120    avg_loss:0.307, val_acc:0.948]
Epoch [21/120    avg_loss:0.325, val_acc:0.973]
Epoch [22/120    avg_loss:0.363, val_acc:0.933]
Epoch [23/120    avg_loss:0.304, val_acc:0.977]
Epoch [24/120    avg_loss:0.294, val_acc:0.956]
Epoch [25/120    avg_loss:0.241, val_acc:0.975]
Epoch [26/120    avg_loss:0.247, val_acc:0.985]
Epoch [27/120    avg_loss:0.245, val_acc:0.965]
Epoch [28/120    avg_loss:0.234, val_acc:0.965]
Epoch [29/120    avg_loss:0.215, val_acc:0.979]
Epoch [30/120    avg_loss:0.190, val_acc:0.973]
Epoch [31/120    avg_loss:0.192, val_acc:0.960]
Epoch [32/120    avg_loss:0.229, val_acc:0.973]
Epoch [33/120    avg_loss:0.214, val_acc:0.979]
Epoch [34/120    avg_loss:0.232, val_acc:0.958]
Epoch [35/120    avg_loss:0.239, val_acc:0.967]
Epoch [36/120    avg_loss:0.195, val_acc:0.990]
Epoch [37/120    avg_loss:0.151, val_acc:0.985]
Epoch [38/120    avg_loss:0.151, val_acc:0.985]
Epoch [39/120    avg_loss:0.148, val_acc:0.969]
Epoch [40/120    avg_loss:0.187, val_acc:0.971]
Epoch [41/120    avg_loss:0.136, val_acc:0.985]
Epoch [42/120    avg_loss:0.145, val_acc:0.983]
Epoch [43/120    avg_loss:0.185, val_acc:0.979]
Epoch [44/120    avg_loss:0.164, val_acc:0.994]
Epoch [45/120    avg_loss:0.116, val_acc:0.965]
Epoch [46/120    avg_loss:0.179, val_acc:0.985]
Epoch [47/120    avg_loss:0.107, val_acc:0.996]
Epoch [48/120    avg_loss:0.113, val_acc:0.996]
Epoch [49/120    avg_loss:0.134, val_acc:0.963]
Epoch [50/120    avg_loss:0.136, val_acc:0.979]
Epoch [51/120    avg_loss:0.104, val_acc:0.990]
Epoch [52/120    avg_loss:0.085, val_acc:0.990]
Epoch [53/120    avg_loss:0.096, val_acc:0.992]
Epoch [54/120    avg_loss:0.094, val_acc:0.988]
Epoch [55/120    avg_loss:0.100, val_acc:0.990]
Epoch [56/120    avg_loss:0.095, val_acc:0.992]
Epoch [57/120    avg_loss:0.075, val_acc:0.985]
Epoch [58/120    avg_loss:0.072, val_acc:0.992]
Epoch [59/120    avg_loss:0.056, val_acc:0.996]
Epoch [60/120    avg_loss:0.060, val_acc:0.994]
Epoch [61/120    avg_loss:0.062, val_acc:0.990]
Epoch [62/120    avg_loss:0.071, val_acc:0.990]
Epoch [63/120    avg_loss:0.084, val_acc:0.975]
Epoch [64/120    avg_loss:0.098, val_acc:0.988]
Epoch [65/120    avg_loss:0.067, val_acc:0.996]
Epoch [66/120    avg_loss:0.043, val_acc:0.996]
Epoch [67/120    avg_loss:0.055, val_acc:0.996]
Epoch [68/120    avg_loss:0.035, val_acc:0.994]
Epoch [69/120    avg_loss:0.041, val_acc:0.994]
Epoch [70/120    avg_loss:0.054, val_acc:0.996]
Epoch [71/120    avg_loss:0.040, val_acc:0.996]
Epoch [72/120    avg_loss:0.044, val_acc:0.994]
Epoch [73/120    avg_loss:0.038, val_acc:0.994]
Epoch [74/120    avg_loss:0.074, val_acc:0.990]
Epoch [75/120    avg_loss:0.098, val_acc:0.990]
Epoch [76/120    avg_loss:0.086, val_acc:0.994]
Epoch [77/120    avg_loss:0.072, val_acc:0.985]
Epoch [78/120    avg_loss:0.060, val_acc:0.996]
Epoch [79/120    avg_loss:0.033, val_acc:0.998]
Epoch [80/120    avg_loss:0.027, val_acc:0.996]
Epoch [81/120    avg_loss:0.033, val_acc:0.992]
Epoch [82/120    avg_loss:0.028, val_acc:0.996]
Epoch [83/120    avg_loss:0.038, val_acc:0.992]
Epoch [84/120    avg_loss:0.052, val_acc:0.994]
Epoch [85/120    avg_loss:0.047, val_acc:0.994]
Epoch [86/120    avg_loss:0.036, val_acc:0.992]
Epoch [87/120    avg_loss:0.026, val_acc:0.998]
Epoch [88/120    avg_loss:0.020, val_acc:0.998]
Epoch [89/120    avg_loss:0.019, val_acc:0.996]
Epoch [90/120    avg_loss:0.018, val_acc:0.994]
Epoch [91/120    avg_loss:0.016, val_acc:0.996]
Epoch [92/120    avg_loss:0.020, val_acc:0.998]
Epoch [93/120    avg_loss:0.023, val_acc:0.994]
Epoch [94/120    avg_loss:0.032, val_acc:0.996]
Epoch [95/120    avg_loss:0.035, val_acc:0.992]
Epoch [96/120    avg_loss:0.039, val_acc:0.996]
Epoch [97/120    avg_loss:0.026, val_acc:0.992]
Epoch [98/120    avg_loss:0.021, val_acc:0.994]
Epoch [99/120    avg_loss:0.018, val_acc:0.998]
Epoch [100/120    avg_loss:0.016, val_acc:0.998]
Epoch [101/120    avg_loss:0.013, val_acc:0.998]
Epoch [102/120    avg_loss:0.013, val_acc:1.000]
Epoch [103/120    avg_loss:0.013, val_acc:1.000]
Epoch [104/120    avg_loss:0.012, val_acc:0.998]
Epoch [105/120    avg_loss:0.008, val_acc:1.000]
Epoch [106/120    avg_loss:0.009, val_acc:0.996]
Epoch [107/120    avg_loss:0.012, val_acc:0.998]
Epoch [108/120    avg_loss:0.011, val_acc:0.998]
Epoch [109/120    avg_loss:0.008, val_acc:0.996]
Epoch [110/120    avg_loss:0.014, val_acc:0.994]
Epoch [111/120    avg_loss:0.013, val_acc:0.998]
Epoch [112/120    avg_loss:0.012, val_acc:0.996]
Epoch [113/120    avg_loss:0.014, val_acc:0.996]
Epoch [114/120    avg_loss:0.017, val_acc:0.994]
Epoch [115/120    avg_loss:0.040, val_acc:0.998]
Epoch [116/120    avg_loss:0.025, val_acc:0.996]
Epoch [117/120    avg_loss:0.025, val_acc:0.998]
Epoch [118/120    avg_loss:0.026, val_acc:0.994]
Epoch [119/120    avg_loss:0.024, val_acc:0.994]
Epoch [120/120    avg_loss:0.016, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   2   0   0   0   0   0   0   2   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.80810234541578

F1 scores:
[       nan 1.         1.         1.         0.98454746 0.98269896
 1.         1.         1.         1.         1.         0.99734043
 0.9956044  1.        ]

Kappa:
0.9978634927242336
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3311637828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.469, val_acc:0.398]
Epoch [2/120    avg_loss:2.096, val_acc:0.562]
Epoch [3/120    avg_loss:1.850, val_acc:0.635]
Epoch [4/120    avg_loss:1.596, val_acc:0.690]
Epoch [5/120    avg_loss:1.380, val_acc:0.738]
Epoch [6/120    avg_loss:1.180, val_acc:0.760]
Epoch [7/120    avg_loss:1.048, val_acc:0.775]
Epoch [8/120    avg_loss:0.955, val_acc:0.800]
Epoch [9/120    avg_loss:0.817, val_acc:0.802]
Epoch [10/120    avg_loss:0.752, val_acc:0.796]
Epoch [11/120    avg_loss:0.651, val_acc:0.825]
Epoch [12/120    avg_loss:0.608, val_acc:0.812]
Epoch [13/120    avg_loss:0.555, val_acc:0.812]
Epoch [14/120    avg_loss:0.541, val_acc:0.858]
Epoch [15/120    avg_loss:0.488, val_acc:0.831]
Epoch [16/120    avg_loss:0.452, val_acc:0.854]
Epoch [17/120    avg_loss:0.407, val_acc:0.863]
Epoch [18/120    avg_loss:0.371, val_acc:0.921]
Epoch [19/120    avg_loss:0.385, val_acc:0.923]
Epoch [20/120    avg_loss:0.403, val_acc:0.950]
Epoch [21/120    avg_loss:0.373, val_acc:0.950]
Epoch [22/120    avg_loss:0.356, val_acc:0.956]
Epoch [23/120    avg_loss:0.311, val_acc:0.963]
Epoch [24/120    avg_loss:0.338, val_acc:0.921]
Epoch [25/120    avg_loss:0.304, val_acc:0.960]
Epoch [26/120    avg_loss:0.259, val_acc:0.958]
Epoch [27/120    avg_loss:0.247, val_acc:0.956]
Epoch [28/120    avg_loss:0.212, val_acc:0.963]
Epoch [29/120    avg_loss:0.231, val_acc:0.956]
Epoch [30/120    avg_loss:0.237, val_acc:0.952]
Epoch [31/120    avg_loss:0.197, val_acc:0.967]
Epoch [32/120    avg_loss:0.166, val_acc:0.954]
Epoch [33/120    avg_loss:0.175, val_acc:0.944]
Epoch [34/120    avg_loss:0.185, val_acc:0.965]
Epoch [35/120    avg_loss:0.144, val_acc:0.977]
Epoch [36/120    avg_loss:0.171, val_acc:0.973]
Epoch [37/120    avg_loss:0.125, val_acc:0.973]
Epoch [38/120    avg_loss:0.140, val_acc:0.977]
Epoch [39/120    avg_loss:0.146, val_acc:0.981]
Epoch [40/120    avg_loss:0.111, val_acc:0.975]
Epoch [41/120    avg_loss:0.120, val_acc:0.983]
Epoch [42/120    avg_loss:0.114, val_acc:0.975]
Epoch [43/120    avg_loss:0.130, val_acc:0.973]
Epoch [44/120    avg_loss:0.123, val_acc:0.981]
Epoch [45/120    avg_loss:0.096, val_acc:0.981]
Epoch [46/120    avg_loss:0.107, val_acc:0.981]
Epoch [47/120    avg_loss:0.096, val_acc:0.973]
Epoch [48/120    avg_loss:0.081, val_acc:0.985]
Epoch [49/120    avg_loss:0.068, val_acc:0.988]
Epoch [50/120    avg_loss:0.062, val_acc:0.992]
Epoch [51/120    avg_loss:0.063, val_acc:0.985]
Epoch [52/120    avg_loss:0.077, val_acc:0.965]
Epoch [53/120    avg_loss:0.101, val_acc:0.967]
Epoch [54/120    avg_loss:0.124, val_acc:0.985]
Epoch [55/120    avg_loss:0.071, val_acc:0.985]
Epoch [56/120    avg_loss:0.050, val_acc:0.992]
Epoch [57/120    avg_loss:0.071, val_acc:0.977]
Epoch [58/120    avg_loss:0.197, val_acc:0.952]
Epoch [59/120    avg_loss:0.142, val_acc:0.971]
Epoch [60/120    avg_loss:0.128, val_acc:0.979]
Epoch [61/120    avg_loss:0.105, val_acc:0.977]
Epoch [62/120    avg_loss:0.101, val_acc:0.977]
Epoch [63/120    avg_loss:0.056, val_acc:0.990]
Epoch [64/120    avg_loss:0.049, val_acc:0.983]
Epoch [65/120    avg_loss:0.040, val_acc:0.990]
Epoch [66/120    avg_loss:0.039, val_acc:0.990]
Epoch [67/120    avg_loss:0.044, val_acc:0.981]
Epoch [68/120    avg_loss:0.098, val_acc:0.979]
Epoch [69/120    avg_loss:0.070, val_acc:0.981]
Epoch [70/120    avg_loss:0.055, val_acc:0.983]
Epoch [71/120    avg_loss:0.045, val_acc:0.985]
Epoch [72/120    avg_loss:0.041, val_acc:0.992]
Epoch [73/120    avg_loss:0.030, val_acc:0.992]
Epoch [74/120    avg_loss:0.034, val_acc:0.992]
Epoch [75/120    avg_loss:0.036, val_acc:0.992]
Epoch [76/120    avg_loss:0.026, val_acc:0.990]
Epoch [77/120    avg_loss:0.033, val_acc:0.992]
Epoch [78/120    avg_loss:0.028, val_acc:0.992]
Epoch [79/120    avg_loss:0.026, val_acc:0.992]
Epoch [80/120    avg_loss:0.022, val_acc:0.992]
Epoch [81/120    avg_loss:0.026, val_acc:0.992]
Epoch [82/120    avg_loss:0.030, val_acc:0.992]
Epoch [83/120    avg_loss:0.027, val_acc:0.992]
Epoch [84/120    avg_loss:0.025, val_acc:0.992]
Epoch [85/120    avg_loss:0.032, val_acc:0.992]
Epoch [86/120    avg_loss:0.022, val_acc:0.992]
Epoch [87/120    avg_loss:0.021, val_acc:0.992]
Epoch [88/120    avg_loss:0.023, val_acc:0.992]
Epoch [89/120    avg_loss:0.022, val_acc:0.992]
Epoch [90/120    avg_loss:0.026, val_acc:0.992]
Epoch [91/120    avg_loss:0.034, val_acc:0.992]
Epoch [92/120    avg_loss:0.020, val_acc:0.992]
Epoch [93/120    avg_loss:0.028, val_acc:0.992]
Epoch [94/120    avg_loss:0.023, val_acc:0.992]
Epoch [95/120    avg_loss:0.017, val_acc:0.992]
Epoch [96/120    avg_loss:0.024, val_acc:0.992]
Epoch [97/120    avg_loss:0.022, val_acc:0.992]
Epoch [98/120    avg_loss:0.026, val_acc:0.992]
Epoch [99/120    avg_loss:0.021, val_acc:0.992]
Epoch [100/120    avg_loss:0.019, val_acc:0.992]
Epoch [101/120    avg_loss:0.020, val_acc:0.992]
Epoch [102/120    avg_loss:0.020, val_acc:0.992]
Epoch [103/120    avg_loss:0.020, val_acc:0.992]
Epoch [104/120    avg_loss:0.017, val_acc:0.992]
Epoch [105/120    avg_loss:0.017, val_acc:0.992]
Epoch [106/120    avg_loss:0.020, val_acc:0.992]
Epoch [107/120    avg_loss:0.021, val_acc:0.992]
Epoch [108/120    avg_loss:0.019, val_acc:0.992]
Epoch [109/120    avg_loss:0.020, val_acc:0.992]
Epoch [110/120    avg_loss:0.016, val_acc:0.992]
Epoch [111/120    avg_loss:0.016, val_acc:0.992]
Epoch [112/120    avg_loss:0.016, val_acc:0.992]
Epoch [113/120    avg_loss:0.026, val_acc:0.992]
Epoch [114/120    avg_loss:0.019, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.992]
Epoch [116/120    avg_loss:0.016, val_acc:0.992]
Epoch [117/120    avg_loss:0.017, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.019, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         1.         1.         0.95878525 0.93286219
 1.         1.         1.         1.         1.         0.99080158
 0.99221357 1.        ]

Kappa:
0.9938280690551713
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa30da09898>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.503, val_acc:0.490]
Epoch [2/120    avg_loss:2.111, val_acc:0.581]
Epoch [3/120    avg_loss:1.839, val_acc:0.608]
Epoch [4/120    avg_loss:1.608, val_acc:0.688]
Epoch [5/120    avg_loss:1.407, val_acc:0.700]
Epoch [6/120    avg_loss:1.220, val_acc:0.733]
Epoch [7/120    avg_loss:1.054, val_acc:0.752]
Epoch [8/120    avg_loss:0.967, val_acc:0.781]
Epoch [9/120    avg_loss:0.871, val_acc:0.785]
Epoch [10/120    avg_loss:0.795, val_acc:0.787]
Epoch [11/120    avg_loss:0.787, val_acc:0.796]
Epoch [12/120    avg_loss:0.706, val_acc:0.815]
Epoch [13/120    avg_loss:0.654, val_acc:0.810]
Epoch [14/120    avg_loss:0.615, val_acc:0.848]
Epoch [15/120    avg_loss:0.565, val_acc:0.812]
Epoch [16/120    avg_loss:0.582, val_acc:0.902]
Epoch [17/120    avg_loss:0.501, val_acc:0.904]
Epoch [18/120    avg_loss:0.429, val_acc:0.931]
Epoch [19/120    avg_loss:0.435, val_acc:0.910]
Epoch [20/120    avg_loss:0.399, val_acc:0.933]
Epoch [21/120    avg_loss:0.411, val_acc:0.898]
Epoch [22/120    avg_loss:0.360, val_acc:0.952]
Epoch [23/120    avg_loss:0.318, val_acc:0.944]
Epoch [24/120    avg_loss:0.311, val_acc:0.946]
Epoch [25/120    avg_loss:0.300, val_acc:0.950]
Epoch [26/120    avg_loss:0.251, val_acc:0.963]
Epoch [27/120    avg_loss:0.272, val_acc:0.927]
Epoch [28/120    avg_loss:0.292, val_acc:0.946]
Epoch [29/120    avg_loss:0.233, val_acc:0.940]
Epoch [30/120    avg_loss:0.256, val_acc:0.956]
Epoch [31/120    avg_loss:0.198, val_acc:0.950]
Epoch [32/120    avg_loss:0.203, val_acc:0.965]
Epoch [33/120    avg_loss:0.226, val_acc:0.960]
Epoch [34/120    avg_loss:0.192, val_acc:0.946]
Epoch [35/120    avg_loss:0.188, val_acc:0.963]
Epoch [36/120    avg_loss:0.171, val_acc:0.973]
Epoch [37/120    avg_loss:0.140, val_acc:0.960]
Epoch [38/120    avg_loss:0.125, val_acc:0.975]
Epoch [39/120    avg_loss:0.198, val_acc:0.960]
Epoch [40/120    avg_loss:0.188, val_acc:0.967]
Epoch [41/120    avg_loss:0.158, val_acc:0.975]
Epoch [42/120    avg_loss:0.123, val_acc:0.977]
Epoch [43/120    avg_loss:0.097, val_acc:0.983]
Epoch [44/120    avg_loss:0.093, val_acc:0.969]
Epoch [45/120    avg_loss:0.083, val_acc:0.988]
Epoch [46/120    avg_loss:0.087, val_acc:0.985]
Epoch [47/120    avg_loss:0.081, val_acc:0.971]
Epoch [48/120    avg_loss:0.063, val_acc:0.979]
Epoch [49/120    avg_loss:0.097, val_acc:0.990]
Epoch [50/120    avg_loss:0.076, val_acc:0.988]
Epoch [51/120    avg_loss:0.104, val_acc:0.983]
Epoch [52/120    avg_loss:0.080, val_acc:0.985]
Epoch [53/120    avg_loss:0.065, val_acc:0.981]
Epoch [54/120    avg_loss:0.051, val_acc:0.990]
Epoch [55/120    avg_loss:0.086, val_acc:0.985]
Epoch [56/120    avg_loss:0.060, val_acc:0.988]
Epoch [57/120    avg_loss:0.067, val_acc:0.990]
Epoch [58/120    avg_loss:0.073, val_acc:0.983]
Epoch [59/120    avg_loss:0.075, val_acc:0.992]
Epoch [60/120    avg_loss:0.053, val_acc:0.992]
Epoch [61/120    avg_loss:0.041, val_acc:0.994]
Epoch [62/120    avg_loss:0.034, val_acc:0.992]
Epoch [63/120    avg_loss:0.045, val_acc:0.985]
Epoch [64/120    avg_loss:0.072, val_acc:0.992]
Epoch [65/120    avg_loss:0.064, val_acc:0.956]
Epoch [66/120    avg_loss:0.056, val_acc:0.994]
Epoch [67/120    avg_loss:0.047, val_acc:0.983]
Epoch [68/120    avg_loss:0.050, val_acc:0.990]
Epoch [69/120    avg_loss:0.040, val_acc:0.992]
Epoch [70/120    avg_loss:0.048, val_acc:0.983]
Epoch [71/120    avg_loss:0.069, val_acc:0.975]
Epoch [72/120    avg_loss:0.038, val_acc:0.981]
Epoch [73/120    avg_loss:0.048, val_acc:0.990]
Epoch [74/120    avg_loss:0.054, val_acc:0.983]
Epoch [75/120    avg_loss:0.051, val_acc:0.996]
Epoch [76/120    avg_loss:0.038, val_acc:0.990]
Epoch [77/120    avg_loss:0.024, val_acc:0.990]
Epoch [78/120    avg_loss:0.026, val_acc:0.994]
Epoch [79/120    avg_loss:0.021, val_acc:0.994]
Epoch [80/120    avg_loss:0.023, val_acc:0.988]
Epoch [81/120    avg_loss:0.130, val_acc:0.977]
Epoch [82/120    avg_loss:0.102, val_acc:0.979]
Epoch [83/120    avg_loss:0.114, val_acc:0.977]
Epoch [84/120    avg_loss:0.057, val_acc:0.990]
Epoch [85/120    avg_loss:0.036, val_acc:0.990]
Epoch [86/120    avg_loss:0.043, val_acc:0.981]
Epoch [87/120    avg_loss:0.045, val_acc:0.990]
Epoch [88/120    avg_loss:0.061, val_acc:0.988]
Epoch [89/120    avg_loss:0.041, val_acc:0.988]
Epoch [90/120    avg_loss:0.033, val_acc:0.988]
Epoch [91/120    avg_loss:0.028, val_acc:0.990]
Epoch [92/120    avg_loss:0.027, val_acc:0.992]
Epoch [93/120    avg_loss:0.017, val_acc:0.990]
Epoch [94/120    avg_loss:0.025, val_acc:0.992]
Epoch [95/120    avg_loss:0.019, val_acc:0.992]
Epoch [96/120    avg_loss:0.026, val_acc:0.992]
Epoch [97/120    avg_loss:0.018, val_acc:0.992]
Epoch [98/120    avg_loss:0.015, val_acc:0.994]
Epoch [99/120    avg_loss:0.016, val_acc:0.992]
Epoch [100/120    avg_loss:0.017, val_acc:0.992]
Epoch [101/120    avg_loss:0.025, val_acc:0.992]
Epoch [102/120    avg_loss:0.019, val_acc:0.992]
Epoch [103/120    avg_loss:0.018, val_acc:0.992]
Epoch [104/120    avg_loss:0.017, val_acc:0.992]
Epoch [105/120    avg_loss:0.016, val_acc:0.992]
Epoch [106/120    avg_loss:0.017, val_acc:0.992]
Epoch [107/120    avg_loss:0.022, val_acc:0.992]
Epoch [108/120    avg_loss:0.016, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.019, val_acc:0.992]
Epoch [111/120    avg_loss:0.019, val_acc:0.992]
Epoch [112/120    avg_loss:0.022, val_acc:0.992]
Epoch [113/120    avg_loss:0.021, val_acc:0.992]
Epoch [114/120    avg_loss:0.029, val_acc:0.992]
Epoch [115/120    avg_loss:0.015, val_acc:0.992]
Epoch [116/120    avg_loss:0.018, val_acc:0.992]
Epoch [117/120    avg_loss:0.015, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.020, val_acc:0.992]
Epoch [120/120    avg_loss:0.017, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.68017057569296

F1 scores:
[       nan 0.99853801 0.99095023 1.         0.98206278 0.97315436
 0.99516908 0.97826087 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9964394652405767
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7707bd828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.445, val_acc:0.402]
Epoch [2/120    avg_loss:2.094, val_acc:0.517]
Epoch [3/120    avg_loss:1.881, val_acc:0.515]
Epoch [4/120    avg_loss:1.656, val_acc:0.569]
Epoch [5/120    avg_loss:1.459, val_acc:0.690]
Epoch [6/120    avg_loss:1.250, val_acc:0.727]
Epoch [7/120    avg_loss:1.108, val_acc:0.750]
Epoch [8/120    avg_loss:1.010, val_acc:0.787]
Epoch [9/120    avg_loss:0.903, val_acc:0.802]
Epoch [10/120    avg_loss:0.836, val_acc:0.792]
Epoch [11/120    avg_loss:0.793, val_acc:0.802]
Epoch [12/120    avg_loss:0.661, val_acc:0.817]
Epoch [13/120    avg_loss:0.613, val_acc:0.800]
Epoch [14/120    avg_loss:0.587, val_acc:0.835]
Epoch [15/120    avg_loss:0.545, val_acc:0.867]
Epoch [16/120    avg_loss:0.503, val_acc:0.904]
Epoch [17/120    avg_loss:0.428, val_acc:0.919]
Epoch [18/120    avg_loss:0.382, val_acc:0.940]
Epoch [19/120    avg_loss:0.388, val_acc:0.921]
Epoch [20/120    avg_loss:0.447, val_acc:0.898]
Epoch [21/120    avg_loss:0.408, val_acc:0.902]
Epoch [22/120    avg_loss:0.363, val_acc:0.946]
Epoch [23/120    avg_loss:0.302, val_acc:0.963]
Epoch [24/120    avg_loss:0.306, val_acc:0.965]
Epoch [25/120    avg_loss:0.283, val_acc:0.950]
Epoch [26/120    avg_loss:0.258, val_acc:0.975]
Epoch [27/120    avg_loss:0.237, val_acc:0.963]
Epoch [28/120    avg_loss:0.267, val_acc:0.960]
Epoch [29/120    avg_loss:0.202, val_acc:0.975]
Epoch [30/120    avg_loss:0.236, val_acc:0.965]
Epoch [31/120    avg_loss:0.224, val_acc:0.971]
Epoch [32/120    avg_loss:0.236, val_acc:0.958]
Epoch [33/120    avg_loss:0.262, val_acc:0.933]
Epoch [34/120    avg_loss:0.241, val_acc:0.977]
Epoch [35/120    avg_loss:0.195, val_acc:0.977]
Epoch [36/120    avg_loss:0.158, val_acc:0.990]
Epoch [37/120    avg_loss:0.151, val_acc:0.975]
Epoch [38/120    avg_loss:0.188, val_acc:0.971]
Epoch [39/120    avg_loss:0.187, val_acc:0.967]
Epoch [40/120    avg_loss:0.257, val_acc:0.942]
Epoch [41/120    avg_loss:0.232, val_acc:0.977]
Epoch [42/120    avg_loss:0.184, val_acc:0.967]
Epoch [43/120    avg_loss:0.180, val_acc:0.988]
Epoch [44/120    avg_loss:0.182, val_acc:0.977]
Epoch [45/120    avg_loss:0.152, val_acc:0.977]
Epoch [46/120    avg_loss:0.121, val_acc:0.990]
Epoch [47/120    avg_loss:0.163, val_acc:0.958]
Epoch [48/120    avg_loss:0.156, val_acc:0.917]
Epoch [49/120    avg_loss:0.132, val_acc:0.979]
Epoch [50/120    avg_loss:0.125, val_acc:0.952]
Epoch [51/120    avg_loss:0.130, val_acc:0.979]
Epoch [52/120    avg_loss:0.126, val_acc:0.983]
Epoch [53/120    avg_loss:0.132, val_acc:0.992]
Epoch [54/120    avg_loss:0.095, val_acc:0.992]
Epoch [55/120    avg_loss:0.101, val_acc:0.994]
Epoch [56/120    avg_loss:0.081, val_acc:0.996]
Epoch [57/120    avg_loss:0.079, val_acc:0.988]
Epoch [58/120    avg_loss:0.114, val_acc:0.992]
Epoch [59/120    avg_loss:0.114, val_acc:0.996]
Epoch [60/120    avg_loss:0.077, val_acc:0.992]
Epoch [61/120    avg_loss:0.070, val_acc:0.998]
Epoch [62/120    avg_loss:0.067, val_acc:0.996]
Epoch [63/120    avg_loss:0.060, val_acc:0.996]
Epoch [64/120    avg_loss:0.063, val_acc:0.988]
Epoch [65/120    avg_loss:0.052, val_acc:0.990]
Epoch [66/120    avg_loss:0.081, val_acc:0.994]
Epoch [67/120    avg_loss:0.051, val_acc:0.994]
Epoch [68/120    avg_loss:0.072, val_acc:0.996]
Epoch [69/120    avg_loss:0.089, val_acc:0.998]
Epoch [70/120    avg_loss:0.063, val_acc:0.994]
Epoch [71/120    avg_loss:0.066, val_acc:0.994]
Epoch [72/120    avg_loss:0.056, val_acc:0.994]
Epoch [73/120    avg_loss:0.047, val_acc:0.998]
Epoch [74/120    avg_loss:0.057, val_acc:0.998]
Epoch [75/120    avg_loss:0.059, val_acc:0.994]
Epoch [76/120    avg_loss:0.036, val_acc:0.998]
Epoch [77/120    avg_loss:0.046, val_acc:0.998]
Epoch [78/120    avg_loss:0.041, val_acc:0.996]
Epoch [79/120    avg_loss:0.038, val_acc:0.998]
Epoch [80/120    avg_loss:0.046, val_acc:0.996]
Epoch [81/120    avg_loss:0.042, val_acc:0.998]
Epoch [82/120    avg_loss:0.044, val_acc:0.996]
Epoch [83/120    avg_loss:0.052, val_acc:0.998]
Epoch [84/120    avg_loss:0.039, val_acc:0.994]
Epoch [85/120    avg_loss:0.108, val_acc:0.998]
Epoch [86/120    avg_loss:0.054, val_acc:0.979]
Epoch [87/120    avg_loss:0.083, val_acc:0.996]
Epoch [88/120    avg_loss:0.057, val_acc:0.985]
Epoch [89/120    avg_loss:0.058, val_acc:0.988]
Epoch [90/120    avg_loss:0.053, val_acc:0.994]
Epoch [91/120    avg_loss:0.032, val_acc:0.998]
Epoch [92/120    avg_loss:0.049, val_acc:0.996]
Epoch [93/120    avg_loss:0.066, val_acc:0.992]
Epoch [94/120    avg_loss:0.048, val_acc:0.996]
Epoch [95/120    avg_loss:0.029, val_acc:0.996]
Epoch [96/120    avg_loss:0.024, val_acc:0.996]
Epoch [97/120    avg_loss:0.021, val_acc:0.998]
Epoch [98/120    avg_loss:0.023, val_acc:0.994]
Epoch [99/120    avg_loss:0.022, val_acc:0.998]
Epoch [100/120    avg_loss:0.020, val_acc:0.998]
Epoch [101/120    avg_loss:0.023, val_acc:1.000]
Epoch [102/120    avg_loss:0.021, val_acc:0.998]
Epoch [103/120    avg_loss:0.021, val_acc:0.998]
Epoch [104/120    avg_loss:0.021, val_acc:0.998]
Epoch [105/120    avg_loss:0.018, val_acc:0.998]
Epoch [106/120    avg_loss:0.015, val_acc:1.000]
Epoch [107/120    avg_loss:0.015, val_acc:0.998]
Epoch [108/120    avg_loss:0.019, val_acc:1.000]
Epoch [109/120    avg_loss:0.012, val_acc:0.998]
Epoch [110/120    avg_loss:0.013, val_acc:0.998]
Epoch [111/120    avg_loss:0.009, val_acc:0.998]
Epoch [112/120    avg_loss:0.015, val_acc:1.000]
Epoch [113/120    avg_loss:0.008, val_acc:0.998]
Epoch [114/120    avg_loss:0.040, val_acc:0.908]
Epoch [115/120    avg_loss:0.133, val_acc:0.994]
Epoch [116/120    avg_loss:0.057, val_acc:0.996]
Epoch [117/120    avg_loss:0.049, val_acc:0.990]
Epoch [118/120    avg_loss:0.073, val_acc:0.994]
Epoch [119/120    avg_loss:0.047, val_acc:0.998]
Epoch [120/120    avg_loss:0.053, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   1   0   0   0   0   0   0   2   0]
 [  0   0   0   7   5 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.99319728 0.98501071 0.98245614 0.95340502
 1.         0.98378378 1.         1.         1.         0.98950131
 0.98888889 1.        ]

Kappa:
0.9938277254079858
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3b6eb4710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.499, val_acc:0.508]
Epoch [2/120    avg_loss:2.121, val_acc:0.506]
Epoch [3/120    avg_loss:1.878, val_acc:0.571]
Epoch [4/120    avg_loss:1.661, val_acc:0.602]
Epoch [5/120    avg_loss:1.451, val_acc:0.685]
Epoch [6/120    avg_loss:1.256, val_acc:0.721]
Epoch [7/120    avg_loss:1.128, val_acc:0.752]
Epoch [8/120    avg_loss:1.002, val_acc:0.792]
Epoch [9/120    avg_loss:0.900, val_acc:0.777]
Epoch [10/120    avg_loss:0.813, val_acc:0.806]
Epoch [11/120    avg_loss:0.732, val_acc:0.829]
Epoch [12/120    avg_loss:0.674, val_acc:0.842]
Epoch [13/120    avg_loss:0.627, val_acc:0.856]
Epoch [14/120    avg_loss:0.622, val_acc:0.848]
Epoch [15/120    avg_loss:0.535, val_acc:0.863]
Epoch [16/120    avg_loss:0.521, val_acc:0.887]
Epoch [17/120    avg_loss:0.537, val_acc:0.879]
Epoch [18/120    avg_loss:0.441, val_acc:0.912]
Epoch [19/120    avg_loss:0.458, val_acc:0.921]
Epoch [20/120    avg_loss:0.376, val_acc:0.935]
Epoch [21/120    avg_loss:0.400, val_acc:0.915]
Epoch [22/120    avg_loss:0.396, val_acc:0.935]
Epoch [23/120    avg_loss:0.343, val_acc:0.942]
Epoch [24/120    avg_loss:0.327, val_acc:0.950]
Epoch [25/120    avg_loss:0.273, val_acc:0.938]
Epoch [26/120    avg_loss:0.291, val_acc:0.935]
Epoch [27/120    avg_loss:0.258, val_acc:0.944]
Epoch [28/120    avg_loss:0.240, val_acc:0.956]
Epoch [29/120    avg_loss:0.252, val_acc:0.954]
Epoch [30/120    avg_loss:0.240, val_acc:0.958]
Epoch [31/120    avg_loss:0.210, val_acc:0.956]
Epoch [32/120    avg_loss:0.225, val_acc:0.973]
Epoch [33/120    avg_loss:0.213, val_acc:0.958]
Epoch [34/120    avg_loss:0.224, val_acc:0.946]
Epoch [35/120    avg_loss:0.169, val_acc:0.963]
Epoch [36/120    avg_loss:0.171, val_acc:0.965]
Epoch [37/120    avg_loss:0.164, val_acc:0.971]
Epoch [38/120    avg_loss:0.166, val_acc:0.969]
Epoch [39/120    avg_loss:0.164, val_acc:0.965]
Epoch [40/120    avg_loss:0.148, val_acc:0.965]
Epoch [41/120    avg_loss:0.176, val_acc:0.967]
Epoch [42/120    avg_loss:0.242, val_acc:0.944]
Epoch [43/120    avg_loss:0.171, val_acc:0.967]
Epoch [44/120    avg_loss:0.154, val_acc:0.973]
Epoch [45/120    avg_loss:0.125, val_acc:0.979]
Epoch [46/120    avg_loss:0.112, val_acc:0.983]
Epoch [47/120    avg_loss:0.086, val_acc:0.985]
Epoch [48/120    avg_loss:0.100, val_acc:0.981]
Epoch [49/120    avg_loss:0.127, val_acc:0.971]
Epoch [50/120    avg_loss:0.105, val_acc:0.971]
Epoch [51/120    avg_loss:0.088, val_acc:0.983]
Epoch [52/120    avg_loss:0.065, val_acc:0.977]
Epoch [53/120    avg_loss:0.074, val_acc:0.983]
Epoch [54/120    avg_loss:0.071, val_acc:0.983]
Epoch [55/120    avg_loss:0.117, val_acc:0.952]
Epoch [56/120    avg_loss:0.107, val_acc:0.979]
Epoch [57/120    avg_loss:0.091, val_acc:0.975]
Epoch [58/120    avg_loss:0.100, val_acc:0.973]
Epoch [59/120    avg_loss:0.104, val_acc:0.975]
Epoch [60/120    avg_loss:0.075, val_acc:0.983]
Epoch [61/120    avg_loss:0.070, val_acc:0.985]
Epoch [62/120    avg_loss:0.049, val_acc:0.994]
Epoch [63/120    avg_loss:0.045, val_acc:0.994]
Epoch [64/120    avg_loss:0.050, val_acc:0.994]
Epoch [65/120    avg_loss:0.046, val_acc:0.992]
Epoch [66/120    avg_loss:0.043, val_acc:0.992]
Epoch [67/120    avg_loss:0.046, val_acc:0.994]
Epoch [68/120    avg_loss:0.035, val_acc:0.992]
Epoch [69/120    avg_loss:0.040, val_acc:0.992]
Epoch [70/120    avg_loss:0.040, val_acc:0.992]
Epoch [71/120    avg_loss:0.047, val_acc:0.992]
Epoch [72/120    avg_loss:0.039, val_acc:0.992]
Epoch [73/120    avg_loss:0.040, val_acc:0.992]
Epoch [74/120    avg_loss:0.040, val_acc:0.992]
Epoch [75/120    avg_loss:0.035, val_acc:0.994]
Epoch [76/120    avg_loss:0.041, val_acc:0.992]
Epoch [77/120    avg_loss:0.037, val_acc:0.992]
Epoch [78/120    avg_loss:0.034, val_acc:0.992]
Epoch [79/120    avg_loss:0.040, val_acc:0.992]
Epoch [80/120    avg_loss:0.039, val_acc:0.992]
Epoch [81/120    avg_loss:0.031, val_acc:0.992]
Epoch [82/120    avg_loss:0.033, val_acc:0.992]
Epoch [83/120    avg_loss:0.031, val_acc:0.992]
Epoch [84/120    avg_loss:0.038, val_acc:0.992]
Epoch [85/120    avg_loss:0.035, val_acc:0.992]
Epoch [86/120    avg_loss:0.044, val_acc:0.990]
Epoch [87/120    avg_loss:0.037, val_acc:0.990]
Epoch [88/120    avg_loss:0.034, val_acc:0.990]
Epoch [89/120    avg_loss:0.038, val_acc:0.990]
Epoch [90/120    avg_loss:0.031, val_acc:0.990]
Epoch [91/120    avg_loss:0.031, val_acc:0.990]
Epoch [92/120    avg_loss:0.036, val_acc:0.990]
Epoch [93/120    avg_loss:0.030, val_acc:0.990]
Epoch [94/120    avg_loss:0.037, val_acc:0.990]
Epoch [95/120    avg_loss:0.031, val_acc:0.990]
Epoch [96/120    avg_loss:0.031, val_acc:0.990]
Epoch [97/120    avg_loss:0.037, val_acc:0.990]
Epoch [98/120    avg_loss:0.050, val_acc:0.990]
Epoch [99/120    avg_loss:0.038, val_acc:0.990]
Epoch [100/120    avg_loss:0.039, val_acc:0.990]
Epoch [101/120    avg_loss:0.040, val_acc:0.990]
Epoch [102/120    avg_loss:0.035, val_acc:0.990]
Epoch [103/120    avg_loss:0.031, val_acc:0.990]
Epoch [104/120    avg_loss:0.032, val_acc:0.990]
Epoch [105/120    avg_loss:0.034, val_acc:0.990]
Epoch [106/120    avg_loss:0.034, val_acc:0.990]
Epoch [107/120    avg_loss:0.028, val_acc:0.990]
Epoch [108/120    avg_loss:0.032, val_acc:0.990]
Epoch [109/120    avg_loss:0.036, val_acc:0.990]
Epoch [110/120    avg_loss:0.026, val_acc:0.990]
Epoch [111/120    avg_loss:0.032, val_acc:0.990]
Epoch [112/120    avg_loss:0.027, val_acc:0.990]
Epoch [113/120    avg_loss:0.034, val_acc:0.990]
Epoch [114/120    avg_loss:0.030, val_acc:0.990]
Epoch [115/120    avg_loss:0.029, val_acc:0.990]
Epoch [116/120    avg_loss:0.036, val_acc:0.990]
Epoch [117/120    avg_loss:0.030, val_acc:0.990]
Epoch [118/120    avg_loss:0.035, val_acc:0.990]
Epoch [119/120    avg_loss:0.033, val_acc:0.990]
Epoch [120/120    avg_loss:0.039, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   6   0   0 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99412628 0.99545455 1.         0.94432071 0.91525424
 0.98095238 0.98924731 1.         0.99363057 1.         0.99734043
 0.99113082 1.        ]

Kappa:
0.9897945344815895
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d2bc9f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.506, val_acc:0.469]
Epoch [2/120    avg_loss:2.113, val_acc:0.525]
Epoch [3/120    avg_loss:1.872, val_acc:0.617]
Epoch [4/120    avg_loss:1.614, val_acc:0.648]
Epoch [5/120    avg_loss:1.400, val_acc:0.683]
Epoch [6/120    avg_loss:1.242, val_acc:0.690]
Epoch [7/120    avg_loss:1.103, val_acc:0.696]
Epoch [8/120    avg_loss:0.992, val_acc:0.760]
Epoch [9/120    avg_loss:0.960, val_acc:0.717]
Epoch [10/120    avg_loss:0.889, val_acc:0.779]
Epoch [11/120    avg_loss:0.780, val_acc:0.798]
Epoch [12/120    avg_loss:0.701, val_acc:0.794]
Epoch [13/120    avg_loss:0.647, val_acc:0.867]
Epoch [14/120    avg_loss:0.560, val_acc:0.848]
Epoch [15/120    avg_loss:0.547, val_acc:0.808]
Epoch [16/120    avg_loss:0.475, val_acc:0.848]
Epoch [17/120    avg_loss:0.481, val_acc:0.890]
Epoch [18/120    avg_loss:0.459, val_acc:0.919]
Epoch [19/120    avg_loss:0.400, val_acc:0.927]
Epoch [20/120    avg_loss:0.356, val_acc:0.919]
Epoch [21/120    avg_loss:0.357, val_acc:0.929]
Epoch [22/120    avg_loss:0.293, val_acc:0.929]
Epoch [23/120    avg_loss:0.297, val_acc:0.940]
Epoch [24/120    avg_loss:0.260, val_acc:0.931]
Epoch [25/120    avg_loss:0.266, val_acc:0.954]
Epoch [26/120    avg_loss:0.262, val_acc:0.967]
Epoch [27/120    avg_loss:0.220, val_acc:0.950]
Epoch [28/120    avg_loss:0.270, val_acc:0.954]
Epoch [29/120    avg_loss:0.272, val_acc:0.944]
Epoch [30/120    avg_loss:0.200, val_acc:0.960]
Epoch [31/120    avg_loss:0.212, val_acc:0.965]
Epoch [32/120    avg_loss:0.188, val_acc:0.950]
Epoch [33/120    avg_loss:0.220, val_acc:0.925]
Epoch [34/120    avg_loss:0.282, val_acc:0.954]
Epoch [35/120    avg_loss:0.223, val_acc:0.952]
Epoch [36/120    avg_loss:0.176, val_acc:0.975]
Epoch [37/120    avg_loss:0.214, val_acc:0.952]
Epoch [38/120    avg_loss:0.188, val_acc:0.967]
Epoch [39/120    avg_loss:0.165, val_acc:0.952]
Epoch [40/120    avg_loss:0.172, val_acc:0.948]
Epoch [41/120    avg_loss:0.167, val_acc:0.950]
Epoch [42/120    avg_loss:0.161, val_acc:0.965]
Epoch [43/120    avg_loss:0.133, val_acc:0.965]
Epoch [44/120    avg_loss:0.152, val_acc:0.975]
Epoch [45/120    avg_loss:0.099, val_acc:0.975]
Epoch [46/120    avg_loss:0.113, val_acc:0.975]
Epoch [47/120    avg_loss:0.137, val_acc:0.979]
Epoch [48/120    avg_loss:0.110, val_acc:0.981]
Epoch [49/120    avg_loss:0.099, val_acc:0.956]
Epoch [50/120    avg_loss:0.111, val_acc:0.983]
Epoch [51/120    avg_loss:0.147, val_acc:0.956]
Epoch [52/120    avg_loss:0.165, val_acc:0.979]
Epoch [53/120    avg_loss:0.130, val_acc:0.963]
Epoch [54/120    avg_loss:0.197, val_acc:0.979]
Epoch [55/120    avg_loss:0.130, val_acc:0.979]
Epoch [56/120    avg_loss:0.152, val_acc:0.900]
Epoch [57/120    avg_loss:0.109, val_acc:0.979]
Epoch [58/120    avg_loss:0.084, val_acc:0.990]
Epoch [59/120    avg_loss:0.092, val_acc:0.981]
Epoch [60/120    avg_loss:0.082, val_acc:0.979]
Epoch [61/120    avg_loss:0.055, val_acc:0.990]
Epoch [62/120    avg_loss:0.046, val_acc:0.983]
Epoch [63/120    avg_loss:0.109, val_acc:0.971]
Epoch [64/120    avg_loss:0.104, val_acc:0.992]
Epoch [65/120    avg_loss:0.080, val_acc:0.981]
Epoch [66/120    avg_loss:0.056, val_acc:0.981]
Epoch [67/120    avg_loss:0.066, val_acc:0.988]
Epoch [68/120    avg_loss:0.048, val_acc:0.992]
Epoch [69/120    avg_loss:0.043, val_acc:0.983]
Epoch [70/120    avg_loss:0.053, val_acc:0.996]
Epoch [71/120    avg_loss:0.043, val_acc:0.981]
Epoch [72/120    avg_loss:0.058, val_acc:0.985]
Epoch [73/120    avg_loss:0.058, val_acc:0.992]
Epoch [74/120    avg_loss:0.065, val_acc:0.983]
Epoch [75/120    avg_loss:0.099, val_acc:0.975]
Epoch [76/120    avg_loss:0.056, val_acc:0.977]
Epoch [77/120    avg_loss:0.076, val_acc:0.992]
Epoch [78/120    avg_loss:0.058, val_acc:0.990]
Epoch [79/120    avg_loss:0.040, val_acc:0.988]
Epoch [80/120    avg_loss:0.056, val_acc:0.981]
Epoch [81/120    avg_loss:0.061, val_acc:0.990]
Epoch [82/120    avg_loss:0.044, val_acc:0.990]
Epoch [83/120    avg_loss:0.048, val_acc:0.985]
Epoch [84/120    avg_loss:0.022, val_acc:0.990]
Epoch [85/120    avg_loss:0.026, val_acc:0.992]
Epoch [86/120    avg_loss:0.018, val_acc:0.992]
Epoch [87/120    avg_loss:0.034, val_acc:0.994]
Epoch [88/120    avg_loss:0.026, val_acc:0.992]
Epoch [89/120    avg_loss:0.018, val_acc:0.992]
Epoch [90/120    avg_loss:0.019, val_acc:0.990]
Epoch [91/120    avg_loss:0.028, val_acc:0.990]
Epoch [92/120    avg_loss:0.024, val_acc:0.992]
Epoch [93/120    avg_loss:0.020, val_acc:0.992]
Epoch [94/120    avg_loss:0.021, val_acc:0.992]
Epoch [95/120    avg_loss:0.017, val_acc:0.992]
Epoch [96/120    avg_loss:0.030, val_acc:0.992]
Epoch [97/120    avg_loss:0.027, val_acc:0.992]
Epoch [98/120    avg_loss:0.024, val_acc:0.992]
Epoch [99/120    avg_loss:0.021, val_acc:0.992]
Epoch [100/120    avg_loss:0.019, val_acc:0.992]
Epoch [101/120    avg_loss:0.016, val_acc:0.992]
Epoch [102/120    avg_loss:0.021, val_acc:0.992]
Epoch [103/120    avg_loss:0.019, val_acc:0.992]
Epoch [104/120    avg_loss:0.016, val_acc:0.992]
Epoch [105/120    avg_loss:0.024, val_acc:0.994]
Epoch [106/120    avg_loss:0.017, val_acc:0.994]
Epoch [107/120    avg_loss:0.019, val_acc:0.994]
Epoch [108/120    avg_loss:0.018, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.021, val_acc:0.994]
Epoch [111/120    avg_loss:0.025, val_acc:0.994]
Epoch [112/120    avg_loss:0.017, val_acc:0.994]
Epoch [113/120    avg_loss:0.018, val_acc:0.994]
Epoch [114/120    avg_loss:0.020, val_acc:0.994]
Epoch [115/120    avg_loss:0.017, val_acc:0.994]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.023, val_acc:0.994]
Epoch [118/120    avg_loss:0.020, val_acc:0.994]
Epoch [119/120    avg_loss:0.019, val_acc:0.994]
Epoch [120/120    avg_loss:0.022, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  11   0   0   0   0   0   0   2   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99780541 1.         1.         0.96613995 0.95652174
 0.99277108 1.         1.         1.         1.         0.99080158
 0.9900111  1.        ]

Kappa:
0.9940661408938543
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8a20196a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.809, val_acc:0.629]
Epoch [2/120    avg_loss:1.309, val_acc:0.715]
Epoch [3/120    avg_loss:1.013, val_acc:0.709]
Epoch [4/120    avg_loss:0.896, val_acc:0.801]
Epoch [5/120    avg_loss:0.969, val_acc:0.770]
Epoch [6/120    avg_loss:0.761, val_acc:0.814]
Epoch [7/120    avg_loss:0.715, val_acc:0.855]
Epoch [8/120    avg_loss:0.716, val_acc:0.859]
Epoch [9/120    avg_loss:0.672, val_acc:0.797]
Epoch [10/120    avg_loss:0.621, val_acc:0.844]
Epoch [11/120    avg_loss:0.699, val_acc:0.898]
Epoch [12/120    avg_loss:0.550, val_acc:0.867]
Epoch [13/120    avg_loss:0.473, val_acc:0.855]
Epoch [14/120    avg_loss:0.463, val_acc:0.811]
Epoch [15/120    avg_loss:0.471, val_acc:0.865]
Epoch [16/120    avg_loss:0.523, val_acc:0.832]
Epoch [17/120    avg_loss:0.468, val_acc:0.881]
Epoch [18/120    avg_loss:0.480, val_acc:0.893]
Epoch [19/120    avg_loss:0.377, val_acc:0.896]
Epoch [20/120    avg_loss:0.400, val_acc:0.922]
Epoch [21/120    avg_loss:0.350, val_acc:0.922]
Epoch [22/120    avg_loss:0.347, val_acc:0.912]
Epoch [23/120    avg_loss:0.303, val_acc:0.928]
Epoch [24/120    avg_loss:0.278, val_acc:0.928]
Epoch [25/120    avg_loss:0.308, val_acc:0.932]
Epoch [26/120    avg_loss:0.310, val_acc:0.932]
Epoch [27/120    avg_loss:0.262, val_acc:0.924]
Epoch [28/120    avg_loss:0.230, val_acc:0.920]
Epoch [29/120    avg_loss:0.236, val_acc:0.908]
Epoch [30/120    avg_loss:0.260, val_acc:0.926]
Epoch [31/120    avg_loss:0.271, val_acc:0.949]
Epoch [32/120    avg_loss:0.224, val_acc:0.936]
Epoch [33/120    avg_loss:0.216, val_acc:0.951]
Epoch [34/120    avg_loss:0.267, val_acc:0.914]
Epoch [35/120    avg_loss:0.222, val_acc:0.939]
Epoch [36/120    avg_loss:0.171, val_acc:0.932]
Epoch [37/120    avg_loss:0.192, val_acc:0.934]
Epoch [38/120    avg_loss:0.128, val_acc:0.957]
Epoch [39/120    avg_loss:0.156, val_acc:0.941]
Epoch [40/120    avg_loss:0.141, val_acc:0.961]
Epoch [41/120    avg_loss:0.147, val_acc:0.953]
Epoch [42/120    avg_loss:0.177, val_acc:0.936]
Epoch [43/120    avg_loss:0.269, val_acc:0.941]
Epoch [44/120    avg_loss:0.188, val_acc:0.957]
Epoch [45/120    avg_loss:0.128, val_acc:0.957]
Epoch [46/120    avg_loss:0.137, val_acc:0.979]
Epoch [47/120    avg_loss:0.140, val_acc:0.971]
Epoch [48/120    avg_loss:0.127, val_acc:0.971]
Epoch [49/120    avg_loss:0.118, val_acc:0.963]
Epoch [50/120    avg_loss:0.123, val_acc:0.951]
Epoch [51/120    avg_loss:0.106, val_acc:0.969]
Epoch [52/120    avg_loss:0.091, val_acc:0.977]
Epoch [53/120    avg_loss:0.123, val_acc:0.979]
Epoch [54/120    avg_loss:0.101, val_acc:0.955]
Epoch [55/120    avg_loss:0.056, val_acc:0.973]
Epoch [56/120    avg_loss:0.041, val_acc:0.971]
Epoch [57/120    avg_loss:0.047, val_acc:0.969]
Epoch [58/120    avg_loss:0.099, val_acc:0.947]
Epoch [59/120    avg_loss:0.118, val_acc:0.953]
Epoch [60/120    avg_loss:0.103, val_acc:0.969]
Epoch [61/120    avg_loss:0.087, val_acc:0.951]
Epoch [62/120    avg_loss:0.082, val_acc:0.984]
Epoch [63/120    avg_loss:0.085, val_acc:0.986]
Epoch [64/120    avg_loss:0.032, val_acc:0.965]
Epoch [65/120    avg_loss:0.104, val_acc:0.975]
Epoch [66/120    avg_loss:0.100, val_acc:0.961]
Epoch [67/120    avg_loss:0.045, val_acc:0.963]
Epoch [68/120    avg_loss:0.053, val_acc:0.982]
Epoch [69/120    avg_loss:0.022, val_acc:0.986]
Epoch [70/120    avg_loss:0.038, val_acc:0.984]
Epoch [71/120    avg_loss:0.095, val_acc:0.973]
Epoch [72/120    avg_loss:0.066, val_acc:0.992]
Epoch [73/120    avg_loss:0.047, val_acc:0.969]
Epoch [74/120    avg_loss:0.041, val_acc:0.961]
Epoch [75/120    avg_loss:0.060, val_acc:0.975]
Epoch [76/120    avg_loss:0.067, val_acc:0.977]
Epoch [77/120    avg_loss:0.035, val_acc:0.984]
Epoch [78/120    avg_loss:0.030, val_acc:0.984]
Epoch [79/120    avg_loss:0.024, val_acc:0.979]
Epoch [80/120    avg_loss:0.040, val_acc:0.973]
Epoch [81/120    avg_loss:0.044, val_acc:0.971]
Epoch [82/120    avg_loss:0.030, val_acc:0.984]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.025, val_acc:0.979]
Epoch [85/120    avg_loss:0.014, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.992]
Epoch [87/120    avg_loss:0.011, val_acc:0.994]
Epoch [88/120    avg_loss:0.007, val_acc:0.994]
Epoch [89/120    avg_loss:0.013, val_acc:0.994]
Epoch [90/120    avg_loss:0.008, val_acc:0.992]
Epoch [91/120    avg_loss:0.012, val_acc:0.994]
Epoch [92/120    avg_loss:0.011, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.011, val_acc:0.992]
Epoch [97/120    avg_loss:0.007, val_acc:0.992]
Epoch [98/120    avg_loss:0.008, val_acc:0.992]
Epoch [99/120    avg_loss:0.008, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.005, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.994]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.009, val_acc:0.992]
Epoch [107/120    avg_loss:0.013, val_acc:0.992]
Epoch [108/120    avg_loss:0.014, val_acc:0.994]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.013, val_acc:0.992]
Epoch [114/120    avg_loss:0.006, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.994]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   0 221   6   2   0   0   0   1   0   0   0   0]
 [  0   0   0   1 213  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 204   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.98165138 0.97787611 0.91612903 0.88501742
 0.99512195 0.96907216 0.99741602 0.99893276 1.         0.99734043
 0.99669239 1.        ]

Kappa:
0.987419211441619
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb54fc4a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.878, val_acc:0.547]
Epoch [2/120    avg_loss:1.201, val_acc:0.602]
Epoch [3/120    avg_loss:1.136, val_acc:0.660]
Epoch [4/120    avg_loss:0.948, val_acc:0.719]
Epoch [5/120    avg_loss:0.952, val_acc:0.783]
Epoch [6/120    avg_loss:0.775, val_acc:0.820]
Epoch [7/120    avg_loss:0.790, val_acc:0.818]
Epoch [8/120    avg_loss:0.705, val_acc:0.834]
Epoch [9/120    avg_loss:0.657, val_acc:0.891]
Epoch [10/120    avg_loss:0.598, val_acc:0.848]
Epoch [11/120    avg_loss:0.569, val_acc:0.854]
Epoch [12/120    avg_loss:0.551, val_acc:0.861]
Epoch [13/120    avg_loss:0.630, val_acc:0.850]
Epoch [14/120    avg_loss:0.592, val_acc:0.850]
Epoch [15/120    avg_loss:0.548, val_acc:0.889]
Epoch [16/120    avg_loss:0.462, val_acc:0.887]
Epoch [17/120    avg_loss:0.437, val_acc:0.900]
Epoch [18/120    avg_loss:0.438, val_acc:0.902]
Epoch [19/120    avg_loss:0.453, val_acc:0.898]
Epoch [20/120    avg_loss:0.330, val_acc:0.859]
Epoch [21/120    avg_loss:0.310, val_acc:0.906]
Epoch [22/120    avg_loss:0.397, val_acc:0.885]
Epoch [23/120    avg_loss:0.359, val_acc:0.908]
Epoch [24/120    avg_loss:0.395, val_acc:0.877]
Epoch [25/120    avg_loss:0.296, val_acc:0.910]
Epoch [26/120    avg_loss:0.320, val_acc:0.947]
Epoch [27/120    avg_loss:0.347, val_acc:0.920]
Epoch [28/120    avg_loss:0.290, val_acc:0.877]
Epoch [29/120    avg_loss:0.319, val_acc:0.924]
Epoch [30/120    avg_loss:0.235, val_acc:0.930]
Epoch [31/120    avg_loss:0.326, val_acc:0.941]
Epoch [32/120    avg_loss:0.259, val_acc:0.939]
Epoch [33/120    avg_loss:0.405, val_acc:0.916]
Epoch [34/120    avg_loss:0.328, val_acc:0.883]
Epoch [35/120    avg_loss:0.253, val_acc:0.934]
Epoch [36/120    avg_loss:0.230, val_acc:0.945]
Epoch [37/120    avg_loss:0.230, val_acc:0.936]
Epoch [38/120    avg_loss:0.219, val_acc:0.949]
Epoch [39/120    avg_loss:0.169, val_acc:0.912]
Epoch [40/120    avg_loss:0.223, val_acc:0.926]
Epoch [41/120    avg_loss:0.193, val_acc:0.943]
Epoch [42/120    avg_loss:0.168, val_acc:0.963]
Epoch [43/120    avg_loss:0.141, val_acc:0.928]
Epoch [44/120    avg_loss:0.164, val_acc:0.951]
Epoch [45/120    avg_loss:0.141, val_acc:0.951]
Epoch [46/120    avg_loss:0.207, val_acc:0.947]
Epoch [47/120    avg_loss:0.221, val_acc:0.928]
Epoch [48/120    avg_loss:0.153, val_acc:0.955]
Epoch [49/120    avg_loss:0.128, val_acc:0.977]
Epoch [50/120    avg_loss:0.159, val_acc:0.926]
Epoch [51/120    avg_loss:0.259, val_acc:0.922]
Epoch [52/120    avg_loss:0.126, val_acc:0.949]
Epoch [53/120    avg_loss:0.113, val_acc:0.949]
Epoch [54/120    avg_loss:0.100, val_acc:0.967]
Epoch [55/120    avg_loss:0.113, val_acc:0.943]
Epoch [56/120    avg_loss:0.067, val_acc:0.959]
Epoch [57/120    avg_loss:0.126, val_acc:0.953]
Epoch [58/120    avg_loss:0.109, val_acc:0.967]
Epoch [59/120    avg_loss:0.112, val_acc:0.957]
Epoch [60/120    avg_loss:0.087, val_acc:0.977]
Epoch [61/120    avg_loss:0.063, val_acc:0.947]
Epoch [62/120    avg_loss:0.055, val_acc:0.951]
Epoch [63/120    avg_loss:0.081, val_acc:0.961]
Epoch [64/120    avg_loss:0.072, val_acc:0.943]
Epoch [65/120    avg_loss:0.076, val_acc:0.957]
Epoch [66/120    avg_loss:0.082, val_acc:0.971]
Epoch [67/120    avg_loss:0.095, val_acc:0.957]
Epoch [68/120    avg_loss:0.136, val_acc:0.969]
Epoch [69/120    avg_loss:0.112, val_acc:0.963]
Epoch [70/120    avg_loss:0.142, val_acc:0.938]
Epoch [71/120    avg_loss:0.087, val_acc:0.963]
Epoch [72/120    avg_loss:0.071, val_acc:0.967]
Epoch [73/120    avg_loss:0.040, val_acc:0.975]
Epoch [74/120    avg_loss:0.052, val_acc:0.979]
Epoch [75/120    avg_loss:0.038, val_acc:0.980]
Epoch [76/120    avg_loss:0.032, val_acc:0.977]
Epoch [77/120    avg_loss:0.017, val_acc:0.975]
Epoch [78/120    avg_loss:0.018, val_acc:0.975]
Epoch [79/120    avg_loss:0.030, val_acc:0.977]
Epoch [80/120    avg_loss:0.035, val_acc:0.977]
Epoch [81/120    avg_loss:0.025, val_acc:0.977]
Epoch [82/120    avg_loss:0.016, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.017, val_acc:0.984]
Epoch [85/120    avg_loss:0.018, val_acc:0.986]
Epoch [86/120    avg_loss:0.033, val_acc:0.980]
Epoch [87/120    avg_loss:0.014, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.980]
Epoch [89/120    avg_loss:0.022, val_acc:0.980]
Epoch [90/120    avg_loss:0.015, val_acc:0.980]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.019, val_acc:0.980]
Epoch [93/120    avg_loss:0.013, val_acc:0.980]
Epoch [94/120    avg_loss:0.018, val_acc:0.979]
Epoch [95/120    avg_loss:0.015, val_acc:0.980]
Epoch [96/120    avg_loss:0.029, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.018, val_acc:0.982]
Epoch [100/120    avg_loss:0.019, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.018, val_acc:0.982]
Epoch [103/120    avg_loss:0.015, val_acc:0.982]
Epoch [104/120    avg_loss:0.021, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.022, val_acc:0.982]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.021, val_acc:0.982]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.017, val_acc:0.982]
Epoch [114/120    avg_loss:0.025, val_acc:0.982]
Epoch [115/120    avg_loss:0.014, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.982]
Epoch [117/120    avg_loss:0.014, val_acc:0.982]
Epoch [118/120    avg_loss:0.017, val_acc:0.982]
Epoch [119/120    avg_loss:0.012, val_acc:0.982]
Epoch [120/120    avg_loss:0.025, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 220   7   1   0   0   2   0   0   0   0   0]
 [  0   0   0   4 208  14   0   0   0   0   0   0   1   0]
 [  0   0   0   2  10 131   2   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   7   0   0   0   0   0   0   0   0   0 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 1.         0.97505669 0.96491228 0.91028446 0.90034364
 0.98288509 0.99470899 0.99742931 1.         1.         0.99734043
 0.98563536 1.        ]

Kappa:
0.98552001855284
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e39bc9748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.755, val_acc:0.576]
Epoch [2/120    avg_loss:1.242, val_acc:0.633]
Epoch [3/120    avg_loss:1.096, val_acc:0.730]
Epoch [4/120    avg_loss:1.083, val_acc:0.777]
Epoch [5/120    avg_loss:0.854, val_acc:0.764]
Epoch [6/120    avg_loss:0.772, val_acc:0.809]
Epoch [7/120    avg_loss:0.759, val_acc:0.836]
Epoch [8/120    avg_loss:0.682, val_acc:0.848]
Epoch [9/120    avg_loss:0.530, val_acc:0.836]
Epoch [10/120    avg_loss:0.712, val_acc:0.742]
Epoch [11/120    avg_loss:0.603, val_acc:0.854]
Epoch [12/120    avg_loss:0.553, val_acc:0.836]
Epoch [13/120    avg_loss:0.606, val_acc:0.842]
Epoch [14/120    avg_loss:0.519, val_acc:0.869]
Epoch [15/120    avg_loss:0.459, val_acc:0.893]
Epoch [16/120    avg_loss:0.443, val_acc:0.893]
Epoch [17/120    avg_loss:0.369, val_acc:0.879]
Epoch [18/120    avg_loss:0.361, val_acc:0.846]
Epoch [19/120    avg_loss:0.431, val_acc:0.875]
Epoch [20/120    avg_loss:0.347, val_acc:0.896]
Epoch [21/120    avg_loss:0.244, val_acc:0.873]
Epoch [22/120    avg_loss:0.364, val_acc:0.922]
Epoch [23/120    avg_loss:0.332, val_acc:0.891]
Epoch [24/120    avg_loss:0.316, val_acc:0.891]
Epoch [25/120    avg_loss:0.319, val_acc:0.898]
Epoch [26/120    avg_loss:0.250, val_acc:0.920]
Epoch [27/120    avg_loss:0.325, val_acc:0.918]
Epoch [28/120    avg_loss:0.327, val_acc:0.887]
Epoch [29/120    avg_loss:0.257, val_acc:0.906]
Epoch [30/120    avg_loss:0.265, val_acc:0.932]
Epoch [31/120    avg_loss:0.250, val_acc:0.922]
Epoch [32/120    avg_loss:0.281, val_acc:0.928]
Epoch [33/120    avg_loss:0.168, val_acc:0.922]
Epoch [34/120    avg_loss:0.137, val_acc:0.920]
Epoch [35/120    avg_loss:0.206, val_acc:0.908]
Epoch [36/120    avg_loss:0.133, val_acc:0.932]
Epoch [37/120    avg_loss:0.186, val_acc:0.939]
Epoch [38/120    avg_loss:0.207, val_acc:0.936]
Epoch [39/120    avg_loss:0.197, val_acc:0.926]
Epoch [40/120    avg_loss:0.229, val_acc:0.922]
Epoch [41/120    avg_loss:0.195, val_acc:0.916]
Epoch [42/120    avg_loss:0.139, val_acc:0.934]
Epoch [43/120    avg_loss:0.134, val_acc:0.953]
Epoch [44/120    avg_loss:0.160, val_acc:0.939]
Epoch [45/120    avg_loss:0.144, val_acc:0.936]
Epoch [46/120    avg_loss:0.189, val_acc:0.941]
Epoch [47/120    avg_loss:0.118, val_acc:0.938]
Epoch [48/120    avg_loss:0.123, val_acc:0.930]
Epoch [49/120    avg_loss:0.120, val_acc:0.963]
Epoch [50/120    avg_loss:0.157, val_acc:0.930]
Epoch [51/120    avg_loss:0.116, val_acc:0.920]
Epoch [52/120    avg_loss:0.117, val_acc:0.924]
Epoch [53/120    avg_loss:0.121, val_acc:0.949]
Epoch [54/120    avg_loss:0.074, val_acc:0.949]
Epoch [55/120    avg_loss:0.097, val_acc:0.955]
Epoch [56/120    avg_loss:0.171, val_acc:0.938]
Epoch [57/120    avg_loss:0.091, val_acc:0.957]
Epoch [58/120    avg_loss:0.103, val_acc:0.939]
Epoch [59/120    avg_loss:0.102, val_acc:0.955]
Epoch [60/120    avg_loss:0.051, val_acc:0.945]
Epoch [61/120    avg_loss:0.082, val_acc:0.945]
Epoch [62/120    avg_loss:0.055, val_acc:0.957]
Epoch [63/120    avg_loss:0.065, val_acc:0.961]
Epoch [64/120    avg_loss:0.024, val_acc:0.965]
Epoch [65/120    avg_loss:0.041, val_acc:0.965]
Epoch [66/120    avg_loss:0.033, val_acc:0.965]
Epoch [67/120    avg_loss:0.026, val_acc:0.965]
Epoch [68/120    avg_loss:0.025, val_acc:0.963]
Epoch [69/120    avg_loss:0.024, val_acc:0.963]
Epoch [70/120    avg_loss:0.026, val_acc:0.965]
Epoch [71/120    avg_loss:0.030, val_acc:0.963]
Epoch [72/120    avg_loss:0.027, val_acc:0.965]
Epoch [73/120    avg_loss:0.018, val_acc:0.969]
Epoch [74/120    avg_loss:0.029, val_acc:0.967]
Epoch [75/120    avg_loss:0.020, val_acc:0.965]
Epoch [76/120    avg_loss:0.016, val_acc:0.967]
Epoch [77/120    avg_loss:0.022, val_acc:0.963]
Epoch [78/120    avg_loss:0.017, val_acc:0.965]
Epoch [79/120    avg_loss:0.018, val_acc:0.967]
Epoch [80/120    avg_loss:0.025, val_acc:0.967]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.027, val_acc:0.965]
Epoch [83/120    avg_loss:0.017, val_acc:0.967]
Epoch [84/120    avg_loss:0.017, val_acc:0.963]
Epoch [85/120    avg_loss:0.038, val_acc:0.961]
Epoch [86/120    avg_loss:0.021, val_acc:0.967]
Epoch [87/120    avg_loss:0.012, val_acc:0.967]
Epoch [88/120    avg_loss:0.017, val_acc:0.967]
Epoch [89/120    avg_loss:0.027, val_acc:0.965]
Epoch [90/120    avg_loss:0.031, val_acc:0.965]
Epoch [91/120    avg_loss:0.014, val_acc:0.965]
Epoch [92/120    avg_loss:0.020, val_acc:0.965]
Epoch [93/120    avg_loss:0.016, val_acc:0.965]
Epoch [94/120    avg_loss:0.016, val_acc:0.965]
Epoch [95/120    avg_loss:0.022, val_acc:0.967]
Epoch [96/120    avg_loss:0.016, val_acc:0.965]
Epoch [97/120    avg_loss:0.017, val_acc:0.967]
Epoch [98/120    avg_loss:0.017, val_acc:0.967]
Epoch [99/120    avg_loss:0.020, val_acc:0.967]
Epoch [100/120    avg_loss:0.013, val_acc:0.967]
Epoch [101/120    avg_loss:0.021, val_acc:0.967]
Epoch [102/120    avg_loss:0.024, val_acc:0.967]
Epoch [103/120    avg_loss:0.012, val_acc:0.967]
Epoch [104/120    avg_loss:0.017, val_acc:0.967]
Epoch [105/120    avg_loss:0.017, val_acc:0.967]
Epoch [106/120    avg_loss:0.014, val_acc:0.967]
Epoch [107/120    avg_loss:0.024, val_acc:0.967]
Epoch [108/120    avg_loss:0.016, val_acc:0.967]
Epoch [109/120    avg_loss:0.032, val_acc:0.967]
Epoch [110/120    avg_loss:0.023, val_acc:0.967]
Epoch [111/120    avg_loss:0.015, val_acc:0.967]
Epoch [112/120    avg_loss:0.022, val_acc:0.967]
Epoch [113/120    avg_loss:0.013, val_acc:0.967]
Epoch [114/120    avg_loss:0.012, val_acc:0.967]
Epoch [115/120    avg_loss:0.021, val_acc:0.967]
Epoch [116/120    avg_loss:0.020, val_acc:0.967]
Epoch [117/120    avg_loss:0.014, val_acc:0.967]
Epoch [118/120    avg_loss:0.021, val_acc:0.967]
Epoch [119/120    avg_loss:0.013, val_acc:0.967]
Epoch [120/120    avg_loss:0.021, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 214   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   1 225   2   0   0   0   0   2   0   0   0   0]
 [  0   0   0   3 204  18   0   0   0   0   0   0   2   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   5   0   0   5   4 192   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   4   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.35820895522387

F1 scores:
[       nan 0.99491649 0.98165138 0.98253275 0.87553648 0.84027778
 0.96482412 0.97894737 1.         0.9978678  1.         1.
 0.99224806 1.        ]

Kappa:
0.9817186559210385
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbf759ee748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.847, val_acc:0.635]
Epoch [2/120    avg_loss:1.255, val_acc:0.674]
Epoch [3/120    avg_loss:0.976, val_acc:0.775]
Epoch [4/120    avg_loss:0.856, val_acc:0.756]
Epoch [5/120    avg_loss:0.881, val_acc:0.803]
Epoch [6/120    avg_loss:0.793, val_acc:0.803]
Epoch [7/120    avg_loss:0.612, val_acc:0.826]
Epoch [8/120    avg_loss:0.682, val_acc:0.822]
Epoch [9/120    avg_loss:0.665, val_acc:0.855]
Epoch [10/120    avg_loss:0.577, val_acc:0.777]
Epoch [11/120    avg_loss:0.517, val_acc:0.861]
Epoch [12/120    avg_loss:0.513, val_acc:0.857]
Epoch [13/120    avg_loss:0.564, val_acc:0.857]
Epoch [14/120    avg_loss:0.491, val_acc:0.836]
Epoch [15/120    avg_loss:0.485, val_acc:0.844]
Epoch [16/120    avg_loss:0.377, val_acc:0.875]
Epoch [17/120    avg_loss:0.490, val_acc:0.877]
Epoch [18/120    avg_loss:0.391, val_acc:0.887]
Epoch [19/120    avg_loss:0.409, val_acc:0.910]
Epoch [20/120    avg_loss:0.383, val_acc:0.904]
Epoch [21/120    avg_loss:0.264, val_acc:0.910]
Epoch [22/120    avg_loss:0.285, val_acc:0.904]
Epoch [23/120    avg_loss:0.326, val_acc:0.910]
Epoch [24/120    avg_loss:0.268, val_acc:0.908]
Epoch [25/120    avg_loss:0.266, val_acc:0.914]
Epoch [26/120    avg_loss:0.318, val_acc:0.912]
Epoch [27/120    avg_loss:0.245, val_acc:0.939]
Epoch [28/120    avg_loss:0.241, val_acc:0.885]
Epoch [29/120    avg_loss:0.294, val_acc:0.928]
Epoch [30/120    avg_loss:0.208, val_acc:0.902]
Epoch [31/120    avg_loss:0.308, val_acc:0.920]
Epoch [32/120    avg_loss:0.240, val_acc:0.936]
Epoch [33/120    avg_loss:0.228, val_acc:0.908]
Epoch [34/120    avg_loss:0.178, val_acc:0.953]
Epoch [35/120    avg_loss:0.253, val_acc:0.918]
Epoch [36/120    avg_loss:0.231, val_acc:0.883]
Epoch [37/120    avg_loss:0.212, val_acc:0.926]
Epoch [38/120    avg_loss:0.161, val_acc:0.955]
Epoch [39/120    avg_loss:0.183, val_acc:0.945]
Epoch [40/120    avg_loss:0.151, val_acc:0.924]
Epoch [41/120    avg_loss:0.170, val_acc:0.947]
Epoch [42/120    avg_loss:0.177, val_acc:0.955]
Epoch [43/120    avg_loss:0.193, val_acc:0.957]
Epoch [44/120    avg_loss:0.192, val_acc:0.959]
Epoch [45/120    avg_loss:0.171, val_acc:0.939]
Epoch [46/120    avg_loss:0.102, val_acc:0.951]
Epoch [47/120    avg_loss:0.139, val_acc:0.961]
Epoch [48/120    avg_loss:0.151, val_acc:0.959]
Epoch [49/120    avg_loss:0.114, val_acc:0.912]
Epoch [50/120    avg_loss:0.131, val_acc:0.951]
Epoch [51/120    avg_loss:0.168, val_acc:0.955]
Epoch [52/120    avg_loss:0.104, val_acc:0.953]
Epoch [53/120    avg_loss:0.098, val_acc:0.971]
Epoch [54/120    avg_loss:0.060, val_acc:0.971]
Epoch [55/120    avg_loss:0.052, val_acc:0.971]
Epoch [56/120    avg_loss:0.101, val_acc:0.965]
Epoch [57/120    avg_loss:0.047, val_acc:0.971]
Epoch [58/120    avg_loss:0.039, val_acc:0.971]
Epoch [59/120    avg_loss:0.065, val_acc:0.955]
Epoch [60/120    avg_loss:0.080, val_acc:0.967]
Epoch [61/120    avg_loss:0.064, val_acc:0.979]
Epoch [62/120    avg_loss:0.084, val_acc:0.963]
Epoch [63/120    avg_loss:0.076, val_acc:0.949]
Epoch [64/120    avg_loss:0.084, val_acc:0.951]
Epoch [65/120    avg_loss:0.072, val_acc:0.967]
Epoch [66/120    avg_loss:0.086, val_acc:0.908]
Epoch [67/120    avg_loss:0.081, val_acc:0.971]
Epoch [68/120    avg_loss:0.086, val_acc:0.965]
Epoch [69/120    avg_loss:0.059, val_acc:0.963]
Epoch [70/120    avg_loss:0.077, val_acc:0.967]
Epoch [71/120    avg_loss:0.075, val_acc:0.963]
Epoch [72/120    avg_loss:0.079, val_acc:0.971]
Epoch [73/120    avg_loss:0.071, val_acc:0.957]
Epoch [74/120    avg_loss:0.077, val_acc:0.979]
Epoch [75/120    avg_loss:0.101, val_acc:0.975]
Epoch [76/120    avg_loss:0.072, val_acc:0.965]
Epoch [77/120    avg_loss:0.064, val_acc:0.975]
Epoch [78/120    avg_loss:0.045, val_acc:0.969]
Epoch [79/120    avg_loss:0.089, val_acc:0.975]
Epoch [80/120    avg_loss:0.071, val_acc:0.955]
Epoch [81/120    avg_loss:0.066, val_acc:0.965]
Epoch [82/120    avg_loss:0.071, val_acc:0.965]
Epoch [83/120    avg_loss:0.090, val_acc:0.975]
Epoch [84/120    avg_loss:0.030, val_acc:0.973]
Epoch [85/120    avg_loss:0.051, val_acc:0.969]
Epoch [86/120    avg_loss:0.039, val_acc:0.967]
Epoch [87/120    avg_loss:0.032, val_acc:0.965]
Epoch [88/120    avg_loss:0.039, val_acc:0.973]
Epoch [89/120    avg_loss:0.020, val_acc:0.975]
Epoch [90/120    avg_loss:0.014, val_acc:0.977]
Epoch [91/120    avg_loss:0.028, val_acc:0.975]
Epoch [92/120    avg_loss:0.010, val_acc:0.977]
Epoch [93/120    avg_loss:0.011, val_acc:0.977]
Epoch [94/120    avg_loss:0.009, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.977]
Epoch [97/120    avg_loss:0.007, val_acc:0.977]
Epoch [98/120    avg_loss:0.016, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.005, val_acc:0.979]
Epoch [102/120    avg_loss:0.026, val_acc:0.979]
Epoch [103/120    avg_loss:0.019, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.980]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.017, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.979]
Epoch [110/120    avg_loss:0.014, val_acc:0.977]
Epoch [111/120    avg_loss:0.012, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.979]
Epoch [113/120    avg_loss:0.009, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.979]
Epoch [119/120    avg_loss:0.016, val_acc:0.979]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 213   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   1 224   2   0   0   0   3   0   0   0   0   0]
 [  0   0   0   2 210  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 0.99708879 0.98383372 0.98245614 0.92715232 0.9261745
 0.98271605 0.97916667 0.99356499 1.         1.         1.
 1.         1.        ]

Kappa:
0.9893173046504271
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd3049566a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.886, val_acc:0.660]
Epoch [2/120    avg_loss:1.238, val_acc:0.762]
Epoch [3/120    avg_loss:1.027, val_acc:0.689]
Epoch [4/120    avg_loss:0.982, val_acc:0.816]
Epoch [5/120    avg_loss:0.909, val_acc:0.809]
Epoch [6/120    avg_loss:0.774, val_acc:0.816]
Epoch [7/120    avg_loss:0.795, val_acc:0.779]
Epoch [8/120    avg_loss:0.675, val_acc:0.826]
Epoch [9/120    avg_loss:0.679, val_acc:0.793]
Epoch [10/120    avg_loss:0.761, val_acc:0.846]
Epoch [11/120    avg_loss:0.759, val_acc:0.838]
Epoch [12/120    avg_loss:0.538, val_acc:0.861]
Epoch [13/120    avg_loss:0.569, val_acc:0.865]
Epoch [14/120    avg_loss:0.495, val_acc:0.854]
Epoch [15/120    avg_loss:0.506, val_acc:0.840]
Epoch [16/120    avg_loss:0.549, val_acc:0.867]
Epoch [17/120    avg_loss:0.556, val_acc:0.840]
Epoch [18/120    avg_loss:0.571, val_acc:0.848]
Epoch [19/120    avg_loss:0.526, val_acc:0.904]
Epoch [20/120    avg_loss:0.442, val_acc:0.900]
Epoch [21/120    avg_loss:0.408, val_acc:0.889]
Epoch [22/120    avg_loss:0.475, val_acc:0.854]
Epoch [23/120    avg_loss:0.482, val_acc:0.846]
Epoch [24/120    avg_loss:0.505, val_acc:0.914]
Epoch [25/120    avg_loss:0.377, val_acc:0.891]
Epoch [26/120    avg_loss:0.372, val_acc:0.869]
Epoch [27/120    avg_loss:0.377, val_acc:0.906]
Epoch [28/120    avg_loss:0.292, val_acc:0.887]
Epoch [29/120    avg_loss:0.378, val_acc:0.889]
Epoch [30/120    avg_loss:0.292, val_acc:0.922]
Epoch [31/120    avg_loss:0.291, val_acc:0.916]
Epoch [32/120    avg_loss:0.265, val_acc:0.906]
Epoch [33/120    avg_loss:0.387, val_acc:0.887]
Epoch [34/120    avg_loss:0.234, val_acc:0.914]
Epoch [35/120    avg_loss:0.252, val_acc:0.902]
Epoch [36/120    avg_loss:0.238, val_acc:0.904]
Epoch [37/120    avg_loss:0.202, val_acc:0.912]
Epoch [38/120    avg_loss:0.214, val_acc:0.877]
Epoch [39/120    avg_loss:0.258, val_acc:0.920]
Epoch [40/120    avg_loss:0.232, val_acc:0.908]
Epoch [41/120    avg_loss:0.272, val_acc:0.920]
Epoch [42/120    avg_loss:0.235, val_acc:0.910]
Epoch [43/120    avg_loss:0.258, val_acc:0.924]
Epoch [44/120    avg_loss:0.242, val_acc:0.920]
Epoch [45/120    avg_loss:0.185, val_acc:0.887]
Epoch [46/120    avg_loss:0.292, val_acc:0.928]
Epoch [47/120    avg_loss:0.258, val_acc:0.910]
Epoch [48/120    avg_loss:0.209, val_acc:0.938]
Epoch [49/120    avg_loss:0.164, val_acc:0.939]
Epoch [50/120    avg_loss:0.231, val_acc:0.945]
Epoch [51/120    avg_loss:0.132, val_acc:0.951]
Epoch [52/120    avg_loss:0.145, val_acc:0.932]
Epoch [53/120    avg_loss:0.132, val_acc:0.926]
Epoch [54/120    avg_loss:0.209, val_acc:0.934]
Epoch [55/120    avg_loss:0.215, val_acc:0.932]
Epoch [56/120    avg_loss:0.167, val_acc:0.939]
Epoch [57/120    avg_loss:0.169, val_acc:0.900]
Epoch [58/120    avg_loss:0.188, val_acc:0.945]
Epoch [59/120    avg_loss:0.170, val_acc:0.945]
Epoch [60/120    avg_loss:0.124, val_acc:0.959]
Epoch [61/120    avg_loss:0.116, val_acc:0.949]
Epoch [62/120    avg_loss:0.099, val_acc:0.943]
Epoch [63/120    avg_loss:0.143, val_acc:0.939]
Epoch [64/120    avg_loss:0.104, val_acc:0.963]
Epoch [65/120    avg_loss:0.085, val_acc:0.949]
Epoch [66/120    avg_loss:0.087, val_acc:0.963]
Epoch [67/120    avg_loss:0.145, val_acc:0.953]
Epoch [68/120    avg_loss:0.121, val_acc:0.967]
Epoch [69/120    avg_loss:0.131, val_acc:0.957]
Epoch [70/120    avg_loss:0.107, val_acc:0.934]
Epoch [71/120    avg_loss:0.122, val_acc:0.936]
Epoch [72/120    avg_loss:0.140, val_acc:0.957]
Epoch [73/120    avg_loss:0.088, val_acc:0.955]
Epoch [74/120    avg_loss:0.081, val_acc:0.949]
Epoch [75/120    avg_loss:0.115, val_acc:0.939]
Epoch [76/120    avg_loss:0.038, val_acc:0.957]
Epoch [77/120    avg_loss:0.066, val_acc:0.926]
Epoch [78/120    avg_loss:0.086, val_acc:0.945]
Epoch [79/120    avg_loss:0.063, val_acc:0.939]
Epoch [80/120    avg_loss:0.214, val_acc:0.906]
Epoch [81/120    avg_loss:0.167, val_acc:0.953]
Epoch [82/120    avg_loss:0.069, val_acc:0.961]
Epoch [83/120    avg_loss:0.032, val_acc:0.963]
Epoch [84/120    avg_loss:0.057, val_acc:0.969]
Epoch [85/120    avg_loss:0.072, val_acc:0.967]
Epoch [86/120    avg_loss:0.032, val_acc:0.971]
Epoch [87/120    avg_loss:0.034, val_acc:0.969]
Epoch [88/120    avg_loss:0.035, val_acc:0.971]
Epoch [89/120    avg_loss:0.031, val_acc:0.973]
Epoch [90/120    avg_loss:0.022, val_acc:0.973]
Epoch [91/120    avg_loss:0.034, val_acc:0.971]
Epoch [92/120    avg_loss:0.026, val_acc:0.973]
Epoch [93/120    avg_loss:0.028, val_acc:0.973]
Epoch [94/120    avg_loss:0.033, val_acc:0.975]
Epoch [95/120    avg_loss:0.022, val_acc:0.973]
Epoch [96/120    avg_loss:0.029, val_acc:0.973]
Epoch [97/120    avg_loss:0.021, val_acc:0.973]
Epoch [98/120    avg_loss:0.031, val_acc:0.973]
Epoch [99/120    avg_loss:0.037, val_acc:0.975]
Epoch [100/120    avg_loss:0.018, val_acc:0.973]
Epoch [101/120    avg_loss:0.028, val_acc:0.973]
Epoch [102/120    avg_loss:0.018, val_acc:0.973]
Epoch [103/120    avg_loss:0.036, val_acc:0.973]
Epoch [104/120    avg_loss:0.025, val_acc:0.973]
Epoch [105/120    avg_loss:0.027, val_acc:0.973]
Epoch [106/120    avg_loss:0.013, val_acc:0.973]
Epoch [107/120    avg_loss:0.020, val_acc:0.973]
Epoch [108/120    avg_loss:0.019, val_acc:0.973]
Epoch [109/120    avg_loss:0.021, val_acc:0.975]
Epoch [110/120    avg_loss:0.033, val_acc:0.973]
Epoch [111/120    avg_loss:0.019, val_acc:0.975]
Epoch [112/120    avg_loss:0.044, val_acc:0.979]
Epoch [113/120    avg_loss:0.028, val_acc:0.975]
Epoch [114/120    avg_loss:0.028, val_acc:0.975]
Epoch [115/120    avg_loss:0.022, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.979]
Epoch [117/120    avg_loss:0.022, val_acc:0.975]
Epoch [118/120    avg_loss:0.024, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.013, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   1   0   0   0   0   7   0]
 [  0   0   1 226   1   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 212  11   0   0   0   0   0   0   4   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   1 196   2   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   5   0   0   0   0   0   0   0   0   4 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 0.99927061 0.96788991 0.99122807 0.90987124 0.89045936
 0.97512438 0.98429319 0.99870968 0.9978678  1.         0.99206349
 0.97582418 1.        ]

Kappa:
0.983856122883799
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38c38d2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.820, val_acc:0.723]
Epoch [2/120    avg_loss:1.208, val_acc:0.746]
Epoch [3/120    avg_loss:1.156, val_acc:0.740]
Epoch [4/120    avg_loss:1.092, val_acc:0.723]
Epoch [5/120    avg_loss:0.830, val_acc:0.797]
Epoch [6/120    avg_loss:0.678, val_acc:0.830]
Epoch [7/120    avg_loss:0.764, val_acc:0.744]
Epoch [8/120    avg_loss:0.741, val_acc:0.852]
Epoch [9/120    avg_loss:0.549, val_acc:0.766]
Epoch [10/120    avg_loss:0.710, val_acc:0.842]
Epoch [11/120    avg_loss:0.563, val_acc:0.781]
Epoch [12/120    avg_loss:0.659, val_acc:0.801]
Epoch [13/120    avg_loss:0.569, val_acc:0.861]
Epoch [14/120    avg_loss:0.566, val_acc:0.895]
Epoch [15/120    avg_loss:0.441, val_acc:0.871]
Epoch [16/120    avg_loss:0.573, val_acc:0.871]
Epoch [17/120    avg_loss:0.419, val_acc:0.832]
Epoch [18/120    avg_loss:0.438, val_acc:0.885]
Epoch [19/120    avg_loss:0.466, val_acc:0.869]
Epoch [20/120    avg_loss:0.343, val_acc:0.889]
Epoch [21/120    avg_loss:0.399, val_acc:0.867]
Epoch [22/120    avg_loss:0.479, val_acc:0.881]
Epoch [23/120    avg_loss:0.369, val_acc:0.918]
Epoch [24/120    avg_loss:0.369, val_acc:0.900]
Epoch [25/120    avg_loss:0.328, val_acc:0.910]
Epoch [26/120    avg_loss:0.300, val_acc:0.908]
Epoch [27/120    avg_loss:0.334, val_acc:0.904]
Epoch [28/120    avg_loss:0.381, val_acc:0.898]
Epoch [29/120    avg_loss:0.297, val_acc:0.900]
Epoch [30/120    avg_loss:0.342, val_acc:0.889]
Epoch [31/120    avg_loss:0.215, val_acc:0.871]
Epoch [32/120    avg_loss:0.226, val_acc:0.891]
Epoch [33/120    avg_loss:0.213, val_acc:0.912]
Epoch [34/120    avg_loss:0.254, val_acc:0.904]
Epoch [35/120    avg_loss:0.221, val_acc:0.914]
Epoch [36/120    avg_loss:0.278, val_acc:0.912]
Epoch [37/120    avg_loss:0.230, val_acc:0.936]
Epoch [38/120    avg_loss:0.175, val_acc:0.928]
Epoch [39/120    avg_loss:0.195, val_acc:0.938]
Epoch [40/120    avg_loss:0.164, val_acc:0.938]
Epoch [41/120    avg_loss:0.130, val_acc:0.941]
Epoch [42/120    avg_loss:0.114, val_acc:0.934]
Epoch [43/120    avg_loss:0.117, val_acc:0.932]
Epoch [44/120    avg_loss:0.132, val_acc:0.943]
Epoch [45/120    avg_loss:0.137, val_acc:0.938]
Epoch [46/120    avg_loss:0.120, val_acc:0.941]
Epoch [47/120    avg_loss:0.112, val_acc:0.941]
Epoch [48/120    avg_loss:0.109, val_acc:0.943]
Epoch [49/120    avg_loss:0.136, val_acc:0.949]
Epoch [50/120    avg_loss:0.123, val_acc:0.941]
Epoch [51/120    avg_loss:0.141, val_acc:0.947]
Epoch [52/120    avg_loss:0.134, val_acc:0.943]
Epoch [53/120    avg_loss:0.108, val_acc:0.943]
Epoch [54/120    avg_loss:0.091, val_acc:0.953]
Epoch [55/120    avg_loss:0.120, val_acc:0.947]
Epoch [56/120    avg_loss:0.138, val_acc:0.951]
Epoch [57/120    avg_loss:0.134, val_acc:0.949]
Epoch [58/120    avg_loss:0.117, val_acc:0.945]
Epoch [59/120    avg_loss:0.085, val_acc:0.947]
Epoch [60/120    avg_loss:0.094, val_acc:0.949]
Epoch [61/120    avg_loss:0.095, val_acc:0.949]
Epoch [62/120    avg_loss:0.085, val_acc:0.949]
Epoch [63/120    avg_loss:0.096, val_acc:0.949]
Epoch [64/120    avg_loss:0.067, val_acc:0.945]
Epoch [65/120    avg_loss:0.106, val_acc:0.947]
Epoch [66/120    avg_loss:0.081, val_acc:0.949]
Epoch [67/120    avg_loss:0.111, val_acc:0.951]
Epoch [68/120    avg_loss:0.082, val_acc:0.953]
Epoch [69/120    avg_loss:0.093, val_acc:0.953]
Epoch [70/120    avg_loss:0.082, val_acc:0.953]
Epoch [71/120    avg_loss:0.081, val_acc:0.953]
Epoch [72/120    avg_loss:0.083, val_acc:0.953]
Epoch [73/120    avg_loss:0.088, val_acc:0.953]
Epoch [74/120    avg_loss:0.092, val_acc:0.953]
Epoch [75/120    avg_loss:0.082, val_acc:0.951]
Epoch [76/120    avg_loss:0.067, val_acc:0.951]
Epoch [77/120    avg_loss:0.107, val_acc:0.951]
Epoch [78/120    avg_loss:0.081, val_acc:0.951]
Epoch [79/120    avg_loss:0.074, val_acc:0.951]
Epoch [80/120    avg_loss:0.091, val_acc:0.951]
Epoch [81/120    avg_loss:0.085, val_acc:0.951]
Epoch [82/120    avg_loss:0.067, val_acc:0.951]
Epoch [83/120    avg_loss:0.085, val_acc:0.951]
Epoch [84/120    avg_loss:0.109, val_acc:0.951]
Epoch [85/120    avg_loss:0.095, val_acc:0.953]
Epoch [86/120    avg_loss:0.103, val_acc:0.951]
Epoch [87/120    avg_loss:0.095, val_acc:0.951]
Epoch [88/120    avg_loss:0.105, val_acc:0.951]
Epoch [89/120    avg_loss:0.061, val_acc:0.951]
Epoch [90/120    avg_loss:0.088, val_acc:0.951]
Epoch [91/120    avg_loss:0.088, val_acc:0.951]
Epoch [92/120    avg_loss:0.081, val_acc:0.951]
Epoch [93/120    avg_loss:0.068, val_acc:0.949]
Epoch [94/120    avg_loss:0.102, val_acc:0.949]
Epoch [95/120    avg_loss:0.081, val_acc:0.949]
Epoch [96/120    avg_loss:0.085, val_acc:0.949]
Epoch [97/120    avg_loss:0.096, val_acc:0.949]
Epoch [98/120    avg_loss:0.072, val_acc:0.949]
Epoch [99/120    avg_loss:0.083, val_acc:0.949]
Epoch [100/120    avg_loss:0.069, val_acc:0.949]
Epoch [101/120    avg_loss:0.094, val_acc:0.949]
Epoch [102/120    avg_loss:0.085, val_acc:0.949]
Epoch [103/120    avg_loss:0.065, val_acc:0.949]
Epoch [104/120    avg_loss:0.132, val_acc:0.949]
Epoch [105/120    avg_loss:0.068, val_acc:0.949]
Epoch [106/120    avg_loss:0.105, val_acc:0.949]
Epoch [107/120    avg_loss:0.071, val_acc:0.949]
Epoch [108/120    avg_loss:0.081, val_acc:0.949]
Epoch [109/120    avg_loss:0.084, val_acc:0.949]
Epoch [110/120    avg_loss:0.096, val_acc:0.949]
Epoch [111/120    avg_loss:0.068, val_acc:0.949]
Epoch [112/120    avg_loss:0.066, val_acc:0.949]
Epoch [113/120    avg_loss:0.109, val_acc:0.949]
Epoch [114/120    avg_loss:0.076, val_acc:0.949]
Epoch [115/120    avg_loss:0.083, val_acc:0.949]
Epoch [116/120    avg_loss:0.084, val_acc:0.949]
Epoch [117/120    avg_loss:0.095, val_acc:0.949]
Epoch [118/120    avg_loss:0.073, val_acc:0.949]
Epoch [119/120    avg_loss:0.115, val_acc:0.949]
Epoch [120/120    avg_loss:0.086, val_acc:0.949]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   0 217  10   1   0   0   1   1   0   0   0   0]
 [  0   0   0   0 182  45   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 132   4   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   2   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.95309168443497

F1 scores:
[       nan 1.         0.95104895 0.96659243 0.85046729 0.81733746
 0.99038462 0.92079208 0.99612903 0.99893276 1.         0.99734043
 0.99224806 1.        ]

Kappa:
0.977217735130972
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94bb6f47b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.875, val_acc:0.670]
Epoch [2/120    avg_loss:1.168, val_acc:0.730]
Epoch [3/120    avg_loss:1.178, val_acc:0.617]
Epoch [4/120    avg_loss:1.125, val_acc:0.688]
Epoch [5/120    avg_loss:0.877, val_acc:0.809]
Epoch [6/120    avg_loss:0.719, val_acc:0.740]
Epoch [7/120    avg_loss:0.700, val_acc:0.805]
Epoch [8/120    avg_loss:0.770, val_acc:0.822]
Epoch [9/120    avg_loss:0.630, val_acc:0.865]
Epoch [10/120    avg_loss:0.734, val_acc:0.848]
Epoch [11/120    avg_loss:0.571, val_acc:0.828]
Epoch [12/120    avg_loss:0.409, val_acc:0.863]
Epoch [13/120    avg_loss:0.566, val_acc:0.842]
Epoch [14/120    avg_loss:0.496, val_acc:0.775]
Epoch [15/120    avg_loss:0.454, val_acc:0.877]
Epoch [16/120    avg_loss:0.473, val_acc:0.875]
Epoch [17/120    avg_loss:0.374, val_acc:0.869]
Epoch [18/120    avg_loss:0.281, val_acc:0.887]
Epoch [19/120    avg_loss:0.433, val_acc:0.891]
Epoch [20/120    avg_loss:0.367, val_acc:0.900]
Epoch [21/120    avg_loss:0.326, val_acc:0.895]
Epoch [22/120    avg_loss:0.371, val_acc:0.906]
Epoch [23/120    avg_loss:0.327, val_acc:0.916]
Epoch [24/120    avg_loss:0.282, val_acc:0.902]
Epoch [25/120    avg_loss:0.281, val_acc:0.871]
Epoch [26/120    avg_loss:0.232, val_acc:0.926]
Epoch [27/120    avg_loss:0.166, val_acc:0.912]
Epoch [28/120    avg_loss:0.233, val_acc:0.932]
Epoch [29/120    avg_loss:0.272, val_acc:0.932]
Epoch [30/120    avg_loss:0.290, val_acc:0.920]
Epoch [31/120    avg_loss:0.199, val_acc:0.918]
Epoch [32/120    avg_loss:0.295, val_acc:0.918]
Epoch [33/120    avg_loss:0.274, val_acc:0.930]
Epoch [34/120    avg_loss:0.197, val_acc:0.949]
Epoch [35/120    avg_loss:0.196, val_acc:0.939]
Epoch [36/120    avg_loss:0.192, val_acc:0.928]
Epoch [37/120    avg_loss:0.227, val_acc:0.910]
Epoch [38/120    avg_loss:0.172, val_acc:0.928]
Epoch [39/120    avg_loss:0.186, val_acc:0.965]
Epoch [40/120    avg_loss:0.213, val_acc:0.922]
Epoch [41/120    avg_loss:0.192, val_acc:0.945]
Epoch [42/120    avg_loss:0.181, val_acc:0.951]
Epoch [43/120    avg_loss:0.189, val_acc:0.908]
Epoch [44/120    avg_loss:0.222, val_acc:0.955]
Epoch [45/120    avg_loss:0.171, val_acc:0.945]
Epoch [46/120    avg_loss:0.114, val_acc:0.939]
Epoch [47/120    avg_loss:0.123, val_acc:0.953]
Epoch [48/120    avg_loss:0.241, val_acc:0.939]
Epoch [49/120    avg_loss:0.102, val_acc:0.959]
Epoch [50/120    avg_loss:0.126, val_acc:0.941]
Epoch [51/120    avg_loss:0.107, val_acc:0.967]
Epoch [52/120    avg_loss:0.115, val_acc:0.963]
Epoch [53/120    avg_loss:0.112, val_acc:0.938]
Epoch [54/120    avg_loss:0.099, val_acc:0.949]
Epoch [55/120    avg_loss:0.110, val_acc:0.951]
Epoch [56/120    avg_loss:0.112, val_acc:0.961]
Epoch [57/120    avg_loss:0.052, val_acc:0.967]
Epoch [58/120    avg_loss:0.079, val_acc:0.967]
Epoch [59/120    avg_loss:0.110, val_acc:0.957]
Epoch [60/120    avg_loss:0.108, val_acc:0.922]
Epoch [61/120    avg_loss:0.166, val_acc:0.949]
Epoch [62/120    avg_loss:0.066, val_acc:0.973]
Epoch [63/120    avg_loss:0.066, val_acc:0.961]
Epoch [64/120    avg_loss:0.062, val_acc:0.957]
Epoch [65/120    avg_loss:0.084, val_acc:0.973]
Epoch [66/120    avg_loss:0.098, val_acc:0.941]
Epoch [67/120    avg_loss:0.082, val_acc:0.961]
Epoch [68/120    avg_loss:0.080, val_acc:0.965]
Epoch [69/120    avg_loss:0.048, val_acc:0.975]
Epoch [70/120    avg_loss:0.032, val_acc:0.979]
Epoch [71/120    avg_loss:0.030, val_acc:0.975]
Epoch [72/120    avg_loss:0.082, val_acc:0.963]
Epoch [73/120    avg_loss:0.025, val_acc:0.977]
Epoch [74/120    avg_loss:0.030, val_acc:0.963]
Epoch [75/120    avg_loss:0.032, val_acc:0.975]
Epoch [76/120    avg_loss:0.027, val_acc:0.975]
Epoch [77/120    avg_loss:0.032, val_acc:0.971]
Epoch [78/120    avg_loss:0.076, val_acc:0.971]
Epoch [79/120    avg_loss:0.070, val_acc:0.977]
Epoch [80/120    avg_loss:0.081, val_acc:0.961]
Epoch [81/120    avg_loss:0.077, val_acc:0.980]
Epoch [82/120    avg_loss:0.028, val_acc:0.979]
Epoch [83/120    avg_loss:0.038, val_acc:0.979]
Epoch [84/120    avg_loss:0.037, val_acc:0.951]
Epoch [85/120    avg_loss:0.147, val_acc:0.963]
Epoch [86/120    avg_loss:0.028, val_acc:0.963]
Epoch [87/120    avg_loss:0.049, val_acc:0.965]
Epoch [88/120    avg_loss:0.037, val_acc:0.986]
Epoch [89/120    avg_loss:0.015, val_acc:0.979]
Epoch [90/120    avg_loss:0.014, val_acc:0.979]
Epoch [91/120    avg_loss:0.020, val_acc:0.961]
Epoch [92/120    avg_loss:0.035, val_acc:0.971]
Epoch [93/120    avg_loss:0.036, val_acc:0.967]
Epoch [94/120    avg_loss:0.041, val_acc:0.977]
Epoch [95/120    avg_loss:0.022, val_acc:0.977]
Epoch [96/120    avg_loss:0.013, val_acc:0.973]
Epoch [97/120    avg_loss:0.013, val_acc:0.973]
Epoch [98/120    avg_loss:0.033, val_acc:0.969]
Epoch [99/120    avg_loss:0.016, val_acc:0.975]
Epoch [100/120    avg_loss:0.019, val_acc:0.973]
Epoch [101/120    avg_loss:0.016, val_acc:0.977]
Epoch [102/120    avg_loss:0.013, val_acc:0.979]
Epoch [103/120    avg_loss:0.020, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.008, val_acc:0.977]
Epoch [108/120    avg_loss:0.012, val_acc:0.977]
Epoch [109/120    avg_loss:0.007, val_acc:0.979]
Epoch [110/120    avg_loss:0.020, val_acc:0.979]
Epoch [111/120    avg_loss:0.005, val_acc:0.979]
Epoch [112/120    avg_loss:0.007, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   2 217   4   0   0   0   0   7   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.98861048 0.97091723 0.93939394 0.91608392
 1.         0.98412698 1.         0.99257688 1.         1.
 1.         1.        ]

Kappa:
0.9905038598317067
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3eb84867b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.869, val_acc:0.694]
Epoch [2/120    avg_loss:1.198, val_acc:0.683]
Epoch [3/120    avg_loss:1.086, val_acc:0.742]
Epoch [4/120    avg_loss:0.984, val_acc:0.766]
Epoch [5/120    avg_loss:1.010, val_acc:0.798]
Epoch [6/120    avg_loss:0.881, val_acc:0.794]
Epoch [7/120    avg_loss:0.747, val_acc:0.819]
Epoch [8/120    avg_loss:0.664, val_acc:0.808]
Epoch [9/120    avg_loss:0.622, val_acc:0.831]
Epoch [10/120    avg_loss:0.695, val_acc:0.812]
Epoch [11/120    avg_loss:0.684, val_acc:0.849]
Epoch [12/120    avg_loss:0.564, val_acc:0.831]
Epoch [13/120    avg_loss:0.620, val_acc:0.857]
Epoch [14/120    avg_loss:0.612, val_acc:0.855]
Epoch [15/120    avg_loss:0.564, val_acc:0.881]
Epoch [16/120    avg_loss:0.505, val_acc:0.887]
Epoch [17/120    avg_loss:0.467, val_acc:0.851]
Epoch [18/120    avg_loss:0.460, val_acc:0.881]
Epoch [19/120    avg_loss:0.509, val_acc:0.873]
Epoch [20/120    avg_loss:0.372, val_acc:0.881]
Epoch [21/120    avg_loss:0.411, val_acc:0.893]
Epoch [22/120    avg_loss:0.348, val_acc:0.889]
Epoch [23/120    avg_loss:0.455, val_acc:0.877]
Epoch [24/120    avg_loss:0.411, val_acc:0.879]
Epoch [25/120    avg_loss:0.404, val_acc:0.885]
Epoch [26/120    avg_loss:0.406, val_acc:0.885]
Epoch [27/120    avg_loss:0.423, val_acc:0.851]
Epoch [28/120    avg_loss:0.325, val_acc:0.915]
Epoch [29/120    avg_loss:0.404, val_acc:0.897]
Epoch [30/120    avg_loss:0.315, val_acc:0.907]
Epoch [31/120    avg_loss:0.361, val_acc:0.885]
Epoch [32/120    avg_loss:0.356, val_acc:0.877]
Epoch [33/120    avg_loss:0.311, val_acc:0.915]
Epoch [34/120    avg_loss:0.263, val_acc:0.935]
Epoch [35/120    avg_loss:0.238, val_acc:0.907]
Epoch [36/120    avg_loss:0.279, val_acc:0.933]
Epoch [37/120    avg_loss:0.232, val_acc:0.893]
Epoch [38/120    avg_loss:0.208, val_acc:0.942]
Epoch [39/120    avg_loss:0.227, val_acc:0.937]
Epoch [40/120    avg_loss:0.226, val_acc:0.915]
Epoch [41/120    avg_loss:0.248, val_acc:0.931]
Epoch [42/120    avg_loss:0.240, val_acc:0.909]
Epoch [43/120    avg_loss:0.258, val_acc:0.931]
Epoch [44/120    avg_loss:0.236, val_acc:0.940]
Epoch [45/120    avg_loss:0.164, val_acc:0.899]
Epoch [46/120    avg_loss:0.317, val_acc:0.931]
Epoch [47/120    avg_loss:0.180, val_acc:0.940]
Epoch [48/120    avg_loss:0.152, val_acc:0.933]
Epoch [49/120    avg_loss:0.177, val_acc:0.942]
Epoch [50/120    avg_loss:0.240, val_acc:0.927]
Epoch [51/120    avg_loss:0.210, val_acc:0.925]
Epoch [52/120    avg_loss:0.136, val_acc:0.946]
Epoch [53/120    avg_loss:0.125, val_acc:0.942]
Epoch [54/120    avg_loss:0.180, val_acc:0.940]
Epoch [55/120    avg_loss:0.150, val_acc:0.937]
Epoch [56/120    avg_loss:0.104, val_acc:0.935]
Epoch [57/120    avg_loss:0.145, val_acc:0.942]
Epoch [58/120    avg_loss:0.109, val_acc:0.940]
Epoch [59/120    avg_loss:0.116, val_acc:0.952]
Epoch [60/120    avg_loss:0.168, val_acc:0.952]
Epoch [61/120    avg_loss:0.142, val_acc:0.913]
Epoch [62/120    avg_loss:0.196, val_acc:0.942]
Epoch [63/120    avg_loss:0.179, val_acc:0.909]
Epoch [64/120    avg_loss:0.177, val_acc:0.946]
Epoch [65/120    avg_loss:0.122, val_acc:0.952]
Epoch [66/120    avg_loss:0.123, val_acc:0.966]
Epoch [67/120    avg_loss:0.108, val_acc:0.962]
Epoch [68/120    avg_loss:0.156, val_acc:0.944]
Epoch [69/120    avg_loss:0.135, val_acc:0.921]
Epoch [70/120    avg_loss:0.112, val_acc:0.948]
Epoch [71/120    avg_loss:0.075, val_acc:0.960]
Epoch [72/120    avg_loss:0.090, val_acc:0.956]
Epoch [73/120    avg_loss:0.112, val_acc:0.962]
Epoch [74/120    avg_loss:0.087, val_acc:0.960]
Epoch [75/120    avg_loss:0.090, val_acc:0.956]
Epoch [76/120    avg_loss:0.081, val_acc:0.960]
Epoch [77/120    avg_loss:0.130, val_acc:0.940]
Epoch [78/120    avg_loss:0.144, val_acc:0.956]
Epoch [79/120    avg_loss:0.117, val_acc:0.940]
Epoch [80/120    avg_loss:0.125, val_acc:0.958]
Epoch [81/120    avg_loss:0.067, val_acc:0.964]
Epoch [82/120    avg_loss:0.044, val_acc:0.966]
Epoch [83/120    avg_loss:0.054, val_acc:0.974]
Epoch [84/120    avg_loss:0.036, val_acc:0.970]
Epoch [85/120    avg_loss:0.037, val_acc:0.972]
Epoch [86/120    avg_loss:0.023, val_acc:0.972]
Epoch [87/120    avg_loss:0.031, val_acc:0.972]
Epoch [88/120    avg_loss:0.025, val_acc:0.972]
Epoch [89/120    avg_loss:0.042, val_acc:0.972]
Epoch [90/120    avg_loss:0.028, val_acc:0.974]
Epoch [91/120    avg_loss:0.022, val_acc:0.974]
Epoch [92/120    avg_loss:0.035, val_acc:0.976]
Epoch [93/120    avg_loss:0.032, val_acc:0.976]
Epoch [94/120    avg_loss:0.028, val_acc:0.974]
Epoch [95/120    avg_loss:0.048, val_acc:0.974]
Epoch [96/120    avg_loss:0.032, val_acc:0.974]
Epoch [97/120    avg_loss:0.035, val_acc:0.976]
Epoch [98/120    avg_loss:0.032, val_acc:0.976]
Epoch [99/120    avg_loss:0.030, val_acc:0.976]
Epoch [100/120    avg_loss:0.019, val_acc:0.978]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.014, val_acc:0.974]
Epoch [103/120    avg_loss:0.022, val_acc:0.976]
Epoch [104/120    avg_loss:0.037, val_acc:0.972]
Epoch [105/120    avg_loss:0.031, val_acc:0.976]
Epoch [106/120    avg_loss:0.033, val_acc:0.976]
Epoch [107/120    avg_loss:0.019, val_acc:0.974]
Epoch [108/120    avg_loss:0.029, val_acc:0.970]
Epoch [109/120    avg_loss:0.016, val_acc:0.972]
Epoch [110/120    avg_loss:0.021, val_acc:0.972]
Epoch [111/120    avg_loss:0.018, val_acc:0.972]
Epoch [112/120    avg_loss:0.017, val_acc:0.974]
Epoch [113/120    avg_loss:0.018, val_acc:0.972]
Epoch [114/120    avg_loss:0.028, val_acc:0.972]
Epoch [115/120    avg_loss:0.031, val_acc:0.972]
Epoch [116/120    avg_loss:0.009, val_acc:0.972]
Epoch [117/120    avg_loss:0.027, val_acc:0.972]
Epoch [118/120    avg_loss:0.019, val_acc:0.972]
Epoch [119/120    avg_loss:0.025, val_acc:0.972]
Epoch [120/120    avg_loss:0.024, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   0   0   0   0   0   9   0]
 [  0   0   0 222   6   0   0   0   1   1   0   0   0   0]
 [  0   0   0   1 210  13   0   0   0   0   0   0   3   0]
 [  0   0   0   1   8 132   4   0   0   0   0   0   0   0]
 [  0   1   0   0   6   0 199   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1   0 363   0   0   0]
 [  0   2   0   0   0   0   0   0   0   0   0 373   2   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 0.997815   0.96774194 0.97797357 0.9190372  0.91034483
 0.97310513 1.         0.99742931 0.99893276 0.99862448 0.99466667
 0.97923497 1.        ]

Kappa:
0.9848050404580478
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c96984710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.855, val_acc:0.664]
Epoch [2/120    avg_loss:1.161, val_acc:0.658]
Epoch [3/120    avg_loss:1.032, val_acc:0.662]
Epoch [4/120    avg_loss:1.006, val_acc:0.803]
Epoch [5/120    avg_loss:0.847, val_acc:0.787]
Epoch [6/120    avg_loss:0.823, val_acc:0.812]
Epoch [7/120    avg_loss:0.741, val_acc:0.852]
Epoch [8/120    avg_loss:0.684, val_acc:0.836]
Epoch [9/120    avg_loss:0.543, val_acc:0.828]
Epoch [10/120    avg_loss:0.698, val_acc:0.852]
Epoch [11/120    avg_loss:0.561, val_acc:0.797]
Epoch [12/120    avg_loss:0.487, val_acc:0.840]
Epoch [13/120    avg_loss:0.557, val_acc:0.871]
Epoch [14/120    avg_loss:0.574, val_acc:0.854]
Epoch [15/120    avg_loss:0.462, val_acc:0.869]
Epoch [16/120    avg_loss:0.430, val_acc:0.855]
Epoch [17/120    avg_loss:0.509, val_acc:0.877]
Epoch [18/120    avg_loss:0.386, val_acc:0.904]
Epoch [19/120    avg_loss:0.402, val_acc:0.871]
Epoch [20/120    avg_loss:0.405, val_acc:0.895]
Epoch [21/120    avg_loss:0.445, val_acc:0.889]
Epoch [22/120    avg_loss:0.375, val_acc:0.893]
Epoch [23/120    avg_loss:0.383, val_acc:0.885]
Epoch [24/120    avg_loss:0.338, val_acc:0.900]
Epoch [25/120    avg_loss:0.394, val_acc:0.863]
Epoch [26/120    avg_loss:0.416, val_acc:0.887]
Epoch [27/120    avg_loss:0.243, val_acc:0.902]
Epoch [28/120    avg_loss:0.358, val_acc:0.891]
Epoch [29/120    avg_loss:0.323, val_acc:0.926]
Epoch [30/120    avg_loss:0.221, val_acc:0.883]
Epoch [31/120    avg_loss:0.296, val_acc:0.887]
Epoch [32/120    avg_loss:0.324, val_acc:0.889]
Epoch [33/120    avg_loss:0.238, val_acc:0.926]
Epoch [34/120    avg_loss:0.182, val_acc:0.924]
Epoch [35/120    avg_loss:0.212, val_acc:0.873]
Epoch [36/120    avg_loss:0.234, val_acc:0.910]
Epoch [37/120    avg_loss:0.255, val_acc:0.928]
Epoch [38/120    avg_loss:0.224, val_acc:0.875]
Epoch [39/120    avg_loss:0.284, val_acc:0.920]
Epoch [40/120    avg_loss:0.192, val_acc:0.918]
Epoch [41/120    avg_loss:0.289, val_acc:0.939]
Epoch [42/120    avg_loss:0.251, val_acc:0.926]
Epoch [43/120    avg_loss:0.228, val_acc:0.916]
Epoch [44/120    avg_loss:0.182, val_acc:0.930]
Epoch [45/120    avg_loss:0.172, val_acc:0.918]
Epoch [46/120    avg_loss:0.165, val_acc:0.924]
Epoch [47/120    avg_loss:0.149, val_acc:0.918]
Epoch [48/120    avg_loss:0.189, val_acc:0.916]
Epoch [49/120    avg_loss:0.166, val_acc:0.936]
Epoch [50/120    avg_loss:0.180, val_acc:0.951]
Epoch [51/120    avg_loss:0.183, val_acc:0.914]
Epoch [52/120    avg_loss:0.177, val_acc:0.932]
Epoch [53/120    avg_loss:0.158, val_acc:0.945]
Epoch [54/120    avg_loss:0.209, val_acc:0.932]
Epoch [55/120    avg_loss:0.145, val_acc:0.939]
Epoch [56/120    avg_loss:0.135, val_acc:0.947]
Epoch [57/120    avg_loss:0.123, val_acc:0.951]
Epoch [58/120    avg_loss:0.133, val_acc:0.961]
Epoch [59/120    avg_loss:0.162, val_acc:0.938]
Epoch [60/120    avg_loss:0.156, val_acc:0.936]
Epoch [61/120    avg_loss:0.146, val_acc:0.943]
Epoch [62/120    avg_loss:0.159, val_acc:0.961]
Epoch [63/120    avg_loss:0.056, val_acc:0.957]
Epoch [64/120    avg_loss:0.122, val_acc:0.953]
Epoch [65/120    avg_loss:0.162, val_acc:0.920]
Epoch [66/120    avg_loss:0.144, val_acc:0.943]
Epoch [67/120    avg_loss:0.134, val_acc:0.938]
Epoch [68/120    avg_loss:0.068, val_acc:0.977]
Epoch [69/120    avg_loss:0.073, val_acc:0.969]
Epoch [70/120    avg_loss:0.104, val_acc:0.953]
Epoch [71/120    avg_loss:0.147, val_acc:0.945]
Epoch [72/120    avg_loss:0.144, val_acc:0.969]
Epoch [73/120    avg_loss:0.169, val_acc:0.955]
Epoch [74/120    avg_loss:0.052, val_acc:0.967]
Epoch [75/120    avg_loss:0.056, val_acc:0.971]
Epoch [76/120    avg_loss:0.089, val_acc:0.941]
Epoch [77/120    avg_loss:0.046, val_acc:0.967]
Epoch [78/120    avg_loss:0.056, val_acc:0.971]
Epoch [79/120    avg_loss:0.039, val_acc:0.961]
Epoch [80/120    avg_loss:0.059, val_acc:0.973]
Epoch [81/120    avg_loss:0.076, val_acc:0.963]
Epoch [82/120    avg_loss:0.083, val_acc:0.967]
Epoch [83/120    avg_loss:0.054, val_acc:0.973]
Epoch [84/120    avg_loss:0.037, val_acc:0.973]
Epoch [85/120    avg_loss:0.025, val_acc:0.975]
Epoch [86/120    avg_loss:0.022, val_acc:0.977]
Epoch [87/120    avg_loss:0.033, val_acc:0.977]
Epoch [88/120    avg_loss:0.016, val_acc:0.977]
Epoch [89/120    avg_loss:0.026, val_acc:0.980]
Epoch [90/120    avg_loss:0.023, val_acc:0.977]
Epoch [91/120    avg_loss:0.028, val_acc:0.980]
Epoch [92/120    avg_loss:0.015, val_acc:0.980]
Epoch [93/120    avg_loss:0.029, val_acc:0.982]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.030, val_acc:0.979]
Epoch [96/120    avg_loss:0.020, val_acc:0.980]
Epoch [97/120    avg_loss:0.017, val_acc:0.980]
Epoch [98/120    avg_loss:0.024, val_acc:0.980]
Epoch [99/120    avg_loss:0.018, val_acc:0.980]
Epoch [100/120    avg_loss:0.022, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.028, val_acc:0.980]
Epoch [103/120    avg_loss:0.021, val_acc:0.979]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.023, val_acc:0.980]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.982]
Epoch [110/120    avg_loss:0.031, val_acc:0.984]
Epoch [111/120    avg_loss:0.018, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.024, val_acc:0.982]
Epoch [116/120    avg_loss:0.027, val_acc:0.980]
Epoch [117/120    avg_loss:0.016, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.019, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 207  22   1   0   0   0   0   0   0   0   0]
 [  0   0   0   5 213   9   0   0   0   0   0   0   0   0]
 [  0   0   0   1  10 134   0   0   0   0   0   0   0   0]
 [  0   3   0   0   2   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.997815   0.99319728 0.93453725 0.89873418 0.92733564
 0.98771499 1.         1.         1.         1.         0.99867198
 0.99557522 1.        ]

Kappa:
0.9864686516423634
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb034e45748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.825, val_acc:0.602]
Epoch [2/120    avg_loss:1.297, val_acc:0.682]
Epoch [3/120    avg_loss:1.148, val_acc:0.746]
Epoch [4/120    avg_loss:1.028, val_acc:0.756]
Epoch [5/120    avg_loss:0.893, val_acc:0.789]
Epoch [6/120    avg_loss:0.858, val_acc:0.764]
Epoch [7/120    avg_loss:0.735, val_acc:0.785]
Epoch [8/120    avg_loss:0.657, val_acc:0.820]
Epoch [9/120    avg_loss:0.661, val_acc:0.850]
Epoch [10/120    avg_loss:0.752, val_acc:0.824]
Epoch [11/120    avg_loss:0.683, val_acc:0.830]
Epoch [12/120    avg_loss:0.621, val_acc:0.873]
Epoch [13/120    avg_loss:0.664, val_acc:0.812]
Epoch [14/120    avg_loss:0.601, val_acc:0.854]
Epoch [15/120    avg_loss:0.715, val_acc:0.842]
Epoch [16/120    avg_loss:0.492, val_acc:0.848]
Epoch [17/120    avg_loss:0.466, val_acc:0.869]
Epoch [18/120    avg_loss:0.481, val_acc:0.879]
Epoch [19/120    avg_loss:0.390, val_acc:0.857]
Epoch [20/120    avg_loss:0.497, val_acc:0.881]
Epoch [21/120    avg_loss:0.469, val_acc:0.781]
Epoch [22/120    avg_loss:0.498, val_acc:0.875]
Epoch [23/120    avg_loss:0.405, val_acc:0.881]
Epoch [24/120    avg_loss:0.338, val_acc:0.893]
Epoch [25/120    avg_loss:0.404, val_acc:0.893]
Epoch [26/120    avg_loss:0.415, val_acc:0.885]
Epoch [27/120    avg_loss:0.400, val_acc:0.908]
Epoch [28/120    avg_loss:0.252, val_acc:0.920]
Epoch [29/120    avg_loss:0.285, val_acc:0.910]
Epoch [30/120    avg_loss:0.294, val_acc:0.889]
Epoch [31/120    avg_loss:0.385, val_acc:0.898]
Epoch [32/120    avg_loss:0.405, val_acc:0.893]
Epoch [33/120    avg_loss:0.315, val_acc:0.900]
Epoch [34/120    avg_loss:0.293, val_acc:0.906]
Epoch [35/120    avg_loss:0.356, val_acc:0.838]
Epoch [36/120    avg_loss:0.323, val_acc:0.920]
Epoch [37/120    avg_loss:0.262, val_acc:0.922]
Epoch [38/120    avg_loss:0.291, val_acc:0.922]
Epoch [39/120    avg_loss:0.321, val_acc:0.904]
Epoch [40/120    avg_loss:0.277, val_acc:0.908]
Epoch [41/120    avg_loss:0.232, val_acc:0.926]
Epoch [42/120    avg_loss:0.266, val_acc:0.918]
Epoch [43/120    avg_loss:0.303, val_acc:0.906]
Epoch [44/120    avg_loss:0.249, val_acc:0.932]
Epoch [45/120    avg_loss:0.242, val_acc:0.906]
Epoch [46/120    avg_loss:0.216, val_acc:0.939]
Epoch [47/120    avg_loss:0.276, val_acc:0.891]
Epoch [48/120    avg_loss:0.306, val_acc:0.906]
Epoch [49/120    avg_loss:0.202, val_acc:0.932]
Epoch [50/120    avg_loss:0.213, val_acc:0.910]
Epoch [51/120    avg_loss:0.212, val_acc:0.951]
Epoch [52/120    avg_loss:0.183, val_acc:0.941]
Epoch [53/120    avg_loss:0.155, val_acc:0.934]
Epoch [54/120    avg_loss:0.148, val_acc:0.943]
Epoch [55/120    avg_loss:0.152, val_acc:0.959]
Epoch [56/120    avg_loss:0.184, val_acc:0.889]
Epoch [57/120    avg_loss:0.149, val_acc:0.910]
Epoch [58/120    avg_loss:0.154, val_acc:0.939]
Epoch [59/120    avg_loss:0.137, val_acc:0.941]
Epoch [60/120    avg_loss:0.130, val_acc:0.924]
Epoch [61/120    avg_loss:0.187, val_acc:0.957]
Epoch [62/120    avg_loss:0.197, val_acc:0.939]
Epoch [63/120    avg_loss:0.107, val_acc:0.955]
Epoch [64/120    avg_loss:0.157, val_acc:0.943]
Epoch [65/120    avg_loss:0.113, val_acc:0.953]
Epoch [66/120    avg_loss:0.087, val_acc:0.953]
Epoch [67/120    avg_loss:0.130, val_acc:0.943]
Epoch [68/120    avg_loss:0.132, val_acc:0.963]
Epoch [69/120    avg_loss:0.080, val_acc:0.973]
Epoch [70/120    avg_loss:0.057, val_acc:0.961]
Epoch [71/120    avg_loss:0.072, val_acc:0.949]
Epoch [72/120    avg_loss:0.114, val_acc:0.947]
Epoch [73/120    avg_loss:0.100, val_acc:0.951]
Epoch [74/120    avg_loss:0.130, val_acc:0.945]
Epoch [75/120    avg_loss:0.158, val_acc:0.939]
Epoch [76/120    avg_loss:0.155, val_acc:0.943]
Epoch [77/120    avg_loss:0.290, val_acc:0.959]
Epoch [78/120    avg_loss:0.099, val_acc:0.957]
Epoch [79/120    avg_loss:0.053, val_acc:0.957]
Epoch [80/120    avg_loss:0.057, val_acc:0.953]
Epoch [81/120    avg_loss:0.069, val_acc:0.943]
Epoch [82/120    avg_loss:0.122, val_acc:0.953]
Epoch [83/120    avg_loss:0.058, val_acc:0.961]
Epoch [84/120    avg_loss:0.086, val_acc:0.971]
Epoch [85/120    avg_loss:0.045, val_acc:0.971]
Epoch [86/120    avg_loss:0.048, val_acc:0.971]
Epoch [87/120    avg_loss:0.047, val_acc:0.971]
Epoch [88/120    avg_loss:0.044, val_acc:0.971]
Epoch [89/120    avg_loss:0.036, val_acc:0.971]
Epoch [90/120    avg_loss:0.040, val_acc:0.971]
Epoch [91/120    avg_loss:0.040, val_acc:0.973]
Epoch [92/120    avg_loss:0.044, val_acc:0.979]
Epoch [93/120    avg_loss:0.033, val_acc:0.975]
Epoch [94/120    avg_loss:0.046, val_acc:0.979]
Epoch [95/120    avg_loss:0.057, val_acc:0.971]
Epoch [96/120    avg_loss:0.039, val_acc:0.971]
Epoch [97/120    avg_loss:0.035, val_acc:0.973]
Epoch [98/120    avg_loss:0.036, val_acc:0.971]
Epoch [99/120    avg_loss:0.043, val_acc:0.973]
Epoch [100/120    avg_loss:0.026, val_acc:0.973]
Epoch [101/120    avg_loss:0.036, val_acc:0.975]
Epoch [102/120    avg_loss:0.026, val_acc:0.975]
Epoch [103/120    avg_loss:0.021, val_acc:0.979]
Epoch [104/120    avg_loss:0.023, val_acc:0.980]
Epoch [105/120    avg_loss:0.034, val_acc:0.980]
Epoch [106/120    avg_loss:0.025, val_acc:0.980]
Epoch [107/120    avg_loss:0.019, val_acc:0.980]
Epoch [108/120    avg_loss:0.043, val_acc:0.979]
Epoch [109/120    avg_loss:0.027, val_acc:0.975]
Epoch [110/120    avg_loss:0.043, val_acc:0.979]
Epoch [111/120    avg_loss:0.025, val_acc:0.979]
Epoch [112/120    avg_loss:0.035, val_acc:0.982]
Epoch [113/120    avg_loss:0.036, val_acc:0.977]
Epoch [114/120    avg_loss:0.029, val_acc:0.977]
Epoch [115/120    avg_loss:0.037, val_acc:0.975]
Epoch [116/120    avg_loss:0.028, val_acc:0.975]
Epoch [117/120    avg_loss:0.021, val_acc:0.979]
Epoch [118/120    avg_loss:0.017, val_acc:0.977]
Epoch [119/120    avg_loss:0.020, val_acc:0.980]
Epoch [120/120    avg_loss:0.025, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   1   1   0   0   0   0   0   0   0   0]
 [  0   0   0   4 204  18   0   0   0   0   0   0   1   0]
 [  0   0   0   7   7 131   0   0   0   0   0   0   0   0]
 [  0   2   0   0   5   3 196   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 0.99854227 0.98871332 0.97228145 0.91891892 0.87919463
 0.97512438 1.         1.         1.         1.         0.99734043
 0.99115044 1.        ]

Kappa:
0.9867068370152636
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f298765dd68>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.746, val_acc:0.658]
Epoch [2/120    avg_loss:1.109, val_acc:0.721]
Epoch [3/120    avg_loss:1.121, val_acc:0.783]
Epoch [4/120    avg_loss:0.958, val_acc:0.748]
Epoch [5/120    avg_loss:0.746, val_acc:0.719]
Epoch [6/120    avg_loss:0.796, val_acc:0.824]
Epoch [7/120    avg_loss:0.641, val_acc:0.871]
Epoch [8/120    avg_loss:0.606, val_acc:0.832]
Epoch [9/120    avg_loss:0.618, val_acc:0.801]
Epoch [10/120    avg_loss:0.596, val_acc:0.877]
Epoch [11/120    avg_loss:0.597, val_acc:0.848]
Epoch [12/120    avg_loss:0.515, val_acc:0.869]
Epoch [13/120    avg_loss:0.596, val_acc:0.891]
Epoch [14/120    avg_loss:0.499, val_acc:0.863]
Epoch [15/120    avg_loss:0.468, val_acc:0.906]
Epoch [16/120    avg_loss:0.392, val_acc:0.928]
Epoch [17/120    avg_loss:0.377, val_acc:0.930]
Epoch [18/120    avg_loss:0.340, val_acc:0.930]
Epoch [19/120    avg_loss:0.365, val_acc:0.918]
Epoch [20/120    avg_loss:0.471, val_acc:0.865]
Epoch [21/120    avg_loss:0.397, val_acc:0.902]
Epoch [22/120    avg_loss:0.302, val_acc:0.885]
Epoch [23/120    avg_loss:0.284, val_acc:0.902]
Epoch [24/120    avg_loss:0.333, val_acc:0.910]
Epoch [25/120    avg_loss:0.359, val_acc:0.932]
Epoch [26/120    avg_loss:0.344, val_acc:0.949]
Epoch [27/120    avg_loss:0.273, val_acc:0.926]
Epoch [28/120    avg_loss:0.343, val_acc:0.928]
Epoch [29/120    avg_loss:0.228, val_acc:0.953]
Epoch [30/120    avg_loss:0.181, val_acc:0.912]
Epoch [31/120    avg_loss:0.209, val_acc:0.947]
Epoch [32/120    avg_loss:0.137, val_acc:0.947]
Epoch [33/120    avg_loss:0.139, val_acc:0.934]
Epoch [34/120    avg_loss:0.164, val_acc:0.943]
Epoch [35/120    avg_loss:0.172, val_acc:0.957]
Epoch [36/120    avg_loss:0.186, val_acc:0.939]
Epoch [37/120    avg_loss:0.136, val_acc:0.967]
Epoch [38/120    avg_loss:0.136, val_acc:0.939]
Epoch [39/120    avg_loss:0.188, val_acc:0.955]
Epoch [40/120    avg_loss:0.101, val_acc:0.957]
Epoch [41/120    avg_loss:0.112, val_acc:0.938]
Epoch [42/120    avg_loss:0.170, val_acc:0.955]
Epoch [43/120    avg_loss:0.121, val_acc:0.965]
Epoch [44/120    avg_loss:0.134, val_acc:0.975]
Epoch [45/120    avg_loss:0.086, val_acc:0.975]
Epoch [46/120    avg_loss:0.106, val_acc:0.957]
Epoch [47/120    avg_loss:0.161, val_acc:0.957]
Epoch [48/120    avg_loss:0.160, val_acc:0.975]
Epoch [49/120    avg_loss:0.162, val_acc:0.967]
Epoch [50/120    avg_loss:0.110, val_acc:0.975]
Epoch [51/120    avg_loss:0.131, val_acc:0.965]
Epoch [52/120    avg_loss:0.093, val_acc:0.977]
Epoch [53/120    avg_loss:0.106, val_acc:0.961]
Epoch [54/120    avg_loss:0.097, val_acc:0.977]
Epoch [55/120    avg_loss:0.050, val_acc:0.979]
Epoch [56/120    avg_loss:0.053, val_acc:0.980]
Epoch [57/120    avg_loss:0.035, val_acc:0.988]
Epoch [58/120    avg_loss:0.043, val_acc:0.975]
Epoch [59/120    avg_loss:0.026, val_acc:0.990]
Epoch [60/120    avg_loss:0.060, val_acc:0.980]
Epoch [61/120    avg_loss:0.087, val_acc:0.934]
Epoch [62/120    avg_loss:0.055, val_acc:0.984]
Epoch [63/120    avg_loss:0.050, val_acc:0.984]
Epoch [64/120    avg_loss:0.044, val_acc:0.977]
Epoch [65/120    avg_loss:0.092, val_acc:0.986]
Epoch [66/120    avg_loss:0.035, val_acc:0.984]
Epoch [67/120    avg_loss:0.047, val_acc:0.973]
Epoch [68/120    avg_loss:0.041, val_acc:0.979]
Epoch [69/120    avg_loss:0.025, val_acc:0.980]
Epoch [70/120    avg_loss:0.055, val_acc:0.984]
Epoch [71/120    avg_loss:0.019, val_acc:0.988]
Epoch [72/120    avg_loss:0.012, val_acc:0.986]
Epoch [73/120    avg_loss:0.013, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.020, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.990]
Epoch [78/120    avg_loss:0.017, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.028, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.990]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.013, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.013, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.009, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.020, val_acc:0.988]
Epoch [101/120    avg_loss:0.012, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 222   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 130   1   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   2 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.98648649 0.98230088 0.91880342 0.90592334
 0.98529412 0.99465241 1.         1.         1.         0.9973545
 0.99333333 1.        ]

Kappa:
0.988843447138473
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f832a772cc0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.787, val_acc:0.645]
Epoch [2/120    avg_loss:1.266, val_acc:0.692]
Epoch [3/120    avg_loss:1.037, val_acc:0.704]
Epoch [4/120    avg_loss:1.039, val_acc:0.651]
Epoch [5/120    avg_loss:1.030, val_acc:0.782]
Epoch [6/120    avg_loss:0.813, val_acc:0.790]
Epoch [7/120    avg_loss:0.679, val_acc:0.845]
Epoch [8/120    avg_loss:0.746, val_acc:0.800]
Epoch [9/120    avg_loss:0.902, val_acc:0.813]
Epoch [10/120    avg_loss:0.630, val_acc:0.851]
Epoch [11/120    avg_loss:0.587, val_acc:0.835]
Epoch [12/120    avg_loss:0.606, val_acc:0.879]
Epoch [13/120    avg_loss:0.572, val_acc:0.867]
Epoch [14/120    avg_loss:0.581, val_acc:0.812]
Epoch [15/120    avg_loss:0.613, val_acc:0.851]
Epoch [16/120    avg_loss:0.490, val_acc:0.865]
Epoch [17/120    avg_loss:0.513, val_acc:0.857]
Epoch [18/120    avg_loss:0.441, val_acc:0.869]
Epoch [19/120    avg_loss:0.471, val_acc:0.927]
Epoch [20/120    avg_loss:0.419, val_acc:0.909]
Epoch [21/120    avg_loss:0.430, val_acc:0.849]
Epoch [22/120    avg_loss:0.431, val_acc:0.905]
Epoch [23/120    avg_loss:0.442, val_acc:0.903]
Epoch [24/120    avg_loss:0.394, val_acc:0.845]
Epoch [25/120    avg_loss:0.351, val_acc:0.883]
Epoch [26/120    avg_loss:0.433, val_acc:0.903]
Epoch [27/120    avg_loss:0.360, val_acc:0.871]
Epoch [28/120    avg_loss:0.292, val_acc:0.907]
Epoch [29/120    avg_loss:0.279, val_acc:0.923]
Epoch [30/120    avg_loss:0.264, val_acc:0.937]
Epoch [31/120    avg_loss:0.244, val_acc:0.948]
Epoch [32/120    avg_loss:0.229, val_acc:0.937]
Epoch [33/120    avg_loss:0.222, val_acc:0.903]
Epoch [34/120    avg_loss:0.267, val_acc:0.935]
Epoch [35/120    avg_loss:0.302, val_acc:0.877]
Epoch [36/120    avg_loss:0.309, val_acc:0.903]
Epoch [37/120    avg_loss:0.284, val_acc:0.915]
Epoch [38/120    avg_loss:0.232, val_acc:0.946]
Epoch [39/120    avg_loss:0.151, val_acc:0.954]
Epoch [40/120    avg_loss:0.166, val_acc:0.929]
Epoch [41/120    avg_loss:0.161, val_acc:0.942]
Epoch [42/120    avg_loss:0.157, val_acc:0.980]
Epoch [43/120    avg_loss:0.148, val_acc:0.954]
Epoch [44/120    avg_loss:0.132, val_acc:0.899]
Epoch [45/120    avg_loss:0.257, val_acc:0.927]
Epoch [46/120    avg_loss:0.226, val_acc:0.954]
Epoch [47/120    avg_loss:0.136, val_acc:0.968]
Epoch [48/120    avg_loss:0.129, val_acc:0.958]
Epoch [49/120    avg_loss:0.157, val_acc:0.942]
Epoch [50/120    avg_loss:0.169, val_acc:0.937]
Epoch [51/120    avg_loss:0.227, val_acc:0.948]
Epoch [52/120    avg_loss:0.092, val_acc:0.952]
Epoch [53/120    avg_loss:0.093, val_acc:0.962]
Epoch [54/120    avg_loss:0.111, val_acc:0.966]
Epoch [55/120    avg_loss:0.073, val_acc:0.976]
Epoch [56/120    avg_loss:0.057, val_acc:0.982]
Epoch [57/120    avg_loss:0.050, val_acc:0.972]
Epoch [58/120    avg_loss:0.035, val_acc:0.978]
Epoch [59/120    avg_loss:0.034, val_acc:0.980]
Epoch [60/120    avg_loss:0.033, val_acc:0.980]
Epoch [61/120    avg_loss:0.046, val_acc:0.978]
Epoch [62/120    avg_loss:0.029, val_acc:0.970]
Epoch [63/120    avg_loss:0.082, val_acc:0.978]
Epoch [64/120    avg_loss:0.031, val_acc:0.978]
Epoch [65/120    avg_loss:0.036, val_acc:0.982]
Epoch [66/120    avg_loss:0.034, val_acc:0.980]
Epoch [67/120    avg_loss:0.036, val_acc:0.980]
Epoch [68/120    avg_loss:0.032, val_acc:0.980]
Epoch [69/120    avg_loss:0.032, val_acc:0.980]
Epoch [70/120    avg_loss:0.030, val_acc:0.976]
Epoch [71/120    avg_loss:0.031, val_acc:0.978]
Epoch [72/120    avg_loss:0.039, val_acc:0.984]
Epoch [73/120    avg_loss:0.036, val_acc:0.980]
Epoch [74/120    avg_loss:0.033, val_acc:0.974]
Epoch [75/120    avg_loss:0.024, val_acc:0.982]
Epoch [76/120    avg_loss:0.045, val_acc:0.978]
Epoch [77/120    avg_loss:0.029, val_acc:0.978]
Epoch [78/120    avg_loss:0.022, val_acc:0.982]
Epoch [79/120    avg_loss:0.029, val_acc:0.982]
Epoch [80/120    avg_loss:0.033, val_acc:0.984]
Epoch [81/120    avg_loss:0.024, val_acc:0.982]
Epoch [82/120    avg_loss:0.023, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.978]
Epoch [84/120    avg_loss:0.035, val_acc:0.978]
Epoch [85/120    avg_loss:0.031, val_acc:0.978]
Epoch [86/120    avg_loss:0.049, val_acc:0.982]
Epoch [87/120    avg_loss:0.025, val_acc:0.982]
Epoch [88/120    avg_loss:0.052, val_acc:0.974]
Epoch [89/120    avg_loss:0.025, val_acc:0.976]
Epoch [90/120    avg_loss:0.021, val_acc:0.984]
Epoch [91/120    avg_loss:0.049, val_acc:0.976]
Epoch [92/120    avg_loss:0.041, val_acc:0.976]
Epoch [93/120    avg_loss:0.039, val_acc:0.974]
Epoch [94/120    avg_loss:0.036, val_acc:0.984]
Epoch [95/120    avg_loss:0.034, val_acc:0.982]
Epoch [96/120    avg_loss:0.037, val_acc:0.988]
Epoch [97/120    avg_loss:0.031, val_acc:0.982]
Epoch [98/120    avg_loss:0.017, val_acc:0.984]
Epoch [99/120    avg_loss:0.029, val_acc:0.984]
Epoch [100/120    avg_loss:0.042, val_acc:0.984]
Epoch [101/120    avg_loss:0.035, val_acc:0.984]
Epoch [102/120    avg_loss:0.042, val_acc:0.982]
Epoch [103/120    avg_loss:0.024, val_acc:0.984]
Epoch [104/120    avg_loss:0.043, val_acc:0.984]
Epoch [105/120    avg_loss:0.049, val_acc:0.984]
Epoch [106/120    avg_loss:0.020, val_acc:0.984]
Epoch [107/120    avg_loss:0.027, val_acc:0.984]
Epoch [108/120    avg_loss:0.018, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.986]
Epoch [110/120    avg_loss:0.023, val_acc:0.986]
Epoch [111/120    avg_loss:0.016, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.023, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.020, val_acc:0.986]
Epoch [116/120    avg_loss:0.016, val_acc:0.986]
Epoch [117/120    avg_loss:0.019, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.021, val_acc:0.986]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   2   0   0   0   0   2   0]
 [  0   0   1 228   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 215  10   0   0   0   0   0   0   2   0]
 [  0   0   0   1  12 131   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 205   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.97949886 0.99346405 0.94713656 0.91608392
 0.99514563 0.98429319 0.998713   1.         1.         1.
 0.99116998 1.        ]

Kappa:
0.9912168510530693
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb59189a6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.824, val_acc:0.605]
Epoch [2/120    avg_loss:1.209, val_acc:0.635]
Epoch [3/120    avg_loss:1.094, val_acc:0.762]
Epoch [4/120    avg_loss:0.908, val_acc:0.687]
Epoch [5/120    avg_loss:0.808, val_acc:0.782]
Epoch [6/120    avg_loss:0.906, val_acc:0.784]
Epoch [7/120    avg_loss:0.767, val_acc:0.806]
Epoch [8/120    avg_loss:0.624, val_acc:0.831]
Epoch [9/120    avg_loss:0.568, val_acc:0.857]
Epoch [10/120    avg_loss:0.577, val_acc:0.853]
Epoch [11/120    avg_loss:0.504, val_acc:0.865]
Epoch [12/120    avg_loss:0.527, val_acc:0.837]
Epoch [13/120    avg_loss:0.530, val_acc:0.843]
Epoch [14/120    avg_loss:0.457, val_acc:0.875]
Epoch [15/120    avg_loss:0.437, val_acc:0.899]
Epoch [16/120    avg_loss:0.357, val_acc:0.859]
Epoch [17/120    avg_loss:0.404, val_acc:0.907]
Epoch [18/120    avg_loss:0.436, val_acc:0.859]
Epoch [19/120    avg_loss:0.382, val_acc:0.855]
Epoch [20/120    avg_loss:0.536, val_acc:0.841]
Epoch [21/120    avg_loss:0.381, val_acc:0.857]
Epoch [22/120    avg_loss:0.379, val_acc:0.885]
Epoch [23/120    avg_loss:0.353, val_acc:0.901]
Epoch [24/120    avg_loss:0.301, val_acc:0.925]
Epoch [25/120    avg_loss:0.350, val_acc:0.915]
Epoch [26/120    avg_loss:0.288, val_acc:0.917]
Epoch [27/120    avg_loss:0.282, val_acc:0.923]
Epoch [28/120    avg_loss:0.262, val_acc:0.909]
Epoch [29/120    avg_loss:0.274, val_acc:0.921]
Epoch [30/120    avg_loss:0.232, val_acc:0.911]
Epoch [31/120    avg_loss:0.237, val_acc:0.923]
Epoch [32/120    avg_loss:0.276, val_acc:0.903]
Epoch [33/120    avg_loss:0.195, val_acc:0.948]
Epoch [34/120    avg_loss:0.198, val_acc:0.905]
Epoch [35/120    avg_loss:0.158, val_acc:0.950]
Epoch [36/120    avg_loss:0.155, val_acc:0.940]
Epoch [37/120    avg_loss:0.228, val_acc:0.940]
Epoch [38/120    avg_loss:0.158, val_acc:0.942]
Epoch [39/120    avg_loss:0.106, val_acc:0.931]
Epoch [40/120    avg_loss:0.127, val_acc:0.931]
Epoch [41/120    avg_loss:0.139, val_acc:0.950]
Epoch [42/120    avg_loss:0.158, val_acc:0.905]
Epoch [43/120    avg_loss:0.136, val_acc:0.954]
Epoch [44/120    avg_loss:0.193, val_acc:0.933]
Epoch [45/120    avg_loss:0.150, val_acc:0.952]
Epoch [46/120    avg_loss:0.111, val_acc:0.962]
Epoch [47/120    avg_loss:0.122, val_acc:0.933]
Epoch [48/120    avg_loss:0.211, val_acc:0.915]
Epoch [49/120    avg_loss:0.278, val_acc:0.903]
Epoch [50/120    avg_loss:0.199, val_acc:0.944]
Epoch [51/120    avg_loss:0.124, val_acc:0.944]
Epoch [52/120    avg_loss:0.096, val_acc:0.948]
Epoch [53/120    avg_loss:0.169, val_acc:0.950]
Epoch [54/120    avg_loss:0.075, val_acc:0.970]
Epoch [55/120    avg_loss:0.056, val_acc:0.970]
Epoch [56/120    avg_loss:0.067, val_acc:0.954]
Epoch [57/120    avg_loss:0.055, val_acc:0.962]
Epoch [58/120    avg_loss:0.070, val_acc:0.944]
Epoch [59/120    avg_loss:0.074, val_acc:0.956]
Epoch [60/120    avg_loss:0.054, val_acc:0.968]
Epoch [61/120    avg_loss:0.043, val_acc:0.970]
Epoch [62/120    avg_loss:0.094, val_acc:0.948]
Epoch [63/120    avg_loss:0.074, val_acc:0.929]
Epoch [64/120    avg_loss:0.079, val_acc:0.948]
Epoch [65/120    avg_loss:0.089, val_acc:0.942]
Epoch [66/120    avg_loss:0.067, val_acc:0.964]
Epoch [67/120    avg_loss:0.040, val_acc:0.976]
Epoch [68/120    avg_loss:0.044, val_acc:0.974]
Epoch [69/120    avg_loss:0.084, val_acc:0.974]
Epoch [70/120    avg_loss:0.093, val_acc:0.927]
Epoch [71/120    avg_loss:0.052, val_acc:0.978]
Epoch [72/120    avg_loss:0.040, val_acc:0.970]
Epoch [73/120    avg_loss:0.079, val_acc:0.962]
Epoch [74/120    avg_loss:0.152, val_acc:0.905]
Epoch [75/120    avg_loss:0.084, val_acc:0.968]
Epoch [76/120    avg_loss:0.038, val_acc:0.962]
Epoch [77/120    avg_loss:0.060, val_acc:0.970]
Epoch [78/120    avg_loss:0.028, val_acc:0.972]
Epoch [79/120    avg_loss:0.017, val_acc:0.976]
Epoch [80/120    avg_loss:0.022, val_acc:0.970]
Epoch [81/120    avg_loss:0.026, val_acc:0.970]
Epoch [82/120    avg_loss:0.036, val_acc:0.972]
Epoch [83/120    avg_loss:0.023, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.984]
Epoch [85/120    avg_loss:0.041, val_acc:0.966]
Epoch [86/120    avg_loss:0.023, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.990]
Epoch [91/120    avg_loss:0.017, val_acc:0.984]
Epoch [92/120    avg_loss:0.019, val_acc:0.976]
Epoch [93/120    avg_loss:0.015, val_acc:0.976]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.016, val_acc:0.974]
Epoch [97/120    avg_loss:0.020, val_acc:0.976]
Epoch [98/120    avg_loss:0.021, val_acc:0.988]
Epoch [99/120    avg_loss:0.015, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.009, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.012, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.010, val_acc:0.990]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 221   5   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   2   7 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   1 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.99545455 0.97571744 0.94039735 0.92517007
 0.99756691 1.         1.         0.99680511 1.         0.9986755
 0.99668508 1.        ]

Kappa:
0.9916915216622504
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ffbd55780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.818, val_acc:0.705]
Epoch [2/120    avg_loss:1.331, val_acc:0.584]
Epoch [3/120    avg_loss:1.329, val_acc:0.760]
Epoch [4/120    avg_loss:1.088, val_acc:0.775]
Epoch [5/120    avg_loss:1.048, val_acc:0.785]
Epoch [6/120    avg_loss:0.708, val_acc:0.826]
Epoch [7/120    avg_loss:0.634, val_acc:0.855]
Epoch [8/120    avg_loss:0.770, val_acc:0.820]
Epoch [9/120    avg_loss:0.737, val_acc:0.820]
Epoch [10/120    avg_loss:0.625, val_acc:0.879]
Epoch [11/120    avg_loss:0.598, val_acc:0.828]
Epoch [12/120    avg_loss:0.610, val_acc:0.848]
Epoch [13/120    avg_loss:0.483, val_acc:0.906]
Epoch [14/120    avg_loss:0.479, val_acc:0.887]
Epoch [15/120    avg_loss:0.396, val_acc:0.859]
Epoch [16/120    avg_loss:0.486, val_acc:0.848]
Epoch [17/120    avg_loss:0.445, val_acc:0.908]
Epoch [18/120    avg_loss:0.445, val_acc:0.914]
Epoch [19/120    avg_loss:0.464, val_acc:0.889]
Epoch [20/120    avg_loss:0.416, val_acc:0.891]
Epoch [21/120    avg_loss:0.322, val_acc:0.920]
Epoch [22/120    avg_loss:0.398, val_acc:0.900]
Epoch [23/120    avg_loss:0.341, val_acc:0.908]
Epoch [24/120    avg_loss:0.330, val_acc:0.887]
Epoch [25/120    avg_loss:0.276, val_acc:0.893]
Epoch [26/120    avg_loss:0.281, val_acc:0.938]
Epoch [27/120    avg_loss:0.226, val_acc:0.930]
Epoch [28/120    avg_loss:0.235, val_acc:0.939]
Epoch [29/120    avg_loss:0.294, val_acc:0.930]
Epoch [30/120    avg_loss:0.256, val_acc:0.943]
Epoch [31/120    avg_loss:0.211, val_acc:0.938]
Epoch [32/120    avg_loss:0.201, val_acc:0.965]
Epoch [33/120    avg_loss:0.167, val_acc:0.932]
Epoch [34/120    avg_loss:0.138, val_acc:0.939]
Epoch [35/120    avg_loss:0.325, val_acc:0.914]
Epoch [36/120    avg_loss:0.236, val_acc:0.932]
Epoch [37/120    avg_loss:0.240, val_acc:0.926]
Epoch [38/120    avg_loss:0.164, val_acc:0.967]
Epoch [39/120    avg_loss:0.097, val_acc:0.957]
Epoch [40/120    avg_loss:0.142, val_acc:0.951]
Epoch [41/120    avg_loss:0.190, val_acc:0.932]
Epoch [42/120    avg_loss:0.144, val_acc:0.934]
Epoch [43/120    avg_loss:0.195, val_acc:0.965]
Epoch [44/120    avg_loss:0.198, val_acc:0.932]
Epoch [45/120    avg_loss:0.226, val_acc:0.904]
Epoch [46/120    avg_loss:0.205, val_acc:0.932]
Epoch [47/120    avg_loss:0.154, val_acc:0.959]
Epoch [48/120    avg_loss:0.106, val_acc:0.949]
Epoch [49/120    avg_loss:0.171, val_acc:0.959]
Epoch [50/120    avg_loss:0.070, val_acc:0.977]
Epoch [51/120    avg_loss:0.099, val_acc:0.975]
Epoch [52/120    avg_loss:0.050, val_acc:0.979]
Epoch [53/120    avg_loss:0.037, val_acc:0.979]
Epoch [54/120    avg_loss:0.038, val_acc:0.945]
Epoch [55/120    avg_loss:0.061, val_acc:0.965]
Epoch [56/120    avg_loss:0.065, val_acc:0.953]
Epoch [57/120    avg_loss:0.108, val_acc:0.926]
Epoch [58/120    avg_loss:0.161, val_acc:0.959]
Epoch [59/120    avg_loss:0.187, val_acc:0.963]
Epoch [60/120    avg_loss:0.111, val_acc:0.967]
Epoch [61/120    avg_loss:0.107, val_acc:0.938]
Epoch [62/120    avg_loss:0.089, val_acc:0.971]
Epoch [63/120    avg_loss:0.144, val_acc:0.967]
Epoch [64/120    avg_loss:0.105, val_acc:0.969]
Epoch [65/120    avg_loss:0.090, val_acc:0.965]
Epoch [66/120    avg_loss:0.100, val_acc:0.965]
Epoch [67/120    avg_loss:0.039, val_acc:0.975]
Epoch [68/120    avg_loss:0.025, val_acc:0.979]
Epoch [69/120    avg_loss:0.030, val_acc:0.979]
Epoch [70/120    avg_loss:0.025, val_acc:0.980]
Epoch [71/120    avg_loss:0.017, val_acc:0.982]
Epoch [72/120    avg_loss:0.032, val_acc:0.982]
Epoch [73/120    avg_loss:0.026, val_acc:0.982]
Epoch [74/120    avg_loss:0.020, val_acc:0.982]
Epoch [75/120    avg_loss:0.021, val_acc:0.982]
Epoch [76/120    avg_loss:0.027, val_acc:0.982]
Epoch [77/120    avg_loss:0.037, val_acc:0.982]
Epoch [78/120    avg_loss:0.022, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.982]
Epoch [80/120    avg_loss:0.031, val_acc:0.982]
Epoch [81/120    avg_loss:0.024, val_acc:0.986]
Epoch [82/120    avg_loss:0.019, val_acc:0.984]
Epoch [83/120    avg_loss:0.025, val_acc:0.986]
Epoch [84/120    avg_loss:0.021, val_acc:0.984]
Epoch [85/120    avg_loss:0.027, val_acc:0.984]
Epoch [86/120    avg_loss:0.021, val_acc:0.984]
Epoch [87/120    avg_loss:0.021, val_acc:0.984]
Epoch [88/120    avg_loss:0.033, val_acc:0.982]
Epoch [89/120    avg_loss:0.022, val_acc:0.986]
Epoch [90/120    avg_loss:0.015, val_acc:0.988]
Epoch [91/120    avg_loss:0.028, val_acc:0.982]
Epoch [92/120    avg_loss:0.017, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.984]
Epoch [94/120    avg_loss:0.019, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.986]
Epoch [96/120    avg_loss:0.014, val_acc:0.986]
Epoch [97/120    avg_loss:0.015, val_acc:0.986]
Epoch [98/120    avg_loss:0.021, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.014, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.988]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.015, val_acc:0.988]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.015, val_acc:0.984]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.984]
Epoch [110/120    avg_loss:0.017, val_acc:0.984]
Epoch [111/120    avg_loss:0.020, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.017, val_acc:0.984]
Epoch [114/120    avg_loss:0.017, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.014, val_acc:0.984]
Epoch [117/120    avg_loss:0.026, val_acc:0.984]
Epoch [118/120    avg_loss:0.016, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 207  16   0   0   0   0   0   0   4   0]
 [  0   0   0   1  14 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   4 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.99545455 0.99346405 0.91796009 0.89347079
 0.99266504 1.         0.998713   1.         1.         0.99472296
 0.99005525 1.        ]

Kappa:
0.9893175658141455
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f98c99826d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.940, val_acc:0.589]
Epoch [2/120    avg_loss:1.131, val_acc:0.696]
Epoch [3/120    avg_loss:0.998, val_acc:0.730]
Epoch [4/120    avg_loss:1.194, val_acc:0.696]
Epoch [5/120    avg_loss:0.879, val_acc:0.804]
Epoch [6/120    avg_loss:0.844, val_acc:0.796]
Epoch [7/120    avg_loss:0.690, val_acc:0.790]
Epoch [8/120    avg_loss:0.736, val_acc:0.819]
Epoch [9/120    avg_loss:0.607, val_acc:0.819]
Epoch [10/120    avg_loss:0.531, val_acc:0.853]
Epoch [11/120    avg_loss:0.573, val_acc:0.827]
Epoch [12/120    avg_loss:0.683, val_acc:0.794]
Epoch [13/120    avg_loss:0.527, val_acc:0.869]
Epoch [14/120    avg_loss:0.462, val_acc:0.829]
Epoch [15/120    avg_loss:0.443, val_acc:0.885]
Epoch [16/120    avg_loss:0.444, val_acc:0.853]
Epoch [17/120    avg_loss:0.409, val_acc:0.833]
Epoch [18/120    avg_loss:0.397, val_acc:0.873]
Epoch [19/120    avg_loss:0.400, val_acc:0.847]
Epoch [20/120    avg_loss:0.452, val_acc:0.812]
Epoch [21/120    avg_loss:0.408, val_acc:0.901]
Epoch [22/120    avg_loss:0.288, val_acc:0.879]
Epoch [23/120    avg_loss:0.413, val_acc:0.859]
Epoch [24/120    avg_loss:0.318, val_acc:0.895]
Epoch [25/120    avg_loss:0.318, val_acc:0.887]
Epoch [26/120    avg_loss:0.385, val_acc:0.891]
Epoch [27/120    avg_loss:0.441, val_acc:0.879]
Epoch [28/120    avg_loss:0.244, val_acc:0.923]
Epoch [29/120    avg_loss:0.241, val_acc:0.915]
Epoch [30/120    avg_loss:0.280, val_acc:0.883]
Epoch [31/120    avg_loss:0.333, val_acc:0.903]
Epoch [32/120    avg_loss:0.306, val_acc:0.889]
Epoch [33/120    avg_loss:0.254, val_acc:0.921]
Epoch [34/120    avg_loss:0.224, val_acc:0.923]
Epoch [35/120    avg_loss:0.214, val_acc:0.907]
Epoch [36/120    avg_loss:0.220, val_acc:0.923]
Epoch [37/120    avg_loss:0.163, val_acc:0.940]
Epoch [38/120    avg_loss:0.166, val_acc:0.935]
Epoch [39/120    avg_loss:0.142, val_acc:0.917]
Epoch [40/120    avg_loss:0.168, val_acc:0.909]
Epoch [41/120    avg_loss:0.162, val_acc:0.938]
Epoch [42/120    avg_loss:0.170, val_acc:0.940]
Epoch [43/120    avg_loss:0.151, val_acc:0.938]
Epoch [44/120    avg_loss:0.122, val_acc:0.937]
Epoch [45/120    avg_loss:0.103, val_acc:0.942]
Epoch [46/120    avg_loss:0.123, val_acc:0.937]
Epoch [47/120    avg_loss:0.142, val_acc:0.952]
Epoch [48/120    avg_loss:0.227, val_acc:0.921]
Epoch [49/120    avg_loss:0.133, val_acc:0.917]
Epoch [50/120    avg_loss:0.206, val_acc:0.915]
Epoch [51/120    avg_loss:0.146, val_acc:0.937]
Epoch [52/120    avg_loss:0.115, val_acc:0.958]
Epoch [53/120    avg_loss:0.123, val_acc:0.958]
Epoch [54/120    avg_loss:0.073, val_acc:0.958]
Epoch [55/120    avg_loss:0.125, val_acc:0.952]
Epoch [56/120    avg_loss:0.113, val_acc:0.940]
Epoch [57/120    avg_loss:0.078, val_acc:0.948]
Epoch [58/120    avg_loss:0.094, val_acc:0.933]
Epoch [59/120    avg_loss:0.101, val_acc:0.960]
Epoch [60/120    avg_loss:0.089, val_acc:0.944]
Epoch [61/120    avg_loss:0.141, val_acc:0.929]
Epoch [62/120    avg_loss:0.136, val_acc:0.962]
Epoch [63/120    avg_loss:0.100, val_acc:0.944]
Epoch [64/120    avg_loss:0.082, val_acc:0.933]
Epoch [65/120    avg_loss:0.107, val_acc:0.944]
Epoch [66/120    avg_loss:0.152, val_acc:0.931]
Epoch [67/120    avg_loss:0.198, val_acc:0.931]
Epoch [68/120    avg_loss:0.154, val_acc:0.950]
Epoch [69/120    avg_loss:0.104, val_acc:0.952]
Epoch [70/120    avg_loss:0.057, val_acc:0.966]
Epoch [71/120    avg_loss:0.120, val_acc:0.954]
Epoch [72/120    avg_loss:0.075, val_acc:0.966]
Epoch [73/120    avg_loss:0.104, val_acc:0.964]
Epoch [74/120    avg_loss:0.105, val_acc:0.960]
Epoch [75/120    avg_loss:0.048, val_acc:0.974]
Epoch [76/120    avg_loss:0.067, val_acc:0.950]
Epoch [77/120    avg_loss:0.067, val_acc:0.958]
Epoch [78/120    avg_loss:0.022, val_acc:0.972]
Epoch [79/120    avg_loss:0.029, val_acc:0.958]
Epoch [80/120    avg_loss:0.083, val_acc:0.954]
Epoch [81/120    avg_loss:0.053, val_acc:0.954]
Epoch [82/120    avg_loss:0.060, val_acc:0.964]
Epoch [83/120    avg_loss:0.061, val_acc:0.964]
Epoch [84/120    avg_loss:0.064, val_acc:0.972]
Epoch [85/120    avg_loss:0.038, val_acc:0.976]
Epoch [86/120    avg_loss:0.029, val_acc:0.972]
Epoch [87/120    avg_loss:0.026, val_acc:0.964]
Epoch [88/120    avg_loss:0.093, val_acc:0.942]
Epoch [89/120    avg_loss:0.054, val_acc:0.966]
Epoch [90/120    avg_loss:0.029, val_acc:0.964]
Epoch [91/120    avg_loss:0.047, val_acc:0.966]
Epoch [92/120    avg_loss:0.025, val_acc:0.966]
Epoch [93/120    avg_loss:0.065, val_acc:0.950]
Epoch [94/120    avg_loss:0.037, val_acc:0.958]
Epoch [95/120    avg_loss:0.028, val_acc:0.966]
Epoch [96/120    avg_loss:0.022, val_acc:0.958]
Epoch [97/120    avg_loss:0.069, val_acc:0.964]
Epoch [98/120    avg_loss:0.039, val_acc:0.950]
Epoch [99/120    avg_loss:0.045, val_acc:0.960]
Epoch [100/120    avg_loss:0.013, val_acc:0.964]
Epoch [101/120    avg_loss:0.014, val_acc:0.966]
Epoch [102/120    avg_loss:0.032, val_acc:0.970]
Epoch [103/120    avg_loss:0.035, val_acc:0.966]
Epoch [104/120    avg_loss:0.012, val_acc:0.968]
Epoch [105/120    avg_loss:0.008, val_acc:0.966]
Epoch [106/120    avg_loss:0.018, val_acc:0.968]
Epoch [107/120    avg_loss:0.023, val_acc:0.968]
Epoch [108/120    avg_loss:0.029, val_acc:0.972]
Epoch [109/120    avg_loss:0.013, val_acc:0.970]
Epoch [110/120    avg_loss:0.016, val_acc:0.970]
Epoch [111/120    avg_loss:0.008, val_acc:0.970]
Epoch [112/120    avg_loss:0.008, val_acc:0.970]
Epoch [113/120    avg_loss:0.009, val_acc:0.970]
Epoch [114/120    avg_loss:0.006, val_acc:0.970]
Epoch [115/120    avg_loss:0.012, val_acc:0.970]
Epoch [116/120    avg_loss:0.014, val_acc:0.970]
Epoch [117/120    avg_loss:0.007, val_acc:0.970]
Epoch [118/120    avg_loss:0.013, val_acc:0.970]
Epoch [119/120    avg_loss:0.013, val_acc:0.970]
Epoch [120/120    avg_loss:0.009, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 214   9   0   0   0   3   4   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 133   1   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 201   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   1 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.98871332 0.96396396 0.91576674 0.90784983
 0.98529412 0.99470899 0.99614891 0.99574468 1.         0.9986755
 0.99333333 1.        ]

Kappa:
0.9871814935711085
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa91fb376a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.893, val_acc:0.651]
Epoch [2/120    avg_loss:1.250, val_acc:0.712]
Epoch [3/120    avg_loss:1.212, val_acc:0.657]
Epoch [4/120    avg_loss:1.081, val_acc:0.760]
Epoch [5/120    avg_loss:0.883, val_acc:0.806]
Epoch [6/120    avg_loss:0.850, val_acc:0.726]
Epoch [7/120    avg_loss:0.905, val_acc:0.796]
Epoch [8/120    avg_loss:0.716, val_acc:0.825]
Epoch [9/120    avg_loss:0.754, val_acc:0.847]
Epoch [10/120    avg_loss:0.563, val_acc:0.863]
Epoch [11/120    avg_loss:0.551, val_acc:0.833]
Epoch [12/120    avg_loss:0.621, val_acc:0.855]
Epoch [13/120    avg_loss:0.560, val_acc:0.883]
Epoch [14/120    avg_loss:0.461, val_acc:0.897]
Epoch [15/120    avg_loss:0.512, val_acc:0.857]
Epoch [16/120    avg_loss:0.644, val_acc:0.877]
Epoch [17/120    avg_loss:0.492, val_acc:0.899]
Epoch [18/120    avg_loss:0.432, val_acc:0.855]
Epoch [19/120    avg_loss:0.487, val_acc:0.883]
Epoch [20/120    avg_loss:0.492, val_acc:0.895]
Epoch [21/120    avg_loss:0.397, val_acc:0.905]
Epoch [22/120    avg_loss:0.335, val_acc:0.889]
Epoch [23/120    avg_loss:0.427, val_acc:0.887]
Epoch [24/120    avg_loss:0.464, val_acc:0.891]
Epoch [25/120    avg_loss:0.340, val_acc:0.905]
Epoch [26/120    avg_loss:0.377, val_acc:0.907]
Epoch [27/120    avg_loss:0.427, val_acc:0.909]
Epoch [28/120    avg_loss:0.388, val_acc:0.893]
Epoch [29/120    avg_loss:0.259, val_acc:0.899]
Epoch [30/120    avg_loss:0.255, val_acc:0.907]
Epoch [31/120    avg_loss:0.396, val_acc:0.917]
Epoch [32/120    avg_loss:0.312, val_acc:0.899]
Epoch [33/120    avg_loss:0.280, val_acc:0.917]
Epoch [34/120    avg_loss:0.309, val_acc:0.931]
Epoch [35/120    avg_loss:0.221, val_acc:0.931]
Epoch [36/120    avg_loss:0.232, val_acc:0.956]
Epoch [37/120    avg_loss:0.180, val_acc:0.919]
Epoch [38/120    avg_loss:0.200, val_acc:0.940]
Epoch [39/120    avg_loss:0.202, val_acc:0.925]
Epoch [40/120    avg_loss:0.262, val_acc:0.909]
Epoch [41/120    avg_loss:0.150, val_acc:0.915]
Epoch [42/120    avg_loss:0.274, val_acc:0.897]
Epoch [43/120    avg_loss:0.233, val_acc:0.921]
Epoch [44/120    avg_loss:0.143, val_acc:0.948]
Epoch [45/120    avg_loss:0.126, val_acc:0.942]
Epoch [46/120    avg_loss:0.138, val_acc:0.940]
Epoch [47/120    avg_loss:0.140, val_acc:0.938]
Epoch [48/120    avg_loss:0.119, val_acc:0.938]
Epoch [49/120    avg_loss:0.188, val_acc:0.954]
Epoch [50/120    avg_loss:0.120, val_acc:0.962]
Epoch [51/120    avg_loss:0.075, val_acc:0.966]
Epoch [52/120    avg_loss:0.063, val_acc:0.968]
Epoch [53/120    avg_loss:0.062, val_acc:0.968]
Epoch [54/120    avg_loss:0.097, val_acc:0.974]
Epoch [55/120    avg_loss:0.065, val_acc:0.974]
Epoch [56/120    avg_loss:0.059, val_acc:0.972]
Epoch [57/120    avg_loss:0.064, val_acc:0.970]
Epoch [58/120    avg_loss:0.046, val_acc:0.974]
Epoch [59/120    avg_loss:0.039, val_acc:0.970]
Epoch [60/120    avg_loss:0.059, val_acc:0.968]
Epoch [61/120    avg_loss:0.041, val_acc:0.974]
Epoch [62/120    avg_loss:0.050, val_acc:0.970]
Epoch [63/120    avg_loss:0.044, val_acc:0.972]
Epoch [64/120    avg_loss:0.040, val_acc:0.970]
Epoch [65/120    avg_loss:0.044, val_acc:0.972]
Epoch [66/120    avg_loss:0.069, val_acc:0.968]
Epoch [67/120    avg_loss:0.036, val_acc:0.972]
Epoch [68/120    avg_loss:0.070, val_acc:0.974]
Epoch [69/120    avg_loss:0.051, val_acc:0.966]
Epoch [70/120    avg_loss:0.057, val_acc:0.968]
Epoch [71/120    avg_loss:0.038, val_acc:0.972]
Epoch [72/120    avg_loss:0.047, val_acc:0.970]
Epoch [73/120    avg_loss:0.033, val_acc:0.974]
Epoch [74/120    avg_loss:0.035, val_acc:0.972]
Epoch [75/120    avg_loss:0.029, val_acc:0.968]
Epoch [76/120    avg_loss:0.044, val_acc:0.968]
Epoch [77/120    avg_loss:0.042, val_acc:0.972]
Epoch [78/120    avg_loss:0.048, val_acc:0.970]
Epoch [79/120    avg_loss:0.051, val_acc:0.970]
Epoch [80/120    avg_loss:0.055, val_acc:0.972]
Epoch [81/120    avg_loss:0.054, val_acc:0.972]
Epoch [82/120    avg_loss:0.035, val_acc:0.972]
Epoch [83/120    avg_loss:0.036, val_acc:0.974]
Epoch [84/120    avg_loss:0.036, val_acc:0.974]
Epoch [85/120    avg_loss:0.037, val_acc:0.972]
Epoch [86/120    avg_loss:0.044, val_acc:0.972]
Epoch [87/120    avg_loss:0.032, val_acc:0.974]
Epoch [88/120    avg_loss:0.041, val_acc:0.974]
Epoch [89/120    avg_loss:0.034, val_acc:0.972]
Epoch [90/120    avg_loss:0.025, val_acc:0.972]
Epoch [91/120    avg_loss:0.034, val_acc:0.970]
Epoch [92/120    avg_loss:0.025, val_acc:0.972]
Epoch [93/120    avg_loss:0.038, val_acc:0.970]
Epoch [94/120    avg_loss:0.041, val_acc:0.972]
Epoch [95/120    avg_loss:0.027, val_acc:0.966]
Epoch [96/120    avg_loss:0.041, val_acc:0.978]
Epoch [97/120    avg_loss:0.039, val_acc:0.974]
Epoch [98/120    avg_loss:0.052, val_acc:0.970]
Epoch [99/120    avg_loss:0.036, val_acc:0.972]
Epoch [100/120    avg_loss:0.032, val_acc:0.974]
Epoch [101/120    avg_loss:0.043, val_acc:0.974]
Epoch [102/120    avg_loss:0.033, val_acc:0.976]
Epoch [103/120    avg_loss:0.049, val_acc:0.968]
Epoch [104/120    avg_loss:0.048, val_acc:0.972]
Epoch [105/120    avg_loss:0.045, val_acc:0.970]
Epoch [106/120    avg_loss:0.039, val_acc:0.968]
Epoch [107/120    avg_loss:0.024, val_acc:0.968]
Epoch [108/120    avg_loss:0.027, val_acc:0.968]
Epoch [109/120    avg_loss:0.048, val_acc:0.972]
Epoch [110/120    avg_loss:0.025, val_acc:0.974]
Epoch [111/120    avg_loss:0.040, val_acc:0.972]
Epoch [112/120    avg_loss:0.030, val_acc:0.976]
Epoch [113/120    avg_loss:0.023, val_acc:0.974]
Epoch [114/120    avg_loss:0.037, val_acc:0.976]
Epoch [115/120    avg_loss:0.026, val_acc:0.970]
Epoch [116/120    avg_loss:0.021, val_acc:0.970]
Epoch [117/120    avg_loss:0.027, val_acc:0.970]
Epoch [118/120    avg_loss:0.045, val_acc:0.970]
Epoch [119/120    avg_loss:0.027, val_acc:0.972]
Epoch [120/120    avg_loss:0.023, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 217   8   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 201   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.99319728 0.99563319 0.94347826 0.93006993
 0.98771499 0.99470899 1.         0.99893276 1.         1.
 0.99558499 1.        ]

Kappa:
0.9926409845683385
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f36586f9748>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.881, val_acc:0.600]
Epoch [2/120    avg_loss:1.173, val_acc:0.660]
Epoch [3/120    avg_loss:1.012, val_acc:0.742]
Epoch [4/120    avg_loss:1.023, val_acc:0.758]
Epoch [5/120    avg_loss:0.833, val_acc:0.791]
Epoch [6/120    avg_loss:0.735, val_acc:0.844]
Epoch [7/120    avg_loss:0.685, val_acc:0.814]
Epoch [8/120    avg_loss:0.742, val_acc:0.850]
Epoch [9/120    avg_loss:0.691, val_acc:0.822]
Epoch [10/120    avg_loss:0.626, val_acc:0.863]
Epoch [11/120    avg_loss:0.694, val_acc:0.887]
Epoch [12/120    avg_loss:0.548, val_acc:0.875]
Epoch [13/120    avg_loss:0.623, val_acc:0.871]
Epoch [14/120    avg_loss:0.641, val_acc:0.918]
Epoch [15/120    avg_loss:0.513, val_acc:0.896]
Epoch [16/120    avg_loss:0.409, val_acc:0.898]
Epoch [17/120    avg_loss:0.404, val_acc:0.875]
Epoch [18/120    avg_loss:0.473, val_acc:0.906]
Epoch [19/120    avg_loss:0.375, val_acc:0.869]
Epoch [20/120    avg_loss:0.473, val_acc:0.912]
Epoch [21/120    avg_loss:0.388, val_acc:0.908]
Epoch [22/120    avg_loss:0.338, val_acc:0.918]
Epoch [23/120    avg_loss:0.456, val_acc:0.873]
Epoch [24/120    avg_loss:0.325, val_acc:0.936]
Epoch [25/120    avg_loss:0.328, val_acc:0.914]
Epoch [26/120    avg_loss:0.356, val_acc:0.912]
Epoch [27/120    avg_loss:0.289, val_acc:0.941]
Epoch [28/120    avg_loss:0.312, val_acc:0.877]
Epoch [29/120    avg_loss:0.372, val_acc:0.936]
Epoch [30/120    avg_loss:0.299, val_acc:0.953]
Epoch [31/120    avg_loss:0.195, val_acc:0.957]
Epoch [32/120    avg_loss:0.225, val_acc:0.959]
Epoch [33/120    avg_loss:0.238, val_acc:0.918]
Epoch [34/120    avg_loss:0.264, val_acc:0.930]
Epoch [35/120    avg_loss:0.237, val_acc:0.936]
Epoch [36/120    avg_loss:0.181, val_acc:0.955]
Epoch [37/120    avg_loss:0.171, val_acc:0.963]
Epoch [38/120    avg_loss:0.216, val_acc:0.910]
Epoch [39/120    avg_loss:0.221, val_acc:0.936]
Epoch [40/120    avg_loss:0.204, val_acc:0.961]
Epoch [41/120    avg_loss:0.122, val_acc:0.959]
Epoch [42/120    avg_loss:0.102, val_acc:0.957]
Epoch [43/120    avg_loss:0.172, val_acc:0.945]
Epoch [44/120    avg_loss:0.069, val_acc:0.975]
Epoch [45/120    avg_loss:0.199, val_acc:0.959]
Epoch [46/120    avg_loss:0.161, val_acc:0.959]
Epoch [47/120    avg_loss:0.146, val_acc:0.953]
Epoch [48/120    avg_loss:0.120, val_acc:0.969]
Epoch [49/120    avg_loss:0.257, val_acc:0.918]
Epoch [50/120    avg_loss:0.141, val_acc:0.973]
Epoch [51/120    avg_loss:0.135, val_acc:0.963]
Epoch [52/120    avg_loss:0.129, val_acc:0.977]
Epoch [53/120    avg_loss:0.130, val_acc:0.975]
Epoch [54/120    avg_loss:0.189, val_acc:0.930]
Epoch [55/120    avg_loss:0.180, val_acc:0.969]
Epoch [56/120    avg_loss:0.122, val_acc:0.955]
Epoch [57/120    avg_loss:0.095, val_acc:0.979]
Epoch [58/120    avg_loss:0.050, val_acc:0.967]
Epoch [59/120    avg_loss:0.045, val_acc:0.967]
Epoch [60/120    avg_loss:0.043, val_acc:0.975]
Epoch [61/120    avg_loss:0.055, val_acc:0.971]
Epoch [62/120    avg_loss:0.042, val_acc:0.977]
Epoch [63/120    avg_loss:0.079, val_acc:0.979]
Epoch [64/120    avg_loss:0.071, val_acc:0.965]
Epoch [65/120    avg_loss:0.146, val_acc:0.945]
Epoch [66/120    avg_loss:0.052, val_acc:0.971]
Epoch [67/120    avg_loss:0.057, val_acc:0.975]
Epoch [68/120    avg_loss:0.041, val_acc:0.971]
Epoch [69/120    avg_loss:0.049, val_acc:0.967]
Epoch [70/120    avg_loss:0.086, val_acc:0.984]
Epoch [71/120    avg_loss:0.103, val_acc:0.957]
Epoch [72/120    avg_loss:0.078, val_acc:0.982]
Epoch [73/120    avg_loss:0.179, val_acc:0.953]
Epoch [74/120    avg_loss:0.129, val_acc:0.973]
Epoch [75/120    avg_loss:0.113, val_acc:0.975]
Epoch [76/120    avg_loss:0.063, val_acc:0.980]
Epoch [77/120    avg_loss:0.043, val_acc:0.982]
Epoch [78/120    avg_loss:0.038, val_acc:0.979]
Epoch [79/120    avg_loss:0.037, val_acc:0.986]
Epoch [80/120    avg_loss:0.030, val_acc:0.986]
Epoch [81/120    avg_loss:0.031, val_acc:0.979]
Epoch [82/120    avg_loss:0.032, val_acc:0.975]
Epoch [83/120    avg_loss:0.066, val_acc:0.980]
Epoch [84/120    avg_loss:0.038, val_acc:0.975]
Epoch [85/120    avg_loss:0.045, val_acc:0.957]
Epoch [86/120    avg_loss:0.039, val_acc:0.984]
Epoch [87/120    avg_loss:0.055, val_acc:0.980]
Epoch [88/120    avg_loss:0.032, val_acc:0.986]
Epoch [89/120    avg_loss:0.018, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.019, val_acc:0.988]
Epoch [92/120    avg_loss:0.027, val_acc:0.992]
Epoch [93/120    avg_loss:0.030, val_acc:0.992]
Epoch [94/120    avg_loss:0.025, val_acc:0.992]
Epoch [95/120    avg_loss:0.031, val_acc:0.988]
Epoch [96/120    avg_loss:0.021, val_acc:0.975]
Epoch [97/120    avg_loss:0.034, val_acc:0.992]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.990]
Epoch [101/120    avg_loss:0.035, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.990]
Epoch [103/120    avg_loss:0.057, val_acc:0.986]
Epoch [104/120    avg_loss:0.026, val_acc:0.973]
Epoch [105/120    avg_loss:0.021, val_acc:0.986]
Epoch [106/120    avg_loss:0.028, val_acc:0.990]
Epoch [107/120    avg_loss:0.018, val_acc:0.988]
Epoch [108/120    avg_loss:0.018, val_acc:0.992]
Epoch [109/120    avg_loss:0.057, val_acc:0.947]
Epoch [110/120    avg_loss:0.106, val_acc:0.982]
Epoch [111/120    avg_loss:0.047, val_acc:0.969]
Epoch [112/120    avg_loss:0.060, val_acc:0.965]
Epoch [113/120    avg_loss:0.052, val_acc:0.969]
Epoch [114/120    avg_loss:0.054, val_acc:0.977]
Epoch [115/120    avg_loss:0.035, val_acc:0.988]
Epoch [116/120    avg_loss:0.030, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.018, val_acc:0.984]
Epoch [119/120    avg_loss:0.016, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   2 216   5   0   0   0   0   0   0   4   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8   0 198   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99854227 0.99545455 0.98468271 0.91525424 0.92473118
 0.98019802 1.         0.99741602 1.         1.         1.
 0.99339207 1.        ]

Kappa:
0.9895541104519691
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0da9bb748>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.845, val_acc:0.595]
Epoch [2/120    avg_loss:1.310, val_acc:0.728]
Epoch [3/120    avg_loss:1.006, val_acc:0.786]
Epoch [4/120    avg_loss:1.017, val_acc:0.790]
Epoch [5/120    avg_loss:0.932, val_acc:0.746]
Epoch [6/120    avg_loss:0.913, val_acc:0.806]
Epoch [7/120    avg_loss:0.741, val_acc:0.823]
Epoch [8/120    avg_loss:0.666, val_acc:0.813]
Epoch [9/120    avg_loss:0.634, val_acc:0.815]
Epoch [10/120    avg_loss:0.488, val_acc:0.879]
Epoch [11/120    avg_loss:0.660, val_acc:0.889]
Epoch [12/120    avg_loss:0.470, val_acc:0.897]
Epoch [13/120    avg_loss:0.424, val_acc:0.827]
Epoch [14/120    avg_loss:0.485, val_acc:0.857]
Epoch [15/120    avg_loss:0.542, val_acc:0.877]
Epoch [16/120    avg_loss:0.441, val_acc:0.877]
Epoch [17/120    avg_loss:0.396, val_acc:0.915]
Epoch [18/120    avg_loss:0.379, val_acc:0.913]
Epoch [19/120    avg_loss:0.411, val_acc:0.901]
Epoch [20/120    avg_loss:0.377, val_acc:0.895]
Epoch [21/120    avg_loss:0.264, val_acc:0.925]
Epoch [22/120    avg_loss:0.268, val_acc:0.931]
Epoch [23/120    avg_loss:0.373, val_acc:0.929]
Epoch [24/120    avg_loss:0.227, val_acc:0.927]
Epoch [25/120    avg_loss:0.409, val_acc:0.891]
Epoch [26/120    avg_loss:0.265, val_acc:0.935]
Epoch [27/120    avg_loss:0.283, val_acc:0.940]
Epoch [28/120    avg_loss:0.158, val_acc:0.933]
Epoch [29/120    avg_loss:0.221, val_acc:0.946]
Epoch [30/120    avg_loss:0.258, val_acc:0.929]
Epoch [31/120    avg_loss:0.197, val_acc:0.942]
Epoch [32/120    avg_loss:0.286, val_acc:0.925]
Epoch [33/120    avg_loss:0.132, val_acc:0.948]
Epoch [34/120    avg_loss:0.184, val_acc:0.917]
Epoch [35/120    avg_loss:0.294, val_acc:0.940]
Epoch [36/120    avg_loss:0.201, val_acc:0.942]
Epoch [37/120    avg_loss:0.170, val_acc:0.923]
Epoch [38/120    avg_loss:0.205, val_acc:0.954]
Epoch [39/120    avg_loss:0.161, val_acc:0.950]
Epoch [40/120    avg_loss:0.132, val_acc:0.946]
Epoch [41/120    avg_loss:0.180, val_acc:0.954]
Epoch [42/120    avg_loss:0.101, val_acc:0.960]
Epoch [43/120    avg_loss:0.131, val_acc:0.950]
Epoch [44/120    avg_loss:0.157, val_acc:0.952]
Epoch [45/120    avg_loss:0.139, val_acc:0.931]
Epoch [46/120    avg_loss:0.168, val_acc:0.938]
Epoch [47/120    avg_loss:0.183, val_acc:0.958]
Epoch [48/120    avg_loss:0.113, val_acc:0.911]
Epoch [49/120    avg_loss:0.085, val_acc:0.958]
Epoch [50/120    avg_loss:0.101, val_acc:0.968]
Epoch [51/120    avg_loss:0.083, val_acc:0.972]
Epoch [52/120    avg_loss:0.125, val_acc:0.960]
Epoch [53/120    avg_loss:0.110, val_acc:0.968]
Epoch [54/120    avg_loss:0.086, val_acc:0.970]
Epoch [55/120    avg_loss:0.054, val_acc:0.968]
Epoch [56/120    avg_loss:0.045, val_acc:0.976]
Epoch [57/120    avg_loss:0.048, val_acc:0.968]
Epoch [58/120    avg_loss:0.096, val_acc:0.948]
Epoch [59/120    avg_loss:0.073, val_acc:0.970]
Epoch [60/120    avg_loss:0.086, val_acc:0.948]
Epoch [61/120    avg_loss:0.074, val_acc:0.970]
Epoch [62/120    avg_loss:0.057, val_acc:0.970]
Epoch [63/120    avg_loss:0.080, val_acc:0.974]
Epoch [64/120    avg_loss:0.125, val_acc:0.968]
Epoch [65/120    avg_loss:0.212, val_acc:0.962]
Epoch [66/120    avg_loss:0.070, val_acc:0.958]
Epoch [67/120    avg_loss:0.062, val_acc:0.970]
Epoch [68/120    avg_loss:0.055, val_acc:0.980]
Epoch [69/120    avg_loss:0.064, val_acc:0.974]
Epoch [70/120    avg_loss:0.056, val_acc:0.970]
Epoch [71/120    avg_loss:0.053, val_acc:0.978]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.021, val_acc:0.980]
Epoch [74/120    avg_loss:0.028, val_acc:0.958]
Epoch [75/120    avg_loss:0.049, val_acc:0.952]
Epoch [76/120    avg_loss:0.055, val_acc:0.968]
Epoch [77/120    avg_loss:0.046, val_acc:0.970]
Epoch [78/120    avg_loss:0.057, val_acc:0.970]
Epoch [79/120    avg_loss:0.033, val_acc:0.968]
Epoch [80/120    avg_loss:0.035, val_acc:0.970]
Epoch [81/120    avg_loss:0.085, val_acc:0.976]
Epoch [82/120    avg_loss:0.042, val_acc:0.978]
Epoch [83/120    avg_loss:0.032, val_acc:0.974]
Epoch [84/120    avg_loss:0.046, val_acc:0.972]
Epoch [85/120    avg_loss:0.044, val_acc:0.984]
Epoch [86/120    avg_loss:0.047, val_acc:0.982]
Epoch [87/120    avg_loss:0.038, val_acc:0.978]
Epoch [88/120    avg_loss:0.041, val_acc:0.976]
Epoch [89/120    avg_loss:0.058, val_acc:0.980]
Epoch [90/120    avg_loss:0.023, val_acc:0.986]
Epoch [91/120    avg_loss:0.017, val_acc:0.980]
Epoch [92/120    avg_loss:0.041, val_acc:0.976]
Epoch [93/120    avg_loss:0.035, val_acc:0.982]
Epoch [94/120    avg_loss:0.041, val_acc:0.978]
Epoch [95/120    avg_loss:0.016, val_acc:0.986]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.986]
Epoch [98/120    avg_loss:0.028, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.034, val_acc:0.982]
Epoch [101/120    avg_loss:0.021, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.027, val_acc:0.982]
Epoch [104/120    avg_loss:0.033, val_acc:0.956]
Epoch [105/120    avg_loss:0.040, val_acc:0.964]
Epoch [106/120    avg_loss:0.030, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.980]
Epoch [108/120    avg_loss:0.019, val_acc:0.972]
Epoch [109/120    avg_loss:0.018, val_acc:0.990]
Epoch [110/120    avg_loss:0.009, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.003, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 226   0   0   0   0   1   2   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         0.9977221  0.99122807 0.95194508 0.93159609
 1.         1.         0.998713   0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.994065695208741
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa30f42d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.859, val_acc:0.629]
Epoch [2/120    avg_loss:1.282, val_acc:0.643]
Epoch [3/120    avg_loss:1.123, val_acc:0.702]
Epoch [4/120    avg_loss:1.011, val_acc:0.766]
Epoch [5/120    avg_loss:0.799, val_acc:0.808]
Epoch [6/120    avg_loss:0.800, val_acc:0.750]
Epoch [7/120    avg_loss:0.747, val_acc:0.837]
Epoch [8/120    avg_loss:0.662, val_acc:0.845]
Epoch [9/120    avg_loss:0.620, val_acc:0.857]
Epoch [10/120    avg_loss:0.621, val_acc:0.851]
Epoch [11/120    avg_loss:0.510, val_acc:0.883]
Epoch [12/120    avg_loss:0.431, val_acc:0.881]
Epoch [13/120    avg_loss:0.488, val_acc:0.903]
Epoch [14/120    avg_loss:0.463, val_acc:0.871]
Epoch [15/120    avg_loss:0.489, val_acc:0.869]
Epoch [16/120    avg_loss:0.382, val_acc:0.877]
Epoch [17/120    avg_loss:0.479, val_acc:0.889]
Epoch [18/120    avg_loss:0.524, val_acc:0.901]
Epoch [19/120    avg_loss:0.350, val_acc:0.923]
Epoch [20/120    avg_loss:0.296, val_acc:0.929]
Epoch [21/120    avg_loss:0.409, val_acc:0.885]
Epoch [22/120    avg_loss:0.375, val_acc:0.879]
Epoch [23/120    avg_loss:0.366, val_acc:0.913]
Epoch [24/120    avg_loss:0.324, val_acc:0.935]
Epoch [25/120    avg_loss:0.353, val_acc:0.909]
Epoch [26/120    avg_loss:0.371, val_acc:0.903]
Epoch [27/120    avg_loss:0.248, val_acc:0.929]
Epoch [28/120    avg_loss:0.328, val_acc:0.895]
Epoch [29/120    avg_loss:0.261, val_acc:0.921]
Epoch [30/120    avg_loss:0.355, val_acc:0.919]
Epoch [31/120    avg_loss:0.244, val_acc:0.923]
Epoch [32/120    avg_loss:0.242, val_acc:0.919]
Epoch [33/120    avg_loss:0.273, val_acc:0.935]
Epoch [34/120    avg_loss:0.293, val_acc:0.927]
Epoch [35/120    avg_loss:0.278, val_acc:0.891]
Epoch [36/120    avg_loss:0.277, val_acc:0.905]
Epoch [37/120    avg_loss:0.237, val_acc:0.940]
Epoch [38/120    avg_loss:0.173, val_acc:0.925]
Epoch [39/120    avg_loss:0.220, val_acc:0.933]
Epoch [40/120    avg_loss:0.184, val_acc:0.952]
Epoch [41/120    avg_loss:0.151, val_acc:0.962]
Epoch [42/120    avg_loss:0.120, val_acc:0.954]
Epoch [43/120    avg_loss:0.186, val_acc:0.905]
Epoch [44/120    avg_loss:0.156, val_acc:0.925]
Epoch [45/120    avg_loss:0.151, val_acc:0.937]
Epoch [46/120    avg_loss:0.146, val_acc:0.940]
Epoch [47/120    avg_loss:0.114, val_acc:0.960]
Epoch [48/120    avg_loss:0.130, val_acc:0.952]
Epoch [49/120    avg_loss:0.076, val_acc:0.952]
Epoch [50/120    avg_loss:0.140, val_acc:0.937]
Epoch [51/120    avg_loss:0.097, val_acc:0.956]
Epoch [52/120    avg_loss:0.060, val_acc:0.956]
Epoch [53/120    avg_loss:0.085, val_acc:0.960]
Epoch [54/120    avg_loss:0.044, val_acc:0.980]
Epoch [55/120    avg_loss:0.080, val_acc:0.954]
Epoch [56/120    avg_loss:0.068, val_acc:0.972]
Epoch [57/120    avg_loss:0.131, val_acc:0.952]
Epoch [58/120    avg_loss:0.119, val_acc:0.935]
Epoch [59/120    avg_loss:0.113, val_acc:0.946]
Epoch [60/120    avg_loss:0.081, val_acc:0.956]
Epoch [61/120    avg_loss:0.153, val_acc:0.950]
Epoch [62/120    avg_loss:0.107, val_acc:0.960]
Epoch [63/120    avg_loss:0.081, val_acc:0.976]
Epoch [64/120    avg_loss:0.060, val_acc:0.984]
Epoch [65/120    avg_loss:0.068, val_acc:0.958]
Epoch [66/120    avg_loss:0.112, val_acc:0.944]
Epoch [67/120    avg_loss:0.105, val_acc:0.956]
Epoch [68/120    avg_loss:0.070, val_acc:0.970]
Epoch [69/120    avg_loss:0.054, val_acc:0.974]
Epoch [70/120    avg_loss:0.073, val_acc:0.940]
Epoch [71/120    avg_loss:0.100, val_acc:0.970]
Epoch [72/120    avg_loss:0.103, val_acc:0.944]
Epoch [73/120    avg_loss:0.067, val_acc:0.978]
Epoch [74/120    avg_loss:0.045, val_acc:0.962]
Epoch [75/120    avg_loss:0.052, val_acc:0.974]
Epoch [76/120    avg_loss:0.052, val_acc:0.986]
Epoch [77/120    avg_loss:0.058, val_acc:0.958]
Epoch [78/120    avg_loss:0.033, val_acc:0.966]
Epoch [79/120    avg_loss:0.076, val_acc:0.964]
Epoch [80/120    avg_loss:0.063, val_acc:0.950]
Epoch [81/120    avg_loss:0.027, val_acc:0.968]
Epoch [82/120    avg_loss:0.033, val_acc:0.976]
Epoch [83/120    avg_loss:0.020, val_acc:0.968]
Epoch [84/120    avg_loss:0.060, val_acc:0.970]
Epoch [85/120    avg_loss:0.039, val_acc:0.980]
Epoch [86/120    avg_loss:0.046, val_acc:0.968]
Epoch [87/120    avg_loss:0.033, val_acc:0.966]
Epoch [88/120    avg_loss:0.040, val_acc:0.946]
Epoch [89/120    avg_loss:0.048, val_acc:0.974]
Epoch [90/120    avg_loss:0.033, val_acc:0.976]
Epoch [91/120    avg_loss:0.025, val_acc:0.982]
Epoch [92/120    avg_loss:0.025, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.986]
Epoch [94/120    avg_loss:0.019, val_acc:0.984]
Epoch [95/120    avg_loss:0.019, val_acc:0.978]
Epoch [96/120    avg_loss:0.015, val_acc:0.982]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.020, val_acc:0.982]
Epoch [99/120    avg_loss:0.014, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.978]
Epoch [108/120    avg_loss:0.013, val_acc:0.970]
Epoch [109/120    avg_loss:0.011, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.027, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.013, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.019, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220   3   0   0   0   2   5   0   0   0   0]
 [  0   0   0   0 209  14   0   0   0   0   0   0   4   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.9977221  0.97777778 0.92070485 0.90344828
 0.99756691 1.         0.99742931 0.9946865  1.         1.
 0.99449945 1.        ]

Kappa:
0.9895540591612005
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2497a96710>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.823, val_acc:0.682]
Epoch [2/120    avg_loss:1.264, val_acc:0.678]
Epoch [3/120    avg_loss:1.226, val_acc:0.697]
Epoch [4/120    avg_loss:0.968, val_acc:0.760]
Epoch [5/120    avg_loss:0.858, val_acc:0.758]
Epoch [6/120    avg_loss:0.863, val_acc:0.805]
Epoch [7/120    avg_loss:0.754, val_acc:0.836]
Epoch [8/120    avg_loss:0.710, val_acc:0.811]
Epoch [9/120    avg_loss:0.683, val_acc:0.842]
Epoch [10/120    avg_loss:0.603, val_acc:0.861]
Epoch [11/120    avg_loss:0.534, val_acc:0.828]
Epoch [12/120    avg_loss:0.617, val_acc:0.865]
Epoch [13/120    avg_loss:0.459, val_acc:0.895]
Epoch [14/120    avg_loss:0.648, val_acc:0.861]
Epoch [15/120    avg_loss:0.432, val_acc:0.865]
Epoch [16/120    avg_loss:0.519, val_acc:0.871]
Epoch [17/120    avg_loss:0.455, val_acc:0.863]
Epoch [18/120    avg_loss:0.426, val_acc:0.916]
Epoch [19/120    avg_loss:0.418, val_acc:0.893]
Epoch [20/120    avg_loss:0.383, val_acc:0.922]
Epoch [21/120    avg_loss:0.284, val_acc:0.906]
Epoch [22/120    avg_loss:0.462, val_acc:0.930]
Epoch [23/120    avg_loss:0.369, val_acc:0.898]
Epoch [24/120    avg_loss:0.339, val_acc:0.898]
Epoch [25/120    avg_loss:0.366, val_acc:0.934]
Epoch [26/120    avg_loss:0.280, val_acc:0.908]
Epoch [27/120    avg_loss:0.284, val_acc:0.947]
Epoch [28/120    avg_loss:0.293, val_acc:0.900]
Epoch [29/120    avg_loss:0.183, val_acc:0.957]
Epoch [30/120    avg_loss:0.176, val_acc:0.957]
Epoch [31/120    avg_loss:0.242, val_acc:0.930]
Epoch [32/120    avg_loss:0.226, val_acc:0.926]
Epoch [33/120    avg_loss:0.214, val_acc:0.945]
Epoch [34/120    avg_loss:0.193, val_acc:0.949]
Epoch [35/120    avg_loss:0.156, val_acc:0.957]
Epoch [36/120    avg_loss:0.229, val_acc:0.947]
Epoch [37/120    avg_loss:0.135, val_acc:0.941]
Epoch [38/120    avg_loss:0.263, val_acc:0.928]
Epoch [39/120    avg_loss:0.252, val_acc:0.939]
Epoch [40/120    avg_loss:0.128, val_acc:0.951]
Epoch [41/120    avg_loss:0.130, val_acc:0.957]
Epoch [42/120    avg_loss:0.154, val_acc:0.961]
Epoch [43/120    avg_loss:0.131, val_acc:0.973]
Epoch [44/120    avg_loss:0.136, val_acc:0.922]
Epoch [45/120    avg_loss:0.170, val_acc:0.963]
Epoch [46/120    avg_loss:0.095, val_acc:0.977]
Epoch [47/120    avg_loss:0.142, val_acc:0.965]
Epoch [48/120    avg_loss:0.135, val_acc:0.961]
Epoch [49/120    avg_loss:0.095, val_acc:0.982]
Epoch [50/120    avg_loss:0.074, val_acc:0.977]
Epoch [51/120    avg_loss:0.066, val_acc:0.955]
Epoch [52/120    avg_loss:0.186, val_acc:0.934]
Epoch [53/120    avg_loss:0.239, val_acc:0.955]
Epoch [54/120    avg_loss:0.152, val_acc:0.961]
Epoch [55/120    avg_loss:0.093, val_acc:0.980]
Epoch [56/120    avg_loss:0.062, val_acc:0.986]
Epoch [57/120    avg_loss:0.037, val_acc:0.980]
Epoch [58/120    avg_loss:0.051, val_acc:0.969]
Epoch [59/120    avg_loss:0.047, val_acc:0.982]
Epoch [60/120    avg_loss:0.026, val_acc:0.988]
Epoch [61/120    avg_loss:0.053, val_acc:0.973]
Epoch [62/120    avg_loss:0.081, val_acc:0.977]
Epoch [63/120    avg_loss:0.056, val_acc:0.977]
Epoch [64/120    avg_loss:0.040, val_acc:0.982]
Epoch [65/120    avg_loss:0.032, val_acc:0.990]
Epoch [66/120    avg_loss:0.046, val_acc:0.971]
Epoch [67/120    avg_loss:0.044, val_acc:0.971]
Epoch [68/120    avg_loss:0.057, val_acc:0.988]
Epoch [69/120    avg_loss:0.097, val_acc:0.965]
Epoch [70/120    avg_loss:0.044, val_acc:0.979]
Epoch [71/120    avg_loss:0.051, val_acc:0.986]
Epoch [72/120    avg_loss:0.033, val_acc:0.980]
Epoch [73/120    avg_loss:0.052, val_acc:0.975]
Epoch [74/120    avg_loss:0.046, val_acc:0.980]
Epoch [75/120    avg_loss:0.035, val_acc:0.992]
Epoch [76/120    avg_loss:0.053, val_acc:0.980]
Epoch [77/120    avg_loss:0.048, val_acc:0.988]
Epoch [78/120    avg_loss:0.044, val_acc:0.977]
Epoch [79/120    avg_loss:0.037, val_acc:0.990]
Epoch [80/120    avg_loss:0.034, val_acc:0.982]
Epoch [81/120    avg_loss:0.038, val_acc:0.979]
Epoch [82/120    avg_loss:0.075, val_acc:0.979]
Epoch [83/120    avg_loss:0.033, val_acc:0.982]
Epoch [84/120    avg_loss:0.027, val_acc:0.990]
Epoch [85/120    avg_loss:0.028, val_acc:0.984]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.015, val_acc:0.992]
Epoch [88/120    avg_loss:0.010, val_acc:0.992]
Epoch [89/120    avg_loss:0.005, val_acc:0.994]
Epoch [90/120    avg_loss:0.007, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.994]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.994]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.994]
Epoch [98/120    avg_loss:0.017, val_acc:0.977]
Epoch [99/120    avg_loss:0.045, val_acc:0.979]
Epoch [100/120    avg_loss:0.068, val_acc:0.982]
Epoch [101/120    avg_loss:0.049, val_acc:0.980]
Epoch [102/120    avg_loss:0.088, val_acc:0.986]
Epoch [103/120    avg_loss:0.056, val_acc:0.990]
Epoch [104/120    avg_loss:0.014, val_acc:0.990]
Epoch [105/120    avg_loss:0.015, val_acc:0.990]
Epoch [106/120    avg_loss:0.018, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.994]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.008, val_acc:0.994]
Epoch [110/120    avg_loss:0.009, val_acc:0.996]
Epoch [111/120    avg_loss:0.011, val_acc:0.990]
Epoch [112/120    avg_loss:0.014, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.992]
Epoch [114/120    avg_loss:0.025, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   2 210  14   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 217   7   0   0   0   0   0   0   3   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.99086758 0.95454545 0.92144374 0.93333333
 0.99756691 0.98947368 0.99487179 1.         1.         0.9986755
 0.99559471 1.        ]

Kappa:
0.9890797063496897
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4206acc748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.994, val_acc:0.615]
Epoch [2/120    avg_loss:1.264, val_acc:0.756]
Epoch [3/120    avg_loss:1.045, val_acc:0.688]
Epoch [4/120    avg_loss:0.928, val_acc:0.724]
Epoch [5/120    avg_loss:0.805, val_acc:0.796]
Epoch [6/120    avg_loss:0.758, val_acc:0.720]
Epoch [7/120    avg_loss:0.779, val_acc:0.786]
Epoch [8/120    avg_loss:0.577, val_acc:0.823]
Epoch [9/120    avg_loss:0.574, val_acc:0.845]
Epoch [10/120    avg_loss:0.589, val_acc:0.865]
Epoch [11/120    avg_loss:0.695, val_acc:0.845]
Epoch [12/120    avg_loss:0.525, val_acc:0.821]
Epoch [13/120    avg_loss:0.587, val_acc:0.750]
Epoch [14/120    avg_loss:0.661, val_acc:0.837]
Epoch [15/120    avg_loss:0.552, val_acc:0.873]
Epoch [16/120    avg_loss:0.420, val_acc:0.871]
Epoch [17/120    avg_loss:0.384, val_acc:0.891]
Epoch [18/120    avg_loss:0.438, val_acc:0.891]
Epoch [19/120    avg_loss:0.390, val_acc:0.883]
Epoch [20/120    avg_loss:0.424, val_acc:0.889]
Epoch [21/120    avg_loss:0.345, val_acc:0.927]
Epoch [22/120    avg_loss:0.377, val_acc:0.897]
Epoch [23/120    avg_loss:0.273, val_acc:0.901]
Epoch [24/120    avg_loss:0.316, val_acc:0.897]
Epoch [25/120    avg_loss:0.278, val_acc:0.915]
Epoch [26/120    avg_loss:0.284, val_acc:0.869]
Epoch [27/120    avg_loss:0.206, val_acc:0.915]
Epoch [28/120    avg_loss:0.232, val_acc:0.879]
Epoch [29/120    avg_loss:0.292, val_acc:0.901]
Epoch [30/120    avg_loss:0.159, val_acc:0.944]
Epoch [31/120    avg_loss:0.190, val_acc:0.942]
Epoch [32/120    avg_loss:0.220, val_acc:0.925]
Epoch [33/120    avg_loss:0.290, val_acc:0.925]
Epoch [34/120    avg_loss:0.182, val_acc:0.915]
Epoch [35/120    avg_loss:0.276, val_acc:0.931]
Epoch [36/120    avg_loss:0.221, val_acc:0.944]
Epoch [37/120    avg_loss:0.151, val_acc:0.923]
Epoch [38/120    avg_loss:0.149, val_acc:0.958]
Epoch [39/120    avg_loss:0.201, val_acc:0.938]
Epoch [40/120    avg_loss:0.132, val_acc:0.960]
Epoch [41/120    avg_loss:0.103, val_acc:0.976]
Epoch [42/120    avg_loss:0.122, val_acc:0.956]
Epoch [43/120    avg_loss:0.119, val_acc:0.956]
Epoch [44/120    avg_loss:0.144, val_acc:0.966]
Epoch [45/120    avg_loss:0.121, val_acc:0.964]
Epoch [46/120    avg_loss:0.225, val_acc:0.853]
Epoch [47/120    avg_loss:0.124, val_acc:0.966]
Epoch [48/120    avg_loss:0.085, val_acc:0.972]
Epoch [49/120    avg_loss:0.070, val_acc:0.974]
Epoch [50/120    avg_loss:0.113, val_acc:0.972]
Epoch [51/120    avg_loss:0.066, val_acc:0.974]
Epoch [52/120    avg_loss:0.078, val_acc:0.968]
Epoch [53/120    avg_loss:0.128, val_acc:0.954]
Epoch [54/120    avg_loss:0.228, val_acc:0.946]
Epoch [55/120    avg_loss:0.095, val_acc:0.980]
Epoch [56/120    avg_loss:0.117, val_acc:0.978]
Epoch [57/120    avg_loss:0.058, val_acc:0.984]
Epoch [58/120    avg_loss:0.064, val_acc:0.986]
Epoch [59/120    avg_loss:0.050, val_acc:0.986]
Epoch [60/120    avg_loss:0.043, val_acc:0.990]
Epoch [61/120    avg_loss:0.038, val_acc:0.990]
Epoch [62/120    avg_loss:0.053, val_acc:0.990]
Epoch [63/120    avg_loss:0.036, val_acc:0.992]
Epoch [64/120    avg_loss:0.033, val_acc:0.992]
Epoch [65/120    avg_loss:0.040, val_acc:0.992]
Epoch [66/120    avg_loss:0.034, val_acc:0.992]
Epoch [67/120    avg_loss:0.040, val_acc:0.992]
Epoch [68/120    avg_loss:0.038, val_acc:0.992]
Epoch [69/120    avg_loss:0.048, val_acc:0.992]
Epoch [70/120    avg_loss:0.043, val_acc:0.992]
Epoch [71/120    avg_loss:0.036, val_acc:0.992]
Epoch [72/120    avg_loss:0.036, val_acc:0.992]
Epoch [73/120    avg_loss:0.050, val_acc:0.992]
Epoch [74/120    avg_loss:0.029, val_acc:0.992]
Epoch [75/120    avg_loss:0.021, val_acc:0.992]
Epoch [76/120    avg_loss:0.039, val_acc:0.992]
Epoch [77/120    avg_loss:0.031, val_acc:0.992]
Epoch [78/120    avg_loss:0.027, val_acc:0.992]
Epoch [79/120    avg_loss:0.015, val_acc:0.992]
Epoch [80/120    avg_loss:0.032, val_acc:0.992]
Epoch [81/120    avg_loss:0.029, val_acc:0.992]
Epoch [82/120    avg_loss:0.028, val_acc:0.996]
Epoch [83/120    avg_loss:0.026, val_acc:0.994]
Epoch [84/120    avg_loss:0.024, val_acc:0.994]
Epoch [85/120    avg_loss:0.035, val_acc:0.996]
Epoch [86/120    avg_loss:0.024, val_acc:0.994]
Epoch [87/120    avg_loss:0.019, val_acc:0.994]
Epoch [88/120    avg_loss:0.021, val_acc:0.994]
Epoch [89/120    avg_loss:0.033, val_acc:0.992]
Epoch [90/120    avg_loss:0.022, val_acc:0.992]
Epoch [91/120    avg_loss:0.025, val_acc:0.992]
Epoch [92/120    avg_loss:0.015, val_acc:0.992]
Epoch [93/120    avg_loss:0.023, val_acc:0.992]
Epoch [94/120    avg_loss:0.022, val_acc:0.994]
Epoch [95/120    avg_loss:0.026, val_acc:0.992]
Epoch [96/120    avg_loss:0.033, val_acc:0.994]
Epoch [97/120    avg_loss:0.015, val_acc:0.996]
Epoch [98/120    avg_loss:0.034, val_acc:0.994]
Epoch [99/120    avg_loss:0.016, val_acc:0.994]
Epoch [100/120    avg_loss:0.024, val_acc:0.996]
Epoch [101/120    avg_loss:0.027, val_acc:0.996]
Epoch [102/120    avg_loss:0.019, val_acc:0.996]
Epoch [103/120    avg_loss:0.022, val_acc:0.996]
Epoch [104/120    avg_loss:0.021, val_acc:0.992]
Epoch [105/120    avg_loss:0.016, val_acc:0.992]
Epoch [106/120    avg_loss:0.026, val_acc:0.994]
Epoch [107/120    avg_loss:0.021, val_acc:0.994]
Epoch [108/120    avg_loss:0.026, val_acc:0.992]
Epoch [109/120    avg_loss:0.015, val_acc:0.990]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.016, val_acc:0.992]
Epoch [112/120    avg_loss:0.021, val_acc:0.994]
Epoch [113/120    avg_loss:0.012, val_acc:0.994]
Epoch [114/120    avg_loss:0.028, val_acc:0.994]
Epoch [115/120    avg_loss:0.014, val_acc:0.996]
Epoch [116/120    avg_loss:0.011, val_acc:0.996]
Epoch [117/120    avg_loss:0.030, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.014, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 209  17   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.9977221  0.95216401 0.92241379 0.93602694
 1.         1.         0.99742931 0.99893276 1.         0.98562092
 0.9877095  1.        ]

Kappa:
0.9878941082939554
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f096da9c748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.773, val_acc:0.712]
Epoch [2/120    avg_loss:1.125, val_acc:0.712]
Epoch [3/120    avg_loss:1.155, val_acc:0.728]
Epoch [4/120    avg_loss:0.910, val_acc:0.722]
Epoch [5/120    avg_loss:0.909, val_acc:0.798]
Epoch [6/120    avg_loss:0.791, val_acc:0.839]
Epoch [7/120    avg_loss:0.643, val_acc:0.835]
Epoch [8/120    avg_loss:0.721, val_acc:0.831]
Epoch [9/120    avg_loss:0.644, val_acc:0.847]
Epoch [10/120    avg_loss:0.524, val_acc:0.881]
Epoch [11/120    avg_loss:0.543, val_acc:0.871]
Epoch [12/120    avg_loss:0.498, val_acc:0.847]
Epoch [13/120    avg_loss:0.563, val_acc:0.855]
Epoch [14/120    avg_loss:0.498, val_acc:0.883]
Epoch [15/120    avg_loss:0.358, val_acc:0.887]
Epoch [16/120    avg_loss:0.365, val_acc:0.907]
Epoch [17/120    avg_loss:0.357, val_acc:0.899]
Epoch [18/120    avg_loss:0.324, val_acc:0.885]
Epoch [19/120    avg_loss:0.319, val_acc:0.907]
Epoch [20/120    avg_loss:0.365, val_acc:0.879]
Epoch [21/120    avg_loss:0.366, val_acc:0.895]
Epoch [22/120    avg_loss:0.235, val_acc:0.940]
Epoch [23/120    avg_loss:0.241, val_acc:0.937]
Epoch [24/120    avg_loss:0.339, val_acc:0.917]
Epoch [25/120    avg_loss:0.297, val_acc:0.909]
Epoch [26/120    avg_loss:0.224, val_acc:0.925]
Epoch [27/120    avg_loss:0.220, val_acc:0.946]
Epoch [28/120    avg_loss:0.280, val_acc:0.911]
Epoch [29/120    avg_loss:0.188, val_acc:0.869]
Epoch [30/120    avg_loss:0.194, val_acc:0.931]
Epoch [31/120    avg_loss:0.134, val_acc:0.966]
Epoch [32/120    avg_loss:0.246, val_acc:0.905]
Epoch [33/120    avg_loss:0.227, val_acc:0.948]
Epoch [34/120    avg_loss:0.112, val_acc:0.968]
Epoch [35/120    avg_loss:0.134, val_acc:0.968]
Epoch [36/120    avg_loss:0.104, val_acc:0.970]
Epoch [37/120    avg_loss:0.120, val_acc:0.962]
Epoch [38/120    avg_loss:0.132, val_acc:0.952]
Epoch [39/120    avg_loss:0.177, val_acc:0.937]
Epoch [40/120    avg_loss:0.189, val_acc:0.950]
Epoch [41/120    avg_loss:0.158, val_acc:0.964]
Epoch [42/120    avg_loss:0.136, val_acc:0.958]
Epoch [43/120    avg_loss:0.100, val_acc:0.968]
Epoch [44/120    avg_loss:0.184, val_acc:0.933]
Epoch [45/120    avg_loss:0.163, val_acc:0.964]
Epoch [46/120    avg_loss:0.094, val_acc:0.974]
Epoch [47/120    avg_loss:0.090, val_acc:0.976]
Epoch [48/120    avg_loss:0.088, val_acc:0.984]
Epoch [49/120    avg_loss:0.079, val_acc:0.974]
Epoch [50/120    avg_loss:0.084, val_acc:0.962]
Epoch [51/120    avg_loss:0.094, val_acc:0.978]
Epoch [52/120    avg_loss:0.043, val_acc:0.984]
Epoch [53/120    avg_loss:0.062, val_acc:0.960]
Epoch [54/120    avg_loss:0.065, val_acc:0.970]
Epoch [55/120    avg_loss:0.068, val_acc:0.986]
Epoch [56/120    avg_loss:0.041, val_acc:0.972]
Epoch [57/120    avg_loss:0.061, val_acc:0.970]
Epoch [58/120    avg_loss:0.049, val_acc:0.950]
Epoch [59/120    avg_loss:0.080, val_acc:0.954]
Epoch [60/120    avg_loss:0.050, val_acc:0.982]
Epoch [61/120    avg_loss:0.128, val_acc:0.950]
Epoch [62/120    avg_loss:0.077, val_acc:0.984]
Epoch [63/120    avg_loss:0.106, val_acc:0.968]
Epoch [64/120    avg_loss:0.091, val_acc:0.990]
Epoch [65/120    avg_loss:0.075, val_acc:0.972]
Epoch [66/120    avg_loss:0.052, val_acc:0.986]
Epoch [67/120    avg_loss:0.029, val_acc:0.980]
Epoch [68/120    avg_loss:0.059, val_acc:0.978]
Epoch [69/120    avg_loss:0.080, val_acc:0.964]
Epoch [70/120    avg_loss:0.044, val_acc:0.992]
Epoch [71/120    avg_loss:0.017, val_acc:0.992]
Epoch [72/120    avg_loss:0.040, val_acc:0.986]
Epoch [73/120    avg_loss:0.043, val_acc:0.984]
Epoch [74/120    avg_loss:0.024, val_acc:0.992]
Epoch [75/120    avg_loss:0.018, val_acc:0.986]
Epoch [76/120    avg_loss:0.034, val_acc:0.976]
Epoch [77/120    avg_loss:0.029, val_acc:0.990]
Epoch [78/120    avg_loss:0.013, val_acc:0.992]
Epoch [79/120    avg_loss:0.013, val_acc:0.992]
Epoch [80/120    avg_loss:0.014, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.013, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.992]
Epoch [84/120    avg_loss:0.011, val_acc:0.992]
Epoch [85/120    avg_loss:0.027, val_acc:0.962]
Epoch [86/120    avg_loss:0.025, val_acc:0.992]
Epoch [87/120    avg_loss:0.016, val_acc:0.994]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.015, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.992]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.970]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.010, val_acc:0.994]
Epoch [104/120    avg_loss:0.023, val_acc:0.994]
Epoch [105/120    avg_loss:0.014, val_acc:0.994]
Epoch [106/120    avg_loss:0.008, val_acc:0.994]
Epoch [107/120    avg_loss:0.003, val_acc:0.994]
Epoch [108/120    avg_loss:0.003, val_acc:0.994]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.010, val_acc:0.994]
Epoch [112/120    avg_loss:0.003, val_acc:0.994]
Epoch [113/120    avg_loss:0.003, val_acc:0.994]
Epoch [114/120    avg_loss:0.007, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.994]
Epoch [116/120    avg_loss:0.007, val_acc:0.994]
Epoch [117/120    avg_loss:0.006, val_acc:0.994]
Epoch [118/120    avg_loss:0.013, val_acc:0.992]
Epoch [119/120    avg_loss:0.014, val_acc:0.994]
Epoch [120/120    avg_loss:0.004, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   9   0   0   0   0   0   0   2   0]
 [  0   0   0   1  12 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         0.9977221  0.99126638 0.9452954  0.92307692
 1.         1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9935904522185752
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4754f76a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.982, val_acc:0.577]
Epoch [2/120    avg_loss:1.294, val_acc:0.700]
Epoch [3/120    avg_loss:1.202, val_acc:0.587]
Epoch [4/120    avg_loss:1.141, val_acc:0.720]
Epoch [5/120    avg_loss:0.882, val_acc:0.768]
Epoch [6/120    avg_loss:0.693, val_acc:0.792]
Epoch [7/120    avg_loss:0.808, val_acc:0.817]
Epoch [8/120    avg_loss:0.682, val_acc:0.839]
Epoch [9/120    avg_loss:0.660, val_acc:0.796]
Epoch [10/120    avg_loss:0.620, val_acc:0.853]
Epoch [11/120    avg_loss:0.549, val_acc:0.841]
Epoch [12/120    avg_loss:0.502, val_acc:0.875]
Epoch [13/120    avg_loss:0.444, val_acc:0.845]
Epoch [14/120    avg_loss:0.505, val_acc:0.863]
Epoch [15/120    avg_loss:0.534, val_acc:0.859]
Epoch [16/120    avg_loss:0.400, val_acc:0.869]
Epoch [17/120    avg_loss:0.450, val_acc:0.893]
Epoch [18/120    avg_loss:0.411, val_acc:0.875]
Epoch [19/120    avg_loss:0.390, val_acc:0.903]
Epoch [20/120    avg_loss:0.359, val_acc:0.907]
Epoch [21/120    avg_loss:0.358, val_acc:0.877]
Epoch [22/120    avg_loss:0.357, val_acc:0.915]
Epoch [23/120    avg_loss:0.321, val_acc:0.887]
Epoch [24/120    avg_loss:0.355, val_acc:0.885]
Epoch [25/120    avg_loss:0.328, val_acc:0.907]
Epoch [26/120    avg_loss:0.248, val_acc:0.915]
Epoch [27/120    avg_loss:0.220, val_acc:0.883]
Epoch [28/120    avg_loss:0.279, val_acc:0.917]
Epoch [29/120    avg_loss:0.309, val_acc:0.913]
Epoch [30/120    avg_loss:0.290, val_acc:0.897]
Epoch [31/120    avg_loss:0.240, val_acc:0.919]
Epoch [32/120    avg_loss:0.229, val_acc:0.923]
Epoch [33/120    avg_loss:0.230, val_acc:0.944]
Epoch [34/120    avg_loss:0.234, val_acc:0.889]
Epoch [35/120    avg_loss:0.227, val_acc:0.883]
Epoch [36/120    avg_loss:0.216, val_acc:0.883]
Epoch [37/120    avg_loss:0.153, val_acc:0.915]
Epoch [38/120    avg_loss:0.192, val_acc:0.905]
Epoch [39/120    avg_loss:0.162, val_acc:0.940]
Epoch [40/120    avg_loss:0.190, val_acc:0.935]
Epoch [41/120    avg_loss:0.230, val_acc:0.931]
Epoch [42/120    avg_loss:0.193, val_acc:0.933]
Epoch [43/120    avg_loss:0.287, val_acc:0.917]
Epoch [44/120    avg_loss:0.212, val_acc:0.962]
Epoch [45/120    avg_loss:0.139, val_acc:0.958]
Epoch [46/120    avg_loss:0.081, val_acc:0.956]
Epoch [47/120    avg_loss:0.137, val_acc:0.954]
Epoch [48/120    avg_loss:0.162, val_acc:0.933]
Epoch [49/120    avg_loss:0.171, val_acc:0.925]
Epoch [50/120    avg_loss:0.138, val_acc:0.960]
Epoch [51/120    avg_loss:0.091, val_acc:0.960]
Epoch [52/120    avg_loss:0.099, val_acc:0.962]
Epoch [53/120    avg_loss:0.128, val_acc:0.960]
Epoch [54/120    avg_loss:0.139, val_acc:0.960]
Epoch [55/120    avg_loss:0.135, val_acc:0.925]
Epoch [56/120    avg_loss:0.106, val_acc:0.944]
Epoch [57/120    avg_loss:0.079, val_acc:0.982]
Epoch [58/120    avg_loss:0.088, val_acc:0.935]
Epoch [59/120    avg_loss:0.074, val_acc:0.948]
Epoch [60/120    avg_loss:0.095, val_acc:0.952]
Epoch [61/120    avg_loss:0.074, val_acc:0.962]
Epoch [62/120    avg_loss:0.217, val_acc:0.956]
Epoch [63/120    avg_loss:0.088, val_acc:0.935]
Epoch [64/120    avg_loss:0.059, val_acc:0.964]
Epoch [65/120    avg_loss:0.081, val_acc:0.968]
Epoch [66/120    avg_loss:0.044, val_acc:0.976]
Epoch [67/120    avg_loss:0.035, val_acc:0.974]
Epoch [68/120    avg_loss:0.069, val_acc:0.964]
Epoch [69/120    avg_loss:0.035, val_acc:0.976]
Epoch [70/120    avg_loss:0.036, val_acc:0.982]
Epoch [71/120    avg_loss:0.059, val_acc:0.976]
Epoch [72/120    avg_loss:0.051, val_acc:0.956]
Epoch [73/120    avg_loss:0.029, val_acc:0.980]
Epoch [74/120    avg_loss:0.038, val_acc:0.976]
Epoch [75/120    avg_loss:0.072, val_acc:0.962]
Epoch [76/120    avg_loss:0.027, val_acc:0.982]
Epoch [77/120    avg_loss:0.032, val_acc:0.984]
Epoch [78/120    avg_loss:0.022, val_acc:0.968]
Epoch [79/120    avg_loss:0.041, val_acc:0.978]
Epoch [80/120    avg_loss:0.033, val_acc:0.984]
Epoch [81/120    avg_loss:0.018, val_acc:0.986]
Epoch [82/120    avg_loss:0.033, val_acc:0.976]
Epoch [83/120    avg_loss:0.064, val_acc:0.974]
Epoch [84/120    avg_loss:0.052, val_acc:0.978]
Epoch [85/120    avg_loss:0.091, val_acc:0.976]
Epoch [86/120    avg_loss:0.052, val_acc:0.946]
Epoch [87/120    avg_loss:0.067, val_acc:0.984]
Epoch [88/120    avg_loss:0.037, val_acc:0.978]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.031, val_acc:0.982]
Epoch [91/120    avg_loss:0.033, val_acc:0.986]
Epoch [92/120    avg_loss:0.014, val_acc:0.986]
Epoch [93/120    avg_loss:0.018, val_acc:0.990]
Epoch [94/120    avg_loss:0.023, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.984]
Epoch [96/120    avg_loss:0.032, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.972]
Epoch [98/120    avg_loss:0.045, val_acc:0.974]
Epoch [99/120    avg_loss:0.028, val_acc:0.984]
Epoch [100/120    avg_loss:0.021, val_acc:0.984]
Epoch [101/120    avg_loss:0.035, val_acc:0.990]
Epoch [102/120    avg_loss:0.028, val_acc:0.984]
Epoch [103/120    avg_loss:0.015, val_acc:0.992]
Epoch [104/120    avg_loss:0.017, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.992]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.022, val_acc:0.996]
Epoch [108/120    avg_loss:0.012, val_acc:0.994]
Epoch [109/120    avg_loss:0.032, val_acc:0.990]
Epoch [110/120    avg_loss:0.015, val_acc:0.994]
Epoch [111/120    avg_loss:0.013, val_acc:0.992]
Epoch [112/120    avg_loss:0.012, val_acc:0.990]
Epoch [113/120    avg_loss:0.047, val_acc:0.986]
Epoch [114/120    avg_loss:0.040, val_acc:0.986]
Epoch [115/120    avg_loss:0.015, val_acc:0.992]
Epoch [116/120    avg_loss:0.026, val_acc:0.978]
Epoch [117/120    avg_loss:0.034, val_acc:0.980]
Epoch [118/120    avg_loss:0.031, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.994]
Epoch [120/120    avg_loss:0.011, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  12   0   0   0   0   0   0   5   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         1.         0.98454746 0.92715232 0.92832765
 1.         1.         1.         1.         1.         0.9973545
 0.99229923 1.        ]

Kappa:
0.9916913500804468
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9795cea748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.860, val_acc:0.627]
Epoch [2/120    avg_loss:1.179, val_acc:0.698]
Epoch [3/120    avg_loss:1.018, val_acc:0.694]
Epoch [4/120    avg_loss:1.005, val_acc:0.758]
Epoch [5/120    avg_loss:0.884, val_acc:0.796]
Epoch [6/120    avg_loss:0.812, val_acc:0.813]
Epoch [7/120    avg_loss:0.733, val_acc:0.837]
Epoch [8/120    avg_loss:0.724, val_acc:0.839]
Epoch [9/120    avg_loss:0.566, val_acc:0.875]
Epoch [10/120    avg_loss:0.579, val_acc:0.871]
Epoch [11/120    avg_loss:0.648, val_acc:0.891]
Epoch [12/120    avg_loss:0.418, val_acc:0.901]
Epoch [13/120    avg_loss:0.357, val_acc:0.919]
Epoch [14/120    avg_loss:0.456, val_acc:0.780]
Epoch [15/120    avg_loss:0.409, val_acc:0.911]
Epoch [16/120    avg_loss:0.378, val_acc:0.927]
Epoch [17/120    avg_loss:0.423, val_acc:0.899]
Epoch [18/120    avg_loss:0.371, val_acc:0.869]
Epoch [19/120    avg_loss:0.362, val_acc:0.917]
Epoch [20/120    avg_loss:0.298, val_acc:0.907]
Epoch [21/120    avg_loss:0.300, val_acc:0.940]
Epoch [22/120    avg_loss:0.317, val_acc:0.915]
Epoch [23/120    avg_loss:0.226, val_acc:0.925]
Epoch [24/120    avg_loss:0.257, val_acc:0.937]
Epoch [25/120    avg_loss:0.337, val_acc:0.907]
Epoch [26/120    avg_loss:0.312, val_acc:0.935]
Epoch [27/120    avg_loss:0.213, val_acc:0.956]
Epoch [28/120    avg_loss:0.134, val_acc:0.940]
Epoch [29/120    avg_loss:0.170, val_acc:0.964]
Epoch [30/120    avg_loss:0.180, val_acc:0.925]
Epoch [31/120    avg_loss:0.209, val_acc:0.962]
Epoch [32/120    avg_loss:0.144, val_acc:0.966]
Epoch [33/120    avg_loss:0.212, val_acc:0.968]
Epoch [34/120    avg_loss:0.151, val_acc:0.968]
Epoch [35/120    avg_loss:0.142, val_acc:0.964]
Epoch [36/120    avg_loss:0.118, val_acc:0.974]
Epoch [37/120    avg_loss:0.115, val_acc:0.970]
Epoch [38/120    avg_loss:0.066, val_acc:0.980]
Epoch [39/120    avg_loss:0.106, val_acc:0.964]
Epoch [40/120    avg_loss:0.176, val_acc:0.950]
Epoch [41/120    avg_loss:0.099, val_acc:0.974]
Epoch [42/120    avg_loss:0.068, val_acc:0.980]
Epoch [43/120    avg_loss:0.078, val_acc:0.966]
Epoch [44/120    avg_loss:0.070, val_acc:0.970]
Epoch [45/120    avg_loss:0.046, val_acc:0.972]
Epoch [46/120    avg_loss:0.048, val_acc:0.980]
Epoch [47/120    avg_loss:0.044, val_acc:0.978]
Epoch [48/120    avg_loss:0.045, val_acc:0.982]
Epoch [49/120    avg_loss:0.049, val_acc:0.986]
Epoch [50/120    avg_loss:0.035, val_acc:0.980]
Epoch [51/120    avg_loss:0.063, val_acc:0.978]
Epoch [52/120    avg_loss:0.101, val_acc:0.978]
Epoch [53/120    avg_loss:0.095, val_acc:0.966]
Epoch [54/120    avg_loss:0.048, val_acc:0.980]
Epoch [55/120    avg_loss:0.040, val_acc:0.986]
Epoch [56/120    avg_loss:0.038, val_acc:0.986]
Epoch [57/120    avg_loss:0.039, val_acc:0.962]
Epoch [58/120    avg_loss:0.060, val_acc:0.986]
Epoch [59/120    avg_loss:0.076, val_acc:0.982]
Epoch [60/120    avg_loss:0.049, val_acc:0.966]
Epoch [61/120    avg_loss:0.063, val_acc:0.976]
Epoch [62/120    avg_loss:0.063, val_acc:0.976]
Epoch [63/120    avg_loss:0.028, val_acc:0.984]
Epoch [64/120    avg_loss:0.029, val_acc:0.984]
Epoch [65/120    avg_loss:0.023, val_acc:0.992]
Epoch [66/120    avg_loss:0.014, val_acc:0.988]
Epoch [67/120    avg_loss:0.011, val_acc:0.994]
Epoch [68/120    avg_loss:0.010, val_acc:0.994]
Epoch [69/120    avg_loss:0.019, val_acc:0.994]
Epoch [70/120    avg_loss:0.015, val_acc:0.984]
Epoch [71/120    avg_loss:0.027, val_acc:0.988]
Epoch [72/120    avg_loss:0.022, val_acc:0.990]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.994]
Epoch [76/120    avg_loss:0.006, val_acc:0.994]
Epoch [77/120    avg_loss:0.010, val_acc:0.992]
Epoch [78/120    avg_loss:0.014, val_acc:0.984]
Epoch [79/120    avg_loss:0.029, val_acc:0.984]
Epoch [80/120    avg_loss:0.016, val_acc:0.990]
Epoch [81/120    avg_loss:0.018, val_acc:0.992]
Epoch [82/120    avg_loss:0.017, val_acc:0.986]
Epoch [83/120    avg_loss:0.024, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.992]
Epoch [85/120    avg_loss:0.033, val_acc:0.990]
Epoch [86/120    avg_loss:0.027, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.990]
Epoch [88/120    avg_loss:0.027, val_acc:0.994]
Epoch [89/120    avg_loss:0.014, val_acc:0.992]
Epoch [90/120    avg_loss:0.011, val_acc:0.994]
Epoch [91/120    avg_loss:0.007, val_acc:0.990]
Epoch [92/120    avg_loss:0.010, val_acc:0.998]
Epoch [93/120    avg_loss:0.007, val_acc:0.994]
Epoch [94/120    avg_loss:0.009, val_acc:0.994]
Epoch [95/120    avg_loss:0.008, val_acc:0.994]
Epoch [96/120    avg_loss:0.006, val_acc:0.996]
Epoch [97/120    avg_loss:0.005, val_acc:0.996]
Epoch [98/120    avg_loss:0.007, val_acc:0.996]
Epoch [99/120    avg_loss:0.010, val_acc:0.994]
Epoch [100/120    avg_loss:0.003, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.996]
Epoch [102/120    avg_loss:0.004, val_acc:0.994]
Epoch [103/120    avg_loss:0.003, val_acc:0.992]
Epoch [104/120    avg_loss:0.002, val_acc:0.996]
Epoch [105/120    avg_loss:0.002, val_acc:0.996]
Epoch [106/120    avg_loss:0.003, val_acc:0.996]
Epoch [107/120    avg_loss:0.003, val_acc:0.996]
Epoch [108/120    avg_loss:0.007, val_acc:0.996]
Epoch [109/120    avg_loss:0.003, val_acc:0.996]
Epoch [110/120    avg_loss:0.005, val_acc:0.996]
Epoch [111/120    avg_loss:0.004, val_acc:0.996]
Epoch [112/120    avg_loss:0.005, val_acc:0.996]
Epoch [113/120    avg_loss:0.002, val_acc:0.996]
Epoch [114/120    avg_loss:0.004, val_acc:0.996]
Epoch [115/120    avg_loss:0.003, val_acc:0.996]
Epoch [116/120    avg_loss:0.004, val_acc:0.996]
Epoch [117/120    avg_loss:0.002, val_acc:0.996]
Epoch [118/120    avg_loss:0.003, val_acc:0.996]
Epoch [119/120    avg_loss:0.002, val_acc:0.996]
Epoch [120/120    avg_loss:0.005, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 226   1   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   2  10 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.98648649 0.98689956 0.96521739 0.93992933
 1.         0.9726776  1.         0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.9938275613810652
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff7b99b67b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.862, val_acc:0.653]
Epoch [2/120    avg_loss:1.287, val_acc:0.706]
Epoch [3/120    avg_loss:1.017, val_acc:0.740]
Epoch [4/120    avg_loss:0.942, val_acc:0.770]
Epoch [5/120    avg_loss:0.921, val_acc:0.754]
Epoch [6/120    avg_loss:0.795, val_acc:0.784]
Epoch [7/120    avg_loss:0.867, val_acc:0.835]
Epoch [8/120    avg_loss:0.577, val_acc:0.849]
Epoch [9/120    avg_loss:0.625, val_acc:0.764]
Epoch [10/120    avg_loss:0.685, val_acc:0.839]
Epoch [11/120    avg_loss:0.614, val_acc:0.863]
Epoch [12/120    avg_loss:0.479, val_acc:0.871]
Epoch [13/120    avg_loss:0.501, val_acc:0.883]
Epoch [14/120    avg_loss:0.562, val_acc:0.889]
Epoch [15/120    avg_loss:0.461, val_acc:0.843]
Epoch [16/120    avg_loss:0.400, val_acc:0.885]
Epoch [17/120    avg_loss:0.434, val_acc:0.881]
Epoch [18/120    avg_loss:0.393, val_acc:0.889]
Epoch [19/120    avg_loss:0.455, val_acc:0.921]
Epoch [20/120    avg_loss:0.283, val_acc:0.863]
Epoch [21/120    avg_loss:0.347, val_acc:0.925]
Epoch [22/120    avg_loss:0.351, val_acc:0.952]
Epoch [23/120    avg_loss:0.294, val_acc:0.903]
Epoch [24/120    avg_loss:0.250, val_acc:0.915]
Epoch [25/120    avg_loss:0.308, val_acc:0.883]
Epoch [26/120    avg_loss:0.382, val_acc:0.887]
Epoch [27/120    avg_loss:0.329, val_acc:0.883]
Epoch [28/120    avg_loss:0.338, val_acc:0.913]
Epoch [29/120    avg_loss:0.255, val_acc:0.915]
Epoch [30/120    avg_loss:0.173, val_acc:0.940]
Epoch [31/120    avg_loss:0.241, val_acc:0.937]
Epoch [32/120    avg_loss:0.248, val_acc:0.927]
Epoch [33/120    avg_loss:0.292, val_acc:0.909]
Epoch [34/120    avg_loss:0.221, val_acc:0.921]
Epoch [35/120    avg_loss:0.243, val_acc:0.891]
Epoch [36/120    avg_loss:0.156, val_acc:0.956]
Epoch [37/120    avg_loss:0.136, val_acc:0.964]
Epoch [38/120    avg_loss:0.105, val_acc:0.960]
Epoch [39/120    avg_loss:0.079, val_acc:0.964]
Epoch [40/120    avg_loss:0.084, val_acc:0.962]
Epoch [41/120    avg_loss:0.117, val_acc:0.964]
Epoch [42/120    avg_loss:0.064, val_acc:0.968]
Epoch [43/120    avg_loss:0.098, val_acc:0.970]
Epoch [44/120    avg_loss:0.100, val_acc:0.970]
Epoch [45/120    avg_loss:0.080, val_acc:0.970]
Epoch [46/120    avg_loss:0.063, val_acc:0.970]
Epoch [47/120    avg_loss:0.085, val_acc:0.966]
Epoch [48/120    avg_loss:0.071, val_acc:0.968]
Epoch [49/120    avg_loss:0.072, val_acc:0.968]
Epoch [50/120    avg_loss:0.075, val_acc:0.966]
Epoch [51/120    avg_loss:0.052, val_acc:0.968]
Epoch [52/120    avg_loss:0.062, val_acc:0.968]
Epoch [53/120    avg_loss:0.049, val_acc:0.968]
Epoch [54/120    avg_loss:0.050, val_acc:0.966]
Epoch [55/120    avg_loss:0.077, val_acc:0.966]
Epoch [56/120    avg_loss:0.093, val_acc:0.968]
Epoch [57/120    avg_loss:0.090, val_acc:0.966]
Epoch [58/120    avg_loss:0.057, val_acc:0.968]
Epoch [59/120    avg_loss:0.064, val_acc:0.972]
Epoch [60/120    avg_loss:0.060, val_acc:0.968]
Epoch [61/120    avg_loss:0.104, val_acc:0.970]
Epoch [62/120    avg_loss:0.076, val_acc:0.970]
Epoch [63/120    avg_loss:0.074, val_acc:0.964]
Epoch [64/120    avg_loss:0.076, val_acc:0.968]
Epoch [65/120    avg_loss:0.067, val_acc:0.974]
Epoch [66/120    avg_loss:0.040, val_acc:0.974]
Epoch [67/120    avg_loss:0.044, val_acc:0.974]
Epoch [68/120    avg_loss:0.072, val_acc:0.974]
Epoch [69/120    avg_loss:0.047, val_acc:0.974]
Epoch [70/120    avg_loss:0.050, val_acc:0.976]
Epoch [71/120    avg_loss:0.060, val_acc:0.974]
Epoch [72/120    avg_loss:0.040, val_acc:0.976]
Epoch [73/120    avg_loss:0.059, val_acc:0.972]
Epoch [74/120    avg_loss:0.048, val_acc:0.974]
Epoch [75/120    avg_loss:0.040, val_acc:0.974]
Epoch [76/120    avg_loss:0.046, val_acc:0.978]
Epoch [77/120    avg_loss:0.060, val_acc:0.974]
Epoch [78/120    avg_loss:0.053, val_acc:0.974]
Epoch [79/120    avg_loss:0.040, val_acc:0.976]
Epoch [80/120    avg_loss:0.036, val_acc:0.980]
Epoch [81/120    avg_loss:0.062, val_acc:0.978]
Epoch [82/120    avg_loss:0.058, val_acc:0.980]
Epoch [83/120    avg_loss:0.046, val_acc:0.980]
Epoch [84/120    avg_loss:0.052, val_acc:0.978]
Epoch [85/120    avg_loss:0.025, val_acc:0.978]
Epoch [86/120    avg_loss:0.039, val_acc:0.976]
Epoch [87/120    avg_loss:0.050, val_acc:0.980]
Epoch [88/120    avg_loss:0.036, val_acc:0.978]
Epoch [89/120    avg_loss:0.040, val_acc:0.978]
Epoch [90/120    avg_loss:0.048, val_acc:0.980]
Epoch [91/120    avg_loss:0.053, val_acc:0.982]
Epoch [92/120    avg_loss:0.031, val_acc:0.980]
Epoch [93/120    avg_loss:0.045, val_acc:0.982]
Epoch [94/120    avg_loss:0.035, val_acc:0.982]
Epoch [95/120    avg_loss:0.032, val_acc:0.982]
Epoch [96/120    avg_loss:0.032, val_acc:0.984]
Epoch [97/120    avg_loss:0.034, val_acc:0.984]
Epoch [98/120    avg_loss:0.028, val_acc:0.984]
Epoch [99/120    avg_loss:0.047, val_acc:0.982]
Epoch [100/120    avg_loss:0.064, val_acc:0.976]
Epoch [101/120    avg_loss:0.037, val_acc:0.982]
Epoch [102/120    avg_loss:0.042, val_acc:0.984]
Epoch [103/120    avg_loss:0.036, val_acc:0.982]
Epoch [104/120    avg_loss:0.034, val_acc:0.984]
Epoch [105/120    avg_loss:0.029, val_acc:0.982]
Epoch [106/120    avg_loss:0.043, val_acc:0.986]
Epoch [107/120    avg_loss:0.025, val_acc:0.984]
Epoch [108/120    avg_loss:0.051, val_acc:0.986]
Epoch [109/120    avg_loss:0.061, val_acc:0.984]
Epoch [110/120    avg_loss:0.026, val_acc:0.984]
Epoch [111/120    avg_loss:0.047, val_acc:0.984]
Epoch [112/120    avg_loss:0.034, val_acc:0.978]
Epoch [113/120    avg_loss:0.031, val_acc:0.980]
Epoch [114/120    avg_loss:0.028, val_acc:0.980]
Epoch [115/120    avg_loss:0.021, val_acc:0.978]
Epoch [116/120    avg_loss:0.031, val_acc:0.984]
Epoch [117/120    avg_loss:0.026, val_acc:0.984]
Epoch [118/120    avg_loss:0.032, val_acc:0.982]
Epoch [119/120    avg_loss:0.030, val_acc:0.976]
Epoch [120/120    avg_loss:0.036, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   1   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 209  14   0   0   0   0   0   0   4   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99926954 1.         0.99343545 0.94356659 0.92976589
 1.         1.         0.99742931 1.         1.         0.99603699
 0.99228225 1.        ]

Kappa:
0.9926412520004492
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe047c36780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.824, val_acc:0.532]
Epoch [2/120    avg_loss:1.225, val_acc:0.627]
Epoch [3/120    avg_loss:1.103, val_acc:0.673]
Epoch [4/120    avg_loss:1.043, val_acc:0.688]
Epoch [5/120    avg_loss:0.956, val_acc:0.786]
Epoch [6/120    avg_loss:0.844, val_acc:0.829]
Epoch [7/120    avg_loss:0.723, val_acc:0.821]
Epoch [8/120    avg_loss:0.636, val_acc:0.817]
Epoch [9/120    avg_loss:0.717, val_acc:0.849]
Epoch [10/120    avg_loss:0.681, val_acc:0.841]
Epoch [11/120    avg_loss:0.511, val_acc:0.831]
Epoch [12/120    avg_loss:0.509, val_acc:0.863]
Epoch [13/120    avg_loss:0.451, val_acc:0.833]
Epoch [14/120    avg_loss:0.549, val_acc:0.861]
Epoch [15/120    avg_loss:0.386, val_acc:0.891]
Epoch [16/120    avg_loss:0.390, val_acc:0.879]
Epoch [17/120    avg_loss:0.416, val_acc:0.851]
Epoch [18/120    avg_loss:0.497, val_acc:0.855]
Epoch [19/120    avg_loss:0.440, val_acc:0.871]
Epoch [20/120    avg_loss:0.375, val_acc:0.863]
Epoch [21/120    avg_loss:0.429, val_acc:0.887]
Epoch [22/120    avg_loss:0.398, val_acc:0.901]
Epoch [23/120    avg_loss:0.362, val_acc:0.899]
Epoch [24/120    avg_loss:0.298, val_acc:0.905]
Epoch [25/120    avg_loss:0.276, val_acc:0.913]
Epoch [26/120    avg_loss:0.292, val_acc:0.885]
Epoch [27/120    avg_loss:0.331, val_acc:0.907]
Epoch [28/120    avg_loss:0.220, val_acc:0.889]
Epoch [29/120    avg_loss:0.284, val_acc:0.885]
Epoch [30/120    avg_loss:0.253, val_acc:0.911]
Epoch [31/120    avg_loss:0.239, val_acc:0.931]
Epoch [32/120    avg_loss:0.176, val_acc:0.933]
Epoch [33/120    avg_loss:0.179, val_acc:0.909]
Epoch [34/120    avg_loss:0.149, val_acc:0.921]
Epoch [35/120    avg_loss:0.189, val_acc:0.927]
Epoch [36/120    avg_loss:0.114, val_acc:0.919]
Epoch [37/120    avg_loss:0.174, val_acc:0.903]
Epoch [38/120    avg_loss:0.265, val_acc:0.925]
Epoch [39/120    avg_loss:0.200, val_acc:0.948]
Epoch [40/120    avg_loss:0.185, val_acc:0.944]
Epoch [41/120    avg_loss:0.123, val_acc:0.940]
Epoch [42/120    avg_loss:0.100, val_acc:0.946]
Epoch [43/120    avg_loss:0.102, val_acc:0.952]
Epoch [44/120    avg_loss:0.171, val_acc:0.927]
Epoch [45/120    avg_loss:0.155, val_acc:0.925]
Epoch [46/120    avg_loss:0.147, val_acc:0.879]
Epoch [47/120    avg_loss:0.173, val_acc:0.879]
Epoch [48/120    avg_loss:0.176, val_acc:0.927]
Epoch [49/120    avg_loss:0.167, val_acc:0.940]
Epoch [50/120    avg_loss:0.103, val_acc:0.958]
Epoch [51/120    avg_loss:0.090, val_acc:0.970]
Epoch [52/120    avg_loss:0.085, val_acc:0.962]
Epoch [53/120    avg_loss:0.041, val_acc:0.972]
Epoch [54/120    avg_loss:0.081, val_acc:0.950]
Epoch [55/120    avg_loss:0.106, val_acc:0.960]
Epoch [56/120    avg_loss:0.072, val_acc:0.964]
Epoch [57/120    avg_loss:0.046, val_acc:0.958]
Epoch [58/120    avg_loss:0.065, val_acc:0.968]
Epoch [59/120    avg_loss:0.090, val_acc:0.940]
Epoch [60/120    avg_loss:0.080, val_acc:0.974]
Epoch [61/120    avg_loss:0.094, val_acc:0.946]
Epoch [62/120    avg_loss:0.146, val_acc:0.968]
Epoch [63/120    avg_loss:0.063, val_acc:0.972]
Epoch [64/120    avg_loss:0.055, val_acc:0.962]
Epoch [65/120    avg_loss:0.097, val_acc:0.972]
Epoch [66/120    avg_loss:0.043, val_acc:0.974]
Epoch [67/120    avg_loss:0.074, val_acc:0.966]
Epoch [68/120    avg_loss:0.057, val_acc:0.974]
Epoch [69/120    avg_loss:0.040, val_acc:0.978]
Epoch [70/120    avg_loss:0.045, val_acc:0.970]
Epoch [71/120    avg_loss:0.103, val_acc:0.944]
Epoch [72/120    avg_loss:0.071, val_acc:0.970]
Epoch [73/120    avg_loss:0.045, val_acc:0.964]
Epoch [74/120    avg_loss:0.062, val_acc:0.962]
Epoch [75/120    avg_loss:0.031, val_acc:0.986]
Epoch [76/120    avg_loss:0.041, val_acc:0.986]
Epoch [77/120    avg_loss:0.069, val_acc:0.954]
Epoch [78/120    avg_loss:0.103, val_acc:0.940]
Epoch [79/120    avg_loss:0.073, val_acc:0.972]
Epoch [80/120    avg_loss:0.054, val_acc:0.980]
Epoch [81/120    avg_loss:0.024, val_acc:0.978]
Epoch [82/120    avg_loss:0.017, val_acc:0.978]
Epoch [83/120    avg_loss:0.032, val_acc:0.980]
Epoch [84/120    avg_loss:0.048, val_acc:0.962]
Epoch [85/120    avg_loss:0.038, val_acc:0.974]
Epoch [86/120    avg_loss:0.035, val_acc:0.956]
Epoch [87/120    avg_loss:0.035, val_acc:0.976]
Epoch [88/120    avg_loss:0.039, val_acc:0.966]
Epoch [89/120    avg_loss:0.027, val_acc:0.978]
Epoch [90/120    avg_loss:0.025, val_acc:0.974]
Epoch [91/120    avg_loss:0.011, val_acc:0.976]
Epoch [92/120    avg_loss:0.016, val_acc:0.980]
Epoch [93/120    avg_loss:0.024, val_acc:0.970]
Epoch [94/120    avg_loss:0.010, val_acc:0.978]
Epoch [95/120    avg_loss:0.009, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.978]
Epoch [97/120    avg_loss:0.011, val_acc:0.978]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.029, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.015, val_acc:0.978]
Epoch [103/120    avg_loss:0.021, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.006, val_acc:0.978]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.022, val_acc:0.978]
Epoch [108/120    avg_loss:0.024, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.980]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.019, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.019, val_acc:0.980]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.016, val_acc:0.980]
Epoch [117/120    avg_loss:0.008, val_acc:0.980]
Epoch [118/120    avg_loss:0.009, val_acc:0.980]
Epoch [119/120    avg_loss:0.015, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  12   0   0   0   0   0   0   5   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         1.         0.99782135 0.92920354 0.90972222
 1.         1.         1.         1.         1.         0.9973545
 0.99229923 1.        ]

Kappa:
0.991928565220721
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6042d83748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.832, val_acc:0.552]
Epoch [2/120    avg_loss:1.279, val_acc:0.724]
Epoch [3/120    avg_loss:1.133, val_acc:0.690]
Epoch [4/120    avg_loss:1.075, val_acc:0.724]
Epoch [5/120    avg_loss:0.896, val_acc:0.810]
Epoch [6/120    avg_loss:0.679, val_acc:0.827]
Epoch [7/120    avg_loss:0.790, val_acc:0.750]
Epoch [8/120    avg_loss:0.672, val_acc:0.851]
Epoch [9/120    avg_loss:0.619, val_acc:0.788]
Epoch [10/120    avg_loss:0.612, val_acc:0.859]
Epoch [11/120    avg_loss:0.548, val_acc:0.837]
Epoch [12/120    avg_loss:0.524, val_acc:0.881]
Epoch [13/120    avg_loss:0.486, val_acc:0.885]
Epoch [14/120    avg_loss:0.412, val_acc:0.875]
Epoch [15/120    avg_loss:0.465, val_acc:0.867]
Epoch [16/120    avg_loss:0.354, val_acc:0.903]
Epoch [17/120    avg_loss:0.392, val_acc:0.877]
Epoch [18/120    avg_loss:0.544, val_acc:0.887]
Epoch [19/120    avg_loss:0.536, val_acc:0.931]
Epoch [20/120    avg_loss:0.396, val_acc:0.899]
Epoch [21/120    avg_loss:0.364, val_acc:0.895]
Epoch [22/120    avg_loss:0.341, val_acc:0.905]
Epoch [23/120    avg_loss:0.260, val_acc:0.929]
Epoch [24/120    avg_loss:0.278, val_acc:0.897]
Epoch [25/120    avg_loss:0.256, val_acc:0.925]
Epoch [26/120    avg_loss:0.302, val_acc:0.921]
Epoch [27/120    avg_loss:0.238, val_acc:0.901]
Epoch [28/120    avg_loss:0.305, val_acc:0.897]
Epoch [29/120    avg_loss:0.239, val_acc:0.895]
Epoch [30/120    avg_loss:0.318, val_acc:0.915]
Epoch [31/120    avg_loss:0.314, val_acc:0.899]
Epoch [32/120    avg_loss:0.357, val_acc:0.927]
Epoch [33/120    avg_loss:0.201, val_acc:0.948]
Epoch [34/120    avg_loss:0.165, val_acc:0.946]
Epoch [35/120    avg_loss:0.146, val_acc:0.946]
Epoch [36/120    avg_loss:0.118, val_acc:0.946]
Epoch [37/120    avg_loss:0.098, val_acc:0.948]
Epoch [38/120    avg_loss:0.096, val_acc:0.948]
Epoch [39/120    avg_loss:0.130, val_acc:0.948]
Epoch [40/120    avg_loss:0.107, val_acc:0.946]
Epoch [41/120    avg_loss:0.091, val_acc:0.950]
Epoch [42/120    avg_loss:0.096, val_acc:0.948]
Epoch [43/120    avg_loss:0.090, val_acc:0.948]
Epoch [44/120    avg_loss:0.087, val_acc:0.950]
Epoch [45/120    avg_loss:0.104, val_acc:0.954]
Epoch [46/120    avg_loss:0.085, val_acc:0.954]
Epoch [47/120    avg_loss:0.079, val_acc:0.954]
Epoch [48/120    avg_loss:0.083, val_acc:0.954]
Epoch [49/120    avg_loss:0.095, val_acc:0.956]
Epoch [50/120    avg_loss:0.056, val_acc:0.954]
Epoch [51/120    avg_loss:0.078, val_acc:0.952]
Epoch [52/120    avg_loss:0.057, val_acc:0.952]
Epoch [53/120    avg_loss:0.071, val_acc:0.950]
Epoch [54/120    avg_loss:0.088, val_acc:0.956]
Epoch [55/120    avg_loss:0.080, val_acc:0.960]
Epoch [56/120    avg_loss:0.083, val_acc:0.958]
Epoch [57/120    avg_loss:0.061, val_acc:0.960]
Epoch [58/120    avg_loss:0.088, val_acc:0.956]
Epoch [59/120    avg_loss:0.076, val_acc:0.956]
Epoch [60/120    avg_loss:0.089, val_acc:0.964]
Epoch [61/120    avg_loss:0.059, val_acc:0.958]
Epoch [62/120    avg_loss:0.067, val_acc:0.958]
Epoch [63/120    avg_loss:0.061, val_acc:0.958]
Epoch [64/120    avg_loss:0.076, val_acc:0.962]
Epoch [65/120    avg_loss:0.081, val_acc:0.958]
Epoch [66/120    avg_loss:0.070, val_acc:0.962]
Epoch [67/120    avg_loss:0.055, val_acc:0.958]
Epoch [68/120    avg_loss:0.068, val_acc:0.962]
Epoch [69/120    avg_loss:0.055, val_acc:0.966]
Epoch [70/120    avg_loss:0.049, val_acc:0.968]
Epoch [71/120    avg_loss:0.056, val_acc:0.964]
Epoch [72/120    avg_loss:0.049, val_acc:0.964]
Epoch [73/120    avg_loss:0.048, val_acc:0.968]
Epoch [74/120    avg_loss:0.054, val_acc:0.970]
Epoch [75/120    avg_loss:0.052, val_acc:0.966]
Epoch [76/120    avg_loss:0.065, val_acc:0.966]
Epoch [77/120    avg_loss:0.045, val_acc:0.964]
Epoch [78/120    avg_loss:0.051, val_acc:0.964]
Epoch [79/120    avg_loss:0.062, val_acc:0.964]
Epoch [80/120    avg_loss:0.073, val_acc:0.968]
Epoch [81/120    avg_loss:0.051, val_acc:0.968]
Epoch [82/120    avg_loss:0.044, val_acc:0.968]
Epoch [83/120    avg_loss:0.077, val_acc:0.966]
Epoch [84/120    avg_loss:0.059, val_acc:0.968]
Epoch [85/120    avg_loss:0.037, val_acc:0.972]
Epoch [86/120    avg_loss:0.064, val_acc:0.972]
Epoch [87/120    avg_loss:0.064, val_acc:0.970]
Epoch [88/120    avg_loss:0.045, val_acc:0.966]
Epoch [89/120    avg_loss:0.052, val_acc:0.968]
Epoch [90/120    avg_loss:0.044, val_acc:0.970]
Epoch [91/120    avg_loss:0.041, val_acc:0.972]
Epoch [92/120    avg_loss:0.043, val_acc:0.974]
Epoch [93/120    avg_loss:0.065, val_acc:0.972]
Epoch [94/120    avg_loss:0.037, val_acc:0.970]
Epoch [95/120    avg_loss:0.049, val_acc:0.972]
Epoch [96/120    avg_loss:0.037, val_acc:0.972]
Epoch [97/120    avg_loss:0.056, val_acc:0.974]
Epoch [98/120    avg_loss:0.064, val_acc:0.970]
Epoch [99/120    avg_loss:0.044, val_acc:0.972]
Epoch [100/120    avg_loss:0.066, val_acc:0.974]
Epoch [101/120    avg_loss:0.051, val_acc:0.974]
Epoch [102/120    avg_loss:0.050, val_acc:0.974]
Epoch [103/120    avg_loss:0.045, val_acc:0.976]
Epoch [104/120    avg_loss:0.045, val_acc:0.976]
Epoch [105/120    avg_loss:0.037, val_acc:0.976]
Epoch [106/120    avg_loss:0.036, val_acc:0.974]
Epoch [107/120    avg_loss:0.040, val_acc:0.976]
Epoch [108/120    avg_loss:0.056, val_acc:0.974]
Epoch [109/120    avg_loss:0.038, val_acc:0.974]
Epoch [110/120    avg_loss:0.033, val_acc:0.976]
Epoch [111/120    avg_loss:0.054, val_acc:0.976]
Epoch [112/120    avg_loss:0.038, val_acc:0.976]
Epoch [113/120    avg_loss:0.037, val_acc:0.976]
Epoch [114/120    avg_loss:0.027, val_acc:0.974]
Epoch [115/120    avg_loss:0.029, val_acc:0.974]
Epoch [116/120    avg_loss:0.041, val_acc:0.972]
Epoch [117/120    avg_loss:0.054, val_acc:0.972]
Epoch [118/120    avg_loss:0.037, val_acc:0.972]
Epoch [119/120    avg_loss:0.040, val_acc:0.972]
Epoch [120/120    avg_loss:0.035, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 199  29   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 211  11   0   0   0   0   0   0   5   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 203   3   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 1.         0.99771167 0.92773893 0.88469602 0.92783505
 0.99266504 0.98429319 0.99742931 1.         1.         0.9973545
 0.99120879 1.        ]

Kappa:
0.985044193958891
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf08b37710>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.788, val_acc:0.536]
Epoch [2/120    avg_loss:1.306, val_acc:0.641]
Epoch [3/120    avg_loss:1.007, val_acc:0.732]
Epoch [4/120    avg_loss:0.964, val_acc:0.728]
Epoch [5/120    avg_loss:0.919, val_acc:0.810]
Epoch [6/120    avg_loss:0.792, val_acc:0.760]
Epoch [7/120    avg_loss:0.770, val_acc:0.829]
Epoch [8/120    avg_loss:0.546, val_acc:0.819]
Epoch [9/120    avg_loss:0.658, val_acc:0.796]
Epoch [10/120    avg_loss:0.585, val_acc:0.843]
Epoch [11/120    avg_loss:0.642, val_acc:0.774]
Epoch [12/120    avg_loss:0.556, val_acc:0.837]
Epoch [13/120    avg_loss:0.524, val_acc:0.849]
Epoch [14/120    avg_loss:0.505, val_acc:0.827]
Epoch [15/120    avg_loss:0.478, val_acc:0.879]
Epoch [16/120    avg_loss:0.489, val_acc:0.851]
Epoch [17/120    avg_loss:0.370, val_acc:0.873]
Epoch [18/120    avg_loss:0.396, val_acc:0.851]
Epoch [19/120    avg_loss:0.419, val_acc:0.897]
Epoch [20/120    avg_loss:0.426, val_acc:0.879]
Epoch [21/120    avg_loss:0.401, val_acc:0.837]
Epoch [22/120    avg_loss:0.355, val_acc:0.907]
Epoch [23/120    avg_loss:0.315, val_acc:0.909]
Epoch [24/120    avg_loss:0.287, val_acc:0.905]
Epoch [25/120    avg_loss:0.235, val_acc:0.903]
Epoch [26/120    avg_loss:0.284, val_acc:0.905]
Epoch [27/120    avg_loss:0.349, val_acc:0.911]
Epoch [28/120    avg_loss:0.221, val_acc:0.933]
Epoch [29/120    avg_loss:0.164, val_acc:0.948]
Epoch [30/120    avg_loss:0.221, val_acc:0.911]
Epoch [31/120    avg_loss:0.209, val_acc:0.935]
Epoch [32/120    avg_loss:0.150, val_acc:0.923]
Epoch [33/120    avg_loss:0.148, val_acc:0.931]
Epoch [34/120    avg_loss:0.161, val_acc:0.950]
Epoch [35/120    avg_loss:0.150, val_acc:0.942]
Epoch [36/120    avg_loss:0.139, val_acc:0.946]
Epoch [37/120    avg_loss:0.115, val_acc:0.942]
Epoch [38/120    avg_loss:0.128, val_acc:0.923]
Epoch [39/120    avg_loss:0.172, val_acc:0.925]
Epoch [40/120    avg_loss:0.184, val_acc:0.917]
Epoch [41/120    avg_loss:0.124, val_acc:0.942]
Epoch [42/120    avg_loss:0.185, val_acc:0.942]
Epoch [43/120    avg_loss:0.095, val_acc:0.952]
Epoch [44/120    avg_loss:0.131, val_acc:0.956]
Epoch [45/120    avg_loss:0.115, val_acc:0.946]
Epoch [46/120    avg_loss:0.083, val_acc:0.944]
Epoch [47/120    avg_loss:0.109, val_acc:0.952]
Epoch [48/120    avg_loss:0.073, val_acc:0.964]
Epoch [49/120    avg_loss:0.071, val_acc:0.968]
Epoch [50/120    avg_loss:0.107, val_acc:0.899]
Epoch [51/120    avg_loss:0.105, val_acc:0.956]
Epoch [52/120    avg_loss:0.088, val_acc:0.952]
Epoch [53/120    avg_loss:0.108, val_acc:0.954]
Epoch [54/120    avg_loss:0.071, val_acc:0.974]
Epoch [55/120    avg_loss:0.086, val_acc:0.946]
Epoch [56/120    avg_loss:0.130, val_acc:0.956]
Epoch [57/120    avg_loss:0.090, val_acc:0.964]
Epoch [58/120    avg_loss:0.059, val_acc:0.956]
Epoch [59/120    avg_loss:0.041, val_acc:0.962]
Epoch [60/120    avg_loss:0.041, val_acc:0.966]
Epoch [61/120    avg_loss:0.045, val_acc:0.942]
Epoch [62/120    avg_loss:0.039, val_acc:0.970]
Epoch [63/120    avg_loss:0.038, val_acc:0.964]
Epoch [64/120    avg_loss:0.067, val_acc:0.966]
Epoch [65/120    avg_loss:0.023, val_acc:0.966]
Epoch [66/120    avg_loss:0.050, val_acc:0.974]
Epoch [67/120    avg_loss:0.041, val_acc:0.972]
Epoch [68/120    avg_loss:0.055, val_acc:0.970]
Epoch [69/120    avg_loss:0.053, val_acc:0.964]
Epoch [70/120    avg_loss:0.058, val_acc:0.966]
Epoch [71/120    avg_loss:0.035, val_acc:0.972]
Epoch [72/120    avg_loss:0.075, val_acc:0.952]
Epoch [73/120    avg_loss:0.053, val_acc:0.964]
Epoch [74/120    avg_loss:0.048, val_acc:0.944]
Epoch [75/120    avg_loss:0.054, val_acc:0.954]
Epoch [76/120    avg_loss:0.053, val_acc:0.960]
Epoch [77/120    avg_loss:0.040, val_acc:0.956]
Epoch [78/120    avg_loss:0.033, val_acc:0.966]
Epoch [79/120    avg_loss:0.035, val_acc:0.966]
Epoch [80/120    avg_loss:0.048, val_acc:0.970]
Epoch [81/120    avg_loss:0.010, val_acc:0.972]
Epoch [82/120    avg_loss:0.019, val_acc:0.972]
Epoch [83/120    avg_loss:0.011, val_acc:0.972]
Epoch [84/120    avg_loss:0.012, val_acc:0.974]
Epoch [85/120    avg_loss:0.015, val_acc:0.976]
Epoch [86/120    avg_loss:0.015, val_acc:0.976]
Epoch [87/120    avg_loss:0.012, val_acc:0.976]
Epoch [88/120    avg_loss:0.012, val_acc:0.978]
Epoch [89/120    avg_loss:0.011, val_acc:0.974]
Epoch [90/120    avg_loss:0.012, val_acc:0.976]
Epoch [91/120    avg_loss:0.025, val_acc:0.978]
Epoch [92/120    avg_loss:0.027, val_acc:0.978]
Epoch [93/120    avg_loss:0.015, val_acc:0.982]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.017, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.012, val_acc:0.982]
Epoch [103/120    avg_loss:0.007, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.026, val_acc:0.982]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.018, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.982]
Epoch [112/120    avg_loss:0.005, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   2   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 209  21   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 216   5   0   0   0   0   0   0   5   0]
 [  0   0   0   3   6 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99853801 1.         0.94356659 0.91914894 0.94444444
 1.         1.         1.         1.         1.         0.99341238
 0.98896247 1.        ]

Kappa:
0.988605720735994
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3485617b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.827, val_acc:0.645]
Epoch [2/120    avg_loss:1.314, val_acc:0.671]
Epoch [3/120    avg_loss:1.093, val_acc:0.649]
Epoch [4/120    avg_loss:0.899, val_acc:0.762]
Epoch [5/120    avg_loss:0.904, val_acc:0.760]
Epoch [6/120    avg_loss:0.885, val_acc:0.786]
Epoch [7/120    avg_loss:0.681, val_acc:0.806]
Epoch [8/120    avg_loss:0.565, val_acc:0.835]
Epoch [9/120    avg_loss:0.597, val_acc:0.843]
Epoch [10/120    avg_loss:0.622, val_acc:0.879]
Epoch [11/120    avg_loss:0.601, val_acc:0.863]
Epoch [12/120    avg_loss:0.573, val_acc:0.839]
Epoch [13/120    avg_loss:0.515, val_acc:0.883]
Epoch [14/120    avg_loss:0.421, val_acc:0.883]
Epoch [15/120    avg_loss:0.418, val_acc:0.905]
Epoch [16/120    avg_loss:0.477, val_acc:0.861]
Epoch [17/120    avg_loss:0.431, val_acc:0.909]
Epoch [18/120    avg_loss:0.424, val_acc:0.901]
Epoch [19/120    avg_loss:0.443, val_acc:0.909]
Epoch [20/120    avg_loss:0.312, val_acc:0.913]
Epoch [21/120    avg_loss:0.321, val_acc:0.909]
Epoch [22/120    avg_loss:0.371, val_acc:0.893]
Epoch [23/120    avg_loss:0.249, val_acc:0.911]
Epoch [24/120    avg_loss:0.315, val_acc:0.861]
Epoch [25/120    avg_loss:0.340, val_acc:0.937]
Epoch [26/120    avg_loss:0.200, val_acc:0.923]
Epoch [27/120    avg_loss:0.278, val_acc:0.923]
Epoch [28/120    avg_loss:0.274, val_acc:0.915]
Epoch [29/120    avg_loss:0.273, val_acc:0.921]
Epoch [30/120    avg_loss:0.156, val_acc:0.937]
Epoch [31/120    avg_loss:0.308, val_acc:0.915]
Epoch [32/120    avg_loss:0.202, val_acc:0.938]
Epoch [33/120    avg_loss:0.153, val_acc:0.940]
Epoch [34/120    avg_loss:0.168, val_acc:0.944]
Epoch [35/120    avg_loss:0.220, val_acc:0.952]
Epoch [36/120    avg_loss:0.246, val_acc:0.938]
Epoch [37/120    avg_loss:0.210, val_acc:0.909]
Epoch [38/120    avg_loss:0.129, val_acc:0.952]
Epoch [39/120    avg_loss:0.156, val_acc:0.935]
Epoch [40/120    avg_loss:0.165, val_acc:0.907]
Epoch [41/120    avg_loss:0.188, val_acc:0.944]
Epoch [42/120    avg_loss:0.089, val_acc:0.960]
Epoch [43/120    avg_loss:0.081, val_acc:0.958]
Epoch [44/120    avg_loss:0.139, val_acc:0.962]
Epoch [45/120    avg_loss:0.111, val_acc:0.946]
Epoch [46/120    avg_loss:0.124, val_acc:0.960]
Epoch [47/120    avg_loss:0.107, val_acc:0.970]
Epoch [48/120    avg_loss:0.106, val_acc:0.946]
Epoch [49/120    avg_loss:0.135, val_acc:0.946]
Epoch [50/120    avg_loss:0.109, val_acc:0.956]
Epoch [51/120    avg_loss:0.109, val_acc:0.958]
Epoch [52/120    avg_loss:0.080, val_acc:0.968]
Epoch [53/120    avg_loss:0.064, val_acc:0.972]
Epoch [54/120    avg_loss:0.039, val_acc:0.976]
Epoch [55/120    avg_loss:0.053, val_acc:0.966]
Epoch [56/120    avg_loss:0.066, val_acc:0.982]
Epoch [57/120    avg_loss:0.032, val_acc:0.986]
Epoch [58/120    avg_loss:0.043, val_acc:0.980]
Epoch [59/120    avg_loss:0.032, val_acc:0.986]
Epoch [60/120    avg_loss:0.078, val_acc:0.956]
Epoch [61/120    avg_loss:0.071, val_acc:0.980]
Epoch [62/120    avg_loss:0.046, val_acc:0.986]
Epoch [63/120    avg_loss:0.043, val_acc:0.982]
Epoch [64/120    avg_loss:0.050, val_acc:0.980]
Epoch [65/120    avg_loss:0.076, val_acc:0.964]
Epoch [66/120    avg_loss:0.104, val_acc:0.962]
Epoch [67/120    avg_loss:0.099, val_acc:0.976]
Epoch [68/120    avg_loss:0.064, val_acc:0.970]
Epoch [69/120    avg_loss:0.035, val_acc:0.966]
Epoch [70/120    avg_loss:0.034, val_acc:0.972]
Epoch [71/120    avg_loss:0.024, val_acc:0.980]
Epoch [72/120    avg_loss:0.053, val_acc:0.948]
Epoch [73/120    avg_loss:0.086, val_acc:0.950]
Epoch [74/120    avg_loss:0.075, val_acc:0.984]
Epoch [75/120    avg_loss:0.049, val_acc:0.970]
Epoch [76/120    avg_loss:0.032, val_acc:0.980]
Epoch [77/120    avg_loss:0.022, val_acc:0.980]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.019, val_acc:0.984]
Epoch [80/120    avg_loss:0.017, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.016, val_acc:0.986]
Epoch [83/120    avg_loss:0.020, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.024, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.018, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.018, val_acc:0.988]
Epoch [100/120    avg_loss:0.022, val_acc:0.990]
Epoch [101/120    avg_loss:0.015, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.010, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.990]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.009, val_acc:0.990]
Epoch [110/120    avg_loss:0.014, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.990]
Epoch [113/120    avg_loss:0.017, val_acc:0.988]
Epoch [114/120    avg_loss:0.013, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 1.         1.         1.         0.95404814 0.92682927
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9950149273796041
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c7adfa780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.699, val_acc:0.663]
Epoch [2/120    avg_loss:1.258, val_acc:0.708]
Epoch [3/120    avg_loss:1.131, val_acc:0.690]
Epoch [4/120    avg_loss:0.978, val_acc:0.778]
Epoch [5/120    avg_loss:0.858, val_acc:0.800]
Epoch [6/120    avg_loss:0.754, val_acc:0.835]
Epoch [7/120    avg_loss:0.752, val_acc:0.835]
Epoch [8/120    avg_loss:0.690, val_acc:0.815]
Epoch [9/120    avg_loss:0.689, val_acc:0.782]
Epoch [10/120    avg_loss:0.647, val_acc:0.819]
Epoch [11/120    avg_loss:0.562, val_acc:0.869]
Epoch [12/120    avg_loss:0.464, val_acc:0.881]
Epoch [13/120    avg_loss:0.490, val_acc:0.839]
Epoch [14/120    avg_loss:0.512, val_acc:0.823]
Epoch [15/120    avg_loss:0.580, val_acc:0.843]
Epoch [16/120    avg_loss:0.464, val_acc:0.891]
Epoch [17/120    avg_loss:0.395, val_acc:0.891]
Epoch [18/120    avg_loss:0.408, val_acc:0.905]
Epoch [19/120    avg_loss:0.286, val_acc:0.907]
Epoch [20/120    avg_loss:0.405, val_acc:0.901]
Epoch [21/120    avg_loss:0.383, val_acc:0.905]
Epoch [22/120    avg_loss:0.284, val_acc:0.925]
Epoch [23/120    avg_loss:0.276, val_acc:0.903]
Epoch [24/120    avg_loss:0.363, val_acc:0.891]
Epoch [25/120    avg_loss:0.281, val_acc:0.929]
Epoch [26/120    avg_loss:0.276, val_acc:0.942]
Epoch [27/120    avg_loss:0.177, val_acc:0.946]
Epoch [28/120    avg_loss:0.193, val_acc:0.917]
Epoch [29/120    avg_loss:0.256, val_acc:0.931]
Epoch [30/120    avg_loss:0.209, val_acc:0.946]
Epoch [31/120    avg_loss:0.161, val_acc:0.956]
Epoch [32/120    avg_loss:0.185, val_acc:0.952]
Epoch [33/120    avg_loss:0.257, val_acc:0.958]
Epoch [34/120    avg_loss:0.188, val_acc:0.966]
Epoch [35/120    avg_loss:0.112, val_acc:0.954]
Epoch [36/120    avg_loss:0.119, val_acc:0.923]
Epoch [37/120    avg_loss:0.123, val_acc:0.952]
Epoch [38/120    avg_loss:0.144, val_acc:0.960]
Epoch [39/120    avg_loss:0.178, val_acc:0.938]
Epoch [40/120    avg_loss:0.173, val_acc:0.956]
Epoch [41/120    avg_loss:0.119, val_acc:0.944]
Epoch [42/120    avg_loss:0.081, val_acc:0.962]
Epoch [43/120    avg_loss:0.141, val_acc:0.938]
Epoch [44/120    avg_loss:0.205, val_acc:0.942]
Epoch [45/120    avg_loss:0.124, val_acc:0.966]
Epoch [46/120    avg_loss:0.210, val_acc:0.944]
Epoch [47/120    avg_loss:0.204, val_acc:0.952]
Epoch [48/120    avg_loss:0.146, val_acc:0.960]
Epoch [49/120    avg_loss:0.148, val_acc:0.980]
Epoch [50/120    avg_loss:0.091, val_acc:0.978]
Epoch [51/120    avg_loss:0.070, val_acc:0.984]
Epoch [52/120    avg_loss:0.060, val_acc:0.966]
Epoch [53/120    avg_loss:0.147, val_acc:0.974]
Epoch [54/120    avg_loss:0.050, val_acc:0.982]
Epoch [55/120    avg_loss:0.071, val_acc:0.974]
Epoch [56/120    avg_loss:0.053, val_acc:0.980]
Epoch [57/120    avg_loss:0.168, val_acc:0.970]
Epoch [58/120    avg_loss:0.138, val_acc:0.984]
Epoch [59/120    avg_loss:0.130, val_acc:0.970]
Epoch [60/120    avg_loss:0.138, val_acc:0.976]
Epoch [61/120    avg_loss:0.058, val_acc:0.974]
Epoch [62/120    avg_loss:0.045, val_acc:0.986]
Epoch [63/120    avg_loss:0.022, val_acc:0.962]
Epoch [64/120    avg_loss:0.087, val_acc:0.976]
Epoch [65/120    avg_loss:0.060, val_acc:0.972]
Epoch [66/120    avg_loss:0.055, val_acc:0.986]
Epoch [67/120    avg_loss:0.034, val_acc:0.990]
Epoch [68/120    avg_loss:0.024, val_acc:0.992]
Epoch [69/120    avg_loss:0.036, val_acc:0.984]
Epoch [70/120    avg_loss:0.079, val_acc:0.990]
Epoch [71/120    avg_loss:0.023, val_acc:0.990]
Epoch [72/120    avg_loss:0.027, val_acc:0.990]
Epoch [73/120    avg_loss:0.039, val_acc:0.990]
Epoch [74/120    avg_loss:0.028, val_acc:0.990]
Epoch [75/120    avg_loss:0.033, val_acc:0.992]
Epoch [76/120    avg_loss:0.023, val_acc:0.986]
Epoch [77/120    avg_loss:0.015, val_acc:0.992]
Epoch [78/120    avg_loss:0.010, val_acc:0.992]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.016, val_acc:0.994]
Epoch [81/120    avg_loss:0.015, val_acc:0.990]
Epoch [82/120    avg_loss:0.034, val_acc:0.994]
Epoch [83/120    avg_loss:0.016, val_acc:0.994]
Epoch [84/120    avg_loss:0.027, val_acc:0.990]
Epoch [85/120    avg_loss:0.020, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.994]
Epoch [87/120    avg_loss:0.008, val_acc:0.992]
Epoch [88/120    avg_loss:0.025, val_acc:0.994]
Epoch [89/120    avg_loss:0.010, val_acc:0.994]
Epoch [90/120    avg_loss:0.029, val_acc:0.988]
Epoch [91/120    avg_loss:0.030, val_acc:0.986]
Epoch [92/120    avg_loss:0.023, val_acc:0.996]
Epoch [93/120    avg_loss:0.006, val_acc:0.994]
Epoch [94/120    avg_loss:0.004, val_acc:0.996]
Epoch [95/120    avg_loss:0.012, val_acc:0.996]
Epoch [96/120    avg_loss:0.007, val_acc:0.994]
Epoch [97/120    avg_loss:0.007, val_acc:0.996]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.014, val_acc:0.996]
Epoch [100/120    avg_loss:0.012, val_acc:0.994]
Epoch [101/120    avg_loss:0.011, val_acc:0.992]
Epoch [102/120    avg_loss:0.016, val_acc:0.994]
Epoch [103/120    avg_loss:0.004, val_acc:0.994]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.019, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.994]
Epoch [107/120    avg_loss:0.005, val_acc:0.994]
Epoch [108/120    avg_loss:0.006, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.002, val_acc:0.994]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.005, val_acc:0.994]
Epoch [115/120    avg_loss:0.004, val_acc:0.994]
Epoch [116/120    avg_loss:0.002, val_acc:0.994]
Epoch [117/120    avg_loss:0.004, val_acc:0.994]
Epoch [118/120    avg_loss:0.003, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 222   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   1  11 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99926954 0.9977221  0.98013245 0.93939394 0.92041522
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.992641325745718
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f349d7e77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.819, val_acc:0.635]
Epoch [2/120    avg_loss:1.370, val_acc:0.698]
Epoch [3/120    avg_loss:1.043, val_acc:0.772]
Epoch [4/120    avg_loss:0.860, val_acc:0.750]
Epoch [5/120    avg_loss:0.834, val_acc:0.768]
Epoch [6/120    avg_loss:0.772, val_acc:0.760]
Epoch [7/120    avg_loss:0.763, val_acc:0.817]
Epoch [8/120    avg_loss:0.679, val_acc:0.865]
Epoch [9/120    avg_loss:0.623, val_acc:0.851]
Epoch [10/120    avg_loss:0.534, val_acc:0.863]
Epoch [11/120    avg_loss:0.475, val_acc:0.855]
Epoch [12/120    avg_loss:0.450, val_acc:0.879]
Epoch [13/120    avg_loss:0.438, val_acc:0.827]
Epoch [14/120    avg_loss:0.530, val_acc:0.881]
Epoch [15/120    avg_loss:0.564, val_acc:0.883]
Epoch [16/120    avg_loss:0.427, val_acc:0.923]
Epoch [17/120    avg_loss:0.390, val_acc:0.915]
Epoch [18/120    avg_loss:0.272, val_acc:0.895]
Epoch [19/120    avg_loss:0.426, val_acc:0.873]
Epoch [20/120    avg_loss:0.353, val_acc:0.909]
Epoch [21/120    avg_loss:0.296, val_acc:0.915]
Epoch [22/120    avg_loss:0.295, val_acc:0.901]
Epoch [23/120    avg_loss:0.286, val_acc:0.919]
Epoch [24/120    avg_loss:0.216, val_acc:0.893]
Epoch [25/120    avg_loss:0.288, val_acc:0.901]
Epoch [26/120    avg_loss:0.259, val_acc:0.917]
Epoch [27/120    avg_loss:0.263, val_acc:0.929]
Epoch [28/120    avg_loss:0.130, val_acc:0.931]
Epoch [29/120    avg_loss:0.145, val_acc:0.960]
Epoch [30/120    avg_loss:0.148, val_acc:0.927]
Epoch [31/120    avg_loss:0.163, val_acc:0.956]
Epoch [32/120    avg_loss:0.174, val_acc:0.938]
Epoch [33/120    avg_loss:0.163, val_acc:0.938]
Epoch [34/120    avg_loss:0.214, val_acc:0.925]
Epoch [35/120    avg_loss:0.202, val_acc:0.913]
Epoch [36/120    avg_loss:0.177, val_acc:0.931]
Epoch [37/120    avg_loss:0.213, val_acc:0.935]
Epoch [38/120    avg_loss:0.164, val_acc:0.938]
Epoch [39/120    avg_loss:0.059, val_acc:0.952]
Epoch [40/120    avg_loss:0.120, val_acc:0.935]
Epoch [41/120    avg_loss:0.104, val_acc:0.948]
Epoch [42/120    avg_loss:0.065, val_acc:0.966]
Epoch [43/120    avg_loss:0.074, val_acc:0.962]
Epoch [44/120    avg_loss:0.076, val_acc:0.931]
Epoch [45/120    avg_loss:0.075, val_acc:0.956]
Epoch [46/120    avg_loss:0.083, val_acc:0.929]
Epoch [47/120    avg_loss:0.093, val_acc:0.952]
Epoch [48/120    avg_loss:0.117, val_acc:0.952]
Epoch [49/120    avg_loss:0.135, val_acc:0.933]
Epoch [50/120    avg_loss:0.082, val_acc:0.940]
Epoch [51/120    avg_loss:0.122, val_acc:0.954]
Epoch [52/120    avg_loss:0.055, val_acc:0.966]
Epoch [53/120    avg_loss:0.044, val_acc:0.966]
Epoch [54/120    avg_loss:0.069, val_acc:0.964]
Epoch [55/120    avg_loss:0.040, val_acc:0.976]
Epoch [56/120    avg_loss:0.047, val_acc:0.970]
Epoch [57/120    avg_loss:0.043, val_acc:0.968]
Epoch [58/120    avg_loss:0.031, val_acc:0.968]
Epoch [59/120    avg_loss:0.023, val_acc:0.976]
Epoch [60/120    avg_loss:0.049, val_acc:0.952]
Epoch [61/120    avg_loss:0.037, val_acc:0.960]
Epoch [62/120    avg_loss:0.028, val_acc:0.972]
Epoch [63/120    avg_loss:0.062, val_acc:0.956]
Epoch [64/120    avg_loss:0.053, val_acc:0.944]
Epoch [65/120    avg_loss:0.071, val_acc:0.956]
Epoch [66/120    avg_loss:0.080, val_acc:0.974]
Epoch [67/120    avg_loss:0.043, val_acc:0.984]
Epoch [68/120    avg_loss:0.045, val_acc:0.982]
Epoch [69/120    avg_loss:0.022, val_acc:0.974]
Epoch [70/120    avg_loss:0.034, val_acc:0.968]
Epoch [71/120    avg_loss:0.049, val_acc:0.976]
Epoch [72/120    avg_loss:0.039, val_acc:0.978]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.028, val_acc:0.974]
Epoch [75/120    avg_loss:0.014, val_acc:0.976]
Epoch [76/120    avg_loss:0.031, val_acc:0.964]
Epoch [77/120    avg_loss:0.029, val_acc:0.984]
Epoch [78/120    avg_loss:0.020, val_acc:0.980]
Epoch [79/120    avg_loss:0.095, val_acc:0.938]
Epoch [80/120    avg_loss:0.054, val_acc:0.968]
Epoch [81/120    avg_loss:0.063, val_acc:0.956]
Epoch [82/120    avg_loss:0.068, val_acc:0.974]
Epoch [83/120    avg_loss:0.043, val_acc:0.972]
Epoch [84/120    avg_loss:0.025, val_acc:0.982]
Epoch [85/120    avg_loss:0.052, val_acc:0.946]
Epoch [86/120    avg_loss:0.031, val_acc:0.966]
Epoch [87/120    avg_loss:0.032, val_acc:0.982]
Epoch [88/120    avg_loss:0.029, val_acc:0.982]
Epoch [89/120    avg_loss:0.015, val_acc:0.976]
Epoch [90/120    avg_loss:0.032, val_acc:0.980]
Epoch [91/120    avg_loss:0.020, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.013, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.012, val_acc:0.986]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.027, val_acc:0.986]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.014, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.018, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   0   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 220   2   0   0   0   0   0   0   5   0]
 [  0   0   0   1   6 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 0.99926954 1.         0.99346405 0.97130243 0.96503497
 1.         1.         1.         0.9978678  1.         1.
 0.99451153 1.        ]

Kappa:
0.9959642131549441
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc45a487748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.930, val_acc:0.675]
Epoch [2/120    avg_loss:1.112, val_acc:0.712]
Epoch [3/120    avg_loss:0.986, val_acc:0.726]
Epoch [4/120    avg_loss:0.887, val_acc:0.772]
Epoch [5/120    avg_loss:0.751, val_acc:0.790]
Epoch [6/120    avg_loss:0.782, val_acc:0.770]
Epoch [7/120    avg_loss:0.673, val_acc:0.827]
Epoch [8/120    avg_loss:0.615, val_acc:0.835]
Epoch [9/120    avg_loss:0.607, val_acc:0.798]
Epoch [10/120    avg_loss:0.465, val_acc:0.873]
Epoch [11/120    avg_loss:0.475, val_acc:0.845]
Epoch [12/120    avg_loss:0.421, val_acc:0.889]
Epoch [13/120    avg_loss:0.386, val_acc:0.877]
Epoch [14/120    avg_loss:0.473, val_acc:0.853]
Epoch [15/120    avg_loss:0.466, val_acc:0.823]
Epoch [16/120    avg_loss:0.377, val_acc:0.855]
Epoch [17/120    avg_loss:0.328, val_acc:0.895]
Epoch [18/120    avg_loss:0.331, val_acc:0.899]
Epoch [19/120    avg_loss:0.312, val_acc:0.827]
Epoch [20/120    avg_loss:0.364, val_acc:0.913]
Epoch [21/120    avg_loss:0.300, val_acc:0.875]
Epoch [22/120    avg_loss:0.274, val_acc:0.913]
Epoch [23/120    avg_loss:0.306, val_acc:0.901]
Epoch [24/120    avg_loss:0.338, val_acc:0.915]
Epoch [25/120    avg_loss:0.374, val_acc:0.899]
Epoch [26/120    avg_loss:0.334, val_acc:0.919]
Epoch [27/120    avg_loss:0.222, val_acc:0.929]
Epoch [28/120    avg_loss:0.183, val_acc:0.911]
Epoch [29/120    avg_loss:0.169, val_acc:0.909]
Epoch [30/120    avg_loss:0.202, val_acc:0.913]
Epoch [31/120    avg_loss:0.221, val_acc:0.931]
Epoch [32/120    avg_loss:0.240, val_acc:0.946]
Epoch [33/120    avg_loss:0.185, val_acc:0.938]
Epoch [34/120    avg_loss:0.159, val_acc:0.952]
Epoch [35/120    avg_loss:0.121, val_acc:0.946]
Epoch [36/120    avg_loss:0.119, val_acc:0.929]
Epoch [37/120    avg_loss:0.105, val_acc:0.925]
Epoch [38/120    avg_loss:0.105, val_acc:0.933]
Epoch [39/120    avg_loss:0.170, val_acc:0.940]
Epoch [40/120    avg_loss:0.199, val_acc:0.935]
Epoch [41/120    avg_loss:0.097, val_acc:0.968]
Epoch [42/120    avg_loss:0.107, val_acc:0.929]
Epoch [43/120    avg_loss:0.112, val_acc:0.948]
Epoch [44/120    avg_loss:0.128, val_acc:0.954]
Epoch [45/120    avg_loss:0.100, val_acc:0.956]
Epoch [46/120    avg_loss:0.109, val_acc:0.962]
Epoch [47/120    avg_loss:0.065, val_acc:0.954]
Epoch [48/120    avg_loss:0.061, val_acc:0.970]
Epoch [49/120    avg_loss:0.045, val_acc:0.964]
Epoch [50/120    avg_loss:0.051, val_acc:0.974]
Epoch [51/120    avg_loss:0.055, val_acc:0.980]
Epoch [52/120    avg_loss:0.070, val_acc:0.962]
Epoch [53/120    avg_loss:0.032, val_acc:0.986]
Epoch [54/120    avg_loss:0.034, val_acc:0.980]
Epoch [55/120    avg_loss:0.026, val_acc:0.972]
Epoch [56/120    avg_loss:0.036, val_acc:0.984]
Epoch [57/120    avg_loss:0.120, val_acc:0.929]
Epoch [58/120    avg_loss:0.079, val_acc:0.964]
Epoch [59/120    avg_loss:0.201, val_acc:0.909]
Epoch [60/120    avg_loss:0.073, val_acc:0.956]
Epoch [61/120    avg_loss:0.047, val_acc:0.970]
Epoch [62/120    avg_loss:0.043, val_acc:0.964]
Epoch [63/120    avg_loss:0.060, val_acc:0.982]
Epoch [64/120    avg_loss:0.075, val_acc:0.968]
Epoch [65/120    avg_loss:0.040, val_acc:0.976]
Epoch [66/120    avg_loss:0.055, val_acc:0.970]
Epoch [67/120    avg_loss:0.032, val_acc:0.978]
Epoch [68/120    avg_loss:0.032, val_acc:0.982]
Epoch [69/120    avg_loss:0.021, val_acc:0.980]
Epoch [70/120    avg_loss:0.032, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.019, val_acc:0.982]
Epoch [74/120    avg_loss:0.025, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.986]
Epoch [76/120    avg_loss:0.020, val_acc:0.984]
Epoch [77/120    avg_loss:0.018, val_acc:0.980]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.986]
Epoch [80/120    avg_loss:0.024, val_acc:0.986]
Epoch [81/120    avg_loss:0.013, val_acc:0.984]
Epoch [82/120    avg_loss:0.014, val_acc:0.984]
Epoch [83/120    avg_loss:0.025, val_acc:0.986]
Epoch [84/120    avg_loss:0.040, val_acc:0.984]
Epoch [85/120    avg_loss:0.017, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.016, val_acc:0.986]
Epoch [89/120    avg_loss:0.038, val_acc:0.984]
Epoch [90/120    avg_loss:0.016, val_acc:0.990]
Epoch [91/120    avg_loss:0.022, val_acc:0.988]
Epoch [92/120    avg_loss:0.019, val_acc:0.994]
Epoch [93/120    avg_loss:0.020, val_acc:0.994]
Epoch [94/120    avg_loss:0.016, val_acc:0.994]
Epoch [95/120    avg_loss:0.007, val_acc:0.994]
Epoch [96/120    avg_loss:0.028, val_acc:0.994]
Epoch [97/120    avg_loss:0.011, val_acc:0.994]
Epoch [98/120    avg_loss:0.007, val_acc:0.994]
Epoch [99/120    avg_loss:0.015, val_acc:0.994]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.014, val_acc:0.994]
Epoch [102/120    avg_loss:0.016, val_acc:0.990]
Epoch [103/120    avg_loss:0.020, val_acc:0.994]
Epoch [104/120    avg_loss:0.017, val_acc:0.992]
Epoch [105/120    avg_loss:0.008, val_acc:0.992]
Epoch [106/120    avg_loss:0.018, val_acc:0.992]
Epoch [107/120    avg_loss:0.012, val_acc:0.994]
Epoch [108/120    avg_loss:0.012, val_acc:0.994]
Epoch [109/120    avg_loss:0.012, val_acc:0.994]
Epoch [110/120    avg_loss:0.012, val_acc:0.994]
Epoch [111/120    avg_loss:0.012, val_acc:0.994]
Epoch [112/120    avg_loss:0.009, val_acc:0.994]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.016, val_acc:0.992]
Epoch [117/120    avg_loss:0.019, val_acc:0.994]
Epoch [118/120    avg_loss:0.008, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.009, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 213  17   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   4   0   0   0   0   0   0   5   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   3 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99926954 0.99319728 0.96162528 0.93763441 0.97260274
 1.         0.99465241 1.         1.         1.         0.99603699
 0.98896247 1.        ]

Kappa:
0.9914543169365275
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faac85bb7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.925, val_acc:0.627]
Epoch [2/120    avg_loss:1.242, val_acc:0.679]
Epoch [3/120    avg_loss:1.178, val_acc:0.744]
Epoch [4/120    avg_loss:0.919, val_acc:0.732]
Epoch [5/120    avg_loss:0.791, val_acc:0.819]
Epoch [6/120    avg_loss:0.833, val_acc:0.752]
Epoch [7/120    avg_loss:0.730, val_acc:0.821]
Epoch [8/120    avg_loss:0.623, val_acc:0.810]
Epoch [9/120    avg_loss:0.510, val_acc:0.810]
Epoch [10/120    avg_loss:0.557, val_acc:0.847]
Epoch [11/120    avg_loss:0.502, val_acc:0.859]
Epoch [12/120    avg_loss:0.514, val_acc:0.877]
Epoch [13/120    avg_loss:0.412, val_acc:0.879]
Epoch [14/120    avg_loss:0.442, val_acc:0.841]
Epoch [15/120    avg_loss:0.395, val_acc:0.877]
Epoch [16/120    avg_loss:0.348, val_acc:0.875]
Epoch [17/120    avg_loss:0.428, val_acc:0.869]
Epoch [18/120    avg_loss:0.411, val_acc:0.859]
Epoch [19/120    avg_loss:0.364, val_acc:0.889]
Epoch [20/120    avg_loss:0.282, val_acc:0.913]
Epoch [21/120    avg_loss:0.331, val_acc:0.815]
Epoch [22/120    avg_loss:0.408, val_acc:0.897]
Epoch [23/120    avg_loss:0.341, val_acc:0.915]
Epoch [24/120    avg_loss:0.353, val_acc:0.909]
Epoch [25/120    avg_loss:0.169, val_acc:0.942]
Epoch [26/120    avg_loss:0.291, val_acc:0.954]
Epoch [27/120    avg_loss:0.224, val_acc:0.942]
Epoch [28/120    avg_loss:0.181, val_acc:0.909]
Epoch [29/120    avg_loss:0.175, val_acc:0.921]
Epoch [30/120    avg_loss:0.219, val_acc:0.940]
Epoch [31/120    avg_loss:0.196, val_acc:0.923]
Epoch [32/120    avg_loss:0.142, val_acc:0.942]
Epoch [33/120    avg_loss:0.124, val_acc:0.960]
Epoch [34/120    avg_loss:0.081, val_acc:0.948]
Epoch [35/120    avg_loss:0.133, val_acc:0.938]
Epoch [36/120    avg_loss:0.115, val_acc:0.887]
Epoch [37/120    avg_loss:0.174, val_acc:0.946]
Epoch [38/120    avg_loss:0.123, val_acc:0.948]
Epoch [39/120    avg_loss:0.121, val_acc:0.935]
Epoch [40/120    avg_loss:0.132, val_acc:0.931]
Epoch [41/120    avg_loss:0.155, val_acc:0.929]
Epoch [42/120    avg_loss:0.123, val_acc:0.950]
Epoch [43/120    avg_loss:0.091, val_acc:0.940]
Epoch [44/120    avg_loss:0.117, val_acc:0.960]
Epoch [45/120    avg_loss:0.089, val_acc:0.970]
Epoch [46/120    avg_loss:0.074, val_acc:0.966]
Epoch [47/120    avg_loss:0.096, val_acc:0.960]
Epoch [48/120    avg_loss:0.089, val_acc:0.958]
Epoch [49/120    avg_loss:0.059, val_acc:0.956]
Epoch [50/120    avg_loss:0.042, val_acc:0.958]
Epoch [51/120    avg_loss:0.060, val_acc:0.956]
Epoch [52/120    avg_loss:0.040, val_acc:0.970]
Epoch [53/120    avg_loss:0.070, val_acc:0.966]
Epoch [54/120    avg_loss:0.081, val_acc:0.954]
Epoch [55/120    avg_loss:0.065, val_acc:0.968]
Epoch [56/120    avg_loss:0.052, val_acc:0.976]
Epoch [57/120    avg_loss:0.104, val_acc:0.952]
Epoch [58/120    avg_loss:0.146, val_acc:0.960]
Epoch [59/120    avg_loss:0.045, val_acc:0.966]
Epoch [60/120    avg_loss:0.043, val_acc:0.950]
Epoch [61/120    avg_loss:0.038, val_acc:0.978]
Epoch [62/120    avg_loss:0.024, val_acc:0.982]
Epoch [63/120    avg_loss:0.022, val_acc:0.974]
Epoch [64/120    avg_loss:0.012, val_acc:0.984]
Epoch [65/120    avg_loss:0.024, val_acc:0.978]
Epoch [66/120    avg_loss:0.021, val_acc:0.982]
Epoch [67/120    avg_loss:0.059, val_acc:0.976]
Epoch [68/120    avg_loss:0.062, val_acc:0.986]
Epoch [69/120    avg_loss:0.038, val_acc:0.966]
Epoch [70/120    avg_loss:0.037, val_acc:0.982]
Epoch [71/120    avg_loss:0.035, val_acc:0.978]
Epoch [72/120    avg_loss:0.044, val_acc:0.974]
Epoch [73/120    avg_loss:0.026, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.029, val_acc:0.982]
Epoch [76/120    avg_loss:0.036, val_acc:0.980]
Epoch [77/120    avg_loss:0.027, val_acc:0.980]
Epoch [78/120    avg_loss:0.019, val_acc:0.988]
Epoch [79/120    avg_loss:0.025, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.978]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.992]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.014, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.986]
Epoch [87/120    avg_loss:0.010, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.015, val_acc:0.990]
Epoch [92/120    avg_loss:0.038, val_acc:0.974]
Epoch [93/120    avg_loss:0.014, val_acc:0.984]
Epoch [94/120    avg_loss:0.016, val_acc:0.990]
Epoch [95/120    avg_loss:0.036, val_acc:0.976]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.011, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.009, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.010, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.016, val_acc:0.994]
Epoch [112/120    avg_loss:0.004, val_acc:0.994]
Epoch [113/120    avg_loss:0.011, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.004, val_acc:0.994]
Epoch [116/120    avg_loss:0.013, val_acc:0.996]
Epoch [117/120    avg_loss:0.012, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.012, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   2 220   4   1   0   0   0   0   0   0   0]
 [  0   0   0   2   0 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99926954 1.         0.97142857 0.96491228 0.97610922
 0.99757869 1.         1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9950152145026739
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd717a947b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.850, val_acc:0.607]
Epoch [2/120    avg_loss:1.219, val_acc:0.645]
Epoch [3/120    avg_loss:0.991, val_acc:0.726]
Epoch [4/120    avg_loss:1.094, val_acc:0.736]
Epoch [5/120    avg_loss:0.774, val_acc:0.800]
Epoch [6/120    avg_loss:0.596, val_acc:0.798]
Epoch [7/120    avg_loss:0.663, val_acc:0.796]
Epoch [8/120    avg_loss:0.665, val_acc:0.849]
Epoch [9/120    avg_loss:0.568, val_acc:0.861]
Epoch [10/120    avg_loss:0.562, val_acc:0.841]
Epoch [11/120    avg_loss:0.556, val_acc:0.857]
Epoch [12/120    avg_loss:0.496, val_acc:0.843]
Epoch [13/120    avg_loss:0.557, val_acc:0.812]
Epoch [14/120    avg_loss:0.486, val_acc:0.857]
Epoch [15/120    avg_loss:0.533, val_acc:0.877]
Epoch [16/120    avg_loss:0.351, val_acc:0.903]
Epoch [17/120    avg_loss:0.329, val_acc:0.907]
Epoch [18/120    avg_loss:0.379, val_acc:0.875]
Epoch [19/120    avg_loss:0.251, val_acc:0.911]
Epoch [20/120    avg_loss:0.381, val_acc:0.895]
Epoch [21/120    avg_loss:0.283, val_acc:0.893]
Epoch [22/120    avg_loss:0.295, val_acc:0.923]
Epoch [23/120    avg_loss:0.263, val_acc:0.909]
Epoch [24/120    avg_loss:0.288, val_acc:0.931]
Epoch [25/120    avg_loss:0.207, val_acc:0.931]
Epoch [26/120    avg_loss:0.165, val_acc:0.935]
Epoch [27/120    avg_loss:0.244, val_acc:0.863]
Epoch [28/120    avg_loss:0.268, val_acc:0.933]
Epoch [29/120    avg_loss:0.154, val_acc:0.933]
Epoch [30/120    avg_loss:0.189, val_acc:0.935]
Epoch [31/120    avg_loss:0.217, val_acc:0.958]
Epoch [32/120    avg_loss:0.170, val_acc:0.921]
Epoch [33/120    avg_loss:0.144, val_acc:0.962]
Epoch [34/120    avg_loss:0.148, val_acc:0.925]
Epoch [35/120    avg_loss:0.155, val_acc:0.915]
Epoch [36/120    avg_loss:0.140, val_acc:0.927]
Epoch [37/120    avg_loss:0.134, val_acc:0.950]
Epoch [38/120    avg_loss:0.121, val_acc:0.964]
Epoch [39/120    avg_loss:0.079, val_acc:0.972]
Epoch [40/120    avg_loss:0.096, val_acc:0.960]
Epoch [41/120    avg_loss:0.097, val_acc:0.968]
Epoch [42/120    avg_loss:0.110, val_acc:0.976]
Epoch [43/120    avg_loss:0.083, val_acc:0.962]
Epoch [44/120    avg_loss:0.076, val_acc:0.972]
Epoch [45/120    avg_loss:0.077, val_acc:0.952]
Epoch [46/120    avg_loss:0.077, val_acc:0.968]
Epoch [47/120    avg_loss:0.106, val_acc:0.958]
Epoch [48/120    avg_loss:0.131, val_acc:0.964]
Epoch [49/120    avg_loss:0.104, val_acc:0.966]
Epoch [50/120    avg_loss:0.095, val_acc:0.968]
Epoch [51/120    avg_loss:0.054, val_acc:0.976]
Epoch [52/120    avg_loss:0.045, val_acc:0.946]
Epoch [53/120    avg_loss:0.052, val_acc:0.982]
Epoch [54/120    avg_loss:0.051, val_acc:0.938]
Epoch [55/120    avg_loss:0.091, val_acc:0.960]
Epoch [56/120    avg_loss:0.139, val_acc:0.940]
Epoch [57/120    avg_loss:0.115, val_acc:0.972]
Epoch [58/120    avg_loss:0.073, val_acc:0.970]
Epoch [59/120    avg_loss:0.047, val_acc:0.982]
Epoch [60/120    avg_loss:0.070, val_acc:0.966]
Epoch [61/120    avg_loss:0.085, val_acc:0.962]
Epoch [62/120    avg_loss:0.050, val_acc:0.984]
Epoch [63/120    avg_loss:0.035, val_acc:0.919]
Epoch [64/120    avg_loss:0.042, val_acc:0.990]
Epoch [65/120    avg_loss:0.042, val_acc:0.974]
Epoch [66/120    avg_loss:0.056, val_acc:0.984]
Epoch [67/120    avg_loss:0.045, val_acc:0.974]
Epoch [68/120    avg_loss:0.041, val_acc:0.990]
Epoch [69/120    avg_loss:0.064, val_acc:0.974]
Epoch [70/120    avg_loss:0.116, val_acc:0.976]
Epoch [71/120    avg_loss:0.034, val_acc:0.982]
Epoch [72/120    avg_loss:0.026, val_acc:0.978]
Epoch [73/120    avg_loss:0.036, val_acc:0.986]
Epoch [74/120    avg_loss:0.018, val_acc:0.992]
Epoch [75/120    avg_loss:0.034, val_acc:0.990]
Epoch [76/120    avg_loss:0.035, val_acc:0.992]
Epoch [77/120    avg_loss:0.062, val_acc:0.980]
Epoch [78/120    avg_loss:0.021, val_acc:0.980]
Epoch [79/120    avg_loss:0.012, val_acc:0.992]
Epoch [80/120    avg_loss:0.013, val_acc:0.990]
Epoch [81/120    avg_loss:0.014, val_acc:0.988]
Epoch [82/120    avg_loss:0.013, val_acc:0.982]
Epoch [83/120    avg_loss:0.013, val_acc:0.990]
Epoch [84/120    avg_loss:0.060, val_acc:0.980]
Epoch [85/120    avg_loss:0.022, val_acc:0.978]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.015, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.024, val_acc:0.990]
Epoch [94/120    avg_loss:0.018, val_acc:0.984]
Epoch [95/120    avg_loss:0.025, val_acc:0.986]
Epoch [96/120    avg_loss:0.028, val_acc:0.986]
Epoch [97/120    avg_loss:0.014, val_acc:0.994]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.014, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.998]
Epoch [103/120    avg_loss:0.007, val_acc:0.996]
Epoch [104/120    avg_loss:0.006, val_acc:0.994]
Epoch [105/120    avg_loss:0.009, val_acc:0.996]
Epoch [106/120    avg_loss:0.073, val_acc:0.921]
Epoch [107/120    avg_loss:0.187, val_acc:0.956]
Epoch [108/120    avg_loss:0.055, val_acc:0.980]
Epoch [109/120    avg_loss:0.058, val_acc:0.992]
Epoch [110/120    avg_loss:0.019, val_acc:0.988]
Epoch [111/120    avg_loss:0.018, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.996]
Epoch [115/120    avg_loss:0.014, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.021, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.008, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   1   1   0   0   0   0   0   5   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99926954 0.99545455 0.99563319 0.95032397 0.93478261
 0.99757869 1.         1.         1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.9938275270124153
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1529b9780>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.848, val_acc:0.675]
Epoch [2/120    avg_loss:1.352, val_acc:0.631]
Epoch [3/120    avg_loss:1.031, val_acc:0.671]
Epoch [4/120    avg_loss:0.900, val_acc:0.786]
Epoch [5/120    avg_loss:0.719, val_acc:0.806]
Epoch [6/120    avg_loss:0.626, val_acc:0.835]
Epoch [7/120    avg_loss:0.657, val_acc:0.772]
Epoch [8/120    avg_loss:0.642, val_acc:0.774]
Epoch [9/120    avg_loss:0.548, val_acc:0.798]
Epoch [10/120    avg_loss:0.690, val_acc:0.829]
Epoch [11/120    avg_loss:0.516, val_acc:0.871]
Epoch [12/120    avg_loss:0.439, val_acc:0.841]
Epoch [13/120    avg_loss:0.370, val_acc:0.907]
Epoch [14/120    avg_loss:0.364, val_acc:0.873]
Epoch [15/120    avg_loss:0.441, val_acc:0.877]
Epoch [16/120    avg_loss:0.335, val_acc:0.853]
Epoch [17/120    avg_loss:0.316, val_acc:0.915]
Epoch [18/120    avg_loss:0.284, val_acc:0.927]
Epoch [19/120    avg_loss:0.222, val_acc:0.899]
Epoch [20/120    avg_loss:0.309, val_acc:0.875]
Epoch [21/120    avg_loss:0.307, val_acc:0.907]
Epoch [22/120    avg_loss:0.236, val_acc:0.917]
Epoch [23/120    avg_loss:0.193, val_acc:0.931]
Epoch [24/120    avg_loss:0.186, val_acc:0.925]
Epoch [25/120    avg_loss:0.229, val_acc:0.909]
Epoch [26/120    avg_loss:0.194, val_acc:0.925]
Epoch [27/120    avg_loss:0.134, val_acc:0.952]
Epoch [28/120    avg_loss:0.143, val_acc:0.944]
Epoch [29/120    avg_loss:0.114, val_acc:0.958]
Epoch [30/120    avg_loss:0.095, val_acc:0.946]
Epoch [31/120    avg_loss:0.081, val_acc:0.954]
Epoch [32/120    avg_loss:0.132, val_acc:0.942]
Epoch [33/120    avg_loss:0.106, val_acc:0.952]
Epoch [34/120    avg_loss:0.110, val_acc:0.919]
Epoch [35/120    avg_loss:0.114, val_acc:0.958]
Epoch [36/120    avg_loss:0.108, val_acc:0.921]
Epoch [37/120    avg_loss:0.077, val_acc:0.962]
Epoch [38/120    avg_loss:0.109, val_acc:0.956]
Epoch [39/120    avg_loss:0.065, val_acc:0.958]
Epoch [40/120    avg_loss:0.055, val_acc:0.948]
Epoch [41/120    avg_loss:0.060, val_acc:0.962]
Epoch [42/120    avg_loss:0.063, val_acc:0.966]
Epoch [43/120    avg_loss:0.109, val_acc:0.974]
Epoch [44/120    avg_loss:0.169, val_acc:0.944]
Epoch [45/120    avg_loss:0.139, val_acc:0.940]
Epoch [46/120    avg_loss:0.090, val_acc:0.907]
Epoch [47/120    avg_loss:0.179, val_acc:0.954]
Epoch [48/120    avg_loss:0.103, val_acc:0.960]
Epoch [49/120    avg_loss:0.051, val_acc:0.970]
Epoch [50/120    avg_loss:0.038, val_acc:0.976]
Epoch [51/120    avg_loss:0.046, val_acc:0.921]
Epoch [52/120    avg_loss:0.073, val_acc:0.974]
Epoch [53/120    avg_loss:0.046, val_acc:0.980]
Epoch [54/120    avg_loss:0.038, val_acc:0.968]
Epoch [55/120    avg_loss:0.041, val_acc:0.978]
Epoch [56/120    avg_loss:0.027, val_acc:0.988]
Epoch [57/120    avg_loss:0.021, val_acc:0.972]
Epoch [58/120    avg_loss:0.031, val_acc:0.974]
Epoch [59/120    avg_loss:0.035, val_acc:0.984]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.034, val_acc:0.974]
Epoch [62/120    avg_loss:0.055, val_acc:0.970]
Epoch [63/120    avg_loss:0.020, val_acc:0.978]
Epoch [64/120    avg_loss:0.010, val_acc:0.982]
Epoch [65/120    avg_loss:0.053, val_acc:0.966]
Epoch [66/120    avg_loss:0.090, val_acc:0.970]
Epoch [67/120    avg_loss:0.026, val_acc:0.976]
Epoch [68/120    avg_loss:0.022, val_acc:0.980]
Epoch [69/120    avg_loss:0.047, val_acc:0.978]
Epoch [70/120    avg_loss:0.032, val_acc:0.988]
Epoch [71/120    avg_loss:0.021, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.988]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.027, val_acc:0.992]
Epoch [75/120    avg_loss:0.015, val_acc:0.990]
Epoch [76/120    avg_loss:0.013, val_acc:0.990]
Epoch [77/120    avg_loss:0.016, val_acc:0.992]
Epoch [78/120    avg_loss:0.008, val_acc:0.992]
Epoch [79/120    avg_loss:0.014, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.011, val_acc:0.990]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.013, val_acc:0.990]
Epoch [85/120    avg_loss:0.006, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.021, val_acc:0.992]
Epoch [89/120    avg_loss:0.009, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.990]
Epoch [92/120    avg_loss:0.008, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.019, val_acc:0.990]
Epoch [97/120    avg_loss:0.013, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.014, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.024, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 204  26   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   3   0   0   0   0   0   0   8   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.99545455 0.94009217 0.90566038 0.96140351
 1.         0.98924731 1.         1.         1.         1.
 0.99124726 1.        ]

Kappa:
0.9888417218261449
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8ed5196a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.837, val_acc:0.514]
Epoch [2/120    avg_loss:1.290, val_acc:0.667]
Epoch [3/120    avg_loss:1.040, val_acc:0.744]
Epoch [4/120    avg_loss:0.883, val_acc:0.792]
Epoch [5/120    avg_loss:0.858, val_acc:0.744]
Epoch [6/120    avg_loss:0.722, val_acc:0.835]
Epoch [7/120    avg_loss:0.650, val_acc:0.810]
Epoch [8/120    avg_loss:0.527, val_acc:0.877]
Epoch [9/120    avg_loss:0.472, val_acc:0.869]
Epoch [10/120    avg_loss:0.594, val_acc:0.863]
Epoch [11/120    avg_loss:0.531, val_acc:0.837]
Epoch [12/120    avg_loss:0.470, val_acc:0.837]
Epoch [13/120    avg_loss:0.405, val_acc:0.881]
Epoch [14/120    avg_loss:0.392, val_acc:0.903]
Epoch [15/120    avg_loss:0.358, val_acc:0.893]
Epoch [16/120    avg_loss:0.356, val_acc:0.885]
Epoch [17/120    avg_loss:0.453, val_acc:0.907]
Epoch [18/120    avg_loss:0.350, val_acc:0.907]
Epoch [19/120    avg_loss:0.302, val_acc:0.905]
Epoch [20/120    avg_loss:0.451, val_acc:0.865]
Epoch [21/120    avg_loss:0.449, val_acc:0.901]
Epoch [22/120    avg_loss:0.285, val_acc:0.917]
Epoch [23/120    avg_loss:0.221, val_acc:0.901]
Epoch [24/120    avg_loss:0.322, val_acc:0.877]
Epoch [25/120    avg_loss:0.340, val_acc:0.921]
Epoch [26/120    avg_loss:0.156, val_acc:0.935]
Epoch [27/120    avg_loss:0.170, val_acc:0.933]
Epoch [28/120    avg_loss:0.195, val_acc:0.927]
Epoch [29/120    avg_loss:0.138, val_acc:0.938]
Epoch [30/120    avg_loss:0.105, val_acc:0.931]
Epoch [31/120    avg_loss:0.100, val_acc:0.956]
Epoch [32/120    avg_loss:0.120, val_acc:0.935]
Epoch [33/120    avg_loss:0.175, val_acc:0.944]
Epoch [34/120    avg_loss:0.158, val_acc:0.962]
Epoch [35/120    avg_loss:0.143, val_acc:0.956]
Epoch [36/120    avg_loss:0.135, val_acc:0.958]
Epoch [37/120    avg_loss:0.138, val_acc:0.940]
Epoch [38/120    avg_loss:0.078, val_acc:0.954]
Epoch [39/120    avg_loss:0.098, val_acc:0.935]
Epoch [40/120    avg_loss:0.089, val_acc:0.948]
Epoch [41/120    avg_loss:0.059, val_acc:0.976]
Epoch [42/120    avg_loss:0.109, val_acc:0.966]
Epoch [43/120    avg_loss:0.151, val_acc:0.950]
Epoch [44/120    avg_loss:0.083, val_acc:0.964]
Epoch [45/120    avg_loss:0.037, val_acc:0.956]
Epoch [46/120    avg_loss:0.049, val_acc:0.978]
Epoch [47/120    avg_loss:0.073, val_acc:0.958]
Epoch [48/120    avg_loss:0.062, val_acc:0.968]
Epoch [49/120    avg_loss:0.059, val_acc:0.958]
Epoch [50/120    avg_loss:0.038, val_acc:0.956]
Epoch [51/120    avg_loss:0.040, val_acc:0.962]
Epoch [52/120    avg_loss:0.040, val_acc:0.966]
Epoch [53/120    avg_loss:0.054, val_acc:0.962]
Epoch [54/120    avg_loss:0.056, val_acc:0.954]
Epoch [55/120    avg_loss:0.046, val_acc:0.968]
Epoch [56/120    avg_loss:0.055, val_acc:0.962]
Epoch [57/120    avg_loss:0.061, val_acc:0.966]
Epoch [58/120    avg_loss:0.074, val_acc:0.964]
Epoch [59/120    avg_loss:0.061, val_acc:0.968]
Epoch [60/120    avg_loss:0.043, val_acc:0.974]
Epoch [61/120    avg_loss:0.021, val_acc:0.974]
Epoch [62/120    avg_loss:0.025, val_acc:0.974]
Epoch [63/120    avg_loss:0.025, val_acc:0.974]
Epoch [64/120    avg_loss:0.023, val_acc:0.976]
Epoch [65/120    avg_loss:0.011, val_acc:0.978]
Epoch [66/120    avg_loss:0.019, val_acc:0.978]
Epoch [67/120    avg_loss:0.028, val_acc:0.978]
Epoch [68/120    avg_loss:0.029, val_acc:0.976]
Epoch [69/120    avg_loss:0.029, val_acc:0.978]
Epoch [70/120    avg_loss:0.024, val_acc:0.976]
Epoch [71/120    avg_loss:0.011, val_acc:0.976]
Epoch [72/120    avg_loss:0.019, val_acc:0.976]
Epoch [73/120    avg_loss:0.014, val_acc:0.976]
Epoch [74/120    avg_loss:0.022, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.978]
Epoch [76/120    avg_loss:0.011, val_acc:0.978]
Epoch [77/120    avg_loss:0.019, val_acc:0.978]
Epoch [78/120    avg_loss:0.011, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.978]
Epoch [80/120    avg_loss:0.018, val_acc:0.980]
Epoch [81/120    avg_loss:0.018, val_acc:0.978]
Epoch [82/120    avg_loss:0.021, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.980]
Epoch [84/120    avg_loss:0.012, val_acc:0.978]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.010, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.978]
Epoch [88/120    avg_loss:0.020, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.978]
Epoch [90/120    avg_loss:0.017, val_acc:0.976]
Epoch [91/120    avg_loss:0.017, val_acc:0.976]
Epoch [92/120    avg_loss:0.012, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.976]
Epoch [94/120    avg_loss:0.025, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.980]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.016, val_acc:0.980]
Epoch [102/120    avg_loss:0.015, val_acc:0.980]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.023, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.016, val_acc:0.982]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.013, val_acc:0.982]
Epoch [110/120    avg_loss:0.012, val_acc:0.980]
Epoch [111/120    avg_loss:0.015, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.043, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.014, val_acc:0.976]
Epoch [120/120    avg_loss:0.008, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 206  23   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   5   0   0   0   0   0   0   2   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99926954 0.99545455 0.94495413 0.92436975 0.95862069
 1.         0.99465241 1.         1.         1.         0.99341238
 0.99224806 1.        ]

Kappa:
0.9895553962579712
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc15d0906d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.791, val_acc:0.694]
Epoch [2/120    avg_loss:1.252, val_acc:0.688]
Epoch [3/120    avg_loss:1.026, val_acc:0.708]
Epoch [4/120    avg_loss:0.909, val_acc:0.726]
Epoch [5/120    avg_loss:0.824, val_acc:0.778]
Epoch [6/120    avg_loss:0.788, val_acc:0.849]
Epoch [7/120    avg_loss:0.805, val_acc:0.841]
Epoch [8/120    avg_loss:0.604, val_acc:0.843]
Epoch [9/120    avg_loss:0.607, val_acc:0.909]
Epoch [10/120    avg_loss:0.555, val_acc:0.893]
Epoch [11/120    avg_loss:0.539, val_acc:0.869]
Epoch [12/120    avg_loss:0.501, val_acc:0.857]
Epoch [13/120    avg_loss:0.569, val_acc:0.895]
Epoch [14/120    avg_loss:0.490, val_acc:0.903]
Epoch [15/120    avg_loss:0.445, val_acc:0.871]
Epoch [16/120    avg_loss:0.375, val_acc:0.885]
Epoch [17/120    avg_loss:0.349, val_acc:0.915]
Epoch [18/120    avg_loss:0.281, val_acc:0.917]
Epoch [19/120    avg_loss:0.358, val_acc:0.929]
Epoch [20/120    avg_loss:0.311, val_acc:0.909]
Epoch [21/120    avg_loss:0.255, val_acc:0.917]
Epoch [22/120    avg_loss:0.302, val_acc:0.899]
Epoch [23/120    avg_loss:0.334, val_acc:0.893]
Epoch [24/120    avg_loss:0.295, val_acc:0.954]
Epoch [25/120    avg_loss:0.305, val_acc:0.925]
Epoch [26/120    avg_loss:0.273, val_acc:0.938]
Epoch [27/120    avg_loss:0.307, val_acc:0.950]
Epoch [28/120    avg_loss:0.330, val_acc:0.927]
Epoch [29/120    avg_loss:0.213, val_acc:0.946]
Epoch [30/120    avg_loss:0.218, val_acc:0.944]
Epoch [31/120    avg_loss:0.156, val_acc:0.948]
Epoch [32/120    avg_loss:0.201, val_acc:0.940]
Epoch [33/120    avg_loss:0.218, val_acc:0.944]
Epoch [34/120    avg_loss:0.168, val_acc:0.952]
Epoch [35/120    avg_loss:0.181, val_acc:0.966]
Epoch [36/120    avg_loss:0.178, val_acc:0.958]
Epoch [37/120    avg_loss:0.172, val_acc:0.948]
Epoch [38/120    avg_loss:0.114, val_acc:0.960]
Epoch [39/120    avg_loss:0.173, val_acc:0.911]
Epoch [40/120    avg_loss:0.249, val_acc:0.940]
Epoch [41/120    avg_loss:0.181, val_acc:0.966]
Epoch [42/120    avg_loss:0.076, val_acc:0.958]
Epoch [43/120    avg_loss:0.067, val_acc:0.970]
Epoch [44/120    avg_loss:0.133, val_acc:0.968]
Epoch [45/120    avg_loss:0.080, val_acc:0.972]
Epoch [46/120    avg_loss:0.076, val_acc:0.974]
Epoch [47/120    avg_loss:0.077, val_acc:0.974]
Epoch [48/120    avg_loss:0.068, val_acc:0.982]
Epoch [49/120    avg_loss:0.125, val_acc:0.968]
Epoch [50/120    avg_loss:0.074, val_acc:0.976]
Epoch [51/120    avg_loss:0.061, val_acc:0.984]
Epoch [52/120    avg_loss:0.057, val_acc:0.972]
Epoch [53/120    avg_loss:0.162, val_acc:0.974]
Epoch [54/120    avg_loss:0.083, val_acc:0.956]
Epoch [55/120    avg_loss:0.137, val_acc:0.974]
Epoch [56/120    avg_loss:0.086, val_acc:0.970]
Epoch [57/120    avg_loss:0.048, val_acc:0.980]
Epoch [58/120    avg_loss:0.042, val_acc:0.982]
Epoch [59/120    avg_loss:0.044, val_acc:0.982]
Epoch [60/120    avg_loss:0.085, val_acc:0.974]
Epoch [61/120    avg_loss:0.054, val_acc:0.962]
Epoch [62/120    avg_loss:0.067, val_acc:0.972]
Epoch [63/120    avg_loss:0.049, val_acc:0.978]
Epoch [64/120    avg_loss:0.028, val_acc:0.980]
Epoch [65/120    avg_loss:0.032, val_acc:0.984]
Epoch [66/120    avg_loss:0.033, val_acc:0.984]
Epoch [67/120    avg_loss:0.028, val_acc:0.984]
Epoch [68/120    avg_loss:0.021, val_acc:0.986]
Epoch [69/120    avg_loss:0.030, val_acc:0.988]
Epoch [70/120    avg_loss:0.021, val_acc:0.986]
Epoch [71/120    avg_loss:0.034, val_acc:0.984]
Epoch [72/120    avg_loss:0.025, val_acc:0.984]
Epoch [73/120    avg_loss:0.016, val_acc:0.988]
Epoch [74/120    avg_loss:0.013, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.988]
Epoch [76/120    avg_loss:0.022, val_acc:0.986]
Epoch [77/120    avg_loss:0.038, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.015, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.019, val_acc:0.986]
Epoch [83/120    avg_loss:0.022, val_acc:0.988]
Epoch [84/120    avg_loss:0.032, val_acc:0.988]
Epoch [85/120    avg_loss:0.023, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.021, val_acc:0.988]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.014, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.020, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.016, val_acc:0.986]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.015, val_acc:0.988]
Epoch [98/120    avg_loss:0.018, val_acc:0.988]
Epoch [99/120    avg_loss:0.023, val_acc:0.990]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.016, val_acc:0.990]
Epoch [102/120    avg_loss:0.010, val_acc:0.990]
Epoch [103/120    avg_loss:0.014, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.025, val_acc:0.986]
Epoch [106/120    avg_loss:0.020, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.016, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.024, val_acc:0.988]
Epoch [111/120    avg_loss:0.018, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.019, val_acc:0.990]
Epoch [114/120    avg_loss:0.020, val_acc:0.992]
Epoch [115/120    avg_loss:0.016, val_acc:0.992]
Epoch [116/120    avg_loss:0.013, val_acc:0.992]
Epoch [117/120    avg_loss:0.017, val_acc:0.990]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.027, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   3   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 209  16   0   0   0   2   2   0   0   0   0]
 [  0   0   0   0 218   4   0   0   0   0   0   0   5   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99780541 0.99319728 0.95216401 0.91022965 0.91039427
 1.         1.         0.99742931 0.9978678  1.         1.
 0.99229923 1.        ]

Kappa:
0.98741819511106
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f278f7748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.941, val_acc:0.663]
Epoch [2/120    avg_loss:1.210, val_acc:0.718]
Epoch [3/120    avg_loss:1.032, val_acc:0.706]
Epoch [4/120    avg_loss:1.017, val_acc:0.722]
Epoch [5/120    avg_loss:0.755, val_acc:0.810]
Epoch [6/120    avg_loss:0.734, val_acc:0.792]
Epoch [7/120    avg_loss:0.777, val_acc:0.764]
Epoch [8/120    avg_loss:0.622, val_acc:0.802]
Epoch [9/120    avg_loss:0.676, val_acc:0.881]
Epoch [10/120    avg_loss:0.544, val_acc:0.798]
Epoch [11/120    avg_loss:0.591, val_acc:0.867]
Epoch [12/120    avg_loss:0.564, val_acc:0.843]
Epoch [13/120    avg_loss:0.421, val_acc:0.865]
Epoch [14/120    avg_loss:0.512, val_acc:0.913]
Epoch [15/120    avg_loss:0.394, val_acc:0.889]
Epoch [16/120    avg_loss:0.440, val_acc:0.859]
Epoch [17/120    avg_loss:0.494, val_acc:0.893]
Epoch [18/120    avg_loss:0.371, val_acc:0.901]
Epoch [19/120    avg_loss:0.395, val_acc:0.915]
Epoch [20/120    avg_loss:0.371, val_acc:0.873]
Epoch [21/120    avg_loss:0.469, val_acc:0.883]
Epoch [22/120    avg_loss:0.388, val_acc:0.903]
Epoch [23/120    avg_loss:0.376, val_acc:0.899]
Epoch [24/120    avg_loss:0.388, val_acc:0.905]
Epoch [25/120    avg_loss:0.300, val_acc:0.905]
Epoch [26/120    avg_loss:0.312, val_acc:0.905]
Epoch [27/120    avg_loss:0.284, val_acc:0.909]
Epoch [28/120    avg_loss:0.245, val_acc:0.935]
Epoch [29/120    avg_loss:0.202, val_acc:0.946]
Epoch [30/120    avg_loss:0.201, val_acc:0.933]
Epoch [31/120    avg_loss:0.222, val_acc:0.954]
Epoch [32/120    avg_loss:0.253, val_acc:0.923]
Epoch [33/120    avg_loss:0.181, val_acc:0.942]
Epoch [34/120    avg_loss:0.128, val_acc:0.954]
Epoch [35/120    avg_loss:0.114, val_acc:0.952]
Epoch [36/120    avg_loss:0.098, val_acc:0.966]
Epoch [37/120    avg_loss:0.149, val_acc:0.954]
Epoch [38/120    avg_loss:0.148, val_acc:0.948]
Epoch [39/120    avg_loss:0.096, val_acc:0.966]
Epoch [40/120    avg_loss:0.069, val_acc:0.960]
Epoch [41/120    avg_loss:0.075, val_acc:0.958]
Epoch [42/120    avg_loss:0.087, val_acc:0.929]
Epoch [43/120    avg_loss:0.198, val_acc:0.958]
Epoch [44/120    avg_loss:0.082, val_acc:0.966]
Epoch [45/120    avg_loss:0.121, val_acc:0.946]
Epoch [46/120    avg_loss:0.133, val_acc:0.974]
Epoch [47/120    avg_loss:0.105, val_acc:0.968]
Epoch [48/120    avg_loss:0.129, val_acc:0.954]
Epoch [49/120    avg_loss:0.100, val_acc:0.972]
Epoch [50/120    avg_loss:0.041, val_acc:0.974]
Epoch [51/120    avg_loss:0.067, val_acc:0.970]
Epoch [52/120    avg_loss:0.040, val_acc:0.984]
Epoch [53/120    avg_loss:0.043, val_acc:0.970]
Epoch [54/120    avg_loss:0.038, val_acc:0.984]
Epoch [55/120    avg_loss:0.127, val_acc:0.909]
Epoch [56/120    avg_loss:0.084, val_acc:0.970]
Epoch [57/120    avg_loss:0.135, val_acc:0.952]
Epoch [58/120    avg_loss:0.108, val_acc:0.974]
Epoch [59/120    avg_loss:0.040, val_acc:0.980]
Epoch [60/120    avg_loss:0.040, val_acc:0.966]
Epoch [61/120    avg_loss:0.031, val_acc:0.978]
Epoch [62/120    avg_loss:0.027, val_acc:0.984]
Epoch [63/120    avg_loss:0.011, val_acc:0.982]
Epoch [64/120    avg_loss:0.043, val_acc:0.980]
Epoch [65/120    avg_loss:0.033, val_acc:0.986]
Epoch [66/120    avg_loss:0.083, val_acc:0.986]
Epoch [67/120    avg_loss:0.052, val_acc:0.972]
Epoch [68/120    avg_loss:0.060, val_acc:0.978]
Epoch [69/120    avg_loss:0.029, val_acc:0.984]
Epoch [70/120    avg_loss:0.012, val_acc:0.988]
Epoch [71/120    avg_loss:0.040, val_acc:0.964]
Epoch [72/120    avg_loss:0.043, val_acc:0.976]
Epoch [73/120    avg_loss:0.050, val_acc:0.980]
Epoch [74/120    avg_loss:0.027, val_acc:0.978]
Epoch [75/120    avg_loss:0.028, val_acc:0.986]
Epoch [76/120    avg_loss:0.019, val_acc:0.984]
Epoch [77/120    avg_loss:0.048, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.018, val_acc:0.982]
Epoch [81/120    avg_loss:0.020, val_acc:0.984]
Epoch [82/120    avg_loss:0.026, val_acc:0.980]
Epoch [83/120    avg_loss:0.018, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.994]
Epoch [88/120    avg_loss:0.008, val_acc:0.992]
Epoch [89/120    avg_loss:0.010, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.990]
Epoch [91/120    avg_loss:0.028, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.020, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.037, val_acc:0.972]
Epoch [101/120    avg_loss:0.036, val_acc:0.990]
Epoch [102/120    avg_loss:0.014, val_acc:0.992]
Epoch [103/120    avg_loss:0.024, val_acc:0.994]
Epoch [104/120    avg_loss:0.013, val_acc:0.996]
Epoch [105/120    avg_loss:0.017, val_acc:0.996]
Epoch [106/120    avg_loss:0.008, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   4   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   7   0   0   0   0   0   0   3   0]
 [  0   0   0   1   5 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99486427 1.         0.99565217 0.96444444 0.94237288
 0.99277108 1.         1.         1.         1.         0.9986755
 0.99559471 1.        ]

Kappa:
0.9940663877337533
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb143a3a6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.989, val_acc:0.556]
Epoch [2/120    avg_loss:1.164, val_acc:0.641]
Epoch [3/120    avg_loss:0.998, val_acc:0.786]
Epoch [4/120    avg_loss:0.856, val_acc:0.770]
Epoch [5/120    avg_loss:0.780, val_acc:0.827]
Epoch [6/120    avg_loss:0.762, val_acc:0.760]
Epoch [7/120    avg_loss:0.775, val_acc:0.786]
Epoch [8/120    avg_loss:0.637, val_acc:0.815]
Epoch [9/120    avg_loss:0.545, val_acc:0.855]
Epoch [10/120    avg_loss:0.513, val_acc:0.861]
Epoch [11/120    avg_loss:0.508, val_acc:0.885]
Epoch [12/120    avg_loss:0.454, val_acc:0.891]
Epoch [13/120    avg_loss:0.488, val_acc:0.883]
Epoch [14/120    avg_loss:0.347, val_acc:0.907]
Epoch [15/120    avg_loss:0.322, val_acc:0.907]
Epoch [16/120    avg_loss:0.251, val_acc:0.911]
Epoch [17/120    avg_loss:0.264, val_acc:0.929]
Epoch [18/120    avg_loss:0.267, val_acc:0.931]
Epoch [19/120    avg_loss:0.231, val_acc:0.923]
Epoch [20/120    avg_loss:0.198, val_acc:0.901]
Epoch [21/120    avg_loss:0.337, val_acc:0.931]
Epoch [22/120    avg_loss:0.244, val_acc:0.931]
Epoch [23/120    avg_loss:0.279, val_acc:0.927]
Epoch [24/120    avg_loss:0.165, val_acc:0.962]
Epoch [25/120    avg_loss:0.269, val_acc:0.964]
Epoch [26/120    avg_loss:0.198, val_acc:0.960]
Epoch [27/120    avg_loss:0.124, val_acc:0.966]
Epoch [28/120    avg_loss:0.126, val_acc:0.972]
Epoch [29/120    avg_loss:0.106, val_acc:0.921]
Epoch [30/120    avg_loss:0.179, val_acc:0.950]
Epoch [31/120    avg_loss:0.146, val_acc:0.968]
Epoch [32/120    avg_loss:0.114, val_acc:0.966]
Epoch [33/120    avg_loss:0.088, val_acc:0.970]
Epoch [34/120    avg_loss:0.073, val_acc:0.980]
Epoch [35/120    avg_loss:0.108, val_acc:0.962]
Epoch [36/120    avg_loss:0.105, val_acc:0.964]
Epoch [37/120    avg_loss:0.158, val_acc:0.968]
Epoch [38/120    avg_loss:0.096, val_acc:0.974]
Epoch [39/120    avg_loss:0.099, val_acc:0.970]
Epoch [40/120    avg_loss:0.054, val_acc:0.960]
Epoch [41/120    avg_loss:0.045, val_acc:0.976]
Epoch [42/120    avg_loss:0.031, val_acc:0.982]
Epoch [43/120    avg_loss:0.090, val_acc:0.954]
Epoch [44/120    avg_loss:0.144, val_acc:0.984]
Epoch [45/120    avg_loss:0.062, val_acc:0.952]
Epoch [46/120    avg_loss:0.090, val_acc:0.972]
Epoch [47/120    avg_loss:0.085, val_acc:0.978]
Epoch [48/120    avg_loss:0.040, val_acc:0.982]
Epoch [49/120    avg_loss:0.030, val_acc:0.984]
Epoch [50/120    avg_loss:0.023, val_acc:0.962]
Epoch [51/120    avg_loss:0.030, val_acc:0.984]
Epoch [52/120    avg_loss:0.052, val_acc:0.978]
Epoch [53/120    avg_loss:0.055, val_acc:0.974]
Epoch [54/120    avg_loss:0.024, val_acc:0.984]
Epoch [55/120    avg_loss:0.041, val_acc:0.974]
Epoch [56/120    avg_loss:0.032, val_acc:0.970]
Epoch [57/120    avg_loss:0.039, val_acc:0.958]
Epoch [58/120    avg_loss:0.032, val_acc:0.982]
Epoch [59/120    avg_loss:0.018, val_acc:0.982]
Epoch [60/120    avg_loss:0.027, val_acc:0.976]
Epoch [61/120    avg_loss:0.013, val_acc:0.984]
Epoch [62/120    avg_loss:0.094, val_acc:0.984]
Epoch [63/120    avg_loss:0.070, val_acc:0.984]
Epoch [64/120    avg_loss:0.022, val_acc:0.984]
Epoch [65/120    avg_loss:0.013, val_acc:0.978]
Epoch [66/120    avg_loss:0.027, val_acc:0.972]
Epoch [67/120    avg_loss:0.040, val_acc:0.986]
Epoch [68/120    avg_loss:0.017, val_acc:0.990]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.028, val_acc:0.980]
Epoch [71/120    avg_loss:0.014, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.982]
Epoch [73/120    avg_loss:0.023, val_acc:0.986]
Epoch [74/120    avg_loss:0.026, val_acc:0.988]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.036, val_acc:0.982]
Epoch [77/120    avg_loss:0.064, val_acc:0.966]
Epoch [78/120    avg_loss:0.017, val_acc:0.988]
Epoch [79/120    avg_loss:0.019, val_acc:0.992]
Epoch [80/120    avg_loss:0.017, val_acc:0.984]
Epoch [81/120    avg_loss:0.010, val_acc:0.980]
Epoch [82/120    avg_loss:0.012, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.016, val_acc:0.990]
Epoch [85/120    avg_loss:0.040, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.023, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.003, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   9   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 217   5   0   0   0   0   0   0   5   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         1.         0.97550111 0.92144374 0.91696751
 1.         1.         1.         0.9978678  1.         1.
 0.99451153 1.        ]

Kappa:
0.9907408836986451
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fadec605748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.859, val_acc:0.645]
Epoch [2/120    avg_loss:1.222, val_acc:0.706]
Epoch [3/120    avg_loss:0.956, val_acc:0.796]
Epoch [4/120    avg_loss:0.778, val_acc:0.738]
Epoch [5/120    avg_loss:0.798, val_acc:0.790]
Epoch [6/120    avg_loss:0.691, val_acc:0.786]
Epoch [7/120    avg_loss:0.692, val_acc:0.812]
Epoch [8/120    avg_loss:0.629, val_acc:0.784]
Epoch [9/120    avg_loss:0.671, val_acc:0.796]
Epoch [10/120    avg_loss:0.570, val_acc:0.847]
Epoch [11/120    avg_loss:0.543, val_acc:0.857]
Epoch [12/120    avg_loss:0.458, val_acc:0.867]
Epoch [13/120    avg_loss:0.393, val_acc:0.817]
Epoch [14/120    avg_loss:0.523, val_acc:0.853]
Epoch [15/120    avg_loss:0.447, val_acc:0.869]
Epoch [16/120    avg_loss:0.492, val_acc:0.827]
Epoch [17/120    avg_loss:0.390, val_acc:0.891]
Epoch [18/120    avg_loss:0.388, val_acc:0.885]
Epoch [19/120    avg_loss:0.384, val_acc:0.879]
Epoch [20/120    avg_loss:0.328, val_acc:0.907]
Epoch [21/120    avg_loss:0.270, val_acc:0.933]
Epoch [22/120    avg_loss:0.455, val_acc:0.905]
Epoch [23/120    avg_loss:0.321, val_acc:0.927]
Epoch [24/120    avg_loss:0.248, val_acc:0.923]
Epoch [25/120    avg_loss:0.265, val_acc:0.911]
Epoch [26/120    avg_loss:0.170, val_acc:0.913]
Epoch [27/120    avg_loss:0.196, val_acc:0.940]
Epoch [28/120    avg_loss:0.272, val_acc:0.919]
Epoch [29/120    avg_loss:0.256, val_acc:0.954]
Epoch [30/120    avg_loss:0.193, val_acc:0.942]
Epoch [31/120    avg_loss:0.309, val_acc:0.891]
Epoch [32/120    avg_loss:0.190, val_acc:0.938]
Epoch [33/120    avg_loss:0.172, val_acc:0.923]
Epoch [34/120    avg_loss:0.134, val_acc:0.962]
Epoch [35/120    avg_loss:0.256, val_acc:0.907]
Epoch [36/120    avg_loss:0.249, val_acc:0.950]
Epoch [37/120    avg_loss:0.124, val_acc:0.940]
Epoch [38/120    avg_loss:0.091, val_acc:0.944]
Epoch [39/120    avg_loss:0.142, val_acc:0.956]
Epoch [40/120    avg_loss:0.138, val_acc:0.925]
Epoch [41/120    avg_loss:0.199, val_acc:0.962]
Epoch [42/120    avg_loss:0.089, val_acc:0.952]
Epoch [43/120    avg_loss:0.089, val_acc:0.942]
Epoch [44/120    avg_loss:0.079, val_acc:0.935]
Epoch [45/120    avg_loss:0.130, val_acc:0.929]
Epoch [46/120    avg_loss:0.216, val_acc:0.942]
Epoch [47/120    avg_loss:0.125, val_acc:0.978]
Epoch [48/120    avg_loss:0.070, val_acc:0.976]
Epoch [49/120    avg_loss:0.084, val_acc:0.968]
Epoch [50/120    avg_loss:0.112, val_acc:0.976]
Epoch [51/120    avg_loss:0.063, val_acc:0.968]
Epoch [52/120    avg_loss:0.061, val_acc:0.972]
Epoch [53/120    avg_loss:0.123, val_acc:0.970]
Epoch [54/120    avg_loss:0.064, val_acc:0.974]
Epoch [55/120    avg_loss:0.067, val_acc:0.974]
Epoch [56/120    avg_loss:0.099, val_acc:0.958]
Epoch [57/120    avg_loss:0.068, val_acc:0.984]
Epoch [58/120    avg_loss:0.052, val_acc:0.970]
Epoch [59/120    avg_loss:0.066, val_acc:0.980]
Epoch [60/120    avg_loss:0.064, val_acc:0.962]
Epoch [61/120    avg_loss:0.059, val_acc:0.978]
Epoch [62/120    avg_loss:0.143, val_acc:0.952]
Epoch [63/120    avg_loss:0.046, val_acc:0.982]
Epoch [64/120    avg_loss:0.040, val_acc:0.944]
Epoch [65/120    avg_loss:0.043, val_acc:0.974]
Epoch [66/120    avg_loss:0.063, val_acc:0.974]
Epoch [67/120    avg_loss:0.034, val_acc:0.982]
Epoch [68/120    avg_loss:0.163, val_acc:0.952]
Epoch [69/120    avg_loss:0.098, val_acc:0.974]
Epoch [70/120    avg_loss:0.075, val_acc:0.988]
Epoch [71/120    avg_loss:0.026, val_acc:0.980]
Epoch [72/120    avg_loss:0.049, val_acc:0.935]
Epoch [73/120    avg_loss:0.025, val_acc:0.984]
Epoch [74/120    avg_loss:0.019, val_acc:0.990]
Epoch [75/120    avg_loss:0.052, val_acc:0.974]
Epoch [76/120    avg_loss:0.045, val_acc:0.982]
Epoch [77/120    avg_loss:0.057, val_acc:0.974]
Epoch [78/120    avg_loss:0.026, val_acc:0.976]
Epoch [79/120    avg_loss:0.021, val_acc:0.986]
Epoch [80/120    avg_loss:0.014, val_acc:0.990]
Epoch [81/120    avg_loss:0.015, val_acc:0.994]
Epoch [82/120    avg_loss:0.013, val_acc:0.992]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.016, val_acc:0.986]
Epoch [85/120    avg_loss:0.110, val_acc:0.986]
Epoch [86/120    avg_loss:0.058, val_acc:0.974]
Epoch [87/120    avg_loss:0.039, val_acc:0.982]
Epoch [88/120    avg_loss:0.013, val_acc:0.978]
Epoch [89/120    avg_loss:0.021, val_acc:0.984]
Epoch [90/120    avg_loss:0.031, val_acc:0.964]
Epoch [91/120    avg_loss:0.027, val_acc:0.992]
Epoch [92/120    avg_loss:0.018, val_acc:0.996]
Epoch [93/120    avg_loss:0.008, val_acc:0.996]
Epoch [94/120    avg_loss:0.011, val_acc:0.994]
Epoch [95/120    avg_loss:0.019, val_acc:0.984]
Epoch [96/120    avg_loss:0.045, val_acc:0.978]
Epoch [97/120    avg_loss:0.047, val_acc:0.978]
Epoch [98/120    avg_loss:0.038, val_acc:0.980]
Epoch [99/120    avg_loss:0.036, val_acc:0.984]
Epoch [100/120    avg_loss:0.025, val_acc:0.984]
Epoch [101/120    avg_loss:0.024, val_acc:0.984]
Epoch [102/120    avg_loss:0.026, val_acc:0.976]
Epoch [103/120    avg_loss:0.030, val_acc:0.968]
Epoch [104/120    avg_loss:0.069, val_acc:0.980]
Epoch [105/120    avg_loss:0.021, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.018, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.013, val_acc:0.988]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   4   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 218  11   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 209  13   0   0   0   0   0   0   5   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 0.99707174 0.99310345 0.97321429 0.92477876 0.92715232
 1.         0.98429319 0.998713   1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.9900309557599749
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdbb9f6f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.817, val_acc:0.635]
Epoch [2/120    avg_loss:1.139, val_acc:0.673]
Epoch [3/120    avg_loss:1.097, val_acc:0.774]
Epoch [4/120    avg_loss:0.866, val_acc:0.704]
Epoch [5/120    avg_loss:0.806, val_acc:0.748]
Epoch [6/120    avg_loss:0.754, val_acc:0.796]
Epoch [7/120    avg_loss:0.685, val_acc:0.843]
Epoch [8/120    avg_loss:0.655, val_acc:0.847]
Epoch [9/120    avg_loss:0.544, val_acc:0.806]
Epoch [10/120    avg_loss:0.586, val_acc:0.839]
Epoch [11/120    avg_loss:0.529, val_acc:0.843]
Epoch [12/120    avg_loss:0.493, val_acc:0.861]
Epoch [13/120    avg_loss:0.493, val_acc:0.887]
Epoch [14/120    avg_loss:0.385, val_acc:0.855]
Epoch [15/120    avg_loss:0.405, val_acc:0.901]
Epoch [16/120    avg_loss:0.316, val_acc:0.891]
Epoch [17/120    avg_loss:0.314, val_acc:0.903]
Epoch [18/120    avg_loss:0.313, val_acc:0.901]
Epoch [19/120    avg_loss:0.277, val_acc:0.885]
Epoch [20/120    avg_loss:0.273, val_acc:0.929]
Epoch [21/120    avg_loss:0.235, val_acc:0.901]
Epoch [22/120    avg_loss:0.306, val_acc:0.889]
Epoch [23/120    avg_loss:0.284, val_acc:0.923]
Epoch [24/120    avg_loss:0.161, val_acc:0.911]
Epoch [25/120    avg_loss:0.203, val_acc:0.952]
Epoch [26/120    avg_loss:0.159, val_acc:0.925]
Epoch [27/120    avg_loss:0.232, val_acc:0.938]
Epoch [28/120    avg_loss:0.131, val_acc:0.897]
Epoch [29/120    avg_loss:0.220, val_acc:0.919]
Epoch [30/120    avg_loss:0.199, val_acc:0.938]
Epoch [31/120    avg_loss:0.125, val_acc:0.948]
Epoch [32/120    avg_loss:0.103, val_acc:0.927]
Epoch [33/120    avg_loss:0.163, val_acc:0.952]
Epoch [34/120    avg_loss:0.102, val_acc:0.927]
Epoch [35/120    avg_loss:0.111, val_acc:0.958]
Epoch [36/120    avg_loss:0.112, val_acc:0.933]
Epoch [37/120    avg_loss:0.087, val_acc:0.923]
Epoch [38/120    avg_loss:0.136, val_acc:0.952]
Epoch [39/120    avg_loss:0.125, val_acc:0.944]
Epoch [40/120    avg_loss:0.114, val_acc:0.958]
Epoch [41/120    avg_loss:0.090, val_acc:0.962]
Epoch [42/120    avg_loss:0.070, val_acc:0.960]
Epoch [43/120    avg_loss:0.063, val_acc:0.952]
Epoch [44/120    avg_loss:0.082, val_acc:0.946]
Epoch [45/120    avg_loss:0.060, val_acc:0.956]
Epoch [46/120    avg_loss:0.050, val_acc:0.960]
Epoch [47/120    avg_loss:0.087, val_acc:0.958]
Epoch [48/120    avg_loss:0.035, val_acc:0.958]
Epoch [49/120    avg_loss:0.042, val_acc:0.964]
Epoch [50/120    avg_loss:0.040, val_acc:0.956]
Epoch [51/120    avg_loss:0.039, val_acc:0.974]
Epoch [52/120    avg_loss:0.020, val_acc:0.974]
Epoch [53/120    avg_loss:0.079, val_acc:0.964]
Epoch [54/120    avg_loss:0.094, val_acc:0.958]
Epoch [55/120    avg_loss:0.034, val_acc:0.974]
Epoch [56/120    avg_loss:0.015, val_acc:0.960]
Epoch [57/120    avg_loss:0.037, val_acc:0.970]
Epoch [58/120    avg_loss:0.028, val_acc:0.968]
Epoch [59/120    avg_loss:0.023, val_acc:0.974]
Epoch [60/120    avg_loss:0.037, val_acc:0.956]
Epoch [61/120    avg_loss:0.040, val_acc:0.938]
Epoch [62/120    avg_loss:0.117, val_acc:0.968]
Epoch [63/120    avg_loss:0.041, val_acc:0.944]
Epoch [64/120    avg_loss:0.133, val_acc:0.899]
Epoch [65/120    avg_loss:0.119, val_acc:0.950]
Epoch [66/120    avg_loss:0.037, val_acc:0.974]
Epoch [67/120    avg_loss:0.073, val_acc:0.966]
Epoch [68/120    avg_loss:0.074, val_acc:0.968]
Epoch [69/120    avg_loss:0.075, val_acc:0.956]
Epoch [70/120    avg_loss:0.039, val_acc:0.970]
Epoch [71/120    avg_loss:0.045, val_acc:0.946]
Epoch [72/120    avg_loss:0.060, val_acc:0.972]
Epoch [73/120    avg_loss:0.026, val_acc:0.970]
Epoch [74/120    avg_loss:0.063, val_acc:0.944]
Epoch [75/120    avg_loss:0.033, val_acc:0.972]
Epoch [76/120    avg_loss:0.012, val_acc:0.970]
Epoch [77/120    avg_loss:0.023, val_acc:0.968]
Epoch [78/120    avg_loss:0.019, val_acc:0.962]
Epoch [79/120    avg_loss:0.027, val_acc:0.970]
Epoch [80/120    avg_loss:0.016, val_acc:0.972]
Epoch [81/120    avg_loss:0.014, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.978]
Epoch [83/120    avg_loss:0.006, val_acc:0.978]
Epoch [84/120    avg_loss:0.011, val_acc:0.978]
Epoch [85/120    avg_loss:0.010, val_acc:0.980]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.010, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.009, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.019, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.980]
Epoch [102/120    avg_loss:0.008, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.980]
Epoch [104/120    avg_loss:0.005, val_acc:0.980]
Epoch [105/120    avg_loss:0.012, val_acc:0.980]
Epoch [106/120    avg_loss:0.007, val_acc:0.980]
Epoch [107/120    avg_loss:0.004, val_acc:0.980]
Epoch [108/120    avg_loss:0.019, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.030, val_acc:0.978]
Epoch [113/120    avg_loss:0.006, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.004, val_acc:0.980]
Epoch [118/120    avg_loss:0.004, val_acc:0.980]
Epoch [119/120    avg_loss:0.004, val_acc:0.980]
Epoch [120/120    avg_loss:0.004, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   9   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 213  17   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   3   0   0   0   0   0   0   3   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99338722 0.99078341 0.96162528 0.92662474 0.91724138
 1.         0.97916667 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9886079936846021
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f434b662710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.873, val_acc:0.656]
Epoch [2/120    avg_loss:1.132, val_acc:0.681]
Epoch [3/120    avg_loss:1.074, val_acc:0.675]
Epoch [4/120    avg_loss:1.001, val_acc:0.746]
Epoch [5/120    avg_loss:0.840, val_acc:0.829]
Epoch [6/120    avg_loss:0.808, val_acc:0.781]
Epoch [7/120    avg_loss:0.743, val_acc:0.856]
Epoch [8/120    avg_loss:0.627, val_acc:0.806]
Epoch [9/120    avg_loss:0.485, val_acc:0.850]
Epoch [10/120    avg_loss:0.567, val_acc:0.850]
Epoch [11/120    avg_loss:0.606, val_acc:0.867]
Epoch [12/120    avg_loss:0.486, val_acc:0.860]
Epoch [13/120    avg_loss:0.531, val_acc:0.898]
Epoch [14/120    avg_loss:0.488, val_acc:0.858]
Epoch [15/120    avg_loss:0.539, val_acc:0.902]
Epoch [16/120    avg_loss:0.385, val_acc:0.904]
Epoch [17/120    avg_loss:0.292, val_acc:0.929]
Epoch [18/120    avg_loss:0.395, val_acc:0.896]
Epoch [19/120    avg_loss:0.353, val_acc:0.919]
Epoch [20/120    avg_loss:0.357, val_acc:0.912]
Epoch [21/120    avg_loss:0.304, val_acc:0.919]
Epoch [22/120    avg_loss:0.424, val_acc:0.915]
Epoch [23/120    avg_loss:0.318, val_acc:0.921]
Epoch [24/120    avg_loss:0.290, val_acc:0.923]
Epoch [25/120    avg_loss:0.221, val_acc:0.940]
Epoch [26/120    avg_loss:0.287, val_acc:0.917]
Epoch [27/120    avg_loss:0.273, val_acc:0.938]
Epoch [28/120    avg_loss:0.253, val_acc:0.933]
Epoch [29/120    avg_loss:0.200, val_acc:0.935]
Epoch [30/120    avg_loss:0.242, val_acc:0.929]
Epoch [31/120    avg_loss:0.227, val_acc:0.940]
Epoch [32/120    avg_loss:0.117, val_acc:0.960]
Epoch [33/120    avg_loss:0.141, val_acc:0.952]
Epoch [34/120    avg_loss:0.124, val_acc:0.969]
Epoch [35/120    avg_loss:0.160, val_acc:0.958]
Epoch [36/120    avg_loss:0.102, val_acc:0.975]
Epoch [37/120    avg_loss:0.142, val_acc:0.954]
Epoch [38/120    avg_loss:0.160, val_acc:0.956]
Epoch [39/120    avg_loss:0.174, val_acc:0.946]
Epoch [40/120    avg_loss:0.100, val_acc:0.977]
Epoch [41/120    avg_loss:0.098, val_acc:0.927]
Epoch [42/120    avg_loss:0.073, val_acc:0.973]
Epoch [43/120    avg_loss:0.049, val_acc:0.988]
Epoch [44/120    avg_loss:0.067, val_acc:0.979]
Epoch [45/120    avg_loss:0.109, val_acc:0.969]
Epoch [46/120    avg_loss:0.078, val_acc:0.975]
Epoch [47/120    avg_loss:0.075, val_acc:0.971]
Epoch [48/120    avg_loss:0.097, val_acc:0.981]
Epoch [49/120    avg_loss:0.136, val_acc:0.971]
Epoch [50/120    avg_loss:0.143, val_acc:0.935]
Epoch [51/120    avg_loss:0.163, val_acc:0.971]
Epoch [52/120    avg_loss:0.063, val_acc:0.981]
Epoch [53/120    avg_loss:0.088, val_acc:0.960]
Epoch [54/120    avg_loss:0.046, val_acc:0.983]
Epoch [55/120    avg_loss:0.040, val_acc:0.988]
Epoch [56/120    avg_loss:0.073, val_acc:0.973]
Epoch [57/120    avg_loss:0.100, val_acc:0.973]
Epoch [58/120    avg_loss:0.158, val_acc:0.960]
Epoch [59/120    avg_loss:0.083, val_acc:0.958]
Epoch [60/120    avg_loss:0.032, val_acc:0.981]
Epoch [61/120    avg_loss:0.030, val_acc:0.990]
Epoch [62/120    avg_loss:0.031, val_acc:0.983]
Epoch [63/120    avg_loss:0.027, val_acc:0.996]
Epoch [64/120    avg_loss:0.032, val_acc:0.977]
Epoch [65/120    avg_loss:0.043, val_acc:0.979]
Epoch [66/120    avg_loss:0.029, val_acc:0.981]
Epoch [67/120    avg_loss:0.077, val_acc:0.975]
Epoch [68/120    avg_loss:0.076, val_acc:0.971]
Epoch [69/120    avg_loss:0.051, val_acc:0.985]
Epoch [70/120    avg_loss:0.020, val_acc:0.992]
Epoch [71/120    avg_loss:0.072, val_acc:0.963]
Epoch [72/120    avg_loss:0.127, val_acc:0.946]
Epoch [73/120    avg_loss:0.067, val_acc:0.979]
Epoch [74/120    avg_loss:0.025, val_acc:0.990]
Epoch [75/120    avg_loss:0.021, val_acc:0.983]
Epoch [76/120    avg_loss:0.027, val_acc:0.983]
Epoch [77/120    avg_loss:0.044, val_acc:0.985]
Epoch [78/120    avg_loss:0.027, val_acc:0.990]
Epoch [79/120    avg_loss:0.020, val_acc:0.994]
Epoch [80/120    avg_loss:0.017, val_acc:0.996]
Epoch [81/120    avg_loss:0.025, val_acc:0.996]
Epoch [82/120    avg_loss:0.010, val_acc:0.996]
Epoch [83/120    avg_loss:0.020, val_acc:0.996]
Epoch [84/120    avg_loss:0.023, val_acc:0.996]
Epoch [85/120    avg_loss:0.025, val_acc:0.996]
Epoch [86/120    avg_loss:0.025, val_acc:0.996]
Epoch [87/120    avg_loss:0.018, val_acc:0.996]
Epoch [88/120    avg_loss:0.012, val_acc:0.996]
Epoch [89/120    avg_loss:0.038, val_acc:0.996]
Epoch [90/120    avg_loss:0.012, val_acc:0.996]
Epoch [91/120    avg_loss:0.012, val_acc:0.996]
Epoch [92/120    avg_loss:0.011, val_acc:0.996]
Epoch [93/120    avg_loss:0.021, val_acc:0.994]
Epoch [94/120    avg_loss:0.022, val_acc:0.994]
Epoch [95/120    avg_loss:0.012, val_acc:0.996]
Epoch [96/120    avg_loss:0.022, val_acc:0.996]
Epoch [97/120    avg_loss:0.037, val_acc:0.996]
Epoch [98/120    avg_loss:0.011, val_acc:0.994]
Epoch [99/120    avg_loss:0.015, val_acc:0.994]
Epoch [100/120    avg_loss:0.015, val_acc:0.996]
Epoch [101/120    avg_loss:0.009, val_acc:0.996]
Epoch [102/120    avg_loss:0.016, val_acc:0.998]
Epoch [103/120    avg_loss:0.012, val_acc:0.994]
Epoch [104/120    avg_loss:0.010, val_acc:0.994]
Epoch [105/120    avg_loss:0.020, val_acc:0.996]
Epoch [106/120    avg_loss:0.022, val_acc:0.996]
Epoch [107/120    avg_loss:0.011, val_acc:0.994]
Epoch [108/120    avg_loss:0.010, val_acc:0.994]
Epoch [109/120    avg_loss:0.009, val_acc:0.994]
Epoch [110/120    avg_loss:0.008, val_acc:0.996]
Epoch [111/120    avg_loss:0.013, val_acc:0.996]
Epoch [112/120    avg_loss:0.026, val_acc:0.996]
Epoch [113/120    avg_loss:0.012, val_acc:0.996]
Epoch [114/120    avg_loss:0.011, val_acc:0.996]
Epoch [115/120    avg_loss:0.009, val_acc:0.996]
Epoch [116/120    avg_loss:0.011, val_acc:0.996]
Epoch [117/120    avg_loss:0.007, val_acc:0.996]
Epoch [118/120    avg_loss:0.007, val_acc:0.996]
Epoch [119/120    avg_loss:0.006, val_acc:0.996]
Epoch [120/120    avg_loss:0.013, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   6   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   7   0   0   0   0   0   0   0   0   0 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 0.99560117 0.97272727 1.         0.96760259 0.93006993
 1.         0.97409326 1.         1.         1.         1.
 0.99111111 1.        ]

Kappa:
0.9921681787592325
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8b342126d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.836, val_acc:0.504]
Epoch [2/120    avg_loss:1.233, val_acc:0.676]
Epoch [3/120    avg_loss:1.105, val_acc:0.756]
Epoch [4/120    avg_loss:0.892, val_acc:0.707]
Epoch [5/120    avg_loss:0.863, val_acc:0.797]
Epoch [6/120    avg_loss:0.800, val_acc:0.826]
Epoch [7/120    avg_loss:0.727, val_acc:0.830]
Epoch [8/120    avg_loss:0.611, val_acc:0.861]
Epoch [9/120    avg_loss:0.576, val_acc:0.857]
Epoch [10/120    avg_loss:0.569, val_acc:0.838]
Epoch [11/120    avg_loss:0.504, val_acc:0.869]
Epoch [12/120    avg_loss:0.416, val_acc:0.902]
Epoch [13/120    avg_loss:0.416, val_acc:0.873]
Epoch [14/120    avg_loss:0.421, val_acc:0.902]
Epoch [15/120    avg_loss:0.390, val_acc:0.916]
Epoch [16/120    avg_loss:0.365, val_acc:0.877]
Epoch [17/120    avg_loss:0.371, val_acc:0.926]
Epoch [18/120    avg_loss:0.285, val_acc:0.945]
Epoch [19/120    avg_loss:0.198, val_acc:0.943]
Epoch [20/120    avg_loss:0.257, val_acc:0.943]
Epoch [21/120    avg_loss:0.282, val_acc:0.918]
Epoch [22/120    avg_loss:0.284, val_acc:0.953]
Epoch [23/120    avg_loss:0.166, val_acc:0.951]
Epoch [24/120    avg_loss:0.242, val_acc:0.924]
Epoch [25/120    avg_loss:0.215, val_acc:0.965]
Epoch [26/120    avg_loss:0.197, val_acc:0.963]
Epoch [27/120    avg_loss:0.187, val_acc:0.969]
Epoch [28/120    avg_loss:0.147, val_acc:0.975]
Epoch [29/120    avg_loss:0.120, val_acc:0.975]
Epoch [30/120    avg_loss:0.104, val_acc:0.932]
Epoch [31/120    avg_loss:0.230, val_acc:0.969]
Epoch [32/120    avg_loss:0.093, val_acc:0.961]
Epoch [33/120    avg_loss:0.078, val_acc:0.971]
Epoch [34/120    avg_loss:0.141, val_acc:0.980]
Epoch [35/120    avg_loss:0.104, val_acc:0.961]
Epoch [36/120    avg_loss:0.082, val_acc:0.984]
Epoch [37/120    avg_loss:0.108, val_acc:0.986]
Epoch [38/120    avg_loss:0.052, val_acc:0.982]
Epoch [39/120    avg_loss:0.122, val_acc:0.982]
Epoch [40/120    avg_loss:0.059, val_acc:0.988]
Epoch [41/120    avg_loss:0.031, val_acc:0.986]
Epoch [42/120    avg_loss:0.036, val_acc:0.986]
Epoch [43/120    avg_loss:0.037, val_acc:0.992]
Epoch [44/120    avg_loss:0.028, val_acc:0.990]
Epoch [45/120    avg_loss:0.033, val_acc:0.996]
Epoch [46/120    avg_loss:0.057, val_acc:0.982]
Epoch [47/120    avg_loss:0.066, val_acc:0.975]
Epoch [48/120    avg_loss:0.049, val_acc:0.975]
Epoch [49/120    avg_loss:0.078, val_acc:0.980]
Epoch [50/120    avg_loss:0.040, val_acc:0.982]
Epoch [51/120    avg_loss:0.037, val_acc:0.990]
Epoch [52/120    avg_loss:0.063, val_acc:0.990]
Epoch [53/120    avg_loss:0.032, val_acc:0.988]
Epoch [54/120    avg_loss:0.031, val_acc:0.996]
Epoch [55/120    avg_loss:0.029, val_acc:0.994]
Epoch [56/120    avg_loss:0.037, val_acc:0.963]
Epoch [57/120    avg_loss:0.035, val_acc:0.988]
Epoch [58/120    avg_loss:0.021, val_acc:0.988]
Epoch [59/120    avg_loss:0.006, val_acc:0.990]
Epoch [60/120    avg_loss:0.013, val_acc:0.992]
Epoch [61/120    avg_loss:0.010, val_acc:0.996]
Epoch [62/120    avg_loss:0.014, val_acc:0.994]
Epoch [63/120    avg_loss:0.008, val_acc:0.996]
Epoch [64/120    avg_loss:0.008, val_acc:0.994]
Epoch [65/120    avg_loss:0.022, val_acc:0.986]
Epoch [66/120    avg_loss:0.018, val_acc:0.992]
Epoch [67/120    avg_loss:0.012, val_acc:0.990]
Epoch [68/120    avg_loss:0.057, val_acc:0.990]
Epoch [69/120    avg_loss:0.013, val_acc:0.996]
Epoch [70/120    avg_loss:0.037, val_acc:0.994]
Epoch [71/120    avg_loss:0.030, val_acc:0.990]
Epoch [72/120    avg_loss:0.009, val_acc:0.992]
Epoch [73/120    avg_loss:0.016, val_acc:0.990]
Epoch [74/120    avg_loss:0.009, val_acc:0.998]
Epoch [75/120    avg_loss:0.012, val_acc:0.998]
Epoch [76/120    avg_loss:0.009, val_acc:0.998]
Epoch [77/120    avg_loss:0.008, val_acc:0.992]
Epoch [78/120    avg_loss:0.006, val_acc:0.992]
Epoch [79/120    avg_loss:0.016, val_acc:0.984]
Epoch [80/120    avg_loss:0.012, val_acc:0.998]
Epoch [81/120    avg_loss:0.007, val_acc:0.998]
Epoch [82/120    avg_loss:0.055, val_acc:0.957]
Epoch [83/120    avg_loss:0.093, val_acc:0.975]
Epoch [84/120    avg_loss:0.042, val_acc:0.994]
Epoch [85/120    avg_loss:0.025, val_acc:0.992]
Epoch [86/120    avg_loss:0.014, val_acc:0.992]
Epoch [87/120    avg_loss:0.013, val_acc:0.994]
Epoch [88/120    avg_loss:0.069, val_acc:0.986]
Epoch [89/120    avg_loss:0.051, val_acc:0.982]
Epoch [90/120    avg_loss:0.030, val_acc:0.994]
Epoch [91/120    avg_loss:0.019, val_acc:0.998]
Epoch [92/120    avg_loss:0.014, val_acc:0.994]
Epoch [93/120    avg_loss:0.005, val_acc:0.996]
Epoch [94/120    avg_loss:0.014, val_acc:0.988]
Epoch [95/120    avg_loss:0.010, val_acc:0.994]
Epoch [96/120    avg_loss:0.042, val_acc:0.986]
Epoch [97/120    avg_loss:0.033, val_acc:0.992]
Epoch [98/120    avg_loss:0.027, val_acc:0.994]
Epoch [99/120    avg_loss:0.010, val_acc:0.994]
Epoch [100/120    avg_loss:0.009, val_acc:0.998]
Epoch [101/120    avg_loss:0.012, val_acc:0.998]
Epoch [102/120    avg_loss:0.005, val_acc:0.998]
Epoch [103/120    avg_loss:0.010, val_acc:0.998]
Epoch [104/120    avg_loss:0.033, val_acc:0.998]
Epoch [105/120    avg_loss:0.012, val_acc:1.000]
Epoch [106/120    avg_loss:0.007, val_acc:1.000]
Epoch [107/120    avg_loss:0.005, val_acc:0.998]
Epoch [108/120    avg_loss:0.010, val_acc:0.998]
Epoch [109/120    avg_loss:0.010, val_acc:0.996]
Epoch [110/120    avg_loss:0.008, val_acc:1.000]
Epoch [111/120    avg_loss:0.005, val_acc:0.998]
Epoch [112/120    avg_loss:0.018, val_acc:0.998]
Epoch [113/120    avg_loss:0.007, val_acc:0.998]
Epoch [114/120    avg_loss:0.004, val_acc:1.000]
Epoch [115/120    avg_loss:0.006, val_acc:0.998]
Epoch [116/120    avg_loss:0.003, val_acc:1.000]
Epoch [117/120    avg_loss:0.002, val_acc:1.000]
Epoch [118/120    avg_loss:0.004, val_acc:1.000]
Epoch [119/120    avg_loss:0.003, val_acc:0.998]
Epoch [120/120    avg_loss:0.004, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   2   0   0   0   0   0   0   4   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         1.         1.         0.96086957 0.95
 1.         1.         0.998713   1.         1.         1.
 0.99449945 1.        ]

Kappa:
0.9954893736050454
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4a6fce780>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.892, val_acc:0.625]
Epoch [2/120    avg_loss:1.262, val_acc:0.703]
Epoch [3/120    avg_loss:1.068, val_acc:0.713]
Epoch [4/120    avg_loss:0.978, val_acc:0.811]
Epoch [5/120    avg_loss:0.837, val_acc:0.793]
Epoch [6/120    avg_loss:0.735, val_acc:0.820]
Epoch [7/120    avg_loss:0.648, val_acc:0.826]
Epoch [8/120    avg_loss:0.651, val_acc:0.855]
Epoch [9/120    avg_loss:0.666, val_acc:0.805]
Epoch [10/120    avg_loss:0.477, val_acc:0.852]
Epoch [11/120    avg_loss:0.533, val_acc:0.881]
Epoch [12/120    avg_loss:0.426, val_acc:0.873]
Epoch [13/120    avg_loss:0.452, val_acc:0.873]
Epoch [14/120    avg_loss:0.364, val_acc:0.895]
Epoch [15/120    avg_loss:0.507, val_acc:0.873]
Epoch [16/120    avg_loss:0.438, val_acc:0.898]
Epoch [17/120    avg_loss:0.355, val_acc:0.881]
Epoch [18/120    avg_loss:0.317, val_acc:0.908]
Epoch [19/120    avg_loss:0.278, val_acc:0.904]
Epoch [20/120    avg_loss:0.236, val_acc:0.906]
Epoch [21/120    avg_loss:0.319, val_acc:0.906]
Epoch [22/120    avg_loss:0.266, val_acc:0.941]
Epoch [23/120    avg_loss:0.248, val_acc:0.945]
Epoch [24/120    avg_loss:0.209, val_acc:0.932]
Epoch [25/120    avg_loss:0.215, val_acc:0.922]
Epoch [26/120    avg_loss:0.217, val_acc:0.947]
Epoch [27/120    avg_loss:0.167, val_acc:0.951]
Epoch [28/120    avg_loss:0.193, val_acc:0.959]
Epoch [29/120    avg_loss:0.184, val_acc:0.939]
Epoch [30/120    avg_loss:0.183, val_acc:0.939]
Epoch [31/120    avg_loss:0.175, val_acc:0.939]
Epoch [32/120    avg_loss:0.129, val_acc:0.941]
Epoch [33/120    avg_loss:0.165, val_acc:0.949]
Epoch [34/120    avg_loss:0.181, val_acc:0.936]
Epoch [35/120    avg_loss:0.115, val_acc:0.945]
Epoch [36/120    avg_loss:0.149, val_acc:0.967]
Epoch [37/120    avg_loss:0.109, val_acc:0.957]
Epoch [38/120    avg_loss:0.078, val_acc:0.967]
Epoch [39/120    avg_loss:0.091, val_acc:0.961]
Epoch [40/120    avg_loss:0.077, val_acc:0.971]
Epoch [41/120    avg_loss:0.125, val_acc:0.949]
Epoch [42/120    avg_loss:0.070, val_acc:0.965]
Epoch [43/120    avg_loss:0.073, val_acc:0.973]
Epoch [44/120    avg_loss:0.072, val_acc:0.975]
Epoch [45/120    avg_loss:0.126, val_acc:0.973]
Epoch [46/120    avg_loss:0.095, val_acc:0.965]
Epoch [47/120    avg_loss:0.174, val_acc:0.949]
Epoch [48/120    avg_loss:0.179, val_acc:0.951]
Epoch [49/120    avg_loss:0.081, val_acc:0.977]
Epoch [50/120    avg_loss:0.068, val_acc:0.980]
Epoch [51/120    avg_loss:0.110, val_acc:0.975]
Epoch [52/120    avg_loss:0.130, val_acc:0.975]
Epoch [53/120    avg_loss:0.049, val_acc:0.977]
Epoch [54/120    avg_loss:0.038, val_acc:0.982]
Epoch [55/120    avg_loss:0.106, val_acc:0.967]
Epoch [56/120    avg_loss:0.032, val_acc:0.982]
Epoch [57/120    avg_loss:0.058, val_acc:0.963]
Epoch [58/120    avg_loss:0.157, val_acc:0.971]
Epoch [59/120    avg_loss:0.044, val_acc:0.975]
Epoch [60/120    avg_loss:0.046, val_acc:0.980]
Epoch [61/120    avg_loss:0.048, val_acc:0.980]
Epoch [62/120    avg_loss:0.034, val_acc:0.973]
Epoch [63/120    avg_loss:0.019, val_acc:0.982]
Epoch [64/120    avg_loss:0.017, val_acc:0.988]
Epoch [65/120    avg_loss:0.012, val_acc:0.988]
Epoch [66/120    avg_loss:0.020, val_acc:0.986]
Epoch [67/120    avg_loss:0.022, val_acc:0.988]
Epoch [68/120    avg_loss:0.053, val_acc:0.982]
Epoch [69/120    avg_loss:0.047, val_acc:0.988]
Epoch [70/120    avg_loss:0.025, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.990]
Epoch [72/120    avg_loss:0.024, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.992]
Epoch [77/120    avg_loss:0.012, val_acc:0.992]
Epoch [78/120    avg_loss:0.014, val_acc:0.992]
Epoch [79/120    avg_loss:0.006, val_acc:0.990]
Epoch [80/120    avg_loss:0.007, val_acc:0.992]
Epoch [81/120    avg_loss:0.016, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.984]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.994]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.992]
Epoch [88/120    avg_loss:0.007, val_acc:0.994]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.012, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.992]
Epoch [95/120    avg_loss:0.007, val_acc:0.992]
Epoch [96/120    avg_loss:0.017, val_acc:0.982]
Epoch [97/120    avg_loss:0.029, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.992]
Epoch [101/120    avg_loss:0.015, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.012, val_acc:0.992]
Epoch [105/120    avg_loss:0.004, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.992]
Epoch [108/120    avg_loss:0.010, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.992]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.002, val_acc:0.992]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   7   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215  15   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99486427 1.         0.96629213 0.93723849 0.92682927
 1.         1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9912181214087503
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff97bac26d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.831, val_acc:0.651]
Epoch [2/120    avg_loss:1.285, val_acc:0.734]
Epoch [3/120    avg_loss:1.072, val_acc:0.786]
Epoch [4/120    avg_loss:0.893, val_acc:0.776]
Epoch [5/120    avg_loss:0.827, val_acc:0.835]
Epoch [6/120    avg_loss:0.772, val_acc:0.867]
Epoch [7/120    avg_loss:0.701, val_acc:0.869]
Epoch [8/120    avg_loss:0.615, val_acc:0.837]
Epoch [9/120    avg_loss:0.577, val_acc:0.875]
Epoch [10/120    avg_loss:0.609, val_acc:0.857]
Epoch [11/120    avg_loss:0.431, val_acc:0.913]
Epoch [12/120    avg_loss:0.457, val_acc:0.905]
Epoch [13/120    avg_loss:0.355, val_acc:0.873]
Epoch [14/120    avg_loss:0.396, val_acc:0.841]
Epoch [15/120    avg_loss:0.407, val_acc:0.923]
Epoch [16/120    avg_loss:0.356, val_acc:0.935]
Epoch [17/120    avg_loss:0.278, val_acc:0.929]
Epoch [18/120    avg_loss:0.312, val_acc:0.909]
Epoch [19/120    avg_loss:0.279, val_acc:0.938]
Epoch [20/120    avg_loss:0.247, val_acc:0.933]
Epoch [21/120    avg_loss:0.182, val_acc:0.923]
Epoch [22/120    avg_loss:0.226, val_acc:0.950]
Epoch [23/120    avg_loss:0.390, val_acc:0.921]
Epoch [24/120    avg_loss:0.172, val_acc:0.929]
Epoch [25/120    avg_loss:0.303, val_acc:0.921]
Epoch [26/120    avg_loss:0.221, val_acc:0.956]
Epoch [27/120    avg_loss:0.171, val_acc:0.948]
Epoch [28/120    avg_loss:0.139, val_acc:0.946]
Epoch [29/120    avg_loss:0.153, val_acc:0.950]
Epoch [30/120    avg_loss:0.142, val_acc:0.976]
Epoch [31/120    avg_loss:0.109, val_acc:0.974]
Epoch [32/120    avg_loss:0.101, val_acc:0.966]
Epoch [33/120    avg_loss:0.090, val_acc:0.962]
Epoch [34/120    avg_loss:0.117, val_acc:0.962]
Epoch [35/120    avg_loss:0.104, val_acc:0.966]
Epoch [36/120    avg_loss:0.081, val_acc:0.970]
Epoch [37/120    avg_loss:0.111, val_acc:0.974]
Epoch [38/120    avg_loss:0.081, val_acc:0.972]
Epoch [39/120    avg_loss:0.108, val_acc:0.966]
Epoch [40/120    avg_loss:0.071, val_acc:0.972]
Epoch [41/120    avg_loss:0.071, val_acc:0.982]
Epoch [42/120    avg_loss:0.044, val_acc:0.974]
Epoch [43/120    avg_loss:0.026, val_acc:0.988]
Epoch [44/120    avg_loss:0.021, val_acc:0.980]
Epoch [45/120    avg_loss:0.065, val_acc:0.978]
Epoch [46/120    avg_loss:0.060, val_acc:0.974]
Epoch [47/120    avg_loss:0.023, val_acc:0.978]
Epoch [48/120    avg_loss:0.033, val_acc:0.970]
Epoch [49/120    avg_loss:0.060, val_acc:0.982]
Epoch [50/120    avg_loss:0.045, val_acc:0.980]
Epoch [51/120    avg_loss:0.027, val_acc:0.978]
Epoch [52/120    avg_loss:0.020, val_acc:0.982]
Epoch [53/120    avg_loss:0.028, val_acc:0.976]
Epoch [54/120    avg_loss:0.017, val_acc:0.984]
Epoch [55/120    avg_loss:0.024, val_acc:0.980]
Epoch [56/120    avg_loss:0.045, val_acc:0.976]
Epoch [57/120    avg_loss:0.045, val_acc:0.978]
Epoch [58/120    avg_loss:0.019, val_acc:0.978]
Epoch [59/120    avg_loss:0.021, val_acc:0.980]
Epoch [60/120    avg_loss:0.011, val_acc:0.980]
Epoch [61/120    avg_loss:0.017, val_acc:0.980]
Epoch [62/120    avg_loss:0.015, val_acc:0.980]
Epoch [63/120    avg_loss:0.008, val_acc:0.980]
Epoch [64/120    avg_loss:0.008, val_acc:0.980]
Epoch [65/120    avg_loss:0.023, val_acc:0.980]
Epoch [66/120    avg_loss:0.013, val_acc:0.984]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.986]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.021, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.006, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.013, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.010, val_acc:0.986]
Epoch [87/120    avg_loss:0.017, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.986]
Epoch [89/120    avg_loss:0.014, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.020, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.025, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.022, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.016, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.026, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.020, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.986]
Epoch [109/120    avg_loss:0.019, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.014, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.015, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.012, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   6   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218  12   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   1   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   3   0   0   0   0   0   2 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99560117 1.         0.97321429 0.94537815 0.92361111
 1.         1.         1.         1.         1.         0.9973545
 0.99334812 1.        ]

Kappa:
0.9912183191742456
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72ab5d3710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.813, val_acc:0.609]
Epoch [2/120    avg_loss:1.357, val_acc:0.673]
Epoch [3/120    avg_loss:1.113, val_acc:0.726]
Epoch [4/120    avg_loss:0.912, val_acc:0.746]
Epoch [5/120    avg_loss:0.867, val_acc:0.742]
Epoch [6/120    avg_loss:0.728, val_acc:0.827]
Epoch [7/120    avg_loss:0.596, val_acc:0.829]
Epoch [8/120    avg_loss:0.634, val_acc:0.851]
Epoch [9/120    avg_loss:0.563, val_acc:0.865]
Epoch [10/120    avg_loss:0.549, val_acc:0.885]
Epoch [11/120    avg_loss:0.533, val_acc:0.863]
Epoch [12/120    avg_loss:0.597, val_acc:0.833]
Epoch [13/120    avg_loss:0.536, val_acc:0.895]
Epoch [14/120    avg_loss:0.471, val_acc:0.887]
Epoch [15/120    avg_loss:0.409, val_acc:0.865]
Epoch [16/120    avg_loss:0.410, val_acc:0.861]
Epoch [17/120    avg_loss:0.442, val_acc:0.897]
Epoch [18/120    avg_loss:0.427, val_acc:0.873]
Epoch [19/120    avg_loss:0.429, val_acc:0.861]
Epoch [20/120    avg_loss:0.403, val_acc:0.919]
Epoch [21/120    avg_loss:0.333, val_acc:0.921]
Epoch [22/120    avg_loss:0.358, val_acc:0.929]
Epoch [23/120    avg_loss:0.281, val_acc:0.935]
Epoch [24/120    avg_loss:0.276, val_acc:0.938]
Epoch [25/120    avg_loss:0.291, val_acc:0.911]
Epoch [26/120    avg_loss:0.220, val_acc:0.944]
Epoch [27/120    avg_loss:0.204, val_acc:0.944]
Epoch [28/120    avg_loss:0.154, val_acc:0.952]
Epoch [29/120    avg_loss:0.214, val_acc:0.938]
Epoch [30/120    avg_loss:0.188, val_acc:0.954]
Epoch [31/120    avg_loss:0.173, val_acc:0.954]
Epoch [32/120    avg_loss:0.161, val_acc:0.952]
Epoch [33/120    avg_loss:0.154, val_acc:0.968]
Epoch [34/120    avg_loss:0.182, val_acc:0.950]
Epoch [35/120    avg_loss:0.196, val_acc:0.970]
Epoch [36/120    avg_loss:0.175, val_acc:0.962]
Epoch [37/120    avg_loss:0.105, val_acc:0.964]
Epoch [38/120    avg_loss:0.275, val_acc:0.938]
Epoch [39/120    avg_loss:0.133, val_acc:0.938]
Epoch [40/120    avg_loss:0.195, val_acc:0.954]
Epoch [41/120    avg_loss:0.129, val_acc:0.968]
Epoch [42/120    avg_loss:0.196, val_acc:0.964]
Epoch [43/120    avg_loss:0.140, val_acc:0.972]
Epoch [44/120    avg_loss:0.070, val_acc:0.976]
Epoch [45/120    avg_loss:0.111, val_acc:0.956]
Epoch [46/120    avg_loss:0.109, val_acc:0.960]
Epoch [47/120    avg_loss:0.102, val_acc:0.982]
Epoch [48/120    avg_loss:0.060, val_acc:0.968]
Epoch [49/120    avg_loss:0.053, val_acc:0.984]
Epoch [50/120    avg_loss:0.061, val_acc:0.984]
Epoch [51/120    avg_loss:0.068, val_acc:0.984]
Epoch [52/120    avg_loss:0.124, val_acc:0.954]
Epoch [53/120    avg_loss:0.096, val_acc:0.974]
Epoch [54/120    avg_loss:0.069, val_acc:0.972]
Epoch [55/120    avg_loss:0.081, val_acc:0.968]
Epoch [56/120    avg_loss:0.109, val_acc:0.972]
Epoch [57/120    avg_loss:0.112, val_acc:0.950]
Epoch [58/120    avg_loss:0.070, val_acc:0.978]
Epoch [59/120    avg_loss:0.072, val_acc:0.968]
Epoch [60/120    avg_loss:0.154, val_acc:0.948]
Epoch [61/120    avg_loss:0.106, val_acc:0.970]
Epoch [62/120    avg_loss:0.063, val_acc:0.984]
Epoch [63/120    avg_loss:0.032, val_acc:0.988]
Epoch [64/120    avg_loss:0.045, val_acc:0.984]
Epoch [65/120    avg_loss:0.048, val_acc:0.990]
Epoch [66/120    avg_loss:0.050, val_acc:0.988]
Epoch [67/120    avg_loss:0.044, val_acc:0.976]
Epoch [68/120    avg_loss:0.044, val_acc:0.984]
Epoch [69/120    avg_loss:0.032, val_acc:0.988]
Epoch [70/120    avg_loss:0.035, val_acc:0.988]
Epoch [71/120    avg_loss:0.015, val_acc:0.988]
Epoch [72/120    avg_loss:0.017, val_acc:0.988]
Epoch [73/120    avg_loss:0.031, val_acc:0.988]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.039, val_acc:0.976]
Epoch [76/120    avg_loss:0.034, val_acc:0.990]
Epoch [77/120    avg_loss:0.011, val_acc:0.988]
Epoch [78/120    avg_loss:0.061, val_acc:0.984]
Epoch [79/120    avg_loss:0.035, val_acc:0.990]
Epoch [80/120    avg_loss:0.024, val_acc:0.980]
Epoch [81/120    avg_loss:0.024, val_acc:0.990]
Epoch [82/120    avg_loss:0.038, val_acc:0.978]
Epoch [83/120    avg_loss:0.018, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.988]
Epoch [86/120    avg_loss:0.017, val_acc:0.986]
Epoch [87/120    avg_loss:0.018, val_acc:0.982]
Epoch [88/120    avg_loss:0.024, val_acc:0.984]
Epoch [89/120    avg_loss:0.045, val_acc:0.984]
Epoch [90/120    avg_loss:0.033, val_acc:0.990]
Epoch [91/120    avg_loss:0.018, val_acc:0.992]
Epoch [92/120    avg_loss:0.010, val_acc:0.992]
Epoch [93/120    avg_loss:0.014, val_acc:0.990]
Epoch [94/120    avg_loss:0.020, val_acc:0.980]
Epoch [95/120    avg_loss:0.071, val_acc:0.988]
Epoch [96/120    avg_loss:0.076, val_acc:0.980]
Epoch [97/120    avg_loss:0.043, val_acc:0.988]
Epoch [98/120    avg_loss:0.029, val_acc:0.988]
Epoch [99/120    avg_loss:0.022, val_acc:0.990]
Epoch [100/120    avg_loss:0.014, val_acc:0.990]
Epoch [101/120    avg_loss:0.021, val_acc:0.984]
Epoch [102/120    avg_loss:0.020, val_acc:0.992]
Epoch [103/120    avg_loss:0.016, val_acc:0.994]
Epoch [104/120    avg_loss:0.019, val_acc:0.994]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.025, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.016, val_acc:0.992]
Epoch [111/120    avg_loss:0.030, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.008, val_acc:0.994]
Epoch [114/120    avg_loss:0.009, val_acc:0.994]
Epoch [115/120    avg_loss:0.014, val_acc:0.994]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.015, val_acc:0.994]
Epoch [118/120    avg_loss:0.033, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   9   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   3   0   0   0   0   0   0   4   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99338722 1.         0.98004435 0.94017094 0.91724138
 1.         1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9912183285067793
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7dd415748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.835, val_acc:0.609]
Epoch [2/120    avg_loss:1.274, val_acc:0.660]
Epoch [3/120    avg_loss:1.039, val_acc:0.699]
Epoch [4/120    avg_loss:0.912, val_acc:0.754]
Epoch [5/120    avg_loss:0.816, val_acc:0.758]
Epoch [6/120    avg_loss:0.769, val_acc:0.799]
Epoch [7/120    avg_loss:0.746, val_acc:0.758]
Epoch [8/120    avg_loss:0.631, val_acc:0.766]
Epoch [9/120    avg_loss:0.660, val_acc:0.863]
Epoch [10/120    avg_loss:0.584, val_acc:0.830]
Epoch [11/120    avg_loss:0.563, val_acc:0.824]
Epoch [12/120    avg_loss:0.453, val_acc:0.879]
Epoch [13/120    avg_loss:0.471, val_acc:0.895]
Epoch [14/120    avg_loss:0.373, val_acc:0.916]
Epoch [15/120    avg_loss:0.338, val_acc:0.910]
Epoch [16/120    avg_loss:0.335, val_acc:0.900]
Epoch [17/120    avg_loss:0.363, val_acc:0.910]
Epoch [18/120    avg_loss:0.370, val_acc:0.855]
Epoch [19/120    avg_loss:0.254, val_acc:0.930]
Epoch [20/120    avg_loss:0.237, val_acc:0.865]
Epoch [21/120    avg_loss:0.268, val_acc:0.912]
Epoch [22/120    avg_loss:0.271, val_acc:0.879]
Epoch [23/120    avg_loss:0.260, val_acc:0.920]
Epoch [24/120    avg_loss:0.151, val_acc:0.943]
Epoch [25/120    avg_loss:0.259, val_acc:0.941]
Epoch [26/120    avg_loss:0.229, val_acc:0.943]
Epoch [27/120    avg_loss:0.239, val_acc:0.928]
Epoch [28/120    avg_loss:0.221, val_acc:0.959]
Epoch [29/120    avg_loss:0.186, val_acc:0.961]
Epoch [30/120    avg_loss:0.175, val_acc:0.904]
Epoch [31/120    avg_loss:0.204, val_acc:0.918]
Epoch [32/120    avg_loss:0.172, val_acc:0.930]
Epoch [33/120    avg_loss:0.132, val_acc:0.945]
Epoch [34/120    avg_loss:0.137, val_acc:0.943]
Epoch [35/120    avg_loss:0.119, val_acc:0.955]
Epoch [36/120    avg_loss:0.093, val_acc:0.959]
Epoch [37/120    avg_loss:0.148, val_acc:0.941]
Epoch [38/120    avg_loss:0.165, val_acc:0.961]
Epoch [39/120    avg_loss:0.120, val_acc:0.967]
Epoch [40/120    avg_loss:0.104, val_acc:0.971]
Epoch [41/120    avg_loss:0.107, val_acc:0.977]
Epoch [42/120    avg_loss:0.056, val_acc:0.955]
Epoch [43/120    avg_loss:0.065, val_acc:0.965]
Epoch [44/120    avg_loss:0.059, val_acc:0.959]
Epoch [45/120    avg_loss:0.160, val_acc:0.955]
Epoch [46/120    avg_loss:0.102, val_acc:0.971]
Epoch [47/120    avg_loss:0.140, val_acc:0.943]
Epoch [48/120    avg_loss:0.055, val_acc:0.977]
Epoch [49/120    avg_loss:0.041, val_acc:0.980]
Epoch [50/120    avg_loss:0.041, val_acc:0.982]
Epoch [51/120    avg_loss:0.038, val_acc:0.984]
Epoch [52/120    avg_loss:0.034, val_acc:0.975]
Epoch [53/120    avg_loss:0.090, val_acc:0.963]
Epoch [54/120    avg_loss:0.076, val_acc:0.936]
Epoch [55/120    avg_loss:0.059, val_acc:0.982]
Epoch [56/120    avg_loss:0.043, val_acc:0.984]
Epoch [57/120    avg_loss:0.026, val_acc:0.990]
Epoch [58/120    avg_loss:0.038, val_acc:0.975]
Epoch [59/120    avg_loss:0.028, val_acc:0.977]
Epoch [60/120    avg_loss:0.037, val_acc:0.980]
Epoch [61/120    avg_loss:0.040, val_acc:0.971]
Epoch [62/120    avg_loss:0.144, val_acc:0.957]
Epoch [63/120    avg_loss:0.093, val_acc:0.945]
Epoch [64/120    avg_loss:0.099, val_acc:0.967]
Epoch [65/120    avg_loss:0.064, val_acc:0.973]
Epoch [66/120    avg_loss:0.048, val_acc:0.977]
Epoch [67/120    avg_loss:0.031, val_acc:0.973]
Epoch [68/120    avg_loss:0.050, val_acc:0.977]
Epoch [69/120    avg_loss:0.028, val_acc:0.982]
Epoch [70/120    avg_loss:0.032, val_acc:0.980]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.984]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.011, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.017, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.023, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.018, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.984]
Epoch [103/120    avg_loss:0.020, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.019, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.017, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.016, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.984]
Epoch [120/120    avg_loss:0.011, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   7   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99340659 0.99545455 1.         0.95633188 0.92041522
 1.         1.         0.99741602 1.         1.         1.
 0.99339207 1.        ]

Kappa:
0.9926419547551323
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d26560780>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.880, val_acc:0.597]
Epoch [2/120    avg_loss:1.348, val_acc:0.532]
Epoch [3/120    avg_loss:1.083, val_acc:0.746]
Epoch [4/120    avg_loss:0.897, val_acc:0.790]
Epoch [5/120    avg_loss:0.671, val_acc:0.817]
Epoch [6/120    avg_loss:0.721, val_acc:0.764]
Epoch [7/120    avg_loss:0.637, val_acc:0.831]
Epoch [8/120    avg_loss:0.567, val_acc:0.843]
Epoch [9/120    avg_loss:0.589, val_acc:0.875]
Epoch [10/120    avg_loss:0.548, val_acc:0.875]
Epoch [11/120    avg_loss:0.522, val_acc:0.877]
Epoch [12/120    avg_loss:0.502, val_acc:0.863]
Epoch [13/120    avg_loss:0.545, val_acc:0.895]
Epoch [14/120    avg_loss:0.397, val_acc:0.899]
Epoch [15/120    avg_loss:0.404, val_acc:0.893]
Epoch [16/120    avg_loss:0.375, val_acc:0.869]
Epoch [17/120    avg_loss:0.343, val_acc:0.901]
Epoch [18/120    avg_loss:0.418, val_acc:0.929]
Epoch [19/120    avg_loss:0.258, val_acc:0.917]
Epoch [20/120    avg_loss:0.351, val_acc:0.925]
Epoch [21/120    avg_loss:0.182, val_acc:0.931]
Epoch [22/120    avg_loss:0.223, val_acc:0.938]
Epoch [23/120    avg_loss:0.239, val_acc:0.925]
Epoch [24/120    avg_loss:0.215, val_acc:0.948]
Epoch [25/120    avg_loss:0.189, val_acc:0.940]
Epoch [26/120    avg_loss:0.161, val_acc:0.944]
Epoch [27/120    avg_loss:0.269, val_acc:0.917]
Epoch [28/120    avg_loss:0.152, val_acc:0.948]
Epoch [29/120    avg_loss:0.118, val_acc:0.948]
Epoch [30/120    avg_loss:0.211, val_acc:0.940]
Epoch [31/120    avg_loss:0.188, val_acc:0.942]
Epoch [32/120    avg_loss:0.335, val_acc:0.877]
Epoch [33/120    avg_loss:0.234, val_acc:0.942]
Epoch [34/120    avg_loss:0.184, val_acc:0.954]
Epoch [35/120    avg_loss:0.164, val_acc:0.966]
Epoch [36/120    avg_loss:0.124, val_acc:0.948]
Epoch [37/120    avg_loss:0.111, val_acc:0.964]
Epoch [38/120    avg_loss:0.085, val_acc:0.962]
Epoch [39/120    avg_loss:0.119, val_acc:0.946]
Epoch [40/120    avg_loss:0.102, val_acc:0.976]
Epoch [41/120    avg_loss:0.072, val_acc:0.960]
Epoch [42/120    avg_loss:0.079, val_acc:0.978]
Epoch [43/120    avg_loss:0.063, val_acc:0.974]
Epoch [44/120    avg_loss:0.087, val_acc:0.970]
Epoch [45/120    avg_loss:0.056, val_acc:0.978]
Epoch [46/120    avg_loss:0.078, val_acc:0.972]
Epoch [47/120    avg_loss:0.070, val_acc:0.982]
Epoch [48/120    avg_loss:0.107, val_acc:0.968]
Epoch [49/120    avg_loss:0.142, val_acc:0.970]
Epoch [50/120    avg_loss:0.136, val_acc:0.974]
Epoch [51/120    avg_loss:0.063, val_acc:0.974]
Epoch [52/120    avg_loss:0.067, val_acc:0.972]
Epoch [53/120    avg_loss:0.058, val_acc:0.976]
Epoch [54/120    avg_loss:0.084, val_acc:0.980]
Epoch [55/120    avg_loss:0.029, val_acc:0.976]
Epoch [56/120    avg_loss:0.025, val_acc:0.976]
Epoch [57/120    avg_loss:0.054, val_acc:0.984]
Epoch [58/120    avg_loss:0.048, val_acc:0.984]
Epoch [59/120    avg_loss:0.029, val_acc:0.990]
Epoch [60/120    avg_loss:0.014, val_acc:0.990]
Epoch [61/120    avg_loss:0.035, val_acc:0.972]
Epoch [62/120    avg_loss:0.050, val_acc:0.976]
Epoch [63/120    avg_loss:0.038, val_acc:0.982]
Epoch [64/120    avg_loss:0.050, val_acc:0.988]
Epoch [65/120    avg_loss:0.025, val_acc:0.990]
Epoch [66/120    avg_loss:0.020, val_acc:0.988]
Epoch [67/120    avg_loss:0.012, val_acc:0.990]
Epoch [68/120    avg_loss:0.017, val_acc:0.992]
Epoch [69/120    avg_loss:0.039, val_acc:0.988]
Epoch [70/120    avg_loss:0.025, val_acc:0.988]
Epoch [71/120    avg_loss:0.060, val_acc:0.974]
Epoch [72/120    avg_loss:0.025, val_acc:0.990]
Epoch [73/120    avg_loss:0.021, val_acc:0.992]
Epoch [74/120    avg_loss:0.025, val_acc:0.988]
Epoch [75/120    avg_loss:0.017, val_acc:0.990]
Epoch [76/120    avg_loss:0.016, val_acc:0.990]
Epoch [77/120    avg_loss:0.045, val_acc:0.982]
Epoch [78/120    avg_loss:0.049, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.994]
Epoch [83/120    avg_loss:0.030, val_acc:0.986]
Epoch [84/120    avg_loss:0.017, val_acc:0.996]
Epoch [85/120    avg_loss:0.008, val_acc:0.994]
Epoch [86/120    avg_loss:0.012, val_acc:0.994]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.011, val_acc:0.994]
Epoch [89/120    avg_loss:0.013, val_acc:0.994]
Epoch [90/120    avg_loss:0.010, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.992]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.016, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.974]
Epoch [95/120    avg_loss:0.010, val_acc:0.994]
Epoch [96/120    avg_loss:0.011, val_acc:0.974]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.005, val_acc:0.992]
Epoch [99/120    avg_loss:0.007, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.994]
Epoch [101/120    avg_loss:0.006, val_acc:0.994]
Epoch [102/120    avg_loss:0.005, val_acc:0.994]
Epoch [103/120    avg_loss:0.004, val_acc:0.994]
Epoch [104/120    avg_loss:0.009, val_acc:0.996]
Epoch [105/120    avg_loss:0.004, val_acc:0.994]
Epoch [106/120    avg_loss:0.003, val_acc:0.994]
Epoch [107/120    avg_loss:0.004, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.004, val_acc:0.994]
Epoch [111/120    avg_loss:0.003, val_acc:0.996]
Epoch [112/120    avg_loss:0.005, val_acc:0.998]
Epoch [113/120    avg_loss:0.014, val_acc:0.998]
Epoch [114/120    avg_loss:0.003, val_acc:0.998]
Epoch [115/120    avg_loss:0.004, val_acc:0.996]
Epoch [116/120    avg_loss:0.003, val_acc:0.996]
Epoch [117/120    avg_loss:0.006, val_acc:0.996]
Epoch [118/120    avg_loss:0.003, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.005, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   9   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 213  17   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   1  11 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 0.99338722 1.         0.95945946 0.93305439 0.91408935
 1.         1.         1.         1.         1.         0.99472296
 0.99556541 1.        ]

Kappa:
0.9890829705371772
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6cba0357b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.902, val_acc:0.498]
Epoch [2/120    avg_loss:1.359, val_acc:0.654]
Epoch [3/120    avg_loss:1.082, val_acc:0.734]
Epoch [4/120    avg_loss:0.905, val_acc:0.789]
Epoch [5/120    avg_loss:0.805, val_acc:0.818]
Epoch [6/120    avg_loss:0.749, val_acc:0.828]
Epoch [7/120    avg_loss:0.559, val_acc:0.768]
Epoch [8/120    avg_loss:0.524, val_acc:0.842]
Epoch [9/120    avg_loss:0.553, val_acc:0.861]
Epoch [10/120    avg_loss:0.563, val_acc:0.861]
Epoch [11/120    avg_loss:0.523, val_acc:0.848]
Epoch [12/120    avg_loss:0.454, val_acc:0.873]
Epoch [13/120    avg_loss:0.454, val_acc:0.881]
Epoch [14/120    avg_loss:0.462, val_acc:0.834]
Epoch [15/120    avg_loss:0.419, val_acc:0.887]
Epoch [16/120    avg_loss:0.374, val_acc:0.867]
Epoch [17/120    avg_loss:0.337, val_acc:0.893]
Epoch [18/120    avg_loss:0.310, val_acc:0.924]
Epoch [19/120    avg_loss:0.323, val_acc:0.889]
Epoch [20/120    avg_loss:0.310, val_acc:0.816]
Epoch [21/120    avg_loss:0.377, val_acc:0.910]
Epoch [22/120    avg_loss:0.243, val_acc:0.920]
Epoch [23/120    avg_loss:0.236, val_acc:0.912]
Epoch [24/120    avg_loss:0.270, val_acc:0.928]
Epoch [25/120    avg_loss:0.175, val_acc:0.900]
Epoch [26/120    avg_loss:0.181, val_acc:0.936]
Epoch [27/120    avg_loss:0.176, val_acc:0.930]
Epoch [28/120    avg_loss:0.218, val_acc:0.939]
Epoch [29/120    avg_loss:0.157, val_acc:0.957]
Epoch [30/120    avg_loss:0.153, val_acc:0.951]
Epoch [31/120    avg_loss:0.125, val_acc:0.957]
Epoch [32/120    avg_loss:0.207, val_acc:0.930]
Epoch [33/120    avg_loss:0.163, val_acc:0.943]
Epoch [34/120    avg_loss:0.128, val_acc:0.943]
Epoch [35/120    avg_loss:0.078, val_acc:0.963]
Epoch [36/120    avg_loss:0.058, val_acc:0.961]
Epoch [37/120    avg_loss:0.128, val_acc:0.961]
Epoch [38/120    avg_loss:0.070, val_acc:0.973]
Epoch [39/120    avg_loss:0.068, val_acc:0.959]
Epoch [40/120    avg_loss:0.108, val_acc:0.973]
Epoch [41/120    avg_loss:0.042, val_acc:0.971]
Epoch [42/120    avg_loss:0.078, val_acc:0.965]
Epoch [43/120    avg_loss:0.105, val_acc:0.939]
Epoch [44/120    avg_loss:0.066, val_acc:0.971]
Epoch [45/120    avg_loss:0.051, val_acc:0.957]
Epoch [46/120    avg_loss:0.083, val_acc:0.961]
Epoch [47/120    avg_loss:0.066, val_acc:0.975]
Epoch [48/120    avg_loss:0.044, val_acc:0.980]
Epoch [49/120    avg_loss:0.079, val_acc:0.941]
Epoch [50/120    avg_loss:0.108, val_acc:0.959]
Epoch [51/120    avg_loss:0.064, val_acc:0.967]
Epoch [52/120    avg_loss:0.037, val_acc:0.947]
Epoch [53/120    avg_loss:0.079, val_acc:0.965]
Epoch [54/120    avg_loss:0.127, val_acc:0.947]
Epoch [55/120    avg_loss:0.073, val_acc:0.969]
Epoch [56/120    avg_loss:0.047, val_acc:0.971]
Epoch [57/120    avg_loss:0.043, val_acc:0.973]
Epoch [58/120    avg_loss:0.053, val_acc:0.969]
Epoch [59/120    avg_loss:0.096, val_acc:0.975]
Epoch [60/120    avg_loss:0.050, val_acc:0.977]
Epoch [61/120    avg_loss:0.028, val_acc:0.988]
Epoch [62/120    avg_loss:0.052, val_acc:0.951]
Epoch [63/120    avg_loss:0.068, val_acc:0.971]
Epoch [64/120    avg_loss:0.078, val_acc:0.963]
Epoch [65/120    avg_loss:0.075, val_acc:0.949]
Epoch [66/120    avg_loss:0.039, val_acc:0.967]
Epoch [67/120    avg_loss:0.067, val_acc:0.980]
Epoch [68/120    avg_loss:0.059, val_acc:0.967]
Epoch [69/120    avg_loss:0.043, val_acc:0.984]
Epoch [70/120    avg_loss:0.018, val_acc:0.988]
Epoch [71/120    avg_loss:0.014, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.988]
Epoch [73/120    avg_loss:0.007, val_acc:0.990]
Epoch [74/120    avg_loss:0.011, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.955]
Epoch [76/120    avg_loss:0.019, val_acc:0.994]
Epoch [77/120    avg_loss:0.005, val_acc:0.994]
Epoch [78/120    avg_loss:0.016, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.984]
Epoch [80/120    avg_loss:0.022, val_acc:0.990]
Epoch [81/120    avg_loss:0.016, val_acc:0.992]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.990]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.022, val_acc:0.977]
Epoch [86/120    avg_loss:0.025, val_acc:0.982]
Epoch [87/120    avg_loss:0.012, val_acc:0.988]
Epoch [88/120    avg_loss:0.029, val_acc:0.984]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.007, val_acc:0.992]
Epoch [96/120    avg_loss:0.003, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.005, val_acc:0.994]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.005, val_acc:0.994]
Epoch [102/120    avg_loss:0.012, val_acc:0.994]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.994]
Epoch [105/120    avg_loss:0.008, val_acc:0.994]
Epoch [106/120    avg_loss:0.003, val_acc:0.994]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.994]
Epoch [116/120    avg_loss:0.016, val_acc:0.994]
Epoch [117/120    avg_loss:0.004, val_acc:0.994]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.004, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   3   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   1   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 216   5   0   0   0   0   0   0   6   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99707602 1.         0.99343545 0.95154185 0.9375
 1.         1.         0.996139   1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9933530649244395
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37bebc1748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.814, val_acc:0.623]
Epoch [2/120    avg_loss:1.172, val_acc:0.728]
Epoch [3/120    avg_loss:1.152, val_acc:0.661]
Epoch [4/120    avg_loss:0.981, val_acc:0.784]
Epoch [5/120    avg_loss:0.867, val_acc:0.760]
Epoch [6/120    avg_loss:0.875, val_acc:0.800]
Epoch [7/120    avg_loss:0.711, val_acc:0.808]
Epoch [8/120    avg_loss:0.780, val_acc:0.853]
Epoch [9/120    avg_loss:0.595, val_acc:0.855]
Epoch [10/120    avg_loss:0.515, val_acc:0.885]
Epoch [11/120    avg_loss:0.482, val_acc:0.843]
Epoch [12/120    avg_loss:0.566, val_acc:0.845]
Epoch [13/120    avg_loss:0.473, val_acc:0.901]
Epoch [14/120    avg_loss:0.473, val_acc:0.885]
Epoch [15/120    avg_loss:0.542, val_acc:0.895]
Epoch [16/120    avg_loss:0.417, val_acc:0.887]
Epoch [17/120    avg_loss:0.356, val_acc:0.895]
Epoch [18/120    avg_loss:0.286, val_acc:0.905]
Epoch [19/120    avg_loss:0.300, val_acc:0.901]
Epoch [20/120    avg_loss:0.350, val_acc:0.889]
Epoch [21/120    avg_loss:0.248, val_acc:0.905]
Epoch [22/120    avg_loss:0.248, val_acc:0.919]
Epoch [23/120    avg_loss:0.349, val_acc:0.927]
Epoch [24/120    avg_loss:0.201, val_acc:0.944]
Epoch [25/120    avg_loss:0.310, val_acc:0.909]
Epoch [26/120    avg_loss:0.256, val_acc:0.942]
Epoch [27/120    avg_loss:0.259, val_acc:0.913]
Epoch [28/120    avg_loss:0.211, val_acc:0.919]
Epoch [29/120    avg_loss:0.174, val_acc:0.905]
Epoch [30/120    avg_loss:0.235, val_acc:0.942]
Epoch [31/120    avg_loss:0.135, val_acc:0.925]
Epoch [32/120    avg_loss:0.162, val_acc:0.964]
Epoch [33/120    avg_loss:0.125, val_acc:0.958]
Epoch [34/120    avg_loss:0.165, val_acc:0.968]
Epoch [35/120    avg_loss:0.103, val_acc:0.935]
Epoch [36/120    avg_loss:0.140, val_acc:0.927]
Epoch [37/120    avg_loss:0.289, val_acc:0.948]
Epoch [38/120    avg_loss:0.132, val_acc:0.976]
Epoch [39/120    avg_loss:0.102, val_acc:0.954]
Epoch [40/120    avg_loss:0.137, val_acc:0.929]
Epoch [41/120    avg_loss:0.103, val_acc:0.968]
Epoch [42/120    avg_loss:0.131, val_acc:0.984]
Epoch [43/120    avg_loss:0.083, val_acc:0.970]
Epoch [44/120    avg_loss:0.069, val_acc:0.966]
Epoch [45/120    avg_loss:0.055, val_acc:0.970]
Epoch [46/120    avg_loss:0.086, val_acc:0.948]
Epoch [47/120    avg_loss:0.107, val_acc:0.962]
Epoch [48/120    avg_loss:0.100, val_acc:0.980]
Epoch [49/120    avg_loss:0.124, val_acc:0.950]
Epoch [50/120    avg_loss:0.078, val_acc:0.980]
Epoch [51/120    avg_loss:0.078, val_acc:0.982]
Epoch [52/120    avg_loss:0.124, val_acc:0.962]
Epoch [53/120    avg_loss:0.073, val_acc:0.974]
Epoch [54/120    avg_loss:0.096, val_acc:0.984]
Epoch [55/120    avg_loss:0.037, val_acc:0.980]
Epoch [56/120    avg_loss:0.033, val_acc:0.992]
Epoch [57/120    avg_loss:0.044, val_acc:0.984]
Epoch [58/120    avg_loss:0.034, val_acc:0.988]
Epoch [59/120    avg_loss:0.024, val_acc:0.982]
Epoch [60/120    avg_loss:0.066, val_acc:0.984]
Epoch [61/120    avg_loss:0.045, val_acc:0.972]
Epoch [62/120    avg_loss:0.051, val_acc:0.988]
Epoch [63/120    avg_loss:0.056, val_acc:0.958]
Epoch [64/120    avg_loss:0.044, val_acc:0.990]
Epoch [65/120    avg_loss:0.039, val_acc:0.962]
Epoch [66/120    avg_loss:0.043, val_acc:0.984]
Epoch [67/120    avg_loss:0.038, val_acc:0.990]
Epoch [68/120    avg_loss:0.037, val_acc:0.986]
Epoch [69/120    avg_loss:0.028, val_acc:0.990]
Epoch [70/120    avg_loss:0.019, val_acc:0.990]
Epoch [71/120    avg_loss:0.016, val_acc:0.986]
Epoch [72/120    avg_loss:0.016, val_acc:0.992]
Epoch [73/120    avg_loss:0.018, val_acc:0.994]
Epoch [74/120    avg_loss:0.021, val_acc:0.996]
Epoch [75/120    avg_loss:0.008, val_acc:0.996]
Epoch [76/120    avg_loss:0.012, val_acc:0.996]
Epoch [77/120    avg_loss:0.012, val_acc:0.996]
Epoch [78/120    avg_loss:0.023, val_acc:0.996]
Epoch [79/120    avg_loss:0.011, val_acc:0.996]
Epoch [80/120    avg_loss:0.007, val_acc:0.996]
Epoch [81/120    avg_loss:0.015, val_acc:0.996]
Epoch [82/120    avg_loss:0.006, val_acc:0.996]
Epoch [83/120    avg_loss:0.014, val_acc:0.996]
Epoch [84/120    avg_loss:0.016, val_acc:0.996]
Epoch [85/120    avg_loss:0.013, val_acc:0.996]
Epoch [86/120    avg_loss:0.027, val_acc:0.996]
Epoch [87/120    avg_loss:0.009, val_acc:0.996]
Epoch [88/120    avg_loss:0.007, val_acc:0.996]
Epoch [89/120    avg_loss:0.009, val_acc:0.996]
Epoch [90/120    avg_loss:0.009, val_acc:0.996]
Epoch [91/120    avg_loss:0.009, val_acc:0.996]
Epoch [92/120    avg_loss:0.011, val_acc:0.996]
Epoch [93/120    avg_loss:0.016, val_acc:0.996]
Epoch [94/120    avg_loss:0.012, val_acc:0.994]
Epoch [95/120    avg_loss:0.017, val_acc:0.992]
Epoch [96/120    avg_loss:0.007, val_acc:0.996]
Epoch [97/120    avg_loss:0.010, val_acc:0.994]
Epoch [98/120    avg_loss:0.007, val_acc:0.994]
Epoch [99/120    avg_loss:0.009, val_acc:0.994]
Epoch [100/120    avg_loss:0.007, val_acc:0.994]
Epoch [101/120    avg_loss:0.008, val_acc:0.994]
Epoch [102/120    avg_loss:0.006, val_acc:0.994]
Epoch [103/120    avg_loss:0.008, val_acc:0.994]
Epoch [104/120    avg_loss:0.011, val_acc:0.996]
Epoch [105/120    avg_loss:0.008, val_acc:0.994]
Epoch [106/120    avg_loss:0.017, val_acc:0.994]
Epoch [107/120    avg_loss:0.015, val_acc:0.994]
Epoch [108/120    avg_loss:0.005, val_acc:0.994]
Epoch [109/120    avg_loss:0.008, val_acc:0.994]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.007, val_acc:0.994]
Epoch [112/120    avg_loss:0.011, val_acc:0.994]
Epoch [113/120    avg_loss:0.010, val_acc:0.994]
Epoch [114/120    avg_loss:0.019, val_acc:0.994]
Epoch [115/120    avg_loss:0.010, val_acc:0.994]
Epoch [116/120    avg_loss:0.008, val_acc:0.994]
Epoch [117/120    avg_loss:0.007, val_acc:0.994]
Epoch [118/120    avg_loss:0.014, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.009, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   6   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   9   0   0   0   0   0   0 379   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.98907502 0.99545455 1.         0.96760259 0.93006993
 1.         1.         0.98826597 1.         1.         1.
 0.99668508 1.        ]

Kappa:
0.9924037892175006
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0d819e748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.951, val_acc:0.627]
Epoch [2/120    avg_loss:1.178, val_acc:0.785]
Epoch [3/120    avg_loss:1.056, val_acc:0.699]
Epoch [4/120    avg_loss:0.856, val_acc:0.822]
Epoch [5/120    avg_loss:0.750, val_acc:0.820]
Epoch [6/120    avg_loss:0.636, val_acc:0.777]
Epoch [7/120    avg_loss:0.636, val_acc:0.836]
Epoch [8/120    avg_loss:0.597, val_acc:0.871]
Epoch [9/120    avg_loss:0.680, val_acc:0.871]
Epoch [10/120    avg_loss:0.503, val_acc:0.877]
Epoch [11/120    avg_loss:0.547, val_acc:0.895]
Epoch [12/120    avg_loss:0.476, val_acc:0.904]
Epoch [13/120    avg_loss:0.429, val_acc:0.834]
Epoch [14/120    avg_loss:0.439, val_acc:0.891]
Epoch [15/120    avg_loss:0.531, val_acc:0.900]
Epoch [16/120    avg_loss:0.414, val_acc:0.891]
Epoch [17/120    avg_loss:0.298, val_acc:0.902]
Epoch [18/120    avg_loss:0.453, val_acc:0.910]
Epoch [19/120    avg_loss:0.336, val_acc:0.902]
Epoch [20/120    avg_loss:0.321, val_acc:0.928]
Epoch [21/120    avg_loss:0.287, val_acc:0.936]
Epoch [22/120    avg_loss:0.343, val_acc:0.943]
Epoch [23/120    avg_loss:0.284, val_acc:0.934]
Epoch [24/120    avg_loss:0.290, val_acc:0.939]
Epoch [25/120    avg_loss:0.287, val_acc:0.887]
Epoch [26/120    avg_loss:0.335, val_acc:0.932]
Epoch [27/120    avg_loss:0.403, val_acc:0.871]
Epoch [28/120    avg_loss:0.320, val_acc:0.932]
Epoch [29/120    avg_loss:0.234, val_acc:0.945]
Epoch [30/120    avg_loss:0.260, val_acc:0.932]
Epoch [31/120    avg_loss:0.183, val_acc:0.959]
Epoch [32/120    avg_loss:0.273, val_acc:0.947]
Epoch [33/120    avg_loss:0.289, val_acc:0.961]
Epoch [34/120    avg_loss:0.185, val_acc:0.945]
Epoch [35/120    avg_loss:0.241, val_acc:0.955]
Epoch [36/120    avg_loss:0.246, val_acc:0.947]
Epoch [37/120    avg_loss:0.199, val_acc:0.941]
Epoch [38/120    avg_loss:0.168, val_acc:0.963]
Epoch [39/120    avg_loss:0.147, val_acc:0.959]
Epoch [40/120    avg_loss:0.192, val_acc:0.961]
Epoch [41/120    avg_loss:0.185, val_acc:0.951]
Epoch [42/120    avg_loss:0.174, val_acc:0.955]
Epoch [43/120    avg_loss:0.205, val_acc:0.955]
Epoch [44/120    avg_loss:0.105, val_acc:0.971]
Epoch [45/120    avg_loss:0.138, val_acc:0.947]
Epoch [46/120    avg_loss:0.151, val_acc:0.932]
Epoch [47/120    avg_loss:0.128, val_acc:0.939]
Epoch [48/120    avg_loss:0.125, val_acc:0.963]
Epoch [49/120    avg_loss:0.099, val_acc:0.971]
Epoch [50/120    avg_loss:0.114, val_acc:0.969]
Epoch [51/120    avg_loss:0.095, val_acc:0.961]
Epoch [52/120    avg_loss:0.113, val_acc:0.912]
Epoch [53/120    avg_loss:0.206, val_acc:0.953]
Epoch [54/120    avg_loss:0.156, val_acc:0.959]
Epoch [55/120    avg_loss:0.120, val_acc:0.955]
Epoch [56/120    avg_loss:0.112, val_acc:0.951]
Epoch [57/120    avg_loss:0.105, val_acc:0.961]
Epoch [58/120    avg_loss:0.131, val_acc:0.955]
Epoch [59/120    avg_loss:0.109, val_acc:0.969]
Epoch [60/120    avg_loss:0.060, val_acc:0.969]
Epoch [61/120    avg_loss:0.140, val_acc:0.959]
Epoch [62/120    avg_loss:0.091, val_acc:0.965]
Epoch [63/120    avg_loss:0.056, val_acc:0.973]
Epoch [64/120    avg_loss:0.064, val_acc:0.975]
Epoch [65/120    avg_loss:0.039, val_acc:0.977]
Epoch [66/120    avg_loss:0.042, val_acc:0.979]
Epoch [67/120    avg_loss:0.041, val_acc:0.977]
Epoch [68/120    avg_loss:0.066, val_acc:0.977]
Epoch [69/120    avg_loss:0.046, val_acc:0.975]
Epoch [70/120    avg_loss:0.049, val_acc:0.975]
Epoch [71/120    avg_loss:0.037, val_acc:0.975]
Epoch [72/120    avg_loss:0.045, val_acc:0.977]
Epoch [73/120    avg_loss:0.040, val_acc:0.979]
Epoch [74/120    avg_loss:0.066, val_acc:0.980]
Epoch [75/120    avg_loss:0.048, val_acc:0.977]
Epoch [76/120    avg_loss:0.048, val_acc:0.984]
Epoch [77/120    avg_loss:0.026, val_acc:0.980]
Epoch [78/120    avg_loss:0.030, val_acc:0.979]
Epoch [79/120    avg_loss:0.038, val_acc:0.980]
Epoch [80/120    avg_loss:0.037, val_acc:0.977]
Epoch [81/120    avg_loss:0.034, val_acc:0.980]
Epoch [82/120    avg_loss:0.057, val_acc:0.979]
Epoch [83/120    avg_loss:0.029, val_acc:0.982]
Epoch [84/120    avg_loss:0.028, val_acc:0.982]
Epoch [85/120    avg_loss:0.026, val_acc:0.982]
Epoch [86/120    avg_loss:0.034, val_acc:0.979]
Epoch [87/120    avg_loss:0.050, val_acc:0.982]
Epoch [88/120    avg_loss:0.035, val_acc:0.982]
Epoch [89/120    avg_loss:0.025, val_acc:0.982]
Epoch [90/120    avg_loss:0.031, val_acc:0.982]
Epoch [91/120    avg_loss:0.042, val_acc:0.982]
Epoch [92/120    avg_loss:0.025, val_acc:0.982]
Epoch [93/120    avg_loss:0.028, val_acc:0.982]
Epoch [94/120    avg_loss:0.048, val_acc:0.982]
Epoch [95/120    avg_loss:0.030, val_acc:0.982]
Epoch [96/120    avg_loss:0.027, val_acc:0.982]
Epoch [97/120    avg_loss:0.036, val_acc:0.982]
Epoch [98/120    avg_loss:0.034, val_acc:0.982]
Epoch [99/120    avg_loss:0.026, val_acc:0.982]
Epoch [100/120    avg_loss:0.031, val_acc:0.982]
Epoch [101/120    avg_loss:0.029, val_acc:0.982]
Epoch [102/120    avg_loss:0.027, val_acc:0.982]
Epoch [103/120    avg_loss:0.021, val_acc:0.982]
Epoch [104/120    avg_loss:0.023, val_acc:0.982]
Epoch [105/120    avg_loss:0.026, val_acc:0.982]
Epoch [106/120    avg_loss:0.041, val_acc:0.982]
Epoch [107/120    avg_loss:0.033, val_acc:0.982]
Epoch [108/120    avg_loss:0.028, val_acc:0.982]
Epoch [109/120    avg_loss:0.039, val_acc:0.982]
Epoch [110/120    avg_loss:0.030, val_acc:0.982]
Epoch [111/120    avg_loss:0.030, val_acc:0.982]
Epoch [112/120    avg_loss:0.031, val_acc:0.982]
Epoch [113/120    avg_loss:0.021, val_acc:0.982]
Epoch [114/120    avg_loss:0.029, val_acc:0.982]
Epoch [115/120    avg_loss:0.048, val_acc:0.982]
Epoch [116/120    avg_loss:0.032, val_acc:0.982]
Epoch [117/120    avg_loss:0.035, val_acc:0.982]
Epoch [118/120    avg_loss:0.044, val_acc:0.982]
Epoch [119/120    avg_loss:0.036, val_acc:0.982]
Epoch [120/120    avg_loss:0.030, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   5   0   0   0   0   4   0]
 [  0   0   0 218   7   0   0   0   1   4   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0  11   0   0   5   0 190   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 0.99203476 0.96551724 0.97321429 0.9217759  0.91166078
 0.95959596 0.96875    0.998713   0.99574468 1.         1.
 0.99005525 1.        ]

Kappa:
0.9838529097900063
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa82f2d27b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.986, val_acc:0.537]
Epoch [2/120    avg_loss:1.228, val_acc:0.740]
Epoch [3/120    avg_loss:1.105, val_acc:0.764]
Epoch [4/120    avg_loss:0.887, val_acc:0.830]
Epoch [5/120    avg_loss:0.698, val_acc:0.836]
Epoch [6/120    avg_loss:0.591, val_acc:0.832]
Epoch [7/120    avg_loss:0.682, val_acc:0.812]
Epoch [8/120    avg_loss:0.568, val_acc:0.826]
Epoch [9/120    avg_loss:0.610, val_acc:0.826]
Epoch [10/120    avg_loss:0.521, val_acc:0.904]
Epoch [11/120    avg_loss:0.426, val_acc:0.871]
Epoch [12/120    avg_loss:0.416, val_acc:0.893]
Epoch [13/120    avg_loss:0.415, val_acc:0.893]
Epoch [14/120    avg_loss:0.335, val_acc:0.906]
Epoch [15/120    avg_loss:0.364, val_acc:0.939]
Epoch [16/120    avg_loss:0.350, val_acc:0.895]
Epoch [17/120    avg_loss:0.363, val_acc:0.912]
Epoch [18/120    avg_loss:0.360, val_acc:0.930]
Epoch [19/120    avg_loss:0.220, val_acc:0.924]
Epoch [20/120    avg_loss:0.311, val_acc:0.889]
Epoch [21/120    avg_loss:0.382, val_acc:0.936]
Epoch [22/120    avg_loss:0.301, val_acc:0.938]
Epoch [23/120    avg_loss:0.235, val_acc:0.939]
Epoch [24/120    avg_loss:0.281, val_acc:0.893]
Epoch [25/120    avg_loss:0.577, val_acc:0.875]
Epoch [26/120    avg_loss:0.362, val_acc:0.908]
Epoch [27/120    avg_loss:0.239, val_acc:0.928]
Epoch [28/120    avg_loss:0.246, val_acc:0.938]
Epoch [29/120    avg_loss:0.198, val_acc:0.926]
Epoch [30/120    avg_loss:0.307, val_acc:0.924]
Epoch [31/120    avg_loss:0.240, val_acc:0.951]
Epoch [32/120    avg_loss:0.242, val_acc:0.936]
Epoch [33/120    avg_loss:0.192, val_acc:0.920]
Epoch [34/120    avg_loss:0.169, val_acc:0.936]
Epoch [35/120    avg_loss:0.161, val_acc:0.932]
Epoch [36/120    avg_loss:0.258, val_acc:0.930]
Epoch [37/120    avg_loss:0.208, val_acc:0.943]
Epoch [38/120    avg_loss:0.178, val_acc:0.949]
Epoch [39/120    avg_loss:0.178, val_acc:0.945]
Epoch [40/120    avg_loss:0.186, val_acc:0.949]
Epoch [41/120    avg_loss:0.156, val_acc:0.953]
Epoch [42/120    avg_loss:0.129, val_acc:0.963]
Epoch [43/120    avg_loss:0.106, val_acc:0.965]
Epoch [44/120    avg_loss:0.133, val_acc:0.969]
Epoch [45/120    avg_loss:0.113, val_acc:0.961]
Epoch [46/120    avg_loss:0.112, val_acc:0.955]
Epoch [47/120    avg_loss:0.140, val_acc:0.918]
Epoch [48/120    avg_loss:0.119, val_acc:0.932]
Epoch [49/120    avg_loss:0.125, val_acc:0.949]
Epoch [50/120    avg_loss:0.079, val_acc:0.949]
Epoch [51/120    avg_loss:0.135, val_acc:0.959]
Epoch [52/120    avg_loss:0.068, val_acc:0.955]
Epoch [53/120    avg_loss:0.102, val_acc:0.967]
Epoch [54/120    avg_loss:0.074, val_acc:0.955]
Epoch [55/120    avg_loss:0.132, val_acc:0.963]
Epoch [56/120    avg_loss:0.086, val_acc:0.971]
Epoch [57/120    avg_loss:0.124, val_acc:0.943]
Epoch [58/120    avg_loss:0.164, val_acc:0.953]
Epoch [59/120    avg_loss:0.147, val_acc:0.957]
Epoch [60/120    avg_loss:0.116, val_acc:0.955]
Epoch [61/120    avg_loss:0.127, val_acc:0.965]
Epoch [62/120    avg_loss:0.084, val_acc:0.953]
Epoch [63/120    avg_loss:0.056, val_acc:0.959]
Epoch [64/120    avg_loss:0.092, val_acc:0.953]
Epoch [65/120    avg_loss:0.155, val_acc:0.959]
Epoch [66/120    avg_loss:0.061, val_acc:0.965]
Epoch [67/120    avg_loss:0.074, val_acc:0.982]
Epoch [68/120    avg_loss:0.119, val_acc:0.941]
Epoch [69/120    avg_loss:0.081, val_acc:0.965]
Epoch [70/120    avg_loss:0.075, val_acc:0.967]
Epoch [71/120    avg_loss:0.116, val_acc:0.969]
Epoch [72/120    avg_loss:0.079, val_acc:0.967]
Epoch [73/120    avg_loss:0.092, val_acc:0.953]
Epoch [74/120    avg_loss:0.061, val_acc:0.967]
Epoch [75/120    avg_loss:0.052, val_acc:0.975]
Epoch [76/120    avg_loss:0.091, val_acc:0.973]
Epoch [77/120    avg_loss:0.081, val_acc:0.955]
Epoch [78/120    avg_loss:0.092, val_acc:0.953]
Epoch [79/120    avg_loss:0.071, val_acc:0.965]
Epoch [80/120    avg_loss:0.082, val_acc:0.971]
Epoch [81/120    avg_loss:0.076, val_acc:0.977]
Epoch [82/120    avg_loss:0.045, val_acc:0.979]
Epoch [83/120    avg_loss:0.041, val_acc:0.979]
Epoch [84/120    avg_loss:0.023, val_acc:0.980]
Epoch [85/120    avg_loss:0.034, val_acc:0.980]
Epoch [86/120    avg_loss:0.036, val_acc:0.984]
Epoch [87/120    avg_loss:0.024, val_acc:0.980]
Epoch [88/120    avg_loss:0.022, val_acc:0.982]
Epoch [89/120    avg_loss:0.028, val_acc:0.984]
Epoch [90/120    avg_loss:0.020, val_acc:0.984]
Epoch [91/120    avg_loss:0.022, val_acc:0.986]
Epoch [92/120    avg_loss:0.030, val_acc:0.988]
Epoch [93/120    avg_loss:0.044, val_acc:0.988]
Epoch [94/120    avg_loss:0.022, val_acc:0.986]
Epoch [95/120    avg_loss:0.027, val_acc:0.986]
Epoch [96/120    avg_loss:0.014, val_acc:0.986]
Epoch [97/120    avg_loss:0.023, val_acc:0.986]
Epoch [98/120    avg_loss:0.015, val_acc:0.986]
Epoch [99/120    avg_loss:0.027, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.988]
Epoch [101/120    avg_loss:0.047, val_acc:0.988]
Epoch [102/120    avg_loss:0.026, val_acc:0.984]
Epoch [103/120    avg_loss:0.016, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.015, val_acc:0.980]
Epoch [106/120    avg_loss:0.032, val_acc:0.980]
Epoch [107/120    avg_loss:0.020, val_acc:0.980]
Epoch [108/120    avg_loss:0.025, val_acc:0.984]
Epoch [109/120    avg_loss:0.015, val_acc:0.986]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.018, val_acc:0.988]
Epoch [112/120    avg_loss:0.039, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.984]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.984]
Epoch [118/120    avg_loss:0.021, val_acc:0.984]
Epoch [119/120    avg_loss:0.020, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 215   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 219   5   0   0   0   2   4   0   0   0   0]
 [  0   0   0   0 210  15   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   1   0   0   5   0 200   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.997815   0.97949886 0.97550111 0.91503268 0.90784983
 0.98522167 0.98947368 0.99742931 0.99574468 1.         1.
 0.99224806 1.        ]

Kappa:
0.986942992330052
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa20c69e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.930, val_acc:0.656]
Epoch [2/120    avg_loss:1.181, val_acc:0.625]
Epoch [3/120    avg_loss:1.027, val_acc:0.707]
Epoch [4/120    avg_loss:0.835, val_acc:0.812]
Epoch [5/120    avg_loss:0.704, val_acc:0.791]
Epoch [6/120    avg_loss:0.703, val_acc:0.850]
Epoch [7/120    avg_loss:0.589, val_acc:0.869]
Epoch [8/120    avg_loss:0.504, val_acc:0.875]
Epoch [9/120    avg_loss:0.498, val_acc:0.861]
Epoch [10/120    avg_loss:0.425, val_acc:0.877]
Epoch [11/120    avg_loss:0.576, val_acc:0.863]
Epoch [12/120    avg_loss:0.448, val_acc:0.844]
Epoch [13/120    avg_loss:0.393, val_acc:0.896]
Epoch [14/120    avg_loss:0.318, val_acc:0.863]
Epoch [15/120    avg_loss:0.422, val_acc:0.865]
Epoch [16/120    avg_loss:0.347, val_acc:0.850]
Epoch [17/120    avg_loss:0.431, val_acc:0.904]
Epoch [18/120    avg_loss:0.301, val_acc:0.893]
Epoch [19/120    avg_loss:0.277, val_acc:0.904]
Epoch [20/120    avg_loss:0.355, val_acc:0.914]
Epoch [21/120    avg_loss:0.379, val_acc:0.893]
Epoch [22/120    avg_loss:0.284, val_acc:0.920]
Epoch [23/120    avg_loss:0.300, val_acc:0.898]
Epoch [24/120    avg_loss:0.267, val_acc:0.867]
Epoch [25/120    avg_loss:0.318, val_acc:0.932]
Epoch [26/120    avg_loss:0.287, val_acc:0.881]
Epoch [27/120    avg_loss:0.260, val_acc:0.947]
Epoch [28/120    avg_loss:0.225, val_acc:0.908]
Epoch [29/120    avg_loss:0.311, val_acc:0.912]
Epoch [30/120    avg_loss:0.275, val_acc:0.914]
Epoch [31/120    avg_loss:0.227, val_acc:0.934]
Epoch [32/120    avg_loss:0.198, val_acc:0.939]
Epoch [33/120    avg_loss:0.229, val_acc:0.930]
Epoch [34/120    avg_loss:0.140, val_acc:0.945]
Epoch [35/120    avg_loss:0.190, val_acc:0.943]
Epoch [36/120    avg_loss:0.153, val_acc:0.914]
Epoch [37/120    avg_loss:0.249, val_acc:0.932]
Epoch [38/120    avg_loss:0.171, val_acc:0.951]
Epoch [39/120    avg_loss:0.150, val_acc:0.955]
Epoch [40/120    avg_loss:0.169, val_acc:0.912]
Epoch [41/120    avg_loss:0.198, val_acc:0.945]
Epoch [42/120    avg_loss:0.261, val_acc:0.916]
Epoch [43/120    avg_loss:0.223, val_acc:0.941]
Epoch [44/120    avg_loss:0.200, val_acc:0.955]
Epoch [45/120    avg_loss:0.092, val_acc:0.928]
Epoch [46/120    avg_loss:0.157, val_acc:0.908]
Epoch [47/120    avg_loss:0.147, val_acc:0.928]
Epoch [48/120    avg_loss:0.159, val_acc:0.941]
Epoch [49/120    avg_loss:0.101, val_acc:0.943]
Epoch [50/120    avg_loss:0.112, val_acc:0.938]
Epoch [51/120    avg_loss:0.160, val_acc:0.941]
Epoch [52/120    avg_loss:0.115, val_acc:0.945]
Epoch [53/120    avg_loss:0.138, val_acc:0.951]
Epoch [54/120    avg_loss:0.100, val_acc:0.941]
Epoch [55/120    avg_loss:0.107, val_acc:0.953]
Epoch [56/120    avg_loss:0.180, val_acc:0.932]
Epoch [57/120    avg_loss:0.177, val_acc:0.928]
Epoch [58/120    avg_loss:0.126, val_acc:0.945]
Epoch [59/120    avg_loss:0.098, val_acc:0.949]
Epoch [60/120    avg_loss:0.078, val_acc:0.951]
Epoch [61/120    avg_loss:0.057, val_acc:0.953]
Epoch [62/120    avg_loss:0.057, val_acc:0.953]
Epoch [63/120    avg_loss:0.051, val_acc:0.953]
Epoch [64/120    avg_loss:0.064, val_acc:0.957]
Epoch [65/120    avg_loss:0.059, val_acc:0.957]
Epoch [66/120    avg_loss:0.051, val_acc:0.959]
Epoch [67/120    avg_loss:0.054, val_acc:0.953]
Epoch [68/120    avg_loss:0.059, val_acc:0.953]
Epoch [69/120    avg_loss:0.070, val_acc:0.961]
Epoch [70/120    avg_loss:0.051, val_acc:0.961]
Epoch [71/120    avg_loss:0.050, val_acc:0.963]
Epoch [72/120    avg_loss:0.037, val_acc:0.965]
Epoch [73/120    avg_loss:0.055, val_acc:0.959]
Epoch [74/120    avg_loss:0.043, val_acc:0.959]
Epoch [75/120    avg_loss:0.040, val_acc:0.961]
Epoch [76/120    avg_loss:0.037, val_acc:0.965]
Epoch [77/120    avg_loss:0.032, val_acc:0.965]
Epoch [78/120    avg_loss:0.038, val_acc:0.963]
Epoch [79/120    avg_loss:0.047, val_acc:0.967]
Epoch [80/120    avg_loss:0.043, val_acc:0.967]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.032, val_acc:0.967]
Epoch [83/120    avg_loss:0.039, val_acc:0.969]
Epoch [84/120    avg_loss:0.051, val_acc:0.969]
Epoch [85/120    avg_loss:0.037, val_acc:0.965]
Epoch [86/120    avg_loss:0.048, val_acc:0.969]
Epoch [87/120    avg_loss:0.041, val_acc:0.965]
Epoch [88/120    avg_loss:0.040, val_acc:0.961]
Epoch [89/120    avg_loss:0.028, val_acc:0.963]
Epoch [90/120    avg_loss:0.034, val_acc:0.969]
Epoch [91/120    avg_loss:0.030, val_acc:0.969]
Epoch [92/120    avg_loss:0.046, val_acc:0.971]
Epoch [93/120    avg_loss:0.047, val_acc:0.969]
Epoch [94/120    avg_loss:0.029, val_acc:0.973]
Epoch [95/120    avg_loss:0.050, val_acc:0.969]
Epoch [96/120    avg_loss:0.030, val_acc:0.971]
Epoch [97/120    avg_loss:0.036, val_acc:0.971]
Epoch [98/120    avg_loss:0.026, val_acc:0.971]
Epoch [99/120    avg_loss:0.030, val_acc:0.975]
Epoch [100/120    avg_loss:0.025, val_acc:0.971]
Epoch [101/120    avg_loss:0.048, val_acc:0.977]
Epoch [102/120    avg_loss:0.030, val_acc:0.975]
Epoch [103/120    avg_loss:0.029, val_acc:0.977]
Epoch [104/120    avg_loss:0.024, val_acc:0.971]
Epoch [105/120    avg_loss:0.031, val_acc:0.973]
Epoch [106/120    avg_loss:0.024, val_acc:0.973]
Epoch [107/120    avg_loss:0.030, val_acc:0.971]
Epoch [108/120    avg_loss:0.038, val_acc:0.969]
Epoch [109/120    avg_loss:0.038, val_acc:0.973]
Epoch [110/120    avg_loss:0.034, val_acc:0.977]
Epoch [111/120    avg_loss:0.027, val_acc:0.979]
Epoch [112/120    avg_loss:0.024, val_acc:0.973]
Epoch [113/120    avg_loss:0.018, val_acc:0.975]
Epoch [114/120    avg_loss:0.027, val_acc:0.973]
Epoch [115/120    avg_loss:0.018, val_acc:0.977]
Epoch [116/120    avg_loss:0.031, val_acc:0.975]
Epoch [117/120    avg_loss:0.020, val_acc:0.975]
Epoch [118/120    avg_loss:0.047, val_acc:0.977]
Epoch [119/120    avg_loss:0.024, val_acc:0.975]
Epoch [120/120    avg_loss:0.036, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   2   0   0   0   0   3   0]
 [  0   0   0 218   8   0   0   0   0   4   0   0   0   0]
 [  0   0   0   0 210  14   0   0   0   0   0   0   3   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0  11   0   0   1   0 194   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 0.99203476 0.97716895 0.97321429 0.91106291 0.89965398
 0.97       0.98947368 1.         0.99574468 1.         1.
 0.98787211 1.        ]

Kappa:
0.9843278583279708
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b5b5377f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.921, val_acc:0.529]
Epoch [2/120    avg_loss:1.197, val_acc:0.719]
Epoch [3/120    avg_loss:0.926, val_acc:0.760]
Epoch [4/120    avg_loss:0.787, val_acc:0.717]
Epoch [5/120    avg_loss:0.697, val_acc:0.838]
Epoch [6/120    avg_loss:0.686, val_acc:0.852]
Epoch [7/120    avg_loss:0.610, val_acc:0.762]
Epoch [8/120    avg_loss:0.561, val_acc:0.861]
Epoch [9/120    avg_loss:0.566, val_acc:0.854]
Epoch [10/120    avg_loss:0.504, val_acc:0.836]
Epoch [11/120    avg_loss:0.514, val_acc:0.842]
Epoch [12/120    avg_loss:0.577, val_acc:0.852]
Epoch [13/120    avg_loss:0.539, val_acc:0.891]
Epoch [14/120    avg_loss:0.461, val_acc:0.859]
Epoch [15/120    avg_loss:0.379, val_acc:0.891]
Epoch [16/120    avg_loss:0.352, val_acc:0.895]
Epoch [17/120    avg_loss:0.348, val_acc:0.883]
Epoch [18/120    avg_loss:0.326, val_acc:0.918]
Epoch [19/120    avg_loss:0.394, val_acc:0.877]
Epoch [20/120    avg_loss:0.409, val_acc:0.857]
Epoch [21/120    avg_loss:0.403, val_acc:0.895]
Epoch [22/120    avg_loss:0.322, val_acc:0.900]
Epoch [23/120    avg_loss:0.280, val_acc:0.932]
Epoch [24/120    avg_loss:0.234, val_acc:0.904]
Epoch [25/120    avg_loss:0.286, val_acc:0.879]
Epoch [26/120    avg_loss:0.338, val_acc:0.908]
Epoch [27/120    avg_loss:0.257, val_acc:0.922]
Epoch [28/120    avg_loss:0.251, val_acc:0.932]
Epoch [29/120    avg_loss:0.206, val_acc:0.932]
Epoch [30/120    avg_loss:0.166, val_acc:0.920]
Epoch [31/120    avg_loss:0.216, val_acc:0.928]
Epoch [32/120    avg_loss:0.207, val_acc:0.936]
Epoch [33/120    avg_loss:0.281, val_acc:0.928]
Epoch [34/120    avg_loss:0.223, val_acc:0.934]
Epoch [35/120    avg_loss:0.311, val_acc:0.918]
Epoch [36/120    avg_loss:0.220, val_acc:0.918]
Epoch [37/120    avg_loss:0.253, val_acc:0.934]
Epoch [38/120    avg_loss:0.193, val_acc:0.928]
Epoch [39/120    avg_loss:0.179, val_acc:0.943]
Epoch [40/120    avg_loss:0.192, val_acc:0.953]
Epoch [41/120    avg_loss:0.145, val_acc:0.938]
Epoch [42/120    avg_loss:0.165, val_acc:0.943]
Epoch [43/120    avg_loss:0.176, val_acc:0.932]
Epoch [44/120    avg_loss:0.185, val_acc:0.934]
Epoch [45/120    avg_loss:0.183, val_acc:0.951]
Epoch [46/120    avg_loss:0.175, val_acc:0.922]
Epoch [47/120    avg_loss:0.182, val_acc:0.926]
Epoch [48/120    avg_loss:0.178, val_acc:0.939]
Epoch [49/120    avg_loss:0.145, val_acc:0.949]
Epoch [50/120    avg_loss:0.149, val_acc:0.963]
Epoch [51/120    avg_loss:0.083, val_acc:0.953]
Epoch [52/120    avg_loss:0.137, val_acc:0.957]
Epoch [53/120    avg_loss:0.059, val_acc:0.967]
Epoch [54/120    avg_loss:0.050, val_acc:0.969]
Epoch [55/120    avg_loss:0.169, val_acc:0.934]
Epoch [56/120    avg_loss:0.205, val_acc:0.959]
Epoch [57/120    avg_loss:0.104, val_acc:0.959]
Epoch [58/120    avg_loss:0.109, val_acc:0.953]
Epoch [59/120    avg_loss:0.078, val_acc:0.961]
Epoch [60/120    avg_loss:0.076, val_acc:0.969]
Epoch [61/120    avg_loss:0.070, val_acc:0.965]
Epoch [62/120    avg_loss:0.052, val_acc:0.971]
Epoch [63/120    avg_loss:0.124, val_acc:0.951]
Epoch [64/120    avg_loss:0.117, val_acc:0.963]
Epoch [65/120    avg_loss:0.170, val_acc:0.938]
Epoch [66/120    avg_loss:0.127, val_acc:0.955]
Epoch [67/120    avg_loss:0.092, val_acc:0.916]
Epoch [68/120    avg_loss:0.131, val_acc:0.971]
Epoch [69/120    avg_loss:0.131, val_acc:0.965]
Epoch [70/120    avg_loss:0.087, val_acc:0.955]
Epoch [71/120    avg_loss:0.084, val_acc:0.969]
Epoch [72/120    avg_loss:0.045, val_acc:0.969]
Epoch [73/120    avg_loss:0.100, val_acc:0.959]
Epoch [74/120    avg_loss:0.120, val_acc:0.955]
Epoch [75/120    avg_loss:0.122, val_acc:0.936]
Epoch [76/120    avg_loss:0.092, val_acc:0.947]
Epoch [77/120    avg_loss:0.074, val_acc:0.969]
Epoch [78/120    avg_loss:0.038, val_acc:0.961]
Epoch [79/120    avg_loss:0.072, val_acc:0.975]
Epoch [80/120    avg_loss:0.117, val_acc:0.963]
Epoch [81/120    avg_loss:0.063, val_acc:0.959]
Epoch [82/120    avg_loss:0.092, val_acc:0.955]
Epoch [83/120    avg_loss:0.058, val_acc:0.969]
Epoch [84/120    avg_loss:0.058, val_acc:0.965]
Epoch [85/120    avg_loss:0.082, val_acc:0.967]
Epoch [86/120    avg_loss:0.111, val_acc:0.969]
Epoch [87/120    avg_loss:0.086, val_acc:0.984]
Epoch [88/120    avg_loss:0.025, val_acc:0.984]
Epoch [89/120    avg_loss:0.019, val_acc:0.973]
Epoch [90/120    avg_loss:0.048, val_acc:0.969]
Epoch [91/120    avg_loss:0.038, val_acc:0.979]
Epoch [92/120    avg_loss:0.037, val_acc:0.979]
Epoch [93/120    avg_loss:0.041, val_acc:0.965]
Epoch [94/120    avg_loss:0.042, val_acc:0.971]
Epoch [95/120    avg_loss:0.034, val_acc:0.973]
Epoch [96/120    avg_loss:0.022, val_acc:0.980]
Epoch [97/120    avg_loss:0.035, val_acc:0.971]
Epoch [98/120    avg_loss:0.085, val_acc:0.961]
Epoch [99/120    avg_loss:0.116, val_acc:0.949]
Epoch [100/120    avg_loss:0.090, val_acc:0.967]
Epoch [101/120    avg_loss:0.070, val_acc:0.973]
Epoch [102/120    avg_loss:0.040, val_acc:0.975]
Epoch [103/120    avg_loss:0.029, val_acc:0.973]
Epoch [104/120    avg_loss:0.019, val_acc:0.971]
Epoch [105/120    avg_loss:0.020, val_acc:0.973]
Epoch [106/120    avg_loss:0.023, val_acc:0.973]
Epoch [107/120    avg_loss:0.024, val_acc:0.977]
Epoch [108/120    avg_loss:0.013, val_acc:0.979]
Epoch [109/120    avg_loss:0.013, val_acc:0.979]
Epoch [110/120    avg_loss:0.030, val_acc:0.975]
Epoch [111/120    avg_loss:0.017, val_acc:0.980]
Epoch [112/120    avg_loss:0.024, val_acc:0.979]
Epoch [113/120    avg_loss:0.028, val_acc:0.979]
Epoch [114/120    avg_loss:0.024, val_acc:0.977]
Epoch [115/120    avg_loss:0.014, val_acc:0.977]
Epoch [116/120    avg_loss:0.025, val_acc:0.977]
Epoch [117/120    avg_loss:0.008, val_acc:0.977]
Epoch [118/120    avg_loss:0.013, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.977]
Epoch [120/120    avg_loss:0.027, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 217   7   2   0   0   0   4   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.97949886 0.97091723 0.93220339 0.91929825
 0.99019608 0.97916667 1.         0.99574468 1.         1.
 0.99445061 1.        ]

Kappa:
0.9888432133630801
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbe430ff7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.961, val_acc:0.658]
Epoch [2/120    avg_loss:1.162, val_acc:0.770]
Epoch [3/120    avg_loss:0.891, val_acc:0.721]
Epoch [4/120    avg_loss:0.913, val_acc:0.812]
Epoch [5/120    avg_loss:0.710, val_acc:0.838]
Epoch [6/120    avg_loss:0.623, val_acc:0.855]
Epoch [7/120    avg_loss:0.639, val_acc:0.865]
Epoch [8/120    avg_loss:0.626, val_acc:0.863]
Epoch [9/120    avg_loss:0.498, val_acc:0.867]
Epoch [10/120    avg_loss:0.621, val_acc:0.859]
Epoch [11/120    avg_loss:0.469, val_acc:0.875]
Epoch [12/120    avg_loss:0.402, val_acc:0.910]
Epoch [13/120    avg_loss:0.418, val_acc:0.906]
Epoch [14/120    avg_loss:0.342, val_acc:0.900]
Epoch [15/120    avg_loss:0.302, val_acc:0.932]
Epoch [16/120    avg_loss:0.274, val_acc:0.939]
Epoch [17/120    avg_loss:0.358, val_acc:0.887]
Epoch [18/120    avg_loss:0.308, val_acc:0.887]
Epoch [19/120    avg_loss:0.313, val_acc:0.906]
Epoch [20/120    avg_loss:0.286, val_acc:0.904]
Epoch [21/120    avg_loss:0.337, val_acc:0.930]
Epoch [22/120    avg_loss:0.263, val_acc:0.920]
Epoch [23/120    avg_loss:0.310, val_acc:0.924]
Epoch [24/120    avg_loss:0.221, val_acc:0.895]
Epoch [25/120    avg_loss:0.302, val_acc:0.908]
Epoch [26/120    avg_loss:0.274, val_acc:0.947]
Epoch [27/120    avg_loss:0.251, val_acc:0.916]
Epoch [28/120    avg_loss:0.233, val_acc:0.904]
Epoch [29/120    avg_loss:0.169, val_acc:0.955]
Epoch [30/120    avg_loss:0.166, val_acc:0.951]
Epoch [31/120    avg_loss:0.179, val_acc:0.926]
Epoch [32/120    avg_loss:0.182, val_acc:0.963]
Epoch [33/120    avg_loss:0.184, val_acc:0.959]
Epoch [34/120    avg_loss:0.201, val_acc:0.949]
Epoch [35/120    avg_loss:0.199, val_acc:0.932]
Epoch [36/120    avg_loss:0.171, val_acc:0.924]
Epoch [37/120    avg_loss:0.118, val_acc:0.967]
Epoch [38/120    avg_loss:0.153, val_acc:0.965]
Epoch [39/120    avg_loss:0.136, val_acc:0.957]
Epoch [40/120    avg_loss:0.103, val_acc:0.947]
Epoch [41/120    avg_loss:0.144, val_acc:0.955]
Epoch [42/120    avg_loss:0.104, val_acc:0.957]
Epoch [43/120    avg_loss:0.076, val_acc:0.957]
Epoch [44/120    avg_loss:0.182, val_acc:0.877]
Epoch [45/120    avg_loss:0.235, val_acc:0.936]
Epoch [46/120    avg_loss:0.156, val_acc:0.943]
Epoch [47/120    avg_loss:0.190, val_acc:0.961]
Epoch [48/120    avg_loss:0.127, val_acc:0.932]
Epoch [49/120    avg_loss:0.101, val_acc:0.959]
Epoch [50/120    avg_loss:0.116, val_acc:0.932]
Epoch [51/120    avg_loss:0.125, val_acc:0.963]
Epoch [52/120    avg_loss:0.059, val_acc:0.969]
Epoch [53/120    avg_loss:0.064, val_acc:0.973]
Epoch [54/120    avg_loss:0.046, val_acc:0.973]
Epoch [55/120    avg_loss:0.050, val_acc:0.973]
Epoch [56/120    avg_loss:0.044, val_acc:0.975]
Epoch [57/120    avg_loss:0.040, val_acc:0.977]
Epoch [58/120    avg_loss:0.043, val_acc:0.979]
Epoch [59/120    avg_loss:0.035, val_acc:0.975]
Epoch [60/120    avg_loss:0.038, val_acc:0.977]
Epoch [61/120    avg_loss:0.044, val_acc:0.980]
Epoch [62/120    avg_loss:0.045, val_acc:0.980]
Epoch [63/120    avg_loss:0.037, val_acc:0.979]
Epoch [64/120    avg_loss:0.036, val_acc:0.973]
Epoch [65/120    avg_loss:0.032, val_acc:0.977]
Epoch [66/120    avg_loss:0.037, val_acc:0.980]
Epoch [67/120    avg_loss:0.028, val_acc:0.980]
Epoch [68/120    avg_loss:0.051, val_acc:0.979]
Epoch [69/120    avg_loss:0.028, val_acc:0.982]
Epoch [70/120    avg_loss:0.034, val_acc:0.980]
Epoch [71/120    avg_loss:0.029, val_acc:0.975]
Epoch [72/120    avg_loss:0.036, val_acc:0.975]
Epoch [73/120    avg_loss:0.049, val_acc:0.977]
Epoch [74/120    avg_loss:0.025, val_acc:0.979]
Epoch [75/120    avg_loss:0.049, val_acc:0.980]
Epoch [76/120    avg_loss:0.037, val_acc:0.980]
Epoch [77/120    avg_loss:0.022, val_acc:0.979]
Epoch [78/120    avg_loss:0.027, val_acc:0.982]
Epoch [79/120    avg_loss:0.033, val_acc:0.973]
Epoch [80/120    avg_loss:0.039, val_acc:0.973]
Epoch [81/120    avg_loss:0.030, val_acc:0.975]
Epoch [82/120    avg_loss:0.027, val_acc:0.977]
Epoch [83/120    avg_loss:0.033, val_acc:0.982]
Epoch [84/120    avg_loss:0.018, val_acc:0.982]
Epoch [85/120    avg_loss:0.028, val_acc:0.982]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.029, val_acc:0.980]
Epoch [88/120    avg_loss:0.043, val_acc:0.980]
Epoch [89/120    avg_loss:0.038, val_acc:0.984]
Epoch [90/120    avg_loss:0.039, val_acc:0.980]
Epoch [91/120    avg_loss:0.034, val_acc:0.977]
Epoch [92/120    avg_loss:0.019, val_acc:0.982]
Epoch [93/120    avg_loss:0.022, val_acc:0.979]
Epoch [94/120    avg_loss:0.018, val_acc:0.980]
Epoch [95/120    avg_loss:0.031, val_acc:0.980]
Epoch [96/120    avg_loss:0.021, val_acc:0.982]
Epoch [97/120    avg_loss:0.027, val_acc:0.982]
Epoch [98/120    avg_loss:0.024, val_acc:0.982]
Epoch [99/120    avg_loss:0.026, val_acc:0.982]
Epoch [100/120    avg_loss:0.020, val_acc:0.982]
Epoch [101/120    avg_loss:0.021, val_acc:0.980]
Epoch [102/120    avg_loss:0.024, val_acc:0.982]
Epoch [103/120    avg_loss:0.027, val_acc:0.982]
Epoch [104/120    avg_loss:0.017, val_acc:0.982]
Epoch [105/120    avg_loss:0.018, val_acc:0.982]
Epoch [106/120    avg_loss:0.028, val_acc:0.982]
Epoch [107/120    avg_loss:0.024, val_acc:0.982]
Epoch [108/120    avg_loss:0.023, val_acc:0.982]
Epoch [109/120    avg_loss:0.023, val_acc:0.982]
Epoch [110/120    avg_loss:0.024, val_acc:0.982]
Epoch [111/120    avg_loss:0.026, val_acc:0.982]
Epoch [112/120    avg_loss:0.022, val_acc:0.982]
Epoch [113/120    avg_loss:0.020, val_acc:0.982]
Epoch [114/120    avg_loss:0.036, val_acc:0.982]
Epoch [115/120    avg_loss:0.018, val_acc:0.982]
Epoch [116/120    avg_loss:0.023, val_acc:0.982]
Epoch [117/120    avg_loss:0.027, val_acc:0.982]
Epoch [118/120    avg_loss:0.027, val_acc:0.982]
Epoch [119/120    avg_loss:0.033, val_acc:0.982]
Epoch [120/120    avg_loss:0.042, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   4   0   0   0   0   3   0]
 [  0   0   0 221   7   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  27 118   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   6   0   0   0   0   2   0   0   0   0 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.95711061 0.98004435 0.89669421 0.86446886
 0.98522167 0.93617021 0.998713   0.99893276 1.         1.
 0.98779134 1.        ]

Kappa:
0.9826705178876527
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75ebad4748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.982, val_acc:0.523]
Epoch [2/120    avg_loss:1.182, val_acc:0.658]
Epoch [3/120    avg_loss:0.888, val_acc:0.852]
Epoch [4/120    avg_loss:0.720, val_acc:0.836]
Epoch [5/120    avg_loss:0.690, val_acc:0.814]
Epoch [6/120    avg_loss:0.588, val_acc:0.861]
Epoch [7/120    avg_loss:0.747, val_acc:0.840]
Epoch [8/120    avg_loss:0.574, val_acc:0.885]
Epoch [9/120    avg_loss:0.540, val_acc:0.902]
Epoch [10/120    avg_loss:0.469, val_acc:0.900]
Epoch [11/120    avg_loss:0.479, val_acc:0.924]
Epoch [12/120    avg_loss:0.485, val_acc:0.846]
Epoch [13/120    avg_loss:0.487, val_acc:0.912]
Epoch [14/120    avg_loss:0.380, val_acc:0.902]
Epoch [15/120    avg_loss:0.446, val_acc:0.877]
Epoch [16/120    avg_loss:0.350, val_acc:0.916]
Epoch [17/120    avg_loss:0.264, val_acc:0.895]
Epoch [18/120    avg_loss:0.345, val_acc:0.914]
Epoch [19/120    avg_loss:0.391, val_acc:0.938]
Epoch [20/120    avg_loss:0.325, val_acc:0.922]
Epoch [21/120    avg_loss:0.356, val_acc:0.889]
Epoch [22/120    avg_loss:0.388, val_acc:0.920]
Epoch [23/120    avg_loss:0.274, val_acc:0.932]
Epoch [24/120    avg_loss:0.204, val_acc:0.906]
Epoch [25/120    avg_loss:0.317, val_acc:0.930]
Epoch [26/120    avg_loss:0.289, val_acc:0.943]
Epoch [27/120    avg_loss:0.204, val_acc:0.914]
Epoch [28/120    avg_loss:0.281, val_acc:0.934]
Epoch [29/120    avg_loss:0.236, val_acc:0.943]
Epoch [30/120    avg_loss:0.188, val_acc:0.955]
Epoch [31/120    avg_loss:0.195, val_acc:0.938]
Epoch [32/120    avg_loss:0.189, val_acc:0.855]
Epoch [33/120    avg_loss:0.291, val_acc:0.941]
Epoch [34/120    avg_loss:0.216, val_acc:0.963]
Epoch [35/120    avg_loss:0.184, val_acc:0.953]
Epoch [36/120    avg_loss:0.136, val_acc:0.939]
Epoch [37/120    avg_loss:0.191, val_acc:0.959]
Epoch [38/120    avg_loss:0.122, val_acc:0.928]
Epoch [39/120    avg_loss:0.257, val_acc:0.938]
Epoch [40/120    avg_loss:0.184, val_acc:0.975]
Epoch [41/120    avg_loss:0.154, val_acc:0.965]
Epoch [42/120    avg_loss:0.104, val_acc:0.971]
Epoch [43/120    avg_loss:0.159, val_acc:0.975]
Epoch [44/120    avg_loss:0.142, val_acc:0.973]
Epoch [45/120    avg_loss:0.104, val_acc:0.955]
Epoch [46/120    avg_loss:0.169, val_acc:0.930]
Epoch [47/120    avg_loss:0.194, val_acc:0.971]
Epoch [48/120    avg_loss:0.144, val_acc:0.963]
Epoch [49/120    avg_loss:0.212, val_acc:0.961]
Epoch [50/120    avg_loss:0.154, val_acc:0.957]
Epoch [51/120    avg_loss:0.090, val_acc:0.961]
Epoch [52/120    avg_loss:0.115, val_acc:0.922]
Epoch [53/120    avg_loss:0.126, val_acc:0.979]
Epoch [54/120    avg_loss:0.084, val_acc:0.979]
Epoch [55/120    avg_loss:0.079, val_acc:0.971]
Epoch [56/120    avg_loss:0.074, val_acc:0.977]
Epoch [57/120    avg_loss:0.115, val_acc:0.967]
Epoch [58/120    avg_loss:0.086, val_acc:0.977]
Epoch [59/120    avg_loss:0.072, val_acc:0.922]
Epoch [60/120    avg_loss:0.147, val_acc:0.982]
Epoch [61/120    avg_loss:0.070, val_acc:0.979]
Epoch [62/120    avg_loss:0.058, val_acc:0.975]
Epoch [63/120    avg_loss:0.148, val_acc:0.961]
Epoch [64/120    avg_loss:0.124, val_acc:0.969]
Epoch [65/120    avg_loss:0.060, val_acc:0.949]
Epoch [66/120    avg_loss:0.131, val_acc:0.969]
Epoch [67/120    avg_loss:0.069, val_acc:0.982]
Epoch [68/120    avg_loss:0.039, val_acc:0.973]
Epoch [69/120    avg_loss:0.041, val_acc:0.979]
Epoch [70/120    avg_loss:0.052, val_acc:0.969]
Epoch [71/120    avg_loss:0.042, val_acc:0.984]
Epoch [72/120    avg_loss:0.079, val_acc:0.967]
Epoch [73/120    avg_loss:0.086, val_acc:0.973]
Epoch [74/120    avg_loss:0.097, val_acc:0.971]
Epoch [75/120    avg_loss:0.097, val_acc:0.980]
Epoch [76/120    avg_loss:0.081, val_acc:0.980]
Epoch [77/120    avg_loss:0.043, val_acc:0.982]
Epoch [78/120    avg_loss:0.090, val_acc:0.975]
Epoch [79/120    avg_loss:0.035, val_acc:0.975]
Epoch [80/120    avg_loss:0.057, val_acc:0.986]
Epoch [81/120    avg_loss:0.037, val_acc:0.977]
Epoch [82/120    avg_loss:0.070, val_acc:0.988]
Epoch [83/120    avg_loss:0.040, val_acc:0.980]
Epoch [84/120    avg_loss:0.104, val_acc:0.980]
Epoch [85/120    avg_loss:0.067, val_acc:0.965]
Epoch [86/120    avg_loss:0.114, val_acc:0.979]
Epoch [87/120    avg_loss:0.036, val_acc:0.982]
Epoch [88/120    avg_loss:0.046, val_acc:0.979]
Epoch [89/120    avg_loss:0.041, val_acc:0.986]
Epoch [90/120    avg_loss:0.034, val_acc:0.990]
Epoch [91/120    avg_loss:0.082, val_acc:0.965]
Epoch [92/120    avg_loss:0.087, val_acc:0.982]
Epoch [93/120    avg_loss:0.025, val_acc:0.984]
Epoch [94/120    avg_loss:0.025, val_acc:0.984]
Epoch [95/120    avg_loss:0.047, val_acc:0.982]
Epoch [96/120    avg_loss:0.046, val_acc:0.973]
Epoch [97/120    avg_loss:0.068, val_acc:0.971]
Epoch [98/120    avg_loss:0.039, val_acc:0.975]
Epoch [99/120    avg_loss:0.069, val_acc:0.988]
Epoch [100/120    avg_loss:0.039, val_acc:0.984]
Epoch [101/120    avg_loss:0.037, val_acc:0.986]
Epoch [102/120    avg_loss:0.081, val_acc:0.982]
Epoch [103/120    avg_loss:0.055, val_acc:0.977]
Epoch [104/120    avg_loss:0.025, val_acc:0.984]
Epoch [105/120    avg_loss:0.031, val_acc:0.982]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.988]
Epoch [108/120    avg_loss:0.020, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.988]
Epoch [110/120    avg_loss:0.014, val_acc:0.986]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.019, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.019, val_acc:0.990]
Epoch [115/120    avg_loss:0.021, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.992]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 220   6   2   0   0   0   2   0   0   0   0]
 [  0   0   0   0 223   2   0   0   0   0   0   0   2   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.97949886 0.97777778 0.94291755 0.92418773
 1.         0.99470899 1.         0.9978678  1.         1.
 0.98896247 1.        ]

Kappa:
0.9905040799807531
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b1e3287b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.952, val_acc:0.656]
Epoch [2/120    avg_loss:1.151, val_acc:0.666]
Epoch [3/120    avg_loss:0.906, val_acc:0.713]
Epoch [4/120    avg_loss:0.753, val_acc:0.715]
Epoch [5/120    avg_loss:0.753, val_acc:0.848]
Epoch [6/120    avg_loss:0.634, val_acc:0.844]
Epoch [7/120    avg_loss:0.571, val_acc:0.812]
Epoch [8/120    avg_loss:0.607, val_acc:0.836]
Epoch [9/120    avg_loss:0.613, val_acc:0.828]
Epoch [10/120    avg_loss:0.610, val_acc:0.855]
Epoch [11/120    avg_loss:0.703, val_acc:0.848]
Epoch [12/120    avg_loss:0.602, val_acc:0.846]
Epoch [13/120    avg_loss:0.473, val_acc:0.904]
Epoch [14/120    avg_loss:0.402, val_acc:0.887]
Epoch [15/120    avg_loss:0.343, val_acc:0.906]
Epoch [16/120    avg_loss:0.383, val_acc:0.852]
Epoch [17/120    avg_loss:0.330, val_acc:0.908]
Epoch [18/120    avg_loss:0.452, val_acc:0.816]
Epoch [19/120    avg_loss:0.434, val_acc:0.895]
Epoch [20/120    avg_loss:0.436, val_acc:0.902]
Epoch [21/120    avg_loss:0.259, val_acc:0.918]
Epoch [22/120    avg_loss:0.243, val_acc:0.893]
Epoch [23/120    avg_loss:0.331, val_acc:0.879]
Epoch [24/120    avg_loss:0.372, val_acc:0.885]
Epoch [25/120    avg_loss:0.375, val_acc:0.914]
Epoch [26/120    avg_loss:0.229, val_acc:0.914]
Epoch [27/120    avg_loss:0.269, val_acc:0.902]
Epoch [28/120    avg_loss:0.250, val_acc:0.924]
Epoch [29/120    avg_loss:0.220, val_acc:0.924]
Epoch [30/120    avg_loss:0.294, val_acc:0.916]
Epoch [31/120    avg_loss:0.203, val_acc:0.926]
Epoch [32/120    avg_loss:0.241, val_acc:0.928]
Epoch [33/120    avg_loss:0.188, val_acc:0.900]
Epoch [34/120    avg_loss:0.169, val_acc:0.900]
Epoch [35/120    avg_loss:0.275, val_acc:0.922]
Epoch [36/120    avg_loss:0.259, val_acc:0.906]
Epoch [37/120    avg_loss:0.209, val_acc:0.930]
Epoch [38/120    avg_loss:0.254, val_acc:0.924]
Epoch [39/120    avg_loss:0.232, val_acc:0.934]
Epoch [40/120    avg_loss:0.159, val_acc:0.938]
Epoch [41/120    avg_loss:0.082, val_acc:0.949]
Epoch [42/120    avg_loss:0.117, val_acc:0.945]
Epoch [43/120    avg_loss:0.134, val_acc:0.934]
Epoch [44/120    avg_loss:0.170, val_acc:0.930]
Epoch [45/120    avg_loss:0.194, val_acc:0.939]
Epoch [46/120    avg_loss:0.207, val_acc:0.943]
Epoch [47/120    avg_loss:0.167, val_acc:0.938]
Epoch [48/120    avg_loss:0.153, val_acc:0.938]
Epoch [49/120    avg_loss:0.175, val_acc:0.928]
Epoch [50/120    avg_loss:0.154, val_acc:0.941]
Epoch [51/120    avg_loss:0.157, val_acc:0.939]
Epoch [52/120    avg_loss:0.184, val_acc:0.945]
Epoch [53/120    avg_loss:0.156, val_acc:0.938]
Epoch [54/120    avg_loss:0.157, val_acc:0.930]
Epoch [55/120    avg_loss:0.102, val_acc:0.939]
Epoch [56/120    avg_loss:0.089, val_acc:0.945]
Epoch [57/120    avg_loss:0.062, val_acc:0.945]
Epoch [58/120    avg_loss:0.095, val_acc:0.949]
Epoch [59/120    avg_loss:0.055, val_acc:0.947]
Epoch [60/120    avg_loss:0.059, val_acc:0.951]
Epoch [61/120    avg_loss:0.059, val_acc:0.951]
Epoch [62/120    avg_loss:0.054, val_acc:0.951]
Epoch [63/120    avg_loss:0.043, val_acc:0.953]
Epoch [64/120    avg_loss:0.065, val_acc:0.963]
Epoch [65/120    avg_loss:0.039, val_acc:0.965]
Epoch [66/120    avg_loss:0.060, val_acc:0.969]
Epoch [67/120    avg_loss:0.051, val_acc:0.969]
Epoch [68/120    avg_loss:0.059, val_acc:0.971]
Epoch [69/120    avg_loss:0.050, val_acc:0.973]
Epoch [70/120    avg_loss:0.036, val_acc:0.971]
Epoch [71/120    avg_loss:0.036, val_acc:0.971]
Epoch [72/120    avg_loss:0.037, val_acc:0.969]
Epoch [73/120    avg_loss:0.048, val_acc:0.975]
Epoch [74/120    avg_loss:0.055, val_acc:0.969]
Epoch [75/120    avg_loss:0.051, val_acc:0.975]
Epoch [76/120    avg_loss:0.041, val_acc:0.965]
Epoch [77/120    avg_loss:0.039, val_acc:0.973]
Epoch [78/120    avg_loss:0.050, val_acc:0.973]
Epoch [79/120    avg_loss:0.055, val_acc:0.975]
Epoch [80/120    avg_loss:0.030, val_acc:0.973]
Epoch [81/120    avg_loss:0.048, val_acc:0.975]
Epoch [82/120    avg_loss:0.048, val_acc:0.973]
Epoch [83/120    avg_loss:0.031, val_acc:0.973]
Epoch [84/120    avg_loss:0.033, val_acc:0.969]
Epoch [85/120    avg_loss:0.029, val_acc:0.969]
Epoch [86/120    avg_loss:0.043, val_acc:0.973]
Epoch [87/120    avg_loss:0.049, val_acc:0.971]
Epoch [88/120    avg_loss:0.039, val_acc:0.969]
Epoch [89/120    avg_loss:0.043, val_acc:0.975]
Epoch [90/120    avg_loss:0.038, val_acc:0.975]
Epoch [91/120    avg_loss:0.046, val_acc:0.975]
Epoch [92/120    avg_loss:0.037, val_acc:0.975]
Epoch [93/120    avg_loss:0.032, val_acc:0.973]
Epoch [94/120    avg_loss:0.032, val_acc:0.975]
Epoch [95/120    avg_loss:0.041, val_acc:0.973]
Epoch [96/120    avg_loss:0.041, val_acc:0.975]
Epoch [97/120    avg_loss:0.039, val_acc:0.971]
Epoch [98/120    avg_loss:0.039, val_acc:0.973]
Epoch [99/120    avg_loss:0.043, val_acc:0.977]
Epoch [100/120    avg_loss:0.059, val_acc:0.977]
Epoch [101/120    avg_loss:0.030, val_acc:0.975]
Epoch [102/120    avg_loss:0.036, val_acc:0.973]
Epoch [103/120    avg_loss:0.032, val_acc:0.971]
Epoch [104/120    avg_loss:0.036, val_acc:0.971]
Epoch [105/120    avg_loss:0.032, val_acc:0.975]
Epoch [106/120    avg_loss:0.037, val_acc:0.973]
Epoch [107/120    avg_loss:0.030, val_acc:0.975]
Epoch [108/120    avg_loss:0.032, val_acc:0.975]
Epoch [109/120    avg_loss:0.054, val_acc:0.973]
Epoch [110/120    avg_loss:0.029, val_acc:0.969]
Epoch [111/120    avg_loss:0.041, val_acc:0.969]
Epoch [112/120    avg_loss:0.039, val_acc:0.971]
Epoch [113/120    avg_loss:0.028, val_acc:0.973]
Epoch [114/120    avg_loss:0.043, val_acc:0.973]
Epoch [115/120    avg_loss:0.030, val_acc:0.973]
Epoch [116/120    avg_loss:0.032, val_acc:0.973]
Epoch [117/120    avg_loss:0.028, val_acc:0.973]
Epoch [118/120    avg_loss:0.044, val_acc:0.973]
Epoch [119/120    avg_loss:0.035, val_acc:0.973]
Epoch [120/120    avg_loss:0.025, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 210   0   0   0   0   5   0   0   0   0   3   0]
 [  0   0   0 223   5   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 217   8   0   0   1   0   0   0   1   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   2   0   0   4   0 200   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.997815   0.96551724 0.98454746 0.92735043 0.91872792
 0.98522167 0.96875    0.99742931 0.99893276 1.         1.
 0.99005525 1.        ]

Kappa:
0.9874175843422247
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f56a4798780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.965, val_acc:0.504]
Epoch [2/120    avg_loss:1.168, val_acc:0.742]
Epoch [3/120    avg_loss:0.961, val_acc:0.750]
Epoch [4/120    avg_loss:0.858, val_acc:0.809]
Epoch [5/120    avg_loss:0.681, val_acc:0.793]
Epoch [6/120    avg_loss:0.700, val_acc:0.758]
Epoch [7/120    avg_loss:0.590, val_acc:0.879]
Epoch [8/120    avg_loss:0.516, val_acc:0.854]
Epoch [9/120    avg_loss:0.449, val_acc:0.877]
Epoch [10/120    avg_loss:0.525, val_acc:0.832]
Epoch [11/120    avg_loss:0.485, val_acc:0.836]
Epoch [12/120    avg_loss:0.362, val_acc:0.881]
Epoch [13/120    avg_loss:0.412, val_acc:0.895]
Epoch [14/120    avg_loss:0.344, val_acc:0.895]
Epoch [15/120    avg_loss:0.378, val_acc:0.902]
Epoch [16/120    avg_loss:0.301, val_acc:0.916]
Epoch [17/120    avg_loss:0.383, val_acc:0.900]
Epoch [18/120    avg_loss:0.486, val_acc:0.875]
Epoch [19/120    avg_loss:0.334, val_acc:0.895]
Epoch [20/120    avg_loss:0.276, val_acc:0.920]
Epoch [21/120    avg_loss:0.266, val_acc:0.889]
Epoch [22/120    avg_loss:0.282, val_acc:0.920]
Epoch [23/120    avg_loss:0.256, val_acc:0.885]
Epoch [24/120    avg_loss:0.306, val_acc:0.861]
Epoch [25/120    avg_loss:0.308, val_acc:0.920]
Epoch [26/120    avg_loss:0.250, val_acc:0.941]
Epoch [27/120    avg_loss:0.397, val_acc:0.938]
Epoch [28/120    avg_loss:0.208, val_acc:0.957]
Epoch [29/120    avg_loss:0.261, val_acc:0.910]
Epoch [30/120    avg_loss:0.231, val_acc:0.934]
Epoch [31/120    avg_loss:0.201, val_acc:0.932]
Epoch [32/120    avg_loss:0.209, val_acc:0.949]
Epoch [33/120    avg_loss:0.181, val_acc:0.939]
Epoch [34/120    avg_loss:0.230, val_acc:0.941]
Epoch [35/120    avg_loss:0.195, val_acc:0.949]
Epoch [36/120    avg_loss:0.220, val_acc:0.928]
Epoch [37/120    avg_loss:0.268, val_acc:0.896]
Epoch [38/120    avg_loss:0.182, val_acc:0.955]
Epoch [39/120    avg_loss:0.239, val_acc:0.918]
Epoch [40/120    avg_loss:0.191, val_acc:0.961]
Epoch [41/120    avg_loss:0.205, val_acc:0.936]
Epoch [42/120    avg_loss:0.319, val_acc:0.930]
Epoch [43/120    avg_loss:0.219, val_acc:0.953]
Epoch [44/120    avg_loss:0.166, val_acc:0.943]
Epoch [45/120    avg_loss:0.139, val_acc:0.969]
Epoch [46/120    avg_loss:0.109, val_acc:0.943]
Epoch [47/120    avg_loss:0.130, val_acc:0.963]
Epoch [48/120    avg_loss:0.085, val_acc:0.953]
Epoch [49/120    avg_loss:0.116, val_acc:0.945]
Epoch [50/120    avg_loss:0.115, val_acc:0.959]
Epoch [51/120    avg_loss:0.118, val_acc:0.957]
Epoch [52/120    avg_loss:0.091, val_acc:0.961]
Epoch [53/120    avg_loss:0.140, val_acc:0.939]
Epoch [54/120    avg_loss:0.119, val_acc:0.955]
Epoch [55/120    avg_loss:0.191, val_acc:0.963]
Epoch [56/120    avg_loss:0.114, val_acc:0.961]
Epoch [57/120    avg_loss:0.105, val_acc:0.973]
Epoch [58/120    avg_loss:0.053, val_acc:0.971]
Epoch [59/120    avg_loss:0.102, val_acc:0.961]
Epoch [60/120    avg_loss:0.085, val_acc:0.965]
Epoch [61/120    avg_loss:0.077, val_acc:0.973]
Epoch [62/120    avg_loss:0.058, val_acc:0.963]
Epoch [63/120    avg_loss:0.168, val_acc:0.918]
Epoch [64/120    avg_loss:0.102, val_acc:0.979]
Epoch [65/120    avg_loss:0.068, val_acc:0.955]
Epoch [66/120    avg_loss:0.113, val_acc:0.947]
Epoch [67/120    avg_loss:0.112, val_acc:0.945]
Epoch [68/120    avg_loss:0.128, val_acc:0.949]
Epoch [69/120    avg_loss:0.095, val_acc:0.959]
Epoch [70/120    avg_loss:0.103, val_acc:0.955]
Epoch [71/120    avg_loss:0.042, val_acc:0.973]
Epoch [72/120    avg_loss:0.053, val_acc:0.965]
Epoch [73/120    avg_loss:0.056, val_acc:0.973]
Epoch [74/120    avg_loss:0.053, val_acc:0.967]
Epoch [75/120    avg_loss:0.082, val_acc:0.967]
Epoch [76/120    avg_loss:0.049, val_acc:0.945]
Epoch [77/120    avg_loss:0.084, val_acc:0.955]
Epoch [78/120    avg_loss:0.056, val_acc:0.971]
Epoch [79/120    avg_loss:0.046, val_acc:0.971]
Epoch [80/120    avg_loss:0.038, val_acc:0.979]
Epoch [81/120    avg_loss:0.024, val_acc:0.979]
Epoch [82/120    avg_loss:0.032, val_acc:0.977]
Epoch [83/120    avg_loss:0.026, val_acc:0.977]
Epoch [84/120    avg_loss:0.036, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.977]
Epoch [86/120    avg_loss:0.017, val_acc:0.977]
Epoch [87/120    avg_loss:0.024, val_acc:0.979]
Epoch [88/120    avg_loss:0.023, val_acc:0.980]
Epoch [89/120    avg_loss:0.029, val_acc:0.980]
Epoch [90/120    avg_loss:0.042, val_acc:0.980]
Epoch [91/120    avg_loss:0.027, val_acc:0.982]
Epoch [92/120    avg_loss:0.035, val_acc:0.980]
Epoch [93/120    avg_loss:0.021, val_acc:0.980]
Epoch [94/120    avg_loss:0.016, val_acc:0.980]
Epoch [95/120    avg_loss:0.025, val_acc:0.979]
Epoch [96/120    avg_loss:0.026, val_acc:0.980]
Epoch [97/120    avg_loss:0.024, val_acc:0.980]
Epoch [98/120    avg_loss:0.026, val_acc:0.982]
Epoch [99/120    avg_loss:0.012, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.982]
Epoch [101/120    avg_loss:0.024, val_acc:0.980]
Epoch [102/120    avg_loss:0.017, val_acc:0.982]
Epoch [103/120    avg_loss:0.016, val_acc:0.984]
Epoch [104/120    avg_loss:0.021, val_acc:0.984]
Epoch [105/120    avg_loss:0.016, val_acc:0.984]
Epoch [106/120    avg_loss:0.027, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.984]
Epoch [109/120    avg_loss:0.023, val_acc:0.982]
Epoch [110/120    avg_loss:0.018, val_acc:0.986]
Epoch [111/120    avg_loss:0.028, val_acc:0.984]
Epoch [112/120    avg_loss:0.021, val_acc:0.982]
Epoch [113/120    avg_loss:0.025, val_acc:0.984]
Epoch [114/120    avg_loss:0.017, val_acc:0.984]
Epoch [115/120    avg_loss:0.025, val_acc:0.984]
Epoch [116/120    avg_loss:0.019, val_acc:0.986]
Epoch [117/120    avg_loss:0.023, val_acc:0.984]
Epoch [118/120    avg_loss:0.018, val_acc:0.986]
Epoch [119/120    avg_loss:0.035, val_acc:0.984]
Epoch [120/120    avg_loss:0.027, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   2   0   0   0   0   3   0]
 [  0   0   0 219   6   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 215  10   0   0   0   0   1   0   1   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   1   0   0   0   0   0   0   0   0   0 376   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 0.99927061 0.97716895 0.97550111 0.90909091 0.88086643
 0.99512195 0.98947368 0.99487179 0.99893276 0.99862826 0.99867198
 0.99005525 1.        ]

Kappa:
0.9859931379486343
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77ff1c06d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.938, val_acc:0.650]
Epoch [2/120    avg_loss:1.143, val_acc:0.781]
Epoch [3/120    avg_loss:0.923, val_acc:0.748]
Epoch [4/120    avg_loss:0.798, val_acc:0.826]
Epoch [5/120    avg_loss:0.771, val_acc:0.822]
Epoch [6/120    avg_loss:0.674, val_acc:0.836]
Epoch [7/120    avg_loss:0.555, val_acc:0.838]
Epoch [8/120    avg_loss:0.587, val_acc:0.869]
Epoch [9/120    avg_loss:0.582, val_acc:0.834]
Epoch [10/120    avg_loss:0.506, val_acc:0.885]
Epoch [11/120    avg_loss:0.463, val_acc:0.861]
Epoch [12/120    avg_loss:0.539, val_acc:0.904]
Epoch [13/120    avg_loss:0.350, val_acc:0.918]
Epoch [14/120    avg_loss:0.400, val_acc:0.914]
Epoch [15/120    avg_loss:0.367, val_acc:0.867]
Epoch [16/120    avg_loss:0.453, val_acc:0.900]
Epoch [17/120    avg_loss:0.327, val_acc:0.902]
Epoch [18/120    avg_loss:0.352, val_acc:0.896]
Epoch [19/120    avg_loss:0.324, val_acc:0.945]
Epoch [20/120    avg_loss:0.317, val_acc:0.912]
Epoch [21/120    avg_loss:0.268, val_acc:0.941]
Epoch [22/120    avg_loss:0.343, val_acc:0.916]
Epoch [23/120    avg_loss:0.369, val_acc:0.889]
Epoch [24/120    avg_loss:0.236, val_acc:0.939]
Epoch [25/120    avg_loss:0.276, val_acc:0.920]
Epoch [26/120    avg_loss:0.219, val_acc:0.953]
Epoch [27/120    avg_loss:0.147, val_acc:0.943]
Epoch [28/120    avg_loss:0.251, val_acc:0.959]
Epoch [29/120    avg_loss:0.218, val_acc:0.938]
Epoch [30/120    avg_loss:0.131, val_acc:0.971]
Epoch [31/120    avg_loss:0.190, val_acc:0.965]
Epoch [32/120    avg_loss:0.142, val_acc:0.922]
Epoch [33/120    avg_loss:0.155, val_acc:0.947]
Epoch [34/120    avg_loss:0.140, val_acc:0.938]
Epoch [35/120    avg_loss:0.267, val_acc:0.938]
Epoch [36/120    avg_loss:0.151, val_acc:0.955]
Epoch [37/120    avg_loss:0.178, val_acc:0.926]
Epoch [38/120    avg_loss:0.159, val_acc:0.941]
Epoch [39/120    avg_loss:0.194, val_acc:0.955]
Epoch [40/120    avg_loss:0.179, val_acc:0.951]
Epoch [41/120    avg_loss:0.119, val_acc:0.957]
Epoch [42/120    avg_loss:0.149, val_acc:0.959]
Epoch [43/120    avg_loss:0.201, val_acc:0.963]
Epoch [44/120    avg_loss:0.104, val_acc:0.973]
Epoch [45/120    avg_loss:0.101, val_acc:0.973]
Epoch [46/120    avg_loss:0.070, val_acc:0.977]
Epoch [47/120    avg_loss:0.069, val_acc:0.977]
Epoch [48/120    avg_loss:0.088, val_acc:0.977]
Epoch [49/120    avg_loss:0.056, val_acc:0.977]
Epoch [50/120    avg_loss:0.056, val_acc:0.977]
Epoch [51/120    avg_loss:0.114, val_acc:0.977]
Epoch [52/120    avg_loss:0.073, val_acc:0.979]
Epoch [53/120    avg_loss:0.060, val_acc:0.975]
Epoch [54/120    avg_loss:0.038, val_acc:0.979]
Epoch [55/120    avg_loss:0.046, val_acc:0.979]
Epoch [56/120    avg_loss:0.082, val_acc:0.980]
Epoch [57/120    avg_loss:0.054, val_acc:0.980]
Epoch [58/120    avg_loss:0.053, val_acc:0.980]
Epoch [59/120    avg_loss:0.052, val_acc:0.979]
Epoch [60/120    avg_loss:0.056, val_acc:0.977]
Epoch [61/120    avg_loss:0.046, val_acc:0.977]
Epoch [62/120    avg_loss:0.045, val_acc:0.980]
Epoch [63/120    avg_loss:0.033, val_acc:0.980]
Epoch [64/120    avg_loss:0.047, val_acc:0.980]
Epoch [65/120    avg_loss:0.054, val_acc:0.977]
Epoch [66/120    avg_loss:0.046, val_acc:0.980]
Epoch [67/120    avg_loss:0.032, val_acc:0.982]
Epoch [68/120    avg_loss:0.053, val_acc:0.982]
Epoch [69/120    avg_loss:0.041, val_acc:0.982]
Epoch [70/120    avg_loss:0.045, val_acc:0.984]
Epoch [71/120    avg_loss:0.048, val_acc:0.982]
Epoch [72/120    avg_loss:0.049, val_acc:0.979]
Epoch [73/120    avg_loss:0.047, val_acc:0.979]
Epoch [74/120    avg_loss:0.051, val_acc:0.984]
Epoch [75/120    avg_loss:0.043, val_acc:0.984]
Epoch [76/120    avg_loss:0.065, val_acc:0.984]
Epoch [77/120    avg_loss:0.048, val_acc:0.982]
Epoch [78/120    avg_loss:0.042, val_acc:0.984]
Epoch [79/120    avg_loss:0.037, val_acc:0.984]
Epoch [80/120    avg_loss:0.040, val_acc:0.982]
Epoch [81/120    avg_loss:0.033, val_acc:0.984]
Epoch [82/120    avg_loss:0.035, val_acc:0.984]
Epoch [83/120    avg_loss:0.031, val_acc:0.982]
Epoch [84/120    avg_loss:0.034, val_acc:0.984]
Epoch [85/120    avg_loss:0.028, val_acc:0.986]
Epoch [86/120    avg_loss:0.043, val_acc:0.984]
Epoch [87/120    avg_loss:0.044, val_acc:0.986]
Epoch [88/120    avg_loss:0.032, val_acc:0.986]
Epoch [89/120    avg_loss:0.032, val_acc:0.986]
Epoch [90/120    avg_loss:0.059, val_acc:0.986]
Epoch [91/120    avg_loss:0.042, val_acc:0.984]
Epoch [92/120    avg_loss:0.029, val_acc:0.984]
Epoch [93/120    avg_loss:0.029, val_acc:0.984]
Epoch [94/120    avg_loss:0.049, val_acc:0.984]
Epoch [95/120    avg_loss:0.036, val_acc:0.984]
Epoch [96/120    avg_loss:0.032, val_acc:0.986]
Epoch [97/120    avg_loss:0.039, val_acc:0.986]
Epoch [98/120    avg_loss:0.033, val_acc:0.986]
Epoch [99/120    avg_loss:0.038, val_acc:0.984]
Epoch [100/120    avg_loss:0.037, val_acc:0.986]
Epoch [101/120    avg_loss:0.033, val_acc:0.986]
Epoch [102/120    avg_loss:0.021, val_acc:0.986]
Epoch [103/120    avg_loss:0.028, val_acc:0.986]
Epoch [104/120    avg_loss:0.027, val_acc:0.986]
Epoch [105/120    avg_loss:0.035, val_acc:0.986]
Epoch [106/120    avg_loss:0.025, val_acc:0.986]
Epoch [107/120    avg_loss:0.027, val_acc:0.984]
Epoch [108/120    avg_loss:0.038, val_acc:0.984]
Epoch [109/120    avg_loss:0.024, val_acc:0.986]
Epoch [110/120    avg_loss:0.030, val_acc:0.982]
Epoch [111/120    avg_loss:0.030, val_acc:0.982]
Epoch [112/120    avg_loss:0.035, val_acc:0.982]
Epoch [113/120    avg_loss:0.031, val_acc:0.982]
Epoch [114/120    avg_loss:0.029, val_acc:0.984]
Epoch [115/120    avg_loss:0.028, val_acc:0.986]
Epoch [116/120    avg_loss:0.031, val_acc:0.984]
Epoch [117/120    avg_loss:0.029, val_acc:0.984]
Epoch [118/120    avg_loss:0.031, val_acc:0.988]
Epoch [119/120    avg_loss:0.032, val_acc:0.986]
Epoch [120/120    avg_loss:0.028, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 214   6   2   0   0   6   2   0   0   0   0]
 [  0   0   0   0 223   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   6   0   0   7   0 193   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 0.99563953 0.98412698 0.96396396 0.93894737 0.93992933
 0.96741855 0.98947368 0.99232737 0.9978678  1.         1.
 0.99334812 1.        ]

Kappa:
0.9876537891218785
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab09e277f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.963, val_acc:0.575]
Epoch [2/120    avg_loss:1.314, val_acc:0.716]
Epoch [3/120    avg_loss:0.856, val_acc:0.750]
Epoch [4/120    avg_loss:0.774, val_acc:0.833]
Epoch [5/120    avg_loss:0.717, val_acc:0.808]
Epoch [6/120    avg_loss:0.659, val_acc:0.841]
Epoch [7/120    avg_loss:0.550, val_acc:0.857]
Epoch [8/120    avg_loss:0.576, val_acc:0.871]
Epoch [9/120    avg_loss:0.438, val_acc:0.863]
Epoch [10/120    avg_loss:0.476, val_acc:0.855]
Epoch [11/120    avg_loss:0.477, val_acc:0.871]
Epoch [12/120    avg_loss:0.397, val_acc:0.891]
Epoch [13/120    avg_loss:0.317, val_acc:0.875]
Epoch [14/120    avg_loss:0.443, val_acc:0.885]
Epoch [15/120    avg_loss:0.419, val_acc:0.865]
Epoch [16/120    avg_loss:0.351, val_acc:0.907]
Epoch [17/120    avg_loss:0.335, val_acc:0.915]
Epoch [18/120    avg_loss:0.315, val_acc:0.901]
Epoch [19/120    avg_loss:0.283, val_acc:0.903]
Epoch [20/120    avg_loss:0.301, val_acc:0.915]
Epoch [21/120    avg_loss:0.344, val_acc:0.853]
Epoch [22/120    avg_loss:0.262, val_acc:0.911]
Epoch [23/120    avg_loss:0.254, val_acc:0.944]
Epoch [24/120    avg_loss:0.258, val_acc:0.919]
Epoch [25/120    avg_loss:0.287, val_acc:0.921]
Epoch [26/120    avg_loss:0.269, val_acc:0.919]
Epoch [27/120    avg_loss:0.273, val_acc:0.893]
Epoch [28/120    avg_loss:0.286, val_acc:0.950]
Epoch [29/120    avg_loss:0.212, val_acc:0.933]
Epoch [30/120    avg_loss:0.265, val_acc:0.933]
Epoch [31/120    avg_loss:0.159, val_acc:0.942]
Epoch [32/120    avg_loss:0.237, val_acc:0.903]
Epoch [33/120    avg_loss:0.194, val_acc:0.933]
Epoch [34/120    avg_loss:0.226, val_acc:0.927]
Epoch [35/120    avg_loss:0.148, val_acc:0.931]
Epoch [36/120    avg_loss:0.172, val_acc:0.931]
Epoch [37/120    avg_loss:0.165, val_acc:0.940]
Epoch [38/120    avg_loss:0.173, val_acc:0.944]
Epoch [39/120    avg_loss:0.174, val_acc:0.960]
Epoch [40/120    avg_loss:0.081, val_acc:0.958]
Epoch [41/120    avg_loss:0.079, val_acc:0.968]
Epoch [42/120    avg_loss:0.073, val_acc:0.954]
Epoch [43/120    avg_loss:0.110, val_acc:0.950]
Epoch [44/120    avg_loss:0.117, val_acc:0.952]
Epoch [45/120    avg_loss:0.165, val_acc:0.942]
Epoch [46/120    avg_loss:0.093, val_acc:0.938]
Epoch [47/120    avg_loss:0.139, val_acc:0.946]
Epoch [48/120    avg_loss:0.115, val_acc:0.889]
Epoch [49/120    avg_loss:0.152, val_acc:0.933]
Epoch [50/120    avg_loss:0.113, val_acc:0.948]
Epoch [51/120    avg_loss:0.180, val_acc:0.950]
Epoch [52/120    avg_loss:0.148, val_acc:0.976]
Epoch [53/120    avg_loss:0.126, val_acc:0.954]
Epoch [54/120    avg_loss:0.202, val_acc:0.954]
Epoch [55/120    avg_loss:0.132, val_acc:0.950]
Epoch [56/120    avg_loss:0.102, val_acc:0.940]
Epoch [57/120    avg_loss:0.185, val_acc:0.942]
Epoch [58/120    avg_loss:0.074, val_acc:0.950]
Epoch [59/120    avg_loss:0.097, val_acc:0.962]
Epoch [60/120    avg_loss:0.062, val_acc:0.958]
Epoch [61/120    avg_loss:0.069, val_acc:0.962]
Epoch [62/120    avg_loss:0.033, val_acc:0.950]
Epoch [63/120    avg_loss:0.103, val_acc:0.970]
Epoch [64/120    avg_loss:0.072, val_acc:0.976]
Epoch [65/120    avg_loss:0.058, val_acc:0.978]
Epoch [66/120    avg_loss:0.058, val_acc:0.968]
Epoch [67/120    avg_loss:0.055, val_acc:0.978]
Epoch [68/120    avg_loss:0.048, val_acc:0.974]
Epoch [69/120    avg_loss:0.082, val_acc:0.948]
Epoch [70/120    avg_loss:0.068, val_acc:0.972]
Epoch [71/120    avg_loss:0.027, val_acc:0.972]
Epoch [72/120    avg_loss:0.065, val_acc:0.966]
Epoch [73/120    avg_loss:0.061, val_acc:0.972]
Epoch [74/120    avg_loss:0.037, val_acc:0.978]
Epoch [75/120    avg_loss:0.029, val_acc:0.986]
Epoch [76/120    avg_loss:0.026, val_acc:0.970]
Epoch [77/120    avg_loss:0.053, val_acc:0.970]
Epoch [78/120    avg_loss:0.032, val_acc:0.990]
Epoch [79/120    avg_loss:0.041, val_acc:0.968]
Epoch [80/120    avg_loss:0.023, val_acc:0.986]
Epoch [81/120    avg_loss:0.029, val_acc:0.974]
Epoch [82/120    avg_loss:0.046, val_acc:0.980]
Epoch [83/120    avg_loss:0.058, val_acc:0.968]
Epoch [84/120    avg_loss:0.056, val_acc:0.974]
Epoch [85/120    avg_loss:0.065, val_acc:0.972]
Epoch [86/120    avg_loss:0.114, val_acc:0.960]
Epoch [87/120    avg_loss:0.048, val_acc:0.946]
Epoch [88/120    avg_loss:0.101, val_acc:0.970]
Epoch [89/120    avg_loss:0.067, val_acc:0.964]
Epoch [90/120    avg_loss:0.041, val_acc:0.970]
Epoch [91/120    avg_loss:0.088, val_acc:0.968]
Epoch [92/120    avg_loss:0.055, val_acc:0.980]
Epoch [93/120    avg_loss:0.020, val_acc:0.982]
Epoch [94/120    avg_loss:0.030, val_acc:0.984]
Epoch [95/120    avg_loss:0.028, val_acc:0.984]
Epoch [96/120    avg_loss:0.028, val_acc:0.984]
Epoch [97/120    avg_loss:0.019, val_acc:0.984]
Epoch [98/120    avg_loss:0.018, val_acc:0.984]
Epoch [99/120    avg_loss:0.028, val_acc:0.984]
Epoch [100/120    avg_loss:0.022, val_acc:0.986]
Epoch [101/120    avg_loss:0.022, val_acc:0.984]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.019, val_acc:0.986]
Epoch [104/120    avg_loss:0.019, val_acc:0.986]
Epoch [105/120    avg_loss:0.025, val_acc:0.986]
Epoch [106/120    avg_loss:0.016, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.022, val_acc:0.986]
Epoch [109/120    avg_loss:0.017, val_acc:0.986]
Epoch [110/120    avg_loss:0.017, val_acc:0.986]
Epoch [111/120    avg_loss:0.024, val_acc:0.986]
Epoch [112/120    avg_loss:0.022, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.986]
Epoch [114/120    avg_loss:0.013, val_acc:0.986]
Epoch [115/120    avg_loss:0.019, val_acc:0.986]
Epoch [116/120    avg_loss:0.016, val_acc:0.986]
Epoch [117/120    avg_loss:0.014, val_acc:0.986]
Epoch [118/120    avg_loss:0.021, val_acc:0.986]
Epoch [119/120    avg_loss:0.013, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   1   0   0   0   0   1   0]
 [  0   0   0 225   3   2   0   0   0   0   0   0   0   0]
 [  0   0   0   1 219   5   0   0   0   0   0   0   2   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12   0 193   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.98636364 0.98684211 0.91060291 0.90252708
 0.96741855 0.98947368 1.         1.         1.         1.
 0.99226519 1.        ]

Kappa:
0.9876556526385282
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c15544780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.966, val_acc:0.552]
Epoch [2/120    avg_loss:1.187, val_acc:0.740]
Epoch [3/120    avg_loss:0.884, val_acc:0.831]
Epoch [4/120    avg_loss:0.791, val_acc:0.835]
Epoch [5/120    avg_loss:0.852, val_acc:0.782]
Epoch [6/120    avg_loss:0.725, val_acc:0.740]
Epoch [7/120    avg_loss:0.564, val_acc:0.881]
Epoch [8/120    avg_loss:0.623, val_acc:0.887]
Epoch [9/120    avg_loss:0.416, val_acc:0.907]
Epoch [10/120    avg_loss:0.469, val_acc:0.911]
Epoch [11/120    avg_loss:0.461, val_acc:0.873]
Epoch [12/120    avg_loss:0.495, val_acc:0.889]
Epoch [13/120    avg_loss:0.473, val_acc:0.899]
Epoch [14/120    avg_loss:0.413, val_acc:0.871]
Epoch [15/120    avg_loss:0.397, val_acc:0.871]
Epoch [16/120    avg_loss:0.380, val_acc:0.865]
Epoch [17/120    avg_loss:0.306, val_acc:0.919]
Epoch [18/120    avg_loss:0.244, val_acc:0.927]
Epoch [19/120    avg_loss:0.201, val_acc:0.938]
Epoch [20/120    avg_loss:0.240, val_acc:0.903]
Epoch [21/120    avg_loss:0.202, val_acc:0.935]
Epoch [22/120    avg_loss:0.364, val_acc:0.909]
Epoch [23/120    avg_loss:0.330, val_acc:0.889]
Epoch [24/120    avg_loss:0.306, val_acc:0.919]
Epoch [25/120    avg_loss:0.198, val_acc:0.911]
Epoch [26/120    avg_loss:0.307, val_acc:0.940]
Epoch [27/120    avg_loss:0.259, val_acc:0.935]
Epoch [28/120    avg_loss:0.253, val_acc:0.929]
Epoch [29/120    avg_loss:0.164, val_acc:0.956]
Epoch [30/120    avg_loss:0.212, val_acc:0.919]
Epoch [31/120    avg_loss:0.238, val_acc:0.907]
Epoch [32/120    avg_loss:0.244, val_acc:0.935]
Epoch [33/120    avg_loss:0.229, val_acc:0.952]
Epoch [34/120    avg_loss:0.352, val_acc:0.950]
Epoch [35/120    avg_loss:0.182, val_acc:0.952]
Epoch [36/120    avg_loss:0.102, val_acc:0.954]
Epoch [37/120    avg_loss:0.087, val_acc:0.966]
Epoch [38/120    avg_loss:0.124, val_acc:0.946]
Epoch [39/120    avg_loss:0.143, val_acc:0.954]
Epoch [40/120    avg_loss:0.085, val_acc:0.954]
Epoch [41/120    avg_loss:0.145, val_acc:0.968]
Epoch [42/120    avg_loss:0.144, val_acc:0.968]
Epoch [43/120    avg_loss:0.087, val_acc:0.970]
Epoch [44/120    avg_loss:0.147, val_acc:0.954]
Epoch [45/120    avg_loss:0.084, val_acc:0.976]
Epoch [46/120    avg_loss:0.080, val_acc:0.976]
Epoch [47/120    avg_loss:0.233, val_acc:0.950]
Epoch [48/120    avg_loss:0.090, val_acc:0.972]
Epoch [49/120    avg_loss:0.062, val_acc:0.976]
Epoch [50/120    avg_loss:0.071, val_acc:0.968]
Epoch [51/120    avg_loss:0.104, val_acc:0.954]
Epoch [52/120    avg_loss:0.145, val_acc:0.944]
Epoch [53/120    avg_loss:0.083, val_acc:0.964]
Epoch [54/120    avg_loss:0.083, val_acc:0.974]
Epoch [55/120    avg_loss:0.067, val_acc:0.968]
Epoch [56/120    avg_loss:0.077, val_acc:0.980]
Epoch [57/120    avg_loss:0.047, val_acc:0.980]
Epoch [58/120    avg_loss:0.067, val_acc:0.938]
Epoch [59/120    avg_loss:0.093, val_acc:0.968]
Epoch [60/120    avg_loss:0.066, val_acc:0.974]
Epoch [61/120    avg_loss:0.074, val_acc:0.978]
Epoch [62/120    avg_loss:0.082, val_acc:0.968]
Epoch [63/120    avg_loss:0.082, val_acc:0.978]
Epoch [64/120    avg_loss:0.058, val_acc:0.984]
Epoch [65/120    avg_loss:0.061, val_acc:0.982]
Epoch [66/120    avg_loss:0.035, val_acc:0.982]
Epoch [67/120    avg_loss:0.027, val_acc:0.970]
Epoch [68/120    avg_loss:0.056, val_acc:0.982]
Epoch [69/120    avg_loss:0.053, val_acc:0.978]
Epoch [70/120    avg_loss:0.021, val_acc:0.980]
Epoch [71/120    avg_loss:0.028, val_acc:0.980]
Epoch [72/120    avg_loss:0.059, val_acc:0.982]
Epoch [73/120    avg_loss:0.066, val_acc:0.942]
Epoch [74/120    avg_loss:0.121, val_acc:0.954]
Epoch [75/120    avg_loss:0.141, val_acc:0.970]
Epoch [76/120    avg_loss:0.080, val_acc:0.966]
Epoch [77/120    avg_loss:0.071, val_acc:0.956]
Epoch [78/120    avg_loss:0.062, val_acc:0.970]
Epoch [79/120    avg_loss:0.033, val_acc:0.972]
Epoch [80/120    avg_loss:0.031, val_acc:0.974]
Epoch [81/120    avg_loss:0.045, val_acc:0.976]
Epoch [82/120    avg_loss:0.015, val_acc:0.978]
Epoch [83/120    avg_loss:0.036, val_acc:0.980]
Epoch [84/120    avg_loss:0.021, val_acc:0.978]
Epoch [85/120    avg_loss:0.032, val_acc:0.980]
Epoch [86/120    avg_loss:0.034, val_acc:0.980]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.021, val_acc:0.982]
Epoch [89/120    avg_loss:0.024, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.014, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.982]
Epoch [94/120    avg_loss:0.019, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.018, val_acc:0.982]
Epoch [97/120    avg_loss:0.019, val_acc:0.982]
Epoch [98/120    avg_loss:0.029, val_acc:0.982]
Epoch [99/120    avg_loss:0.022, val_acc:0.982]
Epoch [100/120    avg_loss:0.016, val_acc:0.982]
Epoch [101/120    avg_loss:0.022, val_acc:0.982]
Epoch [102/120    avg_loss:0.014, val_acc:0.982]
Epoch [103/120    avg_loss:0.022, val_acc:0.982]
Epoch [104/120    avg_loss:0.014, val_acc:0.982]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.017, val_acc:0.982]
Epoch [107/120    avg_loss:0.021, val_acc:0.982]
Epoch [108/120    avg_loss:0.021, val_acc:0.982]
Epoch [109/120    avg_loss:0.018, val_acc:0.982]
Epoch [110/120    avg_loss:0.024, val_acc:0.982]
Epoch [111/120    avg_loss:0.032, val_acc:0.982]
Epoch [112/120    avg_loss:0.017, val_acc:0.982]
Epoch [113/120    avg_loss:0.015, val_acc:0.982]
Epoch [114/120    avg_loss:0.014, val_acc:0.982]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.015, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.017, val_acc:0.982]
Epoch [119/120    avg_loss:0.053, val_acc:0.982]
Epoch [120/120    avg_loss:0.014, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   6 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.98855835 0.98678414 0.93965517 0.94158076
 0.98771499 0.98429319 1.         1.         1.         0.99210526
 0.99109131 1.        ]

Kappa:
0.9907427295682613
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50c2e8c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.982, val_acc:0.677]
Epoch [2/120    avg_loss:1.205, val_acc:0.750]
Epoch [3/120    avg_loss:0.852, val_acc:0.764]
Epoch [4/120    avg_loss:0.828, val_acc:0.782]
Epoch [5/120    avg_loss:0.741, val_acc:0.819]
Epoch [6/120    avg_loss:0.724, val_acc:0.776]
Epoch [7/120    avg_loss:0.731, val_acc:0.810]
Epoch [8/120    avg_loss:0.616, val_acc:0.798]
Epoch [9/120    avg_loss:0.589, val_acc:0.760]
Epoch [10/120    avg_loss:0.553, val_acc:0.847]
Epoch [11/120    avg_loss:0.487, val_acc:0.881]
Epoch [12/120    avg_loss:0.452, val_acc:0.907]
Epoch [13/120    avg_loss:0.357, val_acc:0.865]
Epoch [14/120    avg_loss:0.461, val_acc:0.923]
Epoch [15/120    avg_loss:0.388, val_acc:0.917]
Epoch [16/120    avg_loss:0.341, val_acc:0.883]
Epoch [17/120    avg_loss:0.264, val_acc:0.893]
Epoch [18/120    avg_loss:0.303, val_acc:0.899]
Epoch [19/120    avg_loss:0.397, val_acc:0.847]
Epoch [20/120    avg_loss:0.451, val_acc:0.925]
Epoch [21/120    avg_loss:0.307, val_acc:0.923]
Epoch [22/120    avg_loss:0.243, val_acc:0.929]
Epoch [23/120    avg_loss:0.247, val_acc:0.933]
Epoch [24/120    avg_loss:0.233, val_acc:0.954]
Epoch [25/120    avg_loss:0.169, val_acc:0.929]
Epoch [26/120    avg_loss:0.187, val_acc:0.942]
Epoch [27/120    avg_loss:0.166, val_acc:0.940]
Epoch [28/120    avg_loss:0.187, val_acc:0.954]
Epoch [29/120    avg_loss:0.192, val_acc:0.891]
Epoch [30/120    avg_loss:0.278, val_acc:0.954]
Epoch [31/120    avg_loss:0.230, val_acc:0.952]
Epoch [32/120    avg_loss:0.248, val_acc:0.946]
Epoch [33/120    avg_loss:0.172, val_acc:0.927]
Epoch [34/120    avg_loss:0.146, val_acc:0.923]
Epoch [35/120    avg_loss:0.247, val_acc:0.927]
Epoch [36/120    avg_loss:0.191, val_acc:0.956]
Epoch [37/120    avg_loss:0.142, val_acc:0.958]
Epoch [38/120    avg_loss:0.145, val_acc:0.956]
Epoch [39/120    avg_loss:0.180, val_acc:0.938]
Epoch [40/120    avg_loss:0.134, val_acc:0.948]
Epoch [41/120    avg_loss:0.130, val_acc:0.950]
Epoch [42/120    avg_loss:0.103, val_acc:0.956]
Epoch [43/120    avg_loss:0.129, val_acc:0.968]
Epoch [44/120    avg_loss:0.097, val_acc:0.958]
Epoch [45/120    avg_loss:0.124, val_acc:0.946]
Epoch [46/120    avg_loss:0.132, val_acc:0.966]
Epoch [47/120    avg_loss:0.109, val_acc:0.948]
Epoch [48/120    avg_loss:0.182, val_acc:0.968]
Epoch [49/120    avg_loss:0.146, val_acc:0.964]
Epoch [50/120    avg_loss:0.133, val_acc:0.964]
Epoch [51/120    avg_loss:0.059, val_acc:0.978]
Epoch [52/120    avg_loss:0.050, val_acc:0.970]
Epoch [53/120    avg_loss:0.159, val_acc:0.905]
Epoch [54/120    avg_loss:0.273, val_acc:0.925]
Epoch [55/120    avg_loss:0.138, val_acc:0.958]
Epoch [56/120    avg_loss:0.051, val_acc:0.968]
Epoch [57/120    avg_loss:0.078, val_acc:0.966]
Epoch [58/120    avg_loss:0.097, val_acc:0.970]
Epoch [59/120    avg_loss:0.081, val_acc:0.972]
Epoch [60/120    avg_loss:0.056, val_acc:0.972]
Epoch [61/120    avg_loss:0.055, val_acc:0.978]
Epoch [62/120    avg_loss:0.079, val_acc:0.964]
Epoch [63/120    avg_loss:0.125, val_acc:0.960]
Epoch [64/120    avg_loss:0.063, val_acc:0.956]
Epoch [65/120    avg_loss:0.066, val_acc:0.972]
Epoch [66/120    avg_loss:0.067, val_acc:0.968]
Epoch [67/120    avg_loss:0.049, val_acc:0.960]
Epoch [68/120    avg_loss:0.076, val_acc:0.978]
Epoch [69/120    avg_loss:0.036, val_acc:0.960]
Epoch [70/120    avg_loss:0.059, val_acc:0.976]
Epoch [71/120    avg_loss:0.062, val_acc:0.972]
Epoch [72/120    avg_loss:0.046, val_acc:0.962]
Epoch [73/120    avg_loss:0.053, val_acc:0.946]
Epoch [74/120    avg_loss:0.043, val_acc:0.972]
Epoch [75/120    avg_loss:0.055, val_acc:0.972]
Epoch [76/120    avg_loss:0.095, val_acc:0.966]
Epoch [77/120    avg_loss:0.050, val_acc:0.978]
Epoch [78/120    avg_loss:0.023, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.026, val_acc:0.972]
Epoch [81/120    avg_loss:0.022, val_acc:0.980]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.026, val_acc:0.980]
Epoch [84/120    avg_loss:0.017, val_acc:0.982]
Epoch [85/120    avg_loss:0.034, val_acc:0.972]
Epoch [86/120    avg_loss:0.046, val_acc:0.966]
Epoch [87/120    avg_loss:0.036, val_acc:0.972]
Epoch [88/120    avg_loss:0.018, val_acc:0.972]
Epoch [89/120    avg_loss:0.022, val_acc:0.978]
Epoch [90/120    avg_loss:0.038, val_acc:0.974]
Epoch [91/120    avg_loss:0.027, val_acc:0.980]
Epoch [92/120    avg_loss:0.025, val_acc:0.980]
Epoch [93/120    avg_loss:0.014, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.015, val_acc:0.984]
Epoch [98/120    avg_loss:0.011, val_acc:0.982]
Epoch [99/120    avg_loss:0.011, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.984]
Epoch [108/120    avg_loss:0.020, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   3   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 210  14   0   0   0   0   0   0   3   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   8   0   0   2   0 196   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   5 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99419448 0.99545455 0.98454746 0.92511013 0.9109589
 0.97512438 1.         0.99614891 0.99893276 1.         0.99341238
 0.98891353 1.        ]

Kappa:
0.9874160804135597
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff31e8707b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.925, val_acc:0.560]
Epoch [2/120    avg_loss:1.157, val_acc:0.669]
Epoch [3/120    avg_loss:0.954, val_acc:0.784]
Epoch [4/120    avg_loss:0.893, val_acc:0.819]
Epoch [5/120    avg_loss:0.678, val_acc:0.845]
Epoch [6/120    avg_loss:0.846, val_acc:0.831]
Epoch [7/120    avg_loss:0.669, val_acc:0.833]
Epoch [8/120    avg_loss:0.638, val_acc:0.831]
Epoch [9/120    avg_loss:0.553, val_acc:0.845]
Epoch [10/120    avg_loss:0.566, val_acc:0.879]
Epoch [11/120    avg_loss:0.516, val_acc:0.847]
Epoch [12/120    avg_loss:0.443, val_acc:0.885]
Epoch [13/120    avg_loss:0.487, val_acc:0.871]
Epoch [14/120    avg_loss:0.473, val_acc:0.873]
Epoch [15/120    avg_loss:0.494, val_acc:0.921]
Epoch [16/120    avg_loss:0.419, val_acc:0.931]
Epoch [17/120    avg_loss:0.356, val_acc:0.885]
Epoch [18/120    avg_loss:0.425, val_acc:0.938]
Epoch [19/120    avg_loss:0.299, val_acc:0.901]
Epoch [20/120    avg_loss:0.389, val_acc:0.917]
Epoch [21/120    avg_loss:0.366, val_acc:0.897]
Epoch [22/120    avg_loss:0.264, val_acc:0.929]
Epoch [23/120    avg_loss:0.281, val_acc:0.927]
Epoch [24/120    avg_loss:0.256, val_acc:0.944]
Epoch [25/120    avg_loss:0.257, val_acc:0.925]
Epoch [26/120    avg_loss:0.220, val_acc:0.897]
Epoch [27/120    avg_loss:0.274, val_acc:0.938]
Epoch [28/120    avg_loss:0.238, val_acc:0.923]
Epoch [29/120    avg_loss:0.258, val_acc:0.940]
Epoch [30/120    avg_loss:0.208, val_acc:0.923]
Epoch [31/120    avg_loss:0.198, val_acc:0.940]
Epoch [32/120    avg_loss:0.160, val_acc:0.966]
Epoch [33/120    avg_loss:0.174, val_acc:0.972]
Epoch [34/120    avg_loss:0.107, val_acc:0.960]
Epoch [35/120    avg_loss:0.203, val_acc:0.938]
Epoch [36/120    avg_loss:0.183, val_acc:0.915]
Epoch [37/120    avg_loss:0.175, val_acc:0.962]
Epoch [38/120    avg_loss:0.216, val_acc:0.935]
Epoch [39/120    avg_loss:0.157, val_acc:0.948]
Epoch [40/120    avg_loss:0.169, val_acc:0.944]
Epoch [41/120    avg_loss:0.140, val_acc:0.935]
Epoch [42/120    avg_loss:0.091, val_acc:0.956]
Epoch [43/120    avg_loss:0.129, val_acc:0.917]
Epoch [44/120    avg_loss:0.181, val_acc:0.968]
Epoch [45/120    avg_loss:0.140, val_acc:0.966]
Epoch [46/120    avg_loss:0.108, val_acc:0.933]
Epoch [47/120    avg_loss:0.185, val_acc:0.958]
Epoch [48/120    avg_loss:0.123, val_acc:0.970]
Epoch [49/120    avg_loss:0.055, val_acc:0.972]
Epoch [50/120    avg_loss:0.079, val_acc:0.974]
Epoch [51/120    avg_loss:0.086, val_acc:0.974]
Epoch [52/120    avg_loss:0.058, val_acc:0.978]
Epoch [53/120    avg_loss:0.066, val_acc:0.974]
Epoch [54/120    avg_loss:0.049, val_acc:0.974]
Epoch [55/120    avg_loss:0.044, val_acc:0.978]
Epoch [56/120    avg_loss:0.089, val_acc:0.976]
Epoch [57/120    avg_loss:0.049, val_acc:0.974]
Epoch [58/120    avg_loss:0.061, val_acc:0.974]
Epoch [59/120    avg_loss:0.043, val_acc:0.982]
Epoch [60/120    avg_loss:0.053, val_acc:0.982]
Epoch [61/120    avg_loss:0.048, val_acc:0.980]
Epoch [62/120    avg_loss:0.045, val_acc:0.980]
Epoch [63/120    avg_loss:0.036, val_acc:0.978]
Epoch [64/120    avg_loss:0.045, val_acc:0.978]
Epoch [65/120    avg_loss:0.050, val_acc:0.978]
Epoch [66/120    avg_loss:0.067, val_acc:0.982]
Epoch [67/120    avg_loss:0.038, val_acc:0.982]
Epoch [68/120    avg_loss:0.049, val_acc:0.978]
Epoch [69/120    avg_loss:0.072, val_acc:0.978]
Epoch [70/120    avg_loss:0.065, val_acc:0.986]
Epoch [71/120    avg_loss:0.040, val_acc:0.984]
Epoch [72/120    avg_loss:0.030, val_acc:0.982]
Epoch [73/120    avg_loss:0.035, val_acc:0.980]
Epoch [74/120    avg_loss:0.035, val_acc:0.982]
Epoch [75/120    avg_loss:0.042, val_acc:0.980]
Epoch [76/120    avg_loss:0.041, val_acc:0.982]
Epoch [77/120    avg_loss:0.050, val_acc:0.980]
Epoch [78/120    avg_loss:0.047, val_acc:0.984]
Epoch [79/120    avg_loss:0.030, val_acc:0.986]
Epoch [80/120    avg_loss:0.039, val_acc:0.984]
Epoch [81/120    avg_loss:0.044, val_acc:0.986]
Epoch [82/120    avg_loss:0.036, val_acc:0.986]
Epoch [83/120    avg_loss:0.037, val_acc:0.986]
Epoch [84/120    avg_loss:0.041, val_acc:0.986]
Epoch [85/120    avg_loss:0.028, val_acc:0.988]
Epoch [86/120    avg_loss:0.039, val_acc:0.988]
Epoch [87/120    avg_loss:0.034, val_acc:0.986]
Epoch [88/120    avg_loss:0.050, val_acc:0.986]
Epoch [89/120    avg_loss:0.023, val_acc:0.986]
Epoch [90/120    avg_loss:0.024, val_acc:0.986]
Epoch [91/120    avg_loss:0.027, val_acc:0.986]
Epoch [92/120    avg_loss:0.027, val_acc:0.988]
Epoch [93/120    avg_loss:0.028, val_acc:0.988]
Epoch [94/120    avg_loss:0.042, val_acc:0.986]
Epoch [95/120    avg_loss:0.029, val_acc:0.986]
Epoch [96/120    avg_loss:0.024, val_acc:0.988]
Epoch [97/120    avg_loss:0.030, val_acc:0.986]
Epoch [98/120    avg_loss:0.042, val_acc:0.988]
Epoch [99/120    avg_loss:0.029, val_acc:0.988]
Epoch [100/120    avg_loss:0.027, val_acc:0.988]
Epoch [101/120    avg_loss:0.036, val_acc:0.986]
Epoch [102/120    avg_loss:0.020, val_acc:0.986]
Epoch [103/120    avg_loss:0.033, val_acc:0.986]
Epoch [104/120    avg_loss:0.040, val_acc:0.988]
Epoch [105/120    avg_loss:0.031, val_acc:0.986]
Epoch [106/120    avg_loss:0.034, val_acc:0.986]
Epoch [107/120    avg_loss:0.044, val_acc:0.986]
Epoch [108/120    avg_loss:0.026, val_acc:0.986]
Epoch [109/120    avg_loss:0.036, val_acc:0.984]
Epoch [110/120    avg_loss:0.030, val_acc:0.986]
Epoch [111/120    avg_loss:0.035, val_acc:0.988]
Epoch [112/120    avg_loss:0.030, val_acc:0.988]
Epoch [113/120    avg_loss:0.023, val_acc:0.988]
Epoch [114/120    avg_loss:0.027, val_acc:0.986]
Epoch [115/120    avg_loss:0.029, val_acc:0.988]
Epoch [116/120    avg_loss:0.026, val_acc:0.988]
Epoch [117/120    avg_loss:0.032, val_acc:0.988]
Epoch [118/120    avg_loss:0.039, val_acc:0.984]
Epoch [119/120    avg_loss:0.036, val_acc:0.990]
Epoch [120/120    avg_loss:0.024, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   6   0   0   3   0 197   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 0.99563953 0.98206278 0.98230088 0.94936709 0.95373665
 0.97766749 0.96703297 1.         1.         1.         1.
 0.99778761 1.        ]

Kappa:
0.9909777086832644
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda89f67780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.067, val_acc:0.570]
Epoch [2/120    avg_loss:1.215, val_acc:0.654]
Epoch [3/120    avg_loss:0.950, val_acc:0.797]
Epoch [4/120    avg_loss:0.824, val_acc:0.826]
Epoch [5/120    avg_loss:0.573, val_acc:0.859]
Epoch [6/120    avg_loss:0.720, val_acc:0.785]
Epoch [7/120    avg_loss:0.706, val_acc:0.838]
Epoch [8/120    avg_loss:0.660, val_acc:0.879]
Epoch [9/120    avg_loss:0.584, val_acc:0.859]
Epoch [10/120    avg_loss:0.400, val_acc:0.908]
Epoch [11/120    avg_loss:0.356, val_acc:0.902]
Epoch [12/120    avg_loss:0.422, val_acc:0.912]
Epoch [13/120    avg_loss:0.330, val_acc:0.916]
Epoch [14/120    avg_loss:0.399, val_acc:0.916]
Epoch [15/120    avg_loss:0.296, val_acc:0.924]
Epoch [16/120    avg_loss:0.285, val_acc:0.854]
Epoch [17/120    avg_loss:0.343, val_acc:0.918]
Epoch [18/120    avg_loss:0.306, val_acc:0.928]
Epoch [19/120    avg_loss:0.343, val_acc:0.920]
Epoch [20/120    avg_loss:0.367, val_acc:0.914]
Epoch [21/120    avg_loss:0.250, val_acc:0.957]
Epoch [22/120    avg_loss:0.266, val_acc:0.916]
Epoch [23/120    avg_loss:0.277, val_acc:0.951]
Epoch [24/120    avg_loss:0.293, val_acc:0.936]
Epoch [25/120    avg_loss:0.197, val_acc:0.957]
Epoch [26/120    avg_loss:0.178, val_acc:0.955]
Epoch [27/120    avg_loss:0.247, val_acc:0.957]
Epoch [28/120    avg_loss:0.146, val_acc:0.967]
Epoch [29/120    avg_loss:0.142, val_acc:0.961]
Epoch [30/120    avg_loss:0.157, val_acc:0.969]
Epoch [31/120    avg_loss:0.163, val_acc:0.971]
Epoch [32/120    avg_loss:0.131, val_acc:0.965]
Epoch [33/120    avg_loss:0.181, val_acc:0.920]
Epoch [34/120    avg_loss:0.188, val_acc:0.953]
Epoch [35/120    avg_loss:0.163, val_acc:0.947]
Epoch [36/120    avg_loss:0.133, val_acc:0.973]
Epoch [37/120    avg_loss:0.130, val_acc:0.951]
Epoch [38/120    avg_loss:0.109, val_acc:0.971]
Epoch [39/120    avg_loss:0.114, val_acc:0.957]
Epoch [40/120    avg_loss:0.108, val_acc:0.973]
Epoch [41/120    avg_loss:0.078, val_acc:0.971]
Epoch [42/120    avg_loss:0.095, val_acc:0.975]
Epoch [43/120    avg_loss:0.124, val_acc:0.973]
Epoch [44/120    avg_loss:0.125, val_acc:0.971]
Epoch [45/120    avg_loss:0.079, val_acc:0.979]
Epoch [46/120    avg_loss:0.082, val_acc:0.961]
Epoch [47/120    avg_loss:0.120, val_acc:0.973]
Epoch [48/120    avg_loss:0.136, val_acc:0.963]
Epoch [49/120    avg_loss:0.115, val_acc:0.945]
Epoch [50/120    avg_loss:0.155, val_acc:0.955]
Epoch [51/120    avg_loss:0.104, val_acc:0.973]
Epoch [52/120    avg_loss:0.064, val_acc:0.973]
Epoch [53/120    avg_loss:0.052, val_acc:0.982]
Epoch [54/120    avg_loss:0.094, val_acc:0.975]
Epoch [55/120    avg_loss:0.063, val_acc:0.959]
Epoch [56/120    avg_loss:0.107, val_acc:0.979]
Epoch [57/120    avg_loss:0.032, val_acc:0.980]
Epoch [58/120    avg_loss:0.039, val_acc:0.980]
Epoch [59/120    avg_loss:0.075, val_acc:0.979]
Epoch [60/120    avg_loss:0.057, val_acc:0.986]
Epoch [61/120    avg_loss:0.046, val_acc:0.979]
Epoch [62/120    avg_loss:0.104, val_acc:0.971]
Epoch [63/120    avg_loss:0.045, val_acc:0.979]
Epoch [64/120    avg_loss:0.026, val_acc:0.982]
Epoch [65/120    avg_loss:0.031, val_acc:0.986]
Epoch [66/120    avg_loss:0.030, val_acc:0.973]
Epoch [67/120    avg_loss:0.023, val_acc:0.980]
Epoch [68/120    avg_loss:0.037, val_acc:0.984]
Epoch [69/120    avg_loss:0.027, val_acc:0.977]
Epoch [70/120    avg_loss:0.036, val_acc:0.982]
Epoch [71/120    avg_loss:0.039, val_acc:0.982]
Epoch [72/120    avg_loss:0.020, val_acc:0.984]
Epoch [73/120    avg_loss:0.024, val_acc:0.975]
Epoch [74/120    avg_loss:0.015, val_acc:0.982]
Epoch [75/120    avg_loss:0.040, val_acc:0.973]
Epoch [76/120    avg_loss:0.030, val_acc:0.980]
Epoch [77/120    avg_loss:0.034, val_acc:0.977]
Epoch [78/120    avg_loss:0.016, val_acc:0.984]
Epoch [79/120    avg_loss:0.018, val_acc:0.986]
Epoch [80/120    avg_loss:0.028, val_acc:0.986]
Epoch [81/120    avg_loss:0.018, val_acc:0.986]
Epoch [82/120    avg_loss:0.015, val_acc:0.986]
Epoch [83/120    avg_loss:0.017, val_acc:0.988]
Epoch [84/120    avg_loss:0.025, val_acc:0.988]
Epoch [85/120    avg_loss:0.009, val_acc:0.988]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.012, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.018, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   3   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.9977221  0.99122807 0.9254386  0.91216216
 0.98771499 1.         1.         0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.9914544371787253
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f25d4339780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.020, val_acc:0.651]
Epoch [2/120    avg_loss:1.193, val_acc:0.724]
Epoch [3/120    avg_loss:0.931, val_acc:0.806]
Epoch [4/120    avg_loss:0.794, val_acc:0.817]
Epoch [5/120    avg_loss:0.673, val_acc:0.827]
Epoch [6/120    avg_loss:0.615, val_acc:0.847]
Epoch [7/120    avg_loss:0.653, val_acc:0.851]
Epoch [8/120    avg_loss:0.520, val_acc:0.893]
Epoch [9/120    avg_loss:0.552, val_acc:0.913]
Epoch [10/120    avg_loss:0.435, val_acc:0.913]
Epoch [11/120    avg_loss:0.493, val_acc:0.887]
Epoch [12/120    avg_loss:0.404, val_acc:0.877]
Epoch [13/120    avg_loss:0.419, val_acc:0.913]
Epoch [14/120    avg_loss:0.379, val_acc:0.913]
Epoch [15/120    avg_loss:0.372, val_acc:0.921]
Epoch [16/120    avg_loss:0.361, val_acc:0.905]
Epoch [17/120    avg_loss:0.303, val_acc:0.944]
Epoch [18/120    avg_loss:0.421, val_acc:0.931]
Epoch [19/120    avg_loss:0.326, val_acc:0.919]
Epoch [20/120    avg_loss:0.240, val_acc:0.931]
Epoch [21/120    avg_loss:0.314, val_acc:0.927]
Epoch [22/120    avg_loss:0.318, val_acc:0.954]
Epoch [23/120    avg_loss:0.345, val_acc:0.966]
Epoch [24/120    avg_loss:0.210, val_acc:0.952]
Epoch [25/120    avg_loss:0.197, val_acc:0.942]
Epoch [26/120    avg_loss:0.237, val_acc:0.946]
Epoch [27/120    avg_loss:0.233, val_acc:0.966]
Epoch [28/120    avg_loss:0.118, val_acc:0.968]
Epoch [29/120    avg_loss:0.173, val_acc:0.956]
Epoch [30/120    avg_loss:0.130, val_acc:0.968]
Epoch [31/120    avg_loss:0.095, val_acc:0.958]
Epoch [32/120    avg_loss:0.187, val_acc:0.960]
Epoch [33/120    avg_loss:0.110, val_acc:0.978]
Epoch [34/120    avg_loss:0.148, val_acc:0.978]
Epoch [35/120    avg_loss:0.166, val_acc:0.972]
Epoch [36/120    avg_loss:0.084, val_acc:0.970]
Epoch [37/120    avg_loss:0.113, val_acc:0.966]
Epoch [38/120    avg_loss:0.133, val_acc:0.958]
Epoch [39/120    avg_loss:0.137, val_acc:0.974]
Epoch [40/120    avg_loss:0.089, val_acc:0.966]
Epoch [41/120    avg_loss:0.065, val_acc:0.974]
Epoch [42/120    avg_loss:0.112, val_acc:0.972]
Epoch [43/120    avg_loss:0.135, val_acc:0.964]
Epoch [44/120    avg_loss:0.121, val_acc:0.976]
Epoch [45/120    avg_loss:0.068, val_acc:0.968]
Epoch [46/120    avg_loss:0.157, val_acc:0.980]
Epoch [47/120    avg_loss:0.145, val_acc:0.950]
Epoch [48/120    avg_loss:0.154, val_acc:0.976]
Epoch [49/120    avg_loss:0.104, val_acc:0.972]
Epoch [50/120    avg_loss:0.087, val_acc:0.944]
Epoch [51/120    avg_loss:0.076, val_acc:0.986]
Epoch [52/120    avg_loss:0.081, val_acc:0.986]
Epoch [53/120    avg_loss:0.045, val_acc:0.976]
Epoch [54/120    avg_loss:0.051, val_acc:0.980]
Epoch [55/120    avg_loss:0.081, val_acc:0.982]
Epoch [56/120    avg_loss:0.067, val_acc:0.982]
Epoch [57/120    avg_loss:0.036, val_acc:0.982]
Epoch [58/120    avg_loss:0.068, val_acc:0.976]
Epoch [59/120    avg_loss:0.037, val_acc:0.984]
Epoch [60/120    avg_loss:0.034, val_acc:0.968]
Epoch [61/120    avg_loss:0.139, val_acc:0.927]
Epoch [62/120    avg_loss:0.133, val_acc:0.952]
Epoch [63/120    avg_loss:0.078, val_acc:0.982]
Epoch [64/120    avg_loss:0.057, val_acc:0.980]
Epoch [65/120    avg_loss:0.042, val_acc:0.968]
Epoch [66/120    avg_loss:0.056, val_acc:0.986]
Epoch [67/120    avg_loss:0.027, val_acc:0.988]
Epoch [68/120    avg_loss:0.031, val_acc:0.986]
Epoch [69/120    avg_loss:0.016, val_acc:0.986]
Epoch [70/120    avg_loss:0.031, val_acc:0.986]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.025, val_acc:0.984]
Epoch [73/120    avg_loss:0.016, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.986]
Epoch [75/120    avg_loss:0.025, val_acc:0.988]
Epoch [76/120    avg_loss:0.018, val_acc:0.988]
Epoch [77/120    avg_loss:0.021, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.019, val_acc:0.986]
Epoch [80/120    avg_loss:0.015, val_acc:0.986]
Epoch [81/120    avg_loss:0.017, val_acc:0.988]
Epoch [82/120    avg_loss:0.020, val_acc:0.988]
Epoch [83/120    avg_loss:0.023, val_acc:0.990]
Epoch [84/120    avg_loss:0.013, val_acc:0.990]
Epoch [85/120    avg_loss:0.019, val_acc:0.988]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.017, val_acc:0.990]
Epoch [88/120    avg_loss:0.019, val_acc:0.992]
Epoch [89/120    avg_loss:0.010, val_acc:0.992]
Epoch [90/120    avg_loss:0.031, val_acc:0.990]
Epoch [91/120    avg_loss:0.021, val_acc:0.992]
Epoch [92/120    avg_loss:0.016, val_acc:0.992]
Epoch [93/120    avg_loss:0.010, val_acc:0.992]
Epoch [94/120    avg_loss:0.015, val_acc:0.990]
Epoch [95/120    avg_loss:0.013, val_acc:0.990]
Epoch [96/120    avg_loss:0.012, val_acc:0.990]
Epoch [97/120    avg_loss:0.020, val_acc:0.990]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.990]
Epoch [100/120    avg_loss:0.016, val_acc:0.990]
Epoch [101/120    avg_loss:0.018, val_acc:0.990]
Epoch [102/120    avg_loss:0.019, val_acc:0.990]
Epoch [103/120    avg_loss:0.025, val_acc:0.990]
Epoch [104/120    avg_loss:0.014, val_acc:0.990]
Epoch [105/120    avg_loss:0.020, val_acc:0.990]
Epoch [106/120    avg_loss:0.021, val_acc:0.990]
Epoch [107/120    avg_loss:0.013, val_acc:0.990]
Epoch [108/120    avg_loss:0.017, val_acc:0.990]
Epoch [109/120    avg_loss:0.026, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.011, val_acc:0.990]
Epoch [112/120    avg_loss:0.018, val_acc:0.990]
Epoch [113/120    avg_loss:0.012, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.990]
Epoch [115/120    avg_loss:0.018, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.017, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.012, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 226   2   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 211  13   0   0   0   0   0   0   3   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   5 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.99316629 0.99122807 0.93362832 0.93243243
 0.98771499 0.99470899 0.998713   0.99893276 1.         0.99341238
 0.98891353 1.        ]

Kappa:
0.9905048821405842
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc38ee27828>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.977, val_acc:0.577]
Epoch [2/120    avg_loss:1.149, val_acc:0.692]
Epoch [3/120    avg_loss:0.899, val_acc:0.812]
Epoch [4/120    avg_loss:0.910, val_acc:0.786]
Epoch [5/120    avg_loss:0.703, val_acc:0.881]
Epoch [6/120    avg_loss:0.644, val_acc:0.857]
Epoch [7/120    avg_loss:0.621, val_acc:0.790]
Epoch [8/120    avg_loss:0.593, val_acc:0.889]
Epoch [9/120    avg_loss:0.454, val_acc:0.917]
Epoch [10/120    avg_loss:0.482, val_acc:0.895]
Epoch [11/120    avg_loss:0.428, val_acc:0.819]
Epoch [12/120    avg_loss:0.479, val_acc:0.881]
Epoch [13/120    avg_loss:0.421, val_acc:0.909]
Epoch [14/120    avg_loss:0.390, val_acc:0.877]
Epoch [15/120    avg_loss:0.350, val_acc:0.889]
Epoch [16/120    avg_loss:0.364, val_acc:0.919]
Epoch [17/120    avg_loss:0.328, val_acc:0.897]
Epoch [18/120    avg_loss:0.381, val_acc:0.917]
Epoch [19/120    avg_loss:0.317, val_acc:0.927]
Epoch [20/120    avg_loss:0.274, val_acc:0.933]
Epoch [21/120    avg_loss:0.316, val_acc:0.935]
Epoch [22/120    avg_loss:0.193, val_acc:0.927]
Epoch [23/120    avg_loss:0.252, val_acc:0.948]
Epoch [24/120    avg_loss:0.214, val_acc:0.944]
Epoch [25/120    avg_loss:0.255, val_acc:0.905]
Epoch [26/120    avg_loss:0.271, val_acc:0.907]
Epoch [27/120    avg_loss:0.264, val_acc:0.940]
Epoch [28/120    avg_loss:0.233, val_acc:0.966]
Epoch [29/120    avg_loss:0.147, val_acc:0.964]
Epoch [30/120    avg_loss:0.133, val_acc:0.956]
Epoch [31/120    avg_loss:0.115, val_acc:0.958]
Epoch [32/120    avg_loss:0.105, val_acc:0.966]
Epoch [33/120    avg_loss:0.179, val_acc:0.944]
Epoch [34/120    avg_loss:0.116, val_acc:0.958]
Epoch [35/120    avg_loss:0.194, val_acc:0.954]
Epoch [36/120    avg_loss:0.242, val_acc:0.893]
Epoch [37/120    avg_loss:0.194, val_acc:0.956]
Epoch [38/120    avg_loss:0.119, val_acc:0.933]
Epoch [39/120    avg_loss:0.181, val_acc:0.956]
Epoch [40/120    avg_loss:0.162, val_acc:0.962]
Epoch [41/120    avg_loss:0.147, val_acc:0.964]
Epoch [42/120    avg_loss:0.155, val_acc:0.938]
Epoch [43/120    avg_loss:0.098, val_acc:0.962]
Epoch [44/120    avg_loss:0.126, val_acc:0.966]
Epoch [45/120    avg_loss:0.123, val_acc:0.964]
Epoch [46/120    avg_loss:0.078, val_acc:0.962]
Epoch [47/120    avg_loss:0.192, val_acc:0.935]
Epoch [48/120    avg_loss:0.167, val_acc:0.950]
Epoch [49/120    avg_loss:0.184, val_acc:0.950]
Epoch [50/120    avg_loss:0.139, val_acc:0.956]
Epoch [51/120    avg_loss:0.111, val_acc:0.960]
Epoch [52/120    avg_loss:0.062, val_acc:0.980]
Epoch [53/120    avg_loss:0.060, val_acc:0.970]
Epoch [54/120    avg_loss:0.054, val_acc:0.958]
Epoch [55/120    avg_loss:0.064, val_acc:0.974]
Epoch [56/120    avg_loss:0.069, val_acc:0.964]
Epoch [57/120    avg_loss:0.045, val_acc:0.968]
Epoch [58/120    avg_loss:0.100, val_acc:0.964]
Epoch [59/120    avg_loss:0.070, val_acc:0.968]
Epoch [60/120    avg_loss:0.149, val_acc:0.948]
Epoch [61/120    avg_loss:0.111, val_acc:0.956]
Epoch [62/120    avg_loss:0.174, val_acc:0.954]
Epoch [63/120    avg_loss:0.064, val_acc:0.980]
Epoch [64/120    avg_loss:0.066, val_acc:0.968]
Epoch [65/120    avg_loss:0.115, val_acc:0.950]
Epoch [66/120    avg_loss:0.070, val_acc:0.962]
Epoch [67/120    avg_loss:0.055, val_acc:0.978]
Epoch [68/120    avg_loss:0.041, val_acc:0.956]
Epoch [69/120    avg_loss:0.053, val_acc:0.976]
Epoch [70/120    avg_loss:0.053, val_acc:0.978]
Epoch [71/120    avg_loss:0.044, val_acc:0.976]
Epoch [72/120    avg_loss:0.045, val_acc:0.968]
Epoch [73/120    avg_loss:0.043, val_acc:0.984]
Epoch [74/120    avg_loss:0.020, val_acc:0.978]
Epoch [75/120    avg_loss:0.027, val_acc:0.984]
Epoch [76/120    avg_loss:0.053, val_acc:0.968]
Epoch [77/120    avg_loss:0.047, val_acc:0.972]
Epoch [78/120    avg_loss:0.033, val_acc:0.976]
Epoch [79/120    avg_loss:0.019, val_acc:0.978]
Epoch [80/120    avg_loss:0.021, val_acc:0.974]
Epoch [81/120    avg_loss:0.027, val_acc:0.976]
Epoch [82/120    avg_loss:0.044, val_acc:0.978]
Epoch [83/120    avg_loss:0.041, val_acc:0.970]
Epoch [84/120    avg_loss:0.033, val_acc:0.978]
Epoch [85/120    avg_loss:0.017, val_acc:0.972]
Epoch [86/120    avg_loss:0.024, val_acc:0.980]
Epoch [87/120    avg_loss:0.051, val_acc:0.976]
Epoch [88/120    avg_loss:0.038, val_acc:0.978]
Epoch [89/120    avg_loss:0.027, val_acc:0.980]
Epoch [90/120    avg_loss:0.024, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.976]
Epoch [92/120    avg_loss:0.032, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.978]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.026, val_acc:0.980]
Epoch [96/120    avg_loss:0.013, val_acc:0.980]
Epoch [97/120    avg_loss:0.038, val_acc:0.980]
Epoch [98/120    avg_loss:0.021, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.982]
Epoch [100/120    avg_loss:0.027, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.016, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.976]
Epoch [105/120    avg_loss:0.008, val_acc:0.976]
Epoch [106/120    avg_loss:0.012, val_acc:0.976]
Epoch [107/120    avg_loss:0.012, val_acc:0.976]
Epoch [108/120    avg_loss:0.015, val_acc:0.976]
Epoch [109/120    avg_loss:0.010, val_acc:0.976]
Epoch [110/120    avg_loss:0.017, val_acc:0.976]
Epoch [111/120    avg_loss:0.011, val_acc:0.976]
Epoch [112/120    avg_loss:0.011, val_acc:0.976]
Epoch [113/120    avg_loss:0.014, val_acc:0.976]
Epoch [114/120    avg_loss:0.006, val_acc:0.976]
Epoch [115/120    avg_loss:0.015, val_acc:0.976]
Epoch [116/120    avg_loss:0.007, val_acc:0.976]
Epoch [117/120    avg_loss:0.011, val_acc:0.976]
Epoch [118/120    avg_loss:0.014, val_acc:0.976]
Epoch [119/120    avg_loss:0.014, val_acc:0.976]
Epoch [120/120    avg_loss:0.008, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   1   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 210  14   0   0   0   0   0   0   3   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.99545455 0.99343545 0.92511013 0.9109589
 0.99019608 1.         0.99742931 1.         1.         1.
 0.99448732 1.        ]

Kappa:
0.990979302454392
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fce29b26780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.977, val_acc:0.619]
Epoch [2/120    avg_loss:1.176, val_acc:0.702]
Epoch [3/120    avg_loss:0.912, val_acc:0.823]
Epoch [4/120    avg_loss:0.763, val_acc:0.806]
Epoch [5/120    avg_loss:0.681, val_acc:0.833]
Epoch [6/120    avg_loss:0.625, val_acc:0.861]
Epoch [7/120    avg_loss:0.527, val_acc:0.849]
Epoch [8/120    avg_loss:0.563, val_acc:0.865]
Epoch [9/120    avg_loss:0.503, val_acc:0.871]
Epoch [10/120    avg_loss:0.424, val_acc:0.825]
Epoch [11/120    avg_loss:0.487, val_acc:0.857]
Epoch [12/120    avg_loss:0.425, val_acc:0.859]
Epoch [13/120    avg_loss:0.474, val_acc:0.875]
Epoch [14/120    avg_loss:0.410, val_acc:0.883]
Epoch [15/120    avg_loss:0.389, val_acc:0.917]
Epoch [16/120    avg_loss:0.292, val_acc:0.907]
Epoch [17/120    avg_loss:0.308, val_acc:0.895]
Epoch [18/120    avg_loss:0.354, val_acc:0.867]
Epoch [19/120    avg_loss:0.325, val_acc:0.911]
Epoch [20/120    avg_loss:0.302, val_acc:0.919]
Epoch [21/120    avg_loss:0.244, val_acc:0.917]
Epoch [22/120    avg_loss:0.212, val_acc:0.905]
Epoch [23/120    avg_loss:0.306, val_acc:0.897]
Epoch [24/120    avg_loss:0.287, val_acc:0.935]
Epoch [25/120    avg_loss:0.218, val_acc:0.917]
Epoch [26/120    avg_loss:0.241, val_acc:0.925]
Epoch [27/120    avg_loss:0.251, val_acc:0.921]
Epoch [28/120    avg_loss:0.195, val_acc:0.925]
Epoch [29/120    avg_loss:0.223, val_acc:0.931]
Epoch [30/120    avg_loss:0.133, val_acc:0.940]
Epoch [31/120    avg_loss:0.125, val_acc:0.927]
Epoch [32/120    avg_loss:0.195, val_acc:0.931]
Epoch [33/120    avg_loss:0.126, val_acc:0.944]
Epoch [34/120    avg_loss:0.129, val_acc:0.925]
Epoch [35/120    avg_loss:0.212, val_acc:0.911]
Epoch [36/120    avg_loss:0.179, val_acc:0.944]
Epoch [37/120    avg_loss:0.106, val_acc:0.935]
Epoch [38/120    avg_loss:0.072, val_acc:0.952]
Epoch [39/120    avg_loss:0.066, val_acc:0.958]
Epoch [40/120    avg_loss:0.114, val_acc:0.956]
Epoch [41/120    avg_loss:0.183, val_acc:0.940]
Epoch [42/120    avg_loss:0.103, val_acc:0.948]
Epoch [43/120    avg_loss:0.123, val_acc:0.958]
Epoch [44/120    avg_loss:0.117, val_acc:0.976]
Epoch [45/120    avg_loss:0.066, val_acc:0.952]
Epoch [46/120    avg_loss:0.100, val_acc:0.964]
Epoch [47/120    avg_loss:0.088, val_acc:0.950]
Epoch [48/120    avg_loss:0.059, val_acc:0.958]
Epoch [49/120    avg_loss:0.072, val_acc:0.964]
Epoch [50/120    avg_loss:0.058, val_acc:0.966]
Epoch [51/120    avg_loss:0.071, val_acc:0.940]
Epoch [52/120    avg_loss:0.152, val_acc:0.952]
Epoch [53/120    avg_loss:0.102, val_acc:0.958]
Epoch [54/120    avg_loss:0.100, val_acc:0.964]
Epoch [55/120    avg_loss:0.085, val_acc:0.940]
Epoch [56/120    avg_loss:0.074, val_acc:0.946]
Epoch [57/120    avg_loss:0.100, val_acc:0.974]
Epoch [58/120    avg_loss:0.037, val_acc:0.978]
Epoch [59/120    avg_loss:0.044, val_acc:0.980]
Epoch [60/120    avg_loss:0.037, val_acc:0.980]
Epoch [61/120    avg_loss:0.026, val_acc:0.978]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.024, val_acc:0.978]
Epoch [64/120    avg_loss:0.021, val_acc:0.976]
Epoch [65/120    avg_loss:0.016, val_acc:0.976]
Epoch [66/120    avg_loss:0.035, val_acc:0.976]
Epoch [67/120    avg_loss:0.043, val_acc:0.976]
Epoch [68/120    avg_loss:0.034, val_acc:0.978]
Epoch [69/120    avg_loss:0.027, val_acc:0.982]
Epoch [70/120    avg_loss:0.034, val_acc:0.988]
Epoch [71/120    avg_loss:0.019, val_acc:0.986]
Epoch [72/120    avg_loss:0.016, val_acc:0.986]
Epoch [73/120    avg_loss:0.026, val_acc:0.986]
Epoch [74/120    avg_loss:0.039, val_acc:0.984]
Epoch [75/120    avg_loss:0.024, val_acc:0.980]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.023, val_acc:0.984]
Epoch [79/120    avg_loss:0.023, val_acc:0.984]
Epoch [80/120    avg_loss:0.018, val_acc:0.986]
Epoch [81/120    avg_loss:0.023, val_acc:0.986]
Epoch [82/120    avg_loss:0.024, val_acc:0.986]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.017, val_acc:0.984]
Epoch [85/120    avg_loss:0.030, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.024, val_acc:0.984]
Epoch [88/120    avg_loss:0.029, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.017, val_acc:0.986]
Epoch [91/120    avg_loss:0.016, val_acc:0.986]
Epoch [92/120    avg_loss:0.018, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.029, val_acc:0.988]
Epoch [95/120    avg_loss:0.019, val_acc:0.986]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.027, val_acc:0.988]
Epoch [99/120    avg_loss:0.013, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.988]
Epoch [101/120    avg_loss:0.022, val_acc:0.988]
Epoch [102/120    avg_loss:0.014, val_acc:0.988]
Epoch [103/120    avg_loss:0.027, val_acc:0.988]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.013, val_acc:0.990]
Epoch [106/120    avg_loss:0.025, val_acc:0.990]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.017, val_acc:0.990]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.019, val_acc:0.988]
Epoch [111/120    avg_loss:0.035, val_acc:0.986]
Epoch [112/120    avg_loss:0.030, val_acc:0.988]
Epoch [113/120    avg_loss:0.039, val_acc:0.990]
Epoch [114/120    avg_loss:0.018, val_acc:0.990]
Epoch [115/120    avg_loss:0.026, val_acc:0.990]
Epoch [116/120    avg_loss:0.018, val_acc:0.990]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.026, val_acc:0.988]
Epoch [119/120    avg_loss:0.018, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 212  13   0   0   0   5   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.99316629 0.95927602 0.93569845 0.94771242
 1.         0.99470899 0.99359795 1.         1.         1.
 0.99778761 1.        ]

Kappa:
0.9912173991557204
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6e8b74b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:1.953, val_acc:0.605]
Epoch [2/120    avg_loss:1.146, val_acc:0.649]
Epoch [3/120    avg_loss:1.106, val_acc:0.782]
Epoch [4/120    avg_loss:0.862, val_acc:0.756]
Epoch [5/120    avg_loss:0.659, val_acc:0.740]
Epoch [6/120    avg_loss:0.634, val_acc:0.833]
Epoch [7/120    avg_loss:0.634, val_acc:0.845]
Epoch [8/120    avg_loss:0.606, val_acc:0.841]
Epoch [9/120    avg_loss:0.531, val_acc:0.861]
Epoch [10/120    avg_loss:0.483, val_acc:0.887]
Epoch [11/120    avg_loss:0.529, val_acc:0.885]
Epoch [12/120    avg_loss:0.515, val_acc:0.879]
Epoch [13/120    avg_loss:0.434, val_acc:0.905]
Epoch [14/120    avg_loss:0.432, val_acc:0.875]
Epoch [15/120    avg_loss:0.485, val_acc:0.891]
Epoch [16/120    avg_loss:0.366, val_acc:0.887]
Epoch [17/120    avg_loss:0.309, val_acc:0.843]
Epoch [18/120    avg_loss:0.370, val_acc:0.925]
Epoch [19/120    avg_loss:0.300, val_acc:0.925]
Epoch [20/120    avg_loss:0.246, val_acc:0.905]
Epoch [21/120    avg_loss:0.350, val_acc:0.889]
Epoch [22/120    avg_loss:0.254, val_acc:0.909]
Epoch [23/120    avg_loss:0.261, val_acc:0.931]
Epoch [24/120    avg_loss:0.249, val_acc:0.929]
Epoch [25/120    avg_loss:0.269, val_acc:0.913]
Epoch [26/120    avg_loss:0.207, val_acc:0.944]
Epoch [27/120    avg_loss:0.177, val_acc:0.921]
Epoch [28/120    avg_loss:0.236, val_acc:0.944]
Epoch [29/120    avg_loss:0.215, val_acc:0.935]
Epoch [30/120    avg_loss:0.180, val_acc:0.940]
Epoch [31/120    avg_loss:0.181, val_acc:0.929]
Epoch [32/120    avg_loss:0.187, val_acc:0.905]
Epoch [33/120    avg_loss:0.249, val_acc:0.923]
Epoch [34/120    avg_loss:0.199, val_acc:0.952]
Epoch [35/120    avg_loss:0.151, val_acc:0.938]
Epoch [36/120    avg_loss:0.139, val_acc:0.925]
Epoch [37/120    avg_loss:0.115, val_acc:0.931]
Epoch [38/120    avg_loss:0.154, val_acc:0.958]
Epoch [39/120    avg_loss:0.126, val_acc:0.944]
Epoch [40/120    avg_loss:0.133, val_acc:0.954]
Epoch [41/120    avg_loss:0.071, val_acc:0.962]
Epoch [42/120    avg_loss:0.069, val_acc:0.954]
Epoch [43/120    avg_loss:0.101, val_acc:0.935]
Epoch [44/120    avg_loss:0.091, val_acc:0.962]
Epoch [45/120    avg_loss:0.094, val_acc:0.964]
Epoch [46/120    avg_loss:0.072, val_acc:0.952]
Epoch [47/120    avg_loss:0.119, val_acc:0.935]
Epoch [48/120    avg_loss:0.082, val_acc:0.952]
Epoch [49/120    avg_loss:0.148, val_acc:0.944]
Epoch [50/120    avg_loss:0.103, val_acc:0.942]
Epoch [51/120    avg_loss:0.104, val_acc:0.935]
Epoch [52/120    avg_loss:0.096, val_acc:0.919]
Epoch [53/120    avg_loss:0.168, val_acc:0.940]
Epoch [54/120    avg_loss:0.129, val_acc:0.946]
Epoch [55/120    avg_loss:0.147, val_acc:0.954]
Epoch [56/120    avg_loss:0.098, val_acc:0.964]
Epoch [57/120    avg_loss:0.070, val_acc:0.968]
Epoch [58/120    avg_loss:0.039, val_acc:0.968]
Epoch [59/120    avg_loss:0.039, val_acc:0.956]
Epoch [60/120    avg_loss:0.039, val_acc:0.960]
Epoch [61/120    avg_loss:0.076, val_acc:0.974]
Epoch [62/120    avg_loss:0.030, val_acc:0.978]
Epoch [63/120    avg_loss:0.023, val_acc:0.974]
Epoch [64/120    avg_loss:0.038, val_acc:0.974]
Epoch [65/120    avg_loss:0.053, val_acc:0.960]
Epoch [66/120    avg_loss:0.021, val_acc:0.970]
Epoch [67/120    avg_loss:0.021, val_acc:0.974]
Epoch [68/120    avg_loss:0.012, val_acc:0.976]
Epoch [69/120    avg_loss:0.097, val_acc:0.964]
Epoch [70/120    avg_loss:0.120, val_acc:0.972]
Epoch [71/120    avg_loss:0.107, val_acc:0.964]
Epoch [72/120    avg_loss:0.064, val_acc:0.976]
Epoch [73/120    avg_loss:0.049, val_acc:0.962]
Epoch [74/120    avg_loss:0.045, val_acc:0.970]
Epoch [75/120    avg_loss:0.028, val_acc:0.974]
Epoch [76/120    avg_loss:0.017, val_acc:0.972]
Epoch [77/120    avg_loss:0.019, val_acc:0.972]
Epoch [78/120    avg_loss:0.025, val_acc:0.974]
Epoch [79/120    avg_loss:0.055, val_acc:0.978]
Epoch [80/120    avg_loss:0.017, val_acc:0.980]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.978]
Epoch [83/120    avg_loss:0.018, val_acc:0.976]
Epoch [84/120    avg_loss:0.015, val_acc:0.978]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.013, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.976]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.014, val_acc:0.978]
Epoch [90/120    avg_loss:0.021, val_acc:0.976]
Epoch [91/120    avg_loss:0.018, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.982]
Epoch [94/120    avg_loss:0.019, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.980]
Epoch [97/120    avg_loss:0.031, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.982]
Epoch [99/120    avg_loss:0.017, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.015, val_acc:0.980]
Epoch [102/120    avg_loss:0.009, val_acc:0.978]
Epoch [103/120    avg_loss:0.012, val_acc:0.978]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.020, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.017, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.023, val_acc:0.982]
Epoch [111/120    avg_loss:0.013, val_acc:0.982]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.022, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   3   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 217   7   0   0   0   0   0   0   3   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.9977221  0.98678414 0.94347826 0.94809689
 0.98771499 1.         0.99742931 0.99893276 1.         1.
 0.99559471 1.        ]

Kappa:
0.9928781324448698
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0dc5ed0748>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.096, val_acc:0.556]
Epoch [2/120    avg_loss:1.365, val_acc:0.708]
Epoch [3/120    avg_loss:0.989, val_acc:0.726]
Epoch [4/120    avg_loss:0.819, val_acc:0.784]
Epoch [5/120    avg_loss:0.655, val_acc:0.857]
Epoch [6/120    avg_loss:0.657, val_acc:0.857]
Epoch [7/120    avg_loss:0.546, val_acc:0.859]
Epoch [8/120    avg_loss:0.656, val_acc:0.875]
Epoch [9/120    avg_loss:0.420, val_acc:0.897]
Epoch [10/120    avg_loss:0.481, val_acc:0.875]
Epoch [11/120    avg_loss:0.428, val_acc:0.885]
Epoch [12/120    avg_loss:0.357, val_acc:0.891]
Epoch [13/120    avg_loss:0.290, val_acc:0.861]
Epoch [14/120    avg_loss:0.441, val_acc:0.847]
Epoch [15/120    avg_loss:0.467, val_acc:0.909]
Epoch [16/120    avg_loss:0.314, val_acc:0.895]
Epoch [17/120    avg_loss:0.292, val_acc:0.907]
Epoch [18/120    avg_loss:0.348, val_acc:0.919]
Epoch [19/120    avg_loss:0.269, val_acc:0.925]
Epoch [20/120    avg_loss:0.239, val_acc:0.905]
Epoch [21/120    avg_loss:0.343, val_acc:0.877]
Epoch [22/120    avg_loss:0.399, val_acc:0.911]
Epoch [23/120    avg_loss:0.326, val_acc:0.935]
Epoch [24/120    avg_loss:0.211, val_acc:0.956]
Epoch [25/120    avg_loss:0.191, val_acc:0.921]
Epoch [26/120    avg_loss:0.248, val_acc:0.946]
Epoch [27/120    avg_loss:0.203, val_acc:0.927]
Epoch [28/120    avg_loss:0.211, val_acc:0.940]
Epoch [29/120    avg_loss:0.232, val_acc:0.909]
Epoch [30/120    avg_loss:0.196, val_acc:0.952]
Epoch [31/120    avg_loss:0.178, val_acc:0.940]
Epoch [32/120    avg_loss:0.195, val_acc:0.927]
Epoch [33/120    avg_loss:0.131, val_acc:0.950]
Epoch [34/120    avg_loss:0.139, val_acc:0.956]
Epoch [35/120    avg_loss:0.219, val_acc:0.956]
Epoch [36/120    avg_loss:0.160, val_acc:0.966]
Epoch [37/120    avg_loss:0.150, val_acc:0.958]
Epoch [38/120    avg_loss:0.144, val_acc:0.968]
Epoch [39/120    avg_loss:0.153, val_acc:0.919]
Epoch [40/120    avg_loss:0.135, val_acc:0.942]
Epoch [41/120    avg_loss:0.160, val_acc:0.958]
Epoch [42/120    avg_loss:0.093, val_acc:0.925]
Epoch [43/120    avg_loss:0.095, val_acc:0.970]
Epoch [44/120    avg_loss:0.083, val_acc:0.960]
Epoch [45/120    avg_loss:0.083, val_acc:0.956]
Epoch [46/120    avg_loss:0.083, val_acc:0.964]
Epoch [47/120    avg_loss:0.088, val_acc:0.968]
Epoch [48/120    avg_loss:0.067, val_acc:0.974]
Epoch [49/120    avg_loss:0.100, val_acc:0.944]
Epoch [50/120    avg_loss:0.096, val_acc:0.948]
Epoch [51/120    avg_loss:0.107, val_acc:0.942]
Epoch [52/120    avg_loss:0.109, val_acc:0.968]
Epoch [53/120    avg_loss:0.070, val_acc:0.968]
Epoch [54/120    avg_loss:0.085, val_acc:0.968]
Epoch [55/120    avg_loss:0.091, val_acc:0.968]
Epoch [56/120    avg_loss:0.061, val_acc:0.946]
Epoch [57/120    avg_loss:0.083, val_acc:0.970]
Epoch [58/120    avg_loss:0.104, val_acc:0.970]
Epoch [59/120    avg_loss:0.057, val_acc:0.962]
Epoch [60/120    avg_loss:0.062, val_acc:0.962]
Epoch [61/120    avg_loss:0.043, val_acc:0.978]
Epoch [62/120    avg_loss:0.051, val_acc:0.978]
Epoch [63/120    avg_loss:0.066, val_acc:0.984]
Epoch [64/120    avg_loss:0.033, val_acc:0.976]
Epoch [65/120    avg_loss:0.038, val_acc:0.980]
Epoch [66/120    avg_loss:0.056, val_acc:0.966]
Epoch [67/120    avg_loss:0.068, val_acc:0.976]
Epoch [68/120    avg_loss:0.041, val_acc:0.980]
Epoch [69/120    avg_loss:0.037, val_acc:0.978]
Epoch [70/120    avg_loss:0.095, val_acc:0.980]
Epoch [71/120    avg_loss:0.033, val_acc:0.978]
Epoch [72/120    avg_loss:0.035, val_acc:0.980]
Epoch [73/120    avg_loss:0.051, val_acc:0.978]
Epoch [74/120    avg_loss:0.021, val_acc:0.984]
Epoch [75/120    avg_loss:0.033, val_acc:0.980]
Epoch [76/120    avg_loss:0.024, val_acc:0.984]
Epoch [77/120    avg_loss:0.022, val_acc:0.990]
Epoch [78/120    avg_loss:0.047, val_acc:0.984]
Epoch [79/120    avg_loss:0.037, val_acc:0.962]
Epoch [80/120    avg_loss:0.065, val_acc:0.931]
Epoch [81/120    avg_loss:0.218, val_acc:0.889]
Epoch [82/120    avg_loss:0.118, val_acc:0.966]
Epoch [83/120    avg_loss:0.116, val_acc:0.960]
Epoch [84/120    avg_loss:0.081, val_acc:0.944]
Epoch [85/120    avg_loss:0.090, val_acc:0.984]
Epoch [86/120    avg_loss:0.040, val_acc:0.978]
Epoch [87/120    avg_loss:0.027, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.980]
Epoch [89/120    avg_loss:0.022, val_acc:0.984]
Epoch [90/120    avg_loss:0.018, val_acc:0.990]
Epoch [91/120    avg_loss:0.053, val_acc:0.982]
Epoch [92/120    avg_loss:0.020, val_acc:0.984]
Epoch [93/120    avg_loss:0.020, val_acc:0.984]
Epoch [94/120    avg_loss:0.049, val_acc:0.944]
Epoch [95/120    avg_loss:0.025, val_acc:0.972]
Epoch [96/120    avg_loss:0.043, val_acc:0.956]
Epoch [97/120    avg_loss:0.132, val_acc:0.960]
Epoch [98/120    avg_loss:0.031, val_acc:0.976]
Epoch [99/120    avg_loss:0.043, val_acc:0.954]
Epoch [100/120    avg_loss:0.055, val_acc:0.962]
Epoch [101/120    avg_loss:0.063, val_acc:0.974]
Epoch [102/120    avg_loss:0.040, val_acc:0.984]
Epoch [103/120    avg_loss:0.041, val_acc:0.982]
Epoch [104/120    avg_loss:0.030, val_acc:0.988]
Epoch [105/120    avg_loss:0.017, val_acc:0.988]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.015, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.034, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.016, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 225   0   0   0   0   1   3   0   0   0   0]
 [  0   0   0   0 212  12   0   0   0   0   0   0   3   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.9977221  0.98901099 0.94013304 0.91724138
 1.         1.         0.998713   0.99680511 1.         0.99472296
 0.99226519 1.        ]

Kappa:
0.9914537334110481
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8cd584828>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.017, val_acc:0.555]
Epoch [2/120    avg_loss:1.209, val_acc:0.701]
Epoch [3/120    avg_loss:0.985, val_acc:0.789]
Epoch [4/120    avg_loss:0.839, val_acc:0.734]
Epoch [5/120    avg_loss:0.817, val_acc:0.727]
Epoch [6/120    avg_loss:0.631, val_acc:0.883]
Epoch [7/120    avg_loss:0.699, val_acc:0.863]
Epoch [8/120    avg_loss:0.554, val_acc:0.816]
Epoch [9/120    avg_loss:0.567, val_acc:0.861]
Epoch [10/120    avg_loss:0.505, val_acc:0.871]
Epoch [11/120    avg_loss:0.393, val_acc:0.875]
Epoch [12/120    avg_loss:0.552, val_acc:0.893]
Epoch [13/120    avg_loss:0.452, val_acc:0.844]
Epoch [14/120    avg_loss:0.424, val_acc:0.877]
Epoch [15/120    avg_loss:0.393, val_acc:0.918]
Epoch [16/120    avg_loss:0.323, val_acc:0.928]
Epoch [17/120    avg_loss:0.360, val_acc:0.900]
Epoch [18/120    avg_loss:0.471, val_acc:0.934]
Epoch [19/120    avg_loss:0.317, val_acc:0.918]
Epoch [20/120    avg_loss:0.266, val_acc:0.920]
Epoch [21/120    avg_loss:0.394, val_acc:0.912]
Epoch [22/120    avg_loss:0.290, val_acc:0.943]
Epoch [23/120    avg_loss:0.278, val_acc:0.914]
Epoch [24/120    avg_loss:0.303, val_acc:0.912]
Epoch [25/120    avg_loss:0.221, val_acc:0.922]
Epoch [26/120    avg_loss:0.188, val_acc:0.957]
Epoch [27/120    avg_loss:0.202, val_acc:0.918]
Epoch [28/120    avg_loss:0.223, val_acc:0.914]
Epoch [29/120    avg_loss:0.227, val_acc:0.918]
Epoch [30/120    avg_loss:0.161, val_acc:0.951]
Epoch [31/120    avg_loss:0.174, val_acc:0.959]
Epoch [32/120    avg_loss:0.128, val_acc:0.941]
Epoch [33/120    avg_loss:0.192, val_acc:0.955]
Epoch [34/120    avg_loss:0.151, val_acc:0.959]
Epoch [35/120    avg_loss:0.211, val_acc:0.959]
Epoch [36/120    avg_loss:0.188, val_acc:0.947]
Epoch [37/120    avg_loss:0.172, val_acc:0.957]
Epoch [38/120    avg_loss:0.129, val_acc:0.971]
Epoch [39/120    avg_loss:0.157, val_acc:0.947]
Epoch [40/120    avg_loss:0.137, val_acc:0.955]
Epoch [41/120    avg_loss:0.114, val_acc:0.971]
Epoch [42/120    avg_loss:0.081, val_acc:0.977]
Epoch [43/120    avg_loss:0.059, val_acc:0.967]
Epoch [44/120    avg_loss:0.074, val_acc:0.980]
Epoch [45/120    avg_loss:0.100, val_acc:0.967]
Epoch [46/120    avg_loss:0.122, val_acc:0.955]
Epoch [47/120    avg_loss:0.219, val_acc:0.924]
Epoch [48/120    avg_loss:0.205, val_acc:0.945]
Epoch [49/120    avg_loss:0.144, val_acc:0.941]
Epoch [50/120    avg_loss:0.162, val_acc:0.973]
Epoch [51/120    avg_loss:0.088, val_acc:0.943]
Epoch [52/120    avg_loss:0.095, val_acc:0.977]
Epoch [53/120    avg_loss:0.080, val_acc:0.982]
Epoch [54/120    avg_loss:0.130, val_acc:0.957]
Epoch [55/120    avg_loss:0.158, val_acc:0.971]
Epoch [56/120    avg_loss:0.082, val_acc:0.977]
Epoch [57/120    avg_loss:0.091, val_acc:0.971]
Epoch [58/120    avg_loss:0.048, val_acc:0.975]
Epoch [59/120    avg_loss:0.060, val_acc:0.977]
Epoch [60/120    avg_loss:0.034, val_acc:0.986]
Epoch [61/120    avg_loss:0.047, val_acc:0.967]
Epoch [62/120    avg_loss:0.083, val_acc:0.975]
Epoch [63/120    avg_loss:0.065, val_acc:0.979]
Epoch [64/120    avg_loss:0.064, val_acc:0.977]
Epoch [65/120    avg_loss:0.046, val_acc:0.986]
Epoch [66/120    avg_loss:0.029, val_acc:0.979]
Epoch [67/120    avg_loss:0.052, val_acc:0.982]
Epoch [68/120    avg_loss:0.039, val_acc:0.979]
Epoch [69/120    avg_loss:0.049, val_acc:0.979]
Epoch [70/120    avg_loss:0.044, val_acc:0.986]
Epoch [71/120    avg_loss:0.034, val_acc:0.986]
Epoch [72/120    avg_loss:0.068, val_acc:0.971]
Epoch [73/120    avg_loss:0.050, val_acc:0.979]
Epoch [74/120    avg_loss:0.063, val_acc:0.949]
Epoch [75/120    avg_loss:0.094, val_acc:0.973]
Epoch [76/120    avg_loss:0.131, val_acc:0.951]
Epoch [77/120    avg_loss:0.096, val_acc:0.961]
Epoch [78/120    avg_loss:0.064, val_acc:0.980]
Epoch [79/120    avg_loss:0.056, val_acc:0.977]
Epoch [80/120    avg_loss:0.034, val_acc:0.980]
Epoch [81/120    avg_loss:0.033, val_acc:0.982]
Epoch [82/120    avg_loss:0.046, val_acc:0.980]
Epoch [83/120    avg_loss:0.022, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.984]
Epoch [85/120    avg_loss:0.020, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.021, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.021, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.015, val_acc:0.988]
Epoch [100/120    avg_loss:0.017, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.021, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.988]
Epoch [106/120    avg_loss:0.016, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.015, val_acc:0.988]
Epoch [110/120    avg_loss:0.015, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.017, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.012, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         0.99545455 0.99782135 0.95614035 0.93379791
 1.         1.         1.         0.99893276 1.         1.
 0.99668508 1.        ]

Kappa:
0.9945401596640135
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6ae83c87f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.926, val_acc:0.633]
Epoch [2/120    avg_loss:1.150, val_acc:0.671]
Epoch [3/120    avg_loss:0.910, val_acc:0.794]
Epoch [4/120    avg_loss:0.772, val_acc:0.748]
Epoch [5/120    avg_loss:0.629, val_acc:0.857]
Epoch [6/120    avg_loss:0.699, val_acc:0.831]
Epoch [7/120    avg_loss:0.819, val_acc:0.827]
Epoch [8/120    avg_loss:0.625, val_acc:0.839]
Epoch [9/120    avg_loss:0.426, val_acc:0.879]
Epoch [10/120    avg_loss:0.527, val_acc:0.899]
Epoch [11/120    avg_loss:0.445, val_acc:0.881]
Epoch [12/120    avg_loss:0.428, val_acc:0.877]
Epoch [13/120    avg_loss:0.381, val_acc:0.903]
Epoch [14/120    avg_loss:0.319, val_acc:0.913]
Epoch [15/120    avg_loss:0.262, val_acc:0.921]
Epoch [16/120    avg_loss:0.363, val_acc:0.909]
Epoch [17/120    avg_loss:0.317, val_acc:0.915]
Epoch [18/120    avg_loss:0.335, val_acc:0.929]
Epoch [19/120    avg_loss:0.267, val_acc:0.929]
Epoch [20/120    avg_loss:0.213, val_acc:0.950]
Epoch [21/120    avg_loss:0.281, val_acc:0.931]
Epoch [22/120    avg_loss:0.246, val_acc:0.931]
Epoch [23/120    avg_loss:0.218, val_acc:0.940]
Epoch [24/120    avg_loss:0.177, val_acc:0.938]
Epoch [25/120    avg_loss:0.201, val_acc:0.952]
Epoch [26/120    avg_loss:0.164, val_acc:0.956]
Epoch [27/120    avg_loss:0.202, val_acc:0.954]
Epoch [28/120    avg_loss:0.159, val_acc:0.968]
Epoch [29/120    avg_loss:0.123, val_acc:0.968]
Epoch [30/120    avg_loss:0.090, val_acc:0.954]
Epoch [31/120    avg_loss:0.128, val_acc:0.940]
Epoch [32/120    avg_loss:0.152, val_acc:0.968]
Epoch [33/120    avg_loss:0.119, val_acc:0.962]
Epoch [34/120    avg_loss:0.139, val_acc:0.964]
Epoch [35/120    avg_loss:0.185, val_acc:0.940]
Epoch [36/120    avg_loss:0.259, val_acc:0.907]
Epoch [37/120    avg_loss:0.202, val_acc:0.962]
Epoch [38/120    avg_loss:0.187, val_acc:0.950]
Epoch [39/120    avg_loss:0.086, val_acc:0.960]
Epoch [40/120    avg_loss:0.146, val_acc:0.966]
Epoch [41/120    avg_loss:0.131, val_acc:0.929]
Epoch [42/120    avg_loss:0.188, val_acc:0.950]
Epoch [43/120    avg_loss:0.169, val_acc:0.970]
Epoch [44/120    avg_loss:0.066, val_acc:0.980]
Epoch [45/120    avg_loss:0.092, val_acc:0.970]
Epoch [46/120    avg_loss:0.097, val_acc:0.970]
Epoch [47/120    avg_loss:0.104, val_acc:0.956]
Epoch [48/120    avg_loss:0.128, val_acc:0.970]
Epoch [49/120    avg_loss:0.089, val_acc:0.966]
Epoch [50/120    avg_loss:0.061, val_acc:0.974]
Epoch [51/120    avg_loss:0.077, val_acc:0.976]
Epoch [52/120    avg_loss:0.097, val_acc:0.966]
Epoch [53/120    avg_loss:0.099, val_acc:0.970]
Epoch [54/120    avg_loss:0.048, val_acc:0.974]
Epoch [55/120    avg_loss:0.047, val_acc:0.982]
Epoch [56/120    avg_loss:0.052, val_acc:0.984]
Epoch [57/120    avg_loss:0.038, val_acc:0.958]
Epoch [58/120    avg_loss:0.158, val_acc:0.952]
Epoch [59/120    avg_loss:0.132, val_acc:0.944]
Epoch [60/120    avg_loss:0.088, val_acc:0.940]
Epoch [61/120    avg_loss:0.068, val_acc:0.968]
Epoch [62/120    avg_loss:0.057, val_acc:0.972]
Epoch [63/120    avg_loss:0.056, val_acc:0.980]
Epoch [64/120    avg_loss:0.072, val_acc:0.990]
Epoch [65/120    avg_loss:0.032, val_acc:0.990]
Epoch [66/120    avg_loss:0.051, val_acc:0.978]
Epoch [67/120    avg_loss:0.025, val_acc:0.984]
Epoch [68/120    avg_loss:0.030, val_acc:0.982]
Epoch [69/120    avg_loss:0.023, val_acc:0.988]
Epoch [70/120    avg_loss:0.045, val_acc:0.986]
Epoch [71/120    avg_loss:0.088, val_acc:0.978]
Epoch [72/120    avg_loss:0.024, val_acc:0.986]
Epoch [73/120    avg_loss:0.022, val_acc:0.990]
Epoch [74/120    avg_loss:0.020, val_acc:0.988]
Epoch [75/120    avg_loss:0.029, val_acc:0.990]
Epoch [76/120    avg_loss:0.021, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.023, val_acc:0.990]
Epoch [79/120    avg_loss:0.018, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.028, val_acc:0.986]
Epoch [82/120    avg_loss:0.021, val_acc:0.988]
Epoch [83/120    avg_loss:0.013, val_acc:0.990]
Epoch [84/120    avg_loss:0.016, val_acc:0.990]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.012, val_acc:0.986]
Epoch [87/120    avg_loss:0.015, val_acc:0.974]
Epoch [88/120    avg_loss:0.061, val_acc:0.986]
Epoch [89/120    avg_loss:0.019, val_acc:0.990]
Epoch [90/120    avg_loss:0.014, val_acc:0.988]
Epoch [91/120    avg_loss:0.044, val_acc:0.974]
Epoch [92/120    avg_loss:0.014, val_acc:0.990]
Epoch [93/120    avg_loss:0.102, val_acc:0.933]
Epoch [94/120    avg_loss:0.066, val_acc:0.982]
Epoch [95/120    avg_loss:0.046, val_acc:0.962]
Epoch [96/120    avg_loss:0.042, val_acc:0.984]
Epoch [97/120    avg_loss:0.018, val_acc:0.986]
Epoch [98/120    avg_loss:0.022, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.014, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.992]
Epoch [103/120    avg_loss:0.015, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.988]
Epoch [107/120    avg_loss:0.020, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.982]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.015, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   0   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 208  13   0   0   0   0   0   0   6   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   3 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.99545455 0.99343545 0.93483146 0.92150171
 1.         1.         0.998713   0.99893276 1.         0.99603699
 0.98898678 1.        ]

Kappa:
0.9914538207935464
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d9fc58780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.063, val_acc:0.528]
Epoch [2/120    avg_loss:1.260, val_acc:0.690]
Epoch [3/120    avg_loss:0.939, val_acc:0.738]
Epoch [4/120    avg_loss:0.765, val_acc:0.754]
Epoch [5/120    avg_loss:0.820, val_acc:0.827]
Epoch [6/120    avg_loss:0.694, val_acc:0.746]
Epoch [7/120    avg_loss:0.598, val_acc:0.849]
Epoch [8/120    avg_loss:0.623, val_acc:0.857]
Epoch [9/120    avg_loss:0.454, val_acc:0.881]
Epoch [10/120    avg_loss:0.448, val_acc:0.881]
Epoch [11/120    avg_loss:0.392, val_acc:0.887]
Epoch [12/120    avg_loss:0.502, val_acc:0.871]
Epoch [13/120    avg_loss:0.502, val_acc:0.905]
Epoch [14/120    avg_loss:0.390, val_acc:0.901]
Epoch [15/120    avg_loss:0.418, val_acc:0.901]
Epoch [16/120    avg_loss:0.277, val_acc:0.925]
Epoch [17/120    avg_loss:0.210, val_acc:0.919]
Epoch [18/120    avg_loss:0.264, val_acc:0.913]
Epoch [19/120    avg_loss:0.218, val_acc:0.915]
Epoch [20/120    avg_loss:0.230, val_acc:0.909]
Epoch [21/120    avg_loss:0.186, val_acc:0.940]
Epoch [22/120    avg_loss:0.261, val_acc:0.940]
Epoch [23/120    avg_loss:0.301, val_acc:0.895]
Epoch [24/120    avg_loss:0.191, val_acc:0.921]
Epoch [25/120    avg_loss:0.222, val_acc:0.933]
Epoch [26/120    avg_loss:0.191, val_acc:0.940]
Epoch [27/120    avg_loss:0.149, val_acc:0.948]
Epoch [28/120    avg_loss:0.177, val_acc:0.954]
Epoch [29/120    avg_loss:0.127, val_acc:0.940]
Epoch [30/120    avg_loss:0.277, val_acc:0.915]
Epoch [31/120    avg_loss:0.188, val_acc:0.954]
Epoch [32/120    avg_loss:0.129, val_acc:0.944]
Epoch [33/120    avg_loss:0.217, val_acc:0.950]
Epoch [34/120    avg_loss:0.149, val_acc:0.942]
Epoch [35/120    avg_loss:0.106, val_acc:0.944]
Epoch [36/120    avg_loss:0.196, val_acc:0.917]
Epoch [37/120    avg_loss:0.141, val_acc:0.952]
Epoch [38/120    avg_loss:0.145, val_acc:0.970]
Epoch [39/120    avg_loss:0.115, val_acc:0.962]
Epoch [40/120    avg_loss:0.116, val_acc:0.964]
Epoch [41/120    avg_loss:0.086, val_acc:0.950]
Epoch [42/120    avg_loss:0.134, val_acc:0.950]
Epoch [43/120    avg_loss:0.093, val_acc:0.978]
Epoch [44/120    avg_loss:0.074, val_acc:0.952]
Epoch [45/120    avg_loss:0.112, val_acc:0.978]
Epoch [46/120    avg_loss:0.064, val_acc:0.976]
Epoch [47/120    avg_loss:0.107, val_acc:0.944]
Epoch [48/120    avg_loss:0.114, val_acc:0.933]
Epoch [49/120    avg_loss:0.092, val_acc:0.974]
Epoch [50/120    avg_loss:0.057, val_acc:0.982]
Epoch [51/120    avg_loss:0.058, val_acc:0.976]
Epoch [52/120    avg_loss:0.049, val_acc:0.974]
Epoch [53/120    avg_loss:0.030, val_acc:0.976]
Epoch [54/120    avg_loss:0.053, val_acc:0.970]
Epoch [55/120    avg_loss:0.055, val_acc:0.974]
Epoch [56/120    avg_loss:0.050, val_acc:0.976]
Epoch [57/120    avg_loss:0.034, val_acc:0.982]
Epoch [58/120    avg_loss:0.032, val_acc:0.972]
Epoch [59/120    avg_loss:0.066, val_acc:0.972]
Epoch [60/120    avg_loss:0.044, val_acc:0.980]
Epoch [61/120    avg_loss:0.079, val_acc:0.974]
Epoch [62/120    avg_loss:0.046, val_acc:0.968]
Epoch [63/120    avg_loss:0.035, val_acc:0.986]
Epoch [64/120    avg_loss:0.039, val_acc:0.976]
Epoch [65/120    avg_loss:0.054, val_acc:0.976]
Epoch [66/120    avg_loss:0.045, val_acc:0.974]
Epoch [67/120    avg_loss:0.064, val_acc:0.968]
Epoch [68/120    avg_loss:0.086, val_acc:0.982]
Epoch [69/120    avg_loss:0.049, val_acc:0.974]
Epoch [70/120    avg_loss:0.017, val_acc:0.982]
Epoch [71/120    avg_loss:0.016, val_acc:0.980]
Epoch [72/120    avg_loss:0.027, val_acc:0.976]
Epoch [73/120    avg_loss:0.074, val_acc:0.980]
Epoch [74/120    avg_loss:0.022, val_acc:0.978]
Epoch [75/120    avg_loss:0.028, val_acc:0.984]
Epoch [76/120    avg_loss:0.015, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.984]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.984]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.013, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.023, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.015, val_acc:0.986]
Epoch [94/120    avg_loss:0.014, val_acc:0.986]
Epoch [95/120    avg_loss:0.019, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.013, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   3   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8   0 198   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         1.         0.98678414 0.94339623 0.94244604
 0.98019802 1.         0.99742931 0.99893276 1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9924031993828611
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f560d8d47b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.916, val_acc:0.675]
Epoch [2/120    avg_loss:1.064, val_acc:0.736]
Epoch [3/120    avg_loss:0.840, val_acc:0.738]
Epoch [4/120    avg_loss:0.710, val_acc:0.808]
Epoch [5/120    avg_loss:0.616, val_acc:0.879]
Epoch [6/120    avg_loss:0.548, val_acc:0.843]
Epoch [7/120    avg_loss:0.536, val_acc:0.845]
Epoch [8/120    avg_loss:0.521, val_acc:0.829]
Epoch [9/120    avg_loss:0.509, val_acc:0.877]
Epoch [10/120    avg_loss:0.421, val_acc:0.905]
Epoch [11/120    avg_loss:0.357, val_acc:0.911]
Epoch [12/120    avg_loss:0.361, val_acc:0.891]
Epoch [13/120    avg_loss:0.319, val_acc:0.911]
Epoch [14/120    avg_loss:0.468, val_acc:0.861]
Epoch [15/120    avg_loss:0.364, val_acc:0.897]
Epoch [16/120    avg_loss:0.358, val_acc:0.889]
Epoch [17/120    avg_loss:0.405, val_acc:0.911]
Epoch [18/120    avg_loss:0.279, val_acc:0.950]
Epoch [19/120    avg_loss:0.295, val_acc:0.913]
Epoch [20/120    avg_loss:0.240, val_acc:0.950]
Epoch [21/120    avg_loss:0.175, val_acc:0.964]
Epoch [22/120    avg_loss:0.141, val_acc:0.921]
Epoch [23/120    avg_loss:0.269, val_acc:0.869]
Epoch [24/120    avg_loss:0.284, val_acc:0.889]
Epoch [25/120    avg_loss:0.223, val_acc:0.952]
Epoch [26/120    avg_loss:0.362, val_acc:0.948]
Epoch [27/120    avg_loss:0.190, val_acc:0.954]
Epoch [28/120    avg_loss:0.235, val_acc:0.944]
Epoch [29/120    avg_loss:0.308, val_acc:0.865]
Epoch [30/120    avg_loss:0.283, val_acc:0.956]
Epoch [31/120    avg_loss:0.196, val_acc:0.954]
Epoch [32/120    avg_loss:0.246, val_acc:0.968]
Epoch [33/120    avg_loss:0.139, val_acc:0.970]
Epoch [34/120    avg_loss:0.111, val_acc:0.976]
Epoch [35/120    avg_loss:0.091, val_acc:0.968]
Epoch [36/120    avg_loss:0.114, val_acc:0.968]
Epoch [37/120    avg_loss:0.163, val_acc:0.962]
Epoch [38/120    avg_loss:0.148, val_acc:0.956]
Epoch [39/120    avg_loss:0.130, val_acc:0.966]
Epoch [40/120    avg_loss:0.167, val_acc:0.966]
Epoch [41/120    avg_loss:0.119, val_acc:0.972]
Epoch [42/120    avg_loss:0.106, val_acc:0.980]
Epoch [43/120    avg_loss:0.073, val_acc:0.984]
Epoch [44/120    avg_loss:0.034, val_acc:0.974]
Epoch [45/120    avg_loss:0.050, val_acc:0.970]
Epoch [46/120    avg_loss:0.085, val_acc:0.976]
Epoch [47/120    avg_loss:0.076, val_acc:0.958]
Epoch [48/120    avg_loss:0.082, val_acc:0.968]
Epoch [49/120    avg_loss:0.085, val_acc:0.954]
Epoch [50/120    avg_loss:0.114, val_acc:0.972]
Epoch [51/120    avg_loss:0.076, val_acc:0.986]
Epoch [52/120    avg_loss:0.110, val_acc:0.966]
Epoch [53/120    avg_loss:0.050, val_acc:0.980]
Epoch [54/120    avg_loss:0.062, val_acc:0.984]
Epoch [55/120    avg_loss:0.090, val_acc:0.986]
Epoch [56/120    avg_loss:0.066, val_acc:0.964]
Epoch [57/120    avg_loss:0.050, val_acc:0.958]
Epoch [58/120    avg_loss:0.057, val_acc:0.982]
Epoch [59/120    avg_loss:0.043, val_acc:0.988]
Epoch [60/120    avg_loss:0.075, val_acc:0.974]
Epoch [61/120    avg_loss:0.087, val_acc:0.960]
Epoch [62/120    avg_loss:0.084, val_acc:0.974]
Epoch [63/120    avg_loss:0.088, val_acc:0.946]
Epoch [64/120    avg_loss:0.112, val_acc:0.986]
Epoch [65/120    avg_loss:0.076, val_acc:0.988]
Epoch [66/120    avg_loss:0.048, val_acc:0.990]
Epoch [67/120    avg_loss:0.055, val_acc:0.986]
Epoch [68/120    avg_loss:0.043, val_acc:0.974]
Epoch [69/120    avg_loss:0.034, val_acc:0.986]
Epoch [70/120    avg_loss:0.017, val_acc:0.994]
Epoch [71/120    avg_loss:0.019, val_acc:0.992]
Epoch [72/120    avg_loss:0.020, val_acc:0.996]
Epoch [73/120    avg_loss:0.013, val_acc:0.996]
Epoch [74/120    avg_loss:0.009, val_acc:0.998]
Epoch [75/120    avg_loss:0.022, val_acc:0.988]
Epoch [76/120    avg_loss:0.019, val_acc:0.986]
Epoch [77/120    avg_loss:0.026, val_acc:0.996]
Epoch [78/120    avg_loss:0.036, val_acc:0.986]
Epoch [79/120    avg_loss:0.063, val_acc:0.990]
Epoch [80/120    avg_loss:0.059, val_acc:0.996]
Epoch [81/120    avg_loss:0.034, val_acc:0.990]
Epoch [82/120    avg_loss:0.020, val_acc:0.990]
Epoch [83/120    avg_loss:0.017, val_acc:0.992]
Epoch [84/120    avg_loss:0.016, val_acc:0.992]
Epoch [85/120    avg_loss:0.024, val_acc:0.996]
Epoch [86/120    avg_loss:0.017, val_acc:0.992]
Epoch [87/120    avg_loss:0.017, val_acc:0.982]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.012, val_acc:0.994]
Epoch [91/120    avg_loss:0.007, val_acc:0.994]
Epoch [92/120    avg_loss:0.009, val_acc:0.996]
Epoch [93/120    avg_loss:0.008, val_acc:0.996]
Epoch [94/120    avg_loss:0.012, val_acc:0.996]
Epoch [95/120    avg_loss:0.013, val_acc:0.996]
Epoch [96/120    avg_loss:0.016, val_acc:0.996]
Epoch [97/120    avg_loss:0.014, val_acc:0.996]
Epoch [98/120    avg_loss:0.009, val_acc:0.996]
Epoch [99/120    avg_loss:0.016, val_acc:0.996]
Epoch [100/120    avg_loss:0.010, val_acc:0.996]
Epoch [101/120    avg_loss:0.006, val_acc:0.996]
Epoch [102/120    avg_loss:0.007, val_acc:0.996]
Epoch [103/120    avg_loss:0.007, val_acc:0.996]
Epoch [104/120    avg_loss:0.005, val_acc:0.996]
Epoch [105/120    avg_loss:0.007, val_acc:0.996]
Epoch [106/120    avg_loss:0.006, val_acc:0.996]
Epoch [107/120    avg_loss:0.015, val_acc:0.996]
Epoch [108/120    avg_loss:0.011, val_acc:0.996]
Epoch [109/120    avg_loss:0.013, val_acc:0.996]
Epoch [110/120    avg_loss:0.007, val_acc:0.996]
Epoch [111/120    avg_loss:0.006, val_acc:0.996]
Epoch [112/120    avg_loss:0.008, val_acc:0.996]
Epoch [113/120    avg_loss:0.005, val_acc:0.996]
Epoch [114/120    avg_loss:0.015, val_acc:0.996]
Epoch [115/120    avg_loss:0.008, val_acc:0.996]
Epoch [116/120    avg_loss:0.007, val_acc:0.996]
Epoch [117/120    avg_loss:0.006, val_acc:0.996]
Epoch [118/120    avg_loss:0.009, val_acc:0.996]
Epoch [119/120    avg_loss:0.012, val_acc:0.996]
Epoch [120/120    avg_loss:0.008, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   7   0   0   0   0   0   0   3   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  14 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         1.         1.         0.96659243 0.96949153
 0.99266504 1.         1.         1.         1.         0.98177083
 0.98100559 1.        ]

Kappa:
0.9931161932027103
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc883ac7780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.955, val_acc:0.633]
Epoch [2/120    avg_loss:1.137, val_acc:0.766]
Epoch [3/120    avg_loss:0.819, val_acc:0.788]
Epoch [4/120    avg_loss:0.685, val_acc:0.821]
Epoch [5/120    avg_loss:0.725, val_acc:0.798]
Epoch [6/120    avg_loss:0.684, val_acc:0.847]
Epoch [7/120    avg_loss:0.516, val_acc:0.827]
Epoch [8/120    avg_loss:0.508, val_acc:0.877]
Epoch [9/120    avg_loss:0.475, val_acc:0.857]
Epoch [10/120    avg_loss:0.489, val_acc:0.865]
Epoch [11/120    avg_loss:0.531, val_acc:0.847]
Epoch [12/120    avg_loss:0.416, val_acc:0.863]
Epoch [13/120    avg_loss:0.304, val_acc:0.911]
Epoch [14/120    avg_loss:0.360, val_acc:0.875]
Epoch [15/120    avg_loss:0.387, val_acc:0.911]
Epoch [16/120    avg_loss:0.292, val_acc:0.907]
Epoch [17/120    avg_loss:0.351, val_acc:0.927]
Epoch [18/120    avg_loss:0.289, val_acc:0.923]
Epoch [19/120    avg_loss:0.322, val_acc:0.905]
Epoch [20/120    avg_loss:0.211, val_acc:0.950]
Epoch [21/120    avg_loss:0.182, val_acc:0.931]
Epoch [22/120    avg_loss:0.161, val_acc:0.938]
Epoch [23/120    avg_loss:0.201, val_acc:0.944]
Epoch [24/120    avg_loss:0.249, val_acc:0.929]
Epoch [25/120    avg_loss:0.154, val_acc:0.915]
Epoch [26/120    avg_loss:0.174, val_acc:0.915]
Epoch [27/120    avg_loss:0.226, val_acc:0.954]
Epoch [28/120    avg_loss:0.118, val_acc:0.948]
Epoch [29/120    avg_loss:0.141, val_acc:0.944]
Epoch [30/120    avg_loss:0.151, val_acc:0.946]
Epoch [31/120    avg_loss:0.160, val_acc:0.933]
Epoch [32/120    avg_loss:0.132, val_acc:0.944]
Epoch [33/120    avg_loss:0.138, val_acc:0.923]
Epoch [34/120    avg_loss:0.066, val_acc:0.970]
Epoch [35/120    avg_loss:0.065, val_acc:0.958]
Epoch [36/120    avg_loss:0.086, val_acc:0.966]
Epoch [37/120    avg_loss:0.083, val_acc:0.948]
Epoch [38/120    avg_loss:0.164, val_acc:0.942]
Epoch [39/120    avg_loss:0.129, val_acc:0.956]
Epoch [40/120    avg_loss:0.100, val_acc:0.976]
Epoch [41/120    avg_loss:0.079, val_acc:0.974]
Epoch [42/120    avg_loss:0.208, val_acc:0.954]
Epoch [43/120    avg_loss:0.085, val_acc:0.962]
Epoch [44/120    avg_loss:0.103, val_acc:0.950]
Epoch [45/120    avg_loss:0.085, val_acc:0.966]
Epoch [46/120    avg_loss:0.075, val_acc:0.962]
Epoch [47/120    avg_loss:0.050, val_acc:0.988]
Epoch [48/120    avg_loss:0.061, val_acc:0.974]
Epoch [49/120    avg_loss:0.055, val_acc:0.980]
Epoch [50/120    avg_loss:0.041, val_acc:0.964]
Epoch [51/120    avg_loss:0.043, val_acc:0.978]
Epoch [52/120    avg_loss:0.034, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.986]
Epoch [54/120    avg_loss:0.037, val_acc:0.970]
Epoch [55/120    avg_loss:0.037, val_acc:0.984]
Epoch [56/120    avg_loss:0.020, val_acc:0.992]
Epoch [57/120    avg_loss:0.028, val_acc:0.986]
Epoch [58/120    avg_loss:0.130, val_acc:0.958]
Epoch [59/120    avg_loss:0.058, val_acc:0.978]
Epoch [60/120    avg_loss:0.051, val_acc:0.980]
Epoch [61/120    avg_loss:0.022, val_acc:0.992]
Epoch [62/120    avg_loss:0.051, val_acc:0.988]
Epoch [63/120    avg_loss:0.042, val_acc:0.980]
Epoch [64/120    avg_loss:0.039, val_acc:0.982]
Epoch [65/120    avg_loss:0.016, val_acc:0.990]
Epoch [66/120    avg_loss:0.036, val_acc:0.990]
Epoch [67/120    avg_loss:0.032, val_acc:0.976]
Epoch [68/120    avg_loss:0.045, val_acc:0.992]
Epoch [69/120    avg_loss:0.026, val_acc:0.986]
Epoch [70/120    avg_loss:0.018, val_acc:0.988]
Epoch [71/120    avg_loss:0.027, val_acc:0.992]
Epoch [72/120    avg_loss:0.082, val_acc:0.978]
Epoch [73/120    avg_loss:0.056, val_acc:0.986]
Epoch [74/120    avg_loss:0.024, val_acc:0.984]
Epoch [75/120    avg_loss:0.018, val_acc:0.990]
Epoch [76/120    avg_loss:0.018, val_acc:0.992]
Epoch [77/120    avg_loss:0.015, val_acc:0.994]
Epoch [78/120    avg_loss:0.016, val_acc:0.990]
Epoch [79/120    avg_loss:0.011, val_acc:0.992]
Epoch [80/120    avg_loss:0.008, val_acc:0.990]
Epoch [81/120    avg_loss:0.012, val_acc:0.998]
Epoch [82/120    avg_loss:0.009, val_acc:0.994]
Epoch [83/120    avg_loss:0.007, val_acc:0.996]
Epoch [84/120    avg_loss:0.008, val_acc:0.998]
Epoch [85/120    avg_loss:0.005, val_acc:0.998]
Epoch [86/120    avg_loss:0.004, val_acc:0.998]
Epoch [87/120    avg_loss:0.025, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.992]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.023, val_acc:0.996]
Epoch [91/120    avg_loss:0.007, val_acc:0.994]
Epoch [92/120    avg_loss:0.009, val_acc:0.992]
Epoch [93/120    avg_loss:0.009, val_acc:0.992]
Epoch [94/120    avg_loss:0.013, val_acc:0.992]
Epoch [95/120    avg_loss:0.014, val_acc:0.994]
Epoch [96/120    avg_loss:0.035, val_acc:0.978]
Epoch [97/120    avg_loss:0.041, val_acc:0.982]
Epoch [98/120    avg_loss:0.027, val_acc:0.986]
Epoch [99/120    avg_loss:0.022, val_acc:0.984]
Epoch [100/120    avg_loss:0.025, val_acc:0.988]
Epoch [101/120    avg_loss:0.011, val_acc:0.992]
Epoch [102/120    avg_loss:0.027, val_acc:0.998]
Epoch [103/120    avg_loss:0.010, val_acc:0.998]
Epoch [104/120    avg_loss:0.009, val_acc:0.998]
Epoch [105/120    avg_loss:0.006, val_acc:0.998]
Epoch [106/120    avg_loss:0.006, val_acc:0.998]
Epoch [107/120    avg_loss:0.005, val_acc:0.998]
Epoch [108/120    avg_loss:0.005, val_acc:0.998]
Epoch [109/120    avg_loss:0.011, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.011, val_acc:0.994]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.003, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.008, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   9   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.99545455 1.         0.94945055 0.92682927
 1.         0.98924731 1.         1.         1.         0.98950131
 0.98888889 1.        ]

Kappa:
0.9921662771822893
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdef1794828>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.056, val_acc:0.579]
Epoch [2/120    avg_loss:1.222, val_acc:0.651]
Epoch [3/120    avg_loss:0.979, val_acc:0.758]
Epoch [4/120    avg_loss:0.750, val_acc:0.827]
Epoch [5/120    avg_loss:0.595, val_acc:0.873]
Epoch [6/120    avg_loss:0.716, val_acc:0.796]
Epoch [7/120    avg_loss:0.624, val_acc:0.774]
Epoch [8/120    avg_loss:0.559, val_acc:0.825]
Epoch [9/120    avg_loss:0.397, val_acc:0.895]
Epoch [10/120    avg_loss:0.489, val_acc:0.889]
Epoch [11/120    avg_loss:0.363, val_acc:0.905]
Epoch [12/120    avg_loss:0.378, val_acc:0.909]
Epoch [13/120    avg_loss:0.309, val_acc:0.933]
Epoch [14/120    avg_loss:0.423, val_acc:0.911]
Epoch [15/120    avg_loss:0.328, val_acc:0.877]
Epoch [16/120    avg_loss:0.273, val_acc:0.921]
Epoch [17/120    avg_loss:0.237, val_acc:0.956]
Epoch [18/120    avg_loss:0.228, val_acc:0.929]
Epoch [19/120    avg_loss:0.245, val_acc:0.929]
Epoch [20/120    avg_loss:0.205, val_acc:0.948]
Epoch [21/120    avg_loss:0.214, val_acc:0.940]
Epoch [22/120    avg_loss:0.241, val_acc:0.954]
Epoch [23/120    avg_loss:0.191, val_acc:0.952]
Epoch [24/120    avg_loss:0.174, val_acc:0.944]
Epoch [25/120    avg_loss:0.162, val_acc:0.962]
Epoch [26/120    avg_loss:0.135, val_acc:0.946]
Epoch [27/120    avg_loss:0.241, val_acc:0.954]
Epoch [28/120    avg_loss:0.198, val_acc:0.950]
Epoch [29/120    avg_loss:0.144, val_acc:0.964]
Epoch [30/120    avg_loss:0.092, val_acc:0.954]
Epoch [31/120    avg_loss:0.088, val_acc:0.940]
Epoch [32/120    avg_loss:0.104, val_acc:0.952]
Epoch [33/120    avg_loss:0.107, val_acc:0.938]
Epoch [34/120    avg_loss:0.079, val_acc:0.970]
Epoch [35/120    avg_loss:0.110, val_acc:0.964]
Epoch [36/120    avg_loss:0.100, val_acc:0.970]
Epoch [37/120    avg_loss:0.119, val_acc:0.954]
Epoch [38/120    avg_loss:0.124, val_acc:0.968]
Epoch [39/120    avg_loss:0.119, val_acc:0.935]
Epoch [40/120    avg_loss:0.225, val_acc:0.913]
Epoch [41/120    avg_loss:0.145, val_acc:0.956]
Epoch [42/120    avg_loss:0.106, val_acc:0.960]
Epoch [43/120    avg_loss:0.098, val_acc:0.948]
Epoch [44/120    avg_loss:0.114, val_acc:0.962]
Epoch [45/120    avg_loss:0.128, val_acc:0.966]
Epoch [46/120    avg_loss:0.065, val_acc:0.970]
Epoch [47/120    avg_loss:0.090, val_acc:0.960]
Epoch [48/120    avg_loss:0.090, val_acc:0.964]
Epoch [49/120    avg_loss:0.091, val_acc:0.968]
Epoch [50/120    avg_loss:0.072, val_acc:0.976]
Epoch [51/120    avg_loss:0.039, val_acc:0.980]
Epoch [52/120    avg_loss:0.061, val_acc:0.966]
Epoch [53/120    avg_loss:0.061, val_acc:0.972]
Epoch [54/120    avg_loss:0.059, val_acc:0.962]
Epoch [55/120    avg_loss:0.064, val_acc:0.970]
Epoch [56/120    avg_loss:0.065, val_acc:0.970]
Epoch [57/120    avg_loss:0.049, val_acc:0.976]
Epoch [58/120    avg_loss:0.050, val_acc:0.978]
Epoch [59/120    avg_loss:0.031, val_acc:0.978]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.018, val_acc:0.982]
Epoch [62/120    avg_loss:0.023, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.982]
Epoch [64/120    avg_loss:0.015, val_acc:0.976]
Epoch [65/120    avg_loss:0.020, val_acc:0.978]
Epoch [66/120    avg_loss:0.024, val_acc:0.974]
Epoch [67/120    avg_loss:0.038, val_acc:0.978]
Epoch [68/120    avg_loss:0.094, val_acc:0.946]
Epoch [69/120    avg_loss:0.054, val_acc:0.966]
Epoch [70/120    avg_loss:0.040, val_acc:0.976]
Epoch [71/120    avg_loss:0.033, val_acc:0.980]
Epoch [72/120    avg_loss:0.024, val_acc:0.986]
Epoch [73/120    avg_loss:0.013, val_acc:0.986]
Epoch [74/120    avg_loss:0.089, val_acc:0.980]
Epoch [75/120    avg_loss:0.084, val_acc:0.960]
Epoch [76/120    avg_loss:0.058, val_acc:0.948]
Epoch [77/120    avg_loss:0.099, val_acc:0.970]
Epoch [78/120    avg_loss:0.145, val_acc:0.970]
Epoch [79/120    avg_loss:0.079, val_acc:0.976]
Epoch [80/120    avg_loss:0.056, val_acc:0.978]
Epoch [81/120    avg_loss:0.037, val_acc:0.978]
Epoch [82/120    avg_loss:0.054, val_acc:0.978]
Epoch [83/120    avg_loss:0.021, val_acc:0.984]
Epoch [84/120    avg_loss:0.037, val_acc:0.980]
Epoch [85/120    avg_loss:0.022, val_acc:0.980]
Epoch [86/120    avg_loss:0.008, val_acc:0.974]
Epoch [87/120    avg_loss:0.012, val_acc:0.982]
Epoch [88/120    avg_loss:0.015, val_acc:0.980]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.014, val_acc:0.984]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.021, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.012, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.017, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  12   0   0   0   0   0   0   4   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.9977221  0.99563319 0.95259594 0.94983278
 1.         0.99465241 1.         1.         1.         0.98691099
 0.98444444 1.        ]

Kappa:
0.9924039760690829
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc0378e1748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.911, val_acc:0.605]
Epoch [2/120    avg_loss:1.108, val_acc:0.702]
Epoch [3/120    avg_loss:1.101, val_acc:0.744]
Epoch [4/120    avg_loss:0.731, val_acc:0.798]
Epoch [5/120    avg_loss:0.635, val_acc:0.853]
Epoch [6/120    avg_loss:0.597, val_acc:0.794]
Epoch [7/120    avg_loss:0.579, val_acc:0.869]
Epoch [8/120    avg_loss:0.609, val_acc:0.853]
Epoch [9/120    avg_loss:0.539, val_acc:0.903]
Epoch [10/120    avg_loss:0.449, val_acc:0.919]
Epoch [11/120    avg_loss:0.457, val_acc:0.913]
Epoch [12/120    avg_loss:0.415, val_acc:0.931]
Epoch [13/120    avg_loss:0.311, val_acc:0.913]
Epoch [14/120    avg_loss:0.253, val_acc:0.933]
Epoch [15/120    avg_loss:0.308, val_acc:0.946]
Epoch [16/120    avg_loss:0.285, val_acc:0.921]
Epoch [17/120    avg_loss:0.306, val_acc:0.931]
Epoch [18/120    avg_loss:0.303, val_acc:0.954]
Epoch [19/120    avg_loss:0.248, val_acc:0.933]
Epoch [20/120    avg_loss:0.365, val_acc:0.950]
Epoch [21/120    avg_loss:0.234, val_acc:0.907]
Epoch [22/120    avg_loss:0.314, val_acc:0.925]
Epoch [23/120    avg_loss:0.299, val_acc:0.919]
Epoch [24/120    avg_loss:0.264, val_acc:0.968]
Epoch [25/120    avg_loss:0.184, val_acc:0.954]
Epoch [26/120    avg_loss:0.142, val_acc:0.970]
Epoch [27/120    avg_loss:0.198, val_acc:0.958]
Epoch [28/120    avg_loss:0.339, val_acc:0.942]
Epoch [29/120    avg_loss:0.198, val_acc:0.970]
Epoch [30/120    avg_loss:0.129, val_acc:0.970]
Epoch [31/120    avg_loss:0.099, val_acc:0.978]
Epoch [32/120    avg_loss:0.141, val_acc:0.964]
Epoch [33/120    avg_loss:0.099, val_acc:0.980]
Epoch [34/120    avg_loss:0.114, val_acc:0.956]
Epoch [35/120    avg_loss:0.138, val_acc:0.976]
Epoch [36/120    avg_loss:0.136, val_acc:0.956]
Epoch [37/120    avg_loss:0.160, val_acc:0.980]
Epoch [38/120    avg_loss:0.108, val_acc:0.938]
Epoch [39/120    avg_loss:0.107, val_acc:0.974]
Epoch [40/120    avg_loss:0.085, val_acc:0.976]
Epoch [41/120    avg_loss:0.079, val_acc:0.968]
Epoch [42/120    avg_loss:0.122, val_acc:0.964]
Epoch [43/120    avg_loss:0.098, val_acc:0.966]
Epoch [44/120    avg_loss:0.094, val_acc:0.966]
Epoch [45/120    avg_loss:0.140, val_acc:0.976]
Epoch [46/120    avg_loss:0.106, val_acc:0.974]
Epoch [47/120    avg_loss:0.068, val_acc:0.978]
Epoch [48/120    avg_loss:0.084, val_acc:0.968]
Epoch [49/120    avg_loss:0.091, val_acc:0.982]
Epoch [50/120    avg_loss:0.102, val_acc:0.968]
Epoch [51/120    avg_loss:0.109, val_acc:0.978]
Epoch [52/120    avg_loss:0.051, val_acc:0.982]
Epoch [53/120    avg_loss:0.050, val_acc:0.978]
Epoch [54/120    avg_loss:0.043, val_acc:0.986]
Epoch [55/120    avg_loss:0.026, val_acc:0.988]
Epoch [56/120    avg_loss:0.027, val_acc:0.988]
Epoch [57/120    avg_loss:0.050, val_acc:0.988]
Epoch [58/120    avg_loss:0.040, val_acc:0.970]
Epoch [59/120    avg_loss:0.052, val_acc:0.970]
Epoch [60/120    avg_loss:0.063, val_acc:0.982]
Epoch [61/120    avg_loss:0.021, val_acc:0.982]
Epoch [62/120    avg_loss:0.025, val_acc:0.988]
Epoch [63/120    avg_loss:0.047, val_acc:0.968]
Epoch [64/120    avg_loss:0.026, val_acc:0.988]
Epoch [65/120    avg_loss:0.029, val_acc:0.990]
Epoch [66/120    avg_loss:0.052, val_acc:0.982]
Epoch [67/120    avg_loss:0.078, val_acc:0.970]
Epoch [68/120    avg_loss:0.032, val_acc:0.990]
Epoch [69/120    avg_loss:0.013, val_acc:0.988]
Epoch [70/120    avg_loss:0.012, val_acc:0.992]
Epoch [71/120    avg_loss:0.012, val_acc:0.996]
Epoch [72/120    avg_loss:0.024, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.986]
Epoch [74/120    avg_loss:0.023, val_acc:0.986]
Epoch [75/120    avg_loss:0.012, val_acc:0.990]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.040, val_acc:0.984]
Epoch [78/120    avg_loss:0.053, val_acc:0.984]
Epoch [79/120    avg_loss:0.023, val_acc:0.994]
Epoch [80/120    avg_loss:0.022, val_acc:0.990]
Epoch [81/120    avg_loss:0.015, val_acc:0.988]
Epoch [82/120    avg_loss:0.068, val_acc:0.982]
Epoch [83/120    avg_loss:0.039, val_acc:0.964]
Epoch [84/120    avg_loss:0.062, val_acc:0.990]
Epoch [85/120    avg_loss:0.025, val_acc:0.992]
Epoch [86/120    avg_loss:0.019, val_acc:0.992]
Epoch [87/120    avg_loss:0.023, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.022, val_acc:0.988]
Epoch [91/120    avg_loss:0.031, val_acc:0.990]
Epoch [92/120    avg_loss:0.019, val_acc:0.990]
Epoch [93/120    avg_loss:0.014, val_acc:0.990]
Epoch [94/120    avg_loss:0.022, val_acc:0.990]
Epoch [95/120    avg_loss:0.028, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.009, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.990]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.008, val_acc:0.990]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.014, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.016, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.018, val_acc:0.990]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 210  12   0   0   0   1   6   0   0   0   0]
 [  0   0   0   0 211  13   0   0   0   0   0   0   3   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.9977221  0.95454545 0.91144708 0.91034483
 1.         1.         0.998713   0.99363057 1.         0.9973545
 0.99448732 1.        ]

Kappa:
0.9878922803226396
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fddc96bb7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.960, val_acc:0.567]
Epoch [2/120    avg_loss:1.153, val_acc:0.702]
Epoch [3/120    avg_loss:0.913, val_acc:0.817]
Epoch [4/120    avg_loss:0.686, val_acc:0.810]
Epoch [5/120    avg_loss:0.574, val_acc:0.825]
Epoch [6/120    avg_loss:0.550, val_acc:0.845]
Epoch [7/120    avg_loss:0.639, val_acc:0.879]
Epoch [8/120    avg_loss:0.522, val_acc:0.831]
Epoch [9/120    avg_loss:0.511, val_acc:0.879]
Epoch [10/120    avg_loss:0.458, val_acc:0.873]
Epoch [11/120    avg_loss:0.408, val_acc:0.895]
Epoch [12/120    avg_loss:0.386, val_acc:0.881]
Epoch [13/120    avg_loss:0.334, val_acc:0.929]
Epoch [14/120    avg_loss:0.255, val_acc:0.917]
Epoch [15/120    avg_loss:0.278, val_acc:0.935]
Epoch [16/120    avg_loss:0.254, val_acc:0.946]
Epoch [17/120    avg_loss:0.249, val_acc:0.960]
Epoch [18/120    avg_loss:0.249, val_acc:0.938]
Epoch [19/120    avg_loss:0.250, val_acc:0.907]
Epoch [20/120    avg_loss:0.364, val_acc:0.931]
Epoch [21/120    avg_loss:0.201, val_acc:0.935]
Epoch [22/120    avg_loss:0.167, val_acc:0.954]
Epoch [23/120    avg_loss:0.264, val_acc:0.921]
Epoch [24/120    avg_loss:0.232, val_acc:0.929]
Epoch [25/120    avg_loss:0.232, val_acc:0.927]
Epoch [26/120    avg_loss:0.199, val_acc:0.972]
Epoch [27/120    avg_loss:0.156, val_acc:0.921]
Epoch [28/120    avg_loss:0.213, val_acc:0.960]
Epoch [29/120    avg_loss:0.182, val_acc:0.952]
Epoch [30/120    avg_loss:0.158, val_acc:0.958]
Epoch [31/120    avg_loss:0.264, val_acc:0.942]
Epoch [32/120    avg_loss:0.226, val_acc:0.935]
Epoch [33/120    avg_loss:0.179, val_acc:0.962]
Epoch [34/120    avg_loss:0.107, val_acc:0.972]
Epoch [35/120    avg_loss:0.070, val_acc:0.978]
Epoch [36/120    avg_loss:0.106, val_acc:0.978]
Epoch [37/120    avg_loss:0.126, val_acc:0.970]
Epoch [38/120    avg_loss:0.131, val_acc:0.960]
Epoch [39/120    avg_loss:0.138, val_acc:0.972]
Epoch [40/120    avg_loss:0.184, val_acc:0.972]
Epoch [41/120    avg_loss:0.101, val_acc:0.974]
Epoch [42/120    avg_loss:0.056, val_acc:0.980]
Epoch [43/120    avg_loss:0.071, val_acc:0.974]
Epoch [44/120    avg_loss:0.078, val_acc:0.968]
Epoch [45/120    avg_loss:0.075, val_acc:0.974]
Epoch [46/120    avg_loss:0.072, val_acc:0.982]
Epoch [47/120    avg_loss:0.077, val_acc:0.976]
Epoch [48/120    avg_loss:0.122, val_acc:0.992]
Epoch [49/120    avg_loss:0.057, val_acc:0.982]
Epoch [50/120    avg_loss:0.088, val_acc:0.978]
Epoch [51/120    avg_loss:0.058, val_acc:0.986]
Epoch [52/120    avg_loss:0.082, val_acc:0.962]
Epoch [53/120    avg_loss:0.078, val_acc:0.976]
Epoch [54/120    avg_loss:0.073, val_acc:0.988]
Epoch [55/120    avg_loss:0.046, val_acc:0.980]
Epoch [56/120    avg_loss:0.051, val_acc:0.994]
Epoch [57/120    avg_loss:0.079, val_acc:0.980]
Epoch [58/120    avg_loss:0.112, val_acc:0.990]
Epoch [59/120    avg_loss:0.088, val_acc:0.968]
Epoch [60/120    avg_loss:0.071, val_acc:0.978]
Epoch [61/120    avg_loss:0.098, val_acc:0.956]
Epoch [62/120    avg_loss:0.150, val_acc:0.935]
Epoch [63/120    avg_loss:0.142, val_acc:0.972]
Epoch [64/120    avg_loss:0.100, val_acc:0.982]
Epoch [65/120    avg_loss:0.046, val_acc:0.986]
Epoch [66/120    avg_loss:0.029, val_acc:0.994]
Epoch [67/120    avg_loss:0.025, val_acc:0.988]
Epoch [68/120    avg_loss:0.030, val_acc:0.996]
Epoch [69/120    avg_loss:0.021, val_acc:0.992]
Epoch [70/120    avg_loss:0.017, val_acc:0.998]
Epoch [71/120    avg_loss:0.033, val_acc:0.994]
Epoch [72/120    avg_loss:0.050, val_acc:0.978]
Epoch [73/120    avg_loss:0.051, val_acc:0.986]
Epoch [74/120    avg_loss:0.034, val_acc:0.976]
Epoch [75/120    avg_loss:0.034, val_acc:0.994]
Epoch [76/120    avg_loss:0.020, val_acc:0.998]
Epoch [77/120    avg_loss:0.047, val_acc:0.990]
Epoch [78/120    avg_loss:0.023, val_acc:0.992]
Epoch [79/120    avg_loss:0.018, val_acc:0.990]
Epoch [80/120    avg_loss:0.009, val_acc:0.992]
Epoch [81/120    avg_loss:0.022, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.032, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.996]
Epoch [85/120    avg_loss:0.019, val_acc:0.996]
Epoch [86/120    avg_loss:0.012, val_acc:0.996]
Epoch [87/120    avg_loss:0.018, val_acc:0.996]
Epoch [88/120    avg_loss:0.010, val_acc:0.996]
Epoch [89/120    avg_loss:0.018, val_acc:0.994]
Epoch [90/120    avg_loss:0.014, val_acc:0.996]
Epoch [91/120    avg_loss:0.008, val_acc:0.996]
Epoch [92/120    avg_loss:0.007, val_acc:0.996]
Epoch [93/120    avg_loss:0.008, val_acc:0.996]
Epoch [94/120    avg_loss:0.007, val_acc:0.996]
Epoch [95/120    avg_loss:0.013, val_acc:0.996]
Epoch [96/120    avg_loss:0.022, val_acc:0.996]
Epoch [97/120    avg_loss:0.009, val_acc:0.992]
Epoch [98/120    avg_loss:0.020, val_acc:0.994]
Epoch [99/120    avg_loss:0.019, val_acc:0.994]
Epoch [100/120    avg_loss:0.005, val_acc:0.996]
Epoch [101/120    avg_loss:0.005, val_acc:0.996]
Epoch [102/120    avg_loss:0.011, val_acc:0.996]
Epoch [103/120    avg_loss:0.005, val_acc:0.996]
Epoch [104/120    avg_loss:0.007, val_acc:0.996]
Epoch [105/120    avg_loss:0.012, val_acc:0.996]
Epoch [106/120    avg_loss:0.008, val_acc:0.996]
Epoch [107/120    avg_loss:0.006, val_acc:0.996]
Epoch [108/120    avg_loss:0.013, val_acc:0.996]
Epoch [109/120    avg_loss:0.006, val_acc:0.996]
Epoch [110/120    avg_loss:0.007, val_acc:0.996]
Epoch [111/120    avg_loss:0.013, val_acc:0.996]
Epoch [112/120    avg_loss:0.006, val_acc:0.996]
Epoch [113/120    avg_loss:0.007, val_acc:0.996]
Epoch [114/120    avg_loss:0.014, val_acc:0.996]
Epoch [115/120    avg_loss:0.007, val_acc:0.996]
Epoch [116/120    avg_loss:0.016, val_acc:0.996]
Epoch [117/120    avg_loss:0.007, val_acc:0.996]
Epoch [118/120    avg_loss:0.006, val_acc:0.996]
Epoch [119/120    avg_loss:0.007, val_acc:0.996]
Epoch [120/120    avg_loss:0.005, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         1.         0.99782135 0.94967177 0.91986063
 1.         1.         0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9943027285860594
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15af9a5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.048, val_acc:0.591]
Epoch [2/120    avg_loss:1.217, val_acc:0.649]
Epoch [3/120    avg_loss:0.957, val_acc:0.740]
Epoch [4/120    avg_loss:0.775, val_acc:0.754]
Epoch [5/120    avg_loss:0.701, val_acc:0.812]
Epoch [6/120    avg_loss:0.680, val_acc:0.772]
Epoch [7/120    avg_loss:0.606, val_acc:0.827]
Epoch [8/120    avg_loss:0.491, val_acc:0.855]
Epoch [9/120    avg_loss:0.513, val_acc:0.855]
Epoch [10/120    avg_loss:0.433, val_acc:0.831]
Epoch [11/120    avg_loss:0.533, val_acc:0.853]
Epoch [12/120    avg_loss:0.439, val_acc:0.879]
Epoch [13/120    avg_loss:0.429, val_acc:0.863]
Epoch [14/120    avg_loss:0.433, val_acc:0.859]
Epoch [15/120    avg_loss:0.290, val_acc:0.931]
Epoch [16/120    avg_loss:0.196, val_acc:0.929]
Epoch [17/120    avg_loss:0.211, val_acc:0.889]
Epoch [18/120    avg_loss:0.246, val_acc:0.931]
Epoch [19/120    avg_loss:0.241, val_acc:0.917]
Epoch [20/120    avg_loss:0.243, val_acc:0.893]
Epoch [21/120    avg_loss:0.199, val_acc:0.933]
Epoch [22/120    avg_loss:0.205, val_acc:0.942]
Epoch [23/120    avg_loss:0.197, val_acc:0.929]
Epoch [24/120    avg_loss:0.127, val_acc:0.944]
Epoch [25/120    avg_loss:0.310, val_acc:0.851]
Epoch [26/120    avg_loss:0.332, val_acc:0.935]
Epoch [27/120    avg_loss:0.170, val_acc:0.946]
Epoch [28/120    avg_loss:0.232, val_acc:0.869]
Epoch [29/120    avg_loss:0.165, val_acc:0.956]
Epoch [30/120    avg_loss:0.187, val_acc:0.938]
Epoch [31/120    avg_loss:0.250, val_acc:0.925]
Epoch [32/120    avg_loss:0.147, val_acc:0.946]
Epoch [33/120    avg_loss:0.173, val_acc:0.944]
Epoch [34/120    avg_loss:0.149, val_acc:0.944]
Epoch [35/120    avg_loss:0.077, val_acc:0.958]
Epoch [36/120    avg_loss:0.075, val_acc:0.962]
Epoch [37/120    avg_loss:0.109, val_acc:0.966]
Epoch [38/120    avg_loss:0.094, val_acc:0.986]
Epoch [39/120    avg_loss:0.055, val_acc:0.986]
Epoch [40/120    avg_loss:0.038, val_acc:0.972]
Epoch [41/120    avg_loss:0.076, val_acc:0.960]
Epoch [42/120    avg_loss:0.083, val_acc:0.988]
Epoch [43/120    avg_loss:0.090, val_acc:0.927]
Epoch [44/120    avg_loss:0.161, val_acc:0.940]
Epoch [45/120    avg_loss:0.073, val_acc:0.980]
Epoch [46/120    avg_loss:0.126, val_acc:0.984]
Epoch [47/120    avg_loss:0.066, val_acc:0.978]
Epoch [48/120    avg_loss:0.053, val_acc:0.976]
Epoch [49/120    avg_loss:0.096, val_acc:0.978]
Epoch [50/120    avg_loss:0.064, val_acc:0.964]
Epoch [51/120    avg_loss:0.049, val_acc:0.968]
Epoch [52/120    avg_loss:0.049, val_acc:0.972]
Epoch [53/120    avg_loss:0.053, val_acc:0.956]
Epoch [54/120    avg_loss:0.031, val_acc:0.986]
Epoch [55/120    avg_loss:0.021, val_acc:0.978]
Epoch [56/120    avg_loss:0.034, val_acc:0.982]
Epoch [57/120    avg_loss:0.020, val_acc:0.986]
Epoch [58/120    avg_loss:0.021, val_acc:0.988]
Epoch [59/120    avg_loss:0.011, val_acc:0.988]
Epoch [60/120    avg_loss:0.017, val_acc:0.988]
Epoch [61/120    avg_loss:0.018, val_acc:0.988]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.017, val_acc:0.986]
Epoch [64/120    avg_loss:0.012, val_acc:0.986]
Epoch [65/120    avg_loss:0.011, val_acc:0.988]
Epoch [66/120    avg_loss:0.011, val_acc:0.990]
Epoch [67/120    avg_loss:0.027, val_acc:0.990]
Epoch [68/120    avg_loss:0.013, val_acc:0.986]
Epoch [69/120    avg_loss:0.020, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.990]
Epoch [71/120    avg_loss:0.029, val_acc:0.988]
Epoch [72/120    avg_loss:0.014, val_acc:0.986]
Epoch [73/120    avg_loss:0.020, val_acc:0.986]
Epoch [74/120    avg_loss:0.010, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.990]
Epoch [79/120    avg_loss:0.018, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.027, val_acc:0.986]
Epoch [82/120    avg_loss:0.014, val_acc:0.990]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.012, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.009, val_acc:0.990]
Epoch [88/120    avg_loss:0.019, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.988]
Epoch [90/120    avg_loss:0.017, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.013, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.016, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.019, val_acc:0.986]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.018, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.013, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.014, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.017, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 226   2   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  12   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11   0 195   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.99545455 0.99122807 0.93569845 0.9602649
 0.97256858 0.99465241 1.         0.99893276 1.         0.9843342
 0.98218263 1.        ]

Kappa:
0.9895554353770402
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9622c66748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.120, val_acc:0.566]
Epoch [2/120    avg_loss:1.214, val_acc:0.738]
Epoch [3/120    avg_loss:0.959, val_acc:0.807]
Epoch [4/120    avg_loss:0.673, val_acc:0.867]
Epoch [5/120    avg_loss:0.680, val_acc:0.877]
Epoch [6/120    avg_loss:0.608, val_acc:0.840]
Epoch [7/120    avg_loss:0.578, val_acc:0.865]
Epoch [8/120    avg_loss:0.489, val_acc:0.879]
Epoch [9/120    avg_loss:0.549, val_acc:0.893]
Epoch [10/120    avg_loss:0.630, val_acc:0.855]
Epoch [11/120    avg_loss:0.558, val_acc:0.832]
Epoch [12/120    avg_loss:0.404, val_acc:0.877]
Epoch [13/120    avg_loss:0.357, val_acc:0.891]
Epoch [14/120    avg_loss:0.297, val_acc:0.918]
Epoch [15/120    avg_loss:0.259, val_acc:0.926]
Epoch [16/120    avg_loss:0.301, val_acc:0.932]
Epoch [17/120    avg_loss:0.331, val_acc:0.906]
Epoch [18/120    avg_loss:0.243, val_acc:0.941]
Epoch [19/120    avg_loss:0.223, val_acc:0.939]
Epoch [20/120    avg_loss:0.202, val_acc:0.900]
Epoch [21/120    avg_loss:0.283, val_acc:0.936]
Epoch [22/120    avg_loss:0.246, val_acc:0.957]
Epoch [23/120    avg_loss:0.196, val_acc:0.951]
Epoch [24/120    avg_loss:0.176, val_acc:0.914]
Epoch [25/120    avg_loss:0.267, val_acc:0.943]
Epoch [26/120    avg_loss:0.222, val_acc:0.943]
Epoch [27/120    avg_loss:0.232, val_acc:0.932]
Epoch [28/120    avg_loss:0.189, val_acc:0.951]
Epoch [29/120    avg_loss:0.109, val_acc:0.957]
Epoch [30/120    avg_loss:0.157, val_acc:0.924]
Epoch [31/120    avg_loss:0.122, val_acc:0.941]
Epoch [32/120    avg_loss:0.111, val_acc:0.945]
Epoch [33/120    avg_loss:0.139, val_acc:0.963]
Epoch [34/120    avg_loss:0.089, val_acc:0.969]
Epoch [35/120    avg_loss:0.128, val_acc:0.963]
Epoch [36/120    avg_loss:0.149, val_acc:0.947]
Epoch [37/120    avg_loss:0.127, val_acc:0.971]
Epoch [38/120    avg_loss:0.094, val_acc:0.980]
Epoch [39/120    avg_loss:0.141, val_acc:0.967]
Epoch [40/120    avg_loss:0.140, val_acc:0.965]
Epoch [41/120    avg_loss:0.079, val_acc:0.973]
Epoch [42/120    avg_loss:0.121, val_acc:0.979]
Epoch [43/120    avg_loss:0.075, val_acc:0.941]
Epoch [44/120    avg_loss:0.115, val_acc:0.969]
Epoch [45/120    avg_loss:0.071, val_acc:0.967]
Epoch [46/120    avg_loss:0.074, val_acc:0.967]
Epoch [47/120    avg_loss:0.088, val_acc:0.945]
Epoch [48/120    avg_loss:0.123, val_acc:0.961]
Epoch [49/120    avg_loss:0.086, val_acc:0.971]
Epoch [50/120    avg_loss:0.125, val_acc:0.961]
Epoch [51/120    avg_loss:0.055, val_acc:0.969]
Epoch [52/120    avg_loss:0.036, val_acc:0.977]
Epoch [53/120    avg_loss:0.033, val_acc:0.979]
Epoch [54/120    avg_loss:0.030, val_acc:0.979]
Epoch [55/120    avg_loss:0.024, val_acc:0.979]
Epoch [56/120    avg_loss:0.023, val_acc:0.980]
Epoch [57/120    avg_loss:0.036, val_acc:0.980]
Epoch [58/120    avg_loss:0.026, val_acc:0.982]
Epoch [59/120    avg_loss:0.037, val_acc:0.982]
Epoch [60/120    avg_loss:0.024, val_acc:0.984]
Epoch [61/120    avg_loss:0.030, val_acc:0.984]
Epoch [62/120    avg_loss:0.021, val_acc:0.984]
Epoch [63/120    avg_loss:0.037, val_acc:0.980]
Epoch [64/120    avg_loss:0.021, val_acc:0.980]
Epoch [65/120    avg_loss:0.023, val_acc:0.982]
Epoch [66/120    avg_loss:0.045, val_acc:0.979]
Epoch [67/120    avg_loss:0.036, val_acc:0.984]
Epoch [68/120    avg_loss:0.035, val_acc:0.979]
Epoch [69/120    avg_loss:0.021, val_acc:0.980]
Epoch [70/120    avg_loss:0.020, val_acc:0.984]
Epoch [71/120    avg_loss:0.019, val_acc:0.982]
Epoch [72/120    avg_loss:0.026, val_acc:0.979]
Epoch [73/120    avg_loss:0.017, val_acc:0.982]
Epoch [74/120    avg_loss:0.016, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.032, val_acc:0.980]
Epoch [77/120    avg_loss:0.022, val_acc:0.979]
Epoch [78/120    avg_loss:0.026, val_acc:0.979]
Epoch [79/120    avg_loss:0.022, val_acc:0.980]
Epoch [80/120    avg_loss:0.032, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.988]
Epoch [82/120    avg_loss:0.018, val_acc:0.984]
Epoch [83/120    avg_loss:0.029, val_acc:0.982]
Epoch [84/120    avg_loss:0.013, val_acc:0.984]
Epoch [85/120    avg_loss:0.046, val_acc:0.988]
Epoch [86/120    avg_loss:0.024, val_acc:0.984]
Epoch [87/120    avg_loss:0.033, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.984]
Epoch [89/120    avg_loss:0.019, val_acc:0.984]
Epoch [90/120    avg_loss:0.020, val_acc:0.988]
Epoch [91/120    avg_loss:0.016, val_acc:0.988]
Epoch [92/120    avg_loss:0.018, val_acc:0.984]
Epoch [93/120    avg_loss:0.019, val_acc:0.984]
Epoch [94/120    avg_loss:0.030, val_acc:0.984]
Epoch [95/120    avg_loss:0.026, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.984]
Epoch [99/120    avg_loss:0.022, val_acc:0.984]
Epoch [100/120    avg_loss:0.019, val_acc:0.982]
Epoch [101/120    avg_loss:0.016, val_acc:0.982]
Epoch [102/120    avg_loss:0.027, val_acc:0.984]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.025, val_acc:0.982]
Epoch [105/120    avg_loss:0.018, val_acc:0.982]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.982]
Epoch [110/120    avg_loss:0.012, val_acc:0.982]
Epoch [111/120    avg_loss:0.017, val_acc:0.982]
Epoch [112/120    avg_loss:0.014, val_acc:0.982]
Epoch [113/120    avg_loss:0.013, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.982]
Epoch [115/120    avg_loss:0.012, val_acc:0.982]
Epoch [116/120    avg_loss:0.018, val_acc:0.982]
Epoch [117/120    avg_loss:0.014, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.016, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213   9   0   0   0   0   0   0   5   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10   0 196   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.9977221  0.99782135 0.92008639 0.92682927
 0.97512438 0.99465241 1.         1.         1.         0.98305085
 0.97995546 1.        ]

Kappa:
0.9878931046019768
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89b3daa710>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:1.938, val_acc:0.560]
Epoch [2/120    avg_loss:1.167, val_acc:0.704]
Epoch [3/120    avg_loss:0.919, val_acc:0.800]
Epoch [4/120    avg_loss:0.944, val_acc:0.756]
Epoch [5/120    avg_loss:0.800, val_acc:0.869]
Epoch [6/120    avg_loss:0.625, val_acc:0.903]
Epoch [7/120    avg_loss:0.463, val_acc:0.869]
Epoch [8/120    avg_loss:0.573, val_acc:0.909]
Epoch [9/120    avg_loss:0.495, val_acc:0.877]
Epoch [10/120    avg_loss:0.406, val_acc:0.889]
Epoch [11/120    avg_loss:0.490, val_acc:0.931]
Epoch [12/120    avg_loss:0.335, val_acc:0.863]
Epoch [13/120    avg_loss:0.397, val_acc:0.913]
Epoch [14/120    avg_loss:0.436, val_acc:0.944]
Epoch [15/120    avg_loss:0.353, val_acc:0.913]
Epoch [16/120    avg_loss:0.367, val_acc:0.933]
Epoch [17/120    avg_loss:0.360, val_acc:0.919]
Epoch [18/120    avg_loss:0.330, val_acc:0.921]
Epoch [19/120    avg_loss:0.214, val_acc:0.929]
Epoch [20/120    avg_loss:0.200, val_acc:0.956]
Epoch [21/120    avg_loss:0.270, val_acc:0.944]
Epoch [22/120    avg_loss:0.298, val_acc:0.929]
Epoch [23/120    avg_loss:0.196, val_acc:0.938]
Epoch [24/120    avg_loss:0.180, val_acc:0.915]
Epoch [25/120    avg_loss:0.269, val_acc:0.931]
Epoch [26/120    avg_loss:0.208, val_acc:0.950]
Epoch [27/120    avg_loss:0.184, val_acc:0.966]
Epoch [28/120    avg_loss:0.236, val_acc:0.958]
Epoch [29/120    avg_loss:0.137, val_acc:0.976]
Epoch [30/120    avg_loss:0.122, val_acc:0.954]
Epoch [31/120    avg_loss:0.163, val_acc:0.964]
Epoch [32/120    avg_loss:0.143, val_acc:0.940]
Epoch [33/120    avg_loss:0.168, val_acc:0.970]
Epoch [34/120    avg_loss:0.136, val_acc:0.962]
Epoch [35/120    avg_loss:0.083, val_acc:0.938]
Epoch [36/120    avg_loss:0.133, val_acc:0.978]
Epoch [37/120    avg_loss:0.107, val_acc:0.960]
Epoch [38/120    avg_loss:0.104, val_acc:0.935]
Epoch [39/120    avg_loss:0.097, val_acc:0.986]
Epoch [40/120    avg_loss:0.081, val_acc:0.976]
Epoch [41/120    avg_loss:0.074, val_acc:0.976]
Epoch [42/120    avg_loss:0.071, val_acc:0.982]
Epoch [43/120    avg_loss:0.062, val_acc:0.968]
Epoch [44/120    avg_loss:0.089, val_acc:0.974]
Epoch [45/120    avg_loss:0.117, val_acc:0.972]
Epoch [46/120    avg_loss:0.074, val_acc:0.976]
Epoch [47/120    avg_loss:0.076, val_acc:0.970]
Epoch [48/120    avg_loss:0.080, val_acc:0.903]
Epoch [49/120    avg_loss:0.122, val_acc:0.970]
Epoch [50/120    avg_loss:0.099, val_acc:0.982]
Epoch [51/120    avg_loss:0.136, val_acc:0.954]
Epoch [52/120    avg_loss:0.143, val_acc:0.978]
Epoch [53/120    avg_loss:0.053, val_acc:0.978]
Epoch [54/120    avg_loss:0.065, val_acc:0.982]
Epoch [55/120    avg_loss:0.044, val_acc:0.982]
Epoch [56/120    avg_loss:0.039, val_acc:0.982]
Epoch [57/120    avg_loss:0.028, val_acc:0.980]
Epoch [58/120    avg_loss:0.036, val_acc:0.980]
Epoch [59/120    avg_loss:0.047, val_acc:0.982]
Epoch [60/120    avg_loss:0.028, val_acc:0.984]
Epoch [61/120    avg_loss:0.036, val_acc:0.982]
Epoch [62/120    avg_loss:0.026, val_acc:0.984]
Epoch [63/120    avg_loss:0.032, val_acc:0.982]
Epoch [64/120    avg_loss:0.029, val_acc:0.984]
Epoch [65/120    avg_loss:0.025, val_acc:0.984]
Epoch [66/120    avg_loss:0.031, val_acc:0.986]
Epoch [67/120    avg_loss:0.024, val_acc:0.986]
Epoch [68/120    avg_loss:0.031, val_acc:0.986]
Epoch [69/120    avg_loss:0.030, val_acc:0.986]
Epoch [70/120    avg_loss:0.029, val_acc:0.986]
Epoch [71/120    avg_loss:0.025, val_acc:0.986]
Epoch [72/120    avg_loss:0.030, val_acc:0.986]
Epoch [73/120    avg_loss:0.028, val_acc:0.986]
Epoch [74/120    avg_loss:0.039, val_acc:0.986]
Epoch [75/120    avg_loss:0.023, val_acc:0.986]
Epoch [76/120    avg_loss:0.022, val_acc:0.986]
Epoch [77/120    avg_loss:0.020, val_acc:0.986]
Epoch [78/120    avg_loss:0.018, val_acc:0.986]
Epoch [79/120    avg_loss:0.027, val_acc:0.986]
Epoch [80/120    avg_loss:0.031, val_acc:0.986]
Epoch [81/120    avg_loss:0.030, val_acc:0.986]
Epoch [82/120    avg_loss:0.022, val_acc:0.986]
Epoch [83/120    avg_loss:0.021, val_acc:0.986]
Epoch [84/120    avg_loss:0.026, val_acc:0.986]
Epoch [85/120    avg_loss:0.040, val_acc:0.986]
Epoch [86/120    avg_loss:0.029, val_acc:0.986]
Epoch [87/120    avg_loss:0.034, val_acc:0.986]
Epoch [88/120    avg_loss:0.032, val_acc:0.986]
Epoch [89/120    avg_loss:0.019, val_acc:0.986]
Epoch [90/120    avg_loss:0.022, val_acc:0.986]
Epoch [91/120    avg_loss:0.030, val_acc:0.986]
Epoch [92/120    avg_loss:0.057, val_acc:0.986]
Epoch [93/120    avg_loss:0.038, val_acc:0.986]
Epoch [94/120    avg_loss:0.041, val_acc:0.986]
Epoch [95/120    avg_loss:0.022, val_acc:0.986]
Epoch [96/120    avg_loss:0.019, val_acc:0.986]
Epoch [97/120    avg_loss:0.026, val_acc:0.986]
Epoch [98/120    avg_loss:0.030, val_acc:0.986]
Epoch [99/120    avg_loss:0.026, val_acc:0.986]
Epoch [100/120    avg_loss:0.023, val_acc:0.986]
Epoch [101/120    avg_loss:0.020, val_acc:0.986]
Epoch [102/120    avg_loss:0.036, val_acc:0.988]
Epoch [103/120    avg_loss:0.021, val_acc:0.988]
Epoch [104/120    avg_loss:0.022, val_acc:0.988]
Epoch [105/120    avg_loss:0.045, val_acc:0.988]
Epoch [106/120    avg_loss:0.022, val_acc:0.988]
Epoch [107/120    avg_loss:0.025, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.034, val_acc:0.988]
Epoch [110/120    avg_loss:0.024, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.029, val_acc:0.988]
Epoch [113/120    avg_loss:0.034, val_acc:0.988]
Epoch [114/120    avg_loss:0.026, val_acc:0.988]
Epoch [115/120    avg_loss:0.026, val_acc:0.988]
Epoch [116/120    avg_loss:0.036, val_acc:0.988]
Epoch [117/120    avg_loss:0.025, val_acc:0.988]
Epoch [118/120    avg_loss:0.036, val_acc:0.988]
Epoch [119/120    avg_loss:0.031, val_acc:0.988]
Epoch [120/120    avg_loss:0.032, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   5   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.99545455 0.98454746 0.94922737 0.93918919
 1.         0.98924731 0.99742931 1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9931159702151957
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15e6ef48d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.928, val_acc:0.565]
Epoch [2/120    avg_loss:1.188, val_acc:0.724]
Epoch [3/120    avg_loss:0.884, val_acc:0.823]
Epoch [4/120    avg_loss:0.691, val_acc:0.853]
Epoch [5/120    avg_loss:0.671, val_acc:0.861]
Epoch [6/120    avg_loss:0.592, val_acc:0.871]
Epoch [7/120    avg_loss:0.607, val_acc:0.881]
Epoch [8/120    avg_loss:0.470, val_acc:0.875]
Epoch [9/120    avg_loss:0.430, val_acc:0.863]
Epoch [10/120    avg_loss:0.468, val_acc:0.911]
Epoch [11/120    avg_loss:0.367, val_acc:0.911]
Epoch [12/120    avg_loss:0.418, val_acc:0.899]
Epoch [13/120    avg_loss:0.297, val_acc:0.938]
Epoch [14/120    avg_loss:0.337, val_acc:0.881]
Epoch [15/120    avg_loss:0.288, val_acc:0.899]
Epoch [16/120    avg_loss:0.325, val_acc:0.944]
Epoch [17/120    avg_loss:0.231, val_acc:0.935]
Epoch [18/120    avg_loss:0.358, val_acc:0.919]
Epoch [19/120    avg_loss:0.247, val_acc:0.925]
Epoch [20/120    avg_loss:0.360, val_acc:0.942]
Epoch [21/120    avg_loss:0.236, val_acc:0.919]
Epoch [22/120    avg_loss:0.177, val_acc:0.938]
Epoch [23/120    avg_loss:0.213, val_acc:0.948]
Epoch [24/120    avg_loss:0.235, val_acc:0.952]
Epoch [25/120    avg_loss:0.118, val_acc:0.972]
Epoch [26/120    avg_loss:0.172, val_acc:0.966]
Epoch [27/120    avg_loss:0.139, val_acc:0.935]
Epoch [28/120    avg_loss:0.168, val_acc:0.976]
Epoch [29/120    avg_loss:0.125, val_acc:0.962]
Epoch [30/120    avg_loss:0.107, val_acc:0.972]
Epoch [31/120    avg_loss:0.138, val_acc:0.966]
Epoch [32/120    avg_loss:0.108, val_acc:0.952]
Epoch [33/120    avg_loss:0.099, val_acc:0.968]
Epoch [34/120    avg_loss:0.069, val_acc:0.974]
Epoch [35/120    avg_loss:0.047, val_acc:0.980]
Epoch [36/120    avg_loss:0.061, val_acc:0.982]
Epoch [37/120    avg_loss:0.047, val_acc:0.988]
Epoch [38/120    avg_loss:0.062, val_acc:0.970]
Epoch [39/120    avg_loss:0.061, val_acc:0.984]
Epoch [40/120    avg_loss:0.100, val_acc:0.974]
Epoch [41/120    avg_loss:0.069, val_acc:0.968]
Epoch [42/120    avg_loss:0.067, val_acc:0.988]
Epoch [43/120    avg_loss:0.057, val_acc:0.984]
Epoch [44/120    avg_loss:0.035, val_acc:0.974]
Epoch [45/120    avg_loss:0.069, val_acc:0.980]
Epoch [46/120    avg_loss:0.052, val_acc:0.976]
Epoch [47/120    avg_loss:0.045, val_acc:0.980]
Epoch [48/120    avg_loss:0.071, val_acc:0.986]
Epoch [49/120    avg_loss:0.111, val_acc:0.966]
Epoch [50/120    avg_loss:0.066, val_acc:0.972]
Epoch [51/120    avg_loss:0.032, val_acc:0.982]
Epoch [52/120    avg_loss:0.052, val_acc:0.984]
Epoch [53/120    avg_loss:0.033, val_acc:0.988]
Epoch [54/120    avg_loss:0.025, val_acc:0.982]
Epoch [55/120    avg_loss:0.087, val_acc:0.976]
Epoch [56/120    avg_loss:0.048, val_acc:0.972]
Epoch [57/120    avg_loss:0.048, val_acc:0.980]
Epoch [58/120    avg_loss:0.026, val_acc:0.980]
Epoch [59/120    avg_loss:0.019, val_acc:0.988]
Epoch [60/120    avg_loss:0.013, val_acc:0.984]
Epoch [61/120    avg_loss:0.010, val_acc:0.988]
Epoch [62/120    avg_loss:0.027, val_acc:0.968]
Epoch [63/120    avg_loss:0.027, val_acc:0.980]
Epoch [64/120    avg_loss:0.042, val_acc:0.986]
Epoch [65/120    avg_loss:0.021, val_acc:0.982]
Epoch [66/120    avg_loss:0.009, val_acc:0.984]
Epoch [67/120    avg_loss:0.017, val_acc:0.986]
Epoch [68/120    avg_loss:0.012, val_acc:0.982]
Epoch [69/120    avg_loss:0.021, val_acc:0.994]
Epoch [70/120    avg_loss:0.016, val_acc:0.994]
Epoch [71/120    avg_loss:0.030, val_acc:0.990]
Epoch [72/120    avg_loss:0.049, val_acc:0.990]
Epoch [73/120    avg_loss:0.029, val_acc:0.982]
Epoch [74/120    avg_loss:0.013, val_acc:0.992]
Epoch [75/120    avg_loss:0.014, val_acc:0.994]
Epoch [76/120    avg_loss:0.016, val_acc:0.988]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.023, val_acc:0.982]
Epoch [79/120    avg_loss:0.008, val_acc:0.990]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.988]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.004, val_acc:0.994]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.004, val_acc:0.994]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.024, val_acc:0.982]
Epoch [91/120    avg_loss:0.039, val_acc:0.992]
Epoch [92/120    avg_loss:0.040, val_acc:0.980]
Epoch [93/120    avg_loss:0.044, val_acc:0.968]
Epoch [94/120    avg_loss:0.048, val_acc:0.982]
Epoch [95/120    avg_loss:0.023, val_acc:0.988]
Epoch [96/120    avg_loss:0.022, val_acc:0.984]
Epoch [97/120    avg_loss:0.019, val_acc:0.978]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.012, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.025, val_acc:0.986]
Epoch [103/120    avg_loss:0.015, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.018, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.003, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   1   0   0   0   0   0   0   2   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.99095023 0.99782135 0.96137339 0.94584838
 1.         0.97826087 1.         1.         1.         0.99080158
 0.9900111  1.        ]

Kappa:
0.9931154210468478
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f43e68897b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.907, val_acc:0.641]
Epoch [2/120    avg_loss:1.167, val_acc:0.669]
Epoch [3/120    avg_loss:0.966, val_acc:0.716]
Epoch [4/120    avg_loss:0.925, val_acc:0.760]
Epoch [5/120    avg_loss:0.787, val_acc:0.766]
Epoch [6/120    avg_loss:0.705, val_acc:0.837]
Epoch [7/120    avg_loss:0.730, val_acc:0.885]
Epoch [8/120    avg_loss:0.448, val_acc:0.889]
Epoch [9/120    avg_loss:0.434, val_acc:0.905]
Epoch [10/120    avg_loss:0.410, val_acc:0.875]
Epoch [11/120    avg_loss:0.547, val_acc:0.923]
Epoch [12/120    avg_loss:0.450, val_acc:0.919]
Epoch [13/120    avg_loss:0.341, val_acc:0.931]
Epoch [14/120    avg_loss:0.272, val_acc:0.927]
Epoch [15/120    avg_loss:0.297, val_acc:0.919]
Epoch [16/120    avg_loss:0.271, val_acc:0.956]
Epoch [17/120    avg_loss:0.209, val_acc:0.956]
Epoch [18/120    avg_loss:0.147, val_acc:0.962]
Epoch [19/120    avg_loss:0.242, val_acc:0.917]
Epoch [20/120    avg_loss:0.253, val_acc:0.948]
Epoch [21/120    avg_loss:0.345, val_acc:0.909]
Epoch [22/120    avg_loss:0.290, val_acc:0.903]
Epoch [23/120    avg_loss:0.231, val_acc:0.962]
Epoch [24/120    avg_loss:0.111, val_acc:0.972]
Epoch [25/120    avg_loss:0.180, val_acc:0.946]
Epoch [26/120    avg_loss:0.175, val_acc:0.935]
Epoch [27/120    avg_loss:0.239, val_acc:0.954]
Epoch [28/120    avg_loss:0.192, val_acc:0.938]
Epoch [29/120    avg_loss:0.234, val_acc:0.933]
Epoch [30/120    avg_loss:0.159, val_acc:0.978]
Epoch [31/120    avg_loss:0.100, val_acc:0.984]
Epoch [32/120    avg_loss:0.086, val_acc:0.966]
Epoch [33/120    avg_loss:0.095, val_acc:0.974]
Epoch [34/120    avg_loss:0.123, val_acc:0.976]
Epoch [35/120    avg_loss:0.102, val_acc:0.966]
Epoch [36/120    avg_loss:0.063, val_acc:0.982]
Epoch [37/120    avg_loss:0.063, val_acc:0.986]
Epoch [38/120    avg_loss:0.098, val_acc:0.964]
Epoch [39/120    avg_loss:0.152, val_acc:0.966]
Epoch [40/120    avg_loss:0.079, val_acc:0.984]
Epoch [41/120    avg_loss:0.061, val_acc:0.982]
Epoch [42/120    avg_loss:0.083, val_acc:0.986]
Epoch [43/120    avg_loss:0.171, val_acc:0.960]
Epoch [44/120    avg_loss:0.130, val_acc:0.984]
Epoch [45/120    avg_loss:0.070, val_acc:0.980]
Epoch [46/120    avg_loss:0.093, val_acc:0.964]
Epoch [47/120    avg_loss:0.132, val_acc:0.968]
Epoch [48/120    avg_loss:0.090, val_acc:0.978]
Epoch [49/120    avg_loss:0.123, val_acc:0.986]
Epoch [50/120    avg_loss:0.063, val_acc:0.988]
Epoch [51/120    avg_loss:0.048, val_acc:0.966]
Epoch [52/120    avg_loss:0.045, val_acc:0.988]
Epoch [53/120    avg_loss:0.026, val_acc:0.986]
Epoch [54/120    avg_loss:0.065, val_acc:0.980]
Epoch [55/120    avg_loss:0.061, val_acc:0.986]
Epoch [56/120    avg_loss:0.059, val_acc:0.968]
Epoch [57/120    avg_loss:0.115, val_acc:0.978]
Epoch [58/120    avg_loss:0.029, val_acc:0.988]
Epoch [59/120    avg_loss:0.045, val_acc:0.976]
Epoch [60/120    avg_loss:0.076, val_acc:0.976]
Epoch [61/120    avg_loss:0.084, val_acc:0.986]
Epoch [62/120    avg_loss:0.038, val_acc:0.988]
Epoch [63/120    avg_loss:0.037, val_acc:0.988]
Epoch [64/120    avg_loss:0.020, val_acc:0.988]
Epoch [65/120    avg_loss:0.022, val_acc:0.986]
Epoch [66/120    avg_loss:0.019, val_acc:0.992]
Epoch [67/120    avg_loss:0.061, val_acc:0.974]
Epoch [68/120    avg_loss:0.103, val_acc:0.984]
Epoch [69/120    avg_loss:0.037, val_acc:0.984]
Epoch [70/120    avg_loss:0.028, val_acc:0.988]
Epoch [71/120    avg_loss:0.016, val_acc:0.988]
Epoch [72/120    avg_loss:0.022, val_acc:0.988]
Epoch [73/120    avg_loss:0.011, val_acc:0.988]
Epoch [74/120    avg_loss:0.031, val_acc:0.986]
Epoch [75/120    avg_loss:0.056, val_acc:0.986]
Epoch [76/120    avg_loss:0.024, val_acc:0.986]
Epoch [77/120    avg_loss:0.023, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.992]
Epoch [79/120    avg_loss:0.012, val_acc:0.992]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.992]
Epoch [83/120    avg_loss:0.013, val_acc:0.990]
Epoch [84/120    avg_loss:0.015, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.008, val_acc:0.992]
Epoch [88/120    avg_loss:0.006, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.018, val_acc:0.986]
Epoch [93/120    avg_loss:0.018, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.015, val_acc:0.992]
Epoch [96/120    avg_loss:0.009, val_acc:0.994]
Epoch [97/120    avg_loss:0.011, val_acc:0.996]
Epoch [98/120    avg_loss:0.009, val_acc:0.992]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.019, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.011, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   6   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 223   2   0   0   0   0   0   0   2   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99926954 1.         0.98004435 0.94092827 0.92363636
 1.         1.         1.         0.99680511 1.         0.99472296
 0.99336283 1.        ]

Kappa:
0.9914535685906801
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50a09c6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.001, val_acc:0.650]
Epoch [2/120    avg_loss:1.105, val_acc:0.708]
Epoch [3/120    avg_loss:0.891, val_acc:0.815]
Epoch [4/120    avg_loss:0.743, val_acc:0.771]
Epoch [5/120    avg_loss:0.725, val_acc:0.856]
Epoch [6/120    avg_loss:0.640, val_acc:0.873]
Epoch [7/120    avg_loss:0.615, val_acc:0.827]
Epoch [8/120    avg_loss:0.581, val_acc:0.867]
Epoch [9/120    avg_loss:0.417, val_acc:0.921]
Epoch [10/120    avg_loss:0.406, val_acc:0.923]
Epoch [11/120    avg_loss:0.537, val_acc:0.904]
Epoch [12/120    avg_loss:0.465, val_acc:0.958]
Epoch [13/120    avg_loss:0.352, val_acc:0.896]
Epoch [14/120    avg_loss:0.405, val_acc:0.915]
Epoch [15/120    avg_loss:0.292, val_acc:0.946]
Epoch [16/120    avg_loss:0.220, val_acc:0.948]
Epoch [17/120    avg_loss:0.278, val_acc:0.948]
Epoch [18/120    avg_loss:0.219, val_acc:0.963]
Epoch [19/120    avg_loss:0.227, val_acc:0.950]
Epoch [20/120    avg_loss:0.227, val_acc:0.952]
Epoch [21/120    avg_loss:0.190, val_acc:0.954]
Epoch [22/120    avg_loss:0.179, val_acc:0.923]
Epoch [23/120    avg_loss:0.173, val_acc:0.940]
Epoch [24/120    avg_loss:0.105, val_acc:0.948]
Epoch [25/120    avg_loss:0.204, val_acc:0.942]
Epoch [26/120    avg_loss:0.225, val_acc:0.952]
Epoch [27/120    avg_loss:0.145, val_acc:0.944]
Epoch [28/120    avg_loss:0.182, val_acc:0.948]
Epoch [29/120    avg_loss:0.133, val_acc:0.929]
Epoch [30/120    avg_loss:0.240, val_acc:0.954]
Epoch [31/120    avg_loss:0.102, val_acc:0.958]
Epoch [32/120    avg_loss:0.091, val_acc:0.977]
Epoch [33/120    avg_loss:0.065, val_acc:0.979]
Epoch [34/120    avg_loss:0.064, val_acc:0.981]
Epoch [35/120    avg_loss:0.060, val_acc:0.979]
Epoch [36/120    avg_loss:0.043, val_acc:0.981]
Epoch [37/120    avg_loss:0.052, val_acc:0.983]
Epoch [38/120    avg_loss:0.045, val_acc:0.981]
Epoch [39/120    avg_loss:0.041, val_acc:0.983]
Epoch [40/120    avg_loss:0.059, val_acc:0.977]
Epoch [41/120    avg_loss:0.045, val_acc:0.981]
Epoch [42/120    avg_loss:0.055, val_acc:0.983]
Epoch [43/120    avg_loss:0.035, val_acc:0.981]
Epoch [44/120    avg_loss:0.059, val_acc:0.981]
Epoch [45/120    avg_loss:0.042, val_acc:0.983]
Epoch [46/120    avg_loss:0.036, val_acc:0.985]
Epoch [47/120    avg_loss:0.042, val_acc:0.979]
Epoch [48/120    avg_loss:0.044, val_acc:0.981]
Epoch [49/120    avg_loss:0.054, val_acc:0.981]
Epoch [50/120    avg_loss:0.069, val_acc:0.985]
Epoch [51/120    avg_loss:0.042, val_acc:0.988]
Epoch [52/120    avg_loss:0.027, val_acc:0.988]
Epoch [53/120    avg_loss:0.038, val_acc:0.988]
Epoch [54/120    avg_loss:0.035, val_acc:0.985]
Epoch [55/120    avg_loss:0.041, val_acc:0.985]
Epoch [56/120    avg_loss:0.035, val_acc:0.983]
Epoch [57/120    avg_loss:0.030, val_acc:0.983]
Epoch [58/120    avg_loss:0.042, val_acc:0.988]
Epoch [59/120    avg_loss:0.044, val_acc:0.988]
Epoch [60/120    avg_loss:0.043, val_acc:0.985]
Epoch [61/120    avg_loss:0.037, val_acc:0.983]
Epoch [62/120    avg_loss:0.031, val_acc:0.985]
Epoch [63/120    avg_loss:0.027, val_acc:0.985]
Epoch [64/120    avg_loss:0.044, val_acc:0.988]
Epoch [65/120    avg_loss:0.040, val_acc:0.985]
Epoch [66/120    avg_loss:0.023, val_acc:0.983]
Epoch [67/120    avg_loss:0.027, val_acc:0.979]
Epoch [68/120    avg_loss:0.029, val_acc:0.990]
Epoch [69/120    avg_loss:0.025, val_acc:0.990]
Epoch [70/120    avg_loss:0.029, val_acc:0.988]
Epoch [71/120    avg_loss:0.035, val_acc:0.981]
Epoch [72/120    avg_loss:0.033, val_acc:0.983]
Epoch [73/120    avg_loss:0.023, val_acc:0.985]
Epoch [74/120    avg_loss:0.035, val_acc:0.985]
Epoch [75/120    avg_loss:0.019, val_acc:0.988]
Epoch [76/120    avg_loss:0.030, val_acc:0.981]
Epoch [77/120    avg_loss:0.026, val_acc:0.985]
Epoch [78/120    avg_loss:0.027, val_acc:0.988]
Epoch [79/120    avg_loss:0.030, val_acc:0.988]
Epoch [80/120    avg_loss:0.029, val_acc:0.988]
Epoch [81/120    avg_loss:0.029, val_acc:0.988]
Epoch [82/120    avg_loss:0.021, val_acc:0.983]
Epoch [83/120    avg_loss:0.037, val_acc:0.983]
Epoch [84/120    avg_loss:0.031, val_acc:0.985]
Epoch [85/120    avg_loss:0.020, val_acc:0.985]
Epoch [86/120    avg_loss:0.030, val_acc:0.985]
Epoch [87/120    avg_loss:0.037, val_acc:0.985]
Epoch [88/120    avg_loss:0.027, val_acc:0.985]
Epoch [89/120    avg_loss:0.031, val_acc:0.985]
Epoch [90/120    avg_loss:0.023, val_acc:0.985]
Epoch [91/120    avg_loss:0.021, val_acc:0.985]
Epoch [92/120    avg_loss:0.022, val_acc:0.985]
Epoch [93/120    avg_loss:0.023, val_acc:0.985]
Epoch [94/120    avg_loss:0.029, val_acc:0.985]
Epoch [95/120    avg_loss:0.017, val_acc:0.985]
Epoch [96/120    avg_loss:0.022, val_acc:0.985]
Epoch [97/120    avg_loss:0.023, val_acc:0.985]
Epoch [98/120    avg_loss:0.030, val_acc:0.985]
Epoch [99/120    avg_loss:0.027, val_acc:0.985]
Epoch [100/120    avg_loss:0.022, val_acc:0.985]
Epoch [101/120    avg_loss:0.020, val_acc:0.985]
Epoch [102/120    avg_loss:0.030, val_acc:0.985]
Epoch [103/120    avg_loss:0.029, val_acc:0.985]
Epoch [104/120    avg_loss:0.025, val_acc:0.985]
Epoch [105/120    avg_loss:0.022, val_acc:0.985]
Epoch [106/120    avg_loss:0.033, val_acc:0.985]
Epoch [107/120    avg_loss:0.032, val_acc:0.985]
Epoch [108/120    avg_loss:0.031, val_acc:0.985]
Epoch [109/120    avg_loss:0.026, val_acc:0.985]
Epoch [110/120    avg_loss:0.027, val_acc:0.985]
Epoch [111/120    avg_loss:0.020, val_acc:0.985]
Epoch [112/120    avg_loss:0.020, val_acc:0.985]
Epoch [113/120    avg_loss:0.019, val_acc:0.985]
Epoch [114/120    avg_loss:0.023, val_acc:0.985]
Epoch [115/120    avg_loss:0.031, val_acc:0.985]
Epoch [116/120    avg_loss:0.036, val_acc:0.985]
Epoch [117/120    avg_loss:0.026, val_acc:0.985]
Epoch [118/120    avg_loss:0.025, val_acc:0.985]
Epoch [119/120    avg_loss:0.025, val_acc:0.985]
Epoch [120/120    avg_loss:0.024, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.98426966 1.         0.94577007 0.91489362
 1.         0.96132597 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9924031178600482
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f019c29e748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.892, val_acc:0.590]
Epoch [2/120    avg_loss:1.204, val_acc:0.683]
Epoch [3/120    avg_loss:0.966, val_acc:0.685]
Epoch [4/120    avg_loss:0.922, val_acc:0.800]
Epoch [5/120    avg_loss:0.777, val_acc:0.823]
Epoch [6/120    avg_loss:0.709, val_acc:0.821]
Epoch [7/120    avg_loss:0.678, val_acc:0.808]
Epoch [8/120    avg_loss:0.612, val_acc:0.860]
Epoch [9/120    avg_loss:0.397, val_acc:0.856]
Epoch [10/120    avg_loss:0.543, val_acc:0.858]
Epoch [11/120    avg_loss:0.430, val_acc:0.858]
Epoch [12/120    avg_loss:0.399, val_acc:0.902]
Epoch [13/120    avg_loss:0.375, val_acc:0.927]
Epoch [14/120    avg_loss:0.313, val_acc:0.896]
Epoch [15/120    avg_loss:0.302, val_acc:0.923]
Epoch [16/120    avg_loss:0.236, val_acc:0.927]
Epoch [17/120    avg_loss:0.217, val_acc:0.950]
Epoch [18/120    avg_loss:0.225, val_acc:0.917]
Epoch [19/120    avg_loss:0.297, val_acc:0.908]
Epoch [20/120    avg_loss:0.249, val_acc:0.944]
Epoch [21/120    avg_loss:0.139, val_acc:0.960]
Epoch [22/120    avg_loss:0.157, val_acc:0.931]
Epoch [23/120    avg_loss:0.219, val_acc:0.944]
Epoch [24/120    avg_loss:0.162, val_acc:0.956]
Epoch [25/120    avg_loss:0.158, val_acc:0.950]
Epoch [26/120    avg_loss:0.138, val_acc:0.967]
Epoch [27/120    avg_loss:0.189, val_acc:0.921]
Epoch [28/120    avg_loss:0.133, val_acc:0.960]
Epoch [29/120    avg_loss:0.092, val_acc:0.983]
Epoch [30/120    avg_loss:0.091, val_acc:0.971]
Epoch [31/120    avg_loss:0.072, val_acc:0.988]
Epoch [32/120    avg_loss:0.067, val_acc:0.969]
Epoch [33/120    avg_loss:0.090, val_acc:0.977]
Epoch [34/120    avg_loss:0.097, val_acc:0.965]
Epoch [35/120    avg_loss:0.139, val_acc:0.994]
Epoch [36/120    avg_loss:0.081, val_acc:0.990]
Epoch [37/120    avg_loss:0.075, val_acc:0.985]
Epoch [38/120    avg_loss:0.081, val_acc:0.975]
Epoch [39/120    avg_loss:0.088, val_acc:0.973]
Epoch [40/120    avg_loss:0.051, val_acc:0.988]
Epoch [41/120    avg_loss:0.041, val_acc:0.990]
Epoch [42/120    avg_loss:0.067, val_acc:0.985]
Epoch [43/120    avg_loss:0.113, val_acc:0.948]
Epoch [44/120    avg_loss:0.121, val_acc:0.969]
Epoch [45/120    avg_loss:0.137, val_acc:0.969]
Epoch [46/120    avg_loss:0.130, val_acc:0.956]
Epoch [47/120    avg_loss:0.105, val_acc:0.969]
Epoch [48/120    avg_loss:0.103, val_acc:0.979]
Epoch [49/120    avg_loss:0.049, val_acc:0.981]
Epoch [50/120    avg_loss:0.036, val_acc:0.985]
Epoch [51/120    avg_loss:0.036, val_acc:0.990]
Epoch [52/120    avg_loss:0.024, val_acc:0.988]
Epoch [53/120    avg_loss:0.020, val_acc:0.985]
Epoch [54/120    avg_loss:0.043, val_acc:0.985]
Epoch [55/120    avg_loss:0.024, val_acc:0.988]
Epoch [56/120    avg_loss:0.021, val_acc:0.988]
Epoch [57/120    avg_loss:0.029, val_acc:0.988]
Epoch [58/120    avg_loss:0.016, val_acc:0.990]
Epoch [59/120    avg_loss:0.028, val_acc:0.992]
Epoch [60/120    avg_loss:0.023, val_acc:0.988]
Epoch [61/120    avg_loss:0.020, val_acc:0.990]
Epoch [62/120    avg_loss:0.033, val_acc:0.990]
Epoch [63/120    avg_loss:0.019, val_acc:0.990]
Epoch [64/120    avg_loss:0.035, val_acc:0.992]
Epoch [65/120    avg_loss:0.018, val_acc:0.992]
Epoch [66/120    avg_loss:0.022, val_acc:0.992]
Epoch [67/120    avg_loss:0.018, val_acc:0.990]
Epoch [68/120    avg_loss:0.023, val_acc:0.990]
Epoch [69/120    avg_loss:0.022, val_acc:0.990]
Epoch [70/120    avg_loss:0.016, val_acc:0.990]
Epoch [71/120    avg_loss:0.034, val_acc:0.990]
Epoch [72/120    avg_loss:0.029, val_acc:0.990]
Epoch [73/120    avg_loss:0.030, val_acc:0.990]
Epoch [74/120    avg_loss:0.028, val_acc:0.990]
Epoch [75/120    avg_loss:0.020, val_acc:0.990]
Epoch [76/120    avg_loss:0.014, val_acc:0.990]
Epoch [77/120    avg_loss:0.019, val_acc:0.990]
Epoch [78/120    avg_loss:0.028, val_acc:0.990]
Epoch [79/120    avg_loss:0.022, val_acc:0.990]
Epoch [80/120    avg_loss:0.019, val_acc:0.990]
Epoch [81/120    avg_loss:0.036, val_acc:0.990]
Epoch [82/120    avg_loss:0.025, val_acc:0.990]
Epoch [83/120    avg_loss:0.028, val_acc:0.990]
Epoch [84/120    avg_loss:0.022, val_acc:0.990]
Epoch [85/120    avg_loss:0.024, val_acc:0.990]
Epoch [86/120    avg_loss:0.027, val_acc:0.990]
Epoch [87/120    avg_loss:0.022, val_acc:0.990]
Epoch [88/120    avg_loss:0.021, val_acc:0.990]
Epoch [89/120    avg_loss:0.020, val_acc:0.990]
Epoch [90/120    avg_loss:0.036, val_acc:0.990]
Epoch [91/120    avg_loss:0.025, val_acc:0.990]
Epoch [92/120    avg_loss:0.027, val_acc:0.990]
Epoch [93/120    avg_loss:0.021, val_acc:0.990]
Epoch [94/120    avg_loss:0.019, val_acc:0.990]
Epoch [95/120    avg_loss:0.019, val_acc:0.990]
Epoch [96/120    avg_loss:0.017, val_acc:0.990]
Epoch [97/120    avg_loss:0.027, val_acc:0.990]
Epoch [98/120    avg_loss:0.023, val_acc:0.990]
Epoch [99/120    avg_loss:0.026, val_acc:0.990]
Epoch [100/120    avg_loss:0.022, val_acc:0.990]
Epoch [101/120    avg_loss:0.015, val_acc:0.990]
Epoch [102/120    avg_loss:0.020, val_acc:0.990]
Epoch [103/120    avg_loss:0.035, val_acc:0.990]
Epoch [104/120    avg_loss:0.014, val_acc:0.990]
Epoch [105/120    avg_loss:0.023, val_acc:0.990]
Epoch [106/120    avg_loss:0.020, val_acc:0.990]
Epoch [107/120    avg_loss:0.026, val_acc:0.990]
Epoch [108/120    avg_loss:0.024, val_acc:0.990]
Epoch [109/120    avg_loss:0.026, val_acc:0.990]
Epoch [110/120    avg_loss:0.028, val_acc:0.990]
Epoch [111/120    avg_loss:0.031, val_acc:0.990]
Epoch [112/120    avg_loss:0.016, val_acc:0.990]
Epoch [113/120    avg_loss:0.021, val_acc:0.990]
Epoch [114/120    avg_loss:0.023, val_acc:0.990]
Epoch [115/120    avg_loss:0.032, val_acc:0.990]
Epoch [116/120    avg_loss:0.018, val_acc:0.990]
Epoch [117/120    avg_loss:0.028, val_acc:0.990]
Epoch [118/120    avg_loss:0.022, val_acc:0.990]
Epoch [119/120    avg_loss:0.024, val_acc:0.990]
Epoch [120/120    avg_loss:0.018, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210   9   0   0   0   0   0   0   8   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.99319728 0.98901099 0.9375     0.94880546
 1.         0.99465241 1.         1.         1.         1.
 0.98903509 1.        ]

Kappa:
0.9926407264294954
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f43cedbf828>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.939, val_acc:0.627]
Epoch [2/120    avg_loss:1.177, val_acc:0.750]
Epoch [3/120    avg_loss:0.925, val_acc:0.657]
Epoch [4/120    avg_loss:0.868, val_acc:0.706]
Epoch [5/120    avg_loss:0.624, val_acc:0.812]
Epoch [6/120    avg_loss:0.539, val_acc:0.879]
Epoch [7/120    avg_loss:0.540, val_acc:0.837]
Epoch [8/120    avg_loss:0.589, val_acc:0.849]
Epoch [9/120    avg_loss:0.468, val_acc:0.921]
Epoch [10/120    avg_loss:0.563, val_acc:0.831]
Epoch [11/120    avg_loss:0.434, val_acc:0.915]
Epoch [12/120    avg_loss:0.365, val_acc:0.905]
Epoch [13/120    avg_loss:0.517, val_acc:0.841]
Epoch [14/120    avg_loss:0.459, val_acc:0.915]
Epoch [15/120    avg_loss:0.290, val_acc:0.925]
Epoch [16/120    avg_loss:0.317, val_acc:0.948]
Epoch [17/120    avg_loss:0.281, val_acc:0.946]
Epoch [18/120    avg_loss:0.214, val_acc:0.927]
Epoch [19/120    avg_loss:0.231, val_acc:0.948]
Epoch [20/120    avg_loss:0.245, val_acc:0.958]
Epoch [21/120    avg_loss:0.220, val_acc:0.958]
Epoch [22/120    avg_loss:0.122, val_acc:0.940]
Epoch [23/120    avg_loss:0.214, val_acc:0.956]
Epoch [24/120    avg_loss:0.189, val_acc:0.944]
Epoch [25/120    avg_loss:0.134, val_acc:0.964]
Epoch [26/120    avg_loss:0.113, val_acc:0.952]
Epoch [27/120    avg_loss:0.276, val_acc:0.933]
Epoch [28/120    avg_loss:0.228, val_acc:0.954]
Epoch [29/120    avg_loss:0.273, val_acc:0.923]
Epoch [30/120    avg_loss:0.157, val_acc:0.970]
Epoch [31/120    avg_loss:0.127, val_acc:0.968]
Epoch [32/120    avg_loss:0.158, val_acc:0.931]
Epoch [33/120    avg_loss:0.136, val_acc:0.966]
Epoch [34/120    avg_loss:0.115, val_acc:0.952]
Epoch [35/120    avg_loss:0.171, val_acc:0.944]
Epoch [36/120    avg_loss:0.363, val_acc:0.948]
Epoch [37/120    avg_loss:0.168, val_acc:0.921]
Epoch [38/120    avg_loss:0.125, val_acc:0.925]
Epoch [39/120    avg_loss:0.101, val_acc:0.938]
Epoch [40/120    avg_loss:0.092, val_acc:0.962]
Epoch [41/120    avg_loss:0.078, val_acc:0.948]
Epoch [42/120    avg_loss:0.070, val_acc:0.974]
Epoch [43/120    avg_loss:0.096, val_acc:0.925]
Epoch [44/120    avg_loss:0.081, val_acc:0.966]
Epoch [45/120    avg_loss:0.068, val_acc:0.976]
Epoch [46/120    avg_loss:0.051, val_acc:0.984]
Epoch [47/120    avg_loss:0.061, val_acc:0.970]
Epoch [48/120    avg_loss:0.074, val_acc:0.968]
Epoch [49/120    avg_loss:0.094, val_acc:0.984]
Epoch [50/120    avg_loss:0.066, val_acc:0.978]
Epoch [51/120    avg_loss:0.048, val_acc:0.980]
Epoch [52/120    avg_loss:0.041, val_acc:0.980]
Epoch [53/120    avg_loss:0.079, val_acc:0.978]
Epoch [54/120    avg_loss:0.053, val_acc:0.968]
Epoch [55/120    avg_loss:0.050, val_acc:0.982]
Epoch [56/120    avg_loss:0.034, val_acc:0.980]
Epoch [57/120    avg_loss:0.061, val_acc:0.976]
Epoch [58/120    avg_loss:0.040, val_acc:0.984]
Epoch [59/120    avg_loss:0.037, val_acc:0.986]
Epoch [60/120    avg_loss:0.046, val_acc:0.986]
Epoch [61/120    avg_loss:0.018, val_acc:0.986]
Epoch [62/120    avg_loss:0.036, val_acc:0.976]
Epoch [63/120    avg_loss:0.049, val_acc:0.982]
Epoch [64/120    avg_loss:0.086, val_acc:0.980]
Epoch [65/120    avg_loss:0.018, val_acc:0.970]
Epoch [66/120    avg_loss:0.099, val_acc:0.962]
Epoch [67/120    avg_loss:0.046, val_acc:0.982]
Epoch [68/120    avg_loss:0.031, val_acc:0.982]
Epoch [69/120    avg_loss:0.040, val_acc:0.984]
Epoch [70/120    avg_loss:0.055, val_acc:0.978]
Epoch [71/120    avg_loss:0.026, val_acc:0.982]
Epoch [72/120    avg_loss:0.030, val_acc:0.986]
Epoch [73/120    avg_loss:0.030, val_acc:0.978]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.027, val_acc:0.984]
Epoch [76/120    avg_loss:0.056, val_acc:0.984]
Epoch [77/120    avg_loss:0.030, val_acc:0.984]
Epoch [78/120    avg_loss:0.020, val_acc:0.988]
Epoch [79/120    avg_loss:0.037, val_acc:0.970]
Epoch [80/120    avg_loss:0.044, val_acc:0.990]
Epoch [81/120    avg_loss:0.022, val_acc:0.980]
Epoch [82/120    avg_loss:0.037, val_acc:0.986]
Epoch [83/120    avg_loss:0.016, val_acc:0.970]
Epoch [84/120    avg_loss:0.020, val_acc:0.982]
Epoch [85/120    avg_loss:0.023, val_acc:0.978]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.015, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.978]
Epoch [89/120    avg_loss:0.022, val_acc:0.990]
Epoch [90/120    avg_loss:0.025, val_acc:0.982]
Epoch [91/120    avg_loss:0.022, val_acc:0.982]
Epoch [92/120    avg_loss:0.025, val_acc:0.984]
Epoch [93/120    avg_loss:0.017, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.016, val_acc:0.976]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.018, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.012, val_acc:0.988]
Epoch [106/120    avg_loss:0.025, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.021, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.008, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215   4   0   0   0   0   0   0   8   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99926954 0.99543379 1.         0.94713656 0.95833333
 0.98771499 0.99465241 1.         1.         1.         0.9973545
 0.98795181 1.        ]

Kappa:
0.9931153015144616
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde949657b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.908, val_acc:0.655]
Epoch [2/120    avg_loss:1.120, val_acc:0.740]
Epoch [3/120    avg_loss:0.898, val_acc:0.758]
Epoch [4/120    avg_loss:0.796, val_acc:0.829]
Epoch [5/120    avg_loss:0.581, val_acc:0.881]
Epoch [6/120    avg_loss:0.559, val_acc:0.891]
Epoch [7/120    avg_loss:0.604, val_acc:0.931]
Epoch [8/120    avg_loss:0.519, val_acc:0.891]
Epoch [9/120    avg_loss:0.542, val_acc:0.891]
Epoch [10/120    avg_loss:0.535, val_acc:0.883]
Epoch [11/120    avg_loss:0.427, val_acc:0.903]
Epoch [12/120    avg_loss:0.389, val_acc:0.919]
Epoch [13/120    avg_loss:0.325, val_acc:0.925]
Epoch [14/120    avg_loss:0.271, val_acc:0.960]
Epoch [15/120    avg_loss:0.282, val_acc:0.919]
Epoch [16/120    avg_loss:0.275, val_acc:0.946]
Epoch [17/120    avg_loss:0.284, val_acc:0.950]
Epoch [18/120    avg_loss:0.191, val_acc:0.954]
Epoch [19/120    avg_loss:0.233, val_acc:0.935]
Epoch [20/120    avg_loss:0.210, val_acc:0.915]
Epoch [21/120    avg_loss:0.201, val_acc:0.960]
Epoch [22/120    avg_loss:0.299, val_acc:0.897]
Epoch [23/120    avg_loss:0.245, val_acc:0.952]
Epoch [24/120    avg_loss:0.142, val_acc:0.940]
Epoch [25/120    avg_loss:0.142, val_acc:0.956]
Epoch [26/120    avg_loss:0.171, val_acc:0.962]
Epoch [27/120    avg_loss:0.143, val_acc:0.968]
Epoch [28/120    avg_loss:0.116, val_acc:0.974]
Epoch [29/120    avg_loss:0.152, val_acc:0.938]
Epoch [30/120    avg_loss:0.078, val_acc:0.982]
Epoch [31/120    avg_loss:0.156, val_acc:0.925]
Epoch [32/120    avg_loss:0.316, val_acc:0.950]
Epoch [33/120    avg_loss:0.159, val_acc:0.976]
Epoch [34/120    avg_loss:0.138, val_acc:0.976]
Epoch [35/120    avg_loss:0.120, val_acc:0.948]
Epoch [36/120    avg_loss:0.116, val_acc:0.964]
Epoch [37/120    avg_loss:0.173, val_acc:0.980]
Epoch [38/120    avg_loss:0.064, val_acc:0.978]
Epoch [39/120    avg_loss:0.072, val_acc:0.978]
Epoch [40/120    avg_loss:0.076, val_acc:0.986]
Epoch [41/120    avg_loss:0.077, val_acc:0.976]
Epoch [42/120    avg_loss:0.044, val_acc:0.980]
Epoch [43/120    avg_loss:0.106, val_acc:0.986]
Epoch [44/120    avg_loss:0.036, val_acc:0.982]
Epoch [45/120    avg_loss:0.028, val_acc:0.992]
Epoch [46/120    avg_loss:0.025, val_acc:0.992]
Epoch [47/120    avg_loss:0.078, val_acc:0.996]
Epoch [48/120    avg_loss:0.049, val_acc:0.988]
Epoch [49/120    avg_loss:0.074, val_acc:0.982]
Epoch [50/120    avg_loss:0.100, val_acc:0.976]
Epoch [51/120    avg_loss:0.071, val_acc:0.980]
Epoch [52/120    avg_loss:0.064, val_acc:0.984]
Epoch [53/120    avg_loss:0.065, val_acc:0.970]
Epoch [54/120    avg_loss:0.081, val_acc:0.984]
Epoch [55/120    avg_loss:0.056, val_acc:0.966]
Epoch [56/120    avg_loss:0.065, val_acc:0.988]
Epoch [57/120    avg_loss:0.094, val_acc:0.986]
Epoch [58/120    avg_loss:0.043, val_acc:0.988]
Epoch [59/120    avg_loss:0.023, val_acc:0.990]
Epoch [60/120    avg_loss:0.038, val_acc:0.986]
Epoch [61/120    avg_loss:0.018, val_acc:0.988]
Epoch [62/120    avg_loss:0.020, val_acc:0.990]
Epoch [63/120    avg_loss:0.023, val_acc:0.992]
Epoch [64/120    avg_loss:0.022, val_acc:0.998]
Epoch [65/120    avg_loss:0.017, val_acc:0.996]
Epoch [66/120    avg_loss:0.036, val_acc:0.996]
Epoch [67/120    avg_loss:0.016, val_acc:0.994]
Epoch [68/120    avg_loss:0.015, val_acc:0.992]
Epoch [69/120    avg_loss:0.010, val_acc:0.992]
Epoch [70/120    avg_loss:0.022, val_acc:0.992]
Epoch [71/120    avg_loss:0.012, val_acc:0.994]
Epoch [72/120    avg_loss:0.013, val_acc:0.994]
Epoch [73/120    avg_loss:0.018, val_acc:0.992]
Epoch [74/120    avg_loss:0.013, val_acc:0.994]
Epoch [75/120    avg_loss:0.013, val_acc:0.994]
Epoch [76/120    avg_loss:0.013, val_acc:0.994]
Epoch [77/120    avg_loss:0.012, val_acc:0.994]
Epoch [78/120    avg_loss:0.008, val_acc:0.994]
Epoch [79/120    avg_loss:0.014, val_acc:0.994]
Epoch [80/120    avg_loss:0.017, val_acc:0.994]
Epoch [81/120    avg_loss:0.016, val_acc:0.994]
Epoch [82/120    avg_loss:0.010, val_acc:0.994]
Epoch [83/120    avg_loss:0.015, val_acc:0.994]
Epoch [84/120    avg_loss:0.029, val_acc:0.994]
Epoch [85/120    avg_loss:0.012, val_acc:0.996]
Epoch [86/120    avg_loss:0.017, val_acc:0.996]
Epoch [87/120    avg_loss:0.018, val_acc:0.996]
Epoch [88/120    avg_loss:0.014, val_acc:0.996]
Epoch [89/120    avg_loss:0.016, val_acc:0.994]
Epoch [90/120    avg_loss:0.010, val_acc:0.994]
Epoch [91/120    avg_loss:0.011, val_acc:0.994]
Epoch [92/120    avg_loss:0.012, val_acc:0.994]
Epoch [93/120    avg_loss:0.013, val_acc:0.994]
Epoch [94/120    avg_loss:0.012, val_acc:0.994]
Epoch [95/120    avg_loss:0.014, val_acc:0.994]
Epoch [96/120    avg_loss:0.011, val_acc:0.994]
Epoch [97/120    avg_loss:0.009, val_acc:0.994]
Epoch [98/120    avg_loss:0.011, val_acc:0.994]
Epoch [99/120    avg_loss:0.010, val_acc:0.994]
Epoch [100/120    avg_loss:0.015, val_acc:0.994]
Epoch [101/120    avg_loss:0.015, val_acc:0.994]
Epoch [102/120    avg_loss:0.012, val_acc:0.994]
Epoch [103/120    avg_loss:0.011, val_acc:0.994]
Epoch [104/120    avg_loss:0.014, val_acc:0.994]
Epoch [105/120    avg_loss:0.009, val_acc:0.994]
Epoch [106/120    avg_loss:0.008, val_acc:0.994]
Epoch [107/120    avg_loss:0.014, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.011, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.994]
Epoch [111/120    avg_loss:0.015, val_acc:0.994]
Epoch [112/120    avg_loss:0.018, val_acc:0.994]
Epoch [113/120    avg_loss:0.014, val_acc:0.994]
Epoch [114/120    avg_loss:0.016, val_acc:0.994]
Epoch [115/120    avg_loss:0.012, val_acc:0.994]
Epoch [116/120    avg_loss:0.013, val_acc:0.994]
Epoch [117/120    avg_loss:0.019, val_acc:0.994]
Epoch [118/120    avg_loss:0.013, val_acc:0.994]
Epoch [119/120    avg_loss:0.009, val_acc:0.994]
Epoch [120/120    avg_loss:0.010, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213   9   0   0   0   0   0   0   5   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   5 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.99319728 1.         0.95515695 0.94880546
 1.         0.99465241 1.         1.         1.         0.99341238
 0.98672566 1.        ]

Kappa:
0.9933532651022275
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a80dd77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.038, val_acc:0.571]
Epoch [2/120    avg_loss:1.223, val_acc:0.708]
Epoch [3/120    avg_loss:1.031, val_acc:0.770]
Epoch [4/120    avg_loss:0.857, val_acc:0.810]
Epoch [5/120    avg_loss:0.846, val_acc:0.806]
Epoch [6/120    avg_loss:0.603, val_acc:0.863]
Epoch [7/120    avg_loss:0.505, val_acc:0.871]
Epoch [8/120    avg_loss:0.505, val_acc:0.903]
Epoch [9/120    avg_loss:0.470, val_acc:0.883]
Epoch [10/120    avg_loss:0.385, val_acc:0.927]
Epoch [11/120    avg_loss:0.336, val_acc:0.954]
Epoch [12/120    avg_loss:0.382, val_acc:0.889]
Epoch [13/120    avg_loss:0.506, val_acc:0.833]
Epoch [14/120    avg_loss:0.433, val_acc:0.907]
Epoch [15/120    avg_loss:0.282, val_acc:0.915]
Epoch [16/120    avg_loss:0.299, val_acc:0.940]
Epoch [17/120    avg_loss:0.198, val_acc:0.962]
Epoch [18/120    avg_loss:0.247, val_acc:0.946]
Epoch [19/120    avg_loss:0.183, val_acc:0.968]
Epoch [20/120    avg_loss:0.227, val_acc:0.938]
Epoch [21/120    avg_loss:0.157, val_acc:0.962]
Epoch [22/120    avg_loss:0.243, val_acc:0.958]
Epoch [23/120    avg_loss:0.152, val_acc:0.933]
Epoch [24/120    avg_loss:0.146, val_acc:0.972]
Epoch [25/120    avg_loss:0.090, val_acc:0.978]
Epoch [26/120    avg_loss:0.167, val_acc:0.972]
Epoch [27/120    avg_loss:0.115, val_acc:0.980]
Epoch [28/120    avg_loss:0.086, val_acc:0.988]
Epoch [29/120    avg_loss:0.088, val_acc:0.984]
Epoch [30/120    avg_loss:0.083, val_acc:0.954]
Epoch [31/120    avg_loss:0.152, val_acc:0.964]
Epoch [32/120    avg_loss:0.114, val_acc:0.976]
Epoch [33/120    avg_loss:0.070, val_acc:0.966]
Epoch [34/120    avg_loss:0.116, val_acc:0.970]
Epoch [35/120    avg_loss:0.125, val_acc:0.962]
Epoch [36/120    avg_loss:0.166, val_acc:0.982]
Epoch [37/120    avg_loss:0.115, val_acc:0.986]
Epoch [38/120    avg_loss:0.086, val_acc:0.970]
Epoch [39/120    avg_loss:0.076, val_acc:0.982]
Epoch [40/120    avg_loss:0.071, val_acc:0.974]
Epoch [41/120    avg_loss:0.047, val_acc:0.982]
Epoch [42/120    avg_loss:0.041, val_acc:0.988]
Epoch [43/120    avg_loss:0.081, val_acc:0.996]
Epoch [44/120    avg_loss:0.040, val_acc:0.996]
Epoch [45/120    avg_loss:0.019, val_acc:0.996]
Epoch [46/120    avg_loss:0.025, val_acc:0.998]
Epoch [47/120    avg_loss:0.020, val_acc:0.998]
Epoch [48/120    avg_loss:0.033, val_acc:0.998]
Epoch [49/120    avg_loss:0.018, val_acc:0.996]
Epoch [50/120    avg_loss:0.025, val_acc:0.998]
Epoch [51/120    avg_loss:0.015, val_acc:0.998]
Epoch [52/120    avg_loss:0.024, val_acc:0.998]
Epoch [53/120    avg_loss:0.017, val_acc:1.000]
Epoch [54/120    avg_loss:0.029, val_acc:0.998]
Epoch [55/120    avg_loss:0.016, val_acc:0.994]
Epoch [56/120    avg_loss:0.019, val_acc:0.996]
Epoch [57/120    avg_loss:0.022, val_acc:0.998]
Epoch [58/120    avg_loss:0.034, val_acc:1.000]
Epoch [59/120    avg_loss:0.017, val_acc:1.000]
Epoch [60/120    avg_loss:0.016, val_acc:1.000]
Epoch [61/120    avg_loss:0.033, val_acc:0.998]
Epoch [62/120    avg_loss:0.033, val_acc:0.994]
Epoch [63/120    avg_loss:0.027, val_acc:0.996]
Epoch [64/120    avg_loss:0.022, val_acc:0.998]
Epoch [65/120    avg_loss:0.035, val_acc:0.996]
Epoch [66/120    avg_loss:0.018, val_acc:0.996]
Epoch [67/120    avg_loss:0.019, val_acc:0.998]
Epoch [68/120    avg_loss:0.016, val_acc:0.998]
Epoch [69/120    avg_loss:0.014, val_acc:0.998]
Epoch [70/120    avg_loss:0.026, val_acc:0.998]
Epoch [71/120    avg_loss:0.030, val_acc:0.998]
Epoch [72/120    avg_loss:0.021, val_acc:0.998]
Epoch [73/120    avg_loss:0.023, val_acc:1.000]
Epoch [74/120    avg_loss:0.020, val_acc:0.994]
Epoch [75/120    avg_loss:0.016, val_acc:0.996]
Epoch [76/120    avg_loss:0.025, val_acc:0.996]
Epoch [77/120    avg_loss:0.022, val_acc:0.996]
Epoch [78/120    avg_loss:0.018, val_acc:0.996]
Epoch [79/120    avg_loss:0.019, val_acc:0.994]
Epoch [80/120    avg_loss:0.018, val_acc:0.996]
Epoch [81/120    avg_loss:0.018, val_acc:0.998]
Epoch [82/120    avg_loss:0.022, val_acc:0.998]
Epoch [83/120    avg_loss:0.026, val_acc:0.998]
Epoch [84/120    avg_loss:0.018, val_acc:0.998]
Epoch [85/120    avg_loss:0.020, val_acc:1.000]
Epoch [86/120    avg_loss:0.021, val_acc:0.998]
Epoch [87/120    avg_loss:0.021, val_acc:0.998]
Epoch [88/120    avg_loss:0.017, val_acc:0.998]
Epoch [89/120    avg_loss:0.025, val_acc:0.998]
Epoch [90/120    avg_loss:0.013, val_acc:0.998]
Epoch [91/120    avg_loss:0.014, val_acc:0.998]
Epoch [92/120    avg_loss:0.024, val_acc:0.998]
Epoch [93/120    avg_loss:0.016, val_acc:0.998]
Epoch [94/120    avg_loss:0.025, val_acc:0.998]
Epoch [95/120    avg_loss:0.016, val_acc:0.998]
Epoch [96/120    avg_loss:0.032, val_acc:0.996]
Epoch [97/120    avg_loss:0.022, val_acc:0.996]
Epoch [98/120    avg_loss:0.025, val_acc:0.998]
Epoch [99/120    avg_loss:0.017, val_acc:0.998]
Epoch [100/120    avg_loss:0.018, val_acc:0.998]
Epoch [101/120    avg_loss:0.013, val_acc:0.998]
Epoch [102/120    avg_loss:0.019, val_acc:0.998]
Epoch [103/120    avg_loss:0.018, val_acc:0.998]
Epoch [104/120    avg_loss:0.019, val_acc:0.998]
Epoch [105/120    avg_loss:0.036, val_acc:0.998]
Epoch [106/120    avg_loss:0.016, val_acc:0.998]
Epoch [107/120    avg_loss:0.013, val_acc:0.998]
Epoch [108/120    avg_loss:0.015, val_acc:0.998]
Epoch [109/120    avg_loss:0.013, val_acc:0.998]
Epoch [110/120    avg_loss:0.021, val_acc:0.996]
Epoch [111/120    avg_loss:0.015, val_acc:0.996]
Epoch [112/120    avg_loss:0.018, val_acc:0.996]
Epoch [113/120    avg_loss:0.014, val_acc:0.996]
Epoch [114/120    avg_loss:0.021, val_acc:0.996]
Epoch [115/120    avg_loss:0.015, val_acc:0.996]
Epoch [116/120    avg_loss:0.011, val_acc:0.996]
Epoch [117/120    avg_loss:0.019, val_acc:0.996]
Epoch [118/120    avg_loss:0.013, val_acc:0.996]
Epoch [119/120    avg_loss:0.013, val_acc:0.996]
Epoch [120/120    avg_loss:0.023, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.99095023 1.         0.95909091 0.94701987
 1.         0.97826087 1.         1.         1.         0.9973545
 0.99558499 1.        ]

Kappa:
0.9943028981424391
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ca17217b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.004, val_acc:0.607]
Epoch [2/120    avg_loss:1.219, val_acc:0.720]
Epoch [3/120    avg_loss:0.895, val_acc:0.762]
Epoch [4/120    avg_loss:0.771, val_acc:0.823]
Epoch [5/120    avg_loss:0.582, val_acc:0.841]
Epoch [6/120    avg_loss:0.626, val_acc:0.883]
Epoch [7/120    avg_loss:0.559, val_acc:0.831]
Epoch [8/120    avg_loss:0.489, val_acc:0.907]
Epoch [9/120    avg_loss:0.408, val_acc:0.865]
Epoch [10/120    avg_loss:0.397, val_acc:0.917]
Epoch [11/120    avg_loss:0.393, val_acc:0.913]
Epoch [12/120    avg_loss:0.312, val_acc:0.929]
Epoch [13/120    avg_loss:0.343, val_acc:0.948]
Epoch [14/120    avg_loss:0.354, val_acc:0.938]
Epoch [15/120    avg_loss:0.263, val_acc:0.940]
Epoch [16/120    avg_loss:0.262, val_acc:0.940]
Epoch [17/120    avg_loss:0.224, val_acc:0.966]
Epoch [18/120    avg_loss:0.175, val_acc:0.933]
Epoch [19/120    avg_loss:0.259, val_acc:0.942]
Epoch [20/120    avg_loss:0.226, val_acc:0.962]
Epoch [21/120    avg_loss:0.186, val_acc:0.966]
Epoch [22/120    avg_loss:0.163, val_acc:0.938]
Epoch [23/120    avg_loss:0.266, val_acc:0.911]
Epoch [24/120    avg_loss:0.312, val_acc:0.968]
Epoch [25/120    avg_loss:0.154, val_acc:0.968]
Epoch [26/120    avg_loss:0.140, val_acc:0.968]
Epoch [27/120    avg_loss:0.186, val_acc:0.919]
Epoch [28/120    avg_loss:0.187, val_acc:0.962]
Epoch [29/120    avg_loss:0.109, val_acc:0.980]
Epoch [30/120    avg_loss:0.100, val_acc:0.974]
Epoch [31/120    avg_loss:0.085, val_acc:0.980]
Epoch [32/120    avg_loss:0.074, val_acc:0.956]
Epoch [33/120    avg_loss:0.082, val_acc:0.982]
Epoch [34/120    avg_loss:0.067, val_acc:0.980]
Epoch [35/120    avg_loss:0.065, val_acc:0.984]
Epoch [36/120    avg_loss:0.046, val_acc:0.996]
Epoch [37/120    avg_loss:0.081, val_acc:0.984]
Epoch [38/120    avg_loss:0.074, val_acc:0.962]
Epoch [39/120    avg_loss:0.097, val_acc:0.938]
Epoch [40/120    avg_loss:0.083, val_acc:0.942]
Epoch [41/120    avg_loss:0.288, val_acc:0.946]
Epoch [42/120    avg_loss:0.116, val_acc:0.950]
Epoch [43/120    avg_loss:0.080, val_acc:0.978]
Epoch [44/120    avg_loss:0.074, val_acc:0.984]
Epoch [45/120    avg_loss:0.084, val_acc:0.984]
Epoch [46/120    avg_loss:0.059, val_acc:0.982]
Epoch [47/120    avg_loss:0.064, val_acc:0.980]
Epoch [48/120    avg_loss:0.038, val_acc:0.980]
Epoch [49/120    avg_loss:0.049, val_acc:0.968]
Epoch [50/120    avg_loss:0.041, val_acc:0.974]
Epoch [51/120    avg_loss:0.072, val_acc:0.986]
Epoch [52/120    avg_loss:0.014, val_acc:0.990]
Epoch [53/120    avg_loss:0.034, val_acc:0.990]
Epoch [54/120    avg_loss:0.018, val_acc:0.992]
Epoch [55/120    avg_loss:0.022, val_acc:0.992]
Epoch [56/120    avg_loss:0.016, val_acc:0.992]
Epoch [57/120    avg_loss:0.018, val_acc:0.992]
Epoch [58/120    avg_loss:0.015, val_acc:0.992]
Epoch [59/120    avg_loss:0.015, val_acc:0.994]
Epoch [60/120    avg_loss:0.027, val_acc:0.994]
Epoch [61/120    avg_loss:0.017, val_acc:0.994]
Epoch [62/120    avg_loss:0.013, val_acc:0.996]
Epoch [63/120    avg_loss:0.013, val_acc:0.994]
Epoch [64/120    avg_loss:0.014, val_acc:0.996]
Epoch [65/120    avg_loss:0.024, val_acc:0.996]
Epoch [66/120    avg_loss:0.024, val_acc:0.996]
Epoch [67/120    avg_loss:0.014, val_acc:0.996]
Epoch [68/120    avg_loss:0.013, val_acc:0.992]
Epoch [69/120    avg_loss:0.014, val_acc:0.994]
Epoch [70/120    avg_loss:0.013, val_acc:0.994]
Epoch [71/120    avg_loss:0.013, val_acc:0.994]
Epoch [72/120    avg_loss:0.012, val_acc:0.998]
Epoch [73/120    avg_loss:0.012, val_acc:0.998]
Epoch [74/120    avg_loss:0.012, val_acc:0.996]
Epoch [75/120    avg_loss:0.011, val_acc:0.996]
Epoch [76/120    avg_loss:0.020, val_acc:0.994]
Epoch [77/120    avg_loss:0.010, val_acc:0.994]
Epoch [78/120    avg_loss:0.021, val_acc:0.996]
Epoch [79/120    avg_loss:0.014, val_acc:0.998]
Epoch [80/120    avg_loss:0.012, val_acc:0.994]
Epoch [81/120    avg_loss:0.010, val_acc:0.992]
Epoch [82/120    avg_loss:0.010, val_acc:0.994]
Epoch [83/120    avg_loss:0.016, val_acc:0.996]
Epoch [84/120    avg_loss:0.020, val_acc:0.996]
Epoch [85/120    avg_loss:0.019, val_acc:0.992]
Epoch [86/120    avg_loss:0.020, val_acc:0.992]
Epoch [87/120    avg_loss:0.016, val_acc:0.992]
Epoch [88/120    avg_loss:0.017, val_acc:0.992]
Epoch [89/120    avg_loss:0.013, val_acc:0.992]
Epoch [90/120    avg_loss:0.011, val_acc:0.992]
Epoch [91/120    avg_loss:0.023, val_acc:0.992]
Epoch [92/120    avg_loss:0.010, val_acc:0.994]
Epoch [93/120    avg_loss:0.010, val_acc:0.994]
Epoch [94/120    avg_loss:0.014, val_acc:0.994]
Epoch [95/120    avg_loss:0.017, val_acc:0.994]
Epoch [96/120    avg_loss:0.015, val_acc:0.994]
Epoch [97/120    avg_loss:0.013, val_acc:0.994]
Epoch [98/120    avg_loss:0.012, val_acc:0.992]
Epoch [99/120    avg_loss:0.011, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.012, val_acc:0.992]
Epoch [102/120    avg_loss:0.014, val_acc:0.992]
Epoch [103/120    avg_loss:0.025, val_acc:0.992]
Epoch [104/120    avg_loss:0.021, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.992]
Epoch [106/120    avg_loss:0.012, val_acc:0.992]
Epoch [107/120    avg_loss:0.013, val_acc:0.992]
Epoch [108/120    avg_loss:0.010, val_acc:0.992]
Epoch [109/120    avg_loss:0.011, val_acc:0.992]
Epoch [110/120    avg_loss:0.009, val_acc:0.992]
Epoch [111/120    avg_loss:0.020, val_acc:0.992]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.014, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.012, val_acc:0.992]
Epoch [119/120    avg_loss:0.012, val_acc:0.992]
Epoch [120/120    avg_loss:0.015, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0  10 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.98426966 0.98004435 0.94577007 0.94520548
 1.         0.9726776  1.         1.         1.         0.98691099
 0.98657718 1.        ]

Kappa:
0.9900303795561742
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c1a576860>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.910, val_acc:0.610]
Epoch [2/120    avg_loss:1.140, val_acc:0.665]
Epoch [3/120    avg_loss:0.895, val_acc:0.752]
Epoch [4/120    avg_loss:0.708, val_acc:0.790]
Epoch [5/120    avg_loss:0.741, val_acc:0.825]
Epoch [6/120    avg_loss:0.570, val_acc:0.863]
Epoch [7/120    avg_loss:0.514, val_acc:0.873]
Epoch [8/120    avg_loss:0.455, val_acc:0.877]
Epoch [9/120    avg_loss:0.423, val_acc:0.885]
Epoch [10/120    avg_loss:0.371, val_acc:0.908]
Epoch [11/120    avg_loss:0.335, val_acc:0.894]
Epoch [12/120    avg_loss:0.359, val_acc:0.902]
Epoch [13/120    avg_loss:0.277, val_acc:0.940]
Epoch [14/120    avg_loss:0.261, val_acc:0.929]
Epoch [15/120    avg_loss:0.263, val_acc:0.935]
Epoch [16/120    avg_loss:0.339, val_acc:0.944]
Epoch [17/120    avg_loss:0.262, val_acc:0.944]
Epoch [18/120    avg_loss:0.227, val_acc:0.935]
Epoch [19/120    avg_loss:0.169, val_acc:0.950]
Epoch [20/120    avg_loss:0.151, val_acc:0.952]
Epoch [21/120    avg_loss:0.286, val_acc:0.948]
Epoch [22/120    avg_loss:0.183, val_acc:0.956]
Epoch [23/120    avg_loss:0.144, val_acc:0.967]
Epoch [24/120    avg_loss:0.145, val_acc:0.958]
Epoch [25/120    avg_loss:0.097, val_acc:0.967]
Epoch [26/120    avg_loss:0.198, val_acc:0.946]
Epoch [27/120    avg_loss:0.128, val_acc:0.952]
Epoch [28/120    avg_loss:0.122, val_acc:0.963]
Epoch [29/120    avg_loss:0.120, val_acc:0.963]
Epoch [30/120    avg_loss:0.106, val_acc:0.973]
Epoch [31/120    avg_loss:0.080, val_acc:0.973]
Epoch [32/120    avg_loss:0.084, val_acc:0.979]
Epoch [33/120    avg_loss:0.060, val_acc:0.967]
Epoch [34/120    avg_loss:0.074, val_acc:0.975]
Epoch [35/120    avg_loss:0.090, val_acc:0.979]
Epoch [36/120    avg_loss:0.128, val_acc:0.958]
Epoch [37/120    avg_loss:0.162, val_acc:0.975]
Epoch [38/120    avg_loss:0.093, val_acc:0.981]
Epoch [39/120    avg_loss:0.054, val_acc:0.975]
Epoch [40/120    avg_loss:0.073, val_acc:0.977]
Epoch [41/120    avg_loss:0.039, val_acc:0.981]
Epoch [42/120    avg_loss:0.023, val_acc:0.992]
Epoch [43/120    avg_loss:0.070, val_acc:0.950]
Epoch [44/120    avg_loss:0.092, val_acc:0.983]
Epoch [45/120    avg_loss:0.062, val_acc:0.977]
Epoch [46/120    avg_loss:0.071, val_acc:0.971]
Epoch [47/120    avg_loss:0.051, val_acc:0.973]
Epoch [48/120    avg_loss:0.033, val_acc:0.979]
Epoch [49/120    avg_loss:0.042, val_acc:0.981]
Epoch [50/120    avg_loss:0.061, val_acc:0.973]
Epoch [51/120    avg_loss:0.070, val_acc:0.988]
Epoch [52/120    avg_loss:0.039, val_acc:0.990]
Epoch [53/120    avg_loss:0.035, val_acc:0.988]
Epoch [54/120    avg_loss:0.025, val_acc:0.985]
Epoch [55/120    avg_loss:0.028, val_acc:0.985]
Epoch [56/120    avg_loss:0.027, val_acc:0.985]
Epoch [57/120    avg_loss:0.015, val_acc:0.990]
Epoch [58/120    avg_loss:0.013, val_acc:0.990]
Epoch [59/120    avg_loss:0.040, val_acc:0.985]
Epoch [60/120    avg_loss:0.011, val_acc:0.988]
Epoch [61/120    avg_loss:0.011, val_acc:0.988]
Epoch [62/120    avg_loss:0.009, val_acc:0.988]
Epoch [63/120    avg_loss:0.011, val_acc:0.988]
Epoch [64/120    avg_loss:0.013, val_acc:0.988]
Epoch [65/120    avg_loss:0.016, val_acc:0.990]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.012, val_acc:0.988]
Epoch [68/120    avg_loss:0.021, val_acc:0.985]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.024, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.985]
Epoch [72/120    avg_loss:0.009, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.013, val_acc:0.985]
Epoch [75/120    avg_loss:0.013, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.014, val_acc:0.985]
Epoch [80/120    avg_loss:0.014, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.015, val_acc:0.985]
Epoch [83/120    avg_loss:0.018, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.022, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.985]
Epoch [88/120    avg_loss:0.019, val_acc:0.985]
Epoch [89/120    avg_loss:0.024, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.012, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.016, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.014, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.018, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.019, val_acc:0.985]
Epoch [105/120    avg_loss:0.019, val_acc:0.985]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.014, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.039, val_acc:0.985]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.014, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.985]
Epoch [114/120    avg_loss:0.018, val_acc:0.985]
Epoch [115/120    avg_loss:0.021, val_acc:0.985]
Epoch [116/120    avg_loss:0.013, val_acc:0.985]
Epoch [117/120    avg_loss:0.037, val_acc:0.985]
Epoch [118/120    avg_loss:0.017, val_acc:0.985]
Epoch [119/120    avg_loss:0.026, val_acc:0.985]
Epoch [120/120    avg_loss:0.015, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 1.         0.99545455 1.         0.97777778 0.96598639
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9971514337884493
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ba60f76d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:1.920, val_acc:0.673]
Epoch [2/120    avg_loss:1.076, val_acc:0.732]
Epoch [3/120    avg_loss:0.883, val_acc:0.790]
Epoch [4/120    avg_loss:0.679, val_acc:0.750]
Epoch [5/120    avg_loss:0.649, val_acc:0.718]
Epoch [6/120    avg_loss:0.736, val_acc:0.810]
Epoch [7/120    avg_loss:0.471, val_acc:0.859]
Epoch [8/120    avg_loss:0.501, val_acc:0.867]
Epoch [9/120    avg_loss:0.571, val_acc:0.869]
Epoch [10/120    avg_loss:0.446, val_acc:0.905]
Epoch [11/120    avg_loss:0.377, val_acc:0.895]
Epoch [12/120    avg_loss:0.314, val_acc:0.891]
Epoch [13/120    avg_loss:0.333, val_acc:0.889]
Epoch [14/120    avg_loss:0.269, val_acc:0.931]
Epoch [15/120    avg_loss:0.268, val_acc:0.889]
Epoch [16/120    avg_loss:0.357, val_acc:0.903]
Epoch [17/120    avg_loss:0.297, val_acc:0.927]
Epoch [18/120    avg_loss:0.217, val_acc:0.931]
Epoch [19/120    avg_loss:0.218, val_acc:0.933]
Epoch [20/120    avg_loss:0.228, val_acc:0.921]
Epoch [21/120    avg_loss:0.260, val_acc:0.891]
Epoch [22/120    avg_loss:0.190, val_acc:0.925]
Epoch [23/120    avg_loss:0.228, val_acc:0.923]
Epoch [24/120    avg_loss:0.164, val_acc:0.944]
Epoch [25/120    avg_loss:0.274, val_acc:0.946]
Epoch [26/120    avg_loss:0.167, val_acc:0.950]
Epoch [27/120    avg_loss:0.131, val_acc:0.974]
Epoch [28/120    avg_loss:0.101, val_acc:0.962]
Epoch [29/120    avg_loss:0.104, val_acc:0.966]
Epoch [30/120    avg_loss:0.093, val_acc:0.966]
Epoch [31/120    avg_loss:0.048, val_acc:0.978]
Epoch [32/120    avg_loss:0.058, val_acc:0.972]
Epoch [33/120    avg_loss:0.073, val_acc:0.966]
Epoch [34/120    avg_loss:0.081, val_acc:0.972]
Epoch [35/120    avg_loss:0.108, val_acc:0.964]
Epoch [36/120    avg_loss:0.072, val_acc:0.960]
Epoch [37/120    avg_loss:0.102, val_acc:0.972]
Epoch [38/120    avg_loss:0.090, val_acc:0.960]
Epoch [39/120    avg_loss:0.133, val_acc:0.954]
Epoch [40/120    avg_loss:0.121, val_acc:0.948]
Epoch [41/120    avg_loss:0.115, val_acc:0.950]
Epoch [42/120    avg_loss:0.159, val_acc:0.905]
Epoch [43/120    avg_loss:0.131, val_acc:0.960]
Epoch [44/120    avg_loss:0.106, val_acc:0.950]
Epoch [45/120    avg_loss:0.123, val_acc:0.978]
Epoch [46/120    avg_loss:0.044, val_acc:0.986]
Epoch [47/120    avg_loss:0.034, val_acc:0.986]
Epoch [48/120    avg_loss:0.036, val_acc:0.988]
Epoch [49/120    avg_loss:0.025, val_acc:0.986]
Epoch [50/120    avg_loss:0.030, val_acc:0.984]
Epoch [51/120    avg_loss:0.019, val_acc:0.986]
Epoch [52/120    avg_loss:0.027, val_acc:0.984]
Epoch [53/120    avg_loss:0.029, val_acc:0.982]
Epoch [54/120    avg_loss:0.030, val_acc:0.984]
Epoch [55/120    avg_loss:0.025, val_acc:0.984]
Epoch [56/120    avg_loss:0.024, val_acc:0.984]
Epoch [57/120    avg_loss:0.023, val_acc:0.984]
Epoch [58/120    avg_loss:0.035, val_acc:0.984]
Epoch [59/120    avg_loss:0.022, val_acc:0.986]
Epoch [60/120    avg_loss:0.022, val_acc:0.988]
Epoch [61/120    avg_loss:0.019, val_acc:0.988]
Epoch [62/120    avg_loss:0.029, val_acc:0.988]
Epoch [63/120    avg_loss:0.015, val_acc:0.988]
Epoch [64/120    avg_loss:0.035, val_acc:0.988]
Epoch [65/120    avg_loss:0.025, val_acc:0.988]
Epoch [66/120    avg_loss:0.020, val_acc:0.986]
Epoch [67/120    avg_loss:0.021, val_acc:0.986]
Epoch [68/120    avg_loss:0.020, val_acc:0.986]
Epoch [69/120    avg_loss:0.013, val_acc:0.988]
Epoch [70/120    avg_loss:0.019, val_acc:0.988]
Epoch [71/120    avg_loss:0.021, val_acc:0.988]
Epoch [72/120    avg_loss:0.018, val_acc:0.990]
Epoch [73/120    avg_loss:0.021, val_acc:0.990]
Epoch [74/120    avg_loss:0.020, val_acc:0.990]
Epoch [75/120    avg_loss:0.017, val_acc:0.992]
Epoch [76/120    avg_loss:0.018, val_acc:0.990]
Epoch [77/120    avg_loss:0.022, val_acc:0.988]
Epoch [78/120    avg_loss:0.027, val_acc:0.988]
Epoch [79/120    avg_loss:0.018, val_acc:0.988]
Epoch [80/120    avg_loss:0.020, val_acc:0.988]
Epoch [81/120    avg_loss:0.032, val_acc:0.988]
Epoch [82/120    avg_loss:0.011, val_acc:0.990]
Epoch [83/120    avg_loss:0.013, val_acc:0.990]
Epoch [84/120    avg_loss:0.015, val_acc:0.992]
Epoch [85/120    avg_loss:0.018, val_acc:0.992]
Epoch [86/120    avg_loss:0.022, val_acc:0.992]
Epoch [87/120    avg_loss:0.018, val_acc:0.992]
Epoch [88/120    avg_loss:0.025, val_acc:0.992]
Epoch [89/120    avg_loss:0.021, val_acc:0.990]
Epoch [90/120    avg_loss:0.012, val_acc:0.988]
Epoch [91/120    avg_loss:0.029, val_acc:0.990]
Epoch [92/120    avg_loss:0.015, val_acc:0.992]
Epoch [93/120    avg_loss:0.018, val_acc:0.992]
Epoch [94/120    avg_loss:0.021, val_acc:0.990]
Epoch [95/120    avg_loss:0.026, val_acc:0.990]
Epoch [96/120    avg_loss:0.017, val_acc:0.990]
Epoch [97/120    avg_loss:0.020, val_acc:0.990]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.029, val_acc:0.988]
Epoch [100/120    avg_loss:0.013, val_acc:0.990]
Epoch [101/120    avg_loss:0.014, val_acc:0.990]
Epoch [102/120    avg_loss:0.017, val_acc:0.990]
Epoch [103/120    avg_loss:0.027, val_acc:0.992]
Epoch [104/120    avg_loss:0.025, val_acc:0.988]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.018, val_acc:0.990]
Epoch [108/120    avg_loss:0.017, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.015, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.018, val_acc:0.990]
Epoch [113/120    avg_loss:0.016, val_acc:0.992]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.994]
Epoch [116/120    avg_loss:0.016, val_acc:0.994]
Epoch [117/120    avg_loss:0.012, val_acc:0.994]
Epoch [118/120    avg_loss:0.021, val_acc:0.994]
Epoch [119/120    avg_loss:0.013, val_acc:0.994]
Epoch [120/120    avg_loss:0.010, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  12   0   0   0   0   0   0   5   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 1.         1.         1.         0.9610984  0.9602649
 1.         1.         1.         1.         1.         0.99472296
 0.99007718 1.        ]

Kappa:
0.9950150293146037
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55123e2828>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.930, val_acc:0.615]
Epoch [2/120    avg_loss:1.197, val_acc:0.746]
Epoch [3/120    avg_loss:0.815, val_acc:0.742]
Epoch [4/120    avg_loss:0.729, val_acc:0.831]
Epoch [5/120    avg_loss:0.645, val_acc:0.869]
Epoch [6/120    avg_loss:0.495, val_acc:0.825]
Epoch [7/120    avg_loss:0.431, val_acc:0.902]
Epoch [8/120    avg_loss:0.454, val_acc:0.908]
Epoch [9/120    avg_loss:0.364, val_acc:0.881]
Epoch [10/120    avg_loss:0.345, val_acc:0.850]
Epoch [11/120    avg_loss:0.304, val_acc:0.925]
Epoch [12/120    avg_loss:0.312, val_acc:0.921]
Epoch [13/120    avg_loss:0.280, val_acc:0.898]
Epoch [14/120    avg_loss:0.221, val_acc:0.919]
Epoch [15/120    avg_loss:0.298, val_acc:0.890]
Epoch [16/120    avg_loss:0.320, val_acc:0.948]
Epoch [17/120    avg_loss:0.229, val_acc:0.931]
Epoch [18/120    avg_loss:0.215, val_acc:0.963]
Epoch [19/120    avg_loss:0.172, val_acc:0.948]
Epoch [20/120    avg_loss:0.169, val_acc:0.917]
Epoch [21/120    avg_loss:0.173, val_acc:0.963]
Epoch [22/120    avg_loss:0.149, val_acc:0.956]
Epoch [23/120    avg_loss:0.131, val_acc:0.973]
Epoch [24/120    avg_loss:0.154, val_acc:0.948]
Epoch [25/120    avg_loss:0.101, val_acc:0.971]
Epoch [26/120    avg_loss:0.100, val_acc:0.963]
Epoch [27/120    avg_loss:0.082, val_acc:0.944]
Epoch [28/120    avg_loss:0.121, val_acc:0.971]
Epoch [29/120    avg_loss:0.086, val_acc:0.948]
Epoch [30/120    avg_loss:0.101, val_acc:0.944]
Epoch [31/120    avg_loss:0.160, val_acc:0.958]
Epoch [32/120    avg_loss:0.090, val_acc:0.979]
Epoch [33/120    avg_loss:0.117, val_acc:0.956]
Epoch [34/120    avg_loss:0.109, val_acc:0.958]
Epoch [35/120    avg_loss:0.127, val_acc:0.969]
Epoch [36/120    avg_loss:0.085, val_acc:0.975]
Epoch [37/120    avg_loss:0.094, val_acc:0.965]
Epoch [38/120    avg_loss:0.084, val_acc:0.983]
Epoch [39/120    avg_loss:0.063, val_acc:0.979]
Epoch [40/120    avg_loss:0.081, val_acc:0.975]
Epoch [41/120    avg_loss:0.070, val_acc:0.963]
Epoch [42/120    avg_loss:0.047, val_acc:0.979]
Epoch [43/120    avg_loss:0.033, val_acc:0.981]
Epoch [44/120    avg_loss:0.061, val_acc:0.988]
Epoch [45/120    avg_loss:0.080, val_acc:0.969]
Epoch [46/120    avg_loss:0.052, val_acc:0.965]
Epoch [47/120    avg_loss:0.049, val_acc:0.979]
Epoch [48/120    avg_loss:0.032, val_acc:0.992]
Epoch [49/120    avg_loss:0.039, val_acc:0.985]
Epoch [50/120    avg_loss:0.091, val_acc:0.971]
Epoch [51/120    avg_loss:0.048, val_acc:0.990]
Epoch [52/120    avg_loss:0.040, val_acc:0.983]
Epoch [53/120    avg_loss:0.022, val_acc:0.990]
Epoch [54/120    avg_loss:0.030, val_acc:0.990]
Epoch [55/120    avg_loss:0.038, val_acc:0.942]
Epoch [56/120    avg_loss:0.058, val_acc:0.988]
Epoch [57/120    avg_loss:0.039, val_acc:0.992]
Epoch [58/120    avg_loss:0.032, val_acc:0.994]
Epoch [59/120    avg_loss:0.016, val_acc:0.994]
Epoch [60/120    avg_loss:0.019, val_acc:0.996]
Epoch [61/120    avg_loss:0.028, val_acc:0.994]
Epoch [62/120    avg_loss:0.031, val_acc:0.996]
Epoch [63/120    avg_loss:0.037, val_acc:0.940]
Epoch [64/120    avg_loss:0.044, val_acc:0.975]
Epoch [65/120    avg_loss:0.058, val_acc:0.985]
Epoch [66/120    avg_loss:0.047, val_acc:0.979]
Epoch [67/120    avg_loss:0.038, val_acc:0.983]
Epoch [68/120    avg_loss:0.033, val_acc:0.994]
Epoch [69/120    avg_loss:0.033, val_acc:0.983]
Epoch [70/120    avg_loss:0.024, val_acc:0.992]
Epoch [71/120    avg_loss:0.024, val_acc:0.996]
Epoch [72/120    avg_loss:0.010, val_acc:0.994]
Epoch [73/120    avg_loss:0.014, val_acc:0.992]
Epoch [74/120    avg_loss:0.005, val_acc:0.994]
Epoch [75/120    avg_loss:0.009, val_acc:0.992]
Epoch [76/120    avg_loss:0.030, val_acc:0.996]
Epoch [77/120    avg_loss:0.021, val_acc:0.992]
Epoch [78/120    avg_loss:0.011, val_acc:0.992]
Epoch [79/120    avg_loss:0.013, val_acc:0.992]
Epoch [80/120    avg_loss:0.005, val_acc:0.994]
Epoch [81/120    avg_loss:0.009, val_acc:0.992]
Epoch [82/120    avg_loss:0.009, val_acc:0.994]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.008, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.994]
Epoch [86/120    avg_loss:0.006, val_acc:0.994]
Epoch [87/120    avg_loss:0.005, val_acc:0.996]
Epoch [88/120    avg_loss:0.006, val_acc:0.994]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.011, val_acc:0.994]
Epoch [91/120    avg_loss:0.003, val_acc:0.994]
Epoch [92/120    avg_loss:0.015, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.994]
Epoch [94/120    avg_loss:0.009, val_acc:0.994]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.015, val_acc:0.958]
Epoch [97/120    avg_loss:0.019, val_acc:0.996]
Epoch [98/120    avg_loss:0.025, val_acc:0.990]
Epoch [99/120    avg_loss:0.059, val_acc:0.990]
Epoch [100/120    avg_loss:0.030, val_acc:0.973]
Epoch [101/120    avg_loss:0.019, val_acc:0.994]
Epoch [102/120    avg_loss:0.027, val_acc:0.985]
Epoch [103/120    avg_loss:0.028, val_acc:0.975]
Epoch [104/120    avg_loss:0.053, val_acc:0.992]
Epoch [105/120    avg_loss:0.041, val_acc:0.992]
Epoch [106/120    avg_loss:0.013, val_acc:0.992]
Epoch [107/120    avg_loss:0.008, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.998]
Epoch [110/120    avg_loss:0.058, val_acc:0.992]
Epoch [111/120    avg_loss:0.036, val_acc:0.990]
Epoch [112/120    avg_loss:0.079, val_acc:0.975]
Epoch [113/120    avg_loss:0.032, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.023, val_acc:0.981]
Epoch [119/120    avg_loss:0.016, val_acc:0.994]
Epoch [120/120    avg_loss:0.010, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   7   0   0   0   0 199   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99491649 0.98426966 1.         0.96760259 0.95
 0.98271605 0.96132597 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9931140996871399
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99b18c2710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.988, val_acc:0.591]
Epoch [2/120    avg_loss:1.277, val_acc:0.692]
Epoch [3/120    avg_loss:1.031, val_acc:0.819]
Epoch [4/120    avg_loss:0.765, val_acc:0.833]
Epoch [5/120    avg_loss:0.712, val_acc:0.861]
Epoch [6/120    avg_loss:0.701, val_acc:0.794]
Epoch [7/120    avg_loss:0.530, val_acc:0.867]
Epoch [8/120    avg_loss:0.524, val_acc:0.887]
Epoch [9/120    avg_loss:0.456, val_acc:0.915]
Epoch [10/120    avg_loss:0.413, val_acc:0.885]
Epoch [11/120    avg_loss:0.468, val_acc:0.881]
Epoch [12/120    avg_loss:0.447, val_acc:0.915]
Epoch [13/120    avg_loss:0.308, val_acc:0.917]
Epoch [14/120    avg_loss:0.341, val_acc:0.938]
Epoch [15/120    avg_loss:0.268, val_acc:0.925]
Epoch [16/120    avg_loss:0.248, val_acc:0.921]
Epoch [17/120    avg_loss:0.368, val_acc:0.913]
Epoch [18/120    avg_loss:0.272, val_acc:0.954]
Epoch [19/120    avg_loss:0.194, val_acc:0.925]
Epoch [20/120    avg_loss:0.216, val_acc:0.958]
Epoch [21/120    avg_loss:0.195, val_acc:0.927]
Epoch [22/120    avg_loss:0.249, val_acc:0.915]
Epoch [23/120    avg_loss:0.215, val_acc:0.952]
Epoch [24/120    avg_loss:0.196, val_acc:0.946]
Epoch [25/120    avg_loss:0.225, val_acc:0.931]
Epoch [26/120    avg_loss:0.201, val_acc:0.962]
Epoch [27/120    avg_loss:0.149, val_acc:0.946]
Epoch [28/120    avg_loss:0.104, val_acc:0.970]
Epoch [29/120    avg_loss:0.100, val_acc:0.956]
Epoch [30/120    avg_loss:0.132, val_acc:0.956]
Epoch [31/120    avg_loss:0.151, val_acc:0.942]
Epoch [32/120    avg_loss:0.187, val_acc:0.942]
Epoch [33/120    avg_loss:0.167, val_acc:0.954]
Epoch [34/120    avg_loss:0.112, val_acc:0.956]
Epoch [35/120    avg_loss:0.088, val_acc:0.968]
Epoch [36/120    avg_loss:0.058, val_acc:0.960]
Epoch [37/120    avg_loss:0.070, val_acc:0.972]
Epoch [38/120    avg_loss:0.061, val_acc:0.976]
Epoch [39/120    avg_loss:0.058, val_acc:0.970]
Epoch [40/120    avg_loss:0.060, val_acc:0.966]
Epoch [41/120    avg_loss:0.059, val_acc:0.970]
Epoch [42/120    avg_loss:0.089, val_acc:0.962]
Epoch [43/120    avg_loss:0.126, val_acc:0.964]
Epoch [44/120    avg_loss:0.116, val_acc:0.956]
Epoch [45/120    avg_loss:0.087, val_acc:0.960]
Epoch [46/120    avg_loss:0.063, val_acc:0.980]
Epoch [47/120    avg_loss:0.054, val_acc:0.962]
Epoch [48/120    avg_loss:0.081, val_acc:0.972]
Epoch [49/120    avg_loss:0.081, val_acc:0.976]
Epoch [50/120    avg_loss:0.079, val_acc:0.984]
Epoch [51/120    avg_loss:0.062, val_acc:0.962]
Epoch [52/120    avg_loss:0.094, val_acc:0.946]
Epoch [53/120    avg_loss:0.111, val_acc:0.976]
Epoch [54/120    avg_loss:0.060, val_acc:0.972]
Epoch [55/120    avg_loss:0.084, val_acc:0.956]
Epoch [56/120    avg_loss:0.068, val_acc:0.978]
Epoch [57/120    avg_loss:0.029, val_acc:0.982]
Epoch [58/120    avg_loss:0.039, val_acc:0.980]
Epoch [59/120    avg_loss:0.046, val_acc:0.980]
Epoch [60/120    avg_loss:0.061, val_acc:0.988]
Epoch [61/120    avg_loss:0.026, val_acc:0.984]
Epoch [62/120    avg_loss:0.030, val_acc:0.984]
Epoch [63/120    avg_loss:0.025, val_acc:0.970]
Epoch [64/120    avg_loss:0.057, val_acc:0.976]
Epoch [65/120    avg_loss:0.042, val_acc:0.984]
Epoch [66/120    avg_loss:0.047, val_acc:0.968]
Epoch [67/120    avg_loss:0.026, val_acc:0.984]
Epoch [68/120    avg_loss:0.017, val_acc:0.988]
Epoch [69/120    avg_loss:0.015, val_acc:0.986]
Epoch [70/120    avg_loss:0.011, val_acc:0.980]
Epoch [71/120    avg_loss:0.017, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.986]
Epoch [73/120    avg_loss:0.041, val_acc:0.988]
Epoch [74/120    avg_loss:0.021, val_acc:0.982]
Epoch [75/120    avg_loss:0.021, val_acc:0.976]
Epoch [76/120    avg_loss:0.032, val_acc:0.984]
Epoch [77/120    avg_loss:0.013, val_acc:0.984]
Epoch [78/120    avg_loss:0.014, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.988]
Epoch [80/120    avg_loss:0.023, val_acc:0.986]
Epoch [81/120    avg_loss:0.022, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.020, val_acc:0.986]
Epoch [84/120    avg_loss:0.019, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.013, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.003, val_acc:0.986]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.031, val_acc:0.990]
Epoch [114/120    avg_loss:0.021, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.992]
Epoch [116/120    avg_loss:0.012, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.014, val_acc:0.986]
Epoch [119/120    avg_loss:0.017, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   6   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   5   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0  12   0   0   0   0   0   0 376   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.9869186  1.         1.         0.96086957 0.92041522
 1.         1.         0.98429319 1.         1.         0.98305085
 0.98434004 1.        ]

Kappa:
0.9883682286473912
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3fbda1eda0>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.001, val_acc:0.595]
Epoch [2/120    avg_loss:1.166, val_acc:0.784]
Epoch [3/120    avg_loss:0.808, val_acc:0.740]
Epoch [4/120    avg_loss:0.711, val_acc:0.812]
Epoch [5/120    avg_loss:0.669, val_acc:0.778]
Epoch [6/120    avg_loss:0.628, val_acc:0.837]
Epoch [7/120    avg_loss:0.552, val_acc:0.869]
Epoch [8/120    avg_loss:0.576, val_acc:0.825]
Epoch [9/120    avg_loss:0.430, val_acc:0.875]
Epoch [10/120    avg_loss:0.494, val_acc:0.865]
Epoch [11/120    avg_loss:0.344, val_acc:0.893]
Epoch [12/120    avg_loss:0.335, val_acc:0.887]
Epoch [13/120    avg_loss:0.389, val_acc:0.881]
Epoch [14/120    avg_loss:0.370, val_acc:0.887]
Epoch [15/120    avg_loss:0.403, val_acc:0.883]
Epoch [16/120    avg_loss:0.368, val_acc:0.903]
Epoch [17/120    avg_loss:0.239, val_acc:0.950]
Epoch [18/120    avg_loss:0.286, val_acc:0.952]
Epoch [19/120    avg_loss:0.345, val_acc:0.899]
Epoch [20/120    avg_loss:0.234, val_acc:0.972]
Epoch [21/120    avg_loss:0.217, val_acc:0.962]
Epoch [22/120    avg_loss:0.192, val_acc:0.960]
Epoch [23/120    avg_loss:0.167, val_acc:0.964]
Epoch [24/120    avg_loss:0.172, val_acc:0.948]
Epoch [25/120    avg_loss:0.222, val_acc:0.907]
Epoch [26/120    avg_loss:0.167, val_acc:0.960]
Epoch [27/120    avg_loss:0.156, val_acc:0.972]
Epoch [28/120    avg_loss:0.128, val_acc:0.942]
Epoch [29/120    avg_loss:0.138, val_acc:0.907]
Epoch [30/120    avg_loss:0.203, val_acc:0.962]
Epoch [31/120    avg_loss:0.169, val_acc:0.978]
Epoch [32/120    avg_loss:0.127, val_acc:0.966]
Epoch [33/120    avg_loss:0.094, val_acc:0.909]
Epoch [34/120    avg_loss:0.128, val_acc:0.976]
Epoch [35/120    avg_loss:0.114, val_acc:0.962]
Epoch [36/120    avg_loss:0.154, val_acc:0.952]
Epoch [37/120    avg_loss:0.138, val_acc:0.988]
Epoch [38/120    avg_loss:0.071, val_acc:0.970]
Epoch [39/120    avg_loss:0.064, val_acc:0.966]
Epoch [40/120    avg_loss:0.072, val_acc:0.976]
Epoch [41/120    avg_loss:0.063, val_acc:0.976]
Epoch [42/120    avg_loss:0.071, val_acc:0.982]
Epoch [43/120    avg_loss:0.048, val_acc:0.980]
Epoch [44/120    avg_loss:0.070, val_acc:0.978]
Epoch [45/120    avg_loss:0.071, val_acc:0.976]
Epoch [46/120    avg_loss:0.107, val_acc:0.982]
Epoch [47/120    avg_loss:0.067, val_acc:0.992]
Epoch [48/120    avg_loss:0.053, val_acc:0.990]
Epoch [49/120    avg_loss:0.056, val_acc:0.988]
Epoch [50/120    avg_loss:0.083, val_acc:0.978]
Epoch [51/120    avg_loss:0.055, val_acc:0.954]
Epoch [52/120    avg_loss:0.079, val_acc:0.974]
Epoch [53/120    avg_loss:0.068, val_acc:0.976]
Epoch [54/120    avg_loss:0.087, val_acc:0.974]
Epoch [55/120    avg_loss:0.075, val_acc:0.988]
Epoch [56/120    avg_loss:0.030, val_acc:0.992]
Epoch [57/120    avg_loss:0.065, val_acc:0.980]
Epoch [58/120    avg_loss:0.066, val_acc:0.927]
Epoch [59/120    avg_loss:0.142, val_acc:0.962]
Epoch [60/120    avg_loss:0.081, val_acc:0.982]
Epoch [61/120    avg_loss:0.026, val_acc:0.982]
Epoch [62/120    avg_loss:0.036, val_acc:0.992]
Epoch [63/120    avg_loss:0.054, val_acc:0.982]
Epoch [64/120    avg_loss:0.032, val_acc:0.988]
Epoch [65/120    avg_loss:0.037, val_acc:0.982]
Epoch [66/120    avg_loss:0.050, val_acc:0.982]
Epoch [67/120    avg_loss:0.029, val_acc:0.986]
Epoch [68/120    avg_loss:0.050, val_acc:0.990]
Epoch [69/120    avg_loss:0.033, val_acc:0.986]
Epoch [70/120    avg_loss:0.037, val_acc:0.986]
Epoch [71/120    avg_loss:0.023, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.992]
Epoch [73/120    avg_loss:0.027, val_acc:0.990]
Epoch [74/120    avg_loss:0.048, val_acc:0.992]
Epoch [75/120    avg_loss:0.035, val_acc:0.990]
Epoch [76/120    avg_loss:0.029, val_acc:0.988]
Epoch [77/120    avg_loss:0.040, val_acc:0.984]
Epoch [78/120    avg_loss:0.080, val_acc:0.960]
Epoch [79/120    avg_loss:0.073, val_acc:0.986]
Epoch [80/120    avg_loss:0.035, val_acc:0.980]
Epoch [81/120    avg_loss:0.026, val_acc:0.994]
Epoch [82/120    avg_loss:0.032, val_acc:0.992]
Epoch [83/120    avg_loss:0.045, val_acc:0.980]
Epoch [84/120    avg_loss:0.067, val_acc:0.992]
Epoch [85/120    avg_loss:0.028, val_acc:0.996]
Epoch [86/120    avg_loss:0.036, val_acc:0.994]
Epoch [87/120    avg_loss:0.034, val_acc:0.968]
Epoch [88/120    avg_loss:0.035, val_acc:0.992]
Epoch [89/120    avg_loss:0.017, val_acc:0.996]
Epoch [90/120    avg_loss:0.010, val_acc:0.992]
Epoch [91/120    avg_loss:0.008, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.994]
Epoch [93/120    avg_loss:0.014, val_acc:0.994]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.015, val_acc:0.992]
Epoch [96/120    avg_loss:0.009, val_acc:0.992]
Epoch [97/120    avg_loss:0.007, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.033, val_acc:0.988]
Epoch [100/120    avg_loss:0.041, val_acc:0.980]
Epoch [101/120    avg_loss:0.012, val_acc:0.992]
Epoch [102/120    avg_loss:0.006, val_acc:0.992]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.010, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.009, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.992]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.009, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   2   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   7   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   2 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.9751693  0.96989967
 1.         1.         1.         1.         1.         0.9973545
 0.99116998 1.        ]

Kappa:
0.9959647950765742
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8a887d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.010, val_acc:0.517]
Epoch [2/120    avg_loss:1.300, val_acc:0.640]
Epoch [3/120    avg_loss:1.005, val_acc:0.760]
Epoch [4/120    avg_loss:0.776, val_acc:0.810]
Epoch [5/120    avg_loss:0.652, val_acc:0.806]
Epoch [6/120    avg_loss:0.639, val_acc:0.785]
Epoch [7/120    avg_loss:0.628, val_acc:0.831]
Epoch [8/120    avg_loss:0.546, val_acc:0.869]
Epoch [9/120    avg_loss:0.521, val_acc:0.890]
Epoch [10/120    avg_loss:0.546, val_acc:0.892]
Epoch [11/120    avg_loss:0.428, val_acc:0.894]
Epoch [12/120    avg_loss:0.534, val_acc:0.871]
Epoch [13/120    avg_loss:0.446, val_acc:0.935]
Epoch [14/120    avg_loss:0.371, val_acc:0.917]
Epoch [15/120    avg_loss:0.337, val_acc:0.942]
Epoch [16/120    avg_loss:0.349, val_acc:0.933]
Epoch [17/120    avg_loss:0.314, val_acc:0.927]
Epoch [18/120    avg_loss:0.298, val_acc:0.925]
Epoch [19/120    avg_loss:0.279, val_acc:0.938]
Epoch [20/120    avg_loss:0.259, val_acc:0.946]
Epoch [21/120    avg_loss:0.249, val_acc:0.944]
Epoch [22/120    avg_loss:0.301, val_acc:0.933]
Epoch [23/120    avg_loss:0.273, val_acc:0.942]
Epoch [24/120    avg_loss:0.190, val_acc:0.969]
Epoch [25/120    avg_loss:0.281, val_acc:0.963]
Epoch [26/120    avg_loss:0.223, val_acc:0.942]
Epoch [27/120    avg_loss:0.153, val_acc:0.963]
Epoch [28/120    avg_loss:0.190, val_acc:0.954]
Epoch [29/120    avg_loss:0.229, val_acc:0.960]
Epoch [30/120    avg_loss:0.136, val_acc:0.977]
Epoch [31/120    avg_loss:0.111, val_acc:0.971]
Epoch [32/120    avg_loss:0.125, val_acc:0.973]
Epoch [33/120    avg_loss:0.125, val_acc:0.985]
Epoch [34/120    avg_loss:0.117, val_acc:0.971]
Epoch [35/120    avg_loss:0.138, val_acc:0.975]
Epoch [36/120    avg_loss:0.099, val_acc:0.967]
Epoch [37/120    avg_loss:0.133, val_acc:0.912]
Epoch [38/120    avg_loss:0.140, val_acc:0.973]
Epoch [39/120    avg_loss:0.069, val_acc:0.979]
Epoch [40/120    avg_loss:0.074, val_acc:0.988]
Epoch [41/120    avg_loss:0.088, val_acc:0.983]
Epoch [42/120    avg_loss:0.051, val_acc:0.981]
Epoch [43/120    avg_loss:0.045, val_acc:0.990]
Epoch [44/120    avg_loss:0.033, val_acc:0.990]
Epoch [45/120    avg_loss:0.028, val_acc:0.992]
Epoch [46/120    avg_loss:0.031, val_acc:0.994]
Epoch [47/120    avg_loss:0.049, val_acc:0.981]
Epoch [48/120    avg_loss:0.053, val_acc:0.992]
Epoch [49/120    avg_loss:0.131, val_acc:0.977]
Epoch [50/120    avg_loss:0.116, val_acc:0.990]
Epoch [51/120    avg_loss:0.087, val_acc:0.975]
Epoch [52/120    avg_loss:0.052, val_acc:0.992]
Epoch [53/120    avg_loss:0.064, val_acc:0.981]
Epoch [54/120    avg_loss:0.088, val_acc:0.971]
Epoch [55/120    avg_loss:0.072, val_acc:0.985]
Epoch [56/120    avg_loss:0.099, val_acc:0.994]
Epoch [57/120    avg_loss:0.074, val_acc:0.985]
Epoch [58/120    avg_loss:0.142, val_acc:0.985]
Epoch [59/120    avg_loss:0.086, val_acc:0.973]
Epoch [60/120    avg_loss:0.093, val_acc:0.992]
Epoch [61/120    avg_loss:0.051, val_acc:0.990]
Epoch [62/120    avg_loss:0.040, val_acc:0.992]
Epoch [63/120    avg_loss:0.033, val_acc:0.981]
Epoch [64/120    avg_loss:0.039, val_acc:0.981]
Epoch [65/120    avg_loss:0.058, val_acc:0.975]
Epoch [66/120    avg_loss:0.039, val_acc:0.994]
Epoch [67/120    avg_loss:0.033, val_acc:0.983]
Epoch [68/120    avg_loss:0.040, val_acc:0.992]
Epoch [69/120    avg_loss:0.037, val_acc:0.992]
Epoch [70/120    avg_loss:0.025, val_acc:0.996]
Epoch [71/120    avg_loss:0.043, val_acc:0.990]
Epoch [72/120    avg_loss:0.043, val_acc:0.992]
Epoch [73/120    avg_loss:0.023, val_acc:0.994]
Epoch [74/120    avg_loss:0.033, val_acc:0.985]
Epoch [75/120    avg_loss:0.017, val_acc:0.994]
Epoch [76/120    avg_loss:0.014, val_acc:0.994]
Epoch [77/120    avg_loss:0.016, val_acc:0.996]
Epoch [78/120    avg_loss:0.012, val_acc:0.996]
Epoch [79/120    avg_loss:0.030, val_acc:0.996]
Epoch [80/120    avg_loss:0.017, val_acc:0.992]
Epoch [81/120    avg_loss:0.011, val_acc:0.998]
Epoch [82/120    avg_loss:0.010, val_acc:0.998]
Epoch [83/120    avg_loss:0.015, val_acc:1.000]
Epoch [84/120    avg_loss:0.017, val_acc:0.988]
Epoch [85/120    avg_loss:0.056, val_acc:0.969]
Epoch [86/120    avg_loss:0.017, val_acc:0.998]
Epoch [87/120    avg_loss:0.018, val_acc:0.992]
Epoch [88/120    avg_loss:0.022, val_acc:0.992]
Epoch [89/120    avg_loss:0.023, val_acc:0.990]
Epoch [90/120    avg_loss:0.023, val_acc:0.994]
Epoch [91/120    avg_loss:0.019, val_acc:0.992]
Epoch [92/120    avg_loss:0.021, val_acc:0.990]
Epoch [93/120    avg_loss:0.010, val_acc:0.996]
Epoch [94/120    avg_loss:0.005, val_acc:0.996]
Epoch [95/120    avg_loss:0.009, val_acc:1.000]
Epoch [96/120    avg_loss:0.022, val_acc:0.996]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.011, val_acc:0.988]
Epoch [99/120    avg_loss:0.032, val_acc:0.990]
Epoch [100/120    avg_loss:0.034, val_acc:0.992]
Epoch [101/120    avg_loss:0.019, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.021, val_acc:1.000]
Epoch [104/120    avg_loss:0.016, val_acc:0.983]
Epoch [105/120    avg_loss:0.018, val_acc:0.992]
Epoch [106/120    avg_loss:0.015, val_acc:0.994]
Epoch [107/120    avg_loss:0.013, val_acc:0.994]
Epoch [108/120    avg_loss:0.057, val_acc:0.998]
Epoch [109/120    avg_loss:0.020, val_acc:0.998]
Epoch [110/120    avg_loss:0.039, val_acc:0.985]
Epoch [111/120    avg_loss:0.019, val_acc:0.998]
Epoch [112/120    avg_loss:0.011, val_acc:0.998]
Epoch [113/120    avg_loss:0.027, val_acc:0.998]
Epoch [114/120    avg_loss:0.074, val_acc:0.988]
Epoch [115/120    avg_loss:0.085, val_acc:0.985]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.021, val_acc:0.996]
Epoch [118/120    avg_loss:0.012, val_acc:1.000]
Epoch [119/120    avg_loss:0.008, val_acc:1.000]
Epoch [120/120    avg_loss:0.015, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   7   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 222   1   0   0   0   0   0   0   4   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 0.99486427 0.9977221  0.99782135 0.96312364 0.93006993
 1.         1.         0.998713   1.         1.         1.
 0.99449945 1.        ]

Kappa:
0.9938287225040588
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c7e002748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.047, val_acc:0.623]
Epoch [2/120    avg_loss:1.174, val_acc:0.738]
Epoch [3/120    avg_loss:0.843, val_acc:0.796]
Epoch [4/120    avg_loss:0.718, val_acc:0.867]
Epoch [5/120    avg_loss:0.564, val_acc:0.887]
Epoch [6/120    avg_loss:0.652, val_acc:0.892]
Epoch [7/120    avg_loss:0.414, val_acc:0.919]
Epoch [8/120    avg_loss:0.439, val_acc:0.869]
Epoch [9/120    avg_loss:0.511, val_acc:0.923]
Epoch [10/120    avg_loss:0.371, val_acc:0.931]
Epoch [11/120    avg_loss:0.350, val_acc:0.942]
Epoch [12/120    avg_loss:0.283, val_acc:0.944]
Epoch [13/120    avg_loss:0.294, val_acc:0.940]
Epoch [14/120    avg_loss:0.227, val_acc:0.921]
Epoch [15/120    avg_loss:0.225, val_acc:0.958]
Epoch [16/120    avg_loss:0.261, val_acc:0.915]
Epoch [17/120    avg_loss:0.255, val_acc:0.965]
Epoch [18/120    avg_loss:0.216, val_acc:0.867]
Epoch [19/120    avg_loss:0.390, val_acc:0.929]
Epoch [20/120    avg_loss:0.290, val_acc:0.900]
Epoch [21/120    avg_loss:0.259, val_acc:0.938]
Epoch [22/120    avg_loss:0.154, val_acc:0.969]
Epoch [23/120    avg_loss:0.165, val_acc:0.940]
Epoch [24/120    avg_loss:0.175, val_acc:0.944]
Epoch [25/120    avg_loss:0.190, val_acc:0.952]
Epoch [26/120    avg_loss:0.151, val_acc:0.958]
Epoch [27/120    avg_loss:0.168, val_acc:0.948]
Epoch [28/120    avg_loss:0.182, val_acc:0.952]
Epoch [29/120    avg_loss:0.187, val_acc:0.965]
Epoch [30/120    avg_loss:0.097, val_acc:0.967]
Epoch [31/120    avg_loss:0.088, val_acc:0.977]
Epoch [32/120    avg_loss:0.057, val_acc:0.973]
Epoch [33/120    avg_loss:0.083, val_acc:0.979]
Epoch [34/120    avg_loss:0.071, val_acc:0.977]
Epoch [35/120    avg_loss:0.077, val_acc:0.990]
Epoch [36/120    avg_loss:0.126, val_acc:0.942]
Epoch [37/120    avg_loss:0.216, val_acc:0.944]
Epoch [38/120    avg_loss:0.264, val_acc:0.969]
Epoch [39/120    avg_loss:0.073, val_acc:0.975]
Epoch [40/120    avg_loss:0.064, val_acc:0.983]
Epoch [41/120    avg_loss:0.092, val_acc:0.977]
Epoch [42/120    avg_loss:0.053, val_acc:0.983]
Epoch [43/120    avg_loss:0.039, val_acc:0.979]
Epoch [44/120    avg_loss:0.050, val_acc:0.981]
Epoch [45/120    avg_loss:0.056, val_acc:0.981]
Epoch [46/120    avg_loss:0.048, val_acc:0.988]
Epoch [47/120    avg_loss:0.043, val_acc:0.985]
Epoch [48/120    avg_loss:0.032, val_acc:0.979]
Epoch [49/120    avg_loss:0.032, val_acc:0.985]
Epoch [50/120    avg_loss:0.021, val_acc:0.990]
Epoch [51/120    avg_loss:0.038, val_acc:0.988]
Epoch [52/120    avg_loss:0.025, val_acc:0.988]
Epoch [53/120    avg_loss:0.024, val_acc:0.988]
Epoch [54/120    avg_loss:0.020, val_acc:0.988]
Epoch [55/120    avg_loss:0.020, val_acc:0.988]
Epoch [56/120    avg_loss:0.016, val_acc:0.990]
Epoch [57/120    avg_loss:0.025, val_acc:0.990]
Epoch [58/120    avg_loss:0.022, val_acc:0.992]
Epoch [59/120    avg_loss:0.032, val_acc:0.990]
Epoch [60/120    avg_loss:0.021, val_acc:0.992]
Epoch [61/120    avg_loss:0.025, val_acc:0.992]
Epoch [62/120    avg_loss:0.020, val_acc:0.992]
Epoch [63/120    avg_loss:0.018, val_acc:0.992]
Epoch [64/120    avg_loss:0.021, val_acc:0.992]
Epoch [65/120    avg_loss:0.013, val_acc:0.992]
Epoch [66/120    avg_loss:0.013, val_acc:0.992]
Epoch [67/120    avg_loss:0.016, val_acc:0.992]
Epoch [68/120    avg_loss:0.012, val_acc:0.992]
Epoch [69/120    avg_loss:0.016, val_acc:0.992]
Epoch [70/120    avg_loss:0.018, val_acc:0.992]
Epoch [71/120    avg_loss:0.015, val_acc:0.992]
Epoch [72/120    avg_loss:0.008, val_acc:0.992]
Epoch [73/120    avg_loss:0.017, val_acc:0.992]
Epoch [74/120    avg_loss:0.018, val_acc:0.992]
Epoch [75/120    avg_loss:0.014, val_acc:0.992]
Epoch [76/120    avg_loss:0.019, val_acc:0.992]
Epoch [77/120    avg_loss:0.027, val_acc:0.992]
Epoch [78/120    avg_loss:0.007, val_acc:0.992]
Epoch [79/120    avg_loss:0.018, val_acc:0.992]
Epoch [80/120    avg_loss:0.026, val_acc:0.992]
Epoch [81/120    avg_loss:0.015, val_acc:0.990]
Epoch [82/120    avg_loss:0.024, val_acc:0.990]
Epoch [83/120    avg_loss:0.015, val_acc:0.990]
Epoch [84/120    avg_loss:0.010, val_acc:0.990]
Epoch [85/120    avg_loss:0.016, val_acc:0.990]
Epoch [86/120    avg_loss:0.015, val_acc:0.990]
Epoch [87/120    avg_loss:0.018, val_acc:0.990]
Epoch [88/120    avg_loss:0.028, val_acc:0.992]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.016, val_acc:0.992]
Epoch [91/120    avg_loss:0.010, val_acc:0.992]
Epoch [92/120    avg_loss:0.014, val_acc:0.992]
Epoch [93/120    avg_loss:0.017, val_acc:0.992]
Epoch [94/120    avg_loss:0.010, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.022, val_acc:0.994]
Epoch [97/120    avg_loss:0.010, val_acc:0.994]
Epoch [98/120    avg_loss:0.013, val_acc:0.994]
Epoch [99/120    avg_loss:0.026, val_acc:0.992]
Epoch [100/120    avg_loss:0.015, val_acc:0.990]
Epoch [101/120    avg_loss:0.026, val_acc:0.990]
Epoch [102/120    avg_loss:0.018, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.990]
Epoch [104/120    avg_loss:0.016, val_acc:0.992]
Epoch [105/120    avg_loss:0.013, val_acc:0.992]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.022, val_acc:0.990]
Epoch [109/120    avg_loss:0.014, val_acc:0.990]
Epoch [110/120    avg_loss:0.019, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.009, val_acc:0.990]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.020, val_acc:0.990]
Epoch [115/120    avg_loss:0.013, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.018, val_acc:0.990]
Epoch [119/120    avg_loss:0.010, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   6   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  10   0   0   0   0   0   0   4   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99414348 0.99319728 1.         0.95730337 0.93023256
 1.         0.98378378 0.99741602 1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9928791968138884
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b2eb90780>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.967, val_acc:0.577]
Epoch [2/120    avg_loss:1.235, val_acc:0.742]
Epoch [3/120    avg_loss:0.968, val_acc:0.780]
Epoch [4/120    avg_loss:0.788, val_acc:0.726]
Epoch [5/120    avg_loss:0.702, val_acc:0.760]
Epoch [6/120    avg_loss:0.670, val_acc:0.845]
Epoch [7/120    avg_loss:0.521, val_acc:0.879]
Epoch [8/120    avg_loss:0.456, val_acc:0.875]
Epoch [9/120    avg_loss:0.472, val_acc:0.879]
Epoch [10/120    avg_loss:0.366, val_acc:0.903]
Epoch [11/120    avg_loss:0.389, val_acc:0.929]
Epoch [12/120    avg_loss:0.333, val_acc:0.923]
Epoch [13/120    avg_loss:0.285, val_acc:0.935]
Epoch [14/120    avg_loss:0.330, val_acc:0.911]
Epoch [15/120    avg_loss:0.278, val_acc:0.923]
Epoch [16/120    avg_loss:0.248, val_acc:0.952]
Epoch [17/120    avg_loss:0.254, val_acc:0.948]
Epoch [18/120    avg_loss:0.239, val_acc:0.935]
Epoch [19/120    avg_loss:0.164, val_acc:0.966]
Epoch [20/120    avg_loss:0.149, val_acc:0.962]
Epoch [21/120    avg_loss:0.178, val_acc:0.927]
Epoch [22/120    avg_loss:0.212, val_acc:0.954]
Epoch [23/120    avg_loss:0.176, val_acc:0.946]
Epoch [24/120    avg_loss:0.195, val_acc:0.958]
Epoch [25/120    avg_loss:0.160, val_acc:0.966]
Epoch [26/120    avg_loss:0.101, val_acc:0.964]
Epoch [27/120    avg_loss:0.090, val_acc:0.966]
Epoch [28/120    avg_loss:0.144, val_acc:0.923]
Epoch [29/120    avg_loss:0.181, val_acc:0.962]
Epoch [30/120    avg_loss:0.146, val_acc:0.960]
Epoch [31/120    avg_loss:0.097, val_acc:0.978]
Epoch [32/120    avg_loss:0.055, val_acc:0.982]
Epoch [33/120    avg_loss:0.074, val_acc:0.944]
Epoch [34/120    avg_loss:0.159, val_acc:0.946]
Epoch [35/120    avg_loss:0.106, val_acc:0.964]
Epoch [36/120    avg_loss:0.059, val_acc:0.976]
Epoch [37/120    avg_loss:0.060, val_acc:0.948]
Epoch [38/120    avg_loss:0.087, val_acc:0.980]
Epoch [39/120    avg_loss:0.077, val_acc:0.966]
Epoch [40/120    avg_loss:0.060, val_acc:0.980]
Epoch [41/120    avg_loss:0.074, val_acc:0.988]
Epoch [42/120    avg_loss:0.054, val_acc:0.986]
Epoch [43/120    avg_loss:0.063, val_acc:0.976]
Epoch [44/120    avg_loss:0.043, val_acc:0.970]
Epoch [45/120    avg_loss:0.065, val_acc:0.972]
Epoch [46/120    avg_loss:0.069, val_acc:0.976]
Epoch [47/120    avg_loss:0.121, val_acc:0.964]
Epoch [48/120    avg_loss:0.075, val_acc:0.974]
Epoch [49/120    avg_loss:0.069, val_acc:0.948]
Epoch [50/120    avg_loss:0.048, val_acc:0.964]
Epoch [51/120    avg_loss:0.058, val_acc:0.986]
Epoch [52/120    avg_loss:0.024, val_acc:0.978]
Epoch [53/120    avg_loss:0.039, val_acc:0.990]
Epoch [54/120    avg_loss:0.031, val_acc:0.990]
Epoch [55/120    avg_loss:0.037, val_acc:0.992]
Epoch [56/120    avg_loss:0.045, val_acc:0.976]
Epoch [57/120    avg_loss:0.055, val_acc:0.986]
Epoch [58/120    avg_loss:0.027, val_acc:0.988]
Epoch [59/120    avg_loss:0.017, val_acc:0.986]
Epoch [60/120    avg_loss:0.030, val_acc:0.988]
Epoch [61/120    avg_loss:0.045, val_acc:0.990]
Epoch [62/120    avg_loss:0.025, val_acc:0.986]
Epoch [63/120    avg_loss:0.031, val_acc:0.994]
Epoch [64/120    avg_loss:0.014, val_acc:0.994]
Epoch [65/120    avg_loss:0.014, val_acc:0.994]
Epoch [66/120    avg_loss:0.028, val_acc:0.988]
Epoch [67/120    avg_loss:0.016, val_acc:0.992]
Epoch [68/120    avg_loss:0.020, val_acc:0.988]
Epoch [69/120    avg_loss:0.029, val_acc:0.986]
Epoch [70/120    avg_loss:0.039, val_acc:0.984]
Epoch [71/120    avg_loss:0.018, val_acc:0.992]
Epoch [72/120    avg_loss:0.017, val_acc:0.978]
Epoch [73/120    avg_loss:0.029, val_acc:0.992]
Epoch [74/120    avg_loss:0.032, val_acc:0.994]
Epoch [75/120    avg_loss:0.046, val_acc:0.982]
Epoch [76/120    avg_loss:0.027, val_acc:0.994]
Epoch [77/120    avg_loss:0.043, val_acc:0.988]
Epoch [78/120    avg_loss:0.015, val_acc:0.988]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.984]
Epoch [81/120    avg_loss:0.043, val_acc:0.925]
Epoch [82/120    avg_loss:0.086, val_acc:0.982]
Epoch [83/120    avg_loss:0.069, val_acc:0.978]
Epoch [84/120    avg_loss:0.046, val_acc:0.994]
Epoch [85/120    avg_loss:0.038, val_acc:0.992]
Epoch [86/120    avg_loss:0.020, val_acc:0.994]
Epoch [87/120    avg_loss:0.009, val_acc:0.992]
Epoch [88/120    avg_loss:0.020, val_acc:0.996]
Epoch [89/120    avg_loss:0.023, val_acc:0.988]
Epoch [90/120    avg_loss:0.017, val_acc:0.998]
Epoch [91/120    avg_loss:0.006, val_acc:0.998]
Epoch [92/120    avg_loss:0.012, val_acc:0.998]
Epoch [93/120    avg_loss:0.027, val_acc:0.996]
Epoch [94/120    avg_loss:0.014, val_acc:0.974]
Epoch [95/120    avg_loss:0.011, val_acc:0.992]
Epoch [96/120    avg_loss:0.009, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.996]
Epoch [98/120    avg_loss:0.004, val_acc:0.996]
Epoch [99/120    avg_loss:0.008, val_acc:0.998]
Epoch [100/120    avg_loss:0.005, val_acc:0.998]
Epoch [101/120    avg_loss:0.005, val_acc:0.994]
Epoch [102/120    avg_loss:0.003, val_acc:0.998]
Epoch [103/120    avg_loss:0.004, val_acc:0.998]
Epoch [104/120    avg_loss:0.009, val_acc:0.996]
Epoch [105/120    avg_loss:0.007, val_acc:1.000]
Epoch [106/120    avg_loss:0.009, val_acc:0.996]
Epoch [107/120    avg_loss:0.006, val_acc:1.000]
Epoch [108/120    avg_loss:0.007, val_acc:0.998]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.004, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.996]
Epoch [112/120    avg_loss:0.003, val_acc:0.998]
Epoch [113/120    avg_loss:0.006, val_acc:1.000]
Epoch [114/120    avg_loss:0.005, val_acc:1.000]
Epoch [115/120    avg_loss:0.019, val_acc:1.000]
Epoch [116/120    avg_loss:0.003, val_acc:1.000]
Epoch [117/120    avg_loss:0.003, val_acc:1.000]
Epoch [118/120    avg_loss:0.008, val_acc:0.998]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99926954 0.9977221  1.         0.96982759 0.94661922
 1.         0.99465241 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9950149831415932
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6fa1b94748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.951, val_acc:0.536]
Epoch [2/120    avg_loss:1.142, val_acc:0.744]
Epoch [3/120    avg_loss:0.883, val_acc:0.774]
Epoch [4/120    avg_loss:0.841, val_acc:0.752]
Epoch [5/120    avg_loss:0.751, val_acc:0.790]
Epoch [6/120    avg_loss:0.685, val_acc:0.853]
Epoch [7/120    avg_loss:0.584, val_acc:0.853]
Epoch [8/120    avg_loss:0.523, val_acc:0.897]
Epoch [9/120    avg_loss:0.437, val_acc:0.879]
Epoch [10/120    avg_loss:0.505, val_acc:0.911]
Epoch [11/120    avg_loss:0.356, val_acc:0.867]
Epoch [12/120    avg_loss:0.434, val_acc:0.897]
Epoch [13/120    avg_loss:0.299, val_acc:0.938]
Epoch [14/120    avg_loss:0.329, val_acc:0.923]
Epoch [15/120    avg_loss:0.267, val_acc:0.935]
Epoch [16/120    avg_loss:0.358, val_acc:0.881]
Epoch [17/120    avg_loss:0.318, val_acc:0.909]
Epoch [18/120    avg_loss:0.429, val_acc:0.935]
Epoch [19/120    avg_loss:0.228, val_acc:0.952]
Epoch [20/120    avg_loss:0.178, val_acc:0.948]
Epoch [21/120    avg_loss:0.278, val_acc:0.952]
Epoch [22/120    avg_loss:0.221, val_acc:0.946]
Epoch [23/120    avg_loss:0.193, val_acc:0.933]
Epoch [24/120    avg_loss:0.153, val_acc:0.958]
Epoch [25/120    avg_loss:0.209, val_acc:0.935]
Epoch [26/120    avg_loss:0.169, val_acc:0.948]
Epoch [27/120    avg_loss:0.157, val_acc:0.960]
Epoch [28/120    avg_loss:0.088, val_acc:0.931]
Epoch [29/120    avg_loss:0.138, val_acc:0.972]
Epoch [30/120    avg_loss:0.093, val_acc:0.968]
Epoch [31/120    avg_loss:0.091, val_acc:0.966]
Epoch [32/120    avg_loss:0.098, val_acc:0.950]
Epoch [33/120    avg_loss:0.116, val_acc:0.923]
Epoch [34/120    avg_loss:0.059, val_acc:0.972]
Epoch [35/120    avg_loss:0.130, val_acc:0.921]
Epoch [36/120    avg_loss:0.124, val_acc:0.958]
Epoch [37/120    avg_loss:0.058, val_acc:0.966]
Epoch [38/120    avg_loss:0.057, val_acc:0.968]
Epoch [39/120    avg_loss:0.051, val_acc:0.968]
Epoch [40/120    avg_loss:0.075, val_acc:0.958]
Epoch [41/120    avg_loss:0.076, val_acc:0.940]
Epoch [42/120    avg_loss:0.064, val_acc:0.974]
Epoch [43/120    avg_loss:0.066, val_acc:0.958]
Epoch [44/120    avg_loss:0.085, val_acc:0.978]
Epoch [45/120    avg_loss:0.087, val_acc:0.972]
Epoch [46/120    avg_loss:0.059, val_acc:0.974]
Epoch [47/120    avg_loss:0.037, val_acc:0.982]
Epoch [48/120    avg_loss:0.030, val_acc:0.978]
Epoch [49/120    avg_loss:0.037, val_acc:0.976]
Epoch [50/120    avg_loss:0.042, val_acc:0.972]
Epoch [51/120    avg_loss:0.051, val_acc:0.974]
Epoch [52/120    avg_loss:0.035, val_acc:0.974]
Epoch [53/120    avg_loss:0.039, val_acc:0.980]
Epoch [54/120    avg_loss:0.039, val_acc:0.980]
Epoch [55/120    avg_loss:0.034, val_acc:0.974]
Epoch [56/120    avg_loss:0.075, val_acc:0.990]
Epoch [57/120    avg_loss:0.025, val_acc:0.982]
Epoch [58/120    avg_loss:0.024, val_acc:0.980]
Epoch [59/120    avg_loss:0.026, val_acc:0.984]
Epoch [60/120    avg_loss:0.015, val_acc:0.982]
Epoch [61/120    avg_loss:0.018, val_acc:0.984]
Epoch [62/120    avg_loss:0.017, val_acc:0.986]
Epoch [63/120    avg_loss:0.018, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.990]
Epoch [65/120    avg_loss:0.008, val_acc:0.988]
Epoch [66/120    avg_loss:0.016, val_acc:0.978]
Epoch [67/120    avg_loss:0.014, val_acc:0.980]
Epoch [68/120    avg_loss:0.020, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.021, val_acc:0.972]
Epoch [71/120    avg_loss:0.019, val_acc:0.976]
Epoch [72/120    avg_loss:0.006, val_acc:0.984]
Epoch [73/120    avg_loss:0.026, val_acc:0.974]
Epoch [74/120    avg_loss:0.025, val_acc:0.982]
Epoch [75/120    avg_loss:0.013, val_acc:0.978]
Epoch [76/120    avg_loss:0.010, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.015, val_acc:0.986]
Epoch [85/120    avg_loss:0.005, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.004, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.020, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.012, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         1.         0.98678414 0.95074946 0.94326241
 1.         1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9945399877690058
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d7a683780>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.960, val_acc:0.637]
Epoch [2/120    avg_loss:1.197, val_acc:0.698]
Epoch [3/120    avg_loss:1.008, val_acc:0.742]
Epoch [4/120    avg_loss:0.768, val_acc:0.850]
Epoch [5/120    avg_loss:0.650, val_acc:0.810]
Epoch [6/120    avg_loss:0.685, val_acc:0.902]
Epoch [7/120    avg_loss:0.444, val_acc:0.873]
Epoch [8/120    avg_loss:0.470, val_acc:0.904]
Epoch [9/120    avg_loss:0.565, val_acc:0.852]
Epoch [10/120    avg_loss:0.619, val_acc:0.869]
Epoch [11/120    avg_loss:0.386, val_acc:0.925]
Epoch [12/120    avg_loss:0.298, val_acc:0.935]
Epoch [13/120    avg_loss:0.311, val_acc:0.894]
Epoch [14/120    avg_loss:0.493, val_acc:0.892]
Epoch [15/120    avg_loss:0.390, val_acc:0.858]
Epoch [16/120    avg_loss:0.346, val_acc:0.950]
Epoch [17/120    avg_loss:0.320, val_acc:0.940]
Epoch [18/120    avg_loss:0.283, val_acc:0.927]
Epoch [19/120    avg_loss:0.291, val_acc:0.865]
Epoch [20/120    avg_loss:0.206, val_acc:0.946]
Epoch [21/120    avg_loss:0.199, val_acc:0.942]
Epoch [22/120    avg_loss:0.204, val_acc:0.940]
Epoch [23/120    avg_loss:0.331, val_acc:0.950]
Epoch [24/120    avg_loss:0.231, val_acc:0.956]
Epoch [25/120    avg_loss:0.193, val_acc:0.948]
Epoch [26/120    avg_loss:0.165, val_acc:0.948]
Epoch [27/120    avg_loss:0.132, val_acc:0.935]
Epoch [28/120    avg_loss:0.170, val_acc:0.973]
Epoch [29/120    avg_loss:0.101, val_acc:0.977]
Epoch [30/120    avg_loss:0.170, val_acc:0.958]
Epoch [31/120    avg_loss:0.164, val_acc:0.946]
Epoch [32/120    avg_loss:0.182, val_acc:0.958]
Epoch [33/120    avg_loss:0.105, val_acc:0.975]
Epoch [34/120    avg_loss:0.090, val_acc:0.977]
Epoch [35/120    avg_loss:0.072, val_acc:0.996]
Epoch [36/120    avg_loss:0.058, val_acc:0.981]
Epoch [37/120    avg_loss:0.076, val_acc:0.990]
Epoch [38/120    avg_loss:0.054, val_acc:0.983]
Epoch [39/120    avg_loss:0.078, val_acc:0.979]
Epoch [40/120    avg_loss:0.089, val_acc:0.965]
Epoch [41/120    avg_loss:0.091, val_acc:0.981]
Epoch [42/120    avg_loss:0.060, val_acc:0.992]
Epoch [43/120    avg_loss:0.064, val_acc:0.985]
Epoch [44/120    avg_loss:0.047, val_acc:0.992]
Epoch [45/120    avg_loss:0.046, val_acc:0.992]
Epoch [46/120    avg_loss:0.047, val_acc:0.985]
Epoch [47/120    avg_loss:0.063, val_acc:0.965]
Epoch [48/120    avg_loss:0.038, val_acc:0.990]
Epoch [49/120    avg_loss:0.038, val_acc:0.990]
Epoch [50/120    avg_loss:0.030, val_acc:0.990]
Epoch [51/120    avg_loss:0.029, val_acc:0.990]
Epoch [52/120    avg_loss:0.047, val_acc:0.992]
Epoch [53/120    avg_loss:0.028, val_acc:0.996]
Epoch [54/120    avg_loss:0.017, val_acc:0.996]
Epoch [55/120    avg_loss:0.021, val_acc:0.996]
Epoch [56/120    avg_loss:0.025, val_acc:0.996]
Epoch [57/120    avg_loss:0.025, val_acc:0.994]
Epoch [58/120    avg_loss:0.024, val_acc:0.994]
Epoch [59/120    avg_loss:0.027, val_acc:0.994]
Epoch [60/120    avg_loss:0.016, val_acc:0.994]
Epoch [61/120    avg_loss:0.030, val_acc:0.992]
Epoch [62/120    avg_loss:0.019, val_acc:0.992]
Epoch [63/120    avg_loss:0.022, val_acc:0.994]
Epoch [64/120    avg_loss:0.015, val_acc:0.994]
Epoch [65/120    avg_loss:0.020, val_acc:0.994]
Epoch [66/120    avg_loss:0.024, val_acc:0.994]
Epoch [67/120    avg_loss:0.025, val_acc:0.994]
Epoch [68/120    avg_loss:0.021, val_acc:0.994]
Epoch [69/120    avg_loss:0.022, val_acc:0.994]
Epoch [70/120    avg_loss:0.015, val_acc:0.994]
Epoch [71/120    avg_loss:0.037, val_acc:0.994]
Epoch [72/120    avg_loss:0.018, val_acc:0.994]
Epoch [73/120    avg_loss:0.021, val_acc:0.994]
Epoch [74/120    avg_loss:0.022, val_acc:0.994]
Epoch [75/120    avg_loss:0.016, val_acc:0.994]
Epoch [76/120    avg_loss:0.026, val_acc:0.994]
Epoch [77/120    avg_loss:0.014, val_acc:0.994]
Epoch [78/120    avg_loss:0.019, val_acc:0.994]
Epoch [79/120    avg_loss:0.016, val_acc:0.994]
Epoch [80/120    avg_loss:0.016, val_acc:0.994]
Epoch [81/120    avg_loss:0.020, val_acc:0.994]
Epoch [82/120    avg_loss:0.017, val_acc:0.994]
Epoch [83/120    avg_loss:0.013, val_acc:0.994]
Epoch [84/120    avg_loss:0.018, val_acc:0.994]
Epoch [85/120    avg_loss:0.014, val_acc:0.994]
Epoch [86/120    avg_loss:0.012, val_acc:0.994]
Epoch [87/120    avg_loss:0.023, val_acc:0.994]
Epoch [88/120    avg_loss:0.020, val_acc:0.994]
Epoch [89/120    avg_loss:0.017, val_acc:0.994]
Epoch [90/120    avg_loss:0.018, val_acc:0.994]
Epoch [91/120    avg_loss:0.021, val_acc:0.994]
Epoch [92/120    avg_loss:0.016, val_acc:0.994]
Epoch [93/120    avg_loss:0.029, val_acc:0.994]
Epoch [94/120    avg_loss:0.016, val_acc:0.994]
Epoch [95/120    avg_loss:0.016, val_acc:0.994]
Epoch [96/120    avg_loss:0.014, val_acc:0.994]
Epoch [97/120    avg_loss:0.014, val_acc:0.994]
Epoch [98/120    avg_loss:0.017, val_acc:0.994]
Epoch [99/120    avg_loss:0.022, val_acc:0.994]
Epoch [100/120    avg_loss:0.024, val_acc:0.994]
Epoch [101/120    avg_loss:0.010, val_acc:0.994]
Epoch [102/120    avg_loss:0.012, val_acc:0.994]
Epoch [103/120    avg_loss:0.016, val_acc:0.994]
Epoch [104/120    avg_loss:0.015, val_acc:0.994]
Epoch [105/120    avg_loss:0.018, val_acc:0.994]
Epoch [106/120    avg_loss:0.031, val_acc:0.994]
Epoch [107/120    avg_loss:0.029, val_acc:0.994]
Epoch [108/120    avg_loss:0.012, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.013, val_acc:0.994]
Epoch [111/120    avg_loss:0.028, val_acc:0.994]
Epoch [112/120    avg_loss:0.017, val_acc:0.994]
Epoch [113/120    avg_loss:0.018, val_acc:0.994]
Epoch [114/120    avg_loss:0.013, val_acc:0.994]
Epoch [115/120    avg_loss:0.018, val_acc:0.994]
Epoch [116/120    avg_loss:0.017, val_acc:0.994]
Epoch [117/120    avg_loss:0.017, val_acc:0.994]
Epoch [118/120    avg_loss:0.032, val_acc:0.994]
Epoch [119/120    avg_loss:0.016, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99926954 1.         1.         0.96312364 0.93661972
 1.         1.         1.         1.         1.         0.98305085
 0.98544233 1.        ]

Kappa:
0.9926415302135054
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9638de07b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:1.977, val_acc:0.613]
Epoch [2/120    avg_loss:1.192, val_acc:0.735]
Epoch [3/120    avg_loss:0.950, val_acc:0.765]
Epoch [4/120    avg_loss:0.782, val_acc:0.731]
Epoch [5/120    avg_loss:0.722, val_acc:0.821]
Epoch [6/120    avg_loss:0.636, val_acc:0.854]
Epoch [7/120    avg_loss:0.448, val_acc:0.871]
Epoch [8/120    avg_loss:0.436, val_acc:0.863]
Epoch [9/120    avg_loss:0.510, val_acc:0.858]
Epoch [10/120    avg_loss:0.421, val_acc:0.825]
Epoch [11/120    avg_loss:0.538, val_acc:0.910]
Epoch [12/120    avg_loss:0.403, val_acc:0.919]
Epoch [13/120    avg_loss:0.387, val_acc:0.887]
Epoch [14/120    avg_loss:0.325, val_acc:0.923]
Epoch [15/120    avg_loss:0.332, val_acc:0.890]
Epoch [16/120    avg_loss:0.255, val_acc:0.925]
Epoch [17/120    avg_loss:0.225, val_acc:0.929]
Epoch [18/120    avg_loss:0.235, val_acc:0.921]
Epoch [19/120    avg_loss:0.229, val_acc:0.906]
Epoch [20/120    avg_loss:0.337, val_acc:0.898]
Epoch [21/120    avg_loss:0.347, val_acc:0.935]
Epoch [22/120    avg_loss:0.284, val_acc:0.919]
Epoch [23/120    avg_loss:0.210, val_acc:0.948]
Epoch [24/120    avg_loss:0.107, val_acc:0.971]
Epoch [25/120    avg_loss:0.191, val_acc:0.929]
Epoch [26/120    avg_loss:0.237, val_acc:0.940]
Epoch [27/120    avg_loss:0.176, val_acc:0.952]
Epoch [28/120    avg_loss:0.145, val_acc:0.965]
Epoch [29/120    avg_loss:0.122, val_acc:0.960]
Epoch [30/120    avg_loss:0.114, val_acc:0.963]
Epoch [31/120    avg_loss:0.065, val_acc:0.971]
Epoch [32/120    avg_loss:0.124, val_acc:0.954]
Epoch [33/120    avg_loss:0.154, val_acc:0.977]
Epoch [34/120    avg_loss:0.118, val_acc:0.975]
Epoch [35/120    avg_loss:0.094, val_acc:0.963]
Epoch [36/120    avg_loss:0.107, val_acc:0.971]
Epoch [37/120    avg_loss:0.101, val_acc:0.979]
Epoch [38/120    avg_loss:0.055, val_acc:0.975]
Epoch [39/120    avg_loss:0.065, val_acc:0.975]
Epoch [40/120    avg_loss:0.125, val_acc:0.977]
Epoch [41/120    avg_loss:0.051, val_acc:0.965]
Epoch [42/120    avg_loss:0.044, val_acc:0.967]
Epoch [43/120    avg_loss:0.055, val_acc:0.985]
Epoch [44/120    avg_loss:0.091, val_acc:0.977]
Epoch [45/120    avg_loss:0.126, val_acc:0.977]
Epoch [46/120    avg_loss:0.102, val_acc:0.958]
Epoch [47/120    avg_loss:0.186, val_acc:0.938]
Epoch [48/120    avg_loss:0.171, val_acc:0.975]
Epoch [49/120    avg_loss:0.069, val_acc:0.967]
Epoch [50/120    avg_loss:0.085, val_acc:0.990]
Epoch [51/120    avg_loss:0.068, val_acc:0.985]
Epoch [52/120    avg_loss:0.097, val_acc:0.965]
Epoch [53/120    avg_loss:0.048, val_acc:0.967]
Epoch [54/120    avg_loss:0.034, val_acc:0.981]
Epoch [55/120    avg_loss:0.048, val_acc:0.990]
Epoch [56/120    avg_loss:0.034, val_acc:0.983]
Epoch [57/120    avg_loss:0.019, val_acc:0.981]
Epoch [58/120    avg_loss:0.015, val_acc:0.990]
Epoch [59/120    avg_loss:0.018, val_acc:0.988]
Epoch [60/120    avg_loss:0.018, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.990]
Epoch [62/120    avg_loss:0.029, val_acc:0.988]
Epoch [63/120    avg_loss:0.021, val_acc:0.977]
Epoch [64/120    avg_loss:0.014, val_acc:0.990]
Epoch [65/120    avg_loss:0.028, val_acc:0.983]
Epoch [66/120    avg_loss:0.059, val_acc:0.956]
Epoch [67/120    avg_loss:0.030, val_acc:0.988]
Epoch [68/120    avg_loss:0.023, val_acc:0.990]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.992]
Epoch [71/120    avg_loss:0.012, val_acc:0.988]
Epoch [72/120    avg_loss:0.025, val_acc:0.990]
Epoch [73/120    avg_loss:0.012, val_acc:0.994]
Epoch [74/120    avg_loss:0.039, val_acc:0.958]
Epoch [75/120    avg_loss:0.043, val_acc:0.996]
Epoch [76/120    avg_loss:0.032, val_acc:0.994]
Epoch [77/120    avg_loss:0.017, val_acc:0.992]
Epoch [78/120    avg_loss:0.014, val_acc:0.990]
Epoch [79/120    avg_loss:0.010, val_acc:0.990]
Epoch [80/120    avg_loss:0.007, val_acc:0.992]
Epoch [81/120    avg_loss:0.021, val_acc:0.992]
Epoch [82/120    avg_loss:0.008, val_acc:0.992]
Epoch [83/120    avg_loss:0.009, val_acc:0.990]
Epoch [84/120    avg_loss:0.015, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.992]
Epoch [87/120    avg_loss:0.007, val_acc:0.996]
Epoch [88/120    avg_loss:0.006, val_acc:0.992]
Epoch [89/120    avg_loss:0.009, val_acc:0.992]
Epoch [90/120    avg_loss:0.012, val_acc:0.994]
Epoch [91/120    avg_loss:0.012, val_acc:0.994]
Epoch [92/120    avg_loss:0.006, val_acc:0.994]
Epoch [93/120    avg_loss:0.056, val_acc:0.967]
Epoch [94/120    avg_loss:0.036, val_acc:0.985]
Epoch [95/120    avg_loss:0.126, val_acc:0.963]
Epoch [96/120    avg_loss:0.085, val_acc:0.992]
Epoch [97/120    avg_loss:0.027, val_acc:0.996]
Epoch [98/120    avg_loss:0.019, val_acc:0.988]
Epoch [99/120    avg_loss:0.015, val_acc:0.992]
Epoch [100/120    avg_loss:0.039, val_acc:0.985]
Epoch [101/120    avg_loss:0.044, val_acc:0.992]
Epoch [102/120    avg_loss:0.021, val_acc:0.988]
Epoch [103/120    avg_loss:0.020, val_acc:0.977]
Epoch [104/120    avg_loss:0.037, val_acc:0.975]
Epoch [105/120    avg_loss:0.014, val_acc:0.992]
Epoch [106/120    avg_loss:0.014, val_acc:0.994]
Epoch [107/120    avg_loss:0.010, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.985]
Epoch [109/120    avg_loss:0.038, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.013, val_acc:0.990]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.994]
Epoch [116/120    avg_loss:0.004, val_acc:0.994]
Epoch [117/120    avg_loss:0.006, val_acc:0.994]
Epoch [118/120    avg_loss:0.006, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.004, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   4   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217  13   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   2   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99707174 1.         0.97091723 0.94117647 0.93661972
 1.         1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9924042367232176
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff05f0987f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.009, val_acc:0.608]
Epoch [2/120    avg_loss:1.213, val_acc:0.729]
Epoch [3/120    avg_loss:0.856, val_acc:0.790]
Epoch [4/120    avg_loss:0.902, val_acc:0.752]
Epoch [5/120    avg_loss:0.707, val_acc:0.840]
Epoch [6/120    avg_loss:0.593, val_acc:0.856]
Epoch [7/120    avg_loss:0.598, val_acc:0.881]
Epoch [8/120    avg_loss:0.436, val_acc:0.906]
Epoch [9/120    avg_loss:0.453, val_acc:0.900]
Epoch [10/120    avg_loss:0.401, val_acc:0.904]
Epoch [11/120    avg_loss:0.436, val_acc:0.917]
Epoch [12/120    avg_loss:0.368, val_acc:0.902]
Epoch [13/120    avg_loss:0.332, val_acc:0.921]
Epoch [14/120    avg_loss:0.287, val_acc:0.921]
Epoch [15/120    avg_loss:0.383, val_acc:0.935]
Epoch [16/120    avg_loss:0.345, val_acc:0.900]
Epoch [17/120    avg_loss:0.218, val_acc:0.958]
Epoch [18/120    avg_loss:0.217, val_acc:0.925]
Epoch [19/120    avg_loss:0.181, val_acc:0.938]
Epoch [20/120    avg_loss:0.191, val_acc:0.935]
Epoch [21/120    avg_loss:0.189, val_acc:0.954]
Epoch [22/120    avg_loss:0.198, val_acc:0.969]
Epoch [23/120    avg_loss:0.161, val_acc:0.938]
Epoch [24/120    avg_loss:0.134, val_acc:0.967]
Epoch [25/120    avg_loss:0.185, val_acc:0.940]
Epoch [26/120    avg_loss:0.183, val_acc:0.944]
Epoch [27/120    avg_loss:0.166, val_acc:0.971]
Epoch [28/120    avg_loss:0.183, val_acc:0.956]
Epoch [29/120    avg_loss:0.103, val_acc:0.963]
Epoch [30/120    avg_loss:0.096, val_acc:0.967]
Epoch [31/120    avg_loss:0.151, val_acc:0.938]
Epoch [32/120    avg_loss:0.200, val_acc:0.965]
Epoch [33/120    avg_loss:0.106, val_acc:0.967]
Epoch [34/120    avg_loss:0.155, val_acc:0.958]
Epoch [35/120    avg_loss:0.081, val_acc:0.958]
Epoch [36/120    avg_loss:0.201, val_acc:0.956]
Epoch [37/120    avg_loss:0.111, val_acc:0.967]
Epoch [38/120    avg_loss:0.088, val_acc:0.979]
Epoch [39/120    avg_loss:0.132, val_acc:0.967]
Epoch [40/120    avg_loss:0.166, val_acc:0.979]
Epoch [41/120    avg_loss:0.063, val_acc:0.981]
Epoch [42/120    avg_loss:0.034, val_acc:0.985]
Epoch [43/120    avg_loss:0.029, val_acc:0.981]
Epoch [44/120    avg_loss:0.070, val_acc:0.985]
Epoch [45/120    avg_loss:0.047, val_acc:0.988]
Epoch [46/120    avg_loss:0.039, val_acc:0.988]
Epoch [47/120    avg_loss:0.039, val_acc:0.977]
Epoch [48/120    avg_loss:0.076, val_acc:0.977]
Epoch [49/120    avg_loss:0.049, val_acc:0.988]
Epoch [50/120    avg_loss:0.068, val_acc:0.985]
Epoch [51/120    avg_loss:0.053, val_acc:0.965]
Epoch [52/120    avg_loss:0.050, val_acc:0.985]
Epoch [53/120    avg_loss:0.043, val_acc:0.973]
Epoch [54/120    avg_loss:0.061, val_acc:0.985]
Epoch [55/120    avg_loss:0.022, val_acc:0.992]
Epoch [56/120    avg_loss:0.019, val_acc:0.988]
Epoch [57/120    avg_loss:0.059, val_acc:0.988]
Epoch [58/120    avg_loss:0.032, val_acc:0.992]
Epoch [59/120    avg_loss:0.030, val_acc:0.994]
Epoch [60/120    avg_loss:0.036, val_acc:0.992]
Epoch [61/120    avg_loss:0.020, val_acc:0.988]
Epoch [62/120    avg_loss:0.053, val_acc:0.990]
Epoch [63/120    avg_loss:0.029, val_acc:0.988]
Epoch [64/120    avg_loss:0.097, val_acc:0.971]
Epoch [65/120    avg_loss:0.056, val_acc:0.981]
Epoch [66/120    avg_loss:0.047, val_acc:0.990]
Epoch [67/120    avg_loss:0.056, val_acc:0.988]
Epoch [68/120    avg_loss:0.041, val_acc:0.994]
Epoch [69/120    avg_loss:0.021, val_acc:0.988]
Epoch [70/120    avg_loss:0.018, val_acc:0.990]
Epoch [71/120    avg_loss:0.013, val_acc:0.994]
Epoch [72/120    avg_loss:0.040, val_acc:0.983]
Epoch [73/120    avg_loss:0.022, val_acc:0.988]
Epoch [74/120    avg_loss:0.012, val_acc:0.992]
Epoch [75/120    avg_loss:0.012, val_acc:0.992]
Epoch [76/120    avg_loss:0.010, val_acc:0.992]
Epoch [77/120    avg_loss:0.010, val_acc:0.990]
Epoch [78/120    avg_loss:0.030, val_acc:0.990]
Epoch [79/120    avg_loss:0.010, val_acc:0.992]
Epoch [80/120    avg_loss:0.008, val_acc:0.992]
Epoch [81/120    avg_loss:0.008, val_acc:0.992]
Epoch [82/120    avg_loss:0.010, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.994]
Epoch [84/120    avg_loss:0.007, val_acc:0.994]
Epoch [85/120    avg_loss:0.023, val_acc:0.992]
Epoch [86/120    avg_loss:0.013, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.990]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.992]
Epoch [91/120    avg_loss:0.009, val_acc:0.992]
Epoch [92/120    avg_loss:0.004, val_acc:0.992]
Epoch [93/120    avg_loss:0.016, val_acc:0.994]
Epoch [94/120    avg_loss:0.008, val_acc:0.994]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.004, val_acc:0.996]
Epoch [97/120    avg_loss:0.009, val_acc:0.994]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.012, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.996]
Epoch [101/120    avg_loss:0.004, val_acc:0.996]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.994]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.007, val_acc:0.996]
Epoch [106/120    avg_loss:0.012, val_acc:0.994]
Epoch [107/120    avg_loss:0.008, val_acc:0.994]
Epoch [108/120    avg_loss:0.015, val_acc:0.990]
Epoch [109/120    avg_loss:0.027, val_acc:0.994]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.996]
Epoch [112/120    avg_loss:0.003, val_acc:0.996]
Epoch [113/120    avg_loss:0.007, val_acc:0.996]
Epoch [114/120    avg_loss:0.022, val_acc:0.994]
Epoch [115/120    avg_loss:0.023, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.008, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   3   0   0   0   0   0   0   4   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         1.         1.         0.96069869 0.95035461
 1.         1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9957267964083223
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3bf65ca748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.141, val_acc:0.556]
Epoch [2/120    avg_loss:1.488, val_acc:0.708]
Epoch [3/120    avg_loss:1.100, val_acc:0.688]
Epoch [4/120    avg_loss:0.979, val_acc:0.831]
Epoch [5/120    avg_loss:0.807, val_acc:0.823]
Epoch [6/120    avg_loss:0.668, val_acc:0.768]
Epoch [7/120    avg_loss:0.590, val_acc:0.891]
Epoch [8/120    avg_loss:0.491, val_acc:0.887]
Epoch [9/120    avg_loss:0.523, val_acc:0.883]
Epoch [10/120    avg_loss:0.721, val_acc:0.869]
Epoch [11/120    avg_loss:0.602, val_acc:0.885]
Epoch [12/120    avg_loss:0.394, val_acc:0.907]
Epoch [13/120    avg_loss:0.349, val_acc:0.927]
Epoch [14/120    avg_loss:0.374, val_acc:0.909]
Epoch [15/120    avg_loss:0.347, val_acc:0.925]
Epoch [16/120    avg_loss:0.274, val_acc:0.925]
Epoch [17/120    avg_loss:0.264, val_acc:0.938]
Epoch [18/120    avg_loss:0.249, val_acc:0.942]
Epoch [19/120    avg_loss:0.284, val_acc:0.950]
Epoch [20/120    avg_loss:0.243, val_acc:0.935]
Epoch [21/120    avg_loss:0.218, val_acc:0.937]
Epoch [22/120    avg_loss:0.292, val_acc:0.925]
Epoch [23/120    avg_loss:0.308, val_acc:0.942]
Epoch [24/120    avg_loss:0.269, val_acc:0.917]
Epoch [25/120    avg_loss:0.236, val_acc:0.946]
Epoch [26/120    avg_loss:0.281, val_acc:0.929]
Epoch [27/120    avg_loss:0.202, val_acc:0.938]
Epoch [28/120    avg_loss:0.180, val_acc:0.958]
Epoch [29/120    avg_loss:0.167, val_acc:0.966]
Epoch [30/120    avg_loss:0.160, val_acc:0.940]
Epoch [31/120    avg_loss:0.213, val_acc:0.962]
Epoch [32/120    avg_loss:0.232, val_acc:0.950]
Epoch [33/120    avg_loss:0.117, val_acc:0.962]
Epoch [34/120    avg_loss:0.176, val_acc:0.952]
Epoch [35/120    avg_loss:0.157, val_acc:0.938]
Epoch [36/120    avg_loss:0.129, val_acc:0.966]
Epoch [37/120    avg_loss:0.132, val_acc:0.952]
Epoch [38/120    avg_loss:0.246, val_acc:0.935]
Epoch [39/120    avg_loss:0.127, val_acc:0.960]
Epoch [40/120    avg_loss:0.112, val_acc:0.962]
Epoch [41/120    avg_loss:0.082, val_acc:0.962]
Epoch [42/120    avg_loss:0.120, val_acc:0.944]
Epoch [43/120    avg_loss:0.072, val_acc:0.970]
Epoch [44/120    avg_loss:0.112, val_acc:0.970]
Epoch [45/120    avg_loss:0.115, val_acc:0.958]
Epoch [46/120    avg_loss:0.068, val_acc:0.962]
Epoch [47/120    avg_loss:0.116, val_acc:0.968]
Epoch [48/120    avg_loss:0.098, val_acc:0.968]
Epoch [49/120    avg_loss:0.062, val_acc:0.964]
Epoch [50/120    avg_loss:0.046, val_acc:0.978]
Epoch [51/120    avg_loss:0.047, val_acc:0.972]
Epoch [52/120    avg_loss:0.061, val_acc:0.980]
Epoch [53/120    avg_loss:0.067, val_acc:0.982]
Epoch [54/120    avg_loss:0.039, val_acc:0.968]
Epoch [55/120    avg_loss:0.043, val_acc:0.978]
Epoch [56/120    avg_loss:0.053, val_acc:0.980]
Epoch [57/120    avg_loss:0.134, val_acc:0.958]
Epoch [58/120    avg_loss:0.179, val_acc:0.935]
Epoch [59/120    avg_loss:0.237, val_acc:0.938]
Epoch [60/120    avg_loss:0.216, val_acc:0.966]
Epoch [61/120    avg_loss:0.097, val_acc:0.960]
Epoch [62/120    avg_loss:0.098, val_acc:0.962]
Epoch [63/120    avg_loss:0.079, val_acc:0.976]
Epoch [64/120    avg_loss:0.041, val_acc:0.980]
Epoch [65/120    avg_loss:0.090, val_acc:0.964]
Epoch [66/120    avg_loss:0.146, val_acc:0.972]
Epoch [67/120    avg_loss:0.064, val_acc:0.980]
Epoch [68/120    avg_loss:0.055, val_acc:0.982]
Epoch [69/120    avg_loss:0.046, val_acc:0.982]
Epoch [70/120    avg_loss:0.036, val_acc:0.982]
Epoch [71/120    avg_loss:0.044, val_acc:0.982]
Epoch [72/120    avg_loss:0.041, val_acc:0.980]
Epoch [73/120    avg_loss:0.032, val_acc:0.982]
Epoch [74/120    avg_loss:0.029, val_acc:0.982]
Epoch [75/120    avg_loss:0.026, val_acc:0.978]
Epoch [76/120    avg_loss:0.034, val_acc:0.982]
Epoch [77/120    avg_loss:0.033, val_acc:0.982]
Epoch [78/120    avg_loss:0.038, val_acc:0.980]
Epoch [79/120    avg_loss:0.035, val_acc:0.980]
Epoch [80/120    avg_loss:0.031, val_acc:0.980]
Epoch [81/120    avg_loss:0.027, val_acc:0.980]
Epoch [82/120    avg_loss:0.026, val_acc:0.980]
Epoch [83/120    avg_loss:0.021, val_acc:0.980]
Epoch [84/120    avg_loss:0.021, val_acc:0.980]
Epoch [85/120    avg_loss:0.020, val_acc:0.978]
Epoch [86/120    avg_loss:0.022, val_acc:0.980]
Epoch [87/120    avg_loss:0.020, val_acc:0.980]
Epoch [88/120    avg_loss:0.020, val_acc:0.980]
Epoch [89/120    avg_loss:0.028, val_acc:0.982]
Epoch [90/120    avg_loss:0.026, val_acc:0.982]
Epoch [91/120    avg_loss:0.028, val_acc:0.980]
Epoch [92/120    avg_loss:0.029, val_acc:0.980]
Epoch [93/120    avg_loss:0.027, val_acc:0.984]
Epoch [94/120    avg_loss:0.023, val_acc:0.984]
Epoch [95/120    avg_loss:0.025, val_acc:0.982]
Epoch [96/120    avg_loss:0.015, val_acc:0.984]
Epoch [97/120    avg_loss:0.021, val_acc:0.982]
Epoch [98/120    avg_loss:0.023, val_acc:0.982]
Epoch [99/120    avg_loss:0.019, val_acc:0.984]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.023, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.986]
Epoch [103/120    avg_loss:0.017, val_acc:0.984]
Epoch [104/120    avg_loss:0.033, val_acc:0.984]
Epoch [105/120    avg_loss:0.019, val_acc:0.984]
Epoch [106/120    avg_loss:0.017, val_acc:0.986]
Epoch [107/120    avg_loss:0.027, val_acc:0.982]
Epoch [108/120    avg_loss:0.027, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.024, val_acc:0.986]
Epoch [111/120    avg_loss:0.024, val_acc:0.984]
Epoch [112/120    avg_loss:0.019, val_acc:0.984]
Epoch [113/120    avg_loss:0.017, val_acc:0.982]
Epoch [114/120    avg_loss:0.032, val_acc:0.982]
Epoch [115/120    avg_loss:0.021, val_acc:0.982]
Epoch [116/120    avg_loss:0.023, val_acc:0.982]
Epoch [117/120    avg_loss:0.019, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.024, val_acc:0.984]
Epoch [120/120    avg_loss:0.014, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   4   0   0   0   1   2   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98642534 0.98454746 0.94692144 0.92418773
 1.         0.99470899 0.998713   0.9978678  1.         1.
 0.99445061 1.        ]

Kappa:
0.9919288699906771
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f63da9e56a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.147, val_acc:0.667]
Epoch [2/120    avg_loss:1.408, val_acc:0.756]
Epoch [3/120    avg_loss:0.983, val_acc:0.772]
Epoch [4/120    avg_loss:0.781, val_acc:0.829]
Epoch [5/120    avg_loss:0.670, val_acc:0.831]
Epoch [6/120    avg_loss:0.651, val_acc:0.881]
Epoch [7/120    avg_loss:0.492, val_acc:0.883]
Epoch [8/120    avg_loss:0.453, val_acc:0.877]
Epoch [9/120    avg_loss:0.568, val_acc:0.901]
Epoch [10/120    avg_loss:0.510, val_acc:0.897]
Epoch [11/120    avg_loss:0.492, val_acc:0.909]
Epoch [12/120    avg_loss:0.412, val_acc:0.891]
Epoch [13/120    avg_loss:0.453, val_acc:0.895]
Epoch [14/120    avg_loss:0.388, val_acc:0.909]
Epoch [15/120    avg_loss:0.294, val_acc:0.950]
Epoch [16/120    avg_loss:0.367, val_acc:0.935]
Epoch [17/120    avg_loss:0.352, val_acc:0.913]
Epoch [18/120    avg_loss:0.251, val_acc:0.933]
Epoch [19/120    avg_loss:0.243, val_acc:0.938]
Epoch [20/120    avg_loss:0.199, val_acc:0.937]
Epoch [21/120    avg_loss:0.239, val_acc:0.964]
Epoch [22/120    avg_loss:0.200, val_acc:0.948]
Epoch [23/120    avg_loss:0.271, val_acc:0.940]
Epoch [24/120    avg_loss:0.257, val_acc:0.933]
Epoch [25/120    avg_loss:0.273, val_acc:0.919]
Epoch [26/120    avg_loss:0.338, val_acc:0.921]
Epoch [27/120    avg_loss:0.244, val_acc:0.942]
Epoch [28/120    avg_loss:0.303, val_acc:0.915]
Epoch [29/120    avg_loss:0.284, val_acc:0.915]
Epoch [30/120    avg_loss:0.229, val_acc:0.964]
Epoch [31/120    avg_loss:0.178, val_acc:0.954]
Epoch [32/120    avg_loss:0.174, val_acc:0.956]
Epoch [33/120    avg_loss:0.263, val_acc:0.954]
Epoch [34/120    avg_loss:0.188, val_acc:0.942]
Epoch [35/120    avg_loss:0.153, val_acc:0.966]
Epoch [36/120    avg_loss:0.111, val_acc:0.964]
Epoch [37/120    avg_loss:0.146, val_acc:0.964]
Epoch [38/120    avg_loss:0.126, val_acc:0.966]
Epoch [39/120    avg_loss:0.142, val_acc:0.972]
Epoch [40/120    avg_loss:0.132, val_acc:0.970]
Epoch [41/120    avg_loss:0.165, val_acc:0.970]
Epoch [42/120    avg_loss:0.130, val_acc:0.970]
Epoch [43/120    avg_loss:0.091, val_acc:0.988]
Epoch [44/120    avg_loss:0.079, val_acc:0.982]
Epoch [45/120    avg_loss:0.081, val_acc:0.984]
Epoch [46/120    avg_loss:0.109, val_acc:0.960]
Epoch [47/120    avg_loss:0.174, val_acc:0.968]
Epoch [48/120    avg_loss:0.162, val_acc:0.970]
Epoch [49/120    avg_loss:0.099, val_acc:0.972]
Epoch [50/120    avg_loss:0.133, val_acc:0.982]
Epoch [51/120    avg_loss:0.102, val_acc:0.954]
Epoch [52/120    avg_loss:0.099, val_acc:0.972]
Epoch [53/120    avg_loss:0.069, val_acc:0.976]
Epoch [54/120    avg_loss:0.104, val_acc:0.982]
Epoch [55/120    avg_loss:0.061, val_acc:0.984]
Epoch [56/120    avg_loss:0.108, val_acc:0.976]
Epoch [57/120    avg_loss:0.079, val_acc:0.976]
Epoch [58/120    avg_loss:0.083, val_acc:0.986]
Epoch [59/120    avg_loss:0.084, val_acc:0.988]
Epoch [60/120    avg_loss:0.027, val_acc:0.988]
Epoch [61/120    avg_loss:0.047, val_acc:0.988]
Epoch [62/120    avg_loss:0.031, val_acc:0.988]
Epoch [63/120    avg_loss:0.062, val_acc:0.986]
Epoch [64/120    avg_loss:0.035, val_acc:0.984]
Epoch [65/120    avg_loss:0.027, val_acc:0.986]
Epoch [66/120    avg_loss:0.061, val_acc:0.988]
Epoch [67/120    avg_loss:0.042, val_acc:0.988]
Epoch [68/120    avg_loss:0.028, val_acc:0.990]
Epoch [69/120    avg_loss:0.047, val_acc:0.990]
Epoch [70/120    avg_loss:0.049, val_acc:0.986]
Epoch [71/120    avg_loss:0.025, val_acc:0.990]
Epoch [72/120    avg_loss:0.039, val_acc:0.988]
Epoch [73/120    avg_loss:0.054, val_acc:0.990]
Epoch [74/120    avg_loss:0.029, val_acc:0.990]
Epoch [75/120    avg_loss:0.049, val_acc:0.990]
Epoch [76/120    avg_loss:0.044, val_acc:0.990]
Epoch [77/120    avg_loss:0.036, val_acc:0.990]
Epoch [78/120    avg_loss:0.040, val_acc:0.990]
Epoch [79/120    avg_loss:0.023, val_acc:0.988]
Epoch [80/120    avg_loss:0.025, val_acc:0.990]
Epoch [81/120    avg_loss:0.036, val_acc:0.988]
Epoch [82/120    avg_loss:0.020, val_acc:0.988]
Epoch [83/120    avg_loss:0.030, val_acc:0.988]
Epoch [84/120    avg_loss:0.034, val_acc:0.990]
Epoch [85/120    avg_loss:0.051, val_acc:0.988]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.034, val_acc:0.990]
Epoch [88/120    avg_loss:0.027, val_acc:0.988]
Epoch [89/120    avg_loss:0.021, val_acc:0.988]
Epoch [90/120    avg_loss:0.027, val_acc:0.988]
Epoch [91/120    avg_loss:0.020, val_acc:0.988]
Epoch [92/120    avg_loss:0.030, val_acc:0.988]
Epoch [93/120    avg_loss:0.028, val_acc:0.988]
Epoch [94/120    avg_loss:0.029, val_acc:0.986]
Epoch [95/120    avg_loss:0.031, val_acc:0.988]
Epoch [96/120    avg_loss:0.027, val_acc:0.988]
Epoch [97/120    avg_loss:0.030, val_acc:0.988]
Epoch [98/120    avg_loss:0.032, val_acc:0.988]
Epoch [99/120    avg_loss:0.032, val_acc:0.986]
Epoch [100/120    avg_loss:0.027, val_acc:0.988]
Epoch [101/120    avg_loss:0.016, val_acc:0.988]
Epoch [102/120    avg_loss:0.034, val_acc:0.988]
Epoch [103/120    avg_loss:0.036, val_acc:0.988]
Epoch [104/120    avg_loss:0.030, val_acc:0.988]
Epoch [105/120    avg_loss:0.027, val_acc:0.988]
Epoch [106/120    avg_loss:0.019, val_acc:0.988]
Epoch [107/120    avg_loss:0.048, val_acc:0.988]
Epoch [108/120    avg_loss:0.023, val_acc:0.988]
Epoch [109/120    avg_loss:0.032, val_acc:0.988]
Epoch [110/120    avg_loss:0.025, val_acc:0.988]
Epoch [111/120    avg_loss:0.025, val_acc:0.988]
Epoch [112/120    avg_loss:0.018, val_acc:0.988]
Epoch [113/120    avg_loss:0.025, val_acc:0.988]
Epoch [114/120    avg_loss:0.018, val_acc:0.988]
Epoch [115/120    avg_loss:0.020, val_acc:0.988]
Epoch [116/120    avg_loss:0.024, val_acc:0.988]
Epoch [117/120    avg_loss:0.034, val_acc:0.988]
Epoch [118/120    avg_loss:0.018, val_acc:0.988]
Epoch [119/120    avg_loss:0.034, val_acc:0.988]
Epoch [120/120    avg_loss:0.024, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   2   0   0   0   0   3   0]
 [  0   0   0 219   4   0   0   0   5   2   0   0   0   0]
 [  0   0   0   0 213  12   0   0   0   0   0   0   2   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.97052154 0.97550111 0.93626374 0.92465753
 0.99756691 0.97326203 0.99359795 0.9978678  1.         1.
 0.98896247 1.        ]

Kappa:
0.9883675904193204
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf934897b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.104, val_acc:0.617]
Epoch [2/120    avg_loss:1.394, val_acc:0.776]
Epoch [3/120    avg_loss:1.005, val_acc:0.806]
Epoch [4/120    avg_loss:0.813, val_acc:0.804]
Epoch [5/120    avg_loss:0.719, val_acc:0.794]
Epoch [6/120    avg_loss:0.625, val_acc:0.885]
Epoch [7/120    avg_loss:0.561, val_acc:0.841]
Epoch [8/120    avg_loss:0.707, val_acc:0.839]
Epoch [9/120    avg_loss:0.535, val_acc:0.903]
Epoch [10/120    avg_loss:0.500, val_acc:0.921]
Epoch [11/120    avg_loss:0.427, val_acc:0.881]
Epoch [12/120    avg_loss:0.446, val_acc:0.911]
Epoch [13/120    avg_loss:0.455, val_acc:0.883]
Epoch [14/120    avg_loss:0.389, val_acc:0.921]
Epoch [15/120    avg_loss:0.353, val_acc:0.909]
Epoch [16/120    avg_loss:0.385, val_acc:0.899]
Epoch [17/120    avg_loss:0.333, val_acc:0.895]
Epoch [18/120    avg_loss:0.379, val_acc:0.907]
Epoch [19/120    avg_loss:0.300, val_acc:0.933]
Epoch [20/120    avg_loss:0.263, val_acc:0.938]
Epoch [21/120    avg_loss:0.257, val_acc:0.937]
Epoch [22/120    avg_loss:0.233, val_acc:0.931]
Epoch [23/120    avg_loss:0.197, val_acc:0.940]
Epoch [24/120    avg_loss:0.244, val_acc:0.933]
Epoch [25/120    avg_loss:0.202, val_acc:0.925]
Epoch [26/120    avg_loss:0.235, val_acc:0.938]
Epoch [27/120    avg_loss:0.301, val_acc:0.933]
Epoch [28/120    avg_loss:0.258, val_acc:0.931]
Epoch [29/120    avg_loss:0.270, val_acc:0.919]
Epoch [30/120    avg_loss:0.275, val_acc:0.925]
Epoch [31/120    avg_loss:0.149, val_acc:0.938]
Epoch [32/120    avg_loss:0.167, val_acc:0.940]
Epoch [33/120    avg_loss:0.165, val_acc:0.948]
Epoch [34/120    avg_loss:0.202, val_acc:0.937]
Epoch [35/120    avg_loss:0.200, val_acc:0.938]
Epoch [36/120    avg_loss:0.170, val_acc:0.940]
Epoch [37/120    avg_loss:0.145, val_acc:0.942]
Epoch [38/120    avg_loss:0.179, val_acc:0.952]
Epoch [39/120    avg_loss:0.125, val_acc:0.948]
Epoch [40/120    avg_loss:0.112, val_acc:0.942]
Epoch [41/120    avg_loss:0.099, val_acc:0.956]
Epoch [42/120    avg_loss:0.128, val_acc:0.958]
Epoch [43/120    avg_loss:0.130, val_acc:0.954]
Epoch [44/120    avg_loss:0.087, val_acc:0.962]
Epoch [45/120    avg_loss:0.113, val_acc:0.960]
Epoch [46/120    avg_loss:0.161, val_acc:0.962]
Epoch [47/120    avg_loss:0.113, val_acc:0.948]
Epoch [48/120    avg_loss:0.150, val_acc:0.938]
Epoch [49/120    avg_loss:0.114, val_acc:0.966]
Epoch [50/120    avg_loss:0.073, val_acc:0.958]
Epoch [51/120    avg_loss:0.201, val_acc:0.956]
Epoch [52/120    avg_loss:0.119, val_acc:0.966]
Epoch [53/120    avg_loss:0.117, val_acc:0.938]
Epoch [54/120    avg_loss:0.112, val_acc:0.956]
Epoch [55/120    avg_loss:0.175, val_acc:0.954]
Epoch [56/120    avg_loss:0.138, val_acc:0.964]
Epoch [57/120    avg_loss:0.101, val_acc:0.954]
Epoch [58/120    avg_loss:0.106, val_acc:0.952]
Epoch [59/120    avg_loss:0.076, val_acc:0.964]
Epoch [60/120    avg_loss:0.064, val_acc:0.962]
Epoch [61/120    avg_loss:0.047, val_acc:0.954]
Epoch [62/120    avg_loss:0.088, val_acc:0.974]
Epoch [63/120    avg_loss:0.069, val_acc:0.970]
Epoch [64/120    avg_loss:0.055, val_acc:0.972]
Epoch [65/120    avg_loss:0.034, val_acc:0.974]
Epoch [66/120    avg_loss:0.031, val_acc:0.972]
Epoch [67/120    avg_loss:0.051, val_acc:0.974]
Epoch [68/120    avg_loss:0.035, val_acc:0.976]
Epoch [69/120    avg_loss:0.028, val_acc:0.974]
Epoch [70/120    avg_loss:0.044, val_acc:0.968]
Epoch [71/120    avg_loss:0.038, val_acc:0.972]
Epoch [72/120    avg_loss:0.020, val_acc:0.974]
Epoch [73/120    avg_loss:0.022, val_acc:0.968]
Epoch [74/120    avg_loss:0.048, val_acc:0.960]
Epoch [75/120    avg_loss:0.077, val_acc:0.964]
Epoch [76/120    avg_loss:0.084, val_acc:0.956]
Epoch [77/120    avg_loss:0.092, val_acc:0.948]
Epoch [78/120    avg_loss:0.089, val_acc:0.978]
Epoch [79/120    avg_loss:0.033, val_acc:0.972]
Epoch [80/120    avg_loss:0.038, val_acc:0.970]
Epoch [81/120    avg_loss:0.049, val_acc:0.974]
Epoch [82/120    avg_loss:0.039, val_acc:0.970]
Epoch [83/120    avg_loss:0.053, val_acc:0.962]
Epoch [84/120    avg_loss:0.087, val_acc:0.972]
Epoch [85/120    avg_loss:0.083, val_acc:0.970]
Epoch [86/120    avg_loss:0.042, val_acc:0.974]
Epoch [87/120    avg_loss:0.031, val_acc:0.976]
Epoch [88/120    avg_loss:0.076, val_acc:0.974]
Epoch [89/120    avg_loss:0.056, val_acc:0.954]
Epoch [90/120    avg_loss:0.112, val_acc:0.944]
Epoch [91/120    avg_loss:0.072, val_acc:0.974]
Epoch [92/120    avg_loss:0.040, val_acc:0.978]
Epoch [93/120    avg_loss:0.034, val_acc:0.980]
Epoch [94/120    avg_loss:0.030, val_acc:0.980]
Epoch [95/120    avg_loss:0.025, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.980]
Epoch [97/120    avg_loss:0.021, val_acc:0.980]
Epoch [98/120    avg_loss:0.017, val_acc:0.980]
Epoch [99/120    avg_loss:0.021, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.980]
Epoch [102/120    avg_loss:0.025, val_acc:0.980]
Epoch [103/120    avg_loss:0.016, val_acc:0.980]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.015, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.014, val_acc:0.980]
Epoch [109/120    avg_loss:0.022, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.027, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.017, val_acc:0.980]
Epoch [115/120    avg_loss:0.012, val_acc:0.980]
Epoch [116/120    avg_loss:0.018, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.015, val_acc:0.978]
Epoch [119/120    avg_loss:0.018, val_acc:0.978]
Epoch [120/120    avg_loss:0.018, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 218   6   0   0   0   2   4   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0  21 427   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 1.         0.97727273 0.97321429 0.93723849 0.91176471
 1.         0.9893617  0.99742931 0.99574468 1.         0.97290323
 0.96715742 1.        ]

Kappa:
0.9840949096017082
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6c0e7116d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.002, val_acc:0.702]
Epoch [2/120    avg_loss:1.245, val_acc:0.690]
Epoch [3/120    avg_loss:1.017, val_acc:0.798]
Epoch [4/120    avg_loss:0.896, val_acc:0.800]
Epoch [5/120    avg_loss:0.685, val_acc:0.784]
Epoch [6/120    avg_loss:0.593, val_acc:0.889]
Epoch [7/120    avg_loss:0.526, val_acc:0.873]
Epoch [8/120    avg_loss:0.561, val_acc:0.877]
Epoch [9/120    avg_loss:0.486, val_acc:0.903]
Epoch [10/120    avg_loss:0.360, val_acc:0.927]
Epoch [11/120    avg_loss:0.371, val_acc:0.907]
Epoch [12/120    avg_loss:0.501, val_acc:0.901]
Epoch [13/120    avg_loss:0.415, val_acc:0.901]
Epoch [14/120    avg_loss:0.373, val_acc:0.935]
Epoch [15/120    avg_loss:0.299, val_acc:0.917]
Epoch [16/120    avg_loss:0.286, val_acc:0.911]
Epoch [17/120    avg_loss:0.314, val_acc:0.913]
Epoch [18/120    avg_loss:0.310, val_acc:0.903]
Epoch [19/120    avg_loss:0.342, val_acc:0.893]
Epoch [20/120    avg_loss:0.271, val_acc:0.954]
Epoch [21/120    avg_loss:0.292, val_acc:0.913]
Epoch [22/120    avg_loss:0.279, val_acc:0.948]
Epoch [23/120    avg_loss:0.245, val_acc:0.948]
Epoch [24/120    avg_loss:0.203, val_acc:0.956]
Epoch [25/120    avg_loss:0.185, val_acc:0.950]
Epoch [26/120    avg_loss:0.159, val_acc:0.968]
Epoch [27/120    avg_loss:0.321, val_acc:0.929]
Epoch [28/120    avg_loss:0.281, val_acc:0.925]
Epoch [29/120    avg_loss:0.263, val_acc:0.956]
Epoch [30/120    avg_loss:0.231, val_acc:0.944]
Epoch [31/120    avg_loss:0.145, val_acc:0.972]
Epoch [32/120    avg_loss:0.190, val_acc:0.960]
Epoch [33/120    avg_loss:0.165, val_acc:0.962]
Epoch [34/120    avg_loss:0.148, val_acc:0.952]
Epoch [35/120    avg_loss:0.180, val_acc:0.942]
Epoch [36/120    avg_loss:0.205, val_acc:0.944]
Epoch [37/120    avg_loss:0.176, val_acc:0.968]
Epoch [38/120    avg_loss:0.153, val_acc:0.956]
Epoch [39/120    avg_loss:0.161, val_acc:0.976]
Epoch [40/120    avg_loss:0.148, val_acc:0.962]
Epoch [41/120    avg_loss:0.171, val_acc:0.970]
Epoch [42/120    avg_loss:0.167, val_acc:0.958]
Epoch [43/120    avg_loss:0.116, val_acc:0.976]
Epoch [44/120    avg_loss:0.117, val_acc:0.978]
Epoch [45/120    avg_loss:0.120, val_acc:0.984]
Epoch [46/120    avg_loss:0.147, val_acc:0.958]
Epoch [47/120    avg_loss:0.120, val_acc:0.974]
Epoch [48/120    avg_loss:0.103, val_acc:0.964]
Epoch [49/120    avg_loss:0.176, val_acc:0.982]
Epoch [50/120    avg_loss:0.146, val_acc:0.966]
Epoch [51/120    avg_loss:0.077, val_acc:0.986]
Epoch [52/120    avg_loss:0.104, val_acc:0.966]
Epoch [53/120    avg_loss:0.082, val_acc:0.974]
Epoch [54/120    avg_loss:0.094, val_acc:0.952]
Epoch [55/120    avg_loss:0.131, val_acc:0.966]
Epoch [56/120    avg_loss:0.107, val_acc:0.970]
Epoch [57/120    avg_loss:0.168, val_acc:0.948]
Epoch [58/120    avg_loss:0.154, val_acc:0.972]
Epoch [59/120    avg_loss:0.060, val_acc:0.968]
Epoch [60/120    avg_loss:0.084, val_acc:0.970]
Epoch [61/120    avg_loss:0.067, val_acc:0.984]
Epoch [62/120    avg_loss:0.086, val_acc:0.976]
Epoch [63/120    avg_loss:0.080, val_acc:0.978]
Epoch [64/120    avg_loss:0.054, val_acc:0.982]
Epoch [65/120    avg_loss:0.059, val_acc:0.984]
Epoch [66/120    avg_loss:0.043, val_acc:0.988]
Epoch [67/120    avg_loss:0.025, val_acc:0.988]
Epoch [68/120    avg_loss:0.030, val_acc:0.986]
Epoch [69/120    avg_loss:0.031, val_acc:0.986]
Epoch [70/120    avg_loss:0.030, val_acc:0.984]
Epoch [71/120    avg_loss:0.033, val_acc:0.986]
Epoch [72/120    avg_loss:0.026, val_acc:0.986]
Epoch [73/120    avg_loss:0.026, val_acc:0.986]
Epoch [74/120    avg_loss:0.016, val_acc:0.986]
Epoch [75/120    avg_loss:0.020, val_acc:0.984]
Epoch [76/120    avg_loss:0.022, val_acc:0.984]
Epoch [77/120    avg_loss:0.027, val_acc:0.984]
Epoch [78/120    avg_loss:0.017, val_acc:0.982]
Epoch [79/120    avg_loss:0.016, val_acc:0.984]
Epoch [80/120    avg_loss:0.019, val_acc:0.988]
Epoch [81/120    avg_loss:0.024, val_acc:0.986]
Epoch [82/120    avg_loss:0.014, val_acc:0.986]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.025, val_acc:0.986]
Epoch [85/120    avg_loss:0.027, val_acc:0.982]
Epoch [86/120    avg_loss:0.029, val_acc:0.982]
Epoch [87/120    avg_loss:0.021, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.017, val_acc:0.984]
Epoch [90/120    avg_loss:0.022, val_acc:0.984]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.023, val_acc:0.984]
Epoch [93/120    avg_loss:0.020, val_acc:0.982]
Epoch [94/120    avg_loss:0.017, val_acc:0.984]
Epoch [95/120    avg_loss:0.022, val_acc:0.984]
Epoch [96/120    avg_loss:0.014, val_acc:0.984]
Epoch [97/120    avg_loss:0.019, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.984]
Epoch [99/120    avg_loss:0.014, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.028, val_acc:0.984]
Epoch [102/120    avg_loss:0.023, val_acc:0.984]
Epoch [103/120    avg_loss:0.020, val_acc:0.984]
Epoch [104/120    avg_loss:0.020, val_acc:0.984]
Epoch [105/120    avg_loss:0.018, val_acc:0.984]
Epoch [106/120    avg_loss:0.021, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.027, val_acc:0.982]
Epoch [109/120    avg_loss:0.018, val_acc:0.982]
Epoch [110/120    avg_loss:0.018, val_acc:0.982]
Epoch [111/120    avg_loss:0.024, val_acc:0.982]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.022, val_acc:0.982]
Epoch [114/120    avg_loss:0.020, val_acc:0.982]
Epoch [115/120    avg_loss:0.021, val_acc:0.982]
Epoch [116/120    avg_loss:0.019, val_acc:0.982]
Epoch [117/120    avg_loss:0.035, val_acc:0.982]
Epoch [118/120    avg_loss:0.016, val_acc:0.982]
Epoch [119/120    avg_loss:0.020, val_acc:0.982]
Epoch [120/120    avg_loss:0.018, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   3   0   0   0   0   3   0]
 [  0   0   0 226   3   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99854227 0.97482838 0.99122807 0.9254386  0.89655172
 0.99512195 0.98429319 1.         0.99893276 1.         1.
 0.99005525 1.        ]

Kappa:
0.9886053614325498
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2d0a0f6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.092, val_acc:0.591]
Epoch [2/120    avg_loss:1.355, val_acc:0.688]
Epoch [3/120    avg_loss:0.970, val_acc:0.714]
Epoch [4/120    avg_loss:0.906, val_acc:0.752]
Epoch [5/120    avg_loss:0.760, val_acc:0.766]
Epoch [6/120    avg_loss:0.683, val_acc:0.847]
Epoch [7/120    avg_loss:0.591, val_acc:0.847]
Epoch [8/120    avg_loss:0.535, val_acc:0.835]
Epoch [9/120    avg_loss:0.622, val_acc:0.883]
Epoch [10/120    avg_loss:0.429, val_acc:0.917]
Epoch [11/120    avg_loss:0.443, val_acc:0.875]
Epoch [12/120    avg_loss:0.376, val_acc:0.915]
Epoch [13/120    avg_loss:0.420, val_acc:0.881]
Epoch [14/120    avg_loss:0.416, val_acc:0.923]
Epoch [15/120    avg_loss:0.352, val_acc:0.905]
Epoch [16/120    avg_loss:0.291, val_acc:0.897]
Epoch [17/120    avg_loss:0.394, val_acc:0.944]
Epoch [18/120    avg_loss:0.372, val_acc:0.909]
Epoch [19/120    avg_loss:0.291, val_acc:0.952]
Epoch [20/120    avg_loss:0.264, val_acc:0.903]
Epoch [21/120    avg_loss:0.288, val_acc:0.935]
Epoch [22/120    avg_loss:0.280, val_acc:0.938]
Epoch [23/120    avg_loss:0.166, val_acc:0.948]
Epoch [24/120    avg_loss:0.188, val_acc:0.946]
Epoch [25/120    avg_loss:0.173, val_acc:0.964]
Epoch [26/120    avg_loss:0.210, val_acc:0.958]
Epoch [27/120    avg_loss:0.190, val_acc:0.950]
Epoch [28/120    avg_loss:0.178, val_acc:0.960]
Epoch [29/120    avg_loss:0.203, val_acc:0.946]
Epoch [30/120    avg_loss:0.263, val_acc:0.917]
Epoch [31/120    avg_loss:0.304, val_acc:0.921]
Epoch [32/120    avg_loss:0.326, val_acc:0.929]
Epoch [33/120    avg_loss:0.265, val_acc:0.948]
Epoch [34/120    avg_loss:0.170, val_acc:0.962]
Epoch [35/120    avg_loss:0.170, val_acc:0.950]
Epoch [36/120    avg_loss:0.243, val_acc:0.944]
Epoch [37/120    avg_loss:0.240, val_acc:0.942]
Epoch [38/120    avg_loss:0.174, val_acc:0.952]
Epoch [39/120    avg_loss:0.161, val_acc:0.970]
Epoch [40/120    avg_loss:0.113, val_acc:0.968]
Epoch [41/120    avg_loss:0.093, val_acc:0.970]
Epoch [42/120    avg_loss:0.066, val_acc:0.968]
Epoch [43/120    avg_loss:0.084, val_acc:0.964]
Epoch [44/120    avg_loss:0.078, val_acc:0.964]
Epoch [45/120    avg_loss:0.085, val_acc:0.964]
Epoch [46/120    avg_loss:0.074, val_acc:0.964]
Epoch [47/120    avg_loss:0.116, val_acc:0.966]
Epoch [48/120    avg_loss:0.087, val_acc:0.966]
Epoch [49/120    avg_loss:0.093, val_acc:0.964]
Epoch [50/120    avg_loss:0.070, val_acc:0.966]
Epoch [51/120    avg_loss:0.102, val_acc:0.964]
Epoch [52/120    avg_loss:0.053, val_acc:0.966]
Epoch [53/120    avg_loss:0.078, val_acc:0.964]
Epoch [54/120    avg_loss:0.065, val_acc:0.964]
Epoch [55/120    avg_loss:0.072, val_acc:0.964]
Epoch [56/120    avg_loss:0.080, val_acc:0.964]
Epoch [57/120    avg_loss:0.053, val_acc:0.964]
Epoch [58/120    avg_loss:0.056, val_acc:0.964]
Epoch [59/120    avg_loss:0.066, val_acc:0.964]
Epoch [60/120    avg_loss:0.076, val_acc:0.964]
Epoch [61/120    avg_loss:0.069, val_acc:0.966]
Epoch [62/120    avg_loss:0.063, val_acc:0.966]
Epoch [63/120    avg_loss:0.061, val_acc:0.966]
Epoch [64/120    avg_loss:0.069, val_acc:0.966]
Epoch [65/120    avg_loss:0.061, val_acc:0.964]
Epoch [66/120    avg_loss:0.063, val_acc:0.964]
Epoch [67/120    avg_loss:0.073, val_acc:0.964]
Epoch [68/120    avg_loss:0.062, val_acc:0.964]
Epoch [69/120    avg_loss:0.070, val_acc:0.964]
Epoch [70/120    avg_loss:0.074, val_acc:0.964]
Epoch [71/120    avg_loss:0.065, val_acc:0.964]
Epoch [72/120    avg_loss:0.071, val_acc:0.964]
Epoch [73/120    avg_loss:0.060, val_acc:0.964]
Epoch [74/120    avg_loss:0.093, val_acc:0.964]
Epoch [75/120    avg_loss:0.076, val_acc:0.964]
Epoch [76/120    avg_loss:0.050, val_acc:0.964]
Epoch [77/120    avg_loss:0.078, val_acc:0.964]
Epoch [78/120    avg_loss:0.073, val_acc:0.964]
Epoch [79/120    avg_loss:0.080, val_acc:0.964]
Epoch [80/120    avg_loss:0.070, val_acc:0.964]
Epoch [81/120    avg_loss:0.073, val_acc:0.964]
Epoch [82/120    avg_loss:0.068, val_acc:0.964]
Epoch [83/120    avg_loss:0.070, val_acc:0.964]
Epoch [84/120    avg_loss:0.089, val_acc:0.964]
Epoch [85/120    avg_loss:0.076, val_acc:0.964]
Epoch [86/120    avg_loss:0.059, val_acc:0.964]
Epoch [87/120    avg_loss:0.062, val_acc:0.964]
Epoch [88/120    avg_loss:0.080, val_acc:0.964]
Epoch [89/120    avg_loss:0.068, val_acc:0.964]
Epoch [90/120    avg_loss:0.051, val_acc:0.964]
Epoch [91/120    avg_loss:0.088, val_acc:0.964]
Epoch [92/120    avg_loss:0.075, val_acc:0.964]
Epoch [93/120    avg_loss:0.056, val_acc:0.964]
Epoch [94/120    avg_loss:0.071, val_acc:0.964]
Epoch [95/120    avg_loss:0.076, val_acc:0.964]
Epoch [96/120    avg_loss:0.053, val_acc:0.964]
Epoch [97/120    avg_loss:0.075, val_acc:0.964]
Epoch [98/120    avg_loss:0.059, val_acc:0.964]
Epoch [99/120    avg_loss:0.058, val_acc:0.964]
Epoch [100/120    avg_loss:0.070, val_acc:0.964]
Epoch [101/120    avg_loss:0.078, val_acc:0.964]
Epoch [102/120    avg_loss:0.058, val_acc:0.964]
Epoch [103/120    avg_loss:0.061, val_acc:0.964]
Epoch [104/120    avg_loss:0.047, val_acc:0.964]
Epoch [105/120    avg_loss:0.061, val_acc:0.964]
Epoch [106/120    avg_loss:0.084, val_acc:0.964]
Epoch [107/120    avg_loss:0.068, val_acc:0.964]
Epoch [108/120    avg_loss:0.083, val_acc:0.964]
Epoch [109/120    avg_loss:0.068, val_acc:0.964]
Epoch [110/120    avg_loss:0.064, val_acc:0.964]
Epoch [111/120    avg_loss:0.060, val_acc:0.964]
Epoch [112/120    avg_loss:0.073, val_acc:0.964]
Epoch [113/120    avg_loss:0.066, val_acc:0.964]
Epoch [114/120    avg_loss:0.065, val_acc:0.964]
Epoch [115/120    avg_loss:0.058, val_acc:0.964]
Epoch [116/120    avg_loss:0.055, val_acc:0.964]
Epoch [117/120    avg_loss:0.077, val_acc:0.964]
Epoch [118/120    avg_loss:0.055, val_acc:0.964]
Epoch [119/120    avg_loss:0.076, val_acc:0.964]
Epoch [120/120    avg_loss:0.106, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   5   0   0   0   0   3   0]
 [  0   0   0 219   5   0   0   0   4   2   0   0   0   0]
 [  0   0   0   0 209  15   0   0   0   0   0   0   3   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   8   0   0   5   0 193   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   3   0   0   0   0   0   0   0   0   9 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.97441364605544

F1 scores:
[       nan 0.99419448 0.93777778 0.97550111 0.9047619  0.89273356
 0.96741855 0.875      0.99487179 0.9978678  1.         0.98820446
 0.98       1.        ]

Kappa:
0.9774412190723696
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f371d1e6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.079, val_acc:0.643]
Epoch [2/120    avg_loss:1.280, val_acc:0.782]
Epoch [3/120    avg_loss:0.971, val_acc:0.710]
Epoch [4/120    avg_loss:0.866, val_acc:0.776]
Epoch [5/120    avg_loss:0.685, val_acc:0.867]
Epoch [6/120    avg_loss:0.645, val_acc:0.851]
Epoch [7/120    avg_loss:0.609, val_acc:0.891]
Epoch [8/120    avg_loss:0.636, val_acc:0.903]
Epoch [9/120    avg_loss:0.467, val_acc:0.889]
Epoch [10/120    avg_loss:0.516, val_acc:0.845]
Epoch [11/120    avg_loss:0.543, val_acc:0.921]
Epoch [12/120    avg_loss:0.398, val_acc:0.895]
Epoch [13/120    avg_loss:0.436, val_acc:0.909]
Epoch [14/120    avg_loss:0.387, val_acc:0.917]
Epoch [15/120    avg_loss:0.410, val_acc:0.889]
Epoch [16/120    avg_loss:0.340, val_acc:0.905]
Epoch [17/120    avg_loss:0.290, val_acc:0.937]
Epoch [18/120    avg_loss:0.337, val_acc:0.911]
Epoch [19/120    avg_loss:0.306, val_acc:0.946]
Epoch [20/120    avg_loss:0.222, val_acc:0.946]
Epoch [21/120    avg_loss:0.174, val_acc:0.919]
Epoch [22/120    avg_loss:0.243, val_acc:0.937]
Epoch [23/120    avg_loss:0.215, val_acc:0.948]
Epoch [24/120    avg_loss:0.170, val_acc:0.950]
Epoch [25/120    avg_loss:0.176, val_acc:0.946]
Epoch [26/120    avg_loss:0.179, val_acc:0.946]
Epoch [27/120    avg_loss:0.134, val_acc:0.954]
Epoch [28/120    avg_loss:0.209, val_acc:0.950]
Epoch [29/120    avg_loss:0.228, val_acc:0.929]
Epoch [30/120    avg_loss:0.327, val_acc:0.929]
Epoch [31/120    avg_loss:0.209, val_acc:0.950]
Epoch [32/120    avg_loss:0.158, val_acc:0.960]
Epoch [33/120    avg_loss:0.118, val_acc:0.956]
Epoch [34/120    avg_loss:0.198, val_acc:0.944]
Epoch [35/120    avg_loss:0.215, val_acc:0.909]
Epoch [36/120    avg_loss:0.249, val_acc:0.948]
Epoch [37/120    avg_loss:0.175, val_acc:0.952]
Epoch [38/120    avg_loss:0.154, val_acc:0.942]
Epoch [39/120    avg_loss:0.149, val_acc:0.950]
Epoch [40/120    avg_loss:0.128, val_acc:0.958]
Epoch [41/120    avg_loss:0.149, val_acc:0.954]
Epoch [42/120    avg_loss:0.123, val_acc:0.958]
Epoch [43/120    avg_loss:0.107, val_acc:0.960]
Epoch [44/120    avg_loss:0.117, val_acc:0.982]
Epoch [45/120    avg_loss:0.091, val_acc:0.974]
Epoch [46/120    avg_loss:0.127, val_acc:0.954]
Epoch [47/120    avg_loss:0.095, val_acc:0.956]
Epoch [48/120    avg_loss:0.058, val_acc:0.950]
Epoch [49/120    avg_loss:0.130, val_acc:0.956]
Epoch [50/120    avg_loss:0.173, val_acc:0.954]
Epoch [51/120    avg_loss:0.099, val_acc:0.960]
Epoch [52/120    avg_loss:0.074, val_acc:0.974]
Epoch [53/120    avg_loss:0.087, val_acc:0.972]
Epoch [54/120    avg_loss:0.157, val_acc:0.931]
Epoch [55/120    avg_loss:0.146, val_acc:0.946]
Epoch [56/120    avg_loss:0.138, val_acc:0.956]
Epoch [57/120    avg_loss:0.207, val_acc:0.966]
Epoch [58/120    avg_loss:0.142, val_acc:0.980]
Epoch [59/120    avg_loss:0.074, val_acc:0.984]
Epoch [60/120    avg_loss:0.065, val_acc:0.986]
Epoch [61/120    avg_loss:0.035, val_acc:0.984]
Epoch [62/120    avg_loss:0.071, val_acc:0.986]
Epoch [63/120    avg_loss:0.046, val_acc:0.988]
Epoch [64/120    avg_loss:0.043, val_acc:0.990]
Epoch [65/120    avg_loss:0.038, val_acc:0.990]
Epoch [66/120    avg_loss:0.043, val_acc:0.990]
Epoch [67/120    avg_loss:0.048, val_acc:0.992]
Epoch [68/120    avg_loss:0.056, val_acc:0.994]
Epoch [69/120    avg_loss:0.047, val_acc:0.992]
Epoch [70/120    avg_loss:0.033, val_acc:0.994]
Epoch [71/120    avg_loss:0.047, val_acc:0.992]
Epoch [72/120    avg_loss:0.054, val_acc:0.992]
Epoch [73/120    avg_loss:0.050, val_acc:0.992]
Epoch [74/120    avg_loss:0.043, val_acc:0.990]
Epoch [75/120    avg_loss:0.045, val_acc:0.994]
Epoch [76/120    avg_loss:0.038, val_acc:0.990]
Epoch [77/120    avg_loss:0.048, val_acc:0.988]
Epoch [78/120    avg_loss:0.028, val_acc:0.988]
Epoch [79/120    avg_loss:0.028, val_acc:0.990]
Epoch [80/120    avg_loss:0.025, val_acc:0.990]
Epoch [81/120    avg_loss:0.046, val_acc:0.988]
Epoch [82/120    avg_loss:0.041, val_acc:0.990]
Epoch [83/120    avg_loss:0.036, val_acc:0.988]
Epoch [84/120    avg_loss:0.034, val_acc:0.990]
Epoch [85/120    avg_loss:0.027, val_acc:0.990]
Epoch [86/120    avg_loss:0.058, val_acc:0.990]
Epoch [87/120    avg_loss:0.030, val_acc:0.986]
Epoch [88/120    avg_loss:0.036, val_acc:0.986]
Epoch [89/120    avg_loss:0.027, val_acc:0.986]
Epoch [90/120    avg_loss:0.044, val_acc:0.986]
Epoch [91/120    avg_loss:0.034, val_acc:0.986]
Epoch [92/120    avg_loss:0.024, val_acc:0.984]
Epoch [93/120    avg_loss:0.025, val_acc:0.984]
Epoch [94/120    avg_loss:0.026, val_acc:0.984]
Epoch [95/120    avg_loss:0.043, val_acc:0.984]
Epoch [96/120    avg_loss:0.027, val_acc:0.984]
Epoch [97/120    avg_loss:0.030, val_acc:0.986]
Epoch [98/120    avg_loss:0.034, val_acc:0.986]
Epoch [99/120    avg_loss:0.039, val_acc:0.988]
Epoch [100/120    avg_loss:0.030, val_acc:0.988]
Epoch [101/120    avg_loss:0.037, val_acc:0.988]
Epoch [102/120    avg_loss:0.038, val_acc:0.988]
Epoch [103/120    avg_loss:0.034, val_acc:0.988]
Epoch [104/120    avg_loss:0.023, val_acc:0.988]
Epoch [105/120    avg_loss:0.023, val_acc:0.988]
Epoch [106/120    avg_loss:0.035, val_acc:0.988]
Epoch [107/120    avg_loss:0.052, val_acc:0.988]
Epoch [108/120    avg_loss:0.022, val_acc:0.988]
Epoch [109/120    avg_loss:0.042, val_acc:0.988]
Epoch [110/120    avg_loss:0.029, val_acc:0.988]
Epoch [111/120    avg_loss:0.046, val_acc:0.988]
Epoch [112/120    avg_loss:0.035, val_acc:0.988]
Epoch [113/120    avg_loss:0.032, val_acc:0.988]
Epoch [114/120    avg_loss:0.027, val_acc:0.988]
Epoch [115/120    avg_loss:0.035, val_acc:0.988]
Epoch [116/120    avg_loss:0.032, val_acc:0.988]
Epoch [117/120    avg_loss:0.052, val_acc:0.988]
Epoch [118/120    avg_loss:0.038, val_acc:0.988]
Epoch [119/120    avg_loss:0.036, val_acc:0.988]
Epoch [120/120    avg_loss:0.038, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 222   6   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   5 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.95982143 0.98230088 0.93186813 0.93687708
 0.98522167 0.94444444 0.998713   0.99893276 1.         0.99341238
 0.98553949 1.        ]

Kappa:
0.9867068040472377
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9548d5710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.078, val_acc:0.577]
Epoch [2/120    avg_loss:1.294, val_acc:0.762]
Epoch [3/120    avg_loss:0.921, val_acc:0.776]
Epoch [4/120    avg_loss:0.897, val_acc:0.837]
Epoch [5/120    avg_loss:0.782, val_acc:0.734]
Epoch [6/120    avg_loss:0.705, val_acc:0.875]
Epoch [7/120    avg_loss:0.623, val_acc:0.849]
Epoch [8/120    avg_loss:0.511, val_acc:0.883]
Epoch [9/120    avg_loss:0.516, val_acc:0.871]
Epoch [10/120    avg_loss:0.394, val_acc:0.899]
Epoch [11/120    avg_loss:0.360, val_acc:0.905]
Epoch [12/120    avg_loss:0.403, val_acc:0.897]
Epoch [13/120    avg_loss:0.374, val_acc:0.921]
Epoch [14/120    avg_loss:0.309, val_acc:0.917]
Epoch [15/120    avg_loss:0.384, val_acc:0.927]
Epoch [16/120    avg_loss:0.295, val_acc:0.931]
Epoch [17/120    avg_loss:0.329, val_acc:0.915]
Epoch [18/120    avg_loss:0.299, val_acc:0.905]
Epoch [19/120    avg_loss:0.364, val_acc:0.938]
Epoch [20/120    avg_loss:0.285, val_acc:0.937]
Epoch [21/120    avg_loss:0.255, val_acc:0.917]
Epoch [22/120    avg_loss:0.263, val_acc:0.954]
Epoch [23/120    avg_loss:0.264, val_acc:0.913]
Epoch [24/120    avg_loss:0.224, val_acc:0.937]
Epoch [25/120    avg_loss:0.295, val_acc:0.921]
Epoch [26/120    avg_loss:0.282, val_acc:0.903]
Epoch [27/120    avg_loss:0.246, val_acc:0.958]
Epoch [28/120    avg_loss:0.223, val_acc:0.964]
Epoch [29/120    avg_loss:0.191, val_acc:0.952]
Epoch [30/120    avg_loss:0.215, val_acc:0.954]
Epoch [31/120    avg_loss:0.167, val_acc:0.954]
Epoch [32/120    avg_loss:0.168, val_acc:0.954]
Epoch [33/120    avg_loss:0.172, val_acc:0.927]
Epoch [34/120    avg_loss:0.202, val_acc:0.948]
Epoch [35/120    avg_loss:0.127, val_acc:0.968]
Epoch [36/120    avg_loss:0.149, val_acc:0.962]
Epoch [37/120    avg_loss:0.183, val_acc:0.950]
Epoch [38/120    avg_loss:0.207, val_acc:0.946]
Epoch [39/120    avg_loss:0.240, val_acc:0.956]
Epoch [40/120    avg_loss:0.205, val_acc:0.968]
Epoch [41/120    avg_loss:0.131, val_acc:0.964]
Epoch [42/120    avg_loss:0.142, val_acc:0.970]
Epoch [43/120    avg_loss:0.160, val_acc:0.950]
Epoch [44/120    avg_loss:0.132, val_acc:0.950]
Epoch [45/120    avg_loss:0.121, val_acc:0.919]
Epoch [46/120    avg_loss:0.141, val_acc:0.964]
Epoch [47/120    avg_loss:0.112, val_acc:0.962]
Epoch [48/120    avg_loss:0.141, val_acc:0.960]
Epoch [49/120    avg_loss:0.112, val_acc:0.954]
Epoch [50/120    avg_loss:0.109, val_acc:0.966]
Epoch [51/120    avg_loss:0.103, val_acc:0.972]
Epoch [52/120    avg_loss:0.115, val_acc:0.964]
Epoch [53/120    avg_loss:0.117, val_acc:0.970]
Epoch [54/120    avg_loss:0.076, val_acc:0.972]
Epoch [55/120    avg_loss:0.060, val_acc:0.980]
Epoch [56/120    avg_loss:0.075, val_acc:0.970]
Epoch [57/120    avg_loss:0.089, val_acc:0.984]
Epoch [58/120    avg_loss:0.103, val_acc:0.968]
Epoch [59/120    avg_loss:0.051, val_acc:0.976]
Epoch [60/120    avg_loss:0.060, val_acc:0.970]
Epoch [61/120    avg_loss:0.113, val_acc:0.954]
Epoch [62/120    avg_loss:0.083, val_acc:0.972]
Epoch [63/120    avg_loss:0.052, val_acc:0.976]
Epoch [64/120    avg_loss:0.036, val_acc:0.972]
Epoch [65/120    avg_loss:0.077, val_acc:0.956]
Epoch [66/120    avg_loss:0.073, val_acc:0.976]
Epoch [67/120    avg_loss:0.083, val_acc:0.982]
Epoch [68/120    avg_loss:0.056, val_acc:0.980]
Epoch [69/120    avg_loss:0.044, val_acc:0.978]
Epoch [70/120    avg_loss:0.114, val_acc:0.972]
Epoch [71/120    avg_loss:0.085, val_acc:0.978]
Epoch [72/120    avg_loss:0.053, val_acc:0.976]
Epoch [73/120    avg_loss:0.033, val_acc:0.976]
Epoch [74/120    avg_loss:0.031, val_acc:0.978]
Epoch [75/120    avg_loss:0.031, val_acc:0.982]
Epoch [76/120    avg_loss:0.044, val_acc:0.982]
Epoch [77/120    avg_loss:0.023, val_acc:0.982]
Epoch [78/120    avg_loss:0.028, val_acc:0.982]
Epoch [79/120    avg_loss:0.031, val_acc:0.982]
Epoch [80/120    avg_loss:0.035, val_acc:0.982]
Epoch [81/120    avg_loss:0.026, val_acc:0.982]
Epoch [82/120    avg_loss:0.029, val_acc:0.982]
Epoch [83/120    avg_loss:0.024, val_acc:0.982]
Epoch [84/120    avg_loss:0.022, val_acc:0.982]
Epoch [85/120    avg_loss:0.029, val_acc:0.982]
Epoch [86/120    avg_loss:0.036, val_acc:0.982]
Epoch [87/120    avg_loss:0.025, val_acc:0.982]
Epoch [88/120    avg_loss:0.020, val_acc:0.982]
Epoch [89/120    avg_loss:0.025, val_acc:0.982]
Epoch [90/120    avg_loss:0.020, val_acc:0.982]
Epoch [91/120    avg_loss:0.027, val_acc:0.982]
Epoch [92/120    avg_loss:0.041, val_acc:0.982]
Epoch [93/120    avg_loss:0.030, val_acc:0.982]
Epoch [94/120    avg_loss:0.021, val_acc:0.982]
Epoch [95/120    avg_loss:0.018, val_acc:0.982]
Epoch [96/120    avg_loss:0.024, val_acc:0.982]
Epoch [97/120    avg_loss:0.023, val_acc:0.982]
Epoch [98/120    avg_loss:0.022, val_acc:0.982]
Epoch [99/120    avg_loss:0.029, val_acc:0.982]
Epoch [100/120    avg_loss:0.030, val_acc:0.982]
Epoch [101/120    avg_loss:0.030, val_acc:0.982]
Epoch [102/120    avg_loss:0.015, val_acc:0.982]
Epoch [103/120    avg_loss:0.030, val_acc:0.982]
Epoch [104/120    avg_loss:0.021, val_acc:0.982]
Epoch [105/120    avg_loss:0.015, val_acc:0.982]
Epoch [106/120    avg_loss:0.023, val_acc:0.982]
Epoch [107/120    avg_loss:0.022, val_acc:0.982]
Epoch [108/120    avg_loss:0.023, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.982]
Epoch [110/120    avg_loss:0.029, val_acc:0.982]
Epoch [111/120    avg_loss:0.029, val_acc:0.982]
Epoch [112/120    avg_loss:0.022, val_acc:0.982]
Epoch [113/120    avg_loss:0.025, val_acc:0.982]
Epoch [114/120    avg_loss:0.038, val_acc:0.982]
Epoch [115/120    avg_loss:0.020, val_acc:0.982]
Epoch [116/120    avg_loss:0.027, val_acc:0.982]
Epoch [117/120    avg_loss:0.039, val_acc:0.982]
Epoch [118/120    avg_loss:0.026, val_acc:0.982]
Epoch [119/120    avg_loss:0.022, val_acc:0.982]
Epoch [120/120    avg_loss:0.043, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   6   0   0   0   0   3   0]
 [  0   0   0 226   1   0   0   0   1   2   0   0   0   0]
 [  0   0   0   0 221   3   0   0   0   0   0   0   3   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   4 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 0.99854227 0.96330275 0.99122807 0.95670996 0.94285714
 0.99512195 0.95833333 0.998713   0.9978678  1.         0.99472296
 0.9833887  1.        ]

Kappa:
0.9893168531260924
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fccf4102780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:1.989, val_acc:0.599]
Epoch [2/120    avg_loss:1.240, val_acc:0.673]
Epoch [3/120    avg_loss:0.967, val_acc:0.754]
Epoch [4/120    avg_loss:0.788, val_acc:0.847]
Epoch [5/120    avg_loss:0.611, val_acc:0.857]
Epoch [6/120    avg_loss:0.636, val_acc:0.871]
Epoch [7/120    avg_loss:0.546, val_acc:0.873]
Epoch [8/120    avg_loss:0.518, val_acc:0.899]
Epoch [9/120    avg_loss:0.472, val_acc:0.861]
Epoch [10/120    avg_loss:0.418, val_acc:0.863]
Epoch [11/120    avg_loss:0.424, val_acc:0.897]
Epoch [12/120    avg_loss:0.396, val_acc:0.893]
Epoch [13/120    avg_loss:0.385, val_acc:0.917]
Epoch [14/120    avg_loss:0.349, val_acc:0.895]
Epoch [15/120    avg_loss:0.382, val_acc:0.889]
Epoch [16/120    avg_loss:0.334, val_acc:0.909]
Epoch [17/120    avg_loss:0.296, val_acc:0.903]
Epoch [18/120    avg_loss:0.339, val_acc:0.923]
Epoch [19/120    avg_loss:0.288, val_acc:0.887]
Epoch [20/120    avg_loss:0.298, val_acc:0.925]
Epoch [21/120    avg_loss:0.235, val_acc:0.929]
Epoch [22/120    avg_loss:0.265, val_acc:0.931]
Epoch [23/120    avg_loss:0.252, val_acc:0.909]
Epoch [24/120    avg_loss:0.198, val_acc:0.935]
Epoch [25/120    avg_loss:0.174, val_acc:0.944]
Epoch [26/120    avg_loss:0.201, val_acc:0.925]
Epoch [27/120    avg_loss:0.196, val_acc:0.929]
Epoch [28/120    avg_loss:0.125, val_acc:0.944]
Epoch [29/120    avg_loss:0.168, val_acc:0.948]
Epoch [30/120    avg_loss:0.149, val_acc:0.931]
Epoch [31/120    avg_loss:0.097, val_acc:0.954]
Epoch [32/120    avg_loss:0.215, val_acc:0.938]
Epoch [33/120    avg_loss:0.234, val_acc:0.938]
Epoch [34/120    avg_loss:0.154, val_acc:0.946]
Epoch [35/120    avg_loss:0.095, val_acc:0.942]
Epoch [36/120    avg_loss:0.103, val_acc:0.952]
Epoch [37/120    avg_loss:0.207, val_acc:0.946]
Epoch [38/120    avg_loss:0.152, val_acc:0.938]
Epoch [39/120    avg_loss:0.212, val_acc:0.933]
Epoch [40/120    avg_loss:0.262, val_acc:0.873]
Epoch [41/120    avg_loss:0.161, val_acc:0.954]
Epoch [42/120    avg_loss:0.179, val_acc:0.942]
Epoch [43/120    avg_loss:0.115, val_acc:0.948]
Epoch [44/120    avg_loss:0.089, val_acc:0.956]
Epoch [45/120    avg_loss:0.103, val_acc:0.938]
Epoch [46/120    avg_loss:0.148, val_acc:0.948]
Epoch [47/120    avg_loss:0.117, val_acc:0.942]
Epoch [48/120    avg_loss:0.120, val_acc:0.962]
Epoch [49/120    avg_loss:0.133, val_acc:0.944]
Epoch [50/120    avg_loss:0.063, val_acc:0.958]
Epoch [51/120    avg_loss:0.135, val_acc:0.956]
Epoch [52/120    avg_loss:0.140, val_acc:0.942]
Epoch [53/120    avg_loss:0.135, val_acc:0.942]
Epoch [54/120    avg_loss:0.082, val_acc:0.952]
Epoch [55/120    avg_loss:0.051, val_acc:0.968]
Epoch [56/120    avg_loss:0.086, val_acc:0.954]
Epoch [57/120    avg_loss:0.067, val_acc:0.950]
Epoch [58/120    avg_loss:0.063, val_acc:0.962]
Epoch [59/120    avg_loss:0.050, val_acc:0.970]
Epoch [60/120    avg_loss:0.068, val_acc:0.958]
Epoch [61/120    avg_loss:0.084, val_acc:0.950]
Epoch [62/120    avg_loss:0.079, val_acc:0.956]
Epoch [63/120    avg_loss:0.080, val_acc:0.954]
Epoch [64/120    avg_loss:0.167, val_acc:0.954]
Epoch [65/120    avg_loss:0.118, val_acc:0.940]
Epoch [66/120    avg_loss:0.122, val_acc:0.972]
Epoch [67/120    avg_loss:0.056, val_acc:0.956]
Epoch [68/120    avg_loss:0.073, val_acc:0.937]
Epoch [69/120    avg_loss:0.094, val_acc:0.958]
Epoch [70/120    avg_loss:0.061, val_acc:0.966]
Epoch [71/120    avg_loss:0.071, val_acc:0.970]
Epoch [72/120    avg_loss:0.082, val_acc:0.964]
Epoch [73/120    avg_loss:0.152, val_acc:0.940]
Epoch [74/120    avg_loss:0.186, val_acc:0.938]
Epoch [75/120    avg_loss:0.126, val_acc:0.944]
Epoch [76/120    avg_loss:0.143, val_acc:0.944]
Epoch [77/120    avg_loss:0.085, val_acc:0.944]
Epoch [78/120    avg_loss:0.133, val_acc:0.952]
Epoch [79/120    avg_loss:0.103, val_acc:0.960]
Epoch [80/120    avg_loss:0.033, val_acc:0.960]
Epoch [81/120    avg_loss:0.031, val_acc:0.962]
Epoch [82/120    avg_loss:0.051, val_acc:0.960]
Epoch [83/120    avg_loss:0.029, val_acc:0.966]
Epoch [84/120    avg_loss:0.023, val_acc:0.970]
Epoch [85/120    avg_loss:0.029, val_acc:0.968]
Epoch [86/120    avg_loss:0.029, val_acc:0.966]
Epoch [87/120    avg_loss:0.017, val_acc:0.966]
Epoch [88/120    avg_loss:0.023, val_acc:0.968]
Epoch [89/120    avg_loss:0.027, val_acc:0.968]
Epoch [90/120    avg_loss:0.030, val_acc:0.970]
Epoch [91/120    avg_loss:0.020, val_acc:0.970]
Epoch [92/120    avg_loss:0.020, val_acc:0.970]
Epoch [93/120    avg_loss:0.017, val_acc:0.970]
Epoch [94/120    avg_loss:0.017, val_acc:0.970]
Epoch [95/120    avg_loss:0.028, val_acc:0.970]
Epoch [96/120    avg_loss:0.028, val_acc:0.970]
Epoch [97/120    avg_loss:0.042, val_acc:0.970]
Epoch [98/120    avg_loss:0.014, val_acc:0.970]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.031, val_acc:0.970]
Epoch [101/120    avg_loss:0.020, val_acc:0.970]
Epoch [102/120    avg_loss:0.022, val_acc:0.970]
Epoch [103/120    avg_loss:0.016, val_acc:0.970]
Epoch [104/120    avg_loss:0.019, val_acc:0.970]
Epoch [105/120    avg_loss:0.027, val_acc:0.970]
Epoch [106/120    avg_loss:0.020, val_acc:0.970]
Epoch [107/120    avg_loss:0.019, val_acc:0.970]
Epoch [108/120    avg_loss:0.020, val_acc:0.970]
Epoch [109/120    avg_loss:0.019, val_acc:0.970]
Epoch [110/120    avg_loss:0.029, val_acc:0.970]
Epoch [111/120    avg_loss:0.017, val_acc:0.970]
Epoch [112/120    avg_loss:0.018, val_acc:0.970]
Epoch [113/120    avg_loss:0.031, val_acc:0.970]
Epoch [114/120    avg_loss:0.016, val_acc:0.970]
Epoch [115/120    avg_loss:0.025, val_acc:0.970]
Epoch [116/120    avg_loss:0.019, val_acc:0.970]
Epoch [117/120    avg_loss:0.014, val_acc:0.970]
Epoch [118/120    avg_loss:0.022, val_acc:0.970]
Epoch [119/120    avg_loss:0.025, val_acc:0.970]
Epoch [120/120    avg_loss:0.016, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   3   0   0   0   0   3   0]
 [  0   0   0 220   8   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0  12   0   0   3   0 191   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   1 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 0.99131693 0.97038724 0.97777778 0.9287257  0.92783505
 0.96221662 0.97354497 0.998713   0.99893276 1.         0.9986755
 0.98893805 1.        ]

Kappa:
0.9855155832659801
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa254fd0710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.157, val_acc:0.556]
Epoch [2/120    avg_loss:1.557, val_acc:0.593]
Epoch [3/120    avg_loss:1.107, val_acc:0.704]
Epoch [4/120    avg_loss:0.954, val_acc:0.796]
Epoch [5/120    avg_loss:0.895, val_acc:0.845]
Epoch [6/120    avg_loss:0.557, val_acc:0.881]
Epoch [7/120    avg_loss:0.505, val_acc:0.847]
Epoch [8/120    avg_loss:0.520, val_acc:0.903]
Epoch [9/120    avg_loss:0.485, val_acc:0.845]
Epoch [10/120    avg_loss:0.424, val_acc:0.909]
Epoch [11/120    avg_loss:0.402, val_acc:0.911]
Epoch [12/120    avg_loss:0.447, val_acc:0.911]
Epoch [13/120    avg_loss:0.475, val_acc:0.903]
Epoch [14/120    avg_loss:0.400, val_acc:0.919]
Epoch [15/120    avg_loss:0.460, val_acc:0.923]
Epoch [16/120    avg_loss:0.346, val_acc:0.921]
Epoch [17/120    avg_loss:0.295, val_acc:0.952]
Epoch [18/120    avg_loss:0.228, val_acc:0.940]
Epoch [19/120    avg_loss:0.185, val_acc:0.946]
Epoch [20/120    avg_loss:0.213, val_acc:0.921]
Epoch [21/120    avg_loss:0.221, val_acc:0.942]
Epoch [22/120    avg_loss:0.159, val_acc:0.942]
Epoch [23/120    avg_loss:0.176, val_acc:0.940]
Epoch [24/120    avg_loss:0.220, val_acc:0.931]
Epoch [25/120    avg_loss:0.235, val_acc:0.929]
Epoch [26/120    avg_loss:0.211, val_acc:0.921]
Epoch [27/120    avg_loss:0.188, val_acc:0.972]
Epoch [28/120    avg_loss:0.212, val_acc:0.948]
Epoch [29/120    avg_loss:0.307, val_acc:0.944]
Epoch [30/120    avg_loss:0.233, val_acc:0.948]
Epoch [31/120    avg_loss:0.241, val_acc:0.948]
Epoch [32/120    avg_loss:0.172, val_acc:0.954]
Epoch [33/120    avg_loss:0.112, val_acc:0.954]
Epoch [34/120    avg_loss:0.138, val_acc:0.968]
Epoch [35/120    avg_loss:0.141, val_acc:0.952]
Epoch [36/120    avg_loss:0.285, val_acc:0.948]
Epoch [37/120    avg_loss:0.207, val_acc:0.944]
Epoch [38/120    avg_loss:0.158, val_acc:0.966]
Epoch [39/120    avg_loss:0.168, val_acc:0.956]
Epoch [40/120    avg_loss:0.090, val_acc:0.960]
Epoch [41/120    avg_loss:0.073, val_acc:0.966]
Epoch [42/120    avg_loss:0.065, val_acc:0.972]
Epoch [43/120    avg_loss:0.077, val_acc:0.976]
Epoch [44/120    avg_loss:0.056, val_acc:0.978]
Epoch [45/120    avg_loss:0.066, val_acc:0.978]
Epoch [46/120    avg_loss:0.049, val_acc:0.978]
Epoch [47/120    avg_loss:0.062, val_acc:0.978]
Epoch [48/120    avg_loss:0.057, val_acc:0.976]
Epoch [49/120    avg_loss:0.072, val_acc:0.976]
Epoch [50/120    avg_loss:0.068, val_acc:0.976]
Epoch [51/120    avg_loss:0.062, val_acc:0.974]
Epoch [52/120    avg_loss:0.058, val_acc:0.974]
Epoch [53/120    avg_loss:0.048, val_acc:0.978]
Epoch [54/120    avg_loss:0.057, val_acc:0.972]
Epoch [55/120    avg_loss:0.056, val_acc:0.970]
Epoch [56/120    avg_loss:0.066, val_acc:0.970]
Epoch [57/120    avg_loss:0.055, val_acc:0.970]
Epoch [58/120    avg_loss:0.058, val_acc:0.978]
Epoch [59/120    avg_loss:0.061, val_acc:0.974]
Epoch [60/120    avg_loss:0.079, val_acc:0.974]
Epoch [61/120    avg_loss:0.046, val_acc:0.976]
Epoch [62/120    avg_loss:0.059, val_acc:0.974]
Epoch [63/120    avg_loss:0.076, val_acc:0.974]
Epoch [64/120    avg_loss:0.057, val_acc:0.976]
Epoch [65/120    avg_loss:0.052, val_acc:0.972]
Epoch [66/120    avg_loss:0.039, val_acc:0.972]
Epoch [67/120    avg_loss:0.041, val_acc:0.972]
Epoch [68/120    avg_loss:0.059, val_acc:0.974]
Epoch [69/120    avg_loss:0.052, val_acc:0.980]
Epoch [70/120    avg_loss:0.043, val_acc:0.978]
Epoch [71/120    avg_loss:0.082, val_acc:0.984]
Epoch [72/120    avg_loss:0.061, val_acc:0.982]
Epoch [73/120    avg_loss:0.055, val_acc:0.978]
Epoch [74/120    avg_loss:0.038, val_acc:0.980]
Epoch [75/120    avg_loss:0.045, val_acc:0.978]
Epoch [76/120    avg_loss:0.051, val_acc:0.978]
Epoch [77/120    avg_loss:0.050, val_acc:0.978]
Epoch [78/120    avg_loss:0.035, val_acc:0.976]
Epoch [79/120    avg_loss:0.041, val_acc:0.976]
Epoch [80/120    avg_loss:0.041, val_acc:0.978]
Epoch [81/120    avg_loss:0.048, val_acc:0.972]
Epoch [82/120    avg_loss:0.045, val_acc:0.974]
Epoch [83/120    avg_loss:0.035, val_acc:0.974]
Epoch [84/120    avg_loss:0.036, val_acc:0.972]
Epoch [85/120    avg_loss:0.037, val_acc:0.972]
Epoch [86/120    avg_loss:0.035, val_acc:0.972]
Epoch [87/120    avg_loss:0.044, val_acc:0.972]
Epoch [88/120    avg_loss:0.046, val_acc:0.972]
Epoch [89/120    avg_loss:0.038, val_acc:0.972]
Epoch [90/120    avg_loss:0.029, val_acc:0.972]
Epoch [91/120    avg_loss:0.061, val_acc:0.974]
Epoch [92/120    avg_loss:0.036, val_acc:0.974]
Epoch [93/120    avg_loss:0.046, val_acc:0.974]
Epoch [94/120    avg_loss:0.053, val_acc:0.974]
Epoch [95/120    avg_loss:0.042, val_acc:0.974]
Epoch [96/120    avg_loss:0.032, val_acc:0.974]
Epoch [97/120    avg_loss:0.051, val_acc:0.974]
Epoch [98/120    avg_loss:0.034, val_acc:0.974]
Epoch [99/120    avg_loss:0.052, val_acc:0.974]
Epoch [100/120    avg_loss:0.037, val_acc:0.974]
Epoch [101/120    avg_loss:0.041, val_acc:0.974]
Epoch [102/120    avg_loss:0.050, val_acc:0.974]
Epoch [103/120    avg_loss:0.045, val_acc:0.974]
Epoch [104/120    avg_loss:0.056, val_acc:0.974]
Epoch [105/120    avg_loss:0.042, val_acc:0.974]
Epoch [106/120    avg_loss:0.055, val_acc:0.974]
Epoch [107/120    avg_loss:0.028, val_acc:0.974]
Epoch [108/120    avg_loss:0.051, val_acc:0.974]
Epoch [109/120    avg_loss:0.036, val_acc:0.974]
Epoch [110/120    avg_loss:0.037, val_acc:0.974]
Epoch [111/120    avg_loss:0.036, val_acc:0.974]
Epoch [112/120    avg_loss:0.050, val_acc:0.974]
Epoch [113/120    avg_loss:0.050, val_acc:0.974]
Epoch [114/120    avg_loss:0.049, val_acc:0.974]
Epoch [115/120    avg_loss:0.027, val_acc:0.974]
Epoch [116/120    avg_loss:0.038, val_acc:0.974]
Epoch [117/120    avg_loss:0.039, val_acc:0.974]
Epoch [118/120    avg_loss:0.039, val_acc:0.974]
Epoch [119/120    avg_loss:0.055, val_acc:0.974]
Epoch [120/120    avg_loss:0.043, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   3   0   0   0   0   3   0]
 [  0   0   0 221   4   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   0   0   1   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 0.99854227 0.95302013 0.98004435 0.91503268 0.88194444
 0.99512195 0.9281768  0.99487179 0.99893276 1.         1.
 0.99005525 1.        ]

Kappa:
0.9840933387875858
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb66c89e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.054, val_acc:0.579]
Epoch [2/120    avg_loss:1.233, val_acc:0.734]
Epoch [3/120    avg_loss:0.958, val_acc:0.770]
Epoch [4/120    avg_loss:0.849, val_acc:0.700]
Epoch [5/120    avg_loss:0.833, val_acc:0.738]
Epoch [6/120    avg_loss:0.686, val_acc:0.772]
Epoch [7/120    avg_loss:0.571, val_acc:0.861]
Epoch [8/120    avg_loss:0.534, val_acc:0.855]
Epoch [9/120    avg_loss:0.494, val_acc:0.859]
Epoch [10/120    avg_loss:0.510, val_acc:0.895]
Epoch [11/120    avg_loss:0.386, val_acc:0.897]
Epoch [12/120    avg_loss:0.551, val_acc:0.907]
Epoch [13/120    avg_loss:0.406, val_acc:0.903]
Epoch [14/120    avg_loss:0.331, val_acc:0.911]
Epoch [15/120    avg_loss:0.366, val_acc:0.881]
Epoch [16/120    avg_loss:0.348, val_acc:0.944]
Epoch [17/120    avg_loss:0.247, val_acc:0.915]
Epoch [18/120    avg_loss:0.292, val_acc:0.944]
Epoch [19/120    avg_loss:0.217, val_acc:0.938]
Epoch [20/120    avg_loss:0.236, val_acc:0.946]
Epoch [21/120    avg_loss:0.257, val_acc:0.940]
Epoch [22/120    avg_loss:0.192, val_acc:0.911]
Epoch [23/120    avg_loss:0.219, val_acc:0.927]
Epoch [24/120    avg_loss:0.205, val_acc:0.952]
Epoch [25/120    avg_loss:0.168, val_acc:0.952]
Epoch [26/120    avg_loss:0.169, val_acc:0.946]
Epoch [27/120    avg_loss:0.147, val_acc:0.952]
Epoch [28/120    avg_loss:0.117, val_acc:0.944]
Epoch [29/120    avg_loss:0.110, val_acc:0.960]
Epoch [30/120    avg_loss:0.150, val_acc:0.954]
Epoch [31/120    avg_loss:0.133, val_acc:0.954]
Epoch [32/120    avg_loss:0.148, val_acc:0.942]
Epoch [33/120    avg_loss:0.333, val_acc:0.903]
Epoch [34/120    avg_loss:0.273, val_acc:0.946]
Epoch [35/120    avg_loss:0.171, val_acc:0.956]
Epoch [36/120    avg_loss:0.158, val_acc:0.948]
Epoch [37/120    avg_loss:0.196, val_acc:0.938]
Epoch [38/120    avg_loss:0.146, val_acc:0.956]
Epoch [39/120    avg_loss:0.138, val_acc:0.960]
Epoch [40/120    avg_loss:0.148, val_acc:0.964]
Epoch [41/120    avg_loss:0.130, val_acc:0.952]
Epoch [42/120    avg_loss:0.162, val_acc:0.958]
Epoch [43/120    avg_loss:0.155, val_acc:0.933]
Epoch [44/120    avg_loss:0.106, val_acc:0.970]
Epoch [45/120    avg_loss:0.125, val_acc:0.960]
Epoch [46/120    avg_loss:0.077, val_acc:0.972]
Epoch [47/120    avg_loss:0.101, val_acc:0.972]
Epoch [48/120    avg_loss:0.109, val_acc:0.952]
Epoch [49/120    avg_loss:0.147, val_acc:0.956]
Epoch [50/120    avg_loss:0.191, val_acc:0.946]
Epoch [51/120    avg_loss:0.122, val_acc:0.948]
Epoch [52/120    avg_loss:0.156, val_acc:0.964]
Epoch [53/120    avg_loss:0.087, val_acc:0.964]
Epoch [54/120    avg_loss:0.104, val_acc:0.952]
Epoch [55/120    avg_loss:0.158, val_acc:0.946]
Epoch [56/120    avg_loss:0.118, val_acc:0.964]
Epoch [57/120    avg_loss:0.088, val_acc:0.962]
Epoch [58/120    avg_loss:0.056, val_acc:0.970]
Epoch [59/120    avg_loss:0.049, val_acc:0.962]
Epoch [60/120    avg_loss:0.092, val_acc:0.966]
Epoch [61/120    avg_loss:0.054, val_acc:0.970]
Epoch [62/120    avg_loss:0.043, val_acc:0.974]
Epoch [63/120    avg_loss:0.039, val_acc:0.972]
Epoch [64/120    avg_loss:0.024, val_acc:0.974]
Epoch [65/120    avg_loss:0.045, val_acc:0.974]
Epoch [66/120    avg_loss:0.032, val_acc:0.974]
Epoch [67/120    avg_loss:0.022, val_acc:0.974]
Epoch [68/120    avg_loss:0.030, val_acc:0.976]
Epoch [69/120    avg_loss:0.030, val_acc:0.976]
Epoch [70/120    avg_loss:0.026, val_acc:0.976]
Epoch [71/120    avg_loss:0.020, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.976]
Epoch [73/120    avg_loss:0.026, val_acc:0.974]
Epoch [74/120    avg_loss:0.023, val_acc:0.978]
Epoch [75/120    avg_loss:0.034, val_acc:0.980]
Epoch [76/120    avg_loss:0.034, val_acc:0.970]
Epoch [77/120    avg_loss:0.019, val_acc:0.974]
Epoch [78/120    avg_loss:0.022, val_acc:0.976]
Epoch [79/120    avg_loss:0.045, val_acc:0.978]
Epoch [80/120    avg_loss:0.026, val_acc:0.978]
Epoch [81/120    avg_loss:0.029, val_acc:0.976]
Epoch [82/120    avg_loss:0.026, val_acc:0.976]
Epoch [83/120    avg_loss:0.020, val_acc:0.980]
Epoch [84/120    avg_loss:0.026, val_acc:0.980]
Epoch [85/120    avg_loss:0.022, val_acc:0.982]
Epoch [86/120    avg_loss:0.017, val_acc:0.982]
Epoch [87/120    avg_loss:0.036, val_acc:0.974]
Epoch [88/120    avg_loss:0.028, val_acc:0.974]
Epoch [89/120    avg_loss:0.022, val_acc:0.974]
Epoch [90/120    avg_loss:0.032, val_acc:0.974]
Epoch [91/120    avg_loss:0.025, val_acc:0.978]
Epoch [92/120    avg_loss:0.027, val_acc:0.980]
Epoch [93/120    avg_loss:0.020, val_acc:0.980]
Epoch [94/120    avg_loss:0.023, val_acc:0.976]
Epoch [95/120    avg_loss:0.028, val_acc:0.976]
Epoch [96/120    avg_loss:0.024, val_acc:0.974]
Epoch [97/120    avg_loss:0.019, val_acc:0.980]
Epoch [98/120    avg_loss:0.025, val_acc:0.976]
Epoch [99/120    avg_loss:0.036, val_acc:0.972]
Epoch [100/120    avg_loss:0.020, val_acc:0.972]
Epoch [101/120    avg_loss:0.024, val_acc:0.972]
Epoch [102/120    avg_loss:0.021, val_acc:0.974]
Epoch [103/120    avg_loss:0.027, val_acc:0.974]
Epoch [104/120    avg_loss:0.020, val_acc:0.974]
Epoch [105/120    avg_loss:0.028, val_acc:0.974]
Epoch [106/120    avg_loss:0.017, val_acc:0.974]
Epoch [107/120    avg_loss:0.028, val_acc:0.974]
Epoch [108/120    avg_loss:0.027, val_acc:0.974]
Epoch [109/120    avg_loss:0.014, val_acc:0.974]
Epoch [110/120    avg_loss:0.025, val_acc:0.974]
Epoch [111/120    avg_loss:0.021, val_acc:0.974]
Epoch [112/120    avg_loss:0.022, val_acc:0.974]
Epoch [113/120    avg_loss:0.024, val_acc:0.974]
Epoch [114/120    avg_loss:0.020, val_acc:0.974]
Epoch [115/120    avg_loss:0.014, val_acc:0.974]
Epoch [116/120    avg_loss:0.020, val_acc:0.974]
Epoch [117/120    avg_loss:0.020, val_acc:0.974]
Epoch [118/120    avg_loss:0.016, val_acc:0.974]
Epoch [119/120    avg_loss:0.021, val_acc:0.974]
Epoch [120/120    avg_loss:0.013, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   2   0   0   0   0   3   0]
 [  0   0   0 216   6   0   0   0   7   1   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.97716895 0.96860987 0.91028446 0.8956229
 0.99019608 0.98947368 0.99106003 0.99893276 1.         1.
 0.99115044 1.        ]

Kappa:
0.9859944105191083
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f80e84aa7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.142, val_acc:0.546]
Epoch [2/120    avg_loss:1.372, val_acc:0.637]
Epoch [3/120    avg_loss:1.011, val_acc:0.748]
Epoch [4/120    avg_loss:0.908, val_acc:0.796]
Epoch [5/120    avg_loss:0.691, val_acc:0.845]
Epoch [6/120    avg_loss:0.604, val_acc:0.788]
Epoch [7/120    avg_loss:0.594, val_acc:0.806]
Epoch [8/120    avg_loss:0.466, val_acc:0.857]
Epoch [9/120    avg_loss:0.521, val_acc:0.879]
Epoch [10/120    avg_loss:0.387, val_acc:0.893]
Epoch [11/120    avg_loss:0.372, val_acc:0.879]
Epoch [12/120    avg_loss:0.376, val_acc:0.857]
Epoch [13/120    avg_loss:0.359, val_acc:0.901]
Epoch [14/120    avg_loss:0.323, val_acc:0.923]
Epoch [15/120    avg_loss:0.269, val_acc:0.901]
Epoch [16/120    avg_loss:0.278, val_acc:0.933]
Epoch [17/120    avg_loss:0.229, val_acc:0.921]
Epoch [18/120    avg_loss:0.271, val_acc:0.927]
Epoch [19/120    avg_loss:0.347, val_acc:0.913]
Epoch [20/120    avg_loss:0.232, val_acc:0.905]
Epoch [21/120    avg_loss:0.196, val_acc:0.915]
Epoch [22/120    avg_loss:0.324, val_acc:0.929]
Epoch [23/120    avg_loss:0.280, val_acc:0.915]
Epoch [24/120    avg_loss:0.277, val_acc:0.944]
Epoch [25/120    avg_loss:0.172, val_acc:0.940]
Epoch [26/120    avg_loss:0.131, val_acc:0.938]
Epoch [27/120    avg_loss:0.199, val_acc:0.931]
Epoch [28/120    avg_loss:0.140, val_acc:0.938]
Epoch [29/120    avg_loss:0.168, val_acc:0.935]
Epoch [30/120    avg_loss:0.246, val_acc:0.944]
Epoch [31/120    avg_loss:0.156, val_acc:0.948]
Epoch [32/120    avg_loss:0.133, val_acc:0.952]
Epoch [33/120    avg_loss:0.170, val_acc:0.938]
Epoch [34/120    avg_loss:0.085, val_acc:0.956]
Epoch [35/120    avg_loss:0.085, val_acc:0.950]
Epoch [36/120    avg_loss:0.104, val_acc:0.958]
Epoch [37/120    avg_loss:0.060, val_acc:0.968]
Epoch [38/120    avg_loss:0.106, val_acc:0.962]
Epoch [39/120    avg_loss:0.112, val_acc:0.927]
Epoch [40/120    avg_loss:0.160, val_acc:0.958]
Epoch [41/120    avg_loss:0.136, val_acc:0.970]
Epoch [42/120    avg_loss:0.094, val_acc:0.962]
Epoch [43/120    avg_loss:0.062, val_acc:0.958]
Epoch [44/120    avg_loss:0.160, val_acc:0.909]
Epoch [45/120    avg_loss:0.194, val_acc:0.907]
Epoch [46/120    avg_loss:0.100, val_acc:0.968]
Epoch [47/120    avg_loss:0.061, val_acc:0.974]
Epoch [48/120    avg_loss:0.089, val_acc:0.948]
Epoch [49/120    avg_loss:0.080, val_acc:0.960]
Epoch [50/120    avg_loss:0.072, val_acc:0.956]
Epoch [51/120    avg_loss:0.084, val_acc:0.966]
Epoch [52/120    avg_loss:0.057, val_acc:0.960]
Epoch [53/120    avg_loss:0.066, val_acc:0.960]
Epoch [54/120    avg_loss:0.058, val_acc:0.970]
Epoch [55/120    avg_loss:0.072, val_acc:0.966]
Epoch [56/120    avg_loss:0.068, val_acc:0.958]
Epoch [57/120    avg_loss:0.090, val_acc:0.944]
Epoch [58/120    avg_loss:0.100, val_acc:0.958]
Epoch [59/120    avg_loss:0.078, val_acc:0.968]
Epoch [60/120    avg_loss:0.061, val_acc:0.966]
Epoch [61/120    avg_loss:0.046, val_acc:0.970]
Epoch [62/120    avg_loss:0.034, val_acc:0.974]
Epoch [63/120    avg_loss:0.025, val_acc:0.976]
Epoch [64/120    avg_loss:0.033, val_acc:0.978]
Epoch [65/120    avg_loss:0.020, val_acc:0.976]
Epoch [66/120    avg_loss:0.041, val_acc:0.978]
Epoch [67/120    avg_loss:0.030, val_acc:0.972]
Epoch [68/120    avg_loss:0.022, val_acc:0.976]
Epoch [69/120    avg_loss:0.018, val_acc:0.976]
Epoch [70/120    avg_loss:0.020, val_acc:0.976]
Epoch [71/120    avg_loss:0.030, val_acc:0.976]
Epoch [72/120    avg_loss:0.027, val_acc:0.974]
Epoch [73/120    avg_loss:0.025, val_acc:0.976]
Epoch [74/120    avg_loss:0.052, val_acc:0.980]
Epoch [75/120    avg_loss:0.032, val_acc:0.976]
Epoch [76/120    avg_loss:0.023, val_acc:0.976]
Epoch [77/120    avg_loss:0.021, val_acc:0.976]
Epoch [78/120    avg_loss:0.021, val_acc:0.976]
Epoch [79/120    avg_loss:0.018, val_acc:0.978]
Epoch [80/120    avg_loss:0.025, val_acc:0.976]
Epoch [81/120    avg_loss:0.027, val_acc:0.976]
Epoch [82/120    avg_loss:0.020, val_acc:0.976]
Epoch [83/120    avg_loss:0.020, val_acc:0.974]
Epoch [84/120    avg_loss:0.040, val_acc:0.978]
Epoch [85/120    avg_loss:0.022, val_acc:0.978]
Epoch [86/120    avg_loss:0.022, val_acc:0.976]
Epoch [87/120    avg_loss:0.021, val_acc:0.976]
Epoch [88/120    avg_loss:0.028, val_acc:0.976]
Epoch [89/120    avg_loss:0.012, val_acc:0.976]
Epoch [90/120    avg_loss:0.025, val_acc:0.976]
Epoch [91/120    avg_loss:0.014, val_acc:0.976]
Epoch [92/120    avg_loss:0.023, val_acc:0.976]
Epoch [93/120    avg_loss:0.020, val_acc:0.976]
Epoch [94/120    avg_loss:0.019, val_acc:0.976]
Epoch [95/120    avg_loss:0.015, val_acc:0.976]
Epoch [96/120    avg_loss:0.024, val_acc:0.976]
Epoch [97/120    avg_loss:0.030, val_acc:0.976]
Epoch [98/120    avg_loss:0.026, val_acc:0.976]
Epoch [99/120    avg_loss:0.024, val_acc:0.976]
Epoch [100/120    avg_loss:0.019, val_acc:0.976]
Epoch [101/120    avg_loss:0.020, val_acc:0.976]
Epoch [102/120    avg_loss:0.023, val_acc:0.976]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.022, val_acc:0.976]
Epoch [105/120    avg_loss:0.023, val_acc:0.976]
Epoch [106/120    avg_loss:0.012, val_acc:0.976]
Epoch [107/120    avg_loss:0.017, val_acc:0.976]
Epoch [108/120    avg_loss:0.015, val_acc:0.976]
Epoch [109/120    avg_loss:0.033, val_acc:0.976]
Epoch [110/120    avg_loss:0.011, val_acc:0.976]
Epoch [111/120    avg_loss:0.021, val_acc:0.976]
Epoch [112/120    avg_loss:0.017, val_acc:0.976]
Epoch [113/120    avg_loss:0.013, val_acc:0.976]
Epoch [114/120    avg_loss:0.023, val_acc:0.976]
Epoch [115/120    avg_loss:0.016, val_acc:0.976]
Epoch [116/120    avg_loss:0.020, val_acc:0.976]
Epoch [117/120    avg_loss:0.016, val_acc:0.976]
Epoch [118/120    avg_loss:0.015, val_acc:0.976]
Epoch [119/120    avg_loss:0.020, val_acc:0.976]
Epoch [120/120    avg_loss:0.019, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 227   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   1 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.99545455 0.98678414 0.93995859 0.92592593
 0.99266504 1.         1.         1.         1.         0.9986755
 0.99667774 1.        ]

Kappa:
0.9924033547328117
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a32bd57f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.009, val_acc:0.619]
Epoch [2/120    avg_loss:1.319, val_acc:0.655]
Epoch [3/120    avg_loss:1.021, val_acc:0.798]
Epoch [4/120    avg_loss:0.806, val_acc:0.825]
Epoch [5/120    avg_loss:0.688, val_acc:0.869]
Epoch [6/120    avg_loss:0.528, val_acc:0.885]
Epoch [7/120    avg_loss:0.597, val_acc:0.845]
Epoch [8/120    avg_loss:0.535, val_acc:0.899]
Epoch [9/120    avg_loss:0.448, val_acc:0.871]
Epoch [10/120    avg_loss:0.416, val_acc:0.909]
Epoch [11/120    avg_loss:0.378, val_acc:0.897]
Epoch [12/120    avg_loss:0.455, val_acc:0.893]
Epoch [13/120    avg_loss:0.342, val_acc:0.903]
Epoch [14/120    avg_loss:0.537, val_acc:0.901]
Epoch [15/120    avg_loss:0.386, val_acc:0.901]
Epoch [16/120    avg_loss:0.247, val_acc:0.948]
Epoch [17/120    avg_loss:0.210, val_acc:0.944]
Epoch [18/120    avg_loss:0.316, val_acc:0.927]
Epoch [19/120    avg_loss:0.357, val_acc:0.946]
Epoch [20/120    avg_loss:0.239, val_acc:0.940]
Epoch [21/120    avg_loss:0.272, val_acc:0.942]
Epoch [22/120    avg_loss:0.226, val_acc:0.927]
Epoch [23/120    avg_loss:0.250, val_acc:0.950]
Epoch [24/120    avg_loss:0.164, val_acc:0.952]
Epoch [25/120    avg_loss:0.271, val_acc:0.929]
Epoch [26/120    avg_loss:0.261, val_acc:0.913]
Epoch [27/120    avg_loss:0.263, val_acc:0.952]
Epoch [28/120    avg_loss:0.211, val_acc:0.958]
Epoch [29/120    avg_loss:0.181, val_acc:0.952]
Epoch [30/120    avg_loss:0.174, val_acc:0.970]
Epoch [31/120    avg_loss:0.191, val_acc:0.942]
Epoch [32/120    avg_loss:0.195, val_acc:0.938]
Epoch [33/120    avg_loss:0.146, val_acc:0.952]
Epoch [34/120    avg_loss:0.124, val_acc:0.974]
Epoch [35/120    avg_loss:0.125, val_acc:0.964]
Epoch [36/120    avg_loss:0.156, val_acc:0.964]
Epoch [37/120    avg_loss:0.118, val_acc:0.948]
Epoch [38/120    avg_loss:0.119, val_acc:0.962]
Epoch [39/120    avg_loss:0.108, val_acc:0.958]
Epoch [40/120    avg_loss:0.113, val_acc:0.962]
Epoch [41/120    avg_loss:0.137, val_acc:0.960]
Epoch [42/120    avg_loss:0.119, val_acc:0.938]
Epoch [43/120    avg_loss:0.162, val_acc:0.962]
Epoch [44/120    avg_loss:0.134, val_acc:0.970]
Epoch [45/120    avg_loss:0.151, val_acc:0.970]
Epoch [46/120    avg_loss:0.093, val_acc:0.974]
Epoch [47/120    avg_loss:0.152, val_acc:0.968]
Epoch [48/120    avg_loss:0.203, val_acc:0.952]
Epoch [49/120    avg_loss:0.086, val_acc:0.968]
Epoch [50/120    avg_loss:0.064, val_acc:0.984]
Epoch [51/120    avg_loss:0.072, val_acc:0.992]
Epoch [52/120    avg_loss:0.128, val_acc:0.966]
Epoch [53/120    avg_loss:0.137, val_acc:0.976]
Epoch [54/120    avg_loss:0.086, val_acc:0.970]
Epoch [55/120    avg_loss:0.069, val_acc:0.986]
Epoch [56/120    avg_loss:0.064, val_acc:0.980]
Epoch [57/120    avg_loss:0.057, val_acc:0.980]
Epoch [58/120    avg_loss:0.032, val_acc:0.976]
Epoch [59/120    avg_loss:0.073, val_acc:0.982]
Epoch [60/120    avg_loss:0.079, val_acc:0.976]
Epoch [61/120    avg_loss:0.041, val_acc:0.980]
Epoch [62/120    avg_loss:0.048, val_acc:0.980]
Epoch [63/120    avg_loss:0.048, val_acc:0.978]
Epoch [64/120    avg_loss:0.054, val_acc:0.974]
Epoch [65/120    avg_loss:0.048, val_acc:0.980]
Epoch [66/120    avg_loss:0.033, val_acc:0.980]
Epoch [67/120    avg_loss:0.024, val_acc:0.980]
Epoch [68/120    avg_loss:0.019, val_acc:0.982]
Epoch [69/120    avg_loss:0.026, val_acc:0.982]
Epoch [70/120    avg_loss:0.057, val_acc:0.986]
Epoch [71/120    avg_loss:0.026, val_acc:0.988]
Epoch [72/120    avg_loss:0.039, val_acc:0.988]
Epoch [73/120    avg_loss:0.019, val_acc:0.988]
Epoch [74/120    avg_loss:0.018, val_acc:0.986]
Epoch [75/120    avg_loss:0.035, val_acc:0.986]
Epoch [76/120    avg_loss:0.018, val_acc:0.988]
Epoch [77/120    avg_loss:0.016, val_acc:0.988]
Epoch [78/120    avg_loss:0.022, val_acc:0.988]
Epoch [79/120    avg_loss:0.023, val_acc:0.988]
Epoch [80/120    avg_loss:0.023, val_acc:0.988]
Epoch [81/120    avg_loss:0.021, val_acc:0.988]
Epoch [82/120    avg_loss:0.021, val_acc:0.988]
Epoch [83/120    avg_loss:0.020, val_acc:0.988]
Epoch [84/120    avg_loss:0.024, val_acc:0.988]
Epoch [85/120    avg_loss:0.017, val_acc:0.988]
Epoch [86/120    avg_loss:0.020, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.988]
Epoch [88/120    avg_loss:0.021, val_acc:0.988]
Epoch [89/120    avg_loss:0.050, val_acc:0.988]
Epoch [90/120    avg_loss:0.018, val_acc:0.988]
Epoch [91/120    avg_loss:0.025, val_acc:0.988]
Epoch [92/120    avg_loss:0.032, val_acc:0.988]
Epoch [93/120    avg_loss:0.036, val_acc:0.988]
Epoch [94/120    avg_loss:0.028, val_acc:0.988]
Epoch [95/120    avg_loss:0.032, val_acc:0.988]
Epoch [96/120    avg_loss:0.033, val_acc:0.988]
Epoch [97/120    avg_loss:0.021, val_acc:0.988]
Epoch [98/120    avg_loss:0.027, val_acc:0.988]
Epoch [99/120    avg_loss:0.019, val_acc:0.988]
Epoch [100/120    avg_loss:0.021, val_acc:0.988]
Epoch [101/120    avg_loss:0.016, val_acc:0.988]
Epoch [102/120    avg_loss:0.028, val_acc:0.988]
Epoch [103/120    avg_loss:0.030, val_acc:0.988]
Epoch [104/120    avg_loss:0.015, val_acc:0.988]
Epoch [105/120    avg_loss:0.018, val_acc:0.988]
Epoch [106/120    avg_loss:0.018, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.017, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.988]
Epoch [110/120    avg_loss:0.029, val_acc:0.988]
Epoch [111/120    avg_loss:0.017, val_acc:0.988]
Epoch [112/120    avg_loss:0.024, val_acc:0.988]
Epoch [113/120    avg_loss:0.020, val_acc:0.988]
Epoch [114/120    avg_loss:0.024, val_acc:0.988]
Epoch [115/120    avg_loss:0.023, val_acc:0.988]
Epoch [116/120    avg_loss:0.017, val_acc:0.988]
Epoch [117/120    avg_loss:0.014, val_acc:0.988]
Epoch [118/120    avg_loss:0.028, val_acc:0.988]
Epoch [119/120    avg_loss:0.018, val_acc:0.988]
Epoch [120/120    avg_loss:0.021, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220   9   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 221   4   0   0   0   0   0   0   2   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   1   0   0   3   0 202   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99927061 0.98426966 0.97777778 0.94042553 0.95070423
 0.99019608 0.96703297 1.         0.99893276 1.         1.
 0.99669239 1.        ]

Kappa:
0.9912159022692818
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f194ce33828>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.078, val_acc:0.518]
Epoch [2/120    avg_loss:1.329, val_acc:0.673]
Epoch [3/120    avg_loss:1.003, val_acc:0.724]
Epoch [4/120    avg_loss:0.825, val_acc:0.851]
Epoch [5/120    avg_loss:0.689, val_acc:0.865]
Epoch [6/120    avg_loss:0.587, val_acc:0.837]
Epoch [7/120    avg_loss:0.686, val_acc:0.800]
Epoch [8/120    avg_loss:0.523, val_acc:0.899]
Epoch [9/120    avg_loss:0.347, val_acc:0.903]
Epoch [10/120    avg_loss:0.394, val_acc:0.877]
Epoch [11/120    avg_loss:0.385, val_acc:0.869]
Epoch [12/120    avg_loss:0.448, val_acc:0.869]
Epoch [13/120    avg_loss:0.392, val_acc:0.819]
Epoch [14/120    avg_loss:0.348, val_acc:0.903]
Epoch [15/120    avg_loss:0.289, val_acc:0.929]
Epoch [16/120    avg_loss:0.294, val_acc:0.919]
Epoch [17/120    avg_loss:0.228, val_acc:0.938]
Epoch [18/120    avg_loss:0.210, val_acc:0.937]
Epoch [19/120    avg_loss:0.223, val_acc:0.933]
Epoch [20/120    avg_loss:0.259, val_acc:0.919]
Epoch [21/120    avg_loss:0.212, val_acc:0.933]
Epoch [22/120    avg_loss:0.212, val_acc:0.950]
Epoch [23/120    avg_loss:0.179, val_acc:0.952]
Epoch [24/120    avg_loss:0.181, val_acc:0.937]
Epoch [25/120    avg_loss:0.234, val_acc:0.937]
Epoch [26/120    avg_loss:0.207, val_acc:0.942]
Epoch [27/120    avg_loss:0.193, val_acc:0.944]
Epoch [28/120    avg_loss:0.167, val_acc:0.954]
Epoch [29/120    avg_loss:0.137, val_acc:0.942]
Epoch [30/120    avg_loss:0.104, val_acc:0.956]
Epoch [31/120    avg_loss:0.181, val_acc:0.946]
Epoch [32/120    avg_loss:0.136, val_acc:0.948]
Epoch [33/120    avg_loss:0.152, val_acc:0.948]
Epoch [34/120    avg_loss:0.118, val_acc:0.938]
Epoch [35/120    avg_loss:0.142, val_acc:0.935]
Epoch [36/120    avg_loss:0.131, val_acc:0.948]
Epoch [37/120    avg_loss:0.124, val_acc:0.952]
Epoch [38/120    avg_loss:0.217, val_acc:0.942]
Epoch [39/120    avg_loss:0.177, val_acc:0.944]
Epoch [40/120    avg_loss:0.136, val_acc:0.933]
Epoch [41/120    avg_loss:0.091, val_acc:0.956]
Epoch [42/120    avg_loss:0.108, val_acc:0.962]
Epoch [43/120    avg_loss:0.104, val_acc:0.964]
Epoch [44/120    avg_loss:0.136, val_acc:0.956]
Epoch [45/120    avg_loss:0.092, val_acc:0.956]
Epoch [46/120    avg_loss:0.076, val_acc:0.956]
Epoch [47/120    avg_loss:0.071, val_acc:0.966]
Epoch [48/120    avg_loss:0.120, val_acc:0.952]
Epoch [49/120    avg_loss:0.149, val_acc:0.946]
Epoch [50/120    avg_loss:0.147, val_acc:0.960]
Epoch [51/120    avg_loss:0.095, val_acc:0.956]
Epoch [52/120    avg_loss:0.053, val_acc:0.974]
Epoch [53/120    avg_loss:0.049, val_acc:0.960]
Epoch [54/120    avg_loss:0.089, val_acc:0.960]
Epoch [55/120    avg_loss:0.073, val_acc:0.948]
Epoch [56/120    avg_loss:0.102, val_acc:0.970]
Epoch [57/120    avg_loss:0.064, val_acc:0.974]
Epoch [58/120    avg_loss:0.074, val_acc:0.933]
Epoch [59/120    avg_loss:0.083, val_acc:0.962]
Epoch [60/120    avg_loss:0.051, val_acc:0.964]
Epoch [61/120    avg_loss:0.068, val_acc:0.960]
Epoch [62/120    avg_loss:0.092, val_acc:0.964]
Epoch [63/120    avg_loss:0.059, val_acc:0.962]
Epoch [64/120    avg_loss:0.098, val_acc:0.966]
Epoch [65/120    avg_loss:0.123, val_acc:0.956]
Epoch [66/120    avg_loss:0.050, val_acc:0.974]
Epoch [67/120    avg_loss:0.037, val_acc:0.972]
Epoch [68/120    avg_loss:0.023, val_acc:0.974]
Epoch [69/120    avg_loss:0.029, val_acc:0.974]
Epoch [70/120    avg_loss:0.033, val_acc:0.978]
Epoch [71/120    avg_loss:0.043, val_acc:0.970]
Epoch [72/120    avg_loss:0.028, val_acc:0.962]
Epoch [73/120    avg_loss:0.014, val_acc:0.976]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.040, val_acc:0.966]
Epoch [76/120    avg_loss:0.022, val_acc:0.966]
Epoch [77/120    avg_loss:0.036, val_acc:0.964]
Epoch [78/120    avg_loss:0.016, val_acc:0.974]
Epoch [79/120    avg_loss:0.032, val_acc:0.972]
Epoch [80/120    avg_loss:0.021, val_acc:0.980]
Epoch [81/120    avg_loss:0.030, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.980]
Epoch [83/120    avg_loss:0.033, val_acc:0.982]
Epoch [84/120    avg_loss:0.034, val_acc:0.990]
Epoch [85/120    avg_loss:0.020, val_acc:0.982]
Epoch [86/120    avg_loss:0.017, val_acc:0.980]
Epoch [87/120    avg_loss:0.015, val_acc:0.984]
Epoch [88/120    avg_loss:0.014, val_acc:0.986]
Epoch [89/120    avg_loss:0.018, val_acc:0.964]
Epoch [90/120    avg_loss:0.013, val_acc:0.974]
Epoch [91/120    avg_loss:0.013, val_acc:0.980]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.980]
Epoch [95/120    avg_loss:0.008, val_acc:0.976]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.019, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.025, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.006, val_acc:0.980]
Epoch [103/120    avg_loss:0.007, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.980]
Epoch [105/120    avg_loss:0.006, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.980]
Epoch [110/120    avg_loss:0.004, val_acc:0.980]
Epoch [111/120    avg_loss:0.005, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.013, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.980]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.003, val_acc:0.980]
Epoch [120/120    avg_loss:0.004, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 205  20   0   0   0   1   4   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.99086758 0.94252874 0.90187891 0.91289199
 0.99512195 0.98947368 0.998713   0.99574468 1.         1.
 0.99778761 1.        ]

Kappa:
0.9867062213608508
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f00d4d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.080, val_acc:0.544]
Epoch [2/120    avg_loss:1.298, val_acc:0.716]
Epoch [3/120    avg_loss:0.969, val_acc:0.748]
Epoch [4/120    avg_loss:0.914, val_acc:0.833]
Epoch [5/120    avg_loss:0.713, val_acc:0.827]
Epoch [6/120    avg_loss:0.684, val_acc:0.873]
Epoch [7/120    avg_loss:0.463, val_acc:0.927]
Epoch [8/120    avg_loss:0.474, val_acc:0.891]
Epoch [9/120    avg_loss:0.448, val_acc:0.893]
Epoch [10/120    avg_loss:0.410, val_acc:0.871]
Epoch [11/120    avg_loss:0.421, val_acc:0.931]
Epoch [12/120    avg_loss:0.381, val_acc:0.917]
Epoch [13/120    avg_loss:0.318, val_acc:0.915]
Epoch [14/120    avg_loss:0.310, val_acc:0.923]
Epoch [15/120    avg_loss:0.395, val_acc:0.937]
Epoch [16/120    avg_loss:0.257, val_acc:0.942]
Epoch [17/120    avg_loss:0.287, val_acc:0.935]
Epoch [18/120    avg_loss:0.230, val_acc:0.960]
Epoch [19/120    avg_loss:0.234, val_acc:0.952]
Epoch [20/120    avg_loss:0.196, val_acc:0.944]
Epoch [21/120    avg_loss:0.240, val_acc:0.921]
Epoch [22/120    avg_loss:0.257, val_acc:0.911]
Epoch [23/120    avg_loss:0.224, val_acc:0.972]
Epoch [24/120    avg_loss:0.175, val_acc:0.952]
Epoch [25/120    avg_loss:0.255, val_acc:0.921]
Epoch [26/120    avg_loss:0.263, val_acc:0.964]
Epoch [27/120    avg_loss:0.153, val_acc:0.958]
Epoch [28/120    avg_loss:0.134, val_acc:0.962]
Epoch [29/120    avg_loss:0.106, val_acc:0.960]
Epoch [30/120    avg_loss:0.121, val_acc:0.966]
Epoch [31/120    avg_loss:0.164, val_acc:0.944]
Epoch [32/120    avg_loss:0.158, val_acc:0.972]
Epoch [33/120    avg_loss:0.117, val_acc:0.952]
Epoch [34/120    avg_loss:0.107, val_acc:0.978]
Epoch [35/120    avg_loss:0.090, val_acc:0.968]
Epoch [36/120    avg_loss:0.117, val_acc:0.976]
Epoch [37/120    avg_loss:0.118, val_acc:0.966]
Epoch [38/120    avg_loss:0.118, val_acc:0.962]
Epoch [39/120    avg_loss:0.123, val_acc:0.976]
Epoch [40/120    avg_loss:0.098, val_acc:0.980]
Epoch [41/120    avg_loss:0.073, val_acc:0.990]
Epoch [42/120    avg_loss:0.058, val_acc:0.976]
Epoch [43/120    avg_loss:0.083, val_acc:0.976]
Epoch [44/120    avg_loss:0.073, val_acc:0.986]
Epoch [45/120    avg_loss:0.058, val_acc:0.984]
Epoch [46/120    avg_loss:0.056, val_acc:0.976]
Epoch [47/120    avg_loss:0.066, val_acc:0.980]
Epoch [48/120    avg_loss:0.063, val_acc:0.982]
Epoch [49/120    avg_loss:0.050, val_acc:0.980]
Epoch [50/120    avg_loss:0.059, val_acc:0.984]
Epoch [51/120    avg_loss:0.047, val_acc:0.986]
Epoch [52/120    avg_loss:0.026, val_acc:0.992]
Epoch [53/120    avg_loss:0.039, val_acc:0.988]
Epoch [54/120    avg_loss:0.054, val_acc:0.988]
Epoch [55/120    avg_loss:0.110, val_acc:0.938]
Epoch [56/120    avg_loss:0.071, val_acc:0.974]
Epoch [57/120    avg_loss:0.062, val_acc:0.976]
Epoch [58/120    avg_loss:0.079, val_acc:0.974]
Epoch [59/120    avg_loss:0.067, val_acc:0.964]
Epoch [60/120    avg_loss:0.050, val_acc:0.990]
Epoch [61/120    avg_loss:0.110, val_acc:0.984]
Epoch [62/120    avg_loss:0.052, val_acc:0.992]
Epoch [63/120    avg_loss:0.123, val_acc:0.986]
Epoch [64/120    avg_loss:0.084, val_acc:0.978]
Epoch [65/120    avg_loss:0.070, val_acc:0.988]
Epoch [66/120    avg_loss:0.060, val_acc:0.978]
Epoch [67/120    avg_loss:0.045, val_acc:0.988]
Epoch [68/120    avg_loss:0.056, val_acc:0.988]
Epoch [69/120    avg_loss:0.051, val_acc:0.980]
Epoch [70/120    avg_loss:0.045, val_acc:0.986]
Epoch [71/120    avg_loss:0.038, val_acc:0.986]
Epoch [72/120    avg_loss:0.043, val_acc:0.988]
Epoch [73/120    avg_loss:0.016, val_acc:0.990]
Epoch [74/120    avg_loss:0.029, val_acc:0.986]
Epoch [75/120    avg_loss:0.028, val_acc:0.984]
Epoch [76/120    avg_loss:0.030, val_acc:0.988]
Epoch [77/120    avg_loss:0.031, val_acc:0.992]
Epoch [78/120    avg_loss:0.015, val_acc:0.990]
Epoch [79/120    avg_loss:0.023, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.010, val_acc:0.992]
Epoch [82/120    avg_loss:0.016, val_acc:0.992]
Epoch [83/120    avg_loss:0.013, val_acc:0.992]
Epoch [84/120    avg_loss:0.018, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.992]
Epoch [86/120    avg_loss:0.012, val_acc:0.992]
Epoch [87/120    avg_loss:0.020, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.990]
Epoch [90/120    avg_loss:0.014, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.992]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.014, val_acc:0.990]
Epoch [94/120    avg_loss:0.024, val_acc:0.992]
Epoch [95/120    avg_loss:0.010, val_acc:0.992]
Epoch [96/120    avg_loss:0.023, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.018, val_acc:0.992]
Epoch [99/120    avg_loss:0.010, val_acc:0.992]
Epoch [100/120    avg_loss:0.015, val_acc:0.992]
Epoch [101/120    avg_loss:0.024, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.027, val_acc:0.992]
Epoch [104/120    avg_loss:0.008, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.013, val_acc:0.992]
Epoch [107/120    avg_loss:0.018, val_acc:0.994]
Epoch [108/120    avg_loss:0.018, val_acc:0.994]
Epoch [109/120    avg_loss:0.013, val_acc:0.992]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.013, val_acc:0.992]
Epoch [112/120    avg_loss:0.018, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.010, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.009, val_acc:0.994]
Epoch [119/120    avg_loss:0.011, val_acc:0.994]
Epoch [120/120    avg_loss:0.022, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.98861048 0.98678414 0.93418259 0.93333333
 0.98271605 0.98947368 1.         1.         1.         1.
 0.99778761 1.        ]

Kappa:
0.9914543485112451
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f90e0062710>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.079, val_acc:0.629]
Epoch [2/120    avg_loss:1.326, val_acc:0.706]
Epoch [3/120    avg_loss:0.998, val_acc:0.772]
Epoch [4/120    avg_loss:0.866, val_acc:0.764]
Epoch [5/120    avg_loss:0.764, val_acc:0.776]
Epoch [6/120    avg_loss:0.632, val_acc:0.857]
Epoch [7/120    avg_loss:0.619, val_acc:0.849]
Epoch [8/120    avg_loss:0.578, val_acc:0.849]
Epoch [9/120    avg_loss:0.401, val_acc:0.875]
Epoch [10/120    avg_loss:0.415, val_acc:0.831]
Epoch [11/120    avg_loss:0.396, val_acc:0.903]
Epoch [12/120    avg_loss:0.494, val_acc:0.889]
Epoch [13/120    avg_loss:0.335, val_acc:0.879]
Epoch [14/120    avg_loss:0.269, val_acc:0.940]
Epoch [15/120    avg_loss:0.276, val_acc:0.950]
Epoch [16/120    avg_loss:0.233, val_acc:0.942]
Epoch [17/120    avg_loss:0.254, val_acc:0.942]
Epoch [18/120    avg_loss:0.233, val_acc:0.925]
Epoch [19/120    avg_loss:0.187, val_acc:0.944]
Epoch [20/120    avg_loss:0.236, val_acc:0.946]
Epoch [21/120    avg_loss:0.265, val_acc:0.917]
Epoch [22/120    avg_loss:0.321, val_acc:0.919]
Epoch [23/120    avg_loss:0.306, val_acc:0.921]
Epoch [24/120    avg_loss:0.302, val_acc:0.935]
Epoch [25/120    avg_loss:0.189, val_acc:0.960]
Epoch [26/120    avg_loss:0.232, val_acc:0.937]
Epoch [27/120    avg_loss:0.186, val_acc:0.974]
Epoch [28/120    avg_loss:0.150, val_acc:0.972]
Epoch [29/120    avg_loss:0.150, val_acc:0.950]
Epoch [30/120    avg_loss:0.204, val_acc:0.931]
Epoch [31/120    avg_loss:0.250, val_acc:0.964]
Epoch [32/120    avg_loss:0.177, val_acc:0.938]
Epoch [33/120    avg_loss:0.197, val_acc:0.952]
Epoch [34/120    avg_loss:0.108, val_acc:0.970]
Epoch [35/120    avg_loss:0.100, val_acc:0.974]
Epoch [36/120    avg_loss:0.087, val_acc:0.984]
Epoch [37/120    avg_loss:0.148, val_acc:0.962]
Epoch [38/120    avg_loss:0.106, val_acc:0.970]
Epoch [39/120    avg_loss:0.078, val_acc:0.970]
Epoch [40/120    avg_loss:0.088, val_acc:0.980]
Epoch [41/120    avg_loss:0.102, val_acc:0.958]
Epoch [42/120    avg_loss:0.113, val_acc:0.933]
Epoch [43/120    avg_loss:0.107, val_acc:0.952]
Epoch [44/120    avg_loss:0.167, val_acc:0.964]
Epoch [45/120    avg_loss:0.090, val_acc:0.972]
Epoch [46/120    avg_loss:0.074, val_acc:0.976]
Epoch [47/120    avg_loss:0.071, val_acc:0.974]
Epoch [48/120    avg_loss:0.058, val_acc:0.980]
Epoch [49/120    avg_loss:0.053, val_acc:0.978]
Epoch [50/120    avg_loss:0.059, val_acc:0.982]
Epoch [51/120    avg_loss:0.046, val_acc:0.984]
Epoch [52/120    avg_loss:0.028, val_acc:0.986]
Epoch [53/120    avg_loss:0.042, val_acc:0.984]
Epoch [54/120    avg_loss:0.039, val_acc:0.984]
Epoch [55/120    avg_loss:0.036, val_acc:0.984]
Epoch [56/120    avg_loss:0.026, val_acc:0.984]
Epoch [57/120    avg_loss:0.025, val_acc:0.986]
Epoch [58/120    avg_loss:0.030, val_acc:0.986]
Epoch [59/120    avg_loss:0.033, val_acc:0.986]
Epoch [60/120    avg_loss:0.025, val_acc:0.988]
Epoch [61/120    avg_loss:0.034, val_acc:0.984]
Epoch [62/120    avg_loss:0.032, val_acc:0.988]
Epoch [63/120    avg_loss:0.022, val_acc:0.988]
Epoch [64/120    avg_loss:0.040, val_acc:0.982]
Epoch [65/120    avg_loss:0.029, val_acc:0.986]
Epoch [66/120    avg_loss:0.035, val_acc:0.986]
Epoch [67/120    avg_loss:0.036, val_acc:0.986]
Epoch [68/120    avg_loss:0.031, val_acc:0.986]
Epoch [69/120    avg_loss:0.029, val_acc:0.986]
Epoch [70/120    avg_loss:0.026, val_acc:0.988]
Epoch [71/120    avg_loss:0.030, val_acc:0.988]
Epoch [72/120    avg_loss:0.024, val_acc:0.988]
Epoch [73/120    avg_loss:0.037, val_acc:0.988]
Epoch [74/120    avg_loss:0.030, val_acc:0.988]
Epoch [75/120    avg_loss:0.021, val_acc:0.986]
Epoch [76/120    avg_loss:0.026, val_acc:0.986]
Epoch [77/120    avg_loss:0.032, val_acc:0.988]
Epoch [78/120    avg_loss:0.032, val_acc:0.984]
Epoch [79/120    avg_loss:0.025, val_acc:0.986]
Epoch [80/120    avg_loss:0.035, val_acc:0.988]
Epoch [81/120    avg_loss:0.020, val_acc:0.988]
Epoch [82/120    avg_loss:0.027, val_acc:0.988]
Epoch [83/120    avg_loss:0.032, val_acc:0.988]
Epoch [84/120    avg_loss:0.043, val_acc:0.986]
Epoch [85/120    avg_loss:0.041, val_acc:0.986]
Epoch [86/120    avg_loss:0.046, val_acc:0.986]
Epoch [87/120    avg_loss:0.031, val_acc:0.984]
Epoch [88/120    avg_loss:0.040, val_acc:0.986]
Epoch [89/120    avg_loss:0.025, val_acc:0.988]
Epoch [90/120    avg_loss:0.029, val_acc:0.988]
Epoch [91/120    avg_loss:0.017, val_acc:0.992]
Epoch [92/120    avg_loss:0.057, val_acc:0.992]
Epoch [93/120    avg_loss:0.024, val_acc:0.992]
Epoch [94/120    avg_loss:0.025, val_acc:0.990]
Epoch [95/120    avg_loss:0.020, val_acc:0.986]
Epoch [96/120    avg_loss:0.020, val_acc:0.986]
Epoch [97/120    avg_loss:0.024, val_acc:0.988]
Epoch [98/120    avg_loss:0.024, val_acc:0.988]
Epoch [99/120    avg_loss:0.026, val_acc:0.990]
Epoch [100/120    avg_loss:0.014, val_acc:0.990]
Epoch [101/120    avg_loss:0.016, val_acc:0.990]
Epoch [102/120    avg_loss:0.022, val_acc:0.990]
Epoch [103/120    avg_loss:0.023, val_acc:0.990]
Epoch [104/120    avg_loss:0.019, val_acc:0.992]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.024, val_acc:0.990]
Epoch [107/120    avg_loss:0.026, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.990]
Epoch [110/120    avg_loss:0.028, val_acc:0.990]
Epoch [111/120    avg_loss:0.024, val_acc:0.988]
Epoch [112/120    avg_loss:0.027, val_acc:0.988]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.017, val_acc:0.988]
Epoch [115/120    avg_loss:0.021, val_acc:0.988]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.030, val_acc:0.988]
Epoch [118/120    avg_loss:0.025, val_acc:0.990]
Epoch [119/120    avg_loss:0.023, val_acc:0.988]
Epoch [120/120    avg_loss:0.031, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 208  16   0   0   0   0   0   0   3   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   1 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.98855835 0.98230088 0.9183223  0.91946309
 0.99019608 0.98429319 0.998713   0.99893276 1.         0.9986755
 0.99337748 1.        ]

Kappa:
0.9893180697212955
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b9691e6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.071, val_acc:0.593]
Epoch [2/120    avg_loss:1.315, val_acc:0.698]
Epoch [3/120    avg_loss:0.942, val_acc:0.829]
Epoch [4/120    avg_loss:0.685, val_acc:0.855]
Epoch [5/120    avg_loss:0.562, val_acc:0.871]
Epoch [6/120    avg_loss:0.538, val_acc:0.885]
Epoch [7/120    avg_loss:0.556, val_acc:0.883]
Epoch [8/120    avg_loss:0.373, val_acc:0.919]
Epoch [9/120    avg_loss:0.347, val_acc:0.927]
Epoch [10/120    avg_loss:0.396, val_acc:0.909]
Epoch [11/120    avg_loss:0.341, val_acc:0.913]
Epoch [12/120    avg_loss:0.496, val_acc:0.877]
Epoch [13/120    avg_loss:0.437, val_acc:0.919]
Epoch [14/120    avg_loss:0.428, val_acc:0.931]
Epoch [15/120    avg_loss:0.272, val_acc:0.919]
Epoch [16/120    avg_loss:0.296, val_acc:0.931]
Epoch [17/120    avg_loss:0.251, val_acc:0.925]
Epoch [18/120    avg_loss:0.296, val_acc:0.944]
Epoch [19/120    avg_loss:0.305, val_acc:0.946]
Epoch [20/120    avg_loss:0.232, val_acc:0.937]
Epoch [21/120    avg_loss:0.202, val_acc:0.956]
Epoch [22/120    avg_loss:0.201, val_acc:0.905]
Epoch [23/120    avg_loss:0.367, val_acc:0.907]
Epoch [24/120    avg_loss:0.324, val_acc:0.946]
Epoch [25/120    avg_loss:0.271, val_acc:0.917]
Epoch [26/120    avg_loss:0.173, val_acc:0.952]
Epoch [27/120    avg_loss:0.211, val_acc:0.952]
Epoch [28/120    avg_loss:0.217, val_acc:0.946]
Epoch [29/120    avg_loss:0.160, val_acc:0.958]
Epoch [30/120    avg_loss:0.136, val_acc:0.978]
Epoch [31/120    avg_loss:0.209, val_acc:0.929]
Epoch [32/120    avg_loss:0.202, val_acc:0.966]
Epoch [33/120    avg_loss:0.173, val_acc:0.960]
Epoch [34/120    avg_loss:0.149, val_acc:0.938]
Epoch [35/120    avg_loss:0.135, val_acc:0.950]
Epoch [36/120    avg_loss:0.085, val_acc:0.966]
Epoch [37/120    avg_loss:0.121, val_acc:0.972]
Epoch [38/120    avg_loss:0.128, val_acc:0.976]
Epoch [39/120    avg_loss:0.161, val_acc:0.931]
Epoch [40/120    avg_loss:0.150, val_acc:0.968]
Epoch [41/120    avg_loss:0.111, val_acc:0.972]
Epoch [42/120    avg_loss:0.086, val_acc:0.966]
Epoch [43/120    avg_loss:0.070, val_acc:0.970]
Epoch [44/120    avg_loss:0.071, val_acc:0.972]
Epoch [45/120    avg_loss:0.046, val_acc:0.982]
Epoch [46/120    avg_loss:0.053, val_acc:0.982]
Epoch [47/120    avg_loss:0.051, val_acc:0.984]
Epoch [48/120    avg_loss:0.042, val_acc:0.982]
Epoch [49/120    avg_loss:0.056, val_acc:0.982]
Epoch [50/120    avg_loss:0.060, val_acc:0.982]
Epoch [51/120    avg_loss:0.067, val_acc:0.982]
Epoch [52/120    avg_loss:0.049, val_acc:0.982]
Epoch [53/120    avg_loss:0.048, val_acc:0.984]
Epoch [54/120    avg_loss:0.041, val_acc:0.984]
Epoch [55/120    avg_loss:0.057, val_acc:0.988]
Epoch [56/120    avg_loss:0.040, val_acc:0.988]
Epoch [57/120    avg_loss:0.046, val_acc:0.988]
Epoch [58/120    avg_loss:0.033, val_acc:0.986]
Epoch [59/120    avg_loss:0.034, val_acc:0.986]
Epoch [60/120    avg_loss:0.035, val_acc:0.986]
Epoch [61/120    avg_loss:0.047, val_acc:0.984]
Epoch [62/120    avg_loss:0.063, val_acc:0.984]
Epoch [63/120    avg_loss:0.049, val_acc:0.982]
Epoch [64/120    avg_loss:0.046, val_acc:0.984]
Epoch [65/120    avg_loss:0.033, val_acc:0.984]
Epoch [66/120    avg_loss:0.037, val_acc:0.984]
Epoch [67/120    avg_loss:0.036, val_acc:0.984]
Epoch [68/120    avg_loss:0.045, val_acc:0.986]
Epoch [69/120    avg_loss:0.028, val_acc:0.984]
Epoch [70/120    avg_loss:0.053, val_acc:0.984]
Epoch [71/120    avg_loss:0.041, val_acc:0.984]
Epoch [72/120    avg_loss:0.032, val_acc:0.984]
Epoch [73/120    avg_loss:0.035, val_acc:0.984]
Epoch [74/120    avg_loss:0.029, val_acc:0.984]
Epoch [75/120    avg_loss:0.048, val_acc:0.984]
Epoch [76/120    avg_loss:0.038, val_acc:0.984]
Epoch [77/120    avg_loss:0.035, val_acc:0.984]
Epoch [78/120    avg_loss:0.043, val_acc:0.984]
Epoch [79/120    avg_loss:0.030, val_acc:0.984]
Epoch [80/120    avg_loss:0.023, val_acc:0.984]
Epoch [81/120    avg_loss:0.025, val_acc:0.984]
Epoch [82/120    avg_loss:0.029, val_acc:0.984]
Epoch [83/120    avg_loss:0.035, val_acc:0.984]
Epoch [84/120    avg_loss:0.029, val_acc:0.984]
Epoch [85/120    avg_loss:0.036, val_acc:0.984]
Epoch [86/120    avg_loss:0.033, val_acc:0.984]
Epoch [87/120    avg_loss:0.034, val_acc:0.984]
Epoch [88/120    avg_loss:0.041, val_acc:0.984]
Epoch [89/120    avg_loss:0.041, val_acc:0.984]
Epoch [90/120    avg_loss:0.042, val_acc:0.984]
Epoch [91/120    avg_loss:0.029, val_acc:0.984]
Epoch [92/120    avg_loss:0.030, val_acc:0.984]
Epoch [93/120    avg_loss:0.066, val_acc:0.984]
Epoch [94/120    avg_loss:0.037, val_acc:0.984]
Epoch [95/120    avg_loss:0.034, val_acc:0.984]
Epoch [96/120    avg_loss:0.048, val_acc:0.984]
Epoch [97/120    avg_loss:0.052, val_acc:0.984]
Epoch [98/120    avg_loss:0.035, val_acc:0.984]
Epoch [99/120    avg_loss:0.035, val_acc:0.984]
Epoch [100/120    avg_loss:0.036, val_acc:0.984]
Epoch [101/120    avg_loss:0.035, val_acc:0.984]
Epoch [102/120    avg_loss:0.041, val_acc:0.984]
Epoch [103/120    avg_loss:0.047, val_acc:0.984]
Epoch [104/120    avg_loss:0.031, val_acc:0.984]
Epoch [105/120    avg_loss:0.026, val_acc:0.984]
Epoch [106/120    avg_loss:0.065, val_acc:0.984]
Epoch [107/120    avg_loss:0.032, val_acc:0.984]
Epoch [108/120    avg_loss:0.055, val_acc:0.984]
Epoch [109/120    avg_loss:0.030, val_acc:0.984]
Epoch [110/120    avg_loss:0.039, val_acc:0.984]
Epoch [111/120    avg_loss:0.060, val_acc:0.984]
Epoch [112/120    avg_loss:0.031, val_acc:0.984]
Epoch [113/120    avg_loss:0.032, val_acc:0.984]
Epoch [114/120    avg_loss:0.048, val_acc:0.984]
Epoch [115/120    avg_loss:0.032, val_acc:0.984]
Epoch [116/120    avg_loss:0.033, val_acc:0.984]
Epoch [117/120    avg_loss:0.029, val_acc:0.984]
Epoch [118/120    avg_loss:0.035, val_acc:0.984]
Epoch [119/120    avg_loss:0.045, val_acc:0.984]
Epoch [120/120    avg_loss:0.044, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   4 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.97977528 0.98230088 0.94339623 0.94623656
 0.98771499 0.96174863 1.         0.99893276 1.         0.99472296
 0.99333333 1.        ]

Kappa:
0.9902668297820937
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f681799d7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.108, val_acc:0.581]
Epoch [2/120    avg_loss:1.293, val_acc:0.746]
Epoch [3/120    avg_loss:0.999, val_acc:0.756]
Epoch [4/120    avg_loss:0.828, val_acc:0.802]
Epoch [5/120    avg_loss:0.720, val_acc:0.847]
Epoch [6/120    avg_loss:0.575, val_acc:0.823]
Epoch [7/120    avg_loss:0.604, val_acc:0.863]
Epoch [8/120    avg_loss:0.499, val_acc:0.871]
Epoch [9/120    avg_loss:0.450, val_acc:0.897]
Epoch [10/120    avg_loss:0.415, val_acc:0.917]
Epoch [11/120    avg_loss:0.376, val_acc:0.917]
Epoch [12/120    avg_loss:0.442, val_acc:0.863]
Epoch [13/120    avg_loss:0.372, val_acc:0.925]
Epoch [14/120    avg_loss:0.338, val_acc:0.921]
Epoch [15/120    avg_loss:0.296, val_acc:0.917]
Epoch [16/120    avg_loss:0.412, val_acc:0.933]
Epoch [17/120    avg_loss:0.273, val_acc:0.931]
Epoch [18/120    avg_loss:0.283, val_acc:0.927]
Epoch [19/120    avg_loss:0.255, val_acc:0.929]
Epoch [20/120    avg_loss:0.188, val_acc:0.925]
Epoch [21/120    avg_loss:0.282, val_acc:0.942]
Epoch [22/120    avg_loss:0.325, val_acc:0.931]
Epoch [23/120    avg_loss:0.305, val_acc:0.927]
Epoch [24/120    avg_loss:0.232, val_acc:0.937]
Epoch [25/120    avg_loss:0.210, val_acc:0.942]
Epoch [26/120    avg_loss:0.118, val_acc:0.962]
Epoch [27/120    avg_loss:0.206, val_acc:0.942]
Epoch [28/120    avg_loss:0.144, val_acc:0.937]
Epoch [29/120    avg_loss:0.177, val_acc:0.923]
Epoch [30/120    avg_loss:0.159, val_acc:0.944]
Epoch [31/120    avg_loss:0.152, val_acc:0.923]
Epoch [32/120    avg_loss:0.238, val_acc:0.899]
Epoch [33/120    avg_loss:0.219, val_acc:0.925]
Epoch [34/120    avg_loss:0.189, val_acc:0.946]
Epoch [35/120    avg_loss:0.158, val_acc:0.956]
Epoch [36/120    avg_loss:0.187, val_acc:0.946]
Epoch [37/120    avg_loss:0.154, val_acc:0.948]
Epoch [38/120    avg_loss:0.186, val_acc:0.927]
Epoch [39/120    avg_loss:0.175, val_acc:0.944]
Epoch [40/120    avg_loss:0.109, val_acc:0.952]
Epoch [41/120    avg_loss:0.088, val_acc:0.960]
Epoch [42/120    avg_loss:0.078, val_acc:0.968]
Epoch [43/120    avg_loss:0.055, val_acc:0.970]
Epoch [44/120    avg_loss:0.080, val_acc:0.972]
Epoch [45/120    avg_loss:0.070, val_acc:0.970]
Epoch [46/120    avg_loss:0.076, val_acc:0.970]
Epoch [47/120    avg_loss:0.064, val_acc:0.970]
Epoch [48/120    avg_loss:0.046, val_acc:0.970]
Epoch [49/120    avg_loss:0.061, val_acc:0.976]
Epoch [50/120    avg_loss:0.068, val_acc:0.974]
Epoch [51/120    avg_loss:0.073, val_acc:0.974]
Epoch [52/120    avg_loss:0.046, val_acc:0.974]
Epoch [53/120    avg_loss:0.053, val_acc:0.976]
Epoch [54/120    avg_loss:0.063, val_acc:0.976]
Epoch [55/120    avg_loss:0.048, val_acc:0.978]
Epoch [56/120    avg_loss:0.050, val_acc:0.974]
Epoch [57/120    avg_loss:0.052, val_acc:0.974]
Epoch [58/120    avg_loss:0.068, val_acc:0.972]
Epoch [59/120    avg_loss:0.055, val_acc:0.976]
Epoch [60/120    avg_loss:0.044, val_acc:0.976]
Epoch [61/120    avg_loss:0.043, val_acc:0.976]
Epoch [62/120    avg_loss:0.066, val_acc:0.978]
Epoch [63/120    avg_loss:0.056, val_acc:0.976]
Epoch [64/120    avg_loss:0.050, val_acc:0.978]
Epoch [65/120    avg_loss:0.038, val_acc:0.976]
Epoch [66/120    avg_loss:0.034, val_acc:0.974]
Epoch [67/120    avg_loss:0.052, val_acc:0.978]
Epoch [68/120    avg_loss:0.041, val_acc:0.978]
Epoch [69/120    avg_loss:0.043, val_acc:0.978]
Epoch [70/120    avg_loss:0.036, val_acc:0.978]
Epoch [71/120    avg_loss:0.040, val_acc:0.978]
Epoch [72/120    avg_loss:0.035, val_acc:0.978]
Epoch [73/120    avg_loss:0.047, val_acc:0.978]
Epoch [74/120    avg_loss:0.036, val_acc:0.978]
Epoch [75/120    avg_loss:0.046, val_acc:0.978]
Epoch [76/120    avg_loss:0.039, val_acc:0.976]
Epoch [77/120    avg_loss:0.031, val_acc:0.972]
Epoch [78/120    avg_loss:0.024, val_acc:0.976]
Epoch [79/120    avg_loss:0.057, val_acc:0.978]
Epoch [80/120    avg_loss:0.051, val_acc:0.978]
Epoch [81/120    avg_loss:0.043, val_acc:0.976]
Epoch [82/120    avg_loss:0.045, val_acc:0.978]
Epoch [83/120    avg_loss:0.039, val_acc:0.976]
Epoch [84/120    avg_loss:0.047, val_acc:0.978]
Epoch [85/120    avg_loss:0.042, val_acc:0.978]
Epoch [86/120    avg_loss:0.046, val_acc:0.978]
Epoch [87/120    avg_loss:0.031, val_acc:0.978]
Epoch [88/120    avg_loss:0.043, val_acc:0.974]
Epoch [89/120    avg_loss:0.034, val_acc:0.976]
Epoch [90/120    avg_loss:0.040, val_acc:0.978]
Epoch [91/120    avg_loss:0.036, val_acc:0.976]
Epoch [92/120    avg_loss:0.045, val_acc:0.978]
Epoch [93/120    avg_loss:0.031, val_acc:0.980]
Epoch [94/120    avg_loss:0.050, val_acc:0.976]
Epoch [95/120    avg_loss:0.037, val_acc:0.976]
Epoch [96/120    avg_loss:0.038, val_acc:0.978]
Epoch [97/120    avg_loss:0.024, val_acc:0.978]
Epoch [98/120    avg_loss:0.034, val_acc:0.978]
Epoch [99/120    avg_loss:0.037, val_acc:0.978]
Epoch [100/120    avg_loss:0.043, val_acc:0.978]
Epoch [101/120    avg_loss:0.038, val_acc:0.978]
Epoch [102/120    avg_loss:0.031, val_acc:0.976]
Epoch [103/120    avg_loss:0.030, val_acc:0.976]
Epoch [104/120    avg_loss:0.031, val_acc:0.978]
Epoch [105/120    avg_loss:0.032, val_acc:0.972]
Epoch [106/120    avg_loss:0.029, val_acc:0.976]
Epoch [107/120    avg_loss:0.025, val_acc:0.976]
Epoch [108/120    avg_loss:0.037, val_acc:0.976]
Epoch [109/120    avg_loss:0.030, val_acc:0.976]
Epoch [110/120    avg_loss:0.039, val_acc:0.976]
Epoch [111/120    avg_loss:0.040, val_acc:0.976]
Epoch [112/120    avg_loss:0.052, val_acc:0.976]
Epoch [113/120    avg_loss:0.032, val_acc:0.978]
Epoch [114/120    avg_loss:0.031, val_acc:0.978]
Epoch [115/120    avg_loss:0.035, val_acc:0.978]
Epoch [116/120    avg_loss:0.028, val_acc:0.978]
Epoch [117/120    avg_loss:0.030, val_acc:0.978]
Epoch [118/120    avg_loss:0.039, val_acc:0.980]
Epoch [119/120    avg_loss:0.027, val_acc:0.980]
Epoch [120/120    avg_loss:0.035, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   0   0   1   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   6 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.97757848 0.98454746 0.9375     0.93023256
 1.         0.9673913  1.         0.99893276 1.         0.99210526
 0.9877369  1.        ]

Kappa:
0.9893185633095661
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f90c627b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.146, val_acc:0.613]
Epoch [2/120    avg_loss:1.407, val_acc:0.667]
Epoch [3/120    avg_loss:1.095, val_acc:0.780]
Epoch [4/120    avg_loss:0.861, val_acc:0.744]
Epoch [5/120    avg_loss:0.703, val_acc:0.857]
Epoch [6/120    avg_loss:0.615, val_acc:0.887]
Epoch [7/120    avg_loss:0.516, val_acc:0.857]
Epoch [8/120    avg_loss:0.453, val_acc:0.867]
Epoch [9/120    avg_loss:0.407, val_acc:0.899]
Epoch [10/120    avg_loss:0.419, val_acc:0.889]
Epoch [11/120    avg_loss:0.335, val_acc:0.911]
Epoch [12/120    avg_loss:0.348, val_acc:0.919]
Epoch [13/120    avg_loss:0.434, val_acc:0.907]
Epoch [14/120    avg_loss:0.382, val_acc:0.921]
Epoch [15/120    avg_loss:0.293, val_acc:0.915]
Epoch [16/120    avg_loss:0.313, val_acc:0.885]
Epoch [17/120    avg_loss:0.292, val_acc:0.903]
Epoch [18/120    avg_loss:0.257, val_acc:0.927]
Epoch [19/120    avg_loss:0.219, val_acc:0.940]
Epoch [20/120    avg_loss:0.246, val_acc:0.937]
Epoch [21/120    avg_loss:0.182, val_acc:0.935]
Epoch [22/120    avg_loss:0.152, val_acc:0.935]
Epoch [23/120    avg_loss:0.178, val_acc:0.933]
Epoch [24/120    avg_loss:0.149, val_acc:0.935]
Epoch [25/120    avg_loss:0.182, val_acc:0.938]
Epoch [26/120    avg_loss:0.291, val_acc:0.942]
Epoch [27/120    avg_loss:0.236, val_acc:0.931]
Epoch [28/120    avg_loss:0.175, val_acc:0.952]
Epoch [29/120    avg_loss:0.130, val_acc:0.964]
Epoch [30/120    avg_loss:0.127, val_acc:0.942]
Epoch [31/120    avg_loss:0.133, val_acc:0.929]
Epoch [32/120    avg_loss:0.200, val_acc:0.913]
Epoch [33/120    avg_loss:0.175, val_acc:0.923]
Epoch [34/120    avg_loss:0.210, val_acc:0.923]
Epoch [35/120    avg_loss:0.272, val_acc:0.903]
Epoch [36/120    avg_loss:0.257, val_acc:0.935]
Epoch [37/120    avg_loss:0.160, val_acc:0.946]
Epoch [38/120    avg_loss:0.141, val_acc:0.956]
Epoch [39/120    avg_loss:0.098, val_acc:0.958]
Epoch [40/120    avg_loss:0.195, val_acc:0.948]
Epoch [41/120    avg_loss:0.157, val_acc:0.944]
Epoch [42/120    avg_loss:0.142, val_acc:0.950]
Epoch [43/120    avg_loss:0.090, val_acc:0.962]
Epoch [44/120    avg_loss:0.069, val_acc:0.970]
Epoch [45/120    avg_loss:0.057, val_acc:0.972]
Epoch [46/120    avg_loss:0.064, val_acc:0.974]
Epoch [47/120    avg_loss:0.084, val_acc:0.978]
Epoch [48/120    avg_loss:0.060, val_acc:0.974]
Epoch [49/120    avg_loss:0.039, val_acc:0.976]
Epoch [50/120    avg_loss:0.061, val_acc:0.978]
Epoch [51/120    avg_loss:0.053, val_acc:0.976]
Epoch [52/120    avg_loss:0.051, val_acc:0.976]
Epoch [53/120    avg_loss:0.046, val_acc:0.976]
Epoch [54/120    avg_loss:0.048, val_acc:0.978]
Epoch [55/120    avg_loss:0.061, val_acc:0.978]
Epoch [56/120    avg_loss:0.051, val_acc:0.978]
Epoch [57/120    avg_loss:0.065, val_acc:0.978]
Epoch [58/120    avg_loss:0.042, val_acc:0.978]
Epoch [59/120    avg_loss:0.056, val_acc:0.978]
Epoch [60/120    avg_loss:0.045, val_acc:0.978]
Epoch [61/120    avg_loss:0.054, val_acc:0.974]
Epoch [62/120    avg_loss:0.051, val_acc:0.976]
Epoch [63/120    avg_loss:0.048, val_acc:0.978]
Epoch [64/120    avg_loss:0.043, val_acc:0.978]
Epoch [65/120    avg_loss:0.034, val_acc:0.980]
Epoch [66/120    avg_loss:0.044, val_acc:0.980]
Epoch [67/120    avg_loss:0.043, val_acc:0.980]
Epoch [68/120    avg_loss:0.035, val_acc:0.980]
Epoch [69/120    avg_loss:0.026, val_acc:0.980]
Epoch [70/120    avg_loss:0.048, val_acc:0.980]
Epoch [71/120    avg_loss:0.045, val_acc:0.980]
Epoch [72/120    avg_loss:0.030, val_acc:0.980]
Epoch [73/120    avg_loss:0.033, val_acc:0.980]
Epoch [74/120    avg_loss:0.032, val_acc:0.980]
Epoch [75/120    avg_loss:0.044, val_acc:0.978]
Epoch [76/120    avg_loss:0.035, val_acc:0.980]
Epoch [77/120    avg_loss:0.036, val_acc:0.978]
Epoch [78/120    avg_loss:0.030, val_acc:0.980]
Epoch [79/120    avg_loss:0.038, val_acc:0.980]
Epoch [80/120    avg_loss:0.034, val_acc:0.980]
Epoch [81/120    avg_loss:0.027, val_acc:0.980]
Epoch [82/120    avg_loss:0.053, val_acc:0.982]
Epoch [83/120    avg_loss:0.033, val_acc:0.980]
Epoch [84/120    avg_loss:0.040, val_acc:0.978]
Epoch [85/120    avg_loss:0.031, val_acc:0.978]
Epoch [86/120    avg_loss:0.036, val_acc:0.982]
Epoch [87/120    avg_loss:0.029, val_acc:0.980]
Epoch [88/120    avg_loss:0.028, val_acc:0.978]
Epoch [89/120    avg_loss:0.035, val_acc:0.974]
Epoch [90/120    avg_loss:0.026, val_acc:0.976]
Epoch [91/120    avg_loss:0.045, val_acc:0.978]
Epoch [92/120    avg_loss:0.048, val_acc:0.978]
Epoch [93/120    avg_loss:0.031, val_acc:0.978]
Epoch [94/120    avg_loss:0.026, val_acc:0.980]
Epoch [95/120    avg_loss:0.043, val_acc:0.982]
Epoch [96/120    avg_loss:0.024, val_acc:0.982]
Epoch [97/120    avg_loss:0.038, val_acc:0.980]
Epoch [98/120    avg_loss:0.028, val_acc:0.982]
Epoch [99/120    avg_loss:0.046, val_acc:0.982]
Epoch [100/120    avg_loss:0.024, val_acc:0.982]
Epoch [101/120    avg_loss:0.018, val_acc:0.982]
Epoch [102/120    avg_loss:0.027, val_acc:0.982]
Epoch [103/120    avg_loss:0.058, val_acc:0.982]
Epoch [104/120    avg_loss:0.033, val_acc:0.982]
Epoch [105/120    avg_loss:0.037, val_acc:0.982]
Epoch [106/120    avg_loss:0.037, val_acc:0.984]
Epoch [107/120    avg_loss:0.022, val_acc:0.982]
Epoch [108/120    avg_loss:0.033, val_acc:0.984]
Epoch [109/120    avg_loss:0.026, val_acc:0.980]
Epoch [110/120    avg_loss:0.033, val_acc:0.982]
Epoch [111/120    avg_loss:0.021, val_acc:0.980]
Epoch [112/120    avg_loss:0.021, val_acc:0.982]
Epoch [113/120    avg_loss:0.031, val_acc:0.982]
Epoch [114/120    avg_loss:0.036, val_acc:0.978]
Epoch [115/120    avg_loss:0.041, val_acc:0.978]
Epoch [116/120    avg_loss:0.036, val_acc:0.980]
Epoch [117/120    avg_loss:0.030, val_acc:0.980]
Epoch [118/120    avg_loss:0.021, val_acc:0.980]
Epoch [119/120    avg_loss:0.019, val_acc:0.976]
Epoch [120/120    avg_loss:0.031, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 215  10   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.97986577 0.98230088 0.93275488 0.92361111
 1.         0.96132597 1.         0.99893276 1.         1.
 0.99558499 1.        ]

Kappa:
0.9902667494769941
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2bb5652780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.072, val_acc:0.567]
Epoch [2/120    avg_loss:1.202, val_acc:0.734]
Epoch [3/120    avg_loss:0.940, val_acc:0.827]
Epoch [4/120    avg_loss:0.806, val_acc:0.847]
Epoch [5/120    avg_loss:0.712, val_acc:0.861]
Epoch [6/120    avg_loss:0.567, val_acc:0.851]
Epoch [7/120    avg_loss:0.475, val_acc:0.883]
Epoch [8/120    avg_loss:0.488, val_acc:0.857]
Epoch [9/120    avg_loss:0.509, val_acc:0.841]
Epoch [10/120    avg_loss:0.574, val_acc:0.843]
Epoch [11/120    avg_loss:0.479, val_acc:0.897]
Epoch [12/120    avg_loss:0.418, val_acc:0.879]
Epoch [13/120    avg_loss:0.365, val_acc:0.913]
Epoch [14/120    avg_loss:0.357, val_acc:0.921]
Epoch [15/120    avg_loss:0.425, val_acc:0.887]
Epoch [16/120    avg_loss:0.471, val_acc:0.927]
Epoch [17/120    avg_loss:0.408, val_acc:0.905]
Epoch [18/120    avg_loss:0.330, val_acc:0.901]
Epoch [19/120    avg_loss:0.359, val_acc:0.917]
Epoch [20/120    avg_loss:0.355, val_acc:0.913]
Epoch [21/120    avg_loss:0.298, val_acc:0.948]
Epoch [22/120    avg_loss:0.226, val_acc:0.938]
Epoch [23/120    avg_loss:0.254, val_acc:0.935]
Epoch [24/120    avg_loss:0.288, val_acc:0.944]
Epoch [25/120    avg_loss:0.203, val_acc:0.950]
Epoch [26/120    avg_loss:0.222, val_acc:0.946]
Epoch [27/120    avg_loss:0.207, val_acc:0.954]
Epoch [28/120    avg_loss:0.140, val_acc:0.944]
Epoch [29/120    avg_loss:0.179, val_acc:0.950]
Epoch [30/120    avg_loss:0.127, val_acc:0.956]
Epoch [31/120    avg_loss:0.143, val_acc:0.970]
Epoch [32/120    avg_loss:0.131, val_acc:0.950]
Epoch [33/120    avg_loss:0.200, val_acc:0.944]
Epoch [34/120    avg_loss:0.136, val_acc:0.946]
Epoch [35/120    avg_loss:0.175, val_acc:0.946]
Epoch [36/120    avg_loss:0.160, val_acc:0.960]
Epoch [37/120    avg_loss:0.152, val_acc:0.952]
Epoch [38/120    avg_loss:0.188, val_acc:0.938]
Epoch [39/120    avg_loss:0.120, val_acc:0.968]
Epoch [40/120    avg_loss:0.071, val_acc:0.974]
Epoch [41/120    avg_loss:0.104, val_acc:0.960]
Epoch [42/120    avg_loss:0.088, val_acc:0.964]
Epoch [43/120    avg_loss:0.100, val_acc:0.962]
Epoch [44/120    avg_loss:0.094, val_acc:0.950]
Epoch [45/120    avg_loss:0.137, val_acc:0.962]
Epoch [46/120    avg_loss:0.096, val_acc:0.968]
Epoch [47/120    avg_loss:0.065, val_acc:0.964]
Epoch [48/120    avg_loss:0.066, val_acc:0.972]
Epoch [49/120    avg_loss:0.051, val_acc:0.948]
Epoch [50/120    avg_loss:0.084, val_acc:0.968]
Epoch [51/120    avg_loss:0.083, val_acc:0.978]
Epoch [52/120    avg_loss:0.073, val_acc:0.954]
Epoch [53/120    avg_loss:0.059, val_acc:0.980]
Epoch [54/120    avg_loss:0.034, val_acc:0.978]
Epoch [55/120    avg_loss:0.038, val_acc:0.980]
Epoch [56/120    avg_loss:0.051, val_acc:0.974]
Epoch [57/120    avg_loss:0.085, val_acc:0.968]
Epoch [58/120    avg_loss:0.053, val_acc:0.976]
Epoch [59/120    avg_loss:0.031, val_acc:0.982]
Epoch [60/120    avg_loss:0.086, val_acc:0.968]
Epoch [61/120    avg_loss:0.082, val_acc:0.960]
Epoch [62/120    avg_loss:0.100, val_acc:0.946]
Epoch [63/120    avg_loss:0.151, val_acc:0.960]
Epoch [64/120    avg_loss:0.094, val_acc:0.974]
Epoch [65/120    avg_loss:0.055, val_acc:0.976]
Epoch [66/120    avg_loss:0.042, val_acc:0.982]
Epoch [67/120    avg_loss:0.040, val_acc:0.980]
Epoch [68/120    avg_loss:0.043, val_acc:0.974]
Epoch [69/120    avg_loss:0.034, val_acc:0.978]
Epoch [70/120    avg_loss:0.038, val_acc:0.966]
Epoch [71/120    avg_loss:0.097, val_acc:0.974]
Epoch [72/120    avg_loss:0.065, val_acc:0.964]
Epoch [73/120    avg_loss:0.063, val_acc:0.903]
Epoch [74/120    avg_loss:0.167, val_acc:0.958]
Epoch [75/120    avg_loss:0.077, val_acc:0.970]
Epoch [76/120    avg_loss:0.047, val_acc:0.974]
Epoch [77/120    avg_loss:0.035, val_acc:0.980]
Epoch [78/120    avg_loss:0.021, val_acc:0.982]
Epoch [79/120    avg_loss:0.024, val_acc:0.982]
Epoch [80/120    avg_loss:0.018, val_acc:0.978]
Epoch [81/120    avg_loss:0.014, val_acc:0.980]
Epoch [82/120    avg_loss:0.041, val_acc:0.980]
Epoch [83/120    avg_loss:0.058, val_acc:0.984]
Epoch [84/120    avg_loss:0.050, val_acc:0.984]
Epoch [85/120    avg_loss:0.038, val_acc:0.960]
Epoch [86/120    avg_loss:0.032, val_acc:0.978]
Epoch [87/120    avg_loss:0.036, val_acc:0.978]
Epoch [88/120    avg_loss:0.074, val_acc:0.958]
Epoch [89/120    avg_loss:0.056, val_acc:0.984]
Epoch [90/120    avg_loss:0.067, val_acc:0.958]
Epoch [91/120    avg_loss:0.026, val_acc:0.978]
Epoch [92/120    avg_loss:0.041, val_acc:0.984]
Epoch [93/120    avg_loss:0.033, val_acc:0.980]
Epoch [94/120    avg_loss:0.021, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.021, val_acc:0.990]
Epoch [97/120    avg_loss:0.026, val_acc:0.978]
Epoch [98/120    avg_loss:0.040, val_acc:0.978]
Epoch [99/120    avg_loss:0.020, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.986]
Epoch [102/120    avg_loss:0.017, val_acc:0.986]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.021, val_acc:0.988]
Epoch [120/120    avg_loss:0.014, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 225   0   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 213  12   0   0   0   0   0   0   2   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   6 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.99090909 0.98901099 0.95089286 0.94983278
 0.98771499 0.99470899 0.99614891 0.99893276 1.         0.99210526
 0.98888889 1.        ]

Kappa:
0.9914544964335277
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c7ffed780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.034, val_acc:0.540]
Epoch [2/120    avg_loss:1.326, val_acc:0.687]
Epoch [3/120    avg_loss:0.942, val_acc:0.833]
Epoch [4/120    avg_loss:0.726, val_acc:0.825]
Epoch [5/120    avg_loss:0.657, val_acc:0.873]
Epoch [6/120    avg_loss:0.689, val_acc:0.861]
Epoch [7/120    avg_loss:0.560, val_acc:0.889]
Epoch [8/120    avg_loss:0.467, val_acc:0.891]
Epoch [9/120    avg_loss:0.421, val_acc:0.893]
Epoch [10/120    avg_loss:0.473, val_acc:0.879]
Epoch [11/120    avg_loss:0.396, val_acc:0.911]
Epoch [12/120    avg_loss:0.372, val_acc:0.899]
Epoch [13/120    avg_loss:0.298, val_acc:0.938]
Epoch [14/120    avg_loss:0.359, val_acc:0.911]
Epoch [15/120    avg_loss:0.365, val_acc:0.937]
Epoch [16/120    avg_loss:0.374, val_acc:0.919]
Epoch [17/120    avg_loss:0.349, val_acc:0.907]
Epoch [18/120    avg_loss:0.255, val_acc:0.948]
Epoch [19/120    avg_loss:0.201, val_acc:0.952]
Epoch [20/120    avg_loss:0.200, val_acc:0.933]
Epoch [21/120    avg_loss:0.218, val_acc:0.933]
Epoch [22/120    avg_loss:0.267, val_acc:0.929]
Epoch [23/120    avg_loss:0.202, val_acc:0.948]
Epoch [24/120    avg_loss:0.164, val_acc:0.944]
Epoch [25/120    avg_loss:0.195, val_acc:0.956]
Epoch [26/120    avg_loss:0.155, val_acc:0.964]
Epoch [27/120    avg_loss:0.186, val_acc:0.940]
Epoch [28/120    avg_loss:0.236, val_acc:0.968]
Epoch [29/120    avg_loss:0.120, val_acc:0.958]
Epoch [30/120    avg_loss:0.151, val_acc:0.964]
Epoch [31/120    avg_loss:0.159, val_acc:0.958]
Epoch [32/120    avg_loss:0.141, val_acc:0.938]
Epoch [33/120    avg_loss:0.194, val_acc:0.952]
Epoch [34/120    avg_loss:0.110, val_acc:0.966]
Epoch [35/120    avg_loss:0.136, val_acc:0.962]
Epoch [36/120    avg_loss:0.208, val_acc:0.950]
Epoch [37/120    avg_loss:0.152, val_acc:0.958]
Epoch [38/120    avg_loss:0.101, val_acc:0.966]
Epoch [39/120    avg_loss:0.144, val_acc:0.966]
Epoch [40/120    avg_loss:0.086, val_acc:0.982]
Epoch [41/120    avg_loss:0.054, val_acc:0.968]
Epoch [42/120    avg_loss:0.084, val_acc:0.968]
Epoch [43/120    avg_loss:0.098, val_acc:0.972]
Epoch [44/120    avg_loss:0.067, val_acc:0.970]
Epoch [45/120    avg_loss:0.087, val_acc:0.974]
Epoch [46/120    avg_loss:0.107, val_acc:0.970]
Epoch [47/120    avg_loss:0.093, val_acc:0.978]
Epoch [48/120    avg_loss:0.074, val_acc:0.980]
Epoch [49/120    avg_loss:0.044, val_acc:0.970]
Epoch [50/120    avg_loss:0.127, val_acc:0.980]
Epoch [51/120    avg_loss:0.091, val_acc:0.978]
Epoch [52/120    avg_loss:0.048, val_acc:0.976]
Epoch [53/120    avg_loss:0.077, val_acc:0.974]
Epoch [54/120    avg_loss:0.044, val_acc:0.974]
Epoch [55/120    avg_loss:0.045, val_acc:0.978]
Epoch [56/120    avg_loss:0.021, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.982]
Epoch [58/120    avg_loss:0.045, val_acc:0.978]
Epoch [59/120    avg_loss:0.026, val_acc:0.982]
Epoch [60/120    avg_loss:0.030, val_acc:0.982]
Epoch [61/120    avg_loss:0.024, val_acc:0.982]
Epoch [62/120    avg_loss:0.041, val_acc:0.988]
Epoch [63/120    avg_loss:0.023, val_acc:0.986]
Epoch [64/120    avg_loss:0.029, val_acc:0.988]
Epoch [65/120    avg_loss:0.031, val_acc:0.988]
Epoch [66/120    avg_loss:0.029, val_acc:0.988]
Epoch [67/120    avg_loss:0.032, val_acc:0.990]
Epoch [68/120    avg_loss:0.037, val_acc:0.990]
Epoch [69/120    avg_loss:0.036, val_acc:0.988]
Epoch [70/120    avg_loss:0.036, val_acc:0.986]
Epoch [71/120    avg_loss:0.031, val_acc:0.986]
Epoch [72/120    avg_loss:0.022, val_acc:0.986]
Epoch [73/120    avg_loss:0.020, val_acc:0.984]
Epoch [74/120    avg_loss:0.019, val_acc:0.984]
Epoch [75/120    avg_loss:0.035, val_acc:0.986]
Epoch [76/120    avg_loss:0.025, val_acc:0.986]
Epoch [77/120    avg_loss:0.031, val_acc:0.984]
Epoch [78/120    avg_loss:0.017, val_acc:0.984]
Epoch [79/120    avg_loss:0.018, val_acc:0.984]
Epoch [80/120    avg_loss:0.040, val_acc:0.980]
Epoch [81/120    avg_loss:0.023, val_acc:0.982]
Epoch [82/120    avg_loss:0.034, val_acc:0.982]
Epoch [83/120    avg_loss:0.021, val_acc:0.982]
Epoch [84/120    avg_loss:0.020, val_acc:0.982]
Epoch [85/120    avg_loss:0.028, val_acc:0.984]
Epoch [86/120    avg_loss:0.023, val_acc:0.984]
Epoch [87/120    avg_loss:0.024, val_acc:0.984]
Epoch [88/120    avg_loss:0.016, val_acc:0.984]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.017, val_acc:0.984]
Epoch [91/120    avg_loss:0.020, val_acc:0.984]
Epoch [92/120    avg_loss:0.025, val_acc:0.984]
Epoch [93/120    avg_loss:0.018, val_acc:0.984]
Epoch [94/120    avg_loss:0.023, val_acc:0.984]
Epoch [95/120    avg_loss:0.024, val_acc:0.984]
Epoch [96/120    avg_loss:0.023, val_acc:0.984]
Epoch [97/120    avg_loss:0.026, val_acc:0.984]
Epoch [98/120    avg_loss:0.027, val_acc:0.984]
Epoch [99/120    avg_loss:0.022, val_acc:0.984]
Epoch [100/120    avg_loss:0.030, val_acc:0.984]
Epoch [101/120    avg_loss:0.020, val_acc:0.984]
Epoch [102/120    avg_loss:0.027, val_acc:0.984]
Epoch [103/120    avg_loss:0.030, val_acc:0.984]
Epoch [104/120    avg_loss:0.025, val_acc:0.984]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.018, val_acc:0.984]
Epoch [107/120    avg_loss:0.022, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.984]
Epoch [109/120    avg_loss:0.027, val_acc:0.984]
Epoch [110/120    avg_loss:0.019, val_acc:0.984]
Epoch [111/120    avg_loss:0.017, val_acc:0.984]
Epoch [112/120    avg_loss:0.022, val_acc:0.984]
Epoch [113/120    avg_loss:0.022, val_acc:0.984]
Epoch [114/120    avg_loss:0.032, val_acc:0.984]
Epoch [115/120    avg_loss:0.022, val_acc:0.984]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.022, val_acc:0.984]
Epoch [119/120    avg_loss:0.015, val_acc:0.984]
Epoch [120/120    avg_loss:0.033, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   7 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.98855835 0.98678414 0.93333333 0.9122807
 1.         0.98429319 1.         1.         1.         0.99080158
 0.98996656 1.        ]

Kappa:
0.9897931052772839
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f626760a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.080, val_acc:0.550]
Epoch [2/120    avg_loss:1.301, val_acc:0.734]
Epoch [3/120    avg_loss:0.868, val_acc:0.825]
Epoch [4/120    avg_loss:0.688, val_acc:0.877]
Epoch [5/120    avg_loss:0.539, val_acc:0.871]
Epoch [6/120    avg_loss:0.523, val_acc:0.861]
Epoch [7/120    avg_loss:0.568, val_acc:0.887]
Epoch [8/120    avg_loss:0.404, val_acc:0.905]
Epoch [9/120    avg_loss:0.349, val_acc:0.875]
Epoch [10/120    avg_loss:0.376, val_acc:0.911]
Epoch [11/120    avg_loss:0.399, val_acc:0.925]
Epoch [12/120    avg_loss:0.391, val_acc:0.929]
Epoch [13/120    avg_loss:0.307, val_acc:0.948]
Epoch [14/120    avg_loss:0.304, val_acc:0.946]
Epoch [15/120    avg_loss:0.257, val_acc:0.944]
Epoch [16/120    avg_loss:0.248, val_acc:0.962]
Epoch [17/120    avg_loss:0.176, val_acc:0.950]
Epoch [18/120    avg_loss:0.210, val_acc:0.948]
Epoch [19/120    avg_loss:0.291, val_acc:0.927]
Epoch [20/120    avg_loss:0.239, val_acc:0.923]
Epoch [21/120    avg_loss:0.186, val_acc:0.946]
Epoch [22/120    avg_loss:0.179, val_acc:0.970]
Epoch [23/120    avg_loss:0.135, val_acc:0.962]
Epoch [24/120    avg_loss:0.187, val_acc:0.923]
Epoch [25/120    avg_loss:0.236, val_acc:0.929]
Epoch [26/120    avg_loss:0.186, val_acc:0.968]
Epoch [27/120    avg_loss:0.098, val_acc:0.958]
Epoch [28/120    avg_loss:0.175, val_acc:0.980]
Epoch [29/120    avg_loss:0.147, val_acc:0.964]
Epoch [30/120    avg_loss:0.148, val_acc:0.962]
Epoch [31/120    avg_loss:0.119, val_acc:0.974]
Epoch [32/120    avg_loss:0.152, val_acc:0.980]
Epoch [33/120    avg_loss:0.208, val_acc:0.956]
Epoch [34/120    avg_loss:0.155, val_acc:0.962]
Epoch [35/120    avg_loss:0.113, val_acc:0.976]
Epoch [36/120    avg_loss:0.095, val_acc:0.956]
Epoch [37/120    avg_loss:0.118, val_acc:0.962]
Epoch [38/120    avg_loss:0.117, val_acc:0.984]
Epoch [39/120    avg_loss:0.067, val_acc:0.984]
Epoch [40/120    avg_loss:0.054, val_acc:0.978]
Epoch [41/120    avg_loss:0.078, val_acc:0.980]
Epoch [42/120    avg_loss:0.080, val_acc:0.970]
Epoch [43/120    avg_loss:0.055, val_acc:0.986]
Epoch [44/120    avg_loss:0.097, val_acc:0.964]
Epoch [45/120    avg_loss:0.078, val_acc:0.990]
Epoch [46/120    avg_loss:0.047, val_acc:0.982]
Epoch [47/120    avg_loss:0.031, val_acc:0.992]
Epoch [48/120    avg_loss:0.022, val_acc:0.990]
Epoch [49/120    avg_loss:0.019, val_acc:0.990]
Epoch [50/120    avg_loss:0.021, val_acc:0.994]
Epoch [51/120    avg_loss:0.073, val_acc:0.980]
Epoch [52/120    avg_loss:0.092, val_acc:0.974]
Epoch [53/120    avg_loss:0.058, val_acc:0.984]
Epoch [54/120    avg_loss:0.043, val_acc:0.964]
Epoch [55/120    avg_loss:0.066, val_acc:0.978]
Epoch [56/120    avg_loss:0.046, val_acc:0.978]
Epoch [57/120    avg_loss:0.068, val_acc:0.978]
Epoch [58/120    avg_loss:0.033, val_acc:0.986]
Epoch [59/120    avg_loss:0.031, val_acc:0.990]
Epoch [60/120    avg_loss:0.023, val_acc:0.984]
Epoch [61/120    avg_loss:0.019, val_acc:0.992]
Epoch [62/120    avg_loss:0.022, val_acc:0.982]
Epoch [63/120    avg_loss:0.082, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.984]
Epoch [65/120    avg_loss:0.034, val_acc:0.986]
Epoch [66/120    avg_loss:0.018, val_acc:0.986]
Epoch [67/120    avg_loss:0.022, val_acc:0.986]
Epoch [68/120    avg_loss:0.018, val_acc:0.988]
Epoch [69/120    avg_loss:0.018, val_acc:0.986]
Epoch [70/120    avg_loss:0.012, val_acc:0.988]
Epoch [71/120    avg_loss:0.031, val_acc:0.988]
Epoch [72/120    avg_loss:0.033, val_acc:0.988]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.015, val_acc:0.988]
Epoch [78/120    avg_loss:0.014, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.988]
Epoch [80/120    avg_loss:0.022, val_acc:0.988]
Epoch [81/120    avg_loss:0.015, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.988]
Epoch [83/120    avg_loss:0.020, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.017, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.024, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.013, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.018, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.013, val_acc:0.988]
Epoch [99/120    avg_loss:0.019, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.012, val_acc:0.988]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.016, val_acc:0.988]
Epoch [104/120    avg_loss:0.012, val_acc:0.988]
Epoch [105/120    avg_loss:0.014, val_acc:0.988]
Epoch [106/120    avg_loss:0.012, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.012, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.028, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.013, val_acc:0.988]
Epoch [114/120    avg_loss:0.018, val_acc:0.988]
Epoch [115/120    avg_loss:0.016, val_acc:0.988]
Epoch [116/120    avg_loss:0.019, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.988]
Epoch [118/120    avg_loss:0.012, val_acc:0.988]
Epoch [119/120    avg_loss:0.015, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  12   0   0   0   0   0   0   3   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         1.         0.98004435 0.9380531  0.9602649
 0.99019608 1.         1.         1.         1.         0.9986755
 0.99559471 1.        ]

Kappa:
0.993116051049839
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f5e75c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.143, val_acc:0.546]
Epoch [2/120    avg_loss:1.337, val_acc:0.771]
Epoch [3/120    avg_loss:0.890, val_acc:0.804]
Epoch [4/120    avg_loss:0.806, val_acc:0.827]
Epoch [5/120    avg_loss:0.745, val_acc:0.802]
Epoch [6/120    avg_loss:0.662, val_acc:0.762]
Epoch [7/120    avg_loss:0.619, val_acc:0.831]
Epoch [8/120    avg_loss:0.544, val_acc:0.848]
Epoch [9/120    avg_loss:0.477, val_acc:0.875]
Epoch [10/120    avg_loss:0.401, val_acc:0.915]
Epoch [11/120    avg_loss:0.347, val_acc:0.925]
Epoch [12/120    avg_loss:0.311, val_acc:0.925]
Epoch [13/120    avg_loss:0.272, val_acc:0.931]
Epoch [14/120    avg_loss:0.271, val_acc:0.942]
Epoch [15/120    avg_loss:0.307, val_acc:0.933]
Epoch [16/120    avg_loss:0.281, val_acc:0.931]
Epoch [17/120    avg_loss:0.286, val_acc:0.902]
Epoch [18/120    avg_loss:0.327, val_acc:0.929]
Epoch [19/120    avg_loss:0.342, val_acc:0.938]
Epoch [20/120    avg_loss:0.259, val_acc:0.912]
Epoch [21/120    avg_loss:0.228, val_acc:0.927]
Epoch [22/120    avg_loss:0.253, val_acc:0.948]
Epoch [23/120    avg_loss:0.207, val_acc:0.963]
Epoch [24/120    avg_loss:0.142, val_acc:0.946]
Epoch [25/120    avg_loss:0.200, val_acc:0.944]
Epoch [26/120    avg_loss:0.228, val_acc:0.931]
Epoch [27/120    avg_loss:0.169, val_acc:0.960]
Epoch [28/120    avg_loss:0.143, val_acc:0.965]
Epoch [29/120    avg_loss:0.156, val_acc:0.954]
Epoch [30/120    avg_loss:0.167, val_acc:0.956]
Epoch [31/120    avg_loss:0.128, val_acc:0.940]
Epoch [32/120    avg_loss:0.128, val_acc:0.960]
Epoch [33/120    avg_loss:0.118, val_acc:0.973]
Epoch [34/120    avg_loss:0.124, val_acc:0.958]
Epoch [35/120    avg_loss:0.154, val_acc:0.960]
Epoch [36/120    avg_loss:0.068, val_acc:0.979]
Epoch [37/120    avg_loss:0.056, val_acc:0.973]
Epoch [38/120    avg_loss:0.050, val_acc:0.975]
Epoch [39/120    avg_loss:0.051, val_acc:0.977]
Epoch [40/120    avg_loss:0.180, val_acc:0.948]
Epoch [41/120    avg_loss:0.109, val_acc:0.954]
Epoch [42/120    avg_loss:0.085, val_acc:0.969]
Epoch [43/120    avg_loss:0.102, val_acc:0.954]
Epoch [44/120    avg_loss:0.071, val_acc:0.973]
Epoch [45/120    avg_loss:0.083, val_acc:0.973]
Epoch [46/120    avg_loss:0.051, val_acc:0.983]
Epoch [47/120    avg_loss:0.057, val_acc:0.979]
Epoch [48/120    avg_loss:0.048, val_acc:0.973]
Epoch [49/120    avg_loss:0.060, val_acc:0.979]
Epoch [50/120    avg_loss:0.041, val_acc:0.990]
Epoch [51/120    avg_loss:0.053, val_acc:0.988]
Epoch [52/120    avg_loss:0.083, val_acc:0.979]
Epoch [53/120    avg_loss:0.077, val_acc:0.969]
Epoch [54/120    avg_loss:0.150, val_acc:0.979]
Epoch [55/120    avg_loss:0.053, val_acc:0.981]
Epoch [56/120    avg_loss:0.047, val_acc:0.983]
Epoch [57/120    avg_loss:0.041, val_acc:0.988]
Epoch [58/120    avg_loss:0.028, val_acc:0.985]
Epoch [59/120    avg_loss:0.037, val_acc:0.992]
Epoch [60/120    avg_loss:0.060, val_acc:0.992]
Epoch [61/120    avg_loss:0.044, val_acc:0.988]
Epoch [62/120    avg_loss:0.029, val_acc:0.988]
Epoch [63/120    avg_loss:0.040, val_acc:0.994]
Epoch [64/120    avg_loss:0.021, val_acc:0.996]
Epoch [65/120    avg_loss:0.028, val_acc:0.996]
Epoch [66/120    avg_loss:0.014, val_acc:0.996]
Epoch [67/120    avg_loss:0.026, val_acc:0.992]
Epoch [68/120    avg_loss:0.041, val_acc:0.971]
Epoch [69/120    avg_loss:0.031, val_acc:0.992]
Epoch [70/120    avg_loss:0.015, val_acc:0.990]
Epoch [71/120    avg_loss:0.015, val_acc:0.996]
Epoch [72/120    avg_loss:0.013, val_acc:0.996]
Epoch [73/120    avg_loss:0.015, val_acc:0.985]
Epoch [74/120    avg_loss:0.033, val_acc:0.994]
Epoch [75/120    avg_loss:0.028, val_acc:0.985]
Epoch [76/120    avg_loss:0.016, val_acc:0.996]
Epoch [77/120    avg_loss:0.010, val_acc:0.994]
Epoch [78/120    avg_loss:0.007, val_acc:0.996]
Epoch [79/120    avg_loss:0.008, val_acc:0.996]
Epoch [80/120    avg_loss:0.011, val_acc:1.000]
Epoch [81/120    avg_loss:0.009, val_acc:1.000]
Epoch [82/120    avg_loss:0.008, val_acc:0.998]
Epoch [83/120    avg_loss:0.007, val_acc:1.000]
Epoch [84/120    avg_loss:0.005, val_acc:1.000]
Epoch [85/120    avg_loss:0.008, val_acc:1.000]
Epoch [86/120    avg_loss:0.004, val_acc:0.998]
Epoch [87/120    avg_loss:0.024, val_acc:0.990]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.011, val_acc:0.988]
Epoch [90/120    avg_loss:0.038, val_acc:0.983]
Epoch [91/120    avg_loss:0.033, val_acc:0.985]
Epoch [92/120    avg_loss:0.017, val_acc:0.998]
Epoch [93/120    avg_loss:0.006, val_acc:0.998]
Epoch [94/120    avg_loss:0.018, val_acc:0.996]
Epoch [95/120    avg_loss:0.008, val_acc:0.998]
Epoch [96/120    avg_loss:0.011, val_acc:0.998]
Epoch [97/120    avg_loss:0.008, val_acc:0.998]
Epoch [98/120    avg_loss:0.011, val_acc:0.996]
Epoch [99/120    avg_loss:0.008, val_acc:0.998]
Epoch [100/120    avg_loss:0.008, val_acc:0.998]
Epoch [101/120    avg_loss:0.005, val_acc:0.998]
Epoch [102/120    avg_loss:0.005, val_acc:0.998]
Epoch [103/120    avg_loss:0.007, val_acc:0.998]
Epoch [104/120    avg_loss:0.006, val_acc:0.998]
Epoch [105/120    avg_loss:0.007, val_acc:0.998]
Epoch [106/120    avg_loss:0.010, val_acc:0.998]
Epoch [107/120    avg_loss:0.004, val_acc:0.998]
Epoch [108/120    avg_loss:0.006, val_acc:0.998]
Epoch [109/120    avg_loss:0.012, val_acc:0.998]
Epoch [110/120    avg_loss:0.004, val_acc:0.998]
Epoch [111/120    avg_loss:0.005, val_acc:0.998]
Epoch [112/120    avg_loss:0.006, val_acc:0.998]
Epoch [113/120    avg_loss:0.003, val_acc:0.998]
Epoch [114/120    avg_loss:0.006, val_acc:0.998]
Epoch [115/120    avg_loss:0.009, val_acc:0.998]
Epoch [116/120    avg_loss:0.005, val_acc:0.998]
Epoch [117/120    avg_loss:0.006, val_acc:0.998]
Epoch [118/120    avg_loss:0.005, val_acc:0.998]
Epoch [119/120    avg_loss:0.004, val_acc:0.998]
Epoch [120/120    avg_loss:0.005, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 1.         1.         0.99782135 0.96888889 0.95238095
 1.         1.         1.         0.99893276 1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9952524838417427
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b405a6710>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.081, val_acc:0.629]
Epoch [2/120    avg_loss:1.217, val_acc:0.722]
Epoch [3/120    avg_loss:0.872, val_acc:0.756]
Epoch [4/120    avg_loss:0.684, val_acc:0.833]
Epoch [5/120    avg_loss:0.782, val_acc:0.861]
Epoch [6/120    avg_loss:0.565, val_acc:0.833]
Epoch [7/120    avg_loss:0.497, val_acc:0.831]
Epoch [8/120    avg_loss:0.478, val_acc:0.903]
Epoch [9/120    avg_loss:0.388, val_acc:0.919]
Epoch [10/120    avg_loss:0.427, val_acc:0.919]
Epoch [11/120    avg_loss:0.374, val_acc:0.915]
Epoch [12/120    avg_loss:0.336, val_acc:0.925]
Epoch [13/120    avg_loss:0.263, val_acc:0.921]
Epoch [14/120    avg_loss:0.349, val_acc:0.909]
Epoch [15/120    avg_loss:0.370, val_acc:0.929]
Epoch [16/120    avg_loss:0.338, val_acc:0.944]
Epoch [17/120    avg_loss:0.257, val_acc:0.938]
Epoch [18/120    avg_loss:0.276, val_acc:0.942]
Epoch [19/120    avg_loss:0.203, val_acc:0.938]
Epoch [20/120    avg_loss:0.225, val_acc:0.940]
Epoch [21/120    avg_loss:0.219, val_acc:0.929]
Epoch [22/120    avg_loss:0.189, val_acc:0.956]
Epoch [23/120    avg_loss:0.166, val_acc:0.958]
Epoch [24/120    avg_loss:0.191, val_acc:0.933]
Epoch [25/120    avg_loss:0.215, val_acc:0.946]
Epoch [26/120    avg_loss:0.135, val_acc:0.968]
Epoch [27/120    avg_loss:0.172, val_acc:0.950]
Epoch [28/120    avg_loss:0.152, val_acc:0.929]
Epoch [29/120    avg_loss:0.124, val_acc:0.960]
Epoch [30/120    avg_loss:0.106, val_acc:0.962]
Epoch [31/120    avg_loss:0.136, val_acc:0.958]
Epoch [32/120    avg_loss:0.128, val_acc:0.966]
Epoch [33/120    avg_loss:0.179, val_acc:0.933]
Epoch [34/120    avg_loss:0.137, val_acc:0.954]
Epoch [35/120    avg_loss:0.103, val_acc:0.962]
Epoch [36/120    avg_loss:0.129, val_acc:0.972]
Epoch [37/120    avg_loss:0.134, val_acc:0.950]
Epoch [38/120    avg_loss:0.075, val_acc:0.964]
Epoch [39/120    avg_loss:0.092, val_acc:0.958]
Epoch [40/120    avg_loss:0.074, val_acc:0.958]
Epoch [41/120    avg_loss:0.084, val_acc:0.980]
Epoch [42/120    avg_loss:0.072, val_acc:0.974]
Epoch [43/120    avg_loss:0.086, val_acc:0.968]
Epoch [44/120    avg_loss:0.055, val_acc:0.982]
Epoch [45/120    avg_loss:0.065, val_acc:0.984]
Epoch [46/120    avg_loss:0.029, val_acc:0.984]
Epoch [47/120    avg_loss:0.037, val_acc:0.978]
Epoch [48/120    avg_loss:0.037, val_acc:0.982]
Epoch [49/120    avg_loss:0.016, val_acc:0.980]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.056, val_acc:0.954]
Epoch [52/120    avg_loss:0.124, val_acc:0.966]
Epoch [53/120    avg_loss:0.055, val_acc:0.976]
Epoch [54/120    avg_loss:0.063, val_acc:0.974]
Epoch [55/120    avg_loss:0.035, val_acc:0.984]
Epoch [56/120    avg_loss:0.054, val_acc:0.968]
Epoch [57/120    avg_loss:0.074, val_acc:0.976]
Epoch [58/120    avg_loss:0.061, val_acc:0.976]
Epoch [59/120    avg_loss:0.046, val_acc:0.982]
Epoch [60/120    avg_loss:0.049, val_acc:0.978]
Epoch [61/120    avg_loss:0.056, val_acc:0.976]
Epoch [62/120    avg_loss:0.041, val_acc:0.980]
Epoch [63/120    avg_loss:0.057, val_acc:0.982]
Epoch [64/120    avg_loss:0.021, val_acc:0.988]
Epoch [65/120    avg_loss:0.045, val_acc:0.982]
Epoch [66/120    avg_loss:0.096, val_acc:0.980]
Epoch [67/120    avg_loss:0.079, val_acc:0.972]
Epoch [68/120    avg_loss:0.068, val_acc:0.972]
Epoch [69/120    avg_loss:0.098, val_acc:0.984]
Epoch [70/120    avg_loss:0.076, val_acc:0.966]
Epoch [71/120    avg_loss:0.080, val_acc:0.982]
Epoch [72/120    avg_loss:0.034, val_acc:0.970]
Epoch [73/120    avg_loss:0.028, val_acc:0.988]
Epoch [74/120    avg_loss:0.028, val_acc:0.986]
Epoch [75/120    avg_loss:0.023, val_acc:0.980]
Epoch [76/120    avg_loss:0.028, val_acc:0.980]
Epoch [77/120    avg_loss:0.020, val_acc:0.994]
Epoch [78/120    avg_loss:0.017, val_acc:0.990]
Epoch [79/120    avg_loss:0.016, val_acc:0.988]
Epoch [80/120    avg_loss:0.017, val_acc:0.982]
Epoch [81/120    avg_loss:0.018, val_acc:0.984]
Epoch [82/120    avg_loss:0.035, val_acc:0.986]
Epoch [83/120    avg_loss:0.032, val_acc:0.978]
Epoch [84/120    avg_loss:0.023, val_acc:0.984]
Epoch [85/120    avg_loss:0.014, val_acc:0.982]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.031, val_acc:0.980]
Epoch [88/120    avg_loss:0.020, val_acc:0.988]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.015, val_acc:0.980]
Epoch [92/120    avg_loss:0.014, val_acc:0.980]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.984]
Epoch [117/120    avg_loss:0.019, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 211  19   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  11   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         1.         0.9569161  0.90677966 0.92041522
 1.         1.         1.         1.         1.         0.9843342
 0.984375   1.        ]

Kappa:
0.9867069312087233
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4cf365860>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.117, val_acc:0.579]
Epoch [2/120    avg_loss:1.348, val_acc:0.764]
Epoch [3/120    avg_loss:0.951, val_acc:0.722]
Epoch [4/120    avg_loss:0.867, val_acc:0.812]
Epoch [5/120    avg_loss:0.684, val_acc:0.847]
Epoch [6/120    avg_loss:0.533, val_acc:0.823]
Epoch [7/120    avg_loss:0.492, val_acc:0.853]
Epoch [8/120    avg_loss:0.496, val_acc:0.881]
Epoch [9/120    avg_loss:0.558, val_acc:0.909]
Epoch [10/120    avg_loss:0.434, val_acc:0.887]
Epoch [11/120    avg_loss:0.464, val_acc:0.875]
Epoch [12/120    avg_loss:0.387, val_acc:0.911]
Epoch [13/120    avg_loss:0.320, val_acc:0.921]
Epoch [14/120    avg_loss:0.357, val_acc:0.938]
Epoch [15/120    avg_loss:0.303, val_acc:0.923]
Epoch [16/120    avg_loss:0.236, val_acc:0.956]
Epoch [17/120    avg_loss:0.288, val_acc:0.942]
Epoch [18/120    avg_loss:0.284, val_acc:0.944]
Epoch [19/120    avg_loss:0.222, val_acc:0.933]
Epoch [20/120    avg_loss:0.224, val_acc:0.938]
Epoch [21/120    avg_loss:0.212, val_acc:0.960]
Epoch [22/120    avg_loss:0.164, val_acc:0.954]
Epoch [23/120    avg_loss:0.093, val_acc:0.964]
Epoch [24/120    avg_loss:0.135, val_acc:0.944]
Epoch [25/120    avg_loss:0.129, val_acc:0.942]
Epoch [26/120    avg_loss:0.189, val_acc:0.962]
Epoch [27/120    avg_loss:0.200, val_acc:0.933]
Epoch [28/120    avg_loss:0.117, val_acc:0.968]
Epoch [29/120    avg_loss:0.107, val_acc:0.962]
Epoch [30/120    avg_loss:0.151, val_acc:0.970]
Epoch [31/120    avg_loss:0.208, val_acc:0.938]
Epoch [32/120    avg_loss:0.158, val_acc:0.964]
Epoch [33/120    avg_loss:0.125, val_acc:0.958]
Epoch [34/120    avg_loss:0.178, val_acc:0.970]
Epoch [35/120    avg_loss:0.088, val_acc:0.974]
Epoch [36/120    avg_loss:0.045, val_acc:0.974]
Epoch [37/120    avg_loss:0.080, val_acc:0.972]
Epoch [38/120    avg_loss:0.155, val_acc:0.952]
Epoch [39/120    avg_loss:0.102, val_acc:0.954]
Epoch [40/120    avg_loss:0.104, val_acc:0.964]
Epoch [41/120    avg_loss:0.072, val_acc:0.956]
Epoch [42/120    avg_loss:0.079, val_acc:0.972]
Epoch [43/120    avg_loss:0.063, val_acc:0.982]
Epoch [44/120    avg_loss:0.080, val_acc:0.980]
Epoch [45/120    avg_loss:0.104, val_acc:0.974]
Epoch [46/120    avg_loss:0.055, val_acc:0.982]
Epoch [47/120    avg_loss:0.074, val_acc:0.974]
Epoch [48/120    avg_loss:0.107, val_acc:0.980]
Epoch [49/120    avg_loss:0.086, val_acc:0.984]
Epoch [50/120    avg_loss:0.034, val_acc:0.986]
Epoch [51/120    avg_loss:0.037, val_acc:0.986]
Epoch [52/120    avg_loss:0.047, val_acc:0.986]
Epoch [53/120    avg_loss:0.092, val_acc:0.976]
Epoch [54/120    avg_loss:0.053, val_acc:0.982]
Epoch [55/120    avg_loss:0.073, val_acc:0.964]
Epoch [56/120    avg_loss:0.075, val_acc:0.978]
Epoch [57/120    avg_loss:0.120, val_acc:0.958]
Epoch [58/120    avg_loss:0.132, val_acc:0.980]
Epoch [59/120    avg_loss:0.083, val_acc:0.976]
Epoch [60/120    avg_loss:0.056, val_acc:0.980]
Epoch [61/120    avg_loss:0.043, val_acc:0.984]
Epoch [62/120    avg_loss:0.039, val_acc:0.980]
Epoch [63/120    avg_loss:0.041, val_acc:0.986]
Epoch [64/120    avg_loss:0.033, val_acc:0.988]
Epoch [65/120    avg_loss:0.028, val_acc:0.984]
Epoch [66/120    avg_loss:0.036, val_acc:0.986]
Epoch [67/120    avg_loss:0.024, val_acc:0.986]
Epoch [68/120    avg_loss:0.026, val_acc:0.988]
Epoch [69/120    avg_loss:0.020, val_acc:0.982]
Epoch [70/120    avg_loss:0.033, val_acc:0.990]
Epoch [71/120    avg_loss:0.021, val_acc:0.988]
Epoch [72/120    avg_loss:0.068, val_acc:0.984]
Epoch [73/120    avg_loss:0.019, val_acc:0.992]
Epoch [74/120    avg_loss:0.021, val_acc:0.986]
Epoch [75/120    avg_loss:0.015, val_acc:0.986]
Epoch [76/120    avg_loss:0.024, val_acc:0.988]
Epoch [77/120    avg_loss:0.023, val_acc:0.994]
Epoch [78/120    avg_loss:0.019, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.992]
Epoch [80/120    avg_loss:0.014, val_acc:0.978]
Epoch [81/120    avg_loss:0.035, val_acc:0.976]
Epoch [82/120    avg_loss:0.024, val_acc:0.986]
Epoch [83/120    avg_loss:0.013, val_acc:0.988]
Epoch [84/120    avg_loss:0.014, val_acc:0.988]
Epoch [85/120    avg_loss:0.019, val_acc:0.990]
Epoch [86/120    avg_loss:0.013, val_acc:0.992]
Epoch [87/120    avg_loss:0.014, val_acc:0.994]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.022, val_acc:0.990]
Epoch [90/120    avg_loss:0.030, val_acc:0.984]
Epoch [91/120    avg_loss:0.032, val_acc:0.984]
Epoch [92/120    avg_loss:0.032, val_acc:0.984]
Epoch [93/120    avg_loss:0.018, val_acc:0.986]
Epoch [94/120    avg_loss:0.017, val_acc:0.986]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.022, val_acc:0.990]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.010, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  11   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 1.         1.         1.         0.96583144 0.96345515
 1.         1.         1.         1.         1.         0.99341238
 0.99005525 1.        ]

Kappa:
0.9952524617352465
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff59e76780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.096, val_acc:0.562]
Epoch [2/120    avg_loss:1.322, val_acc:0.752]
Epoch [3/120    avg_loss:1.008, val_acc:0.752]
Epoch [4/120    avg_loss:0.823, val_acc:0.794]
Epoch [5/120    avg_loss:0.820, val_acc:0.812]
Epoch [6/120    avg_loss:0.656, val_acc:0.859]
Epoch [7/120    avg_loss:0.555, val_acc:0.853]
Epoch [8/120    avg_loss:0.449, val_acc:0.875]
Epoch [9/120    avg_loss:0.470, val_acc:0.849]
Epoch [10/120    avg_loss:0.423, val_acc:0.887]
Epoch [11/120    avg_loss:0.429, val_acc:0.903]
Epoch [12/120    avg_loss:0.332, val_acc:0.839]
Epoch [13/120    avg_loss:0.382, val_acc:0.927]
Epoch [14/120    avg_loss:0.450, val_acc:0.867]
Epoch [15/120    avg_loss:0.404, val_acc:0.933]
Epoch [16/120    avg_loss:0.330, val_acc:0.944]
Epoch [17/120    avg_loss:0.256, val_acc:0.948]
Epoch [18/120    avg_loss:0.235, val_acc:0.937]
Epoch [19/120    avg_loss:0.246, val_acc:0.937]
Epoch [20/120    avg_loss:0.202, val_acc:0.909]
Epoch [21/120    avg_loss:0.244, val_acc:0.944]
Epoch [22/120    avg_loss:0.264, val_acc:0.925]
Epoch [23/120    avg_loss:0.200, val_acc:0.954]
Epoch [24/120    avg_loss:0.178, val_acc:0.950]
Epoch [25/120    avg_loss:0.148, val_acc:0.962]
Epoch [26/120    avg_loss:0.102, val_acc:0.954]
Epoch [27/120    avg_loss:0.125, val_acc:0.970]
Epoch [28/120    avg_loss:0.106, val_acc:0.948]
Epoch [29/120    avg_loss:0.179, val_acc:0.944]
Epoch [30/120    avg_loss:0.110, val_acc:0.966]
Epoch [31/120    avg_loss:0.155, val_acc:0.964]
Epoch [32/120    avg_loss:0.177, val_acc:0.952]
Epoch [33/120    avg_loss:0.169, val_acc:0.958]
Epoch [34/120    avg_loss:0.144, val_acc:0.964]
Epoch [35/120    avg_loss:0.109, val_acc:0.956]
Epoch [36/120    avg_loss:0.132, val_acc:0.974]
Epoch [37/120    avg_loss:0.109, val_acc:0.970]
Epoch [38/120    avg_loss:0.098, val_acc:0.970]
Epoch [39/120    avg_loss:0.098, val_acc:0.966]
Epoch [40/120    avg_loss:0.079, val_acc:0.970]
Epoch [41/120    avg_loss:0.069, val_acc:0.978]
Epoch [42/120    avg_loss:0.064, val_acc:0.978]
Epoch [43/120    avg_loss:0.039, val_acc:0.978]
Epoch [44/120    avg_loss:0.064, val_acc:0.982]
Epoch [45/120    avg_loss:0.089, val_acc:0.966]
Epoch [46/120    avg_loss:0.078, val_acc:0.974]
Epoch [47/120    avg_loss:0.063, val_acc:0.978]
Epoch [48/120    avg_loss:0.039, val_acc:0.980]
Epoch [49/120    avg_loss:0.023, val_acc:0.976]
Epoch [50/120    avg_loss:0.030, val_acc:0.980]
Epoch [51/120    avg_loss:0.035, val_acc:0.974]
Epoch [52/120    avg_loss:0.033, val_acc:0.976]
Epoch [53/120    avg_loss:0.049, val_acc:0.978]
Epoch [54/120    avg_loss:0.029, val_acc:0.976]
Epoch [55/120    avg_loss:0.024, val_acc:0.984]
Epoch [56/120    avg_loss:0.037, val_acc:0.980]
Epoch [57/120    avg_loss:0.041, val_acc:0.974]
Epoch [58/120    avg_loss:0.048, val_acc:0.982]
Epoch [59/120    avg_loss:0.029, val_acc:0.974]
Epoch [60/120    avg_loss:0.035, val_acc:0.982]
Epoch [61/120    avg_loss:0.051, val_acc:0.984]
Epoch [62/120    avg_loss:0.025, val_acc:0.978]
Epoch [63/120    avg_loss:0.018, val_acc:0.984]
Epoch [64/120    avg_loss:0.024, val_acc:0.984]
Epoch [65/120    avg_loss:0.023, val_acc:0.978]
Epoch [66/120    avg_loss:0.013, val_acc:0.986]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.980]
Epoch [69/120    avg_loss:0.016, val_acc:0.984]
Epoch [70/120    avg_loss:0.015, val_acc:0.986]
Epoch [71/120    avg_loss:0.015, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.982]
Epoch [73/120    avg_loss:0.006, val_acc:0.984]
Epoch [74/120    avg_loss:0.022, val_acc:0.980]
Epoch [75/120    avg_loss:0.060, val_acc:0.980]
Epoch [76/120    avg_loss:0.042, val_acc:0.984]
Epoch [77/120    avg_loss:0.034, val_acc:0.986]
Epoch [78/120    avg_loss:0.029, val_acc:0.982]
Epoch [79/120    avg_loss:0.019, val_acc:0.984]
Epoch [80/120    avg_loss:0.023, val_acc:0.986]
Epoch [81/120    avg_loss:0.016, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.012, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.990]
Epoch [91/120    avg_loss:0.042, val_acc:0.974]
Epoch [92/120    avg_loss:0.050, val_acc:0.982]
Epoch [93/120    avg_loss:0.026, val_acc:0.980]
Epoch [94/120    avg_loss:0.014, val_acc:0.984]
Epoch [95/120    avg_loss:0.018, val_acc:0.990]
Epoch [96/120    avg_loss:0.015, val_acc:0.984]
Epoch [97/120    avg_loss:0.040, val_acc:0.988]
Epoch [98/120    avg_loss:0.028, val_acc:0.982]
Epoch [99/120    avg_loss:0.027, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.976]
Epoch [105/120    avg_loss:0.005, val_acc:0.976]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         1.         0.99782135 0.95424837 0.92631579
 1.         1.         1.         0.99893276 1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9935906137795882
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6cc30d9780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.121, val_acc:0.585]
Epoch [2/120    avg_loss:1.260, val_acc:0.738]
Epoch [3/120    avg_loss:0.884, val_acc:0.800]
Epoch [4/120    avg_loss:0.827, val_acc:0.754]
Epoch [5/120    avg_loss:0.663, val_acc:0.817]
Epoch [6/120    avg_loss:0.522, val_acc:0.863]
Epoch [7/120    avg_loss:0.453, val_acc:0.875]
Epoch [8/120    avg_loss:0.503, val_acc:0.852]
Epoch [9/120    avg_loss:0.400, val_acc:0.892]
Epoch [10/120    avg_loss:0.373, val_acc:0.900]
Epoch [11/120    avg_loss:0.321, val_acc:0.906]
Epoch [12/120    avg_loss:0.447, val_acc:0.933]
Epoch [13/120    avg_loss:0.394, val_acc:0.921]
Epoch [14/120    avg_loss:0.216, val_acc:0.927]
Epoch [15/120    avg_loss:0.315, val_acc:0.917]
Epoch [16/120    avg_loss:0.256, val_acc:0.946]
Epoch [17/120    avg_loss:0.204, val_acc:0.929]
Epoch [18/120    avg_loss:0.313, val_acc:0.950]
Epoch [19/120    avg_loss:0.265, val_acc:0.925]
Epoch [20/120    avg_loss:0.242, val_acc:0.867]
Epoch [21/120    avg_loss:0.239, val_acc:0.898]
Epoch [22/120    avg_loss:0.212, val_acc:0.938]
Epoch [23/120    avg_loss:0.153, val_acc:0.940]
Epoch [24/120    avg_loss:0.199, val_acc:0.946]
Epoch [25/120    avg_loss:0.115, val_acc:0.942]
Epoch [26/120    avg_loss:0.134, val_acc:0.954]
Epoch [27/120    avg_loss:0.159, val_acc:0.965]
Epoch [28/120    avg_loss:0.079, val_acc:0.969]
Epoch [29/120    avg_loss:0.103, val_acc:0.963]
Epoch [30/120    avg_loss:0.075, val_acc:0.954]
Epoch [31/120    avg_loss:0.124, val_acc:0.942]
Epoch [32/120    avg_loss:0.195, val_acc:0.965]
Epoch [33/120    avg_loss:0.112, val_acc:0.965]
Epoch [34/120    avg_loss:0.147, val_acc:0.967]
Epoch [35/120    avg_loss:0.092, val_acc:0.973]
Epoch [36/120    avg_loss:0.075, val_acc:0.967]
Epoch [37/120    avg_loss:0.140, val_acc:0.944]
Epoch [38/120    avg_loss:0.084, val_acc:0.969]
Epoch [39/120    avg_loss:0.079, val_acc:0.967]
Epoch [40/120    avg_loss:0.097, val_acc:0.944]
Epoch [41/120    avg_loss:0.060, val_acc:0.971]
Epoch [42/120    avg_loss:0.056, val_acc:0.969]
Epoch [43/120    avg_loss:0.061, val_acc:0.965]
Epoch [44/120    avg_loss:0.075, val_acc:0.969]
Epoch [45/120    avg_loss:0.069, val_acc:0.967]
Epoch [46/120    avg_loss:0.065, val_acc:0.969]
Epoch [47/120    avg_loss:0.047, val_acc:0.960]
Epoch [48/120    avg_loss:0.078, val_acc:0.969]
Epoch [49/120    avg_loss:0.064, val_acc:0.979]
Epoch [50/120    avg_loss:0.039, val_acc:0.983]
Epoch [51/120    avg_loss:0.022, val_acc:0.979]
Epoch [52/120    avg_loss:0.025, val_acc:0.977]
Epoch [53/120    avg_loss:0.024, val_acc:0.977]
Epoch [54/120    avg_loss:0.019, val_acc:0.979]
Epoch [55/120    avg_loss:0.027, val_acc:0.977]
Epoch [56/120    avg_loss:0.022, val_acc:0.975]
Epoch [57/120    avg_loss:0.040, val_acc:0.979]
Epoch [58/120    avg_loss:0.025, val_acc:0.979]
Epoch [59/120    avg_loss:0.022, val_acc:0.979]
Epoch [60/120    avg_loss:0.027, val_acc:0.981]
Epoch [61/120    avg_loss:0.021, val_acc:0.975]
Epoch [62/120    avg_loss:0.021, val_acc:0.975]
Epoch [63/120    avg_loss:0.024, val_acc:0.977]
Epoch [64/120    avg_loss:0.029, val_acc:0.977]
Epoch [65/120    avg_loss:0.034, val_acc:0.977]
Epoch [66/120    avg_loss:0.029, val_acc:0.977]
Epoch [67/120    avg_loss:0.020, val_acc:0.977]
Epoch [68/120    avg_loss:0.025, val_acc:0.977]
Epoch [69/120    avg_loss:0.021, val_acc:0.977]
Epoch [70/120    avg_loss:0.016, val_acc:0.977]
Epoch [71/120    avg_loss:0.020, val_acc:0.979]
Epoch [72/120    avg_loss:0.027, val_acc:0.979]
Epoch [73/120    avg_loss:0.035, val_acc:0.979]
Epoch [74/120    avg_loss:0.037, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.977]
Epoch [76/120    avg_loss:0.023, val_acc:0.977]
Epoch [77/120    avg_loss:0.021, val_acc:0.977]
Epoch [78/120    avg_loss:0.020, val_acc:0.977]
Epoch [79/120    avg_loss:0.025, val_acc:0.977]
Epoch [80/120    avg_loss:0.026, val_acc:0.977]
Epoch [81/120    avg_loss:0.020, val_acc:0.977]
Epoch [82/120    avg_loss:0.019, val_acc:0.977]
Epoch [83/120    avg_loss:0.030, val_acc:0.977]
Epoch [84/120    avg_loss:0.022, val_acc:0.977]
Epoch [85/120    avg_loss:0.020, val_acc:0.977]
Epoch [86/120    avg_loss:0.037, val_acc:0.977]
Epoch [87/120    avg_loss:0.019, val_acc:0.977]
Epoch [88/120    avg_loss:0.034, val_acc:0.977]
Epoch [89/120    avg_loss:0.025, val_acc:0.977]
Epoch [90/120    avg_loss:0.022, val_acc:0.977]
Epoch [91/120    avg_loss:0.034, val_acc:0.977]
Epoch [92/120    avg_loss:0.029, val_acc:0.977]
Epoch [93/120    avg_loss:0.037, val_acc:0.977]
Epoch [94/120    avg_loss:0.019, val_acc:0.977]
Epoch [95/120    avg_loss:0.033, val_acc:0.977]
Epoch [96/120    avg_loss:0.015, val_acc:0.977]
Epoch [97/120    avg_loss:0.033, val_acc:0.977]
Epoch [98/120    avg_loss:0.022, val_acc:0.977]
Epoch [99/120    avg_loss:0.017, val_acc:0.977]
Epoch [100/120    avg_loss:0.022, val_acc:0.977]
Epoch [101/120    avg_loss:0.024, val_acc:0.977]
Epoch [102/120    avg_loss:0.024, val_acc:0.977]
Epoch [103/120    avg_loss:0.023, val_acc:0.977]
Epoch [104/120    avg_loss:0.029, val_acc:0.977]
Epoch [105/120    avg_loss:0.015, val_acc:0.977]
Epoch [106/120    avg_loss:0.030, val_acc:0.977]
Epoch [107/120    avg_loss:0.021, val_acc:0.977]
Epoch [108/120    avg_loss:0.027, val_acc:0.977]
Epoch [109/120    avg_loss:0.023, val_acc:0.977]
Epoch [110/120    avg_loss:0.017, val_acc:0.977]
Epoch [111/120    avg_loss:0.028, val_acc:0.977]
Epoch [112/120    avg_loss:0.018, val_acc:0.977]
Epoch [113/120    avg_loss:0.022, val_acc:0.977]
Epoch [114/120    avg_loss:0.024, val_acc:0.977]
Epoch [115/120    avg_loss:0.019, val_acc:0.977]
Epoch [116/120    avg_loss:0.032, val_acc:0.977]
Epoch [117/120    avg_loss:0.018, val_acc:0.977]
Epoch [118/120    avg_loss:0.023, val_acc:0.977]
Epoch [119/120    avg_loss:0.023, val_acc:0.977]
Epoch [120/120    avg_loss:0.026, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  10   0   0   0   0   0   0   3   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   9 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.99095023 0.98901099 0.95749441 0.96666667
 0.99756691 0.98378378 1.         1.         1.         0.98820446
 0.98553949 1.        ]

Kappa:
0.9924040544978722
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f39d9f2b7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.047, val_acc:0.562]
Epoch [2/120    avg_loss:1.258, val_acc:0.688]
Epoch [3/120    avg_loss:0.905, val_acc:0.785]
Epoch [4/120    avg_loss:0.743, val_acc:0.796]
Epoch [5/120    avg_loss:0.612, val_acc:0.760]
Epoch [6/120    avg_loss:0.551, val_acc:0.848]
Epoch [7/120    avg_loss:0.512, val_acc:0.848]
Epoch [8/120    avg_loss:0.561, val_acc:0.900]
Epoch [9/120    avg_loss:0.395, val_acc:0.883]
Epoch [10/120    avg_loss:0.332, val_acc:0.910]
Epoch [11/120    avg_loss:0.316, val_acc:0.917]
Epoch [12/120    avg_loss:0.340, val_acc:0.908]
Epoch [13/120    avg_loss:0.328, val_acc:0.929]
Epoch [14/120    avg_loss:0.290, val_acc:0.919]
Epoch [15/120    avg_loss:0.244, val_acc:0.906]
Epoch [16/120    avg_loss:0.224, val_acc:0.954]
Epoch [17/120    avg_loss:0.194, val_acc:0.944]
Epoch [18/120    avg_loss:0.159, val_acc:0.896]
Epoch [19/120    avg_loss:0.268, val_acc:0.852]
Epoch [20/120    avg_loss:0.304, val_acc:0.919]
Epoch [21/120    avg_loss:0.217, val_acc:0.958]
Epoch [22/120    avg_loss:0.167, val_acc:0.967]
Epoch [23/120    avg_loss:0.166, val_acc:0.963]
Epoch [24/120    avg_loss:0.117, val_acc:0.946]
Epoch [25/120    avg_loss:0.139, val_acc:0.938]
Epoch [26/120    avg_loss:0.102, val_acc:0.948]
Epoch [27/120    avg_loss:0.137, val_acc:0.944]
Epoch [28/120    avg_loss:0.183, val_acc:0.867]
Epoch [29/120    avg_loss:0.367, val_acc:0.900]
Epoch [30/120    avg_loss:0.309, val_acc:0.929]
Epoch [31/120    avg_loss:0.271, val_acc:0.938]
Epoch [32/120    avg_loss:0.132, val_acc:0.956]
Epoch [33/120    avg_loss:0.157, val_acc:0.969]
Epoch [34/120    avg_loss:0.186, val_acc:0.971]
Epoch [35/120    avg_loss:0.149, val_acc:0.933]
Epoch [36/120    avg_loss:0.213, val_acc:0.965]
Epoch [37/120    avg_loss:0.139, val_acc:0.973]
Epoch [38/120    avg_loss:0.084, val_acc:0.973]
Epoch [39/120    avg_loss:0.072, val_acc:0.973]
Epoch [40/120    avg_loss:0.093, val_acc:0.977]
Epoch [41/120    avg_loss:0.068, val_acc:0.977]
Epoch [42/120    avg_loss:0.041, val_acc:0.981]
Epoch [43/120    avg_loss:0.059, val_acc:0.971]
Epoch [44/120    avg_loss:0.043, val_acc:0.973]
Epoch [45/120    avg_loss:0.050, val_acc:0.983]
Epoch [46/120    avg_loss:0.041, val_acc:0.960]
Epoch [47/120    avg_loss:0.030, val_acc:0.973]
Epoch [48/120    avg_loss:0.081, val_acc:0.981]
Epoch [49/120    avg_loss:0.040, val_acc:0.979]
Epoch [50/120    avg_loss:0.057, val_acc:0.975]
Epoch [51/120    avg_loss:0.035, val_acc:0.985]
Epoch [52/120    avg_loss:0.121, val_acc:0.973]
Epoch [53/120    avg_loss:0.100, val_acc:0.967]
Epoch [54/120    avg_loss:0.130, val_acc:0.975]
Epoch [55/120    avg_loss:0.043, val_acc:0.973]
Epoch [56/120    avg_loss:0.035, val_acc:0.971]
Epoch [57/120    avg_loss:0.031, val_acc:0.979]
Epoch [58/120    avg_loss:0.042, val_acc:0.985]
Epoch [59/120    avg_loss:0.035, val_acc:0.977]
Epoch [60/120    avg_loss:0.020, val_acc:0.985]
Epoch [61/120    avg_loss:0.038, val_acc:0.985]
Epoch [62/120    avg_loss:0.021, val_acc:0.983]
Epoch [63/120    avg_loss:0.017, val_acc:0.983]
Epoch [64/120    avg_loss:0.023, val_acc:0.985]
Epoch [65/120    avg_loss:0.023, val_acc:0.994]
Epoch [66/120    avg_loss:0.021, val_acc:0.983]
Epoch [67/120    avg_loss:0.029, val_acc:0.981]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.044, val_acc:0.985]
Epoch [70/120    avg_loss:0.040, val_acc:0.981]
Epoch [71/120    avg_loss:0.026, val_acc:0.988]
Epoch [72/120    avg_loss:0.022, val_acc:0.992]
Epoch [73/120    avg_loss:0.039, val_acc:0.981]
Epoch [74/120    avg_loss:0.038, val_acc:0.971]
Epoch [75/120    avg_loss:0.010, val_acc:0.990]
Epoch [76/120    avg_loss:0.005, val_acc:0.992]
Epoch [77/120    avg_loss:0.011, val_acc:0.990]
Epoch [78/120    avg_loss:0.020, val_acc:0.988]
Epoch [79/120    avg_loss:0.017, val_acc:0.988]
Epoch [80/120    avg_loss:0.023, val_acc:0.990]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.008, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.994]
Epoch [87/120    avg_loss:0.008, val_acc:0.994]
Epoch [88/120    avg_loss:0.006, val_acc:0.994]
Epoch [89/120    avg_loss:0.006, val_acc:0.994]
Epoch [90/120    avg_loss:0.010, val_acc:0.994]
Epoch [91/120    avg_loss:0.011, val_acc:0.994]
Epoch [92/120    avg_loss:0.006, val_acc:0.994]
Epoch [93/120    avg_loss:0.004, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.009, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.992]
Epoch [98/120    avg_loss:0.005, val_acc:0.992]
Epoch [99/120    avg_loss:0.008, val_acc:0.992]
Epoch [100/120    avg_loss:0.009, val_acc:0.992]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.018, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.006, val_acc:0.992]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.009, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.020, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 206  24   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  10   0   0   0   0   0   0   1   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   3 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.9977221  0.94495413 0.92307692 0.9632107
 1.         1.         1.         1.         1.         0.99603699
 0.9944629  1.        ]

Kappa:
0.9905052444859056
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa784e17b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.101, val_acc:0.641]
Epoch [2/120    avg_loss:1.293, val_acc:0.673]
Epoch [3/120    avg_loss:0.912, val_acc:0.802]
Epoch [4/120    avg_loss:0.840, val_acc:0.708]
Epoch [5/120    avg_loss:0.767, val_acc:0.790]
Epoch [6/120    avg_loss:0.655, val_acc:0.853]
Epoch [7/120    avg_loss:0.587, val_acc:0.899]
Epoch [8/120    avg_loss:0.461, val_acc:0.905]
Epoch [9/120    avg_loss:0.415, val_acc:0.903]
Epoch [10/120    avg_loss:0.471, val_acc:0.889]
Epoch [11/120    avg_loss:0.447, val_acc:0.911]
Epoch [12/120    avg_loss:0.366, val_acc:0.923]
Epoch [13/120    avg_loss:0.368, val_acc:0.891]
Epoch [14/120    avg_loss:0.420, val_acc:0.921]
Epoch [15/120    avg_loss:0.225, val_acc:0.931]
Epoch [16/120    avg_loss:0.321, val_acc:0.931]
Epoch [17/120    avg_loss:0.272, val_acc:0.923]
Epoch [18/120    avg_loss:0.223, val_acc:0.954]
Epoch [19/120    avg_loss:0.204, val_acc:0.956]
Epoch [20/120    avg_loss:0.156, val_acc:0.958]
Epoch [21/120    avg_loss:0.164, val_acc:0.952]
Epoch [22/120    avg_loss:0.179, val_acc:0.948]
Epoch [23/120    avg_loss:0.236, val_acc:0.893]
Epoch [24/120    avg_loss:0.200, val_acc:0.958]
Epoch [25/120    avg_loss:0.135, val_acc:0.954]
Epoch [26/120    avg_loss:0.106, val_acc:0.968]
Epoch [27/120    avg_loss:0.108, val_acc:0.960]
Epoch [28/120    avg_loss:0.122, val_acc:0.940]
Epoch [29/120    avg_loss:0.133, val_acc:0.950]
Epoch [30/120    avg_loss:0.127, val_acc:0.923]
Epoch [31/120    avg_loss:0.110, val_acc:0.958]
Epoch [32/120    avg_loss:0.118, val_acc:0.940]
Epoch [33/120    avg_loss:0.077, val_acc:0.964]
Epoch [34/120    avg_loss:0.069, val_acc:0.968]
Epoch [35/120    avg_loss:0.079, val_acc:0.968]
Epoch [36/120    avg_loss:0.090, val_acc:0.968]
Epoch [37/120    avg_loss:0.066, val_acc:0.964]
Epoch [38/120    avg_loss:0.097, val_acc:0.938]
Epoch [39/120    avg_loss:0.089, val_acc:0.960]
Epoch [40/120    avg_loss:0.051, val_acc:0.964]
Epoch [41/120    avg_loss:0.139, val_acc:0.960]
Epoch [42/120    avg_loss:0.107, val_acc:0.966]
Epoch [43/120    avg_loss:0.043, val_acc:0.974]
Epoch [44/120    avg_loss:0.051, val_acc:0.974]
Epoch [45/120    avg_loss:0.055, val_acc:0.980]
Epoch [46/120    avg_loss:0.032, val_acc:0.980]
Epoch [47/120    avg_loss:0.068, val_acc:0.982]
Epoch [48/120    avg_loss:0.051, val_acc:0.976]
Epoch [49/120    avg_loss:0.053, val_acc:0.966]
Epoch [50/120    avg_loss:0.040, val_acc:0.958]
Epoch [51/120    avg_loss:0.041, val_acc:0.974]
Epoch [52/120    avg_loss:0.042, val_acc:0.976]
Epoch [53/120    avg_loss:0.049, val_acc:0.968]
Epoch [54/120    avg_loss:0.076, val_acc:0.982]
Epoch [55/120    avg_loss:0.065, val_acc:0.970]
Epoch [56/120    avg_loss:0.054, val_acc:0.958]
Epoch [57/120    avg_loss:0.085, val_acc:0.952]
Epoch [58/120    avg_loss:0.031, val_acc:0.986]
Epoch [59/120    avg_loss:0.227, val_acc:0.952]
Epoch [60/120    avg_loss:0.150, val_acc:0.978]
Epoch [61/120    avg_loss:0.149, val_acc:0.909]
Epoch [62/120    avg_loss:0.122, val_acc:0.970]
Epoch [63/120    avg_loss:0.059, val_acc:0.968]
Epoch [64/120    avg_loss:0.073, val_acc:0.978]
Epoch [65/120    avg_loss:0.051, val_acc:0.976]
Epoch [66/120    avg_loss:0.042, val_acc:0.980]
Epoch [67/120    avg_loss:0.040, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.986]
Epoch [69/120    avg_loss:0.020, val_acc:0.988]
Epoch [70/120    avg_loss:0.022, val_acc:0.986]
Epoch [71/120    avg_loss:0.029, val_acc:0.982]
Epoch [72/120    avg_loss:0.015, val_acc:0.978]
Epoch [73/120    avg_loss:0.016, val_acc:0.972]
Epoch [74/120    avg_loss:0.024, val_acc:0.986]
Epoch [75/120    avg_loss:0.029, val_acc:0.974]
Epoch [76/120    avg_loss:0.017, val_acc:0.978]
Epoch [77/120    avg_loss:0.025, val_acc:0.980]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.047, val_acc:0.968]
Epoch [81/120    avg_loss:0.093, val_acc:0.964]
Epoch [82/120    avg_loss:0.062, val_acc:0.984]
Epoch [83/120    avg_loss:0.045, val_acc:0.986]
Epoch [84/120    avg_loss:0.031, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.014, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.990]
Epoch [92/120    avg_loss:0.018, val_acc:0.990]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.014, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.9977221  0.99563319 0.95032397 0.93333333
 0.99019608 1.         0.998713   1.         1.         0.98562092
 0.9877095  1.        ]

Kappa:
0.991454351971474
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8a4a80e710>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.118, val_acc:0.694]
Epoch [2/120    avg_loss:1.315, val_acc:0.700]
Epoch [3/120    avg_loss:0.952, val_acc:0.766]
Epoch [4/120    avg_loss:0.779, val_acc:0.798]
Epoch [5/120    avg_loss:0.722, val_acc:0.853]
Epoch [6/120    avg_loss:0.595, val_acc:0.865]
Epoch [7/120    avg_loss:0.489, val_acc:0.891]
Epoch [8/120    avg_loss:0.446, val_acc:0.915]
Epoch [9/120    avg_loss:0.371, val_acc:0.907]
Epoch [10/120    avg_loss:0.399, val_acc:0.905]
Epoch [11/120    avg_loss:0.395, val_acc:0.931]
Epoch [12/120    avg_loss:0.289, val_acc:0.954]
Epoch [13/120    avg_loss:0.244, val_acc:0.889]
Epoch [14/120    avg_loss:0.330, val_acc:0.905]
Epoch [15/120    avg_loss:0.269, val_acc:0.917]
Epoch [16/120    avg_loss:0.245, val_acc:0.937]
Epoch [17/120    avg_loss:0.324, val_acc:0.927]
Epoch [18/120    avg_loss:0.243, val_acc:0.942]
Epoch [19/120    avg_loss:0.242, val_acc:0.927]
Epoch [20/120    avg_loss:0.163, val_acc:0.938]
Epoch [21/120    avg_loss:0.162, val_acc:0.960]
Epoch [22/120    avg_loss:0.177, val_acc:0.946]
Epoch [23/120    avg_loss:0.210, val_acc:0.960]
Epoch [24/120    avg_loss:0.210, val_acc:0.960]
Epoch [25/120    avg_loss:0.151, val_acc:0.954]
Epoch [26/120    avg_loss:0.130, val_acc:0.966]
Epoch [27/120    avg_loss:0.119, val_acc:0.966]
Epoch [28/120    avg_loss:0.138, val_acc:0.952]
Epoch [29/120    avg_loss:0.143, val_acc:0.966]
Epoch [30/120    avg_loss:0.106, val_acc:0.972]
Epoch [31/120    avg_loss:0.154, val_acc:0.923]
Epoch [32/120    avg_loss:0.176, val_acc:0.956]
Epoch [33/120    avg_loss:0.123, val_acc:0.956]
Epoch [34/120    avg_loss:0.087, val_acc:0.972]
Epoch [35/120    avg_loss:0.086, val_acc:0.966]
Epoch [36/120    avg_loss:0.089, val_acc:0.946]
Epoch [37/120    avg_loss:0.121, val_acc:0.931]
Epoch [38/120    avg_loss:0.129, val_acc:0.970]
Epoch [39/120    avg_loss:0.066, val_acc:0.974]
Epoch [40/120    avg_loss:0.058, val_acc:0.964]
Epoch [41/120    avg_loss:0.064, val_acc:0.962]
Epoch [42/120    avg_loss:0.043, val_acc:0.968]
Epoch [43/120    avg_loss:0.055, val_acc:0.972]
Epoch [44/120    avg_loss:0.107, val_acc:0.960]
Epoch [45/120    avg_loss:0.080, val_acc:0.974]
Epoch [46/120    avg_loss:0.052, val_acc:0.978]
Epoch [47/120    avg_loss:0.026, val_acc:0.982]
Epoch [48/120    avg_loss:0.043, val_acc:0.972]
Epoch [49/120    avg_loss:0.104, val_acc:0.958]
Epoch [50/120    avg_loss:0.132, val_acc:0.972]
Epoch [51/120    avg_loss:0.113, val_acc:0.962]
Epoch [52/120    avg_loss:0.037, val_acc:0.978]
Epoch [53/120    avg_loss:0.061, val_acc:0.970]
Epoch [54/120    avg_loss:0.088, val_acc:0.966]
Epoch [55/120    avg_loss:0.106, val_acc:0.978]
Epoch [56/120    avg_loss:0.088, val_acc:0.962]
Epoch [57/120    avg_loss:0.050, val_acc:0.974]
Epoch [58/120    avg_loss:0.158, val_acc:0.970]
Epoch [59/120    avg_loss:0.043, val_acc:0.972]
Epoch [60/120    avg_loss:0.063, val_acc:0.976]
Epoch [61/120    avg_loss:0.040, val_acc:0.978]
Epoch [62/120    avg_loss:0.033, val_acc:0.986]
Epoch [63/120    avg_loss:0.036, val_acc:0.988]
Epoch [64/120    avg_loss:0.024, val_acc:0.986]
Epoch [65/120    avg_loss:0.021, val_acc:0.986]
Epoch [66/120    avg_loss:0.018, val_acc:0.986]
Epoch [67/120    avg_loss:0.024, val_acc:0.986]
Epoch [68/120    avg_loss:0.022, val_acc:0.986]
Epoch [69/120    avg_loss:0.022, val_acc:0.986]
Epoch [70/120    avg_loss:0.023, val_acc:0.986]
Epoch [71/120    avg_loss:0.018, val_acc:0.986]
Epoch [72/120    avg_loss:0.026, val_acc:0.984]
Epoch [73/120    avg_loss:0.017, val_acc:0.986]
Epoch [74/120    avg_loss:0.022, val_acc:0.984]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.026, val_acc:0.986]
Epoch [77/120    avg_loss:0.035, val_acc:0.986]
Epoch [78/120    avg_loss:0.020, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.986]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.016, val_acc:0.986]
Epoch [82/120    avg_loss:0.028, val_acc:0.984]
Epoch [83/120    avg_loss:0.023, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.020, val_acc:0.984]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.023, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.984]
Epoch [89/120    avg_loss:0.022, val_acc:0.984]
Epoch [90/120    avg_loss:0.022, val_acc:0.984]
Epoch [91/120    avg_loss:0.018, val_acc:0.984]
Epoch [92/120    avg_loss:0.021, val_acc:0.984]
Epoch [93/120    avg_loss:0.018, val_acc:0.984]
Epoch [94/120    avg_loss:0.026, val_acc:0.984]
Epoch [95/120    avg_loss:0.019, val_acc:0.984]
Epoch [96/120    avg_loss:0.027, val_acc:0.984]
Epoch [97/120    avg_loss:0.033, val_acc:0.984]
Epoch [98/120    avg_loss:0.016, val_acc:0.984]
Epoch [99/120    avg_loss:0.014, val_acc:0.984]
Epoch [100/120    avg_loss:0.016, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.016, val_acc:0.984]
Epoch [104/120    avg_loss:0.016, val_acc:0.984]
Epoch [105/120    avg_loss:0.015, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.027, val_acc:0.984]
Epoch [108/120    avg_loss:0.028, val_acc:0.984]
Epoch [109/120    avg_loss:0.016, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.024, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.984]
Epoch [113/120    avg_loss:0.018, val_acc:0.984]
Epoch [114/120    avg_loss:0.030, val_acc:0.984]
Epoch [115/120    avg_loss:0.019, val_acc:0.984]
Epoch [116/120    avg_loss:0.030, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.015, val_acc:0.984]
Epoch [119/120    avg_loss:0.025, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.98648649 0.98454746 0.93644068 0.92907801
 0.99266504 0.96703297 1.         1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9907415515582347
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc1471ae898>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.133, val_acc:0.540]
Epoch [2/120    avg_loss:1.327, val_acc:0.750]
Epoch [3/120    avg_loss:0.995, val_acc:0.784]
Epoch [4/120    avg_loss:0.891, val_acc:0.764]
Epoch [5/120    avg_loss:0.715, val_acc:0.808]
Epoch [6/120    avg_loss:0.590, val_acc:0.849]
Epoch [7/120    avg_loss:0.459, val_acc:0.869]
Epoch [8/120    avg_loss:0.572, val_acc:0.897]
Epoch [9/120    avg_loss:0.396, val_acc:0.899]
Epoch [10/120    avg_loss:0.384, val_acc:0.905]
Epoch [11/120    avg_loss:0.335, val_acc:0.931]
Epoch [12/120    avg_loss:0.330, val_acc:0.915]
Epoch [13/120    avg_loss:0.342, val_acc:0.917]
Epoch [14/120    avg_loss:0.299, val_acc:0.897]
Epoch [15/120    avg_loss:0.334, val_acc:0.889]
Epoch [16/120    avg_loss:0.328, val_acc:0.919]
Epoch [17/120    avg_loss:0.234, val_acc:0.907]
Epoch [18/120    avg_loss:0.241, val_acc:0.944]
Epoch [19/120    avg_loss:0.216, val_acc:0.938]
Epoch [20/120    avg_loss:0.185, val_acc:0.954]
Epoch [21/120    avg_loss:0.200, val_acc:0.938]
Epoch [22/120    avg_loss:0.212, val_acc:0.960]
Epoch [23/120    avg_loss:0.149, val_acc:0.950]
Epoch [24/120    avg_loss:0.171, val_acc:0.931]
Epoch [25/120    avg_loss:0.146, val_acc:0.929]
Epoch [26/120    avg_loss:0.157, val_acc:0.962]
Epoch [27/120    avg_loss:0.187, val_acc:0.954]
Epoch [28/120    avg_loss:0.241, val_acc:0.937]
Epoch [29/120    avg_loss:0.238, val_acc:0.960]
Epoch [30/120    avg_loss:0.357, val_acc:0.927]
Epoch [31/120    avg_loss:0.201, val_acc:0.935]
Epoch [32/120    avg_loss:0.139, val_acc:0.948]
Epoch [33/120    avg_loss:0.178, val_acc:0.956]
Epoch [34/120    avg_loss:0.122, val_acc:0.966]
Epoch [35/120    avg_loss:0.231, val_acc:0.954]
Epoch [36/120    avg_loss:0.097, val_acc:0.976]
Epoch [37/120    avg_loss:0.101, val_acc:0.964]
Epoch [38/120    avg_loss:0.066, val_acc:0.968]
Epoch [39/120    avg_loss:0.065, val_acc:0.962]
Epoch [40/120    avg_loss:0.078, val_acc:0.976]
Epoch [41/120    avg_loss:0.104, val_acc:0.960]
Epoch [42/120    avg_loss:0.052, val_acc:0.964]
Epoch [43/120    avg_loss:0.068, val_acc:0.974]
Epoch [44/120    avg_loss:0.106, val_acc:0.952]
Epoch [45/120    avg_loss:0.056, val_acc:0.976]
Epoch [46/120    avg_loss:0.041, val_acc:0.962]
Epoch [47/120    avg_loss:0.054, val_acc:0.970]
Epoch [48/120    avg_loss:0.077, val_acc:0.968]
Epoch [49/120    avg_loss:0.104, val_acc:0.954]
Epoch [50/120    avg_loss:0.093, val_acc:0.980]
Epoch [51/120    avg_loss:0.109, val_acc:0.980]
Epoch [52/120    avg_loss:0.244, val_acc:0.937]
Epoch [53/120    avg_loss:0.123, val_acc:0.972]
Epoch [54/120    avg_loss:0.074, val_acc:0.962]
Epoch [55/120    avg_loss:0.087, val_acc:0.972]
Epoch [56/120    avg_loss:0.085, val_acc:0.978]
Epoch [57/120    avg_loss:0.027, val_acc:0.978]
Epoch [58/120    avg_loss:0.038, val_acc:0.974]
Epoch [59/120    avg_loss:0.096, val_acc:0.968]
Epoch [60/120    avg_loss:0.095, val_acc:0.972]
Epoch [61/120    avg_loss:0.076, val_acc:0.978]
Epoch [62/120    avg_loss:0.061, val_acc:0.980]
Epoch [63/120    avg_loss:0.031, val_acc:0.976]
Epoch [64/120    avg_loss:0.027, val_acc:0.976]
Epoch [65/120    avg_loss:0.038, val_acc:0.976]
Epoch [66/120    avg_loss:0.035, val_acc:0.974]
Epoch [67/120    avg_loss:0.043, val_acc:0.976]
Epoch [68/120    avg_loss:0.044, val_acc:0.976]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.013, val_acc:0.982]
Epoch [71/120    avg_loss:0.024, val_acc:0.980]
Epoch [72/120    avg_loss:0.046, val_acc:0.980]
Epoch [73/120    avg_loss:0.023, val_acc:0.982]
Epoch [74/120    avg_loss:0.037, val_acc:0.988]
Epoch [75/120    avg_loss:0.019, val_acc:0.978]
Epoch [76/120    avg_loss:0.016, val_acc:0.984]
Epoch [77/120    avg_loss:0.011, val_acc:0.988]
Epoch [78/120    avg_loss:0.015, val_acc:0.978]
Epoch [79/120    avg_loss:0.029, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.032, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.019, val_acc:0.988]
Epoch [85/120    avg_loss:0.014, val_acc:0.976]
Epoch [86/120    avg_loss:0.023, val_acc:0.984]
Epoch [87/120    avg_loss:0.016, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.022, val_acc:0.982]
Epoch [90/120    avg_loss:0.016, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.017, val_acc:0.974]
Epoch [93/120    avg_loss:0.074, val_acc:0.980]
Epoch [94/120    avg_loss:0.041, val_acc:0.980]
Epoch [95/120    avg_loss:0.018, val_acc:0.980]
Epoch [96/120    avg_loss:0.019, val_acc:0.986]
Epoch [97/120    avg_loss:0.013, val_acc:0.988]
Epoch [98/120    avg_loss:0.025, val_acc:0.988]
Epoch [99/120    avg_loss:0.015, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.016, val_acc:0.988]
Epoch [103/120    avg_loss:0.018, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.013, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.014, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.022, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   6   0   0   0   0   0   0   3   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         1.         1.         0.96035242 0.96928328
 0.98522167 1.         1.         1.         1.         0.99341238
 0.99115044 1.        ]

Kappa:
0.9945401773503456
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0039d6c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.115, val_acc:0.585]
Epoch [2/120    avg_loss:1.327, val_acc:0.681]
Epoch [3/120    avg_loss:0.954, val_acc:0.727]
Epoch [4/120    avg_loss:0.748, val_acc:0.819]
Epoch [5/120    avg_loss:0.655, val_acc:0.848]
Epoch [6/120    avg_loss:0.600, val_acc:0.840]
Epoch [7/120    avg_loss:0.549, val_acc:0.865]
Epoch [8/120    avg_loss:0.457, val_acc:0.887]
Epoch [9/120    avg_loss:0.589, val_acc:0.894]
Epoch [10/120    avg_loss:0.548, val_acc:0.894]
Epoch [11/120    avg_loss:0.357, val_acc:0.940]
Epoch [12/120    avg_loss:0.332, val_acc:0.929]
Epoch [13/120    avg_loss:0.294, val_acc:0.921]
Epoch [14/120    avg_loss:0.373, val_acc:0.944]
Epoch [15/120    avg_loss:0.304, val_acc:0.944]
Epoch [16/120    avg_loss:0.276, val_acc:0.952]
Epoch [17/120    avg_loss:0.220, val_acc:0.960]
Epoch [18/120    avg_loss:0.248, val_acc:0.935]
Epoch [19/120    avg_loss:0.285, val_acc:0.950]
Epoch [20/120    avg_loss:0.275, val_acc:0.923]
Epoch [21/120    avg_loss:0.273, val_acc:0.960]
Epoch [22/120    avg_loss:0.191, val_acc:0.942]
Epoch [23/120    avg_loss:0.224, val_acc:0.971]
Epoch [24/120    avg_loss:0.216, val_acc:0.956]
Epoch [25/120    avg_loss:0.228, val_acc:0.946]
Epoch [26/120    avg_loss:0.196, val_acc:0.969]
Epoch [27/120    avg_loss:0.092, val_acc:0.977]
Epoch [28/120    avg_loss:0.089, val_acc:0.977]
Epoch [29/120    avg_loss:0.098, val_acc:0.967]
Epoch [30/120    avg_loss:0.139, val_acc:0.950]
Epoch [31/120    avg_loss:0.159, val_acc:0.923]
Epoch [32/120    avg_loss:0.213, val_acc:0.954]
Epoch [33/120    avg_loss:0.158, val_acc:0.975]
Epoch [34/120    avg_loss:0.144, val_acc:0.963]
Epoch [35/120    avg_loss:0.107, val_acc:0.973]
Epoch [36/120    avg_loss:0.096, val_acc:0.979]
Epoch [37/120    avg_loss:0.082, val_acc:0.975]
Epoch [38/120    avg_loss:0.111, val_acc:0.973]
Epoch [39/120    avg_loss:0.128, val_acc:0.979]
Epoch [40/120    avg_loss:0.106, val_acc:0.981]
Epoch [41/120    avg_loss:0.084, val_acc:0.967]
Epoch [42/120    avg_loss:0.103, val_acc:0.971]
Epoch [43/120    avg_loss:0.069, val_acc:0.988]
Epoch [44/120    avg_loss:0.050, val_acc:0.981]
Epoch [45/120    avg_loss:0.087, val_acc:0.971]
Epoch [46/120    avg_loss:0.119, val_acc:0.979]
Epoch [47/120    avg_loss:0.074, val_acc:0.983]
Epoch [48/120    avg_loss:0.055, val_acc:0.985]
Epoch [49/120    avg_loss:0.042, val_acc:0.983]
Epoch [50/120    avg_loss:0.057, val_acc:0.985]
Epoch [51/120    avg_loss:0.081, val_acc:0.985]
Epoch [52/120    avg_loss:0.107, val_acc:0.977]
Epoch [53/120    avg_loss:0.055, val_acc:0.981]
Epoch [54/120    avg_loss:0.063, val_acc:0.985]
Epoch [55/120    avg_loss:0.037, val_acc:0.983]
Epoch [56/120    avg_loss:0.059, val_acc:0.983]
Epoch [57/120    avg_loss:0.032, val_acc:0.985]
Epoch [58/120    avg_loss:0.020, val_acc:0.985]
Epoch [59/120    avg_loss:0.035, val_acc:0.988]
Epoch [60/120    avg_loss:0.019, val_acc:0.985]
Epoch [61/120    avg_loss:0.032, val_acc:0.988]
Epoch [62/120    avg_loss:0.026, val_acc:0.988]
Epoch [63/120    avg_loss:0.021, val_acc:0.988]
Epoch [64/120    avg_loss:0.020, val_acc:0.988]
Epoch [65/120    avg_loss:0.030, val_acc:0.992]
Epoch [66/120    avg_loss:0.026, val_acc:0.992]
Epoch [67/120    avg_loss:0.026, val_acc:0.992]
Epoch [68/120    avg_loss:0.035, val_acc:0.994]
Epoch [69/120    avg_loss:0.017, val_acc:0.994]
Epoch [70/120    avg_loss:0.021, val_acc:0.996]
Epoch [71/120    avg_loss:0.015, val_acc:0.994]
Epoch [72/120    avg_loss:0.029, val_acc:0.994]
Epoch [73/120    avg_loss:0.022, val_acc:0.994]
Epoch [74/120    avg_loss:0.016, val_acc:0.994]
Epoch [75/120    avg_loss:0.027, val_acc:0.994]
Epoch [76/120    avg_loss:0.018, val_acc:0.992]
Epoch [77/120    avg_loss:0.021, val_acc:0.994]
Epoch [78/120    avg_loss:0.017, val_acc:0.996]
Epoch [79/120    avg_loss:0.016, val_acc:0.992]
Epoch [80/120    avg_loss:0.023, val_acc:0.990]
Epoch [81/120    avg_loss:0.017, val_acc:0.990]
Epoch [82/120    avg_loss:0.015, val_acc:0.990]
Epoch [83/120    avg_loss:0.017, val_acc:0.994]
Epoch [84/120    avg_loss:0.012, val_acc:0.994]
Epoch [85/120    avg_loss:0.016, val_acc:0.994]
Epoch [86/120    avg_loss:0.017, val_acc:0.994]
Epoch [87/120    avg_loss:0.019, val_acc:0.990]
Epoch [88/120    avg_loss:0.015, val_acc:0.988]
Epoch [89/120    avg_loss:0.021, val_acc:0.994]
Epoch [90/120    avg_loss:0.011, val_acc:0.994]
Epoch [91/120    avg_loss:0.019, val_acc:0.994]
Epoch [92/120    avg_loss:0.017, val_acc:0.994]
Epoch [93/120    avg_loss:0.014, val_acc:0.994]
Epoch [94/120    avg_loss:0.022, val_acc:0.994]
Epoch [95/120    avg_loss:0.018, val_acc:0.994]
Epoch [96/120    avg_loss:0.019, val_acc:0.994]
Epoch [97/120    avg_loss:0.022, val_acc:0.994]
Epoch [98/120    avg_loss:0.015, val_acc:0.994]
Epoch [99/120    avg_loss:0.021, val_acc:0.994]
Epoch [100/120    avg_loss:0.011, val_acc:0.994]
Epoch [101/120    avg_loss:0.017, val_acc:0.994]
Epoch [102/120    avg_loss:0.013, val_acc:0.994]
Epoch [103/120    avg_loss:0.013, val_acc:0.994]
Epoch [104/120    avg_loss:0.017, val_acc:0.994]
Epoch [105/120    avg_loss:0.014, val_acc:0.994]
Epoch [106/120    avg_loss:0.009, val_acc:0.994]
Epoch [107/120    avg_loss:0.019, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.994]
Epoch [110/120    avg_loss:0.028, val_acc:0.994]
Epoch [111/120    avg_loss:0.030, val_acc:0.994]
Epoch [112/120    avg_loss:0.014, val_acc:0.994]
Epoch [113/120    avg_loss:0.015, val_acc:0.994]
Epoch [114/120    avg_loss:0.019, val_acc:0.994]
Epoch [115/120    avg_loss:0.014, val_acc:0.994]
Epoch [116/120    avg_loss:0.016, val_acc:0.994]
Epoch [117/120    avg_loss:0.016, val_acc:0.994]
Epoch [118/120    avg_loss:0.012, val_acc:0.994]
Epoch [119/120    avg_loss:0.025, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214   8   0   0   0   0   0   0   5   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.99545455 1.         0.95749441 0.95205479
 1.         0.98924731 1.         1.         1.         0.98691099
 0.98335183 1.        ]

Kappa:
0.9926410050547726
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ce7dc97b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.076, val_acc:0.540]
Epoch [2/120    avg_loss:1.292, val_acc:0.685]
Epoch [3/120    avg_loss:0.900, val_acc:0.728]
Epoch [4/120    avg_loss:0.758, val_acc:0.808]
Epoch [5/120    avg_loss:0.638, val_acc:0.841]
Epoch [6/120    avg_loss:0.609, val_acc:0.847]
Epoch [7/120    avg_loss:0.484, val_acc:0.875]
Epoch [8/120    avg_loss:0.450, val_acc:0.871]
Epoch [9/120    avg_loss:0.432, val_acc:0.891]
Epoch [10/120    avg_loss:0.412, val_acc:0.893]
Epoch [11/120    avg_loss:0.295, val_acc:0.921]
Epoch [12/120    avg_loss:0.343, val_acc:0.940]
Epoch [13/120    avg_loss:0.319, val_acc:0.948]
Epoch [14/120    avg_loss:0.261, val_acc:0.948]
Epoch [15/120    avg_loss:0.214, val_acc:0.933]
Epoch [16/120    avg_loss:0.220, val_acc:0.946]
Epoch [17/120    avg_loss:0.169, val_acc:0.958]
Epoch [18/120    avg_loss:0.185, val_acc:0.958]
Epoch [19/120    avg_loss:0.251, val_acc:0.937]
Epoch [20/120    avg_loss:0.160, val_acc:0.958]
Epoch [21/120    avg_loss:0.153, val_acc:0.968]
Epoch [22/120    avg_loss:0.223, val_acc:0.960]
Epoch [23/120    avg_loss:0.212, val_acc:0.948]
Epoch [24/120    avg_loss:0.163, val_acc:0.960]
Epoch [25/120    avg_loss:0.108, val_acc:0.978]
Epoch [26/120    avg_loss:0.148, val_acc:0.964]
Epoch [27/120    avg_loss:0.090, val_acc:0.976]
Epoch [28/120    avg_loss:0.101, val_acc:0.978]
Epoch [29/120    avg_loss:0.101, val_acc:0.970]
Epoch [30/120    avg_loss:0.105, val_acc:0.956]
Epoch [31/120    avg_loss:0.147, val_acc:0.958]
Epoch [32/120    avg_loss:0.085, val_acc:0.982]
Epoch [33/120    avg_loss:0.062, val_acc:0.974]
Epoch [34/120    avg_loss:0.084, val_acc:0.976]
Epoch [35/120    avg_loss:0.060, val_acc:0.974]
Epoch [36/120    avg_loss:0.070, val_acc:0.970]
Epoch [37/120    avg_loss:0.059, val_acc:0.982]
Epoch [38/120    avg_loss:0.059, val_acc:0.984]
Epoch [39/120    avg_loss:0.050, val_acc:0.982]
Epoch [40/120    avg_loss:0.046, val_acc:0.984]
Epoch [41/120    avg_loss:0.037, val_acc:0.982]
Epoch [42/120    avg_loss:0.050, val_acc:0.984]
Epoch [43/120    avg_loss:0.031, val_acc:0.984]
Epoch [44/120    avg_loss:0.054, val_acc:0.982]
Epoch [45/120    avg_loss:0.035, val_acc:0.986]
Epoch [46/120    avg_loss:0.046, val_acc:0.990]
Epoch [47/120    avg_loss:0.053, val_acc:0.956]
Epoch [48/120    avg_loss:0.045, val_acc:0.988]
Epoch [49/120    avg_loss:0.077, val_acc:0.978]
Epoch [50/120    avg_loss:0.093, val_acc:0.986]
Epoch [51/120    avg_loss:0.042, val_acc:0.984]
Epoch [52/120    avg_loss:0.040, val_acc:0.986]
Epoch [53/120    avg_loss:0.046, val_acc:0.988]
Epoch [54/120    avg_loss:0.021, val_acc:0.992]
Epoch [55/120    avg_loss:0.025, val_acc:0.978]
Epoch [56/120    avg_loss:0.037, val_acc:0.984]
Epoch [57/120    avg_loss:0.028, val_acc:0.992]
Epoch [58/120    avg_loss:0.046, val_acc:0.980]
Epoch [59/120    avg_loss:0.021, val_acc:0.990]
Epoch [60/120    avg_loss:0.029, val_acc:0.994]
Epoch [61/120    avg_loss:0.020, val_acc:0.990]
Epoch [62/120    avg_loss:0.024, val_acc:0.986]
Epoch [63/120    avg_loss:0.026, val_acc:0.986]
Epoch [64/120    avg_loss:0.020, val_acc:0.990]
Epoch [65/120    avg_loss:0.019, val_acc:0.994]
Epoch [66/120    avg_loss:0.022, val_acc:0.994]
Epoch [67/120    avg_loss:0.078, val_acc:0.964]
Epoch [68/120    avg_loss:0.051, val_acc:0.992]
Epoch [69/120    avg_loss:0.032, val_acc:0.990]
Epoch [70/120    avg_loss:0.020, val_acc:0.990]
Epoch [71/120    avg_loss:0.021, val_acc:0.990]
Epoch [72/120    avg_loss:0.013, val_acc:0.992]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.017, val_acc:0.990]
Epoch [75/120    avg_loss:0.042, val_acc:0.982]
Epoch [76/120    avg_loss:0.016, val_acc:0.990]
Epoch [77/120    avg_loss:0.012, val_acc:0.994]
Epoch [78/120    avg_loss:0.026, val_acc:0.992]
Epoch [79/120    avg_loss:0.022, val_acc:0.994]
Epoch [80/120    avg_loss:0.025, val_acc:0.988]
Epoch [81/120    avg_loss:0.034, val_acc:0.990]
Epoch [82/120    avg_loss:0.030, val_acc:0.990]
Epoch [83/120    avg_loss:0.016, val_acc:0.996]
Epoch [84/120    avg_loss:0.010, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.994]
Epoch [86/120    avg_loss:0.006, val_acc:0.996]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.006, val_acc:0.992]
Epoch [89/120    avg_loss:0.005, val_acc:0.994]
Epoch [90/120    avg_loss:0.004, val_acc:0.994]
Epoch [91/120    avg_loss:0.007, val_acc:0.994]
Epoch [92/120    avg_loss:0.004, val_acc:0.992]
Epoch [93/120    avg_loss:0.009, val_acc:0.994]
Epoch [94/120    avg_loss:0.006, val_acc:0.994]
Epoch [95/120    avg_loss:0.036, val_acc:0.968]
Epoch [96/120    avg_loss:0.048, val_acc:0.990]
Epoch [97/120    avg_loss:0.048, val_acc:0.966]
Epoch [98/120    avg_loss:0.043, val_acc:0.970]
Epoch [99/120    avg_loss:0.041, val_acc:0.990]
Epoch [100/120    avg_loss:0.016, val_acc:0.990]
Epoch [101/120    avg_loss:0.016, val_acc:0.990]
Epoch [102/120    avg_loss:0.014, val_acc:0.992]
Epoch [103/120    avg_loss:0.008, val_acc:0.992]
Epoch [104/120    avg_loss:0.015, val_acc:0.992]
Epoch [105/120    avg_loss:0.013, val_acc:0.994]
Epoch [106/120    avg_loss:0.009, val_acc:0.994]
Epoch [107/120    avg_loss:0.010, val_acc:0.994]
Epoch [108/120    avg_loss:0.012, val_acc:0.992]
Epoch [109/120    avg_loss:0.008, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.009, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.007, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.017, val_acc:0.992]
Epoch [118/120    avg_loss:0.009, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 1.         1.         1.         0.96995708 0.94964029
 1.         1.         1.         1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.995964360846981
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54f8b267f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.178, val_acc:0.477]
Epoch [2/120    avg_loss:1.394, val_acc:0.685]
Epoch [3/120    avg_loss:0.965, val_acc:0.710]
Epoch [4/120    avg_loss:0.885, val_acc:0.800]
Epoch [5/120    avg_loss:0.763, val_acc:0.840]
Epoch [6/120    avg_loss:0.644, val_acc:0.848]
Epoch [7/120    avg_loss:0.545, val_acc:0.881]
Epoch [8/120    avg_loss:0.472, val_acc:0.877]
Epoch [9/120    avg_loss:0.444, val_acc:0.906]
Epoch [10/120    avg_loss:0.360, val_acc:0.908]
Epoch [11/120    avg_loss:0.377, val_acc:0.906]
Epoch [12/120    avg_loss:0.389, val_acc:0.925]
Epoch [13/120    avg_loss:0.287, val_acc:0.925]
Epoch [14/120    avg_loss:0.264, val_acc:0.931]
Epoch [15/120    avg_loss:0.305, val_acc:0.933]
Epoch [16/120    avg_loss:0.242, val_acc:0.942]
Epoch [17/120    avg_loss:0.202, val_acc:0.933]
Epoch [18/120    avg_loss:0.223, val_acc:0.940]
Epoch [19/120    avg_loss:0.224, val_acc:0.946]
Epoch [20/120    avg_loss:0.142, val_acc:0.960]
Epoch [21/120    avg_loss:0.183, val_acc:0.950]
Epoch [22/120    avg_loss:0.287, val_acc:0.956]
Epoch [23/120    avg_loss:0.220, val_acc:0.921]
Epoch [24/120    avg_loss:0.183, val_acc:0.944]
Epoch [25/120    avg_loss:0.158, val_acc:0.948]
Epoch [26/120    avg_loss:0.166, val_acc:0.938]
Epoch [27/120    avg_loss:0.140, val_acc:0.969]
Epoch [28/120    avg_loss:0.141, val_acc:0.946]
Epoch [29/120    avg_loss:0.149, val_acc:0.965]
Epoch [30/120    avg_loss:0.140, val_acc:0.971]
Epoch [31/120    avg_loss:0.132, val_acc:0.971]
Epoch [32/120    avg_loss:0.225, val_acc:0.927]
Epoch [33/120    avg_loss:0.185, val_acc:0.971]
Epoch [34/120    avg_loss:0.105, val_acc:0.979]
Epoch [35/120    avg_loss:0.099, val_acc:0.971]
Epoch [36/120    avg_loss:0.058, val_acc:0.977]
Epoch [37/120    avg_loss:0.057, val_acc:0.969]
Epoch [38/120    avg_loss:0.111, val_acc:0.965]
Epoch [39/120    avg_loss:0.069, val_acc:0.975]
Epoch [40/120    avg_loss:0.076, val_acc:0.971]
Epoch [41/120    avg_loss:0.053, val_acc:0.973]
Epoch [42/120    avg_loss:0.065, val_acc:0.977]
Epoch [43/120    avg_loss:0.050, val_acc:0.975]
Epoch [44/120    avg_loss:0.139, val_acc:0.973]
Epoch [45/120    avg_loss:0.068, val_acc:0.975]
Epoch [46/120    avg_loss:0.041, val_acc:0.975]
Epoch [47/120    avg_loss:0.110, val_acc:0.975]
Epoch [48/120    avg_loss:0.068, val_acc:0.977]
Epoch [49/120    avg_loss:0.029, val_acc:0.977]
Epoch [50/120    avg_loss:0.035, val_acc:0.977]
Epoch [51/120    avg_loss:0.030, val_acc:0.977]
Epoch [52/120    avg_loss:0.031, val_acc:0.979]
Epoch [53/120    avg_loss:0.037, val_acc:0.979]
Epoch [54/120    avg_loss:0.030, val_acc:0.981]
Epoch [55/120    avg_loss:0.018, val_acc:0.983]
Epoch [56/120    avg_loss:0.025, val_acc:0.983]
Epoch [57/120    avg_loss:0.028, val_acc:0.983]
Epoch [58/120    avg_loss:0.030, val_acc:0.983]
Epoch [59/120    avg_loss:0.019, val_acc:0.985]
Epoch [60/120    avg_loss:0.045, val_acc:0.981]
Epoch [61/120    avg_loss:0.026, val_acc:0.983]
Epoch [62/120    avg_loss:0.026, val_acc:0.988]
Epoch [63/120    avg_loss:0.046, val_acc:0.988]
Epoch [64/120    avg_loss:0.024, val_acc:0.985]
Epoch [65/120    avg_loss:0.032, val_acc:0.985]
Epoch [66/120    avg_loss:0.031, val_acc:0.985]
Epoch [67/120    avg_loss:0.021, val_acc:0.988]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.020, val_acc:0.988]
Epoch [70/120    avg_loss:0.028, val_acc:0.985]
Epoch [71/120    avg_loss:0.024, val_acc:0.985]
Epoch [72/120    avg_loss:0.023, val_acc:0.985]
Epoch [73/120    avg_loss:0.033, val_acc:0.988]
Epoch [74/120    avg_loss:0.019, val_acc:0.988]
Epoch [75/120    avg_loss:0.020, val_acc:0.988]
Epoch [76/120    avg_loss:0.015, val_acc:0.988]
Epoch [77/120    avg_loss:0.014, val_acc:0.988]
Epoch [78/120    avg_loss:0.018, val_acc:0.988]
Epoch [79/120    avg_loss:0.030, val_acc:0.988]
Epoch [80/120    avg_loss:0.017, val_acc:0.988]
Epoch [81/120    avg_loss:0.016, val_acc:0.988]
Epoch [82/120    avg_loss:0.021, val_acc:0.988]
Epoch [83/120    avg_loss:0.019, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.018, val_acc:0.988]
Epoch [86/120    avg_loss:0.019, val_acc:0.988]
Epoch [87/120    avg_loss:0.023, val_acc:0.988]
Epoch [88/120    avg_loss:0.027, val_acc:0.988]
Epoch [89/120    avg_loss:0.017, val_acc:0.988]
Epoch [90/120    avg_loss:0.021, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.012, val_acc:0.988]
Epoch [93/120    avg_loss:0.017, val_acc:0.988]
Epoch [94/120    avg_loss:0.015, val_acc:0.988]
Epoch [95/120    avg_loss:0.027, val_acc:0.988]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.026, val_acc:0.988]
Epoch [98/120    avg_loss:0.022, val_acc:0.985]
Epoch [99/120    avg_loss:0.016, val_acc:0.985]
Epoch [100/120    avg_loss:0.015, val_acc:0.985]
Epoch [101/120    avg_loss:0.016, val_acc:0.985]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.015, val_acc:0.988]
Epoch [104/120    avg_loss:0.019, val_acc:0.985]
Epoch [105/120    avg_loss:0.016, val_acc:0.985]
Epoch [106/120    avg_loss:0.017, val_acc:0.988]
Epoch [107/120    avg_loss:0.014, val_acc:0.985]
Epoch [108/120    avg_loss:0.026, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.019, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.022, val_acc:0.988]
Epoch [113/120    avg_loss:0.018, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.015, val_acc:0.988]
Epoch [117/120    avg_loss:0.014, val_acc:0.988]
Epoch [118/120    avg_loss:0.012, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.988]
Epoch [120/120    avg_loss:0.014, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  11   0   0   0   0   0   0   2   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.99095023 0.99782135 0.95964126 0.94949495
 1.         0.97826087 1.         1.         1.         0.99472296
 0.99336283 1.        ]

Kappa:
0.993828059995873
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe8d6ad6898>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.171, val_acc:0.492]
Epoch [2/120    avg_loss:1.365, val_acc:0.671]
Epoch [3/120    avg_loss:1.021, val_acc:0.785]
Epoch [4/120    avg_loss:0.792, val_acc:0.831]
Epoch [5/120    avg_loss:0.622, val_acc:0.752]
Epoch [6/120    avg_loss:0.608, val_acc:0.873]
Epoch [7/120    avg_loss:0.550, val_acc:0.910]
Epoch [8/120    avg_loss:0.437, val_acc:0.892]
Epoch [9/120    avg_loss:0.426, val_acc:0.875]
Epoch [10/120    avg_loss:0.470, val_acc:0.900]
Epoch [11/120    avg_loss:0.419, val_acc:0.879]
Epoch [12/120    avg_loss:0.329, val_acc:0.877]
Epoch [13/120    avg_loss:0.287, val_acc:0.931]
Epoch [14/120    avg_loss:0.365, val_acc:0.917]
Epoch [15/120    avg_loss:0.285, val_acc:0.965]
Epoch [16/120    avg_loss:0.248, val_acc:0.946]
Epoch [17/120    avg_loss:0.178, val_acc:0.958]
Epoch [18/120    avg_loss:0.157, val_acc:0.944]
Epoch [19/120    avg_loss:0.150, val_acc:0.967]
Epoch [20/120    avg_loss:0.153, val_acc:0.946]
Epoch [21/120    avg_loss:0.216, val_acc:0.923]
Epoch [22/120    avg_loss:0.252, val_acc:0.944]
Epoch [23/120    avg_loss:0.244, val_acc:0.925]
Epoch [24/120    avg_loss:0.187, val_acc:0.958]
Epoch [25/120    avg_loss:0.185, val_acc:0.967]
Epoch [26/120    avg_loss:0.149, val_acc:0.963]
Epoch [27/120    avg_loss:0.127, val_acc:0.983]
Epoch [28/120    avg_loss:0.137, val_acc:0.965]
Epoch [29/120    avg_loss:0.106, val_acc:0.967]
Epoch [30/120    avg_loss:0.076, val_acc:0.979]
Epoch [31/120    avg_loss:0.077, val_acc:0.977]
Epoch [32/120    avg_loss:0.059, val_acc:0.960]
Epoch [33/120    avg_loss:0.084, val_acc:0.973]
Epoch [34/120    avg_loss:0.099, val_acc:0.983]
Epoch [35/120    avg_loss:0.056, val_acc:0.981]
Epoch [36/120    avg_loss:0.081, val_acc:0.983]
Epoch [37/120    avg_loss:0.079, val_acc:0.967]
Epoch [38/120    avg_loss:0.121, val_acc:0.963]
Epoch [39/120    avg_loss:0.102, val_acc:0.956]
Epoch [40/120    avg_loss:0.093, val_acc:0.983]
Epoch [41/120    avg_loss:0.039, val_acc:0.977]
Epoch [42/120    avg_loss:0.052, val_acc:0.983]
Epoch [43/120    avg_loss:0.056, val_acc:0.979]
Epoch [44/120    avg_loss:0.067, val_acc:0.981]
Epoch [45/120    avg_loss:0.072, val_acc:0.979]
Epoch [46/120    avg_loss:0.063, val_acc:0.977]
Epoch [47/120    avg_loss:0.041, val_acc:0.985]
Epoch [48/120    avg_loss:0.044, val_acc:0.981]
Epoch [49/120    avg_loss:0.053, val_acc:0.960]
Epoch [50/120    avg_loss:0.073, val_acc:0.975]
Epoch [51/120    avg_loss:0.083, val_acc:0.983]
Epoch [52/120    avg_loss:0.095, val_acc:0.973]
Epoch [53/120    avg_loss:0.069, val_acc:0.973]
Epoch [54/120    avg_loss:0.061, val_acc:0.971]
Epoch [55/120    avg_loss:0.026, val_acc:0.971]
Epoch [56/120    avg_loss:0.035, val_acc:0.990]
Epoch [57/120    avg_loss:0.117, val_acc:0.977]
Epoch [58/120    avg_loss:0.089, val_acc:0.969]
Epoch [59/120    avg_loss:0.156, val_acc:0.981]
Epoch [60/120    avg_loss:0.256, val_acc:0.963]
Epoch [61/120    avg_loss:0.133, val_acc:0.969]
Epoch [62/120    avg_loss:0.068, val_acc:0.963]
Epoch [63/120    avg_loss:0.039, val_acc:0.969]
Epoch [64/120    avg_loss:0.032, val_acc:0.988]
Epoch [65/120    avg_loss:0.024, val_acc:0.977]
Epoch [66/120    avg_loss:0.024, val_acc:0.977]
Epoch [67/120    avg_loss:0.068, val_acc:0.979]
Epoch [68/120    avg_loss:0.029, val_acc:0.981]
Epoch [69/120    avg_loss:0.024, val_acc:0.977]
Epoch [70/120    avg_loss:0.034, val_acc:0.985]
Epoch [71/120    avg_loss:0.025, val_acc:0.990]
Epoch [72/120    avg_loss:0.029, val_acc:0.988]
Epoch [73/120    avg_loss:0.019, val_acc:0.983]
Epoch [74/120    avg_loss:0.018, val_acc:0.985]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.016, val_acc:0.985]
Epoch [77/120    avg_loss:0.023, val_acc:0.988]
Epoch [78/120    avg_loss:0.019, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.023, val_acc:0.988]
Epoch [83/120    avg_loss:0.016, val_acc:0.990]
Epoch [84/120    avg_loss:0.032, val_acc:0.990]
Epoch [85/120    avg_loss:0.025, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.013, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.016, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.013, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.017, val_acc:0.988]
Epoch [101/120    avg_loss:0.022, val_acc:0.988]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.019, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.012, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.014, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.023, val_acc:0.988]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.015, val_acc:0.988]
Epoch [117/120    avg_loss:0.016, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.020, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   8   0   0   0   0   0   0   3   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.99545455 0.99343545 0.96428571 0.96621622
 1.         0.98924731 1.         1.         1.         0.98950131
 0.98779134 1.        ]

Kappa:
0.993828138717163
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa224c1c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.124, val_acc:0.656]
Epoch [2/120    avg_loss:1.365, val_acc:0.702]
Epoch [3/120    avg_loss:0.985, val_acc:0.792]
Epoch [4/120    avg_loss:0.763, val_acc:0.819]
Epoch [5/120    avg_loss:0.686, val_acc:0.865]
Epoch [6/120    avg_loss:0.579, val_acc:0.894]
Epoch [7/120    avg_loss:0.555, val_acc:0.908]
Epoch [8/120    avg_loss:0.472, val_acc:0.904]
Epoch [9/120    avg_loss:0.373, val_acc:0.912]
Epoch [10/120    avg_loss:0.405, val_acc:0.954]
Epoch [11/120    avg_loss:0.302, val_acc:0.929]
Epoch [12/120    avg_loss:0.264, val_acc:0.927]
Epoch [13/120    avg_loss:0.387, val_acc:0.867]
Epoch [14/120    avg_loss:0.306, val_acc:0.931]
Epoch [15/120    avg_loss:0.264, val_acc:0.923]
Epoch [16/120    avg_loss:0.270, val_acc:0.925]
Epoch [17/120    avg_loss:0.205, val_acc:0.921]
Epoch [18/120    avg_loss:0.283, val_acc:0.935]
Epoch [19/120    avg_loss:0.214, val_acc:0.946]
Epoch [20/120    avg_loss:0.198, val_acc:0.977]
Epoch [21/120    avg_loss:0.109, val_acc:0.971]
Epoch [22/120    avg_loss:0.154, val_acc:0.965]
Epoch [23/120    avg_loss:0.137, val_acc:0.956]
Epoch [24/120    avg_loss:0.075, val_acc:0.965]
Epoch [25/120    avg_loss:0.092, val_acc:0.981]
Epoch [26/120    avg_loss:0.157, val_acc:0.963]
Epoch [27/120    avg_loss:0.141, val_acc:0.952]
Epoch [28/120    avg_loss:0.094, val_acc:0.981]
Epoch [29/120    avg_loss:0.088, val_acc:0.963]
Epoch [30/120    avg_loss:0.149, val_acc:0.988]
Epoch [31/120    avg_loss:0.106, val_acc:0.975]
Epoch [32/120    avg_loss:0.124, val_acc:0.979]
Epoch [33/120    avg_loss:0.113, val_acc:0.933]
Epoch [34/120    avg_loss:0.175, val_acc:0.979]
Epoch [35/120    avg_loss:0.115, val_acc:0.963]
Epoch [36/120    avg_loss:0.135, val_acc:0.969]
Epoch [37/120    avg_loss:0.069, val_acc:0.985]
Epoch [38/120    avg_loss:0.067, val_acc:0.975]
Epoch [39/120    avg_loss:0.164, val_acc:0.958]
Epoch [40/120    avg_loss:0.131, val_acc:0.975]
Epoch [41/120    avg_loss:0.177, val_acc:0.944]
Epoch [42/120    avg_loss:0.207, val_acc:0.971]
Epoch [43/120    avg_loss:0.084, val_acc:0.985]
Epoch [44/120    avg_loss:0.056, val_acc:0.983]
Epoch [45/120    avg_loss:0.074, val_acc:0.990]
Epoch [46/120    avg_loss:0.051, val_acc:0.992]
Epoch [47/120    avg_loss:0.054, val_acc:0.992]
Epoch [48/120    avg_loss:0.051, val_acc:0.990]
Epoch [49/120    avg_loss:0.048, val_acc:0.994]
Epoch [50/120    avg_loss:0.049, val_acc:0.992]
Epoch [51/120    avg_loss:0.050, val_acc:0.994]
Epoch [52/120    avg_loss:0.061, val_acc:0.992]
Epoch [53/120    avg_loss:0.047, val_acc:0.994]
Epoch [54/120    avg_loss:0.044, val_acc:0.994]
Epoch [55/120    avg_loss:0.031, val_acc:0.994]
Epoch [56/120    avg_loss:0.033, val_acc:0.994]
Epoch [57/120    avg_loss:0.049, val_acc:0.992]
Epoch [58/120    avg_loss:0.035, val_acc:0.992]
Epoch [59/120    avg_loss:0.023, val_acc:0.994]
Epoch [60/120    avg_loss:0.043, val_acc:0.994]
Epoch [61/120    avg_loss:0.044, val_acc:0.994]
Epoch [62/120    avg_loss:0.028, val_acc:0.994]
Epoch [63/120    avg_loss:0.033, val_acc:0.994]
Epoch [64/120    avg_loss:0.034, val_acc:0.994]
Epoch [65/120    avg_loss:0.035, val_acc:0.994]
Epoch [66/120    avg_loss:0.049, val_acc:0.992]
Epoch [67/120    avg_loss:0.035, val_acc:0.992]
Epoch [68/120    avg_loss:0.038, val_acc:0.994]
Epoch [69/120    avg_loss:0.027, val_acc:0.994]
Epoch [70/120    avg_loss:0.053, val_acc:0.994]
Epoch [71/120    avg_loss:0.040, val_acc:0.992]
Epoch [72/120    avg_loss:0.050, val_acc:0.992]
Epoch [73/120    avg_loss:0.036, val_acc:0.992]
Epoch [74/120    avg_loss:0.020, val_acc:0.994]
Epoch [75/120    avg_loss:0.032, val_acc:0.994]
Epoch [76/120    avg_loss:0.031, val_acc:0.994]
Epoch [77/120    avg_loss:0.024, val_acc:0.994]
Epoch [78/120    avg_loss:0.034, val_acc:0.994]
Epoch [79/120    avg_loss:0.020, val_acc:0.996]
Epoch [80/120    avg_loss:0.030, val_acc:0.996]
Epoch [81/120    avg_loss:0.030, val_acc:0.992]
Epoch [82/120    avg_loss:0.030, val_acc:0.996]
Epoch [83/120    avg_loss:0.031, val_acc:0.996]
Epoch [84/120    avg_loss:0.032, val_acc:0.996]
Epoch [85/120    avg_loss:0.019, val_acc:0.996]
Epoch [86/120    avg_loss:0.037, val_acc:0.996]
Epoch [87/120    avg_loss:0.029, val_acc:0.996]
Epoch [88/120    avg_loss:0.031, val_acc:0.996]
Epoch [89/120    avg_loss:0.026, val_acc:0.996]
Epoch [90/120    avg_loss:0.046, val_acc:0.996]
Epoch [91/120    avg_loss:0.023, val_acc:0.996]
Epoch [92/120    avg_loss:0.039, val_acc:0.996]
Epoch [93/120    avg_loss:0.030, val_acc:0.996]
Epoch [94/120    avg_loss:0.029, val_acc:0.996]
Epoch [95/120    avg_loss:0.025, val_acc:0.994]
Epoch [96/120    avg_loss:0.024, val_acc:0.994]
Epoch [97/120    avg_loss:0.027, val_acc:0.996]
Epoch [98/120    avg_loss:0.020, val_acc:0.996]
Epoch [99/120    avg_loss:0.037, val_acc:0.996]
Epoch [100/120    avg_loss:0.026, val_acc:0.996]
Epoch [101/120    avg_loss:0.022, val_acc:0.996]
Epoch [102/120    avg_loss:0.024, val_acc:0.994]
Epoch [103/120    avg_loss:0.018, val_acc:0.996]
Epoch [104/120    avg_loss:0.020, val_acc:0.996]
Epoch [105/120    avg_loss:0.029, val_acc:0.996]
Epoch [106/120    avg_loss:0.037, val_acc:0.996]
Epoch [107/120    avg_loss:0.021, val_acc:0.996]
Epoch [108/120    avg_loss:0.022, val_acc:0.996]
Epoch [109/120    avg_loss:0.037, val_acc:0.996]
Epoch [110/120    avg_loss:0.022, val_acc:0.996]
Epoch [111/120    avg_loss:0.026, val_acc:0.996]
Epoch [112/120    avg_loss:0.024, val_acc:0.996]
Epoch [113/120    avg_loss:0.014, val_acc:0.996]
Epoch [114/120    avg_loss:0.033, val_acc:0.996]
Epoch [115/120    avg_loss:0.025, val_acc:0.996]
Epoch [116/120    avg_loss:0.033, val_acc:0.996]
Epoch [117/120    avg_loss:0.031, val_acc:0.996]
Epoch [118/120    avg_loss:0.018, val_acc:0.996]
Epoch [119/120    avg_loss:0.026, val_acc:0.996]
Epoch [120/120    avg_loss:0.024, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  12   0   0   0   0   0   0   5   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.99095023 1.         0.9610984  0.9602649
 1.         0.97826087 1.         1.         1.         0.99080158
 0.98672566 1.        ]

Kappa:
0.9933532809140768
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa53bf327f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.062, val_acc:0.598]
Epoch [2/120    avg_loss:1.379, val_acc:0.635]
Epoch [3/120    avg_loss:0.962, val_acc:0.752]
Epoch [4/120    avg_loss:0.774, val_acc:0.767]
Epoch [5/120    avg_loss:0.608, val_acc:0.775]
Epoch [6/120    avg_loss:0.638, val_acc:0.867]
Epoch [7/120    avg_loss:0.627, val_acc:0.867]
Epoch [8/120    avg_loss:0.531, val_acc:0.885]
Epoch [9/120    avg_loss:0.476, val_acc:0.898]
Epoch [10/120    avg_loss:0.409, val_acc:0.912]
Epoch [11/120    avg_loss:0.356, val_acc:0.908]
Epoch [12/120    avg_loss:0.425, val_acc:0.867]
Epoch [13/120    avg_loss:0.362, val_acc:0.892]
Epoch [14/120    avg_loss:0.317, val_acc:0.896]
Epoch [15/120    avg_loss:0.240, val_acc:0.929]
Epoch [16/120    avg_loss:0.227, val_acc:0.921]
Epoch [17/120    avg_loss:0.257, val_acc:0.935]
Epoch [18/120    avg_loss:0.310, val_acc:0.931]
Epoch [19/120    avg_loss:0.222, val_acc:0.938]
Epoch [20/120    avg_loss:0.187, val_acc:0.927]
Epoch [21/120    avg_loss:0.184, val_acc:0.933]
Epoch [22/120    avg_loss:0.212, val_acc:0.925]
Epoch [23/120    avg_loss:0.216, val_acc:0.925]
Epoch [24/120    avg_loss:0.129, val_acc:0.948]
Epoch [25/120    avg_loss:0.164, val_acc:0.952]
Epoch [26/120    avg_loss:0.218, val_acc:0.958]
Epoch [27/120    avg_loss:0.142, val_acc:0.950]
Epoch [28/120    avg_loss:0.136, val_acc:0.956]
Epoch [29/120    avg_loss:0.107, val_acc:0.965]
Epoch [30/120    avg_loss:0.148, val_acc:0.919]
Epoch [31/120    avg_loss:0.205, val_acc:0.956]
Epoch [32/120    avg_loss:0.085, val_acc:0.960]
Epoch [33/120    avg_loss:0.075, val_acc:0.944]
Epoch [34/120    avg_loss:0.085, val_acc:0.950]
Epoch [35/120    avg_loss:0.077, val_acc:0.971]
Epoch [36/120    avg_loss:0.051, val_acc:0.954]
Epoch [37/120    avg_loss:0.041, val_acc:0.975]
Epoch [38/120    avg_loss:0.064, val_acc:0.971]
Epoch [39/120    avg_loss:0.068, val_acc:0.956]
Epoch [40/120    avg_loss:0.045, val_acc:0.963]
Epoch [41/120    avg_loss:0.064, val_acc:0.971]
Epoch [42/120    avg_loss:0.066, val_acc:0.952]
Epoch [43/120    avg_loss:0.103, val_acc:0.935]
Epoch [44/120    avg_loss:0.168, val_acc:0.927]
Epoch [45/120    avg_loss:0.323, val_acc:0.940]
Epoch [46/120    avg_loss:0.120, val_acc:0.963]
Epoch [47/120    avg_loss:0.144, val_acc:0.952]
Epoch [48/120    avg_loss:0.101, val_acc:0.965]
Epoch [49/120    avg_loss:0.061, val_acc:0.977]
Epoch [50/120    avg_loss:0.037, val_acc:0.983]
Epoch [51/120    avg_loss:0.025, val_acc:0.983]
Epoch [52/120    avg_loss:0.075, val_acc:0.973]
Epoch [53/120    avg_loss:0.084, val_acc:0.969]
Epoch [54/120    avg_loss:0.035, val_acc:0.973]
Epoch [55/120    avg_loss:0.031, val_acc:0.975]
Epoch [56/120    avg_loss:0.022, val_acc:0.971]
Epoch [57/120    avg_loss:0.044, val_acc:0.973]
Epoch [58/120    avg_loss:0.069, val_acc:0.956]
Epoch [59/120    avg_loss:0.063, val_acc:0.983]
Epoch [60/120    avg_loss:0.034, val_acc:0.963]
Epoch [61/120    avg_loss:0.030, val_acc:0.981]
Epoch [62/120    avg_loss:0.022, val_acc:0.983]
Epoch [63/120    avg_loss:0.023, val_acc:0.992]
Epoch [64/120    avg_loss:0.033, val_acc:0.975]
Epoch [65/120    avg_loss:0.041, val_acc:0.977]
Epoch [66/120    avg_loss:0.037, val_acc:0.971]
Epoch [67/120    avg_loss:0.025, val_acc:0.975]
Epoch [68/120    avg_loss:0.016, val_acc:0.990]
Epoch [69/120    avg_loss:0.024, val_acc:0.977]
Epoch [70/120    avg_loss:0.066, val_acc:0.988]
Epoch [71/120    avg_loss:0.045, val_acc:0.965]
Epoch [72/120    avg_loss:0.118, val_acc:0.977]
Epoch [73/120    avg_loss:0.047, val_acc:0.983]
Epoch [74/120    avg_loss:0.071, val_acc:0.981]
Epoch [75/120    avg_loss:0.070, val_acc:0.948]
Epoch [76/120    avg_loss:0.043, val_acc:0.981]
Epoch [77/120    avg_loss:0.041, val_acc:0.981]
Epoch [78/120    avg_loss:0.015, val_acc:0.985]
Epoch [79/120    avg_loss:0.021, val_acc:0.990]
Epoch [80/120    avg_loss:0.011, val_acc:0.990]
Epoch [81/120    avg_loss:0.011, val_acc:0.990]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.009, val_acc:0.990]
Epoch [84/120    avg_loss:0.024, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.016, val_acc:0.990]
Epoch [87/120    avg_loss:0.019, val_acc:0.990]
Epoch [88/120    avg_loss:0.018, val_acc:0.990]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.010, val_acc:0.990]
Epoch [91/120    avg_loss:0.015, val_acc:0.990]
Epoch [92/120    avg_loss:0.012, val_acc:0.990]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.013, val_acc:0.990]
Epoch [98/120    avg_loss:0.013, val_acc:0.990]
Epoch [99/120    avg_loss:0.016, val_acc:0.990]
Epoch [100/120    avg_loss:0.015, val_acc:0.990]
Epoch [101/120    avg_loss:0.014, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.990]
Epoch [103/120    avg_loss:0.012, val_acc:0.990]
Epoch [104/120    avg_loss:0.012, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.015, val_acc:0.990]
Epoch [108/120    avg_loss:0.015, val_acc:0.990]
Epoch [109/120    avg_loss:0.016, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.016, val_acc:0.990]
Epoch [116/120    avg_loss:0.016, val_acc:0.990]
Epoch [117/120    avg_loss:0.010, val_acc:0.990]
Epoch [118/120    avg_loss:0.017, val_acc:0.990]
Epoch [119/120    avg_loss:0.016, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   0   0   0   0   0   2   0]
 [  0   0   0 213  17   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  14 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.99541284 0.96162528 0.93991416 0.96271186
 1.         1.         1.         1.         1.         0.98177083
 0.98210291 1.        ]

Kappa:
0.9895557747490965
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faadc0c0748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.196, val_acc:0.615]
Epoch [2/120    avg_loss:1.388, val_acc:0.765]
Epoch [3/120    avg_loss:0.958, val_acc:0.812]
Epoch [4/120    avg_loss:0.836, val_acc:0.787]
Epoch [5/120    avg_loss:0.652, val_acc:0.850]
Epoch [6/120    avg_loss:0.631, val_acc:0.787]
Epoch [7/120    avg_loss:0.626, val_acc:0.806]
Epoch [8/120    avg_loss:0.644, val_acc:0.877]
Epoch [9/120    avg_loss:0.506, val_acc:0.846]
Epoch [10/120    avg_loss:0.398, val_acc:0.887]
Epoch [11/120    avg_loss:0.395, val_acc:0.885]
Epoch [12/120    avg_loss:0.379, val_acc:0.904]
Epoch [13/120    avg_loss:0.294, val_acc:0.925]
Epoch [14/120    avg_loss:0.210, val_acc:0.944]
Epoch [15/120    avg_loss:0.276, val_acc:0.935]
Epoch [16/120    avg_loss:0.291, val_acc:0.933]
Epoch [17/120    avg_loss:0.204, val_acc:0.946]
Epoch [18/120    avg_loss:0.189, val_acc:0.948]
Epoch [19/120    avg_loss:0.213, val_acc:0.912]
Epoch [20/120    avg_loss:0.247, val_acc:0.942]
Epoch [21/120    avg_loss:0.247, val_acc:0.925]
Epoch [22/120    avg_loss:0.245, val_acc:0.946]
Epoch [23/120    avg_loss:0.206, val_acc:0.952]
Epoch [24/120    avg_loss:0.158, val_acc:0.960]
Epoch [25/120    avg_loss:0.155, val_acc:0.944]
Epoch [26/120    avg_loss:0.160, val_acc:0.952]
Epoch [27/120    avg_loss:0.129, val_acc:0.940]
Epoch [28/120    avg_loss:0.124, val_acc:0.952]
Epoch [29/120    avg_loss:0.119, val_acc:0.960]
Epoch [30/120    avg_loss:0.200, val_acc:0.940]
Epoch [31/120    avg_loss:0.218, val_acc:0.933]
Epoch [32/120    avg_loss:0.208, val_acc:0.971]
Epoch [33/120    avg_loss:0.118, val_acc:0.952]
Epoch [34/120    avg_loss:0.110, val_acc:0.940]
Epoch [35/120    avg_loss:0.109, val_acc:0.965]
Epoch [36/120    avg_loss:0.114, val_acc:0.973]
Epoch [37/120    avg_loss:0.079, val_acc:0.985]
Epoch [38/120    avg_loss:0.108, val_acc:0.973]
Epoch [39/120    avg_loss:0.127, val_acc:0.938]
Epoch [40/120    avg_loss:0.129, val_acc:0.954]
Epoch [41/120    avg_loss:0.121, val_acc:0.965]
Epoch [42/120    avg_loss:0.078, val_acc:0.973]
Epoch [43/120    avg_loss:0.082, val_acc:0.975]
Epoch [44/120    avg_loss:0.057, val_acc:0.973]
Epoch [45/120    avg_loss:0.081, val_acc:0.981]
Epoch [46/120    avg_loss:0.034, val_acc:0.977]
Epoch [47/120    avg_loss:0.046, val_acc:0.975]
Epoch [48/120    avg_loss:0.055, val_acc:0.975]
Epoch [49/120    avg_loss:0.040, val_acc:0.977]
Epoch [50/120    avg_loss:0.093, val_acc:0.965]
Epoch [51/120    avg_loss:0.079, val_acc:0.965]
Epoch [52/120    avg_loss:0.054, val_acc:0.975]
Epoch [53/120    avg_loss:0.038, val_acc:0.981]
Epoch [54/120    avg_loss:0.032, val_acc:0.979]
Epoch [55/120    avg_loss:0.023, val_acc:0.979]
Epoch [56/120    avg_loss:0.044, val_acc:0.979]
Epoch [57/120    avg_loss:0.025, val_acc:0.979]
Epoch [58/120    avg_loss:0.035, val_acc:0.979]
Epoch [59/120    avg_loss:0.025, val_acc:0.979]
Epoch [60/120    avg_loss:0.037, val_acc:0.979]
Epoch [61/120    avg_loss:0.018, val_acc:0.979]
Epoch [62/120    avg_loss:0.030, val_acc:0.979]
Epoch [63/120    avg_loss:0.029, val_acc:0.979]
Epoch [64/120    avg_loss:0.022, val_acc:0.979]
Epoch [65/120    avg_loss:0.021, val_acc:0.979]
Epoch [66/120    avg_loss:0.027, val_acc:0.979]
Epoch [67/120    avg_loss:0.029, val_acc:0.979]
Epoch [68/120    avg_loss:0.020, val_acc:0.979]
Epoch [69/120    avg_loss:0.018, val_acc:0.979]
Epoch [70/120    avg_loss:0.026, val_acc:0.979]
Epoch [71/120    avg_loss:0.024, val_acc:0.979]
Epoch [72/120    avg_loss:0.018, val_acc:0.979]
Epoch [73/120    avg_loss:0.025, val_acc:0.979]
Epoch [74/120    avg_loss:0.025, val_acc:0.979]
Epoch [75/120    avg_loss:0.020, val_acc:0.979]
Epoch [76/120    avg_loss:0.030, val_acc:0.979]
Epoch [77/120    avg_loss:0.020, val_acc:0.979]
Epoch [78/120    avg_loss:0.022, val_acc:0.979]
Epoch [79/120    avg_loss:0.038, val_acc:0.979]
Epoch [80/120    avg_loss:0.023, val_acc:0.979]
Epoch [81/120    avg_loss:0.017, val_acc:0.979]
Epoch [82/120    avg_loss:0.028, val_acc:0.979]
Epoch [83/120    avg_loss:0.031, val_acc:0.979]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.019, val_acc:0.979]
Epoch [86/120    avg_loss:0.019, val_acc:0.979]
Epoch [87/120    avg_loss:0.030, val_acc:0.979]
Epoch [88/120    avg_loss:0.029, val_acc:0.979]
Epoch [89/120    avg_loss:0.033, val_acc:0.979]
Epoch [90/120    avg_loss:0.029, val_acc:0.979]
Epoch [91/120    avg_loss:0.026, val_acc:0.979]
Epoch [92/120    avg_loss:0.037, val_acc:0.979]
Epoch [93/120    avg_loss:0.024, val_acc:0.979]
Epoch [94/120    avg_loss:0.024, val_acc:0.979]
Epoch [95/120    avg_loss:0.021, val_acc:0.979]
Epoch [96/120    avg_loss:0.018, val_acc:0.979]
Epoch [97/120    avg_loss:0.013, val_acc:0.979]
Epoch [98/120    avg_loss:0.018, val_acc:0.979]
Epoch [99/120    avg_loss:0.037, val_acc:0.979]
Epoch [100/120    avg_loss:0.023, val_acc:0.979]
Epoch [101/120    avg_loss:0.024, val_acc:0.979]
Epoch [102/120    avg_loss:0.040, val_acc:0.979]
Epoch [103/120    avg_loss:0.025, val_acc:0.979]
Epoch [104/120    avg_loss:0.023, val_acc:0.979]
Epoch [105/120    avg_loss:0.033, val_acc:0.979]
Epoch [106/120    avg_loss:0.025, val_acc:0.979]
Epoch [107/120    avg_loss:0.024, val_acc:0.979]
Epoch [108/120    avg_loss:0.021, val_acc:0.979]
Epoch [109/120    avg_loss:0.016, val_acc:0.979]
Epoch [110/120    avg_loss:0.031, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.018, val_acc:0.979]
Epoch [113/120    avg_loss:0.017, val_acc:0.979]
Epoch [114/120    avg_loss:0.030, val_acc:0.979]
Epoch [115/120    avg_loss:0.044, val_acc:0.979]
Epoch [116/120    avg_loss:0.022, val_acc:0.979]
Epoch [117/120    avg_loss:0.028, val_acc:0.979]
Epoch [118/120    avg_loss:0.028, val_acc:0.979]
Epoch [119/120    avg_loss:0.038, val_acc:0.979]
Epoch [120/120    avg_loss:0.026, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 203  27   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213   9   0   0   0   0   0   0   5   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 1.         0.99771167 0.93764434 0.89308176 0.93425606
 1.         1.         1.         1.         1.         0.98950131
 0.98451327 1.        ]

Kappa:
0.9857565669742772
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27b1511780>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.109, val_acc:0.635]
Epoch [2/120    avg_loss:1.289, val_acc:0.715]
Epoch [3/120    avg_loss:0.949, val_acc:0.771]
Epoch [4/120    avg_loss:0.805, val_acc:0.750]
Epoch [5/120    avg_loss:0.646, val_acc:0.863]
Epoch [6/120    avg_loss:0.579, val_acc:0.875]
Epoch [7/120    avg_loss:0.650, val_acc:0.858]
Epoch [8/120    avg_loss:0.448, val_acc:0.885]
Epoch [9/120    avg_loss:0.466, val_acc:0.840]
Epoch [10/120    avg_loss:0.522, val_acc:0.873]
Epoch [11/120    avg_loss:0.391, val_acc:0.898]
Epoch [12/120    avg_loss:0.343, val_acc:0.925]
Epoch [13/120    avg_loss:0.338, val_acc:0.931]
Epoch [14/120    avg_loss:0.331, val_acc:0.948]
Epoch [15/120    avg_loss:0.273, val_acc:0.956]
Epoch [16/120    avg_loss:0.196, val_acc:0.969]
Epoch [17/120    avg_loss:0.243, val_acc:0.952]
Epoch [18/120    avg_loss:0.265, val_acc:0.944]
Epoch [19/120    avg_loss:0.299, val_acc:0.950]
Epoch [20/120    avg_loss:0.191, val_acc:0.956]
Epoch [21/120    avg_loss:0.173, val_acc:0.973]
Epoch [22/120    avg_loss:0.204, val_acc:0.983]
Epoch [23/120    avg_loss:0.146, val_acc:0.950]
Epoch [24/120    avg_loss:0.152, val_acc:0.981]
Epoch [25/120    avg_loss:0.122, val_acc:0.969]
Epoch [26/120    avg_loss:0.120, val_acc:0.971]
Epoch [27/120    avg_loss:0.165, val_acc:0.977]
Epoch [28/120    avg_loss:0.115, val_acc:0.971]
Epoch [29/120    avg_loss:0.114, val_acc:0.965]
Epoch [30/120    avg_loss:0.099, val_acc:0.975]
Epoch [31/120    avg_loss:0.065, val_acc:0.973]
Epoch [32/120    avg_loss:0.072, val_acc:0.977]
Epoch [33/120    avg_loss:0.080, val_acc:0.996]
Epoch [34/120    avg_loss:0.055, val_acc:0.985]
Epoch [35/120    avg_loss:0.080, val_acc:0.965]
Epoch [36/120    avg_loss:0.172, val_acc:0.977]
Epoch [37/120    avg_loss:0.147, val_acc:0.990]
Epoch [38/120    avg_loss:0.056, val_acc:0.985]
Epoch [39/120    avg_loss:0.039, val_acc:0.994]
Epoch [40/120    avg_loss:0.049, val_acc:0.988]
Epoch [41/120    avg_loss:0.047, val_acc:0.988]
Epoch [42/120    avg_loss:0.043, val_acc:0.981]
Epoch [43/120    avg_loss:0.092, val_acc:0.971]
Epoch [44/120    avg_loss:0.061, val_acc:0.973]
Epoch [45/120    avg_loss:0.062, val_acc:0.992]
Epoch [46/120    avg_loss:0.076, val_acc:0.985]
Epoch [47/120    avg_loss:0.054, val_acc:0.983]
Epoch [48/120    avg_loss:0.040, val_acc:0.983]
Epoch [49/120    avg_loss:0.027, val_acc:0.983]
Epoch [50/120    avg_loss:0.019, val_acc:0.983]
Epoch [51/120    avg_loss:0.049, val_acc:0.988]
Epoch [52/120    avg_loss:0.030, val_acc:0.985]
Epoch [53/120    avg_loss:0.022, val_acc:0.988]
Epoch [54/120    avg_loss:0.034, val_acc:0.985]
Epoch [55/120    avg_loss:0.019, val_acc:0.983]
Epoch [56/120    avg_loss:0.027, val_acc:0.985]
Epoch [57/120    avg_loss:0.021, val_acc:0.985]
Epoch [58/120    avg_loss:0.025, val_acc:0.985]
Epoch [59/120    avg_loss:0.022, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.985]
Epoch [61/120    avg_loss:0.017, val_acc:0.985]
Epoch [62/120    avg_loss:0.021, val_acc:0.985]
Epoch [63/120    avg_loss:0.027, val_acc:0.985]
Epoch [64/120    avg_loss:0.019, val_acc:0.985]
Epoch [65/120    avg_loss:0.018, val_acc:0.985]
Epoch [66/120    avg_loss:0.031, val_acc:0.985]
Epoch [67/120    avg_loss:0.023, val_acc:0.985]
Epoch [68/120    avg_loss:0.020, val_acc:0.985]
Epoch [69/120    avg_loss:0.024, val_acc:0.985]
Epoch [70/120    avg_loss:0.028, val_acc:0.985]
Epoch [71/120    avg_loss:0.028, val_acc:0.985]
Epoch [72/120    avg_loss:0.020, val_acc:0.985]
Epoch [73/120    avg_loss:0.028, val_acc:0.985]
Epoch [74/120    avg_loss:0.015, val_acc:0.985]
Epoch [75/120    avg_loss:0.030, val_acc:0.985]
Epoch [76/120    avg_loss:0.021, val_acc:0.985]
Epoch [77/120    avg_loss:0.029, val_acc:0.985]
Epoch [78/120    avg_loss:0.038, val_acc:0.985]
Epoch [79/120    avg_loss:0.041, val_acc:0.985]
Epoch [80/120    avg_loss:0.023, val_acc:0.985]
Epoch [81/120    avg_loss:0.023, val_acc:0.985]
Epoch [82/120    avg_loss:0.024, val_acc:0.985]
Epoch [83/120    avg_loss:0.028, val_acc:0.985]
Epoch [84/120    avg_loss:0.023, val_acc:0.985]
Epoch [85/120    avg_loss:0.022, val_acc:0.985]
Epoch [86/120    avg_loss:0.027, val_acc:0.985]
Epoch [87/120    avg_loss:0.025, val_acc:0.985]
Epoch [88/120    avg_loss:0.025, val_acc:0.985]
Epoch [89/120    avg_loss:0.023, val_acc:0.985]
Epoch [90/120    avg_loss:0.018, val_acc:0.985]
Epoch [91/120    avg_loss:0.031, val_acc:0.985]
Epoch [92/120    avg_loss:0.021, val_acc:0.985]
Epoch [93/120    avg_loss:0.016, val_acc:0.985]
Epoch [94/120    avg_loss:0.039, val_acc:0.985]
Epoch [95/120    avg_loss:0.026, val_acc:0.985]
Epoch [96/120    avg_loss:0.020, val_acc:0.985]
Epoch [97/120    avg_loss:0.037, val_acc:0.985]
Epoch [98/120    avg_loss:0.022, val_acc:0.985]
Epoch [99/120    avg_loss:0.024, val_acc:0.985]
Epoch [100/120    avg_loss:0.024, val_acc:0.985]
Epoch [101/120    avg_loss:0.027, val_acc:0.985]
Epoch [102/120    avg_loss:0.019, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.031, val_acc:0.985]
Epoch [105/120    avg_loss:0.026, val_acc:0.985]
Epoch [106/120    avg_loss:0.021, val_acc:0.985]
Epoch [107/120    avg_loss:0.032, val_acc:0.985]
Epoch [108/120    avg_loss:0.018, val_acc:0.985]
Epoch [109/120    avg_loss:0.023, val_acc:0.985]
Epoch [110/120    avg_loss:0.024, val_acc:0.985]
Epoch [111/120    avg_loss:0.017, val_acc:0.985]
Epoch [112/120    avg_loss:0.024, val_acc:0.985]
Epoch [113/120    avg_loss:0.026, val_acc:0.985]
Epoch [114/120    avg_loss:0.022, val_acc:0.985]
Epoch [115/120    avg_loss:0.024, val_acc:0.985]
Epoch [116/120    avg_loss:0.037, val_acc:0.985]
Epoch [117/120    avg_loss:0.022, val_acc:0.985]
Epoch [118/120    avg_loss:0.021, val_acc:0.985]
Epoch [119/120    avg_loss:0.019, val_acc:0.985]
Epoch [120/120    avg_loss:0.026, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  16   0   0   0   0   0   0   5   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.98426966 0.99122807 0.94063927 0.9442623
 1.         0.96132597 1.         1.         1.         0.98562092
 0.98222222 1.        ]

Kappa:
0.9895552545810529
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d62816780>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.126, val_acc:0.590]
Epoch [2/120    avg_loss:1.301, val_acc:0.658]
Epoch [3/120    avg_loss:0.888, val_acc:0.808]
Epoch [4/120    avg_loss:0.711, val_acc:0.760]
Epoch [5/120    avg_loss:0.804, val_acc:0.725]
Epoch [6/120    avg_loss:0.671, val_acc:0.829]
Epoch [7/120    avg_loss:0.562, val_acc:0.863]
Epoch [8/120    avg_loss:0.436, val_acc:0.879]
Epoch [9/120    avg_loss:0.390, val_acc:0.887]
Epoch [10/120    avg_loss:0.362, val_acc:0.892]
Epoch [11/120    avg_loss:0.406, val_acc:0.894]
Epoch [12/120    avg_loss:0.302, val_acc:0.919]
Epoch [13/120    avg_loss:0.256, val_acc:0.946]
Epoch [14/120    avg_loss:0.214, val_acc:0.910]
Epoch [15/120    avg_loss:0.229, val_acc:0.931]
Epoch [16/120    avg_loss:0.222, val_acc:0.929]
Epoch [17/120    avg_loss:0.228, val_acc:0.960]
Epoch [18/120    avg_loss:0.185, val_acc:0.944]
Epoch [19/120    avg_loss:0.240, val_acc:0.927]
Epoch [20/120    avg_loss:0.226, val_acc:0.933]
Epoch [21/120    avg_loss:0.233, val_acc:0.944]
Epoch [22/120    avg_loss:0.201, val_acc:0.956]
Epoch [23/120    avg_loss:0.151, val_acc:0.954]
Epoch [24/120    avg_loss:0.143, val_acc:0.969]
Epoch [25/120    avg_loss:0.120, val_acc:0.965]
Epoch [26/120    avg_loss:0.126, val_acc:0.960]
Epoch [27/120    avg_loss:0.111, val_acc:0.967]
Epoch [28/120    avg_loss:0.081, val_acc:0.944]
Epoch [29/120    avg_loss:0.132, val_acc:0.967]
Epoch [30/120    avg_loss:0.080, val_acc:0.956]
Epoch [31/120    avg_loss:0.123, val_acc:0.950]
Epoch [32/120    avg_loss:0.120, val_acc:0.971]
Epoch [33/120    avg_loss:0.090, val_acc:0.975]
Epoch [34/120    avg_loss:0.106, val_acc:0.956]
Epoch [35/120    avg_loss:0.091, val_acc:0.967]
Epoch [36/120    avg_loss:0.123, val_acc:0.973]
Epoch [37/120    avg_loss:0.120, val_acc:0.967]
Epoch [38/120    avg_loss:0.091, val_acc:0.965]
Epoch [39/120    avg_loss:0.097, val_acc:0.979]
Epoch [40/120    avg_loss:0.082, val_acc:0.981]
Epoch [41/120    avg_loss:0.064, val_acc:0.969]
Epoch [42/120    avg_loss:0.057, val_acc:0.977]
Epoch [43/120    avg_loss:0.065, val_acc:0.969]
Epoch [44/120    avg_loss:0.093, val_acc:0.960]
Epoch [45/120    avg_loss:0.056, val_acc:0.977]
Epoch [46/120    avg_loss:0.053, val_acc:0.983]
Epoch [47/120    avg_loss:0.025, val_acc:0.992]
Epoch [48/120    avg_loss:0.033, val_acc:0.981]
Epoch [49/120    avg_loss:0.024, val_acc:0.990]
Epoch [50/120    avg_loss:0.063, val_acc:0.958]
Epoch [51/120    avg_loss:0.121, val_acc:0.944]
Epoch [52/120    avg_loss:0.073, val_acc:0.975]
Epoch [53/120    avg_loss:0.050, val_acc:0.979]
Epoch [54/120    avg_loss:0.045, val_acc:0.973]
Epoch [55/120    avg_loss:0.028, val_acc:0.983]
Epoch [56/120    avg_loss:0.053, val_acc:0.973]
Epoch [57/120    avg_loss:0.050, val_acc:0.975]
Epoch [58/120    avg_loss:0.065, val_acc:0.960]
Epoch [59/120    avg_loss:0.068, val_acc:0.969]
Epoch [60/120    avg_loss:0.036, val_acc:0.973]
Epoch [61/120    avg_loss:0.020, val_acc:0.979]
Epoch [62/120    avg_loss:0.027, val_acc:0.979]
Epoch [63/120    avg_loss:0.040, val_acc:0.981]
Epoch [64/120    avg_loss:0.024, val_acc:0.979]
Epoch [65/120    avg_loss:0.021, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.977]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.014, val_acc:0.977]
Epoch [70/120    avg_loss:0.026, val_acc:0.977]
Epoch [71/120    avg_loss:0.013, val_acc:0.981]
Epoch [72/120    avg_loss:0.022, val_acc:0.981]
Epoch [73/120    avg_loss:0.024, val_acc:0.981]
Epoch [74/120    avg_loss:0.011, val_acc:0.981]
Epoch [75/120    avg_loss:0.020, val_acc:0.981]
Epoch [76/120    avg_loss:0.032, val_acc:0.981]
Epoch [77/120    avg_loss:0.015, val_acc:0.981]
Epoch [78/120    avg_loss:0.017, val_acc:0.981]
Epoch [79/120    avg_loss:0.020, val_acc:0.981]
Epoch [80/120    avg_loss:0.021, val_acc:0.981]
Epoch [81/120    avg_loss:0.015, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.018, val_acc:0.981]
Epoch [84/120    avg_loss:0.021, val_acc:0.981]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.031, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.981]
Epoch [88/120    avg_loss:0.032, val_acc:0.981]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.014, val_acc:0.981]
Epoch [91/120    avg_loss:0.015, val_acc:0.981]
Epoch [92/120    avg_loss:0.013, val_acc:0.981]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.016, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.027, val_acc:0.981]
Epoch [97/120    avg_loss:0.020, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.981]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.019, val_acc:0.981]
Epoch [101/120    avg_loss:0.014, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.014, val_acc:0.981]
Epoch [104/120    avg_loss:0.020, val_acc:0.981]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.035, val_acc:0.981]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.981]
Epoch [111/120    avg_loss:0.015, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.024, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.018, val_acc:0.981]
Epoch [120/120    avg_loss:0.015, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215   7   0   0   0   0   0   0   5   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.98866213 1.         0.95768374 0.95172414
 1.         0.97826087 1.         1.         1.         0.99472296
 0.98898678 1.        ]

Kappa:
0.9933527944129458
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7145719710>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.221, val_acc:0.558]
Epoch [2/120    avg_loss:1.361, val_acc:0.671]
Epoch [3/120    avg_loss:1.020, val_acc:0.808]
Epoch [4/120    avg_loss:0.748, val_acc:0.831]
Epoch [5/120    avg_loss:0.696, val_acc:0.779]
Epoch [6/120    avg_loss:0.526, val_acc:0.904]
Epoch [7/120    avg_loss:0.501, val_acc:0.906]
Epoch [8/120    avg_loss:0.555, val_acc:0.910]
Epoch [9/120    avg_loss:0.445, val_acc:0.898]
Epoch [10/120    avg_loss:0.421, val_acc:0.902]
Epoch [11/120    avg_loss:0.375, val_acc:0.925]
Epoch [12/120    avg_loss:0.333, val_acc:0.935]
Epoch [13/120    avg_loss:0.294, val_acc:0.919]
Epoch [14/120    avg_loss:0.337, val_acc:0.942]
Epoch [15/120    avg_loss:0.250, val_acc:0.925]
Epoch [16/120    avg_loss:0.279, val_acc:0.956]
Epoch [17/120    avg_loss:0.225, val_acc:0.948]
Epoch [18/120    avg_loss:0.331, val_acc:0.944]
Epoch [19/120    avg_loss:0.440, val_acc:0.923]
Epoch [20/120    avg_loss:0.262, val_acc:0.950]
Epoch [21/120    avg_loss:0.192, val_acc:0.967]
Epoch [22/120    avg_loss:0.208, val_acc:0.925]
Epoch [23/120    avg_loss:0.256, val_acc:0.950]
Epoch [24/120    avg_loss:0.118, val_acc:0.971]
Epoch [25/120    avg_loss:0.183, val_acc:0.960]
Epoch [26/120    avg_loss:0.111, val_acc:0.965]
Epoch [27/120    avg_loss:0.109, val_acc:0.975]
Epoch [28/120    avg_loss:0.094, val_acc:0.960]
Epoch [29/120    avg_loss:0.105, val_acc:0.973]
Epoch [30/120    avg_loss:0.113, val_acc:0.975]
Epoch [31/120    avg_loss:0.123, val_acc:0.971]
Epoch [32/120    avg_loss:0.099, val_acc:0.973]
Epoch [33/120    avg_loss:0.096, val_acc:0.979]
Epoch [34/120    avg_loss:0.063, val_acc:0.969]
Epoch [35/120    avg_loss:0.128, val_acc:0.965]
Epoch [36/120    avg_loss:0.190, val_acc:0.950]
Epoch [37/120    avg_loss:0.165, val_acc:0.923]
Epoch [38/120    avg_loss:0.214, val_acc:0.975]
Epoch [39/120    avg_loss:0.109, val_acc:0.965]
Epoch [40/120    avg_loss:0.093, val_acc:0.965]
Epoch [41/120    avg_loss:0.158, val_acc:0.969]
Epoch [42/120    avg_loss:0.085, val_acc:0.971]
Epoch [43/120    avg_loss:0.093, val_acc:0.960]
Epoch [44/120    avg_loss:0.071, val_acc:0.983]
Epoch [45/120    avg_loss:0.061, val_acc:0.981]
Epoch [46/120    avg_loss:0.066, val_acc:0.983]
Epoch [47/120    avg_loss:0.063, val_acc:0.975]
Epoch [48/120    avg_loss:0.043, val_acc:0.975]
Epoch [49/120    avg_loss:0.083, val_acc:0.985]
Epoch [50/120    avg_loss:0.095, val_acc:0.981]
Epoch [51/120    avg_loss:0.065, val_acc:0.985]
Epoch [52/120    avg_loss:0.044, val_acc:0.985]
Epoch [53/120    avg_loss:0.045, val_acc:0.988]
Epoch [54/120    avg_loss:0.027, val_acc:0.990]
Epoch [55/120    avg_loss:0.015, val_acc:0.990]
Epoch [56/120    avg_loss:0.017, val_acc:0.983]
Epoch [57/120    avg_loss:0.024, val_acc:0.985]
Epoch [58/120    avg_loss:0.016, val_acc:0.985]
Epoch [59/120    avg_loss:0.014, val_acc:0.988]
Epoch [60/120    avg_loss:0.035, val_acc:0.981]
Epoch [61/120    avg_loss:0.061, val_acc:0.977]
Epoch [62/120    avg_loss:0.042, val_acc:0.971]
Epoch [63/120    avg_loss:0.045, val_acc:0.977]
Epoch [64/120    avg_loss:0.044, val_acc:0.979]
Epoch [65/120    avg_loss:0.032, val_acc:0.985]
Epoch [66/120    avg_loss:0.026, val_acc:0.985]
Epoch [67/120    avg_loss:0.022, val_acc:0.975]
Epoch [68/120    avg_loss:0.040, val_acc:0.990]
Epoch [69/120    avg_loss:0.025, val_acc:0.992]
Epoch [70/120    avg_loss:0.020, val_acc:0.990]
Epoch [71/120    avg_loss:0.017, val_acc:0.981]
Epoch [72/120    avg_loss:0.015, val_acc:0.990]
Epoch [73/120    avg_loss:0.051, val_acc:0.981]
Epoch [74/120    avg_loss:0.019, val_acc:0.992]
Epoch [75/120    avg_loss:0.010, val_acc:0.985]
Epoch [76/120    avg_loss:0.017, val_acc:0.992]
Epoch [77/120    avg_loss:0.049, val_acc:0.952]
Epoch [78/120    avg_loss:0.032, val_acc:0.981]
Epoch [79/120    avg_loss:0.018, val_acc:0.985]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.008, val_acc:0.985]
Epoch [83/120    avg_loss:0.027, val_acc:0.983]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.017, val_acc:0.981]
Epoch [87/120    avg_loss:0.020, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.990]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.014, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.990]
Epoch [101/120    avg_loss:0.012, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.009, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         1.         1.         0.97356828 0.95862069
 1.         1.         1.         1.         1.         0.99210526
 0.99333333 1.        ]

Kappa:
0.9957272324299515
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c80f25748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.078, val_acc:0.654]
Epoch [2/120    avg_loss:1.223, val_acc:0.723]
Epoch [3/120    avg_loss:0.911, val_acc:0.725]
Epoch [4/120    avg_loss:0.835, val_acc:0.765]
Epoch [5/120    avg_loss:0.645, val_acc:0.781]
Epoch [6/120    avg_loss:0.567, val_acc:0.858]
Epoch [7/120    avg_loss:0.546, val_acc:0.829]
Epoch [8/120    avg_loss:0.546, val_acc:0.908]
Epoch [9/120    avg_loss:0.368, val_acc:0.898]
Epoch [10/120    avg_loss:0.343, val_acc:0.885]
Epoch [11/120    avg_loss:0.317, val_acc:0.927]
Epoch [12/120    avg_loss:0.256, val_acc:0.950]
Epoch [13/120    avg_loss:0.370, val_acc:0.919]
Epoch [14/120    avg_loss:0.353, val_acc:0.915]
Epoch [15/120    avg_loss:0.218, val_acc:0.948]
Epoch [16/120    avg_loss:0.203, val_acc:0.952]
Epoch [17/120    avg_loss:0.191, val_acc:0.965]
Epoch [18/120    avg_loss:0.215, val_acc:0.935]
Epoch [19/120    avg_loss:0.182, val_acc:0.971]
Epoch [20/120    avg_loss:0.184, val_acc:0.965]
Epoch [21/120    avg_loss:0.117, val_acc:0.960]
Epoch [22/120    avg_loss:0.206, val_acc:0.973]
Epoch [23/120    avg_loss:0.134, val_acc:0.965]
Epoch [24/120    avg_loss:0.115, val_acc:0.979]
Epoch [25/120    avg_loss:0.126, val_acc:0.954]
Epoch [26/120    avg_loss:0.098, val_acc:0.967]
Epoch [27/120    avg_loss:0.135, val_acc:0.948]
Epoch [28/120    avg_loss:0.134, val_acc:0.956]
Epoch [29/120    avg_loss:0.145, val_acc:0.965]
Epoch [30/120    avg_loss:0.101, val_acc:0.967]
Epoch [31/120    avg_loss:0.095, val_acc:0.969]
Epoch [32/120    avg_loss:0.090, val_acc:0.977]
Epoch [33/120    avg_loss:0.079, val_acc:0.975]
Epoch [34/120    avg_loss:0.128, val_acc:0.975]
Epoch [35/120    avg_loss:0.092, val_acc:0.963]
Epoch [36/120    avg_loss:0.057, val_acc:0.985]
Epoch [37/120    avg_loss:0.112, val_acc:0.981]
Epoch [38/120    avg_loss:0.124, val_acc:0.952]
Epoch [39/120    avg_loss:0.135, val_acc:0.960]
Epoch [40/120    avg_loss:0.080, val_acc:0.977]
Epoch [41/120    avg_loss:0.046, val_acc:0.973]
Epoch [42/120    avg_loss:0.120, val_acc:0.977]
Epoch [43/120    avg_loss:0.113, val_acc:0.973]
Epoch [44/120    avg_loss:0.084, val_acc:0.981]
Epoch [45/120    avg_loss:0.057, val_acc:0.988]
Epoch [46/120    avg_loss:0.075, val_acc:0.979]
Epoch [47/120    avg_loss:0.079, val_acc:0.985]
Epoch [48/120    avg_loss:0.057, val_acc:0.983]
Epoch [49/120    avg_loss:0.043, val_acc:0.992]
Epoch [50/120    avg_loss:0.031, val_acc:0.988]
Epoch [51/120    avg_loss:0.033, val_acc:0.990]
Epoch [52/120    avg_loss:0.058, val_acc:0.990]
Epoch [53/120    avg_loss:0.034, val_acc:0.990]
Epoch [54/120    avg_loss:0.032, val_acc:0.988]
Epoch [55/120    avg_loss:0.044, val_acc:0.981]
Epoch [56/120    avg_loss:0.062, val_acc:0.996]
Epoch [57/120    avg_loss:0.049, val_acc:0.979]
Epoch [58/120    avg_loss:0.036, val_acc:0.990]
Epoch [59/120    avg_loss:0.021, val_acc:0.992]
Epoch [60/120    avg_loss:0.017, val_acc:0.996]
Epoch [61/120    avg_loss:0.038, val_acc:0.990]
Epoch [62/120    avg_loss:0.037, val_acc:0.983]
Epoch [63/120    avg_loss:0.084, val_acc:0.971]
Epoch [64/120    avg_loss:0.073, val_acc:0.975]
Epoch [65/120    avg_loss:0.019, val_acc:0.994]
Epoch [66/120    avg_loss:0.022, val_acc:0.994]
Epoch [67/120    avg_loss:0.025, val_acc:0.990]
Epoch [68/120    avg_loss:0.034, val_acc:0.988]
Epoch [69/120    avg_loss:0.035, val_acc:0.994]
Epoch [70/120    avg_loss:0.033, val_acc:0.994]
Epoch [71/120    avg_loss:0.059, val_acc:0.988]
Epoch [72/120    avg_loss:0.021, val_acc:0.988]
Epoch [73/120    avg_loss:0.028, val_acc:0.983]
Epoch [74/120    avg_loss:0.017, val_acc:0.981]
Epoch [75/120    avg_loss:0.024, val_acc:0.988]
Epoch [76/120    avg_loss:0.017, val_acc:0.988]
Epoch [77/120    avg_loss:0.017, val_acc:0.988]
Epoch [78/120    avg_loss:0.011, val_acc:0.990]
Epoch [79/120    avg_loss:0.019, val_acc:0.992]
Epoch [80/120    avg_loss:0.024, val_acc:0.990]
Epoch [81/120    avg_loss:0.016, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.992]
Epoch [84/120    avg_loss:0.011, val_acc:0.992]
Epoch [85/120    avg_loss:0.010, val_acc:0.992]
Epoch [86/120    avg_loss:0.013, val_acc:0.992]
Epoch [87/120    avg_loss:0.015, val_acc:0.992]
Epoch [88/120    avg_loss:0.016, val_acc:0.992]
Epoch [89/120    avg_loss:0.014, val_acc:0.992]
Epoch [90/120    avg_loss:0.014, val_acc:0.992]
Epoch [91/120    avg_loss:0.018, val_acc:0.992]
Epoch [92/120    avg_loss:0.019, val_acc:0.992]
Epoch [93/120    avg_loss:0.011, val_acc:0.992]
Epoch [94/120    avg_loss:0.009, val_acc:0.992]
Epoch [95/120    avg_loss:0.017, val_acc:0.992]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.009, val_acc:0.992]
Epoch [98/120    avg_loss:0.008, val_acc:0.992]
Epoch [99/120    avg_loss:0.010, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.015, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.992]
Epoch [104/120    avg_loss:0.008, val_acc:0.992]
Epoch [105/120    avg_loss:0.014, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.992]
Epoch [110/120    avg_loss:0.012, val_acc:0.992]
Epoch [111/120    avg_loss:0.008, val_acc:0.992]
Epoch [112/120    avg_loss:0.014, val_acc:0.992]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.016, val_acc:0.992]
Epoch [117/120    avg_loss:0.009, val_acc:0.992]
Epoch [118/120    avg_loss:0.019, val_acc:0.992]
Epoch [119/120    avg_loss:0.028, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 0.99926954 1.         1.         0.95842451 0.94366197
 1.         1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9952521454936989
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd18205c748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.100, val_acc:0.529]
Epoch [2/120    avg_loss:1.360, val_acc:0.777]
Epoch [3/120    avg_loss:1.018, val_acc:0.696]
Epoch [4/120    avg_loss:0.820, val_acc:0.810]
Epoch [5/120    avg_loss:0.694, val_acc:0.835]
Epoch [6/120    avg_loss:0.606, val_acc:0.856]
Epoch [7/120    avg_loss:0.532, val_acc:0.873]
Epoch [8/120    avg_loss:0.470, val_acc:0.885]
Epoch [9/120    avg_loss:0.338, val_acc:0.915]
Epoch [10/120    avg_loss:0.413, val_acc:0.900]
Epoch [11/120    avg_loss:0.339, val_acc:0.908]
Epoch [12/120    avg_loss:0.307, val_acc:0.854]
Epoch [13/120    avg_loss:0.273, val_acc:0.906]
Epoch [14/120    avg_loss:0.368, val_acc:0.917]
Epoch [15/120    avg_loss:0.284, val_acc:0.910]
Epoch [16/120    avg_loss:0.239, val_acc:0.917]
Epoch [17/120    avg_loss:0.363, val_acc:0.904]
Epoch [18/120    avg_loss:0.300, val_acc:0.919]
Epoch [19/120    avg_loss:0.204, val_acc:0.942]
Epoch [20/120    avg_loss:0.184, val_acc:0.954]
Epoch [21/120    avg_loss:0.158, val_acc:0.952]
Epoch [22/120    avg_loss:0.206, val_acc:0.944]
Epoch [23/120    avg_loss:0.188, val_acc:0.944]
Epoch [24/120    avg_loss:0.158, val_acc:0.929]
Epoch [25/120    avg_loss:0.137, val_acc:0.938]
Epoch [26/120    avg_loss:0.179, val_acc:0.946]
Epoch [27/120    avg_loss:0.118, val_acc:0.956]
Epoch [28/120    avg_loss:0.119, val_acc:0.965]
Epoch [29/120    avg_loss:0.155, val_acc:0.944]
Epoch [30/120    avg_loss:0.137, val_acc:0.952]
Epoch [31/120    avg_loss:0.090, val_acc:0.975]
Epoch [32/120    avg_loss:0.078, val_acc:0.979]
Epoch [33/120    avg_loss:0.064, val_acc:0.977]
Epoch [34/120    avg_loss:0.082, val_acc:0.971]
Epoch [35/120    avg_loss:0.113, val_acc:0.965]
Epoch [36/120    avg_loss:0.110, val_acc:0.967]
Epoch [37/120    avg_loss:0.131, val_acc:0.954]
Epoch [38/120    avg_loss:0.063, val_acc:0.975]
Epoch [39/120    avg_loss:0.064, val_acc:0.963]
Epoch [40/120    avg_loss:0.064, val_acc:0.979]
Epoch [41/120    avg_loss:0.045, val_acc:0.985]
Epoch [42/120    avg_loss:0.041, val_acc:0.977]
Epoch [43/120    avg_loss:0.070, val_acc:0.965]
Epoch [44/120    avg_loss:0.059, val_acc:0.979]
Epoch [45/120    avg_loss:0.041, val_acc:0.985]
Epoch [46/120    avg_loss:0.032, val_acc:0.979]
Epoch [47/120    avg_loss:0.029, val_acc:0.983]
Epoch [48/120    avg_loss:0.032, val_acc:0.979]
Epoch [49/120    avg_loss:0.030, val_acc:0.983]
Epoch [50/120    avg_loss:0.096, val_acc:0.967]
Epoch [51/120    avg_loss:0.158, val_acc:0.933]
Epoch [52/120    avg_loss:0.154, val_acc:0.958]
Epoch [53/120    avg_loss:0.176, val_acc:0.969]
Epoch [54/120    avg_loss:0.078, val_acc:0.971]
Epoch [55/120    avg_loss:0.064, val_acc:0.981]
Epoch [56/120    avg_loss:0.092, val_acc:0.960]
Epoch [57/120    avg_loss:0.070, val_acc:0.973]
Epoch [58/120    avg_loss:0.033, val_acc:0.983]
Epoch [59/120    avg_loss:0.060, val_acc:0.985]
Epoch [60/120    avg_loss:0.030, val_acc:0.985]
Epoch [61/120    avg_loss:0.032, val_acc:0.985]
Epoch [62/120    avg_loss:0.017, val_acc:0.988]
Epoch [63/120    avg_loss:0.017, val_acc:0.988]
Epoch [64/120    avg_loss:0.018, val_acc:0.988]
Epoch [65/120    avg_loss:0.017, val_acc:0.988]
Epoch [66/120    avg_loss:0.013, val_acc:0.988]
Epoch [67/120    avg_loss:0.027, val_acc:0.988]
Epoch [68/120    avg_loss:0.022, val_acc:0.990]
Epoch [69/120    avg_loss:0.016, val_acc:0.990]
Epoch [70/120    avg_loss:0.022, val_acc:0.992]
Epoch [71/120    avg_loss:0.016, val_acc:0.992]
Epoch [72/120    avg_loss:0.016, val_acc:0.992]
Epoch [73/120    avg_loss:0.013, val_acc:0.992]
Epoch [74/120    avg_loss:0.018, val_acc:0.992]
Epoch [75/120    avg_loss:0.022, val_acc:0.992]
Epoch [76/120    avg_loss:0.016, val_acc:0.992]
Epoch [77/120    avg_loss:0.019, val_acc:0.994]
Epoch [78/120    avg_loss:0.013, val_acc:0.994]
Epoch [79/120    avg_loss:0.018, val_acc:0.994]
Epoch [80/120    avg_loss:0.024, val_acc:0.992]
Epoch [81/120    avg_loss:0.016, val_acc:0.990]
Epoch [82/120    avg_loss:0.013, val_acc:0.992]
Epoch [83/120    avg_loss:0.010, val_acc:0.992]
Epoch [84/120    avg_loss:0.021, val_acc:0.992]
Epoch [85/120    avg_loss:0.026, val_acc:0.988]
Epoch [86/120    avg_loss:0.015, val_acc:0.988]
Epoch [87/120    avg_loss:0.013, val_acc:0.992]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.018, val_acc:0.992]
Epoch [90/120    avg_loss:0.012, val_acc:0.992]
Epoch [91/120    avg_loss:0.011, val_acc:0.992]
Epoch [92/120    avg_loss:0.014, val_acc:0.992]
Epoch [93/120    avg_loss:0.020, val_acc:0.992]
Epoch [94/120    avg_loss:0.018, val_acc:0.992]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.013, val_acc:0.992]
Epoch [97/120    avg_loss:0.019, val_acc:0.992]
Epoch [98/120    avg_loss:0.017, val_acc:0.992]
Epoch [99/120    avg_loss:0.017, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.013, val_acc:0.992]
Epoch [102/120    avg_loss:0.021, val_acc:0.992]
Epoch [103/120    avg_loss:0.013, val_acc:0.992]
Epoch [104/120    avg_loss:0.015, val_acc:0.992]
Epoch [105/120    avg_loss:0.012, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.018, val_acc:0.992]
Epoch [108/120    avg_loss:0.013, val_acc:0.992]
Epoch [109/120    avg_loss:0.017, val_acc:0.992]
Epoch [110/120    avg_loss:0.013, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.020, val_acc:0.992]
Epoch [113/120    avg_loss:0.011, val_acc:0.992]
Epoch [114/120    avg_loss:0.014, val_acc:0.992]
Epoch [115/120    avg_loss:0.013, val_acc:0.992]
Epoch [116/120    avg_loss:0.016, val_acc:0.992]
Epoch [117/120    avg_loss:0.017, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.016, val_acc:0.992]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   2   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  17 436   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 0.99853801 0.9977221  1.         0.95860566 0.93006993
 1.         0.99465241 1.         1.         1.         0.97795071
 0.97977528 1.        ]

Kappa:
0.9907428396692322
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d7e92a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.097, val_acc:0.667]
Epoch [2/120    avg_loss:1.327, val_acc:0.710]
Epoch [3/120    avg_loss:0.978, val_acc:0.742]
Epoch [4/120    avg_loss:0.790, val_acc:0.796]
Epoch [5/120    avg_loss:0.683, val_acc:0.802]
Epoch [6/120    avg_loss:0.620, val_acc:0.802]
Epoch [7/120    avg_loss:0.576, val_acc:0.890]
Epoch [8/120    avg_loss:0.523, val_acc:0.877]
Epoch [9/120    avg_loss:0.527, val_acc:0.865]
Epoch [10/120    avg_loss:0.491, val_acc:0.925]
Epoch [11/120    avg_loss:0.368, val_acc:0.910]
Epoch [12/120    avg_loss:0.438, val_acc:0.896]
Epoch [13/120    avg_loss:0.343, val_acc:0.892]
Epoch [14/120    avg_loss:0.350, val_acc:0.954]
Epoch [15/120    avg_loss:0.266, val_acc:0.946]
Epoch [16/120    avg_loss:0.265, val_acc:0.942]
Epoch [17/120    avg_loss:0.349, val_acc:0.921]
Epoch [18/120    avg_loss:0.401, val_acc:0.910]
Epoch [19/120    avg_loss:0.214, val_acc:0.958]
Epoch [20/120    avg_loss:0.198, val_acc:0.938]
Epoch [21/120    avg_loss:0.180, val_acc:0.958]
Epoch [22/120    avg_loss:0.191, val_acc:0.971]
Epoch [23/120    avg_loss:0.140, val_acc:0.965]
Epoch [24/120    avg_loss:0.134, val_acc:0.983]
Epoch [25/120    avg_loss:0.146, val_acc:0.973]
Epoch [26/120    avg_loss:0.181, val_acc:0.940]
Epoch [27/120    avg_loss:0.267, val_acc:0.948]
Epoch [28/120    avg_loss:0.133, val_acc:0.975]
Epoch [29/120    avg_loss:0.141, val_acc:0.973]
Epoch [30/120    avg_loss:0.123, val_acc:0.940]
Epoch [31/120    avg_loss:0.162, val_acc:0.958]
Epoch [32/120    avg_loss:0.114, val_acc:0.973]
Epoch [33/120    avg_loss:0.115, val_acc:0.954]
Epoch [34/120    avg_loss:0.119, val_acc:0.969]
Epoch [35/120    avg_loss:0.110, val_acc:0.977]
Epoch [36/120    avg_loss:0.116, val_acc:0.950]
Epoch [37/120    avg_loss:0.174, val_acc:0.973]
Epoch [38/120    avg_loss:0.086, val_acc:0.983]
Epoch [39/120    avg_loss:0.076, val_acc:0.988]
Epoch [40/120    avg_loss:0.060, val_acc:0.990]
Epoch [41/120    avg_loss:0.046, val_acc:0.990]
Epoch [42/120    avg_loss:0.069, val_acc:0.990]
Epoch [43/120    avg_loss:0.056, val_acc:0.990]
Epoch [44/120    avg_loss:0.053, val_acc:0.992]
Epoch [45/120    avg_loss:0.054, val_acc:0.990]
Epoch [46/120    avg_loss:0.042, val_acc:0.992]
Epoch [47/120    avg_loss:0.058, val_acc:0.990]
Epoch [48/120    avg_loss:0.046, val_acc:0.988]
Epoch [49/120    avg_loss:0.040, val_acc:0.990]
Epoch [50/120    avg_loss:0.046, val_acc:0.992]
Epoch [51/120    avg_loss:0.044, val_acc:0.992]
Epoch [52/120    avg_loss:0.047, val_acc:0.994]
Epoch [53/120    avg_loss:0.041, val_acc:0.994]
Epoch [54/120    avg_loss:0.040, val_acc:0.988]
Epoch [55/120    avg_loss:0.051, val_acc:0.990]
Epoch [56/120    avg_loss:0.049, val_acc:0.990]
Epoch [57/120    avg_loss:0.053, val_acc:0.990]
Epoch [58/120    avg_loss:0.048, val_acc:0.988]
Epoch [59/120    avg_loss:0.052, val_acc:0.988]
Epoch [60/120    avg_loss:0.042, val_acc:0.988]
Epoch [61/120    avg_loss:0.040, val_acc:0.990]
Epoch [62/120    avg_loss:0.040, val_acc:0.992]
Epoch [63/120    avg_loss:0.050, val_acc:0.990]
Epoch [64/120    avg_loss:0.042, val_acc:0.990]
Epoch [65/120    avg_loss:0.046, val_acc:0.990]
Epoch [66/120    avg_loss:0.043, val_acc:0.990]
Epoch [67/120    avg_loss:0.051, val_acc:0.992]
Epoch [68/120    avg_loss:0.045, val_acc:0.992]
Epoch [69/120    avg_loss:0.030, val_acc:0.992]
Epoch [70/120    avg_loss:0.042, val_acc:0.992]
Epoch [71/120    avg_loss:0.029, val_acc:0.992]
Epoch [72/120    avg_loss:0.028, val_acc:0.992]
Epoch [73/120    avg_loss:0.041, val_acc:0.992]
Epoch [74/120    avg_loss:0.027, val_acc:0.992]
Epoch [75/120    avg_loss:0.033, val_acc:0.992]
Epoch [76/120    avg_loss:0.047, val_acc:0.992]
Epoch [77/120    avg_loss:0.037, val_acc:0.992]
Epoch [78/120    avg_loss:0.037, val_acc:0.992]
Epoch [79/120    avg_loss:0.042, val_acc:0.992]
Epoch [80/120    avg_loss:0.035, val_acc:0.992]
Epoch [81/120    avg_loss:0.030, val_acc:0.992]
Epoch [82/120    avg_loss:0.032, val_acc:0.992]
Epoch [83/120    avg_loss:0.053, val_acc:0.992]
Epoch [84/120    avg_loss:0.045, val_acc:0.992]
Epoch [85/120    avg_loss:0.032, val_acc:0.992]
Epoch [86/120    avg_loss:0.048, val_acc:0.992]
Epoch [87/120    avg_loss:0.034, val_acc:0.992]
Epoch [88/120    avg_loss:0.033, val_acc:0.992]
Epoch [89/120    avg_loss:0.036, val_acc:0.992]
Epoch [90/120    avg_loss:0.029, val_acc:0.992]
Epoch [91/120    avg_loss:0.035, val_acc:0.992]
Epoch [92/120    avg_loss:0.042, val_acc:0.992]
Epoch [93/120    avg_loss:0.031, val_acc:0.992]
Epoch [94/120    avg_loss:0.033, val_acc:0.992]
Epoch [95/120    avg_loss:0.030, val_acc:0.992]
Epoch [96/120    avg_loss:0.037, val_acc:0.992]
Epoch [97/120    avg_loss:0.041, val_acc:0.992]
Epoch [98/120    avg_loss:0.051, val_acc:0.992]
Epoch [99/120    avg_loss:0.025, val_acc:0.992]
Epoch [100/120    avg_loss:0.031, val_acc:0.992]
Epoch [101/120    avg_loss:0.028, val_acc:0.992]
Epoch [102/120    avg_loss:0.033, val_acc:0.992]
Epoch [103/120    avg_loss:0.028, val_acc:0.992]
Epoch [104/120    avg_loss:0.030, val_acc:0.992]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.034, val_acc:0.992]
Epoch [107/120    avg_loss:0.039, val_acc:0.992]
Epoch [108/120    avg_loss:0.038, val_acc:0.992]
Epoch [109/120    avg_loss:0.024, val_acc:0.992]
Epoch [110/120    avg_loss:0.036, val_acc:0.992]
Epoch [111/120    avg_loss:0.030, val_acc:0.992]
Epoch [112/120    avg_loss:0.042, val_acc:0.992]
Epoch [113/120    avg_loss:0.030, val_acc:0.992]
Epoch [114/120    avg_loss:0.037, val_acc:0.992]
Epoch [115/120    avg_loss:0.042, val_acc:0.992]
Epoch [116/120    avg_loss:0.027, val_acc:0.992]
Epoch [117/120    avg_loss:0.036, val_acc:0.992]
Epoch [118/120    avg_loss:0.031, val_acc:0.992]
Epoch [119/120    avg_loss:0.028, val_acc:0.992]
Epoch [120/120    avg_loss:0.038, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214   9   0   0   0   0   0   0   4   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 0.99926954 1.         1.         0.95111111 0.9347079
 1.         1.         1.         1.         1.         0.9986755
 0.99449945 1.        ]

Kappa:
0.9943027620367565
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2c1242710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.103, val_acc:0.592]
Epoch [2/120    avg_loss:1.356, val_acc:0.735]
Epoch [3/120    avg_loss:0.922, val_acc:0.760]
Epoch [4/120    avg_loss:0.806, val_acc:0.815]
Epoch [5/120    avg_loss:0.647, val_acc:0.838]
Epoch [6/120    avg_loss:0.547, val_acc:0.835]
Epoch [7/120    avg_loss:0.503, val_acc:0.871]
Epoch [8/120    avg_loss:0.553, val_acc:0.890]
Epoch [9/120    avg_loss:0.410, val_acc:0.902]
Epoch [10/120    avg_loss:0.407, val_acc:0.927]
Epoch [11/120    avg_loss:0.398, val_acc:0.917]
Epoch [12/120    avg_loss:0.409, val_acc:0.890]
Epoch [13/120    avg_loss:0.333, val_acc:0.929]
Epoch [14/120    avg_loss:0.278, val_acc:0.906]
Epoch [15/120    avg_loss:0.238, val_acc:0.967]
Epoch [16/120    avg_loss:0.245, val_acc:0.969]
Epoch [17/120    avg_loss:0.234, val_acc:0.969]
Epoch [18/120    avg_loss:0.201, val_acc:0.944]
Epoch [19/120    avg_loss:0.248, val_acc:0.954]
Epoch [20/120    avg_loss:0.216, val_acc:0.946]
Epoch [21/120    avg_loss:0.280, val_acc:0.967]
Epoch [22/120    avg_loss:0.222, val_acc:0.963]
Epoch [23/120    avg_loss:0.172, val_acc:0.919]
Epoch [24/120    avg_loss:0.161, val_acc:0.975]
Epoch [25/120    avg_loss:0.160, val_acc:0.969]
Epoch [26/120    avg_loss:0.144, val_acc:0.958]
Epoch [27/120    avg_loss:0.104, val_acc:0.973]
Epoch [28/120    avg_loss:0.120, val_acc:0.979]
Epoch [29/120    avg_loss:0.107, val_acc:0.960]
Epoch [30/120    avg_loss:0.163, val_acc:0.977]
Epoch [31/120    avg_loss:0.111, val_acc:0.979]
Epoch [32/120    avg_loss:0.082, val_acc:0.965]
Epoch [33/120    avg_loss:0.216, val_acc:0.948]
Epoch [34/120    avg_loss:0.169, val_acc:0.973]
Epoch [35/120    avg_loss:0.129, val_acc:0.983]
Epoch [36/120    avg_loss:0.076, val_acc:0.979]
Epoch [37/120    avg_loss:0.078, val_acc:0.985]
Epoch [38/120    avg_loss:0.055, val_acc:0.977]
Epoch [39/120    avg_loss:0.120, val_acc:0.981]
Epoch [40/120    avg_loss:0.128, val_acc:0.983]
Epoch [41/120    avg_loss:0.080, val_acc:0.981]
Epoch [42/120    avg_loss:0.050, val_acc:0.990]
Epoch [43/120    avg_loss:0.065, val_acc:0.975]
Epoch [44/120    avg_loss:0.098, val_acc:0.981]
Epoch [45/120    avg_loss:0.106, val_acc:0.977]
Epoch [46/120    avg_loss:0.069, val_acc:0.985]
Epoch [47/120    avg_loss:0.051, val_acc:0.983]
Epoch [48/120    avg_loss:0.048, val_acc:0.985]
Epoch [49/120    avg_loss:0.040, val_acc:0.983]
Epoch [50/120    avg_loss:0.078, val_acc:0.975]
Epoch [51/120    avg_loss:0.030, val_acc:0.985]
Epoch [52/120    avg_loss:0.040, val_acc:0.990]
Epoch [53/120    avg_loss:0.086, val_acc:0.971]
Epoch [54/120    avg_loss:0.094, val_acc:0.975]
Epoch [55/120    avg_loss:0.081, val_acc:0.981]
Epoch [56/120    avg_loss:0.070, val_acc:0.977]
Epoch [57/120    avg_loss:0.051, val_acc:0.988]
Epoch [58/120    avg_loss:0.027, val_acc:0.983]
Epoch [59/120    avg_loss:0.026, val_acc:0.994]
Epoch [60/120    avg_loss:0.028, val_acc:0.981]
Epoch [61/120    avg_loss:0.037, val_acc:0.988]
Epoch [62/120    avg_loss:0.031, val_acc:0.988]
Epoch [63/120    avg_loss:0.037, val_acc:0.985]
Epoch [64/120    avg_loss:0.032, val_acc:0.983]
Epoch [65/120    avg_loss:0.029, val_acc:0.990]
Epoch [66/120    avg_loss:0.018, val_acc:0.988]
Epoch [67/120    avg_loss:0.018, val_acc:0.992]
Epoch [68/120    avg_loss:0.040, val_acc:0.985]
Epoch [69/120    avg_loss:0.028, val_acc:0.988]
Epoch [70/120    avg_loss:0.046, val_acc:0.985]
Epoch [71/120    avg_loss:0.043, val_acc:0.981]
Epoch [72/120    avg_loss:0.084, val_acc:0.948]
Epoch [73/120    avg_loss:0.037, val_acc:0.971]
Epoch [74/120    avg_loss:0.022, val_acc:0.981]
Epoch [75/120    avg_loss:0.031, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.985]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.026, val_acc:0.990]
Epoch [79/120    avg_loss:0.023, val_acc:0.992]
Epoch [80/120    avg_loss:0.010, val_acc:0.990]
Epoch [81/120    avg_loss:0.029, val_acc:0.992]
Epoch [82/120    avg_loss:0.013, val_acc:0.994]
Epoch [83/120    avg_loss:0.011, val_acc:0.994]
Epoch [84/120    avg_loss:0.019, val_acc:0.994]
Epoch [85/120    avg_loss:0.012, val_acc:0.994]
Epoch [86/120    avg_loss:0.025, val_acc:0.994]
Epoch [87/120    avg_loss:0.011, val_acc:0.994]
Epoch [88/120    avg_loss:0.016, val_acc:0.994]
Epoch [89/120    avg_loss:0.017, val_acc:0.994]
Epoch [90/120    avg_loss:0.011, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.994]
Epoch [92/120    avg_loss:0.014, val_acc:0.994]
Epoch [93/120    avg_loss:0.013, val_acc:0.994]
Epoch [94/120    avg_loss:0.011, val_acc:0.990]
Epoch [95/120    avg_loss:0.018, val_acc:0.992]
Epoch [96/120    avg_loss:0.010, val_acc:0.992]
Epoch [97/120    avg_loss:0.035, val_acc:0.994]
Epoch [98/120    avg_loss:0.019, val_acc:0.994]
Epoch [99/120    avg_loss:0.015, val_acc:0.994]
Epoch [100/120    avg_loss:0.023, val_acc:0.994]
Epoch [101/120    avg_loss:0.014, val_acc:0.992]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.992]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.010, val_acc:0.994]
Epoch [106/120    avg_loss:0.021, val_acc:0.994]
Epoch [107/120    avg_loss:0.012, val_acc:0.994]
Epoch [108/120    avg_loss:0.016, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 0.99926954 1.         1.         0.96760259 0.94326241
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.996201858659742
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8b5c578710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.174, val_acc:0.602]
Epoch [2/120    avg_loss:1.398, val_acc:0.696]
Epoch [3/120    avg_loss:1.007, val_acc:0.767]
Epoch [4/120    avg_loss:0.899, val_acc:0.787]
Epoch [5/120    avg_loss:0.789, val_acc:0.838]
Epoch [6/120    avg_loss:0.621, val_acc:0.833]
Epoch [7/120    avg_loss:0.734, val_acc:0.821]
Epoch [8/120    avg_loss:0.669, val_acc:0.804]
Epoch [9/120    avg_loss:0.481, val_acc:0.875]
Epoch [10/120    avg_loss:0.479, val_acc:0.875]
Epoch [11/120    avg_loss:0.468, val_acc:0.904]
Epoch [12/120    avg_loss:0.330, val_acc:0.912]
Epoch [13/120    avg_loss:0.453, val_acc:0.844]
Epoch [14/120    avg_loss:0.311, val_acc:0.963]
Epoch [15/120    avg_loss:0.333, val_acc:0.944]
Epoch [16/120    avg_loss:0.341, val_acc:0.944]
Epoch [17/120    avg_loss:0.286, val_acc:0.938]
Epoch [18/120    avg_loss:0.236, val_acc:0.952]
Epoch [19/120    avg_loss:0.199, val_acc:0.950]
Epoch [20/120    avg_loss:0.230, val_acc:0.933]
Epoch [21/120    avg_loss:0.172, val_acc:0.958]
Epoch [22/120    avg_loss:0.161, val_acc:0.948]
Epoch [23/120    avg_loss:0.159, val_acc:0.977]
Epoch [24/120    avg_loss:0.152, val_acc:0.950]
Epoch [25/120    avg_loss:0.180, val_acc:0.960]
Epoch [26/120    avg_loss:0.162, val_acc:0.985]
Epoch [27/120    avg_loss:0.122, val_acc:0.992]
Epoch [28/120    avg_loss:0.179, val_acc:0.954]
Epoch [29/120    avg_loss:0.170, val_acc:0.969]
Epoch [30/120    avg_loss:0.094, val_acc:0.985]
Epoch [31/120    avg_loss:0.103, val_acc:0.990]
Epoch [32/120    avg_loss:0.113, val_acc:0.975]
Epoch [33/120    avg_loss:0.136, val_acc:0.973]
Epoch [34/120    avg_loss:0.097, val_acc:0.969]
Epoch [35/120    avg_loss:0.180, val_acc:0.988]
Epoch [36/120    avg_loss:0.100, val_acc:0.977]
Epoch [37/120    avg_loss:0.125, val_acc:0.975]
Epoch [38/120    avg_loss:0.101, val_acc:0.992]
Epoch [39/120    avg_loss:0.128, val_acc:0.979]
Epoch [40/120    avg_loss:0.131, val_acc:0.969]
Epoch [41/120    avg_loss:0.116, val_acc:0.954]
Epoch [42/120    avg_loss:0.063, val_acc:0.994]
Epoch [43/120    avg_loss:0.055, val_acc:0.996]
Epoch [44/120    avg_loss:0.039, val_acc:0.985]
Epoch [45/120    avg_loss:0.094, val_acc:0.981]
Epoch [46/120    avg_loss:0.106, val_acc:0.985]
Epoch [47/120    avg_loss:0.051, val_acc:0.990]
Epoch [48/120    avg_loss:0.051, val_acc:0.979]
Epoch [49/120    avg_loss:0.034, val_acc:0.994]
Epoch [50/120    avg_loss:0.034, val_acc:0.992]
Epoch [51/120    avg_loss:0.020, val_acc:0.994]
Epoch [52/120    avg_loss:0.027, val_acc:0.992]
Epoch [53/120    avg_loss:0.042, val_acc:0.990]
Epoch [54/120    avg_loss:0.032, val_acc:0.994]
Epoch [55/120    avg_loss:0.035, val_acc:0.996]
Epoch [56/120    avg_loss:0.022, val_acc:0.990]
Epoch [57/120    avg_loss:0.020, val_acc:0.992]
Epoch [58/120    avg_loss:0.020, val_acc:0.994]
Epoch [59/120    avg_loss:0.082, val_acc:0.981]
Epoch [60/120    avg_loss:0.066, val_acc:0.990]
Epoch [61/120    avg_loss:0.105, val_acc:0.992]
Epoch [62/120    avg_loss:0.066, val_acc:0.990]
Epoch [63/120    avg_loss:0.027, val_acc:0.994]
Epoch [64/120    avg_loss:0.027, val_acc:0.996]
Epoch [65/120    avg_loss:0.028, val_acc:0.996]
Epoch [66/120    avg_loss:0.024, val_acc:0.998]
Epoch [67/120    avg_loss:0.012, val_acc:0.994]
Epoch [68/120    avg_loss:0.018, val_acc:0.992]
Epoch [69/120    avg_loss:0.017, val_acc:0.990]
Epoch [70/120    avg_loss:0.030, val_acc:1.000]
Epoch [71/120    avg_loss:0.016, val_acc:0.994]
Epoch [72/120    avg_loss:0.013, val_acc:0.994]
Epoch [73/120    avg_loss:0.033, val_acc:0.994]
Epoch [74/120    avg_loss:0.023, val_acc:0.988]
Epoch [75/120    avg_loss:0.016, val_acc:0.988]
Epoch [76/120    avg_loss:0.037, val_acc:0.996]
Epoch [77/120    avg_loss:0.013, val_acc:0.996]
Epoch [78/120    avg_loss:0.013, val_acc:0.998]
Epoch [79/120    avg_loss:0.023, val_acc:0.990]
Epoch [80/120    avg_loss:0.069, val_acc:0.992]
Epoch [81/120    avg_loss:0.050, val_acc:0.985]
Epoch [82/120    avg_loss:0.049, val_acc:0.992]
Epoch [83/120    avg_loss:0.036, val_acc:0.985]
Epoch [84/120    avg_loss:0.043, val_acc:0.990]
Epoch [85/120    avg_loss:0.022, val_acc:0.994]
Epoch [86/120    avg_loss:0.016, val_acc:0.996]
Epoch [87/120    avg_loss:0.011, val_acc:0.996]
Epoch [88/120    avg_loss:0.015, val_acc:0.996]
Epoch [89/120    avg_loss:0.014, val_acc:0.996]
Epoch [90/120    avg_loss:0.009, val_acc:0.996]
Epoch [91/120    avg_loss:0.009, val_acc:0.996]
Epoch [92/120    avg_loss:0.013, val_acc:0.996]
Epoch [93/120    avg_loss:0.010, val_acc:0.996]
Epoch [94/120    avg_loss:0.005, val_acc:0.996]
Epoch [95/120    avg_loss:0.013, val_acc:0.996]
Epoch [96/120    avg_loss:0.010, val_acc:0.996]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.008, val_acc:0.996]
Epoch [99/120    avg_loss:0.010, val_acc:0.996]
Epoch [100/120    avg_loss:0.007, val_acc:0.996]
Epoch [101/120    avg_loss:0.009, val_acc:0.996]
Epoch [102/120    avg_loss:0.014, val_acc:0.996]
Epoch [103/120    avg_loss:0.013, val_acc:0.996]
Epoch [104/120    avg_loss:0.013, val_acc:0.996]
Epoch [105/120    avg_loss:0.012, val_acc:0.996]
Epoch [106/120    avg_loss:0.009, val_acc:0.996]
Epoch [107/120    avg_loss:0.010, val_acc:0.996]
Epoch [108/120    avg_loss:0.009, val_acc:0.996]
Epoch [109/120    avg_loss:0.012, val_acc:0.996]
Epoch [110/120    avg_loss:0.007, val_acc:0.996]
Epoch [111/120    avg_loss:0.007, val_acc:0.996]
Epoch [112/120    avg_loss:0.008, val_acc:0.996]
Epoch [113/120    avg_loss:0.011, val_acc:0.996]
Epoch [114/120    avg_loss:0.006, val_acc:0.996]
Epoch [115/120    avg_loss:0.009, val_acc:0.996]
Epoch [116/120    avg_loss:0.007, val_acc:0.996]
Epoch [117/120    avg_loss:0.006, val_acc:0.996]
Epoch [118/120    avg_loss:0.012, val_acc:0.996]
Epoch [119/120    avg_loss:0.015, val_acc:0.996]
Epoch [120/120    avg_loss:0.007, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   6   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   5   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 0.99560117 1.         1.         0.98660714 0.96345515
 1.         1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9971519161258529
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff62e5e07b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.124, val_acc:0.556]
Epoch [2/120    avg_loss:1.368, val_acc:0.673]
Epoch [3/120    avg_loss:1.086, val_acc:0.787]
Epoch [4/120    avg_loss:0.858, val_acc:0.723]
Epoch [5/120    avg_loss:0.830, val_acc:0.810]
Epoch [6/120    avg_loss:0.645, val_acc:0.833]
Epoch [7/120    avg_loss:0.700, val_acc:0.850]
Epoch [8/120    avg_loss:0.514, val_acc:0.869]
Epoch [9/120    avg_loss:0.542, val_acc:0.881]
Epoch [10/120    avg_loss:0.590, val_acc:0.850]
Epoch [11/120    avg_loss:0.479, val_acc:0.923]
Epoch [12/120    avg_loss:0.403, val_acc:0.912]
Epoch [13/120    avg_loss:0.370, val_acc:0.931]
Epoch [14/120    avg_loss:0.312, val_acc:0.940]
Epoch [15/120    avg_loss:0.271, val_acc:0.938]
Epoch [16/120    avg_loss:0.231, val_acc:0.929]
Epoch [17/120    avg_loss:0.287, val_acc:0.915]
Epoch [18/120    avg_loss:0.226, val_acc:0.950]
Epoch [19/120    avg_loss:0.191, val_acc:0.942]
Epoch [20/120    avg_loss:0.181, val_acc:0.950]
Epoch [21/120    avg_loss:0.150, val_acc:0.954]
Epoch [22/120    avg_loss:0.216, val_acc:0.946]
Epoch [23/120    avg_loss:0.230, val_acc:0.940]
Epoch [24/120    avg_loss:0.194, val_acc:0.956]
Epoch [25/120    avg_loss:0.165, val_acc:0.965]
Epoch [26/120    avg_loss:0.119, val_acc:0.948]
Epoch [27/120    avg_loss:0.120, val_acc:0.971]
Epoch [28/120    avg_loss:0.113, val_acc:0.967]
Epoch [29/120    avg_loss:0.074, val_acc:0.969]
Epoch [30/120    avg_loss:0.094, val_acc:0.975]
Epoch [31/120    avg_loss:0.106, val_acc:0.963]
Epoch [32/120    avg_loss:0.092, val_acc:0.977]
Epoch [33/120    avg_loss:0.101, val_acc:0.971]
Epoch [34/120    avg_loss:0.219, val_acc:0.956]
Epoch [35/120    avg_loss:0.117, val_acc:0.971]
Epoch [36/120    avg_loss:0.129, val_acc:0.965]
Epoch [37/120    avg_loss:0.161, val_acc:0.950]
Epoch [38/120    avg_loss:0.102, val_acc:0.975]
Epoch [39/120    avg_loss:0.243, val_acc:0.948]
Epoch [40/120    avg_loss:0.146, val_acc:0.971]
Epoch [41/120    avg_loss:0.104, val_acc:0.981]
Epoch [42/120    avg_loss:0.076, val_acc:0.967]
Epoch [43/120    avg_loss:0.072, val_acc:0.979]
Epoch [44/120    avg_loss:0.075, val_acc:0.979]
Epoch [45/120    avg_loss:0.075, val_acc:0.981]
Epoch [46/120    avg_loss:0.057, val_acc:0.979]
Epoch [47/120    avg_loss:0.080, val_acc:0.983]
Epoch [48/120    avg_loss:0.065, val_acc:0.983]
Epoch [49/120    avg_loss:0.066, val_acc:0.977]
Epoch [50/120    avg_loss:0.075, val_acc:0.971]
Epoch [51/120    avg_loss:0.078, val_acc:0.988]
Epoch [52/120    avg_loss:0.043, val_acc:0.981]
Epoch [53/120    avg_loss:0.030, val_acc:0.985]
Epoch [54/120    avg_loss:0.044, val_acc:0.985]
Epoch [55/120    avg_loss:0.048, val_acc:0.973]
Epoch [56/120    avg_loss:0.045, val_acc:0.990]
Epoch [57/120    avg_loss:0.033, val_acc:0.992]
Epoch [58/120    avg_loss:0.033, val_acc:0.985]
Epoch [59/120    avg_loss:0.018, val_acc:0.994]
Epoch [60/120    avg_loss:0.025, val_acc:0.983]
Epoch [61/120    avg_loss:0.047, val_acc:0.975]
Epoch [62/120    avg_loss:0.027, val_acc:0.985]
Epoch [63/120    avg_loss:0.029, val_acc:0.992]
Epoch [64/120    avg_loss:0.045, val_acc:0.979]
Epoch [65/120    avg_loss:0.051, val_acc:0.985]
Epoch [66/120    avg_loss:0.043, val_acc:0.981]
Epoch [67/120    avg_loss:0.040, val_acc:0.988]
Epoch [68/120    avg_loss:0.020, val_acc:0.994]
Epoch [69/120    avg_loss:0.016, val_acc:0.994]
Epoch [70/120    avg_loss:0.038, val_acc:0.973]
Epoch [71/120    avg_loss:0.072, val_acc:0.971]
Epoch [72/120    avg_loss:0.054, val_acc:0.981]
Epoch [73/120    avg_loss:0.022, val_acc:0.975]
Epoch [74/120    avg_loss:0.018, val_acc:0.988]
Epoch [75/120    avg_loss:0.014, val_acc:0.992]
Epoch [76/120    avg_loss:0.017, val_acc:0.996]
Epoch [77/120    avg_loss:0.028, val_acc:0.992]
Epoch [78/120    avg_loss:0.037, val_acc:0.992]
Epoch [79/120    avg_loss:0.062, val_acc:0.983]
Epoch [80/120    avg_loss:0.052, val_acc:0.985]
Epoch [81/120    avg_loss:0.082, val_acc:0.988]
Epoch [82/120    avg_loss:0.066, val_acc:0.973]
Epoch [83/120    avg_loss:0.034, val_acc:0.988]
Epoch [84/120    avg_loss:0.034, val_acc:0.994]
Epoch [85/120    avg_loss:0.022, val_acc:0.994]
Epoch [86/120    avg_loss:0.020, val_acc:0.994]
Epoch [87/120    avg_loss:0.017, val_acc:0.981]
Epoch [88/120    avg_loss:0.025, val_acc:0.983]
Epoch [89/120    avg_loss:0.067, val_acc:0.981]
Epoch [90/120    avg_loss:0.030, val_acc:0.983]
Epoch [91/120    avg_loss:0.017, val_acc:0.988]
Epoch [92/120    avg_loss:0.012, val_acc:0.990]
Epoch [93/120    avg_loss:0.017, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.992]
Epoch [96/120    avg_loss:0.014, val_acc:0.992]
Epoch [97/120    avg_loss:0.010, val_acc:0.994]
Epoch [98/120    avg_loss:0.010, val_acc:0.994]
Epoch [99/120    avg_loss:0.008, val_acc:0.994]
Epoch [100/120    avg_loss:0.010, val_acc:0.994]
Epoch [101/120    avg_loss:0.009, val_acc:0.994]
Epoch [102/120    avg_loss:0.014, val_acc:0.994]
Epoch [103/120    avg_loss:0.013, val_acc:0.994]
Epoch [104/120    avg_loss:0.008, val_acc:0.994]
Epoch [105/120    avg_loss:0.007, val_acc:0.994]
Epoch [106/120    avg_loss:0.008, val_acc:0.994]
Epoch [107/120    avg_loss:0.007, val_acc:0.994]
Epoch [108/120    avg_loss:0.010, val_acc:0.994]
Epoch [109/120    avg_loss:0.011, val_acc:0.994]
Epoch [110/120    avg_loss:0.010, val_acc:0.994]
Epoch [111/120    avg_loss:0.007, val_acc:0.994]
Epoch [112/120    avg_loss:0.006, val_acc:0.994]
Epoch [113/120    avg_loss:0.012, val_acc:0.994]
Epoch [114/120    avg_loss:0.012, val_acc:0.994]
Epoch [115/120    avg_loss:0.011, val_acc:0.994]
Epoch [116/120    avg_loss:0.007, val_acc:0.994]
Epoch [117/120    avg_loss:0.013, val_acc:0.994]
Epoch [118/120    avg_loss:0.009, val_acc:0.994]
Epoch [119/120    avg_loss:0.023, val_acc:0.994]
Epoch [120/120    avg_loss:0.011, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   4   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 0.99707174 0.99545455 0.99563319 0.9612069  0.93333333
 1.         0.98924731 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9943031195886343
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f52a33a2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.113, val_acc:0.560]
Epoch [2/120    avg_loss:1.287, val_acc:0.715]
Epoch [3/120    avg_loss:0.916, val_acc:0.833]
Epoch [4/120    avg_loss:0.706, val_acc:0.792]
Epoch [5/120    avg_loss:0.661, val_acc:0.856]
Epoch [6/120    avg_loss:0.594, val_acc:0.881]
Epoch [7/120    avg_loss:0.491, val_acc:0.919]
Epoch [8/120    avg_loss:0.378, val_acc:0.910]
Epoch [9/120    avg_loss:0.502, val_acc:0.925]
Epoch [10/120    avg_loss:0.502, val_acc:0.900]
Epoch [11/120    avg_loss:0.352, val_acc:0.925]
Epoch [12/120    avg_loss:0.261, val_acc:0.935]
Epoch [13/120    avg_loss:0.339, val_acc:0.917]
Epoch [14/120    avg_loss:0.339, val_acc:0.956]
Epoch [15/120    avg_loss:0.174, val_acc:0.950]
Epoch [16/120    avg_loss:0.272, val_acc:0.942]
Epoch [17/120    avg_loss:0.230, val_acc:0.948]
Epoch [18/120    avg_loss:0.221, val_acc:0.946]
Epoch [19/120    avg_loss:0.197, val_acc:0.969]
Epoch [20/120    avg_loss:0.122, val_acc:0.969]
Epoch [21/120    avg_loss:0.124, val_acc:0.940]
Epoch [22/120    avg_loss:0.147, val_acc:0.950]
Epoch [23/120    avg_loss:0.133, val_acc:0.971]
Epoch [24/120    avg_loss:0.123, val_acc:0.952]
Epoch [25/120    avg_loss:0.136, val_acc:0.952]
Epoch [26/120    avg_loss:0.127, val_acc:0.985]
Epoch [27/120    avg_loss:0.130, val_acc:0.954]
Epoch [28/120    avg_loss:0.209, val_acc:0.940]
Epoch [29/120    avg_loss:0.175, val_acc:0.963]
Epoch [30/120    avg_loss:0.150, val_acc:0.958]
Epoch [31/120    avg_loss:0.160, val_acc:0.983]
Epoch [32/120    avg_loss:0.087, val_acc:0.985]
Epoch [33/120    avg_loss:0.094, val_acc:0.979]
Epoch [34/120    avg_loss:0.090, val_acc:0.981]
Epoch [35/120    avg_loss:0.047, val_acc:0.983]
Epoch [36/120    avg_loss:0.049, val_acc:0.985]
Epoch [37/120    avg_loss:0.062, val_acc:0.977]
Epoch [38/120    avg_loss:0.077, val_acc:0.977]
Epoch [39/120    avg_loss:0.053, val_acc:0.988]
Epoch [40/120    avg_loss:0.098, val_acc:0.956]
Epoch [41/120    avg_loss:0.062, val_acc:0.979]
Epoch [42/120    avg_loss:0.037, val_acc:0.979]
Epoch [43/120    avg_loss:0.031, val_acc:0.983]
Epoch [44/120    avg_loss:0.042, val_acc:0.979]
Epoch [45/120    avg_loss:0.047, val_acc:0.985]
Epoch [46/120    avg_loss:0.028, val_acc:0.985]
Epoch [47/120    avg_loss:0.031, val_acc:0.985]
Epoch [48/120    avg_loss:0.063, val_acc:0.988]
Epoch [49/120    avg_loss:0.065, val_acc:0.981]
Epoch [50/120    avg_loss:0.060, val_acc:0.977]
Epoch [51/120    avg_loss:0.065, val_acc:0.967]
Epoch [52/120    avg_loss:0.120, val_acc:0.979]
Epoch [53/120    avg_loss:0.112, val_acc:0.971]
Epoch [54/120    avg_loss:0.061, val_acc:0.981]
Epoch [55/120    avg_loss:0.139, val_acc:0.963]
Epoch [56/120    avg_loss:0.093, val_acc:0.969]
Epoch [57/120    avg_loss:0.057, val_acc:0.988]
Epoch [58/120    avg_loss:0.038, val_acc:0.979]
Epoch [59/120    avg_loss:0.024, val_acc:0.992]
Epoch [60/120    avg_loss:0.019, val_acc:0.994]
Epoch [61/120    avg_loss:0.022, val_acc:0.988]
Epoch [62/120    avg_loss:0.023, val_acc:0.979]
Epoch [63/120    avg_loss:0.023, val_acc:0.990]
Epoch [64/120    avg_loss:0.014, val_acc:0.992]
Epoch [65/120    avg_loss:0.037, val_acc:0.996]
Epoch [66/120    avg_loss:0.021, val_acc:0.992]
Epoch [67/120    avg_loss:0.018, val_acc:0.985]
Epoch [68/120    avg_loss:0.029, val_acc:0.992]
Epoch [69/120    avg_loss:0.013, val_acc:0.994]
Epoch [70/120    avg_loss:0.013, val_acc:0.994]
Epoch [71/120    avg_loss:0.012, val_acc:0.992]
Epoch [72/120    avg_loss:0.016, val_acc:0.992]
Epoch [73/120    avg_loss:0.018, val_acc:0.988]
Epoch [74/120    avg_loss:0.017, val_acc:0.988]
Epoch [75/120    avg_loss:0.018, val_acc:0.977]
Epoch [76/120    avg_loss:0.049, val_acc:0.988]
Epoch [77/120    avg_loss:0.035, val_acc:0.994]
Epoch [78/120    avg_loss:0.025, val_acc:0.992]
Epoch [79/120    avg_loss:0.013, val_acc:0.992]
Epoch [80/120    avg_loss:0.008, val_acc:0.992]
Epoch [81/120    avg_loss:0.010, val_acc:0.992]
Epoch [82/120    avg_loss:0.009, val_acc:0.994]
Epoch [83/120    avg_loss:0.010, val_acc:0.994]
Epoch [84/120    avg_loss:0.006, val_acc:0.994]
Epoch [85/120    avg_loss:0.009, val_acc:0.994]
Epoch [86/120    avg_loss:0.007, val_acc:0.992]
Epoch [87/120    avg_loss:0.009, val_acc:0.992]
Epoch [88/120    avg_loss:0.008, val_acc:0.994]
Epoch [89/120    avg_loss:0.008, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.994]
Epoch [91/120    avg_loss:0.005, val_acc:0.994]
Epoch [92/120    avg_loss:0.012, val_acc:0.994]
Epoch [93/120    avg_loss:0.007, val_acc:0.994]
Epoch [94/120    avg_loss:0.007, val_acc:0.994]
Epoch [95/120    avg_loss:0.006, val_acc:0.994]
Epoch [96/120    avg_loss:0.006, val_acc:0.994]
Epoch [97/120    avg_loss:0.008, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.007, val_acc:0.994]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.006, val_acc:0.994]
Epoch [102/120    avg_loss:0.008, val_acc:0.994]
Epoch [103/120    avg_loss:0.006, val_acc:0.994]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.005, val_acc:0.994]
Epoch [106/120    avg_loss:0.006, val_acc:0.994]
Epoch [107/120    avg_loss:0.005, val_acc:0.994]
Epoch [108/120    avg_loss:0.006, val_acc:0.994]
Epoch [109/120    avg_loss:0.006, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.008, val_acc:0.994]
Epoch [112/120    avg_loss:0.006, val_acc:0.994]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.994]
Epoch [117/120    avg_loss:0.003, val_acc:0.994]
Epoch [118/120    avg_loss:0.008, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.012, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   3   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 0.99780541 0.9977221  1.         0.97368421 0.94845361
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9962021525757873
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4417c84780>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.112, val_acc:0.598]
Epoch [2/120    avg_loss:1.337, val_acc:0.677]
Epoch [3/120    avg_loss:0.936, val_acc:0.748]
Epoch [4/120    avg_loss:0.711, val_acc:0.815]
Epoch [5/120    avg_loss:0.733, val_acc:0.823]
Epoch [6/120    avg_loss:0.565, val_acc:0.871]
Epoch [7/120    avg_loss:0.489, val_acc:0.902]
Epoch [8/120    avg_loss:0.549, val_acc:0.865]
Epoch [9/120    avg_loss:0.484, val_acc:0.856]
Epoch [10/120    avg_loss:0.383, val_acc:0.898]
Epoch [11/120    avg_loss:0.406, val_acc:0.925]
Epoch [12/120    avg_loss:0.311, val_acc:0.933]
Epoch [13/120    avg_loss:0.238, val_acc:0.915]
Epoch [14/120    avg_loss:0.324, val_acc:0.935]
Epoch [15/120    avg_loss:0.277, val_acc:0.921]
Epoch [16/120    avg_loss:0.279, val_acc:0.923]
Epoch [17/120    avg_loss:0.266, val_acc:0.927]
Epoch [18/120    avg_loss:0.183, val_acc:0.956]
Epoch [19/120    avg_loss:0.183, val_acc:0.946]
Epoch [20/120    avg_loss:0.142, val_acc:0.988]
Epoch [21/120    avg_loss:0.183, val_acc:0.958]
Epoch [22/120    avg_loss:0.157, val_acc:0.952]
Epoch [23/120    avg_loss:0.216, val_acc:0.946]
Epoch [24/120    avg_loss:0.173, val_acc:0.948]
Epoch [25/120    avg_loss:0.141, val_acc:0.938]
Epoch [26/120    avg_loss:0.092, val_acc:0.973]
Epoch [27/120    avg_loss:0.076, val_acc:0.983]
Epoch [28/120    avg_loss:0.123, val_acc:0.958]
Epoch [29/120    avg_loss:0.123, val_acc:0.971]
Epoch [30/120    avg_loss:0.118, val_acc:0.950]
Epoch [31/120    avg_loss:0.086, val_acc:0.965]
Epoch [32/120    avg_loss:0.078, val_acc:0.988]
Epoch [33/120    avg_loss:0.081, val_acc:0.988]
Epoch [34/120    avg_loss:0.065, val_acc:0.981]
Epoch [35/120    avg_loss:0.087, val_acc:0.983]
Epoch [36/120    avg_loss:0.128, val_acc:0.990]
Epoch [37/120    avg_loss:0.065, val_acc:0.981]
Epoch [38/120    avg_loss:0.043, val_acc:0.990]
Epoch [39/120    avg_loss:0.045, val_acc:0.985]
Epoch [40/120    avg_loss:0.076, val_acc:0.979]
Epoch [41/120    avg_loss:0.062, val_acc:0.975]
Epoch [42/120    avg_loss:0.050, val_acc:0.981]
Epoch [43/120    avg_loss:0.044, val_acc:0.985]
Epoch [44/120    avg_loss:0.051, val_acc:0.992]
Epoch [45/120    avg_loss:0.049, val_acc:0.981]
Epoch [46/120    avg_loss:0.035, val_acc:0.988]
Epoch [47/120    avg_loss:0.032, val_acc:0.988]
Epoch [48/120    avg_loss:0.045, val_acc:0.994]
Epoch [49/120    avg_loss:0.028, val_acc:0.992]
Epoch [50/120    avg_loss:0.034, val_acc:0.990]
Epoch [51/120    avg_loss:0.022, val_acc:0.985]
Epoch [52/120    avg_loss:0.024, val_acc:0.992]
Epoch [53/120    avg_loss:0.076, val_acc:0.942]
Epoch [54/120    avg_loss:0.181, val_acc:0.958]
Epoch [55/120    avg_loss:0.058, val_acc:0.988]
Epoch [56/120    avg_loss:0.034, val_acc:0.994]
Epoch [57/120    avg_loss:0.019, val_acc:0.994]
Epoch [58/120    avg_loss:0.037, val_acc:0.990]
Epoch [59/120    avg_loss:0.026, val_acc:0.994]
Epoch [60/120    avg_loss:0.033, val_acc:0.988]
Epoch [61/120    avg_loss:0.079, val_acc:0.988]
Epoch [62/120    avg_loss:0.042, val_acc:0.983]
Epoch [63/120    avg_loss:0.023, val_acc:0.990]
Epoch [64/120    avg_loss:0.031, val_acc:0.985]
Epoch [65/120    avg_loss:0.021, val_acc:0.994]
Epoch [66/120    avg_loss:0.014, val_acc:0.992]
Epoch [67/120    avg_loss:0.009, val_acc:0.992]
Epoch [68/120    avg_loss:0.016, val_acc:0.994]
Epoch [69/120    avg_loss:0.013, val_acc:0.992]
Epoch [70/120    avg_loss:0.011, val_acc:0.990]
Epoch [71/120    avg_loss:0.016, val_acc:0.994]
Epoch [72/120    avg_loss:0.014, val_acc:0.994]
Epoch [73/120    avg_loss:0.038, val_acc:0.994]
Epoch [74/120    avg_loss:0.029, val_acc:0.983]
Epoch [75/120    avg_loss:0.009, val_acc:0.996]
Epoch [76/120    avg_loss:0.021, val_acc:0.996]
Epoch [77/120    avg_loss:0.019, val_acc:0.996]
Epoch [78/120    avg_loss:0.029, val_acc:0.990]
Epoch [79/120    avg_loss:0.051, val_acc:0.990]
Epoch [80/120    avg_loss:0.033, val_acc:0.990]
Epoch [81/120    avg_loss:0.040, val_acc:0.988]
Epoch [82/120    avg_loss:0.059, val_acc:0.983]
Epoch [83/120    avg_loss:0.100, val_acc:0.990]
Epoch [84/120    avg_loss:0.054, val_acc:0.977]
Epoch [85/120    avg_loss:0.025, val_acc:0.988]
Epoch [86/120    avg_loss:0.019, val_acc:0.990]
Epoch [87/120    avg_loss:0.018, val_acc:0.996]
Epoch [88/120    avg_loss:0.014, val_acc:0.996]
Epoch [89/120    avg_loss:0.008, val_acc:0.994]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.998]
Epoch [92/120    avg_loss:0.008, val_acc:0.994]
Epoch [93/120    avg_loss:0.018, val_acc:0.992]
Epoch [94/120    avg_loss:0.038, val_acc:0.994]
Epoch [95/120    avg_loss:0.026, val_acc:0.992]
Epoch [96/120    avg_loss:0.015, val_acc:0.979]
Epoch [97/120    avg_loss:0.023, val_acc:0.985]
Epoch [98/120    avg_loss:0.034, val_acc:0.990]
Epoch [99/120    avg_loss:0.016, val_acc:0.994]
Epoch [100/120    avg_loss:0.009, val_acc:0.996]
Epoch [101/120    avg_loss:0.011, val_acc:0.996]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.008, val_acc:0.992]
Epoch [106/120    avg_loss:0.019, val_acc:0.996]
Epoch [107/120    avg_loss:0.018, val_acc:0.996]
Epoch [108/120    avg_loss:0.004, val_acc:0.996]
Epoch [109/120    avg_loss:0.006, val_acc:0.996]
Epoch [110/120    avg_loss:0.006, val_acc:0.996]
Epoch [111/120    avg_loss:0.005, val_acc:0.996]
Epoch [112/120    avg_loss:0.004, val_acc:0.996]
Epoch [113/120    avg_loss:0.004, val_acc:0.996]
Epoch [114/120    avg_loss:0.007, val_acc:0.996]
Epoch [115/120    avg_loss:0.007, val_acc:0.996]
Epoch [116/120    avg_loss:0.005, val_acc:0.996]
Epoch [117/120    avg_loss:0.007, val_acc:0.996]
Epoch [118/120    avg_loss:0.004, val_acc:0.996]
Epoch [119/120    avg_loss:0.008, val_acc:0.996]
Epoch [120/120    avg_loss:0.004, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 1.         1.         1.         0.96536797 0.94326241
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9962017706106332
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5b67d9898>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.112, val_acc:0.588]
Epoch [2/120    avg_loss:1.314, val_acc:0.667]
Epoch [3/120    avg_loss:0.953, val_acc:0.740]
Epoch [4/120    avg_loss:0.739, val_acc:0.740]
Epoch [5/120    avg_loss:0.648, val_acc:0.769]
Epoch [6/120    avg_loss:0.647, val_acc:0.838]
Epoch [7/120    avg_loss:0.514, val_acc:0.879]
Epoch [8/120    avg_loss:0.631, val_acc:0.823]
Epoch [9/120    avg_loss:0.498, val_acc:0.873]
Epoch [10/120    avg_loss:0.519, val_acc:0.877]
Epoch [11/120    avg_loss:0.418, val_acc:0.894]
Epoch [12/120    avg_loss:0.375, val_acc:0.865]
Epoch [13/120    avg_loss:0.386, val_acc:0.908]
Epoch [14/120    avg_loss:0.232, val_acc:0.927]
Epoch [15/120    avg_loss:0.270, val_acc:0.910]
Epoch [16/120    avg_loss:0.226, val_acc:0.938]
Epoch [17/120    avg_loss:0.179, val_acc:0.952]
Epoch [18/120    avg_loss:0.195, val_acc:0.967]
Epoch [19/120    avg_loss:0.140, val_acc:0.958]
Epoch [20/120    avg_loss:0.195, val_acc:0.963]
Epoch [21/120    avg_loss:0.123, val_acc:0.942]
Epoch [22/120    avg_loss:0.167, val_acc:0.971]
Epoch [23/120    avg_loss:0.119, val_acc:0.963]
Epoch [24/120    avg_loss:0.179, val_acc:0.952]
Epoch [25/120    avg_loss:0.135, val_acc:0.969]
Epoch [26/120    avg_loss:0.092, val_acc:0.973]
Epoch [27/120    avg_loss:0.161, val_acc:0.969]
Epoch [28/120    avg_loss:0.101, val_acc:0.971]
Epoch [29/120    avg_loss:0.130, val_acc:0.963]
Epoch [30/120    avg_loss:0.068, val_acc:0.975]
Epoch [31/120    avg_loss:0.072, val_acc:0.979]
Epoch [32/120    avg_loss:0.098, val_acc:0.973]
Epoch [33/120    avg_loss:0.112, val_acc:0.925]
Epoch [34/120    avg_loss:0.103, val_acc:0.971]
Epoch [35/120    avg_loss:0.073, val_acc:0.969]
Epoch [36/120    avg_loss:0.069, val_acc:0.979]
Epoch [37/120    avg_loss:0.060, val_acc:0.973]
Epoch [38/120    avg_loss:0.076, val_acc:0.988]
Epoch [39/120    avg_loss:0.067, val_acc:0.971]
Epoch [40/120    avg_loss:0.043, val_acc:0.985]
Epoch [41/120    avg_loss:0.038, val_acc:0.979]
Epoch [42/120    avg_loss:0.043, val_acc:0.985]
Epoch [43/120    avg_loss:0.056, val_acc:0.981]
Epoch [44/120    avg_loss:0.047, val_acc:0.990]
Epoch [45/120    avg_loss:0.069, val_acc:0.954]
Epoch [46/120    avg_loss:0.148, val_acc:0.963]
Epoch [47/120    avg_loss:0.063, val_acc:0.985]
Epoch [48/120    avg_loss:0.051, val_acc:0.973]
Epoch [49/120    avg_loss:0.044, val_acc:0.981]
Epoch [50/120    avg_loss:0.037, val_acc:0.983]
Epoch [51/120    avg_loss:0.037, val_acc:0.988]
Epoch [52/120    avg_loss:0.039, val_acc:0.990]
Epoch [53/120    avg_loss:0.044, val_acc:0.977]
Epoch [54/120    avg_loss:0.067, val_acc:0.985]
Epoch [55/120    avg_loss:0.040, val_acc:0.940]
Epoch [56/120    avg_loss:0.054, val_acc:0.988]
Epoch [57/120    avg_loss:0.050, val_acc:0.994]
Epoch [58/120    avg_loss:0.023, val_acc:0.994]
Epoch [59/120    avg_loss:0.020, val_acc:0.992]
Epoch [60/120    avg_loss:0.036, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.983]
Epoch [62/120    avg_loss:0.023, val_acc:0.988]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.016, val_acc:0.994]
Epoch [65/120    avg_loss:0.018, val_acc:0.992]
Epoch [66/120    avg_loss:0.016, val_acc:0.996]
Epoch [67/120    avg_loss:0.013, val_acc:0.992]
Epoch [68/120    avg_loss:0.009, val_acc:0.994]
Epoch [69/120    avg_loss:0.010, val_acc:0.996]
Epoch [70/120    avg_loss:0.008, val_acc:0.996]
Epoch [71/120    avg_loss:0.008, val_acc:0.994]
Epoch [72/120    avg_loss:0.019, val_acc:0.992]
Epoch [73/120    avg_loss:0.015, val_acc:0.994]
Epoch [74/120    avg_loss:0.014, val_acc:0.990]
Epoch [75/120    avg_loss:0.017, val_acc:0.983]
Epoch [76/120    avg_loss:0.015, val_acc:0.996]
Epoch [77/120    avg_loss:0.017, val_acc:0.994]
Epoch [78/120    avg_loss:0.011, val_acc:0.992]
Epoch [79/120    avg_loss:0.027, val_acc:0.960]
Epoch [80/120    avg_loss:0.051, val_acc:0.981]
Epoch [81/120    avg_loss:0.048, val_acc:0.979]
Epoch [82/120    avg_loss:0.042, val_acc:0.979]
Epoch [83/120    avg_loss:0.018, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.992]
Epoch [85/120    avg_loss:0.027, val_acc:0.985]
Epoch [86/120    avg_loss:0.029, val_acc:0.983]
Epoch [87/120    avg_loss:0.012, val_acc:0.988]
Epoch [88/120    avg_loss:0.012, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.992]
Epoch [91/120    avg_loss:0.007, val_acc:0.992]
Epoch [92/120    avg_loss:0.011, val_acc:0.992]
Epoch [93/120    avg_loss:0.010, val_acc:0.996]
Epoch [94/120    avg_loss:0.018, val_acc:0.994]
Epoch [95/120    avg_loss:0.011, val_acc:0.992]
Epoch [96/120    avg_loss:0.004, val_acc:0.994]
Epoch [97/120    avg_loss:0.008, val_acc:0.996]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.005, val_acc:0.996]
Epoch [100/120    avg_loss:0.004, val_acc:0.996]
Epoch [101/120    avg_loss:0.004, val_acc:0.996]
Epoch [102/120    avg_loss:0.006, val_acc:0.996]
Epoch [103/120    avg_loss:0.013, val_acc:0.996]
Epoch [104/120    avg_loss:0.004, val_acc:0.996]
Epoch [105/120    avg_loss:0.010, val_acc:0.994]
Epoch [106/120    avg_loss:0.005, val_acc:0.994]
Epoch [107/120    avg_loss:0.006, val_acc:0.994]
Epoch [108/120    avg_loss:0.005, val_acc:0.994]
Epoch [109/120    avg_loss:0.008, val_acc:0.994]
Epoch [110/120    avg_loss:0.006, val_acc:0.994]
Epoch [111/120    avg_loss:0.007, val_acc:0.996]
Epoch [112/120    avg_loss:0.006, val_acc:0.996]
Epoch [113/120    avg_loss:0.007, val_acc:0.996]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.996]
Epoch [116/120    avg_loss:0.005, val_acc:0.996]
Epoch [117/120    avg_loss:0.005, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   4   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 0.99707174 1.         1.         0.98206278 0.97315436
 1.         1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9971516509037435
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7743267748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.062, val_acc:0.596]
Epoch [2/120    avg_loss:1.327, val_acc:0.671]
Epoch [3/120    avg_loss:0.939, val_acc:0.713]
Epoch [4/120    avg_loss:0.707, val_acc:0.765]
Epoch [5/120    avg_loss:0.666, val_acc:0.856]
Epoch [6/120    avg_loss:0.565, val_acc:0.854]
Epoch [7/120    avg_loss:0.475, val_acc:0.908]
Epoch [8/120    avg_loss:0.416, val_acc:0.919]
Epoch [9/120    avg_loss:0.359, val_acc:0.881]
Epoch [10/120    avg_loss:0.539, val_acc:0.790]
Epoch [11/120    avg_loss:0.525, val_acc:0.831]
Epoch [12/120    avg_loss:0.418, val_acc:0.912]
Epoch [13/120    avg_loss:0.396, val_acc:0.927]
Epoch [14/120    avg_loss:0.303, val_acc:0.935]
Epoch [15/120    avg_loss:0.259, val_acc:0.933]
Epoch [16/120    avg_loss:0.266, val_acc:0.952]
Epoch [17/120    avg_loss:0.328, val_acc:0.931]
Epoch [18/120    avg_loss:0.151, val_acc:0.956]
Epoch [19/120    avg_loss:0.203, val_acc:0.931]
Epoch [20/120    avg_loss:0.201, val_acc:0.958]
Epoch [21/120    avg_loss:0.139, val_acc:0.969]
Epoch [22/120    avg_loss:0.156, val_acc:0.944]
Epoch [23/120    avg_loss:0.172, val_acc:0.967]
Epoch [24/120    avg_loss:0.123, val_acc:0.954]
Epoch [25/120    avg_loss:0.153, val_acc:0.952]
Epoch [26/120    avg_loss:0.244, val_acc:0.971]
Epoch [27/120    avg_loss:0.162, val_acc:0.956]
Epoch [28/120    avg_loss:0.150, val_acc:0.975]
Epoch [29/120    avg_loss:0.076, val_acc:0.977]
Epoch [30/120    avg_loss:0.103, val_acc:0.979]
Epoch [31/120    avg_loss:0.120, val_acc:0.975]
Epoch [32/120    avg_loss:0.111, val_acc:0.958]
Epoch [33/120    avg_loss:0.112, val_acc:0.973]
Epoch [34/120    avg_loss:0.129, val_acc:0.975]
Epoch [35/120    avg_loss:0.119, val_acc:0.969]
Epoch [36/120    avg_loss:0.073, val_acc:0.981]
Epoch [37/120    avg_loss:0.071, val_acc:0.979]
Epoch [38/120    avg_loss:0.080, val_acc:0.979]
Epoch [39/120    avg_loss:0.048, val_acc:0.977]
Epoch [40/120    avg_loss:0.045, val_acc:0.979]
Epoch [41/120    avg_loss:0.051, val_acc:0.979]
Epoch [42/120    avg_loss:0.041, val_acc:0.981]
Epoch [43/120    avg_loss:0.035, val_acc:0.979]
Epoch [44/120    avg_loss:0.032, val_acc:0.983]
Epoch [45/120    avg_loss:0.035, val_acc:0.981]
Epoch [46/120    avg_loss:0.028, val_acc:0.979]
Epoch [47/120    avg_loss:0.069, val_acc:0.958]
Epoch [48/120    avg_loss:0.043, val_acc:0.983]
Epoch [49/120    avg_loss:0.042, val_acc:0.975]
Epoch [50/120    avg_loss:0.046, val_acc:0.981]
Epoch [51/120    avg_loss:0.040, val_acc:0.983]
Epoch [52/120    avg_loss:0.044, val_acc:0.969]
Epoch [53/120    avg_loss:0.044, val_acc:0.975]
Epoch [54/120    avg_loss:0.048, val_acc:0.979]
Epoch [55/120    avg_loss:0.026, val_acc:0.988]
Epoch [56/120    avg_loss:0.039, val_acc:0.981]
Epoch [57/120    avg_loss:0.039, val_acc:0.975]
Epoch [58/120    avg_loss:0.079, val_acc:0.981]
Epoch [59/120    avg_loss:0.049, val_acc:0.985]
Epoch [60/120    avg_loss:0.052, val_acc:0.977]
Epoch [61/120    avg_loss:0.045, val_acc:0.988]
Epoch [62/120    avg_loss:0.019, val_acc:0.988]
Epoch [63/120    avg_loss:0.015, val_acc:0.981]
Epoch [64/120    avg_loss:0.037, val_acc:0.988]
Epoch [65/120    avg_loss:0.032, val_acc:0.983]
Epoch [66/120    avg_loss:0.030, val_acc:0.985]
Epoch [67/120    avg_loss:0.014, val_acc:0.988]
Epoch [68/120    avg_loss:0.030, val_acc:0.990]
Epoch [69/120    avg_loss:0.026, val_acc:0.990]
Epoch [70/120    avg_loss:0.012, val_acc:0.990]
Epoch [71/120    avg_loss:0.017, val_acc:0.985]
Epoch [72/120    avg_loss:0.015, val_acc:0.988]
Epoch [73/120    avg_loss:0.014, val_acc:0.988]
Epoch [74/120    avg_loss:0.027, val_acc:0.992]
Epoch [75/120    avg_loss:0.035, val_acc:0.992]
Epoch [76/120    avg_loss:0.108, val_acc:0.940]
Epoch [77/120    avg_loss:0.102, val_acc:0.977]
Epoch [78/120    avg_loss:0.060, val_acc:0.979]
Epoch [79/120    avg_loss:0.036, val_acc:0.992]
Epoch [80/120    avg_loss:0.037, val_acc:0.985]
Epoch [81/120    avg_loss:0.029, val_acc:0.967]
Epoch [82/120    avg_loss:0.040, val_acc:0.985]
Epoch [83/120    avg_loss:0.025, val_acc:0.988]
Epoch [84/120    avg_loss:0.022, val_acc:0.988]
Epoch [85/120    avg_loss:0.013, val_acc:0.985]
Epoch [86/120    avg_loss:0.027, val_acc:0.990]
Epoch [87/120    avg_loss:0.015, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.988]
Epoch [89/120    avg_loss:0.028, val_acc:0.990]
Epoch [90/120    avg_loss:0.024, val_acc:0.975]
Epoch [91/120    avg_loss:0.031, val_acc:0.988]
Epoch [92/120    avg_loss:0.023, val_acc:0.979]
Epoch [93/120    avg_loss:0.022, val_acc:0.981]
Epoch [94/120    avg_loss:0.024, val_acc:0.985]
Epoch [95/120    avg_loss:0.015, val_acc:0.988]
Epoch [96/120    avg_loss:0.013, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.017, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.015, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   6   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   3   0   0   0   0   0   0   4   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   2 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99560117 0.9977221  1.         0.97345133 0.95238095
 1.         1.         1.         1.         1.         0.9973545
 0.99228225 1.        ]

Kappa:
0.9950156347831263
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3d59d27b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.213, val_acc:0.625]
Epoch [2/120    avg_loss:1.462, val_acc:0.698]
Epoch [3/120    avg_loss:1.076, val_acc:0.735]
Epoch [4/120    avg_loss:0.985, val_acc:0.815]
Epoch [5/120    avg_loss:0.839, val_acc:0.819]
Epoch [6/120    avg_loss:0.740, val_acc:0.769]
Epoch [7/120    avg_loss:0.653, val_acc:0.865]
Epoch [8/120    avg_loss:0.493, val_acc:0.896]
Epoch [9/120    avg_loss:0.409, val_acc:0.896]
Epoch [10/120    avg_loss:0.435, val_acc:0.877]
Epoch [11/120    avg_loss:0.409, val_acc:0.917]
Epoch [12/120    avg_loss:0.426, val_acc:0.900]
Epoch [13/120    avg_loss:0.351, val_acc:0.865]
Epoch [14/120    avg_loss:0.416, val_acc:0.921]
Epoch [15/120    avg_loss:0.333, val_acc:0.906]
Epoch [16/120    avg_loss:0.344, val_acc:0.908]
Epoch [17/120    avg_loss:0.323, val_acc:0.931]
Epoch [18/120    avg_loss:0.274, val_acc:0.923]
Epoch [19/120    avg_loss:0.254, val_acc:0.921]
Epoch [20/120    avg_loss:0.292, val_acc:0.919]
Epoch [21/120    avg_loss:0.239, val_acc:0.929]
Epoch [22/120    avg_loss:0.172, val_acc:0.944]
Epoch [23/120    avg_loss:0.208, val_acc:0.929]
Epoch [24/120    avg_loss:0.197, val_acc:0.915]
Epoch [25/120    avg_loss:0.179, val_acc:0.938]
Epoch [26/120    avg_loss:0.213, val_acc:0.938]
Epoch [27/120    avg_loss:0.228, val_acc:0.946]
Epoch [28/120    avg_loss:0.292, val_acc:0.940]
Epoch [29/120    avg_loss:0.192, val_acc:0.925]
Epoch [30/120    avg_loss:0.180, val_acc:0.938]
Epoch [31/120    avg_loss:0.204, val_acc:0.931]
Epoch [32/120    avg_loss:0.185, val_acc:0.960]
Epoch [33/120    avg_loss:0.175, val_acc:0.942]
Epoch [34/120    avg_loss:0.160, val_acc:0.950]
Epoch [35/120    avg_loss:0.124, val_acc:0.956]
Epoch [36/120    avg_loss:0.126, val_acc:0.940]
Epoch [37/120    avg_loss:0.182, val_acc:0.935]
Epoch [38/120    avg_loss:0.134, val_acc:0.933]
Epoch [39/120    avg_loss:0.102, val_acc:0.963]
Epoch [40/120    avg_loss:0.151, val_acc:0.940]
Epoch [41/120    avg_loss:0.106, val_acc:0.938]
Epoch [42/120    avg_loss:0.105, val_acc:0.950]
Epoch [43/120    avg_loss:0.086, val_acc:0.960]
Epoch [44/120    avg_loss:0.111, val_acc:0.950]
Epoch [45/120    avg_loss:0.088, val_acc:0.944]
Epoch [46/120    avg_loss:0.100, val_acc:0.967]
Epoch [47/120    avg_loss:0.126, val_acc:0.956]
Epoch [48/120    avg_loss:0.112, val_acc:0.935]
Epoch [49/120    avg_loss:0.128, val_acc:0.958]
Epoch [50/120    avg_loss:0.155, val_acc:0.944]
Epoch [51/120    avg_loss:0.093, val_acc:0.960]
Epoch [52/120    avg_loss:0.077, val_acc:0.956]
Epoch [53/120    avg_loss:0.088, val_acc:0.967]
Epoch [54/120    avg_loss:0.076, val_acc:0.967]
Epoch [55/120    avg_loss:0.111, val_acc:0.933]
Epoch [56/120    avg_loss:0.123, val_acc:0.954]
Epoch [57/120    avg_loss:0.072, val_acc:0.973]
Epoch [58/120    avg_loss:0.065, val_acc:0.977]
Epoch [59/120    avg_loss:0.074, val_acc:0.963]
Epoch [60/120    avg_loss:0.070, val_acc:0.952]
Epoch [61/120    avg_loss:0.077, val_acc:0.967]
Epoch [62/120    avg_loss:0.071, val_acc:0.971]
Epoch [63/120    avg_loss:0.120, val_acc:0.952]
Epoch [64/120    avg_loss:0.076, val_acc:0.971]
Epoch [65/120    avg_loss:0.075, val_acc:0.965]
Epoch [66/120    avg_loss:0.068, val_acc:0.969]
Epoch [67/120    avg_loss:0.054, val_acc:0.956]
Epoch [68/120    avg_loss:0.036, val_acc:0.969]
Epoch [69/120    avg_loss:0.062, val_acc:0.975]
Epoch [70/120    avg_loss:0.032, val_acc:0.973]
Epoch [71/120    avg_loss:0.027, val_acc:0.981]
Epoch [72/120    avg_loss:0.047, val_acc:0.965]
Epoch [73/120    avg_loss:0.047, val_acc:0.971]
Epoch [74/120    avg_loss:0.059, val_acc:0.975]
Epoch [75/120    avg_loss:0.060, val_acc:0.977]
Epoch [76/120    avg_loss:0.035, val_acc:0.967]
Epoch [77/120    avg_loss:0.047, val_acc:0.958]
Epoch [78/120    avg_loss:0.027, val_acc:0.965]
Epoch [79/120    avg_loss:0.051, val_acc:0.965]
Epoch [80/120    avg_loss:0.080, val_acc:0.950]
Epoch [81/120    avg_loss:0.047, val_acc:0.967]
Epoch [82/120    avg_loss:0.046, val_acc:0.971]
Epoch [83/120    avg_loss:0.045, val_acc:0.973]
Epoch [84/120    avg_loss:0.039, val_acc:0.967]
Epoch [85/120    avg_loss:0.044, val_acc:0.977]
Epoch [86/120    avg_loss:0.023, val_acc:0.977]
Epoch [87/120    avg_loss:0.033, val_acc:0.975]
Epoch [88/120    avg_loss:0.015, val_acc:0.973]
Epoch [89/120    avg_loss:0.011, val_acc:0.973]
Epoch [90/120    avg_loss:0.013, val_acc:0.973]
Epoch [91/120    avg_loss:0.015, val_acc:0.973]
Epoch [92/120    avg_loss:0.021, val_acc:0.975]
Epoch [93/120    avg_loss:0.016, val_acc:0.977]
Epoch [94/120    avg_loss:0.019, val_acc:0.977]
Epoch [95/120    avg_loss:0.019, val_acc:0.981]
Epoch [96/120    avg_loss:0.015, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.017, val_acc:0.979]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.013, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.014, val_acc:0.981]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.016, val_acc:0.983]
Epoch [108/120    avg_loss:0.026, val_acc:0.981]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.023, val_acc:0.981]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.015, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.017, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 219   5   0   0   0   0   0   0   3   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.97949886 0.98230088 0.9279661  0.92857143
 0.99019608 0.99470899 1.         0.99893276 1.         1.
 0.98787211 1.        ]

Kappa:
0.9895545783908747
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5d47d6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.223, val_acc:0.562]
Epoch [2/120    avg_loss:1.473, val_acc:0.631]
Epoch [3/120    avg_loss:1.026, val_acc:0.807]
Epoch [4/120    avg_loss:0.893, val_acc:0.742]
Epoch [5/120    avg_loss:0.729, val_acc:0.867]
Epoch [6/120    avg_loss:0.623, val_acc:0.822]
Epoch [7/120    avg_loss:0.504, val_acc:0.887]
Epoch [8/120    avg_loss:0.572, val_acc:0.883]
Epoch [9/120    avg_loss:0.575, val_acc:0.879]
Epoch [10/120    avg_loss:0.471, val_acc:0.920]
Epoch [11/120    avg_loss:0.405, val_acc:0.943]
Epoch [12/120    avg_loss:0.374, val_acc:0.934]
Epoch [13/120    avg_loss:0.341, val_acc:0.951]
Epoch [14/120    avg_loss:0.254, val_acc:0.941]
Epoch [15/120    avg_loss:0.228, val_acc:0.939]
Epoch [16/120    avg_loss:0.279, val_acc:0.906]
Epoch [17/120    avg_loss:0.296, val_acc:0.953]
Epoch [18/120    avg_loss:0.215, val_acc:0.949]
Epoch [19/120    avg_loss:0.227, val_acc:0.947]
Epoch [20/120    avg_loss:0.265, val_acc:0.934]
Epoch [21/120    avg_loss:0.244, val_acc:0.887]
Epoch [22/120    avg_loss:0.367, val_acc:0.904]
Epoch [23/120    avg_loss:0.321, val_acc:0.930]
Epoch [24/120    avg_loss:0.299, val_acc:0.928]
Epoch [25/120    avg_loss:0.315, val_acc:0.930]
Epoch [26/120    avg_loss:0.237, val_acc:0.947]
Epoch [27/120    avg_loss:0.173, val_acc:0.955]
Epoch [28/120    avg_loss:0.146, val_acc:0.963]
Epoch [29/120    avg_loss:0.228, val_acc:0.943]
Epoch [30/120    avg_loss:0.148, val_acc:0.961]
Epoch [31/120    avg_loss:0.267, val_acc:0.924]
Epoch [32/120    avg_loss:0.205, val_acc:0.936]
Epoch [33/120    avg_loss:0.236, val_acc:0.963]
Epoch [34/120    avg_loss:0.195, val_acc:0.947]
Epoch [35/120    avg_loss:0.201, val_acc:0.947]
Epoch [36/120    avg_loss:0.138, val_acc:0.969]
Epoch [37/120    avg_loss:0.136, val_acc:0.973]
Epoch [38/120    avg_loss:0.131, val_acc:0.965]
Epoch [39/120    avg_loss:0.149, val_acc:0.969]
Epoch [40/120    avg_loss:0.113, val_acc:0.967]
Epoch [41/120    avg_loss:0.143, val_acc:0.971]
Epoch [42/120    avg_loss:0.090, val_acc:0.982]
Epoch [43/120    avg_loss:0.102, val_acc:0.963]
Epoch [44/120    avg_loss:0.093, val_acc:0.977]
Epoch [45/120    avg_loss:0.069, val_acc:0.979]
Epoch [46/120    avg_loss:0.051, val_acc:0.984]
Epoch [47/120    avg_loss:0.068, val_acc:0.982]
Epoch [48/120    avg_loss:0.086, val_acc:0.982]
Epoch [49/120    avg_loss:0.080, val_acc:0.969]
Epoch [50/120    avg_loss:0.132, val_acc:0.967]
Epoch [51/120    avg_loss:0.164, val_acc:0.969]
Epoch [52/120    avg_loss:0.139, val_acc:0.949]
Epoch [53/120    avg_loss:0.110, val_acc:0.977]
Epoch [54/120    avg_loss:0.086, val_acc:0.982]
Epoch [55/120    avg_loss:0.113, val_acc:0.975]
Epoch [56/120    avg_loss:0.054, val_acc:0.982]
Epoch [57/120    avg_loss:0.040, val_acc:0.986]
Epoch [58/120    avg_loss:0.061, val_acc:0.980]
Epoch [59/120    avg_loss:0.065, val_acc:0.984]
Epoch [60/120    avg_loss:0.036, val_acc:0.969]
Epoch [61/120    avg_loss:0.060, val_acc:0.977]
Epoch [62/120    avg_loss:0.042, val_acc:0.984]
Epoch [63/120    avg_loss:0.053, val_acc:0.980]
Epoch [64/120    avg_loss:0.050, val_acc:0.984]
Epoch [65/120    avg_loss:0.041, val_acc:0.977]
Epoch [66/120    avg_loss:0.109, val_acc:0.973]
Epoch [67/120    avg_loss:0.043, val_acc:0.986]
Epoch [68/120    avg_loss:0.081, val_acc:0.980]
Epoch [69/120    avg_loss:0.038, val_acc:0.990]
Epoch [70/120    avg_loss:0.047, val_acc:0.982]
Epoch [71/120    avg_loss:0.042, val_acc:0.982]
Epoch [72/120    avg_loss:0.033, val_acc:0.986]
Epoch [73/120    avg_loss:0.024, val_acc:0.980]
Epoch [74/120    avg_loss:0.028, val_acc:0.988]
Epoch [75/120    avg_loss:0.043, val_acc:0.959]
Epoch [76/120    avg_loss:0.039, val_acc:0.973]
Epoch [77/120    avg_loss:0.086, val_acc:0.980]
Epoch [78/120    avg_loss:0.080, val_acc:0.977]
Epoch [79/120    avg_loss:0.069, val_acc:0.969]
Epoch [80/120    avg_loss:0.105, val_acc:0.980]
Epoch [81/120    avg_loss:0.074, val_acc:0.969]
Epoch [82/120    avg_loss:0.051, val_acc:0.980]
Epoch [83/120    avg_loss:0.043, val_acc:0.984]
Epoch [84/120    avg_loss:0.045, val_acc:0.988]
Epoch [85/120    avg_loss:0.021, val_acc:0.988]
Epoch [86/120    avg_loss:0.028, val_acc:0.990]
Epoch [87/120    avg_loss:0.020, val_acc:0.988]
Epoch [88/120    avg_loss:0.018, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.016, val_acc:0.986]
Epoch [91/120    avg_loss:0.016, val_acc:0.986]
Epoch [92/120    avg_loss:0.017, val_acc:0.986]
Epoch [93/120    avg_loss:0.021, val_acc:0.986]
Epoch [94/120    avg_loss:0.015, val_acc:0.986]
Epoch [95/120    avg_loss:0.030, val_acc:0.988]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.023, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.017, val_acc:0.988]
Epoch [100/120    avg_loss:0.020, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.988]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.014, val_acc:0.988]
Epoch [104/120    avg_loss:0.019, val_acc:0.988]
Epoch [105/120    avg_loss:0.015, val_acc:0.988]
Epoch [106/120    avg_loss:0.015, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.018, val_acc:0.988]
Epoch [109/120    avg_loss:0.025, val_acc:0.988]
Epoch [110/120    avg_loss:0.019, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.988]
Epoch [112/120    avg_loss:0.013, val_acc:0.988]
Epoch [113/120    avg_loss:0.019, val_acc:0.988]
Epoch [114/120    avg_loss:0.017, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.017, val_acc:0.988]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.013, val_acc:0.988]
Epoch [120/120    avg_loss:0.015, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   2   0   0   0   0   5   0]
 [  0   0   0 222   4   0   0   0   2   2   0   0   0   0]
 [  0   0   0   0 219   7   0   0   0   0   0   0   1   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   2 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.97695853 0.98230088 0.93589744 0.91039427
 1.         0.9787234  0.99742931 0.9978678  1.         0.9973545
 0.99009901 1.        ]

Kappa:
0.989078965045905
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27a12dc748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.170, val_acc:0.482]
Epoch [2/120    avg_loss:1.509, val_acc:0.734]
Epoch [3/120    avg_loss:1.118, val_acc:0.779]
Epoch [4/120    avg_loss:1.054, val_acc:0.797]
Epoch [5/120    avg_loss:0.842, val_acc:0.773]
Epoch [6/120    avg_loss:0.661, val_acc:0.867]
Epoch [7/120    avg_loss:0.567, val_acc:0.855]
Epoch [8/120    avg_loss:0.626, val_acc:0.809]
Epoch [9/120    avg_loss:0.695, val_acc:0.887]
Epoch [10/120    avg_loss:0.437, val_acc:0.857]
Epoch [11/120    avg_loss:0.442, val_acc:0.926]
Epoch [12/120    avg_loss:0.409, val_acc:0.914]
Epoch [13/120    avg_loss:0.340, val_acc:0.898]
Epoch [14/120    avg_loss:0.309, val_acc:0.918]
Epoch [15/120    avg_loss:0.343, val_acc:0.914]
Epoch [16/120    avg_loss:0.380, val_acc:0.926]
Epoch [17/120    avg_loss:0.286, val_acc:0.926]
Epoch [18/120    avg_loss:0.296, val_acc:0.920]
Epoch [19/120    avg_loss:0.281, val_acc:0.928]
Epoch [20/120    avg_loss:0.263, val_acc:0.902]
Epoch [21/120    avg_loss:0.304, val_acc:0.934]
Epoch [22/120    avg_loss:0.210, val_acc:0.955]
Epoch [23/120    avg_loss:0.249, val_acc:0.947]
Epoch [24/120    avg_loss:0.302, val_acc:0.949]
Epoch [25/120    avg_loss:0.212, val_acc:0.959]
Epoch [26/120    avg_loss:0.231, val_acc:0.936]
Epoch [27/120    avg_loss:0.246, val_acc:0.959]
Epoch [28/120    avg_loss:0.271, val_acc:0.914]
Epoch [29/120    avg_loss:0.235, val_acc:0.971]
Epoch [30/120    avg_loss:0.167, val_acc:0.965]
Epoch [31/120    avg_loss:0.177, val_acc:0.967]
Epoch [32/120    avg_loss:0.132, val_acc:0.949]
Epoch [33/120    avg_loss:0.179, val_acc:0.971]
Epoch [34/120    avg_loss:0.188, val_acc:0.953]
Epoch [35/120    avg_loss:0.156, val_acc:0.959]
Epoch [36/120    avg_loss:0.193, val_acc:0.967]
Epoch [37/120    avg_loss:0.141, val_acc:0.959]
Epoch [38/120    avg_loss:0.173, val_acc:0.973]
Epoch [39/120    avg_loss:0.161, val_acc:0.961]
Epoch [40/120    avg_loss:0.134, val_acc:0.975]
Epoch [41/120    avg_loss:0.115, val_acc:0.967]
Epoch [42/120    avg_loss:0.094, val_acc:0.977]
Epoch [43/120    avg_loss:0.080, val_acc:0.979]
Epoch [44/120    avg_loss:0.071, val_acc:0.980]
Epoch [45/120    avg_loss:0.092, val_acc:0.973]
Epoch [46/120    avg_loss:0.145, val_acc:0.947]
Epoch [47/120    avg_loss:0.099, val_acc:0.961]
Epoch [48/120    avg_loss:0.083, val_acc:0.984]
Epoch [49/120    avg_loss:0.058, val_acc:0.961]
Epoch [50/120    avg_loss:0.072, val_acc:0.975]
Epoch [51/120    avg_loss:0.073, val_acc:0.977]
Epoch [52/120    avg_loss:0.084, val_acc:0.975]
Epoch [53/120    avg_loss:0.101, val_acc:0.945]
Epoch [54/120    avg_loss:0.175, val_acc:0.969]
Epoch [55/120    avg_loss:0.072, val_acc:0.971]
Epoch [56/120    avg_loss:0.061, val_acc:0.979]
Epoch [57/120    avg_loss:0.102, val_acc:0.980]
Epoch [58/120    avg_loss:0.093, val_acc:0.971]
Epoch [59/120    avg_loss:0.067, val_acc:0.965]
Epoch [60/120    avg_loss:0.071, val_acc:0.980]
Epoch [61/120    avg_loss:0.044, val_acc:0.982]
Epoch [62/120    avg_loss:0.036, val_acc:0.982]
Epoch [63/120    avg_loss:0.035, val_acc:0.984]
Epoch [64/120    avg_loss:0.026, val_acc:0.984]
Epoch [65/120    avg_loss:0.029, val_acc:0.982]
Epoch [66/120    avg_loss:0.050, val_acc:0.984]
Epoch [67/120    avg_loss:0.030, val_acc:0.984]
Epoch [68/120    avg_loss:0.049, val_acc:0.984]
Epoch [69/120    avg_loss:0.025, val_acc:0.984]
Epoch [70/120    avg_loss:0.026, val_acc:0.986]
Epoch [71/120    avg_loss:0.033, val_acc:0.984]
Epoch [72/120    avg_loss:0.028, val_acc:0.986]
Epoch [73/120    avg_loss:0.033, val_acc:0.986]
Epoch [74/120    avg_loss:0.018, val_acc:0.984]
Epoch [75/120    avg_loss:0.027, val_acc:0.984]
Epoch [76/120    avg_loss:0.039, val_acc:0.984]
Epoch [77/120    avg_loss:0.025, val_acc:0.986]
Epoch [78/120    avg_loss:0.023, val_acc:0.984]
Epoch [79/120    avg_loss:0.022, val_acc:0.984]
Epoch [80/120    avg_loss:0.028, val_acc:0.986]
Epoch [81/120    avg_loss:0.027, val_acc:0.986]
Epoch [82/120    avg_loss:0.018, val_acc:0.984]
Epoch [83/120    avg_loss:0.030, val_acc:0.984]
Epoch [84/120    avg_loss:0.028, val_acc:0.988]
Epoch [85/120    avg_loss:0.021, val_acc:0.986]
Epoch [86/120    avg_loss:0.047, val_acc:0.988]
Epoch [87/120    avg_loss:0.025, val_acc:0.986]
Epoch [88/120    avg_loss:0.019, val_acc:0.988]
Epoch [89/120    avg_loss:0.028, val_acc:0.988]
Epoch [90/120    avg_loss:0.026, val_acc:0.988]
Epoch [91/120    avg_loss:0.021, val_acc:0.984]
Epoch [92/120    avg_loss:0.022, val_acc:0.986]
Epoch [93/120    avg_loss:0.024, val_acc:0.988]
Epoch [94/120    avg_loss:0.016, val_acc:0.988]
Epoch [95/120    avg_loss:0.033, val_acc:0.986]
Epoch [96/120    avg_loss:0.021, val_acc:0.984]
Epoch [97/120    avg_loss:0.044, val_acc:0.988]
Epoch [98/120    avg_loss:0.022, val_acc:0.986]
Epoch [99/120    avg_loss:0.036, val_acc:0.988]
Epoch [100/120    avg_loss:0.024, val_acc:0.986]
Epoch [101/120    avg_loss:0.020, val_acc:0.986]
Epoch [102/120    avg_loss:0.021, val_acc:0.988]
Epoch [103/120    avg_loss:0.019, val_acc:0.988]
Epoch [104/120    avg_loss:0.021, val_acc:0.986]
Epoch [105/120    avg_loss:0.025, val_acc:0.988]
Epoch [106/120    avg_loss:0.039, val_acc:0.986]
Epoch [107/120    avg_loss:0.032, val_acc:0.986]
Epoch [108/120    avg_loss:0.018, val_acc:0.986]
Epoch [109/120    avg_loss:0.027, val_acc:0.988]
Epoch [110/120    avg_loss:0.015, val_acc:0.988]
Epoch [111/120    avg_loss:0.024, val_acc:0.986]
Epoch [112/120    avg_loss:0.017, val_acc:0.986]
Epoch [113/120    avg_loss:0.018, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.016, val_acc:0.988]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.025, val_acc:0.988]
Epoch [118/120    avg_loss:0.023, val_acc:0.988]
Epoch [119/120    avg_loss:0.013, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 217   8   0   0   0   3   2   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   3 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97949886 0.97091723 0.93110647 0.92418773
 0.99019608 0.99470899 0.99614891 0.9978678  1.         0.99603699
 0.98779134 1.        ]

Kappa:
0.9881301732766922
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc1a639780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.227, val_acc:0.592]
Epoch [2/120    avg_loss:1.535, val_acc:0.639]
Epoch [3/120    avg_loss:1.169, val_acc:0.826]
Epoch [4/120    avg_loss:0.845, val_acc:0.834]
Epoch [5/120    avg_loss:0.733, val_acc:0.840]
Epoch [6/120    avg_loss:0.719, val_acc:0.854]
Epoch [7/120    avg_loss:0.566, val_acc:0.826]
Epoch [8/120    avg_loss:0.543, val_acc:0.844]
Epoch [9/120    avg_loss:0.556, val_acc:0.898]
Epoch [10/120    avg_loss:0.369, val_acc:0.916]
Epoch [11/120    avg_loss:0.369, val_acc:0.930]
Epoch [12/120    avg_loss:0.309, val_acc:0.910]
Epoch [13/120    avg_loss:0.424, val_acc:0.924]
Epoch [14/120    avg_loss:0.384, val_acc:0.904]
Epoch [15/120    avg_loss:0.347, val_acc:0.924]
Epoch [16/120    avg_loss:0.294, val_acc:0.922]
Epoch [17/120    avg_loss:0.239, val_acc:0.947]
Epoch [18/120    avg_loss:0.211, val_acc:0.934]
Epoch [19/120    avg_loss:0.224, val_acc:0.936]
Epoch [20/120    avg_loss:0.221, val_acc:0.910]
Epoch [21/120    avg_loss:0.233, val_acc:0.951]
Epoch [22/120    avg_loss:0.309, val_acc:0.938]
Epoch [23/120    avg_loss:0.270, val_acc:0.934]
Epoch [24/120    avg_loss:0.220, val_acc:0.949]
Epoch [25/120    avg_loss:0.274, val_acc:0.945]
Epoch [26/120    avg_loss:0.269, val_acc:0.924]
Epoch [27/120    avg_loss:0.235, val_acc:0.959]
Epoch [28/120    avg_loss:0.227, val_acc:0.969]
Epoch [29/120    avg_loss:0.244, val_acc:0.957]
Epoch [30/120    avg_loss:0.138, val_acc:0.951]
Epoch [31/120    avg_loss:0.164, val_acc:0.955]
Epoch [32/120    avg_loss:0.132, val_acc:0.973]
Epoch [33/120    avg_loss:0.119, val_acc:0.959]
Epoch [34/120    avg_loss:0.150, val_acc:0.979]
Epoch [35/120    avg_loss:0.153, val_acc:0.953]
Epoch [36/120    avg_loss:0.130, val_acc:0.975]
Epoch [37/120    avg_loss:0.111, val_acc:0.973]
Epoch [38/120    avg_loss:0.113, val_acc:0.977]
Epoch [39/120    avg_loss:0.130, val_acc:0.973]
Epoch [40/120    avg_loss:0.080, val_acc:0.979]
Epoch [41/120    avg_loss:0.135, val_acc:0.975]
Epoch [42/120    avg_loss:0.096, val_acc:0.971]
Epoch [43/120    avg_loss:0.176, val_acc:0.957]
Epoch [44/120    avg_loss:0.128, val_acc:0.975]
Epoch [45/120    avg_loss:0.102, val_acc:0.975]
Epoch [46/120    avg_loss:0.082, val_acc:0.967]
Epoch [47/120    avg_loss:0.097, val_acc:0.979]
Epoch [48/120    avg_loss:0.104, val_acc:0.965]
Epoch [49/120    avg_loss:0.080, val_acc:0.973]
Epoch [50/120    avg_loss:0.094, val_acc:0.979]
Epoch [51/120    avg_loss:0.084, val_acc:0.959]
Epoch [52/120    avg_loss:0.117, val_acc:0.969]
Epoch [53/120    avg_loss:0.126, val_acc:0.953]
Epoch [54/120    avg_loss:0.077, val_acc:0.973]
Epoch [55/120    avg_loss:0.078, val_acc:0.977]
Epoch [56/120    avg_loss:0.067, val_acc:0.980]
Epoch [57/120    avg_loss:0.077, val_acc:0.969]
Epoch [58/120    avg_loss:0.095, val_acc:0.930]
Epoch [59/120    avg_loss:0.123, val_acc:0.955]
Epoch [60/120    avg_loss:0.086, val_acc:0.977]
Epoch [61/120    avg_loss:0.106, val_acc:0.961]
Epoch [62/120    avg_loss:0.074, val_acc:0.979]
Epoch [63/120    avg_loss:0.056, val_acc:0.975]
Epoch [64/120    avg_loss:0.059, val_acc:0.973]
Epoch [65/120    avg_loss:0.046, val_acc:0.977]
Epoch [66/120    avg_loss:0.037, val_acc:0.986]
Epoch [67/120    avg_loss:0.045, val_acc:0.986]
Epoch [68/120    avg_loss:0.050, val_acc:0.975]
Epoch [69/120    avg_loss:0.067, val_acc:0.984]
Epoch [70/120    avg_loss:0.050, val_acc:0.979]
Epoch [71/120    avg_loss:0.076, val_acc:0.975]
Epoch [72/120    avg_loss:0.048, val_acc:0.982]
Epoch [73/120    avg_loss:0.065, val_acc:0.977]
Epoch [74/120    avg_loss:0.056, val_acc:0.986]
Epoch [75/120    avg_loss:0.030, val_acc:0.982]
Epoch [76/120    avg_loss:0.027, val_acc:0.984]
Epoch [77/120    avg_loss:0.040, val_acc:0.990]
Epoch [78/120    avg_loss:0.040, val_acc:0.982]
Epoch [79/120    avg_loss:0.079, val_acc:0.984]
Epoch [80/120    avg_loss:0.047, val_acc:0.982]
Epoch [81/120    avg_loss:0.066, val_acc:0.982]
Epoch [82/120    avg_loss:0.048, val_acc:0.980]
Epoch [83/120    avg_loss:0.035, val_acc:0.980]
Epoch [84/120    avg_loss:0.050, val_acc:0.984]
Epoch [85/120    avg_loss:0.031, val_acc:0.979]
Epoch [86/120    avg_loss:0.027, val_acc:0.982]
Epoch [87/120    avg_loss:0.014, val_acc:0.982]
Epoch [88/120    avg_loss:0.020, val_acc:0.986]
Epoch [89/120    avg_loss:0.022, val_acc:0.982]
Epoch [90/120    avg_loss:0.023, val_acc:0.982]
Epoch [91/120    avg_loss:0.015, val_acc:0.984]
Epoch [92/120    avg_loss:0.036, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.017, val_acc:0.986]
Epoch [97/120    avg_loss:0.027, val_acc:0.986]
Epoch [98/120    avg_loss:0.012, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.990]
Epoch [101/120    avg_loss:0.012, val_acc:0.990]
Epoch [102/120    avg_loss:0.011, val_acc:0.990]
Epoch [103/120    avg_loss:0.016, val_acc:0.988]
Epoch [104/120    avg_loss:0.018, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.988]
Epoch [112/120    avg_loss:0.017, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.984]
Epoch [114/120    avg_loss:0.018, val_acc:0.984]
Epoch [115/120    avg_loss:0.016, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   5   0   0   0   0   0   0   1   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0  10   0   0   2   0 194   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0   0 832]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99275362 0.99095023 0.98454746 0.94646681 0.94736842
 0.97       1.         0.99742931 1.         1.         1.
 0.9944629  0.99879952]

Kappa:
0.9902655772658477
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a9ad12710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.190, val_acc:0.585]
Epoch [2/120    avg_loss:1.399, val_acc:0.698]
Epoch [3/120    avg_loss:1.049, val_acc:0.750]
Epoch [4/120    avg_loss:0.924, val_acc:0.804]
Epoch [5/120    avg_loss:0.722, val_acc:0.852]
Epoch [6/120    avg_loss:0.556, val_acc:0.894]
Epoch [7/120    avg_loss:0.538, val_acc:0.871]
Epoch [8/120    avg_loss:0.661, val_acc:0.869]
Epoch [9/120    avg_loss:0.433, val_acc:0.923]
Epoch [10/120    avg_loss:0.397, val_acc:0.927]
Epoch [11/120    avg_loss:0.354, val_acc:0.923]
Epoch [12/120    avg_loss:0.374, val_acc:0.927]
Epoch [13/120    avg_loss:0.307, val_acc:0.942]
Epoch [14/120    avg_loss:0.259, val_acc:0.950]
Epoch [15/120    avg_loss:0.356, val_acc:0.933]
Epoch [16/120    avg_loss:0.296, val_acc:0.931]
Epoch [17/120    avg_loss:0.214, val_acc:0.938]
Epoch [18/120    avg_loss:0.290, val_acc:0.946]
Epoch [19/120    avg_loss:0.235, val_acc:0.917]
Epoch [20/120    avg_loss:0.233, val_acc:0.950]
Epoch [21/120    avg_loss:0.231, val_acc:0.950]
Epoch [22/120    avg_loss:0.252, val_acc:0.944]
Epoch [23/120    avg_loss:0.238, val_acc:0.956]
Epoch [24/120    avg_loss:0.284, val_acc:0.935]
Epoch [25/120    avg_loss:0.226, val_acc:0.944]
Epoch [26/120    avg_loss:0.174, val_acc:0.940]
Epoch [27/120    avg_loss:0.193, val_acc:0.952]
Epoch [28/120    avg_loss:0.156, val_acc:0.969]
Epoch [29/120    avg_loss:0.176, val_acc:0.969]
Epoch [30/120    avg_loss:0.186, val_acc:0.965]
Epoch [31/120    avg_loss:0.205, val_acc:0.956]
Epoch [32/120    avg_loss:0.140, val_acc:0.960]
Epoch [33/120    avg_loss:0.108, val_acc:0.971]
Epoch [34/120    avg_loss:0.168, val_acc:0.935]
Epoch [35/120    avg_loss:0.120, val_acc:0.952]
Epoch [36/120    avg_loss:0.115, val_acc:0.965]
Epoch [37/120    avg_loss:0.144, val_acc:0.952]
Epoch [38/120    avg_loss:0.087, val_acc:0.956]
Epoch [39/120    avg_loss:0.139, val_acc:0.940]
Epoch [40/120    avg_loss:0.182, val_acc:0.967]
Epoch [41/120    avg_loss:0.135, val_acc:0.958]
Epoch [42/120    avg_loss:0.144, val_acc:0.975]
Epoch [43/120    avg_loss:0.128, val_acc:0.958]
Epoch [44/120    avg_loss:0.089, val_acc:0.942]
Epoch [45/120    avg_loss:0.109, val_acc:0.967]
Epoch [46/120    avg_loss:0.078, val_acc:0.960]
Epoch [47/120    avg_loss:0.193, val_acc:0.925]
Epoch [48/120    avg_loss:0.133, val_acc:0.977]
Epoch [49/120    avg_loss:0.105, val_acc:0.973]
Epoch [50/120    avg_loss:0.079, val_acc:0.969]
Epoch [51/120    avg_loss:0.057, val_acc:0.979]
Epoch [52/120    avg_loss:0.057, val_acc:0.977]
Epoch [53/120    avg_loss:0.061, val_acc:0.960]
Epoch [54/120    avg_loss:0.104, val_acc:0.975]
Epoch [55/120    avg_loss:0.092, val_acc:0.967]
Epoch [56/120    avg_loss:0.069, val_acc:0.975]
Epoch [57/120    avg_loss:0.052, val_acc:0.969]
Epoch [58/120    avg_loss:0.093, val_acc:0.967]
Epoch [59/120    avg_loss:0.070, val_acc:0.979]
Epoch [60/120    avg_loss:0.052, val_acc:0.969]
Epoch [61/120    avg_loss:0.053, val_acc:0.965]
Epoch [62/120    avg_loss:0.049, val_acc:0.975]
Epoch [63/120    avg_loss:0.044, val_acc:0.981]
Epoch [64/120    avg_loss:0.052, val_acc:0.979]
Epoch [65/120    avg_loss:0.054, val_acc:0.975]
Epoch [66/120    avg_loss:0.074, val_acc:0.971]
Epoch [67/120    avg_loss:0.057, val_acc:0.969]
Epoch [68/120    avg_loss:0.021, val_acc:0.988]
Epoch [69/120    avg_loss:0.031, val_acc:0.992]
Epoch [70/120    avg_loss:0.043, val_acc:0.975]
Epoch [71/120    avg_loss:0.049, val_acc:0.979]
Epoch [72/120    avg_loss:0.042, val_acc:0.977]
Epoch [73/120    avg_loss:0.066, val_acc:0.975]
Epoch [74/120    avg_loss:0.057, val_acc:0.967]
Epoch [75/120    avg_loss:0.089, val_acc:0.967]
Epoch [76/120    avg_loss:0.076, val_acc:0.969]
Epoch [77/120    avg_loss:0.049, val_acc:0.979]
Epoch [78/120    avg_loss:0.057, val_acc:0.971]
Epoch [79/120    avg_loss:0.035, val_acc:0.971]
Epoch [80/120    avg_loss:0.057, val_acc:0.985]
Epoch [81/120    avg_loss:0.022, val_acc:0.969]
Epoch [82/120    avg_loss:0.026, val_acc:0.979]
Epoch [83/120    avg_loss:0.023, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.979]
Epoch [85/120    avg_loss:0.023, val_acc:0.977]
Epoch [86/120    avg_loss:0.016, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.977]
Epoch [88/120    avg_loss:0.017, val_acc:0.977]
Epoch [89/120    avg_loss:0.018, val_acc:0.977]
Epoch [90/120    avg_loss:0.015, val_acc:0.979]
Epoch [91/120    avg_loss:0.022, val_acc:0.981]
Epoch [92/120    avg_loss:0.030, val_acc:0.979]
Epoch [93/120    avg_loss:0.016, val_acc:0.979]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.016, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.981]
Epoch [100/120    avg_loss:0.020, val_acc:0.981]
Epoch [101/120    avg_loss:0.029, val_acc:0.981]
Epoch [102/120    avg_loss:0.019, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.981]
Epoch [104/120    avg_loss:0.011, val_acc:0.981]
Epoch [105/120    avg_loss:0.017, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.009, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.981]
Epoch [113/120    avg_loss:0.018, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.010, val_acc:0.981]
Epoch [120/120    avg_loss:0.011, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   3   0   0   0   0   2   0]
 [  0   0   0 215  11   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.99854227 0.97716895 0.96629213 0.91220557 0.89583333
 0.99512195 0.98429319 0.99614891 0.99893276 1.         1.
 0.99224806 1.        ]

Kappa:
0.9864687612273142
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c7e60b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.261, val_acc:0.598]
Epoch [2/120    avg_loss:1.491, val_acc:0.689]
Epoch [3/120    avg_loss:1.069, val_acc:0.717]
Epoch [4/120    avg_loss:0.912, val_acc:0.742]
Epoch [5/120    avg_loss:0.779, val_acc:0.832]
Epoch [6/120    avg_loss:0.665, val_acc:0.775]
Epoch [7/120    avg_loss:0.593, val_acc:0.885]
Epoch [8/120    avg_loss:0.468, val_acc:0.893]
Epoch [9/120    avg_loss:0.382, val_acc:0.904]
Epoch [10/120    avg_loss:0.326, val_acc:0.881]
Epoch [11/120    avg_loss:0.388, val_acc:0.908]
Epoch [12/120    avg_loss:0.352, val_acc:0.926]
Epoch [13/120    avg_loss:0.337, val_acc:0.916]
Epoch [14/120    avg_loss:0.275, val_acc:0.857]
Epoch [15/120    avg_loss:0.304, val_acc:0.926]
Epoch [16/120    avg_loss:0.317, val_acc:0.916]
Epoch [17/120    avg_loss:0.339, val_acc:0.922]
Epoch [18/120    avg_loss:0.357, val_acc:0.896]
Epoch [19/120    avg_loss:0.308, val_acc:0.936]
Epoch [20/120    avg_loss:0.273, val_acc:0.910]
Epoch [21/120    avg_loss:0.265, val_acc:0.928]
Epoch [22/120    avg_loss:0.295, val_acc:0.924]
Epoch [23/120    avg_loss:0.228, val_acc:0.930]
Epoch [24/120    avg_loss:0.229, val_acc:0.949]
Epoch [25/120    avg_loss:0.270, val_acc:0.936]
Epoch [26/120    avg_loss:0.205, val_acc:0.943]
Epoch [27/120    avg_loss:0.181, val_acc:0.939]
Epoch [28/120    avg_loss:0.286, val_acc:0.943]
Epoch [29/120    avg_loss:0.199, val_acc:0.924]
Epoch [30/120    avg_loss:0.258, val_acc:0.967]
Epoch [31/120    avg_loss:0.213, val_acc:0.941]
Epoch [32/120    avg_loss:0.148, val_acc:0.957]
Epoch [33/120    avg_loss:0.099, val_acc:0.965]
Epoch [34/120    avg_loss:0.097, val_acc:0.969]
Epoch [35/120    avg_loss:0.113, val_acc:0.961]
Epoch [36/120    avg_loss:0.117, val_acc:0.955]
Epoch [37/120    avg_loss:0.179, val_acc:0.951]
Epoch [38/120    avg_loss:0.202, val_acc:0.947]
Epoch [39/120    avg_loss:0.167, val_acc:0.961]
Epoch [40/120    avg_loss:0.082, val_acc:0.961]
Epoch [41/120    avg_loss:0.065, val_acc:0.963]
Epoch [42/120    avg_loss:0.049, val_acc:0.967]
Epoch [43/120    avg_loss:0.065, val_acc:0.947]
Epoch [44/120    avg_loss:0.093, val_acc:0.971]
Epoch [45/120    avg_loss:0.089, val_acc:0.963]
Epoch [46/120    avg_loss:0.127, val_acc:0.971]
Epoch [47/120    avg_loss:0.111, val_acc:0.965]
Epoch [48/120    avg_loss:0.059, val_acc:0.963]
Epoch [49/120    avg_loss:0.078, val_acc:0.973]
Epoch [50/120    avg_loss:0.049, val_acc:0.967]
Epoch [51/120    avg_loss:0.064, val_acc:0.969]
Epoch [52/120    avg_loss:0.077, val_acc:0.953]
Epoch [53/120    avg_loss:0.075, val_acc:0.918]
Epoch [54/120    avg_loss:0.105, val_acc:0.977]
Epoch [55/120    avg_loss:0.120, val_acc:0.973]
Epoch [56/120    avg_loss:0.077, val_acc:0.967]
Epoch [57/120    avg_loss:0.124, val_acc:0.971]
Epoch [58/120    avg_loss:0.076, val_acc:0.965]
Epoch [59/120    avg_loss:0.056, val_acc:0.977]
Epoch [60/120    avg_loss:0.067, val_acc:0.973]
Epoch [61/120    avg_loss:0.060, val_acc:0.967]
Epoch [62/120    avg_loss:0.042, val_acc:0.961]
Epoch [63/120    avg_loss:0.059, val_acc:0.969]
Epoch [64/120    avg_loss:0.069, val_acc:0.967]
Epoch [65/120    avg_loss:0.044, val_acc:0.977]
Epoch [66/120    avg_loss:0.045, val_acc:0.977]
Epoch [67/120    avg_loss:0.046, val_acc:0.971]
Epoch [68/120    avg_loss:0.050, val_acc:0.971]
Epoch [69/120    avg_loss:0.115, val_acc:0.969]
Epoch [70/120    avg_loss:0.057, val_acc:0.977]
Epoch [71/120    avg_loss:0.080, val_acc:0.959]
Epoch [72/120    avg_loss:0.074, val_acc:0.975]
Epoch [73/120    avg_loss:0.034, val_acc:0.967]
Epoch [74/120    avg_loss:0.058, val_acc:0.959]
Epoch [75/120    avg_loss:0.087, val_acc:0.969]
Epoch [76/120    avg_loss:0.061, val_acc:0.977]
Epoch [77/120    avg_loss:0.082, val_acc:0.980]
Epoch [78/120    avg_loss:0.046, val_acc:0.969]
Epoch [79/120    avg_loss:0.041, val_acc:0.979]
Epoch [80/120    avg_loss:0.031, val_acc:0.980]
Epoch [81/120    avg_loss:0.022, val_acc:0.986]
Epoch [82/120    avg_loss:0.024, val_acc:0.986]
Epoch [83/120    avg_loss:0.021, val_acc:0.979]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.028, val_acc:0.982]
Epoch [86/120    avg_loss:0.016, val_acc:0.980]
Epoch [87/120    avg_loss:0.019, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.982]
Epoch [89/120    avg_loss:0.066, val_acc:0.975]
Epoch [90/120    avg_loss:0.052, val_acc:0.980]
Epoch [91/120    avg_loss:0.052, val_acc:0.979]
Epoch [92/120    avg_loss:0.043, val_acc:0.977]
Epoch [93/120    avg_loss:0.051, val_acc:0.980]
Epoch [94/120    avg_loss:0.045, val_acc:0.975]
Epoch [95/120    avg_loss:0.072, val_acc:0.979]
Epoch [96/120    avg_loss:0.021, val_acc:0.979]
Epoch [97/120    avg_loss:0.034, val_acc:0.979]
Epoch [98/120    avg_loss:0.017, val_acc:0.980]
Epoch [99/120    avg_loss:0.024, val_acc:0.980]
Epoch [100/120    avg_loss:0.018, val_acc:0.980]
Epoch [101/120    avg_loss:0.018, val_acc:0.980]
Epoch [102/120    avg_loss:0.018, val_acc:0.982]
Epoch [103/120    avg_loss:0.017, val_acc:0.982]
Epoch [104/120    avg_loss:0.030, val_acc:0.984]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.986]
Epoch [108/120    avg_loss:0.016, val_acc:0.986]
Epoch [109/120    avg_loss:0.017, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.041, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.018, val_acc:0.984]
Epoch [115/120    avg_loss:0.020, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.980]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   0   0   0   0   0   3   0]
 [  0   0   0 222   7   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   2   0   0   3   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   9 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99854227 0.98181818 0.98230088 0.92537313 0.9122807
 0.98771499 1.         0.998713   1.         1.         0.98820446
 0.98100559 1.        ]

Kappa:
0.9869437450795457
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5cbe36780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.196, val_acc:0.588]
Epoch [2/120    avg_loss:1.462, val_acc:0.762]
Epoch [3/120    avg_loss:1.068, val_acc:0.736]
Epoch [4/120    avg_loss:0.921, val_acc:0.793]
Epoch [5/120    avg_loss:0.897, val_acc:0.811]
Epoch [6/120    avg_loss:0.700, val_acc:0.834]
Epoch [7/120    avg_loss:0.649, val_acc:0.832]
Epoch [8/120    avg_loss:0.564, val_acc:0.871]
Epoch [9/120    avg_loss:0.535, val_acc:0.902]
Epoch [10/120    avg_loss:0.447, val_acc:0.900]
Epoch [11/120    avg_loss:0.455, val_acc:0.914]
Epoch [12/120    avg_loss:0.396, val_acc:0.889]
Epoch [13/120    avg_loss:0.393, val_acc:0.930]
Epoch [14/120    avg_loss:0.474, val_acc:0.891]
Epoch [15/120    avg_loss:0.337, val_acc:0.930]
Epoch [16/120    avg_loss:0.377, val_acc:0.934]
Epoch [17/120    avg_loss:0.241, val_acc:0.920]
Epoch [18/120    avg_loss:0.219, val_acc:0.941]
Epoch [19/120    avg_loss:0.248, val_acc:0.943]
Epoch [20/120    avg_loss:0.240, val_acc:0.930]
Epoch [21/120    avg_loss:0.234, val_acc:0.936]
Epoch [22/120    avg_loss:0.223, val_acc:0.910]
Epoch [23/120    avg_loss:0.209, val_acc:0.959]
Epoch [24/120    avg_loss:0.159, val_acc:0.951]
Epoch [25/120    avg_loss:0.191, val_acc:0.963]
Epoch [26/120    avg_loss:0.187, val_acc:0.953]
Epoch [27/120    avg_loss:0.159, val_acc:0.965]
Epoch [28/120    avg_loss:0.172, val_acc:0.945]
Epoch [29/120    avg_loss:0.183, val_acc:0.953]
Epoch [30/120    avg_loss:0.198, val_acc:0.955]
Epoch [31/120    avg_loss:0.173, val_acc:0.941]
Epoch [32/120    avg_loss:0.175, val_acc:0.963]
Epoch [33/120    avg_loss:0.161, val_acc:0.949]
Epoch [34/120    avg_loss:0.177, val_acc:0.969]
Epoch [35/120    avg_loss:0.164, val_acc:0.963]
Epoch [36/120    avg_loss:0.124, val_acc:0.961]
Epoch [37/120    avg_loss:0.133, val_acc:0.973]
Epoch [38/120    avg_loss:0.088, val_acc:0.973]
Epoch [39/120    avg_loss:0.096, val_acc:0.967]
Epoch [40/120    avg_loss:0.081, val_acc:0.975]
Epoch [41/120    avg_loss:0.089, val_acc:0.982]
Epoch [42/120    avg_loss:0.077, val_acc:0.961]
Epoch [43/120    avg_loss:0.147, val_acc:0.965]
Epoch [44/120    avg_loss:0.174, val_acc:0.969]
Epoch [45/120    avg_loss:0.119, val_acc:0.957]
Epoch [46/120    avg_loss:0.117, val_acc:0.975]
Epoch [47/120    avg_loss:0.081, val_acc:0.963]
Epoch [48/120    avg_loss:0.142, val_acc:0.963]
Epoch [49/120    avg_loss:0.046, val_acc:0.967]
Epoch [50/120    avg_loss:0.107, val_acc:0.967]
Epoch [51/120    avg_loss:0.107, val_acc:0.955]
Epoch [52/120    avg_loss:0.142, val_acc:0.967]
Epoch [53/120    avg_loss:0.123, val_acc:0.969]
Epoch [54/120    avg_loss:0.101, val_acc:0.975]
Epoch [55/120    avg_loss:0.053, val_acc:0.969]
Epoch [56/120    avg_loss:0.046, val_acc:0.971]
Epoch [57/120    avg_loss:0.053, val_acc:0.975]
Epoch [58/120    avg_loss:0.062, val_acc:0.977]
Epoch [59/120    avg_loss:0.063, val_acc:0.973]
Epoch [60/120    avg_loss:0.058, val_acc:0.979]
Epoch [61/120    avg_loss:0.053, val_acc:0.980]
Epoch [62/120    avg_loss:0.040, val_acc:0.979]
Epoch [63/120    avg_loss:0.053, val_acc:0.979]
Epoch [64/120    avg_loss:0.036, val_acc:0.980]
Epoch [65/120    avg_loss:0.058, val_acc:0.982]
Epoch [66/120    avg_loss:0.038, val_acc:0.980]
Epoch [67/120    avg_loss:0.049, val_acc:0.977]
Epoch [68/120    avg_loss:0.033, val_acc:0.980]
Epoch [69/120    avg_loss:0.037, val_acc:0.980]
Epoch [70/120    avg_loss:0.032, val_acc:0.980]
Epoch [71/120    avg_loss:0.053, val_acc:0.980]
Epoch [72/120    avg_loss:0.041, val_acc:0.980]
Epoch [73/120    avg_loss:0.049, val_acc:0.980]
Epoch [74/120    avg_loss:0.064, val_acc:0.973]
Epoch [75/120    avg_loss:0.038, val_acc:0.979]
Epoch [76/120    avg_loss:0.043, val_acc:0.979]
Epoch [77/120    avg_loss:0.029, val_acc:0.979]
Epoch [78/120    avg_loss:0.032, val_acc:0.979]
Epoch [79/120    avg_loss:0.037, val_acc:0.979]
Epoch [80/120    avg_loss:0.043, val_acc:0.979]
Epoch [81/120    avg_loss:0.036, val_acc:0.979]
Epoch [82/120    avg_loss:0.051, val_acc:0.979]
Epoch [83/120    avg_loss:0.032, val_acc:0.979]
Epoch [84/120    avg_loss:0.042, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.980]
Epoch [86/120    avg_loss:0.033, val_acc:0.980]
Epoch [87/120    avg_loss:0.032, val_acc:0.980]
Epoch [88/120    avg_loss:0.042, val_acc:0.980]
Epoch [89/120    avg_loss:0.034, val_acc:0.980]
Epoch [90/120    avg_loss:0.040, val_acc:0.980]
Epoch [91/120    avg_loss:0.040, val_acc:0.980]
Epoch [92/120    avg_loss:0.029, val_acc:0.980]
Epoch [93/120    avg_loss:0.046, val_acc:0.980]
Epoch [94/120    avg_loss:0.036, val_acc:0.980]
Epoch [95/120    avg_loss:0.038, val_acc:0.980]
Epoch [96/120    avg_loss:0.037, val_acc:0.980]
Epoch [97/120    avg_loss:0.035, val_acc:0.980]
Epoch [98/120    avg_loss:0.029, val_acc:0.980]
Epoch [99/120    avg_loss:0.042, val_acc:0.980]
Epoch [100/120    avg_loss:0.032, val_acc:0.980]
Epoch [101/120    avg_loss:0.044, val_acc:0.980]
Epoch [102/120    avg_loss:0.031, val_acc:0.980]
Epoch [103/120    avg_loss:0.030, val_acc:0.980]
Epoch [104/120    avg_loss:0.036, val_acc:0.980]
Epoch [105/120    avg_loss:0.036, val_acc:0.980]
Epoch [106/120    avg_loss:0.040, val_acc:0.980]
Epoch [107/120    avg_loss:0.030, val_acc:0.980]
Epoch [108/120    avg_loss:0.034, val_acc:0.980]
Epoch [109/120    avg_loss:0.041, val_acc:0.980]
Epoch [110/120    avg_loss:0.034, val_acc:0.980]
Epoch [111/120    avg_loss:0.034, val_acc:0.980]
Epoch [112/120    avg_loss:0.032, val_acc:0.980]
Epoch [113/120    avg_loss:0.025, val_acc:0.980]
Epoch [114/120    avg_loss:0.024, val_acc:0.980]
Epoch [115/120    avg_loss:0.035, val_acc:0.980]
Epoch [116/120    avg_loss:0.039, val_acc:0.980]
Epoch [117/120    avg_loss:0.024, val_acc:0.980]
Epoch [118/120    avg_loss:0.038, val_acc:0.980]
Epoch [119/120    avg_loss:0.041, val_acc:0.980]
Epoch [120/120    avg_loss:0.030, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   5   0   0   0   0   2   0]
 [  0   0   0 227   1   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0  10   0   0   5   0 191   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.57142857142857

F1 scores:
[       nan 0.99275362 0.94222222 0.99343545 0.93598234 0.92255892
 0.96221662 0.89385475 0.998713   0.99893276 1.         1.
 0.99224806 1.        ]

Kappa:
0.9840913061719586
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba7a1bd748>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.257, val_acc:0.575]
Epoch [2/120    avg_loss:1.473, val_acc:0.631]
Epoch [3/120    avg_loss:1.080, val_acc:0.777]
Epoch [4/120    avg_loss:0.881, val_acc:0.748]
Epoch [5/120    avg_loss:0.768, val_acc:0.783]
Epoch [6/120    avg_loss:0.671, val_acc:0.846]
Epoch [7/120    avg_loss:0.537, val_acc:0.871]
Epoch [8/120    avg_loss:0.514, val_acc:0.865]
Epoch [9/120    avg_loss:0.435, val_acc:0.896]
Epoch [10/120    avg_loss:0.379, val_acc:0.912]
Epoch [11/120    avg_loss:0.347, val_acc:0.908]
Epoch [12/120    avg_loss:0.290, val_acc:0.910]
Epoch [13/120    avg_loss:0.303, val_acc:0.921]
Epoch [14/120    avg_loss:0.242, val_acc:0.917]
Epoch [15/120    avg_loss:0.228, val_acc:0.940]
Epoch [16/120    avg_loss:0.329, val_acc:0.923]
Epoch [17/120    avg_loss:0.252, val_acc:0.919]
Epoch [18/120    avg_loss:0.278, val_acc:0.927]
Epoch [19/120    avg_loss:0.266, val_acc:0.915]
Epoch [20/120    avg_loss:0.294, val_acc:0.925]
Epoch [21/120    avg_loss:0.219, val_acc:0.923]
Epoch [22/120    avg_loss:0.160, val_acc:0.948]
Epoch [23/120    avg_loss:0.190, val_acc:0.946]
Epoch [24/120    avg_loss:0.184, val_acc:0.938]
Epoch [25/120    avg_loss:0.194, val_acc:0.946]
Epoch [26/120    avg_loss:0.149, val_acc:0.946]
Epoch [27/120    avg_loss:0.153, val_acc:0.944]
Epoch [28/120    avg_loss:0.172, val_acc:0.950]
Epoch [29/120    avg_loss:0.144, val_acc:0.942]
Epoch [30/120    avg_loss:0.189, val_acc:0.954]
Epoch [31/120    avg_loss:0.228, val_acc:0.935]
Epoch [32/120    avg_loss:0.243, val_acc:0.950]
Epoch [33/120    avg_loss:0.172, val_acc:0.963]
Epoch [34/120    avg_loss:0.134, val_acc:0.963]
Epoch [35/120    avg_loss:0.120, val_acc:0.956]
Epoch [36/120    avg_loss:0.125, val_acc:0.960]
Epoch [37/120    avg_loss:0.086, val_acc:0.967]
Epoch [38/120    avg_loss:0.082, val_acc:0.960]
Epoch [39/120    avg_loss:0.105, val_acc:0.954]
Epoch [40/120    avg_loss:0.111, val_acc:0.969]
Epoch [41/120    avg_loss:0.088, val_acc:0.948]
Epoch [42/120    avg_loss:0.132, val_acc:0.958]
Epoch [43/120    avg_loss:0.102, val_acc:0.952]
Epoch [44/120    avg_loss:0.130, val_acc:0.958]
Epoch [45/120    avg_loss:0.113, val_acc:0.960]
Epoch [46/120    avg_loss:0.106, val_acc:0.956]
Epoch [47/120    avg_loss:0.113, val_acc:0.963]
Epoch [48/120    avg_loss:0.138, val_acc:0.946]
Epoch [49/120    avg_loss:0.087, val_acc:0.973]
Epoch [50/120    avg_loss:0.072, val_acc:0.967]
Epoch [51/120    avg_loss:0.070, val_acc:0.971]
Epoch [52/120    avg_loss:0.060, val_acc:0.967]
Epoch [53/120    avg_loss:0.056, val_acc:0.973]
Epoch [54/120    avg_loss:0.062, val_acc:0.975]
Epoch [55/120    avg_loss:0.096, val_acc:0.960]
Epoch [56/120    avg_loss:0.096, val_acc:0.975]
Epoch [57/120    avg_loss:0.073, val_acc:0.973]
Epoch [58/120    avg_loss:0.072, val_acc:0.973]
Epoch [59/120    avg_loss:0.091, val_acc:0.965]
Epoch [60/120    avg_loss:0.040, val_acc:0.969]
Epoch [61/120    avg_loss:0.056, val_acc:0.979]
Epoch [62/120    avg_loss:0.040, val_acc:0.975]
Epoch [63/120    avg_loss:0.039, val_acc:0.963]
Epoch [64/120    avg_loss:0.088, val_acc:0.977]
Epoch [65/120    avg_loss:0.080, val_acc:0.975]
Epoch [66/120    avg_loss:0.042, val_acc:0.975]
Epoch [67/120    avg_loss:0.071, val_acc:0.958]
Epoch [68/120    avg_loss:0.095, val_acc:0.952]
Epoch [69/120    avg_loss:0.130, val_acc:0.956]
Epoch [70/120    avg_loss:0.145, val_acc:0.973]
Epoch [71/120    avg_loss:0.117, val_acc:0.952]
Epoch [72/120    avg_loss:0.109, val_acc:0.973]
Epoch [73/120    avg_loss:0.092, val_acc:0.977]
Epoch [74/120    avg_loss:0.096, val_acc:0.967]
Epoch [75/120    avg_loss:0.046, val_acc:0.967]
Epoch [76/120    avg_loss:0.045, val_acc:0.971]
Epoch [77/120    avg_loss:0.038, val_acc:0.975]
Epoch [78/120    avg_loss:0.025, val_acc:0.975]
Epoch [79/120    avg_loss:0.039, val_acc:0.977]
Epoch [80/120    avg_loss:0.049, val_acc:0.981]
Epoch [81/120    avg_loss:0.038, val_acc:0.981]
Epoch [82/120    avg_loss:0.030, val_acc:0.981]
Epoch [83/120    avg_loss:0.033, val_acc:0.981]
Epoch [84/120    avg_loss:0.028, val_acc:0.979]
Epoch [85/120    avg_loss:0.021, val_acc:0.981]
Epoch [86/120    avg_loss:0.023, val_acc:0.981]
Epoch [87/120    avg_loss:0.018, val_acc:0.981]
Epoch [88/120    avg_loss:0.023, val_acc:0.981]
Epoch [89/120    avg_loss:0.024, val_acc:0.979]
Epoch [90/120    avg_loss:0.030, val_acc:0.979]
Epoch [91/120    avg_loss:0.022, val_acc:0.979]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.027, val_acc:0.981]
Epoch [94/120    avg_loss:0.029, val_acc:0.981]
Epoch [95/120    avg_loss:0.021, val_acc:0.981]
Epoch [96/120    avg_loss:0.021, val_acc:0.981]
Epoch [97/120    avg_loss:0.020, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.981]
Epoch [99/120    avg_loss:0.027, val_acc:0.981]
Epoch [100/120    avg_loss:0.020, val_acc:0.981]
Epoch [101/120    avg_loss:0.022, val_acc:0.981]
Epoch [102/120    avg_loss:0.015, val_acc:0.981]
Epoch [103/120    avg_loss:0.018, val_acc:0.981]
Epoch [104/120    avg_loss:0.020, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.981]
Epoch [106/120    avg_loss:0.026, val_acc:0.985]
Epoch [107/120    avg_loss:0.020, val_acc:0.983]
Epoch [108/120    avg_loss:0.023, val_acc:0.983]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.014, val_acc:0.983]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.015, val_acc:0.981]
Epoch [113/120    avg_loss:0.032, val_acc:0.979]
Epoch [114/120    avg_loss:0.020, val_acc:0.979]
Epoch [115/120    avg_loss:0.019, val_acc:0.979]
Epoch [116/120    avg_loss:0.013, val_acc:0.979]
Epoch [117/120    avg_loss:0.047, val_acc:0.979]
Epoch [118/120    avg_loss:0.017, val_acc:0.979]
Epoch [119/120    avg_loss:0.019, val_acc:0.979]
Epoch [120/120    avg_loss:0.015, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 225   1   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98198198 0.98901099 0.94247788 0.91780822
 1.         0.98924731 0.99614891 0.99893276 1.         1.
 0.99224806 1.        ]

Kappa:
0.9909794659096747
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59d3fad7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.238, val_acc:0.516]
Epoch [2/120    avg_loss:1.451, val_acc:0.588]
Epoch [3/120    avg_loss:0.998, val_acc:0.814]
Epoch [4/120    avg_loss:0.825, val_acc:0.814]
Epoch [5/120    avg_loss:0.716, val_acc:0.867]
Epoch [6/120    avg_loss:0.623, val_acc:0.855]
Epoch [7/120    avg_loss:0.575, val_acc:0.824]
Epoch [8/120    avg_loss:0.456, val_acc:0.891]
Epoch [9/120    avg_loss:0.423, val_acc:0.891]
Epoch [10/120    avg_loss:0.475, val_acc:0.912]
Epoch [11/120    avg_loss:0.395, val_acc:0.916]
Epoch [12/120    avg_loss:0.271, val_acc:0.910]
Epoch [13/120    avg_loss:0.256, val_acc:0.918]
Epoch [14/120    avg_loss:0.280, val_acc:0.906]
Epoch [15/120    avg_loss:0.346, val_acc:0.926]
Epoch [16/120    avg_loss:0.293, val_acc:0.912]
Epoch [17/120    avg_loss:0.250, val_acc:0.924]
Epoch [18/120    avg_loss:0.244, val_acc:0.916]
Epoch [19/120    avg_loss:0.169, val_acc:0.926]
Epoch [20/120    avg_loss:0.183, val_acc:0.922]
Epoch [21/120    avg_loss:0.214, val_acc:0.928]
Epoch [22/120    avg_loss:0.316, val_acc:0.943]
Epoch [23/120    avg_loss:0.207, val_acc:0.951]
Epoch [24/120    avg_loss:0.202, val_acc:0.924]
Epoch [25/120    avg_loss:0.168, val_acc:0.963]
Epoch [26/120    avg_loss:0.222, val_acc:0.949]
Epoch [27/120    avg_loss:0.217, val_acc:0.949]
Epoch [28/120    avg_loss:0.124, val_acc:0.955]
Epoch [29/120    avg_loss:0.160, val_acc:0.959]
Epoch [30/120    avg_loss:0.136, val_acc:0.947]
Epoch [31/120    avg_loss:0.117, val_acc:0.957]
Epoch [32/120    avg_loss:0.128, val_acc:0.957]
Epoch [33/120    avg_loss:0.123, val_acc:0.947]
Epoch [34/120    avg_loss:0.134, val_acc:0.957]
Epoch [35/120    avg_loss:0.090, val_acc:0.955]
Epoch [36/120    avg_loss:0.083, val_acc:0.963]
Epoch [37/120    avg_loss:0.090, val_acc:0.957]
Epoch [38/120    avg_loss:0.145, val_acc:0.967]
Epoch [39/120    avg_loss:0.101, val_acc:0.963]
Epoch [40/120    avg_loss:0.090, val_acc:0.963]
Epoch [41/120    avg_loss:0.071, val_acc:0.959]
Epoch [42/120    avg_loss:0.070, val_acc:0.963]
Epoch [43/120    avg_loss:0.113, val_acc:0.943]
Epoch [44/120    avg_loss:0.115, val_acc:0.959]
Epoch [45/120    avg_loss:0.082, val_acc:0.973]
Epoch [46/120    avg_loss:0.054, val_acc:0.967]
Epoch [47/120    avg_loss:0.078, val_acc:0.961]
Epoch [48/120    avg_loss:0.088, val_acc:0.977]
Epoch [49/120    avg_loss:0.074, val_acc:0.967]
Epoch [50/120    avg_loss:0.083, val_acc:0.967]
Epoch [51/120    avg_loss:0.082, val_acc:0.967]
Epoch [52/120    avg_loss:0.066, val_acc:0.969]
Epoch [53/120    avg_loss:0.075, val_acc:0.957]
Epoch [54/120    avg_loss:0.126, val_acc:0.957]
Epoch [55/120    avg_loss:0.150, val_acc:0.947]
Epoch [56/120    avg_loss:0.124, val_acc:0.939]
Epoch [57/120    avg_loss:0.184, val_acc:0.949]
Epoch [58/120    avg_loss:0.127, val_acc:0.953]
Epoch [59/120    avg_loss:0.091, val_acc:0.947]
Epoch [60/120    avg_loss:0.058, val_acc:0.965]
Epoch [61/120    avg_loss:0.051, val_acc:0.955]
Epoch [62/120    avg_loss:0.099, val_acc:0.971]
Epoch [63/120    avg_loss:0.036, val_acc:0.965]
Epoch [64/120    avg_loss:0.029, val_acc:0.969]
Epoch [65/120    avg_loss:0.030, val_acc:0.971]
Epoch [66/120    avg_loss:0.055, val_acc:0.971]
Epoch [67/120    avg_loss:0.027, val_acc:0.973]
Epoch [68/120    avg_loss:0.038, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.973]
Epoch [70/120    avg_loss:0.038, val_acc:0.975]
Epoch [71/120    avg_loss:0.018, val_acc:0.975]
Epoch [72/120    avg_loss:0.029, val_acc:0.975]
Epoch [73/120    avg_loss:0.028, val_acc:0.975]
Epoch [74/120    avg_loss:0.041, val_acc:0.975]
Epoch [75/120    avg_loss:0.019, val_acc:0.975]
Epoch [76/120    avg_loss:0.030, val_acc:0.975]
Epoch [77/120    avg_loss:0.031, val_acc:0.975]
Epoch [78/120    avg_loss:0.015, val_acc:0.975]
Epoch [79/120    avg_loss:0.027, val_acc:0.975]
Epoch [80/120    avg_loss:0.018, val_acc:0.973]
Epoch [81/120    avg_loss:0.019, val_acc:0.975]
Epoch [82/120    avg_loss:0.019, val_acc:0.975]
Epoch [83/120    avg_loss:0.020, val_acc:0.975]
Epoch [84/120    avg_loss:0.039, val_acc:0.975]
Epoch [85/120    avg_loss:0.031, val_acc:0.975]
Epoch [86/120    avg_loss:0.030, val_acc:0.975]
Epoch [87/120    avg_loss:0.030, val_acc:0.975]
Epoch [88/120    avg_loss:0.026, val_acc:0.975]
Epoch [89/120    avg_loss:0.022, val_acc:0.975]
Epoch [90/120    avg_loss:0.029, val_acc:0.975]
Epoch [91/120    avg_loss:0.021, val_acc:0.975]
Epoch [92/120    avg_loss:0.015, val_acc:0.975]
Epoch [93/120    avg_loss:0.017, val_acc:0.975]
Epoch [94/120    avg_loss:0.039, val_acc:0.975]
Epoch [95/120    avg_loss:0.023, val_acc:0.975]
Epoch [96/120    avg_loss:0.026, val_acc:0.975]
Epoch [97/120    avg_loss:0.027, val_acc:0.975]
Epoch [98/120    avg_loss:0.021, val_acc:0.975]
Epoch [99/120    avg_loss:0.012, val_acc:0.975]
Epoch [100/120    avg_loss:0.019, val_acc:0.975]
Epoch [101/120    avg_loss:0.020, val_acc:0.975]
Epoch [102/120    avg_loss:0.017, val_acc:0.975]
Epoch [103/120    avg_loss:0.014, val_acc:0.975]
Epoch [104/120    avg_loss:0.023, val_acc:0.975]
Epoch [105/120    avg_loss:0.028, val_acc:0.975]
Epoch [106/120    avg_loss:0.028, val_acc:0.975]
Epoch [107/120    avg_loss:0.033, val_acc:0.975]
Epoch [108/120    avg_loss:0.018, val_acc:0.975]
Epoch [109/120    avg_loss:0.024, val_acc:0.975]
Epoch [110/120    avg_loss:0.021, val_acc:0.975]
Epoch [111/120    avg_loss:0.023, val_acc:0.975]
Epoch [112/120    avg_loss:0.025, val_acc:0.975]
Epoch [113/120    avg_loss:0.021, val_acc:0.975]
Epoch [114/120    avg_loss:0.041, val_acc:0.975]
Epoch [115/120    avg_loss:0.022, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.975]
Epoch [117/120    avg_loss:0.030, val_acc:0.975]
Epoch [118/120    avg_loss:0.038, val_acc:0.975]
Epoch [119/120    avg_loss:0.019, val_acc:0.975]
Epoch [120/120    avg_loss:0.016, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   3   0   0   0   0   3   0]
 [  0   0   0 224   1   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   1   0   0   2   0 203   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.99927061 0.97260274 0.98678414 0.93361884 0.9
 0.99266504 0.97894737 0.99487179 0.99893276 1.         1.
 0.99115044 1.        ]

Kappa:
0.9883674314401788
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd72c756710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.156, val_acc:0.643]
Epoch [2/120    avg_loss:1.430, val_acc:0.668]
Epoch [3/120    avg_loss:1.123, val_acc:0.807]
Epoch [4/120    avg_loss:0.906, val_acc:0.781]
Epoch [5/120    avg_loss:0.703, val_acc:0.781]
Epoch [6/120    avg_loss:0.630, val_acc:0.842]
Epoch [7/120    avg_loss:0.567, val_acc:0.838]
Epoch [8/120    avg_loss:0.522, val_acc:0.873]
Epoch [9/120    avg_loss:0.466, val_acc:0.885]
Epoch [10/120    avg_loss:0.462, val_acc:0.873]
Epoch [11/120    avg_loss:0.373, val_acc:0.898]
Epoch [12/120    avg_loss:0.320, val_acc:0.883]
Epoch [13/120    avg_loss:0.361, val_acc:0.902]
Epoch [14/120    avg_loss:0.286, val_acc:0.920]
Epoch [15/120    avg_loss:0.363, val_acc:0.850]
Epoch [16/120    avg_loss:0.344, val_acc:0.922]
Epoch [17/120    avg_loss:0.262, val_acc:0.924]
Epoch [18/120    avg_loss:0.349, val_acc:0.928]
Epoch [19/120    avg_loss:0.267, val_acc:0.887]
Epoch [20/120    avg_loss:0.387, val_acc:0.904]
Epoch [21/120    avg_loss:0.262, val_acc:0.924]
Epoch [22/120    avg_loss:0.227, val_acc:0.936]
Epoch [23/120    avg_loss:0.207, val_acc:0.889]
Epoch [24/120    avg_loss:0.185, val_acc:0.947]
Epoch [25/120    avg_loss:0.209, val_acc:0.904]
Epoch [26/120    avg_loss:0.218, val_acc:0.912]
Epoch [27/120    avg_loss:0.219, val_acc:0.949]
Epoch [28/120    avg_loss:0.146, val_acc:0.951]
Epoch [29/120    avg_loss:0.154, val_acc:0.965]
Epoch [30/120    avg_loss:0.130, val_acc:0.938]
Epoch [31/120    avg_loss:0.160, val_acc:0.941]
Epoch [32/120    avg_loss:0.149, val_acc:0.957]
Epoch [33/120    avg_loss:0.151, val_acc:0.949]
Epoch [34/120    avg_loss:0.123, val_acc:0.959]
Epoch [35/120    avg_loss:0.086, val_acc:0.961]
Epoch [36/120    avg_loss:0.082, val_acc:0.965]
Epoch [37/120    avg_loss:0.122, val_acc:0.969]
Epoch [38/120    avg_loss:0.142, val_acc:0.961]
Epoch [39/120    avg_loss:0.153, val_acc:0.953]
Epoch [40/120    avg_loss:0.102, val_acc:0.965]
Epoch [41/120    avg_loss:0.084, val_acc:0.965]
Epoch [42/120    avg_loss:0.091, val_acc:0.969]
Epoch [43/120    avg_loss:0.127, val_acc:0.965]
Epoch [44/120    avg_loss:0.096, val_acc:0.973]
Epoch [45/120    avg_loss:0.083, val_acc:0.977]
Epoch [46/120    avg_loss:0.081, val_acc:0.947]
Epoch [47/120    avg_loss:0.095, val_acc:0.959]
Epoch [48/120    avg_loss:0.091, val_acc:0.955]
Epoch [49/120    avg_loss:0.138, val_acc:0.969]
Epoch [50/120    avg_loss:0.093, val_acc:0.967]
Epoch [51/120    avg_loss:0.086, val_acc:0.963]
Epoch [52/120    avg_loss:0.051, val_acc:0.969]
Epoch [53/120    avg_loss:0.078, val_acc:0.973]
Epoch [54/120    avg_loss:0.050, val_acc:0.977]
Epoch [55/120    avg_loss:0.068, val_acc:0.969]
Epoch [56/120    avg_loss:0.072, val_acc:0.975]
Epoch [57/120    avg_loss:0.064, val_acc:0.963]
Epoch [58/120    avg_loss:0.247, val_acc:0.951]
Epoch [59/120    avg_loss:0.177, val_acc:0.922]
Epoch [60/120    avg_loss:0.206, val_acc:0.963]
Epoch [61/120    avg_loss:0.120, val_acc:0.957]
Epoch [62/120    avg_loss:0.112, val_acc:0.955]
Epoch [63/120    avg_loss:0.106, val_acc:0.941]
Epoch [64/120    avg_loss:0.091, val_acc:0.963]
Epoch [65/120    avg_loss:0.107, val_acc:0.957]
Epoch [66/120    avg_loss:0.066, val_acc:0.957]
Epoch [67/120    avg_loss:0.062, val_acc:0.967]
Epoch [68/120    avg_loss:0.056, val_acc:0.967]
Epoch [69/120    avg_loss:0.050, val_acc:0.965]
Epoch [70/120    avg_loss:0.078, val_acc:0.971]
Epoch [71/120    avg_loss:0.031, val_acc:0.975]
Epoch [72/120    avg_loss:0.031, val_acc:0.977]
Epoch [73/120    avg_loss:0.035, val_acc:0.977]
Epoch [74/120    avg_loss:0.033, val_acc:0.979]
Epoch [75/120    avg_loss:0.043, val_acc:0.979]
Epoch [76/120    avg_loss:0.032, val_acc:0.980]
Epoch [77/120    avg_loss:0.049, val_acc:0.980]
Epoch [78/120    avg_loss:0.027, val_acc:0.980]
Epoch [79/120    avg_loss:0.023, val_acc:0.980]
Epoch [80/120    avg_loss:0.037, val_acc:0.980]
Epoch [81/120    avg_loss:0.022, val_acc:0.980]
Epoch [82/120    avg_loss:0.030, val_acc:0.980]
Epoch [83/120    avg_loss:0.032, val_acc:0.980]
Epoch [84/120    avg_loss:0.033, val_acc:0.980]
Epoch [85/120    avg_loss:0.026, val_acc:0.980]
Epoch [86/120    avg_loss:0.038, val_acc:0.980]
Epoch [87/120    avg_loss:0.037, val_acc:0.980]
Epoch [88/120    avg_loss:0.039, val_acc:0.980]
Epoch [89/120    avg_loss:0.030, val_acc:0.980]
Epoch [90/120    avg_loss:0.018, val_acc:0.980]
Epoch [91/120    avg_loss:0.033, val_acc:0.980]
Epoch [92/120    avg_loss:0.034, val_acc:0.980]
Epoch [93/120    avg_loss:0.039, val_acc:0.980]
Epoch [94/120    avg_loss:0.030, val_acc:0.980]
Epoch [95/120    avg_loss:0.049, val_acc:0.980]
Epoch [96/120    avg_loss:0.025, val_acc:0.979]
Epoch [97/120    avg_loss:0.036, val_acc:0.980]
Epoch [98/120    avg_loss:0.029, val_acc:0.979]
Epoch [99/120    avg_loss:0.031, val_acc:0.980]
Epoch [100/120    avg_loss:0.034, val_acc:0.982]
Epoch [101/120    avg_loss:0.030, val_acc:0.982]
Epoch [102/120    avg_loss:0.025, val_acc:0.984]
Epoch [103/120    avg_loss:0.031, val_acc:0.982]
Epoch [104/120    avg_loss:0.020, val_acc:0.982]
Epoch [105/120    avg_loss:0.031, val_acc:0.980]
Epoch [106/120    avg_loss:0.030, val_acc:0.980]
Epoch [107/120    avg_loss:0.039, val_acc:0.980]
Epoch [108/120    avg_loss:0.018, val_acc:0.980]
Epoch [109/120    avg_loss:0.020, val_acc:0.980]
Epoch [110/120    avg_loss:0.023, val_acc:0.982]
Epoch [111/120    avg_loss:0.025, val_acc:0.982]
Epoch [112/120    avg_loss:0.022, val_acc:0.980]
Epoch [113/120    avg_loss:0.024, val_acc:0.980]
Epoch [114/120    avg_loss:0.024, val_acc:0.982]
Epoch [115/120    avg_loss:0.035, val_acc:0.984]
Epoch [116/120    avg_loss:0.019, val_acc:0.984]
Epoch [117/120    avg_loss:0.020, val_acc:0.982]
Epoch [118/120    avg_loss:0.015, val_acc:0.982]
Epoch [119/120    avg_loss:0.022, val_acc:0.982]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   2   0   0   0   0   3   0]
 [  0   0   0 215  14   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 216   8   0   0   0   0   0   0   3   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   6   0   0   0   0 200   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 0.99563953 0.96179775 0.96629213 0.92307692 0.93379791
 0.98522167 0.95081967 0.998713   1.         1.         1.
 0.98787211 1.        ]

Kappa:
0.9857541773444947
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5be088c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.180, val_acc:0.660]
Epoch [2/120    avg_loss:1.440, val_acc:0.709]
Epoch [3/120    avg_loss:1.083, val_acc:0.809]
Epoch [4/120    avg_loss:0.843, val_acc:0.727]
Epoch [5/120    avg_loss:0.706, val_acc:0.842]
Epoch [6/120    avg_loss:0.644, val_acc:0.879]
Epoch [7/120    avg_loss:0.544, val_acc:0.881]
Epoch [8/120    avg_loss:0.518, val_acc:0.887]
Epoch [9/120    avg_loss:0.406, val_acc:0.906]
Epoch [10/120    avg_loss:0.334, val_acc:0.906]
Epoch [11/120    avg_loss:0.342, val_acc:0.916]
Epoch [12/120    avg_loss:0.378, val_acc:0.914]
Epoch [13/120    avg_loss:0.402, val_acc:0.924]
Epoch [14/120    avg_loss:0.336, val_acc:0.824]
Epoch [15/120    avg_loss:0.361, val_acc:0.887]
Epoch [16/120    avg_loss:0.355, val_acc:0.943]
Epoch [17/120    avg_loss:0.282, val_acc:0.926]
Epoch [18/120    avg_loss:0.306, val_acc:0.910]
Epoch [19/120    avg_loss:0.239, val_acc:0.928]
Epoch [20/120    avg_loss:0.262, val_acc:0.922]
Epoch [21/120    avg_loss:0.312, val_acc:0.943]
Epoch [22/120    avg_loss:0.179, val_acc:0.957]
Epoch [23/120    avg_loss:0.254, val_acc:0.934]
Epoch [24/120    avg_loss:0.163, val_acc:0.965]
Epoch [25/120    avg_loss:0.168, val_acc:0.975]
Epoch [26/120    avg_loss:0.138, val_acc:0.949]
Epoch [27/120    avg_loss:0.130, val_acc:0.969]
Epoch [28/120    avg_loss:0.165, val_acc:0.967]
Epoch [29/120    avg_loss:0.147, val_acc:0.967]
Epoch [30/120    avg_loss:0.137, val_acc:0.953]
Epoch [31/120    avg_loss:0.145, val_acc:0.967]
Epoch [32/120    avg_loss:0.158, val_acc:0.957]
Epoch [33/120    avg_loss:0.157, val_acc:0.957]
Epoch [34/120    avg_loss:0.132, val_acc:0.959]
Epoch [35/120    avg_loss:0.177, val_acc:0.971]
Epoch [36/120    avg_loss:0.166, val_acc:0.949]
Epoch [37/120    avg_loss:0.215, val_acc:0.965]
Epoch [38/120    avg_loss:0.133, val_acc:0.973]
Epoch [39/120    avg_loss:0.105, val_acc:0.975]
Epoch [40/120    avg_loss:0.064, val_acc:0.977]
Epoch [41/120    avg_loss:0.096, val_acc:0.977]
Epoch [42/120    avg_loss:0.102, val_acc:0.980]
Epoch [43/120    avg_loss:0.064, val_acc:0.982]
Epoch [44/120    avg_loss:0.068, val_acc:0.982]
Epoch [45/120    avg_loss:0.054, val_acc:0.984]
Epoch [46/120    avg_loss:0.062, val_acc:0.982]
Epoch [47/120    avg_loss:0.084, val_acc:0.982]
Epoch [48/120    avg_loss:0.066, val_acc:0.982]
Epoch [49/120    avg_loss:0.053, val_acc:0.982]
Epoch [50/120    avg_loss:0.065, val_acc:0.982]
Epoch [51/120    avg_loss:0.084, val_acc:0.982]
Epoch [52/120    avg_loss:0.058, val_acc:0.984]
Epoch [53/120    avg_loss:0.048, val_acc:0.982]
Epoch [54/120    avg_loss:0.057, val_acc:0.984]
Epoch [55/120    avg_loss:0.081, val_acc:0.982]
Epoch [56/120    avg_loss:0.101, val_acc:0.982]
Epoch [57/120    avg_loss:0.047, val_acc:0.982]
Epoch [58/120    avg_loss:0.062, val_acc:0.984]
Epoch [59/120    avg_loss:0.053, val_acc:0.984]
Epoch [60/120    avg_loss:0.056, val_acc:0.984]
Epoch [61/120    avg_loss:0.064, val_acc:0.984]
Epoch [62/120    avg_loss:0.047, val_acc:0.984]
Epoch [63/120    avg_loss:0.068, val_acc:0.984]
Epoch [64/120    avg_loss:0.034, val_acc:0.986]
Epoch [65/120    avg_loss:0.059, val_acc:0.986]
Epoch [66/120    avg_loss:0.061, val_acc:0.982]
Epoch [67/120    avg_loss:0.053, val_acc:0.986]
Epoch [68/120    avg_loss:0.040, val_acc:0.986]
Epoch [69/120    avg_loss:0.047, val_acc:0.984]
Epoch [70/120    avg_loss:0.042, val_acc:0.984]
Epoch [71/120    avg_loss:0.066, val_acc:0.984]
Epoch [72/120    avg_loss:0.040, val_acc:0.986]
Epoch [73/120    avg_loss:0.041, val_acc:0.986]
Epoch [74/120    avg_loss:0.049, val_acc:0.986]
Epoch [75/120    avg_loss:0.030, val_acc:0.986]
Epoch [76/120    avg_loss:0.030, val_acc:0.986]
Epoch [77/120    avg_loss:0.054, val_acc:0.984]
Epoch [78/120    avg_loss:0.075, val_acc:0.986]
Epoch [79/120    avg_loss:0.049, val_acc:0.986]
Epoch [80/120    avg_loss:0.028, val_acc:0.986]
Epoch [81/120    avg_loss:0.034, val_acc:0.986]
Epoch [82/120    avg_loss:0.047, val_acc:0.986]
Epoch [83/120    avg_loss:0.031, val_acc:0.986]
Epoch [84/120    avg_loss:0.038, val_acc:0.986]
Epoch [85/120    avg_loss:0.048, val_acc:0.986]
Epoch [86/120    avg_loss:0.036, val_acc:0.986]
Epoch [87/120    avg_loss:0.042, val_acc:0.986]
Epoch [88/120    avg_loss:0.036, val_acc:0.984]
Epoch [89/120    avg_loss:0.044, val_acc:0.984]
Epoch [90/120    avg_loss:0.034, val_acc:0.984]
Epoch [91/120    avg_loss:0.027, val_acc:0.984]
Epoch [92/120    avg_loss:0.040, val_acc:0.984]
Epoch [93/120    avg_loss:0.047, val_acc:0.986]
Epoch [94/120    avg_loss:0.032, val_acc:0.986]
Epoch [95/120    avg_loss:0.042, val_acc:0.988]
Epoch [96/120    avg_loss:0.048, val_acc:0.986]
Epoch [97/120    avg_loss:0.036, val_acc:0.986]
Epoch [98/120    avg_loss:0.039, val_acc:0.986]
Epoch [99/120    avg_loss:0.046, val_acc:0.986]
Epoch [100/120    avg_loss:0.031, val_acc:0.986]
Epoch [101/120    avg_loss:0.044, val_acc:0.986]
Epoch [102/120    avg_loss:0.026, val_acc:0.986]
Epoch [103/120    avg_loss:0.039, val_acc:0.986]
Epoch [104/120    avg_loss:0.032, val_acc:0.986]
Epoch [105/120    avg_loss:0.050, val_acc:0.986]
Epoch [106/120    avg_loss:0.034, val_acc:0.984]
Epoch [107/120    avg_loss:0.032, val_acc:0.986]
Epoch [108/120    avg_loss:0.052, val_acc:0.986]
Epoch [109/120    avg_loss:0.030, val_acc:0.986]
Epoch [110/120    avg_loss:0.034, val_acc:0.986]
Epoch [111/120    avg_loss:0.049, val_acc:0.986]
Epoch [112/120    avg_loss:0.027, val_acc:0.986]
Epoch [113/120    avg_loss:0.029, val_acc:0.984]
Epoch [114/120    avg_loss:0.023, val_acc:0.984]
Epoch [115/120    avg_loss:0.042, val_acc:0.984]
Epoch [116/120    avg_loss:0.032, val_acc:0.986]
Epoch [117/120    avg_loss:0.024, val_acc:0.984]
Epoch [118/120    avg_loss:0.040, val_acc:0.984]
Epoch [119/120    avg_loss:0.057, val_acc:0.984]
Epoch [120/120    avg_loss:0.029, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0  17 434   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.97309417 0.98678414 0.94623656 0.93333333
 1.         0.94505495 1.         1.         1.         0.97795071
 0.97857948 1.        ]

Kappa:
0.9871818023906248
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2597c3f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.256, val_acc:0.490]
Epoch [2/120    avg_loss:1.491, val_acc:0.679]
Epoch [3/120    avg_loss:1.124, val_acc:0.748]
Epoch [4/120    avg_loss:0.891, val_acc:0.767]
Epoch [5/120    avg_loss:0.741, val_acc:0.856]
Epoch [6/120    avg_loss:0.607, val_acc:0.846]
Epoch [7/120    avg_loss:0.591, val_acc:0.873]
Epoch [8/120    avg_loss:0.522, val_acc:0.894]
Epoch [9/120    avg_loss:0.496, val_acc:0.885]
Epoch [10/120    avg_loss:0.405, val_acc:0.938]
Epoch [11/120    avg_loss:0.299, val_acc:0.929]
Epoch [12/120    avg_loss:0.365, val_acc:0.917]
Epoch [13/120    avg_loss:0.333, val_acc:0.925]
Epoch [14/120    avg_loss:0.271, val_acc:0.942]
Epoch [15/120    avg_loss:0.271, val_acc:0.940]
Epoch [16/120    avg_loss:0.366, val_acc:0.935]
Epoch [17/120    avg_loss:0.297, val_acc:0.910]
Epoch [18/120    avg_loss:0.330, val_acc:0.927]
Epoch [19/120    avg_loss:0.262, val_acc:0.896]
Epoch [20/120    avg_loss:0.213, val_acc:0.940]
Epoch [21/120    avg_loss:0.203, val_acc:0.958]
Epoch [22/120    avg_loss:0.191, val_acc:0.954]
Epoch [23/120    avg_loss:0.224, val_acc:0.956]
Epoch [24/120    avg_loss:0.202, val_acc:0.960]
Epoch [25/120    avg_loss:0.134, val_acc:0.954]
Epoch [26/120    avg_loss:0.169, val_acc:0.954]
Epoch [27/120    avg_loss:0.185, val_acc:0.948]
Epoch [28/120    avg_loss:0.131, val_acc:0.971]
Epoch [29/120    avg_loss:0.136, val_acc:0.960]
Epoch [30/120    avg_loss:0.206, val_acc:0.958]
Epoch [31/120    avg_loss:0.172, val_acc:0.963]
Epoch [32/120    avg_loss:0.217, val_acc:0.933]
Epoch [33/120    avg_loss:0.215, val_acc:0.956]
Epoch [34/120    avg_loss:0.185, val_acc:0.969]
Epoch [35/120    avg_loss:0.146, val_acc:0.950]
Epoch [36/120    avg_loss:0.213, val_acc:0.971]
Epoch [37/120    avg_loss:0.202, val_acc:0.954]
Epoch [38/120    avg_loss:0.163, val_acc:0.973]
Epoch [39/120    avg_loss:0.129, val_acc:0.969]
Epoch [40/120    avg_loss:0.103, val_acc:0.983]
Epoch [41/120    avg_loss:0.078, val_acc:0.975]
Epoch [42/120    avg_loss:0.093, val_acc:0.973]
Epoch [43/120    avg_loss:0.097, val_acc:0.975]
Epoch [44/120    avg_loss:0.051, val_acc:0.979]
Epoch [45/120    avg_loss:0.063, val_acc:0.969]
Epoch [46/120    avg_loss:0.070, val_acc:0.965]
Epoch [47/120    avg_loss:0.059, val_acc:0.983]
Epoch [48/120    avg_loss:0.100, val_acc:0.952]
Epoch [49/120    avg_loss:0.106, val_acc:0.956]
Epoch [50/120    avg_loss:0.139, val_acc:0.973]
Epoch [51/120    avg_loss:0.076, val_acc:0.983]
Epoch [52/120    avg_loss:0.096, val_acc:0.950]
Epoch [53/120    avg_loss:0.113, val_acc:0.975]
Epoch [54/120    avg_loss:0.073, val_acc:0.975]
Epoch [55/120    avg_loss:0.044, val_acc:0.977]
Epoch [56/120    avg_loss:0.094, val_acc:0.985]
Epoch [57/120    avg_loss:0.070, val_acc:0.977]
Epoch [58/120    avg_loss:0.049, val_acc:0.983]
Epoch [59/120    avg_loss:0.062, val_acc:0.975]
Epoch [60/120    avg_loss:0.046, val_acc:0.985]
Epoch [61/120    avg_loss:0.048, val_acc:0.985]
Epoch [62/120    avg_loss:0.032, val_acc:0.981]
Epoch [63/120    avg_loss:0.035, val_acc:0.977]
Epoch [64/120    avg_loss:0.026, val_acc:0.983]
Epoch [65/120    avg_loss:0.040, val_acc:0.990]
Epoch [66/120    avg_loss:0.031, val_acc:0.981]
Epoch [67/120    avg_loss:0.093, val_acc:0.973]
Epoch [68/120    avg_loss:0.094, val_acc:0.975]
Epoch [69/120    avg_loss:0.090, val_acc:0.975]
Epoch [70/120    avg_loss:0.048, val_acc:0.981]
Epoch [71/120    avg_loss:0.070, val_acc:0.960]
Epoch [72/120    avg_loss:0.061, val_acc:0.977]
Epoch [73/120    avg_loss:0.059, val_acc:0.981]
Epoch [74/120    avg_loss:0.055, val_acc:0.977]
Epoch [75/120    avg_loss:0.049, val_acc:0.983]
Epoch [76/120    avg_loss:0.047, val_acc:0.979]
Epoch [77/120    avg_loss:0.037, val_acc:0.981]
Epoch [78/120    avg_loss:0.027, val_acc:0.983]
Epoch [79/120    avg_loss:0.017, val_acc:0.983]
Epoch [80/120    avg_loss:0.036, val_acc:0.983]
Epoch [81/120    avg_loss:0.018, val_acc:0.988]
Epoch [82/120    avg_loss:0.013, val_acc:0.985]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.025, val_acc:0.985]
Epoch [85/120    avg_loss:0.017, val_acc:0.985]
Epoch [86/120    avg_loss:0.016, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.988]
Epoch [88/120    avg_loss:0.026, val_acc:0.990]
Epoch [89/120    avg_loss:0.018, val_acc:0.990]
Epoch [90/120    avg_loss:0.019, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.990]
Epoch [92/120    avg_loss:0.018, val_acc:0.990]
Epoch [93/120    avg_loss:0.015, val_acc:0.992]
Epoch [94/120    avg_loss:0.023, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.985]
Epoch [96/120    avg_loss:0.015, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.016, val_acc:0.988]
Epoch [100/120    avg_loss:0.015, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.015, val_acc:0.985]
Epoch [103/120    avg_loss:0.014, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.017, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.017, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.023, val_acc:0.985]
Epoch [111/120    avg_loss:0.017, val_acc:0.983]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.016, val_acc:0.983]
Epoch [114/120    avg_loss:0.015, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.983]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.016, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.017, val_acc:0.985]
Epoch [120/120    avg_loss:0.027, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   1   0   0   0   0   1   0]
 [  0   0   0 219   9   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98636364 0.97550111 0.94736842 0.95
 0.99512195 0.98395722 0.99742931 1.         1.         1.
 0.99668508 1.        ]

Kappa:
0.9921659988252794
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff651216780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.193, val_acc:0.609]
Epoch [2/120    avg_loss:1.522, val_acc:0.627]
Epoch [3/120    avg_loss:1.116, val_acc:0.781]
Epoch [4/120    avg_loss:0.874, val_acc:0.791]
Epoch [5/120    avg_loss:0.726, val_acc:0.844]
Epoch [6/120    avg_loss:0.589, val_acc:0.875]
Epoch [7/120    avg_loss:0.474, val_acc:0.908]
Epoch [8/120    avg_loss:0.461, val_acc:0.916]
Epoch [9/120    avg_loss:0.450, val_acc:0.916]
Epoch [10/120    avg_loss:0.449, val_acc:0.881]
Epoch [11/120    avg_loss:0.408, val_acc:0.910]
Epoch [12/120    avg_loss:0.331, val_acc:0.924]
Epoch [13/120    avg_loss:0.282, val_acc:0.930]
Epoch [14/120    avg_loss:0.350, val_acc:0.934]
Epoch [15/120    avg_loss:0.248, val_acc:0.924]
Epoch [16/120    avg_loss:0.269, val_acc:0.922]
Epoch [17/120    avg_loss:0.261, val_acc:0.936]
Epoch [18/120    avg_loss:0.277, val_acc:0.930]
Epoch [19/120    avg_loss:0.259, val_acc:0.930]
Epoch [20/120    avg_loss:0.245, val_acc:0.949]
Epoch [21/120    avg_loss:0.203, val_acc:0.932]
Epoch [22/120    avg_loss:0.283, val_acc:0.920]
Epoch [23/120    avg_loss:0.183, val_acc:0.957]
Epoch [24/120    avg_loss:0.211, val_acc:0.951]
Epoch [25/120    avg_loss:0.210, val_acc:0.965]
Epoch [26/120    avg_loss:0.181, val_acc:0.953]
Epoch [27/120    avg_loss:0.166, val_acc:0.959]
Epoch [28/120    avg_loss:0.179, val_acc:0.932]
Epoch [29/120    avg_loss:0.144, val_acc:0.963]
Epoch [30/120    avg_loss:0.191, val_acc:0.969]
Epoch [31/120    avg_loss:0.133, val_acc:0.965]
Epoch [32/120    avg_loss:0.094, val_acc:0.963]
Epoch [33/120    avg_loss:0.142, val_acc:0.973]
Epoch [34/120    avg_loss:0.144, val_acc:0.973]
Epoch [35/120    avg_loss:0.099, val_acc:0.965]
Epoch [36/120    avg_loss:0.098, val_acc:0.971]
Epoch [37/120    avg_loss:0.111, val_acc:0.973]
Epoch [38/120    avg_loss:0.098, val_acc:0.971]
Epoch [39/120    avg_loss:0.119, val_acc:0.965]
Epoch [40/120    avg_loss:0.074, val_acc:0.973]
Epoch [41/120    avg_loss:0.191, val_acc:0.969]
Epoch [42/120    avg_loss:0.146, val_acc:0.957]
Epoch [43/120    avg_loss:0.090, val_acc:0.961]
Epoch [44/120    avg_loss:0.100, val_acc:0.965]
Epoch [45/120    avg_loss:0.100, val_acc:0.971]
Epoch [46/120    avg_loss:0.133, val_acc:0.977]
Epoch [47/120    avg_loss:0.057, val_acc:0.975]
Epoch [48/120    avg_loss:0.111, val_acc:0.969]
Epoch [49/120    avg_loss:0.152, val_acc:0.961]
Epoch [50/120    avg_loss:0.093, val_acc:0.959]
Epoch [51/120    avg_loss:0.079, val_acc:0.969]
Epoch [52/120    avg_loss:0.070, val_acc:0.969]
Epoch [53/120    avg_loss:0.061, val_acc:0.977]
Epoch [54/120    avg_loss:0.050, val_acc:0.979]
Epoch [55/120    avg_loss:0.043, val_acc:0.979]
Epoch [56/120    avg_loss:0.037, val_acc:0.979]
Epoch [57/120    avg_loss:0.037, val_acc:0.963]
Epoch [58/120    avg_loss:0.052, val_acc:0.977]
Epoch [59/120    avg_loss:0.048, val_acc:0.971]
Epoch [60/120    avg_loss:0.043, val_acc:0.965]
Epoch [61/120    avg_loss:0.060, val_acc:0.973]
Epoch [62/120    avg_loss:0.051, val_acc:0.977]
Epoch [63/120    avg_loss:0.047, val_acc:0.977]
Epoch [64/120    avg_loss:0.028, val_acc:0.980]
Epoch [65/120    avg_loss:0.028, val_acc:0.979]
Epoch [66/120    avg_loss:0.027, val_acc:0.977]
Epoch [67/120    avg_loss:0.033, val_acc:0.977]
Epoch [68/120    avg_loss:0.047, val_acc:0.979]
Epoch [69/120    avg_loss:0.020, val_acc:0.979]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.053, val_acc:0.975]
Epoch [72/120    avg_loss:0.048, val_acc:0.980]
Epoch [73/120    avg_loss:0.042, val_acc:0.971]
Epoch [74/120    avg_loss:0.061, val_acc:0.969]
Epoch [75/120    avg_loss:0.045, val_acc:0.984]
Epoch [76/120    avg_loss:0.028, val_acc:0.980]
Epoch [77/120    avg_loss:0.020, val_acc:0.982]
Epoch [78/120    avg_loss:0.037, val_acc:0.979]
Epoch [79/120    avg_loss:0.029, val_acc:0.986]
Epoch [80/120    avg_loss:0.066, val_acc:0.975]
Epoch [81/120    avg_loss:0.035, val_acc:0.977]
Epoch [82/120    avg_loss:0.029, val_acc:0.982]
Epoch [83/120    avg_loss:0.030, val_acc:0.986]
Epoch [84/120    avg_loss:0.054, val_acc:0.982]
Epoch [85/120    avg_loss:0.098, val_acc:0.986]
Epoch [86/120    avg_loss:0.047, val_acc:0.979]
Epoch [87/120    avg_loss:0.069, val_acc:0.982]
Epoch [88/120    avg_loss:0.080, val_acc:0.971]
Epoch [89/120    avg_loss:0.069, val_acc:0.975]
Epoch [90/120    avg_loss:0.036, val_acc:0.975]
Epoch [91/120    avg_loss:0.036, val_acc:0.979]
Epoch [92/120    avg_loss:0.043, val_acc:0.984]
Epoch [93/120    avg_loss:0.050, val_acc:0.980]
Epoch [94/120    avg_loss:0.044, val_acc:0.959]
Epoch [95/120    avg_loss:0.037, val_acc:0.977]
Epoch [96/120    avg_loss:0.041, val_acc:0.971]
Epoch [97/120    avg_loss:0.030, val_acc:0.969]
Epoch [98/120    avg_loss:0.031, val_acc:0.977]
Epoch [99/120    avg_loss:0.022, val_acc:0.979]
Epoch [100/120    avg_loss:0.016, val_acc:0.979]
Epoch [101/120    avg_loss:0.017, val_acc:0.980]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.980]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.015, val_acc:0.982]
Epoch [111/120    avg_loss:0.015, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.014, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 221   7   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.99086758 0.98004435 0.93555094 0.91512915
 0.99756691 0.98947368 1.         0.9978678  1.         1.
 0.99778761 1.        ]

Kappa:
0.9912163011035583
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd81aac5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.276, val_acc:0.553]
Epoch [2/120    avg_loss:1.489, val_acc:0.736]
Epoch [3/120    avg_loss:1.081, val_acc:0.764]
Epoch [4/120    avg_loss:0.900, val_acc:0.795]
Epoch [5/120    avg_loss:0.789, val_acc:0.812]
Epoch [6/120    avg_loss:0.610, val_acc:0.867]
Epoch [7/120    avg_loss:0.625, val_acc:0.861]
Epoch [8/120    avg_loss:0.503, val_acc:0.900]
Epoch [9/120    avg_loss:0.497, val_acc:0.883]
Epoch [10/120    avg_loss:0.435, val_acc:0.895]
Epoch [11/120    avg_loss:0.374, val_acc:0.928]
Epoch [12/120    avg_loss:0.334, val_acc:0.922]
Epoch [13/120    avg_loss:0.339, val_acc:0.914]
Epoch [14/120    avg_loss:0.323, val_acc:0.906]
Epoch [15/120    avg_loss:0.314, val_acc:0.943]
Epoch [16/120    avg_loss:0.287, val_acc:0.922]
Epoch [17/120    avg_loss:0.288, val_acc:0.953]
Epoch [18/120    avg_loss:0.237, val_acc:0.930]
Epoch [19/120    avg_loss:0.218, val_acc:0.918]
Epoch [20/120    avg_loss:0.250, val_acc:0.934]
Epoch [21/120    avg_loss:0.231, val_acc:0.941]
Epoch [22/120    avg_loss:0.196, val_acc:0.926]
Epoch [23/120    avg_loss:0.278, val_acc:0.924]
Epoch [24/120    avg_loss:0.289, val_acc:0.916]
Epoch [25/120    avg_loss:0.238, val_acc:0.910]
Epoch [26/120    avg_loss:0.249, val_acc:0.955]
Epoch [27/120    avg_loss:0.193, val_acc:0.971]
Epoch [28/120    avg_loss:0.223, val_acc:0.939]
Epoch [29/120    avg_loss:0.169, val_acc:0.943]
Epoch [30/120    avg_loss:0.183, val_acc:0.965]
Epoch [31/120    avg_loss:0.153, val_acc:0.953]
Epoch [32/120    avg_loss:0.140, val_acc:0.957]
Epoch [33/120    avg_loss:0.125, val_acc:0.965]
Epoch [34/120    avg_loss:0.089, val_acc:0.971]
Epoch [35/120    avg_loss:0.106, val_acc:0.963]
Epoch [36/120    avg_loss:0.139, val_acc:0.965]
Epoch [37/120    avg_loss:0.072, val_acc:0.975]
Epoch [38/120    avg_loss:0.082, val_acc:0.971]
Epoch [39/120    avg_loss:0.079, val_acc:0.975]
Epoch [40/120    avg_loss:0.108, val_acc:0.969]
Epoch [41/120    avg_loss:0.167, val_acc:0.967]
Epoch [42/120    avg_loss:0.140, val_acc:0.963]
Epoch [43/120    avg_loss:0.106, val_acc:0.963]
Epoch [44/120    avg_loss:0.082, val_acc:0.955]
Epoch [45/120    avg_loss:0.078, val_acc:0.977]
Epoch [46/120    avg_loss:0.078, val_acc:0.973]
Epoch [47/120    avg_loss:0.092, val_acc:0.934]
Epoch [48/120    avg_loss:0.101, val_acc:0.965]
Epoch [49/120    avg_loss:0.060, val_acc:0.979]
Epoch [50/120    avg_loss:0.053, val_acc:0.973]
Epoch [51/120    avg_loss:0.066, val_acc:0.971]
Epoch [52/120    avg_loss:0.034, val_acc:0.975]
Epoch [53/120    avg_loss:0.036, val_acc:0.980]
Epoch [54/120    avg_loss:0.042, val_acc:0.980]
Epoch [55/120    avg_loss:0.056, val_acc:0.980]
Epoch [56/120    avg_loss:0.033, val_acc:0.980]
Epoch [57/120    avg_loss:0.065, val_acc:0.979]
Epoch [58/120    avg_loss:0.076, val_acc:0.973]
Epoch [59/120    avg_loss:0.059, val_acc:0.984]
Epoch [60/120    avg_loss:0.095, val_acc:0.973]
Epoch [61/120    avg_loss:0.039, val_acc:0.982]
Epoch [62/120    avg_loss:0.057, val_acc:0.979]
Epoch [63/120    avg_loss:0.028, val_acc:0.979]
Epoch [64/120    avg_loss:0.034, val_acc:0.980]
Epoch [65/120    avg_loss:0.028, val_acc:0.980]
Epoch [66/120    avg_loss:0.047, val_acc:0.986]
Epoch [67/120    avg_loss:0.041, val_acc:0.982]
Epoch [68/120    avg_loss:0.050, val_acc:0.984]
Epoch [69/120    avg_loss:0.030, val_acc:0.979]
Epoch [70/120    avg_loss:0.044, val_acc:0.975]
Epoch [71/120    avg_loss:0.092, val_acc:0.971]
Epoch [72/120    avg_loss:0.073, val_acc:0.963]
Epoch [73/120    avg_loss:0.130, val_acc:0.955]
Epoch [74/120    avg_loss:0.105, val_acc:0.969]
Epoch [75/120    avg_loss:0.082, val_acc:0.984]
Epoch [76/120    avg_loss:0.038, val_acc:0.986]
Epoch [77/120    avg_loss:0.046, val_acc:0.988]
Epoch [78/120    avg_loss:0.038, val_acc:0.986]
Epoch [79/120    avg_loss:0.061, val_acc:0.990]
Epoch [80/120    avg_loss:0.102, val_acc:0.975]
Epoch [81/120    avg_loss:0.115, val_acc:0.980]
Epoch [82/120    avg_loss:0.094, val_acc:0.973]
Epoch [83/120    avg_loss:0.067, val_acc:0.971]
Epoch [84/120    avg_loss:0.047, val_acc:0.975]
Epoch [85/120    avg_loss:0.034, val_acc:0.979]
Epoch [86/120    avg_loss:0.017, val_acc:0.982]
Epoch [87/120    avg_loss:0.030, val_acc:0.982]
Epoch [88/120    avg_loss:0.034, val_acc:0.977]
Epoch [89/120    avg_loss:0.028, val_acc:0.982]
Epoch [90/120    avg_loss:0.039, val_acc:0.982]
Epoch [91/120    avg_loss:0.042, val_acc:0.982]
Epoch [92/120    avg_loss:0.033, val_acc:0.986]
Epoch [93/120    avg_loss:0.018, val_acc:0.986]
Epoch [94/120    avg_loss:0.017, val_acc:0.990]
Epoch [95/120    avg_loss:0.018, val_acc:0.990]
Epoch [96/120    avg_loss:0.022, val_acc:0.988]
Epoch [97/120    avg_loss:0.017, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.986]
Epoch [101/120    avg_loss:0.018, val_acc:0.986]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.015, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.022, val_acc:0.988]
Epoch [106/120    avg_loss:0.016, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.014, val_acc:0.990]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.016, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.018, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.020, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   1   0   0   0   0   4   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         1.         0.98901099 0.95404814 0.93425606
 0.99756691 1.         0.99742931 0.99574468 1.         1.
 0.99778761 1.        ]

Kappa:
0.9935904016074258
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04f1e5d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.199, val_acc:0.519]
Epoch [2/120    avg_loss:1.543, val_acc:0.627]
Epoch [3/120    avg_loss:1.109, val_acc:0.773]
Epoch [4/120    avg_loss:0.894, val_acc:0.779]
Epoch [5/120    avg_loss:0.720, val_acc:0.819]
Epoch [6/120    avg_loss:0.562, val_acc:0.896]
Epoch [7/120    avg_loss:0.459, val_acc:0.902]
Epoch [8/120    avg_loss:0.438, val_acc:0.898]
Epoch [9/120    avg_loss:0.470, val_acc:0.902]
Epoch [10/120    avg_loss:0.373, val_acc:0.902]
Epoch [11/120    avg_loss:0.298, val_acc:0.917]
Epoch [12/120    avg_loss:0.339, val_acc:0.908]
Epoch [13/120    avg_loss:0.408, val_acc:0.906]
Epoch [14/120    avg_loss:0.245, val_acc:0.923]
Epoch [15/120    avg_loss:0.336, val_acc:0.904]
Epoch [16/120    avg_loss:0.326, val_acc:0.917]
Epoch [17/120    avg_loss:0.317, val_acc:0.908]
Epoch [18/120    avg_loss:0.317, val_acc:0.946]
Epoch [19/120    avg_loss:0.269, val_acc:0.938]
Epoch [20/120    avg_loss:0.196, val_acc:0.958]
Epoch [21/120    avg_loss:0.205, val_acc:0.944]
Epoch [22/120    avg_loss:0.192, val_acc:0.940]
Epoch [23/120    avg_loss:0.195, val_acc:0.960]
Epoch [24/120    avg_loss:0.167, val_acc:0.956]
Epoch [25/120    avg_loss:0.191, val_acc:0.960]
Epoch [26/120    avg_loss:0.203, val_acc:0.950]
Epoch [27/120    avg_loss:0.157, val_acc:0.952]
Epoch [28/120    avg_loss:0.178, val_acc:0.958]
Epoch [29/120    avg_loss:0.187, val_acc:0.958]
Epoch [30/120    avg_loss:0.189, val_acc:0.940]
Epoch [31/120    avg_loss:0.176, val_acc:0.950]
Epoch [32/120    avg_loss:0.148, val_acc:0.960]
Epoch [33/120    avg_loss:0.133, val_acc:0.975]
Epoch [34/120    avg_loss:0.165, val_acc:0.958]
Epoch [35/120    avg_loss:0.148, val_acc:0.956]
Epoch [36/120    avg_loss:0.186, val_acc:0.950]
Epoch [37/120    avg_loss:0.103, val_acc:0.967]
Epoch [38/120    avg_loss:0.168, val_acc:0.912]
Epoch [39/120    avg_loss:0.188, val_acc:0.969]
Epoch [40/120    avg_loss:0.089, val_acc:0.967]
Epoch [41/120    avg_loss:0.136, val_acc:0.979]
Epoch [42/120    avg_loss:0.096, val_acc:0.973]
Epoch [43/120    avg_loss:0.129, val_acc:0.963]
Epoch [44/120    avg_loss:0.130, val_acc:0.950]
Epoch [45/120    avg_loss:0.134, val_acc:0.950]
Epoch [46/120    avg_loss:0.151, val_acc:0.971]
Epoch [47/120    avg_loss:0.067, val_acc:0.973]
Epoch [48/120    avg_loss:0.074, val_acc:0.975]
Epoch [49/120    avg_loss:0.062, val_acc:0.979]
Epoch [50/120    avg_loss:0.083, val_acc:0.983]
Epoch [51/120    avg_loss:0.121, val_acc:0.971]
Epoch [52/120    avg_loss:0.086, val_acc:0.958]
Epoch [53/120    avg_loss:0.066, val_acc:0.965]
Epoch [54/120    avg_loss:0.108, val_acc:0.960]
Epoch [55/120    avg_loss:0.091, val_acc:0.977]
Epoch [56/120    avg_loss:0.056, val_acc:0.979]
Epoch [57/120    avg_loss:0.044, val_acc:0.981]
Epoch [58/120    avg_loss:0.043, val_acc:0.977]
Epoch [59/120    avg_loss:0.050, val_acc:0.981]
Epoch [60/120    avg_loss:0.055, val_acc:0.983]
Epoch [61/120    avg_loss:0.036, val_acc:0.979]
Epoch [62/120    avg_loss:0.058, val_acc:0.975]
Epoch [63/120    avg_loss:0.059, val_acc:0.981]
Epoch [64/120    avg_loss:0.046, val_acc:0.971]
Epoch [65/120    avg_loss:0.042, val_acc:0.985]
Epoch [66/120    avg_loss:0.041, val_acc:0.981]
Epoch [67/120    avg_loss:0.054, val_acc:0.983]
Epoch [68/120    avg_loss:0.083, val_acc:0.983]
Epoch [69/120    avg_loss:0.053, val_acc:0.973]
Epoch [70/120    avg_loss:0.041, val_acc:0.985]
Epoch [71/120    avg_loss:0.037, val_acc:0.988]
Epoch [72/120    avg_loss:0.028, val_acc:0.985]
Epoch [73/120    avg_loss:0.033, val_acc:0.981]
Epoch [74/120    avg_loss:0.027, val_acc:0.994]
Epoch [75/120    avg_loss:0.035, val_acc:0.981]
Epoch [76/120    avg_loss:0.064, val_acc:0.983]
Epoch [77/120    avg_loss:0.036, val_acc:0.988]
Epoch [78/120    avg_loss:0.049, val_acc:0.977]
Epoch [79/120    avg_loss:0.022, val_acc:0.992]
Epoch [80/120    avg_loss:0.027, val_acc:0.988]
Epoch [81/120    avg_loss:0.023, val_acc:0.985]
Epoch [82/120    avg_loss:0.062, val_acc:0.973]
Epoch [83/120    avg_loss:0.052, val_acc:0.988]
Epoch [84/120    avg_loss:0.029, val_acc:0.988]
Epoch [85/120    avg_loss:0.021, val_acc:0.990]
Epoch [86/120    avg_loss:0.017, val_acc:0.992]
Epoch [87/120    avg_loss:0.016, val_acc:0.981]
Epoch [88/120    avg_loss:0.040, val_acc:0.985]
Epoch [89/120    avg_loss:0.022, val_acc:0.988]
Epoch [90/120    avg_loss:0.024, val_acc:0.988]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.015, val_acc:0.990]
Epoch [95/120    avg_loss:0.013, val_acc:0.990]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.009, val_acc:0.992]
Epoch [98/120    avg_loss:0.011, val_acc:0.992]
Epoch [99/120    avg_loss:0.014, val_acc:0.992]
Epoch [100/120    avg_loss:0.020, val_acc:0.992]
Epoch [101/120    avg_loss:0.018, val_acc:0.992]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.017, val_acc:0.992]
Epoch [105/120    avg_loss:0.015, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.994]
Epoch [107/120    avg_loss:0.014, val_acc:0.994]
Epoch [108/120    avg_loss:0.013, val_acc:0.994]
Epoch [109/120    avg_loss:0.011, val_acc:0.994]
Epoch [110/120    avg_loss:0.010, val_acc:0.994]
Epoch [111/120    avg_loss:0.011, val_acc:0.994]
Epoch [112/120    avg_loss:0.012, val_acc:0.994]
Epoch [113/120    avg_loss:0.015, val_acc:0.994]
Epoch [114/120    avg_loss:0.015, val_acc:0.994]
Epoch [115/120    avg_loss:0.012, val_acc:0.994]
Epoch [116/120    avg_loss:0.014, val_acc:0.994]
Epoch [117/120    avg_loss:0.012, val_acc:0.994]
Epoch [118/120    avg_loss:0.007, val_acc:0.994]
Epoch [119/120    avg_loss:0.011, val_acc:0.994]
Epoch [120/120    avg_loss:0.007, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.98642534 0.98454746 0.95074946 0.95104895
 0.99266504 0.9673913  1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9928781558766678
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff18f2e0710>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.267, val_acc:0.570]
Epoch [2/120    avg_loss:1.569, val_acc:0.623]
Epoch [3/120    avg_loss:1.109, val_acc:0.709]
Epoch [4/120    avg_loss:0.861, val_acc:0.824]
Epoch [5/120    avg_loss:0.689, val_acc:0.891]
Epoch [6/120    avg_loss:0.582, val_acc:0.910]
Epoch [7/120    avg_loss:0.472, val_acc:0.889]
Epoch [8/120    avg_loss:0.486, val_acc:0.895]
Epoch [9/120    avg_loss:0.358, val_acc:0.914]
Epoch [10/120    avg_loss:0.396, val_acc:0.920]
Epoch [11/120    avg_loss:0.367, val_acc:0.883]
Epoch [12/120    avg_loss:0.428, val_acc:0.891]
Epoch [13/120    avg_loss:0.321, val_acc:0.932]
Epoch [14/120    avg_loss:0.253, val_acc:0.957]
Epoch [15/120    avg_loss:0.256, val_acc:0.979]
Epoch [16/120    avg_loss:0.239, val_acc:0.961]
Epoch [17/120    avg_loss:0.199, val_acc:0.924]
Epoch [18/120    avg_loss:0.220, val_acc:0.930]
Epoch [19/120    avg_loss:0.276, val_acc:0.973]
Epoch [20/120    avg_loss:0.247, val_acc:0.949]
Epoch [21/120    avg_loss:0.166, val_acc:0.975]
Epoch [22/120    avg_loss:0.206, val_acc:0.961]
Epoch [23/120    avg_loss:0.186, val_acc:0.977]
Epoch [24/120    avg_loss:0.173, val_acc:0.939]
Epoch [25/120    avg_loss:0.161, val_acc:0.969]
Epoch [26/120    avg_loss:0.152, val_acc:0.975]
Epoch [27/120    avg_loss:0.113, val_acc:0.973]
Epoch [28/120    avg_loss:0.134, val_acc:0.965]
Epoch [29/120    avg_loss:0.121, val_acc:0.975]
Epoch [30/120    avg_loss:0.060, val_acc:0.982]
Epoch [31/120    avg_loss:0.084, val_acc:0.982]
Epoch [32/120    avg_loss:0.079, val_acc:0.982]
Epoch [33/120    avg_loss:0.085, val_acc:0.984]
Epoch [34/120    avg_loss:0.066, val_acc:0.982]
Epoch [35/120    avg_loss:0.072, val_acc:0.980]
Epoch [36/120    avg_loss:0.075, val_acc:0.982]
Epoch [37/120    avg_loss:0.072, val_acc:0.986]
Epoch [38/120    avg_loss:0.080, val_acc:0.984]
Epoch [39/120    avg_loss:0.069, val_acc:0.986]
Epoch [40/120    avg_loss:0.056, val_acc:0.986]
Epoch [41/120    avg_loss:0.069, val_acc:0.986]
Epoch [42/120    avg_loss:0.060, val_acc:0.984]
Epoch [43/120    avg_loss:0.066, val_acc:0.984]
Epoch [44/120    avg_loss:0.057, val_acc:0.984]
Epoch [45/120    avg_loss:0.065, val_acc:0.986]
Epoch [46/120    avg_loss:0.057, val_acc:0.988]
Epoch [47/120    avg_loss:0.087, val_acc:0.980]
Epoch [48/120    avg_loss:0.075, val_acc:0.979]
Epoch [49/120    avg_loss:0.062, val_acc:0.982]
Epoch [50/120    avg_loss:0.070, val_acc:0.986]
Epoch [51/120    avg_loss:0.079, val_acc:0.986]
Epoch [52/120    avg_loss:0.073, val_acc:0.986]
Epoch [53/120    avg_loss:0.065, val_acc:0.984]
Epoch [54/120    avg_loss:0.048, val_acc:0.984]
Epoch [55/120    avg_loss:0.069, val_acc:0.982]
Epoch [56/120    avg_loss:0.050, val_acc:0.984]
Epoch [57/120    avg_loss:0.049, val_acc:0.984]
Epoch [58/120    avg_loss:0.071, val_acc:0.984]
Epoch [59/120    avg_loss:0.057, val_acc:0.984]
Epoch [60/120    avg_loss:0.047, val_acc:0.984]
Epoch [61/120    avg_loss:0.057, val_acc:0.984]
Epoch [62/120    avg_loss:0.045, val_acc:0.984]
Epoch [63/120    avg_loss:0.053, val_acc:0.984]
Epoch [64/120    avg_loss:0.068, val_acc:0.984]
Epoch [65/120    avg_loss:0.070, val_acc:0.984]
Epoch [66/120    avg_loss:0.054, val_acc:0.984]
Epoch [67/120    avg_loss:0.036, val_acc:0.982]
Epoch [68/120    avg_loss:0.067, val_acc:0.982]
Epoch [69/120    avg_loss:0.048, val_acc:0.982]
Epoch [70/120    avg_loss:0.039, val_acc:0.982]
Epoch [71/120    avg_loss:0.048, val_acc:0.982]
Epoch [72/120    avg_loss:0.041, val_acc:0.982]
Epoch [73/120    avg_loss:0.051, val_acc:0.982]
Epoch [74/120    avg_loss:0.046, val_acc:0.982]
Epoch [75/120    avg_loss:0.047, val_acc:0.982]
Epoch [76/120    avg_loss:0.056, val_acc:0.982]
Epoch [77/120    avg_loss:0.049, val_acc:0.982]
Epoch [78/120    avg_loss:0.037, val_acc:0.982]
Epoch [79/120    avg_loss:0.063, val_acc:0.982]
Epoch [80/120    avg_loss:0.040, val_acc:0.982]
Epoch [81/120    avg_loss:0.038, val_acc:0.982]
Epoch [82/120    avg_loss:0.054, val_acc:0.982]
Epoch [83/120    avg_loss:0.052, val_acc:0.982]
Epoch [84/120    avg_loss:0.053, val_acc:0.982]
Epoch [85/120    avg_loss:0.051, val_acc:0.982]
Epoch [86/120    avg_loss:0.045, val_acc:0.982]
Epoch [87/120    avg_loss:0.045, val_acc:0.982]
Epoch [88/120    avg_loss:0.055, val_acc:0.982]
Epoch [89/120    avg_loss:0.046, val_acc:0.982]
Epoch [90/120    avg_loss:0.050, val_acc:0.982]
Epoch [91/120    avg_loss:0.065, val_acc:0.982]
Epoch [92/120    avg_loss:0.065, val_acc:0.982]
Epoch [93/120    avg_loss:0.053, val_acc:0.982]
Epoch [94/120    avg_loss:0.067, val_acc:0.982]
Epoch [95/120    avg_loss:0.052, val_acc:0.982]
Epoch [96/120    avg_loss:0.061, val_acc:0.982]
Epoch [97/120    avg_loss:0.058, val_acc:0.982]
Epoch [98/120    avg_loss:0.072, val_acc:0.982]
Epoch [99/120    avg_loss:0.050, val_acc:0.982]
Epoch [100/120    avg_loss:0.063, val_acc:0.982]
Epoch [101/120    avg_loss:0.042, val_acc:0.982]
Epoch [102/120    avg_loss:0.057, val_acc:0.982]
Epoch [103/120    avg_loss:0.049, val_acc:0.982]
Epoch [104/120    avg_loss:0.056, val_acc:0.982]
Epoch [105/120    avg_loss:0.055, val_acc:0.982]
Epoch [106/120    avg_loss:0.044, val_acc:0.982]
Epoch [107/120    avg_loss:0.047, val_acc:0.982]
Epoch [108/120    avg_loss:0.068, val_acc:0.982]
Epoch [109/120    avg_loss:0.049, val_acc:0.982]
Epoch [110/120    avg_loss:0.044, val_acc:0.982]
Epoch [111/120    avg_loss:0.050, val_acc:0.982]
Epoch [112/120    avg_loss:0.071, val_acc:0.982]
Epoch [113/120    avg_loss:0.049, val_acc:0.982]
Epoch [114/120    avg_loss:0.041, val_acc:0.982]
Epoch [115/120    avg_loss:0.047, val_acc:0.982]
Epoch [116/120    avg_loss:0.062, val_acc:0.982]
Epoch [117/120    avg_loss:0.049, val_acc:0.982]
Epoch [118/120    avg_loss:0.049, val_acc:0.982]
Epoch [119/120    avg_loss:0.044, val_acc:0.982]
Epoch [120/120    avg_loss:0.050, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 214  13   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0  13 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.96629213 0.96396396 0.92410714 0.93203883
 1.         0.91712707 0.99487179 1.         1.         0.98305085
 0.98430493 1.        ]

Kappa:
0.9843338555887582
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b477827b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.201, val_acc:0.579]
Epoch [2/120    avg_loss:1.564, val_acc:0.615]
Epoch [3/120    avg_loss:1.157, val_acc:0.777]
Epoch [4/120    avg_loss:0.917, val_acc:0.785]
Epoch [5/120    avg_loss:0.733, val_acc:0.817]
Epoch [6/120    avg_loss:0.692, val_acc:0.835]
Epoch [7/120    avg_loss:0.654, val_acc:0.860]
Epoch [8/120    avg_loss:0.549, val_acc:0.881]
Epoch [9/120    avg_loss:0.457, val_acc:0.904]
Epoch [10/120    avg_loss:0.464, val_acc:0.900]
Epoch [11/120    avg_loss:0.434, val_acc:0.942]
Epoch [12/120    avg_loss:0.390, val_acc:0.942]
Epoch [13/120    avg_loss:0.306, val_acc:0.921]
Epoch [14/120    avg_loss:0.285, val_acc:0.919]
Epoch [15/120    avg_loss:0.301, val_acc:0.902]
Epoch [16/120    avg_loss:0.339, val_acc:0.917]
Epoch [17/120    avg_loss:0.289, val_acc:0.933]
Epoch [18/120    avg_loss:0.422, val_acc:0.917]
Epoch [19/120    avg_loss:0.336, val_acc:0.925]
Epoch [20/120    avg_loss:0.222, val_acc:0.960]
Epoch [21/120    avg_loss:0.247, val_acc:0.935]
Epoch [22/120    avg_loss:0.277, val_acc:0.931]
Epoch [23/120    avg_loss:0.205, val_acc:0.950]
Epoch [24/120    avg_loss:0.165, val_acc:0.969]
Epoch [25/120    avg_loss:0.154, val_acc:0.958]
Epoch [26/120    avg_loss:0.137, val_acc:0.960]
Epoch [27/120    avg_loss:0.241, val_acc:0.942]
Epoch [28/120    avg_loss:0.176, val_acc:0.963]
Epoch [29/120    avg_loss:0.179, val_acc:0.950]
Epoch [30/120    avg_loss:0.169, val_acc:0.958]
Epoch [31/120    avg_loss:0.111, val_acc:0.958]
Epoch [32/120    avg_loss:0.160, val_acc:0.969]
Epoch [33/120    avg_loss:0.196, val_acc:0.952]
Epoch [34/120    avg_loss:0.123, val_acc:0.956]
Epoch [35/120    avg_loss:0.113, val_acc:0.956]
Epoch [36/120    avg_loss:0.169, val_acc:0.963]
Epoch [37/120    avg_loss:0.121, val_acc:0.967]
Epoch [38/120    avg_loss:0.154, val_acc:0.975]
Epoch [39/120    avg_loss:0.108, val_acc:0.969]
Epoch [40/120    avg_loss:0.069, val_acc:0.979]
Epoch [41/120    avg_loss:0.072, val_acc:0.969]
Epoch [42/120    avg_loss:0.103, val_acc:0.971]
Epoch [43/120    avg_loss:0.116, val_acc:0.958]
Epoch [44/120    avg_loss:0.091, val_acc:0.977]
Epoch [45/120    avg_loss:0.098, val_acc:0.965]
Epoch [46/120    avg_loss:0.079, val_acc:0.975]
Epoch [47/120    avg_loss:0.087, val_acc:0.963]
Epoch [48/120    avg_loss:0.082, val_acc:0.973]
Epoch [49/120    avg_loss:0.163, val_acc:0.944]
Epoch [50/120    avg_loss:0.139, val_acc:0.971]
Epoch [51/120    avg_loss:0.098, val_acc:0.971]
Epoch [52/120    avg_loss:0.236, val_acc:0.954]
Epoch [53/120    avg_loss:0.089, val_acc:0.971]
Epoch [54/120    avg_loss:0.060, val_acc:0.975]
Epoch [55/120    avg_loss:0.049, val_acc:0.979]
Epoch [56/120    avg_loss:0.040, val_acc:0.985]
Epoch [57/120    avg_loss:0.045, val_acc:0.988]
Epoch [58/120    avg_loss:0.048, val_acc:0.988]
Epoch [59/120    avg_loss:0.055, val_acc:0.990]
Epoch [60/120    avg_loss:0.059, val_acc:0.983]
Epoch [61/120    avg_loss:0.039, val_acc:0.977]
Epoch [62/120    avg_loss:0.035, val_acc:0.981]
Epoch [63/120    avg_loss:0.045, val_acc:0.983]
Epoch [64/120    avg_loss:0.032, val_acc:0.983]
Epoch [65/120    avg_loss:0.053, val_acc:0.983]
Epoch [66/120    avg_loss:0.036, val_acc:0.983]
Epoch [67/120    avg_loss:0.050, val_acc:0.983]
Epoch [68/120    avg_loss:0.026, val_acc:0.985]
Epoch [69/120    avg_loss:0.036, val_acc:0.985]
Epoch [70/120    avg_loss:0.034, val_acc:0.983]
Epoch [71/120    avg_loss:0.033, val_acc:0.981]
Epoch [72/120    avg_loss:0.054, val_acc:0.983]
Epoch [73/120    avg_loss:0.022, val_acc:0.983]
Epoch [74/120    avg_loss:0.034, val_acc:0.983]
Epoch [75/120    avg_loss:0.028, val_acc:0.983]
Epoch [76/120    avg_loss:0.033, val_acc:0.983]
Epoch [77/120    avg_loss:0.032, val_acc:0.983]
Epoch [78/120    avg_loss:0.031, val_acc:0.983]
Epoch [79/120    avg_loss:0.031, val_acc:0.983]
Epoch [80/120    avg_loss:0.042, val_acc:0.983]
Epoch [81/120    avg_loss:0.036, val_acc:0.983]
Epoch [82/120    avg_loss:0.027, val_acc:0.983]
Epoch [83/120    avg_loss:0.050, val_acc:0.983]
Epoch [84/120    avg_loss:0.038, val_acc:0.983]
Epoch [85/120    avg_loss:0.044, val_acc:0.983]
Epoch [86/120    avg_loss:0.046, val_acc:0.983]
Epoch [87/120    avg_loss:0.039, val_acc:0.983]
Epoch [88/120    avg_loss:0.038, val_acc:0.983]
Epoch [89/120    avg_loss:0.030, val_acc:0.983]
Epoch [90/120    avg_loss:0.033, val_acc:0.983]
Epoch [91/120    avg_loss:0.044, val_acc:0.983]
Epoch [92/120    avg_loss:0.030, val_acc:0.983]
Epoch [93/120    avg_loss:0.032, val_acc:0.983]
Epoch [94/120    avg_loss:0.032, val_acc:0.983]
Epoch [95/120    avg_loss:0.031, val_acc:0.983]
Epoch [96/120    avg_loss:0.045, val_acc:0.983]
Epoch [97/120    avg_loss:0.033, val_acc:0.983]
Epoch [98/120    avg_loss:0.034, val_acc:0.983]
Epoch [99/120    avg_loss:0.030, val_acc:0.983]
Epoch [100/120    avg_loss:0.030, val_acc:0.983]
Epoch [101/120    avg_loss:0.044, val_acc:0.983]
Epoch [102/120    avg_loss:0.032, val_acc:0.983]
Epoch [103/120    avg_loss:0.053, val_acc:0.983]
Epoch [104/120    avg_loss:0.034, val_acc:0.983]
Epoch [105/120    avg_loss:0.023, val_acc:0.983]
Epoch [106/120    avg_loss:0.032, val_acc:0.983]
Epoch [107/120    avg_loss:0.040, val_acc:0.983]
Epoch [108/120    avg_loss:0.036, val_acc:0.983]
Epoch [109/120    avg_loss:0.035, val_acc:0.983]
Epoch [110/120    avg_loss:0.033, val_acc:0.983]
Epoch [111/120    avg_loss:0.026, val_acc:0.983]
Epoch [112/120    avg_loss:0.026, val_acc:0.983]
Epoch [113/120    avg_loss:0.053, val_acc:0.983]
Epoch [114/120    avg_loss:0.045, val_acc:0.983]
Epoch [115/120    avg_loss:0.031, val_acc:0.983]
Epoch [116/120    avg_loss:0.033, val_acc:0.983]
Epoch [117/120    avg_loss:0.040, val_acc:0.983]
Epoch [118/120    avg_loss:0.032, val_acc:0.983]
Epoch [119/120    avg_loss:0.030, val_acc:0.983]
Epoch [120/120    avg_loss:0.024, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   9 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 1.         0.97065463 0.99563319 0.92139738 0.9109589
 0.98522167 0.94054054 1.         1.         1.         0.98820446
 0.98550725 1.        ]

Kappa:
0.9862321327606659
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0e502557f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.171, val_acc:0.580]
Epoch [2/120    avg_loss:1.435, val_acc:0.719]
Epoch [3/120    avg_loss:1.085, val_acc:0.768]
Epoch [4/120    avg_loss:0.911, val_acc:0.799]
Epoch [5/120    avg_loss:0.732, val_acc:0.799]
Epoch [6/120    avg_loss:0.660, val_acc:0.867]
Epoch [7/120    avg_loss:0.554, val_acc:0.863]
Epoch [8/120    avg_loss:0.563, val_acc:0.869]
Epoch [9/120    avg_loss:0.479, val_acc:0.896]
Epoch [10/120    avg_loss:0.370, val_acc:0.904]
Epoch [11/120    avg_loss:0.337, val_acc:0.895]
Epoch [12/120    avg_loss:0.470, val_acc:0.893]
Epoch [13/120    avg_loss:0.355, val_acc:0.887]
Epoch [14/120    avg_loss:0.313, val_acc:0.926]
Epoch [15/120    avg_loss:0.335, val_acc:0.900]
Epoch [16/120    avg_loss:0.302, val_acc:0.926]
Epoch [17/120    avg_loss:0.255, val_acc:0.924]
Epoch [18/120    avg_loss:0.254, val_acc:0.941]
Epoch [19/120    avg_loss:0.203, val_acc:0.939]
Epoch [20/120    avg_loss:0.221, val_acc:0.895]
Epoch [21/120    avg_loss:0.297, val_acc:0.920]
Epoch [22/120    avg_loss:0.207, val_acc:0.932]
Epoch [23/120    avg_loss:0.206, val_acc:0.947]
Epoch [24/120    avg_loss:0.203, val_acc:0.943]
Epoch [25/120    avg_loss:0.170, val_acc:0.934]
Epoch [26/120    avg_loss:0.179, val_acc:0.959]
Epoch [27/120    avg_loss:0.148, val_acc:0.945]
Epoch [28/120    avg_loss:0.192, val_acc:0.953]
Epoch [29/120    avg_loss:0.180, val_acc:0.957]
Epoch [30/120    avg_loss:0.106, val_acc:0.969]
Epoch [31/120    avg_loss:0.102, val_acc:0.961]
Epoch [32/120    avg_loss:0.150, val_acc:0.957]
Epoch [33/120    avg_loss:0.149, val_acc:0.943]
Epoch [34/120    avg_loss:0.157, val_acc:0.959]
Epoch [35/120    avg_loss:0.140, val_acc:0.951]
Epoch [36/120    avg_loss:0.146, val_acc:0.957]
Epoch [37/120    avg_loss:0.121, val_acc:0.959]
Epoch [38/120    avg_loss:0.112, val_acc:0.965]
Epoch [39/120    avg_loss:0.083, val_acc:0.959]
Epoch [40/120    avg_loss:0.156, val_acc:0.951]
Epoch [41/120    avg_loss:0.151, val_acc:0.967]
Epoch [42/120    avg_loss:0.129, val_acc:0.955]
Epoch [43/120    avg_loss:0.140, val_acc:0.963]
Epoch [44/120    avg_loss:0.084, val_acc:0.965]
Epoch [45/120    avg_loss:0.064, val_acc:0.969]
Epoch [46/120    avg_loss:0.051, val_acc:0.977]
Epoch [47/120    avg_loss:0.045, val_acc:0.977]
Epoch [48/120    avg_loss:0.073, val_acc:0.977]
Epoch [49/120    avg_loss:0.068, val_acc:0.977]
Epoch [50/120    avg_loss:0.042, val_acc:0.977]
Epoch [51/120    avg_loss:0.049, val_acc:0.977]
Epoch [52/120    avg_loss:0.054, val_acc:0.977]
Epoch [53/120    avg_loss:0.042, val_acc:0.975]
Epoch [54/120    avg_loss:0.031, val_acc:0.977]
Epoch [55/120    avg_loss:0.039, val_acc:0.977]
Epoch [56/120    avg_loss:0.042, val_acc:0.975]
Epoch [57/120    avg_loss:0.044, val_acc:0.975]
Epoch [58/120    avg_loss:0.032, val_acc:0.975]
Epoch [59/120    avg_loss:0.047, val_acc:0.975]
Epoch [60/120    avg_loss:0.035, val_acc:0.971]
Epoch [61/120    avg_loss:0.058, val_acc:0.971]
Epoch [62/120    avg_loss:0.039, val_acc:0.973]
Epoch [63/120    avg_loss:0.040, val_acc:0.973]
Epoch [64/120    avg_loss:0.050, val_acc:0.969]
Epoch [65/120    avg_loss:0.038, val_acc:0.967]
Epoch [66/120    avg_loss:0.041, val_acc:0.969]
Epoch [67/120    avg_loss:0.036, val_acc:0.967]
Epoch [68/120    avg_loss:0.037, val_acc:0.967]
Epoch [69/120    avg_loss:0.037, val_acc:0.967]
Epoch [70/120    avg_loss:0.033, val_acc:0.969]
Epoch [71/120    avg_loss:0.041, val_acc:0.969]
Epoch [72/120    avg_loss:0.040, val_acc:0.969]
Epoch [73/120    avg_loss:0.049, val_acc:0.967]
Epoch [74/120    avg_loss:0.034, val_acc:0.967]
Epoch [75/120    avg_loss:0.055, val_acc:0.969]
Epoch [76/120    avg_loss:0.038, val_acc:0.969]
Epoch [77/120    avg_loss:0.029, val_acc:0.969]
Epoch [78/120    avg_loss:0.054, val_acc:0.969]
Epoch [79/120    avg_loss:0.050, val_acc:0.971]
Epoch [80/120    avg_loss:0.029, val_acc:0.971]
Epoch [81/120    avg_loss:0.029, val_acc:0.971]
Epoch [82/120    avg_loss:0.027, val_acc:0.971]
Epoch [83/120    avg_loss:0.027, val_acc:0.971]
Epoch [84/120    avg_loss:0.039, val_acc:0.971]
Epoch [85/120    avg_loss:0.040, val_acc:0.971]
Epoch [86/120    avg_loss:0.043, val_acc:0.971]
Epoch [87/120    avg_loss:0.050, val_acc:0.971]
Epoch [88/120    avg_loss:0.024, val_acc:0.971]
Epoch [89/120    avg_loss:0.033, val_acc:0.971]
Epoch [90/120    avg_loss:0.045, val_acc:0.971]
Epoch [91/120    avg_loss:0.038, val_acc:0.971]
Epoch [92/120    avg_loss:0.038, val_acc:0.971]
Epoch [93/120    avg_loss:0.036, val_acc:0.971]
Epoch [94/120    avg_loss:0.039, val_acc:0.971]
Epoch [95/120    avg_loss:0.035, val_acc:0.971]
Epoch [96/120    avg_loss:0.034, val_acc:0.971]
Epoch [97/120    avg_loss:0.048, val_acc:0.971]
Epoch [98/120    avg_loss:0.037, val_acc:0.971]
Epoch [99/120    avg_loss:0.041, val_acc:0.971]
Epoch [100/120    avg_loss:0.046, val_acc:0.971]
Epoch [101/120    avg_loss:0.036, val_acc:0.971]
Epoch [102/120    avg_loss:0.052, val_acc:0.971]
Epoch [103/120    avg_loss:0.053, val_acc:0.971]
Epoch [104/120    avg_loss:0.037, val_acc:0.971]
Epoch [105/120    avg_loss:0.060, val_acc:0.971]
Epoch [106/120    avg_loss:0.037, val_acc:0.971]
Epoch [107/120    avg_loss:0.057, val_acc:0.971]
Epoch [108/120    avg_loss:0.054, val_acc:0.971]
Epoch [109/120    avg_loss:0.023, val_acc:0.971]
Epoch [110/120    avg_loss:0.049, val_acc:0.971]
Epoch [111/120    avg_loss:0.044, val_acc:0.971]
Epoch [112/120    avg_loss:0.021, val_acc:0.971]
Epoch [113/120    avg_loss:0.028, val_acc:0.971]
Epoch [114/120    avg_loss:0.029, val_acc:0.971]
Epoch [115/120    avg_loss:0.046, val_acc:0.971]
Epoch [116/120    avg_loss:0.059, val_acc:0.971]
Epoch [117/120    avg_loss:0.034, val_acc:0.971]
Epoch [118/120    avg_loss:0.040, val_acc:0.971]
Epoch [119/120    avg_loss:0.037, val_acc:0.971]
Epoch [120/120    avg_loss:0.033, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 226   3   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 225   1   0   0   0   0   0   0   1   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0  10 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.95280899 0.99122807 0.95541401 0.93818182
 1.         0.89617486 1.         0.99893276 1.         0.98691099
 0.98547486 1.        ]

Kappa:
0.9874180938510888
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbca3de8748>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.172, val_acc:0.617]
Epoch [2/120    avg_loss:1.504, val_acc:0.758]
Epoch [3/120    avg_loss:1.071, val_acc:0.832]
Epoch [4/120    avg_loss:0.864, val_acc:0.793]
Epoch [5/120    avg_loss:0.690, val_acc:0.898]
Epoch [6/120    avg_loss:0.607, val_acc:0.850]
Epoch [7/120    avg_loss:0.536, val_acc:0.904]
Epoch [8/120    avg_loss:0.463, val_acc:0.857]
Epoch [9/120    avg_loss:0.489, val_acc:0.900]
Epoch [10/120    avg_loss:0.466, val_acc:0.895]
Epoch [11/120    avg_loss:0.415, val_acc:0.904]
Epoch [12/120    avg_loss:0.418, val_acc:0.936]
Epoch [13/120    avg_loss:0.314, val_acc:0.926]
Epoch [14/120    avg_loss:0.316, val_acc:0.941]
Epoch [15/120    avg_loss:0.273, val_acc:0.922]
Epoch [16/120    avg_loss:0.301, val_acc:0.951]
Epoch [17/120    avg_loss:0.274, val_acc:0.967]
Epoch [18/120    avg_loss:0.242, val_acc:0.959]
Epoch [19/120    avg_loss:0.249, val_acc:0.971]
Epoch [20/120    avg_loss:0.189, val_acc:0.943]
Epoch [21/120    avg_loss:0.246, val_acc:0.945]
Epoch [22/120    avg_loss:0.264, val_acc:0.875]
Epoch [23/120    avg_loss:0.208, val_acc:0.961]
Epoch [24/120    avg_loss:0.150, val_acc:0.959]
Epoch [25/120    avg_loss:0.185, val_acc:0.959]
Epoch [26/120    avg_loss:0.239, val_acc:0.938]
Epoch [27/120    avg_loss:0.234, val_acc:0.953]
Epoch [28/120    avg_loss:0.201, val_acc:0.945]
Epoch [29/120    avg_loss:0.192, val_acc:0.961]
Epoch [30/120    avg_loss:0.148, val_acc:0.973]
Epoch [31/120    avg_loss:0.200, val_acc:0.969]
Epoch [32/120    avg_loss:0.131, val_acc:0.971]
Epoch [33/120    avg_loss:0.158, val_acc:0.979]
Epoch [34/120    avg_loss:0.113, val_acc:0.973]
Epoch [35/120    avg_loss:0.114, val_acc:0.963]
Epoch [36/120    avg_loss:0.194, val_acc:0.977]
Epoch [37/120    avg_loss:0.139, val_acc:0.963]
Epoch [38/120    avg_loss:0.101, val_acc:0.975]
Epoch [39/120    avg_loss:0.102, val_acc:0.979]
Epoch [40/120    avg_loss:0.168, val_acc:0.959]
Epoch [41/120    avg_loss:0.131, val_acc:0.959]
Epoch [42/120    avg_loss:0.144, val_acc:0.945]
Epoch [43/120    avg_loss:0.189, val_acc:0.967]
Epoch [44/120    avg_loss:0.116, val_acc:0.971]
Epoch [45/120    avg_loss:0.082, val_acc:0.977]
Epoch [46/120    avg_loss:0.090, val_acc:0.979]
Epoch [47/120    avg_loss:0.089, val_acc:0.980]
Epoch [48/120    avg_loss:0.076, val_acc:0.971]
Epoch [49/120    avg_loss:0.077, val_acc:0.988]
Epoch [50/120    avg_loss:0.052, val_acc:0.984]
Epoch [51/120    avg_loss:0.059, val_acc:0.984]
Epoch [52/120    avg_loss:0.069, val_acc:0.982]
Epoch [53/120    avg_loss:0.116, val_acc:0.984]
Epoch [54/120    avg_loss:0.082, val_acc:0.982]
Epoch [55/120    avg_loss:0.065, val_acc:0.955]
Epoch [56/120    avg_loss:0.071, val_acc:0.979]
Epoch [57/120    avg_loss:0.058, val_acc:0.977]
Epoch [58/120    avg_loss:0.052, val_acc:0.980]
Epoch [59/120    avg_loss:0.061, val_acc:0.990]
Epoch [60/120    avg_loss:0.049, val_acc:0.977]
Epoch [61/120    avg_loss:0.057, val_acc:0.980]
Epoch [62/120    avg_loss:0.065, val_acc:0.979]
Epoch [63/120    avg_loss:0.037, val_acc:0.982]
Epoch [64/120    avg_loss:0.040, val_acc:0.984]
Epoch [65/120    avg_loss:0.054, val_acc:0.961]
Epoch [66/120    avg_loss:0.110, val_acc:0.959]
Epoch [67/120    avg_loss:0.061, val_acc:0.980]
Epoch [68/120    avg_loss:0.072, val_acc:0.988]
Epoch [69/120    avg_loss:0.035, val_acc:0.992]
Epoch [70/120    avg_loss:0.028, val_acc:0.988]
Epoch [71/120    avg_loss:0.084, val_acc:0.982]
Epoch [72/120    avg_loss:0.037, val_acc:0.980]
Epoch [73/120    avg_loss:0.047, val_acc:0.984]
Epoch [74/120    avg_loss:0.035, val_acc:0.977]
Epoch [75/120    avg_loss:0.031, val_acc:0.984]
Epoch [76/120    avg_loss:0.034, val_acc:0.988]
Epoch [77/120    avg_loss:0.037, val_acc:0.982]
Epoch [78/120    avg_loss:0.054, val_acc:0.980]
Epoch [79/120    avg_loss:0.051, val_acc:0.988]
Epoch [80/120    avg_loss:0.047, val_acc:0.980]
Epoch [81/120    avg_loss:0.062, val_acc:0.979]
Epoch [82/120    avg_loss:0.038, val_acc:0.986]
Epoch [83/120    avg_loss:0.039, val_acc:0.992]
Epoch [84/120    avg_loss:0.021, val_acc:0.992]
Epoch [85/120    avg_loss:0.028, val_acc:0.992]
Epoch [86/120    avg_loss:0.017, val_acc:0.992]
Epoch [87/120    avg_loss:0.013, val_acc:0.992]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.010, val_acc:0.992]
Epoch [90/120    avg_loss:0.009, val_acc:0.992]
Epoch [91/120    avg_loss:0.015, val_acc:0.992]
Epoch [92/120    avg_loss:0.017, val_acc:0.992]
Epoch [93/120    avg_loss:0.012, val_acc:0.992]
Epoch [94/120    avg_loss:0.010, val_acc:0.992]
Epoch [95/120    avg_loss:0.016, val_acc:0.992]
Epoch [96/120    avg_loss:0.017, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.992]
Epoch [98/120    avg_loss:0.013, val_acc:0.992]
Epoch [99/120    avg_loss:0.017, val_acc:0.992]
Epoch [100/120    avg_loss:0.018, val_acc:0.992]
Epoch [101/120    avg_loss:0.009, val_acc:0.992]
Epoch [102/120    avg_loss:0.010, val_acc:0.992]
Epoch [103/120    avg_loss:0.012, val_acc:0.992]
Epoch [104/120    avg_loss:0.020, val_acc:0.992]
Epoch [105/120    avg_loss:0.015, val_acc:0.992]
Epoch [106/120    avg_loss:0.009, val_acc:0.992]
Epoch [107/120    avg_loss:0.014, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.013, val_acc:0.992]
Epoch [110/120    avg_loss:0.022, val_acc:0.992]
Epoch [111/120    avg_loss:0.010, val_acc:0.992]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.009, val_acc:0.992]
Epoch [114/120    avg_loss:0.007, val_acc:0.992]
Epoch [115/120    avg_loss:0.014, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.992]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.012, val_acc:0.992]
Epoch [119/120    avg_loss:0.011, val_acc:0.992]
Epoch [120/120    avg_loss:0.016, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.9977221  0.98230088 0.93103448 0.91666667
 1.         1.         1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9921664492620558
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd95b5b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.269, val_acc:0.504]
Epoch [2/120    avg_loss:1.482, val_acc:0.715]
Epoch [3/120    avg_loss:1.018, val_acc:0.731]
Epoch [4/120    avg_loss:0.881, val_acc:0.785]
Epoch [5/120    avg_loss:0.760, val_acc:0.825]
Epoch [6/120    avg_loss:0.688, val_acc:0.806]
Epoch [7/120    avg_loss:0.710, val_acc:0.863]
Epoch [8/120    avg_loss:0.478, val_acc:0.877]
Epoch [9/120    avg_loss:0.417, val_acc:0.883]
Epoch [10/120    avg_loss:0.429, val_acc:0.854]
Epoch [11/120    avg_loss:0.377, val_acc:0.869]
Epoch [12/120    avg_loss:0.331, val_acc:0.892]
Epoch [13/120    avg_loss:0.360, val_acc:0.850]
Epoch [14/120    avg_loss:0.372, val_acc:0.910]
Epoch [15/120    avg_loss:0.345, val_acc:0.910]
Epoch [16/120    avg_loss:0.268, val_acc:0.927]
Epoch [17/120    avg_loss:0.311, val_acc:0.917]
Epoch [18/120    avg_loss:0.339, val_acc:0.912]
Epoch [19/120    avg_loss:0.311, val_acc:0.921]
Epoch [20/120    avg_loss:0.290, val_acc:0.923]
Epoch [21/120    avg_loss:0.236, val_acc:0.938]
Epoch [22/120    avg_loss:0.230, val_acc:0.927]
Epoch [23/120    avg_loss:0.228, val_acc:0.925]
Epoch [24/120    avg_loss:0.229, val_acc:0.948]
Epoch [25/120    avg_loss:0.129, val_acc:0.946]
Epoch [26/120    avg_loss:0.186, val_acc:0.946]
Epoch [27/120    avg_loss:0.214, val_acc:0.935]
Epoch [28/120    avg_loss:0.175, val_acc:0.944]
Epoch [29/120    avg_loss:0.162, val_acc:0.946]
Epoch [30/120    avg_loss:0.150, val_acc:0.942]
Epoch [31/120    avg_loss:0.187, val_acc:0.963]
Epoch [32/120    avg_loss:0.123, val_acc:0.960]
Epoch [33/120    avg_loss:0.116, val_acc:0.944]
Epoch [34/120    avg_loss:0.112, val_acc:0.969]
Epoch [35/120    avg_loss:0.126, val_acc:0.958]
Epoch [36/120    avg_loss:0.126, val_acc:0.935]
Epoch [37/120    avg_loss:0.153, val_acc:0.960]
Epoch [38/120    avg_loss:0.109, val_acc:0.956]
Epoch [39/120    avg_loss:0.078, val_acc:0.950]
Epoch [40/120    avg_loss:0.086, val_acc:0.956]
Epoch [41/120    avg_loss:0.142, val_acc:0.960]
Epoch [42/120    avg_loss:0.124, val_acc:0.952]
Epoch [43/120    avg_loss:0.136, val_acc:0.954]
Epoch [44/120    avg_loss:0.117, val_acc:0.954]
Epoch [45/120    avg_loss:0.163, val_acc:0.963]
Epoch [46/120    avg_loss:0.079, val_acc:0.969]
Epoch [47/120    avg_loss:0.151, val_acc:0.971]
Epoch [48/120    avg_loss:0.187, val_acc:0.917]
Epoch [49/120    avg_loss:0.371, val_acc:0.923]
Epoch [50/120    avg_loss:0.238, val_acc:0.929]
Epoch [51/120    avg_loss:0.173, val_acc:0.956]
Epoch [52/120    avg_loss:0.158, val_acc:0.969]
Epoch [53/120    avg_loss:0.132, val_acc:0.935]
Epoch [54/120    avg_loss:0.181, val_acc:0.944]
Epoch [55/120    avg_loss:0.195, val_acc:0.950]
Epoch [56/120    avg_loss:0.106, val_acc:0.942]
Epoch [57/120    avg_loss:0.118, val_acc:0.967]
Epoch [58/120    avg_loss:0.141, val_acc:0.965]
Epoch [59/120    avg_loss:0.083, val_acc:0.979]
Epoch [60/120    avg_loss:0.036, val_acc:0.963]
Epoch [61/120    avg_loss:0.068, val_acc:0.981]
Epoch [62/120    avg_loss:0.047, val_acc:0.969]
Epoch [63/120    avg_loss:0.040, val_acc:0.981]
Epoch [64/120    avg_loss:0.070, val_acc:0.952]
Epoch [65/120    avg_loss:0.079, val_acc:0.965]
Epoch [66/120    avg_loss:0.063, val_acc:0.973]
Epoch [67/120    avg_loss:0.042, val_acc:0.975]
Epoch [68/120    avg_loss:0.024, val_acc:0.971]
Epoch [69/120    avg_loss:0.056, val_acc:0.975]
Epoch [70/120    avg_loss:0.045, val_acc:0.981]
Epoch [71/120    avg_loss:0.049, val_acc:0.985]
Epoch [72/120    avg_loss:0.103, val_acc:0.971]
Epoch [73/120    avg_loss:0.084, val_acc:0.971]
Epoch [74/120    avg_loss:0.064, val_acc:0.960]
Epoch [75/120    avg_loss:0.064, val_acc:0.973]
Epoch [76/120    avg_loss:0.101, val_acc:0.971]
Epoch [77/120    avg_loss:0.067, val_acc:0.973]
Epoch [78/120    avg_loss:0.044, val_acc:0.981]
Epoch [79/120    avg_loss:0.034, val_acc:0.975]
Epoch [80/120    avg_loss:0.030, val_acc:0.979]
Epoch [81/120    avg_loss:0.029, val_acc:0.979]
Epoch [82/120    avg_loss:0.027, val_acc:0.979]
Epoch [83/120    avg_loss:0.038, val_acc:0.975]
Epoch [84/120    avg_loss:0.030, val_acc:0.983]
Epoch [85/120    avg_loss:0.021, val_acc:0.988]
Epoch [86/120    avg_loss:0.026, val_acc:0.988]
Epoch [87/120    avg_loss:0.021, val_acc:0.985]
Epoch [88/120    avg_loss:0.027, val_acc:0.988]
Epoch [89/120    avg_loss:0.031, val_acc:0.988]
Epoch [90/120    avg_loss:0.015, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.990]
Epoch [92/120    avg_loss:0.024, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.988]
Epoch [94/120    avg_loss:0.017, val_acc:0.985]
Epoch [95/120    avg_loss:0.015, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.981]
Epoch [101/120    avg_loss:0.019, val_acc:0.983]
Epoch [102/120    avg_loss:0.016, val_acc:0.988]
Epoch [103/120    avg_loss:0.025, val_acc:0.985]
Epoch [104/120    avg_loss:0.020, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.017, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.014, val_acc:0.983]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.016, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.012, val_acc:0.983]
Epoch [117/120    avg_loss:0.018, val_acc:0.983]
Epoch [118/120    avg_loss:0.016, val_acc:0.983]
Epoch [119/120    avg_loss:0.031, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   8   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 223   2   0   0   0   0   0   0   2   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   1 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99708879 0.98648649 0.98004435 0.93894737 0.93090909
 0.99019608 0.9726776  1.         0.99893276 1.         0.9986755
 0.99558499 1.        ]

Kappa:
0.9902653269210412
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdec530c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.176, val_acc:0.585]
Epoch [2/120    avg_loss:1.436, val_acc:0.673]
Epoch [3/120    avg_loss:1.089, val_acc:0.765]
Epoch [4/120    avg_loss:0.888, val_acc:0.762]
Epoch [5/120    avg_loss:0.804, val_acc:0.815]
Epoch [6/120    avg_loss:0.794, val_acc:0.796]
Epoch [7/120    avg_loss:0.584, val_acc:0.904]
Epoch [8/120    avg_loss:0.466, val_acc:0.883]
Epoch [9/120    avg_loss:0.442, val_acc:0.902]
Epoch [10/120    avg_loss:0.454, val_acc:0.906]
Epoch [11/120    avg_loss:0.347, val_acc:0.935]
Epoch [12/120    avg_loss:0.353, val_acc:0.906]
Epoch [13/120    avg_loss:0.306, val_acc:0.952]
Epoch [14/120    avg_loss:0.272, val_acc:0.954]
Epoch [15/120    avg_loss:0.246, val_acc:0.956]
Epoch [16/120    avg_loss:0.309, val_acc:0.956]
Epoch [17/120    avg_loss:0.293, val_acc:0.929]
Epoch [18/120    avg_loss:0.353, val_acc:0.935]
Epoch [19/120    avg_loss:0.281, val_acc:0.923]
Epoch [20/120    avg_loss:0.261, val_acc:0.948]
Epoch [21/120    avg_loss:0.292, val_acc:0.902]
Epoch [22/120    avg_loss:0.237, val_acc:0.931]
Epoch [23/120    avg_loss:0.176, val_acc:0.967]
Epoch [24/120    avg_loss:0.180, val_acc:0.983]
Epoch [25/120    avg_loss:0.150, val_acc:0.988]
Epoch [26/120    avg_loss:0.136, val_acc:0.965]
Epoch [27/120    avg_loss:0.252, val_acc:0.967]
Epoch [28/120    avg_loss:0.173, val_acc:0.981]
Epoch [29/120    avg_loss:0.118, val_acc:0.981]
Epoch [30/120    avg_loss:0.136, val_acc:0.977]
Epoch [31/120    avg_loss:0.132, val_acc:0.983]
Epoch [32/120    avg_loss:0.087, val_acc:0.988]
Epoch [33/120    avg_loss:0.092, val_acc:0.969]
Epoch [34/120    avg_loss:0.112, val_acc:0.983]
Epoch [35/120    avg_loss:0.076, val_acc:0.990]
Epoch [36/120    avg_loss:0.061, val_acc:0.981]
Epoch [37/120    avg_loss:0.077, val_acc:0.988]
Epoch [38/120    avg_loss:0.083, val_acc:0.994]
Epoch [39/120    avg_loss:0.116, val_acc:0.983]
Epoch [40/120    avg_loss:0.096, val_acc:0.983]
Epoch [41/120    avg_loss:0.091, val_acc:0.985]
Epoch [42/120    avg_loss:0.063, val_acc:0.990]
Epoch [43/120    avg_loss:0.046, val_acc:0.992]
Epoch [44/120    avg_loss:0.049, val_acc:0.988]
Epoch [45/120    avg_loss:0.044, val_acc:0.996]
Epoch [46/120    avg_loss:0.065, val_acc:0.979]
Epoch [47/120    avg_loss:0.098, val_acc:0.990]
Epoch [48/120    avg_loss:0.056, val_acc:0.975]
Epoch [49/120    avg_loss:0.060, val_acc:0.977]
Epoch [50/120    avg_loss:0.067, val_acc:0.988]
Epoch [51/120    avg_loss:0.035, val_acc:0.994]
Epoch [52/120    avg_loss:0.040, val_acc:0.988]
Epoch [53/120    avg_loss:0.039, val_acc:0.994]
Epoch [54/120    avg_loss:0.063, val_acc:0.979]
Epoch [55/120    avg_loss:0.061, val_acc:0.992]
Epoch [56/120    avg_loss:0.052, val_acc:0.992]
Epoch [57/120    avg_loss:0.039, val_acc:0.985]
Epoch [58/120    avg_loss:0.040, val_acc:0.994]
Epoch [59/120    avg_loss:0.031, val_acc:0.996]
Epoch [60/120    avg_loss:0.025, val_acc:0.996]
Epoch [61/120    avg_loss:0.025, val_acc:0.994]
Epoch [62/120    avg_loss:0.021, val_acc:0.994]
Epoch [63/120    avg_loss:0.026, val_acc:0.994]
Epoch [64/120    avg_loss:0.016, val_acc:0.994]
Epoch [65/120    avg_loss:0.025, val_acc:0.994]
Epoch [66/120    avg_loss:0.015, val_acc:0.994]
Epoch [67/120    avg_loss:0.018, val_acc:0.994]
Epoch [68/120    avg_loss:0.017, val_acc:0.994]
Epoch [69/120    avg_loss:0.011, val_acc:0.994]
Epoch [70/120    avg_loss:0.020, val_acc:0.994]
Epoch [71/120    avg_loss:0.012, val_acc:0.994]
Epoch [72/120    avg_loss:0.018, val_acc:0.994]
Epoch [73/120    avg_loss:0.016, val_acc:0.994]
Epoch [74/120    avg_loss:0.020, val_acc:0.994]
Epoch [75/120    avg_loss:0.017, val_acc:0.994]
Epoch [76/120    avg_loss:0.016, val_acc:0.994]
Epoch [77/120    avg_loss:0.014, val_acc:0.994]
Epoch [78/120    avg_loss:0.015, val_acc:0.994]
Epoch [79/120    avg_loss:0.015, val_acc:0.994]
Epoch [80/120    avg_loss:0.012, val_acc:0.994]
Epoch [81/120    avg_loss:0.016, val_acc:0.994]
Epoch [82/120    avg_loss:0.013, val_acc:0.994]
Epoch [83/120    avg_loss:0.019, val_acc:0.994]
Epoch [84/120    avg_loss:0.015, val_acc:0.994]
Epoch [85/120    avg_loss:0.019, val_acc:0.994]
Epoch [86/120    avg_loss:0.017, val_acc:0.994]
Epoch [87/120    avg_loss:0.014, val_acc:0.994]
Epoch [88/120    avg_loss:0.015, val_acc:0.994]
Epoch [89/120    avg_loss:0.021, val_acc:0.994]
Epoch [90/120    avg_loss:0.011, val_acc:0.994]
Epoch [91/120    avg_loss:0.015, val_acc:0.994]
Epoch [92/120    avg_loss:0.016, val_acc:0.994]
Epoch [93/120    avg_loss:0.014, val_acc:0.994]
Epoch [94/120    avg_loss:0.013, val_acc:0.994]
Epoch [95/120    avg_loss:0.013, val_acc:0.994]
Epoch [96/120    avg_loss:0.012, val_acc:0.996]
Epoch [97/120    avg_loss:0.019, val_acc:0.996]
Epoch [98/120    avg_loss:0.011, val_acc:0.996]
Epoch [99/120    avg_loss:0.015, val_acc:0.996]
Epoch [100/120    avg_loss:0.016, val_acc:0.996]
Epoch [101/120    avg_loss:0.011, val_acc:0.996]
Epoch [102/120    avg_loss:0.012, val_acc:0.996]
Epoch [103/120    avg_loss:0.018, val_acc:0.996]
Epoch [104/120    avg_loss:0.016, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.016, val_acc:0.996]
Epoch [107/120    avg_loss:0.013, val_acc:0.996]
Epoch [108/120    avg_loss:0.017, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.994]
Epoch [110/120    avg_loss:0.018, val_acc:0.994]
Epoch [111/120    avg_loss:0.018, val_acc:0.994]
Epoch [112/120    avg_loss:0.011, val_acc:0.994]
Epoch [113/120    avg_loss:0.017, val_acc:0.996]
Epoch [114/120    avg_loss:0.019, val_acc:0.996]
Epoch [115/120    avg_loss:0.013, val_acc:0.996]
Epoch [116/120    avg_loss:0.018, val_acc:0.996]
Epoch [117/120    avg_loss:0.016, val_acc:0.996]
Epoch [118/120    avg_loss:0.017, val_acc:0.996]
Epoch [119/120    avg_loss:0.011, val_acc:0.996]
Epoch [120/120    avg_loss:0.016, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         0.9977221  1.         0.96017699 0.93835616
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9954897612620449
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1731346748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.243, val_acc:0.487]
Epoch [2/120    avg_loss:1.553, val_acc:0.656]
Epoch [3/120    avg_loss:1.080, val_acc:0.767]
Epoch [4/120    avg_loss:0.841, val_acc:0.769]
Epoch [5/120    avg_loss:0.804, val_acc:0.752]
Epoch [6/120    avg_loss:0.646, val_acc:0.829]
Epoch [7/120    avg_loss:0.634, val_acc:0.863]
Epoch [8/120    avg_loss:0.628, val_acc:0.854]
Epoch [9/120    avg_loss:0.535, val_acc:0.894]
Epoch [10/120    avg_loss:0.420, val_acc:0.898]
Epoch [11/120    avg_loss:0.415, val_acc:0.923]
Epoch [12/120    avg_loss:0.324, val_acc:0.919]
Epoch [13/120    avg_loss:0.335, val_acc:0.923]
Epoch [14/120    avg_loss:0.310, val_acc:0.881]
Epoch [15/120    avg_loss:0.261, val_acc:0.935]
Epoch [16/120    avg_loss:0.226, val_acc:0.948]
Epoch [17/120    avg_loss:0.254, val_acc:0.950]
Epoch [18/120    avg_loss:0.234, val_acc:0.940]
Epoch [19/120    avg_loss:0.255, val_acc:0.931]
Epoch [20/120    avg_loss:0.204, val_acc:0.944]
Epoch [21/120    avg_loss:0.196, val_acc:0.948]
Epoch [22/120    avg_loss:0.148, val_acc:0.960]
Epoch [23/120    avg_loss:0.143, val_acc:0.956]
Epoch [24/120    avg_loss:0.143, val_acc:0.965]
Epoch [25/120    avg_loss:0.162, val_acc:0.971]
Epoch [26/120    avg_loss:0.140, val_acc:0.960]
Epoch [27/120    avg_loss:0.124, val_acc:0.969]
Epoch [28/120    avg_loss:0.103, val_acc:0.944]
Epoch [29/120    avg_loss:0.128, val_acc:0.946]
Epoch [30/120    avg_loss:0.126, val_acc:0.952]
Epoch [31/120    avg_loss:0.164, val_acc:0.956]
Epoch [32/120    avg_loss:0.171, val_acc:0.952]
Epoch [33/120    avg_loss:0.179, val_acc:0.956]
Epoch [34/120    avg_loss:0.120, val_acc:0.954]
Epoch [35/120    avg_loss:0.105, val_acc:0.950]
Epoch [36/120    avg_loss:0.120, val_acc:0.965]
Epoch [37/120    avg_loss:0.078, val_acc:0.969]
Epoch [38/120    avg_loss:0.181, val_acc:0.954]
Epoch [39/120    avg_loss:0.096, val_acc:0.971]
Epoch [40/120    avg_loss:0.087, val_acc:0.971]
Epoch [41/120    avg_loss:0.059, val_acc:0.967]
Epoch [42/120    avg_loss:0.052, val_acc:0.971]
Epoch [43/120    avg_loss:0.048, val_acc:0.969]
Epoch [44/120    avg_loss:0.049, val_acc:0.967]
Epoch [45/120    avg_loss:0.050, val_acc:0.973]
Epoch [46/120    avg_loss:0.047, val_acc:0.973]
Epoch [47/120    avg_loss:0.051, val_acc:0.969]
Epoch [48/120    avg_loss:0.048, val_acc:0.973]
Epoch [49/120    avg_loss:0.034, val_acc:0.971]
Epoch [50/120    avg_loss:0.041, val_acc:0.967]
Epoch [51/120    avg_loss:0.050, val_acc:0.967]
Epoch [52/120    avg_loss:0.041, val_acc:0.969]
Epoch [53/120    avg_loss:0.035, val_acc:0.967]
Epoch [54/120    avg_loss:0.047, val_acc:0.975]
Epoch [55/120    avg_loss:0.038, val_acc:0.977]
Epoch [56/120    avg_loss:0.028, val_acc:0.977]
Epoch [57/120    avg_loss:0.040, val_acc:0.977]
Epoch [58/120    avg_loss:0.041, val_acc:0.977]
Epoch [59/120    avg_loss:0.028, val_acc:0.977]
Epoch [60/120    avg_loss:0.036, val_acc:0.975]
Epoch [61/120    avg_loss:0.030, val_acc:0.977]
Epoch [62/120    avg_loss:0.039, val_acc:0.977]
Epoch [63/120    avg_loss:0.034, val_acc:0.981]
Epoch [64/120    avg_loss:0.036, val_acc:0.979]
Epoch [65/120    avg_loss:0.046, val_acc:0.981]
Epoch [66/120    avg_loss:0.022, val_acc:0.977]
Epoch [67/120    avg_loss:0.042, val_acc:0.981]
Epoch [68/120    avg_loss:0.035, val_acc:0.981]
Epoch [69/120    avg_loss:0.032, val_acc:0.979]
Epoch [70/120    avg_loss:0.036, val_acc:0.981]
Epoch [71/120    avg_loss:0.059, val_acc:0.981]
Epoch [72/120    avg_loss:0.046, val_acc:0.975]
Epoch [73/120    avg_loss:0.055, val_acc:0.979]
Epoch [74/120    avg_loss:0.038, val_acc:0.975]
Epoch [75/120    avg_loss:0.044, val_acc:0.975]
Epoch [76/120    avg_loss:0.036, val_acc:0.971]
Epoch [77/120    avg_loss:0.046, val_acc:0.975]
Epoch [78/120    avg_loss:0.026, val_acc:0.977]
Epoch [79/120    avg_loss:0.037, val_acc:0.977]
Epoch [80/120    avg_loss:0.032, val_acc:0.979]
Epoch [81/120    avg_loss:0.030, val_acc:0.981]
Epoch [82/120    avg_loss:0.026, val_acc:0.979]
Epoch [83/120    avg_loss:0.043, val_acc:0.981]
Epoch [84/120    avg_loss:0.032, val_acc:0.979]
Epoch [85/120    avg_loss:0.042, val_acc:0.979]
Epoch [86/120    avg_loss:0.034, val_acc:0.977]
Epoch [87/120    avg_loss:0.049, val_acc:0.975]
Epoch [88/120    avg_loss:0.045, val_acc:0.979]
Epoch [89/120    avg_loss:0.029, val_acc:0.979]
Epoch [90/120    avg_loss:0.029, val_acc:0.975]
Epoch [91/120    avg_loss:0.031, val_acc:0.973]
Epoch [92/120    avg_loss:0.038, val_acc:0.973]
Epoch [93/120    avg_loss:0.026, val_acc:0.973]
Epoch [94/120    avg_loss:0.047, val_acc:0.975]
Epoch [95/120    avg_loss:0.044, val_acc:0.975]
Epoch [96/120    avg_loss:0.028, val_acc:0.977]
Epoch [97/120    avg_loss:0.036, val_acc:0.977]
Epoch [98/120    avg_loss:0.052, val_acc:0.977]
Epoch [99/120    avg_loss:0.025, val_acc:0.977]
Epoch [100/120    avg_loss:0.031, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.977]
Epoch [102/120    avg_loss:0.033, val_acc:0.977]
Epoch [103/120    avg_loss:0.035, val_acc:0.977]
Epoch [104/120    avg_loss:0.038, val_acc:0.977]
Epoch [105/120    avg_loss:0.019, val_acc:0.977]
Epoch [106/120    avg_loss:0.043, val_acc:0.977]
Epoch [107/120    avg_loss:0.018, val_acc:0.979]
Epoch [108/120    avg_loss:0.028, val_acc:0.979]
Epoch [109/120    avg_loss:0.033, val_acc:0.979]
Epoch [110/120    avg_loss:0.030, val_acc:0.979]
Epoch [111/120    avg_loss:0.029, val_acc:0.979]
Epoch [112/120    avg_loss:0.023, val_acc:0.979]
Epoch [113/120    avg_loss:0.053, val_acc:0.979]
Epoch [114/120    avg_loss:0.034, val_acc:0.979]
Epoch [115/120    avg_loss:0.033, val_acc:0.979]
Epoch [116/120    avg_loss:0.031, val_acc:0.979]
Epoch [117/120    avg_loss:0.026, val_acc:0.979]
Epoch [118/120    avg_loss:0.023, val_acc:0.979]
Epoch [119/120    avg_loss:0.029, val_acc:0.979]
Epoch [120/120    avg_loss:0.021, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.98426966 0.98230088 0.95032397 0.95138889
 1.         0.96132597 1.         1.         1.         0.99341238
 0.99334812 1.        ]

Kappa:
0.9916912869971197
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe9e028b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.279, val_acc:0.594]
Epoch [2/120    avg_loss:1.611, val_acc:0.585]
Epoch [3/120    avg_loss:1.312, val_acc:0.790]
Epoch [4/120    avg_loss:1.002, val_acc:0.835]
Epoch [5/120    avg_loss:0.859, val_acc:0.858]
Epoch [6/120    avg_loss:0.649, val_acc:0.860]
Epoch [7/120    avg_loss:0.521, val_acc:0.910]
Epoch [8/120    avg_loss:0.540, val_acc:0.873]
Epoch [9/120    avg_loss:0.423, val_acc:0.910]
Epoch [10/120    avg_loss:0.409, val_acc:0.900]
Epoch [11/120    avg_loss:0.382, val_acc:0.854]
Epoch [12/120    avg_loss:0.388, val_acc:0.942]
Epoch [13/120    avg_loss:0.335, val_acc:0.944]
Epoch [14/120    avg_loss:0.369, val_acc:0.923]
Epoch [15/120    avg_loss:0.332, val_acc:0.929]
Epoch [16/120    avg_loss:0.277, val_acc:0.938]
Epoch [17/120    avg_loss:0.260, val_acc:0.967]
Epoch [18/120    avg_loss:0.198, val_acc:0.956]
Epoch [19/120    avg_loss:0.227, val_acc:0.967]
Epoch [20/120    avg_loss:0.296, val_acc:0.925]
Epoch [21/120    avg_loss:0.227, val_acc:0.954]
Epoch [22/120    avg_loss:0.308, val_acc:0.975]
Epoch [23/120    avg_loss:0.213, val_acc:0.965]
Epoch [24/120    avg_loss:0.281, val_acc:0.963]
Epoch [25/120    avg_loss:0.240, val_acc:0.950]
Epoch [26/120    avg_loss:0.307, val_acc:0.979]
Epoch [27/120    avg_loss:0.192, val_acc:0.983]
Epoch [28/120    avg_loss:0.129, val_acc:0.988]
Epoch [29/120    avg_loss:0.127, val_acc:0.983]
Epoch [30/120    avg_loss:0.120, val_acc:0.990]
Epoch [31/120    avg_loss:0.127, val_acc:0.981]
Epoch [32/120    avg_loss:0.103, val_acc:0.992]
Epoch [33/120    avg_loss:0.116, val_acc:0.990]
Epoch [34/120    avg_loss:0.055, val_acc:0.994]
Epoch [35/120    avg_loss:0.065, val_acc:0.992]
Epoch [36/120    avg_loss:0.079, val_acc:0.990]
Epoch [37/120    avg_loss:0.077, val_acc:0.992]
Epoch [38/120    avg_loss:0.092, val_acc:0.983]
Epoch [39/120    avg_loss:0.083, val_acc:0.988]
Epoch [40/120    avg_loss:0.078, val_acc:0.981]
Epoch [41/120    avg_loss:0.071, val_acc:0.990]
Epoch [42/120    avg_loss:0.063, val_acc:0.985]
Epoch [43/120    avg_loss:0.064, val_acc:0.988]
Epoch [44/120    avg_loss:0.045, val_acc:0.994]
Epoch [45/120    avg_loss:0.043, val_acc:0.990]
Epoch [46/120    avg_loss:0.034, val_acc:0.983]
Epoch [47/120    avg_loss:0.056, val_acc:0.983]
Epoch [48/120    avg_loss:0.062, val_acc:0.994]
Epoch [49/120    avg_loss:0.056, val_acc:0.994]
Epoch [50/120    avg_loss:0.053, val_acc:0.990]
Epoch [51/120    avg_loss:0.064, val_acc:0.990]
Epoch [52/120    avg_loss:0.028, val_acc:0.992]
Epoch [53/120    avg_loss:0.072, val_acc:0.985]
Epoch [54/120    avg_loss:0.038, val_acc:0.990]
Epoch [55/120    avg_loss:0.033, val_acc:0.985]
Epoch [56/120    avg_loss:0.022, val_acc:0.994]
Epoch [57/120    avg_loss:0.020, val_acc:0.996]
Epoch [58/120    avg_loss:0.014, val_acc:0.996]
Epoch [59/120    avg_loss:0.027, val_acc:0.998]
Epoch [60/120    avg_loss:0.026, val_acc:0.994]
Epoch [61/120    avg_loss:0.034, val_acc:0.985]
Epoch [62/120    avg_loss:0.028, val_acc:0.988]
Epoch [63/120    avg_loss:0.030, val_acc:0.985]
Epoch [64/120    avg_loss:0.034, val_acc:0.988]
Epoch [65/120    avg_loss:0.047, val_acc:0.990]
Epoch [66/120    avg_loss:0.035, val_acc:0.998]
Epoch [67/120    avg_loss:0.029, val_acc:0.996]
Epoch [68/120    avg_loss:0.038, val_acc:0.998]
Epoch [69/120    avg_loss:0.030, val_acc:0.990]
Epoch [70/120    avg_loss:0.038, val_acc:0.990]
Epoch [71/120    avg_loss:0.033, val_acc:0.996]
Epoch [72/120    avg_loss:0.035, val_acc:0.960]
Epoch [73/120    avg_loss:0.040, val_acc:0.990]
Epoch [74/120    avg_loss:0.021, val_acc:0.996]
Epoch [75/120    avg_loss:0.024, val_acc:0.992]
Epoch [76/120    avg_loss:0.023, val_acc:0.992]
Epoch [77/120    avg_loss:0.025, val_acc:0.992]
Epoch [78/120    avg_loss:0.015, val_acc:0.992]
Epoch [79/120    avg_loss:0.027, val_acc:0.992]
Epoch [80/120    avg_loss:0.038, val_acc:0.996]
Epoch [81/120    avg_loss:0.063, val_acc:0.985]
Epoch [82/120    avg_loss:0.033, val_acc:0.985]
Epoch [83/120    avg_loss:0.029, val_acc:0.992]
Epoch [84/120    avg_loss:0.013, val_acc:0.992]
Epoch [85/120    avg_loss:0.012, val_acc:0.994]
Epoch [86/120    avg_loss:0.008, val_acc:0.994]
Epoch [87/120    avg_loss:0.013, val_acc:0.994]
Epoch [88/120    avg_loss:0.012, val_acc:0.996]
Epoch [89/120    avg_loss:0.014, val_acc:0.996]
Epoch [90/120    avg_loss:0.017, val_acc:0.996]
Epoch [91/120    avg_loss:0.019, val_acc:0.996]
Epoch [92/120    avg_loss:0.007, val_acc:0.996]
Epoch [93/120    avg_loss:0.018, val_acc:0.996]
Epoch [94/120    avg_loss:0.010, val_acc:0.996]
Epoch [95/120    avg_loss:0.008, val_acc:0.996]
Epoch [96/120    avg_loss:0.008, val_acc:0.996]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.008, val_acc:0.996]
Epoch [99/120    avg_loss:0.011, val_acc:0.996]
Epoch [100/120    avg_loss:0.012, val_acc:0.996]
Epoch [101/120    avg_loss:0.007, val_acc:0.996]
Epoch [102/120    avg_loss:0.012, val_acc:0.996]
Epoch [103/120    avg_loss:0.009, val_acc:0.996]
Epoch [104/120    avg_loss:0.013, val_acc:0.996]
Epoch [105/120    avg_loss:0.010, val_acc:0.996]
Epoch [106/120    avg_loss:0.009, val_acc:0.996]
Epoch [107/120    avg_loss:0.014, val_acc:0.996]
Epoch [108/120    avg_loss:0.007, val_acc:0.996]
Epoch [109/120    avg_loss:0.008, val_acc:0.996]
Epoch [110/120    avg_loss:0.010, val_acc:0.996]
Epoch [111/120    avg_loss:0.008, val_acc:0.996]
Epoch [112/120    avg_loss:0.010, val_acc:0.996]
Epoch [113/120    avg_loss:0.007, val_acc:0.996]
Epoch [114/120    avg_loss:0.008, val_acc:0.996]
Epoch [115/120    avg_loss:0.012, val_acc:0.996]
Epoch [116/120    avg_loss:0.007, val_acc:0.996]
Epoch [117/120    avg_loss:0.011, val_acc:0.996]
Epoch [118/120    avg_loss:0.008, val_acc:0.996]
Epoch [119/120    avg_loss:0.007, val_acc:0.996]
Epoch [120/120    avg_loss:0.008, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 217   9   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.99771167 0.99782135 0.95175439 0.92682927
 1.         1.         1.         0.99893276 1.         0.9843342
 0.984375   1.        ]

Kappa:
0.9914542537863922
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f09a0af57b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.252, val_acc:0.500]
Epoch [2/120    avg_loss:1.515, val_acc:0.719]
Epoch [3/120    avg_loss:1.089, val_acc:0.762]
Epoch [4/120    avg_loss:0.810, val_acc:0.819]
Epoch [5/120    avg_loss:0.774, val_acc:0.842]
Epoch [6/120    avg_loss:0.542, val_acc:0.844]
Epoch [7/120    avg_loss:0.487, val_acc:0.921]
Epoch [8/120    avg_loss:0.447, val_acc:0.898]
Epoch [9/120    avg_loss:0.499, val_acc:0.927]
Epoch [10/120    avg_loss:0.338, val_acc:0.950]
Epoch [11/120    avg_loss:0.385, val_acc:0.948]
Epoch [12/120    avg_loss:0.312, val_acc:0.908]
Epoch [13/120    avg_loss:0.285, val_acc:0.944]
Epoch [14/120    avg_loss:0.354, val_acc:0.954]
Epoch [15/120    avg_loss:0.264, val_acc:0.954]
Epoch [16/120    avg_loss:0.263, val_acc:0.956]
Epoch [17/120    avg_loss:0.268, val_acc:0.952]
Epoch [18/120    avg_loss:0.228, val_acc:0.935]
Epoch [19/120    avg_loss:0.177, val_acc:0.948]
Epoch [20/120    avg_loss:0.172, val_acc:0.948]
Epoch [21/120    avg_loss:0.208, val_acc:0.954]
Epoch [22/120    avg_loss:0.181, val_acc:0.969]
Epoch [23/120    avg_loss:0.207, val_acc:0.952]
Epoch [24/120    avg_loss:0.177, val_acc:0.969]
Epoch [25/120    avg_loss:0.152, val_acc:0.950]
Epoch [26/120    avg_loss:0.178, val_acc:0.967]
Epoch [27/120    avg_loss:0.105, val_acc:0.971]
Epoch [28/120    avg_loss:0.117, val_acc:0.954]
Epoch [29/120    avg_loss:0.161, val_acc:0.969]
Epoch [30/120    avg_loss:0.096, val_acc:0.971]
Epoch [31/120    avg_loss:0.087, val_acc:0.979]
Epoch [32/120    avg_loss:0.063, val_acc:0.975]
Epoch [33/120    avg_loss:0.124, val_acc:0.983]
Epoch [34/120    avg_loss:0.110, val_acc:0.977]
Epoch [35/120    avg_loss:0.073, val_acc:0.973]
Epoch [36/120    avg_loss:0.055, val_acc:0.981]
Epoch [37/120    avg_loss:0.047, val_acc:0.971]
Epoch [38/120    avg_loss:0.075, val_acc:0.990]
Epoch [39/120    avg_loss:0.047, val_acc:0.981]
Epoch [40/120    avg_loss:0.051, val_acc:0.981]
Epoch [41/120    avg_loss:0.122, val_acc:0.975]
Epoch [42/120    avg_loss:0.194, val_acc:0.967]
Epoch [43/120    avg_loss:0.085, val_acc:0.973]
Epoch [44/120    avg_loss:0.080, val_acc:0.979]
Epoch [45/120    avg_loss:0.076, val_acc:0.975]
Epoch [46/120    avg_loss:0.093, val_acc:0.975]
Epoch [47/120    avg_loss:0.062, val_acc:0.983]
Epoch [48/120    avg_loss:0.092, val_acc:0.958]
Epoch [49/120    avg_loss:0.092, val_acc:0.965]
Epoch [50/120    avg_loss:0.072, val_acc:0.973]
Epoch [51/120    avg_loss:0.098, val_acc:0.973]
Epoch [52/120    avg_loss:0.060, val_acc:0.975]
Epoch [53/120    avg_loss:0.040, val_acc:0.981]
Epoch [54/120    avg_loss:0.042, val_acc:0.985]
Epoch [55/120    avg_loss:0.026, val_acc:0.983]
Epoch [56/120    avg_loss:0.024, val_acc:0.983]
Epoch [57/120    avg_loss:0.037, val_acc:0.983]
Epoch [58/120    avg_loss:0.028, val_acc:0.983]
Epoch [59/120    avg_loss:0.030, val_acc:0.985]
Epoch [60/120    avg_loss:0.045, val_acc:0.979]
Epoch [61/120    avg_loss:0.027, val_acc:0.979]
Epoch [62/120    avg_loss:0.029, val_acc:0.979]
Epoch [63/120    avg_loss:0.025, val_acc:0.979]
Epoch [64/120    avg_loss:0.023, val_acc:0.979]
Epoch [65/120    avg_loss:0.022, val_acc:0.979]
Epoch [66/120    avg_loss:0.024, val_acc:0.979]
Epoch [67/120    avg_loss:0.025, val_acc:0.979]
Epoch [68/120    avg_loss:0.017, val_acc:0.979]
Epoch [69/120    avg_loss:0.022, val_acc:0.979]
Epoch [70/120    avg_loss:0.031, val_acc:0.979]
Epoch [71/120    avg_loss:0.020, val_acc:0.979]
Epoch [72/120    avg_loss:0.037, val_acc:0.981]
Epoch [73/120    avg_loss:0.019, val_acc:0.981]
Epoch [74/120    avg_loss:0.016, val_acc:0.981]
Epoch [75/120    avg_loss:0.028, val_acc:0.981]
Epoch [76/120    avg_loss:0.025, val_acc:0.981]
Epoch [77/120    avg_loss:0.029, val_acc:0.981]
Epoch [78/120    avg_loss:0.020, val_acc:0.981]
Epoch [79/120    avg_loss:0.022, val_acc:0.981]
Epoch [80/120    avg_loss:0.029, val_acc:0.981]
Epoch [81/120    avg_loss:0.018, val_acc:0.981]
Epoch [82/120    avg_loss:0.033, val_acc:0.981]
Epoch [83/120    avg_loss:0.026, val_acc:0.981]
Epoch [84/120    avg_loss:0.045, val_acc:0.981]
Epoch [85/120    avg_loss:0.021, val_acc:0.981]
Epoch [86/120    avg_loss:0.021, val_acc:0.981]
Epoch [87/120    avg_loss:0.042, val_acc:0.981]
Epoch [88/120    avg_loss:0.024, val_acc:0.981]
Epoch [89/120    avg_loss:0.030, val_acc:0.981]
Epoch [90/120    avg_loss:0.018, val_acc:0.981]
Epoch [91/120    avg_loss:0.019, val_acc:0.981]
Epoch [92/120    avg_loss:0.040, val_acc:0.981]
Epoch [93/120    avg_loss:0.022, val_acc:0.981]
Epoch [94/120    avg_loss:0.040, val_acc:0.981]
Epoch [95/120    avg_loss:0.037, val_acc:0.981]
Epoch [96/120    avg_loss:0.019, val_acc:0.981]
Epoch [97/120    avg_loss:0.017, val_acc:0.981]
Epoch [98/120    avg_loss:0.030, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.981]
Epoch [100/120    avg_loss:0.019, val_acc:0.981]
Epoch [101/120    avg_loss:0.022, val_acc:0.981]
Epoch [102/120    avg_loss:0.027, val_acc:0.981]
Epoch [103/120    avg_loss:0.021, val_acc:0.981]
Epoch [104/120    avg_loss:0.029, val_acc:0.981]
Epoch [105/120    avg_loss:0.042, val_acc:0.981]
Epoch [106/120    avg_loss:0.025, val_acc:0.981]
Epoch [107/120    avg_loss:0.021, val_acc:0.981]
Epoch [108/120    avg_loss:0.021, val_acc:0.981]
Epoch [109/120    avg_loss:0.027, val_acc:0.981]
Epoch [110/120    avg_loss:0.022, val_acc:0.981]
Epoch [111/120    avg_loss:0.041, val_acc:0.981]
Epoch [112/120    avg_loss:0.027, val_acc:0.981]
Epoch [113/120    avg_loss:0.022, val_acc:0.981]
Epoch [114/120    avg_loss:0.026, val_acc:0.981]
Epoch [115/120    avg_loss:0.019, val_acc:0.981]
Epoch [116/120    avg_loss:0.026, val_acc:0.981]
Epoch [117/120    avg_loss:0.020, val_acc:0.981]
Epoch [118/120    avg_loss:0.028, val_acc:0.981]
Epoch [119/120    avg_loss:0.024, val_acc:0.981]
Epoch [120/120    avg_loss:0.030, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 216  14   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.9977221  0.96860987 0.93162393 0.9559322
 0.98771499 0.99465241 1.         1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9897931047606939
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6dacce57b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.170, val_acc:0.640]
Epoch [2/120    avg_loss:1.505, val_acc:0.633]
Epoch [3/120    avg_loss:1.108, val_acc:0.812]
Epoch [4/120    avg_loss:0.843, val_acc:0.794]
Epoch [5/120    avg_loss:0.698, val_acc:0.871]
Epoch [6/120    avg_loss:0.517, val_acc:0.904]
Epoch [7/120    avg_loss:0.400, val_acc:0.910]
Epoch [8/120    avg_loss:0.436, val_acc:0.898]
Epoch [9/120    avg_loss:0.530, val_acc:0.917]
Epoch [10/120    avg_loss:0.377, val_acc:0.906]
Epoch [11/120    avg_loss:0.381, val_acc:0.894]
Epoch [12/120    avg_loss:0.266, val_acc:0.938]
Epoch [13/120    avg_loss:0.363, val_acc:0.950]
Epoch [14/120    avg_loss:0.304, val_acc:0.931]
Epoch [15/120    avg_loss:0.341, val_acc:0.921]
Epoch [16/120    avg_loss:0.298, val_acc:0.944]
Epoch [17/120    avg_loss:0.241, val_acc:0.958]
Epoch [18/120    avg_loss:0.156, val_acc:0.950]
Epoch [19/120    avg_loss:0.265, val_acc:0.969]
Epoch [20/120    avg_loss:0.186, val_acc:0.963]
Epoch [21/120    avg_loss:0.298, val_acc:0.925]
Epoch [22/120    avg_loss:0.355, val_acc:0.910]
Epoch [23/120    avg_loss:0.257, val_acc:0.940]
Epoch [24/120    avg_loss:0.189, val_acc:0.958]
Epoch [25/120    avg_loss:0.195, val_acc:0.963]
Epoch [26/120    avg_loss:0.179, val_acc:0.900]
Epoch [27/120    avg_loss:0.206, val_acc:0.965]
Epoch [28/120    avg_loss:0.136, val_acc:0.975]
Epoch [29/120    avg_loss:0.131, val_acc:0.969]
Epoch [30/120    avg_loss:0.121, val_acc:0.983]
Epoch [31/120    avg_loss:0.063, val_acc:0.983]
Epoch [32/120    avg_loss:0.074, val_acc:0.977]
Epoch [33/120    avg_loss:0.106, val_acc:0.967]
Epoch [34/120    avg_loss:0.085, val_acc:0.963]
Epoch [35/120    avg_loss:0.153, val_acc:0.963]
Epoch [36/120    avg_loss:0.097, val_acc:0.971]
Epoch [37/120    avg_loss:0.092, val_acc:0.983]
Epoch [38/120    avg_loss:0.068, val_acc:0.988]
Epoch [39/120    avg_loss:0.071, val_acc:0.985]
Epoch [40/120    avg_loss:0.076, val_acc:0.963]
Epoch [41/120    avg_loss:0.040, val_acc:0.983]
Epoch [42/120    avg_loss:0.041, val_acc:0.979]
Epoch [43/120    avg_loss:0.048, val_acc:0.973]
Epoch [44/120    avg_loss:0.037, val_acc:0.981]
Epoch [45/120    avg_loss:0.113, val_acc:0.994]
Epoch [46/120    avg_loss:0.172, val_acc:0.979]
Epoch [47/120    avg_loss:0.122, val_acc:0.983]
Epoch [48/120    avg_loss:0.045, val_acc:0.979]
Epoch [49/120    avg_loss:0.033, val_acc:0.994]
Epoch [50/120    avg_loss:0.047, val_acc:0.985]
Epoch [51/120    avg_loss:0.029, val_acc:0.988]
Epoch [52/120    avg_loss:0.033, val_acc:0.990]
Epoch [53/120    avg_loss:0.048, val_acc:0.985]
Epoch [54/120    avg_loss:0.061, val_acc:0.983]
Epoch [55/120    avg_loss:0.069, val_acc:0.992]
Epoch [56/120    avg_loss:0.041, val_acc:0.985]
Epoch [57/120    avg_loss:0.040, val_acc:0.990]
Epoch [58/120    avg_loss:0.045, val_acc:0.994]
Epoch [59/120    avg_loss:0.062, val_acc:0.981]
Epoch [60/120    avg_loss:0.049, val_acc:0.990]
Epoch [61/120    avg_loss:0.029, val_acc:0.983]
Epoch [62/120    avg_loss:0.056, val_acc:0.981]
Epoch [63/120    avg_loss:0.048, val_acc:0.979]
Epoch [64/120    avg_loss:0.032, val_acc:0.990]
Epoch [65/120    avg_loss:0.029, val_acc:0.996]
Epoch [66/120    avg_loss:0.022, val_acc:0.983]
Epoch [67/120    avg_loss:0.025, val_acc:0.985]
Epoch [68/120    avg_loss:0.015, val_acc:0.988]
Epoch [69/120    avg_loss:0.029, val_acc:0.990]
Epoch [70/120    avg_loss:0.024, val_acc:0.994]
Epoch [71/120    avg_loss:0.021, val_acc:0.996]
Epoch [72/120    avg_loss:0.034, val_acc:0.990]
Epoch [73/120    avg_loss:0.015, val_acc:0.975]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.029, val_acc:0.994]
Epoch [76/120    avg_loss:0.012, val_acc:0.994]
Epoch [77/120    avg_loss:0.020, val_acc:1.000]
Epoch [78/120    avg_loss:0.017, val_acc:0.994]
Epoch [79/120    avg_loss:0.008, val_acc:0.994]
Epoch [80/120    avg_loss:0.017, val_acc:0.979]
Epoch [81/120    avg_loss:0.015, val_acc:0.994]
Epoch [82/120    avg_loss:0.018, val_acc:0.996]
Epoch [83/120    avg_loss:0.014, val_acc:0.996]
Epoch [84/120    avg_loss:0.010, val_acc:0.996]
Epoch [85/120    avg_loss:0.008, val_acc:0.992]
Epoch [86/120    avg_loss:0.029, val_acc:0.992]
Epoch [87/120    avg_loss:0.013, val_acc:0.996]
Epoch [88/120    avg_loss:0.013, val_acc:0.988]
Epoch [89/120    avg_loss:0.019, val_acc:0.998]
Epoch [90/120    avg_loss:0.009, val_acc:0.998]
Epoch [91/120    avg_loss:0.006, val_acc:0.998]
Epoch [92/120    avg_loss:0.009, val_acc:0.998]
Epoch [93/120    avg_loss:0.010, val_acc:0.998]
Epoch [94/120    avg_loss:0.009, val_acc:0.998]
Epoch [95/120    avg_loss:0.010, val_acc:0.998]
Epoch [96/120    avg_loss:0.007, val_acc:0.998]
Epoch [97/120    avg_loss:0.010, val_acc:0.998]
Epoch [98/120    avg_loss:0.006, val_acc:0.998]
Epoch [99/120    avg_loss:0.009, val_acc:1.000]
Epoch [100/120    avg_loss:0.008, val_acc:1.000]
Epoch [101/120    avg_loss:0.008, val_acc:1.000]
Epoch [102/120    avg_loss:0.005, val_acc:1.000]
Epoch [103/120    avg_loss:0.008, val_acc:1.000]
Epoch [104/120    avg_loss:0.011, val_acc:0.998]
Epoch [105/120    avg_loss:0.017, val_acc:0.998]
Epoch [106/120    avg_loss:0.004, val_acc:0.996]
Epoch [107/120    avg_loss:0.008, val_acc:0.996]
Epoch [108/120    avg_loss:0.008, val_acc:0.998]
Epoch [109/120    avg_loss:0.006, val_acc:0.998]
Epoch [110/120    avg_loss:0.006, val_acc:0.998]
Epoch [111/120    avg_loss:0.004, val_acc:0.998]
Epoch [112/120    avg_loss:0.012, val_acc:1.000]
Epoch [113/120    avg_loss:0.004, val_acc:1.000]
Epoch [114/120    avg_loss:0.010, val_acc:1.000]
Epoch [115/120    avg_loss:0.009, val_acc:1.000]
Epoch [116/120    avg_loss:0.008, val_acc:1.000]
Epoch [117/120    avg_loss:0.009, val_acc:1.000]
Epoch [118/120    avg_loss:0.003, val_acc:1.000]
Epoch [119/120    avg_loss:0.007, val_acc:0.998]
Epoch [120/120    avg_loss:0.008, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 1.         1.         0.99563319 0.96875    0.95973154
 1.         1.         1.         1.         1.         0.99210526
 0.99333333 1.        ]

Kappa:
0.9952526395425366
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f942b04f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.319, val_acc:0.527]
Epoch [2/120    avg_loss:1.576, val_acc:0.625]
Epoch [3/120    avg_loss:1.158, val_acc:0.715]
Epoch [4/120    avg_loss:0.819, val_acc:0.810]
Epoch [5/120    avg_loss:0.677, val_acc:0.821]
Epoch [6/120    avg_loss:0.601, val_acc:0.858]
Epoch [7/120    avg_loss:0.564, val_acc:0.885]
Epoch [8/120    avg_loss:0.446, val_acc:0.873]
Epoch [9/120    avg_loss:0.446, val_acc:0.902]
Epoch [10/120    avg_loss:0.364, val_acc:0.898]
Epoch [11/120    avg_loss:0.382, val_acc:0.919]
Epoch [12/120    avg_loss:0.337, val_acc:0.912]
Epoch [13/120    avg_loss:0.287, val_acc:0.929]
Epoch [14/120    avg_loss:0.314, val_acc:0.917]
Epoch [15/120    avg_loss:0.290, val_acc:0.919]
Epoch [16/120    avg_loss:0.275, val_acc:0.942]
Epoch [17/120    avg_loss:0.318, val_acc:0.921]
Epoch [18/120    avg_loss:0.231, val_acc:0.938]
Epoch [19/120    avg_loss:0.274, val_acc:0.927]
Epoch [20/120    avg_loss:0.291, val_acc:0.935]
Epoch [21/120    avg_loss:0.213, val_acc:0.908]
Epoch [22/120    avg_loss:0.316, val_acc:0.944]
Epoch [23/120    avg_loss:0.204, val_acc:0.973]
Epoch [24/120    avg_loss:0.200, val_acc:0.948]
Epoch [25/120    avg_loss:0.185, val_acc:0.942]
Epoch [26/120    avg_loss:0.154, val_acc:0.958]
Epoch [27/120    avg_loss:0.182, val_acc:0.963]
Epoch [28/120    avg_loss:0.171, val_acc:0.950]
Epoch [29/120    avg_loss:0.142, val_acc:0.958]
Epoch [30/120    avg_loss:0.147, val_acc:0.952]
Epoch [31/120    avg_loss:0.129, val_acc:0.969]
Epoch [32/120    avg_loss:0.149, val_acc:0.950]
Epoch [33/120    avg_loss:0.156, val_acc:0.960]
Epoch [34/120    avg_loss:0.167, val_acc:0.954]
Epoch [35/120    avg_loss:0.121, val_acc:0.958]
Epoch [36/120    avg_loss:0.110, val_acc:0.977]
Epoch [37/120    avg_loss:0.099, val_acc:0.977]
Epoch [38/120    avg_loss:0.073, val_acc:0.952]
Epoch [39/120    avg_loss:0.071, val_acc:0.975]
Epoch [40/120    avg_loss:0.142, val_acc:0.981]
Epoch [41/120    avg_loss:0.055, val_acc:0.977]
Epoch [42/120    avg_loss:0.062, val_acc:0.983]
Epoch [43/120    avg_loss:0.081, val_acc:0.975]
Epoch [44/120    avg_loss:0.078, val_acc:0.965]
Epoch [45/120    avg_loss:0.072, val_acc:0.981]
Epoch [46/120    avg_loss:0.062, val_acc:0.977]
Epoch [47/120    avg_loss:0.072, val_acc:0.979]
Epoch [48/120    avg_loss:0.040, val_acc:0.988]
Epoch [49/120    avg_loss:0.042, val_acc:0.985]
Epoch [50/120    avg_loss:0.058, val_acc:0.983]
Epoch [51/120    avg_loss:0.050, val_acc:0.975]
Epoch [52/120    avg_loss:0.044, val_acc:0.979]
Epoch [53/120    avg_loss:0.029, val_acc:0.983]
Epoch [54/120    avg_loss:0.033, val_acc:0.981]
Epoch [55/120    avg_loss:0.040, val_acc:0.967]
Epoch [56/120    avg_loss:0.047, val_acc:0.979]
Epoch [57/120    avg_loss:0.039, val_acc:0.985]
Epoch [58/120    avg_loss:0.062, val_acc:0.973]
Epoch [59/120    avg_loss:0.043, val_acc:0.977]
Epoch [60/120    avg_loss:0.048, val_acc:0.958]
Epoch [61/120    avg_loss:0.058, val_acc:0.975]
Epoch [62/120    avg_loss:0.091, val_acc:0.983]
Epoch [63/120    avg_loss:0.049, val_acc:0.988]
Epoch [64/120    avg_loss:0.022, val_acc:0.992]
Epoch [65/120    avg_loss:0.021, val_acc:0.990]
Epoch [66/120    avg_loss:0.016, val_acc:0.992]
Epoch [67/120    avg_loss:0.033, val_acc:0.988]
Epoch [68/120    avg_loss:0.014, val_acc:0.988]
Epoch [69/120    avg_loss:0.026, val_acc:0.990]
Epoch [70/120    avg_loss:0.018, val_acc:0.990]
Epoch [71/120    avg_loss:0.016, val_acc:0.990]
Epoch [72/120    avg_loss:0.018, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.990]
Epoch [74/120    avg_loss:0.017, val_acc:0.992]
Epoch [75/120    avg_loss:0.025, val_acc:0.992]
Epoch [76/120    avg_loss:0.019, val_acc:0.992]
Epoch [77/120    avg_loss:0.021, val_acc:0.992]
Epoch [78/120    avg_loss:0.022, val_acc:0.990]
Epoch [79/120    avg_loss:0.018, val_acc:0.990]
Epoch [80/120    avg_loss:0.014, val_acc:0.992]
Epoch [81/120    avg_loss:0.020, val_acc:0.992]
Epoch [82/120    avg_loss:0.020, val_acc:0.992]
Epoch [83/120    avg_loss:0.015, val_acc:0.992]
Epoch [84/120    avg_loss:0.015, val_acc:0.992]
Epoch [85/120    avg_loss:0.014, val_acc:0.992]
Epoch [86/120    avg_loss:0.011, val_acc:0.992]
Epoch [87/120    avg_loss:0.023, val_acc:0.992]
Epoch [88/120    avg_loss:0.015, val_acc:0.992]
Epoch [89/120    avg_loss:0.013, val_acc:0.992]
Epoch [90/120    avg_loss:0.021, val_acc:0.992]
Epoch [91/120    avg_loss:0.012, val_acc:0.992]
Epoch [92/120    avg_loss:0.015, val_acc:0.990]
Epoch [93/120    avg_loss:0.012, val_acc:0.990]
Epoch [94/120    avg_loss:0.016, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.990]
Epoch [96/120    avg_loss:0.013, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.990]
Epoch [98/120    avg_loss:0.016, val_acc:0.992]
Epoch [99/120    avg_loss:0.015, val_acc:0.992]
Epoch [100/120    avg_loss:0.017, val_acc:0.992]
Epoch [101/120    avg_loss:0.012, val_acc:0.994]
Epoch [102/120    avg_loss:0.022, val_acc:0.992]
Epoch [103/120    avg_loss:0.022, val_acc:0.992]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.021, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.018, val_acc:0.990]
Epoch [110/120    avg_loss:0.019, val_acc:0.994]
Epoch [111/120    avg_loss:0.016, val_acc:0.990]
Epoch [112/120    avg_loss:0.016, val_acc:0.990]
Epoch [113/120    avg_loss:0.020, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.015, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99319728 0.98454746 0.93506494 0.93515358
 0.98771499 0.98378378 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9919288450706735
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f327adf47b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.308, val_acc:0.529]
Epoch [2/120    avg_loss:1.662, val_acc:0.602]
Epoch [3/120    avg_loss:1.281, val_acc:0.685]
Epoch [4/120    avg_loss:1.075, val_acc:0.715]
Epoch [5/120    avg_loss:0.924, val_acc:0.800]
Epoch [6/120    avg_loss:0.763, val_acc:0.804]
Epoch [7/120    avg_loss:0.609, val_acc:0.844]
Epoch [8/120    avg_loss:0.618, val_acc:0.896]
Epoch [9/120    avg_loss:0.505, val_acc:0.877]
Epoch [10/120    avg_loss:0.448, val_acc:0.900]
Epoch [11/120    avg_loss:0.445, val_acc:0.885]
Epoch [12/120    avg_loss:0.447, val_acc:0.860]
Epoch [13/120    avg_loss:0.536, val_acc:0.883]
Epoch [14/120    avg_loss:0.524, val_acc:0.900]
Epoch [15/120    avg_loss:0.459, val_acc:0.879]
Epoch [16/120    avg_loss:0.341, val_acc:0.912]
Epoch [17/120    avg_loss:0.271, val_acc:0.933]
Epoch [18/120    avg_loss:0.284, val_acc:0.935]
Epoch [19/120    avg_loss:0.228, val_acc:0.948]
Epoch [20/120    avg_loss:0.260, val_acc:0.938]
Epoch [21/120    avg_loss:0.310, val_acc:0.898]
Epoch [22/120    avg_loss:0.243, val_acc:0.952]
Epoch [23/120    avg_loss:0.215, val_acc:0.923]
Epoch [24/120    avg_loss:0.291, val_acc:0.954]
Epoch [25/120    avg_loss:0.233, val_acc:0.944]
Epoch [26/120    avg_loss:0.173, val_acc:0.952]
Epoch [27/120    avg_loss:0.150, val_acc:0.977]
Epoch [28/120    avg_loss:0.132, val_acc:0.967]
Epoch [29/120    avg_loss:0.142, val_acc:0.954]
Epoch [30/120    avg_loss:0.137, val_acc:0.960]
Epoch [31/120    avg_loss:0.111, val_acc:0.971]
Epoch [32/120    avg_loss:0.149, val_acc:0.971]
Epoch [33/120    avg_loss:0.128, val_acc:0.973]
Epoch [34/120    avg_loss:0.109, val_acc:0.952]
Epoch [35/120    avg_loss:0.131, val_acc:0.975]
Epoch [36/120    avg_loss:0.114, val_acc:0.960]
Epoch [37/120    avg_loss:0.180, val_acc:0.967]
Epoch [38/120    avg_loss:0.102, val_acc:0.983]
Epoch [39/120    avg_loss:0.072, val_acc:0.973]
Epoch [40/120    avg_loss:0.066, val_acc:0.971]
Epoch [41/120    avg_loss:0.063, val_acc:0.971]
Epoch [42/120    avg_loss:0.075, val_acc:0.969]
Epoch [43/120    avg_loss:0.045, val_acc:0.981]
Epoch [44/120    avg_loss:0.049, val_acc:0.981]
Epoch [45/120    avg_loss:0.069, val_acc:0.967]
Epoch [46/120    avg_loss:0.057, val_acc:0.971]
Epoch [47/120    avg_loss:0.057, val_acc:0.981]
Epoch [48/120    avg_loss:0.059, val_acc:0.979]
Epoch [49/120    avg_loss:0.048, val_acc:0.985]
Epoch [50/120    avg_loss:0.050, val_acc:0.979]
Epoch [51/120    avg_loss:0.043, val_acc:0.975]
Epoch [52/120    avg_loss:0.057, val_acc:0.983]
Epoch [53/120    avg_loss:0.029, val_acc:0.975]
Epoch [54/120    avg_loss:0.049, val_acc:0.977]
Epoch [55/120    avg_loss:0.042, val_acc:0.979]
Epoch [56/120    avg_loss:0.032, val_acc:0.988]
Epoch [57/120    avg_loss:0.055, val_acc:0.956]
Epoch [58/120    avg_loss:0.098, val_acc:0.969]
Epoch [59/120    avg_loss:0.119, val_acc:0.979]
Epoch [60/120    avg_loss:0.058, val_acc:0.977]
Epoch [61/120    avg_loss:0.049, val_acc:0.983]
Epoch [62/120    avg_loss:0.026, val_acc:0.983]
Epoch [63/120    avg_loss:0.035, val_acc:0.990]
Epoch [64/120    avg_loss:0.047, val_acc:0.977]
Epoch [65/120    avg_loss:0.038, val_acc:0.983]
Epoch [66/120    avg_loss:0.042, val_acc:0.983]
Epoch [67/120    avg_loss:0.025, val_acc:0.990]
Epoch [68/120    avg_loss:0.022, val_acc:0.992]
Epoch [69/120    avg_loss:0.030, val_acc:0.985]
Epoch [70/120    avg_loss:0.023, val_acc:0.981]
Epoch [71/120    avg_loss:0.017, val_acc:0.988]
Epoch [72/120    avg_loss:0.019, val_acc:0.985]
Epoch [73/120    avg_loss:0.015, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.988]
Epoch [75/120    avg_loss:0.014, val_acc:0.990]
Epoch [76/120    avg_loss:0.011, val_acc:0.992]
Epoch [77/120    avg_loss:0.016, val_acc:0.990]
Epoch [78/120    avg_loss:0.017, val_acc:0.983]
Epoch [79/120    avg_loss:0.020, val_acc:0.988]
Epoch [80/120    avg_loss:0.072, val_acc:0.990]
Epoch [81/120    avg_loss:0.053, val_acc:0.985]
Epoch [82/120    avg_loss:0.028, val_acc:0.988]
Epoch [83/120    avg_loss:0.026, val_acc:0.981]
Epoch [84/120    avg_loss:0.017, val_acc:0.992]
Epoch [85/120    avg_loss:0.024, val_acc:0.988]
Epoch [86/120    avg_loss:0.015, val_acc:0.994]
Epoch [87/120    avg_loss:0.018, val_acc:0.998]
Epoch [88/120    avg_loss:0.015, val_acc:0.994]
Epoch [89/120    avg_loss:0.018, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.992]
Epoch [92/120    avg_loss:0.006, val_acc:0.998]
Epoch [93/120    avg_loss:0.008, val_acc:0.994]
Epoch [94/120    avg_loss:0.028, val_acc:0.998]
Epoch [95/120    avg_loss:0.014, val_acc:0.992]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.024, val_acc:0.996]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.014, val_acc:0.992]
Epoch [101/120    avg_loss:0.044, val_acc:0.998]
Epoch [102/120    avg_loss:0.082, val_acc:0.963]
Epoch [103/120    avg_loss:0.087, val_acc:0.929]
Epoch [104/120    avg_loss:0.059, val_acc:0.967]
Epoch [105/120    avg_loss:0.030, val_acc:0.983]
Epoch [106/120    avg_loss:0.018, val_acc:0.985]
Epoch [107/120    avg_loss:0.014, val_acc:0.994]
Epoch [108/120    avg_loss:0.013, val_acc:0.992]
Epoch [109/120    avg_loss:0.009, val_acc:0.994]
Epoch [110/120    avg_loss:0.012, val_acc:0.996]
Epoch [111/120    avg_loss:0.013, val_acc:0.996]
Epoch [112/120    avg_loss:0.006, val_acc:0.996]
Epoch [113/120    avg_loss:0.022, val_acc:0.996]
Epoch [114/120    avg_loss:0.021, val_acc:0.994]
Epoch [115/120    avg_loss:0.022, val_acc:0.996]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.015, val_acc:0.996]
Epoch [118/120    avg_loss:0.013, val_acc:0.996]
Epoch [119/120    avg_loss:0.006, val_acc:0.996]
Epoch [120/120    avg_loss:0.009, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 208  21   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   8   0   0   0   0   0   0   2   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.9977221  0.94977169 0.91561181 0.94482759
 0.99756691 1.         1.         1.         1.         0.9986755
 0.99669239 1.        ]

Kappa:
0.9900298159018515
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47f3269748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.206, val_acc:0.565]
Epoch [2/120    avg_loss:1.514, val_acc:0.773]
Epoch [3/120    avg_loss:1.104, val_acc:0.773]
Epoch [4/120    avg_loss:0.897, val_acc:0.758]
Epoch [5/120    avg_loss:0.764, val_acc:0.838]
Epoch [6/120    avg_loss:0.609, val_acc:0.854]
Epoch [7/120    avg_loss:0.576, val_acc:0.867]
Epoch [8/120    avg_loss:0.554, val_acc:0.898]
Epoch [9/120    avg_loss:0.485, val_acc:0.929]
Epoch [10/120    avg_loss:0.322, val_acc:0.944]
Epoch [11/120    avg_loss:0.287, val_acc:0.942]
Epoch [12/120    avg_loss:0.325, val_acc:0.935]
Epoch [13/120    avg_loss:0.319, val_acc:0.935]
Epoch [14/120    avg_loss:0.293, val_acc:0.940]
Epoch [15/120    avg_loss:0.275, val_acc:0.948]
Epoch [16/120    avg_loss:0.352, val_acc:0.906]
Epoch [17/120    avg_loss:0.271, val_acc:0.931]
Epoch [18/120    avg_loss:0.260, val_acc:0.942]
Epoch [19/120    avg_loss:0.345, val_acc:0.946]
Epoch [20/120    avg_loss:0.243, val_acc:0.923]
Epoch [21/120    avg_loss:0.229, val_acc:0.935]
Epoch [22/120    avg_loss:0.183, val_acc:0.969]
Epoch [23/120    avg_loss:0.195, val_acc:0.958]
Epoch [24/120    avg_loss:0.245, val_acc:0.935]
Epoch [25/120    avg_loss:0.309, val_acc:0.944]
Epoch [26/120    avg_loss:0.199, val_acc:0.958]
Epoch [27/120    avg_loss:0.175, val_acc:0.963]
Epoch [28/120    avg_loss:0.149, val_acc:0.948]
Epoch [29/120    avg_loss:0.102, val_acc:0.971]
Epoch [30/120    avg_loss:0.120, val_acc:0.985]
Epoch [31/120    avg_loss:0.133, val_acc:0.969]
Epoch [32/120    avg_loss:0.140, val_acc:0.973]
Epoch [33/120    avg_loss:0.126, val_acc:0.969]
Epoch [34/120    avg_loss:0.089, val_acc:0.979]
Epoch [35/120    avg_loss:0.078, val_acc:0.985]
Epoch [36/120    avg_loss:0.118, val_acc:0.969]
Epoch [37/120    avg_loss:0.135, val_acc:0.990]
Epoch [38/120    avg_loss:0.055, val_acc:0.992]
Epoch [39/120    avg_loss:0.072, val_acc:0.977]
Epoch [40/120    avg_loss:0.112, val_acc:0.992]
Epoch [41/120    avg_loss:0.095, val_acc:0.983]
Epoch [42/120    avg_loss:0.067, val_acc:0.979]
Epoch [43/120    avg_loss:0.066, val_acc:0.969]
Epoch [44/120    avg_loss:0.065, val_acc:0.973]
Epoch [45/120    avg_loss:0.051, val_acc:0.981]
Epoch [46/120    avg_loss:0.063, val_acc:0.990]
Epoch [47/120    avg_loss:0.056, val_acc:0.992]
Epoch [48/120    avg_loss:0.097, val_acc:0.985]
Epoch [49/120    avg_loss:0.090, val_acc:0.983]
Epoch [50/120    avg_loss:0.093, val_acc:0.919]
Epoch [51/120    avg_loss:0.091, val_acc:0.988]
Epoch [52/120    avg_loss:0.071, val_acc:0.985]
Epoch [53/120    avg_loss:0.070, val_acc:0.985]
Epoch [54/120    avg_loss:0.046, val_acc:0.985]
Epoch [55/120    avg_loss:0.043, val_acc:0.996]
Epoch [56/120    avg_loss:0.062, val_acc:0.983]
Epoch [57/120    avg_loss:0.051, val_acc:0.988]
Epoch [58/120    avg_loss:0.030, val_acc:0.992]
Epoch [59/120    avg_loss:0.027, val_acc:0.992]
Epoch [60/120    avg_loss:0.017, val_acc:0.992]
Epoch [61/120    avg_loss:0.019, val_acc:0.994]
Epoch [62/120    avg_loss:0.023, val_acc:0.992]
Epoch [63/120    avg_loss:0.043, val_acc:0.988]
Epoch [64/120    avg_loss:0.045, val_acc:0.988]
Epoch [65/120    avg_loss:0.044, val_acc:0.988]
Epoch [66/120    avg_loss:0.069, val_acc:0.977]
Epoch [67/120    avg_loss:0.061, val_acc:0.985]
Epoch [68/120    avg_loss:0.073, val_acc:0.985]
Epoch [69/120    avg_loss:0.024, val_acc:0.994]
Epoch [70/120    avg_loss:0.016, val_acc:0.996]
Epoch [71/120    avg_loss:0.020, val_acc:0.994]
Epoch [72/120    avg_loss:0.022, val_acc:0.994]
Epoch [73/120    avg_loss:0.025, val_acc:0.994]
Epoch [74/120    avg_loss:0.029, val_acc:0.994]
Epoch [75/120    avg_loss:0.041, val_acc:0.994]
Epoch [76/120    avg_loss:0.033, val_acc:0.994]
Epoch [77/120    avg_loss:0.018, val_acc:0.996]
Epoch [78/120    avg_loss:0.017, val_acc:0.994]
Epoch [79/120    avg_loss:0.013, val_acc:0.994]
Epoch [80/120    avg_loss:0.035, val_acc:0.994]
Epoch [81/120    avg_loss:0.026, val_acc:0.994]
Epoch [82/120    avg_loss:0.017, val_acc:0.994]
Epoch [83/120    avg_loss:0.015, val_acc:0.994]
Epoch [84/120    avg_loss:0.018, val_acc:0.994]
Epoch [85/120    avg_loss:0.020, val_acc:0.994]
Epoch [86/120    avg_loss:0.013, val_acc:0.992]
Epoch [87/120    avg_loss:0.015, val_acc:0.996]
Epoch [88/120    avg_loss:0.015, val_acc:0.996]
Epoch [89/120    avg_loss:0.018, val_acc:0.996]
Epoch [90/120    avg_loss:0.013, val_acc:0.996]
Epoch [91/120    avg_loss:0.015, val_acc:0.996]
Epoch [92/120    avg_loss:0.023, val_acc:0.994]
Epoch [93/120    avg_loss:0.013, val_acc:0.994]
Epoch [94/120    avg_loss:0.025, val_acc:0.994]
Epoch [95/120    avg_loss:0.020, val_acc:0.994]
Epoch [96/120    avg_loss:0.013, val_acc:0.994]
Epoch [97/120    avg_loss:0.011, val_acc:0.994]
Epoch [98/120    avg_loss:0.015, val_acc:0.994]
Epoch [99/120    avg_loss:0.011, val_acc:0.996]
Epoch [100/120    avg_loss:0.012, val_acc:0.996]
Epoch [101/120    avg_loss:0.019, val_acc:0.996]
Epoch [102/120    avg_loss:0.010, val_acc:0.996]
Epoch [103/120    avg_loss:0.014, val_acc:0.996]
Epoch [104/120    avg_loss:0.010, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.998]
Epoch [106/120    avg_loss:0.023, val_acc:0.998]
Epoch [107/120    avg_loss:0.011, val_acc:0.998]
Epoch [108/120    avg_loss:0.018, val_acc:0.998]
Epoch [109/120    avg_loss:0.021, val_acc:0.998]
Epoch [110/120    avg_loss:0.018, val_acc:0.998]
Epoch [111/120    avg_loss:0.009, val_acc:0.998]
Epoch [112/120    avg_loss:0.011, val_acc:0.998]
Epoch [113/120    avg_loss:0.020, val_acc:0.998]
Epoch [114/120    avg_loss:0.019, val_acc:0.996]
Epoch [115/120    avg_loss:0.015, val_acc:0.998]
Epoch [116/120    avg_loss:0.011, val_acc:0.998]
Epoch [117/120    avg_loss:0.009, val_acc:0.998]
Epoch [118/120    avg_loss:0.020, val_acc:0.998]
Epoch [119/120    avg_loss:0.014, val_acc:0.996]
Epoch [120/120    avg_loss:0.010, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 217   9   0   0   0   0   0   0   1   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         1.         0.99782135 0.96875    0.96296296
 0.99512195 1.         1.         0.99893276 1.         0.99472296
 0.9944629  1.        ]

Kappa:
0.9954898372788358
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27031067f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.185, val_acc:0.569]
Epoch [2/120    avg_loss:1.499, val_acc:0.738]
Epoch [3/120    avg_loss:1.096, val_acc:0.667]
Epoch [4/120    avg_loss:0.792, val_acc:0.773]
Epoch [5/120    avg_loss:0.683, val_acc:0.783]
Epoch [6/120    avg_loss:0.707, val_acc:0.781]
Epoch [7/120    avg_loss:0.585, val_acc:0.769]
Epoch [8/120    avg_loss:0.516, val_acc:0.829]
Epoch [9/120    avg_loss:0.482, val_acc:0.860]
Epoch [10/120    avg_loss:0.429, val_acc:0.865]
Epoch [11/120    avg_loss:0.353, val_acc:0.890]
Epoch [12/120    avg_loss:0.294, val_acc:0.879]
Epoch [13/120    avg_loss:0.310, val_acc:0.879]
Epoch [14/120    avg_loss:0.313, val_acc:0.883]
Epoch [15/120    avg_loss:0.308, val_acc:0.906]
Epoch [16/120    avg_loss:0.265, val_acc:0.921]
Epoch [17/120    avg_loss:0.244, val_acc:0.902]
Epoch [18/120    avg_loss:0.264, val_acc:0.927]
Epoch [19/120    avg_loss:0.258, val_acc:0.915]
Epoch [20/120    avg_loss:0.228, val_acc:0.938]
Epoch [21/120    avg_loss:0.165, val_acc:0.925]
Epoch [22/120    avg_loss:0.143, val_acc:0.948]
Epoch [23/120    avg_loss:0.240, val_acc:0.925]
Epoch [24/120    avg_loss:0.185, val_acc:0.944]
Epoch [25/120    avg_loss:0.158, val_acc:0.948]
Epoch [26/120    avg_loss:0.114, val_acc:0.954]
Epoch [27/120    avg_loss:0.105, val_acc:0.944]
Epoch [28/120    avg_loss:0.127, val_acc:0.942]
Epoch [29/120    avg_loss:0.132, val_acc:0.960]
Epoch [30/120    avg_loss:0.109, val_acc:0.950]
Epoch [31/120    avg_loss:0.094, val_acc:0.956]
Epoch [32/120    avg_loss:0.089, val_acc:0.960]
Epoch [33/120    avg_loss:0.062, val_acc:0.969]
Epoch [34/120    avg_loss:0.090, val_acc:0.960]
Epoch [35/120    avg_loss:0.073, val_acc:0.960]
Epoch [36/120    avg_loss:0.101, val_acc:0.969]
Epoch [37/120    avg_loss:0.090, val_acc:0.958]
Epoch [38/120    avg_loss:0.087, val_acc:0.963]
Epoch [39/120    avg_loss:0.046, val_acc:0.969]
Epoch [40/120    avg_loss:0.106, val_acc:0.967]
Epoch [41/120    avg_loss:0.093, val_acc:0.967]
Epoch [42/120    avg_loss:0.069, val_acc:0.952]
Epoch [43/120    avg_loss:0.083, val_acc:0.960]
Epoch [44/120    avg_loss:0.062, val_acc:0.969]
Epoch [45/120    avg_loss:0.063, val_acc:0.969]
Epoch [46/120    avg_loss:0.047, val_acc:0.973]
Epoch [47/120    avg_loss:0.044, val_acc:0.960]
Epoch [48/120    avg_loss:0.050, val_acc:0.963]
Epoch [49/120    avg_loss:0.064, val_acc:0.954]
Epoch [50/120    avg_loss:0.062, val_acc:0.954]
Epoch [51/120    avg_loss:0.071, val_acc:0.952]
Epoch [52/120    avg_loss:0.076, val_acc:0.952]
Epoch [53/120    avg_loss:0.062, val_acc:0.963]
Epoch [54/120    avg_loss:0.079, val_acc:0.967]
Epoch [55/120    avg_loss:0.069, val_acc:0.929]
Epoch [56/120    avg_loss:0.092, val_acc:0.981]
Epoch [57/120    avg_loss:0.065, val_acc:0.965]
Epoch [58/120    avg_loss:0.090, val_acc:0.969]
Epoch [59/120    avg_loss:0.061, val_acc:0.969]
Epoch [60/120    avg_loss:0.035, val_acc:0.969]
Epoch [61/120    avg_loss:0.024, val_acc:0.977]
Epoch [62/120    avg_loss:0.070, val_acc:0.973]
Epoch [63/120    avg_loss:0.055, val_acc:0.981]
Epoch [64/120    avg_loss:0.127, val_acc:0.975]
Epoch [65/120    avg_loss:0.115, val_acc:0.944]
Epoch [66/120    avg_loss:0.097, val_acc:0.971]
Epoch [67/120    avg_loss:0.080, val_acc:0.975]
Epoch [68/120    avg_loss:0.026, val_acc:0.977]
Epoch [69/120    avg_loss:0.019, val_acc:0.988]
Epoch [70/120    avg_loss:0.030, val_acc:0.983]
Epoch [71/120    avg_loss:0.020, val_acc:0.977]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.027, val_acc:0.985]
Epoch [74/120    avg_loss:0.022, val_acc:0.975]
Epoch [75/120    avg_loss:0.036, val_acc:0.975]
Epoch [76/120    avg_loss:0.026, val_acc:0.981]
Epoch [77/120    avg_loss:0.026, val_acc:0.983]
Epoch [78/120    avg_loss:0.016, val_acc:0.988]
Epoch [79/120    avg_loss:0.030, val_acc:0.979]
Epoch [80/120    avg_loss:0.036, val_acc:0.977]
Epoch [81/120    avg_loss:0.039, val_acc:0.975]
Epoch [82/120    avg_loss:0.044, val_acc:0.985]
Epoch [83/120    avg_loss:0.023, val_acc:0.975]
Epoch [84/120    avg_loss:0.026, val_acc:0.988]
Epoch [85/120    avg_loss:0.023, val_acc:0.973]
Epoch [86/120    avg_loss:0.021, val_acc:0.985]
Epoch [87/120    avg_loss:0.019, val_acc:0.988]
Epoch [88/120    avg_loss:0.013, val_acc:0.985]
Epoch [89/120    avg_loss:0.016, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.994]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.007, val_acc:0.994]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.009, val_acc:0.992]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.003, val_acc:0.994]
Epoch [106/120    avg_loss:0.003, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.003, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.996]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 1.         1.         0.99563319 0.95860566 0.93706294
 1.         1.         0.998713   1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9950148867558312
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3aa678a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.213, val_acc:0.615]
Epoch [2/120    avg_loss:1.491, val_acc:0.742]
Epoch [3/120    avg_loss:1.102, val_acc:0.735]
Epoch [4/120    avg_loss:0.850, val_acc:0.833]
Epoch [5/120    avg_loss:0.677, val_acc:0.819]
Epoch [6/120    avg_loss:0.549, val_acc:0.854]
Epoch [7/120    avg_loss:0.470, val_acc:0.871]
Epoch [8/120    avg_loss:0.509, val_acc:0.892]
Epoch [9/120    avg_loss:0.395, val_acc:0.904]
Epoch [10/120    avg_loss:0.446, val_acc:0.931]
Epoch [11/120    avg_loss:0.368, val_acc:0.933]
Epoch [12/120    avg_loss:0.380, val_acc:0.885]
Epoch [13/120    avg_loss:0.391, val_acc:0.900]
Epoch [14/120    avg_loss:0.328, val_acc:0.950]
Epoch [15/120    avg_loss:0.238, val_acc:0.948]
Epoch [16/120    avg_loss:0.233, val_acc:0.919]
Epoch [17/120    avg_loss:0.230, val_acc:0.963]
Epoch [18/120    avg_loss:0.203, val_acc:0.956]
Epoch [19/120    avg_loss:0.188, val_acc:0.954]
Epoch [20/120    avg_loss:0.223, val_acc:0.958]
Epoch [21/120    avg_loss:0.178, val_acc:0.948]
Epoch [22/120    avg_loss:0.220, val_acc:0.954]
Epoch [23/120    avg_loss:0.181, val_acc:0.958]
Epoch [24/120    avg_loss:0.137, val_acc:0.967]
Epoch [25/120    avg_loss:0.191, val_acc:0.958]
Epoch [26/120    avg_loss:0.198, val_acc:0.950]
Epoch [27/120    avg_loss:0.131, val_acc:0.963]
Epoch [28/120    avg_loss:0.147, val_acc:0.977]
Epoch [29/120    avg_loss:0.118, val_acc:0.975]
Epoch [30/120    avg_loss:0.074, val_acc:0.963]
Epoch [31/120    avg_loss:0.131, val_acc:0.952]
Epoch [32/120    avg_loss:0.147, val_acc:0.969]
Epoch [33/120    avg_loss:0.107, val_acc:0.971]
Epoch [34/120    avg_loss:0.114, val_acc:0.952]
Epoch [35/120    avg_loss:0.203, val_acc:0.956]
Epoch [36/120    avg_loss:0.234, val_acc:0.942]
Epoch [37/120    avg_loss:0.200, val_acc:0.958]
Epoch [38/120    avg_loss:0.142, val_acc:0.967]
Epoch [39/120    avg_loss:0.059, val_acc:0.977]
Epoch [40/120    avg_loss:0.090, val_acc:0.975]
Epoch [41/120    avg_loss:0.077, val_acc:0.979]
Epoch [42/120    avg_loss:0.102, val_acc:0.967]
Epoch [43/120    avg_loss:0.079, val_acc:0.977]
Epoch [44/120    avg_loss:0.056, val_acc:0.983]
Epoch [45/120    avg_loss:0.139, val_acc:0.983]
Epoch [46/120    avg_loss:0.155, val_acc:0.960]
Epoch [47/120    avg_loss:0.103, val_acc:0.981]
Epoch [48/120    avg_loss:0.084, val_acc:0.985]
Epoch [49/120    avg_loss:0.071, val_acc:0.979]
Epoch [50/120    avg_loss:0.054, val_acc:0.981]
Epoch [51/120    avg_loss:0.067, val_acc:0.990]
Epoch [52/120    avg_loss:0.035, val_acc:0.988]
Epoch [53/120    avg_loss:0.058, val_acc:0.977]
Epoch [54/120    avg_loss:0.046, val_acc:0.983]
Epoch [55/120    avg_loss:0.031, val_acc:0.988]
Epoch [56/120    avg_loss:0.039, val_acc:0.981]
Epoch [57/120    avg_loss:0.024, val_acc:0.988]
Epoch [58/120    avg_loss:0.025, val_acc:0.977]
Epoch [59/120    avg_loss:0.047, val_acc:0.988]
Epoch [60/120    avg_loss:0.034, val_acc:0.983]
Epoch [61/120    avg_loss:0.068, val_acc:0.983]
Epoch [62/120    avg_loss:0.126, val_acc:0.985]
Epoch [63/120    avg_loss:0.090, val_acc:0.979]
Epoch [64/120    avg_loss:0.071, val_acc:0.981]
Epoch [65/120    avg_loss:0.024, val_acc:0.983]
Epoch [66/120    avg_loss:0.035, val_acc:0.981]
Epoch [67/120    avg_loss:0.032, val_acc:0.983]
Epoch [68/120    avg_loss:0.023, val_acc:0.988]
Epoch [69/120    avg_loss:0.019, val_acc:0.988]
Epoch [70/120    avg_loss:0.030, val_acc:0.988]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.025, val_acc:0.988]
Epoch [73/120    avg_loss:0.019, val_acc:0.985]
Epoch [74/120    avg_loss:0.035, val_acc:0.988]
Epoch [75/120    avg_loss:0.016, val_acc:0.988]
Epoch [76/120    avg_loss:0.012, val_acc:0.988]
Epoch [77/120    avg_loss:0.028, val_acc:0.988]
Epoch [78/120    avg_loss:0.017, val_acc:0.988]
Epoch [79/120    avg_loss:0.020, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.016, val_acc:0.988]
Epoch [82/120    avg_loss:0.016, val_acc:0.988]
Epoch [83/120    avg_loss:0.022, val_acc:0.988]
Epoch [84/120    avg_loss:0.018, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.014, val_acc:0.988]
Epoch [87/120    avg_loss:0.017, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.016, val_acc:0.988]
Epoch [90/120    avg_loss:0.022, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.988]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.027, val_acc:0.988]
Epoch [94/120    avg_loss:0.021, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.019, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.019, val_acc:0.988]
Epoch [99/120    avg_loss:0.024, val_acc:0.988]
Epoch [100/120    avg_loss:0.020, val_acc:0.988]
Epoch [101/120    avg_loss:0.027, val_acc:0.988]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.014, val_acc:0.988]
Epoch [104/120    avg_loss:0.012, val_acc:0.988]
Epoch [105/120    avg_loss:0.021, val_acc:0.988]
Epoch [106/120    avg_loss:0.037, val_acc:0.988]
Epoch [107/120    avg_loss:0.018, val_acc:0.988]
Epoch [108/120    avg_loss:0.015, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.017, val_acc:0.988]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.018, val_acc:0.988]
Epoch [113/120    avg_loss:0.016, val_acc:0.988]
Epoch [114/120    avg_loss:0.014, val_acc:0.988]
Epoch [115/120    avg_loss:0.019, val_acc:0.988]
Epoch [116/120    avg_loss:0.017, val_acc:0.988]
Epoch [117/120    avg_loss:0.014, val_acc:0.988]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.988]
Epoch [120/120    avg_loss:0.024, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 224   3   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.99095023 0.98678414 0.95594714 0.94197952
 1.         0.98378378 0.99742931 1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9914544125250252
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3fba7c47b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.211, val_acc:0.619]
Epoch [2/120    avg_loss:1.511, val_acc:0.627]
Epoch [3/120    avg_loss:1.120, val_acc:0.733]
Epoch [4/120    avg_loss:0.817, val_acc:0.806]
Epoch [5/120    avg_loss:0.652, val_acc:0.771]
Epoch [6/120    avg_loss:0.528, val_acc:0.873]
Epoch [7/120    avg_loss:0.409, val_acc:0.885]
Epoch [8/120    avg_loss:0.429, val_acc:0.925]
Epoch [9/120    avg_loss:0.393, val_acc:0.892]
Epoch [10/120    avg_loss:0.396, val_acc:0.890]
Epoch [11/120    avg_loss:0.417, val_acc:0.892]
Epoch [12/120    avg_loss:0.295, val_acc:0.919]
Epoch [13/120    avg_loss:0.312, val_acc:0.904]
Epoch [14/120    avg_loss:0.307, val_acc:0.927]
Epoch [15/120    avg_loss:0.339, val_acc:0.929]
Epoch [16/120    avg_loss:0.296, val_acc:0.929]
Epoch [17/120    avg_loss:0.209, val_acc:0.954]
Epoch [18/120    avg_loss:0.254, val_acc:0.942]
Epoch [19/120    avg_loss:0.175, val_acc:0.971]
Epoch [20/120    avg_loss:0.142, val_acc:0.946]
Epoch [21/120    avg_loss:0.162, val_acc:0.979]
Epoch [22/120    avg_loss:0.130, val_acc:0.977]
Epoch [23/120    avg_loss:0.143, val_acc:0.958]
Epoch [24/120    avg_loss:0.103, val_acc:0.971]
Epoch [25/120    avg_loss:0.140, val_acc:0.981]
Epoch [26/120    avg_loss:0.122, val_acc:0.973]
Epoch [27/120    avg_loss:0.081, val_acc:0.969]
Epoch [28/120    avg_loss:0.120, val_acc:0.975]
Epoch [29/120    avg_loss:0.190, val_acc:0.965]
Epoch [30/120    avg_loss:0.197, val_acc:0.958]
Epoch [31/120    avg_loss:0.109, val_acc:0.975]
Epoch [32/120    avg_loss:0.147, val_acc:0.935]
Epoch [33/120    avg_loss:0.101, val_acc:0.973]
Epoch [34/120    avg_loss:0.106, val_acc:0.977]
Epoch [35/120    avg_loss:0.078, val_acc:0.965]
Epoch [36/120    avg_loss:0.085, val_acc:0.969]
Epoch [37/120    avg_loss:0.081, val_acc:0.977]
Epoch [38/120    avg_loss:0.107, val_acc:0.952]
Epoch [39/120    avg_loss:0.166, val_acc:0.979]
Epoch [40/120    avg_loss:0.061, val_acc:0.981]
Epoch [41/120    avg_loss:0.056, val_acc:0.990]
Epoch [42/120    avg_loss:0.041, val_acc:0.985]
Epoch [43/120    avg_loss:0.039, val_acc:0.988]
Epoch [44/120    avg_loss:0.027, val_acc:0.985]
Epoch [45/120    avg_loss:0.052, val_acc:0.988]
Epoch [46/120    avg_loss:0.025, val_acc:0.990]
Epoch [47/120    avg_loss:0.042, val_acc:0.990]
Epoch [48/120    avg_loss:0.033, val_acc:0.988]
Epoch [49/120    avg_loss:0.041, val_acc:0.990]
Epoch [50/120    avg_loss:0.050, val_acc:0.983]
Epoch [51/120    avg_loss:0.063, val_acc:0.990]
Epoch [52/120    avg_loss:0.032, val_acc:0.990]
Epoch [53/120    avg_loss:0.037, val_acc:0.988]
Epoch [54/120    avg_loss:0.033, val_acc:0.988]
Epoch [55/120    avg_loss:0.042, val_acc:0.990]
Epoch [56/120    avg_loss:0.039, val_acc:0.990]
Epoch [57/120    avg_loss:0.033, val_acc:0.990]
Epoch [58/120    avg_loss:0.038, val_acc:0.990]
Epoch [59/120    avg_loss:0.045, val_acc:0.990]
Epoch [60/120    avg_loss:0.048, val_acc:0.990]
Epoch [61/120    avg_loss:0.068, val_acc:0.990]
Epoch [62/120    avg_loss:0.058, val_acc:0.979]
Epoch [63/120    avg_loss:0.045, val_acc:0.981]
Epoch [64/120    avg_loss:0.032, val_acc:0.990]
Epoch [65/120    avg_loss:0.029, val_acc:0.990]
Epoch [66/120    avg_loss:0.033, val_acc:0.990]
Epoch [67/120    avg_loss:0.029, val_acc:0.990]
Epoch [68/120    avg_loss:0.029, val_acc:0.990]
Epoch [69/120    avg_loss:0.032, val_acc:0.990]
Epoch [70/120    avg_loss:0.043, val_acc:0.985]
Epoch [71/120    avg_loss:0.033, val_acc:0.985]
Epoch [72/120    avg_loss:0.036, val_acc:0.979]
Epoch [73/120    avg_loss:0.028, val_acc:0.977]
Epoch [74/120    avg_loss:0.023, val_acc:0.988]
Epoch [75/120    avg_loss:0.038, val_acc:0.990]
Epoch [76/120    avg_loss:0.022, val_acc:0.990]
Epoch [77/120    avg_loss:0.038, val_acc:0.990]
Epoch [78/120    avg_loss:0.022, val_acc:0.990]
Epoch [79/120    avg_loss:0.031, val_acc:0.990]
Epoch [80/120    avg_loss:0.033, val_acc:0.990]
Epoch [81/120    avg_loss:0.026, val_acc:0.988]
Epoch [82/120    avg_loss:0.027, val_acc:0.988]
Epoch [83/120    avg_loss:0.036, val_acc:0.985]
Epoch [84/120    avg_loss:0.024, val_acc:0.985]
Epoch [85/120    avg_loss:0.023, val_acc:0.990]
Epoch [86/120    avg_loss:0.029, val_acc:0.981]
Epoch [87/120    avg_loss:0.037, val_acc:0.985]
Epoch [88/120    avg_loss:0.020, val_acc:0.988]
Epoch [89/120    avg_loss:0.033, val_acc:0.990]
Epoch [90/120    avg_loss:0.035, val_acc:0.990]
Epoch [91/120    avg_loss:0.028, val_acc:0.988]
Epoch [92/120    avg_loss:0.021, val_acc:0.988]
Epoch [93/120    avg_loss:0.019, val_acc:0.988]
Epoch [94/120    avg_loss:0.025, val_acc:0.990]
Epoch [95/120    avg_loss:0.018, val_acc:0.990]
Epoch [96/120    avg_loss:0.036, val_acc:0.990]
Epoch [97/120    avg_loss:0.029, val_acc:0.992]
Epoch [98/120    avg_loss:0.024, val_acc:0.992]
Epoch [99/120    avg_loss:0.030, val_acc:0.983]
Epoch [100/120    avg_loss:0.055, val_acc:0.988]
Epoch [101/120    avg_loss:0.028, val_acc:0.990]
Epoch [102/120    avg_loss:0.026, val_acc:0.992]
Epoch [103/120    avg_loss:0.040, val_acc:0.990]
Epoch [104/120    avg_loss:0.035, val_acc:0.988]
Epoch [105/120    avg_loss:0.021, val_acc:0.990]
Epoch [106/120    avg_loss:0.025, val_acc:0.990]
Epoch [107/120    avg_loss:0.018, val_acc:0.992]
Epoch [108/120    avg_loss:0.023, val_acc:0.990]
Epoch [109/120    avg_loss:0.034, val_acc:0.990]
Epoch [110/120    avg_loss:0.028, val_acc:0.990]
Epoch [111/120    avg_loss:0.021, val_acc:0.990]
Epoch [112/120    avg_loss:0.030, val_acc:0.992]
Epoch [113/120    avg_loss:0.018, val_acc:0.992]
Epoch [114/120    avg_loss:0.015, val_acc:0.992]
Epoch [115/120    avg_loss:0.024, val_acc:0.992]
Epoch [116/120    avg_loss:0.030, val_acc:0.992]
Epoch [117/120    avg_loss:0.016, val_acc:0.990]
Epoch [118/120    avg_loss:0.036, val_acc:0.992]
Epoch [119/120    avg_loss:0.017, val_acc:0.992]
Epoch [120/120    avg_loss:0.024, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         0.98426966 1.         0.98447894 0.97610922
 1.         0.96132597 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9954896955157269
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86f9f24780>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.221, val_acc:0.592]
Epoch [2/120    avg_loss:1.488, val_acc:0.679]
Epoch [3/120    avg_loss:1.203, val_acc:0.715]
Epoch [4/120    avg_loss:0.925, val_acc:0.808]
Epoch [5/120    avg_loss:0.744, val_acc:0.767]
Epoch [6/120    avg_loss:0.672, val_acc:0.812]
Epoch [7/120    avg_loss:0.605, val_acc:0.863]
Epoch [8/120    avg_loss:0.538, val_acc:0.885]
Epoch [9/120    avg_loss:0.429, val_acc:0.890]
Epoch [10/120    avg_loss:0.467, val_acc:0.887]
Epoch [11/120    avg_loss:0.438, val_acc:0.908]
Epoch [12/120    avg_loss:0.378, val_acc:0.919]
Epoch [13/120    avg_loss:0.272, val_acc:0.917]
Epoch [14/120    avg_loss:0.331, val_acc:0.906]
Epoch [15/120    avg_loss:0.311, val_acc:0.927]
Epoch [16/120    avg_loss:0.351, val_acc:0.935]
Epoch [17/120    avg_loss:0.325, val_acc:0.856]
Epoch [18/120    avg_loss:0.285, val_acc:0.940]
Epoch [19/120    avg_loss:0.196, val_acc:0.952]
Epoch [20/120    avg_loss:0.199, val_acc:0.973]
Epoch [21/120    avg_loss:0.198, val_acc:0.954]
Epoch [22/120    avg_loss:0.235, val_acc:0.960]
Epoch [23/120    avg_loss:0.207, val_acc:0.946]
Epoch [24/120    avg_loss:0.155, val_acc:0.960]
Epoch [25/120    avg_loss:0.223, val_acc:0.958]
Epoch [26/120    avg_loss:0.112, val_acc:0.973]
Epoch [27/120    avg_loss:0.102, val_acc:0.985]
Epoch [28/120    avg_loss:0.079, val_acc:0.973]
Epoch [29/120    avg_loss:0.102, val_acc:0.975]
Epoch [30/120    avg_loss:0.106, val_acc:0.973]
Epoch [31/120    avg_loss:0.095, val_acc:0.963]
Epoch [32/120    avg_loss:0.160, val_acc:0.969]
Epoch [33/120    avg_loss:0.105, val_acc:0.979]
Epoch [34/120    avg_loss:0.118, val_acc:0.977]
Epoch [35/120    avg_loss:0.138, val_acc:0.948]
Epoch [36/120    avg_loss:0.085, val_acc:0.969]
Epoch [37/120    avg_loss:0.152, val_acc:0.975]
Epoch [38/120    avg_loss:0.209, val_acc:0.950]
Epoch [39/120    avg_loss:0.110, val_acc:0.960]
Epoch [40/120    avg_loss:0.083, val_acc:0.979]
Epoch [41/120    avg_loss:0.062, val_acc:0.985]
Epoch [42/120    avg_loss:0.072, val_acc:0.985]
Epoch [43/120    avg_loss:0.034, val_acc:0.985]
Epoch [44/120    avg_loss:0.040, val_acc:0.983]
Epoch [45/120    avg_loss:0.036, val_acc:0.988]
Epoch [46/120    avg_loss:0.050, val_acc:0.985]
Epoch [47/120    avg_loss:0.049, val_acc:0.985]
Epoch [48/120    avg_loss:0.036, val_acc:0.985]
Epoch [49/120    avg_loss:0.036, val_acc:0.988]
Epoch [50/120    avg_loss:0.041, val_acc:0.988]
Epoch [51/120    avg_loss:0.034, val_acc:0.990]
Epoch [52/120    avg_loss:0.041, val_acc:0.990]
Epoch [53/120    avg_loss:0.035, val_acc:0.990]
Epoch [54/120    avg_loss:0.043, val_acc:0.990]
Epoch [55/120    avg_loss:0.031, val_acc:0.988]
Epoch [56/120    avg_loss:0.047, val_acc:0.985]
Epoch [57/120    avg_loss:0.041, val_acc:0.985]
Epoch [58/120    avg_loss:0.050, val_acc:0.985]
Epoch [59/120    avg_loss:0.037, val_acc:0.985]
Epoch [60/120    avg_loss:0.041, val_acc:0.988]
Epoch [61/120    avg_loss:0.032, val_acc:0.988]
Epoch [62/120    avg_loss:0.033, val_acc:0.990]
Epoch [63/120    avg_loss:0.034, val_acc:0.990]
Epoch [64/120    avg_loss:0.040, val_acc:0.988]
Epoch [65/120    avg_loss:0.037, val_acc:0.990]
Epoch [66/120    avg_loss:0.035, val_acc:0.990]
Epoch [67/120    avg_loss:0.040, val_acc:0.988]
Epoch [68/120    avg_loss:0.029, val_acc:0.985]
Epoch [69/120    avg_loss:0.027, val_acc:0.985]
Epoch [70/120    avg_loss:0.024, val_acc:0.985]
Epoch [71/120    avg_loss:0.027, val_acc:0.990]
Epoch [72/120    avg_loss:0.073, val_acc:0.990]
Epoch [73/120    avg_loss:0.027, val_acc:0.988]
Epoch [74/120    avg_loss:0.039, val_acc:0.990]
Epoch [75/120    avg_loss:0.039, val_acc:0.990]
Epoch [76/120    avg_loss:0.041, val_acc:0.988]
Epoch [77/120    avg_loss:0.036, val_acc:0.990]
Epoch [78/120    avg_loss:0.027, val_acc:0.990]
Epoch [79/120    avg_loss:0.021, val_acc:0.988]
Epoch [80/120    avg_loss:0.023, val_acc:0.988]
Epoch [81/120    avg_loss:0.027, val_acc:0.988]
Epoch [82/120    avg_loss:0.028, val_acc:0.985]
Epoch [83/120    avg_loss:0.035, val_acc:0.988]
Epoch [84/120    avg_loss:0.024, val_acc:0.988]
Epoch [85/120    avg_loss:0.024, val_acc:0.988]
Epoch [86/120    avg_loss:0.041, val_acc:0.985]
Epoch [87/120    avg_loss:0.022, val_acc:0.985]
Epoch [88/120    avg_loss:0.020, val_acc:0.985]
Epoch [89/120    avg_loss:0.030, val_acc:0.990]
Epoch [90/120    avg_loss:0.022, val_acc:0.990]
Epoch [91/120    avg_loss:0.054, val_acc:0.990]
Epoch [92/120    avg_loss:0.030, val_acc:0.988]
Epoch [93/120    avg_loss:0.028, val_acc:0.988]
Epoch [94/120    avg_loss:0.030, val_acc:0.990]
Epoch [95/120    avg_loss:0.027, val_acc:0.990]
Epoch [96/120    avg_loss:0.027, val_acc:0.990]
Epoch [97/120    avg_loss:0.040, val_acc:0.992]
Epoch [98/120    avg_loss:0.021, val_acc:0.988]
Epoch [99/120    avg_loss:0.026, val_acc:0.990]
Epoch [100/120    avg_loss:0.024, val_acc:0.990]
Epoch [101/120    avg_loss:0.028, val_acc:0.990]
Epoch [102/120    avg_loss:0.022, val_acc:0.992]
Epoch [103/120    avg_loss:0.029, val_acc:0.992]
Epoch [104/120    avg_loss:0.028, val_acc:0.990]
Epoch [105/120    avg_loss:0.023, val_acc:0.990]
Epoch [106/120    avg_loss:0.018, val_acc:0.988]
Epoch [107/120    avg_loss:0.025, val_acc:0.988]
Epoch [108/120    avg_loss:0.020, val_acc:0.988]
Epoch [109/120    avg_loss:0.022, val_acc:0.990]
Epoch [110/120    avg_loss:0.023, val_acc:0.990]
Epoch [111/120    avg_loss:0.025, val_acc:0.990]
Epoch [112/120    avg_loss:0.025, val_acc:0.990]
Epoch [113/120    avg_loss:0.037, val_acc:0.990]
Epoch [114/120    avg_loss:0.029, val_acc:0.990]
Epoch [115/120    avg_loss:0.018, val_acc:0.990]
Epoch [116/120    avg_loss:0.022, val_acc:0.990]
Epoch [117/120    avg_loss:0.022, val_acc:0.990]
Epoch [118/120    avg_loss:0.049, val_acc:0.992]
Epoch [119/120    avg_loss:0.019, val_acc:0.992]
Epoch [120/120    avg_loss:0.024, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   6   0   0   0   0   0   0   3   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   2   0   0   0   0   0   0   0   0  12 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.99545455 1.         0.97977528 0.97972973
 1.         1.         1.         1.         1.         0.98300654
 0.97991071 1.        ]

Kappa:
0.994303167451044
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79901c2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.214, val_acc:0.598]
Epoch [2/120    avg_loss:1.448, val_acc:0.715]
Epoch [3/120    avg_loss:1.013, val_acc:0.742]
Epoch [4/120    avg_loss:0.837, val_acc:0.829]
Epoch [5/120    avg_loss:0.736, val_acc:0.794]
Epoch [6/120    avg_loss:0.545, val_acc:0.821]
Epoch [7/120    avg_loss:0.488, val_acc:0.902]
Epoch [8/120    avg_loss:0.406, val_acc:0.887]
Epoch [9/120    avg_loss:0.386, val_acc:0.890]
Epoch [10/120    avg_loss:0.395, val_acc:0.910]
Epoch [11/120    avg_loss:0.329, val_acc:0.938]
Epoch [12/120    avg_loss:0.290, val_acc:0.890]
Epoch [13/120    avg_loss:0.342, val_acc:0.912]
Epoch [14/120    avg_loss:0.277, val_acc:0.927]
Epoch [15/120    avg_loss:0.210, val_acc:0.938]
Epoch [16/120    avg_loss:0.197, val_acc:0.954]
Epoch [17/120    avg_loss:0.155, val_acc:0.950]
Epoch [18/120    avg_loss:0.185, val_acc:0.929]
Epoch [19/120    avg_loss:0.172, val_acc:0.952]
Epoch [20/120    avg_loss:0.256, val_acc:0.919]
Epoch [21/120    avg_loss:0.142, val_acc:0.965]
Epoch [22/120    avg_loss:0.124, val_acc:0.950]
Epoch [23/120    avg_loss:0.139, val_acc:0.952]
Epoch [24/120    avg_loss:0.127, val_acc:0.960]
Epoch [25/120    avg_loss:0.198, val_acc:0.935]
Epoch [26/120    avg_loss:0.157, val_acc:0.971]
Epoch [27/120    avg_loss:0.158, val_acc:0.965]
Epoch [28/120    avg_loss:0.091, val_acc:0.969]
Epoch [29/120    avg_loss:0.106, val_acc:0.969]
Epoch [30/120    avg_loss:0.067, val_acc:0.977]
Epoch [31/120    avg_loss:0.064, val_acc:0.983]
Epoch [32/120    avg_loss:0.053, val_acc:0.977]
Epoch [33/120    avg_loss:0.084, val_acc:0.967]
Epoch [34/120    avg_loss:0.090, val_acc:0.973]
Epoch [35/120    avg_loss:0.103, val_acc:0.956]
Epoch [36/120    avg_loss:0.092, val_acc:0.967]
Epoch [37/120    avg_loss:0.050, val_acc:0.975]
Epoch [38/120    avg_loss:0.075, val_acc:0.971]
Epoch [39/120    avg_loss:0.058, val_acc:0.973]
Epoch [40/120    avg_loss:0.030, val_acc:0.977]
Epoch [41/120    avg_loss:0.040, val_acc:0.979]
Epoch [42/120    avg_loss:0.063, val_acc:0.975]
Epoch [43/120    avg_loss:0.039, val_acc:0.983]
Epoch [44/120    avg_loss:0.031, val_acc:0.979]
Epoch [45/120    avg_loss:0.052, val_acc:0.983]
Epoch [46/120    avg_loss:0.039, val_acc:0.981]
Epoch [47/120    avg_loss:0.054, val_acc:0.975]
Epoch [48/120    avg_loss:0.044, val_acc:0.958]
Epoch [49/120    avg_loss:0.092, val_acc:0.971]
Epoch [50/120    avg_loss:0.074, val_acc:0.960]
Epoch [51/120    avg_loss:0.091, val_acc:0.981]
Epoch [52/120    avg_loss:0.081, val_acc:0.973]
Epoch [53/120    avg_loss:0.040, val_acc:0.990]
Epoch [54/120    avg_loss:0.036, val_acc:0.983]
Epoch [55/120    avg_loss:0.019, val_acc:0.981]
Epoch [56/120    avg_loss:0.023, val_acc:0.977]
Epoch [57/120    avg_loss:0.062, val_acc:0.975]
Epoch [58/120    avg_loss:0.034, val_acc:0.981]
Epoch [59/120    avg_loss:0.073, val_acc:0.979]
Epoch [60/120    avg_loss:0.053, val_acc:0.973]
Epoch [61/120    avg_loss:0.040, val_acc:0.977]
Epoch [62/120    avg_loss:0.037, val_acc:0.979]
Epoch [63/120    avg_loss:0.025, val_acc:0.979]
Epoch [64/120    avg_loss:0.053, val_acc:0.985]
Epoch [65/120    avg_loss:0.042, val_acc:0.990]
Epoch [66/120    avg_loss:0.019, val_acc:0.981]
Epoch [67/120    avg_loss:0.026, val_acc:0.990]
Epoch [68/120    avg_loss:0.024, val_acc:0.992]
Epoch [69/120    avg_loss:0.035, val_acc:0.983]
Epoch [70/120    avg_loss:0.043, val_acc:0.985]
Epoch [71/120    avg_loss:0.040, val_acc:0.967]
Epoch [72/120    avg_loss:0.029, val_acc:0.985]
Epoch [73/120    avg_loss:0.011, val_acc:0.994]
Epoch [74/120    avg_loss:0.012, val_acc:0.990]
Epoch [75/120    avg_loss:0.008, val_acc:0.990]
Epoch [76/120    avg_loss:0.013, val_acc:0.992]
Epoch [77/120    avg_loss:0.021, val_acc:0.985]
Epoch [78/120    avg_loss:0.030, val_acc:0.988]
Epoch [79/120    avg_loss:0.020, val_acc:0.990]
Epoch [80/120    avg_loss:0.013, val_acc:0.996]
Epoch [81/120    avg_loss:0.012, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.992]
Epoch [83/120    avg_loss:0.024, val_acc:0.994]
Epoch [84/120    avg_loss:0.012, val_acc:0.996]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.994]
Epoch [87/120    avg_loss:0.015, val_acc:0.994]
Epoch [88/120    avg_loss:0.007, val_acc:0.996]
Epoch [89/120    avg_loss:0.008, val_acc:0.992]
Epoch [90/120    avg_loss:0.010, val_acc:0.990]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.025, val_acc:0.985]
Epoch [94/120    avg_loss:0.047, val_acc:0.985]
Epoch [95/120    avg_loss:0.020, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.017, val_acc:0.992]
Epoch [98/120    avg_loss:0.008, val_acc:0.992]
Epoch [99/120    avg_loss:0.007, val_acc:0.994]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.990]
Epoch [107/120    avg_loss:0.014, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.994]
Epoch [109/120    avg_loss:0.006, val_acc:0.994]
Epoch [110/120    avg_loss:0.004, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.005, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.994]
Epoch [117/120    avg_loss:0.005, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  11   0   0   0   0   0   0   2   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         0.99319728 1.         0.96396396 0.95302013
 1.         0.98378378 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9954897133222106
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda89f0c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.210, val_acc:0.565]
Epoch [2/120    avg_loss:1.517, val_acc:0.706]
Epoch [3/120    avg_loss:1.100, val_acc:0.708]
Epoch [4/120    avg_loss:0.963, val_acc:0.796]
Epoch [5/120    avg_loss:0.754, val_acc:0.817]
Epoch [6/120    avg_loss:0.673, val_acc:0.833]
Epoch [7/120    avg_loss:0.596, val_acc:0.867]
Epoch [8/120    avg_loss:0.493, val_acc:0.881]
Epoch [9/120    avg_loss:0.459, val_acc:0.877]
Epoch [10/120    avg_loss:0.384, val_acc:0.898]
Epoch [11/120    avg_loss:0.406, val_acc:0.935]
Epoch [12/120    avg_loss:0.321, val_acc:0.912]
Epoch [13/120    avg_loss:0.320, val_acc:0.915]
Epoch [14/120    avg_loss:0.383, val_acc:0.944]
Epoch [15/120    avg_loss:0.341, val_acc:0.929]
Epoch [16/120    avg_loss:0.279, val_acc:0.944]
Epoch [17/120    avg_loss:0.234, val_acc:0.950]
Epoch [18/120    avg_loss:0.298, val_acc:0.944]
Epoch [19/120    avg_loss:0.206, val_acc:0.948]
Epoch [20/120    avg_loss:0.223, val_acc:0.950]
Epoch [21/120    avg_loss:0.159, val_acc:0.954]
Epoch [22/120    avg_loss:0.171, val_acc:0.927]
Epoch [23/120    avg_loss:0.179, val_acc:0.977]
Epoch [24/120    avg_loss:0.139, val_acc:0.965]
Epoch [25/120    avg_loss:0.135, val_acc:0.969]
Epoch [26/120    avg_loss:0.132, val_acc:0.963]
Epoch [27/120    avg_loss:0.101, val_acc:0.979]
Epoch [28/120    avg_loss:0.101, val_acc:0.979]
Epoch [29/120    avg_loss:0.108, val_acc:0.981]
Epoch [30/120    avg_loss:0.062, val_acc:0.983]
Epoch [31/120    avg_loss:0.049, val_acc:0.979]
Epoch [32/120    avg_loss:0.055, val_acc:0.977]
Epoch [33/120    avg_loss:0.066, val_acc:0.977]
Epoch [34/120    avg_loss:0.124, val_acc:0.983]
Epoch [35/120    avg_loss:0.142, val_acc:0.948]
Epoch [36/120    avg_loss:0.130, val_acc:0.973]
Epoch [37/120    avg_loss:0.130, val_acc:0.952]
Epoch [38/120    avg_loss:0.110, val_acc:0.985]
Epoch [39/120    avg_loss:0.102, val_acc:0.977]
Epoch [40/120    avg_loss:0.087, val_acc:0.985]
Epoch [41/120    avg_loss:0.092, val_acc:0.988]
Epoch [42/120    avg_loss:0.052, val_acc:0.983]
Epoch [43/120    avg_loss:0.048, val_acc:0.981]
Epoch [44/120    avg_loss:0.047, val_acc:0.988]
Epoch [45/120    avg_loss:0.033, val_acc:0.981]
Epoch [46/120    avg_loss:0.033, val_acc:0.988]
Epoch [47/120    avg_loss:0.046, val_acc:0.971]
Epoch [48/120    avg_loss:0.071, val_acc:0.988]
Epoch [49/120    avg_loss:0.057, val_acc:0.988]
Epoch [50/120    avg_loss:0.047, val_acc:0.983]
Epoch [51/120    avg_loss:0.044, val_acc:0.977]
Epoch [52/120    avg_loss:0.042, val_acc:0.983]
Epoch [53/120    avg_loss:0.063, val_acc:0.988]
Epoch [54/120    avg_loss:0.049, val_acc:0.983]
Epoch [55/120    avg_loss:0.031, val_acc:0.988]
Epoch [56/120    avg_loss:0.030, val_acc:0.988]
Epoch [57/120    avg_loss:0.034, val_acc:0.988]
Epoch [58/120    avg_loss:0.062, val_acc:0.985]
Epoch [59/120    avg_loss:0.051, val_acc:0.992]
Epoch [60/120    avg_loss:0.041, val_acc:0.988]
Epoch [61/120    avg_loss:0.050, val_acc:0.992]
Epoch [62/120    avg_loss:0.038, val_acc:0.988]
Epoch [63/120    avg_loss:0.063, val_acc:0.979]
Epoch [64/120    avg_loss:0.031, val_acc:0.992]
Epoch [65/120    avg_loss:0.021, val_acc:0.992]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.015, val_acc:0.992]
Epoch [68/120    avg_loss:0.014, val_acc:0.992]
Epoch [69/120    avg_loss:0.021, val_acc:0.985]
Epoch [70/120    avg_loss:0.033, val_acc:0.983]
Epoch [71/120    avg_loss:0.050, val_acc:0.973]
Epoch [72/120    avg_loss:0.066, val_acc:0.985]
Epoch [73/120    avg_loss:0.027, val_acc:0.990]
Epoch [74/120    avg_loss:0.019, val_acc:0.990]
Epoch [75/120    avg_loss:0.017, val_acc:0.992]
Epoch [76/120    avg_loss:0.032, val_acc:0.990]
Epoch [77/120    avg_loss:0.046, val_acc:0.979]
Epoch [78/120    avg_loss:0.050, val_acc:0.981]
Epoch [79/120    avg_loss:0.036, val_acc:0.994]
Epoch [80/120    avg_loss:0.027, val_acc:0.994]
Epoch [81/120    avg_loss:0.015, val_acc:0.994]
Epoch [82/120    avg_loss:0.024, val_acc:0.985]
Epoch [83/120    avg_loss:0.017, val_acc:0.992]
Epoch [84/120    avg_loss:0.014, val_acc:0.996]
Epoch [85/120    avg_loss:0.010, val_acc:0.992]
Epoch [86/120    avg_loss:0.010, val_acc:0.992]
Epoch [87/120    avg_loss:0.014, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.994]
Epoch [90/120    avg_loss:0.007, val_acc:0.994]
Epoch [91/120    avg_loss:0.012, val_acc:0.994]
Epoch [92/120    avg_loss:0.007, val_acc:0.994]
Epoch [93/120    avg_loss:0.007, val_acc:0.994]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.010, val_acc:0.992]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.015, val_acc:0.996]
Epoch [98/120    avg_loss:0.041, val_acc:0.988]
Epoch [99/120    avg_loss:0.075, val_acc:0.973]
Epoch [100/120    avg_loss:0.052, val_acc:0.981]
Epoch [101/120    avg_loss:0.055, val_acc:0.994]
Epoch [102/120    avg_loss:0.074, val_acc:0.990]
Epoch [103/120    avg_loss:0.019, val_acc:0.981]
Epoch [104/120    avg_loss:0.019, val_acc:0.988]
Epoch [105/120    avg_loss:0.031, val_acc:0.994]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.013, val_acc:0.994]
Epoch [109/120    avg_loss:0.020, val_acc:0.990]
Epoch [110/120    avg_loss:0.022, val_acc:0.992]
Epoch [111/120    avg_loss:0.015, val_acc:0.992]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.009, val_acc:0.992]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.013, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   7   0   0   0   0   0   0   3   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         0.99319728 1.         0.96444444 0.95532646
 1.         0.98378378 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9954895306849338
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f826ef24828>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.226, val_acc:0.567]
Epoch [2/120    avg_loss:1.486, val_acc:0.633]
Epoch [3/120    avg_loss:1.061, val_acc:0.746]
Epoch [4/120    avg_loss:0.830, val_acc:0.777]
Epoch [5/120    avg_loss:0.665, val_acc:0.842]
Epoch [6/120    avg_loss:0.599, val_acc:0.790]
Epoch [7/120    avg_loss:0.527, val_acc:0.865]
Epoch [8/120    avg_loss:0.452, val_acc:0.917]
Epoch [9/120    avg_loss:0.466, val_acc:0.919]
Epoch [10/120    avg_loss:0.364, val_acc:0.923]
Epoch [11/120    avg_loss:0.370, val_acc:0.948]
Epoch [12/120    avg_loss:0.326, val_acc:0.942]
Epoch [13/120    avg_loss:0.252, val_acc:0.950]
Epoch [14/120    avg_loss:0.225, val_acc:0.942]
Epoch [15/120    avg_loss:0.244, val_acc:0.954]
Epoch [16/120    avg_loss:0.259, val_acc:0.944]
Epoch [17/120    avg_loss:0.251, val_acc:0.942]
Epoch [18/120    avg_loss:0.227, val_acc:0.958]
Epoch [19/120    avg_loss:0.209, val_acc:0.963]
Epoch [20/120    avg_loss:0.184, val_acc:0.950]
Epoch [21/120    avg_loss:0.141, val_acc:0.967]
Epoch [22/120    avg_loss:0.172, val_acc:0.950]
Epoch [23/120    avg_loss:0.139, val_acc:0.975]
Epoch [24/120    avg_loss:0.110, val_acc:0.965]
Epoch [25/120    avg_loss:0.136, val_acc:0.950]
Epoch [26/120    avg_loss:0.139, val_acc:0.946]
Epoch [27/120    avg_loss:0.136, val_acc:0.969]
Epoch [28/120    avg_loss:0.124, val_acc:0.975]
Epoch [29/120    avg_loss:0.166, val_acc:0.963]
Epoch [30/120    avg_loss:0.128, val_acc:0.981]
Epoch [31/120    avg_loss:0.163, val_acc:0.967]
Epoch [32/120    avg_loss:0.147, val_acc:0.975]
Epoch [33/120    avg_loss:0.086, val_acc:0.983]
Epoch [34/120    avg_loss:0.104, val_acc:0.931]
Epoch [35/120    avg_loss:0.121, val_acc:0.981]
Epoch [36/120    avg_loss:0.129, val_acc:0.967]
Epoch [37/120    avg_loss:0.151, val_acc:0.952]
Epoch [38/120    avg_loss:0.117, val_acc:0.963]
Epoch [39/120    avg_loss:0.104, val_acc:0.975]
Epoch [40/120    avg_loss:0.095, val_acc:0.977]
Epoch [41/120    avg_loss:0.086, val_acc:0.979]
Epoch [42/120    avg_loss:0.090, val_acc:0.973]
Epoch [43/120    avg_loss:0.078, val_acc:0.977]
Epoch [44/120    avg_loss:0.148, val_acc:0.946]
Epoch [45/120    avg_loss:0.154, val_acc:0.958]
Epoch [46/120    avg_loss:0.137, val_acc:0.979]
Epoch [47/120    avg_loss:0.080, val_acc:0.981]
Epoch [48/120    avg_loss:0.048, val_acc:0.985]
Epoch [49/120    avg_loss:0.047, val_acc:0.990]
Epoch [50/120    avg_loss:0.034, val_acc:0.990]
Epoch [51/120    avg_loss:0.049, val_acc:0.988]
Epoch [52/120    avg_loss:0.034, val_acc:0.988]
Epoch [53/120    avg_loss:0.042, val_acc:0.990]
Epoch [54/120    avg_loss:0.040, val_acc:0.990]
Epoch [55/120    avg_loss:0.039, val_acc:0.988]
Epoch [56/120    avg_loss:0.049, val_acc:0.990]
Epoch [57/120    avg_loss:0.041, val_acc:0.990]
Epoch [58/120    avg_loss:0.041, val_acc:0.988]
Epoch [59/120    avg_loss:0.038, val_acc:0.985]
Epoch [60/120    avg_loss:0.035, val_acc:0.988]
Epoch [61/120    avg_loss:0.033, val_acc:0.988]
Epoch [62/120    avg_loss:0.033, val_acc:0.985]
Epoch [63/120    avg_loss:0.028, val_acc:0.988]
Epoch [64/120    avg_loss:0.050, val_acc:0.988]
Epoch [65/120    avg_loss:0.039, val_acc:0.990]
Epoch [66/120    avg_loss:0.028, val_acc:0.990]
Epoch [67/120    avg_loss:0.035, val_acc:0.990]
Epoch [68/120    avg_loss:0.027, val_acc:0.990]
Epoch [69/120    avg_loss:0.030, val_acc:0.988]
Epoch [70/120    avg_loss:0.036, val_acc:0.988]
Epoch [71/120    avg_loss:0.049, val_acc:0.988]
Epoch [72/120    avg_loss:0.044, val_acc:0.988]
Epoch [73/120    avg_loss:0.033, val_acc:0.990]
Epoch [74/120    avg_loss:0.026, val_acc:0.988]
Epoch [75/120    avg_loss:0.031, val_acc:0.988]
Epoch [76/120    avg_loss:0.045, val_acc:0.990]
Epoch [77/120    avg_loss:0.033, val_acc:0.990]
Epoch [78/120    avg_loss:0.026, val_acc:0.990]
Epoch [79/120    avg_loss:0.027, val_acc:0.992]
Epoch [80/120    avg_loss:0.024, val_acc:0.990]
Epoch [81/120    avg_loss:0.025, val_acc:0.992]
Epoch [82/120    avg_loss:0.022, val_acc:0.992]
Epoch [83/120    avg_loss:0.032, val_acc:0.992]
Epoch [84/120    avg_loss:0.027, val_acc:0.992]
Epoch [85/120    avg_loss:0.022, val_acc:0.992]
Epoch [86/120    avg_loss:0.022, val_acc:0.992]
Epoch [87/120    avg_loss:0.027, val_acc:0.992]
Epoch [88/120    avg_loss:0.027, val_acc:0.988]
Epoch [89/120    avg_loss:0.021, val_acc:0.990]
Epoch [90/120    avg_loss:0.032, val_acc:0.988]
Epoch [91/120    avg_loss:0.024, val_acc:0.988]
Epoch [92/120    avg_loss:0.027, val_acc:0.988]
Epoch [93/120    avg_loss:0.026, val_acc:0.988]
Epoch [94/120    avg_loss:0.038, val_acc:0.985]
Epoch [95/120    avg_loss:0.034, val_acc:0.985]
Epoch [96/120    avg_loss:0.026, val_acc:0.990]
Epoch [97/120    avg_loss:0.028, val_acc:0.988]
Epoch [98/120    avg_loss:0.029, val_acc:0.988]
Epoch [99/120    avg_loss:0.035, val_acc:0.990]
Epoch [100/120    avg_loss:0.041, val_acc:0.990]
Epoch [101/120    avg_loss:0.042, val_acc:0.990]
Epoch [102/120    avg_loss:0.023, val_acc:0.990]
Epoch [103/120    avg_loss:0.024, val_acc:0.990]
Epoch [104/120    avg_loss:0.022, val_acc:0.990]
Epoch [105/120    avg_loss:0.022, val_acc:0.990]
Epoch [106/120    avg_loss:0.022, val_acc:0.990]
Epoch [107/120    avg_loss:0.023, val_acc:0.990]
Epoch [108/120    avg_loss:0.023, val_acc:0.990]
Epoch [109/120    avg_loss:0.019, val_acc:0.990]
Epoch [110/120    avg_loss:0.027, val_acc:0.990]
Epoch [111/120    avg_loss:0.036, val_acc:0.990]
Epoch [112/120    avg_loss:0.020, val_acc:0.990]
Epoch [113/120    avg_loss:0.024, val_acc:0.990]
Epoch [114/120    avg_loss:0.016, val_acc:0.990]
Epoch [115/120    avg_loss:0.014, val_acc:0.990]
Epoch [116/120    avg_loss:0.020, val_acc:0.990]
Epoch [117/120    avg_loss:0.025, val_acc:0.990]
Epoch [118/120    avg_loss:0.036, val_acc:0.990]
Epoch [119/120    avg_loss:0.024, val_acc:0.990]
Epoch [120/120    avg_loss:0.029, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  12   0   0   0   0   0   0   2   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0  12 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.99095023 0.98901099 0.95089286 0.94983278
 1.         0.98924731 1.         1.         1.         0.9843342
 0.98210291 1.        ]

Kappa:
0.9909801580177928
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04605e97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.195, val_acc:0.573]
Epoch [2/120    avg_loss:1.454, val_acc:0.673]
Epoch [3/120    avg_loss:1.069, val_acc:0.748]
Epoch [4/120    avg_loss:0.876, val_acc:0.785]
Epoch [5/120    avg_loss:0.702, val_acc:0.806]
Epoch [6/120    avg_loss:0.581, val_acc:0.869]
Epoch [7/120    avg_loss:0.499, val_acc:0.856]
Epoch [8/120    avg_loss:0.516, val_acc:0.827]
Epoch [9/120    avg_loss:0.442, val_acc:0.908]
Epoch [10/120    avg_loss:0.383, val_acc:0.915]
Epoch [11/120    avg_loss:0.283, val_acc:0.925]
Epoch [12/120    avg_loss:0.321, val_acc:0.963]
Epoch [13/120    avg_loss:0.269, val_acc:0.965]
Epoch [14/120    avg_loss:0.202, val_acc:0.960]
Epoch [15/120    avg_loss:0.207, val_acc:0.967]
Epoch [16/120    avg_loss:0.226, val_acc:0.954]
Epoch [17/120    avg_loss:0.229, val_acc:0.958]
Epoch [18/120    avg_loss:0.226, val_acc:0.954]
Epoch [19/120    avg_loss:0.190, val_acc:0.977]
Epoch [20/120    avg_loss:0.117, val_acc:0.981]
Epoch [21/120    avg_loss:0.136, val_acc:0.977]
Epoch [22/120    avg_loss:0.118, val_acc:0.954]
Epoch [23/120    avg_loss:0.114, val_acc:0.971]
Epoch [24/120    avg_loss:0.130, val_acc:0.994]
Epoch [25/120    avg_loss:0.110, val_acc:0.969]
Epoch [26/120    avg_loss:0.119, val_acc:0.983]
Epoch [27/120    avg_loss:0.125, val_acc:0.983]
Epoch [28/120    avg_loss:0.151, val_acc:0.983]
Epoch [29/120    avg_loss:0.152, val_acc:0.979]
Epoch [30/120    avg_loss:0.122, val_acc:0.981]
Epoch [31/120    avg_loss:0.157, val_acc:0.981]
Epoch [32/120    avg_loss:0.128, val_acc:0.950]
Epoch [33/120    avg_loss:0.165, val_acc:0.971]
Epoch [34/120    avg_loss:0.134, val_acc:0.979]
Epoch [35/120    avg_loss:0.102, val_acc:0.981]
Epoch [36/120    avg_loss:0.110, val_acc:0.979]
Epoch [37/120    avg_loss:0.096, val_acc:0.973]
Epoch [38/120    avg_loss:0.080, val_acc:0.979]
Epoch [39/120    avg_loss:0.052, val_acc:0.988]
Epoch [40/120    avg_loss:0.056, val_acc:0.990]
Epoch [41/120    avg_loss:0.048, val_acc:0.988]
Epoch [42/120    avg_loss:0.049, val_acc:0.990]
Epoch [43/120    avg_loss:0.030, val_acc:0.988]
Epoch [44/120    avg_loss:0.061, val_acc:0.988]
Epoch [45/120    avg_loss:0.042, val_acc:0.990]
Epoch [46/120    avg_loss:0.040, val_acc:0.990]
Epoch [47/120    avg_loss:0.037, val_acc:0.990]
Epoch [48/120    avg_loss:0.044, val_acc:0.990]
Epoch [49/120    avg_loss:0.044, val_acc:0.990]
Epoch [50/120    avg_loss:0.041, val_acc:0.990]
Epoch [51/120    avg_loss:0.035, val_acc:0.990]
Epoch [52/120    avg_loss:0.025, val_acc:0.990]
Epoch [53/120    avg_loss:0.037, val_acc:0.990]
Epoch [54/120    avg_loss:0.037, val_acc:0.992]
Epoch [55/120    avg_loss:0.039, val_acc:0.992]
Epoch [56/120    avg_loss:0.038, val_acc:0.992]
Epoch [57/120    avg_loss:0.042, val_acc:0.992]
Epoch [58/120    avg_loss:0.048, val_acc:0.992]
Epoch [59/120    avg_loss:0.044, val_acc:0.992]
Epoch [60/120    avg_loss:0.049, val_acc:0.992]
Epoch [61/120    avg_loss:0.035, val_acc:0.992]
Epoch [62/120    avg_loss:0.028, val_acc:0.992]
Epoch [63/120    avg_loss:0.032, val_acc:0.992]
Epoch [64/120    avg_loss:0.041, val_acc:0.992]
Epoch [65/120    avg_loss:0.048, val_acc:0.992]
Epoch [66/120    avg_loss:0.043, val_acc:0.992]
Epoch [67/120    avg_loss:0.041, val_acc:0.992]
Epoch [68/120    avg_loss:0.042, val_acc:0.992]
Epoch [69/120    avg_loss:0.052, val_acc:0.992]
Epoch [70/120    avg_loss:0.040, val_acc:0.992]
Epoch [71/120    avg_loss:0.032, val_acc:0.992]
Epoch [72/120    avg_loss:0.035, val_acc:0.992]
Epoch [73/120    avg_loss:0.039, val_acc:0.992]
Epoch [74/120    avg_loss:0.027, val_acc:0.992]
Epoch [75/120    avg_loss:0.033, val_acc:0.992]
Epoch [76/120    avg_loss:0.036, val_acc:0.992]
Epoch [77/120    avg_loss:0.037, val_acc:0.992]
Epoch [78/120    avg_loss:0.029, val_acc:0.992]
Epoch [79/120    avg_loss:0.033, val_acc:0.992]
Epoch [80/120    avg_loss:0.026, val_acc:0.992]
Epoch [81/120    avg_loss:0.028, val_acc:0.992]
Epoch [82/120    avg_loss:0.027, val_acc:0.992]
Epoch [83/120    avg_loss:0.030, val_acc:0.992]
Epoch [84/120    avg_loss:0.043, val_acc:0.992]
Epoch [85/120    avg_loss:0.032, val_acc:0.992]
Epoch [86/120    avg_loss:0.025, val_acc:0.992]
Epoch [87/120    avg_loss:0.028, val_acc:0.992]
Epoch [88/120    avg_loss:0.028, val_acc:0.992]
Epoch [89/120    avg_loss:0.034, val_acc:0.992]
Epoch [90/120    avg_loss:0.037, val_acc:0.992]
Epoch [91/120    avg_loss:0.046, val_acc:0.992]
Epoch [92/120    avg_loss:0.031, val_acc:0.992]
Epoch [93/120    avg_loss:0.035, val_acc:0.992]
Epoch [94/120    avg_loss:0.052, val_acc:0.992]
Epoch [95/120    avg_loss:0.033, val_acc:0.992]
Epoch [96/120    avg_loss:0.034, val_acc:0.992]
Epoch [97/120    avg_loss:0.029, val_acc:0.992]
Epoch [98/120    avg_loss:0.034, val_acc:0.992]
Epoch [99/120    avg_loss:0.047, val_acc:0.992]
Epoch [100/120    avg_loss:0.028, val_acc:0.992]
Epoch [101/120    avg_loss:0.036, val_acc:0.992]
Epoch [102/120    avg_loss:0.023, val_acc:0.992]
Epoch [103/120    avg_loss:0.030, val_acc:0.992]
Epoch [104/120    avg_loss:0.036, val_acc:0.992]
Epoch [105/120    avg_loss:0.036, val_acc:0.992]
Epoch [106/120    avg_loss:0.032, val_acc:0.992]
Epoch [107/120    avg_loss:0.033, val_acc:0.992]
Epoch [108/120    avg_loss:0.032, val_acc:0.992]
Epoch [109/120    avg_loss:0.033, val_acc:0.992]
Epoch [110/120    avg_loss:0.042, val_acc:0.992]
Epoch [111/120    avg_loss:0.045, val_acc:0.992]
Epoch [112/120    avg_loss:0.037, val_acc:0.992]
Epoch [113/120    avg_loss:0.038, val_acc:0.992]
Epoch [114/120    avg_loss:0.032, val_acc:0.992]
Epoch [115/120    avg_loss:0.035, val_acc:0.992]
Epoch [116/120    avg_loss:0.037, val_acc:0.992]
Epoch [117/120    avg_loss:0.034, val_acc:0.992]
Epoch [118/120    avg_loss:0.038, val_acc:0.992]
Epoch [119/120    avg_loss:0.044, val_acc:0.992]
Epoch [120/120    avg_loss:0.036, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  10   0   0   0   0   0   0   5   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0  15 438   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.9977221  1.         0.95711061 0.9527027
 1.         0.99465241 1.         1.         1.         0.97916667
 0.97658863 1.        ]

Kappa:
0.9914543978192417
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a85267748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.214, val_acc:0.542]
Epoch [2/120    avg_loss:1.467, val_acc:0.619]
Epoch [3/120    avg_loss:1.131, val_acc:0.685]
Epoch [4/120    avg_loss:0.815, val_acc:0.769]
Epoch [5/120    avg_loss:0.725, val_acc:0.844]
Epoch [6/120    avg_loss:0.655, val_acc:0.860]
Epoch [7/120    avg_loss:0.586, val_acc:0.863]
Epoch [8/120    avg_loss:0.548, val_acc:0.902]
Epoch [9/120    avg_loss:0.500, val_acc:0.781]
Epoch [10/120    avg_loss:0.458, val_acc:0.917]
Epoch [11/120    avg_loss:0.347, val_acc:0.894]
Epoch [12/120    avg_loss:0.358, val_acc:0.894]
Epoch [13/120    avg_loss:0.346, val_acc:0.925]
Epoch [14/120    avg_loss:0.351, val_acc:0.933]
Epoch [15/120    avg_loss:0.257, val_acc:0.927]
Epoch [16/120    avg_loss:0.279, val_acc:0.933]
Epoch [17/120    avg_loss:0.196, val_acc:0.954]
Epoch [18/120    avg_loss:0.242, val_acc:0.958]
Epoch [19/120    avg_loss:0.213, val_acc:0.908]
Epoch [20/120    avg_loss:0.268, val_acc:0.927]
Epoch [21/120    avg_loss:0.210, val_acc:0.954]
Epoch [22/120    avg_loss:0.214, val_acc:0.971]
Epoch [23/120    avg_loss:0.173, val_acc:0.963]
Epoch [24/120    avg_loss:0.132, val_acc:0.960]
Epoch [25/120    avg_loss:0.157, val_acc:0.952]
Epoch [26/120    avg_loss:0.177, val_acc:0.954]
Epoch [27/120    avg_loss:0.190, val_acc:0.954]
Epoch [28/120    avg_loss:0.147, val_acc:0.956]
Epoch [29/120    avg_loss:0.206, val_acc:0.969]
Epoch [30/120    avg_loss:0.128, val_acc:0.977]
Epoch [31/120    avg_loss:0.139, val_acc:0.960]
Epoch [32/120    avg_loss:0.118, val_acc:0.960]
Epoch [33/120    avg_loss:0.124, val_acc:0.967]
Epoch [34/120    avg_loss:0.113, val_acc:0.975]
Epoch [35/120    avg_loss:0.143, val_acc:0.971]
Epoch [36/120    avg_loss:0.064, val_acc:0.971]
Epoch [37/120    avg_loss:0.065, val_acc:0.977]
Epoch [38/120    avg_loss:0.060, val_acc:0.985]
Epoch [39/120    avg_loss:0.075, val_acc:0.967]
Epoch [40/120    avg_loss:0.096, val_acc:0.967]
Epoch [41/120    avg_loss:0.076, val_acc:0.985]
Epoch [42/120    avg_loss:0.045, val_acc:0.983]
Epoch [43/120    avg_loss:0.045, val_acc:0.975]
Epoch [44/120    avg_loss:0.066, val_acc:0.983]
Epoch [45/120    avg_loss:0.053, val_acc:0.981]
Epoch [46/120    avg_loss:0.067, val_acc:0.979]
Epoch [47/120    avg_loss:0.067, val_acc:0.973]
Epoch [48/120    avg_loss:0.043, val_acc:0.979]
Epoch [49/120    avg_loss:0.038, val_acc:0.979]
Epoch [50/120    avg_loss:0.041, val_acc:0.979]
Epoch [51/120    avg_loss:0.050, val_acc:0.979]
Epoch [52/120    avg_loss:0.061, val_acc:0.988]
Epoch [53/120    avg_loss:0.055, val_acc:0.977]
Epoch [54/120    avg_loss:0.065, val_acc:0.977]
Epoch [55/120    avg_loss:0.032, val_acc:0.985]
Epoch [56/120    avg_loss:0.053, val_acc:0.985]
Epoch [57/120    avg_loss:0.038, val_acc:0.985]
Epoch [58/120    avg_loss:0.022, val_acc:0.985]
Epoch [59/120    avg_loss:0.067, val_acc:0.988]
Epoch [60/120    avg_loss:0.058, val_acc:0.992]
Epoch [61/120    avg_loss:0.034, val_acc:0.992]
Epoch [62/120    avg_loss:0.035, val_acc:0.983]
Epoch [63/120    avg_loss:0.024, val_acc:0.992]
Epoch [64/120    avg_loss:0.019, val_acc:0.988]
Epoch [65/120    avg_loss:0.016, val_acc:0.990]
Epoch [66/120    avg_loss:0.023, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.990]
Epoch [68/120    avg_loss:0.040, val_acc:0.990]
Epoch [69/120    avg_loss:0.034, val_acc:0.990]
Epoch [70/120    avg_loss:0.015, val_acc:0.990]
Epoch [71/120    avg_loss:0.016, val_acc:0.992]
Epoch [72/120    avg_loss:0.020, val_acc:0.990]
Epoch [73/120    avg_loss:0.019, val_acc:0.994]
Epoch [74/120    avg_loss:0.017, val_acc:0.990]
Epoch [75/120    avg_loss:0.011, val_acc:0.992]
Epoch [76/120    avg_loss:0.032, val_acc:0.988]
Epoch [77/120    avg_loss:0.024, val_acc:0.988]
Epoch [78/120    avg_loss:0.017, val_acc:0.992]
Epoch [79/120    avg_loss:0.018, val_acc:0.992]
Epoch [80/120    avg_loss:0.021, val_acc:0.994]
Epoch [81/120    avg_loss:0.017, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.992]
Epoch [83/120    avg_loss:0.020, val_acc:0.992]
Epoch [84/120    avg_loss:0.051, val_acc:0.983]
Epoch [85/120    avg_loss:0.030, val_acc:0.985]
Epoch [86/120    avg_loss:0.023, val_acc:0.988]
Epoch [87/120    avg_loss:0.017, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.017, val_acc:0.985]
Epoch [91/120    avg_loss:0.014, val_acc:0.983]
Epoch [92/120    avg_loss:0.024, val_acc:0.971]
Epoch [93/120    avg_loss:0.022, val_acc:0.994]
Epoch [94/120    avg_loss:0.014, val_acc:0.994]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.019, val_acc:0.992]
Epoch [97/120    avg_loss:0.015, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.006, val_acc:0.996]
Epoch [100/120    avg_loss:0.015, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.994]
Epoch [104/120    avg_loss:0.006, val_acc:0.996]
Epoch [105/120    avg_loss:0.006, val_acc:0.994]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.994]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.994]
Epoch [114/120    avg_loss:0.009, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.003, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.7867803837953

F1 scores:
[       nan 1.         1.         1.         0.97807018 0.96864111
 1.         1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.997626128740353
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feceb5816d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.183, val_acc:0.469]
Epoch [2/120    avg_loss:1.530, val_acc:0.656]
Epoch [3/120    avg_loss:1.146, val_acc:0.727]
Epoch [4/120    avg_loss:0.876, val_acc:0.808]
Epoch [5/120    avg_loss:0.790, val_acc:0.802]
Epoch [6/120    avg_loss:0.652, val_acc:0.867]
Epoch [7/120    avg_loss:0.582, val_acc:0.798]
Epoch [8/120    avg_loss:0.529, val_acc:0.906]
Epoch [9/120    avg_loss:0.570, val_acc:0.877]
Epoch [10/120    avg_loss:0.454, val_acc:0.879]
Epoch [11/120    avg_loss:0.429, val_acc:0.923]
Epoch [12/120    avg_loss:0.312, val_acc:0.942]
Epoch [13/120    avg_loss:0.285, val_acc:0.944]
Epoch [14/120    avg_loss:0.348, val_acc:0.954]
Epoch [15/120    avg_loss:0.296, val_acc:0.908]
Epoch [16/120    avg_loss:0.339, val_acc:0.915]
Epoch [17/120    avg_loss:0.266, val_acc:0.965]
Epoch [18/120    avg_loss:0.222, val_acc:0.929]
Epoch [19/120    avg_loss:0.192, val_acc:0.942]
Epoch [20/120    avg_loss:0.163, val_acc:0.950]
Epoch [21/120    avg_loss:0.171, val_acc:0.940]
Epoch [22/120    avg_loss:0.211, val_acc:0.946]
Epoch [23/120    avg_loss:0.158, val_acc:0.963]
Epoch [24/120    avg_loss:0.110, val_acc:0.973]
Epoch [25/120    avg_loss:0.125, val_acc:0.983]
Epoch [26/120    avg_loss:0.267, val_acc:0.967]
Epoch [27/120    avg_loss:0.135, val_acc:0.973]
Epoch [28/120    avg_loss:0.158, val_acc:0.969]
Epoch [29/120    avg_loss:0.132, val_acc:0.981]
Epoch [30/120    avg_loss:0.093, val_acc:0.981]
Epoch [31/120    avg_loss:0.094, val_acc:0.975]
Epoch [32/120    avg_loss:0.116, val_acc:0.983]
Epoch [33/120    avg_loss:0.083, val_acc:0.983]
Epoch [34/120    avg_loss:0.108, val_acc:0.985]
Epoch [35/120    avg_loss:0.128, val_acc:0.973]
Epoch [36/120    avg_loss:0.108, val_acc:0.977]
Epoch [37/120    avg_loss:0.133, val_acc:0.969]
Epoch [38/120    avg_loss:0.122, val_acc:0.979]
Epoch [39/120    avg_loss:0.095, val_acc:0.985]
Epoch [40/120    avg_loss:0.060, val_acc:0.994]
Epoch [41/120    avg_loss:0.082, val_acc:0.988]
Epoch [42/120    avg_loss:0.091, val_acc:0.977]
Epoch [43/120    avg_loss:0.047, val_acc:0.992]
Epoch [44/120    avg_loss:0.038, val_acc:1.000]
Epoch [45/120    avg_loss:0.049, val_acc:0.985]
Epoch [46/120    avg_loss:0.046, val_acc:0.996]
Epoch [47/120    avg_loss:0.047, val_acc:0.996]
Epoch [48/120    avg_loss:0.054, val_acc:0.988]
Epoch [49/120    avg_loss:0.037, val_acc:0.985]
Epoch [50/120    avg_loss:0.056, val_acc:0.981]
Epoch [51/120    avg_loss:0.052, val_acc:0.998]
Epoch [52/120    avg_loss:0.045, val_acc:0.996]
Epoch [53/120    avg_loss:0.026, val_acc:0.998]
Epoch [54/120    avg_loss:0.045, val_acc:0.975]
Epoch [55/120    avg_loss:0.031, val_acc:0.988]
Epoch [56/120    avg_loss:0.062, val_acc:0.990]
Epoch [57/120    avg_loss:0.051, val_acc:0.992]
Epoch [58/120    avg_loss:0.041, val_acc:0.996]
Epoch [59/120    avg_loss:0.022, val_acc:0.996]
Epoch [60/120    avg_loss:0.021, val_acc:0.996]
Epoch [61/120    avg_loss:0.021, val_acc:0.996]
Epoch [62/120    avg_loss:0.018, val_acc:0.996]
Epoch [63/120    avg_loss:0.025, val_acc:0.996]
Epoch [64/120    avg_loss:0.023, val_acc:0.996]
Epoch [65/120    avg_loss:0.023, val_acc:0.996]
Epoch [66/120    avg_loss:0.026, val_acc:0.996]
Epoch [67/120    avg_loss:0.021, val_acc:0.994]
Epoch [68/120    avg_loss:0.015, val_acc:0.994]
Epoch [69/120    avg_loss:0.022, val_acc:0.998]
Epoch [70/120    avg_loss:0.016, val_acc:0.998]
Epoch [71/120    avg_loss:0.016, val_acc:0.998]
Epoch [72/120    avg_loss:0.028, val_acc:0.998]
Epoch [73/120    avg_loss:0.016, val_acc:0.998]
Epoch [74/120    avg_loss:0.013, val_acc:0.998]
Epoch [75/120    avg_loss:0.011, val_acc:0.998]
Epoch [76/120    avg_loss:0.017, val_acc:0.998]
Epoch [77/120    avg_loss:0.023, val_acc:0.998]
Epoch [78/120    avg_loss:0.015, val_acc:0.998]
Epoch [79/120    avg_loss:0.023, val_acc:0.998]
Epoch [80/120    avg_loss:0.014, val_acc:0.998]
Epoch [81/120    avg_loss:0.016, val_acc:0.998]
Epoch [82/120    avg_loss:0.021, val_acc:0.998]
Epoch [83/120    avg_loss:0.014, val_acc:0.998]
Epoch [84/120    avg_loss:0.025, val_acc:0.998]
Epoch [85/120    avg_loss:0.018, val_acc:0.998]
Epoch [86/120    avg_loss:0.021, val_acc:0.998]
Epoch [87/120    avg_loss:0.012, val_acc:0.998]
Epoch [88/120    avg_loss:0.015, val_acc:0.998]
Epoch [89/120    avg_loss:0.018, val_acc:0.998]
Epoch [90/120    avg_loss:0.013, val_acc:0.998]
Epoch [91/120    avg_loss:0.011, val_acc:0.998]
Epoch [92/120    avg_loss:0.016, val_acc:0.998]
Epoch [93/120    avg_loss:0.019, val_acc:0.998]
Epoch [94/120    avg_loss:0.017, val_acc:0.998]
Epoch [95/120    avg_loss:0.019, val_acc:0.998]
Epoch [96/120    avg_loss:0.015, val_acc:0.998]
Epoch [97/120    avg_loss:0.022, val_acc:0.998]
Epoch [98/120    avg_loss:0.017, val_acc:0.998]
Epoch [99/120    avg_loss:0.016, val_acc:0.998]
Epoch [100/120    avg_loss:0.016, val_acc:0.998]
Epoch [101/120    avg_loss:0.018, val_acc:0.998]
Epoch [102/120    avg_loss:0.018, val_acc:0.998]
Epoch [103/120    avg_loss:0.019, val_acc:0.998]
Epoch [104/120    avg_loss:0.021, val_acc:0.998]
Epoch [105/120    avg_loss:0.019, val_acc:0.998]
Epoch [106/120    avg_loss:0.018, val_acc:0.998]
Epoch [107/120    avg_loss:0.014, val_acc:0.998]
Epoch [108/120    avg_loss:0.022, val_acc:0.998]
Epoch [109/120    avg_loss:0.019, val_acc:0.998]
Epoch [110/120    avg_loss:0.014, val_acc:0.998]
Epoch [111/120    avg_loss:0.015, val_acc:0.998]
Epoch [112/120    avg_loss:0.014, val_acc:0.998]
Epoch [113/120    avg_loss:0.013, val_acc:0.998]
Epoch [114/120    avg_loss:0.015, val_acc:0.998]
Epoch [115/120    avg_loss:0.012, val_acc:0.998]
Epoch [116/120    avg_loss:0.015, val_acc:0.998]
Epoch [117/120    avg_loss:0.020, val_acc:0.998]
Epoch [118/120    avg_loss:0.017, val_acc:0.998]
Epoch [119/120    avg_loss:0.015, val_acc:0.998]
Epoch [120/120    avg_loss:0.016, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 1.         0.99319728 1.         0.96818182 0.95709571
 1.         0.98378378 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9959646102442617
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3720b63748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.229, val_acc:0.490]
Epoch [2/120    avg_loss:1.447, val_acc:0.765]
Epoch [3/120    avg_loss:1.049, val_acc:0.760]
Epoch [4/120    avg_loss:0.816, val_acc:0.796]
Epoch [5/120    avg_loss:0.757, val_acc:0.817]
Epoch [6/120    avg_loss:0.627, val_acc:0.838]
Epoch [7/120    avg_loss:0.591, val_acc:0.887]
Epoch [8/120    avg_loss:0.525, val_acc:0.877]
Epoch [9/120    avg_loss:0.436, val_acc:0.898]
Epoch [10/120    avg_loss:0.398, val_acc:0.869]
Epoch [11/120    avg_loss:0.363, val_acc:0.931]
Epoch [12/120    avg_loss:0.407, val_acc:0.883]
Epoch [13/120    avg_loss:0.313, val_acc:0.935]
Epoch [14/120    avg_loss:0.417, val_acc:0.898]
Epoch [15/120    avg_loss:0.407, val_acc:0.919]
Epoch [16/120    avg_loss:0.252, val_acc:0.946]
Epoch [17/120    avg_loss:0.228, val_acc:0.940]
Epoch [18/120    avg_loss:0.242, val_acc:0.929]
Epoch [19/120    avg_loss:0.289, val_acc:0.956]
Epoch [20/120    avg_loss:0.176, val_acc:0.963]
Epoch [21/120    avg_loss:0.205, val_acc:0.960]
Epoch [22/120    avg_loss:0.234, val_acc:0.944]
Epoch [23/120    avg_loss:0.225, val_acc:0.952]
Epoch [24/120    avg_loss:0.169, val_acc:0.963]
Epoch [25/120    avg_loss:0.136, val_acc:0.975]
Epoch [26/120    avg_loss:0.113, val_acc:0.977]
Epoch [27/120    avg_loss:0.107, val_acc:0.975]
Epoch [28/120    avg_loss:0.110, val_acc:0.981]
Epoch [29/120    avg_loss:0.088, val_acc:0.977]
Epoch [30/120    avg_loss:0.079, val_acc:0.977]
Epoch [31/120    avg_loss:0.081, val_acc:0.979]
Epoch [32/120    avg_loss:0.086, val_acc:0.990]
Epoch [33/120    avg_loss:0.114, val_acc:0.975]
Epoch [34/120    avg_loss:0.078, val_acc:0.985]
Epoch [35/120    avg_loss:0.082, val_acc:0.990]
Epoch [36/120    avg_loss:0.080, val_acc:0.983]
Epoch [37/120    avg_loss:0.083, val_acc:0.990]
Epoch [38/120    avg_loss:0.071, val_acc:0.985]
Epoch [39/120    avg_loss:0.108, val_acc:0.971]
Epoch [40/120    avg_loss:0.090, val_acc:0.973]
Epoch [41/120    avg_loss:0.098, val_acc:0.979]
Epoch [42/120    avg_loss:0.131, val_acc:0.963]
Epoch [43/120    avg_loss:0.094, val_acc:0.971]
Epoch [44/120    avg_loss:0.077, val_acc:0.990]
Epoch [45/120    avg_loss:0.125, val_acc:0.979]
Epoch [46/120    avg_loss:0.058, val_acc:0.990]
Epoch [47/120    avg_loss:0.071, val_acc:0.988]
Epoch [48/120    avg_loss:0.052, val_acc:0.992]
Epoch [49/120    avg_loss:0.030, val_acc:0.988]
Epoch [50/120    avg_loss:0.036, val_acc:0.990]
Epoch [51/120    avg_loss:0.027, val_acc:0.990]
Epoch [52/120    avg_loss:0.036, val_acc:0.990]
Epoch [53/120    avg_loss:0.046, val_acc:0.994]
Epoch [54/120    avg_loss:0.038, val_acc:0.977]
Epoch [55/120    avg_loss:0.045, val_acc:0.977]
Epoch [56/120    avg_loss:0.049, val_acc:0.979]
Epoch [57/120    avg_loss:0.078, val_acc:0.988]
Epoch [58/120    avg_loss:0.056, val_acc:0.990]
Epoch [59/120    avg_loss:0.088, val_acc:0.992]
Epoch [60/120    avg_loss:0.062, val_acc:0.990]
Epoch [61/120    avg_loss:0.067, val_acc:0.994]
Epoch [62/120    avg_loss:0.043, val_acc:0.992]
Epoch [63/120    avg_loss:0.031, val_acc:0.990]
Epoch [64/120    avg_loss:0.014, val_acc:0.994]
Epoch [65/120    avg_loss:0.026, val_acc:0.988]
Epoch [66/120    avg_loss:0.030, val_acc:0.990]
Epoch [67/120    avg_loss:0.040, val_acc:0.973]
Epoch [68/120    avg_loss:0.034, val_acc:0.990]
Epoch [69/120    avg_loss:0.029, val_acc:0.988]
Epoch [70/120    avg_loss:0.027, val_acc:0.992]
Epoch [71/120    avg_loss:0.018, val_acc:0.992]
Epoch [72/120    avg_loss:0.021, val_acc:0.994]
Epoch [73/120    avg_loss:0.021, val_acc:0.996]
Epoch [74/120    avg_loss:0.042, val_acc:0.992]
Epoch [75/120    avg_loss:0.020, val_acc:0.988]
Epoch [76/120    avg_loss:0.032, val_acc:0.983]
Epoch [77/120    avg_loss:0.039, val_acc:0.985]
Epoch [78/120    avg_loss:0.021, val_acc:0.992]
Epoch [79/120    avg_loss:0.088, val_acc:0.992]
Epoch [80/120    avg_loss:0.032, val_acc:0.990]
Epoch [81/120    avg_loss:0.023, val_acc:0.992]
Epoch [82/120    avg_loss:0.035, val_acc:0.985]
Epoch [83/120    avg_loss:0.023, val_acc:0.990]
Epoch [84/120    avg_loss:0.018, val_acc:0.996]
Epoch [85/120    avg_loss:0.044, val_acc:0.985]
Epoch [86/120    avg_loss:0.035, val_acc:0.994]
Epoch [87/120    avg_loss:0.023, val_acc:0.990]
Epoch [88/120    avg_loss:0.017, val_acc:0.992]
Epoch [89/120    avg_loss:0.022, val_acc:0.996]
Epoch [90/120    avg_loss:0.009, val_acc:0.996]
Epoch [91/120    avg_loss:0.007, val_acc:0.998]
Epoch [92/120    avg_loss:0.015, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.994]
Epoch [94/120    avg_loss:0.045, val_acc:0.981]
Epoch [95/120    avg_loss:0.056, val_acc:0.981]
Epoch [96/120    avg_loss:0.028, val_acc:0.996]
Epoch [97/120    avg_loss:0.012, val_acc:0.992]
Epoch [98/120    avg_loss:0.013, val_acc:0.992]
Epoch [99/120    avg_loss:0.018, val_acc:0.992]
Epoch [100/120    avg_loss:0.031, val_acc:0.992]
Epoch [101/120    avg_loss:0.030, val_acc:0.990]
Epoch [102/120    avg_loss:0.027, val_acc:0.992]
Epoch [103/120    avg_loss:0.033, val_acc:0.992]
Epoch [104/120    avg_loss:0.039, val_acc:0.992]
Epoch [105/120    avg_loss:0.026, val_acc:0.994]
Epoch [106/120    avg_loss:0.014, val_acc:0.994]
Epoch [107/120    avg_loss:0.011, val_acc:0.994]
Epoch [108/120    avg_loss:0.008, val_acc:0.994]
Epoch [109/120    avg_loss:0.010, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.994]
Epoch [111/120    avg_loss:0.013, val_acc:0.994]
Epoch [112/120    avg_loss:0.009, val_acc:0.994]
Epoch [113/120    avg_loss:0.008, val_acc:0.994]
Epoch [114/120    avg_loss:0.027, val_acc:0.994]
Epoch [115/120    avg_loss:0.009, val_acc:0.994]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.006, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.016, val_acc:0.994]
Epoch [120/120    avg_loss:0.007, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   3   0   0   0   0   0   0   4   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 1.         1.         1.         0.969163   0.96503497
 1.         1.         1.         1.         1.         0.99210526
 0.98893805 1.        ]

Kappa:
0.9952521844254034
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71bf7b57f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.243, val_acc:0.615]
Epoch [2/120    avg_loss:1.493, val_acc:0.658]
Epoch [3/120    avg_loss:1.111, val_acc:0.685]
Epoch [4/120    avg_loss:0.851, val_acc:0.835]
Epoch [5/120    avg_loss:0.790, val_acc:0.823]
Epoch [6/120    avg_loss:0.683, val_acc:0.885]
Epoch [7/120    avg_loss:0.540, val_acc:0.898]
Epoch [8/120    avg_loss:0.584, val_acc:0.825]
Epoch [9/120    avg_loss:0.404, val_acc:0.938]
Epoch [10/120    avg_loss:0.417, val_acc:0.885]
Epoch [11/120    avg_loss:0.343, val_acc:0.944]
Epoch [12/120    avg_loss:0.296, val_acc:0.927]
Epoch [13/120    avg_loss:0.317, val_acc:0.940]
Epoch [14/120    avg_loss:0.333, val_acc:0.950]
Epoch [15/120    avg_loss:0.472, val_acc:0.892]
Epoch [16/120    avg_loss:0.292, val_acc:0.954]
Epoch [17/120    avg_loss:0.190, val_acc:0.958]
Epoch [18/120    avg_loss:0.208, val_acc:0.960]
Epoch [19/120    avg_loss:0.195, val_acc:0.927]
Epoch [20/120    avg_loss:0.188, val_acc:0.940]
Epoch [21/120    avg_loss:0.131, val_acc:0.967]
Epoch [22/120    avg_loss:0.164, val_acc:0.975]
Epoch [23/120    avg_loss:0.124, val_acc:0.979]
Epoch [24/120    avg_loss:0.134, val_acc:0.977]
Epoch [25/120    avg_loss:0.149, val_acc:0.971]
Epoch [26/120    avg_loss:0.094, val_acc:0.977]
Epoch [27/120    avg_loss:0.087, val_acc:0.979]
Epoch [28/120    avg_loss:0.076, val_acc:0.971]
Epoch [29/120    avg_loss:0.089, val_acc:0.975]
Epoch [30/120    avg_loss:0.122, val_acc:0.973]
Epoch [31/120    avg_loss:0.103, val_acc:0.983]
Epoch [32/120    avg_loss:0.085, val_acc:0.988]
Epoch [33/120    avg_loss:0.135, val_acc:0.971]
Epoch [34/120    avg_loss:0.085, val_acc:0.977]
Epoch [35/120    avg_loss:0.110, val_acc:0.988]
Epoch [36/120    avg_loss:0.093, val_acc:0.973]
Epoch [37/120    avg_loss:0.161, val_acc:0.985]
Epoch [38/120    avg_loss:0.104, val_acc:0.973]
Epoch [39/120    avg_loss:0.077, val_acc:0.977]
Epoch [40/120    avg_loss:0.056, val_acc:0.990]
Epoch [41/120    avg_loss:0.041, val_acc:0.979]
Epoch [42/120    avg_loss:0.032, val_acc:0.985]
Epoch [43/120    avg_loss:0.031, val_acc:0.996]
Epoch [44/120    avg_loss:0.037, val_acc:0.988]
Epoch [45/120    avg_loss:0.034, val_acc:0.990]
Epoch [46/120    avg_loss:0.033, val_acc:0.992]
Epoch [47/120    avg_loss:0.036, val_acc:0.954]
Epoch [48/120    avg_loss:0.063, val_acc:0.981]
Epoch [49/120    avg_loss:0.035, val_acc:0.985]
Epoch [50/120    avg_loss:0.028, val_acc:0.985]
Epoch [51/120    avg_loss:0.030, val_acc:0.994]
Epoch [52/120    avg_loss:0.015, val_acc:0.988]
Epoch [53/120    avg_loss:0.023, val_acc:0.990]
Epoch [54/120    avg_loss:0.030, val_acc:0.992]
Epoch [55/120    avg_loss:0.025, val_acc:0.985]
Epoch [56/120    avg_loss:0.027, val_acc:0.992]
Epoch [57/120    avg_loss:0.013, val_acc:0.992]
Epoch [58/120    avg_loss:0.022, val_acc:0.992]
Epoch [59/120    avg_loss:0.015, val_acc:0.992]
Epoch [60/120    avg_loss:0.015, val_acc:0.992]
Epoch [61/120    avg_loss:0.019, val_acc:0.992]
Epoch [62/120    avg_loss:0.014, val_acc:0.992]
Epoch [63/120    avg_loss:0.024, val_acc:0.992]
Epoch [64/120    avg_loss:0.011, val_acc:0.992]
Epoch [65/120    avg_loss:0.018, val_acc:0.992]
Epoch [66/120    avg_loss:0.009, val_acc:0.992]
Epoch [67/120    avg_loss:0.010, val_acc:0.992]
Epoch [68/120    avg_loss:0.018, val_acc:0.992]
Epoch [69/120    avg_loss:0.017, val_acc:0.992]
Epoch [70/120    avg_loss:0.011, val_acc:0.992]
Epoch [71/120    avg_loss:0.013, val_acc:0.992]
Epoch [72/120    avg_loss:0.013, val_acc:0.992]
Epoch [73/120    avg_loss:0.012, val_acc:0.992]
Epoch [74/120    avg_loss:0.013, val_acc:0.992]
Epoch [75/120    avg_loss:0.016, val_acc:0.992]
Epoch [76/120    avg_loss:0.015, val_acc:0.992]
Epoch [77/120    avg_loss:0.011, val_acc:0.992]
Epoch [78/120    avg_loss:0.015, val_acc:0.992]
Epoch [79/120    avg_loss:0.008, val_acc:0.992]
Epoch [80/120    avg_loss:0.013, val_acc:0.992]
Epoch [81/120    avg_loss:0.012, val_acc:0.992]
Epoch [82/120    avg_loss:0.010, val_acc:0.992]
Epoch [83/120    avg_loss:0.010, val_acc:0.992]
Epoch [84/120    avg_loss:0.012, val_acc:0.992]
Epoch [85/120    avg_loss:0.013, val_acc:0.992]
Epoch [86/120    avg_loss:0.009, val_acc:0.992]
Epoch [87/120    avg_loss:0.012, val_acc:0.992]
Epoch [88/120    avg_loss:0.012, val_acc:0.992]
Epoch [89/120    avg_loss:0.011, val_acc:0.992]
Epoch [90/120    avg_loss:0.015, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.992]
Epoch [92/120    avg_loss:0.012, val_acc:0.992]
Epoch [93/120    avg_loss:0.026, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.011, val_acc:0.992]
Epoch [97/120    avg_loss:0.013, val_acc:0.992]
Epoch [98/120    avg_loss:0.011, val_acc:0.992]
Epoch [99/120    avg_loss:0.014, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.017, val_acc:0.992]
Epoch [102/120    avg_loss:0.013, val_acc:0.992]
Epoch [103/120    avg_loss:0.013, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.992]
Epoch [105/120    avg_loss:0.021, val_acc:0.992]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.013, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.017, val_acc:0.992]
Epoch [110/120    avg_loss:0.020, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.011, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.017, val_acc:0.992]
Epoch [117/120    avg_loss:0.015, val_acc:0.992]
Epoch [118/120    avg_loss:0.012, val_acc:0.992]
Epoch [119/120    avg_loss:0.007, val_acc:0.992]
Epoch [120/120    avg_loss:0.012, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   7   0   0   0   0   0   0   2   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.76545842217485

F1 scores:
[       nan 1.         1.         1.         0.9753915  0.96949153
 1.         1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9973887984464186
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a4d270dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.176, val_acc:0.567]
Epoch [2/120    avg_loss:1.452, val_acc:0.667]
Epoch [3/120    avg_loss:1.108, val_acc:0.740]
Epoch [4/120    avg_loss:0.890, val_acc:0.748]
Epoch [5/120    avg_loss:0.695, val_acc:0.850]
Epoch [6/120    avg_loss:0.594, val_acc:0.854]
Epoch [7/120    avg_loss:0.516, val_acc:0.852]
Epoch [8/120    avg_loss:0.451, val_acc:0.887]
Epoch [9/120    avg_loss:0.500, val_acc:0.873]
Epoch [10/120    avg_loss:0.465, val_acc:0.875]
Epoch [11/120    avg_loss:0.334, val_acc:0.910]
Epoch [12/120    avg_loss:0.280, val_acc:0.906]
Epoch [13/120    avg_loss:0.262, val_acc:0.898]
Epoch [14/120    avg_loss:0.306, val_acc:0.927]
Epoch [15/120    avg_loss:0.327, val_acc:0.915]
Epoch [16/120    avg_loss:0.273, val_acc:0.942]
Epoch [17/120    avg_loss:0.215, val_acc:0.915]
Epoch [18/120    avg_loss:0.213, val_acc:0.948]
Epoch [19/120    avg_loss:0.238, val_acc:0.948]
Epoch [20/120    avg_loss:0.184, val_acc:0.933]
Epoch [21/120    avg_loss:0.202, val_acc:0.923]
Epoch [22/120    avg_loss:0.226, val_acc:0.956]
Epoch [23/120    avg_loss:0.166, val_acc:0.885]
Epoch [24/120    avg_loss:0.189, val_acc:0.935]
Epoch [25/120    avg_loss:0.128, val_acc:0.954]
Epoch [26/120    avg_loss:0.087, val_acc:0.963]
Epoch [27/120    avg_loss:0.095, val_acc:0.954]
Epoch [28/120    avg_loss:0.071, val_acc:0.965]
Epoch [29/120    avg_loss:0.085, val_acc:0.942]
Epoch [30/120    avg_loss:0.156, val_acc:0.960]
Epoch [31/120    avg_loss:0.103, val_acc:0.960]
Epoch [32/120    avg_loss:0.073, val_acc:0.971]
Epoch [33/120    avg_loss:0.078, val_acc:0.975]
Epoch [34/120    avg_loss:0.052, val_acc:0.960]
Epoch [35/120    avg_loss:0.088, val_acc:0.963]
Epoch [36/120    avg_loss:0.085, val_acc:0.977]
Epoch [37/120    avg_loss:0.073, val_acc:0.969]
Epoch [38/120    avg_loss:0.125, val_acc:0.948]
Epoch [39/120    avg_loss:0.089, val_acc:0.973]
Epoch [40/120    avg_loss:0.083, val_acc:0.969]
Epoch [41/120    avg_loss:0.115, val_acc:0.979]
Epoch [42/120    avg_loss:0.079, val_acc:0.960]
Epoch [43/120    avg_loss:0.047, val_acc:0.977]
Epoch [44/120    avg_loss:0.047, val_acc:0.971]
Epoch [45/120    avg_loss:0.043, val_acc:0.956]
Epoch [46/120    avg_loss:0.056, val_acc:0.975]
Epoch [47/120    avg_loss:0.037, val_acc:0.971]
Epoch [48/120    avg_loss:0.031, val_acc:0.973]
Epoch [49/120    avg_loss:0.060, val_acc:0.983]
Epoch [50/120    avg_loss:0.036, val_acc:0.969]
Epoch [51/120    avg_loss:0.029, val_acc:0.981]
Epoch [52/120    avg_loss:0.030, val_acc:0.988]
Epoch [53/120    avg_loss:0.036, val_acc:0.965]
Epoch [54/120    avg_loss:0.030, val_acc:0.981]
Epoch [55/120    avg_loss:0.054, val_acc:0.958]
Epoch [56/120    avg_loss:0.029, val_acc:0.977]
Epoch [57/120    avg_loss:0.028, val_acc:0.975]
Epoch [58/120    avg_loss:0.029, val_acc:0.971]
Epoch [59/120    avg_loss:0.024, val_acc:0.983]
Epoch [60/120    avg_loss:0.019, val_acc:0.969]
Epoch [61/120    avg_loss:0.015, val_acc:0.990]
Epoch [62/120    avg_loss:0.016, val_acc:0.990]
Epoch [63/120    avg_loss:0.028, val_acc:0.977]
Epoch [64/120    avg_loss:0.064, val_acc:0.973]
Epoch [65/120    avg_loss:0.089, val_acc:0.942]
Epoch [66/120    avg_loss:0.039, val_acc:0.979]
Epoch [67/120    avg_loss:0.024, val_acc:0.981]
Epoch [68/120    avg_loss:0.018, val_acc:0.988]
Epoch [69/120    avg_loss:0.020, val_acc:0.979]
Epoch [70/120    avg_loss:0.023, val_acc:0.983]
Epoch [71/120    avg_loss:0.035, val_acc:0.979]
Epoch [72/120    avg_loss:0.029, val_acc:0.981]
Epoch [73/120    avg_loss:0.038, val_acc:0.977]
Epoch [74/120    avg_loss:0.034, val_acc:0.981]
Epoch [75/120    avg_loss:0.026, val_acc:0.988]
Epoch [76/120    avg_loss:0.032, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.983]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.983]
Epoch [86/120    avg_loss:0.007, val_acc:0.983]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.983]
Epoch [90/120    avg_loss:0.012, val_acc:0.983]
Epoch [91/120    avg_loss:0.021, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.014, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.015, val_acc:0.985]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.014, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.013, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.014, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   5   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.76545842217485

F1 scores:
[       nan 1.         0.9977221  1.         0.97977528 0.98305085
 1.         0.99465241 1.         1.         1.         0.9986755
 0.99449945 1.        ]

Kappa:
0.9973887322302569
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10632007b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.230, val_acc:0.494]
Epoch [2/120    avg_loss:1.473, val_acc:0.627]
Epoch [3/120    avg_loss:1.099, val_acc:0.727]
Epoch [4/120    avg_loss:0.896, val_acc:0.744]
Epoch [5/120    avg_loss:0.754, val_acc:0.821]
Epoch [6/120    avg_loss:0.624, val_acc:0.821]
Epoch [7/120    avg_loss:0.591, val_acc:0.850]
Epoch [8/120    avg_loss:0.564, val_acc:0.844]
Epoch [9/120    avg_loss:0.545, val_acc:0.890]
Epoch [10/120    avg_loss:0.369, val_acc:0.917]
Epoch [11/120    avg_loss:0.355, val_acc:0.921]
Epoch [12/120    avg_loss:0.311, val_acc:0.854]
Epoch [13/120    avg_loss:0.404, val_acc:0.921]
Epoch [14/120    avg_loss:0.348, val_acc:0.912]
Epoch [15/120    avg_loss:0.319, val_acc:0.921]
Epoch [16/120    avg_loss:0.338, val_acc:0.942]
Epoch [17/120    avg_loss:0.305, val_acc:0.921]
Epoch [18/120    avg_loss:0.205, val_acc:0.948]
Epoch [19/120    avg_loss:0.242, val_acc:0.950]
Epoch [20/120    avg_loss:0.227, val_acc:0.948]
Epoch [21/120    avg_loss:0.170, val_acc:0.977]
Epoch [22/120    avg_loss:0.165, val_acc:0.963]
Epoch [23/120    avg_loss:0.180, val_acc:0.963]
Epoch [24/120    avg_loss:0.131, val_acc:0.910]
Epoch [25/120    avg_loss:0.116, val_acc:0.975]
Epoch [26/120    avg_loss:0.085, val_acc:0.971]
Epoch [27/120    avg_loss:0.082, val_acc:0.975]
Epoch [28/120    avg_loss:0.127, val_acc:0.954]
Epoch [29/120    avg_loss:0.186, val_acc:0.950]
Epoch [30/120    avg_loss:0.174, val_acc:0.933]
Epoch [31/120    avg_loss:0.110, val_acc:0.965]
Epoch [32/120    avg_loss:0.121, val_acc:0.979]
Epoch [33/120    avg_loss:0.102, val_acc:0.971]
Epoch [34/120    avg_loss:0.107, val_acc:0.973]
Epoch [35/120    avg_loss:0.052, val_acc:0.979]
Epoch [36/120    avg_loss:0.064, val_acc:0.963]
Epoch [37/120    avg_loss:0.076, val_acc:0.956]
Epoch [38/120    avg_loss:0.079, val_acc:0.973]
Epoch [39/120    avg_loss:0.101, val_acc:0.975]
Epoch [40/120    avg_loss:0.053, val_acc:0.985]
Epoch [41/120    avg_loss:0.074, val_acc:0.981]
Epoch [42/120    avg_loss:0.051, val_acc:0.983]
Epoch [43/120    avg_loss:0.055, val_acc:0.983]
Epoch [44/120    avg_loss:0.044, val_acc:0.983]
Epoch [45/120    avg_loss:0.106, val_acc:0.965]
Epoch [46/120    avg_loss:0.095, val_acc:0.983]
Epoch [47/120    avg_loss:0.085, val_acc:0.981]
Epoch [48/120    avg_loss:0.067, val_acc:0.981]
Epoch [49/120    avg_loss:0.059, val_acc:0.983]
Epoch [50/120    avg_loss:0.033, val_acc:0.973]
Epoch [51/120    avg_loss:0.047, val_acc:0.981]
Epoch [52/120    avg_loss:0.042, val_acc:0.979]
Epoch [53/120    avg_loss:0.023, val_acc:0.981]
Epoch [54/120    avg_loss:0.016, val_acc:0.981]
Epoch [55/120    avg_loss:0.027, val_acc:0.981]
Epoch [56/120    avg_loss:0.022, val_acc:0.981]
Epoch [57/120    avg_loss:0.017, val_acc:0.983]
Epoch [58/120    avg_loss:0.020, val_acc:0.983]
Epoch [59/120    avg_loss:0.024, val_acc:0.983]
Epoch [60/120    avg_loss:0.022, val_acc:0.981]
Epoch [61/120    avg_loss:0.017, val_acc:0.981]
Epoch [62/120    avg_loss:0.017, val_acc:0.981]
Epoch [63/120    avg_loss:0.026, val_acc:0.983]
Epoch [64/120    avg_loss:0.017, val_acc:0.983]
Epoch [65/120    avg_loss:0.024, val_acc:0.985]
Epoch [66/120    avg_loss:0.017, val_acc:0.985]
Epoch [67/120    avg_loss:0.020, val_acc:0.985]
Epoch [68/120    avg_loss:0.023, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.017, val_acc:0.985]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.035, val_acc:0.985]
Epoch [73/120    avg_loss:0.016, val_acc:0.985]
Epoch [74/120    avg_loss:0.018, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.988]
Epoch [76/120    avg_loss:0.014, val_acc:0.988]
Epoch [77/120    avg_loss:0.013, val_acc:0.988]
Epoch [78/120    avg_loss:0.019, val_acc:0.988]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.988]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.019, val_acc:0.988]
Epoch [83/120    avg_loss:0.014, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.017, val_acc:0.985]
Epoch [86/120    avg_loss:0.024, val_acc:0.985]
Epoch [87/120    avg_loss:0.015, val_acc:0.988]
Epoch [88/120    avg_loss:0.019, val_acc:0.988]
Epoch [89/120    avg_loss:0.012, val_acc:0.988]
Epoch [90/120    avg_loss:0.014, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.016, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.013, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.015, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.988]
Epoch [99/120    avg_loss:0.020, val_acc:0.985]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.018, val_acc:0.983]
Epoch [102/120    avg_loss:0.014, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.015, val_acc:0.988]
Epoch [105/120    avg_loss:0.012, val_acc:0.988]
Epoch [106/120    avg_loss:0.015, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.014, val_acc:0.988]
Epoch [111/120    avg_loss:0.014, val_acc:0.988]
Epoch [112/120    avg_loss:0.014, val_acc:0.988]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   8   0   0   0   0   0   0   2   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   6 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 0.99926954 0.99545455 1.         0.97747748 0.96989967
 1.         0.99465241 1.         1.         1.         0.99210526
 0.9900111  1.        ]

Kappa:
0.9954900511621836
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f655fc748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.179, val_acc:0.556]
Epoch [2/120    avg_loss:1.459, val_acc:0.619]
Epoch [3/120    avg_loss:1.168, val_acc:0.692]
Epoch [4/120    avg_loss:0.883, val_acc:0.735]
Epoch [5/120    avg_loss:0.781, val_acc:0.810]
Epoch [6/120    avg_loss:0.696, val_acc:0.806]
Epoch [7/120    avg_loss:0.567, val_acc:0.835]
Epoch [8/120    avg_loss:0.543, val_acc:0.838]
Epoch [9/120    avg_loss:0.436, val_acc:0.904]
Epoch [10/120    avg_loss:0.404, val_acc:0.915]
Epoch [11/120    avg_loss:0.335, val_acc:0.883]
Epoch [12/120    avg_loss:0.327, val_acc:0.877]
Epoch [13/120    avg_loss:0.296, val_acc:0.900]
Epoch [14/120    avg_loss:0.270, val_acc:0.917]
Epoch [15/120    avg_loss:0.264, val_acc:0.942]
Epoch [16/120    avg_loss:0.317, val_acc:0.925]
Epoch [17/120    avg_loss:0.226, val_acc:0.952]
Epoch [18/120    avg_loss:0.141, val_acc:0.952]
Epoch [19/120    avg_loss:0.210, val_acc:0.940]
Epoch [20/120    avg_loss:0.157, val_acc:0.946]
Epoch [21/120    avg_loss:0.144, val_acc:0.935]
Epoch [22/120    avg_loss:0.163, val_acc:0.950]
Epoch [23/120    avg_loss:0.104, val_acc:0.963]
Epoch [24/120    avg_loss:0.102, val_acc:0.971]
Epoch [25/120    avg_loss:0.110, val_acc:0.948]
Epoch [26/120    avg_loss:0.130, val_acc:0.958]
Epoch [27/120    avg_loss:0.093, val_acc:0.954]
Epoch [28/120    avg_loss:0.093, val_acc:0.940]
Epoch [29/120    avg_loss:0.134, val_acc:0.954]
Epoch [30/120    avg_loss:0.117, val_acc:0.948]
Epoch [31/120    avg_loss:0.175, val_acc:0.967]
Epoch [32/120    avg_loss:0.107, val_acc:0.967]
Epoch [33/120    avg_loss:0.115, val_acc:0.967]
Epoch [34/120    avg_loss:0.098, val_acc:0.969]
Epoch [35/120    avg_loss:0.117, val_acc:0.971]
Epoch [36/120    avg_loss:0.106, val_acc:0.981]
Epoch [37/120    avg_loss:0.074, val_acc:0.975]
Epoch [38/120    avg_loss:0.051, val_acc:0.958]
Epoch [39/120    avg_loss:0.041, val_acc:0.977]
Epoch [40/120    avg_loss:0.034, val_acc:0.983]
Epoch [41/120    avg_loss:0.029, val_acc:0.983]
Epoch [42/120    avg_loss:0.061, val_acc:0.990]
Epoch [43/120    avg_loss:0.056, val_acc:0.985]
Epoch [44/120    avg_loss:0.036, val_acc:0.985]
Epoch [45/120    avg_loss:0.030, val_acc:0.981]
Epoch [46/120    avg_loss:0.016, val_acc:0.988]
Epoch [47/120    avg_loss:0.037, val_acc:0.981]
Epoch [48/120    avg_loss:0.030, val_acc:0.985]
Epoch [49/120    avg_loss:0.034, val_acc:0.983]
Epoch [50/120    avg_loss:0.074, val_acc:0.960]
Epoch [51/120    avg_loss:0.041, val_acc:0.973]
Epoch [52/120    avg_loss:0.053, val_acc:0.981]
Epoch [53/120    avg_loss:0.034, val_acc:0.983]
Epoch [54/120    avg_loss:0.023, val_acc:0.988]
Epoch [55/120    avg_loss:0.022, val_acc:0.990]
Epoch [56/120    avg_loss:0.020, val_acc:0.985]
Epoch [57/120    avg_loss:0.015, val_acc:0.981]
Epoch [58/120    avg_loss:0.029, val_acc:0.981]
Epoch [59/120    avg_loss:0.029, val_acc:0.983]
Epoch [60/120    avg_loss:0.016, val_acc:0.996]
Epoch [61/120    avg_loss:0.015, val_acc:0.994]
Epoch [62/120    avg_loss:0.014, val_acc:0.990]
Epoch [63/120    avg_loss:0.012, val_acc:0.983]
Epoch [64/120    avg_loss:0.012, val_acc:0.988]
Epoch [65/120    avg_loss:0.013, val_acc:0.988]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.012, val_acc:0.988]
Epoch [68/120    avg_loss:0.026, val_acc:0.988]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.041, val_acc:0.969]
Epoch [71/120    avg_loss:0.091, val_acc:0.985]
Epoch [72/120    avg_loss:0.146, val_acc:0.963]
Epoch [73/120    avg_loss:0.072, val_acc:0.979]
Epoch [74/120    avg_loss:0.088, val_acc:0.979]
Epoch [75/120    avg_loss:0.036, val_acc:0.981]
Epoch [76/120    avg_loss:0.023, val_acc:0.983]
Epoch [77/120    avg_loss:0.016, val_acc:0.983]
Epoch [78/120    avg_loss:0.020, val_acc:0.988]
Epoch [79/120    avg_loss:0.020, val_acc:0.990]
Epoch [80/120    avg_loss:0.017, val_acc:0.992]
Epoch [81/120    avg_loss:0.014, val_acc:0.990]
Epoch [82/120    avg_loss:0.021, val_acc:0.990]
Epoch [83/120    avg_loss:0.013, val_acc:0.992]
Epoch [84/120    avg_loss:0.019, val_acc:0.994]
Epoch [85/120    avg_loss:0.011, val_acc:0.994]
Epoch [86/120    avg_loss:0.023, val_acc:0.994]
Epoch [87/120    avg_loss:0.024, val_acc:0.994]
Epoch [88/120    avg_loss:0.016, val_acc:0.994]
Epoch [89/120    avg_loss:0.021, val_acc:0.994]
Epoch [90/120    avg_loss:0.016, val_acc:0.994]
Epoch [91/120    avg_loss:0.013, val_acc:0.994]
Epoch [92/120    avg_loss:0.012, val_acc:0.994]
Epoch [93/120    avg_loss:0.025, val_acc:0.994]
Epoch [94/120    avg_loss:0.014, val_acc:0.994]
Epoch [95/120    avg_loss:0.017, val_acc:0.994]
Epoch [96/120    avg_loss:0.018, val_acc:0.994]
Epoch [97/120    avg_loss:0.018, val_acc:0.994]
Epoch [98/120    avg_loss:0.020, val_acc:0.994]
Epoch [99/120    avg_loss:0.013, val_acc:0.994]
Epoch [100/120    avg_loss:0.014, val_acc:0.994]
Epoch [101/120    avg_loss:0.016, val_acc:0.996]
Epoch [102/120    avg_loss:0.013, val_acc:0.996]
Epoch [103/120    avg_loss:0.020, val_acc:0.996]
Epoch [104/120    avg_loss:0.013, val_acc:0.996]
Epoch [105/120    avg_loss:0.017, val_acc:0.996]
Epoch [106/120    avg_loss:0.013, val_acc:0.996]
Epoch [107/120    avg_loss:0.024, val_acc:0.996]
Epoch [108/120    avg_loss:0.015, val_acc:0.996]
Epoch [109/120    avg_loss:0.022, val_acc:0.996]
Epoch [110/120    avg_loss:0.016, val_acc:0.994]
Epoch [111/120    avg_loss:0.015, val_acc:0.996]
Epoch [112/120    avg_loss:0.017, val_acc:0.996]
Epoch [113/120    avg_loss:0.017, val_acc:0.996]
Epoch [114/120    avg_loss:0.019, val_acc:0.996]
Epoch [115/120    avg_loss:0.020, val_acc:0.996]
Epoch [116/120    avg_loss:0.014, val_acc:0.996]
Epoch [117/120    avg_loss:0.016, val_acc:0.996]
Epoch [118/120    avg_loss:0.022, val_acc:0.996]
Epoch [119/120    avg_loss:0.014, val_acc:0.996]
Epoch [120/120    avg_loss:0.014, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 1.         0.99545455 1.         0.97117517 0.96885813
 1.         1.         1.         1.         1.         0.99867198
 0.99229923 1.        ]

Kappa:
0.9962017825301838
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c5a380710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.227, val_acc:0.558]
Epoch [2/120    avg_loss:1.464, val_acc:0.710]
Epoch [3/120    avg_loss:0.984, val_acc:0.727]
Epoch [4/120    avg_loss:0.724, val_acc:0.794]
Epoch [5/120    avg_loss:0.668, val_acc:0.850]
Epoch [6/120    avg_loss:0.612, val_acc:0.838]
Epoch [7/120    avg_loss:0.547, val_acc:0.873]
Epoch [8/120    avg_loss:0.447, val_acc:0.867]
Epoch [9/120    avg_loss:0.394, val_acc:0.867]
Epoch [10/120    avg_loss:0.364, val_acc:0.898]
Epoch [11/120    avg_loss:0.446, val_acc:0.904]
Epoch [12/120    avg_loss:0.407, val_acc:0.908]
Epoch [13/120    avg_loss:0.277, val_acc:0.915]
Epoch [14/120    avg_loss:0.284, val_acc:0.921]
Epoch [15/120    avg_loss:0.231, val_acc:0.931]
Epoch [16/120    avg_loss:0.263, val_acc:0.927]
Epoch [17/120    avg_loss:0.270, val_acc:0.933]
Epoch [18/120    avg_loss:0.217, val_acc:0.927]
Epoch [19/120    avg_loss:0.208, val_acc:0.940]
Epoch [20/120    avg_loss:0.188, val_acc:0.944]
Epoch [21/120    avg_loss:0.175, val_acc:0.935]
Epoch [22/120    avg_loss:0.147, val_acc:0.952]
Epoch [23/120    avg_loss:0.172, val_acc:0.940]
Epoch [24/120    avg_loss:0.170, val_acc:0.954]
Epoch [25/120    avg_loss:0.204, val_acc:0.960]
Epoch [26/120    avg_loss:0.185, val_acc:0.923]
Epoch [27/120    avg_loss:0.171, val_acc:0.960]
Epoch [28/120    avg_loss:0.107, val_acc:0.967]
Epoch [29/120    avg_loss:0.106, val_acc:0.965]
Epoch [30/120    avg_loss:0.088, val_acc:0.971]
Epoch [31/120    avg_loss:0.074, val_acc:0.981]
Epoch [32/120    avg_loss:0.056, val_acc:0.983]
Epoch [33/120    avg_loss:0.068, val_acc:0.979]
Epoch [34/120    avg_loss:0.058, val_acc:0.967]
Epoch [35/120    avg_loss:0.100, val_acc:0.960]
Epoch [36/120    avg_loss:0.091, val_acc:0.971]
Epoch [37/120    avg_loss:0.088, val_acc:0.975]
Epoch [38/120    avg_loss:0.053, val_acc:0.973]
Epoch [39/120    avg_loss:0.078, val_acc:0.963]
Epoch [40/120    avg_loss:0.076, val_acc:0.981]
Epoch [41/120    avg_loss:0.054, val_acc:0.979]
Epoch [42/120    avg_loss:0.084, val_acc:0.965]
Epoch [43/120    avg_loss:0.050, val_acc:0.973]
Epoch [44/120    avg_loss:0.050, val_acc:0.985]
Epoch [45/120    avg_loss:0.035, val_acc:0.985]
Epoch [46/120    avg_loss:0.076, val_acc:0.975]
Epoch [47/120    avg_loss:0.057, val_acc:0.977]
Epoch [48/120    avg_loss:0.046, val_acc:0.975]
Epoch [49/120    avg_loss:0.096, val_acc:0.983]
Epoch [50/120    avg_loss:0.051, val_acc:0.977]
Epoch [51/120    avg_loss:0.035, val_acc:0.977]
Epoch [52/120    avg_loss:0.029, val_acc:0.981]
Epoch [53/120    avg_loss:0.052, val_acc:0.983]
Epoch [54/120    avg_loss:0.030, val_acc:0.985]
Epoch [55/120    avg_loss:0.084, val_acc:0.950]
Epoch [56/120    avg_loss:0.154, val_acc:0.958]
Epoch [57/120    avg_loss:0.073, val_acc:0.956]
Epoch [58/120    avg_loss:0.054, val_acc:0.979]
Epoch [59/120    avg_loss:0.043, val_acc:0.975]
Epoch [60/120    avg_loss:0.026, val_acc:0.983]
Epoch [61/120    avg_loss:0.036, val_acc:0.981]
Epoch [62/120    avg_loss:0.031, val_acc:0.981]
Epoch [63/120    avg_loss:0.020, val_acc:0.981]
Epoch [64/120    avg_loss:0.025, val_acc:0.985]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.013, val_acc:0.992]
Epoch [68/120    avg_loss:0.027, val_acc:0.983]
Epoch [69/120    avg_loss:0.018, val_acc:0.988]
Epoch [70/120    avg_loss:0.018, val_acc:0.988]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.040, val_acc:0.983]
Epoch [74/120    avg_loss:0.021, val_acc:0.979]
Epoch [75/120    avg_loss:0.025, val_acc:0.975]
Epoch [76/120    avg_loss:0.026, val_acc:0.990]
Epoch [77/120    avg_loss:0.020, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.990]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.023, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.010, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.992]
Epoch [88/120    avg_loss:0.011, val_acc:0.994]
Epoch [89/120    avg_loss:0.011, val_acc:0.994]
Epoch [90/120    avg_loss:0.005, val_acc:0.994]
Epoch [91/120    avg_loss:0.012, val_acc:0.994]
Epoch [92/120    avg_loss:0.008, val_acc:0.994]
Epoch [93/120    avg_loss:0.015, val_acc:0.994]
Epoch [94/120    avg_loss:0.008, val_acc:0.994]
Epoch [95/120    avg_loss:0.006, val_acc:0.994]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.009, val_acc:0.994]
Epoch [98/120    avg_loss:0.011, val_acc:0.994]
Epoch [99/120    avg_loss:0.008, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.994]
Epoch [101/120    avg_loss:0.006, val_acc:0.994]
Epoch [102/120    avg_loss:0.007, val_acc:0.994]
Epoch [103/120    avg_loss:0.011, val_acc:0.994]
Epoch [104/120    avg_loss:0.009, val_acc:0.992]
Epoch [105/120    avg_loss:0.014, val_acc:0.992]
Epoch [106/120    avg_loss:0.018, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.994]
Epoch [109/120    avg_loss:0.012, val_acc:0.994]
Epoch [110/120    avg_loss:0.013, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.011, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.014, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.70149253731343

F1 scores:
[       nan 0.99853801 1.         1.         0.98434004 0.97643098
 0.99516908 1.         1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9966769812170992
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa02f1c6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.288, val_acc:0.548]
Epoch [2/120    avg_loss:1.523, val_acc:0.681]
Epoch [3/120    avg_loss:1.157, val_acc:0.631]
Epoch [4/120    avg_loss:1.016, val_acc:0.754]
Epoch [5/120    avg_loss:0.781, val_acc:0.767]
Epoch [6/120    avg_loss:0.591, val_acc:0.885]
Epoch [7/120    avg_loss:0.465, val_acc:0.900]
Epoch [8/120    avg_loss:0.477, val_acc:0.883]
Epoch [9/120    avg_loss:0.404, val_acc:0.902]
Epoch [10/120    avg_loss:0.367, val_acc:0.925]
Epoch [11/120    avg_loss:0.282, val_acc:0.910]
Epoch [12/120    avg_loss:0.269, val_acc:0.935]
Epoch [13/120    avg_loss:0.291, val_acc:0.935]
Epoch [14/120    avg_loss:0.273, val_acc:0.948]
Epoch [15/120    avg_loss:0.306, val_acc:0.935]
Epoch [16/120    avg_loss:0.267, val_acc:0.925]
Epoch [17/120    avg_loss:0.230, val_acc:0.933]
Epoch [18/120    avg_loss:0.206, val_acc:0.938]
Epoch [19/120    avg_loss:0.291, val_acc:0.942]
Epoch [20/120    avg_loss:0.191, val_acc:0.902]
Epoch [21/120    avg_loss:0.269, val_acc:0.948]
Epoch [22/120    avg_loss:0.184, val_acc:0.956]
Epoch [23/120    avg_loss:0.164, val_acc:0.960]
Epoch [24/120    avg_loss:0.187, val_acc:0.965]
Epoch [25/120    avg_loss:0.108, val_acc:0.967]
Epoch [26/120    avg_loss:0.198, val_acc:0.944]
Epoch [27/120    avg_loss:0.115, val_acc:0.971]
Epoch [28/120    avg_loss:0.114, val_acc:0.975]
Epoch [29/120    avg_loss:0.088, val_acc:0.988]
Epoch [30/120    avg_loss:0.087, val_acc:0.979]
Epoch [31/120    avg_loss:0.175, val_acc:0.935]
Epoch [32/120    avg_loss:0.127, val_acc:0.958]
Epoch [33/120    avg_loss:0.105, val_acc:0.977]
Epoch [34/120    avg_loss:0.181, val_acc:0.956]
Epoch [35/120    avg_loss:0.101, val_acc:0.963]
Epoch [36/120    avg_loss:0.090, val_acc:0.971]
Epoch [37/120    avg_loss:0.077, val_acc:0.975]
Epoch [38/120    avg_loss:0.059, val_acc:0.985]
Epoch [39/120    avg_loss:0.058, val_acc:0.975]
Epoch [40/120    avg_loss:0.051, val_acc:0.979]
Epoch [41/120    avg_loss:0.061, val_acc:0.981]
Epoch [42/120    avg_loss:0.110, val_acc:0.965]
Epoch [43/120    avg_loss:0.062, val_acc:0.988]
Epoch [44/120    avg_loss:0.041, val_acc:0.992]
Epoch [45/120    avg_loss:0.024, val_acc:0.994]
Epoch [46/120    avg_loss:0.024, val_acc:0.992]
Epoch [47/120    avg_loss:0.033, val_acc:0.992]
Epoch [48/120    avg_loss:0.024, val_acc:0.992]
Epoch [49/120    avg_loss:0.033, val_acc:0.992]
Epoch [50/120    avg_loss:0.023, val_acc:0.992]
Epoch [51/120    avg_loss:0.032, val_acc:0.992]
Epoch [52/120    avg_loss:0.018, val_acc:0.992]
Epoch [53/120    avg_loss:0.022, val_acc:0.992]
Epoch [54/120    avg_loss:0.025, val_acc:0.992]
Epoch [55/120    avg_loss:0.030, val_acc:0.992]
Epoch [56/120    avg_loss:0.017, val_acc:0.992]
Epoch [57/120    avg_loss:0.023, val_acc:0.992]
Epoch [58/120    avg_loss:0.026, val_acc:0.992]
Epoch [59/120    avg_loss:0.017, val_acc:0.992]
Epoch [60/120    avg_loss:0.021, val_acc:0.992]
Epoch [61/120    avg_loss:0.023, val_acc:0.992]
Epoch [62/120    avg_loss:0.026, val_acc:0.992]
Epoch [63/120    avg_loss:0.021, val_acc:0.992]
Epoch [64/120    avg_loss:0.028, val_acc:0.992]
Epoch [65/120    avg_loss:0.025, val_acc:0.992]
Epoch [66/120    avg_loss:0.030, val_acc:0.992]
Epoch [67/120    avg_loss:0.044, val_acc:0.992]
Epoch [68/120    avg_loss:0.030, val_acc:0.992]
Epoch [69/120    avg_loss:0.019, val_acc:0.992]
Epoch [70/120    avg_loss:0.017, val_acc:0.992]
Epoch [71/120    avg_loss:0.025, val_acc:0.992]
Epoch [72/120    avg_loss:0.036, val_acc:0.992]
Epoch [73/120    avg_loss:0.023, val_acc:0.992]
Epoch [74/120    avg_loss:0.018, val_acc:0.992]
Epoch [75/120    avg_loss:0.023, val_acc:0.992]
Epoch [76/120    avg_loss:0.024, val_acc:0.992]
Epoch [77/120    avg_loss:0.022, val_acc:0.992]
Epoch [78/120    avg_loss:0.022, val_acc:0.992]
Epoch [79/120    avg_loss:0.017, val_acc:0.992]
Epoch [80/120    avg_loss:0.026, val_acc:0.992]
Epoch [81/120    avg_loss:0.024, val_acc:0.992]
Epoch [82/120    avg_loss:0.029, val_acc:0.992]
Epoch [83/120    avg_loss:0.022, val_acc:0.992]
Epoch [84/120    avg_loss:0.030, val_acc:0.992]
Epoch [85/120    avg_loss:0.020, val_acc:0.992]
Epoch [86/120    avg_loss:0.018, val_acc:0.992]
Epoch [87/120    avg_loss:0.027, val_acc:0.992]
Epoch [88/120    avg_loss:0.022, val_acc:0.992]
Epoch [89/120    avg_loss:0.029, val_acc:0.992]
Epoch [90/120    avg_loss:0.020, val_acc:0.992]
Epoch [91/120    avg_loss:0.022, val_acc:0.992]
Epoch [92/120    avg_loss:0.024, val_acc:0.992]
Epoch [93/120    avg_loss:0.028, val_acc:0.992]
Epoch [94/120    avg_loss:0.040, val_acc:0.992]
Epoch [95/120    avg_loss:0.017, val_acc:0.992]
Epoch [96/120    avg_loss:0.028, val_acc:0.992]
Epoch [97/120    avg_loss:0.023, val_acc:0.992]
Epoch [98/120    avg_loss:0.029, val_acc:0.992]
Epoch [99/120    avg_loss:0.022, val_acc:0.992]
Epoch [100/120    avg_loss:0.015, val_acc:0.992]
Epoch [101/120    avg_loss:0.023, val_acc:0.992]
Epoch [102/120    avg_loss:0.023, val_acc:0.992]
Epoch [103/120    avg_loss:0.023, val_acc:0.992]
Epoch [104/120    avg_loss:0.026, val_acc:0.992]
Epoch [105/120    avg_loss:0.034, val_acc:0.992]
Epoch [106/120    avg_loss:0.022, val_acc:0.992]
Epoch [107/120    avg_loss:0.023, val_acc:0.992]
Epoch [108/120    avg_loss:0.032, val_acc:0.992]
Epoch [109/120    avg_loss:0.022, val_acc:0.992]
Epoch [110/120    avg_loss:0.027, val_acc:0.992]
Epoch [111/120    avg_loss:0.018, val_acc:0.992]
Epoch [112/120    avg_loss:0.024, val_acc:0.992]
Epoch [113/120    avg_loss:0.024, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.027, val_acc:0.992]
Epoch [116/120    avg_loss:0.043, val_acc:0.992]
Epoch [117/120    avg_loss:0.019, val_acc:0.992]
Epoch [118/120    avg_loss:0.023, val_acc:0.992]
Epoch [119/120    avg_loss:0.016, val_acc:0.992]
Epoch [120/120    avg_loss:0.031, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  11   0   0   0   0   0   0   3   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   5 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99780541 0.98871332 1.         0.96598639 0.96
 0.99277108 0.97826087 1.         1.         1.         0.99208443
 0.98893805 1.        ]

Kappa:
0.9931165011804586
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5efb8ce7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.184, val_acc:0.577]
Epoch [2/120    avg_loss:1.390, val_acc:0.652]
Epoch [3/120    avg_loss:1.040, val_acc:0.771]
Epoch [4/120    avg_loss:0.789, val_acc:0.777]
Epoch [5/120    avg_loss:0.627, val_acc:0.858]
Epoch [6/120    avg_loss:0.555, val_acc:0.890]
Epoch [7/120    avg_loss:0.493, val_acc:0.894]
Epoch [8/120    avg_loss:0.428, val_acc:0.904]
Epoch [9/120    avg_loss:0.511, val_acc:0.879]
Epoch [10/120    avg_loss:0.393, val_acc:0.917]
Epoch [11/120    avg_loss:0.425, val_acc:0.921]
Epoch [12/120    avg_loss:0.343, val_acc:0.906]
Epoch [13/120    avg_loss:0.289, val_acc:0.940]
Epoch [14/120    avg_loss:0.259, val_acc:0.946]
Epoch [15/120    avg_loss:0.190, val_acc:0.942]
Epoch [16/120    avg_loss:0.204, val_acc:0.950]
Epoch [17/120    avg_loss:0.278, val_acc:0.921]
Epoch [18/120    avg_loss:0.218, val_acc:0.960]
Epoch [19/120    avg_loss:0.141, val_acc:0.963]
Epoch [20/120    avg_loss:0.158, val_acc:0.965]
Epoch [21/120    avg_loss:0.176, val_acc:0.977]
Epoch [22/120    avg_loss:0.148, val_acc:0.977]
Epoch [23/120    avg_loss:0.151, val_acc:0.933]
Epoch [24/120    avg_loss:0.163, val_acc:0.983]
Epoch [25/120    avg_loss:0.137, val_acc:0.979]
Epoch [26/120    avg_loss:0.134, val_acc:0.958]
Epoch [27/120    avg_loss:0.205, val_acc:0.983]
Epoch [28/120    avg_loss:0.134, val_acc:0.985]
Epoch [29/120    avg_loss:0.129, val_acc:0.958]
Epoch [30/120    avg_loss:0.112, val_acc:0.975]
Epoch [31/120    avg_loss:0.063, val_acc:0.979]
Epoch [32/120    avg_loss:0.095, val_acc:0.979]
Epoch [33/120    avg_loss:0.077, val_acc:0.979]
Epoch [34/120    avg_loss:0.053, val_acc:0.992]
Epoch [35/120    avg_loss:0.043, val_acc:0.985]
Epoch [36/120    avg_loss:0.096, val_acc:0.994]
Epoch [37/120    avg_loss:0.071, val_acc:0.973]
Epoch [38/120    avg_loss:0.057, val_acc:0.988]
Epoch [39/120    avg_loss:0.035, val_acc:0.988]
Epoch [40/120    avg_loss:0.050, val_acc:0.983]
Epoch [41/120    avg_loss:0.044, val_acc:0.981]
Epoch [42/120    avg_loss:0.056, val_acc:0.992]
Epoch [43/120    avg_loss:0.045, val_acc:0.985]
Epoch [44/120    avg_loss:0.066, val_acc:0.958]
Epoch [45/120    avg_loss:0.101, val_acc:0.983]
Epoch [46/120    avg_loss:0.063, val_acc:0.990]
Epoch [47/120    avg_loss:0.043, val_acc:0.990]
Epoch [48/120    avg_loss:0.031, val_acc:0.994]
Epoch [49/120    avg_loss:0.031, val_acc:0.983]
Epoch [50/120    avg_loss:0.022, val_acc:0.988]
Epoch [51/120    avg_loss:0.025, val_acc:0.992]
Epoch [52/120    avg_loss:0.035, val_acc:0.985]
Epoch [53/120    avg_loss:0.023, val_acc:0.988]
Epoch [54/120    avg_loss:0.020, val_acc:0.985]
Epoch [55/120    avg_loss:0.021, val_acc:0.988]
Epoch [56/120    avg_loss:0.029, val_acc:0.990]
Epoch [57/120    avg_loss:0.027, val_acc:0.988]
Epoch [58/120    avg_loss:0.035, val_acc:0.988]
Epoch [59/120    avg_loss:0.057, val_acc:0.983]
Epoch [60/120    avg_loss:0.035, val_acc:0.988]
Epoch [61/120    avg_loss:0.023, val_acc:0.994]
Epoch [62/120    avg_loss:0.025, val_acc:0.996]
Epoch [63/120    avg_loss:0.031, val_acc:0.990]
Epoch [64/120    avg_loss:0.028, val_acc:0.990]
Epoch [65/120    avg_loss:0.021, val_acc:0.994]
Epoch [66/120    avg_loss:0.034, val_acc:0.979]
Epoch [67/120    avg_loss:0.018, val_acc:0.985]
Epoch [68/120    avg_loss:0.019, val_acc:0.992]
Epoch [69/120    avg_loss:0.043, val_acc:0.985]
Epoch [70/120    avg_loss:0.031, val_acc:0.969]
Epoch [71/120    avg_loss:0.116, val_acc:0.929]
Epoch [72/120    avg_loss:0.140, val_acc:0.973]
Epoch [73/120    avg_loss:0.095, val_acc:0.981]
Epoch [74/120    avg_loss:0.047, val_acc:0.988]
Epoch [75/120    avg_loss:0.032, val_acc:0.985]
Epoch [76/120    avg_loss:0.031, val_acc:0.988]
Epoch [77/120    avg_loss:0.026, val_acc:0.988]
Epoch [78/120    avg_loss:0.019, val_acc:0.990]
Epoch [79/120    avg_loss:0.020, val_acc:0.990]
Epoch [80/120    avg_loss:0.023, val_acc:0.990]
Epoch [81/120    avg_loss:0.015, val_acc:0.992]
Epoch [82/120    avg_loss:0.024, val_acc:0.992]
Epoch [83/120    avg_loss:0.024, val_acc:0.990]
Epoch [84/120    avg_loss:0.015, val_acc:0.990]
Epoch [85/120    avg_loss:0.014, val_acc:0.990]
Epoch [86/120    avg_loss:0.024, val_acc:0.992]
Epoch [87/120    avg_loss:0.013, val_acc:0.994]
Epoch [88/120    avg_loss:0.011, val_acc:0.994]
Epoch [89/120    avg_loss:0.020, val_acc:0.992]
Epoch [90/120    avg_loss:0.015, val_acc:0.992]
Epoch [91/120    avg_loss:0.012, val_acc:0.992]
Epoch [92/120    avg_loss:0.018, val_acc:0.992]
Epoch [93/120    avg_loss:0.020, val_acc:0.992]
Epoch [94/120    avg_loss:0.011, val_acc:0.992]
Epoch [95/120    avg_loss:0.019, val_acc:0.992]
Epoch [96/120    avg_loss:0.013, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.024, val_acc:0.992]
Epoch [100/120    avg_loss:0.009, val_acc:0.992]
Epoch [101/120    avg_loss:0.018, val_acc:0.992]
Epoch [102/120    avg_loss:0.011, val_acc:0.992]
Epoch [103/120    avg_loss:0.016, val_acc:0.992]
Epoch [104/120    avg_loss:0.015, val_acc:0.992]
Epoch [105/120    avg_loss:0.016, val_acc:0.992]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.026, val_acc:0.992]
Epoch [108/120    avg_loss:0.010, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.009, val_acc:0.992]
Epoch [111/120    avg_loss:0.011, val_acc:0.992]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.012, val_acc:0.992]
Epoch [114/120    avg_loss:0.013, val_acc:0.992]
Epoch [115/120    avg_loss:0.015, val_acc:0.992]
Epoch [116/120    avg_loss:0.012, val_acc:0.992]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.992]
Epoch [120/120    avg_loss:0.032, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   3   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.7228144989339

F1 scores:
[       nan 0.99780541 1.         1.         0.98206278 0.97643098
 1.         1.         1.         1.         1.         0.9973545
 0.99339207 1.        ]

Kappa:
0.9969142278843466
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdbbddad828>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.133, val_acc:0.627]
Epoch [2/120    avg_loss:1.422, val_acc:0.740]
Epoch [3/120    avg_loss:1.016, val_acc:0.756]
Epoch [4/120    avg_loss:0.801, val_acc:0.819]
Epoch [5/120    avg_loss:0.707, val_acc:0.838]
Epoch [6/120    avg_loss:0.592, val_acc:0.808]
Epoch [7/120    avg_loss:0.461, val_acc:0.904]
Epoch [8/120    avg_loss:0.540, val_acc:0.908]
Epoch [9/120    avg_loss:0.416, val_acc:0.900]
Epoch [10/120    avg_loss:0.389, val_acc:0.904]
Epoch [11/120    avg_loss:0.370, val_acc:0.846]
Epoch [12/120    avg_loss:0.310, val_acc:0.940]
Epoch [13/120    avg_loss:0.268, val_acc:0.923]
Epoch [14/120    avg_loss:0.225, val_acc:0.958]
Epoch [15/120    avg_loss:0.247, val_acc:0.942]
Epoch [16/120    avg_loss:0.229, val_acc:0.944]
Epoch [17/120    avg_loss:0.309, val_acc:0.944]
Epoch [18/120    avg_loss:0.270, val_acc:0.944]
Epoch [19/120    avg_loss:0.239, val_acc:0.967]
Epoch [20/120    avg_loss:0.197, val_acc:0.948]
Epoch [21/120    avg_loss:0.213, val_acc:0.960]
Epoch [22/120    avg_loss:0.212, val_acc:0.963]
Epoch [23/120    avg_loss:0.185, val_acc:0.948]
Epoch [24/120    avg_loss:0.157, val_acc:0.965]
Epoch [25/120    avg_loss:0.145, val_acc:0.971]
Epoch [26/120    avg_loss:0.146, val_acc:0.967]
Epoch [27/120    avg_loss:0.121, val_acc:0.969]
Epoch [28/120    avg_loss:0.125, val_acc:0.979]
Epoch [29/120    avg_loss:0.125, val_acc:0.967]
Epoch [30/120    avg_loss:0.122, val_acc:0.990]
Epoch [31/120    avg_loss:0.109, val_acc:0.971]
Epoch [32/120    avg_loss:0.103, val_acc:0.981]
Epoch [33/120    avg_loss:0.112, val_acc:0.983]
Epoch [34/120    avg_loss:0.116, val_acc:0.973]
Epoch [35/120    avg_loss:0.148, val_acc:0.971]
Epoch [36/120    avg_loss:0.092, val_acc:0.981]
Epoch [37/120    avg_loss:0.089, val_acc:0.977]
Epoch [38/120    avg_loss:0.085, val_acc:0.983]
Epoch [39/120    avg_loss:0.104, val_acc:0.990]
Epoch [40/120    avg_loss:0.134, val_acc:0.971]
Epoch [41/120    avg_loss:0.102, val_acc:0.979]
Epoch [42/120    avg_loss:0.089, val_acc:0.988]
Epoch [43/120    avg_loss:0.044, val_acc:0.988]
Epoch [44/120    avg_loss:0.056, val_acc:0.988]
Epoch [45/120    avg_loss:0.087, val_acc:0.975]
Epoch [46/120    avg_loss:0.073, val_acc:0.979]
Epoch [47/120    avg_loss:0.112, val_acc:0.983]
Epoch [48/120    avg_loss:0.041, val_acc:0.996]
Epoch [49/120    avg_loss:0.041, val_acc:0.994]
Epoch [50/120    avg_loss:0.054, val_acc:0.983]
Epoch [51/120    avg_loss:0.092, val_acc:0.985]
Epoch [52/120    avg_loss:0.124, val_acc:0.979]
Epoch [53/120    avg_loss:0.057, val_acc:0.994]
Epoch [54/120    avg_loss:0.051, val_acc:0.990]
Epoch [55/120    avg_loss:0.042, val_acc:0.985]
Epoch [56/120    avg_loss:0.034, val_acc:0.988]
Epoch [57/120    avg_loss:0.040, val_acc:0.996]
Epoch [58/120    avg_loss:0.061, val_acc:0.985]
Epoch [59/120    avg_loss:0.030, val_acc:0.981]
Epoch [60/120    avg_loss:0.042, val_acc:0.992]
Epoch [61/120    avg_loss:0.031, val_acc:0.988]
Epoch [62/120    avg_loss:0.026, val_acc:0.996]
Epoch [63/120    avg_loss:0.030, val_acc:0.996]
Epoch [64/120    avg_loss:0.017, val_acc:0.992]
Epoch [65/120    avg_loss:0.021, val_acc:0.990]
Epoch [66/120    avg_loss:0.050, val_acc:0.988]
Epoch [67/120    avg_loss:0.049, val_acc:0.994]
Epoch [68/120    avg_loss:0.024, val_acc:0.988]
Epoch [69/120    avg_loss:0.024, val_acc:0.996]
Epoch [70/120    avg_loss:0.022, val_acc:0.994]
Epoch [71/120    avg_loss:0.030, val_acc:0.996]
Epoch [72/120    avg_loss:0.023, val_acc:0.996]
Epoch [73/120    avg_loss:0.024, val_acc:0.994]
Epoch [74/120    avg_loss:0.021, val_acc:0.994]
Epoch [75/120    avg_loss:0.024, val_acc:0.998]
Epoch [76/120    avg_loss:0.021, val_acc:0.998]
Epoch [77/120    avg_loss:0.034, val_acc:0.998]
Epoch [78/120    avg_loss:0.012, val_acc:0.992]
Epoch [79/120    avg_loss:0.030, val_acc:0.996]
Epoch [80/120    avg_loss:0.022, val_acc:0.988]
Epoch [81/120    avg_loss:0.025, val_acc:0.992]
Epoch [82/120    avg_loss:0.034, val_acc:0.992]
Epoch [83/120    avg_loss:0.015, val_acc:0.994]
Epoch [84/120    avg_loss:0.027, val_acc:0.996]
Epoch [85/120    avg_loss:0.017, val_acc:0.994]
Epoch [86/120    avg_loss:0.010, val_acc:0.996]
Epoch [87/120    avg_loss:0.015, val_acc:0.996]
Epoch [88/120    avg_loss:0.016, val_acc:0.988]
Epoch [89/120    avg_loss:0.011, val_acc:0.996]
Epoch [90/120    avg_loss:0.013, val_acc:0.998]
Epoch [91/120    avg_loss:0.015, val_acc:0.998]
Epoch [92/120    avg_loss:0.008, val_acc:0.998]
Epoch [93/120    avg_loss:0.009, val_acc:1.000]
Epoch [94/120    avg_loss:0.006, val_acc:1.000]
Epoch [95/120    avg_loss:0.011, val_acc:0.998]
Epoch [96/120    avg_loss:0.014, val_acc:0.996]
Epoch [97/120    avg_loss:0.014, val_acc:0.998]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.996]
Epoch [100/120    avg_loss:0.008, val_acc:1.000]
Epoch [101/120    avg_loss:0.008, val_acc:1.000]
Epoch [102/120    avg_loss:0.007, val_acc:0.998]
Epoch [103/120    avg_loss:0.004, val_acc:0.998]
Epoch [104/120    avg_loss:0.006, val_acc:0.998]
Epoch [105/120    avg_loss:0.006, val_acc:0.998]
Epoch [106/120    avg_loss:0.005, val_acc:1.000]
Epoch [107/120    avg_loss:0.007, val_acc:1.000]
Epoch [108/120    avg_loss:0.006, val_acc:1.000]
Epoch [109/120    avg_loss:0.007, val_acc:0.998]
Epoch [110/120    avg_loss:0.008, val_acc:1.000]
Epoch [111/120    avg_loss:0.024, val_acc:1.000]
Epoch [112/120    avg_loss:0.054, val_acc:0.963]
Epoch [113/120    avg_loss:0.044, val_acc:0.979]
Epoch [114/120    avg_loss:0.070, val_acc:0.988]
Epoch [115/120    avg_loss:0.022, val_acc:0.990]
Epoch [116/120    avg_loss:0.037, val_acc:0.994]
Epoch [117/120    avg_loss:0.018, val_acc:0.992]
Epoch [118/120    avg_loss:0.032, val_acc:0.992]
Epoch [119/120    avg_loss:0.026, val_acc:0.990]
Epoch [120/120    avg_loss:0.031, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   0   0   2   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   3   0   0   0   0   0   0   4   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  18 435   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 0.99853801 1.         0.97777778 0.95238095 0.97222222
 1.         1.         0.99742931 1.         1.         0.97668394
 0.97533632 1.        ]

Kappa:
0.9900304451519105
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe556697b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.282, val_acc:0.621]
Epoch [2/120    avg_loss:1.517, val_acc:0.681]
Epoch [3/120    avg_loss:1.103, val_acc:0.725]
Epoch [4/120    avg_loss:0.820, val_acc:0.775]
Epoch [5/120    avg_loss:0.610, val_acc:0.850]
Epoch [6/120    avg_loss:0.613, val_acc:0.902]
Epoch [7/120    avg_loss:0.484, val_acc:0.927]
Epoch [8/120    avg_loss:0.551, val_acc:0.927]
Epoch [9/120    avg_loss:0.422, val_acc:0.833]
Epoch [10/120    avg_loss:0.398, val_acc:0.933]
Epoch [11/120    avg_loss:0.293, val_acc:0.933]
Epoch [12/120    avg_loss:0.373, val_acc:0.944]
Epoch [13/120    avg_loss:0.316, val_acc:0.944]
Epoch [14/120    avg_loss:0.372, val_acc:0.935]
Epoch [15/120    avg_loss:0.320, val_acc:0.940]
Epoch [16/120    avg_loss:0.324, val_acc:0.933]
Epoch [17/120    avg_loss:0.292, val_acc:0.942]
Epoch [18/120    avg_loss:0.225, val_acc:0.956]
Epoch [19/120    avg_loss:0.191, val_acc:0.971]
Epoch [20/120    avg_loss:0.169, val_acc:0.958]
Epoch [21/120    avg_loss:0.164, val_acc:0.975]
Epoch [22/120    avg_loss:0.116, val_acc:0.973]
Epoch [23/120    avg_loss:0.107, val_acc:0.971]
Epoch [24/120    avg_loss:0.093, val_acc:0.975]
Epoch [25/120    avg_loss:0.124, val_acc:0.981]
Epoch [26/120    avg_loss:0.095, val_acc:0.979]
Epoch [27/120    avg_loss:0.099, val_acc:0.977]
Epoch [28/120    avg_loss:0.102, val_acc:0.977]
Epoch [29/120    avg_loss:0.073, val_acc:0.981]
Epoch [30/120    avg_loss:0.075, val_acc:0.983]
Epoch [31/120    avg_loss:0.121, val_acc:0.973]
Epoch [32/120    avg_loss:0.116, val_acc:0.979]
Epoch [33/120    avg_loss:0.076, val_acc:0.977]
Epoch [34/120    avg_loss:0.068, val_acc:0.971]
Epoch [35/120    avg_loss:0.045, val_acc:0.983]
Epoch [36/120    avg_loss:0.064, val_acc:0.981]
Epoch [37/120    avg_loss:0.063, val_acc:0.979]
Epoch [38/120    avg_loss:0.087, val_acc:0.969]
Epoch [39/120    avg_loss:0.071, val_acc:0.973]
Epoch [40/120    avg_loss:0.069, val_acc:0.985]
Epoch [41/120    avg_loss:0.053, val_acc:0.967]
Epoch [42/120    avg_loss:0.047, val_acc:0.979]
Epoch [43/120    avg_loss:0.099, val_acc:0.958]
Epoch [44/120    avg_loss:0.168, val_acc:0.979]
Epoch [45/120    avg_loss:0.072, val_acc:0.977]
Epoch [46/120    avg_loss:0.057, val_acc:0.985]
Epoch [47/120    avg_loss:0.070, val_acc:0.971]
Epoch [48/120    avg_loss:0.061, val_acc:0.973]
Epoch [49/120    avg_loss:0.039, val_acc:0.979]
Epoch [50/120    avg_loss:0.024, val_acc:0.985]
Epoch [51/120    avg_loss:0.029, val_acc:0.988]
Epoch [52/120    avg_loss:0.027, val_acc:0.981]
Epoch [53/120    avg_loss:0.032, val_acc:0.988]
Epoch [54/120    avg_loss:0.052, val_acc:0.983]
Epoch [55/120    avg_loss:0.030, val_acc:0.979]
Epoch [56/120    avg_loss:0.039, val_acc:0.988]
Epoch [57/120    avg_loss:0.031, val_acc:0.988]
Epoch [58/120    avg_loss:0.026, val_acc:0.985]
Epoch [59/120    avg_loss:0.034, val_acc:0.994]
Epoch [60/120    avg_loss:0.046, val_acc:0.985]
Epoch [61/120    avg_loss:0.046, val_acc:0.985]
Epoch [62/120    avg_loss:0.049, val_acc:0.983]
Epoch [63/120    avg_loss:0.093, val_acc:0.985]
Epoch [64/120    avg_loss:0.074, val_acc:0.979]
Epoch [65/120    avg_loss:0.035, val_acc:0.983]
Epoch [66/120    avg_loss:0.037, val_acc:0.990]
Epoch [67/120    avg_loss:0.069, val_acc:0.988]
Epoch [68/120    avg_loss:0.070, val_acc:0.983]
Epoch [69/120    avg_loss:0.075, val_acc:0.973]
Epoch [70/120    avg_loss:0.048, val_acc:0.985]
Epoch [71/120    avg_loss:0.029, val_acc:0.988]
Epoch [72/120    avg_loss:0.019, val_acc:0.990]
Epoch [73/120    avg_loss:0.012, val_acc:0.992]
Epoch [74/120    avg_loss:0.018, val_acc:0.990]
Epoch [75/120    avg_loss:0.012, val_acc:0.990]
Epoch [76/120    avg_loss:0.010, val_acc:0.988]
Epoch [77/120    avg_loss:0.013, val_acc:0.990]
Epoch [78/120    avg_loss:0.010, val_acc:0.990]
Epoch [79/120    avg_loss:0.026, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.013, val_acc:0.990]
Epoch [82/120    avg_loss:0.015, val_acc:0.990]
Epoch [83/120    avg_loss:0.009, val_acc:0.990]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.013, val_acc:0.990]
Epoch [86/120    avg_loss:0.013, val_acc:0.990]
Epoch [87/120    avg_loss:0.011, val_acc:0.990]
Epoch [88/120    avg_loss:0.015, val_acc:0.990]
Epoch [89/120    avg_loss:0.026, val_acc:0.990]
Epoch [90/120    avg_loss:0.012, val_acc:0.990]
Epoch [91/120    avg_loss:0.019, val_acc:0.990]
Epoch [92/120    avg_loss:0.015, val_acc:0.990]
Epoch [93/120    avg_loss:0.017, val_acc:0.990]
Epoch [94/120    avg_loss:0.010, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.990]
Epoch [98/120    avg_loss:0.013, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.990]
Epoch [100/120    avg_loss:0.014, val_acc:0.990]
Epoch [101/120    avg_loss:0.018, val_acc:0.990]
Epoch [102/120    avg_loss:0.009, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.011, val_acc:0.990]
Epoch [105/120    avg_loss:0.013, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.014, val_acc:0.990]
Epoch [108/120    avg_loss:0.014, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.990]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.017, val_acc:0.990]
Epoch [114/120    avg_loss:0.012, val_acc:0.990]
Epoch [115/120    avg_loss:0.014, val_acc:0.990]
Epoch [116/120    avg_loss:0.015, val_acc:0.990]
Epoch [117/120    avg_loss:0.016, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   1   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   3   0   0   0   0   0   0   3   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.9650655  0.95070423
 0.99277108 0.99465241 1.         1.         1.         0.98950131
 0.98779134 1.        ]

Kappa:
0.993116358691917
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac84cad7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.244, val_acc:0.569]
Epoch [2/120    avg_loss:1.490, val_acc:0.648]
Epoch [3/120    avg_loss:1.079, val_acc:0.717]
Epoch [4/120    avg_loss:0.794, val_acc:0.779]
Epoch [5/120    avg_loss:0.733, val_acc:0.756]
Epoch [6/120    avg_loss:0.625, val_acc:0.804]
Epoch [7/120    avg_loss:0.587, val_acc:0.848]
Epoch [8/120    avg_loss:0.508, val_acc:0.850]
Epoch [9/120    avg_loss:0.549, val_acc:0.885]
Epoch [10/120    avg_loss:0.406, val_acc:0.915]
Epoch [11/120    avg_loss:0.351, val_acc:0.919]
Epoch [12/120    avg_loss:0.318, val_acc:0.896]
Epoch [13/120    avg_loss:0.325, val_acc:0.946]
Epoch [14/120    avg_loss:0.278, val_acc:0.927]
Epoch [15/120    avg_loss:0.227, val_acc:0.931]
Epoch [16/120    avg_loss:0.177, val_acc:0.965]
Epoch [17/120    avg_loss:0.154, val_acc:0.944]
Epoch [18/120    avg_loss:0.198, val_acc:0.958]
Epoch [19/120    avg_loss:0.197, val_acc:0.965]
Epoch [20/120    avg_loss:0.146, val_acc:0.948]
Epoch [21/120    avg_loss:0.139, val_acc:0.965]
Epoch [22/120    avg_loss:0.147, val_acc:0.956]
Epoch [23/120    avg_loss:0.099, val_acc:0.977]
Epoch [24/120    avg_loss:0.098, val_acc:0.975]
Epoch [25/120    avg_loss:0.081, val_acc:0.975]
Epoch [26/120    avg_loss:0.093, val_acc:0.971]
Epoch [27/120    avg_loss:0.105, val_acc:0.973]
Epoch [28/120    avg_loss:0.090, val_acc:0.971]
Epoch [29/120    avg_loss:0.102, val_acc:0.975]
Epoch [30/120    avg_loss:0.108, val_acc:0.967]
Epoch [31/120    avg_loss:0.084, val_acc:0.977]
Epoch [32/120    avg_loss:0.122, val_acc:0.973]
Epoch [33/120    avg_loss:0.056, val_acc:0.971]
Epoch [34/120    avg_loss:0.046, val_acc:0.979]
Epoch [35/120    avg_loss:0.051, val_acc:0.981]
Epoch [36/120    avg_loss:0.040, val_acc:0.975]
Epoch [37/120    avg_loss:0.039, val_acc:0.979]
Epoch [38/120    avg_loss:0.042, val_acc:0.985]
Epoch [39/120    avg_loss:0.035, val_acc:0.981]
Epoch [40/120    avg_loss:0.040, val_acc:0.979]
Epoch [41/120    avg_loss:0.094, val_acc:0.956]
Epoch [42/120    avg_loss:0.046, val_acc:0.956]
Epoch [43/120    avg_loss:0.060, val_acc:0.973]
Epoch [44/120    avg_loss:0.039, val_acc:0.973]
Epoch [45/120    avg_loss:0.034, val_acc:0.979]
Epoch [46/120    avg_loss:0.060, val_acc:0.973]
Epoch [47/120    avg_loss:0.051, val_acc:0.981]
Epoch [48/120    avg_loss:0.039, val_acc:0.942]
Epoch [49/120    avg_loss:0.058, val_acc:0.983]
Epoch [50/120    avg_loss:0.060, val_acc:0.967]
Epoch [51/120    avg_loss:0.049, val_acc:0.935]
Epoch [52/120    avg_loss:0.074, val_acc:0.965]
Epoch [53/120    avg_loss:0.026, val_acc:0.979]
Epoch [54/120    avg_loss:0.020, val_acc:0.981]
Epoch [55/120    avg_loss:0.020, val_acc:0.985]
Epoch [56/120    avg_loss:0.020, val_acc:0.981]
Epoch [57/120    avg_loss:0.021, val_acc:0.981]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.021, val_acc:0.983]
Epoch [60/120    avg_loss:0.020, val_acc:0.983]
Epoch [61/120    avg_loss:0.015, val_acc:0.985]
Epoch [62/120    avg_loss:0.016, val_acc:0.985]
Epoch [63/120    avg_loss:0.027, val_acc:0.983]
Epoch [64/120    avg_loss:0.017, val_acc:0.985]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.985]
Epoch [68/120    avg_loss:0.019, val_acc:0.985]
Epoch [69/120    avg_loss:0.027, val_acc:0.985]
Epoch [70/120    avg_loss:0.013, val_acc:0.985]
Epoch [71/120    avg_loss:0.016, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.012, val_acc:0.985]
Epoch [74/120    avg_loss:0.029, val_acc:0.985]
Epoch [75/120    avg_loss:0.016, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.985]
Epoch [77/120    avg_loss:0.015, val_acc:0.985]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.025, val_acc:0.985]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.017, val_acc:0.988]
Epoch [83/120    avg_loss:0.020, val_acc:0.988]
Epoch [84/120    avg_loss:0.023, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.983]
Epoch [87/120    avg_loss:0.014, val_acc:0.983]
Epoch [88/120    avg_loss:0.018, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.983]
Epoch [92/120    avg_loss:0.019, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.017, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.019, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.017, val_acc:0.985]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.012, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.011, val_acc:0.990]
Epoch [112/120    avg_loss:0.014, val_acc:0.990]
Epoch [113/120    avg_loss:0.008, val_acc:0.990]
Epoch [114/120    avg_loss:0.013, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.015, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.988]
Epoch [120/120    avg_loss:0.011, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   4   0   0   0   0   0   0   2   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.85074626865672

F1 scores:
[       nan 1.         1.         0.99782135 0.9844098  0.98639456
 1.         1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9983383196397814
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f815edd8c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.278, val_acc:0.665]
Epoch [2/120    avg_loss:1.543, val_acc:0.688]
Epoch [3/120    avg_loss:1.094, val_acc:0.727]
Epoch [4/120    avg_loss:0.838, val_acc:0.771]
Epoch [5/120    avg_loss:0.711, val_acc:0.842]
Epoch [6/120    avg_loss:0.552, val_acc:0.904]
Epoch [7/120    avg_loss:0.518, val_acc:0.883]
Epoch [8/120    avg_loss:0.527, val_acc:0.879]
Epoch [9/120    avg_loss:0.408, val_acc:0.915]
Epoch [10/120    avg_loss:0.376, val_acc:0.900]
Epoch [11/120    avg_loss:0.379, val_acc:0.931]
Epoch [12/120    avg_loss:0.289, val_acc:0.952]
Epoch [13/120    avg_loss:0.228, val_acc:0.956]
Epoch [14/120    avg_loss:0.255, val_acc:0.958]
Epoch [15/120    avg_loss:0.236, val_acc:0.940]
Epoch [16/120    avg_loss:0.250, val_acc:0.938]
Epoch [17/120    avg_loss:0.176, val_acc:0.954]
Epoch [18/120    avg_loss:0.181, val_acc:0.977]
Epoch [19/120    avg_loss:0.191, val_acc:0.942]
Epoch [20/120    avg_loss:0.286, val_acc:0.946]
Epoch [21/120    avg_loss:0.242, val_acc:0.967]
Epoch [22/120    avg_loss:0.191, val_acc:0.956]
Epoch [23/120    avg_loss:0.131, val_acc:0.973]
Epoch [24/120    avg_loss:0.108, val_acc:0.971]
Epoch [25/120    avg_loss:0.096, val_acc:0.973]
Epoch [26/120    avg_loss:0.086, val_acc:0.979]
Epoch [27/120    avg_loss:0.078, val_acc:0.977]
Epoch [28/120    avg_loss:0.090, val_acc:0.979]
Epoch [29/120    avg_loss:0.078, val_acc:0.963]
Epoch [30/120    avg_loss:0.111, val_acc:0.971]
Epoch [31/120    avg_loss:0.102, val_acc:0.958]
Epoch [32/120    avg_loss:0.153, val_acc:0.979]
Epoch [33/120    avg_loss:0.106, val_acc:0.973]
Epoch [34/120    avg_loss:0.113, val_acc:0.975]
Epoch [35/120    avg_loss:0.105, val_acc:0.971]
Epoch [36/120    avg_loss:0.087, val_acc:0.985]
Epoch [37/120    avg_loss:0.053, val_acc:0.988]
Epoch [38/120    avg_loss:0.049, val_acc:0.975]
Epoch [39/120    avg_loss:0.077, val_acc:0.971]
Epoch [40/120    avg_loss:0.091, val_acc:0.983]
Epoch [41/120    avg_loss:0.077, val_acc:0.977]
Epoch [42/120    avg_loss:0.066, val_acc:0.973]
Epoch [43/120    avg_loss:0.089, val_acc:0.983]
Epoch [44/120    avg_loss:0.095, val_acc:0.981]
Epoch [45/120    avg_loss:0.065, val_acc:0.992]
Epoch [46/120    avg_loss:0.065, val_acc:0.975]
Epoch [47/120    avg_loss:0.051, val_acc:0.981]
Epoch [48/120    avg_loss:0.031, val_acc:0.983]
Epoch [49/120    avg_loss:0.023, val_acc:0.985]
Epoch [50/120    avg_loss:0.035, val_acc:0.983]
Epoch [51/120    avg_loss:0.033, val_acc:0.990]
Epoch [52/120    avg_loss:0.062, val_acc:0.988]
Epoch [53/120    avg_loss:0.079, val_acc:0.971]
Epoch [54/120    avg_loss:0.064, val_acc:0.969]
Epoch [55/120    avg_loss:0.060, val_acc:0.990]
Epoch [56/120    avg_loss:0.046, val_acc:0.990]
Epoch [57/120    avg_loss:0.034, val_acc:0.994]
Epoch [58/120    avg_loss:0.033, val_acc:0.988]
Epoch [59/120    avg_loss:0.040, val_acc:0.981]
Epoch [60/120    avg_loss:0.047, val_acc:0.985]
Epoch [61/120    avg_loss:0.051, val_acc:0.992]
Epoch [62/120    avg_loss:0.034, val_acc:0.990]
Epoch [63/120    avg_loss:0.035, val_acc:0.994]
Epoch [64/120    avg_loss:0.018, val_acc:0.990]
Epoch [65/120    avg_loss:0.019, val_acc:0.988]
Epoch [66/120    avg_loss:0.014, val_acc:0.992]
Epoch [67/120    avg_loss:0.021, val_acc:0.992]
Epoch [68/120    avg_loss:0.015, val_acc:0.996]
Epoch [69/120    avg_loss:0.014, val_acc:0.992]
Epoch [70/120    avg_loss:0.011, val_acc:0.992]
Epoch [71/120    avg_loss:0.012, val_acc:0.994]
Epoch [72/120    avg_loss:0.010, val_acc:0.992]
Epoch [73/120    avg_loss:0.021, val_acc:0.996]
Epoch [74/120    avg_loss:0.014, val_acc:0.996]
Epoch [75/120    avg_loss:0.016, val_acc:0.994]
Epoch [76/120    avg_loss:0.012, val_acc:0.994]
Epoch [77/120    avg_loss:0.009, val_acc:0.996]
Epoch [78/120    avg_loss:0.042, val_acc:0.996]
Epoch [79/120    avg_loss:0.024, val_acc:0.988]
Epoch [80/120    avg_loss:0.031, val_acc:0.990]
Epoch [81/120    avg_loss:0.021, val_acc:0.994]
Epoch [82/120    avg_loss:0.009, val_acc:0.996]
Epoch [83/120    avg_loss:0.019, val_acc:0.996]
Epoch [84/120    avg_loss:0.013, val_acc:0.998]
Epoch [85/120    avg_loss:0.008, val_acc:0.996]
Epoch [86/120    avg_loss:0.010, val_acc:0.994]
Epoch [87/120    avg_loss:0.019, val_acc:0.992]
Epoch [88/120    avg_loss:0.011, val_acc:0.996]
Epoch [89/120    avg_loss:0.009, val_acc:0.998]
Epoch [90/120    avg_loss:0.007, val_acc:0.996]
Epoch [91/120    avg_loss:0.012, val_acc:0.996]
Epoch [92/120    avg_loss:0.011, val_acc:0.996]
Epoch [93/120    avg_loss:0.052, val_acc:0.963]
Epoch [94/120    avg_loss:0.211, val_acc:0.967]
Epoch [95/120    avg_loss:0.136, val_acc:0.985]
Epoch [96/120    avg_loss:0.048, val_acc:0.988]
Epoch [97/120    avg_loss:0.028, val_acc:0.990]
Epoch [98/120    avg_loss:0.053, val_acc:0.977]
Epoch [99/120    avg_loss:0.092, val_acc:0.985]
Epoch [100/120    avg_loss:0.116, val_acc:0.983]
Epoch [101/120    avg_loss:0.059, val_acc:0.985]
Epoch [102/120    avg_loss:0.045, val_acc:0.992]
Epoch [103/120    avg_loss:0.025, val_acc:0.994]
Epoch [104/120    avg_loss:0.027, val_acc:0.994]
Epoch [105/120    avg_loss:0.011, val_acc:0.994]
Epoch [106/120    avg_loss:0.014, val_acc:0.994]
Epoch [107/120    avg_loss:0.020, val_acc:0.994]
Epoch [108/120    avg_loss:0.014, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.994]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.013, val_acc:0.994]
Epoch [112/120    avg_loss:0.013, val_acc:0.994]
Epoch [113/120    avg_loss:0.008, val_acc:0.994]
Epoch [114/120    avg_loss:0.013, val_acc:0.994]
Epoch [115/120    avg_loss:0.014, val_acc:0.994]
Epoch [116/120    avg_loss:0.014, val_acc:0.994]
Epoch [117/120    avg_loss:0.009, val_acc:0.994]
Epoch [118/120    avg_loss:0.016, val_acc:0.994]
Epoch [119/120    avg_loss:0.016, val_acc:0.994]
Epoch [120/120    avg_loss:0.009, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   3   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   2   0   0   0   0   0   0   4   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 0.99707174 0.99545455 1.         0.97571744 0.96551724
 0.99757869 1.         1.         1.         1.         1.
 0.99339207 1.        ]

Kappa:
0.9959648046751379
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ea1237780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.272, val_acc:0.492]
Epoch [2/120    avg_loss:1.646, val_acc:0.700]
Epoch [3/120    avg_loss:1.304, val_acc:0.771]
Epoch [4/120    avg_loss:1.023, val_acc:0.771]
Epoch [5/120    avg_loss:0.915, val_acc:0.781]
Epoch [6/120    avg_loss:0.868, val_acc:0.817]
Epoch [7/120    avg_loss:0.718, val_acc:0.806]
Epoch [8/120    avg_loss:0.684, val_acc:0.871]
Epoch [9/120    avg_loss:0.551, val_acc:0.898]
Epoch [10/120    avg_loss:0.481, val_acc:0.896]
Epoch [11/120    avg_loss:0.440, val_acc:0.906]
Epoch [12/120    avg_loss:0.420, val_acc:0.892]
Epoch [13/120    avg_loss:0.398, val_acc:0.912]
Epoch [14/120    avg_loss:0.362, val_acc:0.915]
Epoch [15/120    avg_loss:0.418, val_acc:0.919]
Epoch [16/120    avg_loss:0.477, val_acc:0.854]
Epoch [17/120    avg_loss:0.399, val_acc:0.946]
Epoch [18/120    avg_loss:0.327, val_acc:0.927]
Epoch [19/120    avg_loss:0.272, val_acc:0.938]
Epoch [20/120    avg_loss:0.287, val_acc:0.940]
Epoch [21/120    avg_loss:0.297, val_acc:0.935]
Epoch [22/120    avg_loss:0.271, val_acc:0.938]
Epoch [23/120    avg_loss:0.234, val_acc:0.942]
Epoch [24/120    avg_loss:0.242, val_acc:0.942]
Epoch [25/120    avg_loss:0.411, val_acc:0.948]
Epoch [26/120    avg_loss:0.261, val_acc:0.940]
Epoch [27/120    avg_loss:0.266, val_acc:0.940]
Epoch [28/120    avg_loss:0.241, val_acc:0.950]
Epoch [29/120    avg_loss:0.204, val_acc:0.948]
Epoch [30/120    avg_loss:0.180, val_acc:0.946]
Epoch [31/120    avg_loss:0.235, val_acc:0.954]
Epoch [32/120    avg_loss:0.151, val_acc:0.960]
Epoch [33/120    avg_loss:0.141, val_acc:0.967]
Epoch [34/120    avg_loss:0.231, val_acc:0.960]
Epoch [35/120    avg_loss:0.385, val_acc:0.919]
Epoch [36/120    avg_loss:0.311, val_acc:0.952]
Epoch [37/120    avg_loss:0.192, val_acc:0.929]
Epoch [38/120    avg_loss:0.146, val_acc:0.960]
Epoch [39/120    avg_loss:0.159, val_acc:0.958]
Epoch [40/120    avg_loss:0.244, val_acc:0.960]
Epoch [41/120    avg_loss:0.153, val_acc:0.963]
Epoch [42/120    avg_loss:0.093, val_acc:0.958]
Epoch [43/120    avg_loss:0.105, val_acc:0.954]
Epoch [44/120    avg_loss:0.128, val_acc:0.963]
Epoch [45/120    avg_loss:0.101, val_acc:0.958]
Epoch [46/120    avg_loss:0.148, val_acc:0.967]
Epoch [47/120    avg_loss:0.109, val_acc:0.973]
Epoch [48/120    avg_loss:0.099, val_acc:0.967]
Epoch [49/120    avg_loss:0.065, val_acc:0.963]
Epoch [50/120    avg_loss:0.131, val_acc:0.946]
Epoch [51/120    avg_loss:0.140, val_acc:0.971]
Epoch [52/120    avg_loss:0.103, val_acc:0.971]
Epoch [53/120    avg_loss:0.086, val_acc:0.975]
Epoch [54/120    avg_loss:0.103, val_acc:0.971]
Epoch [55/120    avg_loss:0.083, val_acc:0.975]
Epoch [56/120    avg_loss:0.084, val_acc:0.960]
Epoch [57/120    avg_loss:0.085, val_acc:0.950]
Epoch [58/120    avg_loss:0.122, val_acc:0.960]
Epoch [59/120    avg_loss:0.078, val_acc:0.963]
Epoch [60/120    avg_loss:0.081, val_acc:0.958]
Epoch [61/120    avg_loss:0.098, val_acc:0.969]
Epoch [62/120    avg_loss:0.058, val_acc:0.973]
Epoch [63/120    avg_loss:0.100, val_acc:0.963]
Epoch [64/120    avg_loss:0.059, val_acc:0.963]
Epoch [65/120    avg_loss:0.068, val_acc:0.975]
Epoch [66/120    avg_loss:0.044, val_acc:0.977]
Epoch [67/120    avg_loss:0.044, val_acc:0.971]
Epoch [68/120    avg_loss:0.098, val_acc:0.975]
Epoch [69/120    avg_loss:0.058, val_acc:0.975]
Epoch [70/120    avg_loss:0.072, val_acc:0.975]
Epoch [71/120    avg_loss:0.080, val_acc:0.977]
Epoch [72/120    avg_loss:0.073, val_acc:0.969]
Epoch [73/120    avg_loss:0.072, val_acc:0.977]
Epoch [74/120    avg_loss:0.054, val_acc:0.971]
Epoch [75/120    avg_loss:0.075, val_acc:0.977]
Epoch [76/120    avg_loss:0.070, val_acc:0.969]
Epoch [77/120    avg_loss:0.052, val_acc:0.981]
Epoch [78/120    avg_loss:0.039, val_acc:0.979]
Epoch [79/120    avg_loss:0.045, val_acc:0.981]
Epoch [80/120    avg_loss:0.029, val_acc:0.977]
Epoch [81/120    avg_loss:0.036, val_acc:0.983]
Epoch [82/120    avg_loss:0.123, val_acc:0.963]
Epoch [83/120    avg_loss:0.043, val_acc:0.967]
Epoch [84/120    avg_loss:0.060, val_acc:0.963]
Epoch [85/120    avg_loss:0.059, val_acc:0.963]
Epoch [86/120    avg_loss:0.079, val_acc:0.965]
Epoch [87/120    avg_loss:0.072, val_acc:0.983]
Epoch [88/120    avg_loss:0.049, val_acc:0.973]
Epoch [89/120    avg_loss:0.043, val_acc:0.983]
Epoch [90/120    avg_loss:0.036, val_acc:0.973]
Epoch [91/120    avg_loss:0.029, val_acc:0.960]
Epoch [92/120    avg_loss:0.082, val_acc:0.973]
Epoch [93/120    avg_loss:0.048, val_acc:0.965]
Epoch [94/120    avg_loss:0.023, val_acc:0.977]
Epoch [95/120    avg_loss:0.030, val_acc:0.979]
Epoch [96/120    avg_loss:0.038, val_acc:0.960]
Epoch [97/120    avg_loss:0.040, val_acc:0.977]
Epoch [98/120    avg_loss:0.039, val_acc:0.975]
Epoch [99/120    avg_loss:0.059, val_acc:0.979]
Epoch [100/120    avg_loss:0.051, val_acc:0.973]
Epoch [101/120    avg_loss:0.024, val_acc:0.977]
Epoch [102/120    avg_loss:0.029, val_acc:0.988]
Epoch [103/120    avg_loss:0.025, val_acc:0.988]
Epoch [104/120    avg_loss:0.024, val_acc:0.988]
Epoch [105/120    avg_loss:0.022, val_acc:0.981]
Epoch [106/120    avg_loss:0.034, val_acc:0.983]
Epoch [107/120    avg_loss:0.018, val_acc:0.981]
Epoch [108/120    avg_loss:0.025, val_acc:0.981]
Epoch [109/120    avg_loss:0.019, val_acc:0.981]
Epoch [110/120    avg_loss:0.030, val_acc:0.973]
Epoch [111/120    avg_loss:0.042, val_acc:0.985]
Epoch [112/120    avg_loss:0.056, val_acc:0.967]
Epoch [113/120    avg_loss:0.131, val_acc:0.954]
Epoch [114/120    avg_loss:0.142, val_acc:0.969]
Epoch [115/120    avg_loss:0.062, val_acc:0.971]
Epoch [116/120    avg_loss:0.036, val_acc:0.973]
Epoch [117/120    avg_loss:0.042, val_acc:0.977]
Epoch [118/120    avg_loss:0.018, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.979]
Epoch [120/120    avg_loss:0.020, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 209  10   0   0   0   0   0   0   8   0]
 [  0   0   0   1  12 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.97065463 0.98013245 0.91067538 0.91986063
 0.99019608 0.97297297 1.         0.99893276 1.         1.
 0.98245614 1.        ]

Kappa:
0.9867051143851685
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f11e0c08780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.315, val_acc:0.458]
Epoch [2/120    avg_loss:1.594, val_acc:0.650]
Epoch [3/120    avg_loss:1.173, val_acc:0.775]
Epoch [4/120    avg_loss:0.927, val_acc:0.773]
Epoch [5/120    avg_loss:0.823, val_acc:0.817]
Epoch [6/120    avg_loss:0.716, val_acc:0.817]
Epoch [7/120    avg_loss:0.676, val_acc:0.846]
Epoch [8/120    avg_loss:0.625, val_acc:0.844]
Epoch [9/120    avg_loss:0.582, val_acc:0.823]
Epoch [10/120    avg_loss:0.497, val_acc:0.865]
Epoch [11/120    avg_loss:0.460, val_acc:0.871]
Epoch [12/120    avg_loss:0.377, val_acc:0.908]
Epoch [13/120    avg_loss:0.381, val_acc:0.925]
Epoch [14/120    avg_loss:0.312, val_acc:0.904]
Epoch [15/120    avg_loss:0.335, val_acc:0.890]
Epoch [16/120    avg_loss:0.349, val_acc:0.906]
Epoch [17/120    avg_loss:0.343, val_acc:0.915]
Epoch [18/120    avg_loss:0.310, val_acc:0.919]
Epoch [19/120    avg_loss:0.264, val_acc:0.929]
Epoch [20/120    avg_loss:0.246, val_acc:0.940]
Epoch [21/120    avg_loss:0.236, val_acc:0.931]
Epoch [22/120    avg_loss:0.242, val_acc:0.929]
Epoch [23/120    avg_loss:0.185, val_acc:0.938]
Epoch [24/120    avg_loss:0.189, val_acc:0.933]
Epoch [25/120    avg_loss:0.309, val_acc:0.927]
Epoch [26/120    avg_loss:0.299, val_acc:0.927]
Epoch [27/120    avg_loss:0.168, val_acc:0.944]
Epoch [28/120    avg_loss:0.181, val_acc:0.954]
Epoch [29/120    avg_loss:0.175, val_acc:0.944]
Epoch [30/120    avg_loss:0.169, val_acc:0.948]
Epoch [31/120    avg_loss:0.176, val_acc:0.948]
Epoch [32/120    avg_loss:0.206, val_acc:0.946]
Epoch [33/120    avg_loss:0.185, val_acc:0.950]
Epoch [34/120    avg_loss:0.161, val_acc:0.925]
Epoch [35/120    avg_loss:0.188, val_acc:0.948]
Epoch [36/120    avg_loss:0.142, val_acc:0.958]
Epoch [37/120    avg_loss:0.129, val_acc:0.952]
Epoch [38/120    avg_loss:0.151, val_acc:0.942]
Epoch [39/120    avg_loss:0.153, val_acc:0.973]
Epoch [40/120    avg_loss:0.166, val_acc:0.946]
Epoch [41/120    avg_loss:0.196, val_acc:0.952]
Epoch [42/120    avg_loss:0.158, val_acc:0.950]
Epoch [43/120    avg_loss:0.108, val_acc:0.952]
Epoch [44/120    avg_loss:0.109, val_acc:0.958]
Epoch [45/120    avg_loss:0.103, val_acc:0.952]
Epoch [46/120    avg_loss:0.086, val_acc:0.975]
Epoch [47/120    avg_loss:0.096, val_acc:0.969]
Epoch [48/120    avg_loss:0.133, val_acc:0.906]
Epoch [49/120    avg_loss:0.126, val_acc:0.956]
Epoch [50/120    avg_loss:0.105, val_acc:0.967]
Epoch [51/120    avg_loss:0.171, val_acc:0.967]
Epoch [52/120    avg_loss:0.101, val_acc:0.958]
Epoch [53/120    avg_loss:0.117, val_acc:0.973]
Epoch [54/120    avg_loss:0.094, val_acc:0.973]
Epoch [55/120    avg_loss:0.066, val_acc:0.971]
Epoch [56/120    avg_loss:0.059, val_acc:0.971]
Epoch [57/120    avg_loss:0.039, val_acc:0.981]
Epoch [58/120    avg_loss:0.054, val_acc:0.977]
Epoch [59/120    avg_loss:0.090, val_acc:0.967]
Epoch [60/120    avg_loss:0.050, val_acc:0.975]
Epoch [61/120    avg_loss:0.052, val_acc:0.981]
Epoch [62/120    avg_loss:0.044, val_acc:0.981]
Epoch [63/120    avg_loss:0.079, val_acc:0.971]
Epoch [64/120    avg_loss:0.075, val_acc:0.967]
Epoch [65/120    avg_loss:0.091, val_acc:0.975]
Epoch [66/120    avg_loss:0.061, val_acc:0.975]
Epoch [67/120    avg_loss:0.078, val_acc:0.969]
Epoch [68/120    avg_loss:0.081, val_acc:0.977]
Epoch [69/120    avg_loss:0.050, val_acc:0.979]
Epoch [70/120    avg_loss:0.042, val_acc:0.981]
Epoch [71/120    avg_loss:0.039, val_acc:0.981]
Epoch [72/120    avg_loss:0.055, val_acc:0.973]
Epoch [73/120    avg_loss:0.049, val_acc:0.981]
Epoch [74/120    avg_loss:0.050, val_acc:0.981]
Epoch [75/120    avg_loss:0.055, val_acc:0.981]
Epoch [76/120    avg_loss:0.067, val_acc:0.985]
Epoch [77/120    avg_loss:0.134, val_acc:0.975]
Epoch [78/120    avg_loss:0.096, val_acc:0.965]
Epoch [79/120    avg_loss:0.073, val_acc:0.979]
Epoch [80/120    avg_loss:0.069, val_acc:0.969]
Epoch [81/120    avg_loss:0.048, val_acc:0.988]
Epoch [82/120    avg_loss:0.038, val_acc:0.981]
Epoch [83/120    avg_loss:0.041, val_acc:0.977]
Epoch [84/120    avg_loss:0.045, val_acc:0.983]
Epoch [85/120    avg_loss:0.026, val_acc:0.983]
Epoch [86/120    avg_loss:0.023, val_acc:0.981]
Epoch [87/120    avg_loss:0.024, val_acc:0.983]
Epoch [88/120    avg_loss:0.023, val_acc:0.981]
Epoch [89/120    avg_loss:0.029, val_acc:0.981]
Epoch [90/120    avg_loss:0.028, val_acc:0.983]
Epoch [91/120    avg_loss:0.026, val_acc:0.985]
Epoch [92/120    avg_loss:0.029, val_acc:0.985]
Epoch [93/120    avg_loss:0.119, val_acc:0.973]
Epoch [94/120    avg_loss:0.114, val_acc:0.975]
Epoch [95/120    avg_loss:0.069, val_acc:0.977]
Epoch [96/120    avg_loss:0.059, val_acc:0.977]
Epoch [97/120    avg_loss:0.039, val_acc:0.979]
Epoch [98/120    avg_loss:0.018, val_acc:0.985]
Epoch [99/120    avg_loss:0.034, val_acc:0.988]
Epoch [100/120    avg_loss:0.024, val_acc:0.988]
Epoch [101/120    avg_loss:0.028, val_acc:0.985]
Epoch [102/120    avg_loss:0.026, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.013, val_acc:0.990]
Epoch [106/120    avg_loss:0.015, val_acc:0.990]
Epoch [107/120    avg_loss:0.016, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.011, val_acc:0.990]
Epoch [112/120    avg_loss:0.019, val_acc:0.990]
Epoch [113/120    avg_loss:0.017, val_acc:0.990]
Epoch [114/120    avg_loss:0.022, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.017, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.013, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217   9   0   0   0   2   2   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.997815   0.98648649 0.97091723 0.92693111 0.90510949
 0.99266504 0.98924731 0.99742931 0.9978678  1.         1.
 0.99556541 1.        ]

Kappa:
0.9886040589839167
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff790251710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.290, val_acc:0.527]
Epoch [2/120    avg_loss:1.582, val_acc:0.615]
Epoch [3/120    avg_loss:1.217, val_acc:0.719]
Epoch [4/120    avg_loss:1.019, val_acc:0.706]
Epoch [5/120    avg_loss:0.850, val_acc:0.808]
Epoch [6/120    avg_loss:0.743, val_acc:0.790]
Epoch [7/120    avg_loss:0.674, val_acc:0.819]
Epoch [8/120    avg_loss:0.657, val_acc:0.852]
Epoch [9/120    avg_loss:0.510, val_acc:0.883]
Epoch [10/120    avg_loss:0.506, val_acc:0.856]
Epoch [11/120    avg_loss:0.431, val_acc:0.908]
Epoch [12/120    avg_loss:0.406, val_acc:0.904]
Epoch [13/120    avg_loss:0.368, val_acc:0.915]
Epoch [14/120    avg_loss:0.308, val_acc:0.929]
Epoch [15/120    avg_loss:0.321, val_acc:0.929]
Epoch [16/120    avg_loss:0.264, val_acc:0.946]
Epoch [17/120    avg_loss:0.286, val_acc:0.925]
Epoch [18/120    avg_loss:0.289, val_acc:0.944]
Epoch [19/120    avg_loss:0.218, val_acc:0.942]
Epoch [20/120    avg_loss:0.223, val_acc:0.954]
Epoch [21/120    avg_loss:0.223, val_acc:0.963]
Epoch [22/120    avg_loss:0.234, val_acc:0.923]
Epoch [23/120    avg_loss:0.211, val_acc:0.950]
Epoch [24/120    avg_loss:0.206, val_acc:0.931]
Epoch [25/120    avg_loss:0.167, val_acc:0.938]
Epoch [26/120    avg_loss:0.212, val_acc:0.940]
Epoch [27/120    avg_loss:0.239, val_acc:0.958]
Epoch [28/120    avg_loss:0.213, val_acc:0.942]
Epoch [29/120    avg_loss:0.208, val_acc:0.954]
Epoch [30/120    avg_loss:0.165, val_acc:0.954]
Epoch [31/120    avg_loss:0.149, val_acc:0.948]
Epoch [32/120    avg_loss:0.201, val_acc:0.938]
Epoch [33/120    avg_loss:0.250, val_acc:0.933]
Epoch [34/120    avg_loss:0.185, val_acc:0.946]
Epoch [35/120    avg_loss:0.148, val_acc:0.956]
Epoch [36/120    avg_loss:0.126, val_acc:0.963]
Epoch [37/120    avg_loss:0.098, val_acc:0.967]
Epoch [38/120    avg_loss:0.091, val_acc:0.971]
Epoch [39/120    avg_loss:0.095, val_acc:0.971]
Epoch [40/120    avg_loss:0.085, val_acc:0.971]
Epoch [41/120    avg_loss:0.078, val_acc:0.973]
Epoch [42/120    avg_loss:0.094, val_acc:0.969]
Epoch [43/120    avg_loss:0.072, val_acc:0.973]
Epoch [44/120    avg_loss:0.078, val_acc:0.971]
Epoch [45/120    avg_loss:0.076, val_acc:0.971]
Epoch [46/120    avg_loss:0.060, val_acc:0.971]
Epoch [47/120    avg_loss:0.081, val_acc:0.971]
Epoch [48/120    avg_loss:0.069, val_acc:0.973]
Epoch [49/120    avg_loss:0.097, val_acc:0.975]
Epoch [50/120    avg_loss:0.078, val_acc:0.975]
Epoch [51/120    avg_loss:0.088, val_acc:0.971]
Epoch [52/120    avg_loss:0.061, val_acc:0.973]
Epoch [53/120    avg_loss:0.077, val_acc:0.975]
Epoch [54/120    avg_loss:0.094, val_acc:0.971]
Epoch [55/120    avg_loss:0.066, val_acc:0.971]
Epoch [56/120    avg_loss:0.064, val_acc:0.977]
Epoch [57/120    avg_loss:0.067, val_acc:0.977]
Epoch [58/120    avg_loss:0.096, val_acc:0.971]
Epoch [59/120    avg_loss:0.060, val_acc:0.967]
Epoch [60/120    avg_loss:0.068, val_acc:0.967]
Epoch [61/120    avg_loss:0.073, val_acc:0.967]
Epoch [62/120    avg_loss:0.062, val_acc:0.967]
Epoch [63/120    avg_loss:0.068, val_acc:0.971]
Epoch [64/120    avg_loss:0.057, val_acc:0.969]
Epoch [65/120    avg_loss:0.063, val_acc:0.977]
Epoch [66/120    avg_loss:0.064, val_acc:0.971]
Epoch [67/120    avg_loss:0.062, val_acc:0.969]
Epoch [68/120    avg_loss:0.055, val_acc:0.971]
Epoch [69/120    avg_loss:0.063, val_acc:0.971]
Epoch [70/120    avg_loss:0.068, val_acc:0.977]
Epoch [71/120    avg_loss:0.058, val_acc:0.975]
Epoch [72/120    avg_loss:0.051, val_acc:0.973]
Epoch [73/120    avg_loss:0.062, val_acc:0.975]
Epoch [74/120    avg_loss:0.070, val_acc:0.973]
Epoch [75/120    avg_loss:0.087, val_acc:0.975]
Epoch [76/120    avg_loss:0.052, val_acc:0.975]
Epoch [77/120    avg_loss:0.065, val_acc:0.975]
Epoch [78/120    avg_loss:0.071, val_acc:0.971]
Epoch [79/120    avg_loss:0.045, val_acc:0.971]
Epoch [80/120    avg_loss:0.058, val_acc:0.971]
Epoch [81/120    avg_loss:0.054, val_acc:0.971]
Epoch [82/120    avg_loss:0.083, val_acc:0.973]
Epoch [83/120    avg_loss:0.039, val_acc:0.969]
Epoch [84/120    avg_loss:0.040, val_acc:0.969]
Epoch [85/120    avg_loss:0.071, val_acc:0.969]
Epoch [86/120    avg_loss:0.045, val_acc:0.969]
Epoch [87/120    avg_loss:0.047, val_acc:0.969]
Epoch [88/120    avg_loss:0.060, val_acc:0.969]
Epoch [89/120    avg_loss:0.062, val_acc:0.971]
Epoch [90/120    avg_loss:0.052, val_acc:0.971]
Epoch [91/120    avg_loss:0.065, val_acc:0.971]
Epoch [92/120    avg_loss:0.051, val_acc:0.973]
Epoch [93/120    avg_loss:0.051, val_acc:0.971]
Epoch [94/120    avg_loss:0.045, val_acc:0.975]
Epoch [95/120    avg_loss:0.064, val_acc:0.975]
Epoch [96/120    avg_loss:0.058, val_acc:0.975]
Epoch [97/120    avg_loss:0.052, val_acc:0.975]
Epoch [98/120    avg_loss:0.058, val_acc:0.975]
Epoch [99/120    avg_loss:0.055, val_acc:0.975]
Epoch [100/120    avg_loss:0.053, val_acc:0.975]
Epoch [101/120    avg_loss:0.071, val_acc:0.975]
Epoch [102/120    avg_loss:0.084, val_acc:0.975]
Epoch [103/120    avg_loss:0.047, val_acc:0.975]
Epoch [104/120    avg_loss:0.053, val_acc:0.975]
Epoch [105/120    avg_loss:0.055, val_acc:0.975]
Epoch [106/120    avg_loss:0.064, val_acc:0.975]
Epoch [107/120    avg_loss:0.048, val_acc:0.975]
Epoch [108/120    avg_loss:0.070, val_acc:0.975]
Epoch [109/120    avg_loss:0.050, val_acc:0.975]
Epoch [110/120    avg_loss:0.040, val_acc:0.975]
Epoch [111/120    avg_loss:0.092, val_acc:0.975]
Epoch [112/120    avg_loss:0.052, val_acc:0.975]
Epoch [113/120    avg_loss:0.052, val_acc:0.975]
Epoch [114/120    avg_loss:0.074, val_acc:0.975]
Epoch [115/120    avg_loss:0.075, val_acc:0.975]
Epoch [116/120    avg_loss:0.047, val_acc:0.975]
Epoch [117/120    avg_loss:0.057, val_acc:0.975]
Epoch [118/120    avg_loss:0.044, val_acc:0.975]
Epoch [119/120    avg_loss:0.048, val_acc:0.975]
Epoch [120/120    avg_loss:0.071, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 204   0   0   0   0  15   0   0   0   0   0   0]
 [  0   0   0 227   2   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 0.99926954 0.92938497 0.99343545 0.95074946 0.92473118
 0.99757869 0.85863874 0.998713   1.         1.         0.99600533
 0.99226519 1.        ]

Kappa:
0.9859948542692488
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d671b66d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.377, val_acc:0.485]
Epoch [2/120    avg_loss:1.721, val_acc:0.692]
Epoch [3/120    avg_loss:1.309, val_acc:0.710]
Epoch [4/120    avg_loss:1.050, val_acc:0.817]
Epoch [5/120    avg_loss:0.897, val_acc:0.802]
Epoch [6/120    avg_loss:0.748, val_acc:0.869]
Epoch [7/120    avg_loss:0.565, val_acc:0.890]
Epoch [8/120    avg_loss:0.506, val_acc:0.923]
Epoch [9/120    avg_loss:0.469, val_acc:0.940]
Epoch [10/120    avg_loss:0.466, val_acc:0.890]
Epoch [11/120    avg_loss:0.481, val_acc:0.938]
Epoch [12/120    avg_loss:0.374, val_acc:0.933]
Epoch [13/120    avg_loss:0.356, val_acc:0.933]
Epoch [14/120    avg_loss:0.314, val_acc:0.944]
Epoch [15/120    avg_loss:0.317, val_acc:0.929]
Epoch [16/120    avg_loss:0.225, val_acc:0.927]
Epoch [17/120    avg_loss:0.228, val_acc:0.946]
Epoch [18/120    avg_loss:0.253, val_acc:0.946]
Epoch [19/120    avg_loss:0.227, val_acc:0.948]
Epoch [20/120    avg_loss:0.331, val_acc:0.933]
Epoch [21/120    avg_loss:0.247, val_acc:0.965]
Epoch [22/120    avg_loss:0.317, val_acc:0.954]
Epoch [23/120    avg_loss:0.237, val_acc:0.944]
Epoch [24/120    avg_loss:0.240, val_acc:0.933]
Epoch [25/120    avg_loss:0.235, val_acc:0.954]
Epoch [26/120    avg_loss:0.285, val_acc:0.952]
Epoch [27/120    avg_loss:0.224, val_acc:0.958]
Epoch [28/120    avg_loss:0.250, val_acc:0.969]
Epoch [29/120    avg_loss:0.163, val_acc:0.960]
Epoch [30/120    avg_loss:0.142, val_acc:0.960]
Epoch [31/120    avg_loss:0.142, val_acc:0.950]
Epoch [32/120    avg_loss:0.169, val_acc:0.969]
Epoch [33/120    avg_loss:0.129, val_acc:0.969]
Epoch [34/120    avg_loss:0.125, val_acc:0.975]
Epoch [35/120    avg_loss:0.125, val_acc:0.973]
Epoch [36/120    avg_loss:0.096, val_acc:0.973]
Epoch [37/120    avg_loss:0.097, val_acc:0.981]
Epoch [38/120    avg_loss:0.133, val_acc:0.977]
Epoch [39/120    avg_loss:0.078, val_acc:0.975]
Epoch [40/120    avg_loss:0.137, val_acc:0.992]
Epoch [41/120    avg_loss:0.082, val_acc:0.967]
Epoch [42/120    avg_loss:0.095, val_acc:0.971]
Epoch [43/120    avg_loss:0.078, val_acc:0.969]
Epoch [44/120    avg_loss:0.132, val_acc:0.971]
Epoch [45/120    avg_loss:0.149, val_acc:0.946]
Epoch [46/120    avg_loss:0.105, val_acc:0.975]
Epoch [47/120    avg_loss:0.090, val_acc:0.967]
Epoch [48/120    avg_loss:0.174, val_acc:0.985]
Epoch [49/120    avg_loss:0.109, val_acc:0.979]
Epoch [50/120    avg_loss:0.058, val_acc:0.979]
Epoch [51/120    avg_loss:0.121, val_acc:0.988]
Epoch [52/120    avg_loss:0.084, val_acc:0.985]
Epoch [53/120    avg_loss:0.050, val_acc:0.973]
Epoch [54/120    avg_loss:0.059, val_acc:0.977]
Epoch [55/120    avg_loss:0.059, val_acc:0.979]
Epoch [56/120    avg_loss:0.058, val_acc:0.979]
Epoch [57/120    avg_loss:0.043, val_acc:0.981]
Epoch [58/120    avg_loss:0.037, val_acc:0.981]
Epoch [59/120    avg_loss:0.055, val_acc:0.979]
Epoch [60/120    avg_loss:0.043, val_acc:0.981]
Epoch [61/120    avg_loss:0.041, val_acc:0.981]
Epoch [62/120    avg_loss:0.039, val_acc:0.983]
Epoch [63/120    avg_loss:0.047, val_acc:0.981]
Epoch [64/120    avg_loss:0.041, val_acc:0.985]
Epoch [65/120    avg_loss:0.033, val_acc:0.985]
Epoch [66/120    avg_loss:0.043, val_acc:0.985]
Epoch [67/120    avg_loss:0.035, val_acc:0.985]
Epoch [68/120    avg_loss:0.057, val_acc:0.983]
Epoch [69/120    avg_loss:0.049, val_acc:0.981]
Epoch [70/120    avg_loss:0.032, val_acc:0.983]
Epoch [71/120    avg_loss:0.045, val_acc:0.983]
Epoch [72/120    avg_loss:0.048, val_acc:0.983]
Epoch [73/120    avg_loss:0.034, val_acc:0.983]
Epoch [74/120    avg_loss:0.035, val_acc:0.983]
Epoch [75/120    avg_loss:0.045, val_acc:0.983]
Epoch [76/120    avg_loss:0.038, val_acc:0.981]
Epoch [77/120    avg_loss:0.042, val_acc:0.981]
Epoch [78/120    avg_loss:0.025, val_acc:0.981]
Epoch [79/120    avg_loss:0.048, val_acc:0.981]
Epoch [80/120    avg_loss:0.040, val_acc:0.981]
Epoch [81/120    avg_loss:0.040, val_acc:0.981]
Epoch [82/120    avg_loss:0.032, val_acc:0.981]
Epoch [83/120    avg_loss:0.033, val_acc:0.981]
Epoch [84/120    avg_loss:0.036, val_acc:0.981]
Epoch [85/120    avg_loss:0.044, val_acc:0.981]
Epoch [86/120    avg_loss:0.031, val_acc:0.981]
Epoch [87/120    avg_loss:0.047, val_acc:0.981]
Epoch [88/120    avg_loss:0.040, val_acc:0.981]
Epoch [89/120    avg_loss:0.030, val_acc:0.981]
Epoch [90/120    avg_loss:0.041, val_acc:0.981]
Epoch [91/120    avg_loss:0.028, val_acc:0.981]
Epoch [92/120    avg_loss:0.042, val_acc:0.981]
Epoch [93/120    avg_loss:0.054, val_acc:0.981]
Epoch [94/120    avg_loss:0.050, val_acc:0.981]
Epoch [95/120    avg_loss:0.030, val_acc:0.981]
Epoch [96/120    avg_loss:0.038, val_acc:0.981]
Epoch [97/120    avg_loss:0.041, val_acc:0.981]
Epoch [98/120    avg_loss:0.039, val_acc:0.981]
Epoch [99/120    avg_loss:0.044, val_acc:0.981]
Epoch [100/120    avg_loss:0.030, val_acc:0.981]
Epoch [101/120    avg_loss:0.040, val_acc:0.981]
Epoch [102/120    avg_loss:0.035, val_acc:0.981]
Epoch [103/120    avg_loss:0.033, val_acc:0.981]
Epoch [104/120    avg_loss:0.034, val_acc:0.981]
Epoch [105/120    avg_loss:0.032, val_acc:0.981]
Epoch [106/120    avg_loss:0.034, val_acc:0.981]
Epoch [107/120    avg_loss:0.047, val_acc:0.981]
Epoch [108/120    avg_loss:0.037, val_acc:0.981]
Epoch [109/120    avg_loss:0.051, val_acc:0.981]
Epoch [110/120    avg_loss:0.036, val_acc:0.981]
Epoch [111/120    avg_loss:0.031, val_acc:0.981]
Epoch [112/120    avg_loss:0.043, val_acc:0.981]
Epoch [113/120    avg_loss:0.034, val_acc:0.981]
Epoch [114/120    avg_loss:0.049, val_acc:0.981]
Epoch [115/120    avg_loss:0.032, val_acc:0.981]
Epoch [116/120    avg_loss:0.032, val_acc:0.981]
Epoch [117/120    avg_loss:0.039, val_acc:0.981]
Epoch [118/120    avg_loss:0.035, val_acc:0.981]
Epoch [119/120    avg_loss:0.053, val_acc:0.981]
Epoch [120/120    avg_loss:0.040, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   6   0   0   0   0   3   0]
 [  0   0   0 223   4   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 222   3   0   0   1   0   0   0   1   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0   0 833]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 0.997815   0.94594595 0.98454746 0.9466951  0.93140794
 0.99266504 0.88888889 0.99487179 0.99893276 1.         1.
 0.99449945 0.99940012]

Kappa:
0.986703641047719
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc08371b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.288, val_acc:0.496]
Epoch [2/120    avg_loss:1.627, val_acc:0.648]
Epoch [3/120    avg_loss:1.256, val_acc:0.740]
Epoch [4/120    avg_loss:1.088, val_acc:0.790]
Epoch [5/120    avg_loss:0.999, val_acc:0.731]
Epoch [6/120    avg_loss:0.785, val_acc:0.827]
Epoch [7/120    avg_loss:0.698, val_acc:0.852]
Epoch [8/120    avg_loss:0.621, val_acc:0.833]
Epoch [9/120    avg_loss:0.572, val_acc:0.863]
Epoch [10/120    avg_loss:0.505, val_acc:0.877]
Epoch [11/120    avg_loss:0.419, val_acc:0.852]
Epoch [12/120    avg_loss:0.353, val_acc:0.908]
Epoch [13/120    avg_loss:0.335, val_acc:0.925]
Epoch [14/120    avg_loss:0.309, val_acc:0.929]
Epoch [15/120    avg_loss:0.300, val_acc:0.910]
Epoch [16/120    avg_loss:0.365, val_acc:0.910]
Epoch [17/120    avg_loss:0.277, val_acc:0.898]
Epoch [18/120    avg_loss:0.237, val_acc:0.948]
Epoch [19/120    avg_loss:0.232, val_acc:0.931]
Epoch [20/120    avg_loss:0.287, val_acc:0.942]
Epoch [21/120    avg_loss:0.266, val_acc:0.929]
Epoch [22/120    avg_loss:0.227, val_acc:0.944]
Epoch [23/120    avg_loss:0.229, val_acc:0.942]
Epoch [24/120    avg_loss:0.269, val_acc:0.940]
Epoch [25/120    avg_loss:0.208, val_acc:0.938]
Epoch [26/120    avg_loss:0.221, val_acc:0.929]
Epoch [27/120    avg_loss:0.179, val_acc:0.940]
Epoch [28/120    avg_loss:0.142, val_acc:0.958]
Epoch [29/120    avg_loss:0.142, val_acc:0.958]
Epoch [30/120    avg_loss:0.164, val_acc:0.942]
Epoch [31/120    avg_loss:0.171, val_acc:0.950]
Epoch [32/120    avg_loss:0.150, val_acc:0.948]
Epoch [33/120    avg_loss:0.133, val_acc:0.956]
Epoch [34/120    avg_loss:0.138, val_acc:0.950]
Epoch [35/120    avg_loss:0.106, val_acc:0.938]
Epoch [36/120    avg_loss:0.184, val_acc:0.942]
Epoch [37/120    avg_loss:0.131, val_acc:0.946]
Epoch [38/120    avg_loss:0.167, val_acc:0.950]
Epoch [39/120    avg_loss:0.109, val_acc:0.960]
Epoch [40/120    avg_loss:0.122, val_acc:0.967]
Epoch [41/120    avg_loss:0.142, val_acc:0.963]
Epoch [42/120    avg_loss:0.095, val_acc:0.954]
Epoch [43/120    avg_loss:0.104, val_acc:0.958]
Epoch [44/120    avg_loss:0.129, val_acc:0.965]
Epoch [45/120    avg_loss:0.086, val_acc:0.965]
Epoch [46/120    avg_loss:0.114, val_acc:0.965]
Epoch [47/120    avg_loss:0.101, val_acc:0.956]
Epoch [48/120    avg_loss:0.068, val_acc:0.973]
Epoch [49/120    avg_loss:0.109, val_acc:0.969]
Epoch [50/120    avg_loss:0.104, val_acc:0.946]
Epoch [51/120    avg_loss:0.109, val_acc:0.952]
Epoch [52/120    avg_loss:0.092, val_acc:0.967]
Epoch [53/120    avg_loss:0.079, val_acc:0.965]
Epoch [54/120    avg_loss:0.076, val_acc:0.971]
Epoch [55/120    avg_loss:0.068, val_acc:0.973]
Epoch [56/120    avg_loss:0.064, val_acc:0.967]
Epoch [57/120    avg_loss:0.066, val_acc:0.973]
Epoch [58/120    avg_loss:0.071, val_acc:0.975]
Epoch [59/120    avg_loss:0.061, val_acc:0.971]
Epoch [60/120    avg_loss:0.041, val_acc:0.971]
Epoch [61/120    avg_loss:0.076, val_acc:0.973]
Epoch [62/120    avg_loss:0.075, val_acc:0.969]
Epoch [63/120    avg_loss:0.061, val_acc:0.958]
Epoch [64/120    avg_loss:0.108, val_acc:0.975]
Epoch [65/120    avg_loss:0.149, val_acc:0.944]
Epoch [66/120    avg_loss:0.231, val_acc:0.956]
Epoch [67/120    avg_loss:0.076, val_acc:0.969]
Epoch [68/120    avg_loss:0.071, val_acc:0.971]
Epoch [69/120    avg_loss:0.059, val_acc:0.975]
Epoch [70/120    avg_loss:0.046, val_acc:0.981]
Epoch [71/120    avg_loss:0.058, val_acc:0.975]
Epoch [72/120    avg_loss:0.055, val_acc:0.969]
Epoch [73/120    avg_loss:0.049, val_acc:0.981]
Epoch [74/120    avg_loss:0.024, val_acc:0.975]
Epoch [75/120    avg_loss:0.033, val_acc:0.975]
Epoch [76/120    avg_loss:0.033, val_acc:0.979]
Epoch [77/120    avg_loss:0.038, val_acc:0.979]
Epoch [78/120    avg_loss:0.047, val_acc:0.979]
Epoch [79/120    avg_loss:0.033, val_acc:0.979]
Epoch [80/120    avg_loss:0.042, val_acc:0.973]
Epoch [81/120    avg_loss:0.051, val_acc:0.983]
Epoch [82/120    avg_loss:0.056, val_acc:0.981]
Epoch [83/120    avg_loss:0.030, val_acc:0.977]
Epoch [84/120    avg_loss:0.041, val_acc:0.990]
Epoch [85/120    avg_loss:0.049, val_acc:0.981]
Epoch [86/120    avg_loss:0.039, val_acc:0.977]
Epoch [87/120    avg_loss:0.035, val_acc:0.977]
Epoch [88/120    avg_loss:0.025, val_acc:0.977]
Epoch [89/120    avg_loss:0.028, val_acc:0.981]
Epoch [90/120    avg_loss:0.016, val_acc:0.977]
Epoch [91/120    avg_loss:0.015, val_acc:0.981]
Epoch [92/120    avg_loss:0.038, val_acc:0.973]
Epoch [93/120    avg_loss:0.035, val_acc:0.977]
Epoch [94/120    avg_loss:0.042, val_acc:0.981]
Epoch [95/120    avg_loss:0.029, val_acc:0.979]
Epoch [96/120    avg_loss:0.050, val_acc:0.981]
Epoch [97/120    avg_loss:0.022, val_acc:0.979]
Epoch [98/120    avg_loss:0.022, val_acc:0.979]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.018, val_acc:0.979]
Epoch [101/120    avg_loss:0.012, val_acc:0.979]
Epoch [102/120    avg_loss:0.019, val_acc:0.979]
Epoch [103/120    avg_loss:0.014, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.977]
Epoch [105/120    avg_loss:0.015, val_acc:0.979]
Epoch [106/120    avg_loss:0.020, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.977]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.032, val_acc:0.979]
Epoch [110/120    avg_loss:0.015, val_acc:0.977]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.015, val_acc:0.977]
Epoch [113/120    avg_loss:0.007, val_acc:0.977]
Epoch [114/120    avg_loss:0.009, val_acc:0.977]
Epoch [115/120    avg_loss:0.010, val_acc:0.977]
Epoch [116/120    avg_loss:0.027, val_acc:0.977]
Epoch [117/120    avg_loss:0.012, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.975]
Epoch [119/120    avg_loss:0.014, val_acc:0.975]
Epoch [120/120    avg_loss:0.014, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   1   0   0   0   0   1   0]
 [  0   0   0 219   6   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0   0 832]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.997815   0.9752809  0.97550111 0.94315789 0.92363636
 0.99266504 0.9673913  0.99232737 0.99893276 1.         1.
 0.9944629  0.99879952]

Kappa:
0.9886042914622545
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d36d62780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.267, val_acc:0.552]
Epoch [2/120    avg_loss:1.650, val_acc:0.673]
Epoch [3/120    avg_loss:1.270, val_acc:0.740]
Epoch [4/120    avg_loss:0.981, val_acc:0.733]
Epoch [5/120    avg_loss:0.860, val_acc:0.825]
Epoch [6/120    avg_loss:0.741, val_acc:0.810]
Epoch [7/120    avg_loss:0.728, val_acc:0.850]
Epoch [8/120    avg_loss:0.641, val_acc:0.848]
Epoch [9/120    avg_loss:0.597, val_acc:0.842]
Epoch [10/120    avg_loss:0.516, val_acc:0.883]
Epoch [11/120    avg_loss:0.472, val_acc:0.888]
Epoch [12/120    avg_loss:0.525, val_acc:0.890]
Epoch [13/120    avg_loss:0.398, val_acc:0.875]
Epoch [14/120    avg_loss:0.369, val_acc:0.902]
Epoch [15/120    avg_loss:0.315, val_acc:0.904]
Epoch [16/120    avg_loss:0.290, val_acc:0.927]
Epoch [17/120    avg_loss:0.273, val_acc:0.906]
Epoch [18/120    avg_loss:0.330, val_acc:0.925]
Epoch [19/120    avg_loss:0.342, val_acc:0.902]
Epoch [20/120    avg_loss:0.352, val_acc:0.933]
Epoch [21/120    avg_loss:0.314, val_acc:0.910]
Epoch [22/120    avg_loss:0.269, val_acc:0.938]
Epoch [23/120    avg_loss:0.267, val_acc:0.929]
Epoch [24/120    avg_loss:0.201, val_acc:0.948]
Epoch [25/120    avg_loss:0.233, val_acc:0.937]
Epoch [26/120    avg_loss:0.274, val_acc:0.935]
Epoch [27/120    avg_loss:0.232, val_acc:0.946]
Epoch [28/120    avg_loss:0.178, val_acc:0.940]
Epoch [29/120    avg_loss:0.165, val_acc:0.960]
Epoch [30/120    avg_loss:0.163, val_acc:0.954]
Epoch [31/120    avg_loss:0.150, val_acc:0.952]
Epoch [32/120    avg_loss:0.116, val_acc:0.965]
Epoch [33/120    avg_loss:0.096, val_acc:0.971]
Epoch [34/120    avg_loss:0.132, val_acc:0.962]
Epoch [35/120    avg_loss:0.164, val_acc:0.971]
Epoch [36/120    avg_loss:0.346, val_acc:0.938]
Epoch [37/120    avg_loss:0.251, val_acc:0.960]
Epoch [38/120    avg_loss:0.146, val_acc:0.958]
Epoch [39/120    avg_loss:0.118, val_acc:0.958]
Epoch [40/120    avg_loss:0.116, val_acc:0.963]
Epoch [41/120    avg_loss:0.095, val_acc:0.950]
Epoch [42/120    avg_loss:0.095, val_acc:0.967]
Epoch [43/120    avg_loss:0.135, val_acc:0.962]
Epoch [44/120    avg_loss:0.092, val_acc:0.977]
Epoch [45/120    avg_loss:0.070, val_acc:0.979]
Epoch [46/120    avg_loss:0.089, val_acc:0.969]
Epoch [47/120    avg_loss:0.106, val_acc:0.958]
Epoch [48/120    avg_loss:0.095, val_acc:0.971]
Epoch [49/120    avg_loss:0.093, val_acc:0.962]
Epoch [50/120    avg_loss:0.064, val_acc:0.971]
Epoch [51/120    avg_loss:0.062, val_acc:0.967]
Epoch [52/120    avg_loss:0.096, val_acc:0.960]
Epoch [53/120    avg_loss:0.069, val_acc:0.971]
Epoch [54/120    avg_loss:0.059, val_acc:0.973]
Epoch [55/120    avg_loss:0.055, val_acc:0.981]
Epoch [56/120    avg_loss:0.052, val_acc:0.977]
Epoch [57/120    avg_loss:0.067, val_acc:0.973]
Epoch [58/120    avg_loss:0.048, val_acc:0.973]
Epoch [59/120    avg_loss:0.063, val_acc:0.967]
Epoch [60/120    avg_loss:0.062, val_acc:0.973]
Epoch [61/120    avg_loss:0.072, val_acc:0.975]
Epoch [62/120    avg_loss:0.036, val_acc:0.979]
Epoch [63/120    avg_loss:0.070, val_acc:0.975]
Epoch [64/120    avg_loss:0.055, val_acc:0.977]
Epoch [65/120    avg_loss:0.059, val_acc:0.963]
Epoch [66/120    avg_loss:0.059, val_acc:0.975]
Epoch [67/120    avg_loss:0.041, val_acc:0.977]
Epoch [68/120    avg_loss:0.026, val_acc:0.981]
Epoch [69/120    avg_loss:0.050, val_acc:0.985]
Epoch [70/120    avg_loss:0.025, val_acc:0.987]
Epoch [71/120    avg_loss:0.043, val_acc:0.977]
Epoch [72/120    avg_loss:0.069, val_acc:0.962]
Epoch [73/120    avg_loss:0.054, val_acc:0.985]
Epoch [74/120    avg_loss:0.042, val_acc:0.973]
Epoch [75/120    avg_loss:0.044, val_acc:0.977]
Epoch [76/120    avg_loss:0.059, val_acc:0.975]
Epoch [77/120    avg_loss:0.067, val_acc:0.979]
Epoch [78/120    avg_loss:0.049, val_acc:0.975]
Epoch [79/120    avg_loss:0.074, val_acc:0.975]
Epoch [80/120    avg_loss:0.101, val_acc:0.971]
Epoch [81/120    avg_loss:0.095, val_acc:0.975]
Epoch [82/120    avg_loss:0.089, val_acc:0.973]
Epoch [83/120    avg_loss:0.038, val_acc:0.975]
Epoch [84/120    avg_loss:0.020, val_acc:0.977]
Epoch [85/120    avg_loss:0.050, val_acc:0.975]
Epoch [86/120    avg_loss:0.024, val_acc:0.979]
Epoch [87/120    avg_loss:0.025, val_acc:0.979]
Epoch [88/120    avg_loss:0.022, val_acc:0.979]
Epoch [89/120    avg_loss:0.022, val_acc:0.981]
Epoch [90/120    avg_loss:0.022, val_acc:0.981]
Epoch [91/120    avg_loss:0.024, val_acc:0.983]
Epoch [92/120    avg_loss:0.021, val_acc:0.983]
Epoch [93/120    avg_loss:0.018, val_acc:0.983]
Epoch [94/120    avg_loss:0.028, val_acc:0.983]
Epoch [95/120    avg_loss:0.025, val_acc:0.983]
Epoch [96/120    avg_loss:0.035, val_acc:0.983]
Epoch [97/120    avg_loss:0.015, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.983]
Epoch [99/120    avg_loss:0.020, val_acc:0.983]
Epoch [100/120    avg_loss:0.042, val_acc:0.983]
Epoch [101/120    avg_loss:0.035, val_acc:0.983]
Epoch [102/120    avg_loss:0.032, val_acc:0.983]
Epoch [103/120    avg_loss:0.019, val_acc:0.983]
Epoch [104/120    avg_loss:0.022, val_acc:0.983]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.019, val_acc:0.983]
Epoch [107/120    avg_loss:0.027, val_acc:0.983]
Epoch [108/120    avg_loss:0.034, val_acc:0.983]
Epoch [109/120    avg_loss:0.013, val_acc:0.983]
Epoch [110/120    avg_loss:0.030, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.983]
Epoch [112/120    avg_loss:0.023, val_acc:0.983]
Epoch [113/120    avg_loss:0.022, val_acc:0.983]
Epoch [114/120    avg_loss:0.019, val_acc:0.983]
Epoch [115/120    avg_loss:0.021, val_acc:0.983]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.023, val_acc:0.983]
Epoch [118/120    avg_loss:0.029, val_acc:0.983]
Epoch [119/120    avg_loss:0.022, val_acc:0.983]
Epoch [120/120    avg_loss:0.018, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0   6   0   0   0   0   4   0]
 [  0   0   0 218   5   0   0   0   6   1   0   0   0   0]
 [  0   0   0   1 225   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.95216401 0.97104677 0.94142259 0.93772894
 0.99019608 0.93617021 0.99106003 0.99893276 1.         1.
 0.99005525 1.        ]

Kappa:
0.986704903063081
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9957f2a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.349, val_acc:0.646]
Epoch [2/120    avg_loss:1.662, val_acc:0.665]
Epoch [3/120    avg_loss:1.266, val_acc:0.800]
Epoch [4/120    avg_loss:1.025, val_acc:0.796]
Epoch [5/120    avg_loss:0.882, val_acc:0.863]
Epoch [6/120    avg_loss:0.761, val_acc:0.827]
Epoch [7/120    avg_loss:0.637, val_acc:0.863]
Epoch [8/120    avg_loss:0.563, val_acc:0.856]
Epoch [9/120    avg_loss:0.479, val_acc:0.902]
Epoch [10/120    avg_loss:0.457, val_acc:0.908]
Epoch [11/120    avg_loss:0.399, val_acc:0.890]
Epoch [12/120    avg_loss:0.525, val_acc:0.912]
Epoch [13/120    avg_loss:0.493, val_acc:0.904]
Epoch [14/120    avg_loss:0.345, val_acc:0.919]
Epoch [15/120    avg_loss:0.347, val_acc:0.933]
Epoch [16/120    avg_loss:0.299, val_acc:0.925]
Epoch [17/120    avg_loss:0.299, val_acc:0.946]
Epoch [18/120    avg_loss:0.309, val_acc:0.900]
Epoch [19/120    avg_loss:0.346, val_acc:0.940]
Epoch [20/120    avg_loss:0.286, val_acc:0.931]
Epoch [21/120    avg_loss:0.199, val_acc:0.952]
Epoch [22/120    avg_loss:0.203, val_acc:0.940]
Epoch [23/120    avg_loss:0.240, val_acc:0.929]
Epoch [24/120    avg_loss:0.167, val_acc:0.956]
Epoch [25/120    avg_loss:0.242, val_acc:0.950]
Epoch [26/120    avg_loss:0.196, val_acc:0.942]
Epoch [27/120    avg_loss:0.136, val_acc:0.948]
Epoch [28/120    avg_loss:0.146, val_acc:0.948]
Epoch [29/120    avg_loss:0.164, val_acc:0.952]
Epoch [30/120    avg_loss:0.118, val_acc:0.958]
Epoch [31/120    avg_loss:0.132, val_acc:0.942]
Epoch [32/120    avg_loss:0.227, val_acc:0.963]
Epoch [33/120    avg_loss:0.132, val_acc:0.950]
Epoch [34/120    avg_loss:0.157, val_acc:0.965]
Epoch [35/120    avg_loss:0.189, val_acc:0.948]
Epoch [36/120    avg_loss:0.266, val_acc:0.935]
Epoch [37/120    avg_loss:0.197, val_acc:0.956]
Epoch [38/120    avg_loss:0.210, val_acc:0.956]
Epoch [39/120    avg_loss:0.162, val_acc:0.942]
Epoch [40/120    avg_loss:0.208, val_acc:0.965]
Epoch [41/120    avg_loss:0.125, val_acc:0.963]
Epoch [42/120    avg_loss:0.137, val_acc:0.960]
Epoch [43/120    avg_loss:0.150, val_acc:0.975]
Epoch [44/120    avg_loss:0.120, val_acc:0.942]
Epoch [45/120    avg_loss:0.130, val_acc:0.965]
Epoch [46/120    avg_loss:0.101, val_acc:0.952]
Epoch [47/120    avg_loss:0.103, val_acc:0.969]
Epoch [48/120    avg_loss:0.106, val_acc:0.965]
Epoch [49/120    avg_loss:0.049, val_acc:0.971]
Epoch [50/120    avg_loss:0.069, val_acc:0.973]
Epoch [51/120    avg_loss:0.099, val_acc:0.971]
Epoch [52/120    avg_loss:0.063, val_acc:0.971]
Epoch [53/120    avg_loss:0.087, val_acc:0.979]
Epoch [54/120    avg_loss:0.093, val_acc:0.971]
Epoch [55/120    avg_loss:0.092, val_acc:0.977]
Epoch [56/120    avg_loss:0.041, val_acc:0.977]
Epoch [57/120    avg_loss:0.081, val_acc:0.971]
Epoch [58/120    avg_loss:0.051, val_acc:0.985]
Epoch [59/120    avg_loss:0.079, val_acc:0.971]
Epoch [60/120    avg_loss:0.126, val_acc:0.969]
Epoch [61/120    avg_loss:0.116, val_acc:0.975]
Epoch [62/120    avg_loss:0.130, val_acc:0.981]
Epoch [63/120    avg_loss:0.107, val_acc:0.979]
Epoch [64/120    avg_loss:0.085, val_acc:0.981]
Epoch [65/120    avg_loss:0.071, val_acc:0.975]
Epoch [66/120    avg_loss:0.039, val_acc:0.975]
Epoch [67/120    avg_loss:0.043, val_acc:0.983]
Epoch [68/120    avg_loss:0.077, val_acc:0.971]
Epoch [69/120    avg_loss:0.052, val_acc:0.975]
Epoch [70/120    avg_loss:0.068, val_acc:0.971]
Epoch [71/120    avg_loss:0.118, val_acc:0.971]
Epoch [72/120    avg_loss:0.107, val_acc:0.979]
Epoch [73/120    avg_loss:0.088, val_acc:0.977]
Epoch [74/120    avg_loss:0.088, val_acc:0.977]
Epoch [75/120    avg_loss:0.058, val_acc:0.981]
Epoch [76/120    avg_loss:0.039, val_acc:0.979]
Epoch [77/120    avg_loss:0.045, val_acc:0.981]
Epoch [78/120    avg_loss:0.036, val_acc:0.981]
Epoch [79/120    avg_loss:0.046, val_acc:0.983]
Epoch [80/120    avg_loss:0.036, val_acc:0.983]
Epoch [81/120    avg_loss:0.034, val_acc:0.981]
Epoch [82/120    avg_loss:0.034, val_acc:0.981]
Epoch [83/120    avg_loss:0.048, val_acc:0.981]
Epoch [84/120    avg_loss:0.045, val_acc:0.981]
Epoch [85/120    avg_loss:0.027, val_acc:0.981]
Epoch [86/120    avg_loss:0.059, val_acc:0.981]
Epoch [87/120    avg_loss:0.031, val_acc:0.981]
Epoch [88/120    avg_loss:0.036, val_acc:0.981]
Epoch [89/120    avg_loss:0.032, val_acc:0.981]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.034, val_acc:0.981]
Epoch [92/120    avg_loss:0.022, val_acc:0.981]
Epoch [93/120    avg_loss:0.039, val_acc:0.981]
Epoch [94/120    avg_loss:0.026, val_acc:0.981]
Epoch [95/120    avg_loss:0.058, val_acc:0.981]
Epoch [96/120    avg_loss:0.038, val_acc:0.981]
Epoch [97/120    avg_loss:0.040, val_acc:0.981]
Epoch [98/120    avg_loss:0.034, val_acc:0.981]
Epoch [99/120    avg_loss:0.021, val_acc:0.981]
Epoch [100/120    avg_loss:0.043, val_acc:0.981]
Epoch [101/120    avg_loss:0.030, val_acc:0.981]
Epoch [102/120    avg_loss:0.030, val_acc:0.981]
Epoch [103/120    avg_loss:0.038, val_acc:0.981]
Epoch [104/120    avg_loss:0.034, val_acc:0.981]
Epoch [105/120    avg_loss:0.029, val_acc:0.981]
Epoch [106/120    avg_loss:0.031, val_acc:0.981]
Epoch [107/120    avg_loss:0.045, val_acc:0.981]
Epoch [108/120    avg_loss:0.035, val_acc:0.981]
Epoch [109/120    avg_loss:0.044, val_acc:0.981]
Epoch [110/120    avg_loss:0.038, val_acc:0.981]
Epoch [111/120    avg_loss:0.035, val_acc:0.981]
Epoch [112/120    avg_loss:0.038, val_acc:0.981]
Epoch [113/120    avg_loss:0.043, val_acc:0.981]
Epoch [114/120    avg_loss:0.045, val_acc:0.981]
Epoch [115/120    avg_loss:0.024, val_acc:0.981]
Epoch [116/120    avg_loss:0.024, val_acc:0.981]
Epoch [117/120    avg_loss:0.055, val_acc:0.981]
Epoch [118/120    avg_loss:0.026, val_acc:0.981]
Epoch [119/120    avg_loss:0.042, val_acc:0.981]
Epoch [120/120    avg_loss:0.027, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   5   0   0   0   0   3   0]
 [  0   0   0 220   7   0   0   0   1   2   0   0   0   0]
 [  0   0   0   0 212  14   0   0   0   0   0   0   1   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.9569161  0.97777778 0.90987124 0.88028169
 1.         0.91208791 0.998713   0.9978678  1.         1.
 0.9956044  1.        ]

Kappa:
0.9848054465913423
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60345af780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.263, val_acc:0.596]
Epoch [2/120    avg_loss:1.554, val_acc:0.698]
Epoch [3/120    avg_loss:1.165, val_acc:0.738]
Epoch [4/120    avg_loss:0.998, val_acc:0.790]
Epoch [5/120    avg_loss:0.775, val_acc:0.835]
Epoch [6/120    avg_loss:0.668, val_acc:0.856]
Epoch [7/120    avg_loss:0.577, val_acc:0.877]
Epoch [8/120    avg_loss:0.597, val_acc:0.877]
Epoch [9/120    avg_loss:0.567, val_acc:0.873]
Epoch [10/120    avg_loss:0.593, val_acc:0.817]
Epoch [11/120    avg_loss:0.509, val_acc:0.887]
Epoch [12/120    avg_loss:0.437, val_acc:0.867]
Epoch [13/120    avg_loss:0.360, val_acc:0.917]
Epoch [14/120    avg_loss:0.318, val_acc:0.892]
Epoch [15/120    avg_loss:0.322, val_acc:0.925]
Epoch [16/120    avg_loss:0.322, val_acc:0.915]
Epoch [17/120    avg_loss:0.354, val_acc:0.906]
Epoch [18/120    avg_loss:0.399, val_acc:0.917]
Epoch [19/120    avg_loss:0.354, val_acc:0.919]
Epoch [20/120    avg_loss:0.282, val_acc:0.919]
Epoch [21/120    avg_loss:0.237, val_acc:0.933]
Epoch [22/120    avg_loss:0.287, val_acc:0.938]
Epoch [23/120    avg_loss:0.376, val_acc:0.921]
Epoch [24/120    avg_loss:0.367, val_acc:0.865]
Epoch [25/120    avg_loss:0.388, val_acc:0.944]
Epoch [26/120    avg_loss:0.238, val_acc:0.938]
Epoch [27/120    avg_loss:0.215, val_acc:0.944]
Epoch [28/120    avg_loss:0.256, val_acc:0.942]
Epoch [29/120    avg_loss:0.169, val_acc:0.956]
Epoch [30/120    avg_loss:0.166, val_acc:0.946]
Epoch [31/120    avg_loss:0.194, val_acc:0.931]
Epoch [32/120    avg_loss:0.139, val_acc:0.942]
Epoch [33/120    avg_loss:0.154, val_acc:0.946]
Epoch [34/120    avg_loss:0.136, val_acc:0.948]
Epoch [35/120    avg_loss:0.123, val_acc:0.967]
Epoch [36/120    avg_loss:0.115, val_acc:0.963]
Epoch [37/120    avg_loss:0.111, val_acc:0.967]
Epoch [38/120    avg_loss:0.139, val_acc:0.942]
Epoch [39/120    avg_loss:0.184, val_acc:0.971]
Epoch [40/120    avg_loss:0.123, val_acc:0.925]
Epoch [41/120    avg_loss:0.131, val_acc:0.960]
Epoch [42/120    avg_loss:0.122, val_acc:0.969]
Epoch [43/120    avg_loss:0.097, val_acc:0.958]
Epoch [44/120    avg_loss:0.105, val_acc:0.958]
Epoch [45/120    avg_loss:0.235, val_acc:0.942]
Epoch [46/120    avg_loss:0.254, val_acc:0.946]
Epoch [47/120    avg_loss:0.186, val_acc:0.958]
Epoch [48/120    avg_loss:0.112, val_acc:0.958]
Epoch [49/120    avg_loss:0.120, val_acc:0.967]
Epoch [50/120    avg_loss:0.112, val_acc:0.965]
Epoch [51/120    avg_loss:0.122, val_acc:0.954]
Epoch [52/120    avg_loss:0.158, val_acc:0.965]
Epoch [53/120    avg_loss:0.067, val_acc:0.969]
Epoch [54/120    avg_loss:0.078, val_acc:0.973]
Epoch [55/120    avg_loss:0.053, val_acc:0.975]
Epoch [56/120    avg_loss:0.055, val_acc:0.973]
Epoch [57/120    avg_loss:0.053, val_acc:0.973]
Epoch [58/120    avg_loss:0.061, val_acc:0.975]
Epoch [59/120    avg_loss:0.065, val_acc:0.973]
Epoch [60/120    avg_loss:0.071, val_acc:0.973]
Epoch [61/120    avg_loss:0.048, val_acc:0.973]
Epoch [62/120    avg_loss:0.040, val_acc:0.971]
Epoch [63/120    avg_loss:0.048, val_acc:0.973]
Epoch [64/120    avg_loss:0.046, val_acc:0.973]
Epoch [65/120    avg_loss:0.050, val_acc:0.971]
Epoch [66/120    avg_loss:0.053, val_acc:0.971]
Epoch [67/120    avg_loss:0.053, val_acc:0.973]
Epoch [68/120    avg_loss:0.059, val_acc:0.973]
Epoch [69/120    avg_loss:0.033, val_acc:0.971]
Epoch [70/120    avg_loss:0.039, val_acc:0.971]
Epoch [71/120    avg_loss:0.042, val_acc:0.975]
Epoch [72/120    avg_loss:0.036, val_acc:0.973]
Epoch [73/120    avg_loss:0.042, val_acc:0.973]
Epoch [74/120    avg_loss:0.050, val_acc:0.973]
Epoch [75/120    avg_loss:0.041, val_acc:0.971]
Epoch [76/120    avg_loss:0.042, val_acc:0.973]
Epoch [77/120    avg_loss:0.033, val_acc:0.975]
Epoch [78/120    avg_loss:0.056, val_acc:0.973]
Epoch [79/120    avg_loss:0.044, val_acc:0.973]
Epoch [80/120    avg_loss:0.063, val_acc:0.979]
Epoch [81/120    avg_loss:0.052, val_acc:0.979]
Epoch [82/120    avg_loss:0.041, val_acc:0.977]
Epoch [83/120    avg_loss:0.058, val_acc:0.975]
Epoch [84/120    avg_loss:0.034, val_acc:0.973]
Epoch [85/120    avg_loss:0.041, val_acc:0.973]
Epoch [86/120    avg_loss:0.049, val_acc:0.973]
Epoch [87/120    avg_loss:0.037, val_acc:0.971]
Epoch [88/120    avg_loss:0.055, val_acc:0.975]
Epoch [89/120    avg_loss:0.041, val_acc:0.975]
Epoch [90/120    avg_loss:0.051, val_acc:0.975]
Epoch [91/120    avg_loss:0.034, val_acc:0.975]
Epoch [92/120    avg_loss:0.042, val_acc:0.977]
Epoch [93/120    avg_loss:0.034, val_acc:0.977]
Epoch [94/120    avg_loss:0.035, val_acc:0.975]
Epoch [95/120    avg_loss:0.037, val_acc:0.975]
Epoch [96/120    avg_loss:0.031, val_acc:0.977]
Epoch [97/120    avg_loss:0.045, val_acc:0.979]
Epoch [98/120    avg_loss:0.027, val_acc:0.979]
Epoch [99/120    avg_loss:0.048, val_acc:0.977]
Epoch [100/120    avg_loss:0.037, val_acc:0.979]
Epoch [101/120    avg_loss:0.029, val_acc:0.979]
Epoch [102/120    avg_loss:0.040, val_acc:0.979]
Epoch [103/120    avg_loss:0.033, val_acc:0.979]
Epoch [104/120    avg_loss:0.033, val_acc:0.979]
Epoch [105/120    avg_loss:0.031, val_acc:0.977]
Epoch [106/120    avg_loss:0.046, val_acc:0.977]
Epoch [107/120    avg_loss:0.045, val_acc:0.977]
Epoch [108/120    avg_loss:0.030, val_acc:0.979]
Epoch [109/120    avg_loss:0.029, val_acc:0.979]
Epoch [110/120    avg_loss:0.031, val_acc:0.979]
Epoch [111/120    avg_loss:0.035, val_acc:0.977]
Epoch [112/120    avg_loss:0.035, val_acc:0.977]
Epoch [113/120    avg_loss:0.036, val_acc:0.975]
Epoch [114/120    avg_loss:0.031, val_acc:0.975]
Epoch [115/120    avg_loss:0.038, val_acc:0.975]
Epoch [116/120    avg_loss:0.035, val_acc:0.977]
Epoch [117/120    avg_loss:0.032, val_acc:0.977]
Epoch [118/120    avg_loss:0.051, val_acc:0.977]
Epoch [119/120    avg_loss:0.054, val_acc:0.973]
Epoch [120/120    avg_loss:0.046, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0   8   0   0   0   0   2   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 0.99927061 0.9543379  0.98678414 0.92827004 0.90181818
 0.99756691 0.93193717 1.         1.         1.         0.99867198
 0.99005525 1.        ]

Kappa:
0.9867060221861224
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff970ded7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.272, val_acc:0.448]
Epoch [2/120    avg_loss:1.562, val_acc:0.667]
Epoch [3/120    avg_loss:1.186, val_acc:0.777]
Epoch [4/120    avg_loss:0.972, val_acc:0.804]
Epoch [5/120    avg_loss:0.920, val_acc:0.777]
Epoch [6/120    avg_loss:0.778, val_acc:0.829]
Epoch [7/120    avg_loss:0.631, val_acc:0.794]
Epoch [8/120    avg_loss:0.561, val_acc:0.835]
Epoch [9/120    avg_loss:0.497, val_acc:0.896]
Epoch [10/120    avg_loss:0.444, val_acc:0.908]
Epoch [11/120    avg_loss:0.392, val_acc:0.896]
Epoch [12/120    avg_loss:0.416, val_acc:0.856]
Epoch [13/120    avg_loss:0.396, val_acc:0.902]
Epoch [14/120    avg_loss:0.350, val_acc:0.925]
Epoch [15/120    avg_loss:0.338, val_acc:0.831]
Epoch [16/120    avg_loss:0.360, val_acc:0.921]
Epoch [17/120    avg_loss:0.329, val_acc:0.931]
Epoch [18/120    avg_loss:0.254, val_acc:0.925]
Epoch [19/120    avg_loss:0.268, val_acc:0.908]
Epoch [20/120    avg_loss:0.325, val_acc:0.896]
Epoch [21/120    avg_loss:0.295, val_acc:0.919]
Epoch [22/120    avg_loss:0.284, val_acc:0.927]
Epoch [23/120    avg_loss:0.183, val_acc:0.940]
Epoch [24/120    avg_loss:0.223, val_acc:0.956]
Epoch [25/120    avg_loss:0.243, val_acc:0.938]
Epoch [26/120    avg_loss:0.244, val_acc:0.935]
Epoch [27/120    avg_loss:0.204, val_acc:0.919]
Epoch [28/120    avg_loss:0.178, val_acc:0.958]
Epoch [29/120    avg_loss:0.166, val_acc:0.940]
Epoch [30/120    avg_loss:0.209, val_acc:0.954]
Epoch [31/120    avg_loss:0.192, val_acc:0.919]
Epoch [32/120    avg_loss:0.154, val_acc:0.969]
Epoch [33/120    avg_loss:0.115, val_acc:0.967]
Epoch [34/120    avg_loss:0.130, val_acc:0.958]
Epoch [35/120    avg_loss:0.171, val_acc:0.954]
Epoch [36/120    avg_loss:0.147, val_acc:0.956]
Epoch [37/120    avg_loss:0.166, val_acc:0.973]
Epoch [38/120    avg_loss:0.231, val_acc:0.956]
Epoch [39/120    avg_loss:0.162, val_acc:0.952]
Epoch [40/120    avg_loss:0.140, val_acc:0.952]
Epoch [41/120    avg_loss:0.135, val_acc:0.950]
Epoch [42/120    avg_loss:0.136, val_acc:0.952]
Epoch [43/120    avg_loss:0.103, val_acc:0.960]
Epoch [44/120    avg_loss:0.100, val_acc:0.973]
Epoch [45/120    avg_loss:0.052, val_acc:0.977]
Epoch [46/120    avg_loss:0.061, val_acc:0.981]
Epoch [47/120    avg_loss:0.151, val_acc:0.977]
Epoch [48/120    avg_loss:0.073, val_acc:0.977]
Epoch [49/120    avg_loss:0.078, val_acc:0.977]
Epoch [50/120    avg_loss:0.080, val_acc:0.973]
Epoch [51/120    avg_loss:0.070, val_acc:0.965]
Epoch [52/120    avg_loss:0.068, val_acc:0.967]
Epoch [53/120    avg_loss:0.075, val_acc:0.958]
Epoch [54/120    avg_loss:0.181, val_acc:0.948]
Epoch [55/120    avg_loss:0.145, val_acc:0.975]
Epoch [56/120    avg_loss:0.128, val_acc:0.973]
Epoch [57/120    avg_loss:0.091, val_acc:0.950]
Epoch [58/120    avg_loss:0.064, val_acc:0.975]
Epoch [59/120    avg_loss:0.044, val_acc:0.977]
Epoch [60/120    avg_loss:0.040, val_acc:0.977]
Epoch [61/120    avg_loss:0.035, val_acc:0.981]
Epoch [62/120    avg_loss:0.051, val_acc:0.981]
Epoch [63/120    avg_loss:0.045, val_acc:0.981]
Epoch [64/120    avg_loss:0.030, val_acc:0.981]
Epoch [65/120    avg_loss:0.041, val_acc:0.981]
Epoch [66/120    avg_loss:0.030, val_acc:0.981]
Epoch [67/120    avg_loss:0.029, val_acc:0.981]
Epoch [68/120    avg_loss:0.030, val_acc:0.981]
Epoch [69/120    avg_loss:0.024, val_acc:0.981]
Epoch [70/120    avg_loss:0.025, val_acc:0.981]
Epoch [71/120    avg_loss:0.035, val_acc:0.983]
Epoch [72/120    avg_loss:0.044, val_acc:0.985]
Epoch [73/120    avg_loss:0.031, val_acc:0.985]
Epoch [74/120    avg_loss:0.038, val_acc:0.983]
Epoch [75/120    avg_loss:0.038, val_acc:0.981]
Epoch [76/120    avg_loss:0.026, val_acc:0.981]
Epoch [77/120    avg_loss:0.023, val_acc:0.981]
Epoch [78/120    avg_loss:0.032, val_acc:0.981]
Epoch [79/120    avg_loss:0.029, val_acc:0.981]
Epoch [80/120    avg_loss:0.027, val_acc:0.983]
Epoch [81/120    avg_loss:0.045, val_acc:0.981]
Epoch [82/120    avg_loss:0.027, val_acc:0.981]
Epoch [83/120    avg_loss:0.032, val_acc:0.977]
Epoch [84/120    avg_loss:0.033, val_acc:0.979]
Epoch [85/120    avg_loss:0.024, val_acc:0.979]
Epoch [86/120    avg_loss:0.036, val_acc:0.979]
Epoch [87/120    avg_loss:0.030, val_acc:0.983]
Epoch [88/120    avg_loss:0.028, val_acc:0.981]
Epoch [89/120    avg_loss:0.026, val_acc:0.981]
Epoch [90/120    avg_loss:0.031, val_acc:0.979]
Epoch [91/120    avg_loss:0.030, val_acc:0.981]
Epoch [92/120    avg_loss:0.043, val_acc:0.981]
Epoch [93/120    avg_loss:0.038, val_acc:0.979]
Epoch [94/120    avg_loss:0.034, val_acc:0.979]
Epoch [95/120    avg_loss:0.027, val_acc:0.979]
Epoch [96/120    avg_loss:0.025, val_acc:0.979]
Epoch [97/120    avg_loss:0.039, val_acc:0.979]
Epoch [98/120    avg_loss:0.026, val_acc:0.979]
Epoch [99/120    avg_loss:0.023, val_acc:0.979]
Epoch [100/120    avg_loss:0.025, val_acc:0.979]
Epoch [101/120    avg_loss:0.037, val_acc:0.979]
Epoch [102/120    avg_loss:0.034, val_acc:0.979]
Epoch [103/120    avg_loss:0.030, val_acc:0.979]
Epoch [104/120    avg_loss:0.056, val_acc:0.979]
Epoch [105/120    avg_loss:0.042, val_acc:0.979]
Epoch [106/120    avg_loss:0.034, val_acc:0.979]
Epoch [107/120    avg_loss:0.043, val_acc:0.979]
Epoch [108/120    avg_loss:0.038, val_acc:0.979]
Epoch [109/120    avg_loss:0.020, val_acc:0.979]
Epoch [110/120    avg_loss:0.036, val_acc:0.979]
Epoch [111/120    avg_loss:0.039, val_acc:0.979]
Epoch [112/120    avg_loss:0.023, val_acc:0.979]
Epoch [113/120    avg_loss:0.028, val_acc:0.979]
Epoch [114/120    avg_loss:0.033, val_acc:0.979]
Epoch [115/120    avg_loss:0.025, val_acc:0.979]
Epoch [116/120    avg_loss:0.023, val_acc:0.979]
Epoch [117/120    avg_loss:0.034, val_acc:0.979]
Epoch [118/120    avg_loss:0.025, val_acc:0.979]
Epoch [119/120    avg_loss:0.027, val_acc:0.979]
Epoch [120/120    avg_loss:0.027, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   5   0   0   0   0   3   0]
 [  0   0   0 222   4   0   0   0   2   2   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.95475113 0.98230088 0.92537313 0.88888889
 1.         0.93548387 0.99742931 0.9978678  1.         1.
 0.99115044 1.        ]

Kappa:
0.9859934562901788
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f335514a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 180672==>0.18M
----------Training process----------
Epoch [1/120    avg_loss:2.328, val_acc:0.542]
Epoch [2/120    avg_loss:1.651, val_acc:0.667]
Epoch [3/120    avg_loss:1.278, val_acc:0.762]
Epoch [4/120    avg_loss:1.078, val_acc:0.717]
Epoch [5/120    avg_loss:0.851, val_acc:0.835]
Epoch [6/120    avg_loss:0.821, val_acc:0.777]
Epoch [7/120    avg_loss:0.644, val_acc:0.879]
Epoch [8/120    avg_loss:0.545, val_acc:0.867]
Epoch [9/120    avg_loss:0.523, val_acc:0.890]
Epoch [10/120    avg_loss:0.484, val_acc:0.906]
Epoch [11/120    avg_loss:0.394, val_acc:0.879]
Epoch [12/120    avg_loss:0.405, val_acc:0.912]
Epoch [13/120    avg_loss:0.308, val_acc:0.925]
Epoch [14/120    avg_loss:0.298, val_acc:0.935]
Epoch [15/120    avg_loss:0.243, val_acc:0.910]
Epoch [16/120    avg_loss:0.298, val_acc:0.931]
Epoch [17/120    avg_loss:0.251, val_acc:0.931]
Epoch [18/120    avg_loss:0.235, val_acc:0.892]
Epoch [19/120    avg_loss:0.287, val_acc:0.942]
Epoch [20/120    avg_loss:0.293, val_acc:0.929]
Epoch [21/120    avg_loss:0.275, val_acc:0.938]
Epoch [22/120    avg_loss:0.237, val_acc:0.946]
Epoch [23/120    avg_loss:0.183, val_acc:0.942]
Epoch [24/120    avg_loss:0.242, val_acc:0.958]
Epoch [25/120    avg_loss:0.202, val_acc:0.948]
Epoch [26/120    avg_loss:0.244, val_acc:0.938]
Epoch [27/120    avg_loss:0.215, val_acc:0.960]
Epoch [28/120    avg_loss:0.234, val_acc:0.950]
Epoch [29/120    avg_loss:0.210, val_acc:0.931]
Epoch [30/120    avg_loss:0.198, val_acc:0.965]
Epoch [31/120    avg_loss:0.147, val_acc:0.929]
Epoch [32/120    avg_loss:0.204, val_acc:0.963]
Epoch [33/120    avg_loss:0.182, val_acc:0.940]
Epoch [34/120    avg_loss:0.168, val_acc:0.956]
Epoch [35/120    avg_loss:0.135, val_acc:0.958]
Epoch [36/120    avg_loss:0.124, val_acc:0.967]
Epoch [37/120    avg_loss:0.189, val_acc:0.960]
Epoch [38/120    avg_loss:0.175, val_acc:0.958]
Epoch [39/120    avg_loss:0.132, val_acc:0.944]
Epoch [40/120    avg_loss:0.151, val_acc:0.975]
Epoch [41/120    avg_loss:0.128, val_acc:0.958]
Epoch [42/120    avg_loss:0.181, val_acc:0.954]
Epoch [43/120    avg_loss:0.122, val_acc:0.969]
Epoch [44/120    avg_loss:0.135, val_acc:0.973]
Epoch [45/120    avg_loss:0.112, val_acc:0.958]
Epoch [46/120    avg_loss:0.129, val_acc:0.954]
Epoch [47/120    avg_loss:0.130, val_acc:0.967]
Epoch [48/120    avg_loss:0.093, val_acc:0.956]
Epoch [49/120    avg_loss:0.092, val_acc:0.967]
Epoch [50/120    avg_loss:0.082, val_acc:0.960]
Epoch [51/120    avg_loss:0.121, val_acc:0.971]
Epoch [52/120    avg_loss:0.114, val_acc:0.954]
Epoch [53/120    avg_loss:0.136, val_acc:0.981]
Epoch [54/120    avg_loss:0.133, val_acc:0.965]
Epoch [55/120    avg_loss:0.090, val_acc:0.969]
Epoch [56/120    avg_loss:0.091, val_acc:0.990]
Epoch [57/120    avg_loss:0.057, val_acc:0.973]
Epoch [58/120    avg_loss:0.092, val_acc:0.973]
Epoch [59/120    avg_loss:0.118, val_acc:0.952]
Epoch [60/120    avg_loss:0.077, val_acc:0.969]
Epoch [61/120    avg_loss:0.074, val_acc:0.960]
Epoch [62/120    avg_loss:0.079, val_acc:0.975]
Epoch [63/120    avg_loss:0.081, val_acc:0.983]
Epoch [64/120    avg_loss:0.073, val_acc:0.977]
Epoch [65/120    avg_loss:0.064, val_acc:0.981]
Epoch [66/120    avg_loss:0.039, val_acc:0.977]
Epoch [67/120    avg_loss:0.046, val_acc:0.981]
Epoch [68/120    avg_loss:0.039, val_acc:0.975]
Epoch [69/120    avg_loss:0.051, val_acc:0.973]
Epoch [70/120    avg_loss:0.053, val_acc:0.971]
Epoch [71/120    avg_loss:0.059, val_acc:0.979]
Epoch [72/120    avg_loss:0.023, val_acc:0.979]
Epoch [73/120    avg_loss:0.027, val_acc:0.977]
Epoch [74/120    avg_loss:0.034, val_acc:0.981]
Epoch [75/120    avg_loss:0.040, val_acc:0.981]
Epoch [76/120    avg_loss:0.030, val_acc:0.979]
Epoch [77/120    avg_loss:0.026, val_acc:0.979]
Epoch [78/120    avg_loss:0.031, val_acc:0.981]
Epoch [79/120    avg_loss:0.025, val_acc:0.979]
Epoch [80/120    avg_loss:0.034, val_acc:0.981]
Epoch [81/120    avg_loss:0.030, val_acc:0.979]
Epoch [82/120    avg_loss:0.035, val_acc:0.983]
Epoch [83/120    avg_loss:0.026, val_acc:0.983]
Epoch [84/120    avg_loss:0.033, val_acc:0.983]
Epoch [85/120    avg_loss:0.024, val_acc:0.983]
Epoch [86/120    avg_loss:0.022, val_acc:0.983]
Epoch [87/120    avg_loss:0.045, val_acc:0.983]
Epoch [88/120    avg_loss:0.028, val_acc:0.983]
Epoch [89/120    avg_loss:0.031, val_acc:0.983]
Epoch [90/120    avg_loss:0.029, val_acc:0.983]
Epoch [91/120    avg_loss:0.029, val_acc:0.983]
Epoch [92/120    avg_loss:0.024, val_acc:0.983]
Epoch [93/120    avg_loss:0.027, val_acc:0.983]
Epoch [94/120    avg_loss:0.022, val_acc:0.983]
Epoch [95/120    avg_loss:0.025, val_acc:0.983]
Epoch [96/120    avg_loss:0.024, val_acc:0.983]
Epoch [97/120    avg_loss:0.026, val_acc:0.983]
Epoch [98/120    avg_loss:0.025, val_acc:0.983]
Epoch [99/120    avg_loss:0.024, val_acc:0.983]
Epoch [100/120    avg_loss:0.028, val_acc:0.983]
Epoch [101/120    avg_loss:0.021, val_acc:0.983]
Epoch [102/120    avg_loss:0.023, val_acc:0.983]
Epoch [103/120    avg_loss:0.028, val_acc:0.983]
Epoch [104/120    avg_loss:0.022, val_acc:0.983]
Epoch [105/120    avg_loss:0.039, val_acc:0.983]
Epoch [106/120    avg_loss:0.030, val_acc:0.983]
Epoch [107/120    avg_loss:0.032, val_acc:0.983]
Epoch [108/120    avg_loss:0.026, val_acc:0.983]
Epoch [109/120    avg_loss:0.030, val_acc:0.983]
Epoch [110/120    avg_loss:0.026, val_acc:0.983]
Epoch [111/120    avg_loss:0.023, val_acc:0.983]
Epoch [112/120    avg_loss:0.027, val_acc:0.983]
Epoch [113/120    avg_loss:0.023, val_acc:0.983]
Epoch [114/120    avg_loss:0.023, val_acc:0.983]
Epoch [115/120    avg_loss:0.029, val_acc:0.983]
Epoch [116/120    avg_loss:0.033, val_acc:0.983]
Epoch [117/120    avg_loss:0.020, val_acc:0.983]
Epoch [118/120    avg_loss:0.033, val_acc:0.983]
Epoch [119/120    avg_loss:0.022, val_acc:0.983]
Epoch [120/120    avg_loss:0.018, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   4   0   0   0   0   3   0]
 [  0   0   0 219   7   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   1   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 0.99708879 0.95280899 0.97550111 0.93939394 0.92733564
 0.99019608 0.91803279 0.99487179 1.         1.         1.
 0.99115044 1.        ]

Kappa:
0.9862299478234388
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35b4a80710>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.314, val_acc:0.631]
Epoch [2/120    avg_loss:1.651, val_acc:0.635]
Epoch [3/120    avg_loss:1.267, val_acc:0.733]
Epoch [4/120    avg_loss:0.942, val_acc:0.817]
Epoch [5/120    avg_loss:0.823, val_acc:0.846]
Epoch [6/120    avg_loss:0.703, val_acc:0.877]
Epoch [7/120    avg_loss:0.708, val_acc:0.848]
Epoch [8/120    avg_loss:0.569, val_acc:0.923]
Epoch [9/120    avg_loss:0.478, val_acc:0.885]
Epoch [10/120    avg_loss:0.397, val_acc:0.910]
Epoch [11/120    avg_loss:0.391, val_acc:0.873]
Epoch [12/120    avg_loss:0.424, val_acc:0.933]
Epoch [13/120    avg_loss:0.383, val_acc:0.927]
Epoch [14/120    avg_loss:0.294, val_acc:0.948]
Epoch [15/120    avg_loss:0.345, val_acc:0.923]
Epoch [16/120    avg_loss:0.338, val_acc:0.944]
Epoch [17/120    avg_loss:0.242, val_acc:0.946]
Epoch [18/120    avg_loss:0.281, val_acc:0.950]
Epoch [19/120    avg_loss:0.261, val_acc:0.950]
Epoch [20/120    avg_loss:0.179, val_acc:0.952]
Epoch [21/120    avg_loss:0.193, val_acc:0.967]
Epoch [22/120    avg_loss:0.212, val_acc:0.963]
Epoch [23/120    avg_loss:0.180, val_acc:0.948]
Epoch [24/120    avg_loss:0.191, val_acc:0.956]
Epoch [25/120    avg_loss:0.165, val_acc:0.977]
Epoch [26/120    avg_loss:0.144, val_acc:0.965]
Epoch [27/120    avg_loss:0.167, val_acc:0.963]
Epoch [28/120    avg_loss:0.142, val_acc:0.969]
Epoch [29/120    avg_loss:0.173, val_acc:0.977]
Epoch [30/120    avg_loss:0.144, val_acc:0.971]
Epoch [31/120    avg_loss:0.202, val_acc:0.954]
Epoch [32/120    avg_loss:0.202, val_acc:0.946]
Epoch [33/120    avg_loss:0.220, val_acc:0.950]
Epoch [34/120    avg_loss:0.141, val_acc:0.971]
Epoch [35/120    avg_loss:0.112, val_acc:0.975]
Epoch [36/120    avg_loss:0.121, val_acc:0.946]
Epoch [37/120    avg_loss:0.198, val_acc:0.965]
Epoch [38/120    avg_loss:0.129, val_acc:0.973]
Epoch [39/120    avg_loss:0.137, val_acc:0.969]
Epoch [40/120    avg_loss:0.143, val_acc:0.963]
Epoch [41/120    avg_loss:0.096, val_acc:0.963]
Epoch [42/120    avg_loss:0.114, val_acc:0.965]
Epoch [43/120    avg_loss:0.070, val_acc:0.971]
Epoch [44/120    avg_loss:0.062, val_acc:0.971]
Epoch [45/120    avg_loss:0.046, val_acc:0.975]
Epoch [46/120    avg_loss:0.060, val_acc:0.971]
Epoch [47/120    avg_loss:0.062, val_acc:0.971]
Epoch [48/120    avg_loss:0.054, val_acc:0.971]
Epoch [49/120    avg_loss:0.046, val_acc:0.973]
Epoch [50/120    avg_loss:0.059, val_acc:0.973]
Epoch [51/120    avg_loss:0.051, val_acc:0.975]
Epoch [52/120    avg_loss:0.038, val_acc:0.973]
Epoch [53/120    avg_loss:0.051, val_acc:0.971]
Epoch [54/120    avg_loss:0.049, val_acc:0.969]
Epoch [55/120    avg_loss:0.037, val_acc:0.971]
Epoch [56/120    avg_loss:0.028, val_acc:0.971]
Epoch [57/120    avg_loss:0.040, val_acc:0.971]
Epoch [58/120    avg_loss:0.046, val_acc:0.971]
Epoch [59/120    avg_loss:0.036, val_acc:0.971]
Epoch [60/120    avg_loss:0.043, val_acc:0.971]
Epoch [61/120    avg_loss:0.035, val_acc:0.971]
Epoch [62/120    avg_loss:0.050, val_acc:0.971]
Epoch [63/120    avg_loss:0.033, val_acc:0.971]
Epoch [64/120    avg_loss:0.037, val_acc:0.971]
Epoch [65/120    avg_loss:0.057, val_acc:0.971]
Epoch [66/120    avg_loss:0.041, val_acc:0.971]
Epoch [67/120    avg_loss:0.037, val_acc:0.971]
Epoch [68/120    avg_loss:0.039, val_acc:0.971]
Epoch [69/120    avg_loss:0.037, val_acc:0.971]
Epoch [70/120    avg_loss:0.046, val_acc:0.971]
Epoch [71/120    avg_loss:0.041, val_acc:0.971]
Epoch [72/120    avg_loss:0.047, val_acc:0.971]
Epoch [73/120    avg_loss:0.053, val_acc:0.971]
Epoch [74/120    avg_loss:0.032, val_acc:0.971]
Epoch [75/120    avg_loss:0.032, val_acc:0.971]
Epoch [76/120    avg_loss:0.041, val_acc:0.971]
Epoch [77/120    avg_loss:0.052, val_acc:0.971]
Epoch [78/120    avg_loss:0.040, val_acc:0.971]
Epoch [79/120    avg_loss:0.051, val_acc:0.971]
Epoch [80/120    avg_loss:0.053, val_acc:0.971]
Epoch [81/120    avg_loss:0.034, val_acc:0.971]
Epoch [82/120    avg_loss:0.036, val_acc:0.971]
Epoch [83/120    avg_loss:0.033, val_acc:0.971]
Epoch [84/120    avg_loss:0.049, val_acc:0.971]
Epoch [85/120    avg_loss:0.044, val_acc:0.971]
Epoch [86/120    avg_loss:0.040, val_acc:0.971]
Epoch [87/120    avg_loss:0.044, val_acc:0.971]
Epoch [88/120    avg_loss:0.046, val_acc:0.971]
Epoch [89/120    avg_loss:0.038, val_acc:0.971]
Epoch [90/120    avg_loss:0.050, val_acc:0.971]
Epoch [91/120    avg_loss:0.058, val_acc:0.971]
Epoch [92/120    avg_loss:0.050, val_acc:0.971]
Epoch [93/120    avg_loss:0.039, val_acc:0.971]
Epoch [94/120    avg_loss:0.041, val_acc:0.971]
Epoch [95/120    avg_loss:0.040, val_acc:0.971]
Epoch [96/120    avg_loss:0.041, val_acc:0.971]
Epoch [97/120    avg_loss:0.057, val_acc:0.971]
Epoch [98/120    avg_loss:0.039, val_acc:0.971]
Epoch [99/120    avg_loss:0.048, val_acc:0.971]
Epoch [100/120    avg_loss:0.047, val_acc:0.971]
Epoch [101/120    avg_loss:0.046, val_acc:0.971]
Epoch [102/120    avg_loss:0.040, val_acc:0.971]
Epoch [103/120    avg_loss:0.038, val_acc:0.971]
Epoch [104/120    avg_loss:0.040, val_acc:0.971]
Epoch [105/120    avg_loss:0.073, val_acc:0.971]
Epoch [106/120    avg_loss:0.034, val_acc:0.971]
Epoch [107/120    avg_loss:0.050, val_acc:0.971]
Epoch [108/120    avg_loss:0.044, val_acc:0.971]
Epoch [109/120    avg_loss:0.034, val_acc:0.971]
Epoch [110/120    avg_loss:0.041, val_acc:0.971]
Epoch [111/120    avg_loss:0.032, val_acc:0.971]
Epoch [112/120    avg_loss:0.042, val_acc:0.971]
Epoch [113/120    avg_loss:0.055, val_acc:0.971]
Epoch [114/120    avg_loss:0.044, val_acc:0.971]
Epoch [115/120    avg_loss:0.040, val_acc:0.971]
Epoch [116/120    avg_loss:0.031, val_acc:0.971]
Epoch [117/120    avg_loss:0.034, val_acc:0.971]
Epoch [118/120    avg_loss:0.039, val_acc:0.971]
Epoch [119/120    avg_loss:0.068, val_acc:0.971]
Epoch [120/120    avg_loss:0.060, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 201  29   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  21 432   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.88912579957356

F1 scores:
[       nan 0.99636364 0.96846847 0.93271462 0.87733888 0.90344828
 0.98771499 0.92307692 1.         1.         1.         0.97290323
 0.9740699  1.        ]

Kappa:
0.9764969590969843
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff157f467b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.228, val_acc:0.554]
Epoch [2/120    avg_loss:1.627, val_acc:0.671]
Epoch [3/120    avg_loss:1.261, val_acc:0.719]
Epoch [4/120    avg_loss:1.010, val_acc:0.840]
Epoch [5/120    avg_loss:0.814, val_acc:0.846]
Epoch [6/120    avg_loss:0.678, val_acc:0.890]
Epoch [7/120    avg_loss:0.505, val_acc:0.896]
Epoch [8/120    avg_loss:0.465, val_acc:0.879]
Epoch [9/120    avg_loss:0.485, val_acc:0.892]
Epoch [10/120    avg_loss:0.359, val_acc:0.908]
Epoch [11/120    avg_loss:0.347, val_acc:0.923]
Epoch [12/120    avg_loss:0.291, val_acc:0.906]
Epoch [13/120    avg_loss:0.393, val_acc:0.927]
Epoch [14/120    avg_loss:0.334, val_acc:0.933]
Epoch [15/120    avg_loss:0.355, val_acc:0.892]
Epoch [16/120    avg_loss:0.365, val_acc:0.940]
Epoch [17/120    avg_loss:0.293, val_acc:0.929]
Epoch [18/120    avg_loss:0.202, val_acc:0.956]
Epoch [19/120    avg_loss:0.247, val_acc:0.927]
Epoch [20/120    avg_loss:0.207, val_acc:0.946]
Epoch [21/120    avg_loss:0.224, val_acc:0.931]
Epoch [22/120    avg_loss:0.240, val_acc:0.963]
Epoch [23/120    avg_loss:0.199, val_acc:0.952]
Epoch [24/120    avg_loss:0.186, val_acc:0.956]
Epoch [25/120    avg_loss:0.181, val_acc:0.971]
Epoch [26/120    avg_loss:0.140, val_acc:0.956]
Epoch [27/120    avg_loss:0.177, val_acc:0.948]
Epoch [28/120    avg_loss:0.194, val_acc:0.950]
Epoch [29/120    avg_loss:0.177, val_acc:0.973]
Epoch [30/120    avg_loss:0.127, val_acc:0.940]
Epoch [31/120    avg_loss:0.206, val_acc:0.960]
Epoch [32/120    avg_loss:0.171, val_acc:0.965]
Epoch [33/120    avg_loss:0.154, val_acc:0.969]
Epoch [34/120    avg_loss:0.138, val_acc:0.965]
Epoch [35/120    avg_loss:0.115, val_acc:0.977]
Epoch [36/120    avg_loss:0.115, val_acc:0.971]
Epoch [37/120    avg_loss:0.118, val_acc:0.969]
Epoch [38/120    avg_loss:0.129, val_acc:0.973]
Epoch [39/120    avg_loss:0.108, val_acc:0.971]
Epoch [40/120    avg_loss:0.126, val_acc:0.973]
Epoch [41/120    avg_loss:0.102, val_acc:0.971]
Epoch [42/120    avg_loss:0.083, val_acc:0.975]
Epoch [43/120    avg_loss:0.081, val_acc:0.973]
Epoch [44/120    avg_loss:0.082, val_acc:0.985]
Epoch [45/120    avg_loss:0.078, val_acc:0.983]
Epoch [46/120    avg_loss:0.067, val_acc:0.969]
Epoch [47/120    avg_loss:0.133, val_acc:0.948]
Epoch [48/120    avg_loss:0.190, val_acc:0.956]
Epoch [49/120    avg_loss:0.201, val_acc:0.933]
Epoch [50/120    avg_loss:0.148, val_acc:0.956]
Epoch [51/120    avg_loss:0.100, val_acc:0.979]
Epoch [52/120    avg_loss:0.094, val_acc:0.985]
Epoch [53/120    avg_loss:0.073, val_acc:0.975]
Epoch [54/120    avg_loss:0.058, val_acc:0.979]
Epoch [55/120    avg_loss:0.047, val_acc:0.981]
Epoch [56/120    avg_loss:0.079, val_acc:0.988]
Epoch [57/120    avg_loss:0.045, val_acc:0.983]
Epoch [58/120    avg_loss:0.027, val_acc:0.971]
Epoch [59/120    avg_loss:0.051, val_acc:0.975]
Epoch [60/120    avg_loss:0.057, val_acc:0.979]
Epoch [61/120    avg_loss:0.112, val_acc:0.977]
Epoch [62/120    avg_loss:0.093, val_acc:0.973]
Epoch [63/120    avg_loss:0.084, val_acc:0.975]
Epoch [64/120    avg_loss:0.095, val_acc:0.960]
Epoch [65/120    avg_loss:0.072, val_acc:0.977]
Epoch [66/120    avg_loss:0.052, val_acc:0.983]
Epoch [67/120    avg_loss:0.038, val_acc:0.990]
Epoch [68/120    avg_loss:0.025, val_acc:0.981]
Epoch [69/120    avg_loss:0.022, val_acc:0.992]
Epoch [70/120    avg_loss:0.045, val_acc:0.990]
Epoch [71/120    avg_loss:0.036, val_acc:0.988]
Epoch [72/120    avg_loss:0.029, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.990]
Epoch [74/120    avg_loss:0.020, val_acc:0.992]
Epoch [75/120    avg_loss:0.025, val_acc:0.990]
Epoch [76/120    avg_loss:0.124, val_acc:0.952]
Epoch [77/120    avg_loss:0.092, val_acc:0.969]
Epoch [78/120    avg_loss:0.059, val_acc:0.985]
Epoch [79/120    avg_loss:0.058, val_acc:0.985]
Epoch [80/120    avg_loss:0.049, val_acc:0.983]
Epoch [81/120    avg_loss:0.030, val_acc:0.988]
Epoch [82/120    avg_loss:0.044, val_acc:0.983]
Epoch [83/120    avg_loss:0.037, val_acc:0.985]
Epoch [84/120    avg_loss:0.033, val_acc:0.979]
Epoch [85/120    avg_loss:0.050, val_acc:0.979]
Epoch [86/120    avg_loss:0.030, val_acc:0.990]
Epoch [87/120    avg_loss:0.035, val_acc:0.990]
Epoch [88/120    avg_loss:0.021, val_acc:0.990]
Epoch [89/120    avg_loss:0.014, val_acc:0.990]
Epoch [90/120    avg_loss:0.024, val_acc:0.992]
Epoch [91/120    avg_loss:0.015, val_acc:0.992]
Epoch [92/120    avg_loss:0.020, val_acc:0.992]
Epoch [93/120    avg_loss:0.031, val_acc:0.994]
Epoch [94/120    avg_loss:0.020, val_acc:0.992]
Epoch [95/120    avg_loss:0.019, val_acc:0.992]
Epoch [96/120    avg_loss:0.012, val_acc:0.990]
Epoch [97/120    avg_loss:0.015, val_acc:0.990]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.992]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.017, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.990]
Epoch [104/120    avg_loss:0.017, val_acc:0.992]
Epoch [105/120    avg_loss:0.010, val_acc:0.992]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.012, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.992]
Epoch [109/120    avg_loss:0.013, val_acc:0.992]
Epoch [110/120    avg_loss:0.012, val_acc:0.992]
Epoch [111/120    avg_loss:0.026, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.992]
Epoch [113/120    avg_loss:0.015, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.017, val_acc:0.992]
Epoch [116/120    avg_loss:0.013, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.014, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.011, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 208  16   0   0   0   0   0   0   3   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.97959184 0.99563319 0.92857143 0.9047619
 1.         0.95698925 1.         0.99893276 1.         1.
 0.99559471 1.        ]

Kappa:
0.9900296756091336
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1322bc8780>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.309, val_acc:0.519]
Epoch [2/120    avg_loss:1.688, val_acc:0.631]
Epoch [3/120    avg_loss:1.323, val_acc:0.713]
Epoch [4/120    avg_loss:1.053, val_acc:0.810]
Epoch [5/120    avg_loss:0.806, val_acc:0.842]
Epoch [6/120    avg_loss:0.690, val_acc:0.838]
Epoch [7/120    avg_loss:0.669, val_acc:0.887]
Epoch [8/120    avg_loss:0.564, val_acc:0.879]
Epoch [9/120    avg_loss:0.460, val_acc:0.910]
Epoch [10/120    avg_loss:0.407, val_acc:0.917]
Epoch [11/120    avg_loss:0.452, val_acc:0.927]
Epoch [12/120    avg_loss:0.388, val_acc:0.925]
Epoch [13/120    avg_loss:0.420, val_acc:0.898]
Epoch [14/120    avg_loss:0.417, val_acc:0.933]
Epoch [15/120    avg_loss:0.310, val_acc:0.940]
Epoch [16/120    avg_loss:0.282, val_acc:0.931]
Epoch [17/120    avg_loss:0.351, val_acc:0.931]
Epoch [18/120    avg_loss:0.285, val_acc:0.929]
Epoch [19/120    avg_loss:0.273, val_acc:0.960]
Epoch [20/120    avg_loss:0.218, val_acc:0.956]
Epoch [21/120    avg_loss:0.228, val_acc:0.963]
Epoch [22/120    avg_loss:0.195, val_acc:0.958]
Epoch [23/120    avg_loss:0.168, val_acc:0.950]
Epoch [24/120    avg_loss:0.192, val_acc:0.969]
Epoch [25/120    avg_loss:0.150, val_acc:0.954]
Epoch [26/120    avg_loss:0.162, val_acc:0.960]
Epoch [27/120    avg_loss:0.130, val_acc:0.956]
Epoch [28/120    avg_loss:0.140, val_acc:0.967]
Epoch [29/120    avg_loss:0.124, val_acc:0.971]
Epoch [30/120    avg_loss:0.161, val_acc:0.977]
Epoch [31/120    avg_loss:0.171, val_acc:0.971]
Epoch [32/120    avg_loss:0.076, val_acc:0.973]
Epoch [33/120    avg_loss:0.142, val_acc:0.956]
Epoch [34/120    avg_loss:0.096, val_acc:0.977]
Epoch [35/120    avg_loss:0.129, val_acc:0.969]
Epoch [36/120    avg_loss:0.113, val_acc:0.981]
Epoch [37/120    avg_loss:0.108, val_acc:0.981]
Epoch [38/120    avg_loss:0.088, val_acc:0.971]
Epoch [39/120    avg_loss:0.082, val_acc:0.960]
Epoch [40/120    avg_loss:0.074, val_acc:0.965]
Epoch [41/120    avg_loss:0.083, val_acc:0.971]
Epoch [42/120    avg_loss:0.074, val_acc:0.969]
Epoch [43/120    avg_loss:0.080, val_acc:0.971]
Epoch [44/120    avg_loss:0.111, val_acc:0.956]
Epoch [45/120    avg_loss:0.078, val_acc:0.960]
Epoch [46/120    avg_loss:0.089, val_acc:0.973]
Epoch [47/120    avg_loss:0.061, val_acc:0.971]
Epoch [48/120    avg_loss:0.047, val_acc:0.981]
Epoch [49/120    avg_loss:0.044, val_acc:0.977]
Epoch [50/120    avg_loss:0.047, val_acc:0.975]
Epoch [51/120    avg_loss:0.049, val_acc:0.973]
Epoch [52/120    avg_loss:0.046, val_acc:0.979]
Epoch [53/120    avg_loss:0.042, val_acc:0.977]
Epoch [54/120    avg_loss:0.045, val_acc:0.973]
Epoch [55/120    avg_loss:0.054, val_acc:0.973]
Epoch [56/120    avg_loss:0.026, val_acc:0.971]
Epoch [57/120    avg_loss:0.053, val_acc:0.967]
Epoch [58/120    avg_loss:0.055, val_acc:0.971]
Epoch [59/120    avg_loss:0.052, val_acc:0.958]
Epoch [60/120    avg_loss:0.062, val_acc:0.967]
Epoch [61/120    avg_loss:0.065, val_acc:0.965]
Epoch [62/120    avg_loss:0.047, val_acc:0.965]
Epoch [63/120    avg_loss:0.024, val_acc:0.969]
Epoch [64/120    avg_loss:0.033, val_acc:0.969]
Epoch [65/120    avg_loss:0.022, val_acc:0.969]
Epoch [66/120    avg_loss:0.023, val_acc:0.971]
Epoch [67/120    avg_loss:0.023, val_acc:0.971]
Epoch [68/120    avg_loss:0.024, val_acc:0.971]
Epoch [69/120    avg_loss:0.022, val_acc:0.971]
Epoch [70/120    avg_loss:0.016, val_acc:0.971]
Epoch [71/120    avg_loss:0.017, val_acc:0.971]
Epoch [72/120    avg_loss:0.025, val_acc:0.971]
Epoch [73/120    avg_loss:0.013, val_acc:0.971]
Epoch [74/120    avg_loss:0.024, val_acc:0.971]
Epoch [75/120    avg_loss:0.020, val_acc:0.971]
Epoch [76/120    avg_loss:0.014, val_acc:0.971]
Epoch [77/120    avg_loss:0.018, val_acc:0.971]
Epoch [78/120    avg_loss:0.019, val_acc:0.971]
Epoch [79/120    avg_loss:0.019, val_acc:0.971]
Epoch [80/120    avg_loss:0.016, val_acc:0.971]
Epoch [81/120    avg_loss:0.021, val_acc:0.971]
Epoch [82/120    avg_loss:0.018, val_acc:0.971]
Epoch [83/120    avg_loss:0.018, val_acc:0.971]
Epoch [84/120    avg_loss:0.015, val_acc:0.971]
Epoch [85/120    avg_loss:0.015, val_acc:0.971]
Epoch [86/120    avg_loss:0.019, val_acc:0.971]
Epoch [87/120    avg_loss:0.024, val_acc:0.971]
Epoch [88/120    avg_loss:0.022, val_acc:0.971]
Epoch [89/120    avg_loss:0.029, val_acc:0.971]
Epoch [90/120    avg_loss:0.017, val_acc:0.971]
Epoch [91/120    avg_loss:0.020, val_acc:0.971]
Epoch [92/120    avg_loss:0.021, val_acc:0.971]
Epoch [93/120    avg_loss:0.017, val_acc:0.971]
Epoch [94/120    avg_loss:0.018, val_acc:0.971]
Epoch [95/120    avg_loss:0.021, val_acc:0.971]
Epoch [96/120    avg_loss:0.016, val_acc:0.971]
Epoch [97/120    avg_loss:0.021, val_acc:0.971]
Epoch [98/120    avg_loss:0.017, val_acc:0.971]
Epoch [99/120    avg_loss:0.020, val_acc:0.971]
Epoch [100/120    avg_loss:0.017, val_acc:0.971]
Epoch [101/120    avg_loss:0.018, val_acc:0.971]
Epoch [102/120    avg_loss:0.017, val_acc:0.971]
Epoch [103/120    avg_loss:0.018, val_acc:0.971]
Epoch [104/120    avg_loss:0.013, val_acc:0.971]
Epoch [105/120    avg_loss:0.020, val_acc:0.971]
Epoch [106/120    avg_loss:0.020, val_acc:0.971]
Epoch [107/120    avg_loss:0.026, val_acc:0.971]
Epoch [108/120    avg_loss:0.019, val_acc:0.971]
Epoch [109/120    avg_loss:0.021, val_acc:0.971]
Epoch [110/120    avg_loss:0.025, val_acc:0.971]
Epoch [111/120    avg_loss:0.017, val_acc:0.971]
Epoch [112/120    avg_loss:0.019, val_acc:0.971]
Epoch [113/120    avg_loss:0.016, val_acc:0.971]
Epoch [114/120    avg_loss:0.015, val_acc:0.971]
Epoch [115/120    avg_loss:0.024, val_acc:0.971]
Epoch [116/120    avg_loss:0.019, val_acc:0.971]
Epoch [117/120    avg_loss:0.020, val_acc:0.971]
Epoch [118/120    avg_loss:0.025, val_acc:0.971]
Epoch [119/120    avg_loss:0.017, val_acc:0.971]
Epoch [120/120    avg_loss:0.018, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 219  11   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.97727273 0.97550111 0.92110874 0.90909091
 1.         0.95744681 1.         1.         1.         1.
 0.99778761 1.        ]

Kappa:
0.9888431693173441
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fed35d5d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.337, val_acc:0.492]
Epoch [2/120    avg_loss:1.619, val_acc:0.623]
Epoch [3/120    avg_loss:1.235, val_acc:0.733]
Epoch [4/120    avg_loss:0.972, val_acc:0.756]
Epoch [5/120    avg_loss:0.830, val_acc:0.827]
Epoch [6/120    avg_loss:0.685, val_acc:0.844]
Epoch [7/120    avg_loss:0.604, val_acc:0.877]
Epoch [8/120    avg_loss:0.493, val_acc:0.856]
Epoch [9/120    avg_loss:0.514, val_acc:0.873]
Epoch [10/120    avg_loss:0.415, val_acc:0.892]
Epoch [11/120    avg_loss:0.314, val_acc:0.912]
Epoch [12/120    avg_loss:0.365, val_acc:0.904]
Epoch [13/120    avg_loss:0.310, val_acc:0.917]
Epoch [14/120    avg_loss:0.422, val_acc:0.938]
Epoch [15/120    avg_loss:0.363, val_acc:0.929]
Epoch [16/120    avg_loss:0.315, val_acc:0.921]
Epoch [17/120    avg_loss:0.253, val_acc:0.906]
Epoch [18/120    avg_loss:0.248, val_acc:0.898]
Epoch [19/120    avg_loss:0.255, val_acc:0.925]
Epoch [20/120    avg_loss:0.217, val_acc:0.946]
Epoch [21/120    avg_loss:0.264, val_acc:0.910]
Epoch [22/120    avg_loss:0.253, val_acc:0.940]
Epoch [23/120    avg_loss:0.209, val_acc:0.929]
Epoch [24/120    avg_loss:0.123, val_acc:0.948]
Epoch [25/120    avg_loss:0.159, val_acc:0.948]
Epoch [26/120    avg_loss:0.148, val_acc:0.931]
Epoch [27/120    avg_loss:0.127, val_acc:0.952]
Epoch [28/120    avg_loss:0.176, val_acc:0.942]
Epoch [29/120    avg_loss:0.142, val_acc:0.954]
Epoch [30/120    avg_loss:0.138, val_acc:0.954]
Epoch [31/120    avg_loss:0.164, val_acc:0.927]
Epoch [32/120    avg_loss:0.151, val_acc:0.940]
Epoch [33/120    avg_loss:0.119, val_acc:0.954]
Epoch [34/120    avg_loss:0.127, val_acc:0.946]
Epoch [35/120    avg_loss:0.177, val_acc:0.963]
Epoch [36/120    avg_loss:0.130, val_acc:0.938]
Epoch [37/120    avg_loss:0.140, val_acc:0.946]
Epoch [38/120    avg_loss:0.110, val_acc:0.958]
Epoch [39/120    avg_loss:0.132, val_acc:0.946]
Epoch [40/120    avg_loss:0.098, val_acc:0.963]
Epoch [41/120    avg_loss:0.092, val_acc:0.963]
Epoch [42/120    avg_loss:0.112, val_acc:0.965]
Epoch [43/120    avg_loss:0.083, val_acc:0.971]
Epoch [44/120    avg_loss:0.050, val_acc:0.975]
Epoch [45/120    avg_loss:0.074, val_acc:0.969]
Epoch [46/120    avg_loss:0.068, val_acc:0.965]
Epoch [47/120    avg_loss:0.066, val_acc:0.965]
Epoch [48/120    avg_loss:0.071, val_acc:0.979]
Epoch [49/120    avg_loss:0.045, val_acc:0.973]
Epoch [50/120    avg_loss:0.042, val_acc:0.969]
Epoch [51/120    avg_loss:0.039, val_acc:0.965]
Epoch [52/120    avg_loss:0.037, val_acc:0.963]
Epoch [53/120    avg_loss:0.057, val_acc:0.967]
Epoch [54/120    avg_loss:0.055, val_acc:0.960]
Epoch [55/120    avg_loss:0.045, val_acc:0.971]
Epoch [56/120    avg_loss:0.057, val_acc:0.971]
Epoch [57/120    avg_loss:0.093, val_acc:0.956]
Epoch [58/120    avg_loss:0.060, val_acc:0.963]
Epoch [59/120    avg_loss:0.037, val_acc:0.973]
Epoch [60/120    avg_loss:0.029, val_acc:0.971]
Epoch [61/120    avg_loss:0.035, val_acc:0.971]
Epoch [62/120    avg_loss:0.028, val_acc:0.971]
Epoch [63/120    avg_loss:0.020, val_acc:0.975]
Epoch [64/120    avg_loss:0.025, val_acc:0.973]
Epoch [65/120    avg_loss:0.036, val_acc:0.971]
Epoch [66/120    avg_loss:0.028, val_acc:0.973]
Epoch [67/120    avg_loss:0.039, val_acc:0.971]
Epoch [68/120    avg_loss:0.032, val_acc:0.973]
Epoch [69/120    avg_loss:0.022, val_acc:0.977]
Epoch [70/120    avg_loss:0.024, val_acc:0.979]
Epoch [71/120    avg_loss:0.026, val_acc:0.979]
Epoch [72/120    avg_loss:0.041, val_acc:0.977]
Epoch [73/120    avg_loss:0.020, val_acc:0.975]
Epoch [74/120    avg_loss:0.028, val_acc:0.977]
Epoch [75/120    avg_loss:0.031, val_acc:0.975]
Epoch [76/120    avg_loss:0.031, val_acc:0.979]
Epoch [77/120    avg_loss:0.039, val_acc:0.975]
Epoch [78/120    avg_loss:0.022, val_acc:0.973]
Epoch [79/120    avg_loss:0.020, val_acc:0.973]
Epoch [80/120    avg_loss:0.018, val_acc:0.975]
Epoch [81/120    avg_loss:0.025, val_acc:0.975]
Epoch [82/120    avg_loss:0.019, val_acc:0.975]
Epoch [83/120    avg_loss:0.020, val_acc:0.975]
Epoch [84/120    avg_loss:0.026, val_acc:0.977]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.023, val_acc:0.977]
Epoch [87/120    avg_loss:0.033, val_acc:0.981]
Epoch [88/120    avg_loss:0.024, val_acc:0.981]
Epoch [89/120    avg_loss:0.023, val_acc:0.983]
Epoch [90/120    avg_loss:0.017, val_acc:0.983]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.015, val_acc:0.979]
Epoch [93/120    avg_loss:0.021, val_acc:0.979]
Epoch [94/120    avg_loss:0.017, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.018, val_acc:0.977]
Epoch [98/120    avg_loss:0.015, val_acc:0.979]
Epoch [99/120    avg_loss:0.017, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.977]
Epoch [101/120    avg_loss:0.018, val_acc:0.979]
Epoch [102/120    avg_loss:0.013, val_acc:0.979]
Epoch [103/120    avg_loss:0.014, val_acc:0.979]
Epoch [104/120    avg_loss:0.027, val_acc:0.979]
Epoch [105/120    avg_loss:0.018, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.979]
Epoch [107/120    avg_loss:0.013, val_acc:0.979]
Epoch [108/120    avg_loss:0.020, val_acc:0.979]
Epoch [109/120    avg_loss:0.024, val_acc:0.979]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.020, val_acc:0.979]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.048, val_acc:0.979]
Epoch [117/120    avg_loss:0.023, val_acc:0.979]
Epoch [118/120    avg_loss:0.021, val_acc:0.979]
Epoch [119/120    avg_loss:0.018, val_acc:0.979]
Epoch [120/120    avg_loss:0.021, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.99543379 0.97777778 0.91810345 0.90344828
 1.         0.99470899 1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9905049287568507
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b611297b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.487, val_acc:0.438]
Epoch [2/120    avg_loss:1.660, val_acc:0.683]
Epoch [3/120    avg_loss:1.287, val_acc:0.669]
Epoch [4/120    avg_loss:1.097, val_acc:0.750]
Epoch [5/120    avg_loss:0.895, val_acc:0.752]
Epoch [6/120    avg_loss:0.867, val_acc:0.827]
Epoch [7/120    avg_loss:0.678, val_acc:0.831]
Epoch [8/120    avg_loss:0.594, val_acc:0.865]
Epoch [9/120    avg_loss:0.551, val_acc:0.867]
Epoch [10/120    avg_loss:0.512, val_acc:0.894]
Epoch [11/120    avg_loss:0.468, val_acc:0.873]
Epoch [12/120    avg_loss:0.428, val_acc:0.858]
Epoch [13/120    avg_loss:0.472, val_acc:0.910]
Epoch [14/120    avg_loss:0.423, val_acc:0.871]
Epoch [15/120    avg_loss:0.407, val_acc:0.906]
Epoch [16/120    avg_loss:0.311, val_acc:0.915]
Epoch [17/120    avg_loss:0.286, val_acc:0.944]
Epoch [18/120    avg_loss:0.260, val_acc:0.940]
Epoch [19/120    avg_loss:0.279, val_acc:0.885]
Epoch [20/120    avg_loss:0.246, val_acc:0.931]
Epoch [21/120    avg_loss:0.263, val_acc:0.950]
Epoch [22/120    avg_loss:0.183, val_acc:0.944]
Epoch [23/120    avg_loss:0.251, val_acc:0.946]
Epoch [24/120    avg_loss:0.261, val_acc:0.946]
Epoch [25/120    avg_loss:0.276, val_acc:0.967]
Epoch [26/120    avg_loss:0.196, val_acc:0.973]
Epoch [27/120    avg_loss:0.177, val_acc:0.954]
Epoch [28/120    avg_loss:0.147, val_acc:0.971]
Epoch [29/120    avg_loss:0.139, val_acc:0.973]
Epoch [30/120    avg_loss:0.126, val_acc:0.973]
Epoch [31/120    avg_loss:0.116, val_acc:0.975]
Epoch [32/120    avg_loss:0.167, val_acc:0.960]
Epoch [33/120    avg_loss:0.121, val_acc:0.971]
Epoch [34/120    avg_loss:0.179, val_acc:0.963]
Epoch [35/120    avg_loss:0.114, val_acc:0.969]
Epoch [36/120    avg_loss:0.109, val_acc:0.971]
Epoch [37/120    avg_loss:0.118, val_acc:0.971]
Epoch [38/120    avg_loss:0.178, val_acc:0.958]
Epoch [39/120    avg_loss:0.208, val_acc:0.950]
Epoch [40/120    avg_loss:0.136, val_acc:0.960]
Epoch [41/120    avg_loss:0.189, val_acc:0.948]
Epoch [42/120    avg_loss:0.157, val_acc:0.965]
Epoch [43/120    avg_loss:0.121, val_acc:0.973]
Epoch [44/120    avg_loss:0.048, val_acc:0.977]
Epoch [45/120    avg_loss:0.101, val_acc:0.977]
Epoch [46/120    avg_loss:0.082, val_acc:0.973]
Epoch [47/120    avg_loss:0.078, val_acc:0.979]
Epoch [48/120    avg_loss:0.059, val_acc:0.977]
Epoch [49/120    avg_loss:0.055, val_acc:0.975]
Epoch [50/120    avg_loss:0.057, val_acc:0.981]
Epoch [51/120    avg_loss:0.064, val_acc:0.969]
Epoch [52/120    avg_loss:0.095, val_acc:0.977]
Epoch [53/120    avg_loss:0.051, val_acc:0.981]
Epoch [54/120    avg_loss:0.075, val_acc:0.975]
Epoch [55/120    avg_loss:0.050, val_acc:0.973]
Epoch [56/120    avg_loss:0.087, val_acc:0.971]
Epoch [57/120    avg_loss:0.079, val_acc:0.977]
Epoch [58/120    avg_loss:0.057, val_acc:0.975]
Epoch [59/120    avg_loss:0.050, val_acc:0.975]
Epoch [60/120    avg_loss:0.106, val_acc:0.983]
Epoch [61/120    avg_loss:0.075, val_acc:0.985]
Epoch [62/120    avg_loss:0.097, val_acc:0.981]
Epoch [63/120    avg_loss:0.085, val_acc:0.983]
Epoch [64/120    avg_loss:0.036, val_acc:0.983]
Epoch [65/120    avg_loss:0.061, val_acc:0.988]
Epoch [66/120    avg_loss:0.045, val_acc:0.981]
Epoch [67/120    avg_loss:0.035, val_acc:0.977]
Epoch [68/120    avg_loss:0.045, val_acc:0.983]
Epoch [69/120    avg_loss:0.068, val_acc:0.983]
Epoch [70/120    avg_loss:0.061, val_acc:0.967]
Epoch [71/120    avg_loss:0.036, val_acc:0.983]
Epoch [72/120    avg_loss:0.040, val_acc:0.981]
Epoch [73/120    avg_loss:0.025, val_acc:0.981]
Epoch [74/120    avg_loss:0.040, val_acc:0.983]
Epoch [75/120    avg_loss:0.040, val_acc:0.979]
Epoch [76/120    avg_loss:0.037, val_acc:0.977]
Epoch [77/120    avg_loss:0.030, val_acc:0.981]
Epoch [78/120    avg_loss:0.031, val_acc:0.983]
Epoch [79/120    avg_loss:0.035, val_acc:0.985]
Epoch [80/120    avg_loss:0.014, val_acc:0.985]
Epoch [81/120    avg_loss:0.024, val_acc:0.985]
Epoch [82/120    avg_loss:0.028, val_acc:0.988]
Epoch [83/120    avg_loss:0.012, val_acc:0.985]
Epoch [84/120    avg_loss:0.018, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.985]
Epoch [86/120    avg_loss:0.017, val_acc:0.985]
Epoch [87/120    avg_loss:0.016, val_acc:0.985]
Epoch [88/120    avg_loss:0.022, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.019, val_acc:0.985]
Epoch [91/120    avg_loss:0.012, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.014, val_acc:0.985]
Epoch [94/120    avg_loss:0.013, val_acc:0.985]
Epoch [95/120    avg_loss:0.016, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.985]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.015, val_acc:0.983]
Epoch [100/120    avg_loss:0.020, val_acc:0.985]
Epoch [101/120    avg_loss:0.011, val_acc:0.985]
Epoch [102/120    avg_loss:0.013, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.027, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.017, val_acc:0.985]
Epoch [108/120    avg_loss:0.013, val_acc:0.985]
Epoch [109/120    avg_loss:0.014, val_acc:0.985]
Epoch [110/120    avg_loss:0.016, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.019, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.011, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.015, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   2 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.98866213 0.98454746 0.930131   0.91408935
 1.         0.98395722 0.998713   1.         1.         0.9973545
 0.9944629  1.        ]

Kappa:
0.9905048432132482
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74521de7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.260, val_acc:0.635]
Epoch [2/120    avg_loss:1.588, val_acc:0.660]
Epoch [3/120    avg_loss:1.139, val_acc:0.765]
Epoch [4/120    avg_loss:0.877, val_acc:0.815]
Epoch [5/120    avg_loss:0.748, val_acc:0.823]
Epoch [6/120    avg_loss:0.611, val_acc:0.883]
Epoch [7/120    avg_loss:0.470, val_acc:0.898]
Epoch [8/120    avg_loss:0.454, val_acc:0.917]
Epoch [9/120    avg_loss:0.421, val_acc:0.908]
Epoch [10/120    avg_loss:0.373, val_acc:0.917]
Epoch [11/120    avg_loss:0.396, val_acc:0.890]
Epoch [12/120    avg_loss:0.348, val_acc:0.902]
Epoch [13/120    avg_loss:0.414, val_acc:0.931]
Epoch [14/120    avg_loss:0.295, val_acc:0.910]
Epoch [15/120    avg_loss:0.296, val_acc:0.942]
Epoch [16/120    avg_loss:0.286, val_acc:0.931]
Epoch [17/120    avg_loss:0.256, val_acc:0.904]
Epoch [18/120    avg_loss:0.279, val_acc:0.946]
Epoch [19/120    avg_loss:0.285, val_acc:0.927]
Epoch [20/120    avg_loss:0.276, val_acc:0.944]
Epoch [21/120    avg_loss:0.219, val_acc:0.935]
Epoch [22/120    avg_loss:0.226, val_acc:0.929]
Epoch [23/120    avg_loss:0.187, val_acc:0.942]
Epoch [24/120    avg_loss:0.170, val_acc:0.952]
Epoch [25/120    avg_loss:0.165, val_acc:0.948]
Epoch [26/120    avg_loss:0.175, val_acc:0.958]
Epoch [27/120    avg_loss:0.201, val_acc:0.963]
Epoch [28/120    avg_loss:0.227, val_acc:0.923]
Epoch [29/120    avg_loss:0.273, val_acc:0.942]
Epoch [30/120    avg_loss:0.182, val_acc:0.950]
Epoch [31/120    avg_loss:0.153, val_acc:0.950]
Epoch [32/120    avg_loss:0.153, val_acc:0.971]
Epoch [33/120    avg_loss:0.119, val_acc:0.944]
Epoch [34/120    avg_loss:0.198, val_acc:0.963]
Epoch [35/120    avg_loss:0.132, val_acc:0.938]
Epoch [36/120    avg_loss:0.251, val_acc:0.952]
Epoch [37/120    avg_loss:0.188, val_acc:0.927]
Epoch [38/120    avg_loss:0.245, val_acc:0.927]
Epoch [39/120    avg_loss:0.147, val_acc:0.956]
Epoch [40/120    avg_loss:0.158, val_acc:0.963]
Epoch [41/120    avg_loss:0.127, val_acc:0.954]
Epoch [42/120    avg_loss:0.123, val_acc:0.973]
Epoch [43/120    avg_loss:0.099, val_acc:0.958]
Epoch [44/120    avg_loss:0.108, val_acc:0.975]
Epoch [45/120    avg_loss:0.057, val_acc:0.973]
Epoch [46/120    avg_loss:0.065, val_acc:0.975]
Epoch [47/120    avg_loss:0.089, val_acc:0.977]
Epoch [48/120    avg_loss:0.064, val_acc:0.973]
Epoch [49/120    avg_loss:0.080, val_acc:0.973]
Epoch [50/120    avg_loss:0.053, val_acc:0.992]
Epoch [51/120    avg_loss:0.053, val_acc:0.983]
Epoch [52/120    avg_loss:0.061, val_acc:0.981]
Epoch [53/120    avg_loss:0.054, val_acc:0.988]
Epoch [54/120    avg_loss:0.037, val_acc:0.985]
Epoch [55/120    avg_loss:0.048, val_acc:0.975]
Epoch [56/120    avg_loss:0.047, val_acc:0.983]
Epoch [57/120    avg_loss:0.031, val_acc:0.979]
Epoch [58/120    avg_loss:0.041, val_acc:0.985]
Epoch [59/120    avg_loss:0.032, val_acc:0.985]
Epoch [60/120    avg_loss:0.044, val_acc:0.983]
Epoch [61/120    avg_loss:0.023, val_acc:0.981]
Epoch [62/120    avg_loss:0.058, val_acc:0.985]
Epoch [63/120    avg_loss:0.036, val_acc:0.990]
Epoch [64/120    avg_loss:0.024, val_acc:0.988]
Epoch [65/120    avg_loss:0.021, val_acc:0.990]
Epoch [66/120    avg_loss:0.034, val_acc:0.988]
Epoch [67/120    avg_loss:0.044, val_acc:0.988]
Epoch [68/120    avg_loss:0.028, val_acc:0.988]
Epoch [69/120    avg_loss:0.026, val_acc:0.985]
Epoch [70/120    avg_loss:0.033, val_acc:0.990]
Epoch [71/120    avg_loss:0.023, val_acc:0.988]
Epoch [72/120    avg_loss:0.028, val_acc:0.990]
Epoch [73/120    avg_loss:0.028, val_acc:0.990]
Epoch [74/120    avg_loss:0.017, val_acc:0.990]
Epoch [75/120    avg_loss:0.027, val_acc:0.990]
Epoch [76/120    avg_loss:0.018, val_acc:0.990]
Epoch [77/120    avg_loss:0.020, val_acc:0.990]
Epoch [78/120    avg_loss:0.021, val_acc:0.990]
Epoch [79/120    avg_loss:0.022, val_acc:0.990]
Epoch [80/120    avg_loss:0.018, val_acc:0.990]
Epoch [81/120    avg_loss:0.034, val_acc:0.990]
Epoch [82/120    avg_loss:0.027, val_acc:0.990]
Epoch [83/120    avg_loss:0.033, val_acc:0.990]
Epoch [84/120    avg_loss:0.022, val_acc:0.990]
Epoch [85/120    avg_loss:0.020, val_acc:0.988]
Epoch [86/120    avg_loss:0.026, val_acc:0.988]
Epoch [87/120    avg_loss:0.040, val_acc:0.990]
Epoch [88/120    avg_loss:0.018, val_acc:0.990]
Epoch [89/120    avg_loss:0.019, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.990]
Epoch [91/120    avg_loss:0.018, val_acc:0.990]
Epoch [92/120    avg_loss:0.014, val_acc:0.990]
Epoch [93/120    avg_loss:0.023, val_acc:0.990]
Epoch [94/120    avg_loss:0.029, val_acc:0.990]
Epoch [95/120    avg_loss:0.021, val_acc:0.990]
Epoch [96/120    avg_loss:0.023, val_acc:0.990]
Epoch [97/120    avg_loss:0.016, val_acc:0.990]
Epoch [98/120    avg_loss:0.019, val_acc:0.990]
Epoch [99/120    avg_loss:0.021, val_acc:0.990]
Epoch [100/120    avg_loss:0.015, val_acc:0.990]
Epoch [101/120    avg_loss:0.029, val_acc:0.990]
Epoch [102/120    avg_loss:0.016, val_acc:0.990]
Epoch [103/120    avg_loss:0.023, val_acc:0.990]
Epoch [104/120    avg_loss:0.017, val_acc:0.990]
Epoch [105/120    avg_loss:0.027, val_acc:0.990]
Epoch [106/120    avg_loss:0.017, val_acc:0.990]
Epoch [107/120    avg_loss:0.019, val_acc:0.990]
Epoch [108/120    avg_loss:0.020, val_acc:0.990]
Epoch [109/120    avg_loss:0.023, val_acc:0.990]
Epoch [110/120    avg_loss:0.023, val_acc:0.990]
Epoch [111/120    avg_loss:0.022, val_acc:0.990]
Epoch [112/120    avg_loss:0.022, val_acc:0.990]
Epoch [113/120    avg_loss:0.027, val_acc:0.990]
Epoch [114/120    avg_loss:0.019, val_acc:0.990]
Epoch [115/120    avg_loss:0.047, val_acc:0.990]
Epoch [116/120    avg_loss:0.017, val_acc:0.990]
Epoch [117/120    avg_loss:0.039, val_acc:0.990]
Epoch [118/120    avg_loss:0.023, val_acc:0.990]
Epoch [119/120    avg_loss:0.025, val_acc:0.990]
Epoch [120/120    avg_loss:0.020, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 216  13   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   2   0   0   1   0 203   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.99854227 0.98412698 0.96860987 0.92735043 0.93103448
 0.99266504 0.96216216 0.998713   1.         1.         0.98305085
 0.98544233 1.        ]

Kappa:
0.9864689249165285
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faaad15f748>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.515]
Epoch [2/120    avg_loss:1.591, val_acc:0.617]
Epoch [3/120    avg_loss:1.167, val_acc:0.725]
Epoch [4/120    avg_loss:0.914, val_acc:0.787]
Epoch [5/120    avg_loss:0.859, val_acc:0.752]
Epoch [6/120    avg_loss:0.715, val_acc:0.860]
Epoch [7/120    avg_loss:0.637, val_acc:0.856]
Epoch [8/120    avg_loss:0.541, val_acc:0.865]
Epoch [9/120    avg_loss:0.483, val_acc:0.883]
Epoch [10/120    avg_loss:0.460, val_acc:0.885]
Epoch [11/120    avg_loss:0.453, val_acc:0.863]
Epoch [12/120    avg_loss:0.357, val_acc:0.894]
Epoch [13/120    avg_loss:0.382, val_acc:0.906]
Epoch [14/120    avg_loss:0.320, val_acc:0.898]
Epoch [15/120    avg_loss:0.331, val_acc:0.850]
Epoch [16/120    avg_loss:0.362, val_acc:0.942]
Epoch [17/120    avg_loss:0.318, val_acc:0.933]
Epoch [18/120    avg_loss:0.281, val_acc:0.925]
Epoch [19/120    avg_loss:0.318, val_acc:0.944]
Epoch [20/120    avg_loss:0.199, val_acc:0.933]
Epoch [21/120    avg_loss:0.211, val_acc:0.946]
Epoch [22/120    avg_loss:0.162, val_acc:0.938]
Epoch [23/120    avg_loss:0.242, val_acc:0.944]
Epoch [24/120    avg_loss:0.189, val_acc:0.960]
Epoch [25/120    avg_loss:0.204, val_acc:0.925]
Epoch [26/120    avg_loss:0.241, val_acc:0.963]
Epoch [27/120    avg_loss:0.203, val_acc:0.946]
Epoch [28/120    avg_loss:0.177, val_acc:0.948]
Epoch [29/120    avg_loss:0.233, val_acc:0.950]
Epoch [30/120    avg_loss:0.163, val_acc:0.954]
Epoch [31/120    avg_loss:0.134, val_acc:0.975]
Epoch [32/120    avg_loss:0.130, val_acc:0.977]
Epoch [33/120    avg_loss:0.109, val_acc:0.963]
Epoch [34/120    avg_loss:0.090, val_acc:0.963]
Epoch [35/120    avg_loss:0.090, val_acc:0.975]
Epoch [36/120    avg_loss:0.136, val_acc:0.971]
Epoch [37/120    avg_loss:0.098, val_acc:0.975]
Epoch [38/120    avg_loss:0.073, val_acc:0.979]
Epoch [39/120    avg_loss:0.087, val_acc:0.985]
Epoch [40/120    avg_loss:0.077, val_acc:0.977]
Epoch [41/120    avg_loss:0.077, val_acc:0.971]
Epoch [42/120    avg_loss:0.139, val_acc:0.973]
Epoch [43/120    avg_loss:0.130, val_acc:0.963]
Epoch [44/120    avg_loss:0.119, val_acc:0.977]
Epoch [45/120    avg_loss:0.092, val_acc:0.981]
Epoch [46/120    avg_loss:0.054, val_acc:0.979]
Epoch [47/120    avg_loss:0.074, val_acc:0.933]
Epoch [48/120    avg_loss:0.089, val_acc:0.971]
Epoch [49/120    avg_loss:0.086, val_acc:0.967]
Epoch [50/120    avg_loss:0.067, val_acc:0.969]
Epoch [51/120    avg_loss:0.069, val_acc:0.985]
Epoch [52/120    avg_loss:0.045, val_acc:0.983]
Epoch [53/120    avg_loss:0.041, val_acc:0.979]
Epoch [54/120    avg_loss:0.070, val_acc:0.979]
Epoch [55/120    avg_loss:0.061, val_acc:0.983]
Epoch [56/120    avg_loss:0.087, val_acc:0.983]
Epoch [57/120    avg_loss:0.096, val_acc:0.985]
Epoch [58/120    avg_loss:0.074, val_acc:0.992]
Epoch [59/120    avg_loss:0.053, val_acc:0.992]
Epoch [60/120    avg_loss:0.054, val_acc:0.990]
Epoch [61/120    avg_loss:0.056, val_acc:0.975]
Epoch [62/120    avg_loss:0.040, val_acc:0.985]
Epoch [63/120    avg_loss:0.036, val_acc:0.988]
Epoch [64/120    avg_loss:0.050, val_acc:0.990]
Epoch [65/120    avg_loss:0.053, val_acc:0.988]
Epoch [66/120    avg_loss:0.027, val_acc:0.985]
Epoch [67/120    avg_loss:0.045, val_acc:0.981]
Epoch [68/120    avg_loss:0.047, val_acc:0.994]
Epoch [69/120    avg_loss:0.027, val_acc:0.994]
Epoch [70/120    avg_loss:0.055, val_acc:0.988]
Epoch [71/120    avg_loss:0.022, val_acc:0.990]
Epoch [72/120    avg_loss:0.024, val_acc:0.988]
Epoch [73/120    avg_loss:0.042, val_acc:0.988]
Epoch [74/120    avg_loss:0.040, val_acc:0.996]
Epoch [75/120    avg_loss:0.023, val_acc:0.994]
Epoch [76/120    avg_loss:0.013, val_acc:0.983]
Epoch [77/120    avg_loss:0.015, val_acc:0.992]
Epoch [78/120    avg_loss:0.025, val_acc:0.983]
Epoch [79/120    avg_loss:0.024, val_acc:0.988]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.018, val_acc:0.994]
Epoch [82/120    avg_loss:0.013, val_acc:0.996]
Epoch [83/120    avg_loss:0.013, val_acc:0.996]
Epoch [84/120    avg_loss:0.041, val_acc:0.992]
Epoch [85/120    avg_loss:0.069, val_acc:0.992]
Epoch [86/120    avg_loss:0.035, val_acc:0.983]
Epoch [87/120    avg_loss:0.036, val_acc:0.981]
Epoch [88/120    avg_loss:0.040, val_acc:0.983]
Epoch [89/120    avg_loss:0.062, val_acc:0.992]
Epoch [90/120    avg_loss:0.040, val_acc:0.985]
Epoch [91/120    avg_loss:0.076, val_acc:0.969]
Epoch [92/120    avg_loss:0.079, val_acc:0.979]
Epoch [93/120    avg_loss:0.071, val_acc:0.973]
Epoch [94/120    avg_loss:0.043, val_acc:0.983]
Epoch [95/120    avg_loss:0.045, val_acc:0.988]
Epoch [96/120    avg_loss:0.026, val_acc:0.985]
Epoch [97/120    avg_loss:0.027, val_acc:0.988]
Epoch [98/120    avg_loss:0.019, val_acc:0.988]
Epoch [99/120    avg_loss:0.017, val_acc:0.988]
Epoch [100/120    avg_loss:0.033, val_acc:0.990]
Epoch [101/120    avg_loss:0.013, val_acc:0.990]
Epoch [102/120    avg_loss:0.012, val_acc:0.990]
Epoch [103/120    avg_loss:0.012, val_acc:0.990]
Epoch [104/120    avg_loss:0.017, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.990]
Epoch [107/120    avg_loss:0.013, val_acc:0.990]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.009, val_acc:0.990]
Epoch [111/120    avg_loss:0.014, val_acc:0.990]
Epoch [112/120    avg_loss:0.014, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.990]
Epoch [114/120    avg_loss:0.013, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.016, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 223   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 223   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.99090909 0.98454746 0.94092827 0.94285714
 0.98771499 0.99465241 1.         0.99893276 1.         1.
 0.99558499 1.        ]

Kappa:
0.9921659115868178
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f7c7907f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.400, val_acc:0.442]
Epoch [2/120    avg_loss:1.757, val_acc:0.648]
Epoch [3/120    avg_loss:1.323, val_acc:0.677]
Epoch [4/120    avg_loss:1.043, val_acc:0.767]
Epoch [5/120    avg_loss:0.841, val_acc:0.806]
Epoch [6/120    avg_loss:0.717, val_acc:0.806]
Epoch [7/120    avg_loss:0.614, val_acc:0.856]
Epoch [8/120    avg_loss:0.657, val_acc:0.821]
Epoch [9/120    avg_loss:0.578, val_acc:0.887]
Epoch [10/120    avg_loss:0.413, val_acc:0.921]
Epoch [11/120    avg_loss:0.369, val_acc:0.933]
Epoch [12/120    avg_loss:0.311, val_acc:0.946]
Epoch [13/120    avg_loss:0.288, val_acc:0.938]
Epoch [14/120    avg_loss:0.373, val_acc:0.912]
Epoch [15/120    avg_loss:0.293, val_acc:0.944]
Epoch [16/120    avg_loss:0.289, val_acc:0.931]
Epoch [17/120    avg_loss:0.230, val_acc:0.948]
Epoch [18/120    avg_loss:0.268, val_acc:0.944]
Epoch [19/120    avg_loss:0.240, val_acc:0.929]
Epoch [20/120    avg_loss:0.278, val_acc:0.942]
Epoch [21/120    avg_loss:0.206, val_acc:0.933]
Epoch [22/120    avg_loss:0.184, val_acc:0.950]
Epoch [23/120    avg_loss:0.196, val_acc:0.958]
Epoch [24/120    avg_loss:0.145, val_acc:0.963]
Epoch [25/120    avg_loss:0.173, val_acc:0.967]
Epoch [26/120    avg_loss:0.146, val_acc:0.960]
Epoch [27/120    avg_loss:0.167, val_acc:0.960]
Epoch [28/120    avg_loss:0.114, val_acc:0.967]
Epoch [29/120    avg_loss:0.108, val_acc:0.960]
Epoch [30/120    avg_loss:0.108, val_acc:0.973]
Epoch [31/120    avg_loss:0.151, val_acc:0.971]
Epoch [32/120    avg_loss:0.120, val_acc:0.965]
Epoch [33/120    avg_loss:0.118, val_acc:0.971]
Epoch [34/120    avg_loss:0.087, val_acc:0.969]
Epoch [35/120    avg_loss:0.104, val_acc:0.975]
Epoch [36/120    avg_loss:0.099, val_acc:0.963]
Epoch [37/120    avg_loss:0.084, val_acc:0.981]
Epoch [38/120    avg_loss:0.097, val_acc:0.965]
Epoch [39/120    avg_loss:0.122, val_acc:0.965]
Epoch [40/120    avg_loss:0.156, val_acc:0.958]
Epoch [41/120    avg_loss:0.132, val_acc:0.967]
Epoch [42/120    avg_loss:0.096, val_acc:0.971]
Epoch [43/120    avg_loss:0.063, val_acc:0.975]
Epoch [44/120    avg_loss:0.054, val_acc:0.973]
Epoch [45/120    avg_loss:0.068, val_acc:0.967]
Epoch [46/120    avg_loss:0.074, val_acc:0.969]
Epoch [47/120    avg_loss:0.120, val_acc:0.967]
Epoch [48/120    avg_loss:0.087, val_acc:0.971]
Epoch [49/120    avg_loss:0.104, val_acc:0.975]
Epoch [50/120    avg_loss:0.065, val_acc:0.983]
Epoch [51/120    avg_loss:0.051, val_acc:0.971]
Epoch [52/120    avg_loss:0.054, val_acc:0.981]
Epoch [53/120    avg_loss:0.047, val_acc:0.977]
Epoch [54/120    avg_loss:0.044, val_acc:0.971]
Epoch [55/120    avg_loss:0.061, val_acc:0.983]
Epoch [56/120    avg_loss:0.047, val_acc:0.990]
Epoch [57/120    avg_loss:0.041, val_acc:0.975]
Epoch [58/120    avg_loss:0.095, val_acc:0.985]
Epoch [59/120    avg_loss:0.041, val_acc:0.979]
Epoch [60/120    avg_loss:0.048, val_acc:0.979]
Epoch [61/120    avg_loss:0.053, val_acc:0.975]
Epoch [62/120    avg_loss:0.044, val_acc:0.988]
Epoch [63/120    avg_loss:0.041, val_acc:0.977]
Epoch [64/120    avg_loss:0.040, val_acc:0.983]
Epoch [65/120    avg_loss:0.041, val_acc:0.973]
Epoch [66/120    avg_loss:0.029, val_acc:0.988]
Epoch [67/120    avg_loss:0.033, val_acc:0.971]
Epoch [68/120    avg_loss:0.056, val_acc:0.983]
Epoch [69/120    avg_loss:0.069, val_acc:0.935]
Epoch [70/120    avg_loss:0.078, val_acc:0.956]
Epoch [71/120    avg_loss:0.035, val_acc:0.977]
Epoch [72/120    avg_loss:0.028, val_acc:0.988]
Epoch [73/120    avg_loss:0.029, val_acc:0.983]
Epoch [74/120    avg_loss:0.021, val_acc:0.983]
Epoch [75/120    avg_loss:0.022, val_acc:0.983]
Epoch [76/120    avg_loss:0.025, val_acc:0.983]
Epoch [77/120    avg_loss:0.018, val_acc:0.983]
Epoch [78/120    avg_loss:0.026, val_acc:0.983]
Epoch [79/120    avg_loss:0.029, val_acc:0.981]
Epoch [80/120    avg_loss:0.015, val_acc:0.979]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.019, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.981]
Epoch [84/120    avg_loss:0.016, val_acc:0.981]
Epoch [85/120    avg_loss:0.025, val_acc:0.981]
Epoch [86/120    avg_loss:0.016, val_acc:0.981]
Epoch [87/120    avg_loss:0.017, val_acc:0.981]
Epoch [88/120    avg_loss:0.025, val_acc:0.981]
Epoch [89/120    avg_loss:0.022, val_acc:0.983]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.981]
Epoch [92/120    avg_loss:0.030, val_acc:0.981]
Epoch [93/120    avg_loss:0.021, val_acc:0.981]
Epoch [94/120    avg_loss:0.018, val_acc:0.981]
Epoch [95/120    avg_loss:0.018, val_acc:0.981]
Epoch [96/120    avg_loss:0.029, val_acc:0.981]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.981]
Epoch [100/120    avg_loss:0.021, val_acc:0.981]
Epoch [101/120    avg_loss:0.018, val_acc:0.981]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.025, val_acc:0.981]
Epoch [104/120    avg_loss:0.020, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.031, val_acc:0.981]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.031, val_acc:0.981]
Epoch [110/120    avg_loss:0.018, val_acc:0.981]
Epoch [111/120    avg_loss:0.012, val_acc:0.981]
Epoch [112/120    avg_loss:0.024, val_acc:0.981]
Epoch [113/120    avg_loss:0.020, val_acc:0.981]
Epoch [114/120    avg_loss:0.022, val_acc:0.981]
Epoch [115/120    avg_loss:0.015, val_acc:0.981]
Epoch [116/120    avg_loss:0.022, val_acc:0.981]
Epoch [117/120    avg_loss:0.017, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.017, val_acc:0.981]
Epoch [120/120    avg_loss:0.029, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         1.         0.98901099 0.93913043 0.92041522
 1.         1.         1.         1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9926412348674984
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f00f5c59898>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.276, val_acc:0.506]
Epoch [2/120    avg_loss:1.636, val_acc:0.652]
Epoch [3/120    avg_loss:1.226, val_acc:0.752]
Epoch [4/120    avg_loss:0.949, val_acc:0.827]
Epoch [5/120    avg_loss:0.780, val_acc:0.812]
Epoch [6/120    avg_loss:0.695, val_acc:0.833]
Epoch [7/120    avg_loss:0.642, val_acc:0.829]
Epoch [8/120    avg_loss:0.545, val_acc:0.865]
Epoch [9/120    avg_loss:0.541, val_acc:0.900]
Epoch [10/120    avg_loss:0.462, val_acc:0.919]
Epoch [11/120    avg_loss:0.414, val_acc:0.887]
Epoch [12/120    avg_loss:0.455, val_acc:0.896]
Epoch [13/120    avg_loss:0.402, val_acc:0.933]
Epoch [14/120    avg_loss:0.297, val_acc:0.965]
Epoch [15/120    avg_loss:0.335, val_acc:0.927]
Epoch [16/120    avg_loss:0.244, val_acc:0.946]
Epoch [17/120    avg_loss:0.260, val_acc:0.931]
Epoch [18/120    avg_loss:0.261, val_acc:0.971]
Epoch [19/120    avg_loss:0.230, val_acc:0.952]
Epoch [20/120    avg_loss:0.274, val_acc:0.944]
Epoch [21/120    avg_loss:0.351, val_acc:0.915]
Epoch [22/120    avg_loss:0.389, val_acc:0.935]
Epoch [23/120    avg_loss:0.260, val_acc:0.948]
Epoch [24/120    avg_loss:0.206, val_acc:0.960]
Epoch [25/120    avg_loss:0.243, val_acc:0.956]
Epoch [26/120    avg_loss:0.171, val_acc:0.967]
Epoch [27/120    avg_loss:0.161, val_acc:0.965]
Epoch [28/120    avg_loss:0.130, val_acc:0.965]
Epoch [29/120    avg_loss:0.168, val_acc:0.969]
Epoch [30/120    avg_loss:0.119, val_acc:0.965]
Epoch [31/120    avg_loss:0.112, val_acc:0.952]
Epoch [32/120    avg_loss:0.162, val_acc:0.967]
Epoch [33/120    avg_loss:0.108, val_acc:0.971]
Epoch [34/120    avg_loss:0.086, val_acc:0.975]
Epoch [35/120    avg_loss:0.090, val_acc:0.977]
Epoch [36/120    avg_loss:0.094, val_acc:0.975]
Epoch [37/120    avg_loss:0.080, val_acc:0.971]
Epoch [38/120    avg_loss:0.089, val_acc:0.971]
Epoch [39/120    avg_loss:0.080, val_acc:0.971]
Epoch [40/120    avg_loss:0.072, val_acc:0.973]
Epoch [41/120    avg_loss:0.088, val_acc:0.975]
Epoch [42/120    avg_loss:0.083, val_acc:0.977]
Epoch [43/120    avg_loss:0.070, val_acc:0.975]
Epoch [44/120    avg_loss:0.093, val_acc:0.979]
Epoch [45/120    avg_loss:0.090, val_acc:0.979]
Epoch [46/120    avg_loss:0.069, val_acc:0.979]
Epoch [47/120    avg_loss:0.084, val_acc:0.981]
Epoch [48/120    avg_loss:0.071, val_acc:0.977]
Epoch [49/120    avg_loss:0.091, val_acc:0.977]
Epoch [50/120    avg_loss:0.092, val_acc:0.975]
Epoch [51/120    avg_loss:0.083, val_acc:0.973]
Epoch [52/120    avg_loss:0.076, val_acc:0.979]
Epoch [53/120    avg_loss:0.064, val_acc:0.979]
Epoch [54/120    avg_loss:0.066, val_acc:0.977]
Epoch [55/120    avg_loss:0.072, val_acc:0.975]
Epoch [56/120    avg_loss:0.067, val_acc:0.977]
Epoch [57/120    avg_loss:0.068, val_acc:0.979]
Epoch [58/120    avg_loss:0.056, val_acc:0.979]
Epoch [59/120    avg_loss:0.065, val_acc:0.979]
Epoch [60/120    avg_loss:0.063, val_acc:0.977]
Epoch [61/120    avg_loss:0.063, val_acc:0.977]
Epoch [62/120    avg_loss:0.067, val_acc:0.977]
Epoch [63/120    avg_loss:0.047, val_acc:0.979]
Epoch [64/120    avg_loss:0.086, val_acc:0.979]
Epoch [65/120    avg_loss:0.064, val_acc:0.979]
Epoch [66/120    avg_loss:0.053, val_acc:0.979]
Epoch [67/120    avg_loss:0.090, val_acc:0.979]
Epoch [68/120    avg_loss:0.078, val_acc:0.979]
Epoch [69/120    avg_loss:0.072, val_acc:0.977]
Epoch [70/120    avg_loss:0.078, val_acc:0.977]
Epoch [71/120    avg_loss:0.088, val_acc:0.977]
Epoch [72/120    avg_loss:0.064, val_acc:0.979]
Epoch [73/120    avg_loss:0.054, val_acc:0.979]
Epoch [74/120    avg_loss:0.063, val_acc:0.979]
Epoch [75/120    avg_loss:0.066, val_acc:0.979]
Epoch [76/120    avg_loss:0.068, val_acc:0.979]
Epoch [77/120    avg_loss:0.085, val_acc:0.979]
Epoch [78/120    avg_loss:0.061, val_acc:0.979]
Epoch [79/120    avg_loss:0.055, val_acc:0.979]
Epoch [80/120    avg_loss:0.076, val_acc:0.979]
Epoch [81/120    avg_loss:0.066, val_acc:0.979]
Epoch [82/120    avg_loss:0.053, val_acc:0.979]
Epoch [83/120    avg_loss:0.057, val_acc:0.979]
Epoch [84/120    avg_loss:0.102, val_acc:0.979]
Epoch [85/120    avg_loss:0.070, val_acc:0.979]
Epoch [86/120    avg_loss:0.071, val_acc:0.977]
Epoch [87/120    avg_loss:0.071, val_acc:0.977]
Epoch [88/120    avg_loss:0.067, val_acc:0.977]
Epoch [89/120    avg_loss:0.083, val_acc:0.977]
Epoch [90/120    avg_loss:0.065, val_acc:0.977]
Epoch [91/120    avg_loss:0.052, val_acc:0.977]
Epoch [92/120    avg_loss:0.064, val_acc:0.977]
Epoch [93/120    avg_loss:0.077, val_acc:0.977]
Epoch [94/120    avg_loss:0.065, val_acc:0.977]
Epoch [95/120    avg_loss:0.058, val_acc:0.977]
Epoch [96/120    avg_loss:0.072, val_acc:0.977]
Epoch [97/120    avg_loss:0.064, val_acc:0.977]
Epoch [98/120    avg_loss:0.069, val_acc:0.977]
Epoch [99/120    avg_loss:0.071, val_acc:0.977]
Epoch [100/120    avg_loss:0.060, val_acc:0.977]
Epoch [101/120    avg_loss:0.074, val_acc:0.977]
Epoch [102/120    avg_loss:0.054, val_acc:0.977]
Epoch [103/120    avg_loss:0.063, val_acc:0.977]
Epoch [104/120    avg_loss:0.064, val_acc:0.977]
Epoch [105/120    avg_loss:0.052, val_acc:0.977]
Epoch [106/120    avg_loss:0.090, val_acc:0.977]
Epoch [107/120    avg_loss:0.077, val_acc:0.977]
Epoch [108/120    avg_loss:0.074, val_acc:0.977]
Epoch [109/120    avg_loss:0.057, val_acc:0.977]
Epoch [110/120    avg_loss:0.079, val_acc:0.977]
Epoch [111/120    avg_loss:0.060, val_acc:0.977]
Epoch [112/120    avg_loss:0.057, val_acc:0.977]
Epoch [113/120    avg_loss:0.060, val_acc:0.977]
Epoch [114/120    avg_loss:0.053, val_acc:0.977]
Epoch [115/120    avg_loss:0.055, val_acc:0.977]
Epoch [116/120    avg_loss:0.067, val_acc:0.977]
Epoch [117/120    avg_loss:0.058, val_acc:0.977]
Epoch [118/120    avg_loss:0.049, val_acc:0.977]
Epoch [119/120    avg_loss:0.079, val_acc:0.977]
Epoch [120/120    avg_loss:0.076, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 1.         0.95454545 1.         0.93665158 0.90728477
 1.         0.89247312 1.         1.         1.         0.98296199
 0.98550725 1.        ]

Kappa:
0.9855207704494762
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6051dfb7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 193472==>0.19M
----------Training process----------
Epoch [1/120    avg_loss:2.344, val_acc:0.585]
Epoch [2/120    avg_loss:1.663, val_acc:0.658]
Epoch [3/120    avg_loss:1.234, val_acc:0.717]
Epoch [4/120    avg_loss:0.958, val_acc:0.748]
Epoch [5/120    avg_loss:0.769, val_acc:0.792]
Epoch [6/120    avg_loss:0.633, val_acc:0.842]
Epoch [7/120    avg_loss:0.647, val_acc:0.846]
Epoch [8/120    avg_loss:0.492, val_acc:0.883]
Epoch [9/120    avg_loss:0.435, val_acc:0.927]
Epoch [10/120    avg_loss:0.326, val_acc:0.908]
Epoch [11/120    avg_loss:0.411, val_acc:0.938]
Epoch [12/120    avg_loss:0.298, val_acc:0.935]
Epoch [13/120    avg_loss:0.294, val_acc:0.938]
Epoch [14/120    avg_loss:0.288, val_acc:0.942]
Epoch [15/120    avg_loss:0.232, val_acc:0.929]
Epoch [16/120    avg_loss:0.188, val_acc:0.958]
Epoch [17/120    avg_loss:0.251, val_acc:0.946]
Epoch [18/120    avg_loss:0.263, val_acc:0.933]
Epoch [19/120    avg_loss:0.270, val_acc:0.952]
Epoch [20/120    avg_loss:0.231, val_acc:0.950]
Epoch [21/120    avg_loss:0.201, val_acc:0.963]
Epoch [22/120    avg_loss:0.202, val_acc:0.942]
Epoch [23/120    avg_loss:0.273, val_acc:0.956]
Epoch [24/120    avg_loss:0.159, val_acc:0.967]
Epoch [25/120    avg_loss:0.115, val_acc:0.960]
Epoch [26/120    avg_loss:0.115, val_acc:0.963]
Epoch [27/120    avg_loss:0.099, val_acc:0.954]
Epoch [28/120    avg_loss:0.179, val_acc:0.973]
Epoch [29/120    avg_loss:0.125, val_acc:0.965]
Epoch [30/120    avg_loss:0.108, val_acc:0.971]
Epoch [31/120    avg_loss:0.097, val_acc:0.960]
Epoch [32/120    avg_loss:0.136, val_acc:0.963]
Epoch [33/120    avg_loss:0.159, val_acc:0.948]
Epoch [34/120    avg_loss:0.145, val_acc:0.973]
Epoch [35/120    avg_loss:0.123, val_acc:0.977]
Epoch [36/120    avg_loss:0.098, val_acc:0.975]
Epoch [37/120    avg_loss:0.075, val_acc:0.977]
Epoch [38/120    avg_loss:0.064, val_acc:0.983]
Epoch [39/120    avg_loss:0.055, val_acc:0.981]
Epoch [40/120    avg_loss:0.045, val_acc:0.981]
Epoch [41/120    avg_loss:0.084, val_acc:0.975]
Epoch [42/120    avg_loss:0.170, val_acc:0.933]
Epoch [43/120    avg_loss:0.131, val_acc:0.950]
Epoch [44/120    avg_loss:0.102, val_acc:0.971]
Epoch [45/120    avg_loss:0.082, val_acc:0.981]
Epoch [46/120    avg_loss:0.056, val_acc:0.971]
Epoch [47/120    avg_loss:0.077, val_acc:0.985]
Epoch [48/120    avg_loss:0.068, val_acc:0.983]
Epoch [49/120    avg_loss:0.073, val_acc:0.971]
Epoch [50/120    avg_loss:0.084, val_acc:0.971]
Epoch [51/120    avg_loss:0.069, val_acc:0.971]
Epoch [52/120    avg_loss:0.078, val_acc:0.979]
Epoch [53/120    avg_loss:0.072, val_acc:0.981]
Epoch [54/120    avg_loss:0.071, val_acc:0.985]
Epoch [55/120    avg_loss:0.039, val_acc:0.981]
Epoch [56/120    avg_loss:0.030, val_acc:0.981]
Epoch [57/120    avg_loss:0.035, val_acc:0.988]
Epoch [58/120    avg_loss:0.031, val_acc:0.985]
Epoch [59/120    avg_loss:0.039, val_acc:0.981]
Epoch [60/120    avg_loss:0.039, val_acc:0.981]
Epoch [61/120    avg_loss:0.047, val_acc:0.971]
Epoch [62/120    avg_loss:0.038, val_acc:0.983]
Epoch [63/120    avg_loss:0.040, val_acc:0.985]
Epoch [64/120    avg_loss:0.040, val_acc:0.990]
Epoch [65/120    avg_loss:0.043, val_acc:0.979]
Epoch [66/120    avg_loss:0.040, val_acc:0.979]
Epoch [67/120    avg_loss:0.031, val_acc:0.985]
Epoch [68/120    avg_loss:0.044, val_acc:0.975]
Epoch [69/120    avg_loss:0.046, val_acc:0.979]
Epoch [70/120    avg_loss:0.045, val_acc:0.985]
Epoch [71/120    avg_loss:0.060, val_acc:0.985]
Epoch [72/120    avg_loss:0.046, val_acc:0.979]
Epoch [73/120    avg_loss:0.025, val_acc:0.988]
Epoch [74/120    avg_loss:0.025, val_acc:0.985]
Epoch [75/120    avg_loss:0.017, val_acc:0.990]
Epoch [76/120    avg_loss:0.017, val_acc:0.985]
Epoch [77/120    avg_loss:0.020, val_acc:0.983]
Epoch [78/120    avg_loss:0.029, val_acc:0.973]
Epoch [79/120    avg_loss:0.025, val_acc:0.985]
Epoch [80/120    avg_loss:0.033, val_acc:0.983]
Epoch [81/120    avg_loss:0.043, val_acc:0.979]
Epoch [82/120    avg_loss:0.042, val_acc:0.985]
Epoch [83/120    avg_loss:0.053, val_acc:0.981]
Epoch [84/120    avg_loss:0.061, val_acc:0.979]
Epoch [85/120    avg_loss:0.064, val_acc:0.958]
Epoch [86/120    avg_loss:0.055, val_acc:0.977]
Epoch [87/120    avg_loss:0.048, val_acc:0.975]
Epoch [88/120    avg_loss:0.049, val_acc:0.977]
Epoch [89/120    avg_loss:0.030, val_acc:0.977]
Epoch [90/120    avg_loss:0.033, val_acc:0.985]
Epoch [91/120    avg_loss:0.017, val_acc:0.988]
Epoch [92/120    avg_loss:0.015, val_acc:0.988]
Epoch [93/120    avg_loss:0.016, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.027, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.017, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.019, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.017, val_acc:0.985]
Epoch [102/120    avg_loss:0.013, val_acc:0.985]
Epoch [103/120    avg_loss:0.014, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.025, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.019, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.013, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.017, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.016, val_acc:0.985]
Epoch [120/120    avg_loss:0.012, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   3   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0  11 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.99545455 0.98901099 0.96213808 0.95302013
 1.         1.         1.         0.9978678  1.         0.98562092
 0.98544233 1.        ]

Kappa:
0.9924043674279962
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdae619a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.494]
Epoch [2/120    avg_loss:1.640, val_acc:0.692]
Epoch [3/120    avg_loss:1.206, val_acc:0.702]
Epoch [4/120    avg_loss:0.941, val_acc:0.808]
Epoch [5/120    avg_loss:0.786, val_acc:0.812]
Epoch [6/120    avg_loss:0.727, val_acc:0.781]
Epoch [7/120    avg_loss:0.636, val_acc:0.860]
Epoch [8/120    avg_loss:0.531, val_acc:0.877]
Epoch [9/120    avg_loss:0.494, val_acc:0.885]
Epoch [10/120    avg_loss:0.530, val_acc:0.867]
Epoch [11/120    avg_loss:0.402, val_acc:0.912]
Epoch [12/120    avg_loss:0.343, val_acc:0.942]
Epoch [13/120    avg_loss:0.325, val_acc:0.933]
Epoch [14/120    avg_loss:0.435, val_acc:0.938]
Epoch [15/120    avg_loss:0.293, val_acc:0.919]
Epoch [16/120    avg_loss:0.350, val_acc:0.927]
Epoch [17/120    avg_loss:0.332, val_acc:0.954]
Epoch [18/120    avg_loss:0.219, val_acc:0.967]
Epoch [19/120    avg_loss:0.193, val_acc:0.967]
Epoch [20/120    avg_loss:0.229, val_acc:0.969]
Epoch [21/120    avg_loss:0.181, val_acc:0.983]
Epoch [22/120    avg_loss:0.151, val_acc:0.981]
Epoch [23/120    avg_loss:0.172, val_acc:0.950]
Epoch [24/120    avg_loss:0.177, val_acc:0.975]
Epoch [25/120    avg_loss:0.208, val_acc:0.958]
Epoch [26/120    avg_loss:0.179, val_acc:0.979]
Epoch [27/120    avg_loss:0.112, val_acc:0.981]
Epoch [28/120    avg_loss:0.107, val_acc:0.990]
Epoch [29/120    avg_loss:0.096, val_acc:0.988]
Epoch [30/120    avg_loss:0.139, val_acc:0.960]
Epoch [31/120    avg_loss:0.147, val_acc:0.977]
Epoch [32/120    avg_loss:0.176, val_acc:0.960]
Epoch [33/120    avg_loss:0.150, val_acc:0.971]
Epoch [34/120    avg_loss:0.150, val_acc:0.985]
Epoch [35/120    avg_loss:0.134, val_acc:0.969]
Epoch [36/120    avg_loss:0.132, val_acc:0.981]
Epoch [37/120    avg_loss:0.115, val_acc:0.988]
Epoch [38/120    avg_loss:0.107, val_acc:0.983]
Epoch [39/120    avg_loss:0.151, val_acc:0.988]
Epoch [40/120    avg_loss:0.122, val_acc:0.981]
Epoch [41/120    avg_loss:0.099, val_acc:0.994]
Epoch [42/120    avg_loss:0.077, val_acc:0.990]
Epoch [43/120    avg_loss:0.047, val_acc:0.998]
Epoch [44/120    avg_loss:0.071, val_acc:0.992]
Epoch [45/120    avg_loss:0.097, val_acc:0.990]
Epoch [46/120    avg_loss:0.075, val_acc:0.992]
Epoch [47/120    avg_loss:0.069, val_acc:0.994]
Epoch [48/120    avg_loss:0.076, val_acc:0.994]
Epoch [49/120    avg_loss:0.064, val_acc:0.998]
Epoch [50/120    avg_loss:0.065, val_acc:0.994]
Epoch [51/120    avg_loss:0.047, val_acc:0.996]
Epoch [52/120    avg_loss:0.056, val_acc:0.985]
Epoch [53/120    avg_loss:0.083, val_acc:0.985]
Epoch [54/120    avg_loss:0.036, val_acc:0.992]
Epoch [55/120    avg_loss:0.048, val_acc:0.994]
Epoch [56/120    avg_loss:0.035, val_acc:0.988]
Epoch [57/120    avg_loss:0.039, val_acc:0.994]
Epoch [58/120    avg_loss:0.044, val_acc:0.992]
Epoch [59/120    avg_loss:0.041, val_acc:0.994]
Epoch [60/120    avg_loss:0.026, val_acc:0.994]
Epoch [61/120    avg_loss:0.039, val_acc:0.992]
Epoch [62/120    avg_loss:0.034, val_acc:0.994]
Epoch [63/120    avg_loss:0.041, val_acc:0.994]
Epoch [64/120    avg_loss:0.020, val_acc:0.996]
Epoch [65/120    avg_loss:0.019, val_acc:0.996]
Epoch [66/120    avg_loss:0.022, val_acc:0.994]
Epoch [67/120    avg_loss:0.017, val_acc:0.994]
Epoch [68/120    avg_loss:0.018, val_acc:0.994]
Epoch [69/120    avg_loss:0.021, val_acc:0.996]
Epoch [70/120    avg_loss:0.020, val_acc:0.996]
Epoch [71/120    avg_loss:0.012, val_acc:0.994]
Epoch [72/120    avg_loss:0.027, val_acc:0.994]
Epoch [73/120    avg_loss:0.025, val_acc:0.994]
Epoch [74/120    avg_loss:0.020, val_acc:0.994]
Epoch [75/120    avg_loss:0.026, val_acc:0.996]
Epoch [76/120    avg_loss:0.017, val_acc:0.996]
Epoch [77/120    avg_loss:0.015, val_acc:0.996]
Epoch [78/120    avg_loss:0.018, val_acc:0.996]
Epoch [79/120    avg_loss:0.032, val_acc:0.996]
Epoch [80/120    avg_loss:0.020, val_acc:0.996]
Epoch [81/120    avg_loss:0.015, val_acc:0.996]
Epoch [82/120    avg_loss:0.010, val_acc:0.996]
Epoch [83/120    avg_loss:0.020, val_acc:0.996]
Epoch [84/120    avg_loss:0.014, val_acc:0.996]
Epoch [85/120    avg_loss:0.015, val_acc:0.996]
Epoch [86/120    avg_loss:0.016, val_acc:0.996]
Epoch [87/120    avg_loss:0.013, val_acc:0.996]
Epoch [88/120    avg_loss:0.016, val_acc:0.996]
Epoch [89/120    avg_loss:0.013, val_acc:0.996]
Epoch [90/120    avg_loss:0.011, val_acc:0.996]
Epoch [91/120    avg_loss:0.015, val_acc:0.996]
Epoch [92/120    avg_loss:0.019, val_acc:0.996]
Epoch [93/120    avg_loss:0.012, val_acc:0.996]
Epoch [94/120    avg_loss:0.014, val_acc:0.996]
Epoch [95/120    avg_loss:0.028, val_acc:0.996]
Epoch [96/120    avg_loss:0.014, val_acc:0.996]
Epoch [97/120    avg_loss:0.013, val_acc:0.996]
Epoch [98/120    avg_loss:0.010, val_acc:0.996]
Epoch [99/120    avg_loss:0.015, val_acc:0.996]
Epoch [100/120    avg_loss:0.013, val_acc:0.996]
Epoch [101/120    avg_loss:0.016, val_acc:0.996]
Epoch [102/120    avg_loss:0.019, val_acc:0.996]
Epoch [103/120    avg_loss:0.026, val_acc:0.996]
Epoch [104/120    avg_loss:0.011, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.014, val_acc:0.996]
Epoch [108/120    avg_loss:0.013, val_acc:0.996]
Epoch [109/120    avg_loss:0.015, val_acc:0.996]
Epoch [110/120    avg_loss:0.017, val_acc:0.996]
Epoch [111/120    avg_loss:0.015, val_acc:0.996]
Epoch [112/120    avg_loss:0.012, val_acc:0.996]
Epoch [113/120    avg_loss:0.037, val_acc:0.996]
Epoch [114/120    avg_loss:0.013, val_acc:0.996]
Epoch [115/120    avg_loss:0.012, val_acc:0.996]
Epoch [116/120    avg_loss:0.016, val_acc:0.996]
Epoch [117/120    avg_loss:0.020, val_acc:0.996]
Epoch [118/120    avg_loss:0.017, val_acc:0.996]
Epoch [119/120    avg_loss:0.016, val_acc:0.996]
Epoch [120/120    avg_loss:0.010, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.99319728 0.99782135 0.97379913 0.96167247
 1.         0.98378378 1.         1.         1.         0.98562092
 0.9877095  1.        ]

Kappa:
0.9938281502752785
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4acef0f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.302, val_acc:0.506]
Epoch [2/120    avg_loss:1.598, val_acc:0.685]
Epoch [3/120    avg_loss:1.171, val_acc:0.694]
Epoch [4/120    avg_loss:0.870, val_acc:0.779]
Epoch [5/120    avg_loss:0.792, val_acc:0.848]
Epoch [6/120    avg_loss:0.584, val_acc:0.915]
Epoch [7/120    avg_loss:0.515, val_acc:0.904]
Epoch [8/120    avg_loss:0.408, val_acc:0.925]
Epoch [9/120    avg_loss:0.391, val_acc:0.910]
Epoch [10/120    avg_loss:0.341, val_acc:0.929]
Epoch [11/120    avg_loss:0.344, val_acc:0.931]
Epoch [12/120    avg_loss:0.384, val_acc:0.927]
Epoch [13/120    avg_loss:0.301, val_acc:0.935]
Epoch [14/120    avg_loss:0.277, val_acc:0.948]
Epoch [15/120    avg_loss:0.257, val_acc:0.931]
Epoch [16/120    avg_loss:0.188, val_acc:0.960]
Epoch [17/120    avg_loss:0.177, val_acc:0.954]
Epoch [18/120    avg_loss:0.200, val_acc:0.933]
Epoch [19/120    avg_loss:0.214, val_acc:0.952]
Epoch [20/120    avg_loss:0.204, val_acc:0.963]
Epoch [21/120    avg_loss:0.214, val_acc:0.948]
Epoch [22/120    avg_loss:0.225, val_acc:0.942]
Epoch [23/120    avg_loss:0.127, val_acc:0.960]
Epoch [24/120    avg_loss:0.178, val_acc:0.963]
Epoch [25/120    avg_loss:0.168, val_acc:0.948]
Epoch [26/120    avg_loss:0.173, val_acc:0.965]
Epoch [27/120    avg_loss:0.156, val_acc:0.969]
Epoch [28/120    avg_loss:0.097, val_acc:0.977]
Epoch [29/120    avg_loss:0.068, val_acc:0.963]
Epoch [30/120    avg_loss:0.099, val_acc:0.981]
Epoch [31/120    avg_loss:0.119, val_acc:0.969]
Epoch [32/120    avg_loss:0.154, val_acc:0.963]
Epoch [33/120    avg_loss:0.121, val_acc:0.971]
Epoch [34/120    avg_loss:0.135, val_acc:0.971]
Epoch [35/120    avg_loss:0.124, val_acc:0.975]
Epoch [36/120    avg_loss:0.074, val_acc:0.960]
Epoch [37/120    avg_loss:0.082, val_acc:0.971]
Epoch [38/120    avg_loss:0.093, val_acc:0.969]
Epoch [39/120    avg_loss:0.077, val_acc:0.969]
Epoch [40/120    avg_loss:0.082, val_acc:0.977]
Epoch [41/120    avg_loss:0.063, val_acc:0.981]
Epoch [42/120    avg_loss:0.080, val_acc:0.981]
Epoch [43/120    avg_loss:0.065, val_acc:0.963]
Epoch [44/120    avg_loss:0.067, val_acc:0.979]
Epoch [45/120    avg_loss:0.058, val_acc:0.985]
Epoch [46/120    avg_loss:0.078, val_acc:0.985]
Epoch [47/120    avg_loss:0.075, val_acc:0.979]
Epoch [48/120    avg_loss:0.090, val_acc:0.967]
Epoch [49/120    avg_loss:0.061, val_acc:0.973]
Epoch [50/120    avg_loss:0.051, val_acc:0.981]
Epoch [51/120    avg_loss:0.039, val_acc:0.985]
Epoch [52/120    avg_loss:0.028, val_acc:0.988]
Epoch [53/120    avg_loss:0.054, val_acc:0.983]
Epoch [54/120    avg_loss:0.041, val_acc:0.985]
Epoch [55/120    avg_loss:0.047, val_acc:0.988]
Epoch [56/120    avg_loss:0.054, val_acc:0.981]
Epoch [57/120    avg_loss:0.073, val_acc:0.990]
Epoch [58/120    avg_loss:0.039, val_acc:0.985]
Epoch [59/120    avg_loss:0.042, val_acc:0.983]
Epoch [60/120    avg_loss:0.060, val_acc:0.971]
Epoch [61/120    avg_loss:0.055, val_acc:0.985]
Epoch [62/120    avg_loss:0.034, val_acc:0.992]
Epoch [63/120    avg_loss:0.018, val_acc:0.994]
Epoch [64/120    avg_loss:0.036, val_acc:0.992]
Epoch [65/120    avg_loss:0.035, val_acc:0.981]
Epoch [66/120    avg_loss:0.031, val_acc:0.983]
Epoch [67/120    avg_loss:0.027, val_acc:0.994]
Epoch [68/120    avg_loss:0.021, val_acc:0.988]
Epoch [69/120    avg_loss:0.020, val_acc:0.988]
Epoch [70/120    avg_loss:0.011, val_acc:0.988]
Epoch [71/120    avg_loss:0.022, val_acc:0.990]
Epoch [72/120    avg_loss:0.052, val_acc:0.988]
Epoch [73/120    avg_loss:0.026, val_acc:0.979]
Epoch [74/120    avg_loss:0.040, val_acc:0.981]
Epoch [75/120    avg_loss:0.028, val_acc:0.983]
Epoch [76/120    avg_loss:0.026, val_acc:0.990]
Epoch [77/120    avg_loss:0.029, val_acc:0.985]
Epoch [78/120    avg_loss:0.033, val_acc:0.988]
Epoch [79/120    avg_loss:0.039, val_acc:0.965]
Epoch [80/120    avg_loss:0.040, val_acc:0.988]
Epoch [81/120    avg_loss:0.013, val_acc:0.990]
Epoch [82/120    avg_loss:0.023, val_acc:0.992]
Epoch [83/120    avg_loss:0.015, val_acc:0.994]
Epoch [84/120    avg_loss:0.013, val_acc:0.994]
Epoch [85/120    avg_loss:0.018, val_acc:0.994]
Epoch [86/120    avg_loss:0.012, val_acc:0.994]
Epoch [87/120    avg_loss:0.011, val_acc:0.994]
Epoch [88/120    avg_loss:0.015, val_acc:0.994]
Epoch [89/120    avg_loss:0.007, val_acc:0.994]
Epoch [90/120    avg_loss:0.012, val_acc:0.996]
Epoch [91/120    avg_loss:0.017, val_acc:0.994]
Epoch [92/120    avg_loss:0.009, val_acc:0.994]
Epoch [93/120    avg_loss:0.024, val_acc:0.992]
Epoch [94/120    avg_loss:0.009, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.992]
Epoch [96/120    avg_loss:0.008, val_acc:0.990]
Epoch [97/120    avg_loss:0.010, val_acc:0.990]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.019, val_acc:0.990]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.990]
Epoch [102/120    avg_loss:0.009, val_acc:0.990]
Epoch [103/120    avg_loss:0.012, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.990]
Epoch [105/120    avg_loss:0.014, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.990]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.011, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.016, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.016, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   9   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  14 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         1.         0.97550111 0.94989107 0.95238095
 1.         1.         1.         0.9978678  1.         0.98177083
 0.98430493 1.        ]

Kappa:
0.9907425651146997
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1fc99ff898>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.282, val_acc:0.542]
Epoch [2/120    avg_loss:1.673, val_acc:0.665]
Epoch [3/120    avg_loss:1.279, val_acc:0.696]
Epoch [4/120    avg_loss:0.979, val_acc:0.775]
Epoch [5/120    avg_loss:0.762, val_acc:0.856]
Epoch [6/120    avg_loss:0.685, val_acc:0.846]
Epoch [7/120    avg_loss:0.644, val_acc:0.879]
Epoch [8/120    avg_loss:0.601, val_acc:0.883]
Epoch [9/120    avg_loss:0.448, val_acc:0.908]
Epoch [10/120    avg_loss:0.409, val_acc:0.883]
Epoch [11/120    avg_loss:0.353, val_acc:0.927]
Epoch [12/120    avg_loss:0.299, val_acc:0.927]
Epoch [13/120    avg_loss:0.291, val_acc:0.917]
Epoch [14/120    avg_loss:0.337, val_acc:0.852]
Epoch [15/120    avg_loss:0.463, val_acc:0.904]
Epoch [16/120    avg_loss:0.316, val_acc:0.929]
Epoch [17/120    avg_loss:0.250, val_acc:0.954]
Epoch [18/120    avg_loss:0.231, val_acc:0.956]
Epoch [19/120    avg_loss:0.236, val_acc:0.952]
Epoch [20/120    avg_loss:0.238, val_acc:0.948]
Epoch [21/120    avg_loss:0.251, val_acc:0.952]
Epoch [22/120    avg_loss:0.198, val_acc:0.948]
Epoch [23/120    avg_loss:0.200, val_acc:0.956]
Epoch [24/120    avg_loss:0.204, val_acc:0.954]
Epoch [25/120    avg_loss:0.172, val_acc:0.971]
Epoch [26/120    avg_loss:0.102, val_acc:0.969]
Epoch [27/120    avg_loss:0.127, val_acc:0.973]
Epoch [28/120    avg_loss:0.191, val_acc:0.956]
Epoch [29/120    avg_loss:0.159, val_acc:0.954]
Epoch [30/120    avg_loss:0.158, val_acc:0.965]
Epoch [31/120    avg_loss:0.160, val_acc:0.942]
Epoch [32/120    avg_loss:0.119, val_acc:0.971]
Epoch [33/120    avg_loss:0.111, val_acc:0.973]
Epoch [34/120    avg_loss:0.167, val_acc:0.958]
Epoch [35/120    avg_loss:0.178, val_acc:0.983]
Epoch [36/120    avg_loss:0.085, val_acc:0.975]
Epoch [37/120    avg_loss:0.084, val_acc:0.977]
Epoch [38/120    avg_loss:0.091, val_acc:0.975]
Epoch [39/120    avg_loss:0.092, val_acc:0.979]
Epoch [40/120    avg_loss:0.095, val_acc:0.975]
Epoch [41/120    avg_loss:0.079, val_acc:0.969]
Epoch [42/120    avg_loss:0.089, val_acc:0.981]
Epoch [43/120    avg_loss:0.065, val_acc:0.977]
Epoch [44/120    avg_loss:0.077, val_acc:0.977]
Epoch [45/120    avg_loss:0.057, val_acc:0.979]
Epoch [46/120    avg_loss:0.090, val_acc:0.965]
Epoch [47/120    avg_loss:0.127, val_acc:0.977]
Epoch [48/120    avg_loss:0.099, val_acc:0.979]
Epoch [49/120    avg_loss:0.055, val_acc:0.981]
Epoch [50/120    avg_loss:0.041, val_acc:0.979]
Epoch [51/120    avg_loss:0.051, val_acc:0.975]
Epoch [52/120    avg_loss:0.032, val_acc:0.973]
Epoch [53/120    avg_loss:0.034, val_acc:0.973]
Epoch [54/120    avg_loss:0.036, val_acc:0.977]
Epoch [55/120    avg_loss:0.037, val_acc:0.979]
Epoch [56/120    avg_loss:0.036, val_acc:0.977]
Epoch [57/120    avg_loss:0.035, val_acc:0.977]
Epoch [58/120    avg_loss:0.041, val_acc:0.977]
Epoch [59/120    avg_loss:0.032, val_acc:0.977]
Epoch [60/120    avg_loss:0.037, val_acc:0.977]
Epoch [61/120    avg_loss:0.027, val_acc:0.977]
Epoch [62/120    avg_loss:0.044, val_acc:0.977]
Epoch [63/120    avg_loss:0.026, val_acc:0.977]
Epoch [64/120    avg_loss:0.035, val_acc:0.977]
Epoch [65/120    avg_loss:0.039, val_acc:0.977]
Epoch [66/120    avg_loss:0.033, val_acc:0.977]
Epoch [67/120    avg_loss:0.030, val_acc:0.977]
Epoch [68/120    avg_loss:0.026, val_acc:0.977]
Epoch [69/120    avg_loss:0.035, val_acc:0.977]
Epoch [70/120    avg_loss:0.034, val_acc:0.977]
Epoch [71/120    avg_loss:0.027, val_acc:0.977]
Epoch [72/120    avg_loss:0.036, val_acc:0.977]
Epoch [73/120    avg_loss:0.034, val_acc:0.977]
Epoch [74/120    avg_loss:0.034, val_acc:0.977]
Epoch [75/120    avg_loss:0.046, val_acc:0.977]
Epoch [76/120    avg_loss:0.031, val_acc:0.977]
Epoch [77/120    avg_loss:0.021, val_acc:0.977]
Epoch [78/120    avg_loss:0.028, val_acc:0.977]
Epoch [79/120    avg_loss:0.036, val_acc:0.977]
Epoch [80/120    avg_loss:0.034, val_acc:0.977]
Epoch [81/120    avg_loss:0.033, val_acc:0.977]
Epoch [82/120    avg_loss:0.026, val_acc:0.977]
Epoch [83/120    avg_loss:0.028, val_acc:0.977]
Epoch [84/120    avg_loss:0.032, val_acc:0.977]
Epoch [85/120    avg_loss:0.037, val_acc:0.977]
Epoch [86/120    avg_loss:0.042, val_acc:0.977]
Epoch [87/120    avg_loss:0.035, val_acc:0.977]
Epoch [88/120    avg_loss:0.028, val_acc:0.977]
Epoch [89/120    avg_loss:0.037, val_acc:0.977]
Epoch [90/120    avg_loss:0.031, val_acc:0.977]
Epoch [91/120    avg_loss:0.027, val_acc:0.977]
Epoch [92/120    avg_loss:0.027, val_acc:0.977]
Epoch [93/120    avg_loss:0.034, val_acc:0.977]
Epoch [94/120    avg_loss:0.028, val_acc:0.977]
Epoch [95/120    avg_loss:0.023, val_acc:0.977]
Epoch [96/120    avg_loss:0.036, val_acc:0.977]
Epoch [97/120    avg_loss:0.029, val_acc:0.977]
Epoch [98/120    avg_loss:0.027, val_acc:0.977]
Epoch [99/120    avg_loss:0.033, val_acc:0.977]
Epoch [100/120    avg_loss:0.029, val_acc:0.977]
Epoch [101/120    avg_loss:0.031, val_acc:0.977]
Epoch [102/120    avg_loss:0.033, val_acc:0.977]
Epoch [103/120    avg_loss:0.035, val_acc:0.977]
Epoch [104/120    avg_loss:0.029, val_acc:0.977]
Epoch [105/120    avg_loss:0.026, val_acc:0.977]
Epoch [106/120    avg_loss:0.034, val_acc:0.977]
Epoch [107/120    avg_loss:0.030, val_acc:0.977]
Epoch [108/120    avg_loss:0.031, val_acc:0.977]
Epoch [109/120    avg_loss:0.033, val_acc:0.977]
Epoch [110/120    avg_loss:0.024, val_acc:0.977]
Epoch [111/120    avg_loss:0.028, val_acc:0.977]
Epoch [112/120    avg_loss:0.041, val_acc:0.977]
Epoch [113/120    avg_loss:0.041, val_acc:0.977]
Epoch [114/120    avg_loss:0.043, val_acc:0.977]
Epoch [115/120    avg_loss:0.022, val_acc:0.977]
Epoch [116/120    avg_loss:0.025, val_acc:0.977]
Epoch [117/120    avg_loss:0.031, val_acc:0.977]
Epoch [118/120    avg_loss:0.025, val_acc:0.977]
Epoch [119/120    avg_loss:0.033, val_acc:0.977]
Epoch [120/120    avg_loss:0.031, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219  11   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  14 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.98426966 0.97550111 0.93418259 0.93286219
 1.         0.96132597 1.         1.         1.         0.98177083
 0.98320269 1.        ]

Kappa:
0.9876558032201002
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f693e7828>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.322, val_acc:0.517]
Epoch [2/120    avg_loss:1.620, val_acc:0.585]
Epoch [3/120    avg_loss:1.218, val_acc:0.752]
Epoch [4/120    avg_loss:0.961, val_acc:0.777]
Epoch [5/120    avg_loss:0.756, val_acc:0.823]
Epoch [6/120    avg_loss:0.747, val_acc:0.800]
Epoch [7/120    avg_loss:0.574, val_acc:0.875]
Epoch [8/120    avg_loss:0.472, val_acc:0.898]
Epoch [9/120    avg_loss:0.395, val_acc:0.887]
Epoch [10/120    avg_loss:0.467, val_acc:0.879]
Epoch [11/120    avg_loss:0.465, val_acc:0.906]
Epoch [12/120    avg_loss:0.381, val_acc:0.925]
Epoch [13/120    avg_loss:0.301, val_acc:0.938]
Epoch [14/120    avg_loss:0.252, val_acc:0.946]
Epoch [15/120    avg_loss:0.236, val_acc:0.929]
Epoch [16/120    avg_loss:0.289, val_acc:0.919]
Epoch [17/120    avg_loss:0.226, val_acc:0.944]
Epoch [18/120    avg_loss:0.194, val_acc:0.969]
Epoch [19/120    avg_loss:0.185, val_acc:0.948]
Epoch [20/120    avg_loss:0.196, val_acc:0.950]
Epoch [21/120    avg_loss:0.198, val_acc:0.967]
Epoch [22/120    avg_loss:0.222, val_acc:0.942]
Epoch [23/120    avg_loss:0.233, val_acc:0.931]
Epoch [24/120    avg_loss:0.243, val_acc:0.944]
Epoch [25/120    avg_loss:0.215, val_acc:0.938]
Epoch [26/120    avg_loss:0.212, val_acc:0.958]
Epoch [27/120    avg_loss:0.172, val_acc:0.969]
Epoch [28/120    avg_loss:0.190, val_acc:0.965]
Epoch [29/120    avg_loss:0.157, val_acc:0.969]
Epoch [30/120    avg_loss:0.119, val_acc:0.958]
Epoch [31/120    avg_loss:0.094, val_acc:0.973]
Epoch [32/120    avg_loss:0.088, val_acc:0.979]
Epoch [33/120    avg_loss:0.091, val_acc:0.981]
Epoch [34/120    avg_loss:0.070, val_acc:0.973]
Epoch [35/120    avg_loss:0.074, val_acc:0.973]
Epoch [36/120    avg_loss:0.068, val_acc:0.977]
Epoch [37/120    avg_loss:0.062, val_acc:0.975]
Epoch [38/120    avg_loss:0.066, val_acc:0.960]
Epoch [39/120    avg_loss:0.097, val_acc:0.981]
Epoch [40/120    avg_loss:0.054, val_acc:0.969]
Epoch [41/120    avg_loss:0.045, val_acc:0.979]
Epoch [42/120    avg_loss:0.045, val_acc:0.977]
Epoch [43/120    avg_loss:0.070, val_acc:0.975]
Epoch [44/120    avg_loss:0.043, val_acc:0.983]
Epoch [45/120    avg_loss:0.050, val_acc:0.975]
Epoch [46/120    avg_loss:0.039, val_acc:0.979]
Epoch [47/120    avg_loss:0.102, val_acc:0.985]
Epoch [48/120    avg_loss:0.082, val_acc:0.988]
Epoch [49/120    avg_loss:0.102, val_acc:0.958]
Epoch [50/120    avg_loss:0.057, val_acc:0.979]
Epoch [51/120    avg_loss:0.050, val_acc:0.992]
Epoch [52/120    avg_loss:0.042, val_acc:0.979]
Epoch [53/120    avg_loss:0.061, val_acc:0.977]
Epoch [54/120    avg_loss:0.050, val_acc:0.977]
Epoch [55/120    avg_loss:0.041, val_acc:0.988]
Epoch [56/120    avg_loss:0.038, val_acc:0.979]
Epoch [57/120    avg_loss:0.047, val_acc:0.975]
Epoch [58/120    avg_loss:0.086, val_acc:0.977]
Epoch [59/120    avg_loss:0.080, val_acc:0.977]
Epoch [60/120    avg_loss:0.090, val_acc:0.973]
Epoch [61/120    avg_loss:0.075, val_acc:0.977]
Epoch [62/120    avg_loss:0.040, val_acc:0.983]
Epoch [63/120    avg_loss:0.047, val_acc:0.983]
Epoch [64/120    avg_loss:0.025, val_acc:0.985]
Epoch [65/120    avg_loss:0.032, val_acc:0.985]
Epoch [66/120    avg_loss:0.021, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.983]
Epoch [68/120    avg_loss:0.024, val_acc:0.983]
Epoch [69/120    avg_loss:0.015, val_acc:0.983]
Epoch [70/120    avg_loss:0.021, val_acc:0.983]
Epoch [71/120    avg_loss:0.013, val_acc:0.985]
Epoch [72/120    avg_loss:0.027, val_acc:0.985]
Epoch [73/120    avg_loss:0.012, val_acc:0.985]
Epoch [74/120    avg_loss:0.023, val_acc:0.985]
Epoch [75/120    avg_loss:0.014, val_acc:0.988]
Epoch [76/120    avg_loss:0.021, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.990]
Epoch [78/120    avg_loss:0.015, val_acc:0.990]
Epoch [79/120    avg_loss:0.024, val_acc:0.990]
Epoch [80/120    avg_loss:0.019, val_acc:0.990]
Epoch [81/120    avg_loss:0.027, val_acc:0.990]
Epoch [82/120    avg_loss:0.015, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.018, val_acc:0.988]
Epoch [85/120    avg_loss:0.020, val_acc:0.988]
Epoch [86/120    avg_loss:0.023, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.988]
Epoch [89/120    avg_loss:0.024, val_acc:0.988]
Epoch [90/120    avg_loss:0.013, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.988]
Epoch [94/120    avg_loss:0.011, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.988]
Epoch [96/120    avg_loss:0.016, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.988]
Epoch [99/120    avg_loss:0.018, val_acc:0.988]
Epoch [100/120    avg_loss:0.021, val_acc:0.988]
Epoch [101/120    avg_loss:0.020, val_acc:0.988]
Epoch [102/120    avg_loss:0.025, val_acc:0.988]
Epoch [103/120    avg_loss:0.022, val_acc:0.988]
Epoch [104/120    avg_loss:0.014, val_acc:0.988]
Epoch [105/120    avg_loss:0.021, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.028, val_acc:0.988]
Epoch [110/120    avg_loss:0.017, val_acc:0.988]
Epoch [111/120    avg_loss:0.015, val_acc:0.988]
Epoch [112/120    avg_loss:0.016, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.013, val_acc:0.988]
Epoch [115/120    avg_loss:0.022, val_acc:0.988]
Epoch [116/120    avg_loss:0.032, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.988]
Epoch [118/120    avg_loss:0.018, val_acc:0.988]
Epoch [119/120    avg_loss:0.015, val_acc:0.988]
Epoch [120/120    avg_loss:0.020, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99095023 0.99343545 0.96035242 0.94880546
 1.         0.97826087 1.         1.         1.         0.9843342
 0.98657718 1.        ]

Kappa:
0.9919293030022375
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe5c5eb828>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.280, val_acc:0.496]
Epoch [2/120    avg_loss:1.646, val_acc:0.715]
Epoch [3/120    avg_loss:1.250, val_acc:0.796]
Epoch [4/120    avg_loss:0.995, val_acc:0.765]
Epoch [5/120    avg_loss:0.818, val_acc:0.810]
Epoch [6/120    avg_loss:0.689, val_acc:0.823]
Epoch [7/120    avg_loss:0.662, val_acc:0.850]
Epoch [8/120    avg_loss:0.542, val_acc:0.890]
Epoch [9/120    avg_loss:0.530, val_acc:0.892]
Epoch [10/120    avg_loss:0.477, val_acc:0.910]
Epoch [11/120    avg_loss:0.457, val_acc:0.915]
Epoch [12/120    avg_loss:0.316, val_acc:0.919]
Epoch [13/120    avg_loss:0.281, val_acc:0.958]
Epoch [14/120    avg_loss:0.272, val_acc:0.935]
Epoch [15/120    avg_loss:0.283, val_acc:0.933]
Epoch [16/120    avg_loss:0.245, val_acc:0.946]
Epoch [17/120    avg_loss:0.295, val_acc:0.917]
Epoch [18/120    avg_loss:0.292, val_acc:0.950]
Epoch [19/120    avg_loss:0.286, val_acc:0.935]
Epoch [20/120    avg_loss:0.240, val_acc:0.952]
Epoch [21/120    avg_loss:0.273, val_acc:0.952]
Epoch [22/120    avg_loss:0.243, val_acc:0.969]
Epoch [23/120    avg_loss:0.178, val_acc:0.963]
Epoch [24/120    avg_loss:0.197, val_acc:0.967]
Epoch [25/120    avg_loss:0.144, val_acc:0.965]
Epoch [26/120    avg_loss:0.123, val_acc:0.973]
Epoch [27/120    avg_loss:0.104, val_acc:0.975]
Epoch [28/120    avg_loss:0.141, val_acc:0.969]
Epoch [29/120    avg_loss:0.172, val_acc:0.942]
Epoch [30/120    avg_loss:0.174, val_acc:0.958]
Epoch [31/120    avg_loss:0.143, val_acc:0.965]
Epoch [32/120    avg_loss:0.205, val_acc:0.944]
Epoch [33/120    avg_loss:0.171, val_acc:0.971]
Epoch [34/120    avg_loss:0.121, val_acc:0.983]
Epoch [35/120    avg_loss:0.084, val_acc:0.981]
Epoch [36/120    avg_loss:0.072, val_acc:0.979]
Epoch [37/120    avg_loss:0.064, val_acc:0.983]
Epoch [38/120    avg_loss:0.058, val_acc:0.983]
Epoch [39/120    avg_loss:0.043, val_acc:0.988]
Epoch [40/120    avg_loss:0.046, val_acc:0.983]
Epoch [41/120    avg_loss:0.049, val_acc:0.979]
Epoch [42/120    avg_loss:0.060, val_acc:0.967]
Epoch [43/120    avg_loss:0.086, val_acc:0.960]
Epoch [44/120    avg_loss:0.086, val_acc:0.967]
Epoch [45/120    avg_loss:0.100, val_acc:0.985]
Epoch [46/120    avg_loss:0.118, val_acc:0.971]
Epoch [47/120    avg_loss:0.148, val_acc:0.975]
Epoch [48/120    avg_loss:0.081, val_acc:0.969]
Epoch [49/120    avg_loss:0.097, val_acc:0.988]
Epoch [50/120    avg_loss:0.073, val_acc:0.981]
Epoch [51/120    avg_loss:0.041, val_acc:0.977]
Epoch [52/120    avg_loss:0.025, val_acc:0.985]
Epoch [53/120    avg_loss:0.048, val_acc:0.988]
Epoch [54/120    avg_loss:0.026, val_acc:0.990]
Epoch [55/120    avg_loss:0.036, val_acc:0.985]
Epoch [56/120    avg_loss:0.059, val_acc:0.985]
Epoch [57/120    avg_loss:0.056, val_acc:0.977]
Epoch [58/120    avg_loss:0.043, val_acc:0.975]
Epoch [59/120    avg_loss:0.038, val_acc:0.979]
Epoch [60/120    avg_loss:0.045, val_acc:0.977]
Epoch [61/120    avg_loss:0.048, val_acc:0.981]
Epoch [62/120    avg_loss:0.030, val_acc:0.994]
Epoch [63/120    avg_loss:0.027, val_acc:0.988]
Epoch [64/120    avg_loss:0.024, val_acc:0.985]
Epoch [65/120    avg_loss:0.029, val_acc:0.983]
Epoch [66/120    avg_loss:0.039, val_acc:0.988]
Epoch [67/120    avg_loss:0.043, val_acc:0.988]
Epoch [68/120    avg_loss:0.044, val_acc:0.977]
Epoch [69/120    avg_loss:0.116, val_acc:0.990]
Epoch [70/120    avg_loss:0.082, val_acc:0.975]
Epoch [71/120    avg_loss:0.056, val_acc:0.979]
Epoch [72/120    avg_loss:0.047, val_acc:0.977]
Epoch [73/120    avg_loss:0.036, val_acc:0.985]
Epoch [74/120    avg_loss:0.022, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.027, val_acc:0.985]
Epoch [77/120    avg_loss:0.025, val_acc:0.985]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.019, val_acc:0.985]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.017, val_acc:0.985]
Epoch [82/120    avg_loss:0.013, val_acc:0.985]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.016, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.028, val_acc:0.988]
Epoch [88/120    avg_loss:0.016, val_acc:0.988]
Epoch [89/120    avg_loss:0.015, val_acc:0.988]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.988]
Epoch [92/120    avg_loss:0.020, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.985]
Epoch [97/120    avg_loss:0.022, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.021, val_acc:0.988]
Epoch [100/120    avg_loss:0.022, val_acc:0.988]
Epoch [101/120    avg_loss:0.011, val_acc:0.988]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.013, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.016, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.016, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.014, val_acc:0.988]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.988]
Epoch [120/120    avg_loss:0.014, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  17 436   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.98871332 0.98678414 0.94247788 0.93602694
 1.         0.9726776  1.         1.         1.         0.97795071
 0.97977528 1.        ]

Kappa:
0.9886062858865681
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fce61f33710>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.324, val_acc:0.465]
Epoch [2/120    avg_loss:1.580, val_acc:0.625]
Epoch [3/120    avg_loss:1.193, val_acc:0.773]
Epoch [4/120    avg_loss:0.923, val_acc:0.817]
Epoch [5/120    avg_loss:0.815, val_acc:0.844]
Epoch [6/120    avg_loss:0.695, val_acc:0.852]
Epoch [7/120    avg_loss:0.645, val_acc:0.846]
Epoch [8/120    avg_loss:0.577, val_acc:0.846]
Epoch [9/120    avg_loss:0.510, val_acc:0.883]
Epoch [10/120    avg_loss:0.464, val_acc:0.881]
Epoch [11/120    avg_loss:0.512, val_acc:0.894]
Epoch [12/120    avg_loss:0.471, val_acc:0.906]
Epoch [13/120    avg_loss:0.457, val_acc:0.919]
Epoch [14/120    avg_loss:0.345, val_acc:0.923]
Epoch [15/120    avg_loss:0.307, val_acc:0.938]
Epoch [16/120    avg_loss:0.366, val_acc:0.925]
Epoch [17/120    avg_loss:0.259, val_acc:0.929]
Epoch [18/120    avg_loss:0.222, val_acc:0.946]
Epoch [19/120    avg_loss:0.203, val_acc:0.938]
Epoch [20/120    avg_loss:0.216, val_acc:0.950]
Epoch [21/120    avg_loss:0.176, val_acc:0.950]
Epoch [22/120    avg_loss:0.247, val_acc:0.931]
Epoch [23/120    avg_loss:0.250, val_acc:0.938]
Epoch [24/120    avg_loss:0.199, val_acc:0.948]
Epoch [25/120    avg_loss:0.170, val_acc:0.948]
Epoch [26/120    avg_loss:0.207, val_acc:0.950]
Epoch [27/120    avg_loss:0.234, val_acc:0.952]
Epoch [28/120    avg_loss:0.161, val_acc:0.950]
Epoch [29/120    avg_loss:0.169, val_acc:0.944]
Epoch [30/120    avg_loss:0.108, val_acc:0.950]
Epoch [31/120    avg_loss:0.134, val_acc:0.948]
Epoch [32/120    avg_loss:0.131, val_acc:0.954]
Epoch [33/120    avg_loss:0.204, val_acc:0.948]
Epoch [34/120    avg_loss:0.112, val_acc:0.950]
Epoch [35/120    avg_loss:0.100, val_acc:0.956]
Epoch [36/120    avg_loss:0.115, val_acc:0.956]
Epoch [37/120    avg_loss:0.132, val_acc:0.963]
Epoch [38/120    avg_loss:0.116, val_acc:0.958]
Epoch [39/120    avg_loss:0.082, val_acc:0.965]
Epoch [40/120    avg_loss:0.066, val_acc:0.967]
Epoch [41/120    avg_loss:0.080, val_acc:0.971]
Epoch [42/120    avg_loss:0.110, val_acc:0.963]
Epoch [43/120    avg_loss:0.097, val_acc:0.963]
Epoch [44/120    avg_loss:0.074, val_acc:0.963]
Epoch [45/120    avg_loss:0.100, val_acc:0.956]
Epoch [46/120    avg_loss:0.105, val_acc:0.975]
Epoch [47/120    avg_loss:0.093, val_acc:0.971]
Epoch [48/120    avg_loss:0.084, val_acc:0.971]
Epoch [49/120    avg_loss:0.124, val_acc:0.975]
Epoch [50/120    avg_loss:0.085, val_acc:0.963]
Epoch [51/120    avg_loss:0.080, val_acc:0.960]
Epoch [52/120    avg_loss:0.049, val_acc:0.971]
Epoch [53/120    avg_loss:0.034, val_acc:0.971]
Epoch [54/120    avg_loss:0.044, val_acc:0.973]
Epoch [55/120    avg_loss:0.048, val_acc:0.969]
Epoch [56/120    avg_loss:0.041, val_acc:0.971]
Epoch [57/120    avg_loss:0.054, val_acc:0.967]
Epoch [58/120    avg_loss:0.044, val_acc:0.971]
Epoch [59/120    avg_loss:0.029, val_acc:0.973]
Epoch [60/120    avg_loss:0.046, val_acc:0.965]
Epoch [61/120    avg_loss:0.064, val_acc:0.967]
Epoch [62/120    avg_loss:0.087, val_acc:0.971]
Epoch [63/120    avg_loss:0.054, val_acc:0.977]
Epoch [64/120    avg_loss:0.028, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.977]
Epoch [66/120    avg_loss:0.020, val_acc:0.975]
Epoch [67/120    avg_loss:0.035, val_acc:0.979]
Epoch [68/120    avg_loss:0.031, val_acc:0.979]
Epoch [69/120    avg_loss:0.016, val_acc:0.979]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.014, val_acc:0.975]
Epoch [72/120    avg_loss:0.021, val_acc:0.975]
Epoch [73/120    avg_loss:0.024, val_acc:0.975]
Epoch [74/120    avg_loss:0.030, val_acc:0.979]
Epoch [75/120    avg_loss:0.027, val_acc:0.979]
Epoch [76/120    avg_loss:0.021, val_acc:0.981]
Epoch [77/120    avg_loss:0.020, val_acc:0.981]
Epoch [78/120    avg_loss:0.015, val_acc:0.979]
Epoch [79/120    avg_loss:0.022, val_acc:0.979]
Epoch [80/120    avg_loss:0.015, val_acc:0.979]
Epoch [81/120    avg_loss:0.022, val_acc:0.979]
Epoch [82/120    avg_loss:0.021, val_acc:0.977]
Epoch [83/120    avg_loss:0.024, val_acc:0.979]
Epoch [84/120    avg_loss:0.015, val_acc:0.979]
Epoch [85/120    avg_loss:0.016, val_acc:0.979]
Epoch [86/120    avg_loss:0.015, val_acc:0.979]
Epoch [87/120    avg_loss:0.017, val_acc:0.979]
Epoch [88/120    avg_loss:0.016, val_acc:0.979]
Epoch [89/120    avg_loss:0.019, val_acc:0.979]
Epoch [90/120    avg_loss:0.016, val_acc:0.979]
Epoch [91/120    avg_loss:0.017, val_acc:0.979]
Epoch [92/120    avg_loss:0.027, val_acc:0.979]
Epoch [93/120    avg_loss:0.015, val_acc:0.979]
Epoch [94/120    avg_loss:0.019, val_acc:0.979]
Epoch [95/120    avg_loss:0.028, val_acc:0.979]
Epoch [96/120    avg_loss:0.027, val_acc:0.979]
Epoch [97/120    avg_loss:0.023, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.979]
Epoch [99/120    avg_loss:0.019, val_acc:0.979]
Epoch [100/120    avg_loss:0.018, val_acc:0.979]
Epoch [101/120    avg_loss:0.017, val_acc:0.979]
Epoch [102/120    avg_loss:0.020, val_acc:0.979]
Epoch [103/120    avg_loss:0.024, val_acc:0.979]
Epoch [104/120    avg_loss:0.015, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.979]
Epoch [106/120    avg_loss:0.012, val_acc:0.979]
Epoch [107/120    avg_loss:0.014, val_acc:0.979]
Epoch [108/120    avg_loss:0.019, val_acc:0.979]
Epoch [109/120    avg_loss:0.014, val_acc:0.979]
Epoch [110/120    avg_loss:0.020, val_acc:0.979]
Epoch [111/120    avg_loss:0.021, val_acc:0.979]
Epoch [112/120    avg_loss:0.016, val_acc:0.979]
Epoch [113/120    avg_loss:0.016, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.020, val_acc:0.979]
Epoch [116/120    avg_loss:0.015, val_acc:0.979]
Epoch [117/120    avg_loss:0.015, val_acc:0.979]
Epoch [118/120    avg_loss:0.021, val_acc:0.979]
Epoch [119/120    avg_loss:0.015, val_acc:0.979]
Epoch [120/120    avg_loss:0.014, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   1   0   0   5   0 200   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99927061 0.99545455 0.98901099 0.95384615 0.9632107
 0.98522167 0.98924731 1.         1.         1.         0.98305085
 0.98544233 1.        ]

Kappa:
0.9912172893628585
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32943de898>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.316, val_acc:0.519]
Epoch [2/120    avg_loss:1.643, val_acc:0.590]
Epoch [3/120    avg_loss:1.194, val_acc:0.748]
Epoch [4/120    avg_loss:0.881, val_acc:0.877]
Epoch [5/120    avg_loss:0.702, val_acc:0.838]
Epoch [6/120    avg_loss:0.638, val_acc:0.921]
Epoch [7/120    avg_loss:0.506, val_acc:0.906]
Epoch [8/120    avg_loss:0.439, val_acc:0.925]
Epoch [9/120    avg_loss:0.542, val_acc:0.940]
Epoch [10/120    avg_loss:0.364, val_acc:0.942]
Epoch [11/120    avg_loss:0.361, val_acc:0.923]
Epoch [12/120    avg_loss:0.356, val_acc:0.919]
Epoch [13/120    avg_loss:0.269, val_acc:0.942]
Epoch [14/120    avg_loss:0.307, val_acc:0.954]
Epoch [15/120    avg_loss:0.302, val_acc:0.946]
Epoch [16/120    avg_loss:0.274, val_acc:0.965]
Epoch [17/120    avg_loss:0.183, val_acc:0.971]
Epoch [18/120    avg_loss:0.226, val_acc:0.973]
Epoch [19/120    avg_loss:0.143, val_acc:0.956]
Epoch [20/120    avg_loss:0.219, val_acc:0.979]
Epoch [21/120    avg_loss:0.121, val_acc:0.975]
Epoch [22/120    avg_loss:0.113, val_acc:0.977]
Epoch [23/120    avg_loss:0.173, val_acc:0.958]
Epoch [24/120    avg_loss:0.159, val_acc:0.981]
Epoch [25/120    avg_loss:0.108, val_acc:0.967]
Epoch [26/120    avg_loss:0.148, val_acc:0.983]
Epoch [27/120    avg_loss:0.107, val_acc:0.977]
Epoch [28/120    avg_loss:0.118, val_acc:0.977]
Epoch [29/120    avg_loss:0.126, val_acc:0.983]
Epoch [30/120    avg_loss:0.094, val_acc:0.981]
Epoch [31/120    avg_loss:0.066, val_acc:0.981]
Epoch [32/120    avg_loss:0.104, val_acc:0.981]
Epoch [33/120    avg_loss:0.127, val_acc:0.948]
Epoch [34/120    avg_loss:0.150, val_acc:0.969]
Epoch [35/120    avg_loss:0.178, val_acc:0.985]
Epoch [36/120    avg_loss:0.138, val_acc:0.973]
Epoch [37/120    avg_loss:0.229, val_acc:0.979]
Epoch [38/120    avg_loss:0.129, val_acc:0.971]
Epoch [39/120    avg_loss:0.111, val_acc:0.988]
Epoch [40/120    avg_loss:0.081, val_acc:0.985]
Epoch [41/120    avg_loss:0.088, val_acc:0.967]
Epoch [42/120    avg_loss:0.114, val_acc:0.983]
Epoch [43/120    avg_loss:0.086, val_acc:0.985]
Epoch [44/120    avg_loss:0.107, val_acc:0.973]
Epoch [45/120    avg_loss:0.081, val_acc:0.990]
Epoch [46/120    avg_loss:0.093, val_acc:0.990]
Epoch [47/120    avg_loss:0.086, val_acc:0.981]
Epoch [48/120    avg_loss:0.092, val_acc:0.988]
Epoch [49/120    avg_loss:0.083, val_acc:0.985]
Epoch [50/120    avg_loss:0.070, val_acc:0.988]
Epoch [51/120    avg_loss:0.054, val_acc:0.988]
Epoch [52/120    avg_loss:0.048, val_acc:0.990]
Epoch [53/120    avg_loss:0.073, val_acc:0.985]
Epoch [54/120    avg_loss:0.043, val_acc:0.985]
Epoch [55/120    avg_loss:0.051, val_acc:0.985]
Epoch [56/120    avg_loss:0.077, val_acc:0.985]
Epoch [57/120    avg_loss:0.046, val_acc:0.994]
Epoch [58/120    avg_loss:0.049, val_acc:0.996]
Epoch [59/120    avg_loss:0.055, val_acc:0.992]
Epoch [60/120    avg_loss:0.090, val_acc:0.983]
Epoch [61/120    avg_loss:0.072, val_acc:0.988]
Epoch [62/120    avg_loss:0.042, val_acc:0.990]
Epoch [63/120    avg_loss:0.060, val_acc:0.990]
Epoch [64/120    avg_loss:0.077, val_acc:0.992]
Epoch [65/120    avg_loss:0.089, val_acc:0.983]
Epoch [66/120    avg_loss:0.070, val_acc:0.992]
Epoch [67/120    avg_loss:0.090, val_acc:0.960]
Epoch [68/120    avg_loss:0.231, val_acc:0.983]
Epoch [69/120    avg_loss:0.119, val_acc:0.988]
Epoch [70/120    avg_loss:0.070, val_acc:0.985]
Epoch [71/120    avg_loss:0.040, val_acc:0.996]
Epoch [72/120    avg_loss:0.036, val_acc:0.996]
Epoch [73/120    avg_loss:0.037, val_acc:0.996]
Epoch [74/120    avg_loss:0.038, val_acc:0.996]
Epoch [75/120    avg_loss:0.029, val_acc:0.985]
Epoch [76/120    avg_loss:0.069, val_acc:0.988]
Epoch [77/120    avg_loss:0.027, val_acc:0.988]
Epoch [78/120    avg_loss:0.034, val_acc:0.998]
Epoch [79/120    avg_loss:0.025, val_acc:0.994]
Epoch [80/120    avg_loss:0.018, val_acc:0.994]
Epoch [81/120    avg_loss:0.019, val_acc:0.994]
Epoch [82/120    avg_loss:0.021, val_acc:0.994]
Epoch [83/120    avg_loss:0.021, val_acc:0.996]
Epoch [84/120    avg_loss:0.032, val_acc:0.996]
Epoch [85/120    avg_loss:0.035, val_acc:0.992]
Epoch [86/120    avg_loss:0.031, val_acc:0.992]
Epoch [87/120    avg_loss:0.042, val_acc:0.996]
Epoch [88/120    avg_loss:0.029, val_acc:0.998]
Epoch [89/120    avg_loss:0.032, val_acc:0.994]
Epoch [90/120    avg_loss:0.019, val_acc:0.994]
Epoch [91/120    avg_loss:0.017, val_acc:0.998]
Epoch [92/120    avg_loss:0.027, val_acc:0.996]
Epoch [93/120    avg_loss:0.015, val_acc:0.996]
Epoch [94/120    avg_loss:0.009, val_acc:0.998]
Epoch [95/120    avg_loss:0.017, val_acc:0.998]
Epoch [96/120    avg_loss:0.012, val_acc:0.996]
Epoch [97/120    avg_loss:0.011, val_acc:0.998]
Epoch [98/120    avg_loss:0.010, val_acc:0.996]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.994]
Epoch [101/120    avg_loss:0.008, val_acc:0.994]
Epoch [102/120    avg_loss:0.009, val_acc:0.996]
Epoch [103/120    avg_loss:0.008, val_acc:0.996]
Epoch [104/120    avg_loss:0.007, val_acc:0.994]
Epoch [105/120    avg_loss:0.010, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.996]
Epoch [107/120    avg_loss:0.009, val_acc:0.994]
Epoch [108/120    avg_loss:0.020, val_acc:0.990]
Epoch [109/120    avg_loss:0.047, val_acc:0.988]
Epoch [110/120    avg_loss:0.048, val_acc:0.990]
Epoch [111/120    avg_loss:0.011, val_acc:0.996]
Epoch [112/120    avg_loss:0.009, val_acc:0.996]
Epoch [113/120    avg_loss:0.010, val_acc:0.996]
Epoch [114/120    avg_loss:0.012, val_acc:0.996]
Epoch [115/120    avg_loss:0.008, val_acc:0.996]
Epoch [116/120    avg_loss:0.007, val_acc:0.996]
Epoch [117/120    avg_loss:0.015, val_acc:0.996]
Epoch [118/120    avg_loss:0.008, val_acc:0.996]
Epoch [119/120    avg_loss:0.007, val_acc:0.996]
Epoch [120/120    avg_loss:0.008, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 1.         1.         0.98678414 0.97368421 0.97959184
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9971514724275893
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3cf802b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.292, val_acc:0.504]
Epoch [2/120    avg_loss:1.564, val_acc:0.644]
Epoch [3/120    avg_loss:1.131, val_acc:0.785]
Epoch [4/120    avg_loss:0.934, val_acc:0.827]
Epoch [5/120    avg_loss:0.742, val_acc:0.827]
Epoch [6/120    avg_loss:0.650, val_acc:0.867]
Epoch [7/120    avg_loss:0.558, val_acc:0.881]
Epoch [8/120    avg_loss:0.509, val_acc:0.881]
Epoch [9/120    avg_loss:0.502, val_acc:0.869]
Epoch [10/120    avg_loss:0.439, val_acc:0.898]
Epoch [11/120    avg_loss:0.387, val_acc:0.940]
Epoch [12/120    avg_loss:0.314, val_acc:0.952]
Epoch [13/120    avg_loss:0.302, val_acc:0.950]
Epoch [14/120    avg_loss:0.240, val_acc:0.946]
Epoch [15/120    avg_loss:0.243, val_acc:0.963]
Epoch [16/120    avg_loss:0.236, val_acc:0.948]
Epoch [17/120    avg_loss:0.239, val_acc:0.940]
Epoch [18/120    avg_loss:0.266, val_acc:0.946]
Epoch [19/120    avg_loss:0.303, val_acc:0.967]
Epoch [20/120    avg_loss:0.423, val_acc:0.944]
Epoch [21/120    avg_loss:0.215, val_acc:0.973]
Epoch [22/120    avg_loss:0.192, val_acc:0.956]
Epoch [23/120    avg_loss:0.194, val_acc:0.958]
Epoch [24/120    avg_loss:0.137, val_acc:0.983]
Epoch [25/120    avg_loss:0.133, val_acc:0.973]
Epoch [26/120    avg_loss:0.113, val_acc:0.969]
Epoch [27/120    avg_loss:0.118, val_acc:0.969]
Epoch [28/120    avg_loss:0.162, val_acc:0.940]
Epoch [29/120    avg_loss:0.172, val_acc:0.956]
Epoch [30/120    avg_loss:0.157, val_acc:0.952]
Epoch [31/120    avg_loss:0.153, val_acc:0.981]
Epoch [32/120    avg_loss:0.162, val_acc:0.952]
Epoch [33/120    avg_loss:0.141, val_acc:0.965]
Epoch [34/120    avg_loss:0.143, val_acc:0.981]
Epoch [35/120    avg_loss:0.096, val_acc:0.967]
Epoch [36/120    avg_loss:0.093, val_acc:0.979]
Epoch [37/120    avg_loss:0.065, val_acc:0.985]
Epoch [38/120    avg_loss:0.062, val_acc:0.975]
Epoch [39/120    avg_loss:0.057, val_acc:0.985]
Epoch [40/120    avg_loss:0.060, val_acc:0.985]
Epoch [41/120    avg_loss:0.088, val_acc:0.977]
Epoch [42/120    avg_loss:0.063, val_acc:0.979]
Epoch [43/120    avg_loss:0.053, val_acc:0.983]
Epoch [44/120    avg_loss:0.067, val_acc:0.973]
Epoch [45/120    avg_loss:0.072, val_acc:0.983]
Epoch [46/120    avg_loss:0.080, val_acc:0.988]
Epoch [47/120    avg_loss:0.046, val_acc:0.977]
Epoch [48/120    avg_loss:0.053, val_acc:0.981]
Epoch [49/120    avg_loss:0.072, val_acc:0.981]
Epoch [50/120    avg_loss:0.066, val_acc:0.977]
Epoch [51/120    avg_loss:0.074, val_acc:0.981]
Epoch [52/120    avg_loss:0.079, val_acc:0.979]
Epoch [53/120    avg_loss:0.076, val_acc:0.979]
Epoch [54/120    avg_loss:0.047, val_acc:0.981]
Epoch [55/120    avg_loss:0.034, val_acc:0.979]
Epoch [56/120    avg_loss:0.034, val_acc:0.977]
Epoch [57/120    avg_loss:0.032, val_acc:0.985]
Epoch [58/120    avg_loss:0.025, val_acc:0.979]
Epoch [59/120    avg_loss:0.027, val_acc:0.985]
Epoch [60/120    avg_loss:0.025, val_acc:0.990]
Epoch [61/120    avg_loss:0.019, val_acc:0.990]
Epoch [62/120    avg_loss:0.019, val_acc:0.992]
Epoch [63/120    avg_loss:0.024, val_acc:0.992]
Epoch [64/120    avg_loss:0.023, val_acc:0.990]
Epoch [65/120    avg_loss:0.011, val_acc:0.988]
Epoch [66/120    avg_loss:0.024, val_acc:0.988]
Epoch [67/120    avg_loss:0.022, val_acc:0.988]
Epoch [68/120    avg_loss:0.012, val_acc:0.983]
Epoch [69/120    avg_loss:0.014, val_acc:0.985]
Epoch [70/120    avg_loss:0.019, val_acc:0.985]
Epoch [71/120    avg_loss:0.017, val_acc:0.988]
Epoch [72/120    avg_loss:0.029, val_acc:0.985]
Epoch [73/120    avg_loss:0.016, val_acc:0.983]
Epoch [74/120    avg_loss:0.019, val_acc:0.981]
Epoch [75/120    avg_loss:0.019, val_acc:0.981]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.017, val_acc:0.985]
Epoch [78/120    avg_loss:0.013, val_acc:0.985]
Epoch [79/120    avg_loss:0.016, val_acc:0.985]
Epoch [80/120    avg_loss:0.022, val_acc:0.985]
Epoch [81/120    avg_loss:0.013, val_acc:0.985]
Epoch [82/120    avg_loss:0.017, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.985]
Epoch [84/120    avg_loss:0.015, val_acc:0.985]
Epoch [85/120    avg_loss:0.022, val_acc:0.985]
Epoch [86/120    avg_loss:0.018, val_acc:0.985]
Epoch [87/120    avg_loss:0.025, val_acc:0.985]
Epoch [88/120    avg_loss:0.025, val_acc:0.985]
Epoch [89/120    avg_loss:0.022, val_acc:0.983]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.022, val_acc:0.983]
Epoch [92/120    avg_loss:0.018, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.985]
Epoch [94/120    avg_loss:0.023, val_acc:0.985]
Epoch [95/120    avg_loss:0.017, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.023, val_acc:0.983]
Epoch [101/120    avg_loss:0.015, val_acc:0.983]
Epoch [102/120    avg_loss:0.020, val_acc:0.983]
Epoch [103/120    avg_loss:0.035, val_acc:0.983]
Epoch [104/120    avg_loss:0.014, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.022, val_acc:0.983]
Epoch [107/120    avg_loss:0.020, val_acc:0.983]
Epoch [108/120    avg_loss:0.014, val_acc:0.983]
Epoch [109/120    avg_loss:0.018, val_acc:0.983]
Epoch [110/120    avg_loss:0.021, val_acc:0.983]
Epoch [111/120    avg_loss:0.021, val_acc:0.983]
Epoch [112/120    avg_loss:0.014, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.018, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.016, val_acc:0.983]
Epoch [119/120    avg_loss:0.029, val_acc:0.983]
Epoch [120/120    avg_loss:0.020, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  11   0   0   0   0   0   0   2   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.98648649 1.         0.95749441 0.94237288
 1.         0.96703297 1.         1.         1.         0.9843342
 0.984375   1.        ]

Kappa:
0.9912169395187107
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f024e5f6748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.256, val_acc:0.552]
Epoch [2/120    avg_loss:1.574, val_acc:0.604]
Epoch [3/120    avg_loss:1.181, val_acc:0.783]
Epoch [4/120    avg_loss:0.914, val_acc:0.717]
Epoch [5/120    avg_loss:0.783, val_acc:0.810]
Epoch [6/120    avg_loss:0.723, val_acc:0.821]
Epoch [7/120    avg_loss:0.577, val_acc:0.860]
Epoch [8/120    avg_loss:0.510, val_acc:0.827]
Epoch [9/120    avg_loss:0.460, val_acc:0.892]
Epoch [10/120    avg_loss:0.408, val_acc:0.894]
Epoch [11/120    avg_loss:0.474, val_acc:0.890]
Epoch [12/120    avg_loss:0.364, val_acc:0.863]
Epoch [13/120    avg_loss:0.345, val_acc:0.921]
Epoch [14/120    avg_loss:0.312, val_acc:0.929]
Epoch [15/120    avg_loss:0.232, val_acc:0.942]
Epoch [16/120    avg_loss:0.302, val_acc:0.927]
Epoch [17/120    avg_loss:0.240, val_acc:0.944]
Epoch [18/120    avg_loss:0.201, val_acc:0.969]
Epoch [19/120    avg_loss:0.248, val_acc:0.954]
Epoch [20/120    avg_loss:0.280, val_acc:0.942]
Epoch [21/120    avg_loss:0.177, val_acc:0.973]
Epoch [22/120    avg_loss:0.164, val_acc:0.973]
Epoch [23/120    avg_loss:0.278, val_acc:0.917]
Epoch [24/120    avg_loss:0.145, val_acc:0.973]
Epoch [25/120    avg_loss:0.148, val_acc:0.954]
Epoch [26/120    avg_loss:0.117, val_acc:0.956]
Epoch [27/120    avg_loss:0.137, val_acc:0.973]
Epoch [28/120    avg_loss:0.126, val_acc:0.973]
Epoch [29/120    avg_loss:0.138, val_acc:0.954]
Epoch [30/120    avg_loss:0.095, val_acc:0.973]
Epoch [31/120    avg_loss:0.081, val_acc:0.973]
Epoch [32/120    avg_loss:0.080, val_acc:0.973]
Epoch [33/120    avg_loss:0.099, val_acc:0.977]
Epoch [34/120    avg_loss:0.115, val_acc:0.969]
Epoch [35/120    avg_loss:0.138, val_acc:0.952]
Epoch [36/120    avg_loss:0.163, val_acc:0.967]
Epoch [37/120    avg_loss:0.164, val_acc:0.973]
Epoch [38/120    avg_loss:0.119, val_acc:0.971]
Epoch [39/120    avg_loss:0.081, val_acc:0.977]
Epoch [40/120    avg_loss:0.061, val_acc:0.985]
Epoch [41/120    avg_loss:0.046, val_acc:0.979]
Epoch [42/120    avg_loss:0.048, val_acc:0.985]
Epoch [43/120    avg_loss:0.083, val_acc:0.971]
Epoch [44/120    avg_loss:0.093, val_acc:0.985]
Epoch [45/120    avg_loss:0.083, val_acc:0.977]
Epoch [46/120    avg_loss:0.094, val_acc:0.942]
Epoch [47/120    avg_loss:0.153, val_acc:0.969]
Epoch [48/120    avg_loss:0.236, val_acc:0.971]
Epoch [49/120    avg_loss:0.094, val_acc:0.977]
Epoch [50/120    avg_loss:0.108, val_acc:0.977]
Epoch [51/120    avg_loss:0.051, val_acc:0.979]
Epoch [52/120    avg_loss:0.046, val_acc:0.977]
Epoch [53/120    avg_loss:0.050, val_acc:0.985]
Epoch [54/120    avg_loss:0.054, val_acc:0.985]
Epoch [55/120    avg_loss:0.027, val_acc:0.981]
Epoch [56/120    avg_loss:0.034, val_acc:0.981]
Epoch [57/120    avg_loss:0.034, val_acc:0.983]
Epoch [58/120    avg_loss:0.043, val_acc:0.981]
Epoch [59/120    avg_loss:0.055, val_acc:0.990]
Epoch [60/120    avg_loss:0.050, val_acc:0.985]
Epoch [61/120    avg_loss:0.050, val_acc:0.983]
Epoch [62/120    avg_loss:0.036, val_acc:0.983]
Epoch [63/120    avg_loss:0.061, val_acc:0.979]
Epoch [64/120    avg_loss:0.052, val_acc:0.965]
Epoch [65/120    avg_loss:0.052, val_acc:0.983]
Epoch [66/120    avg_loss:0.051, val_acc:0.985]
Epoch [67/120    avg_loss:0.026, val_acc:0.985]
Epoch [68/120    avg_loss:0.039, val_acc:0.985]
Epoch [69/120    avg_loss:0.032, val_acc:0.983]
Epoch [70/120    avg_loss:0.024, val_acc:0.981]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.015, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.988]
Epoch [75/120    avg_loss:0.010, val_acc:0.988]
Epoch [76/120    avg_loss:0.020, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.985]
Epoch [80/120    avg_loss:0.016, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.019, val_acc:0.985]
Epoch [83/120    avg_loss:0.013, val_acc:0.985]
Epoch [84/120    avg_loss:0.016, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.015, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.013, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.988]
Epoch [90/120    avg_loss:0.012, val_acc:0.988]
Epoch [91/120    avg_loss:0.015, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.019, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.013, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.023, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.012, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.988]
Epoch [110/120    avg_loss:0.012, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.014, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.016, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.025, val_acc:0.988]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.99545455 0.98678414 0.95364238 0.94949495
 1.         0.98924731 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9933535621505711
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde6417e748>
supervision:full
center_pixel:True
Network :
Number of parameter: 208832==>0.21M
----------Training process----------
Epoch [1/120    avg_loss:2.302, val_acc:0.535]
Epoch [2/120    avg_loss:1.620, val_acc:0.631]
Epoch [3/120    avg_loss:1.227, val_acc:0.796]
Epoch [4/120    avg_loss:0.894, val_acc:0.723]
Epoch [5/120    avg_loss:0.733, val_acc:0.840]
Epoch [6/120    avg_loss:0.615, val_acc:0.848]
Epoch [7/120    avg_loss:0.630, val_acc:0.858]
Epoch [8/120    avg_loss:0.540, val_acc:0.900]
Epoch [9/120    avg_loss:0.433, val_acc:0.921]
Epoch [10/120    avg_loss:0.388, val_acc:0.931]
Epoch [11/120    avg_loss:0.433, val_acc:0.915]
Epoch [12/120    avg_loss:0.379, val_acc:0.942]
Epoch [13/120    avg_loss:0.273, val_acc:0.946]
Epoch [14/120    avg_loss:0.255, val_acc:0.944]
Epoch [15/120    avg_loss:0.246, val_acc:0.958]
Epoch [16/120    avg_loss:0.172, val_acc:0.973]
Epoch [17/120    avg_loss:0.230, val_acc:0.956]
Epoch [18/120    avg_loss:0.211, val_acc:0.950]
Epoch [19/120    avg_loss:0.233, val_acc:0.963]
Epoch [20/120    avg_loss:0.206, val_acc:0.960]
Epoch [21/120    avg_loss:0.176, val_acc:0.942]
Epoch [22/120    avg_loss:0.158, val_acc:0.971]
Epoch [23/120    avg_loss:0.190, val_acc:0.967]
Epoch [24/120    avg_loss:0.105, val_acc:0.960]
Epoch [25/120    avg_loss:0.153, val_acc:0.975]
Epoch [26/120    avg_loss:0.148, val_acc:0.952]
Epoch [27/120    avg_loss:0.161, val_acc:0.944]
Epoch [28/120    avg_loss:0.096, val_acc:0.971]
Epoch [29/120    avg_loss:0.094, val_acc:0.977]
Epoch [30/120    avg_loss:0.079, val_acc:0.965]
Epoch [31/120    avg_loss:0.089, val_acc:0.979]
Epoch [32/120    avg_loss:0.090, val_acc:0.971]
Epoch [33/120    avg_loss:0.106, val_acc:0.967]
Epoch [34/120    avg_loss:0.088, val_acc:0.967]
Epoch [35/120    avg_loss:0.071, val_acc:0.975]
Epoch [36/120    avg_loss:0.127, val_acc:0.963]
Epoch [37/120    avg_loss:0.108, val_acc:0.963]
Epoch [38/120    avg_loss:0.089, val_acc:0.967]
Epoch [39/120    avg_loss:0.056, val_acc:0.958]
Epoch [40/120    avg_loss:0.100, val_acc:0.971]
Epoch [41/120    avg_loss:0.084, val_acc:0.973]
Epoch [42/120    avg_loss:0.080, val_acc:0.977]
Epoch [43/120    avg_loss:0.053, val_acc:0.977]
Epoch [44/120    avg_loss:0.046, val_acc:0.975]
Epoch [45/120    avg_loss:0.046, val_acc:0.975]
Epoch [46/120    avg_loss:0.038, val_acc:0.971]
Epoch [47/120    avg_loss:0.031, val_acc:0.973]
Epoch [48/120    avg_loss:0.031, val_acc:0.973]
Epoch [49/120    avg_loss:0.047, val_acc:0.975]
Epoch [50/120    avg_loss:0.018, val_acc:0.975]
Epoch [51/120    avg_loss:0.040, val_acc:0.977]
Epoch [52/120    avg_loss:0.023, val_acc:0.975]
Epoch [53/120    avg_loss:0.024, val_acc:0.975]
Epoch [54/120    avg_loss:0.024, val_acc:0.975]
Epoch [55/120    avg_loss:0.025, val_acc:0.975]
Epoch [56/120    avg_loss:0.022, val_acc:0.975]
Epoch [57/120    avg_loss:0.025, val_acc:0.975]
Epoch [58/120    avg_loss:0.031, val_acc:0.977]
Epoch [59/120    avg_loss:0.036, val_acc:0.977]
Epoch [60/120    avg_loss:0.022, val_acc:0.977]
Epoch [61/120    avg_loss:0.021, val_acc:0.977]
Epoch [62/120    avg_loss:0.026, val_acc:0.977]
Epoch [63/120    avg_loss:0.025, val_acc:0.977]
Epoch [64/120    avg_loss:0.020, val_acc:0.977]
Epoch [65/120    avg_loss:0.025, val_acc:0.975]
Epoch [66/120    avg_loss:0.023, val_acc:0.975]
Epoch [67/120    avg_loss:0.031, val_acc:0.975]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.029, val_acc:0.975]
Epoch [70/120    avg_loss:0.035, val_acc:0.977]
Epoch [71/120    avg_loss:0.028, val_acc:0.975]
Epoch [72/120    avg_loss:0.036, val_acc:0.975]
Epoch [73/120    avg_loss:0.036, val_acc:0.975]
Epoch [74/120    avg_loss:0.036, val_acc:0.975]
Epoch [75/120    avg_loss:0.023, val_acc:0.975]
Epoch [76/120    avg_loss:0.030, val_acc:0.975]
Epoch [77/120    avg_loss:0.022, val_acc:0.975]
Epoch [78/120    avg_loss:0.032, val_acc:0.975]
Epoch [79/120    avg_loss:0.021, val_acc:0.975]
Epoch [80/120    avg_loss:0.024, val_acc:0.975]
Epoch [81/120    avg_loss:0.034, val_acc:0.975]
Epoch [82/120    avg_loss:0.037, val_acc:0.975]
Epoch [83/120    avg_loss:0.022, val_acc:0.975]
Epoch [84/120    avg_loss:0.027, val_acc:0.975]
Epoch [85/120    avg_loss:0.024, val_acc:0.975]
Epoch [86/120    avg_loss:0.035, val_acc:0.975]
Epoch [87/120    avg_loss:0.022, val_acc:0.975]
Epoch [88/120    avg_loss:0.025, val_acc:0.975]
Epoch [89/120    avg_loss:0.035, val_acc:0.975]
Epoch [90/120    avg_loss:0.023, val_acc:0.975]
Epoch [91/120    avg_loss:0.024, val_acc:0.975]
Epoch [92/120    avg_loss:0.026, val_acc:0.975]
Epoch [93/120    avg_loss:0.025, val_acc:0.975]
Epoch [94/120    avg_loss:0.038, val_acc:0.975]
Epoch [95/120    avg_loss:0.025, val_acc:0.975]
Epoch [96/120    avg_loss:0.024, val_acc:0.975]
Epoch [97/120    avg_loss:0.034, val_acc:0.975]
Epoch [98/120    avg_loss:0.027, val_acc:0.975]
Epoch [99/120    avg_loss:0.030, val_acc:0.975]
Epoch [100/120    avg_loss:0.031, val_acc:0.975]
Epoch [101/120    avg_loss:0.026, val_acc:0.975]
Epoch [102/120    avg_loss:0.028, val_acc:0.975]
Epoch [103/120    avg_loss:0.025, val_acc:0.975]
Epoch [104/120    avg_loss:0.034, val_acc:0.975]
Epoch [105/120    avg_loss:0.019, val_acc:0.975]
Epoch [106/120    avg_loss:0.027, val_acc:0.975]
Epoch [107/120    avg_loss:0.027, val_acc:0.975]
Epoch [108/120    avg_loss:0.027, val_acc:0.975]
Epoch [109/120    avg_loss:0.043, val_acc:0.975]
Epoch [110/120    avg_loss:0.019, val_acc:0.975]
Epoch [111/120    avg_loss:0.028, val_acc:0.975]
Epoch [112/120    avg_loss:0.027, val_acc:0.975]
Epoch [113/120    avg_loss:0.026, val_acc:0.975]
Epoch [114/120    avg_loss:0.028, val_acc:0.975]
Epoch [115/120    avg_loss:0.034, val_acc:0.975]
Epoch [116/120    avg_loss:0.024, val_acc:0.975]
Epoch [117/120    avg_loss:0.032, val_acc:0.975]
Epoch [118/120    avg_loss:0.020, val_acc:0.975]
Epoch [119/120    avg_loss:0.020, val_acc:0.975]
Epoch [120/120    avg_loss:0.023, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 1.         0.98206278 0.98901099 0.95515695 0.95364238
 1.         0.95555556 1.         1.         1.         0.9843342
 0.98547486 1.        ]

Kappa:
0.990505071005721
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca037aa7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.266, val_acc:0.508]
Epoch [2/120    avg_loss:1.640, val_acc:0.633]
Epoch [3/120    avg_loss:1.250, val_acc:0.704]
Epoch [4/120    avg_loss:0.977, val_acc:0.765]
Epoch [5/120    avg_loss:0.799, val_acc:0.781]
Epoch [6/120    avg_loss:0.708, val_acc:0.892]
Epoch [7/120    avg_loss:0.597, val_acc:0.875]
Epoch [8/120    avg_loss:0.525, val_acc:0.873]
Epoch [9/120    avg_loss:0.452, val_acc:0.915]
Epoch [10/120    avg_loss:0.440, val_acc:0.923]
Epoch [11/120    avg_loss:0.394, val_acc:0.938]
Epoch [12/120    avg_loss:0.302, val_acc:0.940]
Epoch [13/120    avg_loss:0.293, val_acc:0.948]
Epoch [14/120    avg_loss:0.262, val_acc:0.927]
Epoch [15/120    avg_loss:0.274, val_acc:0.935]
Epoch [16/120    avg_loss:0.289, val_acc:0.954]
Epoch [17/120    avg_loss:0.229, val_acc:0.950]
Epoch [18/120    avg_loss:0.200, val_acc:0.933]
Epoch [19/120    avg_loss:0.218, val_acc:0.969]
Epoch [20/120    avg_loss:0.198, val_acc:0.948]
Epoch [21/120    avg_loss:0.177, val_acc:0.965]
Epoch [22/120    avg_loss:0.240, val_acc:0.958]
Epoch [23/120    avg_loss:0.185, val_acc:0.983]
Epoch [24/120    avg_loss:0.160, val_acc:0.954]
Epoch [25/120    avg_loss:0.142, val_acc:0.960]
Epoch [26/120    avg_loss:0.106, val_acc:0.983]
Epoch [27/120    avg_loss:0.109, val_acc:0.981]
Epoch [28/120    avg_loss:0.158, val_acc:0.983]
Epoch [29/120    avg_loss:0.131, val_acc:0.969]
Epoch [30/120    avg_loss:0.122, val_acc:0.988]
Epoch [31/120    avg_loss:0.131, val_acc:0.967]
Epoch [32/120    avg_loss:0.130, val_acc:0.971]
Epoch [33/120    avg_loss:0.115, val_acc:0.958]
Epoch [34/120    avg_loss:0.091, val_acc:0.988]
Epoch [35/120    avg_loss:0.109, val_acc:0.946]
Epoch [36/120    avg_loss:0.106, val_acc:0.981]
Epoch [37/120    avg_loss:0.051, val_acc:0.985]
Epoch [38/120    avg_loss:0.082, val_acc:0.994]
Epoch [39/120    avg_loss:0.081, val_acc:0.971]
Epoch [40/120    avg_loss:0.067, val_acc:0.990]
Epoch [41/120    avg_loss:0.066, val_acc:0.963]
Epoch [42/120    avg_loss:0.089, val_acc:0.988]
Epoch [43/120    avg_loss:0.049, val_acc:0.985]
Epoch [44/120    avg_loss:0.044, val_acc:0.992]
Epoch [45/120    avg_loss:0.030, val_acc:0.996]
Epoch [46/120    avg_loss:0.044, val_acc:1.000]
Epoch [47/120    avg_loss:0.042, val_acc:0.996]
Epoch [48/120    avg_loss:0.050, val_acc:0.992]
Epoch [49/120    avg_loss:0.027, val_acc:0.994]
Epoch [50/120    avg_loss:0.040, val_acc:0.992]
Epoch [51/120    avg_loss:0.039, val_acc:0.992]
Epoch [52/120    avg_loss:0.031, val_acc:0.994]
Epoch [53/120    avg_loss:0.038, val_acc:0.990]
Epoch [54/120    avg_loss:0.034, val_acc:0.990]
Epoch [55/120    avg_loss:0.078, val_acc:0.996]
Epoch [56/120    avg_loss:0.060, val_acc:0.971]
Epoch [57/120    avg_loss:0.075, val_acc:0.971]
Epoch [58/120    avg_loss:0.100, val_acc:0.983]
Epoch [59/120    avg_loss:0.051, val_acc:0.985]
Epoch [60/120    avg_loss:0.041, val_acc:0.988]
Epoch [61/120    avg_loss:0.041, val_acc:0.990]
Epoch [62/120    avg_loss:0.023, val_acc:0.990]
Epoch [63/120    avg_loss:0.029, val_acc:0.994]
Epoch [64/120    avg_loss:0.025, val_acc:0.992]
Epoch [65/120    avg_loss:0.026, val_acc:0.992]
Epoch [66/120    avg_loss:0.031, val_acc:0.994]
Epoch [67/120    avg_loss:0.026, val_acc:0.994]
Epoch [68/120    avg_loss:0.021, val_acc:0.994]
Epoch [69/120    avg_loss:0.020, val_acc:0.996]
Epoch [70/120    avg_loss:0.015, val_acc:0.994]
Epoch [71/120    avg_loss:0.029, val_acc:0.996]
Epoch [72/120    avg_loss:0.018, val_acc:0.996]
Epoch [73/120    avg_loss:0.025, val_acc:0.996]
Epoch [74/120    avg_loss:0.025, val_acc:0.996]
Epoch [75/120    avg_loss:0.023, val_acc:0.996]
Epoch [76/120    avg_loss:0.026, val_acc:0.994]
Epoch [77/120    avg_loss:0.018, val_acc:0.994]
Epoch [78/120    avg_loss:0.020, val_acc:0.994]
Epoch [79/120    avg_loss:0.022, val_acc:0.994]
Epoch [80/120    avg_loss:0.022, val_acc:0.994]
Epoch [81/120    avg_loss:0.033, val_acc:0.994]
Epoch [82/120    avg_loss:0.023, val_acc:0.994]
Epoch [83/120    avg_loss:0.016, val_acc:0.994]
Epoch [84/120    avg_loss:0.030, val_acc:0.996]
Epoch [85/120    avg_loss:0.021, val_acc:0.996]
Epoch [86/120    avg_loss:0.023, val_acc:0.996]
Epoch [87/120    avg_loss:0.019, val_acc:0.996]
Epoch [88/120    avg_loss:0.012, val_acc:0.996]
Epoch [89/120    avg_loss:0.023, val_acc:0.996]
Epoch [90/120    avg_loss:0.025, val_acc:0.996]
Epoch [91/120    avg_loss:0.019, val_acc:0.996]
Epoch [92/120    avg_loss:0.016, val_acc:0.996]
Epoch [93/120    avg_loss:0.018, val_acc:0.996]
Epoch [94/120    avg_loss:0.023, val_acc:0.996]
Epoch [95/120    avg_loss:0.018, val_acc:0.996]
Epoch [96/120    avg_loss:0.028, val_acc:0.996]
Epoch [97/120    avg_loss:0.025, val_acc:0.996]
Epoch [98/120    avg_loss:0.019, val_acc:0.996]
Epoch [99/120    avg_loss:0.035, val_acc:0.996]
Epoch [100/120    avg_loss:0.014, val_acc:0.996]
Epoch [101/120    avg_loss:0.024, val_acc:0.996]
Epoch [102/120    avg_loss:0.023, val_acc:0.996]
Epoch [103/120    avg_loss:0.014, val_acc:0.996]
Epoch [104/120    avg_loss:0.016, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.012, val_acc:0.996]
Epoch [107/120    avg_loss:0.020, val_acc:0.996]
Epoch [108/120    avg_loss:0.022, val_acc:0.996]
Epoch [109/120    avg_loss:0.021, val_acc:0.996]
Epoch [110/120    avg_loss:0.030, val_acc:0.996]
Epoch [111/120    avg_loss:0.012, val_acc:0.996]
Epoch [112/120    avg_loss:0.020, val_acc:0.996]
Epoch [113/120    avg_loss:0.018, val_acc:0.996]
Epoch [114/120    avg_loss:0.018, val_acc:0.996]
Epoch [115/120    avg_loss:0.014, val_acc:0.996]
Epoch [116/120    avg_loss:0.023, val_acc:0.996]
Epoch [117/120    avg_loss:0.021, val_acc:0.996]
Epoch [118/120    avg_loss:0.022, val_acc:0.996]
Epoch [119/120    avg_loss:0.020, val_acc:0.996]
Epoch [120/120    avg_loss:0.023, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  11   0   0   0   0   0   0   2   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   8 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.98648649 1.         0.96613995 0.95652174
 1.         0.97826087 1.         1.         1.         0.98950131
 0.98663697 1.        ]

Kappa:
0.9931162381468819
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cd502d7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.267, val_acc:0.608]
Epoch [2/120    avg_loss:1.548, val_acc:0.673]
Epoch [3/120    avg_loss:1.102, val_acc:0.750]
Epoch [4/120    avg_loss:0.957, val_acc:0.742]
Epoch [5/120    avg_loss:0.766, val_acc:0.781]
Epoch [6/120    avg_loss:0.721, val_acc:0.838]
Epoch [7/120    avg_loss:0.635, val_acc:0.767]
Epoch [8/120    avg_loss:0.543, val_acc:0.821]
Epoch [9/120    avg_loss:0.461, val_acc:0.900]
Epoch [10/120    avg_loss:0.458, val_acc:0.825]
Epoch [11/120    avg_loss:0.455, val_acc:0.915]
Epoch [12/120    avg_loss:0.395, val_acc:0.927]
Epoch [13/120    avg_loss:0.281, val_acc:0.921]
Epoch [14/120    avg_loss:0.266, val_acc:0.952]
Epoch [15/120    avg_loss:0.257, val_acc:0.938]
Epoch [16/120    avg_loss:0.279, val_acc:0.933]
Epoch [17/120    avg_loss:0.295, val_acc:0.960]
Epoch [18/120    avg_loss:0.248, val_acc:0.950]
Epoch [19/120    avg_loss:0.162, val_acc:0.973]
Epoch [20/120    avg_loss:0.148, val_acc:0.958]
Epoch [21/120    avg_loss:0.176, val_acc:0.969]
Epoch [22/120    avg_loss:0.158, val_acc:0.971]
Epoch [23/120    avg_loss:0.191, val_acc:0.950]
Epoch [24/120    avg_loss:0.292, val_acc:0.958]
Epoch [25/120    avg_loss:0.195, val_acc:0.954]
Epoch [26/120    avg_loss:0.154, val_acc:0.969]
Epoch [27/120    avg_loss:0.147, val_acc:0.973]
Epoch [28/120    avg_loss:0.134, val_acc:0.971]
Epoch [29/120    avg_loss:0.109, val_acc:0.956]
Epoch [30/120    avg_loss:0.189, val_acc:0.958]
Epoch [31/120    avg_loss:0.166, val_acc:0.960]
Epoch [32/120    avg_loss:0.072, val_acc:0.979]
Epoch [33/120    avg_loss:0.110, val_acc:0.969]
Epoch [34/120    avg_loss:0.107, val_acc:0.977]
Epoch [35/120    avg_loss:0.100, val_acc:0.981]
Epoch [36/120    avg_loss:0.090, val_acc:0.973]
Epoch [37/120    avg_loss:0.109, val_acc:0.983]
Epoch [38/120    avg_loss:0.083, val_acc:0.977]
Epoch [39/120    avg_loss:0.083, val_acc:0.946]
Epoch [40/120    avg_loss:0.129, val_acc:0.990]
Epoch [41/120    avg_loss:0.087, val_acc:0.973]
Epoch [42/120    avg_loss:0.079, val_acc:0.983]
Epoch [43/120    avg_loss:0.063, val_acc:0.975]
Epoch [44/120    avg_loss:0.051, val_acc:0.990]
Epoch [45/120    avg_loss:0.070, val_acc:0.942]
Epoch [46/120    avg_loss:0.064, val_acc:0.988]
Epoch [47/120    avg_loss:0.050, val_acc:0.971]
Epoch [48/120    avg_loss:0.059, val_acc:0.975]
Epoch [49/120    avg_loss:0.049, val_acc:0.983]
Epoch [50/120    avg_loss:0.074, val_acc:0.965]
Epoch [51/120    avg_loss:0.081, val_acc:0.977]
Epoch [52/120    avg_loss:0.066, val_acc:0.977]
Epoch [53/120    avg_loss:0.057, val_acc:0.988]
Epoch [54/120    avg_loss:0.038, val_acc:0.981]
Epoch [55/120    avg_loss:0.071, val_acc:0.992]
Epoch [56/120    avg_loss:0.047, val_acc:0.975]
Epoch [57/120    avg_loss:0.044, val_acc:0.983]
Epoch [58/120    avg_loss:0.037, val_acc:0.981]
Epoch [59/120    avg_loss:0.022, val_acc:0.988]
Epoch [60/120    avg_loss:0.023, val_acc:0.990]
Epoch [61/120    avg_loss:0.020, val_acc:0.994]
Epoch [62/120    avg_loss:0.027, val_acc:0.975]
Epoch [63/120    avg_loss:0.062, val_acc:0.950]
Epoch [64/120    avg_loss:0.033, val_acc:0.979]
Epoch [65/120    avg_loss:0.024, val_acc:0.990]
Epoch [66/120    avg_loss:0.017, val_acc:0.990]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.027, val_acc:0.990]
Epoch [69/120    avg_loss:0.024, val_acc:0.990]
Epoch [70/120    avg_loss:0.012, val_acc:0.990]
Epoch [71/120    avg_loss:0.038, val_acc:0.994]
Epoch [72/120    avg_loss:0.023, val_acc:0.985]
Epoch [73/120    avg_loss:0.029, val_acc:0.979]
Epoch [74/120    avg_loss:0.023, val_acc:0.988]
Epoch [75/120    avg_loss:0.026, val_acc:0.983]
Epoch [76/120    avg_loss:0.014, val_acc:0.992]
Epoch [77/120    avg_loss:0.014, val_acc:0.996]
Epoch [78/120    avg_loss:0.013, val_acc:0.994]
Epoch [79/120    avg_loss:0.011, val_acc:0.988]
Epoch [80/120    avg_loss:0.021, val_acc:0.992]
Epoch [81/120    avg_loss:0.022, val_acc:0.990]
Epoch [82/120    avg_loss:0.015, val_acc:0.994]
Epoch [83/120    avg_loss:0.014, val_acc:0.988]
Epoch [84/120    avg_loss:0.010, val_acc:0.988]
Epoch [85/120    avg_loss:0.013, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.988]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.025, val_acc:0.983]
Epoch [90/120    avg_loss:0.034, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.988]
Epoch [92/120    avg_loss:0.016, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.990]
Epoch [94/120    avg_loss:0.014, val_acc:0.990]
Epoch [95/120    avg_loss:0.011, val_acc:0.994]
Epoch [96/120    avg_loss:0.013, val_acc:0.996]
Epoch [97/120    avg_loss:0.015, val_acc:0.994]
Epoch [98/120    avg_loss:0.005, val_acc:0.994]
Epoch [99/120    avg_loss:0.008, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.007, val_acc:0.994]
Epoch [102/120    avg_loss:0.007, val_acc:0.994]
Epoch [103/120    avg_loss:0.011, val_acc:0.994]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.015, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.006, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   6   0   0   0   0   0   0   2   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         1.         1.         0.95424837 0.93286219
 1.         1.         1.         1.         1.         0.99210526
 0.99113082 1.        ]

Kappa:
0.9935905158058094
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9d2c54b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.307, val_acc:0.487]
Epoch [2/120    avg_loss:1.644, val_acc:0.592]
Epoch [3/120    avg_loss:1.228, val_acc:0.783]
Epoch [4/120    avg_loss:0.935, val_acc:0.802]
Epoch [5/120    avg_loss:0.748, val_acc:0.812]
Epoch [6/120    avg_loss:0.617, val_acc:0.825]
Epoch [7/120    avg_loss:0.541, val_acc:0.867]
Epoch [8/120    avg_loss:0.453, val_acc:0.885]
Epoch [9/120    avg_loss:0.465, val_acc:0.910]
Epoch [10/120    avg_loss:0.456, val_acc:0.902]
Epoch [11/120    avg_loss:0.428, val_acc:0.906]
Epoch [12/120    avg_loss:0.353, val_acc:0.904]
Epoch [13/120    avg_loss:0.249, val_acc:0.927]
Epoch [14/120    avg_loss:0.319, val_acc:0.925]
Epoch [15/120    avg_loss:0.357, val_acc:0.917]
Epoch [16/120    avg_loss:0.359, val_acc:0.900]
Epoch [17/120    avg_loss:0.366, val_acc:0.910]
Epoch [18/120    avg_loss:0.318, val_acc:0.919]
Epoch [19/120    avg_loss:0.267, val_acc:0.929]
Epoch [20/120    avg_loss:0.225, val_acc:0.942]
Epoch [21/120    avg_loss:0.213, val_acc:0.927]
Epoch [22/120    avg_loss:0.173, val_acc:0.967]
Epoch [23/120    avg_loss:0.138, val_acc:0.940]
Epoch [24/120    avg_loss:0.149, val_acc:0.960]
Epoch [25/120    avg_loss:0.126, val_acc:0.960]
Epoch [26/120    avg_loss:0.154, val_acc:0.969]
Epoch [27/120    avg_loss:0.164, val_acc:0.971]
Epoch [28/120    avg_loss:0.106, val_acc:0.965]
Epoch [29/120    avg_loss:0.089, val_acc:0.969]
Epoch [30/120    avg_loss:0.103, val_acc:0.973]
Epoch [31/120    avg_loss:0.150, val_acc:0.971]
Epoch [32/120    avg_loss:0.089, val_acc:0.963]
Epoch [33/120    avg_loss:0.107, val_acc:0.969]
Epoch [34/120    avg_loss:0.091, val_acc:0.965]
Epoch [35/120    avg_loss:0.087, val_acc:0.960]
Epoch [36/120    avg_loss:0.065, val_acc:0.965]
Epoch [37/120    avg_loss:0.067, val_acc:0.981]
Epoch [38/120    avg_loss:0.097, val_acc:0.965]
Epoch [39/120    avg_loss:0.094, val_acc:0.973]
Epoch [40/120    avg_loss:0.068, val_acc:0.979]
Epoch [41/120    avg_loss:0.124, val_acc:0.965]
Epoch [42/120    avg_loss:0.113, val_acc:0.967]
Epoch [43/120    avg_loss:0.156, val_acc:0.965]
Epoch [44/120    avg_loss:0.113, val_acc:0.977]
Epoch [45/120    avg_loss:0.065, val_acc:0.973]
Epoch [46/120    avg_loss:0.052, val_acc:0.977]
Epoch [47/120    avg_loss:0.034, val_acc:0.981]
Epoch [48/120    avg_loss:0.049, val_acc:0.985]
Epoch [49/120    avg_loss:0.048, val_acc:0.990]
Epoch [50/120    avg_loss:0.022, val_acc:0.985]
Epoch [51/120    avg_loss:0.031, val_acc:0.981]
Epoch [52/120    avg_loss:0.023, val_acc:0.985]
Epoch [53/120    avg_loss:0.027, val_acc:0.988]
Epoch [54/120    avg_loss:0.090, val_acc:0.969]
Epoch [55/120    avg_loss:0.100, val_acc:0.975]
Epoch [56/120    avg_loss:0.104, val_acc:0.971]
Epoch [57/120    avg_loss:0.105, val_acc:0.960]
Epoch [58/120    avg_loss:0.141, val_acc:0.969]
Epoch [59/120    avg_loss:0.059, val_acc:0.981]
Epoch [60/120    avg_loss:0.072, val_acc:0.983]
Epoch [61/120    avg_loss:0.041, val_acc:0.981]
Epoch [62/120    avg_loss:0.023, val_acc:0.981]
Epoch [63/120    avg_loss:0.019, val_acc:0.981]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.029, val_acc:0.981]
Epoch [66/120    avg_loss:0.040, val_acc:0.981]
Epoch [67/120    avg_loss:0.030, val_acc:0.983]
Epoch [68/120    avg_loss:0.025, val_acc:0.985]
Epoch [69/120    avg_loss:0.023, val_acc:0.985]
Epoch [70/120    avg_loss:0.026, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.985]
Epoch [72/120    avg_loss:0.022, val_acc:0.985]
Epoch [73/120    avg_loss:0.029, val_acc:0.983]
Epoch [74/120    avg_loss:0.025, val_acc:0.988]
Epoch [75/120    avg_loss:0.018, val_acc:0.988]
Epoch [76/120    avg_loss:0.037, val_acc:0.988]
Epoch [77/120    avg_loss:0.022, val_acc:0.988]
Epoch [78/120    avg_loss:0.013, val_acc:0.988]
Epoch [79/120    avg_loss:0.017, val_acc:0.988]
Epoch [80/120    avg_loss:0.023, val_acc:0.988]
Epoch [81/120    avg_loss:0.020, val_acc:0.988]
Epoch [82/120    avg_loss:0.018, val_acc:0.988]
Epoch [83/120    avg_loss:0.021, val_acc:0.988]
Epoch [84/120    avg_loss:0.025, val_acc:0.988]
Epoch [85/120    avg_loss:0.028, val_acc:0.988]
Epoch [86/120    avg_loss:0.017, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.988]
Epoch [88/120    avg_loss:0.013, val_acc:0.988]
Epoch [89/120    avg_loss:0.017, val_acc:0.988]
Epoch [90/120    avg_loss:0.015, val_acc:0.988]
Epoch [91/120    avg_loss:0.032, val_acc:0.988]
Epoch [92/120    avg_loss:0.018, val_acc:0.988]
Epoch [93/120    avg_loss:0.021, val_acc:0.988]
Epoch [94/120    avg_loss:0.021, val_acc:0.988]
Epoch [95/120    avg_loss:0.025, val_acc:0.988]
Epoch [96/120    avg_loss:0.022, val_acc:0.988]
Epoch [97/120    avg_loss:0.027, val_acc:0.988]
Epoch [98/120    avg_loss:0.013, val_acc:0.988]
Epoch [99/120    avg_loss:0.029, val_acc:0.988]
Epoch [100/120    avg_loss:0.026, val_acc:0.988]
Epoch [101/120    avg_loss:0.025, val_acc:0.988]
Epoch [102/120    avg_loss:0.024, val_acc:0.988]
Epoch [103/120    avg_loss:0.017, val_acc:0.988]
Epoch [104/120    avg_loss:0.021, val_acc:0.988]
Epoch [105/120    avg_loss:0.022, val_acc:0.988]
Epoch [106/120    avg_loss:0.022, val_acc:0.988]
Epoch [107/120    avg_loss:0.031, val_acc:0.988]
Epoch [108/120    avg_loss:0.032, val_acc:0.988]
Epoch [109/120    avg_loss:0.018, val_acc:0.988]
Epoch [110/120    avg_loss:0.027, val_acc:0.988]
Epoch [111/120    avg_loss:0.023, val_acc:0.988]
Epoch [112/120    avg_loss:0.036, val_acc:0.988]
Epoch [113/120    avg_loss:0.017, val_acc:0.988]
Epoch [114/120    avg_loss:0.025, val_acc:0.988]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.020, val_acc:0.988]
Epoch [117/120    avg_loss:0.022, val_acc:0.988]
Epoch [118/120    avg_loss:0.022, val_acc:0.988]
Epoch [119/120    avg_loss:0.030, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   6   0   0   0   0   0   0   5   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         0.98871332 1.         0.96860987 0.96928328
 1.         0.9726776  1.         1.         1.         0.98947368
 0.98563536 1.        ]

Kappa:
0.9935903591064202
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8123658748>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.256, val_acc:0.402]
Epoch [2/120    avg_loss:1.703, val_acc:0.631]
Epoch [3/120    avg_loss:1.265, val_acc:0.785]
Epoch [4/120    avg_loss:0.940, val_acc:0.804]
Epoch [5/120    avg_loss:0.805, val_acc:0.810]
Epoch [6/120    avg_loss:0.688, val_acc:0.812]
Epoch [7/120    avg_loss:0.640, val_acc:0.879]
Epoch [8/120    avg_loss:0.541, val_acc:0.833]
Epoch [9/120    avg_loss:0.455, val_acc:0.896]
Epoch [10/120    avg_loss:0.404, val_acc:0.900]
Epoch [11/120    avg_loss:0.362, val_acc:0.904]
Epoch [12/120    avg_loss:0.368, val_acc:0.935]
Epoch [13/120    avg_loss:0.357, val_acc:0.948]
Epoch [14/120    avg_loss:0.266, val_acc:0.950]
Epoch [15/120    avg_loss:0.290, val_acc:0.942]
Epoch [16/120    avg_loss:0.269, val_acc:0.942]
Epoch [17/120    avg_loss:0.264, val_acc:0.948]
Epoch [18/120    avg_loss:0.189, val_acc:0.954]
Epoch [19/120    avg_loss:0.185, val_acc:0.965]
Epoch [20/120    avg_loss:0.175, val_acc:0.960]
Epoch [21/120    avg_loss:0.184, val_acc:0.942]
Epoch [22/120    avg_loss:0.276, val_acc:0.948]
Epoch [23/120    avg_loss:0.221, val_acc:0.954]
Epoch [24/120    avg_loss:0.184, val_acc:0.956]
Epoch [25/120    avg_loss:0.182, val_acc:0.969]
Epoch [26/120    avg_loss:0.120, val_acc:0.931]
Epoch [27/120    avg_loss:0.103, val_acc:0.969]
Epoch [28/120    avg_loss:0.074, val_acc:0.979]
Epoch [29/120    avg_loss:0.076, val_acc:0.967]
Epoch [30/120    avg_loss:0.081, val_acc:0.983]
Epoch [31/120    avg_loss:0.074, val_acc:0.963]
Epoch [32/120    avg_loss:0.078, val_acc:0.979]
Epoch [33/120    avg_loss:0.087, val_acc:0.973]
Epoch [34/120    avg_loss:0.096, val_acc:0.956]
Epoch [35/120    avg_loss:0.070, val_acc:0.979]
Epoch [36/120    avg_loss:0.081, val_acc:0.975]
Epoch [37/120    avg_loss:0.088, val_acc:0.971]
Epoch [38/120    avg_loss:0.055, val_acc:0.965]
Epoch [39/120    avg_loss:0.062, val_acc:0.973]
Epoch [40/120    avg_loss:0.041, val_acc:0.973]
Epoch [41/120    avg_loss:0.037, val_acc:0.985]
Epoch [42/120    avg_loss:0.082, val_acc:0.954]
Epoch [43/120    avg_loss:0.161, val_acc:0.965]
Epoch [44/120    avg_loss:0.095, val_acc:0.940]
Epoch [45/120    avg_loss:0.062, val_acc:0.981]
Epoch [46/120    avg_loss:0.059, val_acc:0.979]
Epoch [47/120    avg_loss:0.063, val_acc:0.971]
Epoch [48/120    avg_loss:0.042, val_acc:0.975]
Epoch [49/120    avg_loss:0.051, val_acc:0.977]
Epoch [50/120    avg_loss:0.054, val_acc:0.988]
Epoch [51/120    avg_loss:0.039, val_acc:0.977]
Epoch [52/120    avg_loss:0.045, val_acc:0.971]
Epoch [53/120    avg_loss:0.062, val_acc:0.983]
Epoch [54/120    avg_loss:0.040, val_acc:0.973]
Epoch [55/120    avg_loss:0.055, val_acc:0.977]
Epoch [56/120    avg_loss:0.051, val_acc:0.983]
Epoch [57/120    avg_loss:0.043, val_acc:0.977]
Epoch [58/120    avg_loss:0.075, val_acc:0.971]
Epoch [59/120    avg_loss:0.054, val_acc:0.973]
Epoch [60/120    avg_loss:0.036, val_acc:0.981]
Epoch [61/120    avg_loss:0.060, val_acc:0.985]
Epoch [62/120    avg_loss:0.070, val_acc:0.981]
Epoch [63/120    avg_loss:0.027, val_acc:0.981]
Epoch [64/120    avg_loss:0.041, val_acc:0.981]
Epoch [65/120    avg_loss:0.016, val_acc:0.983]
Epoch [66/120    avg_loss:0.029, val_acc:0.985]
Epoch [67/120    avg_loss:0.025, val_acc:0.985]
Epoch [68/120    avg_loss:0.015, val_acc:0.988]
Epoch [69/120    avg_loss:0.016, val_acc:0.985]
Epoch [70/120    avg_loss:0.017, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.985]
Epoch [72/120    avg_loss:0.019, val_acc:0.988]
Epoch [73/120    avg_loss:0.025, val_acc:0.988]
Epoch [74/120    avg_loss:0.020, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.983]
Epoch [76/120    avg_loss:0.011, val_acc:0.983]
Epoch [77/120    avg_loss:0.020, val_acc:0.983]
Epoch [78/120    avg_loss:0.022, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.015, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.021, val_acc:0.983]
Epoch [83/120    avg_loss:0.016, val_acc:0.983]
Epoch [84/120    avg_loss:0.021, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.015, val_acc:0.983]
Epoch [87/120    avg_loss:0.026, val_acc:0.983]
Epoch [88/120    avg_loss:0.018, val_acc:0.983]
Epoch [89/120    avg_loss:0.019, val_acc:0.983]
Epoch [90/120    avg_loss:0.016, val_acc:0.983]
Epoch [91/120    avg_loss:0.018, val_acc:0.983]
Epoch [92/120    avg_loss:0.023, val_acc:0.983]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.016, val_acc:0.983]
Epoch [95/120    avg_loss:0.016, val_acc:0.983]
Epoch [96/120    avg_loss:0.014, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.018, val_acc:0.983]
Epoch [101/120    avg_loss:0.014, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.020, val_acc:0.983]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.016, val_acc:0.983]
Epoch [106/120    avg_loss:0.015, val_acc:0.983]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.026, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.020, val_acc:0.983]
Epoch [112/120    avg_loss:0.015, val_acc:0.983]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.028, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.983]
Epoch [118/120    avg_loss:0.019, val_acc:0.983]
Epoch [119/120    avg_loss:0.019, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214   9   0   0   0   0   0   0   4   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   6 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.98206278 1.         0.96396396 0.95945946
 1.         0.96132597 1.         1.         1.         0.99210526
 0.98781838 1.        ]

Kappa:
0.9928783411626869
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a218b16d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.302, val_acc:0.535]
Epoch [2/120    avg_loss:1.662, val_acc:0.637]
Epoch [3/120    avg_loss:1.210, val_acc:0.729]
Epoch [4/120    avg_loss:0.982, val_acc:0.783]
Epoch [5/120    avg_loss:0.892, val_acc:0.800]
Epoch [6/120    avg_loss:0.691, val_acc:0.831]
Epoch [7/120    avg_loss:0.579, val_acc:0.869]
Epoch [8/120    avg_loss:0.517, val_acc:0.881]
Epoch [9/120    avg_loss:0.445, val_acc:0.902]
Epoch [10/120    avg_loss:0.428, val_acc:0.904]
Epoch [11/120    avg_loss:0.355, val_acc:0.904]
Epoch [12/120    avg_loss:0.294, val_acc:0.935]
Epoch [13/120    avg_loss:0.310, val_acc:0.956]
Epoch [14/120    avg_loss:0.350, val_acc:0.933]
Epoch [15/120    avg_loss:0.353, val_acc:0.921]
Epoch [16/120    avg_loss:0.320, val_acc:0.912]
Epoch [17/120    avg_loss:0.237, val_acc:0.927]
Epoch [18/120    avg_loss:0.227, val_acc:0.940]
Epoch [19/120    avg_loss:0.302, val_acc:0.896]
Epoch [20/120    avg_loss:0.311, val_acc:0.942]
Epoch [21/120    avg_loss:0.283, val_acc:0.952]
Epoch [22/120    avg_loss:0.264, val_acc:0.956]
Epoch [23/120    avg_loss:0.200, val_acc:0.963]
Epoch [24/120    avg_loss:0.174, val_acc:0.969]
Epoch [25/120    avg_loss:0.159, val_acc:0.975]
Epoch [26/120    avg_loss:0.120, val_acc:0.981]
Epoch [27/120    avg_loss:0.109, val_acc:0.988]
Epoch [28/120    avg_loss:0.081, val_acc:0.990]
Epoch [29/120    avg_loss:0.111, val_acc:0.983]
Epoch [30/120    avg_loss:0.083, val_acc:0.983]
Epoch [31/120    avg_loss:0.086, val_acc:0.975]
Epoch [32/120    avg_loss:0.073, val_acc:0.979]
Epoch [33/120    avg_loss:0.070, val_acc:0.975]
Epoch [34/120    avg_loss:0.061, val_acc:0.985]
Epoch [35/120    avg_loss:0.085, val_acc:0.985]
Epoch [36/120    avg_loss:0.082, val_acc:0.985]
Epoch [37/120    avg_loss:0.100, val_acc:0.977]
Epoch [38/120    avg_loss:0.060, val_acc:0.979]
Epoch [39/120    avg_loss:0.072, val_acc:0.985]
Epoch [40/120    avg_loss:0.092, val_acc:0.979]
Epoch [41/120    avg_loss:0.090, val_acc:0.971]
Epoch [42/120    avg_loss:0.056, val_acc:0.979]
Epoch [43/120    avg_loss:0.063, val_acc:0.990]
Epoch [44/120    avg_loss:0.035, val_acc:0.992]
Epoch [45/120    avg_loss:0.053, val_acc:0.992]
Epoch [46/120    avg_loss:0.050, val_acc:0.994]
Epoch [47/120    avg_loss:0.033, val_acc:0.994]
Epoch [48/120    avg_loss:0.036, val_acc:0.994]
Epoch [49/120    avg_loss:0.035, val_acc:0.994]
Epoch [50/120    avg_loss:0.036, val_acc:0.994]
Epoch [51/120    avg_loss:0.033, val_acc:0.994]
Epoch [52/120    avg_loss:0.046, val_acc:0.994]
Epoch [53/120    avg_loss:0.026, val_acc:0.994]
Epoch [54/120    avg_loss:0.027, val_acc:0.994]
Epoch [55/120    avg_loss:0.071, val_acc:0.992]
Epoch [56/120    avg_loss:0.034, val_acc:0.992]
Epoch [57/120    avg_loss:0.026, val_acc:0.992]
Epoch [58/120    avg_loss:0.028, val_acc:0.992]
Epoch [59/120    avg_loss:0.028, val_acc:0.992]
Epoch [60/120    avg_loss:0.040, val_acc:0.992]
Epoch [61/120    avg_loss:0.049, val_acc:0.994]
Epoch [62/120    avg_loss:0.028, val_acc:0.994]
Epoch [63/120    avg_loss:0.038, val_acc:0.992]
Epoch [64/120    avg_loss:0.038, val_acc:0.994]
Epoch [65/120    avg_loss:0.037, val_acc:0.994]
Epoch [66/120    avg_loss:0.029, val_acc:0.994]
Epoch [67/120    avg_loss:0.020, val_acc:0.994]
Epoch [68/120    avg_loss:0.023, val_acc:0.994]
Epoch [69/120    avg_loss:0.032, val_acc:0.994]
Epoch [70/120    avg_loss:0.032, val_acc:0.994]
Epoch [71/120    avg_loss:0.037, val_acc:0.992]
Epoch [72/120    avg_loss:0.025, val_acc:0.992]
Epoch [73/120    avg_loss:0.031, val_acc:0.994]
Epoch [74/120    avg_loss:0.030, val_acc:0.996]
Epoch [75/120    avg_loss:0.028, val_acc:0.994]
Epoch [76/120    avg_loss:0.023, val_acc:0.992]
Epoch [77/120    avg_loss:0.029, val_acc:0.992]
Epoch [78/120    avg_loss:0.029, val_acc:0.994]
Epoch [79/120    avg_loss:0.031, val_acc:0.994]
Epoch [80/120    avg_loss:0.022, val_acc:0.994]
Epoch [81/120    avg_loss:0.021, val_acc:0.990]
Epoch [82/120    avg_loss:0.035, val_acc:0.992]
Epoch [83/120    avg_loss:0.018, val_acc:0.992]
Epoch [84/120    avg_loss:0.029, val_acc:0.992]
Epoch [85/120    avg_loss:0.022, val_acc:0.992]
Epoch [86/120    avg_loss:0.020, val_acc:0.994]
Epoch [87/120    avg_loss:0.016, val_acc:0.994]
Epoch [88/120    avg_loss:0.030, val_acc:0.994]
Epoch [89/120    avg_loss:0.032, val_acc:0.994]
Epoch [90/120    avg_loss:0.027, val_acc:0.994]
Epoch [91/120    avg_loss:0.026, val_acc:0.994]
Epoch [92/120    avg_loss:0.026, val_acc:0.994]
Epoch [93/120    avg_loss:0.030, val_acc:0.994]
Epoch [94/120    avg_loss:0.031, val_acc:0.994]
Epoch [95/120    avg_loss:0.037, val_acc:0.992]
Epoch [96/120    avg_loss:0.028, val_acc:0.992]
Epoch [97/120    avg_loss:0.024, val_acc:0.992]
Epoch [98/120    avg_loss:0.022, val_acc:0.992]
Epoch [99/120    avg_loss:0.023, val_acc:0.992]
Epoch [100/120    avg_loss:0.021, val_acc:0.992]
Epoch [101/120    avg_loss:0.029, val_acc:0.992]
Epoch [102/120    avg_loss:0.036, val_acc:0.992]
Epoch [103/120    avg_loss:0.019, val_acc:0.992]
Epoch [104/120    avg_loss:0.024, val_acc:0.992]
Epoch [105/120    avg_loss:0.023, val_acc:0.992]
Epoch [106/120    avg_loss:0.026, val_acc:0.992]
Epoch [107/120    avg_loss:0.018, val_acc:0.992]
Epoch [108/120    avg_loss:0.026, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.992]
Epoch [110/120    avg_loss:0.019, val_acc:0.992]
Epoch [111/120    avg_loss:0.025, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.992]
Epoch [113/120    avg_loss:0.022, val_acc:0.992]
Epoch [114/120    avg_loss:0.023, val_acc:0.992]
Epoch [115/120    avg_loss:0.021, val_acc:0.992]
Epoch [116/120    avg_loss:0.017, val_acc:0.992]
Epoch [117/120    avg_loss:0.023, val_acc:0.992]
Epoch [118/120    avg_loss:0.019, val_acc:0.992]
Epoch [119/120    avg_loss:0.024, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  14   0   0   0   0   0   0   3   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         0.99095023 1.         0.95890411 0.95049505
 1.         0.97826087 1.         1.         1.         0.99602649
 0.99339207 1.        ]

Kappa:
0.9940654528168322
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72aed54780>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.301, val_acc:0.552]
Epoch [2/120    avg_loss:1.640, val_acc:0.575]
Epoch [3/120    avg_loss:1.250, val_acc:0.735]
Epoch [4/120    avg_loss:1.068, val_acc:0.767]
Epoch [5/120    avg_loss:0.875, val_acc:0.808]
Epoch [6/120    avg_loss:0.709, val_acc:0.852]
Epoch [7/120    avg_loss:0.594, val_acc:0.879]
Epoch [8/120    avg_loss:0.498, val_acc:0.915]
Epoch [9/120    avg_loss:0.486, val_acc:0.919]
Epoch [10/120    avg_loss:0.399, val_acc:0.929]
Epoch [11/120    avg_loss:0.368, val_acc:0.940]
Epoch [12/120    avg_loss:0.308, val_acc:0.956]
Epoch [13/120    avg_loss:0.338, val_acc:0.931]
Epoch [14/120    avg_loss:0.342, val_acc:0.923]
Epoch [15/120    avg_loss:0.203, val_acc:0.971]
Epoch [16/120    avg_loss:0.220, val_acc:0.975]
Epoch [17/120    avg_loss:0.155, val_acc:0.954]
Epoch [18/120    avg_loss:0.169, val_acc:0.973]
Epoch [19/120    avg_loss:0.196, val_acc:0.977]
Epoch [20/120    avg_loss:0.204, val_acc:0.963]
Epoch [21/120    avg_loss:0.210, val_acc:0.960]
Epoch [22/120    avg_loss:0.263, val_acc:0.969]
Epoch [23/120    avg_loss:0.151, val_acc:0.975]
Epoch [24/120    avg_loss:0.107, val_acc:0.983]
Epoch [25/120    avg_loss:0.113, val_acc:0.973]
Epoch [26/120    avg_loss:0.094, val_acc:0.971]
Epoch [27/120    avg_loss:0.153, val_acc:0.960]
Epoch [28/120    avg_loss:0.129, val_acc:0.988]
Epoch [29/120    avg_loss:0.120, val_acc:0.963]
Epoch [30/120    avg_loss:0.222, val_acc:0.950]
Epoch [31/120    avg_loss:0.154, val_acc:0.923]
Epoch [32/120    avg_loss:0.098, val_acc:0.988]
Epoch [33/120    avg_loss:0.110, val_acc:0.971]
Epoch [34/120    avg_loss:0.093, val_acc:0.985]
Epoch [35/120    avg_loss:0.102, val_acc:0.983]
Epoch [36/120    avg_loss:0.086, val_acc:0.988]
Epoch [37/120    avg_loss:0.084, val_acc:0.994]
Epoch [38/120    avg_loss:0.077, val_acc:0.990]
Epoch [39/120    avg_loss:0.088, val_acc:0.983]
Epoch [40/120    avg_loss:0.103, val_acc:0.960]
Epoch [41/120    avg_loss:0.125, val_acc:0.975]
Epoch [42/120    avg_loss:0.044, val_acc:0.990]
Epoch [43/120    avg_loss:0.067, val_acc:0.990]
Epoch [44/120    avg_loss:0.044, val_acc:0.998]
Epoch [45/120    avg_loss:0.043, val_acc:0.990]
Epoch [46/120    avg_loss:0.058, val_acc:0.992]
Epoch [47/120    avg_loss:0.030, val_acc:0.988]
Epoch [48/120    avg_loss:0.063, val_acc:0.979]
Epoch [49/120    avg_loss:0.085, val_acc:0.985]
Epoch [50/120    avg_loss:0.048, val_acc:0.990]
Epoch [51/120    avg_loss:0.088, val_acc:0.992]
Epoch [52/120    avg_loss:0.122, val_acc:0.979]
Epoch [53/120    avg_loss:0.041, val_acc:0.992]
Epoch [54/120    avg_loss:0.056, val_acc:0.994]
Epoch [55/120    avg_loss:0.032, val_acc:0.994]
Epoch [56/120    avg_loss:0.039, val_acc:0.981]
Epoch [57/120    avg_loss:0.055, val_acc:0.998]
Epoch [58/120    avg_loss:0.023, val_acc:0.992]
Epoch [59/120    avg_loss:0.034, val_acc:0.994]
Epoch [60/120    avg_loss:0.028, val_acc:0.985]
Epoch [61/120    avg_loss:0.023, val_acc:0.990]
Epoch [62/120    avg_loss:0.032, val_acc:0.988]
Epoch [63/120    avg_loss:0.036, val_acc:0.990]
Epoch [64/120    avg_loss:0.016, val_acc:0.994]
Epoch [65/120    avg_loss:0.016, val_acc:0.996]
Epoch [66/120    avg_loss:0.016, val_acc:0.990]
Epoch [67/120    avg_loss:0.016, val_acc:0.990]
Epoch [68/120    avg_loss:0.017, val_acc:0.996]
Epoch [69/120    avg_loss:0.041, val_acc:0.992]
Epoch [70/120    avg_loss:0.032, val_acc:0.983]
Epoch [71/120    avg_loss:0.018, val_acc:0.985]
Epoch [72/120    avg_loss:0.020, val_acc:0.990]
Epoch [73/120    avg_loss:0.035, val_acc:0.992]
Epoch [74/120    avg_loss:0.015, val_acc:0.992]
Epoch [75/120    avg_loss:0.012, val_acc:0.994]
Epoch [76/120    avg_loss:0.012, val_acc:0.994]
Epoch [77/120    avg_loss:0.013, val_acc:0.994]
Epoch [78/120    avg_loss:0.014, val_acc:0.996]
Epoch [79/120    avg_loss:0.015, val_acc:0.996]
Epoch [80/120    avg_loss:0.009, val_acc:0.998]
Epoch [81/120    avg_loss:0.012, val_acc:0.998]
Epoch [82/120    avg_loss:0.009, val_acc:0.998]
Epoch [83/120    avg_loss:0.008, val_acc:0.998]
Epoch [84/120    avg_loss:0.007, val_acc:0.998]
Epoch [85/120    avg_loss:0.017, val_acc:0.998]
Epoch [86/120    avg_loss:0.008, val_acc:0.998]
Epoch [87/120    avg_loss:0.012, val_acc:0.996]
Epoch [88/120    avg_loss:0.009, val_acc:0.996]
Epoch [89/120    avg_loss:0.016, val_acc:0.996]
Epoch [90/120    avg_loss:0.008, val_acc:0.996]
Epoch [91/120    avg_loss:0.009, val_acc:0.996]
Epoch [92/120    avg_loss:0.014, val_acc:0.998]
Epoch [93/120    avg_loss:0.014, val_acc:0.998]
Epoch [94/120    avg_loss:0.016, val_acc:0.996]
Epoch [95/120    avg_loss:0.010, val_acc:0.996]
Epoch [96/120    avg_loss:0.009, val_acc:0.996]
Epoch [97/120    avg_loss:0.007, val_acc:0.998]
Epoch [98/120    avg_loss:0.017, val_acc:0.996]
Epoch [99/120    avg_loss:0.012, val_acc:0.998]
Epoch [100/120    avg_loss:0.012, val_acc:0.998]
Epoch [101/120    avg_loss:0.009, val_acc:0.998]
Epoch [102/120    avg_loss:0.008, val_acc:0.998]
Epoch [103/120    avg_loss:0.011, val_acc:0.998]
Epoch [104/120    avg_loss:0.010, val_acc:0.998]
Epoch [105/120    avg_loss:0.011, val_acc:0.998]
Epoch [106/120    avg_loss:0.007, val_acc:0.998]
Epoch [107/120    avg_loss:0.007, val_acc:0.998]
Epoch [108/120    avg_loss:0.007, val_acc:0.998]
Epoch [109/120    avg_loss:0.023, val_acc:0.996]
Epoch [110/120    avg_loss:0.009, val_acc:0.998]
Epoch [111/120    avg_loss:0.006, val_acc:0.998]
Epoch [112/120    avg_loss:0.008, val_acc:0.998]
Epoch [113/120    avg_loss:0.010, val_acc:1.000]
Epoch [114/120    avg_loss:0.010, val_acc:1.000]
Epoch [115/120    avg_loss:0.007, val_acc:1.000]
Epoch [116/120    avg_loss:0.007, val_acc:1.000]
Epoch [117/120    avg_loss:0.012, val_acc:0.996]
Epoch [118/120    avg_loss:0.008, val_acc:0.996]
Epoch [119/120    avg_loss:0.011, val_acc:0.996]
Epoch [120/120    avg_loss:0.008, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 1.         0.99545455 1.         0.98678414 0.97931034
 1.         0.98924731 1.         1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9957272441078787
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9df27a17f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.239, val_acc:0.565]
Epoch [2/120    avg_loss:1.627, val_acc:0.654]
Epoch [3/120    avg_loss:1.192, val_acc:0.715]
Epoch [4/120    avg_loss:0.958, val_acc:0.775]
Epoch [5/120    avg_loss:0.760, val_acc:0.773]
Epoch [6/120    avg_loss:0.665, val_acc:0.863]
Epoch [7/120    avg_loss:0.565, val_acc:0.810]
Epoch [8/120    avg_loss:0.511, val_acc:0.902]
Epoch [9/120    avg_loss:0.415, val_acc:0.929]
Epoch [10/120    avg_loss:0.393, val_acc:0.925]
Epoch [11/120    avg_loss:0.345, val_acc:0.890]
Epoch [12/120    avg_loss:0.279, val_acc:0.933]
Epoch [13/120    avg_loss:0.332, val_acc:0.938]
Epoch [14/120    avg_loss:0.246, val_acc:0.944]
Epoch [15/120    avg_loss:0.241, val_acc:0.965]
Epoch [16/120    avg_loss:0.206, val_acc:0.942]
Epoch [17/120    avg_loss:0.213, val_acc:0.960]
Epoch [18/120    avg_loss:0.191, val_acc:0.969]
Epoch [19/120    avg_loss:0.155, val_acc:0.979]
Epoch [20/120    avg_loss:0.131, val_acc:0.971]
Epoch [21/120    avg_loss:0.165, val_acc:0.960]
Epoch [22/120    avg_loss:0.179, val_acc:0.967]
Epoch [23/120    avg_loss:0.245, val_acc:0.952]
Epoch [24/120    avg_loss:0.289, val_acc:0.965]
Epoch [25/120    avg_loss:0.125, val_acc:0.979]
Epoch [26/120    avg_loss:0.105, val_acc:0.965]
Epoch [27/120    avg_loss:0.098, val_acc:0.988]
Epoch [28/120    avg_loss:0.098, val_acc:0.979]
Epoch [29/120    avg_loss:0.100, val_acc:0.992]
Epoch [30/120    avg_loss:0.058, val_acc:0.990]
Epoch [31/120    avg_loss:0.066, val_acc:0.963]
Epoch [32/120    avg_loss:0.078, val_acc:0.983]
Epoch [33/120    avg_loss:0.037, val_acc:0.988]
Epoch [34/120    avg_loss:0.070, val_acc:0.983]
Epoch [35/120    avg_loss:0.069, val_acc:0.983]
Epoch [36/120    avg_loss:0.070, val_acc:0.990]
Epoch [37/120    avg_loss:0.058, val_acc:0.992]
Epoch [38/120    avg_loss:0.078, val_acc:0.973]
Epoch [39/120    avg_loss:0.100, val_acc:0.971]
Epoch [40/120    avg_loss:0.097, val_acc:0.965]
Epoch [41/120    avg_loss:0.061, val_acc:0.981]
Epoch [42/120    avg_loss:0.062, val_acc:0.983]
Epoch [43/120    avg_loss:0.048, val_acc:0.990]
Epoch [44/120    avg_loss:0.066, val_acc:0.988]
Epoch [45/120    avg_loss:0.047, val_acc:0.988]
Epoch [46/120    avg_loss:0.067, val_acc:0.983]
Epoch [47/120    avg_loss:0.034, val_acc:0.988]
Epoch [48/120    avg_loss:0.048, val_acc:0.992]
Epoch [49/120    avg_loss:0.041, val_acc:0.983]
Epoch [50/120    avg_loss:0.029, val_acc:0.990]
Epoch [51/120    avg_loss:0.032, val_acc:0.990]
Epoch [52/120    avg_loss:0.025, val_acc:0.992]
Epoch [53/120    avg_loss:0.027, val_acc:0.990]
Epoch [54/120    avg_loss:0.022, val_acc:0.992]
Epoch [55/120    avg_loss:0.091, val_acc:0.981]
Epoch [56/120    avg_loss:0.059, val_acc:0.977]
Epoch [57/120    avg_loss:0.048, val_acc:0.983]
Epoch [58/120    avg_loss:0.036, val_acc:0.983]
Epoch [59/120    avg_loss:0.042, val_acc:0.992]
Epoch [60/120    avg_loss:0.022, val_acc:0.990]
Epoch [61/120    avg_loss:0.023, val_acc:0.983]
Epoch [62/120    avg_loss:0.044, val_acc:0.985]
Epoch [63/120    avg_loss:0.022, val_acc:0.990]
Epoch [64/120    avg_loss:0.017, val_acc:0.994]
Epoch [65/120    avg_loss:0.012, val_acc:0.992]
Epoch [66/120    avg_loss:0.015, val_acc:0.992]
Epoch [67/120    avg_loss:0.012, val_acc:0.992]
Epoch [68/120    avg_loss:0.019, val_acc:0.992]
Epoch [69/120    avg_loss:0.015, val_acc:0.990]
Epoch [70/120    avg_loss:0.026, val_acc:0.996]
Epoch [71/120    avg_loss:0.019, val_acc:0.994]
Epoch [72/120    avg_loss:0.033, val_acc:0.990]
Epoch [73/120    avg_loss:0.025, val_acc:0.990]
Epoch [74/120    avg_loss:0.033, val_acc:0.990]
Epoch [75/120    avg_loss:0.025, val_acc:0.994]
Epoch [76/120    avg_loss:0.072, val_acc:0.990]
Epoch [77/120    avg_loss:0.039, val_acc:0.985]
Epoch [78/120    avg_loss:0.054, val_acc:0.985]
Epoch [79/120    avg_loss:0.054, val_acc:0.990]
Epoch [80/120    avg_loss:0.019, val_acc:0.990]
Epoch [81/120    avg_loss:0.020, val_acc:0.990]
Epoch [82/120    avg_loss:0.013, val_acc:0.990]
Epoch [83/120    avg_loss:0.015, val_acc:0.990]
Epoch [84/120    avg_loss:0.012, val_acc:0.992]
Epoch [85/120    avg_loss:0.012, val_acc:0.992]
Epoch [86/120    avg_loss:0.013, val_acc:0.992]
Epoch [87/120    avg_loss:0.013, val_acc:0.992]
Epoch [88/120    avg_loss:0.005, val_acc:0.994]
Epoch [89/120    avg_loss:0.009, val_acc:0.994]
Epoch [90/120    avg_loss:0.008, val_acc:0.994]
Epoch [91/120    avg_loss:0.015, val_acc:0.994]
Epoch [92/120    avg_loss:0.008, val_acc:0.994]
Epoch [93/120    avg_loss:0.014, val_acc:0.994]
Epoch [94/120    avg_loss:0.013, val_acc:0.994]
Epoch [95/120    avg_loss:0.010, val_acc:0.994]
Epoch [96/120    avg_loss:0.008, val_acc:0.994]
Epoch [97/120    avg_loss:0.013, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.006, val_acc:0.994]
Epoch [100/120    avg_loss:0.008, val_acc:0.994]
Epoch [101/120    avg_loss:0.012, val_acc:0.994]
Epoch [102/120    avg_loss:0.011, val_acc:0.994]
Epoch [103/120    avg_loss:0.009, val_acc:0.994]
Epoch [104/120    avg_loss:0.008, val_acc:0.994]
Epoch [105/120    avg_loss:0.009, val_acc:0.994]
Epoch [106/120    avg_loss:0.006, val_acc:0.994]
Epoch [107/120    avg_loss:0.010, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.011, val_acc:0.994]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.011, val_acc:0.994]
Epoch [114/120    avg_loss:0.013, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.994]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.010, val_acc:0.994]
Epoch [118/120    avg_loss:0.008, val_acc:0.994]
Epoch [119/120    avg_loss:0.009, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   7   0   0   0   0   0   0   3   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.7228144989339

F1 scores:
[       nan 1.         1.         1.         0.9752809  0.97297297
 1.         1.         1.         1.         1.         0.9973545
 0.99448732 1.        ]

Kappa:
0.9969140357771417
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93c3c127f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.293, val_acc:0.460]
Epoch [2/120    avg_loss:1.695, val_acc:0.533]
Epoch [3/120    avg_loss:1.315, val_acc:0.700]
Epoch [4/120    avg_loss:1.107, val_acc:0.704]
Epoch [5/120    avg_loss:0.883, val_acc:0.783]
Epoch [6/120    avg_loss:0.725, val_acc:0.835]
Epoch [7/120    avg_loss:0.621, val_acc:0.812]
Epoch [8/120    avg_loss:0.546, val_acc:0.858]
Epoch [9/120    avg_loss:0.459, val_acc:0.887]
Epoch [10/120    avg_loss:0.439, val_acc:0.875]
Epoch [11/120    avg_loss:0.390, val_acc:0.904]
Epoch [12/120    avg_loss:0.317, val_acc:0.929]
Epoch [13/120    avg_loss:0.349, val_acc:0.915]
Epoch [14/120    avg_loss:0.288, val_acc:0.915]
Epoch [15/120    avg_loss:0.238, val_acc:0.948]
Epoch [16/120    avg_loss:0.250, val_acc:0.944]
Epoch [17/120    avg_loss:0.260, val_acc:0.950]
Epoch [18/120    avg_loss:0.223, val_acc:0.935]
Epoch [19/120    avg_loss:0.281, val_acc:0.929]
Epoch [20/120    avg_loss:0.283, val_acc:0.946]
Epoch [21/120    avg_loss:0.223, val_acc:0.940]
Epoch [22/120    avg_loss:0.200, val_acc:0.956]
Epoch [23/120    avg_loss:0.226, val_acc:0.958]
Epoch [24/120    avg_loss:0.137, val_acc:0.958]
Epoch [25/120    avg_loss:0.148, val_acc:0.946]
Epoch [26/120    avg_loss:0.139, val_acc:0.952]
Epoch [27/120    avg_loss:0.193, val_acc:0.952]
Epoch [28/120    avg_loss:0.125, val_acc:0.950]
Epoch [29/120    avg_loss:0.138, val_acc:0.977]
Epoch [30/120    avg_loss:0.103, val_acc:0.979]
Epoch [31/120    avg_loss:0.075, val_acc:0.981]
Epoch [32/120    avg_loss:0.124, val_acc:0.988]
Epoch [33/120    avg_loss:0.108, val_acc:0.977]
Epoch [34/120    avg_loss:0.085, val_acc:0.981]
Epoch [35/120    avg_loss:0.077, val_acc:0.977]
Epoch [36/120    avg_loss:0.067, val_acc:0.977]
Epoch [37/120    avg_loss:0.097, val_acc:0.965]
Epoch [38/120    avg_loss:0.141, val_acc:0.983]
Epoch [39/120    avg_loss:0.087, val_acc:0.965]
Epoch [40/120    avg_loss:0.087, val_acc:0.985]
Epoch [41/120    avg_loss:0.091, val_acc:0.985]
Epoch [42/120    avg_loss:0.088, val_acc:0.985]
Epoch [43/120    avg_loss:0.076, val_acc:0.992]
Epoch [44/120    avg_loss:0.070, val_acc:0.965]
Epoch [45/120    avg_loss:0.082, val_acc:0.988]
Epoch [46/120    avg_loss:0.069, val_acc:0.963]
Epoch [47/120    avg_loss:0.106, val_acc:0.971]
Epoch [48/120    avg_loss:0.110, val_acc:0.975]
Epoch [49/120    avg_loss:0.074, val_acc:0.973]
Epoch [50/120    avg_loss:0.053, val_acc:0.994]
Epoch [51/120    avg_loss:0.039, val_acc:0.994]
Epoch [52/120    avg_loss:0.033, val_acc:0.992]
Epoch [53/120    avg_loss:0.048, val_acc:0.985]
Epoch [54/120    avg_loss:0.040, val_acc:0.985]
Epoch [55/120    avg_loss:0.032, val_acc:0.988]
Epoch [56/120    avg_loss:0.026, val_acc:0.994]
Epoch [57/120    avg_loss:0.037, val_acc:0.998]
Epoch [58/120    avg_loss:0.024, val_acc:0.994]
Epoch [59/120    avg_loss:0.087, val_acc:0.992]
Epoch [60/120    avg_loss:0.087, val_acc:0.985]
Epoch [61/120    avg_loss:0.046, val_acc:0.988]
Epoch [62/120    avg_loss:0.048, val_acc:0.992]
Epoch [63/120    avg_loss:0.032, val_acc:0.996]
Epoch [64/120    avg_loss:0.020, val_acc:0.996]
Epoch [65/120    avg_loss:0.025, val_acc:0.996]
Epoch [66/120    avg_loss:0.045, val_acc:0.996]
Epoch [67/120    avg_loss:0.026, val_acc:0.992]
Epoch [68/120    avg_loss:0.017, val_acc:0.998]
Epoch [69/120    avg_loss:0.016, val_acc:0.998]
Epoch [70/120    avg_loss:0.013, val_acc:0.996]
Epoch [71/120    avg_loss:0.008, val_acc:0.998]
Epoch [72/120    avg_loss:0.046, val_acc:0.983]
Epoch [73/120    avg_loss:0.025, val_acc:0.998]
Epoch [74/120    avg_loss:0.022, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.992]
Epoch [76/120    avg_loss:0.012, val_acc:0.998]
Epoch [77/120    avg_loss:0.029, val_acc:0.994]
Epoch [78/120    avg_loss:0.016, val_acc:0.990]
Epoch [79/120    avg_loss:0.014, val_acc:0.996]
Epoch [80/120    avg_loss:0.008, val_acc:0.996]
Epoch [81/120    avg_loss:0.007, val_acc:0.996]
Epoch [82/120    avg_loss:0.015, val_acc:0.998]
Epoch [83/120    avg_loss:0.031, val_acc:0.990]
Epoch [84/120    avg_loss:0.062, val_acc:0.985]
Epoch [85/120    avg_loss:0.048, val_acc:0.988]
Epoch [86/120    avg_loss:0.030, val_acc:0.994]
Epoch [87/120    avg_loss:0.020, val_acc:1.000]
Epoch [88/120    avg_loss:0.015, val_acc:0.998]
Epoch [89/120    avg_loss:0.008, val_acc:0.998]
Epoch [90/120    avg_loss:0.028, val_acc:0.998]
Epoch [91/120    avg_loss:0.010, val_acc:0.994]
Epoch [92/120    avg_loss:0.021, val_acc:0.998]
Epoch [93/120    avg_loss:0.015, val_acc:1.000]
Epoch [94/120    avg_loss:0.016, val_acc:0.996]
Epoch [95/120    avg_loss:0.009, val_acc:0.996]
Epoch [96/120    avg_loss:0.007, val_acc:0.996]
Epoch [97/120    avg_loss:0.012, val_acc:0.990]
Epoch [98/120    avg_loss:0.076, val_acc:0.983]
Epoch [99/120    avg_loss:0.047, val_acc:0.983]
Epoch [100/120    avg_loss:0.048, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.992]
Epoch [102/120    avg_loss:0.026, val_acc:0.998]
Epoch [103/120    avg_loss:0.031, val_acc:0.996]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.994]
Epoch [106/120    avg_loss:0.012, val_acc:0.996]
Epoch [107/120    avg_loss:0.009, val_acc:0.996]
Epoch [108/120    avg_loss:0.014, val_acc:0.996]
Epoch [109/120    avg_loss:0.005, val_acc:0.996]
Epoch [110/120    avg_loss:0.007, val_acc:0.996]
Epoch [111/120    avg_loss:0.010, val_acc:0.996]
Epoch [112/120    avg_loss:0.014, val_acc:0.996]
Epoch [113/120    avg_loss:0.006, val_acc:0.996]
Epoch [114/120    avg_loss:0.008, val_acc:0.996]
Epoch [115/120    avg_loss:0.010, val_acc:0.996]
Epoch [116/120    avg_loss:0.007, val_acc:0.996]
Epoch [117/120    avg_loss:0.009, val_acc:0.996]
Epoch [118/120    avg_loss:0.005, val_acc:0.996]
Epoch [119/120    avg_loss:0.009, val_acc:0.998]
Epoch [120/120    avg_loss:0.010, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   4   0   0   0   0   0   0   3   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.99545455 1.         0.96703297 0.95804196
 1.         0.98924731 1.         1.         1.         0.9843342
 0.98327759 1.        ]

Kappa:
0.9931158179482334
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f768f69c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.278, val_acc:0.637]
Epoch [2/120    avg_loss:1.571, val_acc:0.658]
Epoch [3/120    avg_loss:1.166, val_acc:0.723]
Epoch [4/120    avg_loss:0.929, val_acc:0.798]
Epoch [5/120    avg_loss:0.736, val_acc:0.802]
Epoch [6/120    avg_loss:0.740, val_acc:0.806]
Epoch [7/120    avg_loss:0.560, val_acc:0.838]
Epoch [8/120    avg_loss:0.469, val_acc:0.921]
Epoch [9/120    avg_loss:0.416, val_acc:0.923]
Epoch [10/120    avg_loss:0.381, val_acc:0.917]
Epoch [11/120    avg_loss:0.318, val_acc:0.942]
Epoch [12/120    avg_loss:0.314, val_acc:0.933]
Epoch [13/120    avg_loss:0.292, val_acc:0.933]
Epoch [14/120    avg_loss:0.244, val_acc:0.946]
Epoch [15/120    avg_loss:0.231, val_acc:0.917]
Epoch [16/120    avg_loss:0.226, val_acc:0.977]
Epoch [17/120    avg_loss:0.261, val_acc:0.944]
Epoch [18/120    avg_loss:0.227, val_acc:0.956]
Epoch [19/120    avg_loss:0.202, val_acc:0.971]
Epoch [20/120    avg_loss:0.152, val_acc:0.963]
Epoch [21/120    avg_loss:0.150, val_acc:0.971]
Epoch [22/120    avg_loss:0.169, val_acc:0.960]
Epoch [23/120    avg_loss:0.127, val_acc:0.973]
Epoch [24/120    avg_loss:0.154, val_acc:0.963]
Epoch [25/120    avg_loss:0.107, val_acc:0.967]
Epoch [26/120    avg_loss:0.097, val_acc:0.973]
Epoch [27/120    avg_loss:0.161, val_acc:0.973]
Epoch [28/120    avg_loss:0.147, val_acc:0.977]
Epoch [29/120    avg_loss:0.092, val_acc:0.975]
Epoch [30/120    avg_loss:0.119, val_acc:0.981]
Epoch [31/120    avg_loss:0.091, val_acc:0.990]
Epoch [32/120    avg_loss:0.079, val_acc:0.971]
Epoch [33/120    avg_loss:0.074, val_acc:0.983]
Epoch [34/120    avg_loss:0.078, val_acc:0.979]
Epoch [35/120    avg_loss:0.067, val_acc:0.981]
Epoch [36/120    avg_loss:0.070, val_acc:0.969]
Epoch [37/120    avg_loss:0.062, val_acc:0.967]
Epoch [38/120    avg_loss:0.099, val_acc:0.983]
Epoch [39/120    avg_loss:0.077, val_acc:0.969]
Epoch [40/120    avg_loss:0.047, val_acc:0.988]
Epoch [41/120    avg_loss:0.043, val_acc:0.985]
Epoch [42/120    avg_loss:0.058, val_acc:0.985]
Epoch [43/120    avg_loss:0.093, val_acc:0.979]
Epoch [44/120    avg_loss:0.068, val_acc:0.981]
Epoch [45/120    avg_loss:0.048, val_acc:0.981]
Epoch [46/120    avg_loss:0.037, val_acc:0.990]
Epoch [47/120    avg_loss:0.028, val_acc:0.990]
Epoch [48/120    avg_loss:0.031, val_acc:0.990]
Epoch [49/120    avg_loss:0.036, val_acc:0.990]
Epoch [50/120    avg_loss:0.021, val_acc:0.990]
Epoch [51/120    avg_loss:0.028, val_acc:0.990]
Epoch [52/120    avg_loss:0.048, val_acc:0.990]
Epoch [53/120    avg_loss:0.030, val_acc:0.990]
Epoch [54/120    avg_loss:0.024, val_acc:0.990]
Epoch [55/120    avg_loss:0.029, val_acc:0.990]
Epoch [56/120    avg_loss:0.033, val_acc:0.988]
Epoch [57/120    avg_loss:0.024, val_acc:0.990]
Epoch [58/120    avg_loss:0.027, val_acc:0.990]
Epoch [59/120    avg_loss:0.032, val_acc:0.990]
Epoch [60/120    avg_loss:0.025, val_acc:0.990]
Epoch [61/120    avg_loss:0.045, val_acc:0.988]
Epoch [62/120    avg_loss:0.023, val_acc:0.988]
Epoch [63/120    avg_loss:0.024, val_acc:0.988]
Epoch [64/120    avg_loss:0.018, val_acc:0.988]
Epoch [65/120    avg_loss:0.019, val_acc:0.988]
Epoch [66/120    avg_loss:0.019, val_acc:0.988]
Epoch [67/120    avg_loss:0.027, val_acc:0.988]
Epoch [68/120    avg_loss:0.021, val_acc:0.988]
Epoch [69/120    avg_loss:0.026, val_acc:0.988]
Epoch [70/120    avg_loss:0.018, val_acc:0.990]
Epoch [71/120    avg_loss:0.024, val_acc:0.988]
Epoch [72/120    avg_loss:0.018, val_acc:0.988]
Epoch [73/120    avg_loss:0.035, val_acc:0.988]
Epoch [74/120    avg_loss:0.020, val_acc:0.988]
Epoch [75/120    avg_loss:0.027, val_acc:0.988]
Epoch [76/120    avg_loss:0.024, val_acc:0.988]
Epoch [77/120    avg_loss:0.021, val_acc:0.988]
Epoch [78/120    avg_loss:0.027, val_acc:0.988]
Epoch [79/120    avg_loss:0.037, val_acc:0.988]
Epoch [80/120    avg_loss:0.017, val_acc:0.990]
Epoch [81/120    avg_loss:0.019, val_acc:0.988]
Epoch [82/120    avg_loss:0.018, val_acc:0.988]
Epoch [83/120    avg_loss:0.029, val_acc:0.988]
Epoch [84/120    avg_loss:0.018, val_acc:0.988]
Epoch [85/120    avg_loss:0.020, val_acc:0.990]
Epoch [86/120    avg_loss:0.026, val_acc:0.990]
Epoch [87/120    avg_loss:0.024, val_acc:0.990]
Epoch [88/120    avg_loss:0.019, val_acc:0.990]
Epoch [89/120    avg_loss:0.018, val_acc:0.990]
Epoch [90/120    avg_loss:0.025, val_acc:0.990]
Epoch [91/120    avg_loss:0.018, val_acc:0.990]
Epoch [92/120    avg_loss:0.023, val_acc:0.990]
Epoch [93/120    avg_loss:0.020, val_acc:0.990]
Epoch [94/120    avg_loss:0.017, val_acc:0.988]
Epoch [95/120    avg_loss:0.018, val_acc:0.990]
Epoch [96/120    avg_loss:0.024, val_acc:0.990]
Epoch [97/120    avg_loss:0.032, val_acc:0.990]
Epoch [98/120    avg_loss:0.019, val_acc:0.988]
Epoch [99/120    avg_loss:0.027, val_acc:0.988]
Epoch [100/120    avg_loss:0.022, val_acc:0.992]
Epoch [101/120    avg_loss:0.022, val_acc:0.992]
Epoch [102/120    avg_loss:0.019, val_acc:0.992]
Epoch [103/120    avg_loss:0.022, val_acc:0.992]
Epoch [104/120    avg_loss:0.014, val_acc:0.992]
Epoch [105/120    avg_loss:0.016, val_acc:0.992]
Epoch [106/120    avg_loss:0.017, val_acc:0.992]
Epoch [107/120    avg_loss:0.012, val_acc:0.992]
Epoch [108/120    avg_loss:0.022, val_acc:0.992]
Epoch [109/120    avg_loss:0.022, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.011, val_acc:0.992]
Epoch [112/120    avg_loss:0.015, val_acc:0.992]
Epoch [113/120    avg_loss:0.024, val_acc:0.990]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.990]
Epoch [118/120    avg_loss:0.035, val_acc:0.992]
Epoch [119/120    avg_loss:0.017, val_acc:0.990]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   8   0   0   0   0   0   0   2   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.99095023 1.         0.96017699 0.94482759
 1.         0.97826087 1.         1.         1.         0.98950131
 0.98888889 1.        ]

Kappa:
0.9928784323602413
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7e5d4f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 226752==>0.23M
----------Training process----------
Epoch [1/120    avg_loss:2.301, val_acc:0.544]
Epoch [2/120    avg_loss:1.653, val_acc:0.667]
Epoch [3/120    avg_loss:1.215, val_acc:0.738]
Epoch [4/120    avg_loss:0.963, val_acc:0.796]
Epoch [5/120    avg_loss:0.794, val_acc:0.860]
Epoch [6/120    avg_loss:0.715, val_acc:0.817]
Epoch [7/120    avg_loss:0.602, val_acc:0.873]
Epoch [8/120    avg_loss:0.580, val_acc:0.863]
Epoch [9/120    avg_loss:0.489, val_acc:0.898]
Epoch [10/120    avg_loss:0.423, val_acc:0.896]
Epoch [11/120    avg_loss:0.389, val_acc:0.952]
Epoch [12/120    avg_loss:0.289, val_acc:0.954]
Epoch [13/120    avg_loss:0.301, val_acc:0.844]
Epoch [14/120    avg_loss:0.316, val_acc:0.954]
Epoch [15/120    avg_loss:0.269, val_acc:0.942]
Epoch [16/120    avg_loss:0.242, val_acc:0.960]
Epoch [17/120    avg_loss:0.211, val_acc:0.958]
Epoch [18/120    avg_loss:0.164, val_acc:0.973]
Epoch [19/120    avg_loss:0.176, val_acc:0.950]
Epoch [20/120    avg_loss:0.166, val_acc:0.946]
Epoch [21/120    avg_loss:0.181, val_acc:0.950]
Epoch [22/120    avg_loss:0.150, val_acc:0.944]
Epoch [23/120    avg_loss:0.165, val_acc:0.946]
Epoch [24/120    avg_loss:0.319, val_acc:0.958]
Epoch [25/120    avg_loss:0.167, val_acc:0.944]
Epoch [26/120    avg_loss:0.153, val_acc:0.954]
Epoch [27/120    avg_loss:0.154, val_acc:0.971]
Epoch [28/120    avg_loss:0.178, val_acc:0.967]
Epoch [29/120    avg_loss:0.091, val_acc:0.965]
Epoch [30/120    avg_loss:0.079, val_acc:0.975]
Epoch [31/120    avg_loss:0.100, val_acc:0.969]
Epoch [32/120    avg_loss:0.109, val_acc:0.977]
Epoch [33/120    avg_loss:0.094, val_acc:0.971]
Epoch [34/120    avg_loss:0.208, val_acc:0.963]
Epoch [35/120    avg_loss:0.133, val_acc:0.967]
Epoch [36/120    avg_loss:0.139, val_acc:0.977]
Epoch [37/120    avg_loss:0.085, val_acc:0.965]
Epoch [38/120    avg_loss:0.098, val_acc:0.969]
Epoch [39/120    avg_loss:0.082, val_acc:0.956]
Epoch [40/120    avg_loss:0.078, val_acc:0.973]
Epoch [41/120    avg_loss:0.063, val_acc:0.983]
Epoch [42/120    avg_loss:0.090, val_acc:0.975]
Epoch [43/120    avg_loss:0.077, val_acc:0.979]
Epoch [44/120    avg_loss:0.055, val_acc:0.981]
Epoch [45/120    avg_loss:0.044, val_acc:0.981]
Epoch [46/120    avg_loss:0.036, val_acc:0.983]
Epoch [47/120    avg_loss:0.032, val_acc:0.985]
Epoch [48/120    avg_loss:0.029, val_acc:0.988]
Epoch [49/120    avg_loss:0.040, val_acc:0.983]
Epoch [50/120    avg_loss:0.034, val_acc:0.988]
Epoch [51/120    avg_loss:0.085, val_acc:0.983]
Epoch [52/120    avg_loss:0.071, val_acc:0.977]
Epoch [53/120    avg_loss:0.059, val_acc:0.983]
Epoch [54/120    avg_loss:0.028, val_acc:0.981]
Epoch [55/120    avg_loss:0.025, val_acc:0.985]
Epoch [56/120    avg_loss:0.018, val_acc:0.988]
Epoch [57/120    avg_loss:0.035, val_acc:0.983]
Epoch [58/120    avg_loss:0.023, val_acc:0.990]
Epoch [59/120    avg_loss:0.068, val_acc:0.981]
Epoch [60/120    avg_loss:0.035, val_acc:0.985]
Epoch [61/120    avg_loss:0.031, val_acc:0.983]
Epoch [62/120    avg_loss:0.030, val_acc:0.988]
Epoch [63/120    avg_loss:0.029, val_acc:0.994]
Epoch [64/120    avg_loss:0.021, val_acc:0.988]
Epoch [65/120    avg_loss:0.013, val_acc:0.981]
Epoch [66/120    avg_loss:0.024, val_acc:0.985]
Epoch [67/120    avg_loss:0.018, val_acc:0.988]
Epoch [68/120    avg_loss:0.018, val_acc:0.990]
Epoch [69/120    avg_loss:0.017, val_acc:0.990]
Epoch [70/120    avg_loss:0.025, val_acc:0.992]
Epoch [71/120    avg_loss:0.015, val_acc:0.990]
Epoch [72/120    avg_loss:0.021, val_acc:0.988]
Epoch [73/120    avg_loss:0.020, val_acc:0.985]
Epoch [74/120    avg_loss:0.020, val_acc:0.981]
Epoch [75/120    avg_loss:0.016, val_acc:0.988]
Epoch [76/120    avg_loss:0.012, val_acc:0.992]
Epoch [77/120    avg_loss:0.012, val_acc:0.992]
Epoch [78/120    avg_loss:0.014, val_acc:0.992]
Epoch [79/120    avg_loss:0.012, val_acc:0.990]
Epoch [80/120    avg_loss:0.018, val_acc:0.992]
Epoch [81/120    avg_loss:0.016, val_acc:0.992]
Epoch [82/120    avg_loss:0.012, val_acc:0.990]
Epoch [83/120    avg_loss:0.016, val_acc:0.990]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.022, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.014, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.011, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.015, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.012, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.988]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.013, val_acc:0.988]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.021, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.82942430703625

F1 scores:
[       nan 1.         0.99545455 1.         0.98678414 0.97931034
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9981009243315527
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb6f4c74748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.252, val_acc:0.535]
Epoch [2/120    avg_loss:1.542, val_acc:0.646]
Epoch [3/120    avg_loss:1.109, val_acc:0.750]
Epoch [4/120    avg_loss:0.898, val_acc:0.750]
Epoch [5/120    avg_loss:0.718, val_acc:0.827]
Epoch [6/120    avg_loss:0.671, val_acc:0.838]
Epoch [7/120    avg_loss:0.519, val_acc:0.844]
Epoch [8/120    avg_loss:0.474, val_acc:0.898]
Epoch [9/120    avg_loss:0.475, val_acc:0.871]
Epoch [10/120    avg_loss:0.408, val_acc:0.900]
Epoch [11/120    avg_loss:0.390, val_acc:0.917]
Epoch [12/120    avg_loss:0.443, val_acc:0.906]
Epoch [13/120    avg_loss:0.319, val_acc:0.940]
Epoch [14/120    avg_loss:0.237, val_acc:0.952]
Epoch [15/120    avg_loss:0.222, val_acc:0.942]
Epoch [16/120    avg_loss:0.278, val_acc:0.940]
Epoch [17/120    avg_loss:0.261, val_acc:0.902]
Epoch [18/120    avg_loss:0.254, val_acc:0.946]
Epoch [19/120    avg_loss:0.301, val_acc:0.963]
Epoch [20/120    avg_loss:0.226, val_acc:0.902]
Epoch [21/120    avg_loss:0.200, val_acc:0.958]
Epoch [22/120    avg_loss:0.149, val_acc:0.935]
Epoch [23/120    avg_loss:0.197, val_acc:0.965]
Epoch [24/120    avg_loss:0.153, val_acc:0.958]
Epoch [25/120    avg_loss:0.077, val_acc:0.963]
Epoch [26/120    avg_loss:0.107, val_acc:0.973]
Epoch [27/120    avg_loss:0.116, val_acc:0.958]
Epoch [28/120    avg_loss:0.187, val_acc:0.960]
Epoch [29/120    avg_loss:0.158, val_acc:0.950]
Epoch [30/120    avg_loss:0.145, val_acc:0.965]
Epoch [31/120    avg_loss:0.068, val_acc:0.969]
Epoch [32/120    avg_loss:0.121, val_acc:0.975]
Epoch [33/120    avg_loss:0.085, val_acc:0.973]
Epoch [34/120    avg_loss:0.071, val_acc:0.975]
Epoch [35/120    avg_loss:0.049, val_acc:0.977]
Epoch [36/120    avg_loss:0.054, val_acc:0.977]
Epoch [37/120    avg_loss:0.066, val_acc:0.979]
Epoch [38/120    avg_loss:0.040, val_acc:0.981]
Epoch [39/120    avg_loss:0.036, val_acc:0.979]
Epoch [40/120    avg_loss:0.062, val_acc:0.990]
Epoch [41/120    avg_loss:0.050, val_acc:0.965]
Epoch [42/120    avg_loss:0.083, val_acc:0.975]
Epoch [43/120    avg_loss:0.094, val_acc:0.981]
Epoch [44/120    avg_loss:0.082, val_acc:0.981]
Epoch [45/120    avg_loss:0.058, val_acc:0.981]
Epoch [46/120    avg_loss:0.045, val_acc:0.981]
Epoch [47/120    avg_loss:0.038, val_acc:0.983]
Epoch [48/120    avg_loss:0.028, val_acc:0.981]
Epoch [49/120    avg_loss:0.020, val_acc:0.985]
Epoch [50/120    avg_loss:0.034, val_acc:0.983]
Epoch [51/120    avg_loss:0.038, val_acc:0.975]
Epoch [52/120    avg_loss:0.099, val_acc:0.971]
Epoch [53/120    avg_loss:0.052, val_acc:0.981]
Epoch [54/120    avg_loss:0.049, val_acc:0.981]
Epoch [55/120    avg_loss:0.030, val_acc:0.981]
Epoch [56/120    avg_loss:0.030, val_acc:0.988]
Epoch [57/120    avg_loss:0.016, val_acc:0.990]
Epoch [58/120    avg_loss:0.022, val_acc:0.990]
Epoch [59/120    avg_loss:0.025, val_acc:0.985]
Epoch [60/120    avg_loss:0.023, val_acc:0.983]
Epoch [61/120    avg_loss:0.020, val_acc:0.990]
Epoch [62/120    avg_loss:0.017, val_acc:0.990]
Epoch [63/120    avg_loss:0.036, val_acc:0.988]
Epoch [64/120    avg_loss:0.019, val_acc:0.990]
Epoch [65/120    avg_loss:0.029, val_acc:0.990]
Epoch [66/120    avg_loss:0.024, val_acc:0.990]
Epoch [67/120    avg_loss:0.015, val_acc:0.990]
Epoch [68/120    avg_loss:0.015, val_acc:0.990]
Epoch [69/120    avg_loss:0.014, val_acc:0.990]
Epoch [70/120    avg_loss:0.019, val_acc:0.990]
Epoch [71/120    avg_loss:0.029, val_acc:0.992]
Epoch [72/120    avg_loss:0.021, val_acc:0.990]
Epoch [73/120    avg_loss:0.016, val_acc:0.990]
Epoch [74/120    avg_loss:0.015, val_acc:0.992]
Epoch [75/120    avg_loss:0.014, val_acc:0.992]
Epoch [76/120    avg_loss:0.018, val_acc:0.990]
Epoch [77/120    avg_loss:0.029, val_acc:0.988]
Epoch [78/120    avg_loss:0.023, val_acc:0.988]
Epoch [79/120    avg_loss:0.027, val_acc:0.990]
Epoch [80/120    avg_loss:0.016, val_acc:0.992]
Epoch [81/120    avg_loss:0.016, val_acc:0.992]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.016, val_acc:0.990]
Epoch [84/120    avg_loss:0.017, val_acc:0.992]
Epoch [85/120    avg_loss:0.020, val_acc:0.992]
Epoch [86/120    avg_loss:0.015, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.990]
Epoch [88/120    avg_loss:0.019, val_acc:0.992]
Epoch [89/120    avg_loss:0.013, val_acc:0.990]
Epoch [90/120    avg_loss:0.018, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.990]
Epoch [92/120    avg_loss:0.015, val_acc:0.992]
Epoch [93/120    avg_loss:0.016, val_acc:0.994]
Epoch [94/120    avg_loss:0.013, val_acc:0.994]
Epoch [95/120    avg_loss:0.016, val_acc:0.992]
Epoch [96/120    avg_loss:0.012, val_acc:0.992]
Epoch [97/120    avg_loss:0.014, val_acc:0.992]
Epoch [98/120    avg_loss:0.014, val_acc:0.990]
Epoch [99/120    avg_loss:0.019, val_acc:0.992]
Epoch [100/120    avg_loss:0.015, val_acc:0.994]
Epoch [101/120    avg_loss:0.013, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.990]
Epoch [103/120    avg_loss:0.016, val_acc:0.992]
Epoch [104/120    avg_loss:0.014, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.992]
Epoch [107/120    avg_loss:0.017, val_acc:0.992]
Epoch [108/120    avg_loss:0.013, val_acc:0.992]
Epoch [109/120    avg_loss:0.011, val_acc:0.992]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.027, val_acc:0.992]
Epoch [112/120    avg_loss:0.021, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.011, val_acc:0.990]
Epoch [115/120    avg_loss:0.025, val_acc:0.990]
Epoch [116/120    avg_loss:0.011, val_acc:0.990]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.019, val_acc:0.990]
Epoch [119/120    avg_loss:0.017, val_acc:0.990]
Epoch [120/120    avg_loss:0.014, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.89339019189765

F1 scores:
[       nan 1.         1.         1.         0.98886414 0.98305085
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9988131173561215
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff664f4c4e0>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.284, val_acc:0.602]
Epoch [2/120    avg_loss:1.601, val_acc:0.631]
Epoch [3/120    avg_loss:1.202, val_acc:0.725]
Epoch [4/120    avg_loss:0.966, val_acc:0.771]
Epoch [5/120    avg_loss:0.724, val_acc:0.767]
Epoch [6/120    avg_loss:0.663, val_acc:0.858]
Epoch [7/120    avg_loss:0.504, val_acc:0.925]
Epoch [8/120    avg_loss:0.454, val_acc:0.906]
Epoch [9/120    avg_loss:0.348, val_acc:0.931]
Epoch [10/120    avg_loss:0.416, val_acc:0.927]
Epoch [11/120    avg_loss:0.398, val_acc:0.948]
Epoch [12/120    avg_loss:0.275, val_acc:0.960]
Epoch [13/120    avg_loss:0.308, val_acc:0.933]
Epoch [14/120    avg_loss:0.270, val_acc:0.965]
Epoch [15/120    avg_loss:0.231, val_acc:0.958]
Epoch [16/120    avg_loss:0.231, val_acc:0.958]
Epoch [17/120    avg_loss:0.270, val_acc:0.956]
Epoch [18/120    avg_loss:0.236, val_acc:0.965]
Epoch [19/120    avg_loss:0.212, val_acc:0.979]
Epoch [20/120    avg_loss:0.195, val_acc:0.960]
Epoch [21/120    avg_loss:0.207, val_acc:0.969]
Epoch [22/120    avg_loss:0.189, val_acc:0.956]
Epoch [23/120    avg_loss:0.220, val_acc:0.960]
Epoch [24/120    avg_loss:0.140, val_acc:0.988]
Epoch [25/120    avg_loss:0.153, val_acc:0.969]
Epoch [26/120    avg_loss:0.106, val_acc:0.990]
Epoch [27/120    avg_loss:0.106, val_acc:0.994]
Epoch [28/120    avg_loss:0.111, val_acc:0.985]
Epoch [29/120    avg_loss:0.089, val_acc:0.996]
Epoch [30/120    avg_loss:0.092, val_acc:0.990]
Epoch [31/120    avg_loss:0.061, val_acc:0.994]
Epoch [32/120    avg_loss:0.082, val_acc:0.990]
Epoch [33/120    avg_loss:0.152, val_acc:0.990]
Epoch [34/120    avg_loss:0.074, val_acc:0.996]
Epoch [35/120    avg_loss:0.072, val_acc:0.985]
Epoch [36/120    avg_loss:0.065, val_acc:0.969]
Epoch [37/120    avg_loss:0.070, val_acc:0.996]
Epoch [38/120    avg_loss:0.073, val_acc:0.988]
Epoch [39/120    avg_loss:0.090, val_acc:0.996]
Epoch [40/120    avg_loss:0.069, val_acc:0.990]
Epoch [41/120    avg_loss:0.104, val_acc:0.996]
Epoch [42/120    avg_loss:0.092, val_acc:0.985]
Epoch [43/120    avg_loss:0.101, val_acc:0.998]
Epoch [44/120    avg_loss:0.095, val_acc:0.998]
Epoch [45/120    avg_loss:0.073, val_acc:0.994]
Epoch [46/120    avg_loss:0.103, val_acc:0.994]
Epoch [47/120    avg_loss:0.082, val_acc:0.996]
Epoch [48/120    avg_loss:0.079, val_acc:0.983]
Epoch [49/120    avg_loss:0.062, val_acc:0.992]
Epoch [50/120    avg_loss:0.060, val_acc:0.994]
Epoch [51/120    avg_loss:0.054, val_acc:0.988]
Epoch [52/120    avg_loss:0.047, val_acc:0.994]
Epoch [53/120    avg_loss:0.053, val_acc:0.996]
Epoch [54/120    avg_loss:0.059, val_acc:0.998]
Epoch [55/120    avg_loss:0.024, val_acc:0.992]
Epoch [56/120    avg_loss:0.036, val_acc:1.000]
Epoch [57/120    avg_loss:0.026, val_acc:1.000]
Epoch [58/120    avg_loss:0.033, val_acc:0.994]
Epoch [59/120    avg_loss:0.045, val_acc:0.988]
Epoch [60/120    avg_loss:0.066, val_acc:0.996]
Epoch [61/120    avg_loss:0.020, val_acc:0.994]
Epoch [62/120    avg_loss:0.045, val_acc:0.996]
Epoch [63/120    avg_loss:0.030, val_acc:0.998]
Epoch [64/120    avg_loss:0.018, val_acc:0.998]
Epoch [65/120    avg_loss:0.021, val_acc:0.998]
Epoch [66/120    avg_loss:0.020, val_acc:0.996]
Epoch [67/120    avg_loss:0.030, val_acc:1.000]
Epoch [68/120    avg_loss:0.030, val_acc:0.998]
Epoch [69/120    avg_loss:0.040, val_acc:0.996]
Epoch [70/120    avg_loss:0.027, val_acc:0.996]
Epoch [71/120    avg_loss:0.022, val_acc:0.998]
Epoch [72/120    avg_loss:0.024, val_acc:1.000]
Epoch [73/120    avg_loss:0.022, val_acc:1.000]
Epoch [74/120    avg_loss:0.029, val_acc:0.998]
Epoch [75/120    avg_loss:0.020, val_acc:0.998]
Epoch [76/120    avg_loss:0.016, val_acc:0.996]
Epoch [77/120    avg_loss:0.014, val_acc:0.998]
Epoch [78/120    avg_loss:0.021, val_acc:1.000]
Epoch [79/120    avg_loss:0.014, val_acc:1.000]
Epoch [80/120    avg_loss:0.018, val_acc:1.000]
Epoch [81/120    avg_loss:0.011, val_acc:1.000]
Epoch [82/120    avg_loss:0.017, val_acc:1.000]
Epoch [83/120    avg_loss:0.014, val_acc:0.994]
Epoch [84/120    avg_loss:0.015, val_acc:1.000]
Epoch [85/120    avg_loss:0.045, val_acc:0.998]
Epoch [86/120    avg_loss:0.024, val_acc:1.000]
Epoch [87/120    avg_loss:0.027, val_acc:1.000]
Epoch [88/120    avg_loss:0.012, val_acc:1.000]
Epoch [89/120    avg_loss:0.013, val_acc:0.998]
Epoch [90/120    avg_loss:0.006, val_acc:0.998]
Epoch [91/120    avg_loss:0.006, val_acc:0.998]
Epoch [92/120    avg_loss:0.010, val_acc:1.000]
Epoch [93/120    avg_loss:0.018, val_acc:1.000]
Epoch [94/120    avg_loss:0.015, val_acc:1.000]
Epoch [95/120    avg_loss:0.013, val_acc:1.000]
Epoch [96/120    avg_loss:0.014, val_acc:1.000]
Epoch [97/120    avg_loss:0.016, val_acc:1.000]
Epoch [98/120    avg_loss:0.014, val_acc:0.996]
Epoch [99/120    avg_loss:0.010, val_acc:0.998]
Epoch [100/120    avg_loss:0.024, val_acc:1.000]
Epoch [101/120    avg_loss:0.070, val_acc:0.992]
Epoch [102/120    avg_loss:0.047, val_acc:0.994]
Epoch [103/120    avg_loss:0.053, val_acc:1.000]
Epoch [104/120    avg_loss:0.045, val_acc:0.998]
Epoch [105/120    avg_loss:0.153, val_acc:0.983]
Epoch [106/120    avg_loss:0.167, val_acc:0.925]
Epoch [107/120    avg_loss:0.066, val_acc:0.998]
Epoch [108/120    avg_loss:0.040, val_acc:0.994]
Epoch [109/120    avg_loss:0.033, val_acc:0.988]
Epoch [110/120    avg_loss:0.054, val_acc:0.990]
Epoch [111/120    avg_loss:0.037, val_acc:0.996]
Epoch [112/120    avg_loss:0.015, val_acc:0.998]
Epoch [113/120    avg_loss:0.023, val_acc:1.000]
Epoch [114/120    avg_loss:0.014, val_acc:0.998]
Epoch [115/120    avg_loss:0.016, val_acc:0.998]
Epoch [116/120    avg_loss:0.014, val_acc:0.998]
Epoch [117/120    avg_loss:0.009, val_acc:0.998]
Epoch [118/120    avg_loss:0.010, val_acc:0.998]
Epoch [119/120    avg_loss:0.013, val_acc:1.000]
Epoch [120/120    avg_loss:0.019, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.76545842217485

F1 scores:
[       nan 1.         1.         1.         0.98672566 0.97945205
 1.         1.         1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.997388875892598
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f25d03f8828>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.274, val_acc:0.531]
Epoch [2/120    avg_loss:1.664, val_acc:0.615]
Epoch [3/120    avg_loss:1.231, val_acc:0.729]
Epoch [4/120    avg_loss:0.954, val_acc:0.779]
Epoch [5/120    avg_loss:0.772, val_acc:0.779]
Epoch [6/120    avg_loss:0.673, val_acc:0.825]
Epoch [7/120    avg_loss:0.578, val_acc:0.838]
Epoch [8/120    avg_loss:0.513, val_acc:0.844]
Epoch [9/120    avg_loss:0.483, val_acc:0.894]
Epoch [10/120    avg_loss:0.409, val_acc:0.925]
Epoch [11/120    avg_loss:0.408, val_acc:0.912]
Epoch [12/120    avg_loss:0.385, val_acc:0.904]
Epoch [13/120    avg_loss:0.299, val_acc:0.925]
Epoch [14/120    avg_loss:0.304, val_acc:0.935]
Epoch [15/120    avg_loss:0.266, val_acc:0.944]
Epoch [16/120    avg_loss:0.346, val_acc:0.935]
Epoch [17/120    avg_loss:0.237, val_acc:0.935]
Epoch [18/120    avg_loss:0.221, val_acc:0.954]
Epoch [19/120    avg_loss:0.194, val_acc:0.948]
Epoch [20/120    avg_loss:0.183, val_acc:0.954]
Epoch [21/120    avg_loss:0.234, val_acc:0.952]
Epoch [22/120    avg_loss:0.164, val_acc:0.965]
Epoch [23/120    avg_loss:0.191, val_acc:0.944]
Epoch [24/120    avg_loss:0.185, val_acc:0.927]
Epoch [25/120    avg_loss:0.234, val_acc:0.963]
Epoch [26/120    avg_loss:0.206, val_acc:0.956]
Epoch [27/120    avg_loss:0.125, val_acc:0.965]
Epoch [28/120    avg_loss:0.178, val_acc:0.977]
Epoch [29/120    avg_loss:0.140, val_acc:0.969]
Epoch [30/120    avg_loss:0.101, val_acc:0.975]
Epoch [31/120    avg_loss:0.125, val_acc:0.979]
Epoch [32/120    avg_loss:0.103, val_acc:0.975]
Epoch [33/120    avg_loss:0.083, val_acc:0.979]
Epoch [34/120    avg_loss:0.121, val_acc:0.973]
Epoch [35/120    avg_loss:0.120, val_acc:0.971]
Epoch [36/120    avg_loss:0.072, val_acc:0.979]
Epoch [37/120    avg_loss:0.061, val_acc:0.981]
Epoch [38/120    avg_loss:0.055, val_acc:0.983]
Epoch [39/120    avg_loss:0.060, val_acc:0.985]
Epoch [40/120    avg_loss:0.064, val_acc:0.988]
Epoch [41/120    avg_loss:0.045, val_acc:0.981]
Epoch [42/120    avg_loss:0.049, val_acc:0.990]
Epoch [43/120    avg_loss:0.054, val_acc:0.975]
Epoch [44/120    avg_loss:0.069, val_acc:0.990]
Epoch [45/120    avg_loss:0.059, val_acc:0.981]
Epoch [46/120    avg_loss:0.072, val_acc:0.971]
Epoch [47/120    avg_loss:0.051, val_acc:0.965]
Epoch [48/120    avg_loss:0.048, val_acc:0.990]
Epoch [49/120    avg_loss:0.067, val_acc:0.985]
Epoch [50/120    avg_loss:0.063, val_acc:0.979]
Epoch [51/120    avg_loss:0.048, val_acc:0.990]
Epoch [52/120    avg_loss:0.038, val_acc:0.981]
Epoch [53/120    avg_loss:0.029, val_acc:0.992]
Epoch [54/120    avg_loss:0.039, val_acc:0.988]
Epoch [55/120    avg_loss:0.029, val_acc:0.992]
Epoch [56/120    avg_loss:0.023, val_acc:0.979]
Epoch [57/120    avg_loss:0.031, val_acc:0.988]
Epoch [58/120    avg_loss:0.031, val_acc:0.981]
Epoch [59/120    avg_loss:0.012, val_acc:0.981]
Epoch [60/120    avg_loss:0.020, val_acc:0.992]
Epoch [61/120    avg_loss:0.020, val_acc:0.992]
Epoch [62/120    avg_loss:0.022, val_acc:0.992]
Epoch [63/120    avg_loss:0.032, val_acc:0.992]
Epoch [64/120    avg_loss:0.041, val_acc:0.994]
Epoch [65/120    avg_loss:0.029, val_acc:0.992]
Epoch [66/120    avg_loss:0.114, val_acc:0.988]
Epoch [67/120    avg_loss:0.059, val_acc:0.992]
Epoch [68/120    avg_loss:0.084, val_acc:0.981]
Epoch [69/120    avg_loss:0.062, val_acc:0.981]
Epoch [70/120    avg_loss:0.078, val_acc:0.990]
Epoch [71/120    avg_loss:0.043, val_acc:0.994]
Epoch [72/120    avg_loss:0.068, val_acc:0.994]
Epoch [73/120    avg_loss:0.028, val_acc:0.979]
Epoch [74/120    avg_loss:0.025, val_acc:0.992]
Epoch [75/120    avg_loss:0.041, val_acc:0.979]
Epoch [76/120    avg_loss:0.027, val_acc:0.994]
Epoch [77/120    avg_loss:0.042, val_acc:0.975]
Epoch [78/120    avg_loss:0.079, val_acc:0.985]
Epoch [79/120    avg_loss:0.069, val_acc:0.979]
Epoch [80/120    avg_loss:0.075, val_acc:0.975]
Epoch [81/120    avg_loss:0.024, val_acc:0.990]
Epoch [82/120    avg_loss:0.033, val_acc:0.992]
Epoch [83/120    avg_loss:0.019, val_acc:0.988]
Epoch [84/120    avg_loss:0.015, val_acc:0.996]
Epoch [85/120    avg_loss:0.016, val_acc:0.992]
Epoch [86/120    avg_loss:0.012, val_acc:0.994]
Epoch [87/120    avg_loss:0.013, val_acc:0.994]
Epoch [88/120    avg_loss:0.015, val_acc:0.992]
Epoch [89/120    avg_loss:0.023, val_acc:0.988]
Epoch [90/120    avg_loss:0.019, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.994]
Epoch [93/120    avg_loss:0.016, val_acc:0.994]
Epoch [94/120    avg_loss:0.016, val_acc:0.996]
Epoch [95/120    avg_loss:0.025, val_acc:0.990]
Epoch [96/120    avg_loss:0.018, val_acc:0.994]
Epoch [97/120    avg_loss:0.017, val_acc:0.996]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.992]
Epoch [101/120    avg_loss:0.012, val_acc:0.996]
Epoch [102/120    avg_loss:0.007, val_acc:0.996]
Epoch [103/120    avg_loss:0.006, val_acc:0.996]
Epoch [104/120    avg_loss:0.009, val_acc:0.994]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.994]
Epoch [107/120    avg_loss:0.007, val_acc:0.994]
Epoch [108/120    avg_loss:0.006, val_acc:0.994]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.015, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.998]
Epoch [113/120    avg_loss:0.006, val_acc:0.996]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.023, val_acc:0.992]
Epoch [116/120    avg_loss:0.025, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.994]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 0.99780541 1.         1.         0.98886414 0.98639456
 0.99277108 1.         1.         1.         1.         0.98820446
 0.98886414 1.        ]

Kappa:
0.9959649692731879
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2be3547b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.207, val_acc:0.525]
Epoch [2/120    avg_loss:1.603, val_acc:0.644]
Epoch [3/120    avg_loss:1.194, val_acc:0.719]
Epoch [4/120    avg_loss:0.940, val_acc:0.806]
Epoch [5/120    avg_loss:0.746, val_acc:0.762]
Epoch [6/120    avg_loss:0.663, val_acc:0.833]
Epoch [7/120    avg_loss:0.602, val_acc:0.865]
Epoch [8/120    avg_loss:0.525, val_acc:0.885]
Epoch [9/120    avg_loss:0.430, val_acc:0.923]
Epoch [10/120    avg_loss:0.310, val_acc:0.942]
Epoch [11/120    avg_loss:0.271, val_acc:0.952]
Epoch [12/120    avg_loss:0.261, val_acc:0.935]
Epoch [13/120    avg_loss:0.206, val_acc:0.950]
Epoch [14/120    avg_loss:0.289, val_acc:0.944]
Epoch [15/120    avg_loss:0.200, val_acc:0.969]
Epoch [16/120    avg_loss:0.189, val_acc:0.938]
Epoch [17/120    avg_loss:0.213, val_acc:0.960]
Epoch [18/120    avg_loss:0.268, val_acc:0.935]
Epoch [19/120    avg_loss:0.194, val_acc:0.954]
Epoch [20/120    avg_loss:0.158, val_acc:0.967]
Epoch [21/120    avg_loss:0.114, val_acc:0.985]
Epoch [22/120    avg_loss:0.119, val_acc:0.967]
Epoch [23/120    avg_loss:0.139, val_acc:0.981]
Epoch [24/120    avg_loss:0.113, val_acc:0.983]
Epoch [25/120    avg_loss:0.103, val_acc:0.973]
Epoch [26/120    avg_loss:0.121, val_acc:0.979]
Epoch [27/120    avg_loss:0.093, val_acc:0.977]
Epoch [28/120    avg_loss:0.087, val_acc:0.985]
Epoch [29/120    avg_loss:0.076, val_acc:0.979]
Epoch [30/120    avg_loss:0.091, val_acc:0.971]
Epoch [31/120    avg_loss:0.088, val_acc:0.977]
Epoch [32/120    avg_loss:0.087, val_acc:0.979]
Epoch [33/120    avg_loss:0.072, val_acc:0.983]
Epoch [34/120    avg_loss:0.103, val_acc:0.975]
Epoch [35/120    avg_loss:0.061, val_acc:0.988]
Epoch [36/120    avg_loss:0.080, val_acc:0.975]
Epoch [37/120    avg_loss:0.056, val_acc:0.992]
Epoch [38/120    avg_loss:0.061, val_acc:0.977]
Epoch [39/120    avg_loss:0.046, val_acc:0.985]
Epoch [40/120    avg_loss:0.040, val_acc:0.988]
Epoch [41/120    avg_loss:0.076, val_acc:0.971]
Epoch [42/120    avg_loss:0.135, val_acc:0.958]
Epoch [43/120    avg_loss:0.117, val_acc:0.969]
Epoch [44/120    avg_loss:0.046, val_acc:0.983]
Epoch [45/120    avg_loss:0.046, val_acc:0.988]
Epoch [46/120    avg_loss:0.047, val_acc:0.983]
Epoch [47/120    avg_loss:0.041, val_acc:0.973]
Epoch [48/120    avg_loss:0.061, val_acc:0.990]
Epoch [49/120    avg_loss:0.039, val_acc:0.988]
Epoch [50/120    avg_loss:0.026, val_acc:0.988]
Epoch [51/120    avg_loss:0.027, val_acc:0.988]
Epoch [52/120    avg_loss:0.029, val_acc:0.988]
Epoch [53/120    avg_loss:0.022, val_acc:0.988]
Epoch [54/120    avg_loss:0.024, val_acc:0.990]
Epoch [55/120    avg_loss:0.022, val_acc:0.990]
Epoch [56/120    avg_loss:0.021, val_acc:0.990]
Epoch [57/120    avg_loss:0.015, val_acc:0.990]
Epoch [58/120    avg_loss:0.018, val_acc:0.990]
Epoch [59/120    avg_loss:0.018, val_acc:0.990]
Epoch [60/120    avg_loss:0.026, val_acc:0.990]
Epoch [61/120    avg_loss:0.024, val_acc:0.990]
Epoch [62/120    avg_loss:0.026, val_acc:0.992]
Epoch [63/120    avg_loss:0.019, val_acc:0.992]
Epoch [64/120    avg_loss:0.021, val_acc:0.990]
Epoch [65/120    avg_loss:0.018, val_acc:0.990]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.016, val_acc:0.990]
Epoch [68/120    avg_loss:0.019, val_acc:0.990]
Epoch [69/120    avg_loss:0.021, val_acc:0.990]
Epoch [70/120    avg_loss:0.017, val_acc:0.990]
Epoch [71/120    avg_loss:0.015, val_acc:0.990]
Epoch [72/120    avg_loss:0.020, val_acc:0.990]
Epoch [73/120    avg_loss:0.021, val_acc:0.990]
Epoch [74/120    avg_loss:0.012, val_acc:0.990]
Epoch [75/120    avg_loss:0.025, val_acc:0.990]
Epoch [76/120    avg_loss:0.021, val_acc:0.990]
Epoch [77/120    avg_loss:0.016, val_acc:0.990]
Epoch [78/120    avg_loss:0.017, val_acc:0.990]
Epoch [79/120    avg_loss:0.019, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.020, val_acc:0.990]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.030, val_acc:0.990]
Epoch [85/120    avg_loss:0.016, val_acc:0.990]
Epoch [86/120    avg_loss:0.017, val_acc:0.990]
Epoch [87/120    avg_loss:0.022, val_acc:0.990]
Epoch [88/120    avg_loss:0.031, val_acc:0.990]
Epoch [89/120    avg_loss:0.018, val_acc:0.990]
Epoch [90/120    avg_loss:0.020, val_acc:0.990]
Epoch [91/120    avg_loss:0.019, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.990]
Epoch [93/120    avg_loss:0.022, val_acc:0.990]
Epoch [94/120    avg_loss:0.021, val_acc:0.990]
Epoch [95/120    avg_loss:0.023, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.018, val_acc:0.990]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.011, val_acc:0.990]
Epoch [100/120    avg_loss:0.029, val_acc:0.990]
Epoch [101/120    avg_loss:0.020, val_acc:0.990]
Epoch [102/120    avg_loss:0.013, val_acc:0.990]
Epoch [103/120    avg_loss:0.021, val_acc:0.990]
Epoch [104/120    avg_loss:0.022, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.990]
Epoch [107/120    avg_loss:0.015, val_acc:0.990]
Epoch [108/120    avg_loss:0.020, val_acc:0.990]
Epoch [109/120    avg_loss:0.023, val_acc:0.990]
Epoch [110/120    avg_loss:0.021, val_acc:0.990]
Epoch [111/120    avg_loss:0.015, val_acc:0.990]
Epoch [112/120    avg_loss:0.019, val_acc:0.990]
Epoch [113/120    avg_loss:0.018, val_acc:0.990]
Epoch [114/120    avg_loss:0.016, val_acc:0.990]
Epoch [115/120    avg_loss:0.021, val_acc:0.990]
Epoch [116/120    avg_loss:0.017, val_acc:0.990]
Epoch [117/120    avg_loss:0.017, val_acc:0.990]
Epoch [118/120    avg_loss:0.018, val_acc:0.990]
Epoch [119/120    avg_loss:0.014, val_acc:0.990]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.7228144989339

F1 scores:
[       nan 0.99926954 1.         1.         0.97977528 0.96989967
 1.         1.         1.         1.         1.         0.99603699
 0.99557522 1.        ]

Kappa:
0.9969142281966966
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8429218780>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.235, val_acc:0.540]
Epoch [2/120    avg_loss:1.585, val_acc:0.667]
Epoch [3/120    avg_loss:1.138, val_acc:0.742]
Epoch [4/120    avg_loss:0.855, val_acc:0.740]
Epoch [5/120    avg_loss:0.676, val_acc:0.808]
Epoch [6/120    avg_loss:0.567, val_acc:0.877]
Epoch [7/120    avg_loss:0.518, val_acc:0.877]
Epoch [8/120    avg_loss:0.436, val_acc:0.887]
Epoch [9/120    avg_loss:0.372, val_acc:0.900]
Epoch [10/120    avg_loss:0.345, val_acc:0.906]
Epoch [11/120    avg_loss:0.286, val_acc:0.938]
Epoch [12/120    avg_loss:0.351, val_acc:0.927]
Epoch [13/120    avg_loss:0.406, val_acc:0.927]
Epoch [14/120    avg_loss:0.322, val_acc:0.948]
Epoch [15/120    avg_loss:0.280, val_acc:0.912]
Epoch [16/120    avg_loss:0.252, val_acc:0.946]
Epoch [17/120    avg_loss:0.194, val_acc:0.933]
Epoch [18/120    avg_loss:0.184, val_acc:0.963]
Epoch [19/120    avg_loss:0.144, val_acc:0.958]
Epoch [20/120    avg_loss:0.183, val_acc:0.956]
Epoch [21/120    avg_loss:0.194, val_acc:0.958]
Epoch [22/120    avg_loss:0.147, val_acc:0.963]
Epoch [23/120    avg_loss:0.116, val_acc:0.960]
Epoch [24/120    avg_loss:0.131, val_acc:0.963]
Epoch [25/120    avg_loss:0.095, val_acc:0.942]
Epoch [26/120    avg_loss:0.105, val_acc:0.954]
Epoch [27/120    avg_loss:0.182, val_acc:0.971]
Epoch [28/120    avg_loss:0.142, val_acc:0.954]
Epoch [29/120    avg_loss:0.149, val_acc:0.965]
Epoch [30/120    avg_loss:0.108, val_acc:0.958]
Epoch [31/120    avg_loss:0.102, val_acc:0.973]
Epoch [32/120    avg_loss:0.099, val_acc:0.963]
Epoch [33/120    avg_loss:0.091, val_acc:0.973]
Epoch [34/120    avg_loss:0.055, val_acc:0.969]
Epoch [35/120    avg_loss:0.079, val_acc:0.963]
Epoch [36/120    avg_loss:0.074, val_acc:0.969]
Epoch [37/120    avg_loss:0.070, val_acc:0.960]
Epoch [38/120    avg_loss:0.071, val_acc:0.975]
Epoch [39/120    avg_loss:0.063, val_acc:0.975]
Epoch [40/120    avg_loss:0.085, val_acc:0.975]
Epoch [41/120    avg_loss:0.045, val_acc:0.979]
Epoch [42/120    avg_loss:0.037, val_acc:0.988]
Epoch [43/120    avg_loss:0.065, val_acc:0.979]
Epoch [44/120    avg_loss:0.044, val_acc:0.979]
Epoch [45/120    avg_loss:0.045, val_acc:0.977]
Epoch [46/120    avg_loss:0.046, val_acc:0.977]
Epoch [47/120    avg_loss:0.041, val_acc:0.981]
Epoch [48/120    avg_loss:0.070, val_acc:0.975]
Epoch [49/120    avg_loss:0.057, val_acc:0.973]
Epoch [50/120    avg_loss:0.079, val_acc:0.967]
Epoch [51/120    avg_loss:0.102, val_acc:0.969]
Epoch [52/120    avg_loss:0.116, val_acc:0.967]
Epoch [53/120    avg_loss:0.049, val_acc:0.981]
Epoch [54/120    avg_loss:0.041, val_acc:0.983]
Epoch [55/120    avg_loss:0.045, val_acc:0.988]
Epoch [56/120    avg_loss:0.050, val_acc:0.985]
Epoch [57/120    avg_loss:0.026, val_acc:0.994]
Epoch [58/120    avg_loss:0.025, val_acc:0.983]
Epoch [59/120    avg_loss:0.021, val_acc:0.979]
Epoch [60/120    avg_loss:0.022, val_acc:0.990]
Epoch [61/120    avg_loss:0.017, val_acc:0.992]
Epoch [62/120    avg_loss:0.027, val_acc:0.981]
Epoch [63/120    avg_loss:0.044, val_acc:0.971]
Epoch [64/120    avg_loss:0.022, val_acc:0.979]
Epoch [65/120    avg_loss:0.029, val_acc:0.990]
Epoch [66/120    avg_loss:0.030, val_acc:0.983]
Epoch [67/120    avg_loss:0.027, val_acc:0.988]
Epoch [68/120    avg_loss:0.065, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.983]
Epoch [70/120    avg_loss:0.026, val_acc:0.992]
Epoch [71/120    avg_loss:0.021, val_acc:0.994]
Epoch [72/120    avg_loss:0.020, val_acc:0.994]
Epoch [73/120    avg_loss:0.020, val_acc:0.994]
Epoch [74/120    avg_loss:0.012, val_acc:0.994]
Epoch [75/120    avg_loss:0.017, val_acc:0.994]
Epoch [76/120    avg_loss:0.010, val_acc:0.994]
Epoch [77/120    avg_loss:0.012, val_acc:0.994]
Epoch [78/120    avg_loss:0.013, val_acc:0.994]
Epoch [79/120    avg_loss:0.009, val_acc:0.992]
Epoch [80/120    avg_loss:0.014, val_acc:0.992]
Epoch [81/120    avg_loss:0.010, val_acc:0.992]
Epoch [82/120    avg_loss:0.011, val_acc:0.994]
Epoch [83/120    avg_loss:0.020, val_acc:0.994]
Epoch [84/120    avg_loss:0.010, val_acc:0.994]
Epoch [85/120    avg_loss:0.015, val_acc:0.992]
Epoch [86/120    avg_loss:0.035, val_acc:0.992]
Epoch [87/120    avg_loss:0.012, val_acc:0.992]
Epoch [88/120    avg_loss:0.018, val_acc:0.992]
Epoch [89/120    avg_loss:0.014, val_acc:0.992]
Epoch [90/120    avg_loss:0.016, val_acc:0.992]
Epoch [91/120    avg_loss:0.009, val_acc:0.992]
Epoch [92/120    avg_loss:0.011, val_acc:0.992]
Epoch [93/120    avg_loss:0.010, val_acc:0.992]
Epoch [94/120    avg_loss:0.010, val_acc:0.992]
Epoch [95/120    avg_loss:0.014, val_acc:0.992]
Epoch [96/120    avg_loss:0.013, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.992]
Epoch [98/120    avg_loss:0.015, val_acc:0.992]
Epoch [99/120    avg_loss:0.016, val_acc:0.992]
Epoch [100/120    avg_loss:0.009, val_acc:0.992]
Epoch [101/120    avg_loss:0.011, val_acc:0.992]
Epoch [102/120    avg_loss:0.014, val_acc:0.992]
Epoch [103/120    avg_loss:0.008, val_acc:0.992]
Epoch [104/120    avg_loss:0.010, val_acc:0.992]
Epoch [105/120    avg_loss:0.021, val_acc:0.992]
Epoch [106/120    avg_loss:0.013, val_acc:0.992]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.010, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.992]
Epoch [110/120    avg_loss:0.012, val_acc:0.992]
Epoch [111/120    avg_loss:0.011, val_acc:0.992]
Epoch [112/120    avg_loss:0.011, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.014, val_acc:0.992]
Epoch [115/120    avg_loss:0.013, val_acc:0.992]
Epoch [116/120    avg_loss:0.012, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.013, val_acc:0.992]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.016, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   5   0   0   0   0   0   0   2   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.82942430703625

F1 scores:
[       nan 1.         0.9977221  1.         0.98434004 0.98305085
 1.         1.         1.         1.         1.         1.
 0.99669239 1.        ]

Kappa:
0.9981009668164197
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fceadcff748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.307, val_acc:0.560]
Epoch [2/120    avg_loss:1.608, val_acc:0.640]
Epoch [3/120    avg_loss:1.219, val_acc:0.683]
Epoch [4/120    avg_loss:0.973, val_acc:0.740]
Epoch [5/120    avg_loss:0.790, val_acc:0.833]
Epoch [6/120    avg_loss:0.683, val_acc:0.817]
Epoch [7/120    avg_loss:0.602, val_acc:0.842]
Epoch [8/120    avg_loss:0.519, val_acc:0.890]
Epoch [9/120    avg_loss:0.425, val_acc:0.919]
Epoch [10/120    avg_loss:0.340, val_acc:0.917]
Epoch [11/120    avg_loss:0.300, val_acc:0.917]
Epoch [12/120    avg_loss:0.414, val_acc:0.906]
Epoch [13/120    avg_loss:0.295, val_acc:0.900]
Epoch [14/120    avg_loss:0.287, val_acc:0.944]
Epoch [15/120    avg_loss:0.249, val_acc:0.929]
Epoch [16/120    avg_loss:0.281, val_acc:0.917]
Epoch [17/120    avg_loss:0.371, val_acc:0.915]
Epoch [18/120    avg_loss:0.404, val_acc:0.956]
Epoch [19/120    avg_loss:0.215, val_acc:0.938]
Epoch [20/120    avg_loss:0.215, val_acc:0.952]
Epoch [21/120    avg_loss:0.202, val_acc:0.954]
Epoch [22/120    avg_loss:0.201, val_acc:0.944]
Epoch [23/120    avg_loss:0.137, val_acc:0.979]
Epoch [24/120    avg_loss:0.142, val_acc:0.977]
Epoch [25/120    avg_loss:0.145, val_acc:0.981]
Epoch [26/120    avg_loss:0.162, val_acc:0.967]
Epoch [27/120    avg_loss:0.149, val_acc:0.985]
Epoch [28/120    avg_loss:0.145, val_acc:0.965]
Epoch [29/120    avg_loss:0.104, val_acc:0.979]
Epoch [30/120    avg_loss:0.097, val_acc:0.977]
Epoch [31/120    avg_loss:0.075, val_acc:0.979]
Epoch [32/120    avg_loss:0.049, val_acc:0.985]
Epoch [33/120    avg_loss:0.186, val_acc:0.921]
Epoch [34/120    avg_loss:0.225, val_acc:0.971]
Epoch [35/120    avg_loss:0.185, val_acc:0.952]
Epoch [36/120    avg_loss:0.126, val_acc:0.981]
Epoch [37/120    avg_loss:0.109, val_acc:0.985]
Epoch [38/120    avg_loss:0.060, val_acc:0.981]
Epoch [39/120    avg_loss:0.073, val_acc:0.983]
Epoch [40/120    avg_loss:0.042, val_acc:0.985]
Epoch [41/120    avg_loss:0.039, val_acc:0.990]
Epoch [42/120    avg_loss:0.056, val_acc:0.992]
Epoch [43/120    avg_loss:0.075, val_acc:0.979]
Epoch [44/120    avg_loss:0.088, val_acc:0.973]
Epoch [45/120    avg_loss:0.115, val_acc:0.975]
Epoch [46/120    avg_loss:0.076, val_acc:0.988]
Epoch [47/120    avg_loss:0.036, val_acc:0.985]
Epoch [48/120    avg_loss:0.029, val_acc:0.990]
Epoch [49/120    avg_loss:0.059, val_acc:0.990]
Epoch [50/120    avg_loss:0.031, val_acc:0.983]
Epoch [51/120    avg_loss:0.023, val_acc:0.992]
Epoch [52/120    avg_loss:0.023, val_acc:0.992]
Epoch [53/120    avg_loss:0.033, val_acc:0.985]
Epoch [54/120    avg_loss:0.040, val_acc:0.985]
Epoch [55/120    avg_loss:0.028, val_acc:0.994]
Epoch [56/120    avg_loss:0.026, val_acc:0.992]
Epoch [57/120    avg_loss:0.032, val_acc:0.988]
Epoch [58/120    avg_loss:0.037, val_acc:0.988]
Epoch [59/120    avg_loss:0.038, val_acc:0.994]
Epoch [60/120    avg_loss:0.057, val_acc:0.992]
Epoch [61/120    avg_loss:0.036, val_acc:0.985]
Epoch [62/120    avg_loss:0.059, val_acc:0.981]
Epoch [63/120    avg_loss:0.047, val_acc:0.990]
Epoch [64/120    avg_loss:0.033, val_acc:0.983]
Epoch [65/120    avg_loss:0.052, val_acc:0.975]
Epoch [66/120    avg_loss:0.042, val_acc:0.985]
Epoch [67/120    avg_loss:0.049, val_acc:0.994]
Epoch [68/120    avg_loss:0.047, val_acc:0.988]
Epoch [69/120    avg_loss:0.020, val_acc:0.990]
Epoch [70/120    avg_loss:0.039, val_acc:0.981]
Epoch [71/120    avg_loss:0.029, val_acc:0.983]
Epoch [72/120    avg_loss:0.022, val_acc:0.992]
Epoch [73/120    avg_loss:0.020, val_acc:0.990]
Epoch [74/120    avg_loss:0.012, val_acc:0.992]
Epoch [75/120    avg_loss:0.020, val_acc:0.990]
Epoch [76/120    avg_loss:0.017, val_acc:0.988]
Epoch [77/120    avg_loss:0.026, val_acc:0.992]
Epoch [78/120    avg_loss:0.033, val_acc:0.990]
Epoch [79/120    avg_loss:0.023, val_acc:0.990]
Epoch [80/120    avg_loss:0.015, val_acc:0.990]
Epoch [81/120    avg_loss:0.017, val_acc:0.990]
Epoch [82/120    avg_loss:0.024, val_acc:0.992]
Epoch [83/120    avg_loss:0.012, val_acc:0.992]
Epoch [84/120    avg_loss:0.014, val_acc:0.992]
Epoch [85/120    avg_loss:0.024, val_acc:0.992]
Epoch [86/120    avg_loss:0.014, val_acc:0.994]
Epoch [87/120    avg_loss:0.013, val_acc:0.994]
Epoch [88/120    avg_loss:0.016, val_acc:0.994]
Epoch [89/120    avg_loss:0.009, val_acc:0.994]
Epoch [90/120    avg_loss:0.015, val_acc:0.992]
Epoch [91/120    avg_loss:0.018, val_acc:0.992]
Epoch [92/120    avg_loss:0.009, val_acc:0.992]
Epoch [93/120    avg_loss:0.008, val_acc:0.992]
Epoch [94/120    avg_loss:0.014, val_acc:0.992]
Epoch [95/120    avg_loss:0.013, val_acc:0.992]
Epoch [96/120    avg_loss:0.014, val_acc:0.992]
Epoch [97/120    avg_loss:0.007, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.012, val_acc:0.992]
Epoch [102/120    avg_loss:0.010, val_acc:0.992]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.012, val_acc:0.992]
Epoch [105/120    avg_loss:0.009, val_acc:0.992]
Epoch [106/120    avg_loss:0.008, val_acc:0.992]
Epoch [107/120    avg_loss:0.007, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.009, val_acc:0.992]
Epoch [111/120    avg_loss:0.007, val_acc:0.992]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.016, val_acc:0.992]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.992]
Epoch [117/120    avg_loss:0.009, val_acc:0.992]
Epoch [118/120    avg_loss:0.009, val_acc:0.992]
Epoch [119/120    avg_loss:0.016, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.85074626865672

F1 scores:
[       nan 1.         0.9977221  1.         0.98660714 0.97972973
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9983383606822227
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff7a932b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.213, val_acc:0.583]
Epoch [2/120    avg_loss:1.581, val_acc:0.681]
Epoch [3/120    avg_loss:1.145, val_acc:0.762]
Epoch [4/120    avg_loss:0.899, val_acc:0.790]
Epoch [5/120    avg_loss:0.789, val_acc:0.777]
Epoch [6/120    avg_loss:0.668, val_acc:0.844]
Epoch [7/120    avg_loss:0.572, val_acc:0.881]
Epoch [8/120    avg_loss:0.449, val_acc:0.917]
Epoch [9/120    avg_loss:0.423, val_acc:0.908]
Epoch [10/120    avg_loss:0.374, val_acc:0.915]
Epoch [11/120    avg_loss:0.420, val_acc:0.927]
Epoch [12/120    avg_loss:0.335, val_acc:0.948]
Epoch [13/120    avg_loss:0.265, val_acc:0.956]
Epoch [14/120    avg_loss:0.267, val_acc:0.917]
Epoch [15/120    avg_loss:0.290, val_acc:0.935]
Epoch [16/120    avg_loss:0.241, val_acc:0.952]
Epoch [17/120    avg_loss:0.218, val_acc:0.960]
Epoch [18/120    avg_loss:0.171, val_acc:0.954]
Epoch [19/120    avg_loss:0.180, val_acc:0.960]
Epoch [20/120    avg_loss:0.175, val_acc:0.971]
Epoch [21/120    avg_loss:0.132, val_acc:0.971]
Epoch [22/120    avg_loss:0.109, val_acc:0.979]
Epoch [23/120    avg_loss:0.116, val_acc:0.975]
Epoch [24/120    avg_loss:0.191, val_acc:0.923]
Epoch [25/120    avg_loss:0.147, val_acc:0.956]
Epoch [26/120    avg_loss:0.131, val_acc:0.977]
Epoch [27/120    avg_loss:0.138, val_acc:0.958]
Epoch [28/120    avg_loss:0.126, val_acc:0.960]
Epoch [29/120    avg_loss:0.169, val_acc:0.965]
Epoch [30/120    avg_loss:0.104, val_acc:0.971]
Epoch [31/120    avg_loss:0.080, val_acc:0.983]
Epoch [32/120    avg_loss:0.088, val_acc:0.979]
Epoch [33/120    avg_loss:0.062, val_acc:0.983]
Epoch [34/120    avg_loss:0.041, val_acc:0.985]
Epoch [35/120    avg_loss:0.045, val_acc:0.988]
Epoch [36/120    avg_loss:0.054, val_acc:0.992]
Epoch [37/120    avg_loss:0.062, val_acc:0.990]
Epoch [38/120    avg_loss:0.072, val_acc:0.983]
Epoch [39/120    avg_loss:0.086, val_acc:0.981]
Epoch [40/120    avg_loss:0.054, val_acc:0.988]
Epoch [41/120    avg_loss:0.055, val_acc:0.990]
Epoch [42/120    avg_loss:0.080, val_acc:0.990]
Epoch [43/120    avg_loss:0.061, val_acc:0.981]
Epoch [44/120    avg_loss:0.048, val_acc:0.988]
Epoch [45/120    avg_loss:0.043, val_acc:0.992]
Epoch [46/120    avg_loss:0.028, val_acc:0.992]
Epoch [47/120    avg_loss:0.022, val_acc:0.990]
Epoch [48/120    avg_loss:0.039, val_acc:0.988]
Epoch [49/120    avg_loss:0.049, val_acc:0.990]
Epoch [50/120    avg_loss:0.049, val_acc:0.988]
Epoch [51/120    avg_loss:0.070, val_acc:0.981]
Epoch [52/120    avg_loss:0.047, val_acc:0.988]
Epoch [53/120    avg_loss:0.036, val_acc:0.985]
Epoch [54/120    avg_loss:0.059, val_acc:0.994]
Epoch [55/120    avg_loss:0.079, val_acc:0.985]
Epoch [56/120    avg_loss:0.058, val_acc:0.988]
Epoch [57/120    avg_loss:0.056, val_acc:0.985]
Epoch [58/120    avg_loss:0.054, val_acc:0.979]
Epoch [59/120    avg_loss:0.071, val_acc:0.985]
Epoch [60/120    avg_loss:0.046, val_acc:0.994]
Epoch [61/120    avg_loss:0.058, val_acc:0.992]
Epoch [62/120    avg_loss:0.034, val_acc:0.994]
Epoch [63/120    avg_loss:0.041, val_acc:0.992]
Epoch [64/120    avg_loss:0.035, val_acc:0.992]
Epoch [65/120    avg_loss:0.059, val_acc:0.992]
Epoch [66/120    avg_loss:0.042, val_acc:0.992]
Epoch [67/120    avg_loss:0.038, val_acc:0.992]
Epoch [68/120    avg_loss:0.023, val_acc:0.998]
Epoch [69/120    avg_loss:0.016, val_acc:0.996]
Epoch [70/120    avg_loss:0.015, val_acc:0.992]
Epoch [71/120    avg_loss:0.013, val_acc:0.992]
Epoch [72/120    avg_loss:0.020, val_acc:0.998]
Epoch [73/120    avg_loss:0.022, val_acc:0.998]
Epoch [74/120    avg_loss:0.014, val_acc:0.998]
Epoch [75/120    avg_loss:0.011, val_acc:0.996]
Epoch [76/120    avg_loss:0.010, val_acc:0.996]
Epoch [77/120    avg_loss:0.011, val_acc:1.000]
Epoch [78/120    avg_loss:0.010, val_acc:0.998]
Epoch [79/120    avg_loss:0.040, val_acc:0.994]
Epoch [80/120    avg_loss:0.009, val_acc:0.994]
Epoch [81/120    avg_loss:0.030, val_acc:0.998]
Epoch [82/120    avg_loss:0.025, val_acc:0.985]
Epoch [83/120    avg_loss:0.026, val_acc:0.990]
Epoch [84/120    avg_loss:0.018, val_acc:1.000]
Epoch [85/120    avg_loss:0.015, val_acc:0.994]
Epoch [86/120    avg_loss:0.015, val_acc:0.994]
Epoch [87/120    avg_loss:0.010, val_acc:0.998]
Epoch [88/120    avg_loss:0.012, val_acc:0.998]
Epoch [89/120    avg_loss:0.006, val_acc:0.998]
Epoch [90/120    avg_loss:0.006, val_acc:0.998]
Epoch [91/120    avg_loss:0.007, val_acc:1.000]
Epoch [92/120    avg_loss:0.008, val_acc:1.000]
Epoch [93/120    avg_loss:0.006, val_acc:0.996]
Epoch [94/120    avg_loss:0.018, val_acc:0.988]
Epoch [95/120    avg_loss:0.019, val_acc:0.994]
Epoch [96/120    avg_loss:0.010, val_acc:0.994]
Epoch [97/120    avg_loss:0.011, val_acc:1.000]
Epoch [98/120    avg_loss:0.010, val_acc:0.998]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.008, val_acc:0.994]
Epoch [101/120    avg_loss:0.005, val_acc:0.998]
Epoch [102/120    avg_loss:0.007, val_acc:1.000]
Epoch [103/120    avg_loss:0.013, val_acc:0.996]
Epoch [104/120    avg_loss:0.017, val_acc:0.996]
Epoch [105/120    avg_loss:0.204, val_acc:0.952]
Epoch [106/120    avg_loss:0.104, val_acc:0.973]
Epoch [107/120    avg_loss:0.074, val_acc:0.985]
Epoch [108/120    avg_loss:0.102, val_acc:0.975]
Epoch [109/120    avg_loss:0.068, val_acc:0.983]
Epoch [110/120    avg_loss:0.036, val_acc:0.996]
Epoch [111/120    avg_loss:0.044, val_acc:0.994]
Epoch [112/120    avg_loss:0.035, val_acc:0.983]
Epoch [113/120    avg_loss:0.046, val_acc:0.988]
Epoch [114/120    avg_loss:0.037, val_acc:0.992]
Epoch [115/120    avg_loss:0.030, val_acc:0.983]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.017, val_acc:0.990]
Epoch [118/120    avg_loss:0.019, val_acc:0.996]
Epoch [119/120    avg_loss:0.018, val_acc:0.996]
Epoch [120/120    avg_loss:0.007, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   6   0   0   0   0   0   0   5   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   7 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 1.         0.9977221  1.         0.97078652 0.97278912
 1.         1.         1.         1.         1.         0.98947368
 0.98451327 1.        ]

Kappa:
0.9947775783897227
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5445b7710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.531]
Epoch [2/120    avg_loss:1.632, val_acc:0.665]
Epoch [3/120    avg_loss:1.233, val_acc:0.713]
Epoch [4/120    avg_loss:0.928, val_acc:0.756]
Epoch [5/120    avg_loss:0.709, val_acc:0.810]
Epoch [6/120    avg_loss:0.665, val_acc:0.842]
Epoch [7/120    avg_loss:0.516, val_acc:0.894]
Epoch [8/120    avg_loss:0.416, val_acc:0.929]
Epoch [9/120    avg_loss:0.373, val_acc:0.933]
Epoch [10/120    avg_loss:0.305, val_acc:0.938]
Epoch [11/120    avg_loss:0.299, val_acc:0.929]
Epoch [12/120    avg_loss:0.285, val_acc:0.948]
Epoch [13/120    avg_loss:0.302, val_acc:0.956]
Epoch [14/120    avg_loss:0.272, val_acc:0.923]
Epoch [15/120    avg_loss:0.304, val_acc:0.944]
Epoch [16/120    avg_loss:0.210, val_acc:0.954]
Epoch [17/120    avg_loss:0.263, val_acc:0.967]
Epoch [18/120    avg_loss:0.218, val_acc:0.958]
Epoch [19/120    avg_loss:0.170, val_acc:0.973]
Epoch [20/120    avg_loss:0.161, val_acc:0.979]
Epoch [21/120    avg_loss:0.127, val_acc:0.973]
Epoch [22/120    avg_loss:0.112, val_acc:0.981]
Epoch [23/120    avg_loss:0.120, val_acc:0.983]
Epoch [24/120    avg_loss:0.173, val_acc:0.946]
Epoch [25/120    avg_loss:0.143, val_acc:0.983]
Epoch [26/120    avg_loss:0.118, val_acc:0.973]
Epoch [27/120    avg_loss:0.132, val_acc:0.990]
Epoch [28/120    avg_loss:0.117, val_acc:0.990]
Epoch [29/120    avg_loss:0.086, val_acc:0.990]
Epoch [30/120    avg_loss:0.108, val_acc:0.975]
Epoch [31/120    avg_loss:0.094, val_acc:0.990]
Epoch [32/120    avg_loss:0.081, val_acc:0.990]
Epoch [33/120    avg_loss:0.056, val_acc:0.990]
Epoch [34/120    avg_loss:0.117, val_acc:0.981]
Epoch [35/120    avg_loss:0.087, val_acc:0.985]
Epoch [36/120    avg_loss:0.070, val_acc:1.000]
Epoch [37/120    avg_loss:0.038, val_acc:0.985]
Epoch [38/120    avg_loss:0.057, val_acc:0.994]
Epoch [39/120    avg_loss:0.053, val_acc:0.990]
Epoch [40/120    avg_loss:0.080, val_acc:1.000]
Epoch [41/120    avg_loss:0.052, val_acc:0.990]
Epoch [42/120    avg_loss:0.050, val_acc:0.996]
Epoch [43/120    avg_loss:0.038, val_acc:0.992]
Epoch [44/120    avg_loss:0.039, val_acc:0.996]
Epoch [45/120    avg_loss:0.036, val_acc:0.996]
Epoch [46/120    avg_loss:0.048, val_acc:0.996]
Epoch [47/120    avg_loss:0.054, val_acc:0.994]
Epoch [48/120    avg_loss:0.025, val_acc:0.998]
Epoch [49/120    avg_loss:0.042, val_acc:0.998]
Epoch [50/120    avg_loss:0.069, val_acc:0.988]
Epoch [51/120    avg_loss:0.037, val_acc:0.994]
Epoch [52/120    avg_loss:0.043, val_acc:0.994]
Epoch [53/120    avg_loss:0.029, val_acc:0.990]
Epoch [54/120    avg_loss:0.033, val_acc:0.990]
Epoch [55/120    avg_loss:0.049, val_acc:0.994]
Epoch [56/120    avg_loss:0.029, val_acc:0.994]
Epoch [57/120    avg_loss:0.021, val_acc:0.994]
Epoch [58/120    avg_loss:0.025, val_acc:0.994]
Epoch [59/120    avg_loss:0.017, val_acc:1.000]
Epoch [60/120    avg_loss:0.016, val_acc:1.000]
Epoch [61/120    avg_loss:0.020, val_acc:1.000]
Epoch [62/120    avg_loss:0.023, val_acc:1.000]
Epoch [63/120    avg_loss:0.014, val_acc:1.000]
Epoch [64/120    avg_loss:0.026, val_acc:1.000]
Epoch [65/120    avg_loss:0.027, val_acc:1.000]
Epoch [66/120    avg_loss:0.013, val_acc:1.000]
Epoch [67/120    avg_loss:0.018, val_acc:1.000]
Epoch [68/120    avg_loss:0.028, val_acc:1.000]
Epoch [69/120    avg_loss:0.016, val_acc:1.000]
Epoch [70/120    avg_loss:0.018, val_acc:1.000]
Epoch [71/120    avg_loss:0.018, val_acc:1.000]
Epoch [72/120    avg_loss:0.013, val_acc:1.000]
Epoch [73/120    avg_loss:0.014, val_acc:1.000]
Epoch [74/120    avg_loss:0.012, val_acc:1.000]
Epoch [75/120    avg_loss:0.020, val_acc:1.000]
Epoch [76/120    avg_loss:0.017, val_acc:1.000]
Epoch [77/120    avg_loss:0.018, val_acc:1.000]
Epoch [78/120    avg_loss:0.013, val_acc:1.000]
Epoch [79/120    avg_loss:0.016, val_acc:1.000]
Epoch [80/120    avg_loss:0.014, val_acc:1.000]
Epoch [81/120    avg_loss:0.032, val_acc:1.000]
Epoch [82/120    avg_loss:0.012, val_acc:1.000]
Epoch [83/120    avg_loss:0.014, val_acc:1.000]
Epoch [84/120    avg_loss:0.013, val_acc:1.000]
Epoch [85/120    avg_loss:0.014, val_acc:1.000]
Epoch [86/120    avg_loss:0.020, val_acc:1.000]
Epoch [87/120    avg_loss:0.022, val_acc:1.000]
Epoch [88/120    avg_loss:0.015, val_acc:1.000]
Epoch [89/120    avg_loss:0.011, val_acc:1.000]
Epoch [90/120    avg_loss:0.011, val_acc:1.000]
Epoch [91/120    avg_loss:0.011, val_acc:1.000]
Epoch [92/120    avg_loss:0.011, val_acc:1.000]
Epoch [93/120    avg_loss:0.020, val_acc:1.000]
Epoch [94/120    avg_loss:0.008, val_acc:1.000]
Epoch [95/120    avg_loss:0.015, val_acc:1.000]
Epoch [96/120    avg_loss:0.010, val_acc:1.000]
Epoch [97/120    avg_loss:0.014, val_acc:1.000]
Epoch [98/120    avg_loss:0.012, val_acc:1.000]
Epoch [99/120    avg_loss:0.010, val_acc:1.000]
Epoch [100/120    avg_loss:0.013, val_acc:1.000]
Epoch [101/120    avg_loss:0.017, val_acc:1.000]
Epoch [102/120    avg_loss:0.017, val_acc:1.000]
Epoch [103/120    avg_loss:0.029, val_acc:1.000]
Epoch [104/120    avg_loss:0.010, val_acc:1.000]
Epoch [105/120    avg_loss:0.017, val_acc:1.000]
Epoch [106/120    avg_loss:0.012, val_acc:1.000]
Epoch [107/120    avg_loss:0.016, val_acc:1.000]
Epoch [108/120    avg_loss:0.015, val_acc:1.000]
Epoch [109/120    avg_loss:0.021, val_acc:1.000]
Epoch [110/120    avg_loss:0.016, val_acc:1.000]
Epoch [111/120    avg_loss:0.011, val_acc:1.000]
Epoch [112/120    avg_loss:0.011, val_acc:1.000]
Epoch [113/120    avg_loss:0.017, val_acc:1.000]
Epoch [114/120    avg_loss:0.013, val_acc:1.000]
Epoch [115/120    avg_loss:0.012, val_acc:1.000]
Epoch [116/120    avg_loss:0.012, val_acc:1.000]
Epoch [117/120    avg_loss:0.012, val_acc:1.000]
Epoch [118/120    avg_loss:0.016, val_acc:1.000]
Epoch [119/120    avg_loss:0.011, val_acc:1.000]
Epoch [120/120    avg_loss:0.012, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   5 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.68017057569296

F1 scores:
[       nan 1.         0.9977221  1.         0.98206278 0.98639456
 1.         1.         1.         1.         1.         0.99208443
 0.9878453  1.        ]

Kappa:
0.9964392713296505
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f69ef24b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.280, val_acc:0.631]
Epoch [2/120    avg_loss:1.601, val_acc:0.623]
Epoch [3/120    avg_loss:1.198, val_acc:0.721]
Epoch [4/120    avg_loss:0.903, val_acc:0.762]
Epoch [5/120    avg_loss:0.767, val_acc:0.760]
Epoch [6/120    avg_loss:0.692, val_acc:0.800]
Epoch [7/120    avg_loss:0.558, val_acc:0.858]
Epoch [8/120    avg_loss:0.489, val_acc:0.842]
Epoch [9/120    avg_loss:0.415, val_acc:0.933]
Epoch [10/120    avg_loss:0.370, val_acc:0.915]
Epoch [11/120    avg_loss:0.373, val_acc:0.902]
Epoch [12/120    avg_loss:0.373, val_acc:0.906]
Epoch [13/120    avg_loss:0.372, val_acc:0.963]
Epoch [14/120    avg_loss:0.241, val_acc:0.942]
Epoch [15/120    avg_loss:0.214, val_acc:0.944]
Epoch [16/120    avg_loss:0.339, val_acc:0.910]
Epoch [17/120    avg_loss:0.397, val_acc:0.917]
Epoch [18/120    avg_loss:0.248, val_acc:0.950]
Epoch [19/120    avg_loss:0.228, val_acc:0.963]
Epoch [20/120    avg_loss:0.162, val_acc:0.958]
Epoch [21/120    avg_loss:0.191, val_acc:0.971]
Epoch [22/120    avg_loss:0.145, val_acc:0.960]
Epoch [23/120    avg_loss:0.138, val_acc:0.973]
Epoch [24/120    avg_loss:0.142, val_acc:0.977]
Epoch [25/120    avg_loss:0.112, val_acc:0.977]
Epoch [26/120    avg_loss:0.086, val_acc:0.985]
Epoch [27/120    avg_loss:0.074, val_acc:0.971]
Epoch [28/120    avg_loss:0.082, val_acc:0.971]
Epoch [29/120    avg_loss:0.066, val_acc:0.975]
Epoch [30/120    avg_loss:0.084, val_acc:0.988]
Epoch [31/120    avg_loss:0.081, val_acc:0.975]
Epoch [32/120    avg_loss:0.085, val_acc:0.985]
Epoch [33/120    avg_loss:0.085, val_acc:0.969]
Epoch [34/120    avg_loss:0.156, val_acc:0.973]
Epoch [35/120    avg_loss:0.120, val_acc:0.981]
Epoch [36/120    avg_loss:0.078, val_acc:0.967]
Epoch [37/120    avg_loss:0.091, val_acc:0.983]
Epoch [38/120    avg_loss:0.084, val_acc:0.979]
Epoch [39/120    avg_loss:0.063, val_acc:0.983]
Epoch [40/120    avg_loss:0.046, val_acc:0.983]
Epoch [41/120    avg_loss:0.051, val_acc:0.981]
Epoch [42/120    avg_loss:0.038, val_acc:0.988]
Epoch [43/120    avg_loss:0.059, val_acc:0.994]
Epoch [44/120    avg_loss:0.040, val_acc:0.988]
Epoch [45/120    avg_loss:0.040, val_acc:0.992]
Epoch [46/120    avg_loss:0.028, val_acc:0.996]
Epoch [47/120    avg_loss:0.034, val_acc:0.994]
Epoch [48/120    avg_loss:0.028, val_acc:0.992]
Epoch [49/120    avg_loss:0.040, val_acc:0.990]
Epoch [50/120    avg_loss:0.040, val_acc:0.998]
Epoch [51/120    avg_loss:0.060, val_acc:0.979]
Epoch [52/120    avg_loss:0.069, val_acc:0.981]
Epoch [53/120    avg_loss:0.137, val_acc:0.979]
Epoch [54/120    avg_loss:0.071, val_acc:0.994]
Epoch [55/120    avg_loss:0.060, val_acc:0.981]
Epoch [56/120    avg_loss:0.062, val_acc:0.988]
Epoch [57/120    avg_loss:0.037, val_acc:0.994]
Epoch [58/120    avg_loss:0.020, val_acc:0.992]
Epoch [59/120    avg_loss:0.028, val_acc:0.994]
Epoch [60/120    avg_loss:0.051, val_acc:0.981]
Epoch [61/120    avg_loss:0.025, val_acc:0.994]
Epoch [62/120    avg_loss:0.027, val_acc:0.996]
Epoch [63/120    avg_loss:0.018, val_acc:0.992]
Epoch [64/120    avg_loss:0.016, val_acc:0.994]
Epoch [65/120    avg_loss:0.012, val_acc:0.996]
Epoch [66/120    avg_loss:0.017, val_acc:0.996]
Epoch [67/120    avg_loss:0.018, val_acc:0.996]
Epoch [68/120    avg_loss:0.015, val_acc:0.998]
Epoch [69/120    avg_loss:0.016, val_acc:0.998]
Epoch [70/120    avg_loss:0.011, val_acc:0.998]
Epoch [71/120    avg_loss:0.025, val_acc:0.998]
Epoch [72/120    avg_loss:0.019, val_acc:0.998]
Epoch [73/120    avg_loss:0.014, val_acc:0.998]
Epoch [74/120    avg_loss:0.017, val_acc:1.000]
Epoch [75/120    avg_loss:0.012, val_acc:1.000]
Epoch [76/120    avg_loss:0.015, val_acc:1.000]
Epoch [77/120    avg_loss:0.013, val_acc:1.000]
Epoch [78/120    avg_loss:0.012, val_acc:1.000]
Epoch [79/120    avg_loss:0.010, val_acc:1.000]
Epoch [80/120    avg_loss:0.010, val_acc:1.000]
Epoch [81/120    avg_loss:0.014, val_acc:1.000]
Epoch [82/120    avg_loss:0.012, val_acc:1.000]
Epoch [83/120    avg_loss:0.011, val_acc:1.000]
Epoch [84/120    avg_loss:0.010, val_acc:1.000]
Epoch [85/120    avg_loss:0.010, val_acc:1.000]
Epoch [86/120    avg_loss:0.012, val_acc:1.000]
Epoch [87/120    avg_loss:0.011, val_acc:1.000]
Epoch [88/120    avg_loss:0.010, val_acc:0.998]
Epoch [89/120    avg_loss:0.009, val_acc:1.000]
Epoch [90/120    avg_loss:0.017, val_acc:1.000]
Epoch [91/120    avg_loss:0.009, val_acc:1.000]
Epoch [92/120    avg_loss:0.011, val_acc:1.000]
Epoch [93/120    avg_loss:0.010, val_acc:1.000]
Epoch [94/120    avg_loss:0.010, val_acc:1.000]
Epoch [95/120    avg_loss:0.018, val_acc:1.000]
Epoch [96/120    avg_loss:0.016, val_acc:1.000]
Epoch [97/120    avg_loss:0.014, val_acc:1.000]
Epoch [98/120    avg_loss:0.010, val_acc:1.000]
Epoch [99/120    avg_loss:0.014, val_acc:1.000]
Epoch [100/120    avg_loss:0.009, val_acc:1.000]
Epoch [101/120    avg_loss:0.017, val_acc:0.998]
Epoch [102/120    avg_loss:0.012, val_acc:0.998]
Epoch [103/120    avg_loss:0.012, val_acc:1.000]
Epoch [104/120    avg_loss:0.013, val_acc:1.000]
Epoch [105/120    avg_loss:0.010, val_acc:1.000]
Epoch [106/120    avg_loss:0.012, val_acc:1.000]
Epoch [107/120    avg_loss:0.014, val_acc:1.000]
Epoch [108/120    avg_loss:0.011, val_acc:1.000]
Epoch [109/120    avg_loss:0.014, val_acc:1.000]
Epoch [110/120    avg_loss:0.010, val_acc:1.000]
Epoch [111/120    avg_loss:0.010, val_acc:1.000]
Epoch [112/120    avg_loss:0.008, val_acc:1.000]
Epoch [113/120    avg_loss:0.016, val_acc:1.000]
Epoch [114/120    avg_loss:0.013, val_acc:1.000]
Epoch [115/120    avg_loss:0.010, val_acc:1.000]
Epoch [116/120    avg_loss:0.014, val_acc:1.000]
Epoch [117/120    avg_loss:0.013, val_acc:1.000]
Epoch [118/120    avg_loss:0.010, val_acc:1.000]
Epoch [119/120    avg_loss:0.010, val_acc:1.000]
Epoch [120/120    avg_loss:0.012, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   6   0   0   0   0   0   0   4   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.76545842217485

F1 scores:
[       nan 0.99926954 1.         1.         0.97747748 0.97643098
 1.         1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9973888209142133
creating ./logs/logs-2022-01-19KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:160
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd54686d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 247232==>0.25M
----------Training process----------
Epoch [1/120    avg_loss:2.297, val_acc:0.527]
Epoch [2/120    avg_loss:1.600, val_acc:0.669]
Epoch [3/120    avg_loss:1.122, val_acc:0.746]
Epoch [4/120    avg_loss:0.913, val_acc:0.775]
Epoch [5/120    avg_loss:0.702, val_acc:0.852]
Epoch [6/120    avg_loss:0.594, val_acc:0.852]
Epoch [7/120    avg_loss:0.523, val_acc:0.877]
Epoch [8/120    avg_loss:0.526, val_acc:0.885]
Epoch [9/120    avg_loss:0.403, val_acc:0.912]
Epoch [10/120    avg_loss:0.336, val_acc:0.933]
Epoch [11/120    avg_loss:0.300, val_acc:0.952]
Epoch [12/120    avg_loss:0.419, val_acc:0.892]
Epoch [13/120    avg_loss:0.416, val_acc:0.946]
Epoch [14/120    avg_loss:0.344, val_acc:0.944]
Epoch [15/120    avg_loss:0.227, val_acc:0.950]
Epoch [16/120    avg_loss:0.232, val_acc:0.952]
Epoch [17/120    avg_loss:0.199, val_acc:0.942]
Epoch [18/120    avg_loss:0.165, val_acc:0.977]
Epoch [19/120    avg_loss:0.164, val_acc:0.950]
Epoch [20/120    avg_loss:0.213, val_acc:0.977]
Epoch [21/120    avg_loss:0.166, val_acc:0.967]
Epoch [22/120    avg_loss:0.267, val_acc:0.946]
Epoch [23/120    avg_loss:0.201, val_acc:0.971]
Epoch [24/120    avg_loss:0.214, val_acc:0.954]
Epoch [25/120    avg_loss:0.152, val_acc:0.954]
Epoch [26/120    avg_loss:0.111, val_acc:0.979]
Epoch [27/120    avg_loss:0.118, val_acc:0.971]
Epoch [28/120    avg_loss:0.122, val_acc:0.975]
Epoch [29/120    avg_loss:0.099, val_acc:0.981]
Epoch [30/120    avg_loss:0.082, val_acc:0.985]
Epoch [31/120    avg_loss:0.061, val_acc:0.979]
Epoch [32/120    avg_loss:0.087, val_acc:0.983]
Epoch [33/120    avg_loss:0.057, val_acc:0.985]
Epoch [34/120    avg_loss:0.080, val_acc:0.981]
Epoch [35/120    avg_loss:0.106, val_acc:0.981]
Epoch [36/120    avg_loss:0.200, val_acc:0.965]
Epoch [37/120    avg_loss:0.140, val_acc:0.983]
Epoch [38/120    avg_loss:0.086, val_acc:0.973]
Epoch [39/120    avg_loss:0.057, val_acc:0.977]
Epoch [40/120    avg_loss:0.071, val_acc:0.969]
Epoch [41/120    avg_loss:0.050, val_acc:0.981]
Epoch [42/120    avg_loss:0.027, val_acc:0.988]
Epoch [43/120    avg_loss:0.048, val_acc:0.988]
Epoch [44/120    avg_loss:0.037, val_acc:0.985]
Epoch [45/120    avg_loss:0.033, val_acc:0.990]
Epoch [46/120    avg_loss:0.037, val_acc:0.979]
Epoch [47/120    avg_loss:0.057, val_acc:0.975]
Epoch [48/120    avg_loss:0.092, val_acc:0.960]
Epoch [49/120    avg_loss:0.070, val_acc:0.971]
Epoch [50/120    avg_loss:0.079, val_acc:0.981]
Epoch [51/120    avg_loss:0.062, val_acc:0.983]
Epoch [52/120    avg_loss:0.039, val_acc:0.981]
Epoch [53/120    avg_loss:0.030, val_acc:0.981]
Epoch [54/120    avg_loss:0.043, val_acc:0.979]
Epoch [55/120    avg_loss:0.038, val_acc:0.979]
Epoch [56/120    avg_loss:0.036, val_acc:0.981]
Epoch [57/120    avg_loss:0.060, val_acc:0.983]
Epoch [58/120    avg_loss:0.058, val_acc:0.981]
Epoch [59/120    avg_loss:0.041, val_acc:0.983]
Epoch [60/120    avg_loss:0.043, val_acc:0.988]
Epoch [61/120    avg_loss:0.027, val_acc:0.988]
Epoch [62/120    avg_loss:0.025, val_acc:0.988]
Epoch [63/120    avg_loss:0.026, val_acc:0.988]
Epoch [64/120    avg_loss:0.019, val_acc:0.988]
Epoch [65/120    avg_loss:0.016, val_acc:0.990]
Epoch [66/120    avg_loss:0.021, val_acc:0.985]
Epoch [67/120    avg_loss:0.022, val_acc:0.985]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.016, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.985]
Epoch [71/120    avg_loss:0.017, val_acc:0.985]
Epoch [72/120    avg_loss:0.017, val_acc:0.985]
Epoch [73/120    avg_loss:0.031, val_acc:0.983]
Epoch [74/120    avg_loss:0.018, val_acc:0.988]
Epoch [75/120    avg_loss:0.021, val_acc:0.988]
Epoch [76/120    avg_loss:0.019, val_acc:0.988]
Epoch [77/120    avg_loss:0.023, val_acc:0.985]
Epoch [78/120    avg_loss:0.013, val_acc:0.985]
Epoch [79/120    avg_loss:0.016, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.017, val_acc:0.985]
Epoch [82/120    avg_loss:0.023, val_acc:0.985]
Epoch [83/120    avg_loss:0.022, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.017, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.983]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.015, val_acc:0.983]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.019, val_acc:0.983]
Epoch [91/120    avg_loss:0.021, val_acc:0.983]
Epoch [92/120    avg_loss:0.026, val_acc:0.983]
Epoch [93/120    avg_loss:0.018, val_acc:0.983]
Epoch [94/120    avg_loss:0.021, val_acc:0.983]
Epoch [95/120    avg_loss:0.024, val_acc:0.983]
Epoch [96/120    avg_loss:0.020, val_acc:0.983]
Epoch [97/120    avg_loss:0.025, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.983]
Epoch [99/120    avg_loss:0.021, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.017, val_acc:0.983]
Epoch [102/120    avg_loss:0.015, val_acc:0.983]
Epoch [103/120    avg_loss:0.022, val_acc:0.983]
Epoch [104/120    avg_loss:0.017, val_acc:0.983]
Epoch [105/120    avg_loss:0.032, val_acc:0.983]
Epoch [106/120    avg_loss:0.013, val_acc:0.983]
Epoch [107/120    avg_loss:0.020, val_acc:0.983]
Epoch [108/120    avg_loss:0.020, val_acc:0.983]
Epoch [109/120    avg_loss:0.018, val_acc:0.983]
Epoch [110/120    avg_loss:0.023, val_acc:0.983]
Epoch [111/120    avg_loss:0.028, val_acc:0.983]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.016, val_acc:0.983]
Epoch [114/120    avg_loss:0.026, val_acc:0.983]
Epoch [115/120    avg_loss:0.016, val_acc:0.983]
Epoch [116/120    avg_loss:0.021, val_acc:0.983]
Epoch [117/120    avg_loss:0.027, val_acc:0.983]
Epoch [118/120    avg_loss:0.021, val_acc:0.983]
Epoch [119/120    avg_loss:0.015, val_acc:0.983]
Epoch [120/120    avg_loss:0.020, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.74413646055437

F1 scores:
[       nan 1.         1.         1.         0.9753915  0.96621622
 1.         1.         1.         1.         1.         0.9986755
 0.99779249 1.        ]

Kappa:
0.9971514718508934
