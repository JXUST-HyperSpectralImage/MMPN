creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f612d3a0940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.996, val_acc:0.476]
Epoch [2/120    avg_loss:1.540, val_acc:0.281]
Epoch [3/120    avg_loss:1.241, val_acc:0.421]
Epoch [4/120    avg_loss:1.048, val_acc:0.720]
Epoch [5/120    avg_loss:0.859, val_acc:0.720]
Epoch [6/120    avg_loss:0.751, val_acc:0.732]
Epoch [7/120    avg_loss:0.634, val_acc:0.760]
Epoch [8/120    avg_loss:0.498, val_acc:0.720]
Epoch [9/120    avg_loss:1.176, val_acc:0.650]
Epoch [10/120    avg_loss:0.982, val_acc:0.674]
Epoch [11/120    avg_loss:0.786, val_acc:0.727]
Epoch [12/120    avg_loss:0.673, val_acc:0.755]
Epoch [13/120    avg_loss:0.546, val_acc:0.748]
Epoch [14/120    avg_loss:0.464, val_acc:0.758]
Epoch [15/120    avg_loss:0.430, val_acc:0.823]
Epoch [16/120    avg_loss:0.412, val_acc:0.778]
Epoch [17/120    avg_loss:0.382, val_acc:0.845]
Epoch [18/120    avg_loss:0.289, val_acc:0.868]
Epoch [19/120    avg_loss:0.281, val_acc:0.882]
Epoch [20/120    avg_loss:0.262, val_acc:0.863]
Epoch [21/120    avg_loss:0.240, val_acc:0.908]
Epoch [22/120    avg_loss:0.227, val_acc:0.878]
Epoch [23/120    avg_loss:0.227, val_acc:0.858]
Epoch [24/120    avg_loss:0.214, val_acc:0.872]
Epoch [25/120    avg_loss:0.204, val_acc:0.918]
Epoch [26/120    avg_loss:0.174, val_acc:0.927]
Epoch [27/120    avg_loss:0.161, val_acc:0.929]
Epoch [28/120    avg_loss:0.193, val_acc:0.932]
Epoch [29/120    avg_loss:0.143, val_acc:0.942]
Epoch [30/120    avg_loss:0.116, val_acc:0.935]
Epoch [31/120    avg_loss:0.138, val_acc:0.962]
Epoch [32/120    avg_loss:0.124, val_acc:0.942]
Epoch [33/120    avg_loss:0.091, val_acc:0.966]
Epoch [34/120    avg_loss:0.191, val_acc:0.895]
Epoch [35/120    avg_loss:0.209, val_acc:0.933]
Epoch [36/120    avg_loss:0.109, val_acc:0.935]
Epoch [37/120    avg_loss:0.121, val_acc:0.962]
Epoch [38/120    avg_loss:0.105, val_acc:0.951]
Epoch [39/120    avg_loss:0.097, val_acc:0.951]
Epoch [40/120    avg_loss:0.096, val_acc:0.957]
Epoch [41/120    avg_loss:0.053, val_acc:0.969]
Epoch [42/120    avg_loss:0.059, val_acc:0.960]
Epoch [43/120    avg_loss:0.062, val_acc:0.967]
Epoch [44/120    avg_loss:0.077, val_acc:0.970]
Epoch [45/120    avg_loss:0.068, val_acc:0.953]
Epoch [46/120    avg_loss:0.056, val_acc:0.972]
Epoch [47/120    avg_loss:0.050, val_acc:0.953]
Epoch [48/120    avg_loss:0.052, val_acc:0.945]
Epoch [49/120    avg_loss:0.062, val_acc:0.971]
Epoch [50/120    avg_loss:0.053, val_acc:0.972]
Epoch [51/120    avg_loss:0.046, val_acc:0.972]
Epoch [52/120    avg_loss:0.036, val_acc:0.976]
Epoch [53/120    avg_loss:0.045, val_acc:0.949]
Epoch [54/120    avg_loss:0.041, val_acc:0.981]
Epoch [55/120    avg_loss:0.039, val_acc:0.979]
Epoch [56/120    avg_loss:0.033, val_acc:0.980]
Epoch [57/120    avg_loss:0.031, val_acc:0.977]
Epoch [58/120    avg_loss:0.031, val_acc:0.980]
Epoch [59/120    avg_loss:0.027, val_acc:0.981]
Epoch [60/120    avg_loss:0.028, val_acc:0.980]
Epoch [61/120    avg_loss:0.024, val_acc:0.981]
Epoch [62/120    avg_loss:0.038, val_acc:0.965]
Epoch [63/120    avg_loss:0.045, val_acc:0.978]
Epoch [64/120    avg_loss:0.038, val_acc:0.972]
Epoch [65/120    avg_loss:0.063, val_acc:0.977]
Epoch [66/120    avg_loss:0.037, val_acc:0.981]
Epoch [67/120    avg_loss:0.021, val_acc:0.980]
Epoch [68/120    avg_loss:0.025, val_acc:0.978]
Epoch [69/120    avg_loss:0.024, val_acc:0.985]
Epoch [70/120    avg_loss:0.035, val_acc:0.987]
Epoch [71/120    avg_loss:0.030, val_acc:0.977]
Epoch [72/120    avg_loss:0.020, val_acc:0.968]
Epoch [73/120    avg_loss:0.016, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.979]
Epoch [76/120    avg_loss:0.023, val_acc:0.985]
Epoch [77/120    avg_loss:0.032, val_acc:0.984]
Epoch [78/120    avg_loss:0.018, val_acc:0.989]
Epoch [79/120    avg_loss:0.015, val_acc:0.987]
Epoch [80/120    avg_loss:0.015, val_acc:0.983]
Epoch [81/120    avg_loss:0.017, val_acc:0.979]
Epoch [82/120    avg_loss:0.035, val_acc:0.978]
Epoch [83/120    avg_loss:0.045, val_acc:0.985]
Epoch [84/120    avg_loss:0.016, val_acc:0.987]
Epoch [85/120    avg_loss:0.014, val_acc:0.984]
Epoch [86/120    avg_loss:0.053, val_acc:0.965]
Epoch [87/120    avg_loss:0.070, val_acc:0.969]
Epoch [88/120    avg_loss:0.056, val_acc:0.958]
Epoch [89/120    avg_loss:0.048, val_acc:0.973]
Epoch [90/120    avg_loss:0.047, val_acc:0.954]
Epoch [91/120    avg_loss:0.033, val_acc:0.974]
Epoch [92/120    avg_loss:0.022, val_acc:0.979]
Epoch [93/120    avg_loss:0.020, val_acc:0.982]
Epoch [94/120    avg_loss:0.018, val_acc:0.984]
Epoch [95/120    avg_loss:0.033, val_acc:0.980]
Epoch [96/120    avg_loss:0.023, val_acc:0.984]
Epoch [97/120    avg_loss:0.015, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.984]
Epoch [99/120    avg_loss:0.020, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.985]
Epoch [102/120    avg_loss:0.019, val_acc:0.985]
Epoch [103/120    avg_loss:0.018, val_acc:0.986]
Epoch [104/120    avg_loss:0.014, val_acc:0.989]
Epoch [105/120    avg_loss:0.011, val_acc:0.987]
Epoch [106/120    avg_loss:0.011, val_acc:0.986]
Epoch [107/120    avg_loss:0.016, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.015, val_acc:0.987]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.987]
Epoch [112/120    avg_loss:0.012, val_acc:0.987]
Epoch [113/120    avg_loss:0.013, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.987]
Epoch [115/120    avg_loss:0.015, val_acc:0.987]
Epoch [116/120    avg_loss:0.012, val_acc:0.987]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6321     0     9    17     0     0    13    42    30]
 [    0     0 18015     0    56     0    19     0     0     0]
 [    0     1     0  2021     6     0     0     0     1     7]
 [    0    30    33     0  2859     0    14     0    36     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    11     0     0  4867     0     0     0]
 [    0     0     0     0     0     0     1  1288     0     1]
 [    0    28     0     6    51     0     0     0  3486     0]
 [    0     0     0     1    20    54     0     2     0   842]]

Accuracy:
98.82148796182489

F1 scores:
[       nan 0.98673119 0.99701146 0.98971596 0.95602742 0.97972973
 0.9953983  0.99344389 0.97701794 0.9360756 ]

Kappa:
0.9843938248652983
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c475fb908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.932, val_acc:0.138]
Epoch [2/120    avg_loss:1.533, val_acc:0.360]
Epoch [3/120    avg_loss:1.216, val_acc:0.520]
Epoch [4/120    avg_loss:0.985, val_acc:0.635]
Epoch [5/120    avg_loss:0.839, val_acc:0.676]
Epoch [6/120    avg_loss:0.638, val_acc:0.737]
Epoch [7/120    avg_loss:0.534, val_acc:0.814]
Epoch [8/120    avg_loss:0.447, val_acc:0.847]
Epoch [9/120    avg_loss:0.402, val_acc:0.792]
Epoch [10/120    avg_loss:0.393, val_acc:0.760]
Epoch [11/120    avg_loss:0.314, val_acc:0.891]
Epoch [12/120    avg_loss:0.297, val_acc:0.878]
Epoch [13/120    avg_loss:0.333, val_acc:0.883]
Epoch [14/120    avg_loss:0.292, val_acc:0.870]
Epoch [15/120    avg_loss:0.280, val_acc:0.853]
Epoch [16/120    avg_loss:0.213, val_acc:0.924]
Epoch [17/120    avg_loss:0.161, val_acc:0.908]
Epoch [18/120    avg_loss:0.160, val_acc:0.904]
Epoch [19/120    avg_loss:0.175, val_acc:0.847]
Epoch [20/120    avg_loss:0.150, val_acc:0.939]
Epoch [21/120    avg_loss:0.158, val_acc:0.957]
Epoch [22/120    avg_loss:0.159, val_acc:0.951]
Epoch [23/120    avg_loss:0.137, val_acc:0.957]
Epoch [24/120    avg_loss:0.089, val_acc:0.956]
Epoch [25/120    avg_loss:0.139, val_acc:0.961]
Epoch [26/120    avg_loss:0.211, val_acc:0.928]
Epoch [27/120    avg_loss:0.130, val_acc:0.921]
Epoch [28/120    avg_loss:0.179, val_acc:0.903]
Epoch [29/120    avg_loss:0.133, val_acc:0.959]
Epoch [30/120    avg_loss:0.090, val_acc:0.956]
Epoch [31/120    avg_loss:0.070, val_acc:0.953]
Epoch [32/120    avg_loss:0.083, val_acc:0.965]
Epoch [33/120    avg_loss:0.084, val_acc:0.961]
Epoch [34/120    avg_loss:0.080, val_acc:0.964]
Epoch [35/120    avg_loss:0.063, val_acc:0.964]
Epoch [36/120    avg_loss:0.061, val_acc:0.961]
Epoch [37/120    avg_loss:0.055, val_acc:0.953]
Epoch [38/120    avg_loss:0.049, val_acc:0.972]
Epoch [39/120    avg_loss:0.052, val_acc:0.974]
Epoch [40/120    avg_loss:0.034, val_acc:0.975]
Epoch [41/120    avg_loss:0.037, val_acc:0.974]
Epoch [42/120    avg_loss:0.034, val_acc:0.968]
Epoch [43/120    avg_loss:0.035, val_acc:0.961]
Epoch [44/120    avg_loss:0.048, val_acc:0.973]
Epoch [45/120    avg_loss:0.072, val_acc:0.972]
Epoch [46/120    avg_loss:0.035, val_acc:0.973]
Epoch [47/120    avg_loss:0.037, val_acc:0.971]
Epoch [48/120    avg_loss:0.035, val_acc:0.970]
Epoch [49/120    avg_loss:0.032, val_acc:0.958]
Epoch [50/120    avg_loss:0.034, val_acc:0.976]
Epoch [51/120    avg_loss:0.030, val_acc:0.980]
Epoch [52/120    avg_loss:0.035, val_acc:0.971]
Epoch [53/120    avg_loss:0.040, val_acc:0.973]
Epoch [54/120    avg_loss:0.033, val_acc:0.976]
Epoch [55/120    avg_loss:0.040, val_acc:0.978]
Epoch [56/120    avg_loss:0.030, val_acc:0.974]
Epoch [57/120    avg_loss:0.030, val_acc:0.981]
Epoch [58/120    avg_loss:0.022, val_acc:0.978]
Epoch [59/120    avg_loss:0.020, val_acc:0.979]
Epoch [60/120    avg_loss:0.040, val_acc:0.980]
Epoch [61/120    avg_loss:0.028, val_acc:0.968]
Epoch [62/120    avg_loss:0.045, val_acc:0.976]
Epoch [63/120    avg_loss:0.051, val_acc:0.972]
Epoch [64/120    avg_loss:0.027, val_acc:0.983]
Epoch [65/120    avg_loss:0.027, val_acc:0.971]
Epoch [66/120    avg_loss:0.018, val_acc:0.980]
Epoch [67/120    avg_loss:0.015, val_acc:0.984]
Epoch [68/120    avg_loss:0.012, val_acc:0.987]
Epoch [69/120    avg_loss:0.014, val_acc:0.980]
Epoch [70/120    avg_loss:0.043, val_acc:0.957]
Epoch [71/120    avg_loss:0.049, val_acc:0.968]
Epoch [72/120    avg_loss:0.063, val_acc:0.968]
Epoch [73/120    avg_loss:0.065, val_acc:0.945]
Epoch [74/120    avg_loss:0.051, val_acc:0.977]
Epoch [75/120    avg_loss:0.028, val_acc:0.984]
Epoch [76/120    avg_loss:0.017, val_acc:0.980]
Epoch [77/120    avg_loss:0.019, val_acc:0.985]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.037, val_acc:0.977]
Epoch [81/120    avg_loss:0.026, val_acc:0.973]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.011, val_acc:0.979]
Epoch [84/120    avg_loss:0.009, val_acc:0.980]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.009, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.016, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.012, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.009, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     0     6     0    14     1    32     7]
 [    0     0 18041     0    30     0    19     0     0     0]
 [    0     7     0  2009     2     0     0     0    18     0]
 [    0    38    19     0  2874     0     7     0    31     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4870     0     0     8]
 [    0     0     0     0     0     0     3  1283     0     4]
 [    0     3     0    12    60     0     0     0  3477    19]
 [    0     0     0     0    17    74     0     0     0   828]]

Accuracy:
98.95404044055624

F1 scores:
[       nan 0.99159664 0.99811895 0.99038699 0.96426774 0.97242921
 0.99479113 0.996892   0.97545238 0.9261745 ]

Kappa:
0.986145018186383
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f862c1689e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.970, val_acc:0.247]
Epoch [2/120    avg_loss:1.547, val_acc:0.346]
Epoch [3/120    avg_loss:1.268, val_acc:0.472]
Epoch [4/120    avg_loss:1.033, val_acc:0.539]
Epoch [5/120    avg_loss:0.891, val_acc:0.684]
Epoch [6/120    avg_loss:0.734, val_acc:0.753]
Epoch [7/120    avg_loss:0.563, val_acc:0.798]
Epoch [8/120    avg_loss:0.516, val_acc:0.787]
Epoch [9/120    avg_loss:0.432, val_acc:0.840]
Epoch [10/120    avg_loss:0.320, val_acc:0.865]
Epoch [11/120    avg_loss:0.299, val_acc:0.916]
Epoch [12/120    avg_loss:0.260, val_acc:0.848]
Epoch [13/120    avg_loss:0.226, val_acc:0.929]
Epoch [14/120    avg_loss:0.204, val_acc:0.938]
Epoch [15/120    avg_loss:0.187, val_acc:0.933]
Epoch [16/120    avg_loss:0.165, val_acc:0.947]
Epoch [17/120    avg_loss:0.188, val_acc:0.920]
Epoch [18/120    avg_loss:0.152, val_acc:0.931]
Epoch [19/120    avg_loss:0.151, val_acc:0.924]
Epoch [20/120    avg_loss:0.128, val_acc:0.965]
Epoch [21/120    avg_loss:0.129, val_acc:0.952]
Epoch [22/120    avg_loss:0.097, val_acc:0.951]
Epoch [23/120    avg_loss:0.097, val_acc:0.958]
Epoch [24/120    avg_loss:0.085, val_acc:0.965]
Epoch [25/120    avg_loss:0.095, val_acc:0.946]
Epoch [26/120    avg_loss:0.084, val_acc:0.964]
Epoch [27/120    avg_loss:0.112, val_acc:0.950]
Epoch [28/120    avg_loss:1.053, val_acc:0.436]
Epoch [29/120    avg_loss:0.956, val_acc:0.700]
Epoch [30/120    avg_loss:0.690, val_acc:0.745]
Epoch [31/120    avg_loss:0.584, val_acc:0.822]
Epoch [32/120    avg_loss:0.445, val_acc:0.793]
Epoch [33/120    avg_loss:0.403, val_acc:0.825]
Epoch [34/120    avg_loss:0.358, val_acc:0.829]
Epoch [35/120    avg_loss:0.325, val_acc:0.826]
Epoch [36/120    avg_loss:0.324, val_acc:0.823]
Epoch [37/120    avg_loss:0.287, val_acc:0.897]
Epoch [38/120    avg_loss:0.224, val_acc:0.895]
Epoch [39/120    avg_loss:0.219, val_acc:0.894]
Epoch [40/120    avg_loss:0.213, val_acc:0.899]
Epoch [41/120    avg_loss:0.199, val_acc:0.900]
Epoch [42/120    avg_loss:0.195, val_acc:0.905]
Epoch [43/120    avg_loss:0.214, val_acc:0.904]
Epoch [44/120    avg_loss:0.210, val_acc:0.905]
Epoch [45/120    avg_loss:0.180, val_acc:0.905]
Epoch [46/120    avg_loss:0.210, val_acc:0.901]
Epoch [47/120    avg_loss:0.204, val_acc:0.919]
Epoch [48/120    avg_loss:0.184, val_acc:0.911]
Epoch [49/120    avg_loss:0.184, val_acc:0.905]
Epoch [50/120    avg_loss:0.177, val_acc:0.916]
Epoch [51/120    avg_loss:0.172, val_acc:0.915]
Epoch [52/120    avg_loss:0.154, val_acc:0.918]
Epoch [53/120    avg_loss:0.175, val_acc:0.917]
Epoch [54/120    avg_loss:0.157, val_acc:0.916]
Epoch [55/120    avg_loss:0.163, val_acc:0.918]
Epoch [56/120    avg_loss:0.160, val_acc:0.918]
Epoch [57/120    avg_loss:0.180, val_acc:0.918]
Epoch [58/120    avg_loss:0.167, val_acc:0.917]
Epoch [59/120    avg_loss:0.184, val_acc:0.917]
Epoch [60/120    avg_loss:0.169, val_acc:0.918]
Epoch [61/120    avg_loss:0.165, val_acc:0.918]
Epoch [62/120    avg_loss:0.195, val_acc:0.919]
Epoch [63/120    avg_loss:0.185, val_acc:0.920]
Epoch [64/120    avg_loss:0.169, val_acc:0.919]
Epoch [65/120    avg_loss:0.168, val_acc:0.919]
Epoch [66/120    avg_loss:0.157, val_acc:0.918]
Epoch [67/120    avg_loss:0.167, val_acc:0.918]
Epoch [68/120    avg_loss:0.170, val_acc:0.918]
Epoch [69/120    avg_loss:0.173, val_acc:0.918]
Epoch [70/120    avg_loss:0.175, val_acc:0.918]
Epoch [71/120    avg_loss:0.162, val_acc:0.918]
Epoch [72/120    avg_loss:0.163, val_acc:0.918]
Epoch [73/120    avg_loss:0.174, val_acc:0.918]
Epoch [74/120    avg_loss:0.178, val_acc:0.918]
Epoch [75/120    avg_loss:0.159, val_acc:0.918]
Epoch [76/120    avg_loss:0.164, val_acc:0.918]
Epoch [77/120    avg_loss:0.166, val_acc:0.918]
Epoch [78/120    avg_loss:0.169, val_acc:0.918]
Epoch [79/120    avg_loss:0.156, val_acc:0.918]
Epoch [80/120    avg_loss:0.155, val_acc:0.918]
Epoch [81/120    avg_loss:0.164, val_acc:0.918]
Epoch [82/120    avg_loss:0.168, val_acc:0.918]
Epoch [83/120    avg_loss:0.159, val_acc:0.918]
Epoch [84/120    avg_loss:0.161, val_acc:0.918]
Epoch [85/120    avg_loss:0.167, val_acc:0.918]
Epoch [86/120    avg_loss:0.167, val_acc:0.918]
Epoch [87/120    avg_loss:0.148, val_acc:0.918]
Epoch [88/120    avg_loss:0.160, val_acc:0.918]
Epoch [89/120    avg_loss:0.168, val_acc:0.918]
Epoch [90/120    avg_loss:0.156, val_acc:0.918]
Epoch [91/120    avg_loss:0.166, val_acc:0.918]
Epoch [92/120    avg_loss:0.165, val_acc:0.918]
Epoch [93/120    avg_loss:0.162, val_acc:0.918]
Epoch [94/120    avg_loss:0.167, val_acc:0.918]
Epoch [95/120    avg_loss:0.157, val_acc:0.918]
Epoch [96/120    avg_loss:0.174, val_acc:0.918]
Epoch [97/120    avg_loss:0.175, val_acc:0.918]
Epoch [98/120    avg_loss:0.180, val_acc:0.918]
Epoch [99/120    avg_loss:0.159, val_acc:0.918]
Epoch [100/120    avg_loss:0.174, val_acc:0.918]
Epoch [101/120    avg_loss:0.176, val_acc:0.918]
Epoch [102/120    avg_loss:0.174, val_acc:0.918]
Epoch [103/120    avg_loss:0.174, val_acc:0.918]
Epoch [104/120    avg_loss:0.170, val_acc:0.918]
Epoch [105/120    avg_loss:0.161, val_acc:0.918]
Epoch [106/120    avg_loss:0.157, val_acc:0.918]
Epoch [107/120    avg_loss:0.183, val_acc:0.918]
Epoch [108/120    avg_loss:0.172, val_acc:0.918]
Epoch [109/120    avg_loss:0.172, val_acc:0.918]
Epoch [110/120    avg_loss:0.170, val_acc:0.918]
Epoch [111/120    avg_loss:0.173, val_acc:0.918]
Epoch [112/120    avg_loss:0.167, val_acc:0.918]
Epoch [113/120    avg_loss:0.182, val_acc:0.918]
Epoch [114/120    avg_loss:0.171, val_acc:0.918]
Epoch [115/120    avg_loss:0.156, val_acc:0.918]
Epoch [116/120    avg_loss:0.164, val_acc:0.918]
Epoch [117/120    avg_loss:0.167, val_acc:0.918]
Epoch [118/120    avg_loss:0.170, val_acc:0.918]
Epoch [119/120    avg_loss:0.154, val_acc:0.918]
Epoch [120/120    avg_loss:0.180, val_acc:0.918]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5481     0     8   266     0    35    21   476   145]
 [    0     0 17110     0   152     0   828     0     0     0]
 [    0    16     0  1939     2     0     0     8    63     8]
 [    0   121    69     0  2661     0    84     0    37     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     4   206    12     1     0  4629     0    26     0]
 [    0     9     0     0     0     0     8  1267     0     6]
 [    0    86     0     0    70     0    13     0  3402     0]
 [    0     7     0     4    20   104     0     3    10   771]]

Accuracy:
92.94097799628854

F1 scores:
[       nan 0.9017769  0.96462297 0.96974244 0.86621094 0.96129746
 0.88381862 0.97875628 0.89703362 0.83351351]

Kappa:
0.9072924558653404
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4855270940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.927, val_acc:0.236]
Epoch [2/120    avg_loss:1.531, val_acc:0.365]
Epoch [3/120    avg_loss:1.284, val_acc:0.509]
Epoch [4/120    avg_loss:1.041, val_acc:0.697]
Epoch [5/120    avg_loss:0.817, val_acc:0.765]
Epoch [6/120    avg_loss:0.692, val_acc:0.819]
Epoch [7/120    avg_loss:0.596, val_acc:0.824]
Epoch [8/120    avg_loss:0.472, val_acc:0.810]
Epoch [9/120    avg_loss:0.376, val_acc:0.837]
Epoch [10/120    avg_loss:0.413, val_acc:0.875]
Epoch [11/120    avg_loss:0.303, val_acc:0.922]
Epoch [12/120    avg_loss:0.317, val_acc:0.842]
Epoch [13/120    avg_loss:0.354, val_acc:0.904]
Epoch [14/120    avg_loss:0.210, val_acc:0.936]
Epoch [15/120    avg_loss:0.186, val_acc:0.931]
Epoch [16/120    avg_loss:0.168, val_acc:0.942]
Epoch [17/120    avg_loss:0.223, val_acc:0.727]
Epoch [18/120    avg_loss:0.505, val_acc:0.816]
Epoch [19/120    avg_loss:0.395, val_acc:0.925]
Epoch [20/120    avg_loss:0.274, val_acc:0.882]
Epoch [21/120    avg_loss:0.274, val_acc:0.847]
Epoch [22/120    avg_loss:0.207, val_acc:0.945]
Epoch [23/120    avg_loss:0.160, val_acc:0.934]
Epoch [24/120    avg_loss:0.215, val_acc:0.948]
Epoch [25/120    avg_loss:0.136, val_acc:0.958]
Epoch [26/120    avg_loss:0.110, val_acc:0.941]
Epoch [27/120    avg_loss:0.128, val_acc:0.963]
Epoch [28/120    avg_loss:0.098, val_acc:0.925]
Epoch [29/120    avg_loss:0.097, val_acc:0.968]
Epoch [30/120    avg_loss:0.084, val_acc:0.959]
Epoch [31/120    avg_loss:0.097, val_acc:0.964]
Epoch [32/120    avg_loss:0.077, val_acc:0.921]
Epoch [33/120    avg_loss:0.072, val_acc:0.951]
Epoch [34/120    avg_loss:0.058, val_acc:0.968]
Epoch [35/120    avg_loss:0.079, val_acc:0.953]
Epoch [36/120    avg_loss:0.053, val_acc:0.971]
Epoch [37/120    avg_loss:0.041, val_acc:0.972]
Epoch [38/120    avg_loss:0.047, val_acc:0.977]
Epoch [39/120    avg_loss:0.046, val_acc:0.979]
Epoch [40/120    avg_loss:0.031, val_acc:0.976]
Epoch [41/120    avg_loss:0.034, val_acc:0.979]
Epoch [42/120    avg_loss:0.051, val_acc:0.958]
Epoch [43/120    avg_loss:0.046, val_acc:0.967]
Epoch [44/120    avg_loss:0.060, val_acc:0.971]
Epoch [45/120    avg_loss:0.039, val_acc:0.970]
Epoch [46/120    avg_loss:0.033, val_acc:0.970]
Epoch [47/120    avg_loss:0.132, val_acc:0.952]
Epoch [48/120    avg_loss:0.065, val_acc:0.963]
Epoch [49/120    avg_loss:0.085, val_acc:0.970]
Epoch [50/120    avg_loss:0.077, val_acc:0.972]
Epoch [51/120    avg_loss:0.043, val_acc:0.964]
Epoch [52/120    avg_loss:0.037, val_acc:0.976]
Epoch [53/120    avg_loss:0.022, val_acc:0.977]
Epoch [54/120    avg_loss:0.045, val_acc:0.974]
Epoch [55/120    avg_loss:0.028, val_acc:0.977]
Epoch [56/120    avg_loss:0.023, val_acc:0.980]
Epoch [57/120    avg_loss:0.022, val_acc:0.982]
Epoch [58/120    avg_loss:0.018, val_acc:0.980]
Epoch [59/120    avg_loss:0.021, val_acc:0.979]
Epoch [60/120    avg_loss:0.025, val_acc:0.980]
Epoch [61/120    avg_loss:0.019, val_acc:0.980]
Epoch [62/120    avg_loss:0.014, val_acc:0.981]
Epoch [63/120    avg_loss:0.015, val_acc:0.981]
Epoch [64/120    avg_loss:0.016, val_acc:0.981]
Epoch [65/120    avg_loss:0.018, val_acc:0.980]
Epoch [66/120    avg_loss:0.019, val_acc:0.982]
Epoch [67/120    avg_loss:0.016, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.981]
Epoch [69/120    avg_loss:0.014, val_acc:0.981]
Epoch [70/120    avg_loss:0.019, val_acc:0.981]
Epoch [71/120    avg_loss:0.017, val_acc:0.980]
Epoch [72/120    avg_loss:0.014, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.015, val_acc:0.981]
Epoch [75/120    avg_loss:0.019, val_acc:0.981]
Epoch [76/120    avg_loss:0.019, val_acc:0.980]
Epoch [77/120    avg_loss:0.017, val_acc:0.981]
Epoch [78/120    avg_loss:0.017, val_acc:0.981]
Epoch [79/120    avg_loss:0.015, val_acc:0.980]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.981]
Epoch [85/120    avg_loss:0.017, val_acc:0.981]
Epoch [86/120    avg_loss:0.017, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.981]
Epoch [88/120    avg_loss:0.018, val_acc:0.981]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.015, val_acc:0.981]
Epoch [91/120    avg_loss:0.018, val_acc:0.981]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.014, val_acc:0.981]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.981]
Epoch [96/120    avg_loss:0.014, val_acc:0.981]
Epoch [97/120    avg_loss:0.018, val_acc:0.981]
Epoch [98/120    avg_loss:0.014, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.981]
Epoch [101/120    avg_loss:0.017, val_acc:0.981]
Epoch [102/120    avg_loss:0.014, val_acc:0.981]
Epoch [103/120    avg_loss:0.017, val_acc:0.981]
Epoch [104/120    avg_loss:0.018, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.015, val_acc:0.981]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.015, val_acc:0.981]
Epoch [109/120    avg_loss:0.015, val_acc:0.981]
Epoch [110/120    avg_loss:0.015, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.016, val_acc:0.981]
Epoch [113/120    avg_loss:0.015, val_acc:0.981]
Epoch [114/120    avg_loss:0.012, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.981]
Epoch [116/120    avg_loss:0.019, val_acc:0.981]
Epoch [117/120    avg_loss:0.022, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.016, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6352     0     2     9     0     9     0    60     0]
 [    0     0 18033     0    43     0     4     0    10     0]
 [    0     0     0  2032     2     0     0     0     1     1]
 [    0    39    21     0  2870     0    13     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     3     2]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     4     0    30    60     0     0     0  3455    22]
 [    0     0     0     3    16    82     0     4     0   814]]

Accuracy:
98.8672788181139

F1 scores:
[       nan 0.99041085 0.99784197 0.99049476 0.96115204 0.96953938
 0.99682929 0.99806427 0.9692804  0.92552587]

Kappa:
0.9849982487798923
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7656f3908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.999, val_acc:0.491]
Epoch [2/120    avg_loss:1.579, val_acc:0.642]
Epoch [3/120    avg_loss:1.303, val_acc:0.450]
Epoch [4/120    avg_loss:1.059, val_acc:0.627]
Epoch [5/120    avg_loss:0.725, val_acc:0.765]
Epoch [6/120    avg_loss:0.563, val_acc:0.794]
Epoch [7/120    avg_loss:0.417, val_acc:0.834]
Epoch [8/120    avg_loss:0.414, val_acc:0.852]
Epoch [9/120    avg_loss:0.317, val_acc:0.801]
Epoch [10/120    avg_loss:0.312, val_acc:0.875]
Epoch [11/120    avg_loss:0.444, val_acc:0.708]
Epoch [12/120    avg_loss:1.484, val_acc:0.581]
Epoch [13/120    avg_loss:1.291, val_acc:0.616]
Epoch [14/120    avg_loss:1.224, val_acc:0.606]
Epoch [15/120    avg_loss:1.122, val_acc:0.631]
Epoch [16/120    avg_loss:1.076, val_acc:0.646]
Epoch [17/120    avg_loss:0.983, val_acc:0.673]
Epoch [18/120    avg_loss:0.948, val_acc:0.545]
Epoch [19/120    avg_loss:0.960, val_acc:0.665]
Epoch [20/120    avg_loss:0.931, val_acc:0.668]
Epoch [21/120    avg_loss:0.880, val_acc:0.691]
Epoch [22/120    avg_loss:0.868, val_acc:0.710]
Epoch [23/120    avg_loss:0.825, val_acc:0.696]
Epoch [24/120    avg_loss:0.786, val_acc:0.708]
Epoch [25/120    avg_loss:0.772, val_acc:0.719]
Epoch [26/120    avg_loss:0.785, val_acc:0.723]
Epoch [27/120    avg_loss:0.776, val_acc:0.718]
Epoch [28/120    avg_loss:0.772, val_acc:0.722]
Epoch [29/120    avg_loss:0.778, val_acc:0.719]
Epoch [30/120    avg_loss:0.749, val_acc:0.718]
Epoch [31/120    avg_loss:0.794, val_acc:0.715]
Epoch [32/120    avg_loss:0.757, val_acc:0.721]
Epoch [33/120    avg_loss:0.752, val_acc:0.725]
Epoch [34/120    avg_loss:0.754, val_acc:0.725]
Epoch [35/120    avg_loss:0.766, val_acc:0.726]
Epoch [36/120    avg_loss:0.781, val_acc:0.722]
Epoch [37/120    avg_loss:0.747, val_acc:0.721]
Epoch [38/120    avg_loss:0.748, val_acc:0.723]
Epoch [39/120    avg_loss:0.755, val_acc:0.724]
Epoch [40/120    avg_loss:0.718, val_acc:0.723]
Epoch [41/120    avg_loss:0.771, val_acc:0.723]
Epoch [42/120    avg_loss:0.726, val_acc:0.723]
Epoch [43/120    avg_loss:0.774, val_acc:0.723]
Epoch [44/120    avg_loss:0.742, val_acc:0.723]
Epoch [45/120    avg_loss:0.716, val_acc:0.722]
Epoch [46/120    avg_loss:0.762, val_acc:0.722]
Epoch [47/120    avg_loss:0.753, val_acc:0.724]
Epoch [48/120    avg_loss:0.728, val_acc:0.724]
Epoch [49/120    avg_loss:0.760, val_acc:0.724]
Epoch [50/120    avg_loss:0.730, val_acc:0.724]
Epoch [51/120    avg_loss:0.725, val_acc:0.724]
Epoch [52/120    avg_loss:0.717, val_acc:0.724]
Epoch [53/120    avg_loss:0.753, val_acc:0.724]
Epoch [54/120    avg_loss:0.746, val_acc:0.724]
Epoch [55/120    avg_loss:0.734, val_acc:0.724]
Epoch [56/120    avg_loss:0.775, val_acc:0.724]
Epoch [57/120    avg_loss:0.711, val_acc:0.724]
Epoch [58/120    avg_loss:0.717, val_acc:0.724]
Epoch [59/120    avg_loss:0.746, val_acc:0.724]
Epoch [60/120    avg_loss:0.734, val_acc:0.725]
Epoch [61/120    avg_loss:0.762, val_acc:0.724]
Epoch [62/120    avg_loss:0.721, val_acc:0.725]
Epoch [63/120    avg_loss:0.728, val_acc:0.725]
Epoch [64/120    avg_loss:0.738, val_acc:0.725]
Epoch [65/120    avg_loss:0.769, val_acc:0.725]
Epoch [66/120    avg_loss:0.754, val_acc:0.725]
Epoch [67/120    avg_loss:0.764, val_acc:0.725]
Epoch [68/120    avg_loss:0.772, val_acc:0.725]
Epoch [69/120    avg_loss:0.722, val_acc:0.725]
Epoch [70/120    avg_loss:0.735, val_acc:0.725]
Epoch [71/120    avg_loss:0.751, val_acc:0.725]
Epoch [72/120    avg_loss:0.753, val_acc:0.725]
Epoch [73/120    avg_loss:0.739, val_acc:0.725]
Epoch [74/120    avg_loss:0.721, val_acc:0.725]
Epoch [75/120    avg_loss:0.750, val_acc:0.725]
Epoch [76/120    avg_loss:0.740, val_acc:0.725]
Epoch [77/120    avg_loss:0.744, val_acc:0.725]
Epoch [78/120    avg_loss:0.727, val_acc:0.725]
Epoch [79/120    avg_loss:0.749, val_acc:0.725]
Epoch [80/120    avg_loss:0.745, val_acc:0.725]
Epoch [81/120    avg_loss:0.727, val_acc:0.725]
Epoch [82/120    avg_loss:0.715, val_acc:0.725]
Epoch [83/120    avg_loss:0.751, val_acc:0.725]
Epoch [84/120    avg_loss:0.735, val_acc:0.725]
Epoch [85/120    avg_loss:0.752, val_acc:0.725]
Epoch [86/120    avg_loss:0.702, val_acc:0.725]
Epoch [87/120    avg_loss:0.740, val_acc:0.725]
Epoch [88/120    avg_loss:0.758, val_acc:0.725]
Epoch [89/120    avg_loss:0.729, val_acc:0.725]
Epoch [90/120    avg_loss:0.742, val_acc:0.725]
Epoch [91/120    avg_loss:0.763, val_acc:0.725]
Epoch [92/120    avg_loss:0.731, val_acc:0.725]
Epoch [93/120    avg_loss:0.762, val_acc:0.725]
Epoch [94/120    avg_loss:0.721, val_acc:0.725]
Epoch [95/120    avg_loss:0.722, val_acc:0.725]
Epoch [96/120    avg_loss:0.744, val_acc:0.725]
Epoch [97/120    avg_loss:0.720, val_acc:0.725]
Epoch [98/120    avg_loss:0.723, val_acc:0.725]
Epoch [99/120    avg_loss:0.711, val_acc:0.725]
Epoch [100/120    avg_loss:0.773, val_acc:0.725]
Epoch [101/120    avg_loss:0.760, val_acc:0.725]
Epoch [102/120    avg_loss:0.746, val_acc:0.725]
Epoch [103/120    avg_loss:0.726, val_acc:0.725]
Epoch [104/120    avg_loss:0.744, val_acc:0.725]
Epoch [105/120    avg_loss:0.716, val_acc:0.725]
Epoch [106/120    avg_loss:0.761, val_acc:0.725]
Epoch [107/120    avg_loss:0.739, val_acc:0.725]
Epoch [108/120    avg_loss:0.742, val_acc:0.725]
Epoch [109/120    avg_loss:0.735, val_acc:0.725]
Epoch [110/120    avg_loss:0.758, val_acc:0.725]
Epoch [111/120    avg_loss:0.733, val_acc:0.725]
Epoch [112/120    avg_loss:0.724, val_acc:0.725]
Epoch [113/120    avg_loss:0.711, val_acc:0.725]
Epoch [114/120    avg_loss:0.774, val_acc:0.725]
Epoch [115/120    avg_loss:0.753, val_acc:0.725]
Epoch [116/120    avg_loss:0.731, val_acc:0.725]
Epoch [117/120    avg_loss:0.758, val_acc:0.725]
Epoch [118/120    avg_loss:0.735, val_acc:0.725]
Epoch [119/120    avg_loss:0.723, val_acc:0.725]
Epoch [120/120    avg_loss:0.727, val_acc:0.725]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4384    17    17   762     0   256   176   481   339]
 [    0     0 16020     0    22     0  2045     0     3     0]
 [    0    61    70  1489   100     0    56     0   214    46]
 [    0    92   560    20  1680     0   501     0    99    20]
 [    0     0     0     5     0  1300     0     0     0     0]
 [    0     0  1601     7   192     0  2883     0   195     0]
 [    0   112     0     0     5     0     3  1153     0    17]
 [    0   245     2   180   115     0   137     0  2864    28]
 [    0    28     0    11    27   185     0     4     3   661]]

Accuracy:
78.16740173041237

F1 scores:
[       nan 0.77223886 0.88118812 0.79096946 0.57191489 0.93189964
 0.53592341 0.87914602 0.77092867 0.65123153]

Kappa:
0.7109047643642142
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16bca79978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.949, val_acc:0.219]
Epoch [2/120    avg_loss:1.569, val_acc:0.311]
Epoch [3/120    avg_loss:1.335, val_acc:0.461]
Epoch [4/120    avg_loss:1.101, val_acc:0.516]
Epoch [5/120    avg_loss:0.958, val_acc:0.630]
Epoch [6/120    avg_loss:0.797, val_acc:0.715]
Epoch [7/120    avg_loss:0.653, val_acc:0.749]
Epoch [8/120    avg_loss:0.564, val_acc:0.769]
Epoch [9/120    avg_loss:0.493, val_acc:0.795]
Epoch [10/120    avg_loss:0.404, val_acc:0.850]
Epoch [11/120    avg_loss:0.380, val_acc:0.862]
Epoch [12/120    avg_loss:0.339, val_acc:0.882]
Epoch [13/120    avg_loss:0.273, val_acc:0.893]
Epoch [14/120    avg_loss:0.261, val_acc:0.908]
Epoch [15/120    avg_loss:0.240, val_acc:0.879]
Epoch [16/120    avg_loss:0.216, val_acc:0.902]
Epoch [17/120    avg_loss:0.207, val_acc:0.903]
Epoch [18/120    avg_loss:0.244, val_acc:0.894]
Epoch [19/120    avg_loss:0.232, val_acc:0.932]
Epoch [20/120    avg_loss:0.187, val_acc:0.918]
Epoch [21/120    avg_loss:0.163, val_acc:0.936]
Epoch [22/120    avg_loss:0.121, val_acc:0.955]
Epoch [23/120    avg_loss:0.127, val_acc:0.928]
Epoch [24/120    avg_loss:0.183, val_acc:0.949]
Epoch [25/120    avg_loss:0.133, val_acc:0.949]
Epoch [26/120    avg_loss:0.131, val_acc:0.958]
Epoch [27/120    avg_loss:0.118, val_acc:0.924]
Epoch [28/120    avg_loss:0.098, val_acc:0.958]
Epoch [29/120    avg_loss:0.092, val_acc:0.955]
Epoch [30/120    avg_loss:0.079, val_acc:0.964]
Epoch [31/120    avg_loss:0.079, val_acc:0.969]
Epoch [32/120    avg_loss:0.059, val_acc:0.964]
Epoch [33/120    avg_loss:0.076, val_acc:0.969]
Epoch [34/120    avg_loss:0.077, val_acc:0.940]
Epoch [35/120    avg_loss:0.073, val_acc:0.960]
Epoch [36/120    avg_loss:0.058, val_acc:0.969]
Epoch [37/120    avg_loss:0.049, val_acc:0.973]
Epoch [38/120    avg_loss:0.042, val_acc:0.977]
Epoch [39/120    avg_loss:0.051, val_acc:0.973]
Epoch [40/120    avg_loss:0.048, val_acc:0.943]
Epoch [41/120    avg_loss:0.053, val_acc:0.981]
Epoch [42/120    avg_loss:0.033, val_acc:0.975]
Epoch [43/120    avg_loss:0.039, val_acc:0.976]
Epoch [44/120    avg_loss:0.054, val_acc:0.982]
Epoch [45/120    avg_loss:0.038, val_acc:0.978]
Epoch [46/120    avg_loss:0.028, val_acc:0.986]
Epoch [47/120    avg_loss:0.023, val_acc:0.985]
Epoch [48/120    avg_loss:0.018, val_acc:0.984]
Epoch [49/120    avg_loss:0.043, val_acc:0.980]
Epoch [50/120    avg_loss:0.035, val_acc:0.979]
Epoch [51/120    avg_loss:0.023, val_acc:0.986]
Epoch [52/120    avg_loss:0.033, val_acc:0.975]
Epoch [53/120    avg_loss:0.028, val_acc:0.985]
Epoch [54/120    avg_loss:0.022, val_acc:0.988]
Epoch [55/120    avg_loss:0.023, val_acc:0.989]
Epoch [56/120    avg_loss:0.035, val_acc:0.979]
Epoch [57/120    avg_loss:0.024, val_acc:0.991]
Epoch [58/120    avg_loss:0.035, val_acc:0.986]
Epoch [59/120    avg_loss:0.027, val_acc:0.988]
Epoch [60/120    avg_loss:0.019, val_acc:0.985]
Epoch [61/120    avg_loss:0.014, val_acc:0.984]
Epoch [62/120    avg_loss:0.013, val_acc:0.994]
Epoch [63/120    avg_loss:0.024, val_acc:0.981]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.013, val_acc:0.987]
Epoch [66/120    avg_loss:0.008, val_acc:0.993]
Epoch [67/120    avg_loss:0.008, val_acc:0.979]
Epoch [68/120    avg_loss:0.020, val_acc:0.991]
Epoch [69/120    avg_loss:0.009, val_acc:0.992]
Epoch [70/120    avg_loss:0.010, val_acc:0.985]
Epoch [71/120    avg_loss:0.008, val_acc:0.992]
Epoch [72/120    avg_loss:0.009, val_acc:0.990]
Epoch [73/120    avg_loss:0.019, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.975]
Epoch [75/120    avg_loss:0.010, val_acc:0.992]
Epoch [76/120    avg_loss:0.012, val_acc:0.992]
Epoch [77/120    avg_loss:0.007, val_acc:0.994]
Epoch [78/120    avg_loss:0.008, val_acc:0.994]
Epoch [79/120    avg_loss:0.007, val_acc:0.993]
Epoch [80/120    avg_loss:0.007, val_acc:0.993]
Epoch [81/120    avg_loss:0.007, val_acc:0.993]
Epoch [82/120    avg_loss:0.005, val_acc:0.993]
Epoch [83/120    avg_loss:0.005, val_acc:0.993]
Epoch [84/120    avg_loss:0.006, val_acc:0.993]
Epoch [85/120    avg_loss:0.006, val_acc:0.993]
Epoch [86/120    avg_loss:0.006, val_acc:0.993]
Epoch [87/120    avg_loss:0.005, val_acc:0.993]
Epoch [88/120    avg_loss:0.006, val_acc:0.994]
Epoch [89/120    avg_loss:0.005, val_acc:0.993]
Epoch [90/120    avg_loss:0.006, val_acc:0.994]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.994]
Epoch [93/120    avg_loss:0.006, val_acc:0.994]
Epoch [94/120    avg_loss:0.005, val_acc:0.993]
Epoch [95/120    avg_loss:0.004, val_acc:0.995]
Epoch [96/120    avg_loss:0.006, val_acc:0.994]
Epoch [97/120    avg_loss:0.005, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.995]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.009, val_acc:0.994]
Epoch [101/120    avg_loss:0.005, val_acc:0.993]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.006, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.005, val_acc:0.994]
Epoch [106/120    avg_loss:0.004, val_acc:0.994]
Epoch [107/120    avg_loss:0.005, val_acc:0.993]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.993]
Epoch [110/120    avg_loss:0.004, val_acc:0.993]
Epoch [111/120    avg_loss:0.004, val_acc:0.993]
Epoch [112/120    avg_loss:0.006, val_acc:0.993]
Epoch [113/120    avg_loss:0.005, val_acc:0.993]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.007, val_acc:0.993]
Epoch [116/120    avg_loss:0.004, val_acc:0.993]
Epoch [117/120    avg_loss:0.005, val_acc:0.993]
Epoch [118/120    avg_loss:0.004, val_acc:0.993]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.006, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6388     0    31     6     0     0     7     0     0]
 [    0     0 18028     0    55     0     5     0     2     0]
 [    0     4     0  2012     3     0     0     0    15     2]
 [    0    41    19     0  2884     0     0     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    21     0     0  4851     0     5     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0    15     0     4    56     0     0     0  3484    12]
 [    0     0     1     5    15    90     0     0     0   808]]

Accuracy:
98.92511989974213

F1 scores:
[       nan 0.99192547 0.99770331 0.9793137  0.9627775  0.96666667
 0.99671255 0.99613003 0.9807178  0.9266055 ]

Kappa:
0.98576405892203
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ebf23d9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.956, val_acc:0.569]
Epoch [2/120    avg_loss:1.539, val_acc:0.642]
Epoch [3/120    avg_loss:1.208, val_acc:0.752]
Epoch [4/120    avg_loss:0.926, val_acc:0.714]
Epoch [5/120    avg_loss:0.735, val_acc:0.724]
Epoch [6/120    avg_loss:0.650, val_acc:0.786]
Epoch [7/120    avg_loss:0.619, val_acc:0.817]
Epoch [8/120    avg_loss:0.456, val_acc:0.847]
Epoch [9/120    avg_loss:0.412, val_acc:0.840]
Epoch [10/120    avg_loss:0.387, val_acc:0.826]
Epoch [11/120    avg_loss:0.376, val_acc:0.842]
Epoch [12/120    avg_loss:0.387, val_acc:0.791]
Epoch [13/120    avg_loss:0.324, val_acc:0.806]
Epoch [14/120    avg_loss:0.299, val_acc:0.898]
Epoch [15/120    avg_loss:0.213, val_acc:0.900]
Epoch [16/120    avg_loss:0.227, val_acc:0.909]
Epoch [17/120    avg_loss:0.196, val_acc:0.922]
Epoch [18/120    avg_loss:0.164, val_acc:0.956]
Epoch [19/120    avg_loss:0.161, val_acc:0.940]
Epoch [20/120    avg_loss:0.197, val_acc:0.946]
Epoch [21/120    avg_loss:0.158, val_acc:0.924]
Epoch [22/120    avg_loss:0.152, val_acc:0.937]
Epoch [23/120    avg_loss:0.140, val_acc:0.962]
Epoch [24/120    avg_loss:0.177, val_acc:0.951]
Epoch [25/120    avg_loss:0.171, val_acc:0.939]
Epoch [26/120    avg_loss:0.111, val_acc:0.943]
Epoch [27/120    avg_loss:0.150, val_acc:0.884]
Epoch [28/120    avg_loss:0.109, val_acc:0.957]
Epoch [29/120    avg_loss:0.081, val_acc:0.973]
Epoch [30/120    avg_loss:0.078, val_acc:0.956]
Epoch [31/120    avg_loss:0.085, val_acc:0.954]
Epoch [32/120    avg_loss:0.082, val_acc:0.960]
Epoch [33/120    avg_loss:0.072, val_acc:0.975]
Epoch [34/120    avg_loss:0.057, val_acc:0.974]
Epoch [35/120    avg_loss:0.050, val_acc:0.972]
Epoch [36/120    avg_loss:0.056, val_acc:0.972]
Epoch [37/120    avg_loss:0.065, val_acc:0.977]
Epoch [38/120    avg_loss:0.086, val_acc:0.926]
Epoch [39/120    avg_loss:0.083, val_acc:0.957]
Epoch [40/120    avg_loss:0.072, val_acc:0.980]
Epoch [41/120    avg_loss:0.056, val_acc:0.950]
Epoch [42/120    avg_loss:0.053, val_acc:0.975]
Epoch [43/120    avg_loss:0.037, val_acc:0.984]
Epoch [44/120    avg_loss:0.042, val_acc:0.983]
Epoch [45/120    avg_loss:0.037, val_acc:0.980]
Epoch [46/120    avg_loss:0.045, val_acc:0.974]
Epoch [47/120    avg_loss:0.045, val_acc:0.980]
Epoch [48/120    avg_loss:0.031, val_acc:0.984]
Epoch [49/120    avg_loss:0.029, val_acc:0.984]
Epoch [50/120    avg_loss:0.032, val_acc:0.977]
Epoch [51/120    avg_loss:0.025, val_acc:0.983]
Epoch [52/120    avg_loss:0.031, val_acc:0.982]
Epoch [53/120    avg_loss:0.020, val_acc:0.987]
Epoch [54/120    avg_loss:0.042, val_acc:0.939]
Epoch [55/120    avg_loss:0.033, val_acc:0.978]
Epoch [56/120    avg_loss:0.015, val_acc:0.987]
Epoch [57/120    avg_loss:0.019, val_acc:0.983]
Epoch [58/120    avg_loss:0.030, val_acc:0.978]
Epoch [59/120    avg_loss:0.020, val_acc:0.983]
Epoch [60/120    avg_loss:0.018, val_acc:0.984]
Epoch [61/120    avg_loss:0.014, val_acc:0.989]
Epoch [62/120    avg_loss:0.012, val_acc:0.977]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.055, val_acc:0.970]
Epoch [65/120    avg_loss:0.039, val_acc:0.978]
Epoch [66/120    avg_loss:0.033, val_acc:0.986]
Epoch [67/120    avg_loss:0.023, val_acc:0.989]
Epoch [68/120    avg_loss:0.011, val_acc:0.985]
Epoch [69/120    avg_loss:0.017, val_acc:0.984]
Epoch [70/120    avg_loss:0.045, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.019, val_acc:0.971]
Epoch [73/120    avg_loss:0.018, val_acc:0.990]
Epoch [74/120    avg_loss:0.013, val_acc:0.990]
Epoch [75/120    avg_loss:0.056, val_acc:0.979]
Epoch [76/120    avg_loss:0.024, val_acc:0.983]
Epoch [77/120    avg_loss:0.016, val_acc:0.980]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.019, val_acc:0.986]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.977]
Epoch [82/120    avg_loss:0.042, val_acc:0.977]
Epoch [83/120    avg_loss:0.028, val_acc:0.975]
Epoch [84/120    avg_loss:0.045, val_acc:0.984]
Epoch [85/120    avg_loss:0.019, val_acc:0.978]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.990]
Epoch [88/120    avg_loss:0.009, val_acc:0.989]
Epoch [89/120    avg_loss:0.008, val_acc:0.989]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.053, val_acc:0.967]
Epoch [92/120    avg_loss:0.034, val_acc:0.980]
Epoch [93/120    avg_loss:0.142, val_acc:0.871]
Epoch [94/120    avg_loss:0.180, val_acc:0.969]
Epoch [95/120    avg_loss:0.068, val_acc:0.971]
Epoch [96/120    avg_loss:0.044, val_acc:0.973]
Epoch [97/120    avg_loss:0.031, val_acc:0.981]
Epoch [98/120    avg_loss:0.021, val_acc:0.985]
Epoch [99/120    avg_loss:0.065, val_acc:0.954]
Epoch [100/120    avg_loss:0.037, val_acc:0.976]
Epoch [101/120    avg_loss:0.028, val_acc:0.984]
Epoch [102/120    avg_loss:0.019, val_acc:0.980]
Epoch [103/120    avg_loss:0.020, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.985]
Epoch [107/120    avg_loss:0.016, val_acc:0.986]
Epoch [108/120    avg_loss:0.018, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.012, val_acc:0.985]
Epoch [115/120    avg_loss:0.011, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.015, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6334     0     0     1     0    15    17    65     0]
 [    0     0 18055     0    16     0    19     0     0     0]
 [    0    10     0  1987     2     0     0     0    36     1]
 [    0    32    20     1  2882     0     8     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4874     0     0     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     1     0     9    60     0     0     0  3484    17]
 [    0     0     0     2    14    58     0     0     0   845]]

Accuracy:
98.93958017014918

F1 scores:
[       nan 0.98899211 0.99847919 0.98390691 0.96922818 0.97826087
 0.99510004 0.9922899  0.96979819 0.94784072]

Kappa:
0.9859519484294312
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22f56f48d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.978, val_acc:0.245]
Epoch [2/120    avg_loss:1.525, val_acc:0.396]
Epoch [3/120    avg_loss:1.241, val_acc:0.417]
Epoch [4/120    avg_loss:1.009, val_acc:0.572]
Epoch [5/120    avg_loss:0.837, val_acc:0.730]
Epoch [6/120    avg_loss:0.685, val_acc:0.726]
Epoch [7/120    avg_loss:0.616, val_acc:0.758]
Epoch [8/120    avg_loss:0.493, val_acc:0.736]
Epoch [9/120    avg_loss:0.465, val_acc:0.779]
Epoch [10/120    avg_loss:0.392, val_acc:0.805]
Epoch [11/120    avg_loss:0.325, val_acc:0.818]
Epoch [12/120    avg_loss:0.356, val_acc:0.875]
Epoch [13/120    avg_loss:0.254, val_acc:0.888]
Epoch [14/120    avg_loss:0.221, val_acc:0.935]
Epoch [15/120    avg_loss:0.176, val_acc:0.949]
Epoch [16/120    avg_loss:0.231, val_acc:0.940]
Epoch [17/120    avg_loss:0.233, val_acc:0.944]
Epoch [18/120    avg_loss:0.395, val_acc:0.866]
Epoch [19/120    avg_loss:0.237, val_acc:0.889]
Epoch [20/120    avg_loss:0.858, val_acc:0.557]
Epoch [21/120    avg_loss:0.740, val_acc:0.772]
Epoch [22/120    avg_loss:0.546, val_acc:0.808]
Epoch [23/120    avg_loss:0.483, val_acc:0.830]
Epoch [24/120    avg_loss:0.433, val_acc:0.810]
Epoch [25/120    avg_loss:0.435, val_acc:0.838]
Epoch [26/120    avg_loss:0.424, val_acc:0.860]
Epoch [27/120    avg_loss:0.347, val_acc:0.832]
Epoch [28/120    avg_loss:0.273, val_acc:0.889]
Epoch [29/120    avg_loss:0.248, val_acc:0.887]
Epoch [30/120    avg_loss:0.220, val_acc:0.893]
Epoch [31/120    avg_loss:0.232, val_acc:0.917]
Epoch [32/120    avg_loss:0.224, val_acc:0.900]
Epoch [33/120    avg_loss:0.212, val_acc:0.917]
Epoch [34/120    avg_loss:0.211, val_acc:0.910]
Epoch [35/120    avg_loss:0.203, val_acc:0.905]
Epoch [36/120    avg_loss:0.210, val_acc:0.925]
Epoch [37/120    avg_loss:0.181, val_acc:0.917]
Epoch [38/120    avg_loss:0.181, val_acc:0.926]
Epoch [39/120    avg_loss:0.196, val_acc:0.926]
Epoch [40/120    avg_loss:0.195, val_acc:0.925]
Epoch [41/120    avg_loss:0.185, val_acc:0.927]
Epoch [42/120    avg_loss:0.177, val_acc:0.930]
Epoch [43/120    avg_loss:0.158, val_acc:0.932]
Epoch [44/120    avg_loss:0.178, val_acc:0.932]
Epoch [45/120    avg_loss:0.174, val_acc:0.930]
Epoch [46/120    avg_loss:0.174, val_acc:0.931]
Epoch [47/120    avg_loss:0.167, val_acc:0.931]
Epoch [48/120    avg_loss:0.185, val_acc:0.931]
Epoch [49/120    avg_loss:0.171, val_acc:0.931]
Epoch [50/120    avg_loss:0.186, val_acc:0.931]
Epoch [51/120    avg_loss:0.163, val_acc:0.932]
Epoch [52/120    avg_loss:0.179, val_acc:0.931]
Epoch [53/120    avg_loss:0.180, val_acc:0.933]
Epoch [54/120    avg_loss:0.160, val_acc:0.932]
Epoch [55/120    avg_loss:0.170, val_acc:0.932]
Epoch [56/120    avg_loss:0.165, val_acc:0.932]
Epoch [57/120    avg_loss:0.165, val_acc:0.932]
Epoch [58/120    avg_loss:0.163, val_acc:0.932]
Epoch [59/120    avg_loss:0.172, val_acc:0.932]
Epoch [60/120    avg_loss:0.170, val_acc:0.932]
Epoch [61/120    avg_loss:0.155, val_acc:0.932]
Epoch [62/120    avg_loss:0.175, val_acc:0.932]
Epoch [63/120    avg_loss:0.177, val_acc:0.932]
Epoch [64/120    avg_loss:0.180, val_acc:0.932]
Epoch [65/120    avg_loss:0.159, val_acc:0.932]
Epoch [66/120    avg_loss:0.159, val_acc:0.932]
Epoch [67/120    avg_loss:0.163, val_acc:0.932]
Epoch [68/120    avg_loss:0.166, val_acc:0.932]
Epoch [69/120    avg_loss:0.179, val_acc:0.932]
Epoch [70/120    avg_loss:0.169, val_acc:0.932]
Epoch [71/120    avg_loss:0.163, val_acc:0.932]
Epoch [72/120    avg_loss:0.173, val_acc:0.932]
Epoch [73/120    avg_loss:0.164, val_acc:0.932]
Epoch [74/120    avg_loss:0.169, val_acc:0.932]
Epoch [75/120    avg_loss:0.158, val_acc:0.932]
Epoch [76/120    avg_loss:0.169, val_acc:0.932]
Epoch [77/120    avg_loss:0.171, val_acc:0.932]
Epoch [78/120    avg_loss:0.179, val_acc:0.932]
Epoch [79/120    avg_loss:0.166, val_acc:0.932]
Epoch [80/120    avg_loss:0.182, val_acc:0.932]
Epoch [81/120    avg_loss:0.175, val_acc:0.932]
Epoch [82/120    avg_loss:0.170, val_acc:0.932]
Epoch [83/120    avg_loss:0.174, val_acc:0.932]
Epoch [84/120    avg_loss:0.163, val_acc:0.932]
Epoch [85/120    avg_loss:0.160, val_acc:0.932]
Epoch [86/120    avg_loss:0.174, val_acc:0.932]
Epoch [87/120    avg_loss:0.167, val_acc:0.932]
Epoch [88/120    avg_loss:0.163, val_acc:0.932]
Epoch [89/120    avg_loss:0.162, val_acc:0.932]
Epoch [90/120    avg_loss:0.171, val_acc:0.932]
Epoch [91/120    avg_loss:0.174, val_acc:0.932]
Epoch [92/120    avg_loss:0.163, val_acc:0.932]
Epoch [93/120    avg_loss:0.175, val_acc:0.932]
Epoch [94/120    avg_loss:0.177, val_acc:0.932]
Epoch [95/120    avg_loss:0.173, val_acc:0.932]
Epoch [96/120    avg_loss:0.170, val_acc:0.932]
Epoch [97/120    avg_loss:0.185, val_acc:0.932]
Epoch [98/120    avg_loss:0.168, val_acc:0.932]
Epoch [99/120    avg_loss:0.184, val_acc:0.932]
Epoch [100/120    avg_loss:0.166, val_acc:0.932]
Epoch [101/120    avg_loss:0.175, val_acc:0.932]
Epoch [102/120    avg_loss:0.165, val_acc:0.932]
Epoch [103/120    avg_loss:0.155, val_acc:0.932]
Epoch [104/120    avg_loss:0.169, val_acc:0.932]
Epoch [105/120    avg_loss:0.174, val_acc:0.932]
Epoch [106/120    avg_loss:0.169, val_acc:0.932]
Epoch [107/120    avg_loss:0.175, val_acc:0.932]
Epoch [108/120    avg_loss:0.162, val_acc:0.932]
Epoch [109/120    avg_loss:0.167, val_acc:0.932]
Epoch [110/120    avg_loss:0.174, val_acc:0.932]
Epoch [111/120    avg_loss:0.164, val_acc:0.932]
Epoch [112/120    avg_loss:0.162, val_acc:0.932]
Epoch [113/120    avg_loss:0.167, val_acc:0.932]
Epoch [114/120    avg_loss:0.169, val_acc:0.932]
Epoch [115/120    avg_loss:0.182, val_acc:0.932]
Epoch [116/120    avg_loss:0.171, val_acc:0.932]
Epoch [117/120    avg_loss:0.153, val_acc:0.932]
Epoch [118/120    avg_loss:0.174, val_acc:0.932]
Epoch [119/120    avg_loss:0.163, val_acc:0.932]
Epoch [120/120    avg_loss:0.176, val_acc:0.932]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5777     0     7   116     0    24     1   390   117]
 [    0     0 15125     0   373     0  2574     0    18     0]
 [    0    16     0  1912     0     0     0     0    80    28]
 [    0   145    51     0  2709     0    33     0    34     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0    11     0  4850     0    15     1]
 [    0    11     0     0     0     0     1  1272     0     6]
 [    0    38     0    32    52     0     0     0  3449     0]
 [    0    13     0     5    19    69     0     1    10   802]]

Accuracy:
89.65608656881884

F1 scores:
[       nan 0.9293758  0.90930953 0.95791583 0.86660269 0.97424412
 0.78478964 0.99219969 0.9115898  0.85638014]

Kappa:
0.8670702059160932
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e4dad6908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.048, val_acc:0.136]
Epoch [2/120    avg_loss:1.547, val_acc:0.647]
Epoch [3/120    avg_loss:1.274, val_acc:0.682]
Epoch [4/120    avg_loss:1.069, val_acc:0.727]
Epoch [5/120    avg_loss:0.868, val_acc:0.754]
Epoch [6/120    avg_loss:0.737, val_acc:0.762]
Epoch [7/120    avg_loss:0.615, val_acc:0.751]
Epoch [8/120    avg_loss:0.532, val_acc:0.776]
Epoch [9/120    avg_loss:0.461, val_acc:0.829]
Epoch [10/120    avg_loss:0.366, val_acc:0.877]
Epoch [11/120    avg_loss:0.328, val_acc:0.833]
Epoch [12/120    avg_loss:0.286, val_acc:0.911]
Epoch [13/120    avg_loss:0.250, val_acc:0.895]
Epoch [14/120    avg_loss:0.259, val_acc:0.866]
Epoch [15/120    avg_loss:0.246, val_acc:0.894]
Epoch [16/120    avg_loss:0.214, val_acc:0.926]
Epoch [17/120    avg_loss:0.184, val_acc:0.929]
Epoch [18/120    avg_loss:0.151, val_acc:0.928]
Epoch [19/120    avg_loss:0.157, val_acc:0.913]
Epoch [20/120    avg_loss:0.186, val_acc:0.937]
Epoch [21/120    avg_loss:0.151, val_acc:0.960]
Epoch [22/120    avg_loss:0.137, val_acc:0.948]
Epoch [23/120    avg_loss:0.135, val_acc:0.897]
Epoch [24/120    avg_loss:0.106, val_acc:0.958]
Epoch [25/120    avg_loss:0.086, val_acc:0.932]
Epoch [26/120    avg_loss:0.100, val_acc:0.937]
Epoch [27/120    avg_loss:0.106, val_acc:0.956]
Epoch [28/120    avg_loss:0.070, val_acc:0.968]
Epoch [29/120    avg_loss:0.100, val_acc:0.967]
Epoch [30/120    avg_loss:0.081, val_acc:0.973]
Epoch [31/120    avg_loss:0.063, val_acc:0.978]
Epoch [32/120    avg_loss:0.121, val_acc:0.939]
Epoch [33/120    avg_loss:0.082, val_acc:0.965]
Epoch [34/120    avg_loss:0.060, val_acc:0.973]
Epoch [35/120    avg_loss:0.059, val_acc:0.937]
Epoch [36/120    avg_loss:0.061, val_acc:0.977]
Epoch [37/120    avg_loss:0.062, val_acc:0.924]
Epoch [38/120    avg_loss:0.059, val_acc:0.971]
Epoch [39/120    avg_loss:0.046, val_acc:0.979]
Epoch [40/120    avg_loss:0.058, val_acc:0.977]
Epoch [41/120    avg_loss:0.038, val_acc:0.975]
Epoch [42/120    avg_loss:0.040, val_acc:0.978]
Epoch [43/120    avg_loss:0.031, val_acc:0.975]
Epoch [44/120    avg_loss:0.055, val_acc:0.954]
Epoch [45/120    avg_loss:0.049, val_acc:0.972]
Epoch [46/120    avg_loss:0.034, val_acc:0.980]
Epoch [47/120    avg_loss:0.045, val_acc:0.968]
Epoch [48/120    avg_loss:0.042, val_acc:0.985]
Epoch [49/120    avg_loss:0.041, val_acc:0.984]
Epoch [50/120    avg_loss:0.054, val_acc:0.970]
Epoch [51/120    avg_loss:0.037, val_acc:0.982]
Epoch [52/120    avg_loss:0.073, val_acc:0.976]
Epoch [53/120    avg_loss:0.048, val_acc:0.978]
Epoch [54/120    avg_loss:0.042, val_acc:0.978]
Epoch [55/120    avg_loss:0.024, val_acc:0.981]
Epoch [56/120    avg_loss:0.019, val_acc:0.983]
Epoch [57/120    avg_loss:0.028, val_acc:0.984]
Epoch [58/120    avg_loss:0.021, val_acc:0.983]
Epoch [59/120    avg_loss:0.035, val_acc:0.980]
Epoch [60/120    avg_loss:0.036, val_acc:0.964]
Epoch [61/120    avg_loss:0.039, val_acc:0.977]
Epoch [62/120    avg_loss:0.036, val_acc:0.981]
Epoch [63/120    avg_loss:0.027, val_acc:0.983]
Epoch [64/120    avg_loss:0.018, val_acc:0.983]
Epoch [65/120    avg_loss:0.020, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.986]
Epoch [67/120    avg_loss:0.017, val_acc:0.985]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.014, val_acc:0.985]
Epoch [71/120    avg_loss:0.013, val_acc:0.984]
Epoch [72/120    avg_loss:0.015, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.010, val_acc:0.987]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.018, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.011, val_acc:0.987]
Epoch [84/120    avg_loss:0.013, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.018, val_acc:0.990]
Epoch [88/120    avg_loss:0.011, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.989]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.014, val_acc:0.991]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.015, val_acc:0.988]
Epoch [99/120    avg_loss:0.013, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.009, val_acc:0.990]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.989]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     1     0     0     1    10    51     5]
 [    0     2 18013     0    32     0    43     0     0     0]
 [    0     3     0  1890     0     0     0     0   141     2]
 [    0     6     5     0  2941     1    11     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    22     0     0     0     0     0  1267     1     0]
 [    0    11     0    70    33     0     3     0  3454     0]
 [    0     2     0     0     7     7     0     0     0   903]]

Accuracy:
98.84799845757117

F1 scores:
[       nan 0.99112288 0.99772904 0.94570928 0.98279031 0.99694423
 0.99409008 0.98714453 0.95638931 0.98580786]

Kappa:
0.98474601997441
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53208a2940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.930, val_acc:0.459]
Epoch [2/120    avg_loss:1.474, val_acc:0.627]
Epoch [3/120    avg_loss:1.194, val_acc:0.706]
Epoch [4/120    avg_loss:0.980, val_acc:0.667]
Epoch [5/120    avg_loss:0.746, val_acc:0.632]
Epoch [6/120    avg_loss:0.590, val_acc:0.715]
Epoch [7/120    avg_loss:0.505, val_acc:0.756]
Epoch [8/120    avg_loss:0.466, val_acc:0.805]
Epoch [9/120    avg_loss:0.383, val_acc:0.805]
Epoch [10/120    avg_loss:0.324, val_acc:0.795]
Epoch [11/120    avg_loss:0.295, val_acc:0.871]
Epoch [12/120    avg_loss:0.265, val_acc:0.872]
Epoch [13/120    avg_loss:0.272, val_acc:0.863]
Epoch [14/120    avg_loss:0.273, val_acc:0.807]
Epoch [15/120    avg_loss:0.231, val_acc:0.919]
Epoch [16/120    avg_loss:0.208, val_acc:0.878]
Epoch [17/120    avg_loss:0.178, val_acc:0.884]
Epoch [18/120    avg_loss:0.184, val_acc:0.922]
Epoch [19/120    avg_loss:0.181, val_acc:0.891]
Epoch [20/120    avg_loss:0.186, val_acc:0.900]
Epoch [21/120    avg_loss:0.144, val_acc:0.928]
Epoch [22/120    avg_loss:0.111, val_acc:0.929]
Epoch [23/120    avg_loss:0.150, val_acc:0.949]
Epoch [24/120    avg_loss:0.142, val_acc:0.945]
Epoch [25/120    avg_loss:0.117, val_acc:0.916]
Epoch [26/120    avg_loss:0.138, val_acc:0.935]
Epoch [27/120    avg_loss:0.092, val_acc:0.960]
Epoch [28/120    avg_loss:0.081, val_acc:0.965]
Epoch [29/120    avg_loss:0.079, val_acc:0.943]
Epoch [30/120    avg_loss:0.105, val_acc:0.918]
Epoch [31/120    avg_loss:0.080, val_acc:0.955]
Epoch [32/120    avg_loss:0.071, val_acc:0.954]
Epoch [33/120    avg_loss:0.069, val_acc:0.949]
Epoch [34/120    avg_loss:0.067, val_acc:0.948]
Epoch [35/120    avg_loss:0.063, val_acc:0.957]
Epoch [36/120    avg_loss:0.105, val_acc:0.905]
Epoch [37/120    avg_loss:0.096, val_acc:0.957]
Epoch [38/120    avg_loss:0.088, val_acc:0.931]
Epoch [39/120    avg_loss:0.052, val_acc:0.947]
Epoch [40/120    avg_loss:0.076, val_acc:0.952]
Epoch [41/120    avg_loss:0.059, val_acc:0.961]
Epoch [42/120    avg_loss:0.050, val_acc:0.965]
Epoch [43/120    avg_loss:0.041, val_acc:0.967]
Epoch [44/120    avg_loss:0.034, val_acc:0.967]
Epoch [45/120    avg_loss:0.029, val_acc:0.966]
Epoch [46/120    avg_loss:0.034, val_acc:0.964]
Epoch [47/120    avg_loss:0.028, val_acc:0.968]
Epoch [48/120    avg_loss:0.023, val_acc:0.966]
Epoch [49/120    avg_loss:0.023, val_acc:0.967]
Epoch [50/120    avg_loss:0.025, val_acc:0.968]
Epoch [51/120    avg_loss:0.027, val_acc:0.966]
Epoch [52/120    avg_loss:0.022, val_acc:0.965]
Epoch [53/120    avg_loss:0.019, val_acc:0.968]
Epoch [54/120    avg_loss:0.027, val_acc:0.964]
Epoch [55/120    avg_loss:0.021, val_acc:0.967]
Epoch [56/120    avg_loss:0.019, val_acc:0.969]
Epoch [57/120    avg_loss:0.021, val_acc:0.969]
Epoch [58/120    avg_loss:0.029, val_acc:0.964]
Epoch [59/120    avg_loss:0.017, val_acc:0.971]
Epoch [60/120    avg_loss:0.021, val_acc:0.973]
Epoch [61/120    avg_loss:0.019, val_acc:0.970]
Epoch [62/120    avg_loss:0.018, val_acc:0.972]
Epoch [63/120    avg_loss:0.020, val_acc:0.970]
Epoch [64/120    avg_loss:0.017, val_acc:0.969]
Epoch [65/120    avg_loss:0.019, val_acc:0.969]
Epoch [66/120    avg_loss:0.018, val_acc:0.969]
Epoch [67/120    avg_loss:0.016, val_acc:0.969]
Epoch [68/120    avg_loss:0.016, val_acc:0.968]
Epoch [69/120    avg_loss:0.020, val_acc:0.973]
Epoch [70/120    avg_loss:0.017, val_acc:0.971]
Epoch [71/120    avg_loss:0.019, val_acc:0.972]
Epoch [72/120    avg_loss:0.019, val_acc:0.973]
Epoch [73/120    avg_loss:0.016, val_acc:0.971]
Epoch [74/120    avg_loss:0.018, val_acc:0.972]
Epoch [75/120    avg_loss:0.016, val_acc:0.970]
Epoch [76/120    avg_loss:0.018, val_acc:0.972]
Epoch [77/120    avg_loss:0.013, val_acc:0.973]
Epoch [78/120    avg_loss:0.015, val_acc:0.973]
Epoch [79/120    avg_loss:0.016, val_acc:0.973]
Epoch [80/120    avg_loss:0.016, val_acc:0.971]
Epoch [81/120    avg_loss:0.019, val_acc:0.970]
Epoch [82/120    avg_loss:0.020, val_acc:0.974]
Epoch [83/120    avg_loss:0.015, val_acc:0.973]
Epoch [84/120    avg_loss:0.015, val_acc:0.974]
Epoch [85/120    avg_loss:0.016, val_acc:0.972]
Epoch [86/120    avg_loss:0.016, val_acc:0.972]
Epoch [87/120    avg_loss:0.013, val_acc:0.971]
Epoch [88/120    avg_loss:0.015, val_acc:0.973]
Epoch [89/120    avg_loss:0.015, val_acc:0.973]
Epoch [90/120    avg_loss:0.013, val_acc:0.972]
Epoch [91/120    avg_loss:0.017, val_acc:0.970]
Epoch [92/120    avg_loss:0.016, val_acc:0.973]
Epoch [93/120    avg_loss:0.018, val_acc:0.970]
Epoch [94/120    avg_loss:0.013, val_acc:0.972]
Epoch [95/120    avg_loss:0.014, val_acc:0.973]
Epoch [96/120    avg_loss:0.014, val_acc:0.975]
Epoch [97/120    avg_loss:0.021, val_acc:0.976]
Epoch [98/120    avg_loss:0.014, val_acc:0.970]
Epoch [99/120    avg_loss:0.014, val_acc:0.971]
Epoch [100/120    avg_loss:0.014, val_acc:0.972]
Epoch [101/120    avg_loss:0.014, val_acc:0.976]
Epoch [102/120    avg_loss:0.014, val_acc:0.973]
Epoch [103/120    avg_loss:0.012, val_acc:0.973]
Epoch [104/120    avg_loss:0.013, val_acc:0.972]
Epoch [105/120    avg_loss:0.013, val_acc:0.974]
Epoch [106/120    avg_loss:0.013, val_acc:0.972]
Epoch [107/120    avg_loss:0.013, val_acc:0.974]
Epoch [108/120    avg_loss:0.012, val_acc:0.973]
Epoch [109/120    avg_loss:0.013, val_acc:0.976]
Epoch [110/120    avg_loss:0.013, val_acc:0.973]
Epoch [111/120    avg_loss:0.012, val_acc:0.972]
Epoch [112/120    avg_loss:0.013, val_acc:0.975]
Epoch [113/120    avg_loss:0.011, val_acc:0.972]
Epoch [114/120    avg_loss:0.011, val_acc:0.973]
Epoch [115/120    avg_loss:0.011, val_acc:0.972]
Epoch [116/120    avg_loss:0.013, val_acc:0.975]
Epoch [117/120    avg_loss:0.013, val_acc:0.975]
Epoch [118/120    avg_loss:0.014, val_acc:0.974]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.009, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     3     3     0     0     0    74    13]
 [    0     1 18043     0    29     0    15     0     2     0]
 [    0     1     0  1958     0     0     0     0    76     1]
 [    0     7    12     0  2905     0    28     0    17     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    83     0     0     0  4781     0    14     0]
 [    0    22     0     0     0     3     1  1261     2     1]
 [    0    16     0    44    13     0    26     0  3471     1]
 [    0     0     0     0    15    18     0     0     0   886]]

Accuracy:
98.68893548309353

F1 scores:
[       nan 0.98907786 0.99608038 0.96906706 0.97860872 0.99201824
 0.98283482 0.98863191 0.96056455 0.97149123]

Kappa:
0.982619075560322
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd14cbcf898>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.965, val_acc:0.153]
Epoch [2/120    avg_loss:1.542, val_acc:0.435]
Epoch [3/120    avg_loss:1.274, val_acc:0.503]
Epoch [4/120    avg_loss:1.068, val_acc:0.478]
Epoch [5/120    avg_loss:0.870, val_acc:0.592]
Epoch [6/120    avg_loss:0.755, val_acc:0.621]
Epoch [7/120    avg_loss:0.617, val_acc:0.642]
Epoch [8/120    avg_loss:0.518, val_acc:0.695]
Epoch [9/120    avg_loss:0.451, val_acc:0.758]
Epoch [10/120    avg_loss:0.391, val_acc:0.772]
Epoch [11/120    avg_loss:0.350, val_acc:0.749]
Epoch [12/120    avg_loss:0.306, val_acc:0.794]
Epoch [13/120    avg_loss:0.290, val_acc:0.754]
Epoch [14/120    avg_loss:0.273, val_acc:0.873]
Epoch [15/120    avg_loss:0.246, val_acc:0.797]
Epoch [16/120    avg_loss:0.192, val_acc:0.917]
Epoch [17/120    avg_loss:0.183, val_acc:0.898]
Epoch [18/120    avg_loss:0.151, val_acc:0.939]
Epoch [19/120    avg_loss:0.145, val_acc:0.944]
Epoch [20/120    avg_loss:0.125, val_acc:0.892]
Epoch [21/120    avg_loss:0.156, val_acc:0.924]
Epoch [22/120    avg_loss:0.126, val_acc:0.924]
Epoch [23/120    avg_loss:0.120, val_acc:0.943]
Epoch [24/120    avg_loss:0.120, val_acc:0.930]
Epoch [25/120    avg_loss:0.075, val_acc:0.952]
Epoch [26/120    avg_loss:0.069, val_acc:0.962]
Epoch [27/120    avg_loss:0.072, val_acc:0.965]
Epoch [28/120    avg_loss:0.065, val_acc:0.939]
Epoch [29/120    avg_loss:0.077, val_acc:0.878]
Epoch [30/120    avg_loss:0.071, val_acc:0.950]
Epoch [31/120    avg_loss:0.068, val_acc:0.953]
Epoch [32/120    avg_loss:0.045, val_acc:0.964]
Epoch [33/120    avg_loss:0.033, val_acc:0.951]
Epoch [34/120    avg_loss:0.066, val_acc:0.965]
Epoch [35/120    avg_loss:0.049, val_acc:0.972]
Epoch [36/120    avg_loss:0.030, val_acc:0.957]
Epoch [37/120    avg_loss:0.099, val_acc:0.893]
Epoch [38/120    avg_loss:0.096, val_acc:0.920]
Epoch [39/120    avg_loss:0.088, val_acc:0.942]
Epoch [40/120    avg_loss:0.046, val_acc:0.958]
Epoch [41/120    avg_loss:0.127, val_acc:0.930]
Epoch [42/120    avg_loss:0.065, val_acc:0.968]
Epoch [43/120    avg_loss:0.035, val_acc:0.966]
Epoch [44/120    avg_loss:0.033, val_acc:0.959]
Epoch [45/120    avg_loss:0.052, val_acc:0.938]
Epoch [46/120    avg_loss:0.045, val_acc:0.939]
Epoch [47/120    avg_loss:0.053, val_acc:0.965]
Epoch [48/120    avg_loss:0.021, val_acc:0.973]
Epoch [49/120    avg_loss:0.040, val_acc:0.971]
Epoch [50/120    avg_loss:0.030, val_acc:0.976]
Epoch [51/120    avg_loss:0.025, val_acc:0.968]
Epoch [52/120    avg_loss:0.028, val_acc:0.970]
Epoch [53/120    avg_loss:0.021, val_acc:0.966]
Epoch [54/120    avg_loss:0.022, val_acc:0.974]
Epoch [55/120    avg_loss:0.015, val_acc:0.973]
Epoch [56/120    avg_loss:0.035, val_acc:0.968]
Epoch [57/120    avg_loss:0.019, val_acc:0.978]
Epoch [58/120    avg_loss:0.014, val_acc:0.977]
Epoch [59/120    avg_loss:0.017, val_acc:0.979]
Epoch [60/120    avg_loss:0.028, val_acc:0.981]
Epoch [61/120    avg_loss:0.013, val_acc:0.981]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.029, val_acc:0.980]
Epoch [64/120    avg_loss:0.028, val_acc:0.979]
Epoch [65/120    avg_loss:0.016, val_acc:0.978]
Epoch [66/120    avg_loss:0.023, val_acc:0.979]
Epoch [67/120    avg_loss:0.022, val_acc:0.925]
Epoch [68/120    avg_loss:0.015, val_acc:0.982]
Epoch [69/120    avg_loss:0.045, val_acc:0.971]
Epoch [70/120    avg_loss:0.019, val_acc:0.983]
Epoch [71/120    avg_loss:0.020, val_acc:0.973]
Epoch [72/120    avg_loss:0.021, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.982]
Epoch [75/120    avg_loss:0.011, val_acc:0.973]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.007, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.983]
Epoch [79/120    avg_loss:0.009, val_acc:0.979]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.978]
Epoch [82/120    avg_loss:0.019, val_acc:0.959]
Epoch [83/120    avg_loss:0.019, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.974]
Epoch [89/120    avg_loss:0.006, val_acc:0.980]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.004, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.975]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.984]
Epoch [109/120    avg_loss:0.003, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.003, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     0     1     0     4     4    69     4]
 [    0     0 18080     0     3     0     0     0     7     0]
 [    0     4     0  1880     0     0     0     0   150     2]
 [    0    30    13     0  2915     0     5     0     3     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     0     0  4849     0     0     0]
 [    0    17     0     0     0     0     0  1273     0     0]
 [    0    20     0    48    43     0     0     0  3460     0]
 [    0     1     0     0    17     3     0     0     0   898]]

Accuracy:
98.83594823223194

F1 scores:
[       nan 0.98801929 0.99856401 0.94853683 0.97966728 0.99885189
 0.99609696 0.99181924 0.95316804 0.98195735]

Kappa:
0.9845682690633186
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7485c88d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.970, val_acc:0.451]
Epoch [2/120    avg_loss:1.545, val_acc:0.372]
Epoch [3/120    avg_loss:1.275, val_acc:0.403]
Epoch [4/120    avg_loss:1.031, val_acc:0.631]
Epoch [5/120    avg_loss:0.862, val_acc:0.743]
Epoch [6/120    avg_loss:0.674, val_acc:0.755]
Epoch [7/120    avg_loss:0.551, val_acc:0.781]
Epoch [8/120    avg_loss:0.488, val_acc:0.825]
Epoch [9/120    avg_loss:0.415, val_acc:0.809]
Epoch [10/120    avg_loss:0.372, val_acc:0.832]
Epoch [11/120    avg_loss:0.321, val_acc:0.811]
Epoch [12/120    avg_loss:0.289, val_acc:0.870]
Epoch [13/120    avg_loss:0.270, val_acc:0.853]
Epoch [14/120    avg_loss:0.300, val_acc:0.863]
Epoch [15/120    avg_loss:0.236, val_acc:0.867]
Epoch [16/120    avg_loss:0.208, val_acc:0.913]
Epoch [17/120    avg_loss:0.199, val_acc:0.917]
Epoch [18/120    avg_loss:0.219, val_acc:0.893]
Epoch [19/120    avg_loss:0.192, val_acc:0.925]
Epoch [20/120    avg_loss:0.151, val_acc:0.921]
Epoch [21/120    avg_loss:0.164, val_acc:0.926]
Epoch [22/120    avg_loss:0.131, val_acc:0.932]
Epoch [23/120    avg_loss:0.123, val_acc:0.938]
Epoch [24/120    avg_loss:0.148, val_acc:0.938]
Epoch [25/120    avg_loss:0.123, val_acc:0.947]
Epoch [26/120    avg_loss:0.099, val_acc:0.944]
Epoch [27/120    avg_loss:0.090, val_acc:0.948]
Epoch [28/120    avg_loss:0.087, val_acc:0.940]
Epoch [29/120    avg_loss:0.116, val_acc:0.937]
Epoch [30/120    avg_loss:0.107, val_acc:0.932]
Epoch [31/120    avg_loss:0.074, val_acc:0.962]
Epoch [32/120    avg_loss:0.052, val_acc:0.934]
Epoch [33/120    avg_loss:0.071, val_acc:0.963]
Epoch [34/120    avg_loss:0.068, val_acc:0.957]
Epoch [35/120    avg_loss:0.071, val_acc:0.963]
Epoch [36/120    avg_loss:0.073, val_acc:0.948]
Epoch [37/120    avg_loss:0.065, val_acc:0.958]
Epoch [38/120    avg_loss:0.090, val_acc:0.958]
Epoch [39/120    avg_loss:0.060, val_acc:0.967]
Epoch [40/120    avg_loss:0.090, val_acc:0.947]
Epoch [41/120    avg_loss:0.068, val_acc:0.966]
Epoch [42/120    avg_loss:0.049, val_acc:0.959]
Epoch [43/120    avg_loss:0.051, val_acc:0.981]
Epoch [44/120    avg_loss:0.043, val_acc:0.969]
Epoch [45/120    avg_loss:0.052, val_acc:0.974]
Epoch [46/120    avg_loss:0.026, val_acc:0.978]
Epoch [47/120    avg_loss:0.032, val_acc:0.966]
Epoch [48/120    avg_loss:0.053, val_acc:0.975]
Epoch [49/120    avg_loss:0.039, val_acc:0.974]
Epoch [50/120    avg_loss:0.027, val_acc:0.982]
Epoch [51/120    avg_loss:0.033, val_acc:0.981]
Epoch [52/120    avg_loss:0.025, val_acc:0.980]
Epoch [53/120    avg_loss:0.028, val_acc:0.984]
Epoch [54/120    avg_loss:0.013, val_acc:0.983]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.021, val_acc:0.978]
Epoch [57/120    avg_loss:0.027, val_acc:0.973]
Epoch [58/120    avg_loss:0.013, val_acc:0.981]
Epoch [59/120    avg_loss:0.019, val_acc:0.979]
Epoch [60/120    avg_loss:0.019, val_acc:0.979]
Epoch [61/120    avg_loss:0.019, val_acc:0.973]
Epoch [62/120    avg_loss:0.013, val_acc:0.978]
Epoch [63/120    avg_loss:0.017, val_acc:0.978]
Epoch [64/120    avg_loss:0.029, val_acc:0.978]
Epoch [65/120    avg_loss:0.031, val_acc:0.981]
Epoch [66/120    avg_loss:0.015, val_acc:0.962]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.985]
Epoch [70/120    avg_loss:0.011, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.982]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.010, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     0     0     0     0     0    23     3]
 [    0     0 17902     0    92     0    90     0     6     0]
 [    0     1     0  1930     0     0     0     0   104     1]
 [    0    23     2     0  2924     0    14     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     7     0  4869     0     2     0]
 [    0     9     0     0     0     0     0  1277     3     1]
 [    0     4     0    38    33     0     8     0  3487     1]
 [    0     0     0     0     0     8     0     0     0   911]]

Accuracy:
98.83835827729979

F1 scores:
[       nan 0.9951068  0.99472134 0.96403596 0.97013935 0.99694423
 0.98772695 0.99493572 0.96834213 0.99075585]

Kappa:
0.9846367688485342
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32d0d5f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.975, val_acc:0.122]
Epoch [2/120    avg_loss:1.517, val_acc:0.536]
Epoch [3/120    avg_loss:1.260, val_acc:0.602]
Epoch [4/120    avg_loss:0.996, val_acc:0.777]
Epoch [5/120    avg_loss:0.807, val_acc:0.724]
Epoch [6/120    avg_loss:0.704, val_acc:0.650]
Epoch [7/120    avg_loss:0.546, val_acc:0.676]
Epoch [8/120    avg_loss:0.455, val_acc:0.738]
Epoch [9/120    avg_loss:0.420, val_acc:0.842]
Epoch [10/120    avg_loss:0.395, val_acc:0.792]
Epoch [11/120    avg_loss:0.359, val_acc:0.846]
Epoch [12/120    avg_loss:0.289, val_acc:0.840]
Epoch [13/120    avg_loss:0.277, val_acc:0.904]
Epoch [14/120    avg_loss:0.238, val_acc:0.916]
Epoch [15/120    avg_loss:0.287, val_acc:0.910]
Epoch [16/120    avg_loss:0.209, val_acc:0.938]
Epoch [17/120    avg_loss:0.200, val_acc:0.885]
Epoch [18/120    avg_loss:0.168, val_acc:0.939]
Epoch [19/120    avg_loss:0.136, val_acc:0.906]
Epoch [20/120    avg_loss:0.205, val_acc:0.869]
Epoch [21/120    avg_loss:0.123, val_acc:0.939]
Epoch [22/120    avg_loss:0.109, val_acc:0.941]
Epoch [23/120    avg_loss:0.122, val_acc:0.935]
Epoch [24/120    avg_loss:0.117, val_acc:0.932]
Epoch [25/120    avg_loss:0.188, val_acc:0.902]
Epoch [26/120    avg_loss:0.115, val_acc:0.957]
Epoch [27/120    avg_loss:0.100, val_acc:0.955]
Epoch [28/120    avg_loss:0.073, val_acc:0.973]
Epoch [29/120    avg_loss:0.063, val_acc:0.970]
Epoch [30/120    avg_loss:0.040, val_acc:0.939]
Epoch [31/120    avg_loss:0.062, val_acc:0.865]
Epoch [32/120    avg_loss:0.051, val_acc:0.981]
Epoch [33/120    avg_loss:0.059, val_acc:0.968]
Epoch [34/120    avg_loss:0.062, val_acc:0.978]
Epoch [35/120    avg_loss:0.041, val_acc:0.977]
Epoch [36/120    avg_loss:0.048, val_acc:0.949]
Epoch [37/120    avg_loss:0.106, val_acc:0.963]
Epoch [38/120    avg_loss:0.054, val_acc:0.948]
Epoch [39/120    avg_loss:0.036, val_acc:0.980]
Epoch [40/120    avg_loss:0.039, val_acc:0.960]
Epoch [41/120    avg_loss:0.073, val_acc:0.974]
Epoch [42/120    avg_loss:0.028, val_acc:0.978]
Epoch [43/120    avg_loss:0.041, val_acc:0.978]
Epoch [44/120    avg_loss:0.052, val_acc:0.965]
Epoch [45/120    avg_loss:0.027, val_acc:0.990]
Epoch [46/120    avg_loss:0.023, val_acc:0.977]
Epoch [47/120    avg_loss:0.015, val_acc:0.992]
Epoch [48/120    avg_loss:0.019, val_acc:0.978]
Epoch [49/120    avg_loss:0.016, val_acc:0.981]
Epoch [50/120    avg_loss:0.017, val_acc:0.988]
Epoch [51/120    avg_loss:0.024, val_acc:0.978]
Epoch [52/120    avg_loss:0.017, val_acc:0.985]
Epoch [53/120    avg_loss:0.021, val_acc:0.988]
Epoch [54/120    avg_loss:0.039, val_acc:0.981]
Epoch [55/120    avg_loss:0.034, val_acc:0.983]
Epoch [56/120    avg_loss:0.021, val_acc:0.985]
Epoch [57/120    avg_loss:0.013, val_acc:0.987]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.010, val_acc:0.984]
Epoch [60/120    avg_loss:0.018, val_acc:0.988]
Epoch [61/120    avg_loss:0.015, val_acc:0.992]
Epoch [62/120    avg_loss:0.009, val_acc:0.993]
Epoch [63/120    avg_loss:0.008, val_acc:0.993]
Epoch [64/120    avg_loss:0.009, val_acc:0.993]
Epoch [65/120    avg_loss:0.008, val_acc:0.993]
Epoch [66/120    avg_loss:0.009, val_acc:0.993]
Epoch [67/120    avg_loss:0.012, val_acc:0.990]
Epoch [68/120    avg_loss:0.009, val_acc:0.991]
Epoch [69/120    avg_loss:0.008, val_acc:0.992]
Epoch [70/120    avg_loss:0.010, val_acc:0.992]
Epoch [71/120    avg_loss:0.012, val_acc:0.993]
Epoch [72/120    avg_loss:0.009, val_acc:0.993]
Epoch [73/120    avg_loss:0.009, val_acc:0.993]
Epoch [74/120    avg_loss:0.007, val_acc:0.993]
Epoch [75/120    avg_loss:0.009, val_acc:0.994]
Epoch [76/120    avg_loss:0.007, val_acc:0.993]
Epoch [77/120    avg_loss:0.009, val_acc:0.992]
Epoch [78/120    avg_loss:0.006, val_acc:0.993]
Epoch [79/120    avg_loss:0.011, val_acc:0.993]
Epoch [80/120    avg_loss:0.007, val_acc:0.993]
Epoch [81/120    avg_loss:0.008, val_acc:0.993]
Epoch [82/120    avg_loss:0.008, val_acc:0.993]
Epoch [83/120    avg_loss:0.007, val_acc:0.993]
Epoch [84/120    avg_loss:0.007, val_acc:0.993]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.009, val_acc:0.993]
Epoch [87/120    avg_loss:0.008, val_acc:0.993]
Epoch [88/120    avg_loss:0.012, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.992]
Epoch [90/120    avg_loss:0.007, val_acc:0.992]
Epoch [91/120    avg_loss:0.008, val_acc:0.992]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.008, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.007, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.993]
Epoch [98/120    avg_loss:0.006, val_acc:0.993]
Epoch [99/120    avg_loss:0.007, val_acc:0.993]
Epoch [100/120    avg_loss:0.007, val_acc:0.993]
Epoch [101/120    avg_loss:0.010, val_acc:0.993]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.006, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.993]
Epoch [105/120    avg_loss:0.007, val_acc:0.993]
Epoch [106/120    avg_loss:0.008, val_acc:0.993]
Epoch [107/120    avg_loss:0.006, val_acc:0.993]
Epoch [108/120    avg_loss:0.008, val_acc:0.993]
Epoch [109/120    avg_loss:0.009, val_acc:0.993]
Epoch [110/120    avg_loss:0.007, val_acc:0.993]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.006, val_acc:0.993]
Epoch [113/120    avg_loss:0.008, val_acc:0.993]
Epoch [114/120    avg_loss:0.007, val_acc:0.993]
Epoch [115/120    avg_loss:0.008, val_acc:0.993]
Epoch [116/120    avg_loss:0.007, val_acc:0.993]
Epoch [117/120    avg_loss:0.007, val_acc:0.993]
Epoch [118/120    avg_loss:0.007, val_acc:0.993]
Epoch [119/120    avg_loss:0.008, val_acc:0.993]
Epoch [120/120    avg_loss:0.007, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6388     0     0     0     0     1     0    39     4]
 [    0     2 17986     0    39     0    59     0     4     0]
 [    0     1     0  1935     0     0     0     0    99     1]
 [    0    18     0     0  2935     0     8     0     8     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     1     0  4871     0     2     0]
 [    0    20     0     0     0     0     0  1265     2     3]
 [    0     8     5    46    16     0    23     0  3473     0]
 [    0     0     0     0     0     8     0     0     0   911]]

Accuracy:
98.97814089123467

F1 scores:
[       nan 0.99277333 0.9968685  0.96340553 0.98440382 0.99694423
 0.99004065 0.99021526 0.96499028 0.98967952]

Kappa:
0.9864717359710344
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fece5a339b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.945, val_acc:0.110]
Epoch [2/120    avg_loss:1.526, val_acc:0.332]
Epoch [3/120    avg_loss:1.238, val_acc:0.443]
Epoch [4/120    avg_loss:1.029, val_acc:0.698]
Epoch [5/120    avg_loss:0.840, val_acc:0.616]
Epoch [6/120    avg_loss:0.739, val_acc:0.657]
Epoch [7/120    avg_loss:0.593, val_acc:0.728]
Epoch [8/120    avg_loss:0.495, val_acc:0.793]
Epoch [9/120    avg_loss:0.432, val_acc:0.716]
Epoch [10/120    avg_loss:0.384, val_acc:0.829]
Epoch [11/120    avg_loss:0.333, val_acc:0.857]
Epoch [12/120    avg_loss:0.315, val_acc:0.873]
Epoch [13/120    avg_loss:0.282, val_acc:0.825]
Epoch [14/120    avg_loss:0.282, val_acc:0.919]
Epoch [15/120    avg_loss:0.209, val_acc:0.902]
Epoch [16/120    avg_loss:0.192, val_acc:0.938]
Epoch [17/120    avg_loss:0.181, val_acc:0.932]
Epoch [18/120    avg_loss:0.170, val_acc:0.944]
Epoch [19/120    avg_loss:0.167, val_acc:0.925]
Epoch [20/120    avg_loss:0.159, val_acc:0.911]
Epoch [21/120    avg_loss:0.147, val_acc:0.949]
Epoch [22/120    avg_loss:0.147, val_acc:0.932]
Epoch [23/120    avg_loss:0.111, val_acc:0.932]
Epoch [24/120    avg_loss:0.104, val_acc:0.954]
Epoch [25/120    avg_loss:0.096, val_acc:0.957]
Epoch [26/120    avg_loss:0.101, val_acc:0.936]
Epoch [27/120    avg_loss:0.118, val_acc:0.941]
Epoch [28/120    avg_loss:0.101, val_acc:0.948]
Epoch [29/120    avg_loss:0.107, val_acc:0.943]
Epoch [30/120    avg_loss:0.084, val_acc:0.960]
Epoch [31/120    avg_loss:0.097, val_acc:0.958]
Epoch [32/120    avg_loss:0.089, val_acc:0.958]
Epoch [33/120    avg_loss:0.076, val_acc:0.968]
Epoch [34/120    avg_loss:0.081, val_acc:0.941]
Epoch [35/120    avg_loss:0.080, val_acc:0.955]
Epoch [36/120    avg_loss:0.068, val_acc:0.969]
Epoch [37/120    avg_loss:0.043, val_acc:0.968]
Epoch [38/120    avg_loss:0.049, val_acc:0.962]
Epoch [39/120    avg_loss:0.062, val_acc:0.967]
Epoch [40/120    avg_loss:0.053, val_acc:0.968]
Epoch [41/120    avg_loss:0.056, val_acc:0.963]
Epoch [42/120    avg_loss:0.046, val_acc:0.972]
Epoch [43/120    avg_loss:0.044, val_acc:0.963]
Epoch [44/120    avg_loss:0.043, val_acc:0.971]
Epoch [45/120    avg_loss:0.033, val_acc:0.964]
Epoch [46/120    avg_loss:0.033, val_acc:0.969]
Epoch [47/120    avg_loss:0.052, val_acc:0.963]
Epoch [48/120    avg_loss:0.043, val_acc:0.969]
Epoch [49/120    avg_loss:0.046, val_acc:0.960]
Epoch [50/120    avg_loss:0.045, val_acc:0.961]
Epoch [51/120    avg_loss:0.036, val_acc:0.968]
Epoch [52/120    avg_loss:0.035, val_acc:0.973]
Epoch [53/120    avg_loss:0.035, val_acc:0.975]
Epoch [54/120    avg_loss:0.026, val_acc:0.967]
Epoch [55/120    avg_loss:0.032, val_acc:0.959]
Epoch [56/120    avg_loss:0.029, val_acc:0.976]
Epoch [57/120    avg_loss:0.036, val_acc:0.964]
Epoch [58/120    avg_loss:0.026, val_acc:0.972]
Epoch [59/120    avg_loss:0.023, val_acc:0.980]
Epoch [60/120    avg_loss:0.023, val_acc:0.976]
Epoch [61/120    avg_loss:0.023, val_acc:0.970]
Epoch [62/120    avg_loss:0.022, val_acc:0.974]
Epoch [63/120    avg_loss:0.019, val_acc:0.979]
Epoch [64/120    avg_loss:0.012, val_acc:0.979]
Epoch [65/120    avg_loss:0.012, val_acc:0.979]
Epoch [66/120    avg_loss:0.024, val_acc:0.977]
Epoch [67/120    avg_loss:0.025, val_acc:0.975]
Epoch [68/120    avg_loss:0.024, val_acc:0.969]
Epoch [69/120    avg_loss:0.027, val_acc:0.947]
Epoch [70/120    avg_loss:0.033, val_acc:0.971]
Epoch [71/120    avg_loss:0.027, val_acc:0.974]
Epoch [72/120    avg_loss:0.039, val_acc:0.963]
Epoch [73/120    avg_loss:0.020, val_acc:0.972]
Epoch [74/120    avg_loss:0.014, val_acc:0.973]
Epoch [75/120    avg_loss:0.013, val_acc:0.975]
Epoch [76/120    avg_loss:0.014, val_acc:0.976]
Epoch [77/120    avg_loss:0.012, val_acc:0.977]
Epoch [78/120    avg_loss:0.012, val_acc:0.978]
Epoch [79/120    avg_loss:0.010, val_acc:0.978]
Epoch [80/120    avg_loss:0.010, val_acc:0.977]
Epoch [81/120    avg_loss:0.010, val_acc:0.978]
Epoch [82/120    avg_loss:0.008, val_acc:0.977]
Epoch [83/120    avg_loss:0.007, val_acc:0.977]
Epoch [84/120    avg_loss:0.008, val_acc:0.977]
Epoch [85/120    avg_loss:0.006, val_acc:0.978]
Epoch [86/120    avg_loss:0.010, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.008, val_acc:0.978]
Epoch [89/120    avg_loss:0.006, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.978]
Epoch [96/120    avg_loss:0.008, val_acc:0.978]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.006, val_acc:0.977]
Epoch [99/120    avg_loss:0.009, val_acc:0.977]
Epoch [100/120    avg_loss:0.010, val_acc:0.977]
Epoch [101/120    avg_loss:0.007, val_acc:0.977]
Epoch [102/120    avg_loss:0.010, val_acc:0.977]
Epoch [103/120    avg_loss:0.009, val_acc:0.977]
Epoch [104/120    avg_loss:0.008, val_acc:0.977]
Epoch [105/120    avg_loss:0.006, val_acc:0.977]
Epoch [106/120    avg_loss:0.012, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.977]
Epoch [108/120    avg_loss:0.009, val_acc:0.977]
Epoch [109/120    avg_loss:0.012, val_acc:0.977]
Epoch [110/120    avg_loss:0.008, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.014, val_acc:0.977]
Epoch [113/120    avg_loss:0.009, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.012, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.977]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6302     0     0     2     0     0    17   107     4]
 [    0     0 18006     0    17     0    66     0     1     0]
 [    0     1     0  1922     0     0     0     0   113     0]
 [    0    19     5     0  2908     0    29     0     8     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    34     0     0     0     0     0  1252     0     4]
 [    0    10     2    51    23     0    21     2  3462     0]
 [    0     0     0     0    12    14     0     0     0   893]]

Accuracy:
98.63832453666883

F1 scores:
[       nan 0.98484138 0.99747943 0.9588426  0.98011459 0.99466463
 0.98824959 0.97774307 0.95345635 0.97970378]

Kappa:
0.9819708685740549
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc13c05940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.970, val_acc:0.212]
Epoch [2/120    avg_loss:1.515, val_acc:0.278]
Epoch [3/120    avg_loss:1.286, val_acc:0.384]
Epoch [4/120    avg_loss:1.058, val_acc:0.557]
Epoch [5/120    avg_loss:0.888, val_acc:0.714]
Epoch [6/120    avg_loss:0.761, val_acc:0.640]
Epoch [7/120    avg_loss:0.675, val_acc:0.696]
Epoch [8/120    avg_loss:0.569, val_acc:0.748]
Epoch [9/120    avg_loss:0.526, val_acc:0.723]
Epoch [10/120    avg_loss:0.420, val_acc:0.797]
Epoch [11/120    avg_loss:0.398, val_acc:0.767]
Epoch [12/120    avg_loss:0.369, val_acc:0.846]
Epoch [13/120    avg_loss:0.337, val_acc:0.852]
Epoch [14/120    avg_loss:0.288, val_acc:0.897]
Epoch [15/120    avg_loss:0.256, val_acc:0.850]
Epoch [16/120    avg_loss:0.262, val_acc:0.839]
Epoch [17/120    avg_loss:0.241, val_acc:0.877]
Epoch [18/120    avg_loss:0.192, val_acc:0.917]
Epoch [19/120    avg_loss:0.175, val_acc:0.918]
Epoch [20/120    avg_loss:0.150, val_acc:0.942]
Epoch [21/120    avg_loss:0.125, val_acc:0.920]
Epoch [22/120    avg_loss:0.142, val_acc:0.949]
Epoch [23/120    avg_loss:0.143, val_acc:0.956]
Epoch [24/120    avg_loss:0.152, val_acc:0.968]
Epoch [25/120    avg_loss:0.092, val_acc:0.946]
Epoch [26/120    avg_loss:0.105, val_acc:0.930]
Epoch [27/120    avg_loss:0.104, val_acc:0.961]
Epoch [28/120    avg_loss:0.096, val_acc:0.953]
Epoch [29/120    avg_loss:0.105, val_acc:0.954]
Epoch [30/120    avg_loss:0.071, val_acc:0.968]
Epoch [31/120    avg_loss:0.088, val_acc:0.960]
Epoch [32/120    avg_loss:0.071, val_acc:0.887]
Epoch [33/120    avg_loss:0.085, val_acc:0.973]
Epoch [34/120    avg_loss:0.068, val_acc:0.963]
Epoch [35/120    avg_loss:0.067, val_acc:0.965]
Epoch [36/120    avg_loss:0.052, val_acc:0.973]
Epoch [37/120    avg_loss:0.073, val_acc:0.975]
Epoch [38/120    avg_loss:0.052, val_acc:0.977]
Epoch [39/120    avg_loss:0.048, val_acc:0.968]
Epoch [40/120    avg_loss:0.034, val_acc:0.964]
Epoch [41/120    avg_loss:0.060, val_acc:0.963]
Epoch [42/120    avg_loss:0.056, val_acc:0.970]
Epoch [43/120    avg_loss:0.062, val_acc:0.966]
Epoch [44/120    avg_loss:0.038, val_acc:0.976]
Epoch [45/120    avg_loss:0.048, val_acc:0.971]
Epoch [46/120    avg_loss:0.036, val_acc:0.982]
Epoch [47/120    avg_loss:0.033, val_acc:0.973]
Epoch [48/120    avg_loss:0.041, val_acc:0.975]
Epoch [49/120    avg_loss:0.026, val_acc:0.971]
Epoch [50/120    avg_loss:0.046, val_acc:0.960]
Epoch [51/120    avg_loss:0.058, val_acc:0.961]
Epoch [52/120    avg_loss:0.028, val_acc:0.972]
Epoch [53/120    avg_loss:0.041, val_acc:0.954]
Epoch [54/120    avg_loss:0.038, val_acc:0.975]
Epoch [55/120    avg_loss:0.018, val_acc:0.982]
Epoch [56/120    avg_loss:0.018, val_acc:0.978]
Epoch [57/120    avg_loss:0.023, val_acc:0.982]
Epoch [58/120    avg_loss:0.025, val_acc:0.966]
Epoch [59/120    avg_loss:0.027, val_acc:0.960]
Epoch [60/120    avg_loss:0.031, val_acc:0.980]
Epoch [61/120    avg_loss:0.038, val_acc:0.970]
Epoch [62/120    avg_loss:0.019, val_acc:0.968]
Epoch [63/120    avg_loss:0.040, val_acc:0.965]
Epoch [64/120    avg_loss:0.038, val_acc:0.956]
Epoch [65/120    avg_loss:0.026, val_acc:0.977]
Epoch [66/120    avg_loss:0.011, val_acc:0.981]
Epoch [67/120    avg_loss:0.017, val_acc:0.979]
Epoch [68/120    avg_loss:0.019, val_acc:0.978]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.978]
Epoch [71/120    avg_loss:0.042, val_acc:0.953]
Epoch [72/120    avg_loss:0.064, val_acc:0.968]
Epoch [73/120    avg_loss:0.037, val_acc:0.956]
Epoch [74/120    avg_loss:0.027, val_acc:0.973]
Epoch [75/120    avg_loss:0.026, val_acc:0.974]
Epoch [76/120    avg_loss:0.015, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.987]
Epoch [78/120    avg_loss:0.012, val_acc:0.958]
Epoch [79/120    avg_loss:0.016, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.006, val_acc:0.975]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.972]
Epoch [85/120    avg_loss:0.018, val_acc:0.983]
Epoch [86/120    avg_loss:0.016, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.975]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.979]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.969]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.011, val_acc:0.977]
Epoch [103/120    avg_loss:0.009, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.016, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     1     0     0     0    14    39     6]
 [    0     0 18041     0    43     0     2     0     4     0]
 [    0     4     0  1910     0     0     0     0   122     0]
 [    0     6    11     0  2909     0    29     1    13     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     1     0  4870     0     2     0]
 [    0    10     0     0     0     0     0  1277     1     2]
 [    0    22     4    85    35     0     1     0  3424     0]
 [    0     2     0     0     7    13     0     0     0   897]]

Accuracy:
98.82389800689273

F1 scores:
[       nan 0.99190535 0.99809134 0.94742063 0.97502933 0.99504384
 0.99591002 0.98915569 0.95429208 0.9819376 ]

Kappa:
0.9844206466341114
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac6b5ba940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.023, val_acc:0.110]
Epoch [2/120    avg_loss:1.570, val_acc:0.383]
Epoch [3/120    avg_loss:1.320, val_acc:0.552]
Epoch [4/120    avg_loss:1.104, val_acc:0.586]
Epoch [5/120    avg_loss:0.886, val_acc:0.740]
Epoch [6/120    avg_loss:0.709, val_acc:0.693]
Epoch [7/120    avg_loss:0.582, val_acc:0.641]
Epoch [8/120    avg_loss:0.526, val_acc:0.695]
Epoch [9/120    avg_loss:0.481, val_acc:0.706]
Epoch [10/120    avg_loss:0.391, val_acc:0.772]
Epoch [11/120    avg_loss:0.383, val_acc:0.770]
Epoch [12/120    avg_loss:0.351, val_acc:0.782]
Epoch [13/120    avg_loss:0.309, val_acc:0.781]
Epoch [14/120    avg_loss:0.353, val_acc:0.743]
Epoch [15/120    avg_loss:0.298, val_acc:0.813]
Epoch [16/120    avg_loss:0.263, val_acc:0.864]
Epoch [17/120    avg_loss:0.246, val_acc:0.835]
Epoch [18/120    avg_loss:0.219, val_acc:0.877]
Epoch [19/120    avg_loss:0.227, val_acc:0.827]
Epoch [20/120    avg_loss:0.236, val_acc:0.905]
Epoch [21/120    avg_loss:0.186, val_acc:0.917]
Epoch [22/120    avg_loss:0.192, val_acc:0.856]
Epoch [23/120    avg_loss:0.146, val_acc:0.919]
Epoch [24/120    avg_loss:0.185, val_acc:0.897]
Epoch [25/120    avg_loss:0.171, val_acc:0.909]
Epoch [26/120    avg_loss:0.134, val_acc:0.863]
Epoch [27/120    avg_loss:0.121, val_acc:0.949]
Epoch [28/120    avg_loss:0.090, val_acc:0.939]
Epoch [29/120    avg_loss:0.093, val_acc:0.925]
Epoch [30/120    avg_loss:0.075, val_acc:0.936]
Epoch [31/120    avg_loss:0.102, val_acc:0.922]
Epoch [32/120    avg_loss:0.082, val_acc:0.953]
Epoch [33/120    avg_loss:0.080, val_acc:0.953]
Epoch [34/120    avg_loss:0.100, val_acc:0.936]
Epoch [35/120    avg_loss:0.091, val_acc:0.944]
Epoch [36/120    avg_loss:0.067, val_acc:0.951]
Epoch [37/120    avg_loss:0.058, val_acc:0.902]
Epoch [38/120    avg_loss:0.051, val_acc:0.958]
Epoch [39/120    avg_loss:0.042, val_acc:0.964]
Epoch [40/120    avg_loss:0.057, val_acc:0.939]
Epoch [41/120    avg_loss:0.079, val_acc:0.961]
Epoch [42/120    avg_loss:0.052, val_acc:0.938]
Epoch [43/120    avg_loss:0.043, val_acc:0.912]
Epoch [44/120    avg_loss:0.033, val_acc:0.968]
Epoch [45/120    avg_loss:0.029, val_acc:0.971]
Epoch [46/120    avg_loss:0.032, val_acc:0.936]
Epoch [47/120    avg_loss:0.060, val_acc:0.969]
Epoch [48/120    avg_loss:0.043, val_acc:0.896]
Epoch [49/120    avg_loss:0.077, val_acc:0.937]
Epoch [50/120    avg_loss:0.057, val_acc:0.970]
Epoch [51/120    avg_loss:0.047, val_acc:0.952]
Epoch [52/120    avg_loss:0.036, val_acc:0.963]
Epoch [53/120    avg_loss:0.048, val_acc:0.953]
Epoch [54/120    avg_loss:0.042, val_acc:0.943]
Epoch [55/120    avg_loss:0.054, val_acc:0.959]
Epoch [56/120    avg_loss:0.041, val_acc:0.976]
Epoch [57/120    avg_loss:0.032, val_acc:0.950]
Epoch [58/120    avg_loss:0.042, val_acc:0.953]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.023, val_acc:0.958]
Epoch [61/120    avg_loss:0.023, val_acc:0.974]
Epoch [62/120    avg_loss:0.028, val_acc:0.972]
Epoch [63/120    avg_loss:0.028, val_acc:0.968]
Epoch [64/120    avg_loss:0.029, val_acc:0.977]
Epoch [65/120    avg_loss:0.039, val_acc:0.955]
Epoch [66/120    avg_loss:0.039, val_acc:0.961]
Epoch [67/120    avg_loss:0.059, val_acc:0.930]
Epoch [68/120    avg_loss:0.037, val_acc:0.972]
Epoch [69/120    avg_loss:0.023, val_acc:0.973]
Epoch [70/120    avg_loss:0.021, val_acc:0.967]
Epoch [71/120    avg_loss:0.018, val_acc:0.979]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.010, val_acc:0.968]
Epoch [74/120    avg_loss:0.032, val_acc:0.960]
Epoch [75/120    avg_loss:0.016, val_acc:0.974]
Epoch [76/120    avg_loss:0.014, val_acc:0.978]
Epoch [77/120    avg_loss:0.008, val_acc:0.975]
Epoch [78/120    avg_loss:0.007, val_acc:0.978]
Epoch [79/120    avg_loss:0.008, val_acc:0.978]
Epoch [80/120    avg_loss:0.015, val_acc:0.973]
Epoch [81/120    avg_loss:0.007, val_acc:0.978]
Epoch [82/120    avg_loss:0.007, val_acc:0.976]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.010, val_acc:0.975]
Epoch [85/120    avg_loss:0.012, val_acc:0.977]
Epoch [86/120    avg_loss:0.006, val_acc:0.977]
Epoch [87/120    avg_loss:0.005, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.978]
Epoch [89/120    avg_loss:0.006, val_acc:0.978]
Epoch [90/120    avg_loss:0.007, val_acc:0.978]
Epoch [91/120    avg_loss:0.006, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.975]
Epoch [93/120    avg_loss:0.007, val_acc:0.975]
Epoch [94/120    avg_loss:0.007, val_acc:0.976]
Epoch [95/120    avg_loss:0.007, val_acc:0.977]
Epoch [96/120    avg_loss:0.004, val_acc:0.978]
Epoch [97/120    avg_loss:0.005, val_acc:0.978]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.005, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.006, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.006, val_acc:0.978]
Epoch [106/120    avg_loss:0.007, val_acc:0.978]
Epoch [107/120    avg_loss:0.004, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.006, val_acc:0.978]
Epoch [110/120    avg_loss:0.005, val_acc:0.978]
Epoch [111/120    avg_loss:0.005, val_acc:0.978]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.005, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.004, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.006, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6302     0     4     0     0     0    15   111     0]
 [    0     2 18017     0    54     0    13     0     4     0]
 [    0     5     0  1868     0     0     0     0   163     0]
 [    0     9     2     1  2934     0     8    15     0     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    17     0     5     0  4852     0     3     0]
 [    0     3     0     0     0     0     0  1286     0     1]
 [    0    18     0    84    37     0     2     0  3430     0]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
98.57084327476925

F1 scores:
[       nan 0.98684623 0.99745336 0.93563737 0.97767411 0.99504384
 0.9949759  0.98695318 0.94204889 0.9907053 ]

Kappa:
0.9810778034265456
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20442df908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.911, val_acc:0.212]
Epoch [2/120    avg_loss:1.493, val_acc:0.268]
Epoch [3/120    avg_loss:1.226, val_acc:0.460]
Epoch [4/120    avg_loss:1.014, val_acc:0.483]
Epoch [5/120    avg_loss:0.815, val_acc:0.607]
Epoch [6/120    avg_loss:0.692, val_acc:0.779]
Epoch [7/120    avg_loss:0.572, val_acc:0.743]
Epoch [8/120    avg_loss:0.485, val_acc:0.693]
Epoch [9/120    avg_loss:0.439, val_acc:0.735]
Epoch [10/120    avg_loss:0.403, val_acc:0.846]
Epoch [11/120    avg_loss:0.367, val_acc:0.830]
Epoch [12/120    avg_loss:0.373, val_acc:0.744]
Epoch [13/120    avg_loss:0.327, val_acc:0.829]
Epoch [14/120    avg_loss:0.301, val_acc:0.855]
Epoch [15/120    avg_loss:0.272, val_acc:0.892]
Epoch [16/120    avg_loss:0.234, val_acc:0.823]
Epoch [17/120    avg_loss:0.230, val_acc:0.884]
Epoch [18/120    avg_loss:0.269, val_acc:0.897]
Epoch [19/120    avg_loss:0.249, val_acc:0.868]
Epoch [20/120    avg_loss:0.225, val_acc:0.892]
Epoch [21/120    avg_loss:0.186, val_acc:0.862]
Epoch [22/120    avg_loss:0.211, val_acc:0.895]
Epoch [23/120    avg_loss:0.191, val_acc:0.932]
Epoch [24/120    avg_loss:0.165, val_acc:0.912]
Epoch [25/120    avg_loss:0.153, val_acc:0.896]
Epoch [26/120    avg_loss:0.157, val_acc:0.944]
Epoch [27/120    avg_loss:0.156, val_acc:0.932]
Epoch [28/120    avg_loss:0.097, val_acc:0.953]
Epoch [29/120    avg_loss:0.089, val_acc:0.924]
Epoch [30/120    avg_loss:0.113, val_acc:0.929]
Epoch [31/120    avg_loss:0.086, val_acc:0.895]
Epoch [32/120    avg_loss:0.082, val_acc:0.937]
Epoch [33/120    avg_loss:0.071, val_acc:0.955]
Epoch [34/120    avg_loss:0.087, val_acc:0.953]
Epoch [35/120    avg_loss:0.061, val_acc:0.972]
Epoch [36/120    avg_loss:0.049, val_acc:0.964]
Epoch [37/120    avg_loss:0.042, val_acc:0.968]
Epoch [38/120    avg_loss:0.081, val_acc:0.954]
Epoch [39/120    avg_loss:0.074, val_acc:0.948]
Epoch [40/120    avg_loss:0.049, val_acc:0.973]
Epoch [41/120    avg_loss:0.037, val_acc:0.967]
Epoch [42/120    avg_loss:0.038, val_acc:0.954]
Epoch [43/120    avg_loss:0.065, val_acc:0.965]
Epoch [44/120    avg_loss:0.095, val_acc:0.958]
Epoch [45/120    avg_loss:0.059, val_acc:0.960]
Epoch [46/120    avg_loss:0.055, val_acc:0.961]
Epoch [47/120    avg_loss:0.037, val_acc:0.974]
Epoch [48/120    avg_loss:0.030, val_acc:0.952]
Epoch [49/120    avg_loss:0.031, val_acc:0.956]
Epoch [50/120    avg_loss:0.038, val_acc:0.958]
Epoch [51/120    avg_loss:0.032, val_acc:0.970]
Epoch [52/120    avg_loss:0.044, val_acc:0.963]
Epoch [53/120    avg_loss:0.030, val_acc:0.971]
Epoch [54/120    avg_loss:0.025, val_acc:0.974]
Epoch [55/120    avg_loss:0.021, val_acc:0.960]
Epoch [56/120    avg_loss:0.026, val_acc:0.968]
Epoch [57/120    avg_loss:0.019, val_acc:0.978]
Epoch [58/120    avg_loss:0.020, val_acc:0.963]
Epoch [59/120    avg_loss:0.032, val_acc:0.965]
Epoch [60/120    avg_loss:0.017, val_acc:0.974]
Epoch [61/120    avg_loss:0.023, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.974]
Epoch [63/120    avg_loss:0.019, val_acc:0.982]
Epoch [64/120    avg_loss:0.012, val_acc:0.979]
Epoch [65/120    avg_loss:0.025, val_acc:0.978]
Epoch [66/120    avg_loss:0.038, val_acc:0.959]
Epoch [67/120    avg_loss:0.029, val_acc:0.961]
Epoch [68/120    avg_loss:0.043, val_acc:0.975]
Epoch [69/120    avg_loss:0.021, val_acc:0.977]
Epoch [70/120    avg_loss:0.018, val_acc:0.975]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.012, val_acc:0.973]
Epoch [73/120    avg_loss:0.014, val_acc:0.978]
Epoch [74/120    avg_loss:0.015, val_acc:0.973]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.015, val_acc:0.983]
Epoch [77/120    avg_loss:0.021, val_acc:0.983]
Epoch [78/120    avg_loss:0.019, val_acc:0.974]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.015, val_acc:0.979]
Epoch [87/120    avg_loss:0.012, val_acc:0.981]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.979]
Epoch [90/120    avg_loss:0.013, val_acc:0.981]
Epoch [91/120    avg_loss:0.036, val_acc:0.952]
Epoch [92/120    avg_loss:0.025, val_acc:0.976]
Epoch [93/120    avg_loss:0.014, val_acc:0.980]
Epoch [94/120    avg_loss:0.029, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.978]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.987]
Epoch [108/120    avg_loss:0.027, val_acc:0.948]
Epoch [109/120    avg_loss:0.019, val_acc:0.980]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     1     0     0     0     0    65     1]
 [    0     0 18003     0    71     0    13     0     3     0]
 [    0     5     0  1926     0     0     0     0   103     2]
 [    0    10     0     0  2948     0    10     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    43     0     7     0  4824     0     3     0]
 [    0    14     0     0     0     0     0  1276     0     0]
 [    0    22     0    61    43     0     1     0  3444     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.82389800689273

F1 scores:
[       nan 0.99073858 0.99640248 0.95725646 0.97599735 0.99808795
 0.99198026 0.99454404 0.95799722 0.99401849]

Kappa:
0.9844241789260194
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14a4abd8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.992, val_acc:0.171]
Epoch [2/120    avg_loss:1.532, val_acc:0.394]
Epoch [3/120    avg_loss:1.228, val_acc:0.418]
Epoch [4/120    avg_loss:0.993, val_acc:0.504]
Epoch [5/120    avg_loss:0.787, val_acc:0.599]
Epoch [6/120    avg_loss:0.631, val_acc:0.669]
Epoch [7/120    avg_loss:0.558, val_acc:0.692]
Epoch [8/120    avg_loss:0.457, val_acc:0.747]
Epoch [9/120    avg_loss:0.408, val_acc:0.759]
Epoch [10/120    avg_loss:0.341, val_acc:0.794]
Epoch [11/120    avg_loss:0.339, val_acc:0.806]
Epoch [12/120    avg_loss:0.321, val_acc:0.818]
Epoch [13/120    avg_loss:0.279, val_acc:0.804]
Epoch [14/120    avg_loss:0.255, val_acc:0.858]
Epoch [15/120    avg_loss:0.239, val_acc:0.902]
Epoch [16/120    avg_loss:0.190, val_acc:0.931]
Epoch [17/120    avg_loss:0.173, val_acc:0.922]
Epoch [18/120    avg_loss:0.151, val_acc:0.929]
Epoch [19/120    avg_loss:0.158, val_acc:0.914]
Epoch [20/120    avg_loss:0.208, val_acc:0.932]
Epoch [21/120    avg_loss:0.160, val_acc:0.925]
Epoch [22/120    avg_loss:0.124, val_acc:0.949]
Epoch [23/120    avg_loss:0.112, val_acc:0.919]
Epoch [24/120    avg_loss:0.159, val_acc:0.917]
Epoch [25/120    avg_loss:0.109, val_acc:0.934]
Epoch [26/120    avg_loss:0.096, val_acc:0.943]
Epoch [27/120    avg_loss:0.143, val_acc:0.932]
Epoch [28/120    avg_loss:0.088, val_acc:0.933]
Epoch [29/120    avg_loss:0.100, val_acc:0.956]
Epoch [30/120    avg_loss:0.068, val_acc:0.956]
Epoch [31/120    avg_loss:0.078, val_acc:0.920]
Epoch [32/120    avg_loss:0.098, val_acc:0.951]
Epoch [33/120    avg_loss:0.083, val_acc:0.956]
Epoch [34/120    avg_loss:0.064, val_acc:0.966]
Epoch [35/120    avg_loss:0.052, val_acc:0.958]
Epoch [36/120    avg_loss:0.060, val_acc:0.973]
Epoch [37/120    avg_loss:0.049, val_acc:0.958]
Epoch [38/120    avg_loss:0.058, val_acc:0.946]
Epoch [39/120    avg_loss:0.054, val_acc:0.963]
Epoch [40/120    avg_loss:0.039, val_acc:0.963]
Epoch [41/120    avg_loss:0.037, val_acc:0.970]
Epoch [42/120    avg_loss:0.071, val_acc:0.970]
Epoch [43/120    avg_loss:0.037, val_acc:0.972]
Epoch [44/120    avg_loss:0.025, val_acc:0.970]
Epoch [45/120    avg_loss:0.027, val_acc:0.965]
Epoch [46/120    avg_loss:0.032, val_acc:0.976]
Epoch [47/120    avg_loss:0.027, val_acc:0.976]
Epoch [48/120    avg_loss:0.030, val_acc:0.962]
Epoch [49/120    avg_loss:0.062, val_acc:0.955]
Epoch [50/120    avg_loss:0.042, val_acc:0.974]
Epoch [51/120    avg_loss:0.026, val_acc:0.977]
Epoch [52/120    avg_loss:0.069, val_acc:0.968]
Epoch [53/120    avg_loss:0.039, val_acc:0.973]
Epoch [54/120    avg_loss:0.032, val_acc:0.973]
Epoch [55/120    avg_loss:0.025, val_acc:0.969]
Epoch [56/120    avg_loss:0.020, val_acc:0.980]
Epoch [57/120    avg_loss:0.015, val_acc:0.978]
Epoch [58/120    avg_loss:0.017, val_acc:0.975]
Epoch [59/120    avg_loss:0.023, val_acc:0.978]
Epoch [60/120    avg_loss:0.013, val_acc:0.979]
Epoch [61/120    avg_loss:0.018, val_acc:0.976]
Epoch [62/120    avg_loss:0.019, val_acc:0.983]
Epoch [63/120    avg_loss:0.012, val_acc:0.981]
Epoch [64/120    avg_loss:0.046, val_acc:0.969]
Epoch [65/120    avg_loss:0.030, val_acc:0.967]
Epoch [66/120    avg_loss:0.031, val_acc:0.959]
Epoch [67/120    avg_loss:0.040, val_acc:0.967]
Epoch [68/120    avg_loss:0.027, val_acc:0.973]
Epoch [69/120    avg_loss:0.024, val_acc:0.969]
Epoch [70/120    avg_loss:0.018, val_acc:0.971]
Epoch [71/120    avg_loss:0.011, val_acc:0.977]
Epoch [72/120    avg_loss:0.032, val_acc:0.965]
Epoch [73/120    avg_loss:0.033, val_acc:0.979]
Epoch [74/120    avg_loss:0.015, val_acc:0.974]
Epoch [75/120    avg_loss:0.018, val_acc:0.979]
Epoch [76/120    avg_loss:0.010, val_acc:0.977]
Epoch [77/120    avg_loss:0.011, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.981]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.008, val_acc:0.982]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.983]
Epoch [89/120    avg_loss:0.007, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     0     0     0     8     0    56    10]
 [    0     0 18049     0    11     0    25     0     5     0]
 [    0     6     0  1975     0     0     0     0    55     0]
 [    0    22     4     0  2929     0    13     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     8     0  4862     0     0     0]
 [    0    16     0     0     0     0     2  1269     0     3]
 [    0    15     0    47    10     0    31     0  3467     1]
 [    0     0     0     0     9     6     0     0     0   904]]

Accuracy:
99.09623309955896

F1 scores:
[       nan 0.989649   0.99853393 0.9733859  0.98636134 0.99770642
 0.99032488 0.99179367 0.96911251 0.9826087 ]

Kappa:
0.9880275662699863
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5554d978d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.979, val_acc:0.156]
Epoch [2/120    avg_loss:1.554, val_acc:0.480]
Epoch [3/120    avg_loss:1.298, val_acc:0.593]
Epoch [4/120    avg_loss:1.124, val_acc:0.482]
Epoch [5/120    avg_loss:0.969, val_acc:0.626]
Epoch [6/120    avg_loss:0.866, val_acc:0.707]
Epoch [7/120    avg_loss:0.745, val_acc:0.657]
Epoch [8/120    avg_loss:0.649, val_acc:0.701]
Epoch [9/120    avg_loss:0.520, val_acc:0.708]
Epoch [10/120    avg_loss:0.468, val_acc:0.767]
Epoch [11/120    avg_loss:0.410, val_acc:0.772]
Epoch [12/120    avg_loss:0.374, val_acc:0.808]
Epoch [13/120    avg_loss:0.344, val_acc:0.833]
Epoch [14/120    avg_loss:0.290, val_acc:0.840]
Epoch [15/120    avg_loss:0.305, val_acc:0.874]
Epoch [16/120    avg_loss:0.233, val_acc:0.859]
Epoch [17/120    avg_loss:0.214, val_acc:0.898]
Epoch [18/120    avg_loss:0.208, val_acc:0.900]
Epoch [19/120    avg_loss:0.175, val_acc:0.928]
Epoch [20/120    avg_loss:0.168, val_acc:0.909]
Epoch [21/120    avg_loss:0.154, val_acc:0.931]
Epoch [22/120    avg_loss:0.115, val_acc:0.952]
Epoch [23/120    avg_loss:0.114, val_acc:0.864]
Epoch [24/120    avg_loss:0.128, val_acc:0.939]
Epoch [25/120    avg_loss:0.116, val_acc:0.934]
Epoch [26/120    avg_loss:0.089, val_acc:0.948]
Epoch [27/120    avg_loss:0.071, val_acc:0.958]
Epoch [28/120    avg_loss:0.067, val_acc:0.936]
Epoch [29/120    avg_loss:0.066, val_acc:0.948]
Epoch [30/120    avg_loss:0.091, val_acc:0.968]
Epoch [31/120    avg_loss:0.062, val_acc:0.922]
Epoch [32/120    avg_loss:0.069, val_acc:0.954]
Epoch [33/120    avg_loss:0.047, val_acc:0.971]
Epoch [34/120    avg_loss:0.045, val_acc:0.964]
Epoch [35/120    avg_loss:0.044, val_acc:0.969]
Epoch [36/120    avg_loss:0.051, val_acc:0.962]
Epoch [37/120    avg_loss:0.044, val_acc:0.971]
Epoch [38/120    avg_loss:0.037, val_acc:0.974]
Epoch [39/120    avg_loss:0.053, val_acc:0.953]
Epoch [40/120    avg_loss:0.043, val_acc:0.963]
Epoch [41/120    avg_loss:0.036, val_acc:0.974]
Epoch [42/120    avg_loss:0.025, val_acc:0.972]
Epoch [43/120    avg_loss:0.054, val_acc:0.962]
Epoch [44/120    avg_loss:0.073, val_acc:0.953]
Epoch [45/120    avg_loss:0.059, val_acc:0.956]
Epoch [46/120    avg_loss:0.136, val_acc:0.947]
Epoch [47/120    avg_loss:0.101, val_acc:0.965]
Epoch [48/120    avg_loss:0.041, val_acc:0.978]
Epoch [49/120    avg_loss:0.035, val_acc:0.979]
Epoch [50/120    avg_loss:0.030, val_acc:0.980]
Epoch [51/120    avg_loss:0.024, val_acc:0.976]
Epoch [52/120    avg_loss:0.025, val_acc:0.978]
Epoch [53/120    avg_loss:0.023, val_acc:0.977]
Epoch [54/120    avg_loss:0.016, val_acc:0.979]
Epoch [55/120    avg_loss:0.014, val_acc:0.977]
Epoch [56/120    avg_loss:0.018, val_acc:0.979]
Epoch [57/120    avg_loss:0.019, val_acc:0.983]
Epoch [58/120    avg_loss:0.016, val_acc:0.979]
Epoch [59/120    avg_loss:0.018, val_acc:0.974]
Epoch [60/120    avg_loss:0.021, val_acc:0.981]
Epoch [61/120    avg_loss:0.012, val_acc:0.980]
Epoch [62/120    avg_loss:0.019, val_acc:0.983]
Epoch [63/120    avg_loss:0.013, val_acc:0.983]
Epoch [64/120    avg_loss:0.013, val_acc:0.983]
Epoch [65/120    avg_loss:0.008, val_acc:0.985]
Epoch [66/120    avg_loss:0.018, val_acc:0.978]
Epoch [67/120    avg_loss:0.022, val_acc:0.954]
Epoch [68/120    avg_loss:0.236, val_acc:0.892]
Epoch [69/120    avg_loss:0.115, val_acc:0.965]
Epoch [70/120    avg_loss:0.093, val_acc:0.905]
Epoch [71/120    avg_loss:0.078, val_acc:0.959]
Epoch [72/120    avg_loss:0.051, val_acc:0.954]
Epoch [73/120    avg_loss:0.069, val_acc:0.963]
Epoch [74/120    avg_loss:0.037, val_acc:0.974]
Epoch [75/120    avg_loss:0.029, val_acc:0.976]
Epoch [76/120    avg_loss:0.023, val_acc:0.979]
Epoch [77/120    avg_loss:0.027, val_acc:0.982]
Epoch [78/120    avg_loss:0.025, val_acc:0.966]
Epoch [79/120    avg_loss:0.026, val_acc:0.982]
Epoch [80/120    avg_loss:0.017, val_acc:0.983]
Epoch [81/120    avg_loss:0.018, val_acc:0.987]
Epoch [82/120    avg_loss:0.014, val_acc:0.984]
Epoch [83/120    avg_loss:0.016, val_acc:0.984]
Epoch [84/120    avg_loss:0.018, val_acc:0.983]
Epoch [85/120    avg_loss:0.017, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.014, val_acc:0.985]
Epoch [91/120    avg_loss:0.015, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.010, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.012, val_acc:0.986]
Epoch [98/120    avg_loss:0.013, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.012, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.011, val_acc:0.986]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.018, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     0     0     3     0    21     8]
 [    0     1 17966     0    52     0    63     0     8     0]
 [    0     2     0  1988     1     0     0     0    40     5]
 [    0    27    11     1  2896     0    33     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0     5     0     0     0     0     0  1281     0     4]
 [    0     7     0    48    46     0     7     0  3459     4]
 [    0     0     0     0     8    14     0     0     0   897]]

Accuracy:
98.9685007109633

F1 scores:
[       nan 0.99425198 0.99625697 0.97618463 0.96937238 0.99466463
 0.98873897 0.99649942 0.9734065  0.97606094]

Kappa:
0.9863478911420613
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff17e6d99b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.019, val_acc:0.126]
Epoch [2/120    avg_loss:1.544, val_acc:0.626]
Epoch [3/120    avg_loss:1.291, val_acc:0.626]
Epoch [4/120    avg_loss:1.090, val_acc:0.673]
Epoch [5/120    avg_loss:0.887, val_acc:0.588]
Epoch [6/120    avg_loss:0.721, val_acc:0.639]
Epoch [7/120    avg_loss:0.583, val_acc:0.689]
Epoch [8/120    avg_loss:0.513, val_acc:0.763]
Epoch [9/120    avg_loss:0.424, val_acc:0.833]
Epoch [10/120    avg_loss:0.361, val_acc:0.740]
Epoch [11/120    avg_loss:0.303, val_acc:0.868]
Epoch [12/120    avg_loss:0.259, val_acc:0.935]
Epoch [13/120    avg_loss:0.272, val_acc:0.928]
Epoch [14/120    avg_loss:0.221, val_acc:0.912]
Epoch [15/120    avg_loss:0.209, val_acc:0.907]
Epoch [16/120    avg_loss:0.193, val_acc:0.892]
Epoch [17/120    avg_loss:0.190, val_acc:0.871]
Epoch [18/120    avg_loss:0.165, val_acc:0.948]
Epoch [19/120    avg_loss:0.143, val_acc:0.938]
Epoch [20/120    avg_loss:0.145, val_acc:0.897]
Epoch [21/120    avg_loss:0.123, val_acc:0.948]
Epoch [22/120    avg_loss:0.122, val_acc:0.962]
Epoch [23/120    avg_loss:0.110, val_acc:0.892]
Epoch [24/120    avg_loss:0.127, val_acc:0.968]
Epoch [25/120    avg_loss:0.083, val_acc:0.968]
Epoch [26/120    avg_loss:0.075, val_acc:0.974]
Epoch [27/120    avg_loss:0.082, val_acc:0.971]
Epoch [28/120    avg_loss:0.075, val_acc:0.907]
Epoch [29/120    avg_loss:0.079, val_acc:0.949]
Epoch [30/120    avg_loss:0.064, val_acc:0.970]
Epoch [31/120    avg_loss:0.080, val_acc:0.958]
Epoch [32/120    avg_loss:0.069, val_acc:0.978]
Epoch [33/120    avg_loss:0.044, val_acc:0.978]
Epoch [34/120    avg_loss:0.036, val_acc:0.982]
Epoch [35/120    avg_loss:0.072, val_acc:0.970]
Epoch [36/120    avg_loss:0.071, val_acc:0.973]
Epoch [37/120    avg_loss:0.081, val_acc:0.976]
Epoch [38/120    avg_loss:0.052, val_acc:0.970]
Epoch [39/120    avg_loss:0.062, val_acc:0.969]
Epoch [40/120    avg_loss:0.049, val_acc:0.969]
Epoch [41/120    avg_loss:0.059, val_acc:0.978]
Epoch [42/120    avg_loss:0.031, val_acc:0.973]
Epoch [43/120    avg_loss:0.031, val_acc:0.975]
Epoch [44/120    avg_loss:0.028, val_acc:0.982]
Epoch [45/120    avg_loss:0.030, val_acc:0.980]
Epoch [46/120    avg_loss:0.037, val_acc:0.971]
Epoch [47/120    avg_loss:0.024, val_acc:0.978]
Epoch [48/120    avg_loss:0.024, val_acc:0.985]
Epoch [49/120    avg_loss:0.027, val_acc:0.973]
Epoch [50/120    avg_loss:0.043, val_acc:0.928]
Epoch [51/120    avg_loss:0.047, val_acc:0.978]
Epoch [52/120    avg_loss:0.025, val_acc:0.968]
Epoch [53/120    avg_loss:0.024, val_acc:0.984]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.012, val_acc:0.986]
Epoch [56/120    avg_loss:0.029, val_acc:0.976]
Epoch [57/120    avg_loss:0.027, val_acc:0.977]
Epoch [58/120    avg_loss:0.031, val_acc:0.974]
Epoch [59/120    avg_loss:0.022, val_acc:0.963]
Epoch [60/120    avg_loss:0.018, val_acc:0.977]
Epoch [61/120    avg_loss:0.013, val_acc:0.986]
Epoch [62/120    avg_loss:0.017, val_acc:0.982]
Epoch [63/120    avg_loss:0.033, val_acc:0.983]
Epoch [64/120    avg_loss:0.017, val_acc:0.982]
Epoch [65/120    avg_loss:0.013, val_acc:0.990]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.981]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.009, val_acc:0.992]
Epoch [70/120    avg_loss:0.010, val_acc:0.991]
Epoch [71/120    avg_loss:0.008, val_acc:0.985]
Epoch [72/120    avg_loss:0.015, val_acc:0.987]
Epoch [73/120    avg_loss:0.020, val_acc:0.983]
Epoch [74/120    avg_loss:0.022, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.987]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.015, val_acc:0.943]
Epoch [78/120    avg_loss:0.023, val_acc:0.982]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.017, val_acc:0.974]
Epoch [82/120    avg_loss:0.019, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6343     0     0     0     0     6    20    60     3]
 [    0     7 17998     0    79     0     6     0     0     0]
 [    0     0     0  1936     0     0     0     0    98     2]
 [    0    15     5     0  2923     0    11     2    13     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0    11     0     0     0     0     0  1271     0     8]
 [    0    12     0    94    51     0     0     0  3407     7]
 [    0     0     0     0    14    20     0     0     0   885]]

Accuracy:
98.67929530282217

F1 scores:
[       nan 0.98954758 0.99728487 0.95228726 0.96804107 0.99239544
 0.99754551 0.98412698 0.9531403  0.96880131]

Kappa:
0.9825189220425535
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97778e79b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.112, val_acc:0.205]
Epoch [2/120    avg_loss:1.619, val_acc:0.236]
Epoch [3/120    avg_loss:1.383, val_acc:0.333]
Epoch [4/120    avg_loss:1.144, val_acc:0.375]
Epoch [5/120    avg_loss:1.001, val_acc:0.420]
Epoch [6/120    avg_loss:0.876, val_acc:0.494]
Epoch [7/120    avg_loss:0.751, val_acc:0.609]
Epoch [8/120    avg_loss:0.660, val_acc:0.627]
Epoch [9/120    avg_loss:0.566, val_acc:0.662]
Epoch [10/120    avg_loss:0.501, val_acc:0.667]
Epoch [11/120    avg_loss:0.434, val_acc:0.718]
Epoch [12/120    avg_loss:0.380, val_acc:0.782]
Epoch [13/120    avg_loss:0.352, val_acc:0.764]
Epoch [14/120    avg_loss:0.302, val_acc:0.803]
Epoch [15/120    avg_loss:0.246, val_acc:0.898]
Epoch [16/120    avg_loss:0.252, val_acc:0.828]
Epoch [17/120    avg_loss:0.237, val_acc:0.914]
Epoch [18/120    avg_loss:0.204, val_acc:0.910]
Epoch [19/120    avg_loss:0.190, val_acc:0.916]
Epoch [20/120    avg_loss:0.176, val_acc:0.935]
Epoch [21/120    avg_loss:0.172, val_acc:0.886]
Epoch [22/120    avg_loss:0.182, val_acc:0.833]
Epoch [23/120    avg_loss:0.142, val_acc:0.933]
Epoch [24/120    avg_loss:0.187, val_acc:0.910]
Epoch [25/120    avg_loss:0.169, val_acc:0.939]
Epoch [26/120    avg_loss:0.165, val_acc:0.911]
Epoch [27/120    avg_loss:0.189, val_acc:0.937]
Epoch [28/120    avg_loss:0.137, val_acc:0.932]
Epoch [29/120    avg_loss:0.073, val_acc:0.963]
Epoch [30/120    avg_loss:0.071, val_acc:0.963]
Epoch [31/120    avg_loss:0.084, val_acc:0.959]
Epoch [32/120    avg_loss:0.081, val_acc:0.951]
Epoch [33/120    avg_loss:0.088, val_acc:0.945]
Epoch [34/120    avg_loss:0.068, val_acc:0.935]
Epoch [35/120    avg_loss:0.102, val_acc:0.950]
Epoch [36/120    avg_loss:0.088, val_acc:0.966]
Epoch [37/120    avg_loss:0.045, val_acc:0.969]
Epoch [38/120    avg_loss:0.069, val_acc:0.962]
Epoch [39/120    avg_loss:0.049, val_acc:0.975]
Epoch [40/120    avg_loss:0.050, val_acc:0.974]
Epoch [41/120    avg_loss:0.039, val_acc:0.976]
Epoch [42/120    avg_loss:0.049, val_acc:0.973]
Epoch [43/120    avg_loss:0.042, val_acc:0.974]
Epoch [44/120    avg_loss:0.024, val_acc:0.982]
Epoch [45/120    avg_loss:0.037, val_acc:0.978]
Epoch [46/120    avg_loss:0.035, val_acc:0.969]
Epoch [47/120    avg_loss:0.055, val_acc:0.969]
Epoch [48/120    avg_loss:0.049, val_acc:0.974]
Epoch [49/120    avg_loss:0.057, val_acc:0.979]
Epoch [50/120    avg_loss:0.050, val_acc:0.960]
Epoch [51/120    avg_loss:0.034, val_acc:0.980]
Epoch [52/120    avg_loss:0.037, val_acc:0.968]
Epoch [53/120    avg_loss:0.046, val_acc:0.958]
Epoch [54/120    avg_loss:0.029, val_acc:0.974]
Epoch [55/120    avg_loss:0.053, val_acc:0.956]
Epoch [56/120    avg_loss:0.046, val_acc:0.979]
Epoch [57/120    avg_loss:0.029, val_acc:0.969]
Epoch [58/120    avg_loss:0.025, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.982]
Epoch [60/120    avg_loss:0.013, val_acc:0.980]
Epoch [61/120    avg_loss:0.012, val_acc:0.983]
Epoch [62/120    avg_loss:0.018, val_acc:0.981]
Epoch [63/120    avg_loss:0.019, val_acc:0.978]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.011, val_acc:0.984]
Epoch [69/120    avg_loss:0.014, val_acc:0.983]
Epoch [70/120    avg_loss:0.012, val_acc:0.981]
Epoch [71/120    avg_loss:0.012, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.018, val_acc:0.980]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.982]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.981]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.013, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.010, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.013, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.011, val_acc:0.980]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.987]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6306     1     0     0     0     0     0   110    15]
 [    0     1 17998     0    52     0    36     0     3     0]
 [    0     4     0  1986     0     0     0     0    43     3]
 [    0    19     0     0  2942     0     2     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     4     7     0  4858     0     0     0]
 [    0    13     0     0     0     0     2  1274     1     0]
 [    0    16     0    42    54     0     0     3  3449     7]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
98.80943773648568

F1 scores:
[       nan 0.98600579 0.99717436 0.97640118 0.97401093 0.99088838
 0.99386252 0.99259836 0.96032298 0.96389497]

Kappa:
0.9842415947612115
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa390376978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.965, val_acc:0.134]
Epoch [2/120    avg_loss:1.574, val_acc:0.164]
Epoch [3/120    avg_loss:1.329, val_acc:0.389]
Epoch [4/120    avg_loss:1.113, val_acc:0.591]
Epoch [5/120    avg_loss:0.935, val_acc:0.635]
Epoch [6/120    avg_loss:0.781, val_acc:0.673]
Epoch [7/120    avg_loss:0.611, val_acc:0.708]
Epoch [8/120    avg_loss:0.531, val_acc:0.745]
Epoch [9/120    avg_loss:0.464, val_acc:0.736]
Epoch [10/120    avg_loss:0.417, val_acc:0.761]
Epoch [11/120    avg_loss:0.381, val_acc:0.750]
Epoch [12/120    avg_loss:0.336, val_acc:0.787]
Epoch [13/120    avg_loss:0.338, val_acc:0.781]
Epoch [14/120    avg_loss:0.289, val_acc:0.824]
Epoch [15/120    avg_loss:0.267, val_acc:0.788]
Epoch [16/120    avg_loss:0.252, val_acc:0.840]
Epoch [17/120    avg_loss:0.217, val_acc:0.858]
Epoch [18/120    avg_loss:0.206, val_acc:0.908]
Epoch [19/120    avg_loss:0.200, val_acc:0.903]
Epoch [20/120    avg_loss:0.179, val_acc:0.901]
Epoch [21/120    avg_loss:0.163, val_acc:0.919]
Epoch [22/120    avg_loss:0.143, val_acc:0.936]
Epoch [23/120    avg_loss:0.126, val_acc:0.933]
Epoch [24/120    avg_loss:0.136, val_acc:0.921]
Epoch [25/120    avg_loss:0.118, val_acc:0.928]
Epoch [26/120    avg_loss:0.116, val_acc:0.923]
Epoch [27/120    avg_loss:0.106, val_acc:0.940]
Epoch [28/120    avg_loss:0.088, val_acc:0.949]
Epoch [29/120    avg_loss:0.080, val_acc:0.931]
Epoch [30/120    avg_loss:0.059, val_acc:0.933]
Epoch [31/120    avg_loss:0.095, val_acc:0.923]
Epoch [32/120    avg_loss:0.083, val_acc:0.949]
Epoch [33/120    avg_loss:0.087, val_acc:0.958]
Epoch [34/120    avg_loss:0.075, val_acc:0.960]
Epoch [35/120    avg_loss:0.066, val_acc:0.939]
Epoch [36/120    avg_loss:0.083, val_acc:0.952]
Epoch [37/120    avg_loss:0.067, val_acc:0.952]
Epoch [38/120    avg_loss:0.050, val_acc:0.934]
Epoch [39/120    avg_loss:0.059, val_acc:0.956]
Epoch [40/120    avg_loss:0.039, val_acc:0.962]
Epoch [41/120    avg_loss:0.035, val_acc:0.953]
Epoch [42/120    avg_loss:0.038, val_acc:0.961]
Epoch [43/120    avg_loss:0.031, val_acc:0.966]
Epoch [44/120    avg_loss:0.065, val_acc:0.940]
Epoch [45/120    avg_loss:0.063, val_acc:0.957]
Epoch [46/120    avg_loss:0.040, val_acc:0.886]
Epoch [47/120    avg_loss:0.965, val_acc:0.605]
Epoch [48/120    avg_loss:0.722, val_acc:0.651]
Epoch [49/120    avg_loss:0.677, val_acc:0.738]
Epoch [50/120    avg_loss:0.558, val_acc:0.766]
Epoch [51/120    avg_loss:0.538, val_acc:0.799]
Epoch [52/120    avg_loss:0.457, val_acc:0.804]
Epoch [53/120    avg_loss:0.467, val_acc:0.807]
Epoch [54/120    avg_loss:0.390, val_acc:0.778]
Epoch [55/120    avg_loss:0.399, val_acc:0.827]
Epoch [56/120    avg_loss:0.353, val_acc:0.841]
Epoch [57/120    avg_loss:0.289, val_acc:0.837]
Epoch [58/120    avg_loss:0.265, val_acc:0.854]
Epoch [59/120    avg_loss:0.247, val_acc:0.843]
Epoch [60/120    avg_loss:0.244, val_acc:0.867]
Epoch [61/120    avg_loss:0.243, val_acc:0.865]
Epoch [62/120    avg_loss:0.226, val_acc:0.877]
Epoch [63/120    avg_loss:0.223, val_acc:0.869]
Epoch [64/120    avg_loss:0.221, val_acc:0.861]
Epoch [65/120    avg_loss:0.199, val_acc:0.878]
Epoch [66/120    avg_loss:0.206, val_acc:0.890]
Epoch [67/120    avg_loss:0.203, val_acc:0.849]
Epoch [68/120    avg_loss:0.217, val_acc:0.843]
Epoch [69/120    avg_loss:0.221, val_acc:0.842]
Epoch [70/120    avg_loss:0.197, val_acc:0.875]
Epoch [71/120    avg_loss:0.181, val_acc:0.884]
Epoch [72/120    avg_loss:0.210, val_acc:0.882]
Epoch [73/120    avg_loss:0.198, val_acc:0.882]
Epoch [74/120    avg_loss:0.202, val_acc:0.876]
Epoch [75/120    avg_loss:0.197, val_acc:0.880]
Epoch [76/120    avg_loss:0.187, val_acc:0.887]
Epoch [77/120    avg_loss:0.196, val_acc:0.884]
Epoch [78/120    avg_loss:0.190, val_acc:0.885]
Epoch [79/120    avg_loss:0.186, val_acc:0.887]
Epoch [80/120    avg_loss:0.186, val_acc:0.882]
Epoch [81/120    avg_loss:0.199, val_acc:0.880]
Epoch [82/120    avg_loss:0.189, val_acc:0.881]
Epoch [83/120    avg_loss:0.200, val_acc:0.882]
Epoch [84/120    avg_loss:0.189, val_acc:0.878]
Epoch [85/120    avg_loss:0.202, val_acc:0.879]
Epoch [86/120    avg_loss:0.188, val_acc:0.878]
Epoch [87/120    avg_loss:0.194, val_acc:0.878]
Epoch [88/120    avg_loss:0.198, val_acc:0.878]
Epoch [89/120    avg_loss:0.188, val_acc:0.879]
Epoch [90/120    avg_loss:0.189, val_acc:0.881]
Epoch [91/120    avg_loss:0.181, val_acc:0.880]
Epoch [92/120    avg_loss:0.181, val_acc:0.881]
Epoch [93/120    avg_loss:0.178, val_acc:0.882]
Epoch [94/120    avg_loss:0.199, val_acc:0.882]
Epoch [95/120    avg_loss:0.182, val_acc:0.881]
Epoch [96/120    avg_loss:0.191, val_acc:0.881]
Epoch [97/120    avg_loss:0.196, val_acc:0.881]
Epoch [98/120    avg_loss:0.176, val_acc:0.881]
Epoch [99/120    avg_loss:0.194, val_acc:0.881]
Epoch [100/120    avg_loss:0.193, val_acc:0.881]
Epoch [101/120    avg_loss:0.203, val_acc:0.881]
Epoch [102/120    avg_loss:0.186, val_acc:0.881]
Epoch [103/120    avg_loss:0.182, val_acc:0.881]
Epoch [104/120    avg_loss:0.183, val_acc:0.881]
Epoch [105/120    avg_loss:0.191, val_acc:0.881]
Epoch [106/120    avg_loss:0.184, val_acc:0.881]
Epoch [107/120    avg_loss:0.194, val_acc:0.881]
Epoch [108/120    avg_loss:0.193, val_acc:0.881]
Epoch [109/120    avg_loss:0.195, val_acc:0.881]
Epoch [110/120    avg_loss:0.198, val_acc:0.881]
Epoch [111/120    avg_loss:0.181, val_acc:0.881]
Epoch [112/120    avg_loss:0.192, val_acc:0.881]
Epoch [113/120    avg_loss:0.186, val_acc:0.881]
Epoch [114/120    avg_loss:0.181, val_acc:0.881]
Epoch [115/120    avg_loss:0.198, val_acc:0.881]
Epoch [116/120    avg_loss:0.185, val_acc:0.881]
Epoch [117/120    avg_loss:0.178, val_acc:0.881]
Epoch [118/120    avg_loss:0.200, val_acc:0.881]
Epoch [119/120    avg_loss:0.178, val_acc:0.881]
Epoch [120/120    avg_loss:0.206, val_acc:0.881]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5679     2   149    34     0    94    28   292   154]
 [    0     1 17271     0   263     0   555     0     0     0]
 [    0    18     0  1819     0     0     0     0   165    34]
 [    0    47    35     0  2835     0    41     0    12     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    10   237     3     0     0  4614     0    14     0]
 [    0    60     0     0     0     0     0  1205    22     3]
 [    0   170    23   100    39     0    34     0  3204     1]
 [    0    14     0     0    20    34     0     0     0   851]]

Accuracy:
93.4687778661461

F1 scores:
[       nan 0.91368353 0.96870268 0.88580472 0.92000649 0.9871407
 0.90328896 0.95521205 0.88021978 0.86659878]

Kappa:
0.914031839373115
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8a3c6e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.014, val_acc:0.121]
Epoch [2/120    avg_loss:1.648, val_acc:0.268]
Epoch [3/120    avg_loss:1.349, val_acc:0.419]
Epoch [4/120    avg_loss:1.149, val_acc:0.451]
Epoch [5/120    avg_loss:0.933, val_acc:0.612]
Epoch [6/120    avg_loss:0.744, val_acc:0.642]
Epoch [7/120    avg_loss:0.646, val_acc:0.666]
Epoch [8/120    avg_loss:0.529, val_acc:0.734]
Epoch [9/120    avg_loss:0.449, val_acc:0.763]
Epoch [10/120    avg_loss:0.410, val_acc:0.806]
Epoch [11/120    avg_loss:0.357, val_acc:0.823]
Epoch [12/120    avg_loss:0.335, val_acc:0.877]
Epoch [13/120    avg_loss:0.310, val_acc:0.856]
Epoch [14/120    avg_loss:0.298, val_acc:0.840]
Epoch [15/120    avg_loss:0.248, val_acc:0.862]
Epoch [16/120    avg_loss:0.202, val_acc:0.927]
Epoch [17/120    avg_loss:0.192, val_acc:0.937]
Epoch [18/120    avg_loss:0.171, val_acc:0.907]
Epoch [19/120    avg_loss:0.169, val_acc:0.944]
Epoch [20/120    avg_loss:0.145, val_acc:0.922]
Epoch [21/120    avg_loss:0.141, val_acc:0.881]
Epoch [22/120    avg_loss:0.187, val_acc:0.893]
Epoch [23/120    avg_loss:0.168, val_acc:0.946]
Epoch [24/120    avg_loss:0.128, val_acc:0.946]
Epoch [25/120    avg_loss:0.099, val_acc:0.944]
Epoch [26/120    avg_loss:0.106, val_acc:0.958]
Epoch [27/120    avg_loss:0.084, val_acc:0.963]
Epoch [28/120    avg_loss:0.072, val_acc:0.963]
Epoch [29/120    avg_loss:0.075, val_acc:0.949]
Epoch [30/120    avg_loss:0.082, val_acc:0.968]
Epoch [31/120    avg_loss:0.072, val_acc:0.969]
Epoch [32/120    avg_loss:0.074, val_acc:0.963]
Epoch [33/120    avg_loss:0.063, val_acc:0.914]
Epoch [34/120    avg_loss:0.061, val_acc:0.967]
Epoch [35/120    avg_loss:0.043, val_acc:0.964]
Epoch [36/120    avg_loss:0.042, val_acc:0.967]
Epoch [37/120    avg_loss:0.051, val_acc:0.934]
Epoch [38/120    avg_loss:0.038, val_acc:0.973]
Epoch [39/120    avg_loss:0.031, val_acc:0.962]
Epoch [40/120    avg_loss:0.025, val_acc:0.973]
Epoch [41/120    avg_loss:0.039, val_acc:0.973]
Epoch [42/120    avg_loss:0.046, val_acc:0.967]
Epoch [43/120    avg_loss:0.061, val_acc:0.976]
Epoch [44/120    avg_loss:0.037, val_acc:0.973]
Epoch [45/120    avg_loss:0.018, val_acc:0.975]
Epoch [46/120    avg_loss:0.030, val_acc:0.957]
Epoch [47/120    avg_loss:0.028, val_acc:0.969]
Epoch [48/120    avg_loss:0.050, val_acc:0.955]
Epoch [49/120    avg_loss:0.045, val_acc:0.973]
Epoch [50/120    avg_loss:0.033, val_acc:0.972]
Epoch [51/120    avg_loss:0.036, val_acc:0.971]
Epoch [52/120    avg_loss:0.024, val_acc:0.969]
Epoch [53/120    avg_loss:0.038, val_acc:0.963]
Epoch [54/120    avg_loss:0.031, val_acc:0.976]
Epoch [55/120    avg_loss:0.016, val_acc:0.978]
Epoch [56/120    avg_loss:0.018, val_acc:0.976]
Epoch [57/120    avg_loss:0.014, val_acc:0.973]
Epoch [58/120    avg_loss:0.012, val_acc:0.978]
Epoch [59/120    avg_loss:0.009, val_acc:0.975]
Epoch [60/120    avg_loss:0.009, val_acc:0.971]
Epoch [61/120    avg_loss:0.009, val_acc:0.977]
Epoch [62/120    avg_loss:0.010, val_acc:0.976]
Epoch [63/120    avg_loss:0.012, val_acc:0.975]
Epoch [64/120    avg_loss:0.008, val_acc:0.978]
Epoch [65/120    avg_loss:0.011, val_acc:0.979]
Epoch [66/120    avg_loss:0.009, val_acc:0.978]
Epoch [67/120    avg_loss:0.010, val_acc:0.978]
Epoch [68/120    avg_loss:0.011, val_acc:0.978]
Epoch [69/120    avg_loss:0.015, val_acc:0.974]
Epoch [70/120    avg_loss:0.011, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.978]
Epoch [72/120    avg_loss:0.015, val_acc:0.978]
Epoch [73/120    avg_loss:0.014, val_acc:0.977]
Epoch [74/120    avg_loss:0.007, val_acc:0.980]
Epoch [75/120    avg_loss:0.014, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.969]
Epoch [77/120    avg_loss:0.007, val_acc:0.981]
Epoch [78/120    avg_loss:0.019, val_acc:0.971]
Epoch [79/120    avg_loss:0.033, val_acc:0.973]
Epoch [80/120    avg_loss:0.017, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.978]
Epoch [82/120    avg_loss:0.008, val_acc:0.978]
Epoch [83/120    avg_loss:0.011, val_acc:0.976]
Epoch [84/120    avg_loss:0.012, val_acc:0.972]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.008, val_acc:0.973]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.977]
Epoch [90/120    avg_loss:0.011, val_acc:0.979]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.004, val_acc:0.980]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.005, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.935]
Epoch [97/120    avg_loss:0.126, val_acc:0.936]
Epoch [98/120    avg_loss:0.044, val_acc:0.973]
Epoch [99/120    avg_loss:0.018, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.980]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.009, val_acc:0.976]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.017, val_acc:0.958]
Epoch [105/120    avg_loss:0.007, val_acc:0.978]
Epoch [106/120    avg_loss:0.012, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.979]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.005, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.004, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.982]
Epoch [113/120    avg_loss:0.004, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.004, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.981]
Epoch [119/120    avg_loss:0.005, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     0     0     3    11    34     0]
 [    0     0 18041     0    46     0     0     0     3     0]
 [    0     6     0  1967     0     0     0     0    55     8]
 [    0    36     5     0  2904     0    13     2    11     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    25     0     0     0  4847     0     6     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    30     0    23    43     0     0     0  3474     1]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
99.05044224326996

F1 scores:
[       nan 0.99068901 0.99781533 0.97714853 0.9713999  0.99315068
 0.99517503 0.9949865  0.97120492 0.97687225]

Kappa:
0.9874196185703283
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7bfd3cb908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.042, val_acc:0.193]
Epoch [2/120    avg_loss:1.617, val_acc:0.302]
Epoch [3/120    avg_loss:1.324, val_acc:0.404]
Epoch [4/120    avg_loss:1.049, val_acc:0.462]
Epoch [5/120    avg_loss:0.882, val_acc:0.501]
Epoch [6/120    avg_loss:0.706, val_acc:0.586]
Epoch [7/120    avg_loss:0.595, val_acc:0.671]
Epoch [8/120    avg_loss:0.513, val_acc:0.719]
Epoch [9/120    avg_loss:0.468, val_acc:0.741]
Epoch [10/120    avg_loss:0.410, val_acc:0.732]
Epoch [11/120    avg_loss:0.370, val_acc:0.804]
Epoch [12/120    avg_loss:0.321, val_acc:0.825]
Epoch [13/120    avg_loss:0.295, val_acc:0.849]
Epoch [14/120    avg_loss:0.257, val_acc:0.903]
Epoch [15/120    avg_loss:0.224, val_acc:0.901]
Epoch [16/120    avg_loss:0.220, val_acc:0.919]
Epoch [17/120    avg_loss:0.241, val_acc:0.932]
Epoch [18/120    avg_loss:0.157, val_acc:0.931]
Epoch [19/120    avg_loss:0.149, val_acc:0.938]
Epoch [20/120    avg_loss:0.150, val_acc:0.904]
Epoch [21/120    avg_loss:0.206, val_acc:0.928]
Epoch [22/120    avg_loss:0.127, val_acc:0.888]
Epoch [23/120    avg_loss:0.125, val_acc:0.949]
Epoch [24/120    avg_loss:0.130, val_acc:0.887]
Epoch [25/120    avg_loss:0.138, val_acc:0.946]
Epoch [26/120    avg_loss:0.106, val_acc:0.928]
Epoch [27/120    avg_loss:0.110, val_acc:0.947]
Epoch [28/120    avg_loss:0.082, val_acc:0.922]
Epoch [29/120    avg_loss:0.076, val_acc:0.914]
Epoch [30/120    avg_loss:0.068, val_acc:0.966]
Epoch [31/120    avg_loss:0.065, val_acc:0.965]
Epoch [32/120    avg_loss:0.050, val_acc:0.969]
Epoch [33/120    avg_loss:0.065, val_acc:0.948]
Epoch [34/120    avg_loss:0.098, val_acc:0.940]
Epoch [35/120    avg_loss:0.057, val_acc:0.958]
Epoch [36/120    avg_loss:0.076, val_acc:0.943]
Epoch [37/120    avg_loss:0.138, val_acc:0.941]
Epoch [38/120    avg_loss:0.121, val_acc:0.938]
Epoch [39/120    avg_loss:0.050, val_acc:0.958]
Epoch [40/120    avg_loss:0.051, val_acc:0.950]
Epoch [41/120    avg_loss:0.053, val_acc:0.968]
Epoch [42/120    avg_loss:0.050, val_acc:0.931]
Epoch [43/120    avg_loss:0.063, val_acc:0.956]
Epoch [44/120    avg_loss:0.051, val_acc:0.955]
Epoch [45/120    avg_loss:0.042, val_acc:0.975]
Epoch [46/120    avg_loss:0.044, val_acc:0.949]
Epoch [47/120    avg_loss:0.045, val_acc:0.973]
Epoch [48/120    avg_loss:0.037, val_acc:0.961]
Epoch [49/120    avg_loss:0.037, val_acc:0.971]
Epoch [50/120    avg_loss:0.032, val_acc:0.965]
Epoch [51/120    avg_loss:0.033, val_acc:0.965]
Epoch [52/120    avg_loss:0.023, val_acc:0.968]
Epoch [53/120    avg_loss:0.057, val_acc:0.966]
Epoch [54/120    avg_loss:0.044, val_acc:0.958]
Epoch [55/120    avg_loss:0.047, val_acc:0.970]
Epoch [56/120    avg_loss:0.020, val_acc:0.983]
Epoch [57/120    avg_loss:0.020, val_acc:0.966]
Epoch [58/120    avg_loss:0.056, val_acc:0.966]
Epoch [59/120    avg_loss:0.028, val_acc:0.978]
Epoch [60/120    avg_loss:0.021, val_acc:0.973]
Epoch [61/120    avg_loss:0.019, val_acc:0.951]
Epoch [62/120    avg_loss:0.028, val_acc:0.978]
Epoch [63/120    avg_loss:0.014, val_acc:0.979]
Epoch [64/120    avg_loss:0.010, val_acc:0.979]
Epoch [65/120    avg_loss:0.019, val_acc:0.968]
Epoch [66/120    avg_loss:0.044, val_acc:0.968]
Epoch [67/120    avg_loss:0.335, val_acc:0.754]
Epoch [68/120    avg_loss:0.458, val_acc:0.880]
Epoch [69/120    avg_loss:0.199, val_acc:0.903]
Epoch [70/120    avg_loss:0.122, val_acc:0.945]
Epoch [71/120    avg_loss:0.097, val_acc:0.946]
Epoch [72/120    avg_loss:0.094, val_acc:0.948]
Epoch [73/120    avg_loss:0.109, val_acc:0.951]
Epoch [74/120    avg_loss:0.080, val_acc:0.949]
Epoch [75/120    avg_loss:0.084, val_acc:0.953]
Epoch [76/120    avg_loss:0.081, val_acc:0.953]
Epoch [77/120    avg_loss:0.062, val_acc:0.953]
Epoch [78/120    avg_loss:0.068, val_acc:0.952]
Epoch [79/120    avg_loss:0.065, val_acc:0.956]
Epoch [80/120    avg_loss:0.066, val_acc:0.955]
Epoch [81/120    avg_loss:0.070, val_acc:0.960]
Epoch [82/120    avg_loss:0.057, val_acc:0.957]
Epoch [83/120    avg_loss:0.053, val_acc:0.958]
Epoch [84/120    avg_loss:0.053, val_acc:0.958]
Epoch [85/120    avg_loss:0.046, val_acc:0.958]
Epoch [86/120    avg_loss:0.049, val_acc:0.960]
Epoch [87/120    avg_loss:0.045, val_acc:0.959]
Epoch [88/120    avg_loss:0.056, val_acc:0.959]
Epoch [89/120    avg_loss:0.046, val_acc:0.959]
Epoch [90/120    avg_loss:0.046, val_acc:0.959]
Epoch [91/120    avg_loss:0.049, val_acc:0.961]
Epoch [92/120    avg_loss:0.050, val_acc:0.961]
Epoch [93/120    avg_loss:0.044, val_acc:0.961]
Epoch [94/120    avg_loss:0.048, val_acc:0.961]
Epoch [95/120    avg_loss:0.044, val_acc:0.961]
Epoch [96/120    avg_loss:0.047, val_acc:0.961]
Epoch [97/120    avg_loss:0.050, val_acc:0.961]
Epoch [98/120    avg_loss:0.045, val_acc:0.961]
Epoch [99/120    avg_loss:0.047, val_acc:0.961]
Epoch [100/120    avg_loss:0.052, val_acc:0.961]
Epoch [101/120    avg_loss:0.050, val_acc:0.961]
Epoch [102/120    avg_loss:0.043, val_acc:0.961]
Epoch [103/120    avg_loss:0.043, val_acc:0.961]
Epoch [104/120    avg_loss:0.043, val_acc:0.961]
Epoch [105/120    avg_loss:0.047, val_acc:0.961]
Epoch [106/120    avg_loss:0.045, val_acc:0.961]
Epoch [107/120    avg_loss:0.046, val_acc:0.961]
Epoch [108/120    avg_loss:0.046, val_acc:0.961]
Epoch [109/120    avg_loss:0.050, val_acc:0.961]
Epoch [110/120    avg_loss:0.047, val_acc:0.961]
Epoch [111/120    avg_loss:0.047, val_acc:0.961]
Epoch [112/120    avg_loss:0.049, val_acc:0.961]
Epoch [113/120    avg_loss:0.047, val_acc:0.961]
Epoch [114/120    avg_loss:0.053, val_acc:0.961]
Epoch [115/120    avg_loss:0.046, val_acc:0.961]
Epoch [116/120    avg_loss:0.050, val_acc:0.961]
Epoch [117/120    avg_loss:0.048, val_acc:0.961]
Epoch [118/120    avg_loss:0.053, val_acc:0.961]
Epoch [119/120    avg_loss:0.054, val_acc:0.961]
Epoch [120/120    avg_loss:0.048, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6114     0    35     2     0     0    22   221    38]
 [    0     0 17752     0    83     0   254     0     1     0]
 [    0    19     2  1871     0     0     0     0   135     9]
 [    0    28     4     0  2926     0    13     0     0     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     1     0  4853     0     0     0]
 [    0    15     0     0     0     0     3  1265     0     7]
 [    0    63     1    66    54     0    26     0  3359     2]
 [    0     0     0     0     7    18     0     0     0   894]]

Accuracy:
97.21880799170945

F1 scores:
[       nan 0.96503828 0.98971371 0.93363273 0.96807279 0.99315068
 0.96798644 0.98176174 0.92191574 0.95614973]

Kappa:
0.9632813760238524
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f199edee908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.999, val_acc:0.247]
Epoch [2/120    avg_loss:1.576, val_acc:0.276]
Epoch [3/120    avg_loss:1.299, val_acc:0.357]
Epoch [4/120    avg_loss:1.098, val_acc:0.621]
Epoch [5/120    avg_loss:0.898, val_acc:0.708]
Epoch [6/120    avg_loss:0.825, val_acc:0.617]
Epoch [7/120    avg_loss:0.755, val_acc:0.642]
Epoch [8/120    avg_loss:0.625, val_acc:0.642]
Epoch [9/120    avg_loss:0.576, val_acc:0.735]
Epoch [10/120    avg_loss:0.539, val_acc:0.662]
Epoch [11/120    avg_loss:0.459, val_acc:0.718]
Epoch [12/120    avg_loss:0.420, val_acc:0.826]
Epoch [13/120    avg_loss:0.366, val_acc:0.830]
Epoch [14/120    avg_loss:0.307, val_acc:0.858]
Epoch [15/120    avg_loss:0.277, val_acc:0.886]
Epoch [16/120    avg_loss:0.271, val_acc:0.913]
Epoch [17/120    avg_loss:0.240, val_acc:0.907]
Epoch [18/120    avg_loss:0.198, val_acc:0.897]
Epoch [19/120    avg_loss:0.173, val_acc:0.917]
Epoch [20/120    avg_loss:0.159, val_acc:0.919]
Epoch [21/120    avg_loss:0.159, val_acc:0.936]
Epoch [22/120    avg_loss:0.151, val_acc:0.940]
Epoch [23/120    avg_loss:0.140, val_acc:0.937]
Epoch [24/120    avg_loss:0.127, val_acc:0.949]
Epoch [25/120    avg_loss:0.117, val_acc:0.946]
Epoch [26/120    avg_loss:0.098, val_acc:0.952]
Epoch [27/120    avg_loss:0.124, val_acc:0.912]
Epoch [28/120    avg_loss:0.137, val_acc:0.943]
Epoch [29/120    avg_loss:0.087, val_acc:0.932]
Epoch [30/120    avg_loss:0.100, val_acc:0.950]
Epoch [31/120    avg_loss:0.087, val_acc:0.953]
Epoch [32/120    avg_loss:0.092, val_acc:0.954]
Epoch [33/120    avg_loss:0.077, val_acc:0.912]
Epoch [34/120    avg_loss:0.085, val_acc:0.949]
Epoch [35/120    avg_loss:0.077, val_acc:0.960]
Epoch [36/120    avg_loss:0.060, val_acc:0.965]
Epoch [37/120    avg_loss:0.044, val_acc:0.967]
Epoch [38/120    avg_loss:0.057, val_acc:0.948]
Epoch [39/120    avg_loss:0.049, val_acc:0.966]
Epoch [40/120    avg_loss:0.049, val_acc:0.948]
Epoch [41/120    avg_loss:0.038, val_acc:0.970]
Epoch [42/120    avg_loss:0.037, val_acc:0.955]
Epoch [43/120    avg_loss:0.052, val_acc:0.956]
Epoch [44/120    avg_loss:0.034, val_acc:0.971]
Epoch [45/120    avg_loss:0.026, val_acc:0.975]
Epoch [46/120    avg_loss:0.033, val_acc:0.975]
Epoch [47/120    avg_loss:0.040, val_acc:0.965]
Epoch [48/120    avg_loss:0.033, val_acc:0.973]
Epoch [49/120    avg_loss:0.034, val_acc:0.973]
Epoch [50/120    avg_loss:0.033, val_acc:0.979]
Epoch [51/120    avg_loss:0.024, val_acc:0.968]
Epoch [52/120    avg_loss:0.028, val_acc:0.973]
Epoch [53/120    avg_loss:0.039, val_acc:0.966]
Epoch [54/120    avg_loss:0.040, val_acc:0.968]
Epoch [55/120    avg_loss:0.020, val_acc:0.977]
Epoch [56/120    avg_loss:0.019, val_acc:0.973]
Epoch [57/120    avg_loss:0.023, val_acc:0.976]
Epoch [58/120    avg_loss:0.015, val_acc:0.977]
Epoch [59/120    avg_loss:0.021, val_acc:0.977]
Epoch [60/120    avg_loss:0.035, val_acc:0.978]
Epoch [61/120    avg_loss:0.022, val_acc:0.979]
Epoch [62/120    avg_loss:0.021, val_acc:0.974]
Epoch [63/120    avg_loss:0.038, val_acc:0.973]
Epoch [64/120    avg_loss:0.025, val_acc:0.976]
Epoch [65/120    avg_loss:0.013, val_acc:0.980]
Epoch [66/120    avg_loss:0.043, val_acc:0.964]
Epoch [67/120    avg_loss:0.025, val_acc:0.975]
Epoch [68/120    avg_loss:0.028, val_acc:0.979]
Epoch [69/120    avg_loss:0.014, val_acc:0.978]
Epoch [70/120    avg_loss:0.013, val_acc:0.974]
Epoch [71/120    avg_loss:0.011, val_acc:0.973]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.010, val_acc:0.962]
Epoch [74/120    avg_loss:0.013, val_acc:0.982]
Epoch [75/120    avg_loss:0.017, val_acc:0.979]
Epoch [76/120    avg_loss:0.011, val_acc:0.979]
Epoch [77/120    avg_loss:0.029, val_acc:0.979]
Epoch [78/120    avg_loss:0.016, val_acc:0.973]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.025, val_acc:0.974]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.007, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.020, val_acc:0.954]
Epoch [93/120    avg_loss:0.049, val_acc:0.942]
Epoch [94/120    avg_loss:0.051, val_acc:0.973]
Epoch [95/120    avg_loss:0.018, val_acc:0.976]
Epoch [96/120    avg_loss:0.016, val_acc:0.982]
Epoch [97/120    avg_loss:0.028, val_acc:0.978]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.981]
Epoch [100/120    avg_loss:0.012, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.014, val_acc:0.981]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6342     0     0     0     0     1     0    86     3]
 [    0     1 17997     0    52     0    38     0     2     0]
 [    0     4     0  1980     0     0     0     0    44     8]
 [    0    29     5     0  2908     0    15     0     7     8]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     5     0     0  4855     0     1     0]
 [    0     6     0     0     0     0     1  1282     0     1]
 [    0    77     0    29    56     0     0     2  3407     0]
 [    0     0     0     0     3    47     0     0     0   869]]

Accuracy:
98.67929530282217

F1 scores:
[       nan 0.98394229 0.9968152  0.97777778 0.97078952 0.98231088
 0.99203106 0.996115   0.95729137 0.96128319]

Kappa:
0.9825122396461833
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e104ea978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.959, val_acc:0.124]
Epoch [2/120    avg_loss:1.562, val_acc:0.201]
Epoch [3/120    avg_loss:1.342, val_acc:0.662]
Epoch [4/120    avg_loss:1.121, val_acc:0.572]
Epoch [5/120    avg_loss:0.948, val_acc:0.676]
Epoch [6/120    avg_loss:0.734, val_acc:0.662]
Epoch [7/120    avg_loss:0.599, val_acc:0.748]
Epoch [8/120    avg_loss:0.553, val_acc:0.657]
Epoch [9/120    avg_loss:0.464, val_acc:0.770]
Epoch [10/120    avg_loss:0.400, val_acc:0.793]
Epoch [11/120    avg_loss:0.360, val_acc:0.807]
Epoch [12/120    avg_loss:0.337, val_acc:0.865]
Epoch [13/120    avg_loss:0.311, val_acc:0.861]
Epoch [14/120    avg_loss:0.311, val_acc:0.733]
Epoch [15/120    avg_loss:0.306, val_acc:0.568]
Epoch [16/120    avg_loss:0.555, val_acc:0.818]
Epoch [17/120    avg_loss:0.276, val_acc:0.787]
Epoch [18/120    avg_loss:0.234, val_acc:0.855]
Epoch [19/120    avg_loss:0.208, val_acc:0.917]
Epoch [20/120    avg_loss:0.200, val_acc:0.886]
Epoch [21/120    avg_loss:0.410, val_acc:0.746]
Epoch [22/120    avg_loss:0.352, val_acc:0.873]
Epoch [23/120    avg_loss:0.238, val_acc:0.863]
Epoch [24/120    avg_loss:0.243, val_acc:0.873]
Epoch [25/120    avg_loss:0.199, val_acc:0.890]
Epoch [26/120    avg_loss:0.175, val_acc:0.908]
Epoch [27/120    avg_loss:0.171, val_acc:0.931]
Epoch [28/120    avg_loss:0.153, val_acc:0.925]
Epoch [29/120    avg_loss:0.177, val_acc:0.816]
Epoch [30/120    avg_loss:0.187, val_acc:0.938]
Epoch [31/120    avg_loss:0.127, val_acc:0.946]
Epoch [32/120    avg_loss:0.117, val_acc:0.953]
Epoch [33/120    avg_loss:0.079, val_acc:0.943]
Epoch [34/120    avg_loss:0.094, val_acc:0.948]
Epoch [35/120    avg_loss:0.127, val_acc:0.952]
Epoch [36/120    avg_loss:0.082, val_acc:0.950]
Epoch [37/120    avg_loss:0.104, val_acc:0.954]
Epoch [38/120    avg_loss:0.364, val_acc:0.536]
Epoch [39/120    avg_loss:0.988, val_acc:0.537]
Epoch [40/120    avg_loss:0.928, val_acc:0.514]
Epoch [41/120    avg_loss:0.838, val_acc:0.483]
Epoch [42/120    avg_loss:0.838, val_acc:0.496]
Epoch [43/120    avg_loss:0.794, val_acc:0.568]
Epoch [44/120    avg_loss:0.761, val_acc:0.611]
Epoch [45/120    avg_loss:0.723, val_acc:0.579]
Epoch [46/120    avg_loss:0.730, val_acc:0.582]
Epoch [47/120    avg_loss:0.714, val_acc:0.637]
Epoch [48/120    avg_loss:0.692, val_acc:0.660]
Epoch [49/120    avg_loss:0.675, val_acc:0.685]
Epoch [50/120    avg_loss:0.645, val_acc:0.703]
Epoch [51/120    avg_loss:0.646, val_acc:0.712]
Epoch [52/120    avg_loss:0.600, val_acc:0.708]
Epoch [53/120    avg_loss:0.603, val_acc:0.723]
Epoch [54/120    avg_loss:0.605, val_acc:0.713]
Epoch [55/120    avg_loss:0.586, val_acc:0.714]
Epoch [56/120    avg_loss:0.602, val_acc:0.700]
Epoch [57/120    avg_loss:0.589, val_acc:0.717]
Epoch [58/120    avg_loss:0.581, val_acc:0.706]
Epoch [59/120    avg_loss:0.597, val_acc:0.720]
Epoch [60/120    avg_loss:0.610, val_acc:0.723]
Epoch [61/120    avg_loss:0.575, val_acc:0.718]
Epoch [62/120    avg_loss:0.581, val_acc:0.728]
Epoch [63/120    avg_loss:0.578, val_acc:0.719]
Epoch [64/120    avg_loss:0.573, val_acc:0.719]
Epoch [65/120    avg_loss:0.574, val_acc:0.720]
Epoch [66/120    avg_loss:0.571, val_acc:0.721]
Epoch [67/120    avg_loss:0.550, val_acc:0.720]
Epoch [68/120    avg_loss:0.606, val_acc:0.722]
Epoch [69/120    avg_loss:0.567, val_acc:0.720]
Epoch [70/120    avg_loss:0.575, val_acc:0.722]
Epoch [71/120    avg_loss:0.570, val_acc:0.721]
Epoch [72/120    avg_loss:0.553, val_acc:0.723]
Epoch [73/120    avg_loss:0.580, val_acc:0.723]
Epoch [74/120    avg_loss:0.572, val_acc:0.722]
Epoch [75/120    avg_loss:0.582, val_acc:0.724]
Epoch [76/120    avg_loss:0.569, val_acc:0.722]
Epoch [77/120    avg_loss:0.567, val_acc:0.722]
Epoch [78/120    avg_loss:0.585, val_acc:0.722]
Epoch [79/120    avg_loss:0.593, val_acc:0.722]
Epoch [80/120    avg_loss:0.574, val_acc:0.723]
Epoch [81/120    avg_loss:0.567, val_acc:0.723]
Epoch [82/120    avg_loss:0.582, val_acc:0.724]
Epoch [83/120    avg_loss:0.586, val_acc:0.724]
Epoch [84/120    avg_loss:0.586, val_acc:0.724]
Epoch [85/120    avg_loss:0.566, val_acc:0.724]
Epoch [86/120    avg_loss:0.585, val_acc:0.723]
Epoch [87/120    avg_loss:0.579, val_acc:0.723]
Epoch [88/120    avg_loss:0.564, val_acc:0.724]
Epoch [89/120    avg_loss:0.582, val_acc:0.722]
Epoch [90/120    avg_loss:0.567, val_acc:0.722]
Epoch [91/120    avg_loss:0.563, val_acc:0.722]
Epoch [92/120    avg_loss:0.546, val_acc:0.722]
Epoch [93/120    avg_loss:0.566, val_acc:0.722]
Epoch [94/120    avg_loss:0.563, val_acc:0.722]
Epoch [95/120    avg_loss:0.564, val_acc:0.722]
Epoch [96/120    avg_loss:0.551, val_acc:0.722]
Epoch [97/120    avg_loss:0.558, val_acc:0.722]
Epoch [98/120    avg_loss:0.569, val_acc:0.722]
Epoch [99/120    avg_loss:0.580, val_acc:0.722]
Epoch [100/120    avg_loss:0.567, val_acc:0.722]
Epoch [101/120    avg_loss:0.567, val_acc:0.722]
Epoch [102/120    avg_loss:0.579, val_acc:0.722]
Epoch [103/120    avg_loss:0.574, val_acc:0.722]
Epoch [104/120    avg_loss:0.565, val_acc:0.722]
Epoch [105/120    avg_loss:0.571, val_acc:0.722]
Epoch [106/120    avg_loss:0.556, val_acc:0.722]
Epoch [107/120    avg_loss:0.585, val_acc:0.722]
Epoch [108/120    avg_loss:0.567, val_acc:0.722]
Epoch [109/120    avg_loss:0.573, val_acc:0.722]
Epoch [110/120    avg_loss:0.579, val_acc:0.722]
Epoch [111/120    avg_loss:0.586, val_acc:0.722]
Epoch [112/120    avg_loss:0.557, val_acc:0.722]
Epoch [113/120    avg_loss:0.533, val_acc:0.722]
Epoch [114/120    avg_loss:0.580, val_acc:0.722]
Epoch [115/120    avg_loss:0.573, val_acc:0.722]
Epoch [116/120    avg_loss:0.560, val_acc:0.722]
Epoch [117/120    avg_loss:0.555, val_acc:0.722]
Epoch [118/120    avg_loss:0.572, val_acc:0.722]
Epoch [119/120    avg_loss:0.583, val_acc:0.722]
Epoch [120/120    avg_loss:0.575, val_acc:0.722]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4269   620   109    40     0   886    56   250   202]
 [    0    24 12380     0  1075     0  4611     0     0     0]
 [    0    17     3  1554     0     0    26     0   392    44]
 [    0     0   332     0  2479     0   159     0     1     1]
 [    0     0     0     0     0  1303     0     2     0     0]
 [    0     0   694   158   179     0  3784     0    63     0]
 [    0    93     2     0    23     0     1  1149    22     0]
 [    0   160   199    37    17     0   346     0  2812     0]
 [    0    26     0     0    15    24     6     0     0   848]]

Accuracy:
73.69435808449619

F1 scores:
[       nan 0.77470284 0.76608911 0.798151   0.72911765 0.99012158
 0.51493502 0.92030437 0.79088736 0.84210526]

Kappa:
0.6656948728721824
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feeb75988d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.972, val_acc:0.099]
Epoch [2/120    avg_loss:1.547, val_acc:0.294]
Epoch [3/120    avg_loss:1.306, val_acc:0.412]
Epoch [4/120    avg_loss:1.144, val_acc:0.461]
Epoch [5/120    avg_loss:0.955, val_acc:0.509]
Epoch [6/120    avg_loss:0.785, val_acc:0.694]
Epoch [7/120    avg_loss:0.702, val_acc:0.670]
Epoch [8/120    avg_loss:0.528, val_acc:0.635]
Epoch [9/120    avg_loss:0.428, val_acc:0.699]
Epoch [10/120    avg_loss:0.380, val_acc:0.764]
Epoch [11/120    avg_loss:0.379, val_acc:0.782]
Epoch [12/120    avg_loss:0.324, val_acc:0.723]
Epoch [13/120    avg_loss:0.293, val_acc:0.849]
Epoch [14/120    avg_loss:0.296, val_acc:0.828]
Epoch [15/120    avg_loss:0.218, val_acc:0.865]
Epoch [16/120    avg_loss:0.217, val_acc:0.891]
Epoch [17/120    avg_loss:0.215, val_acc:0.909]
Epoch [18/120    avg_loss:0.166, val_acc:0.926]
Epoch [19/120    avg_loss:0.148, val_acc:0.899]
Epoch [20/120    avg_loss:0.151, val_acc:0.925]
Epoch [21/120    avg_loss:0.151, val_acc:0.907]
Epoch [22/120    avg_loss:0.138, val_acc:0.953]
Epoch [23/120    avg_loss:0.107, val_acc:0.916]
Epoch [24/120    avg_loss:0.098, val_acc:0.963]
Epoch [25/120    avg_loss:0.081, val_acc:0.872]
Epoch [26/120    avg_loss:0.081, val_acc:0.969]
Epoch [27/120    avg_loss:0.068, val_acc:0.968]
Epoch [28/120    avg_loss:0.059, val_acc:0.967]
Epoch [29/120    avg_loss:0.117, val_acc:0.882]
Epoch [30/120    avg_loss:0.115, val_acc:0.925]
Epoch [31/120    avg_loss:0.078, val_acc:0.953]
Epoch [32/120    avg_loss:0.089, val_acc:0.958]
Epoch [33/120    avg_loss:0.086, val_acc:0.961]
Epoch [34/120    avg_loss:0.074, val_acc:0.944]
Epoch [35/120    avg_loss:0.074, val_acc:0.950]
Epoch [36/120    avg_loss:0.056, val_acc:0.964]
Epoch [37/120    avg_loss:0.044, val_acc:0.970]
Epoch [38/120    avg_loss:0.070, val_acc:0.966]
Epoch [39/120    avg_loss:0.034, val_acc:0.975]
Epoch [40/120    avg_loss:0.026, val_acc:0.972]
Epoch [41/120    avg_loss:0.046, val_acc:0.960]
Epoch [42/120    avg_loss:0.037, val_acc:0.936]
Epoch [43/120    avg_loss:0.033, val_acc:0.974]
Epoch [44/120    avg_loss:0.022, val_acc:0.973]
Epoch [45/120    avg_loss:0.021, val_acc:0.980]
Epoch [46/120    avg_loss:0.067, val_acc:0.963]
Epoch [47/120    avg_loss:0.039, val_acc:0.975]
Epoch [48/120    avg_loss:0.038, val_acc:0.960]
Epoch [49/120    avg_loss:0.044, val_acc:0.943]
Epoch [50/120    avg_loss:0.037, val_acc:0.974]
Epoch [51/120    avg_loss:0.041, val_acc:0.973]
Epoch [52/120    avg_loss:0.028, val_acc:0.977]
Epoch [53/120    avg_loss:0.024, val_acc:0.977]
Epoch [54/120    avg_loss:0.027, val_acc:0.981]
Epoch [55/120    avg_loss:0.026, val_acc:0.972]
Epoch [56/120    avg_loss:0.027, val_acc:0.973]
Epoch [57/120    avg_loss:0.029, val_acc:0.972]
Epoch [58/120    avg_loss:0.014, val_acc:0.968]
Epoch [59/120    avg_loss:0.023, val_acc:0.976]
Epoch [60/120    avg_loss:0.020, val_acc:0.976]
Epoch [61/120    avg_loss:0.048, val_acc:0.943]
Epoch [62/120    avg_loss:0.039, val_acc:0.975]
Epoch [63/120    avg_loss:0.103, val_acc:0.942]
Epoch [64/120    avg_loss:0.084, val_acc:0.943]
Epoch [65/120    avg_loss:0.076, val_acc:0.958]
Epoch [66/120    avg_loss:0.178, val_acc:0.851]
Epoch [67/120    avg_loss:0.123, val_acc:0.951]
Epoch [68/120    avg_loss:0.070, val_acc:0.963]
Epoch [69/120    avg_loss:0.038, val_acc:0.968]
Epoch [70/120    avg_loss:0.041, val_acc:0.968]
Epoch [71/120    avg_loss:0.035, val_acc:0.970]
Epoch [72/120    avg_loss:0.038, val_acc:0.971]
Epoch [73/120    avg_loss:0.036, val_acc:0.977]
Epoch [74/120    avg_loss:0.033, val_acc:0.974]
Epoch [75/120    avg_loss:0.033, val_acc:0.979]
Epoch [76/120    avg_loss:0.025, val_acc:0.980]
Epoch [77/120    avg_loss:0.028, val_acc:0.978]
Epoch [78/120    avg_loss:0.025, val_acc:0.978]
Epoch [79/120    avg_loss:0.024, val_acc:0.981]
Epoch [80/120    avg_loss:0.019, val_acc:0.981]
Epoch [81/120    avg_loss:0.022, val_acc:0.981]
Epoch [82/120    avg_loss:0.022, val_acc:0.981]
Epoch [83/120    avg_loss:0.023, val_acc:0.980]
Epoch [84/120    avg_loss:0.022, val_acc:0.982]
Epoch [85/120    avg_loss:0.019, val_acc:0.983]
Epoch [86/120    avg_loss:0.022, val_acc:0.983]
Epoch [87/120    avg_loss:0.021, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.983]
Epoch [89/120    avg_loss:0.021, val_acc:0.981]
Epoch [90/120    avg_loss:0.025, val_acc:0.982]
Epoch [91/120    avg_loss:0.020, val_acc:0.982]
Epoch [92/120    avg_loss:0.023, val_acc:0.983]
Epoch [93/120    avg_loss:0.023, val_acc:0.981]
Epoch [94/120    avg_loss:0.026, val_acc:0.978]
Epoch [95/120    avg_loss:0.029, val_acc:0.982]
Epoch [96/120    avg_loss:0.019, val_acc:0.983]
Epoch [97/120    avg_loss:0.021, val_acc:0.984]
Epoch [98/120    avg_loss:0.020, val_acc:0.983]
Epoch [99/120    avg_loss:0.017, val_acc:0.982]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.018, val_acc:0.986]
Epoch [102/120    avg_loss:0.019, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.018, val_acc:0.986]
Epoch [105/120    avg_loss:0.015, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.986]
Epoch [107/120    avg_loss:0.015, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.987]
Epoch [110/120    avg_loss:0.018, val_acc:0.982]
Epoch [111/120    avg_loss:0.014, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.984]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.013, val_acc:0.985]
Epoch [116/120    avg_loss:0.016, val_acc:0.986]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.016, val_acc:0.986]
Epoch [119/120    avg_loss:0.013, val_acc:0.987]
Epoch [120/120    avg_loss:0.020, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6322     0     0     2     0    19     2    60    27]
 [    0     0 17925     0   105     0    57     0     3     0]
 [    0     1     0  1976     0     0     0     0    54     5]
 [    0    17     2     3  2917     0    16     3     7     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     1     0     0     0     0     0  1287     0     2]
 [    0    15     0     5    53     0     0     0  3494     4]
 [    0     0     1     0     7    30     0     0     0   881]]

Accuracy:
98.77328706046804

F1 scores:
[       nan 0.98873944 0.99530803 0.98308458 0.96334214 0.98863636
 0.9905555  0.99690163 0.97204062 0.95501355]

Kappa:
0.9837767078836352
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbfbe83a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.004, val_acc:0.231]
Epoch [2/120    avg_loss:1.586, val_acc:0.282]
Epoch [3/120    avg_loss:1.314, val_acc:0.477]
Epoch [4/120    avg_loss:1.115, val_acc:0.429]
Epoch [5/120    avg_loss:0.954, val_acc:0.651]
Epoch [6/120    avg_loss:0.743, val_acc:0.595]
Epoch [7/120    avg_loss:0.656, val_acc:0.622]
Epoch [8/120    avg_loss:0.558, val_acc:0.658]
Epoch [9/120    avg_loss:0.446, val_acc:0.709]
Epoch [10/120    avg_loss:0.409, val_acc:0.789]
Epoch [11/120    avg_loss:0.382, val_acc:0.776]
Epoch [12/120    avg_loss:0.348, val_acc:0.841]
Epoch [13/120    avg_loss:0.286, val_acc:0.823]
Epoch [14/120    avg_loss:0.247, val_acc:0.756]
Epoch [15/120    avg_loss:0.265, val_acc:0.889]
Epoch [16/120    avg_loss:0.202, val_acc:0.835]
Epoch [17/120    avg_loss:0.231, val_acc:0.821]
Epoch [18/120    avg_loss:0.192, val_acc:0.927]
Epoch [19/120    avg_loss:0.155, val_acc:0.897]
Epoch [20/120    avg_loss:0.148, val_acc:0.845]
Epoch [21/120    avg_loss:0.126, val_acc:0.930]
Epoch [22/120    avg_loss:0.145, val_acc:0.932]
Epoch [23/120    avg_loss:0.150, val_acc:0.895]
Epoch [24/120    avg_loss:0.125, val_acc:0.940]
Epoch [25/120    avg_loss:0.124, val_acc:0.894]
Epoch [26/120    avg_loss:0.145, val_acc:0.923]
Epoch [27/120    avg_loss:0.117, val_acc:0.929]
Epoch [28/120    avg_loss:0.134, val_acc:0.945]
Epoch [29/120    avg_loss:0.096, val_acc:0.946]
Epoch [30/120    avg_loss:0.077, val_acc:0.938]
Epoch [31/120    avg_loss:0.090, val_acc:0.932]
Epoch [32/120    avg_loss:0.073, val_acc:0.942]
Epoch [33/120    avg_loss:0.068, val_acc:0.945]
Epoch [34/120    avg_loss:0.062, val_acc:0.939]
Epoch [35/120    avg_loss:0.075, val_acc:0.927]
Epoch [36/120    avg_loss:0.050, val_acc:0.958]
Epoch [37/120    avg_loss:0.045, val_acc:0.965]
Epoch [38/120    avg_loss:0.075, val_acc:0.913]
Epoch [39/120    avg_loss:0.041, val_acc:0.951]
Epoch [40/120    avg_loss:0.045, val_acc:0.961]
Epoch [41/120    avg_loss:0.041, val_acc:0.953]
Epoch [42/120    avg_loss:0.042, val_acc:0.954]
Epoch [43/120    avg_loss:0.032, val_acc:0.971]
Epoch [44/120    avg_loss:0.028, val_acc:0.967]
Epoch [45/120    avg_loss:0.019, val_acc:0.965]
Epoch [46/120    avg_loss:0.042, val_acc:0.953]
Epoch [47/120    avg_loss:0.040, val_acc:0.970]
Epoch [48/120    avg_loss:0.085, val_acc:0.929]
Epoch [49/120    avg_loss:0.070, val_acc:0.943]
Epoch [50/120    avg_loss:0.085, val_acc:0.948]
Epoch [51/120    avg_loss:0.041, val_acc:0.957]
Epoch [52/120    avg_loss:0.053, val_acc:0.956]
Epoch [53/120    avg_loss:0.044, val_acc:0.896]
Epoch [54/120    avg_loss:0.041, val_acc:0.951]
Epoch [55/120    avg_loss:0.031, val_acc:0.961]
Epoch [56/120    avg_loss:0.022, val_acc:0.958]
Epoch [57/120    avg_loss:0.021, val_acc:0.970]
Epoch [58/120    avg_loss:0.018, val_acc:0.971]
Epoch [59/120    avg_loss:0.017, val_acc:0.971]
Epoch [60/120    avg_loss:0.014, val_acc:0.970]
Epoch [61/120    avg_loss:0.012, val_acc:0.969]
Epoch [62/120    avg_loss:0.014, val_acc:0.972]
Epoch [63/120    avg_loss:0.013, val_acc:0.970]
Epoch [64/120    avg_loss:0.009, val_acc:0.971]
Epoch [65/120    avg_loss:0.011, val_acc:0.974]
Epoch [66/120    avg_loss:0.011, val_acc:0.971]
Epoch [67/120    avg_loss:0.014, val_acc:0.969]
Epoch [68/120    avg_loss:0.011, val_acc:0.969]
Epoch [69/120    avg_loss:0.011, val_acc:0.972]
Epoch [70/120    avg_loss:0.012, val_acc:0.972]
Epoch [71/120    avg_loss:0.012, val_acc:0.969]
Epoch [72/120    avg_loss:0.011, val_acc:0.969]
Epoch [73/120    avg_loss:0.013, val_acc:0.968]
Epoch [74/120    avg_loss:0.011, val_acc:0.969]
Epoch [75/120    avg_loss:0.013, val_acc:0.968]
Epoch [76/120    avg_loss:0.010, val_acc:0.970]
Epoch [77/120    avg_loss:0.013, val_acc:0.969]
Epoch [78/120    avg_loss:0.009, val_acc:0.969]
Epoch [79/120    avg_loss:0.010, val_acc:0.969]
Epoch [80/120    avg_loss:0.011, val_acc:0.969]
Epoch [81/120    avg_loss:0.010, val_acc:0.969]
Epoch [82/120    avg_loss:0.010, val_acc:0.969]
Epoch [83/120    avg_loss:0.010, val_acc:0.969]
Epoch [84/120    avg_loss:0.012, val_acc:0.969]
Epoch [85/120    avg_loss:0.010, val_acc:0.969]
Epoch [86/120    avg_loss:0.011, val_acc:0.969]
Epoch [87/120    avg_loss:0.014, val_acc:0.969]
Epoch [88/120    avg_loss:0.011, val_acc:0.969]
Epoch [89/120    avg_loss:0.012, val_acc:0.969]
Epoch [90/120    avg_loss:0.010, val_acc:0.969]
Epoch [91/120    avg_loss:0.011, val_acc:0.969]
Epoch [92/120    avg_loss:0.011, val_acc:0.969]
Epoch [93/120    avg_loss:0.008, val_acc:0.969]
Epoch [94/120    avg_loss:0.009, val_acc:0.969]
Epoch [95/120    avg_loss:0.012, val_acc:0.969]
Epoch [96/120    avg_loss:0.010, val_acc:0.969]
Epoch [97/120    avg_loss:0.014, val_acc:0.969]
Epoch [98/120    avg_loss:0.010, val_acc:0.969]
Epoch [99/120    avg_loss:0.010, val_acc:0.969]
Epoch [100/120    avg_loss:0.011, val_acc:0.969]
Epoch [101/120    avg_loss:0.013, val_acc:0.969]
Epoch [102/120    avg_loss:0.011, val_acc:0.969]
Epoch [103/120    avg_loss:0.011, val_acc:0.969]
Epoch [104/120    avg_loss:0.009, val_acc:0.969]
Epoch [105/120    avg_loss:0.009, val_acc:0.969]
Epoch [106/120    avg_loss:0.013, val_acc:0.969]
Epoch [107/120    avg_loss:0.009, val_acc:0.969]
Epoch [108/120    avg_loss:0.009, val_acc:0.969]
Epoch [109/120    avg_loss:0.009, val_acc:0.969]
Epoch [110/120    avg_loss:0.014, val_acc:0.969]
Epoch [111/120    avg_loss:0.011, val_acc:0.969]
Epoch [112/120    avg_loss:0.008, val_acc:0.969]
Epoch [113/120    avg_loss:0.010, val_acc:0.969]
Epoch [114/120    avg_loss:0.011, val_acc:0.969]
Epoch [115/120    avg_loss:0.013, val_acc:0.969]
Epoch [116/120    avg_loss:0.010, val_acc:0.969]
Epoch [117/120    avg_loss:0.011, val_acc:0.969]
Epoch [118/120    avg_loss:0.009, val_acc:0.969]
Epoch [119/120    avg_loss:0.012, val_acc:0.969]
Epoch [120/120    avg_loss:0.011, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6278     0     0     2     0    21     5   124     2]
 [    0     0 17906     0    64     0   115     0     5     0]
 [    0     8     0  1887     0     0     0     0   140     1]
 [    0    29     5     0  2903     0    17     0    17     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1     7     0     0     0  4865     0     5     0]
 [    0     6     0     0     0     0     2  1273     0     9]
 [    0    28     0    57    50     0     0     0  3434     2]
 [    0     0     0     0     2    15     0     0     0   902]]

Accuracy:
98.21656664979635

F1 scores:
[       nan 0.98231889 0.99455677 0.94824121 0.96879693 0.99428571
 0.98302687 0.99143302 0.94133772 0.98257081]

Kappa:
0.9764126481937071
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff830099978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.957, val_acc:0.128]
Epoch [2/120    avg_loss:1.570, val_acc:0.219]
Epoch [3/120    avg_loss:1.359, val_acc:0.420]
Epoch [4/120    avg_loss:1.138, val_acc:0.463]
Epoch [5/120    avg_loss:0.957, val_acc:0.549]
Epoch [6/120    avg_loss:0.805, val_acc:0.656]
Epoch [7/120    avg_loss:0.675, val_acc:0.730]
Epoch [8/120    avg_loss:0.551, val_acc:0.713]
Epoch [9/120    avg_loss:0.461, val_acc:0.776]
Epoch [10/120    avg_loss:0.421, val_acc:0.787]
Epoch [11/120    avg_loss:0.354, val_acc:0.776]
Epoch [12/120    avg_loss:0.328, val_acc:0.819]
Epoch [13/120    avg_loss:0.274, val_acc:0.829]
Epoch [14/120    avg_loss:0.281, val_acc:0.839]
Epoch [15/120    avg_loss:0.259, val_acc:0.873]
Epoch [16/120    avg_loss:0.211, val_acc:0.901]
Epoch [17/120    avg_loss:0.177, val_acc:0.910]
Epoch [18/120    avg_loss:0.154, val_acc:0.920]
Epoch [19/120    avg_loss:0.157, val_acc:0.827]
Epoch [20/120    avg_loss:0.164, val_acc:0.868]
Epoch [21/120    avg_loss:0.160, val_acc:0.955]
Epoch [22/120    avg_loss:0.100, val_acc:0.977]
Epoch [23/120    avg_loss:0.099, val_acc:0.945]
Epoch [24/120    avg_loss:0.107, val_acc:0.961]
Epoch [25/120    avg_loss:0.102, val_acc:0.969]
Epoch [26/120    avg_loss:0.079, val_acc:0.978]
Epoch [27/120    avg_loss:0.070, val_acc:0.952]
Epoch [28/120    avg_loss:0.057, val_acc:0.982]
Epoch [29/120    avg_loss:0.073, val_acc:0.971]
Epoch [30/120    avg_loss:0.067, val_acc:0.975]
Epoch [31/120    avg_loss:0.049, val_acc:0.974]
Epoch [32/120    avg_loss:0.053, val_acc:0.974]
Epoch [33/120    avg_loss:0.059, val_acc:0.978]
Epoch [34/120    avg_loss:0.088, val_acc:0.968]
Epoch [35/120    avg_loss:0.069, val_acc:0.985]
Epoch [36/120    avg_loss:0.056, val_acc:0.875]
Epoch [37/120    avg_loss:0.054, val_acc:0.980]
Epoch [38/120    avg_loss:0.038, val_acc:0.986]
Epoch [39/120    avg_loss:0.037, val_acc:0.935]
Epoch [40/120    avg_loss:0.038, val_acc:0.985]
Epoch [41/120    avg_loss:0.029, val_acc:0.973]
Epoch [42/120    avg_loss:0.029, val_acc:0.980]
Epoch [43/120    avg_loss:0.030, val_acc:0.982]
Epoch [44/120    avg_loss:0.024, val_acc:0.986]
Epoch [45/120    avg_loss:0.072, val_acc:0.981]
Epoch [46/120    avg_loss:0.041, val_acc:0.986]
Epoch [47/120    avg_loss:0.047, val_acc:0.984]
Epoch [48/120    avg_loss:0.022, val_acc:0.986]
Epoch [49/120    avg_loss:0.023, val_acc:0.985]
Epoch [50/120    avg_loss:0.039, val_acc:0.991]
Epoch [51/120    avg_loss:0.030, val_acc:0.988]
Epoch [52/120    avg_loss:0.014, val_acc:0.990]
Epoch [53/120    avg_loss:0.015, val_acc:0.990]
Epoch [54/120    avg_loss:0.016, val_acc:0.974]
Epoch [55/120    avg_loss:0.023, val_acc:0.991]
Epoch [56/120    avg_loss:0.016, val_acc:0.993]
Epoch [57/120    avg_loss:0.011, val_acc:0.993]
Epoch [58/120    avg_loss:0.009, val_acc:0.993]
Epoch [59/120    avg_loss:0.011, val_acc:0.991]
Epoch [60/120    avg_loss:0.013, val_acc:0.989]
Epoch [61/120    avg_loss:0.016, val_acc:0.993]
Epoch [62/120    avg_loss:0.013, val_acc:0.991]
Epoch [63/120    avg_loss:0.009, val_acc:0.993]
Epoch [64/120    avg_loss:0.010, val_acc:0.992]
Epoch [65/120    avg_loss:0.008, val_acc:0.992]
Epoch [66/120    avg_loss:0.006, val_acc:0.992]
Epoch [67/120    avg_loss:0.014, val_acc:0.991]
Epoch [68/120    avg_loss:0.011, val_acc:0.992]
Epoch [69/120    avg_loss:0.009, val_acc:0.995]
Epoch [70/120    avg_loss:0.011, val_acc:0.988]
Epoch [71/120    avg_loss:0.027, val_acc:0.989]
Epoch [72/120    avg_loss:0.017, val_acc:0.990]
Epoch [73/120    avg_loss:0.012, val_acc:0.991]
Epoch [74/120    avg_loss:0.006, val_acc:0.993]
Epoch [75/120    avg_loss:0.006, val_acc:0.993]
Epoch [76/120    avg_loss:0.007, val_acc:0.989]
Epoch [77/120    avg_loss:0.013, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.993]
Epoch [79/120    avg_loss:0.011, val_acc:0.991]
Epoch [80/120    avg_loss:0.006, val_acc:0.992]
Epoch [81/120    avg_loss:0.005, val_acc:0.994]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.994]
Epoch [84/120    avg_loss:0.005, val_acc:0.994]
Epoch [85/120    avg_loss:0.005, val_acc:0.994]
Epoch [86/120    avg_loss:0.004, val_acc:0.994]
Epoch [87/120    avg_loss:0.005, val_acc:0.994]
Epoch [88/120    avg_loss:0.004, val_acc:0.994]
Epoch [89/120    avg_loss:0.004, val_acc:0.994]
Epoch [90/120    avg_loss:0.005, val_acc:0.994]
Epoch [91/120    avg_loss:0.005, val_acc:0.994]
Epoch [92/120    avg_loss:0.004, val_acc:0.994]
Epoch [93/120    avg_loss:0.005, val_acc:0.995]
Epoch [94/120    avg_loss:0.004, val_acc:0.995]
Epoch [95/120    avg_loss:0.004, val_acc:0.994]
Epoch [96/120    avg_loss:0.005, val_acc:0.994]
Epoch [97/120    avg_loss:0.004, val_acc:0.994]
Epoch [98/120    avg_loss:0.004, val_acc:0.994]
Epoch [99/120    avg_loss:0.005, val_acc:0.993]
Epoch [100/120    avg_loss:0.004, val_acc:0.993]
Epoch [101/120    avg_loss:0.005, val_acc:0.994]
Epoch [102/120    avg_loss:0.004, val_acc:0.994]
Epoch [103/120    avg_loss:0.004, val_acc:0.994]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.005, val_acc:0.994]
Epoch [106/120    avg_loss:0.005, val_acc:0.994]
Epoch [107/120    avg_loss:0.004, val_acc:0.994]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.004, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.004, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.004, val_acc:0.994]
Epoch [116/120    avg_loss:0.005, val_acc:0.994]
Epoch [117/120    avg_loss:0.004, val_acc:0.994]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.004, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     0     0     3    26    26     0]
 [    0     1 18055     0    29     0     5     0     0     0]
 [    0     7     0  2007     0     0     0     3     9    10]
 [    0    42     9     0  2902     0     6     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     6     0     0  4862     0     7     1]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0    34     0     6    58     0     0     0  3470     3]
 [    0     0     0     0    14    25     0     0     0   880]]

Accuracy:
99.15889427132288

F1 scores:
[       nan 0.98921896 0.99872774 0.98988903 0.97138075 0.99051233
 0.99692434 0.98733205 0.97801578 0.96862961]

Kappa:
0.9888575728662775
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b8afd89b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.001, val_acc:0.163]
Epoch [2/120    avg_loss:1.643, val_acc:0.264]
Epoch [3/120    avg_loss:1.384, val_acc:0.374]
Epoch [4/120    avg_loss:1.204, val_acc:0.421]
Epoch [5/120    avg_loss:1.040, val_acc:0.481]
Epoch [6/120    avg_loss:0.919, val_acc:0.539]
Epoch [7/120    avg_loss:0.798, val_acc:0.709]
Epoch [8/120    avg_loss:0.670, val_acc:0.692]
Epoch [9/120    avg_loss:0.591, val_acc:0.695]
Epoch [10/120    avg_loss:0.503, val_acc:0.802]
Epoch [11/120    avg_loss:0.442, val_acc:0.823]
Epoch [12/120    avg_loss:0.413, val_acc:0.774]
Epoch [13/120    avg_loss:0.334, val_acc:0.838]
Epoch [14/120    avg_loss:0.319, val_acc:0.881]
Epoch [15/120    avg_loss:0.318, val_acc:0.883]
Epoch [16/120    avg_loss:0.279, val_acc:0.874]
Epoch [17/120    avg_loss:0.263, val_acc:0.920]
Epoch [18/120    avg_loss:0.214, val_acc:0.917]
Epoch [19/120    avg_loss:0.242, val_acc:0.883]
Epoch [20/120    avg_loss:0.184, val_acc:0.924]
Epoch [21/120    avg_loss:0.134, val_acc:0.912]
Epoch [22/120    avg_loss:0.152, val_acc:0.911]
Epoch [23/120    avg_loss:0.137, val_acc:0.927]
Epoch [24/120    avg_loss:0.138, val_acc:0.889]
Epoch [25/120    avg_loss:0.133, val_acc:0.945]
Epoch [26/120    avg_loss:0.099, val_acc:0.954]
Epoch [27/120    avg_loss:0.096, val_acc:0.948]
Epoch [28/120    avg_loss:0.084, val_acc:0.949]
Epoch [29/120    avg_loss:0.084, val_acc:0.944]
Epoch [30/120    avg_loss:0.106, val_acc:0.952]
Epoch [31/120    avg_loss:0.115, val_acc:0.952]
Epoch [32/120    avg_loss:0.062, val_acc:0.966]
Epoch [33/120    avg_loss:0.068, val_acc:0.974]
Epoch [34/120    avg_loss:0.078, val_acc:0.945]
Epoch [35/120    avg_loss:0.079, val_acc:0.958]
Epoch [36/120    avg_loss:0.058, val_acc:0.955]
Epoch [37/120    avg_loss:0.077, val_acc:0.960]
Epoch [38/120    avg_loss:0.051, val_acc:0.957]
Epoch [39/120    avg_loss:0.064, val_acc:0.958]
Epoch [40/120    avg_loss:0.099, val_acc:0.952]
Epoch [41/120    avg_loss:0.068, val_acc:0.945]
Epoch [42/120    avg_loss:0.044, val_acc:0.974]
Epoch [43/120    avg_loss:0.039, val_acc:0.973]
Epoch [44/120    avg_loss:0.039, val_acc:0.973]
Epoch [45/120    avg_loss:0.033, val_acc:0.969]
Epoch [46/120    avg_loss:0.042, val_acc:0.897]
Epoch [47/120    avg_loss:0.067, val_acc:0.971]
Epoch [48/120    avg_loss:0.029, val_acc:0.969]
Epoch [49/120    avg_loss:0.021, val_acc:0.972]
Epoch [50/120    avg_loss:0.032, val_acc:0.958]
Epoch [51/120    avg_loss:0.047, val_acc:0.923]
Epoch [52/120    avg_loss:0.040, val_acc:0.975]
Epoch [53/120    avg_loss:0.031, val_acc:0.974]
Epoch [54/120    avg_loss:0.021, val_acc:0.980]
Epoch [55/120    avg_loss:0.024, val_acc:0.982]
Epoch [56/120    avg_loss:0.017, val_acc:0.980]
Epoch [57/120    avg_loss:0.021, val_acc:0.964]
Epoch [58/120    avg_loss:0.025, val_acc:0.978]
Epoch [59/120    avg_loss:0.034, val_acc:0.976]
Epoch [60/120    avg_loss:0.036, val_acc:0.975]
Epoch [61/120    avg_loss:0.019, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.981]
Epoch [63/120    avg_loss:0.009, val_acc:0.986]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.013, val_acc:0.980]
Epoch [66/120    avg_loss:0.015, val_acc:0.982]
Epoch [67/120    avg_loss:0.012, val_acc:0.985]
Epoch [68/120    avg_loss:0.016, val_acc:0.980]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.986]
Epoch [71/120    avg_loss:0.010, val_acc:0.983]
Epoch [72/120    avg_loss:0.019, val_acc:0.980]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.023, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.977]
Epoch [76/120    avg_loss:0.011, val_acc:0.980]
Epoch [77/120    avg_loss:0.009, val_acc:0.980]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.981]
Epoch [80/120    avg_loss:0.013, val_acc:0.978]
Epoch [81/120    avg_loss:0.025, val_acc:0.979]
Epoch [82/120    avg_loss:0.024, val_acc:0.962]
Epoch [83/120    avg_loss:0.033, val_acc:0.969]
Epoch [84/120    avg_loss:0.018, val_acc:0.979]
Epoch [85/120    avg_loss:0.010, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.980]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.980]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.011, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.980]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.007, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.006, val_acc:0.980]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.006, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.980]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.007, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.007, val_acc:0.980]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.980]
Epoch [118/120    avg_loss:0.007, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6337     0     0     0     0    18     5    72     0]
 [    0     0 17995     0    59     0    35     0     1     0]
 [    0     9     0  1982     0     0     0     0    42     3]
 [    0    23    14     0  2904     0    14     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     6     0     4     2     0  4865     0     1     0]
 [    0     3     0     0     0     3     0  1278     0     6]
 [    0    78     0    33    48     0     0     0  3412     0]
 [    0     0     0     0    17    55     0     0     1   846]]

Accuracy:
98.62868435639747

F1 scores:
[       nan 0.98339541 0.99698053 0.97755857 0.96767744 0.97826087
 0.99184506 0.99339293 0.95883097 0.95377678]

Kappa:
0.981843090327534
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1a62fa898>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.955, val_acc:0.226]
Epoch [2/120    avg_loss:1.584, val_acc:0.285]
Epoch [3/120    avg_loss:1.335, val_acc:0.378]
Epoch [4/120    avg_loss:1.113, val_acc:0.462]
Epoch [5/120    avg_loss:0.941, val_acc:0.673]
Epoch [6/120    avg_loss:0.808, val_acc:0.665]
Epoch [7/120    avg_loss:0.697, val_acc:0.631]
Epoch [8/120    avg_loss:0.581, val_acc:0.723]
Epoch [9/120    avg_loss:0.483, val_acc:0.767]
Epoch [10/120    avg_loss:0.414, val_acc:0.799]
Epoch [11/120    avg_loss:0.389, val_acc:0.777]
Epoch [12/120    avg_loss:0.348, val_acc:0.844]
Epoch [13/120    avg_loss:0.310, val_acc:0.894]
Epoch [14/120    avg_loss:0.254, val_acc:0.909]
Epoch [15/120    avg_loss:0.296, val_acc:0.858]
Epoch [16/120    avg_loss:0.229, val_acc:0.914]
Epoch [17/120    avg_loss:0.207, val_acc:0.936]
Epoch [18/120    avg_loss:0.213, val_acc:0.888]
Epoch [19/120    avg_loss:0.192, val_acc:0.897]
Epoch [20/120    avg_loss:0.171, val_acc:0.932]
Epoch [21/120    avg_loss:0.145, val_acc:0.953]
Epoch [22/120    avg_loss:0.134, val_acc:0.917]
Epoch [23/120    avg_loss:0.155, val_acc:0.933]
Epoch [24/120    avg_loss:0.168, val_acc:0.951]
Epoch [25/120    avg_loss:0.139, val_acc:0.958]
Epoch [26/120    avg_loss:0.101, val_acc:0.897]
Epoch [27/120    avg_loss:0.099, val_acc:0.966]
Epoch [28/120    avg_loss:0.083, val_acc:0.958]
Epoch [29/120    avg_loss:0.141, val_acc:0.910]
Epoch [30/120    avg_loss:0.101, val_acc:0.957]
Epoch [31/120    avg_loss:0.092, val_acc:0.943]
Epoch [32/120    avg_loss:0.115, val_acc:0.952]
Epoch [33/120    avg_loss:0.075, val_acc:0.941]
Epoch [34/120    avg_loss:0.044, val_acc:0.965]
Epoch [35/120    avg_loss:0.054, val_acc:0.966]
Epoch [36/120    avg_loss:0.052, val_acc:0.920]
Epoch [37/120    avg_loss:0.068, val_acc:0.968]
Epoch [38/120    avg_loss:0.043, val_acc:0.971]
Epoch [39/120    avg_loss:0.060, val_acc:0.946]
Epoch [40/120    avg_loss:0.064, val_acc:0.967]
Epoch [41/120    avg_loss:0.029, val_acc:0.973]
Epoch [42/120    avg_loss:0.052, val_acc:0.958]
Epoch [43/120    avg_loss:0.045, val_acc:0.969]
Epoch [44/120    avg_loss:0.029, val_acc:0.978]
Epoch [45/120    avg_loss:0.031, val_acc:0.973]
Epoch [46/120    avg_loss:0.038, val_acc:0.968]
Epoch [47/120    avg_loss:0.024, val_acc:0.976]
Epoch [48/120    avg_loss:0.026, val_acc:0.973]
Epoch [49/120    avg_loss:0.022, val_acc:0.973]
Epoch [50/120    avg_loss:0.026, val_acc:0.984]
Epoch [51/120    avg_loss:0.026, val_acc:0.976]
Epoch [52/120    avg_loss:0.029, val_acc:0.973]
Epoch [53/120    avg_loss:0.019, val_acc:0.975]
Epoch [54/120    avg_loss:0.032, val_acc:0.981]
Epoch [55/120    avg_loss:0.021, val_acc:0.953]
Epoch [56/120    avg_loss:0.017, val_acc:0.983]
Epoch [57/120    avg_loss:0.013, val_acc:0.983]
Epoch [58/120    avg_loss:0.051, val_acc:0.969]
Epoch [59/120    avg_loss:0.051, val_acc:0.956]
Epoch [60/120    avg_loss:0.583, val_acc:0.527]
Epoch [61/120    avg_loss:1.077, val_acc:0.626]
Epoch [62/120    avg_loss:0.944, val_acc:0.593]
Epoch [63/120    avg_loss:0.870, val_acc:0.641]
Epoch [64/120    avg_loss:0.813, val_acc:0.656]
Epoch [65/120    avg_loss:0.813, val_acc:0.663]
Epoch [66/120    avg_loss:0.840, val_acc:0.666]
Epoch [67/120    avg_loss:0.812, val_acc:0.673]
Epoch [68/120    avg_loss:0.786, val_acc:0.680]
Epoch [69/120    avg_loss:0.780, val_acc:0.671]
Epoch [70/120    avg_loss:0.785, val_acc:0.676]
Epoch [71/120    avg_loss:0.779, val_acc:0.673]
Epoch [72/120    avg_loss:0.800, val_acc:0.688]
Epoch [73/120    avg_loss:0.784, val_acc:0.683]
Epoch [74/120    avg_loss:0.784, val_acc:0.678]
Epoch [75/120    avg_loss:0.759, val_acc:0.693]
Epoch [76/120    avg_loss:0.753, val_acc:0.685]
Epoch [77/120    avg_loss:0.754, val_acc:0.689]
Epoch [78/120    avg_loss:0.762, val_acc:0.689]
Epoch [79/120    avg_loss:0.743, val_acc:0.689]
Epoch [80/120    avg_loss:0.769, val_acc:0.693]
Epoch [81/120    avg_loss:0.724, val_acc:0.693]
Epoch [82/120    avg_loss:0.742, val_acc:0.693]
Epoch [83/120    avg_loss:0.756, val_acc:0.690]
Epoch [84/120    avg_loss:0.758, val_acc:0.691]
Epoch [85/120    avg_loss:0.740, val_acc:0.691]
Epoch [86/120    avg_loss:0.740, val_acc:0.692]
Epoch [87/120    avg_loss:0.771, val_acc:0.689]
Epoch [88/120    avg_loss:0.725, val_acc:0.690]
Epoch [89/120    avg_loss:0.758, val_acc:0.689]
Epoch [90/120    avg_loss:0.737, val_acc:0.691]
Epoch [91/120    avg_loss:0.746, val_acc:0.692]
Epoch [92/120    avg_loss:0.755, val_acc:0.692]
Epoch [93/120    avg_loss:0.772, val_acc:0.692]
Epoch [94/120    avg_loss:0.763, val_acc:0.691]
Epoch [95/120    avg_loss:0.736, val_acc:0.691]
Epoch [96/120    avg_loss:0.771, val_acc:0.691]
Epoch [97/120    avg_loss:0.770, val_acc:0.690]
Epoch [98/120    avg_loss:0.756, val_acc:0.690]
Epoch [99/120    avg_loss:0.761, val_acc:0.691]
Epoch [100/120    avg_loss:0.742, val_acc:0.691]
Epoch [101/120    avg_loss:0.749, val_acc:0.691]
Epoch [102/120    avg_loss:0.751, val_acc:0.691]
Epoch [103/120    avg_loss:0.764, val_acc:0.691]
Epoch [104/120    avg_loss:0.748, val_acc:0.691]
Epoch [105/120    avg_loss:0.733, val_acc:0.691]
Epoch [106/120    avg_loss:0.772, val_acc:0.691]
Epoch [107/120    avg_loss:0.740, val_acc:0.691]
Epoch [108/120    avg_loss:0.763, val_acc:0.691]
Epoch [109/120    avg_loss:0.760, val_acc:0.691]
Epoch [110/120    avg_loss:0.741, val_acc:0.691]
Epoch [111/120    avg_loss:0.740, val_acc:0.691]
Epoch [112/120    avg_loss:0.753, val_acc:0.691]
Epoch [113/120    avg_loss:0.740, val_acc:0.691]
Epoch [114/120    avg_loss:0.742, val_acc:0.691]
Epoch [115/120    avg_loss:0.752, val_acc:0.691]
Epoch [116/120    avg_loss:0.773, val_acc:0.691]
Epoch [117/120    avg_loss:0.739, val_acc:0.691]
Epoch [118/120    avg_loss:0.764, val_acc:0.691]
Epoch [119/120    avg_loss:0.745, val_acc:0.691]
Epoch [120/120    avg_loss:0.766, val_acc:0.691]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  3771   716     0   176     0  1107    10   497   155]
 [    0     4 11649     0   533     0  4054     0  1850     0]
 [    0     6     3  1557     0     0    30     0   384    56]
 [    0     0   375     0  2242     0   340     0    14     1]
 [    0     0     0     0     0  1301     0     3     0     1]
 [    0     0   498   102   284     0  3994     0     0     0]
 [    0   124     2     0    38     0     3  1072    40    11]
 [    0   212   500    15    38     0   274     0  2532     0]
 [    0    30     0     2    15    54    23     0     1   794]]

Accuracy:
69.67922300147013

F1 scores:
[       nan 0.71292183 0.73188201 0.83890086 0.71197205 0.97819549
 0.54329048 0.90273684 0.56969288 0.81982447]

Kappa:
0.6167417568993254
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda732979b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.004, val_acc:0.155]
Epoch [2/120    avg_loss:1.627, val_acc:0.237]
Epoch [3/120    avg_loss:1.358, val_acc:0.419]
Epoch [4/120    avg_loss:1.184, val_acc:0.453]
Epoch [5/120    avg_loss:0.992, val_acc:0.493]
Epoch [6/120    avg_loss:0.841, val_acc:0.509]
Epoch [7/120    avg_loss:0.715, val_acc:0.580]
Epoch [8/120    avg_loss:0.599, val_acc:0.576]
Epoch [9/120    avg_loss:0.583, val_acc:0.659]
Epoch [10/120    avg_loss:0.476, val_acc:0.752]
Epoch [11/120    avg_loss:0.422, val_acc:0.736]
Epoch [12/120    avg_loss:0.362, val_acc:0.770]
Epoch [13/120    avg_loss:0.352, val_acc:0.831]
Epoch [14/120    avg_loss:0.289, val_acc:0.793]
Epoch [15/120    avg_loss:0.261, val_acc:0.866]
Epoch [16/120    avg_loss:0.281, val_acc:0.810]
Epoch [17/120    avg_loss:0.223, val_acc:0.862]
Epoch [18/120    avg_loss:0.213, val_acc:0.890]
Epoch [19/120    avg_loss:0.207, val_acc:0.864]
Epoch [20/120    avg_loss:0.215, val_acc:0.894]
Epoch [21/120    avg_loss:0.183, val_acc:0.916]
Epoch [22/120    avg_loss:0.182, val_acc:0.866]
Epoch [23/120    avg_loss:0.175, val_acc:0.925]
Epoch [24/120    avg_loss:0.166, val_acc:0.929]
Epoch [25/120    avg_loss:0.151, val_acc:0.850]
Epoch [26/120    avg_loss:0.154, val_acc:0.953]
Epoch [27/120    avg_loss:0.146, val_acc:0.953]
Epoch [28/120    avg_loss:0.104, val_acc:0.957]
Epoch [29/120    avg_loss:0.083, val_acc:0.962]
Epoch [30/120    avg_loss:0.074, val_acc:0.973]
Epoch [31/120    avg_loss:0.062, val_acc:0.955]
Epoch [32/120    avg_loss:0.063, val_acc:0.971]
Epoch [33/120    avg_loss:0.059, val_acc:0.969]
Epoch [34/120    avg_loss:0.053, val_acc:0.951]
Epoch [35/120    avg_loss:0.132, val_acc:0.940]
Epoch [36/120    avg_loss:0.148, val_acc:0.944]
Epoch [37/120    avg_loss:0.089, val_acc:0.956]
Epoch [38/120    avg_loss:0.068, val_acc:0.954]
Epoch [39/120    avg_loss:0.055, val_acc:0.963]
Epoch [40/120    avg_loss:0.075, val_acc:0.969]
Epoch [41/120    avg_loss:0.045, val_acc:0.965]
Epoch [42/120    avg_loss:0.076, val_acc:0.973]
Epoch [43/120    avg_loss:0.052, val_acc:0.962]
Epoch [44/120    avg_loss:0.044, val_acc:0.982]
Epoch [45/120    avg_loss:0.039, val_acc:0.965]
Epoch [46/120    avg_loss:0.025, val_acc:0.989]
Epoch [47/120    avg_loss:0.029, val_acc:0.986]
Epoch [48/120    avg_loss:0.027, val_acc:0.985]
Epoch [49/120    avg_loss:0.019, val_acc:0.986]
Epoch [50/120    avg_loss:0.026, val_acc:0.983]
Epoch [51/120    avg_loss:0.018, val_acc:0.983]
Epoch [52/120    avg_loss:0.024, val_acc:0.969]
Epoch [53/120    avg_loss:0.019, val_acc:0.983]
Epoch [54/120    avg_loss:0.019, val_acc:0.986]
Epoch [55/120    avg_loss:0.018, val_acc:0.986]
Epoch [56/120    avg_loss:0.031, val_acc:0.984]
Epoch [57/120    avg_loss:0.019, val_acc:0.990]
Epoch [58/120    avg_loss:0.030, val_acc:0.979]
Epoch [59/120    avg_loss:0.030, val_acc:0.977]
Epoch [60/120    avg_loss:0.022, val_acc:0.985]
Epoch [61/120    avg_loss:0.018, val_acc:0.975]
Epoch [62/120    avg_loss:0.016, val_acc:0.986]
Epoch [63/120    avg_loss:0.016, val_acc:0.984]
Epoch [64/120    avg_loss:0.028, val_acc:0.988]
Epoch [65/120    avg_loss:0.020, val_acc:0.984]
Epoch [66/120    avg_loss:0.015, val_acc:0.986]
Epoch [67/120    avg_loss:0.024, val_acc:0.989]
Epoch [68/120    avg_loss:0.015, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.988]
Epoch [70/120    avg_loss:0.019, val_acc:0.991]
Epoch [71/120    avg_loss:0.027, val_acc:0.988]
Epoch [72/120    avg_loss:0.017, val_acc:0.989]
Epoch [73/120    avg_loss:0.016, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.991]
Epoch [75/120    avg_loss:0.007, val_acc:0.990]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.992]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.990]
Epoch [80/120    avg_loss:0.009, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.992]
Epoch [82/120    avg_loss:0.008, val_acc:0.989]
Epoch [83/120    avg_loss:0.009, val_acc:0.991]
Epoch [84/120    avg_loss:0.010, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.020, val_acc:0.983]
Epoch [87/120    avg_loss:0.020, val_acc:0.988]
Epoch [88/120    avg_loss:0.013, val_acc:0.984]
Epoch [89/120    avg_loss:0.011, val_acc:0.992]
Epoch [90/120    avg_loss:0.009, val_acc:0.992]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.991]
Epoch [95/120    avg_loss:0.005, val_acc:0.993]
Epoch [96/120    avg_loss:0.004, val_acc:0.992]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.987]
Epoch [101/120    avg_loss:0.027, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.013, val_acc:0.990]
Epoch [104/120    avg_loss:0.009, val_acc:0.991]
Epoch [105/120    avg_loss:0.011, val_acc:0.991]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.993]
Epoch [108/120    avg_loss:0.005, val_acc:0.993]
Epoch [109/120    avg_loss:0.009, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.993]
Epoch [111/120    avg_loss:0.004, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.022, val_acc:0.991]
Epoch [114/120    avg_loss:0.018, val_acc:0.980]
Epoch [115/120    avg_loss:0.010, val_acc:0.989]
Epoch [116/120    avg_loss:0.020, val_acc:0.968]
Epoch [117/120    avg_loss:0.019, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.991]
Epoch [119/120    avg_loss:0.010, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     0     0     0     0    40     0]
 [    0     9 18059     0    13     0     6     0     3     0]
 [    0     5     0  1992     0     0     0     0    28    11]
 [    0    37    14     2  2904     0     4     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    62     1     0     0  4803     0    12     0]
 [    0     6     0     0     0     0     1  1283     0     0]
 [    0    15     0     9    51     0     0     0  3496     0]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.07695273901622

F1 scores:
[       nan 0.99131514 0.99704624 0.98613861 0.97547867 0.98901099
 0.9911267  0.99727944 0.97639994 0.97009967]

Kappa:
0.9877621010173091
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd42a638978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.984, val_acc:0.197]
Epoch [2/120    avg_loss:1.594, val_acc:0.224]
Epoch [3/120    avg_loss:1.372, val_acc:0.378]
Epoch [4/120    avg_loss:1.139, val_acc:0.409]
Epoch [5/120    avg_loss:0.998, val_acc:0.628]
Epoch [6/120    avg_loss:0.870, val_acc:0.642]
Epoch [7/120    avg_loss:0.752, val_acc:0.668]
Epoch [8/120    avg_loss:0.669, val_acc:0.713]
Epoch [9/120    avg_loss:0.569, val_acc:0.782]
Epoch [10/120    avg_loss:0.476, val_acc:0.741]
Epoch [11/120    avg_loss:0.434, val_acc:0.821]
Epoch [12/120    avg_loss:0.375, val_acc:0.805]
Epoch [13/120    avg_loss:0.349, val_acc:0.793]
Epoch [14/120    avg_loss:0.292, val_acc:0.855]
Epoch [15/120    avg_loss:0.268, val_acc:0.888]
Epoch [16/120    avg_loss:0.351, val_acc:0.878]
Epoch [17/120    avg_loss:0.229, val_acc:0.909]
Epoch [18/120    avg_loss:0.201, val_acc:0.898]
Epoch [19/120    avg_loss:0.212, val_acc:0.923]
Epoch [20/120    avg_loss:0.152, val_acc:0.913]
Epoch [21/120    avg_loss:0.135, val_acc:0.938]
Epoch [22/120    avg_loss:0.189, val_acc:0.917]
Epoch [23/120    avg_loss:0.167, val_acc:0.911]
Epoch [24/120    avg_loss:0.145, val_acc:0.939]
Epoch [25/120    avg_loss:0.100, val_acc:0.945]
Epoch [26/120    avg_loss:0.094, val_acc:0.922]
Epoch [27/120    avg_loss:0.091, val_acc:0.954]
Epoch [28/120    avg_loss:0.076, val_acc:0.969]
Epoch [29/120    avg_loss:0.061, val_acc:0.959]
Epoch [30/120    avg_loss:0.053, val_acc:0.967]
Epoch [31/120    avg_loss:0.049, val_acc:0.946]
Epoch [32/120    avg_loss:0.054, val_acc:0.958]
Epoch [33/120    avg_loss:0.056, val_acc:0.967]
Epoch [34/120    avg_loss:0.081, val_acc:0.966]
Epoch [35/120    avg_loss:0.059, val_acc:0.950]
Epoch [36/120    avg_loss:0.053, val_acc:0.936]
Epoch [37/120    avg_loss:0.083, val_acc:0.957]
Epoch [38/120    avg_loss:0.070, val_acc:0.958]
Epoch [39/120    avg_loss:0.050, val_acc:0.956]
Epoch [40/120    avg_loss:0.038, val_acc:0.973]
Epoch [41/120    avg_loss:0.033, val_acc:0.972]
Epoch [42/120    avg_loss:0.027, val_acc:0.973]
Epoch [43/120    avg_loss:0.021, val_acc:0.978]
Epoch [44/120    avg_loss:0.028, val_acc:0.978]
Epoch [45/120    avg_loss:0.026, val_acc:0.975]
Epoch [46/120    avg_loss:0.027, val_acc:0.971]
Epoch [47/120    avg_loss:0.023, val_acc:0.973]
Epoch [48/120    avg_loss:0.026, val_acc:0.971]
Epoch [49/120    avg_loss:0.022, val_acc:0.976]
Epoch [50/120    avg_loss:0.021, val_acc:0.975]
Epoch [51/120    avg_loss:0.029, val_acc:0.964]
Epoch [52/120    avg_loss:0.035, val_acc:0.968]
Epoch [53/120    avg_loss:0.050, val_acc:0.971]
Epoch [54/120    avg_loss:0.021, val_acc:0.973]
Epoch [55/120    avg_loss:0.046, val_acc:0.961]
Epoch [56/120    avg_loss:0.033, val_acc:0.971]
Epoch [57/120    avg_loss:0.020, val_acc:0.973]
Epoch [58/120    avg_loss:0.018, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.978]
Epoch [60/120    avg_loss:0.012, val_acc:0.978]
Epoch [61/120    avg_loss:0.012, val_acc:0.980]
Epoch [62/120    avg_loss:0.013, val_acc:0.979]
Epoch [63/120    avg_loss:0.012, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.979]
Epoch [66/120    avg_loss:0.012, val_acc:0.978]
Epoch [67/120    avg_loss:0.010, val_acc:0.980]
Epoch [68/120    avg_loss:0.013, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.981]
Epoch [70/120    avg_loss:0.009, val_acc:0.981]
Epoch [71/120    avg_loss:0.010, val_acc:0.981]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.015, val_acc:0.981]
Epoch [74/120    avg_loss:0.011, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.982]
Epoch [76/120    avg_loss:0.011, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.010, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.981]
Epoch [81/120    avg_loss:0.010, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.012, val_acc:0.980]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.984]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.981]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.014, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     1     2     0     6     7    77     0]
 [    0     1 17952     0    58     0    69     0    10     0]
 [    0    15     0  1979     0     0     0     0    34     8]
 [    0    22     8     0  2900     0    16     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     3     0     8     0     0  4861     0     6     0]
 [    0    10     0     0     0     0     0  1279     0     1]
 [    0    27     0     3    52     0     0     0  3489     0]
 [    0     0     0     0    13    39     0     0     0   867]]

Accuracy:
98.74195647458608

F1 scores:
[       nan 0.98669157 0.99595007 0.98286566 0.96715024 0.98527746
 0.98901322 0.99301242 0.96741994 0.96601671]

Kappa:
0.9833527330192641
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0942f5e9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.032, val_acc:0.189]
Epoch [2/120    avg_loss:1.696, val_acc:0.256]
Epoch [3/120    avg_loss:1.374, val_acc:0.352]
Epoch [4/120    avg_loss:1.187, val_acc:0.366]
Epoch [5/120    avg_loss:1.031, val_acc:0.445]
Epoch [6/120    avg_loss:0.891, val_acc:0.679]
Epoch [7/120    avg_loss:0.789, val_acc:0.707]
Epoch [8/120    avg_loss:0.670, val_acc:0.671]
Epoch [9/120    avg_loss:0.570, val_acc:0.685]
Epoch [10/120    avg_loss:0.491, val_acc:0.734]
Epoch [11/120    avg_loss:0.407, val_acc:0.801]
Epoch [12/120    avg_loss:0.348, val_acc:0.793]
Epoch [13/120    avg_loss:0.322, val_acc:0.798]
Epoch [14/120    avg_loss:0.298, val_acc:0.827]
Epoch [15/120    avg_loss:0.253, val_acc:0.861]
Epoch [16/120    avg_loss:0.204, val_acc:0.918]
Epoch [17/120    avg_loss:0.280, val_acc:0.885]
Epoch [18/120    avg_loss:0.196, val_acc:0.918]
Epoch [19/120    avg_loss:0.166, val_acc:0.858]
Epoch [20/120    avg_loss:0.169, val_acc:0.928]
Epoch [21/120    avg_loss:0.121, val_acc:0.960]
Epoch [22/120    avg_loss:0.116, val_acc:0.927]
Epoch [23/120    avg_loss:0.114, val_acc:0.932]
Epoch [24/120    avg_loss:0.151, val_acc:0.923]
Epoch [25/120    avg_loss:0.192, val_acc:0.943]
Epoch [26/120    avg_loss:0.134, val_acc:0.950]
Epoch [27/120    avg_loss:0.119, val_acc:0.958]
Epoch [28/120    avg_loss:0.080, val_acc:0.963]
Epoch [29/120    avg_loss:0.062, val_acc:0.966]
Epoch [30/120    avg_loss:0.064, val_acc:0.963]
Epoch [31/120    avg_loss:0.053, val_acc:0.968]
Epoch [32/120    avg_loss:0.075, val_acc:0.971]
Epoch [33/120    avg_loss:0.054, val_acc:0.942]
Epoch [34/120    avg_loss:0.065, val_acc:0.965]
Epoch [35/120    avg_loss:0.049, val_acc:0.965]
Epoch [36/120    avg_loss:0.039, val_acc:0.976]
Epoch [37/120    avg_loss:0.033, val_acc:0.960]
Epoch [38/120    avg_loss:0.040, val_acc:0.965]
Epoch [39/120    avg_loss:0.031, val_acc:0.955]
Epoch [40/120    avg_loss:0.042, val_acc:0.969]
Epoch [41/120    avg_loss:0.033, val_acc:0.974]
Epoch [42/120    avg_loss:0.028, val_acc:0.975]
Epoch [43/120    avg_loss:0.027, val_acc:0.974]
Epoch [44/120    avg_loss:0.030, val_acc:0.974]
Epoch [45/120    avg_loss:0.015, val_acc:0.980]
Epoch [46/120    avg_loss:0.017, val_acc:0.981]
Epoch [47/120    avg_loss:0.018, val_acc:0.977]
Epoch [48/120    avg_loss:0.016, val_acc:0.983]
Epoch [49/120    avg_loss:0.012, val_acc:0.978]
Epoch [50/120    avg_loss:0.012, val_acc:0.982]
Epoch [51/120    avg_loss:0.030, val_acc:0.977]
Epoch [52/120    avg_loss:0.011, val_acc:0.979]
Epoch [53/120    avg_loss:0.040, val_acc:0.955]
Epoch [54/120    avg_loss:0.053, val_acc:0.951]
Epoch [55/120    avg_loss:0.079, val_acc:0.963]
Epoch [56/120    avg_loss:0.030, val_acc:0.979]
Epoch [57/120    avg_loss:0.026, val_acc:0.975]
Epoch [58/120    avg_loss:0.019, val_acc:0.976]
Epoch [59/120    avg_loss:0.014, val_acc:0.977]
Epoch [60/120    avg_loss:0.028, val_acc:0.966]
Epoch [61/120    avg_loss:0.016, val_acc:0.981]
Epoch [62/120    avg_loss:0.017, val_acc:0.982]
Epoch [63/120    avg_loss:0.013, val_acc:0.985]
Epoch [64/120    avg_loss:0.013, val_acc:0.984]
Epoch [65/120    avg_loss:0.011, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.983]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.011, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.009, val_acc:0.985]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.981]
Epoch [89/120    avg_loss:0.011, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     0     3     0     0     9    58     0]
 [    0     0 18006     0    63     0    14     0     7     0]
 [    0     9     0  1955     0     0     0     6    62     4]
 [    0    37     7     0  2905     0     4     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     5     5     1     0     0  4866     0     1     0]
 [    0     2     0     0     0     0     0  1281     0     7]
 [    0     7     0    18    49     0     0     0  3497     0]
 [    0     0     0     0    12    42     0     0     0   865]]

Accuracy:
98.91306967440292

F1 scores:
[       nan 0.98988642 0.99734131 0.97506234 0.96768821 0.9841629
 0.99692686 0.99071926 0.96936937 0.9637883 ]

Kappa:
0.9856086119207134
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8caf30c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.032, val_acc:0.222]
Epoch [2/120    avg_loss:1.664, val_acc:0.306]
Epoch [3/120    avg_loss:1.370, val_acc:0.373]
Epoch [4/120    avg_loss:1.118, val_acc:0.583]
Epoch [5/120    avg_loss:0.944, val_acc:0.637]
Epoch [6/120    avg_loss:0.791, val_acc:0.677]
Epoch [7/120    avg_loss:0.660, val_acc:0.747]
Epoch [8/120    avg_loss:0.568, val_acc:0.752]
Epoch [9/120    avg_loss:0.474, val_acc:0.747]
Epoch [10/120    avg_loss:0.396, val_acc:0.838]
Epoch [11/120    avg_loss:0.391, val_acc:0.827]
Epoch [12/120    avg_loss:0.396, val_acc:0.863]
Epoch [13/120    avg_loss:0.323, val_acc:0.852]
Epoch [14/120    avg_loss:0.270, val_acc:0.880]
Epoch [15/120    avg_loss:0.268, val_acc:0.911]
Epoch [16/120    avg_loss:0.234, val_acc:0.865]
Epoch [17/120    avg_loss:0.221, val_acc:0.912]
Epoch [18/120    avg_loss:0.200, val_acc:0.919]
Epoch [19/120    avg_loss:0.186, val_acc:0.934]
Epoch [20/120    avg_loss:0.159, val_acc:0.943]
Epoch [21/120    avg_loss:0.160, val_acc:0.932]
Epoch [22/120    avg_loss:0.177, val_acc:0.902]
Epoch [23/120    avg_loss:0.169, val_acc:0.891]
Epoch [24/120    avg_loss:0.199, val_acc:0.927]
Epoch [25/120    avg_loss:0.134, val_acc:0.923]
Epoch [26/120    avg_loss:0.115, val_acc:0.939]
Epoch [27/120    avg_loss:0.120, val_acc:0.961]
Epoch [28/120    avg_loss:0.101, val_acc:0.928]
Epoch [29/120    avg_loss:0.085, val_acc:0.956]
Epoch [30/120    avg_loss:0.080, val_acc:0.950]
Epoch [31/120    avg_loss:0.105, val_acc:0.949]
Epoch [32/120    avg_loss:0.084, val_acc:0.968]
Epoch [33/120    avg_loss:0.053, val_acc:0.942]
Epoch [34/120    avg_loss:0.054, val_acc:0.963]
Epoch [35/120    avg_loss:0.073, val_acc:0.934]
Epoch [36/120    avg_loss:0.069, val_acc:0.964]
Epoch [37/120    avg_loss:0.043, val_acc:0.978]
Epoch [38/120    avg_loss:0.055, val_acc:0.962]
Epoch [39/120    avg_loss:0.063, val_acc:0.963]
Epoch [40/120    avg_loss:0.063, val_acc:0.963]
Epoch [41/120    avg_loss:0.051, val_acc:0.943]
Epoch [42/120    avg_loss:0.044, val_acc:0.927]
Epoch [43/120    avg_loss:0.036, val_acc:0.978]
Epoch [44/120    avg_loss:0.061, val_acc:0.967]
Epoch [45/120    avg_loss:0.037, val_acc:0.974]
Epoch [46/120    avg_loss:0.031, val_acc:0.974]
Epoch [47/120    avg_loss:0.038, val_acc:0.943]
Epoch [48/120    avg_loss:0.048, val_acc:0.971]
Epoch [49/120    avg_loss:0.032, val_acc:0.960]
Epoch [50/120    avg_loss:0.033, val_acc:0.968]
Epoch [51/120    avg_loss:0.023, val_acc:0.975]
Epoch [52/120    avg_loss:0.027, val_acc:0.974]
Epoch [53/120    avg_loss:0.039, val_acc:0.969]
Epoch [54/120    avg_loss:0.031, val_acc:0.967]
Epoch [55/120    avg_loss:0.044, val_acc:0.974]
Epoch [56/120    avg_loss:0.078, val_acc:0.968]
Epoch [57/120    avg_loss:0.028, val_acc:0.974]
Epoch [58/120    avg_loss:0.022, val_acc:0.976]
Epoch [59/120    avg_loss:0.023, val_acc:0.977]
Epoch [60/120    avg_loss:0.025, val_acc:0.978]
Epoch [61/120    avg_loss:0.018, val_acc:0.978]
Epoch [62/120    avg_loss:0.016, val_acc:0.978]
Epoch [63/120    avg_loss:0.018, val_acc:0.978]
Epoch [64/120    avg_loss:0.015, val_acc:0.978]
Epoch [65/120    avg_loss:0.018, val_acc:0.978]
Epoch [66/120    avg_loss:0.017, val_acc:0.978]
Epoch [67/120    avg_loss:0.017, val_acc:0.980]
Epoch [68/120    avg_loss:0.020, val_acc:0.978]
Epoch [69/120    avg_loss:0.013, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.978]
Epoch [71/120    avg_loss:0.014, val_acc:0.981]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.016, val_acc:0.981]
Epoch [75/120    avg_loss:0.013, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.015, val_acc:0.981]
Epoch [79/120    avg_loss:0.015, val_acc:0.978]
Epoch [80/120    avg_loss:0.020, val_acc:0.980]
Epoch [81/120    avg_loss:0.012, val_acc:0.983]
Epoch [82/120    avg_loss:0.011, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.011, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.014, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     1     0    63     0]
 [    0     3 17989     0    82     0    16     0     0     0]
 [    0    11     0  1963     0     0     0     0    55     7]
 [    0    33    13     0  2891     0    14     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1     1     0     0     0  4876     0     0     0]
 [    0     1     0     0     0     0     1  1285     0     3]
 [    0    26     0    54    60     0     0     0  3430     1]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
98.76364688019666

F1 scores:
[       nan 0.98920388 0.99681379 0.96866519 0.96062469 0.98788796
 0.99652565 0.99805825 0.96078431 0.96838602]

Kappa:
0.9836323951004958
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f750d13e978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.056, val_acc:0.134]
Epoch [2/120    avg_loss:1.622, val_acc:0.328]
Epoch [3/120    avg_loss:1.333, val_acc:0.688]
Epoch [4/120    avg_loss:1.113, val_acc:0.622]
Epoch [5/120    avg_loss:0.960, val_acc:0.770]
Epoch [6/120    avg_loss:0.781, val_acc:0.755]
Epoch [7/120    avg_loss:0.645, val_acc:0.823]
Epoch [8/120    avg_loss:0.578, val_acc:0.846]
Epoch [9/120    avg_loss:0.467, val_acc:0.798]
Epoch [10/120    avg_loss:0.406, val_acc:0.837]
Epoch [11/120    avg_loss:0.362, val_acc:0.913]
Epoch [12/120    avg_loss:0.301, val_acc:0.901]
Epoch [13/120    avg_loss:0.296, val_acc:0.922]
Epoch [14/120    avg_loss:0.264, val_acc:0.882]
Epoch [15/120    avg_loss:0.249, val_acc:0.921]
Epoch [16/120    avg_loss:0.212, val_acc:0.940]
Epoch [17/120    avg_loss:0.198, val_acc:0.927]
Epoch [18/120    avg_loss:0.156, val_acc:0.910]
Epoch [19/120    avg_loss:0.147, val_acc:0.915]
Epoch [20/120    avg_loss:0.179, val_acc:0.944]
Epoch [21/120    avg_loss:0.187, val_acc:0.919]
Epoch [22/120    avg_loss:0.158, val_acc:0.882]
Epoch [23/120    avg_loss:0.128, val_acc:0.948]
Epoch [24/120    avg_loss:0.126, val_acc:0.878]
Epoch [25/120    avg_loss:0.103, val_acc:0.957]
Epoch [26/120    avg_loss:0.091, val_acc:0.922]
Epoch [27/120    avg_loss:0.176, val_acc:0.901]
Epoch [28/120    avg_loss:0.122, val_acc:0.929]
Epoch [29/120    avg_loss:0.115, val_acc:0.949]
Epoch [30/120    avg_loss:0.093, val_acc:0.945]
Epoch [31/120    avg_loss:0.082, val_acc:0.947]
Epoch [32/120    avg_loss:0.067, val_acc:0.960]
Epoch [33/120    avg_loss:0.067, val_acc:0.969]
Epoch [34/120    avg_loss:0.062, val_acc:0.963]
Epoch [35/120    avg_loss:0.048, val_acc:0.970]
Epoch [36/120    avg_loss:0.044, val_acc:0.962]
Epoch [37/120    avg_loss:0.058, val_acc:0.957]
Epoch [38/120    avg_loss:0.061, val_acc:0.940]
Epoch [39/120    avg_loss:0.089, val_acc:0.946]
Epoch [40/120    avg_loss:0.085, val_acc:0.964]
Epoch [41/120    avg_loss:0.060, val_acc:0.969]
Epoch [42/120    avg_loss:0.037, val_acc:0.978]
Epoch [43/120    avg_loss:0.035, val_acc:0.974]
Epoch [44/120    avg_loss:0.031, val_acc:0.961]
Epoch [45/120    avg_loss:0.036, val_acc:0.981]
Epoch [46/120    avg_loss:0.031, val_acc:0.977]
Epoch [47/120    avg_loss:0.026, val_acc:0.973]
Epoch [48/120    avg_loss:0.033, val_acc:0.961]
Epoch [49/120    avg_loss:0.037, val_acc:0.978]
Epoch [50/120    avg_loss:0.043, val_acc:0.938]
Epoch [51/120    avg_loss:0.039, val_acc:0.979]
Epoch [52/120    avg_loss:0.029, val_acc:0.977]
Epoch [53/120    avg_loss:0.024, val_acc:0.983]
Epoch [54/120    avg_loss:0.032, val_acc:0.974]
Epoch [55/120    avg_loss:0.030, val_acc:0.979]
Epoch [56/120    avg_loss:0.035, val_acc:0.982]
Epoch [57/120    avg_loss:0.024, val_acc:0.975]
Epoch [58/120    avg_loss:0.027, val_acc:0.978]
Epoch [59/120    avg_loss:0.022, val_acc:0.971]
Epoch [60/120    avg_loss:0.014, val_acc:0.971]
Epoch [61/120    avg_loss:0.022, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.971]
Epoch [63/120    avg_loss:0.024, val_acc:0.976]
Epoch [64/120    avg_loss:0.019, val_acc:0.978]
Epoch [65/120    avg_loss:0.016, val_acc:0.982]
Epoch [66/120    avg_loss:0.022, val_acc:0.982]
Epoch [67/120    avg_loss:0.016, val_acc:0.983]
Epoch [68/120    avg_loss:0.010, val_acc:0.983]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.009, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.012, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.014, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     0     0     0     0     5     4    15     0]
 [    0     1 18050     0    29     0     8     0     2     0]
 [    0     0     0  1952     0     0     0     0    81     3]
 [    0    28     3     0  2919     0     6     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1     0     2     0     0  4875     0     0     0]
 [    0     1     0     0     0     0     0  1282     0     7]
 [    0     1     0    17    49     0     0     0  3503     1]
 [    0     1     0     1    16    62     0     0     0   839]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.99557213 0.99881028 0.9740519  0.9754386  0.97679641
 0.99774867 0.99534161 0.97468002 0.94855851]

Kappa:
0.9885067917257638
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f661bedc9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.007, val_acc:0.126]
Epoch [2/120    avg_loss:1.602, val_acc:0.272]
Epoch [3/120    avg_loss:1.325, val_acc:0.459]
Epoch [4/120    avg_loss:1.094, val_acc:0.531]
Epoch [5/120    avg_loss:0.881, val_acc:0.586]
Epoch [6/120    avg_loss:0.749, val_acc:0.690]
Epoch [7/120    avg_loss:0.611, val_acc:0.677]
Epoch [8/120    avg_loss:0.501, val_acc:0.775]
Epoch [9/120    avg_loss:0.478, val_acc:0.704]
Epoch [10/120    avg_loss:0.425, val_acc:0.781]
Epoch [11/120    avg_loss:0.359, val_acc:0.819]
Epoch [12/120    avg_loss:0.328, val_acc:0.823]
Epoch [13/120    avg_loss:0.275, val_acc:0.888]
Epoch [14/120    avg_loss:0.266, val_acc:0.832]
Epoch [15/120    avg_loss:0.279, val_acc:0.891]
Epoch [16/120    avg_loss:0.218, val_acc:0.869]
Epoch [17/120    avg_loss:0.185, val_acc:0.917]
Epoch [18/120    avg_loss:0.173, val_acc:0.878]
Epoch [19/120    avg_loss:0.190, val_acc:0.945]
Epoch [20/120    avg_loss:0.135, val_acc:0.929]
Epoch [21/120    avg_loss:0.112, val_acc:0.949]
Epoch [22/120    avg_loss:0.143, val_acc:0.936]
Epoch [23/120    avg_loss:0.137, val_acc:0.946]
Epoch [24/120    avg_loss:0.114, val_acc:0.904]
Epoch [25/120    avg_loss:0.094, val_acc:0.947]
Epoch [26/120    avg_loss:0.086, val_acc:0.963]
Epoch [27/120    avg_loss:0.114, val_acc:0.934]
Epoch [28/120    avg_loss:0.135, val_acc:0.958]
Epoch [29/120    avg_loss:0.063, val_acc:0.964]
Epoch [30/120    avg_loss:0.057, val_acc:0.955]
Epoch [31/120    avg_loss:0.060, val_acc:0.975]
Epoch [32/120    avg_loss:0.058, val_acc:0.969]
Epoch [33/120    avg_loss:0.052, val_acc:0.975]
Epoch [34/120    avg_loss:0.045, val_acc:0.978]
Epoch [35/120    avg_loss:0.028, val_acc:0.970]
Epoch [36/120    avg_loss:0.046, val_acc:0.981]
Epoch [37/120    avg_loss:0.042, val_acc:0.976]
Epoch [38/120    avg_loss:0.025, val_acc:0.983]
Epoch [39/120    avg_loss:0.032, val_acc:0.979]
Epoch [40/120    avg_loss:0.029, val_acc:0.983]
Epoch [41/120    avg_loss:0.024, val_acc:0.974]
Epoch [42/120    avg_loss:0.036, val_acc:0.964]
Epoch [43/120    avg_loss:0.037, val_acc:0.976]
Epoch [44/120    avg_loss:0.039, val_acc:0.973]
Epoch [45/120    avg_loss:0.025, val_acc:0.987]
Epoch [46/120    avg_loss:0.018, val_acc:0.989]
Epoch [47/120    avg_loss:0.023, val_acc:0.931]
Epoch [48/120    avg_loss:0.032, val_acc:0.983]
Epoch [49/120    avg_loss:0.015, val_acc:0.989]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.039, val_acc:0.972]
Epoch [52/120    avg_loss:0.052, val_acc:0.975]
Epoch [53/120    avg_loss:0.057, val_acc:0.946]
Epoch [54/120    avg_loss:0.046, val_acc:0.976]
Epoch [55/120    avg_loss:0.036, val_acc:0.985]
Epoch [56/120    avg_loss:0.025, val_acc:0.983]
Epoch [57/120    avg_loss:0.020, val_acc:0.987]
Epoch [58/120    avg_loss:0.014, val_acc:0.990]
Epoch [59/120    avg_loss:0.019, val_acc:0.968]
Epoch [60/120    avg_loss:0.035, val_acc:0.980]
Epoch [61/120    avg_loss:0.022, val_acc:0.982]
Epoch [62/120    avg_loss:0.016, val_acc:0.991]
Epoch [63/120    avg_loss:0.033, val_acc:0.984]
Epoch [64/120    avg_loss:0.023, val_acc:0.988]
Epoch [65/120    avg_loss:0.045, val_acc:0.992]
Epoch [66/120    avg_loss:0.017, val_acc:0.990]
Epoch [67/120    avg_loss:0.020, val_acc:0.985]
Epoch [68/120    avg_loss:0.013, val_acc:0.990]
Epoch [69/120    avg_loss:0.010, val_acc:0.991]
Epoch [70/120    avg_loss:0.010, val_acc:0.990]
Epoch [71/120    avg_loss:0.012, val_acc:0.990]
Epoch [72/120    avg_loss:0.013, val_acc:0.974]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.019, val_acc:0.990]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.016, val_acc:0.985]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.011, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.991]
Epoch [81/120    avg_loss:0.010, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.006, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.004, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.005, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.007, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.009, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.015, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     3     0     0     0     0    12     6]
 [    0     0 17986     0    65     0    39     0     0     0]
 [    0     0     0  2025     0     0     0     0     6     5]
 [    0    22     2     0  2927     0     0     0    20     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21    14     3     0  4838     0     1     1]
 [    0     4     0     0     0     0     0  1285     0     1]
 [    0     2     0    40    58     0     0     0  3421    50]
 [    0     0     0     0     3    39     0     0     0   877]]

Accuracy:
98.99260116164173

F1 scores:
[       nan 0.99619299 0.9964819  0.98348713 0.9711347  0.98527746
 0.99190159 0.99805825 0.97311904 0.94301075]

Kappa:
0.9866658634441772
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14ae13a9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.004, val_acc:0.264]
Epoch [2/120    avg_loss:1.616, val_acc:0.268]
Epoch [3/120    avg_loss:1.357, val_acc:0.305]
Epoch [4/120    avg_loss:1.161, val_acc:0.621]
Epoch [5/120    avg_loss:0.969, val_acc:0.531]
Epoch [6/120    avg_loss:0.840, val_acc:0.738]
Epoch [7/120    avg_loss:0.727, val_acc:0.668]
Epoch [8/120    avg_loss:0.631, val_acc:0.686]
Epoch [9/120    avg_loss:0.530, val_acc:0.757]
Epoch [10/120    avg_loss:0.438, val_acc:0.774]
Epoch [11/120    avg_loss:0.406, val_acc:0.810]
Epoch [12/120    avg_loss:0.321, val_acc:0.795]
Epoch [13/120    avg_loss:0.308, val_acc:0.906]
Epoch [14/120    avg_loss:0.242, val_acc:0.876]
Epoch [15/120    avg_loss:0.244, val_acc:0.852]
Epoch [16/120    avg_loss:0.202, val_acc:0.925]
Epoch [17/120    avg_loss:0.187, val_acc:0.916]
Epoch [18/120    avg_loss:0.194, val_acc:0.949]
Epoch [19/120    avg_loss:0.161, val_acc:0.885]
Epoch [20/120    avg_loss:0.112, val_acc:0.947]
Epoch [21/120    avg_loss:0.110, val_acc:0.945]
Epoch [22/120    avg_loss:0.130, val_acc:0.928]
Epoch [23/120    avg_loss:0.096, val_acc:0.962]
Epoch [24/120    avg_loss:0.086, val_acc:0.950]
Epoch [25/120    avg_loss:0.072, val_acc:0.951]
Epoch [26/120    avg_loss:0.134, val_acc:0.947]
Epoch [27/120    avg_loss:0.131, val_acc:0.950]
Epoch [28/120    avg_loss:0.097, val_acc:0.963]
Epoch [29/120    avg_loss:0.070, val_acc:0.961]
Epoch [30/120    avg_loss:0.067, val_acc:0.957]
Epoch [31/120    avg_loss:0.072, val_acc:0.957]
Epoch [32/120    avg_loss:0.043, val_acc:0.965]
Epoch [33/120    avg_loss:0.045, val_acc:0.974]
Epoch [34/120    avg_loss:0.053, val_acc:0.970]
Epoch [35/120    avg_loss:0.054, val_acc:0.976]
Epoch [36/120    avg_loss:0.043, val_acc:0.984]
Epoch [37/120    avg_loss:0.032, val_acc:0.955]
Epoch [38/120    avg_loss:0.047, val_acc:0.975]
Epoch [39/120    avg_loss:0.028, val_acc:0.978]
Epoch [40/120    avg_loss:0.020, val_acc:0.984]
Epoch [41/120    avg_loss:0.024, val_acc:0.985]
Epoch [42/120    avg_loss:0.023, val_acc:0.974]
Epoch [43/120    avg_loss:0.031, val_acc:0.963]
Epoch [44/120    avg_loss:0.032, val_acc:0.981]
Epoch [45/120    avg_loss:0.022, val_acc:0.972]
Epoch [46/120    avg_loss:0.024, val_acc:0.985]
Epoch [47/120    avg_loss:0.017, val_acc:0.976]
Epoch [48/120    avg_loss:0.018, val_acc:0.986]
Epoch [49/120    avg_loss:0.020, val_acc:0.982]
Epoch [50/120    avg_loss:0.028, val_acc:0.980]
Epoch [51/120    avg_loss:0.017, val_acc:0.988]
Epoch [52/120    avg_loss:0.023, val_acc:0.978]
Epoch [53/120    avg_loss:0.026, val_acc:0.979]
Epoch [54/120    avg_loss:0.011, val_acc:0.980]
Epoch [55/120    avg_loss:0.022, val_acc:0.983]
Epoch [56/120    avg_loss:0.019, val_acc:0.976]
Epoch [57/120    avg_loss:0.015, val_acc:0.989]
Epoch [58/120    avg_loss:0.013, val_acc:0.975]
Epoch [59/120    avg_loss:0.015, val_acc:0.959]
Epoch [60/120    avg_loss:0.039, val_acc:0.980]
Epoch [61/120    avg_loss:0.017, val_acc:0.986]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.008, val_acc:0.991]
Epoch [64/120    avg_loss:0.013, val_acc:0.986]
Epoch [65/120    avg_loss:0.009, val_acc:0.989]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.987]
Epoch [68/120    avg_loss:0.018, val_acc:0.986]
Epoch [69/120    avg_loss:0.024, val_acc:0.984]
Epoch [70/120    avg_loss:0.013, val_acc:0.991]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.007, val_acc:0.989]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.015, val_acc:0.980]
Epoch [75/120    avg_loss:0.015, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.006, val_acc:0.991]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.004, val_acc:0.991]
Epoch [80/120    avg_loss:0.004, val_acc:0.991]
Epoch [81/120    avg_loss:0.005, val_acc:0.989]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.991]
Epoch [87/120    avg_loss:0.011, val_acc:0.972]
Epoch [88/120    avg_loss:0.015, val_acc:0.980]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.989]
Epoch [93/120    avg_loss:0.014, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.012, val_acc:0.974]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.003, val_acc:0.988]
Epoch [112/120    avg_loss:0.003, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     5     0     0     0     8    33     6]
 [    0     2 18018     0    69     0     1     0     0     0]
 [    0     0     0  1990     0     0     0     0    42     4]
 [    0    31     5     0  2922     0     1     0    13     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5    10     0     0  4852     0    11     0]
 [    0     1     0     0     0     1     0  1284     0     4]
 [    0     8     0    14    52     0     0     0  3497     0]
 [    0     0     0     0    17    30     0     1     0   871]]

Accuracy:
99.0986431446268

F1 scores:
[       nan 0.9926871  0.99772966 0.98150432 0.96883289 0.98826202
 0.99712289 0.9941928  0.97586159 0.96563193]

Kappa:
0.988065420875962
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22783f3908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.990, val_acc:0.291]
Epoch [2/120    avg_loss:1.637, val_acc:0.303]
Epoch [3/120    avg_loss:1.428, val_acc:0.378]
Epoch [4/120    avg_loss:1.226, val_acc:0.444]
Epoch [5/120    avg_loss:1.038, val_acc:0.520]
Epoch [6/120    avg_loss:0.908, val_acc:0.602]
Epoch [7/120    avg_loss:0.763, val_acc:0.670]
Epoch [8/120    avg_loss:0.710, val_acc:0.672]
Epoch [9/120    avg_loss:0.585, val_acc:0.692]
Epoch [10/120    avg_loss:0.484, val_acc:0.789]
Epoch [11/120    avg_loss:0.464, val_acc:0.780]
Epoch [12/120    avg_loss:0.398, val_acc:0.838]
Epoch [13/120    avg_loss:0.357, val_acc:0.840]
Epoch [14/120    avg_loss:0.309, val_acc:0.903]
Epoch [15/120    avg_loss:0.284, val_acc:0.899]
Epoch [16/120    avg_loss:0.272, val_acc:0.899]
Epoch [17/120    avg_loss:0.208, val_acc:0.945]
Epoch [18/120    avg_loss:0.172, val_acc:0.868]
Epoch [19/120    avg_loss:0.178, val_acc:0.947]
Epoch [20/120    avg_loss:0.171, val_acc:0.949]
Epoch [21/120    avg_loss:0.223, val_acc:0.848]
Epoch [22/120    avg_loss:0.170, val_acc:0.898]
Epoch [23/120    avg_loss:0.143, val_acc:0.926]
Epoch [24/120    avg_loss:0.128, val_acc:0.931]
Epoch [25/120    avg_loss:0.137, val_acc:0.938]
Epoch [26/120    avg_loss:0.096, val_acc:0.960]
Epoch [27/120    avg_loss:0.092, val_acc:0.961]
Epoch [28/120    avg_loss:0.072, val_acc:0.952]
Epoch [29/120    avg_loss:0.088, val_acc:0.907]
Epoch [30/120    avg_loss:0.084, val_acc:0.973]
Epoch [31/120    avg_loss:0.066, val_acc:0.967]
Epoch [32/120    avg_loss:0.050, val_acc:0.970]
Epoch [33/120    avg_loss:0.067, val_acc:0.969]
Epoch [34/120    avg_loss:0.067, val_acc:0.975]
Epoch [35/120    avg_loss:0.042, val_acc:0.981]
Epoch [36/120    avg_loss:0.031, val_acc:0.979]
Epoch [37/120    avg_loss:0.031, val_acc:0.972]
Epoch [38/120    avg_loss:0.057, val_acc:0.948]
Epoch [39/120    avg_loss:0.063, val_acc:0.976]
Epoch [40/120    avg_loss:0.037, val_acc:0.972]
Epoch [41/120    avg_loss:0.030, val_acc:0.983]
Epoch [42/120    avg_loss:0.038, val_acc:0.951]
Epoch [43/120    avg_loss:0.039, val_acc:0.982]
Epoch [44/120    avg_loss:0.039, val_acc:0.978]
Epoch [45/120    avg_loss:0.027, val_acc:0.977]
Epoch [46/120    avg_loss:0.040, val_acc:0.979]
Epoch [47/120    avg_loss:0.029, val_acc:0.977]
Epoch [48/120    avg_loss:0.048, val_acc:0.978]
Epoch [49/120    avg_loss:0.040, val_acc:0.966]
Epoch [50/120    avg_loss:0.024, val_acc:0.984]
Epoch [51/120    avg_loss:0.017, val_acc:0.980]
Epoch [52/120    avg_loss:0.023, val_acc:0.983]
Epoch [53/120    avg_loss:0.026, val_acc:0.978]
Epoch [54/120    avg_loss:0.025, val_acc:0.980]
Epoch [55/120    avg_loss:0.021, val_acc:0.985]
Epoch [56/120    avg_loss:0.024, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.984]
Epoch [58/120    avg_loss:0.024, val_acc:0.986]
Epoch [59/120    avg_loss:0.017, val_acc:0.989]
Epoch [60/120    avg_loss:0.018, val_acc:0.979]
Epoch [61/120    avg_loss:0.015, val_acc:0.977]
Epoch [62/120    avg_loss:0.023, val_acc:0.956]
Epoch [63/120    avg_loss:0.021, val_acc:0.984]
Epoch [64/120    avg_loss:0.016, val_acc:0.985]
Epoch [65/120    avg_loss:0.016, val_acc:0.980]
Epoch [66/120    avg_loss:0.016, val_acc:0.979]
Epoch [67/120    avg_loss:0.013, val_acc:0.984]
Epoch [68/120    avg_loss:0.011, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.977]
Epoch [70/120    avg_loss:0.013, val_acc:0.980]
Epoch [71/120    avg_loss:0.012, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.010, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     3     0    11     0    79     0]
 [    0     0 18025     0    32     0    33     0     0     0]
 [    0     4     0  2025     0     0     0     0     3     4]
 [    0    22    12     0  2903     0    13     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     2     0     0  4872     0     0     0]
 [    0     2     0     0     0     0     2  1285     0     1]
 [    0     6     0    20    57     0     0     0  3482     6]
 [    0     0     0     0    14    48     0     0     0   857]]

Accuracy:
99.0359819728629

F1 scores:
[       nan 0.990082   0.99775816 0.99191771 0.97074068 0.98194131
 0.99337343 0.99805825 0.97303339 0.95914941]

Kappa:
0.9872341579462847
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f934a8519b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.071, val_acc:0.447]
Epoch [2/120    avg_loss:1.706, val_acc:0.499]
Epoch [3/120    avg_loss:1.442, val_acc:0.606]
Epoch [4/120    avg_loss:1.249, val_acc:0.600]
Epoch [5/120    avg_loss:1.060, val_acc:0.680]
Epoch [6/120    avg_loss:0.908, val_acc:0.747]
Epoch [7/120    avg_loss:0.812, val_acc:0.747]
Epoch [8/120    avg_loss:0.662, val_acc:0.729]
Epoch [9/120    avg_loss:0.592, val_acc:0.800]
Epoch [10/120    avg_loss:0.463, val_acc:0.765]
Epoch [11/120    avg_loss:0.389, val_acc:0.777]
Epoch [12/120    avg_loss:0.361, val_acc:0.805]
Epoch [13/120    avg_loss:0.355, val_acc:0.819]
Epoch [14/120    avg_loss:0.312, val_acc:0.855]
Epoch [15/120    avg_loss:0.286, val_acc:0.795]
Epoch [16/120    avg_loss:0.264, val_acc:0.914]
Epoch [17/120    avg_loss:0.225, val_acc:0.879]
Epoch [18/120    avg_loss:0.226, val_acc:0.898]
Epoch [19/120    avg_loss:0.169, val_acc:0.918]
Epoch [20/120    avg_loss:0.170, val_acc:0.910]
Epoch [21/120    avg_loss:0.142, val_acc:0.948]
Epoch [22/120    avg_loss:0.134, val_acc:0.937]
Epoch [23/120    avg_loss:0.126, val_acc:0.921]
Epoch [24/120    avg_loss:0.175, val_acc:0.872]
Epoch [25/120    avg_loss:0.107, val_acc:0.944]
Epoch [26/120    avg_loss:0.105, val_acc:0.930]
Epoch [27/120    avg_loss:0.121, val_acc:0.958]
Epoch [28/120    avg_loss:0.087, val_acc:0.926]
Epoch [29/120    avg_loss:0.077, val_acc:0.846]
Epoch [30/120    avg_loss:0.071, val_acc:0.963]
Epoch [31/120    avg_loss:0.061, val_acc:0.957]
Epoch [32/120    avg_loss:0.101, val_acc:0.951]
Epoch [33/120    avg_loss:0.059, val_acc:0.964]
Epoch [34/120    avg_loss:0.044, val_acc:0.970]
Epoch [35/120    avg_loss:0.040, val_acc:0.974]
Epoch [36/120    avg_loss:0.040, val_acc:0.977]
Epoch [37/120    avg_loss:0.035, val_acc:0.970]
Epoch [38/120    avg_loss:0.057, val_acc:0.970]
Epoch [39/120    avg_loss:0.051, val_acc:0.979]
Epoch [40/120    avg_loss:0.041, val_acc:0.976]
Epoch [41/120    avg_loss:0.045, val_acc:0.974]
Epoch [42/120    avg_loss:0.030, val_acc:0.959]
Epoch [43/120    avg_loss:0.033, val_acc:0.978]
Epoch [44/120    avg_loss:0.038, val_acc:0.979]
Epoch [45/120    avg_loss:0.041, val_acc:0.977]
Epoch [46/120    avg_loss:0.039, val_acc:0.978]
Epoch [47/120    avg_loss:0.033, val_acc:0.977]
Epoch [48/120    avg_loss:0.023, val_acc:0.977]
Epoch [49/120    avg_loss:0.020, val_acc:0.980]
Epoch [50/120    avg_loss:0.016, val_acc:0.980]
Epoch [51/120    avg_loss:0.038, val_acc:0.959]
Epoch [52/120    avg_loss:0.057, val_acc:0.969]
Epoch [53/120    avg_loss:0.061, val_acc:0.978]
Epoch [54/120    avg_loss:0.037, val_acc:0.977]
Epoch [55/120    avg_loss:0.038, val_acc:0.977]
Epoch [56/120    avg_loss:0.025, val_acc:0.985]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.042, val_acc:0.970]
Epoch [59/120    avg_loss:0.122, val_acc:0.956]
Epoch [60/120    avg_loss:0.040, val_acc:0.971]
Epoch [61/120    avg_loss:0.026, val_acc:0.977]
Epoch [62/120    avg_loss:0.033, val_acc:0.977]
Epoch [63/120    avg_loss:0.023, val_acc:0.981]
Epoch [64/120    avg_loss:0.024, val_acc:0.963]
Epoch [65/120    avg_loss:0.048, val_acc:0.980]
Epoch [66/120    avg_loss:0.025, val_acc:0.972]
Epoch [67/120    avg_loss:0.054, val_acc:0.979]
Epoch [68/120    avg_loss:0.031, val_acc:0.981]
Epoch [69/120    avg_loss:0.019, val_acc:0.983]
Epoch [70/120    avg_loss:0.015, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.985]
Epoch [72/120    avg_loss:0.013, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.016, val_acc:0.987]
Epoch [81/120    avg_loss:0.012, val_acc:0.986]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.987]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.987]
Epoch [86/120    avg_loss:0.013, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.009, val_acc:0.987]
Epoch [89/120    avg_loss:0.017, val_acc:0.990]
Epoch [90/120    avg_loss:0.012, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.989]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.989]
Epoch [98/120    avg_loss:0.017, val_acc:0.987]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.010, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.009, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.013, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.012, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.989]
Epoch [119/120    avg_loss:0.012, val_acc:0.989]
Epoch [120/120    avg_loss:0.011, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6322     0     1     0     0    25    17    65     2]
 [    0     0 18032     0    39     0     9     0    10     0]
 [    0     2     0  1983     3     0     0     0    46     2]
 [    0    46    19     0  2878     0     1     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    16     4     0     0  4840     0    18     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0     2     0     2    44     0     0     0  3523     0]
 [    0     0     0     2    14    50     0     0     0   853]]

Accuracy:
98.87209890824958

F1 scores:
[       nan 0.98742679 0.99742788 0.98460775 0.96739496 0.98120301
 0.99251512 0.99306626 0.97038975 0.96058559]

Kappa:
0.9850595516432131
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f955d26a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.097, val_acc:0.163]
Epoch [2/120    avg_loss:1.692, val_acc:0.268]
Epoch [3/120    avg_loss:1.468, val_acc:0.323]
Epoch [4/120    avg_loss:1.280, val_acc:0.392]
Epoch [5/120    avg_loss:1.124, val_acc:0.416]
Epoch [6/120    avg_loss:0.958, val_acc:0.465]
Epoch [7/120    avg_loss:0.832, val_acc:0.599]
Epoch [8/120    avg_loss:0.668, val_acc:0.768]
Epoch [9/120    avg_loss:0.553, val_acc:0.766]
Epoch [10/120    avg_loss:0.514, val_acc:0.805]
Epoch [11/120    avg_loss:0.403, val_acc:0.795]
Epoch [12/120    avg_loss:0.367, val_acc:0.853]
Epoch [13/120    avg_loss:0.350, val_acc:0.838]
Epoch [14/120    avg_loss:0.330, val_acc:0.815]
Epoch [15/120    avg_loss:0.308, val_acc:0.850]
Epoch [16/120    avg_loss:0.275, val_acc:0.876]
Epoch [17/120    avg_loss:0.220, val_acc:0.876]
Epoch [18/120    avg_loss:0.211, val_acc:0.914]
Epoch [19/120    avg_loss:0.217, val_acc:0.906]
Epoch [20/120    avg_loss:0.892, val_acc:0.596]
Epoch [21/120    avg_loss:0.701, val_acc:0.756]
Epoch [22/120    avg_loss:0.489, val_acc:0.785]
Epoch [23/120    avg_loss:0.420, val_acc:0.763]
Epoch [24/120    avg_loss:0.387, val_acc:0.836]
Epoch [25/120    avg_loss:0.342, val_acc:0.858]
Epoch [26/120    avg_loss:0.311, val_acc:0.855]
Epoch [27/120    avg_loss:0.284, val_acc:0.903]
Epoch [28/120    avg_loss:0.242, val_acc:0.903]
Epoch [29/120    avg_loss:0.226, val_acc:0.874]
Epoch [30/120    avg_loss:0.197, val_acc:0.843]
Epoch [31/120    avg_loss:0.178, val_acc:0.904]
Epoch [32/120    avg_loss:0.158, val_acc:0.929]
Epoch [33/120    avg_loss:0.132, val_acc:0.935]
Epoch [34/120    avg_loss:0.136, val_acc:0.935]
Epoch [35/120    avg_loss:0.136, val_acc:0.937]
Epoch [36/120    avg_loss:0.125, val_acc:0.931]
Epoch [37/120    avg_loss:0.118, val_acc:0.933]
Epoch [38/120    avg_loss:0.116, val_acc:0.940]
Epoch [39/120    avg_loss:0.116, val_acc:0.944]
Epoch [40/120    avg_loss:0.118, val_acc:0.945]
Epoch [41/120    avg_loss:0.120, val_acc:0.948]
Epoch [42/120    avg_loss:0.107, val_acc:0.938]
Epoch [43/120    avg_loss:0.101, val_acc:0.955]
Epoch [44/120    avg_loss:0.103, val_acc:0.943]
Epoch [45/120    avg_loss:0.102, val_acc:0.952]
Epoch [46/120    avg_loss:0.111, val_acc:0.955]
Epoch [47/120    avg_loss:0.095, val_acc:0.946]
Epoch [48/120    avg_loss:0.107, val_acc:0.946]
Epoch [49/120    avg_loss:0.102, val_acc:0.957]
Epoch [50/120    avg_loss:0.105, val_acc:0.957]
Epoch [51/120    avg_loss:0.104, val_acc:0.957]
Epoch [52/120    avg_loss:0.097, val_acc:0.952]
Epoch [53/120    avg_loss:0.113, val_acc:0.960]
Epoch [54/120    avg_loss:0.102, val_acc:0.957]
Epoch [55/120    avg_loss:0.100, val_acc:0.962]
Epoch [56/120    avg_loss:0.098, val_acc:0.963]
Epoch [57/120    avg_loss:0.091, val_acc:0.943]
Epoch [58/120    avg_loss:0.086, val_acc:0.961]
Epoch [59/120    avg_loss:0.084, val_acc:0.961]
Epoch [60/120    avg_loss:0.083, val_acc:0.969]
Epoch [61/120    avg_loss:0.086, val_acc:0.960]
Epoch [62/120    avg_loss:0.078, val_acc:0.963]
Epoch [63/120    avg_loss:0.076, val_acc:0.961]
Epoch [64/120    avg_loss:0.082, val_acc:0.958]
Epoch [65/120    avg_loss:0.078, val_acc:0.970]
Epoch [66/120    avg_loss:0.081, val_acc:0.953]
Epoch [67/120    avg_loss:0.083, val_acc:0.957]
Epoch [68/120    avg_loss:0.093, val_acc:0.957]
Epoch [69/120    avg_loss:0.082, val_acc:0.957]
Epoch [70/120    avg_loss:0.078, val_acc:0.963]
Epoch [71/120    avg_loss:0.069, val_acc:0.965]
Epoch [72/120    avg_loss:0.073, val_acc:0.969]
Epoch [73/120    avg_loss:0.069, val_acc:0.969]
Epoch [74/120    avg_loss:0.069, val_acc:0.966]
Epoch [75/120    avg_loss:0.078, val_acc:0.965]
Epoch [76/120    avg_loss:0.074, val_acc:0.969]
Epoch [77/120    avg_loss:0.072, val_acc:0.965]
Epoch [78/120    avg_loss:0.068, val_acc:0.973]
Epoch [79/120    avg_loss:0.067, val_acc:0.974]
Epoch [80/120    avg_loss:0.064, val_acc:0.967]
Epoch [81/120    avg_loss:0.061, val_acc:0.969]
Epoch [82/120    avg_loss:0.064, val_acc:0.967]
Epoch [83/120    avg_loss:0.056, val_acc:0.969]
Epoch [84/120    avg_loss:0.051, val_acc:0.975]
Epoch [85/120    avg_loss:0.062, val_acc:0.971]
Epoch [86/120    avg_loss:0.061, val_acc:0.971]
Epoch [87/120    avg_loss:0.074, val_acc:0.957]
Epoch [88/120    avg_loss:0.059, val_acc:0.966]
Epoch [89/120    avg_loss:0.053, val_acc:0.972]
Epoch [90/120    avg_loss:0.059, val_acc:0.961]
Epoch [91/120    avg_loss:0.061, val_acc:0.968]
Epoch [92/120    avg_loss:0.062, val_acc:0.970]
Epoch [93/120    avg_loss:0.066, val_acc:0.970]
Epoch [94/120    avg_loss:0.063, val_acc:0.974]
Epoch [95/120    avg_loss:0.062, val_acc:0.966]
Epoch [96/120    avg_loss:0.069, val_acc:0.972]
Epoch [97/120    avg_loss:0.061, val_acc:0.970]
Epoch [98/120    avg_loss:0.061, val_acc:0.972]
Epoch [99/120    avg_loss:0.050, val_acc:0.974]
Epoch [100/120    avg_loss:0.050, val_acc:0.976]
Epoch [101/120    avg_loss:0.049, val_acc:0.975]
Epoch [102/120    avg_loss:0.050, val_acc:0.974]
Epoch [103/120    avg_loss:0.050, val_acc:0.975]
Epoch [104/120    avg_loss:0.047, val_acc:0.975]
Epoch [105/120    avg_loss:0.051, val_acc:0.974]
Epoch [106/120    avg_loss:0.041, val_acc:0.974]
Epoch [107/120    avg_loss:0.049, val_acc:0.974]
Epoch [108/120    avg_loss:0.046, val_acc:0.974]
Epoch [109/120    avg_loss:0.047, val_acc:0.975]
Epoch [110/120    avg_loss:0.047, val_acc:0.976]
Epoch [111/120    avg_loss:0.054, val_acc:0.976]
Epoch [112/120    avg_loss:0.047, val_acc:0.974]
Epoch [113/120    avg_loss:0.054, val_acc:0.974]
Epoch [114/120    avg_loss:0.046, val_acc:0.974]
Epoch [115/120    avg_loss:0.047, val_acc:0.973]
Epoch [116/120    avg_loss:0.054, val_acc:0.974]
Epoch [117/120    avg_loss:0.045, val_acc:0.974]
Epoch [118/120    avg_loss:0.052, val_acc:0.973]
Epoch [119/120    avg_loss:0.050, val_acc:0.974]
Epoch [120/120    avg_loss:0.049, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6284     0     1    14     0     0     3   130     0]
 [    0     0 17851     0   128     0   111     0     0     0]
 [    0    12     0  1980     1     0     0     0    40     3]
 [    0    57     9     0  2873     0    12     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6    14     0     0  4846     0    12     0]
 [    0     1     0     0     0     0     2  1279     0     8]
 [    0    35     0    38    75     0     0     0  3416     7]
 [    0     0     0     0    17    30     0     0     0   872]]

Accuracy:
98.10329453160774

F1 scores:
[       nan 0.98026675 0.99293581 0.97321209 0.94506579 0.98863636
 0.9840593  0.99455677 0.95020862 0.96406855]

Kappa:
0.974931242359108
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f23fc4199b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.097, val_acc:0.087]
Epoch [2/120    avg_loss:1.732, val_acc:0.262]
Epoch [3/120    avg_loss:1.423, val_acc:0.359]
Epoch [4/120    avg_loss:1.191, val_acc:0.514]
Epoch [5/120    avg_loss:1.018, val_acc:0.690]
Epoch [6/120    avg_loss:0.832, val_acc:0.711]
Epoch [7/120    avg_loss:0.696, val_acc:0.712]
Epoch [8/120    avg_loss:0.576, val_acc:0.736]
Epoch [9/120    avg_loss:0.522, val_acc:0.772]
Epoch [10/120    avg_loss:0.456, val_acc:0.775]
Epoch [11/120    avg_loss:0.361, val_acc:0.813]
Epoch [12/120    avg_loss:0.314, val_acc:0.812]
Epoch [13/120    avg_loss:0.355, val_acc:0.840]
Epoch [14/120    avg_loss:0.270, val_acc:0.921]
Epoch [15/120    avg_loss:0.249, val_acc:0.887]
Epoch [16/120    avg_loss:0.250, val_acc:0.911]
Epoch [17/120    avg_loss:0.202, val_acc:0.872]
Epoch [18/120    avg_loss:0.268, val_acc:0.884]
Epoch [19/120    avg_loss:0.195, val_acc:0.928]
Epoch [20/120    avg_loss:0.163, val_acc:0.951]
Epoch [21/120    avg_loss:0.139, val_acc:0.945]
Epoch [22/120    avg_loss:0.114, val_acc:0.961]
Epoch [23/120    avg_loss:0.099, val_acc:0.951]
Epoch [24/120    avg_loss:0.088, val_acc:0.964]
Epoch [25/120    avg_loss:0.124, val_acc:0.946]
Epoch [26/120    avg_loss:0.133, val_acc:0.944]
Epoch [27/120    avg_loss:0.089, val_acc:0.962]
Epoch [28/120    avg_loss:0.064, val_acc:0.966]
Epoch [29/120    avg_loss:0.077, val_acc:0.957]
Epoch [30/120    avg_loss:0.096, val_acc:0.932]
Epoch [31/120    avg_loss:0.067, val_acc:0.952]
Epoch [32/120    avg_loss:0.137, val_acc:0.944]
Epoch [33/120    avg_loss:0.088, val_acc:0.975]
Epoch [34/120    avg_loss:0.069, val_acc:0.952]
Epoch [35/120    avg_loss:0.057, val_acc:0.964]
Epoch [36/120    avg_loss:0.051, val_acc:0.977]
Epoch [37/120    avg_loss:0.035, val_acc:0.973]
Epoch [38/120    avg_loss:0.041, val_acc:0.980]
Epoch [39/120    avg_loss:0.043, val_acc:0.977]
Epoch [40/120    avg_loss:0.039, val_acc:0.978]
Epoch [41/120    avg_loss:0.039, val_acc:0.956]
Epoch [42/120    avg_loss:0.031, val_acc:0.977]
Epoch [43/120    avg_loss:0.024, val_acc:0.975]
Epoch [44/120    avg_loss:0.049, val_acc:0.970]
Epoch [45/120    avg_loss:0.039, val_acc:0.977]
Epoch [46/120    avg_loss:0.051, val_acc:0.970]
Epoch [47/120    avg_loss:0.026, val_acc:0.979]
Epoch [48/120    avg_loss:0.026, val_acc:0.974]
Epoch [49/120    avg_loss:0.017, val_acc:0.980]
Epoch [50/120    avg_loss:0.036, val_acc:0.913]
Epoch [51/120    avg_loss:0.044, val_acc:0.967]
Epoch [52/120    avg_loss:0.037, val_acc:0.936]
Epoch [53/120    avg_loss:0.058, val_acc:0.970]
Epoch [54/120    avg_loss:0.043, val_acc:0.973]
Epoch [55/120    avg_loss:0.032, val_acc:0.976]
Epoch [56/120    avg_loss:0.037, val_acc:0.963]
Epoch [57/120    avg_loss:0.024, val_acc:0.978]
Epoch [58/120    avg_loss:0.020, val_acc:0.983]
Epoch [59/120    avg_loss:0.019, val_acc:0.966]
Epoch [60/120    avg_loss:0.025, val_acc:0.977]
Epoch [61/120    avg_loss:0.013, val_acc:0.982]
Epoch [62/120    avg_loss:0.013, val_acc:0.982]
Epoch [63/120    avg_loss:0.017, val_acc:0.973]
Epoch [64/120    avg_loss:0.025, val_acc:0.977]
Epoch [65/120    avg_loss:0.028, val_acc:0.977]
Epoch [66/120    avg_loss:0.028, val_acc:0.970]
Epoch [67/120    avg_loss:0.018, val_acc:0.983]
Epoch [68/120    avg_loss:0.012, val_acc:0.983]
Epoch [69/120    avg_loss:0.015, val_acc:0.977]
Epoch [70/120    avg_loss:0.018, val_acc:0.977]
Epoch [71/120    avg_loss:0.013, val_acc:0.978]
Epoch [72/120    avg_loss:0.013, val_acc:0.978]
Epoch [73/120    avg_loss:0.015, val_acc:0.982]
Epoch [74/120    avg_loss:0.017, val_acc:0.968]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.982]
Epoch [77/120    avg_loss:0.012, val_acc:0.976]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.981]
Epoch [82/120    avg_loss:0.006, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.961]
Epoch [85/120    avg_loss:0.131, val_acc:0.953]
Epoch [86/120    avg_loss:0.060, val_acc:0.955]
Epoch [87/120    avg_loss:0.039, val_acc:0.984]
Epoch [88/120    avg_loss:0.020, val_acc:0.984]
Epoch [89/120    avg_loss:0.032, val_acc:0.979]
Epoch [90/120    avg_loss:0.025, val_acc:0.983]
Epoch [91/120    avg_loss:0.018, val_acc:0.989]
Epoch [92/120    avg_loss:0.012, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.989]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.983]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     0     0     3     1    15     0]
 [    0     2 18001     0    75     0    12     0     0     0]
 [    0     0     0  2030     0     0     0     0     6     0]
 [    0    35    22     0  2889     0     6     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     9     0     0  4849     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     7     0     0    59     0     0     0  3503     2]
 [    0     0     0     0    13    28     0     0     0   878]]

Accuracy:
99.19263490227267

F1 scores:
[       nan 0.99511211 0.99637451 0.99631902 0.96171771 0.9893859
 0.99487074 0.99961255 0.98468025 0.97609783]

Kappa:
0.9893070812026583
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f87a909a908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.040, val_acc:0.157]
Epoch [2/120    avg_loss:1.645, val_acc:0.212]
Epoch [3/120    avg_loss:1.409, val_acc:0.280]
Epoch [4/120    avg_loss:1.240, val_acc:0.361]
Epoch [5/120    avg_loss:1.080, val_acc:0.653]
Epoch [6/120    avg_loss:0.897, val_acc:0.649]
Epoch [7/120    avg_loss:0.756, val_acc:0.645]
Epoch [8/120    avg_loss:0.631, val_acc:0.775]
Epoch [9/120    avg_loss:0.540, val_acc:0.799]
Epoch [10/120    avg_loss:0.447, val_acc:0.815]
Epoch [11/120    avg_loss:0.409, val_acc:0.809]
Epoch [12/120    avg_loss:0.472, val_acc:0.866]
Epoch [13/120    avg_loss:0.350, val_acc:0.823]
Epoch [14/120    avg_loss:0.297, val_acc:0.897]
Epoch [15/120    avg_loss:0.311, val_acc:0.817]
Epoch [16/120    avg_loss:0.240, val_acc:0.906]
Epoch [17/120    avg_loss:0.203, val_acc:0.932]
Epoch [18/120    avg_loss:0.186, val_acc:0.934]
Epoch [19/120    avg_loss:0.149, val_acc:0.925]
Epoch [20/120    avg_loss:0.180, val_acc:0.930]
Epoch [21/120    avg_loss:0.220, val_acc:0.888]
Epoch [22/120    avg_loss:0.231, val_acc:0.940]
Epoch [23/120    avg_loss:0.153, val_acc:0.955]
Epoch [24/120    avg_loss:0.143, val_acc:0.959]
Epoch [25/120    avg_loss:0.108, val_acc:0.963]
Epoch [26/120    avg_loss:0.121, val_acc:0.933]
Epoch [27/120    avg_loss:0.174, val_acc:0.933]
Epoch [28/120    avg_loss:0.160, val_acc:0.943]
Epoch [29/120    avg_loss:0.102, val_acc:0.967]
Epoch [30/120    avg_loss:0.090, val_acc:0.973]
Epoch [31/120    avg_loss:0.055, val_acc:0.976]
Epoch [32/120    avg_loss:0.060, val_acc:0.969]
Epoch [33/120    avg_loss:0.065, val_acc:0.977]
Epoch [34/120    avg_loss:0.069, val_acc:0.960]
Epoch [35/120    avg_loss:0.048, val_acc:0.976]
Epoch [36/120    avg_loss:0.052, val_acc:0.977]
Epoch [37/120    avg_loss:0.067, val_acc:0.975]
Epoch [38/120    avg_loss:0.051, val_acc:0.957]
Epoch [39/120    avg_loss:0.070, val_acc:0.946]
Epoch [40/120    avg_loss:0.061, val_acc:0.980]
Epoch [41/120    avg_loss:0.048, val_acc:0.986]
Epoch [42/120    avg_loss:0.042, val_acc:0.980]
Epoch [43/120    avg_loss:0.029, val_acc:0.984]
Epoch [44/120    avg_loss:0.033, val_acc:0.971]
Epoch [45/120    avg_loss:0.047, val_acc:0.938]
Epoch [46/120    avg_loss:0.116, val_acc:0.960]
Epoch [47/120    avg_loss:0.046, val_acc:0.964]
Epoch [48/120    avg_loss:0.034, val_acc:0.977]
Epoch [49/120    avg_loss:0.033, val_acc:0.980]
Epoch [50/120    avg_loss:0.040, val_acc:0.963]
Epoch [51/120    avg_loss:0.044, val_acc:0.976]
Epoch [52/120    avg_loss:0.054, val_acc:0.957]
Epoch [53/120    avg_loss:0.047, val_acc:0.983]
Epoch [54/120    avg_loss:0.036, val_acc:0.981]
Epoch [55/120    avg_loss:0.020, val_acc:0.982]
Epoch [56/120    avg_loss:0.021, val_acc:0.984]
Epoch [57/120    avg_loss:0.021, val_acc:0.985]
Epoch [58/120    avg_loss:0.021, val_acc:0.986]
Epoch [59/120    avg_loss:0.017, val_acc:0.987]
Epoch [60/120    avg_loss:0.015, val_acc:0.987]
Epoch [61/120    avg_loss:0.018, val_acc:0.988]
Epoch [62/120    avg_loss:0.017, val_acc:0.989]
Epoch [63/120    avg_loss:0.018, val_acc:0.988]
Epoch [64/120    avg_loss:0.019, val_acc:0.989]
Epoch [65/120    avg_loss:0.014, val_acc:0.990]
Epoch [66/120    avg_loss:0.020, val_acc:0.989]
Epoch [67/120    avg_loss:0.016, val_acc:0.989]
Epoch [68/120    avg_loss:0.017, val_acc:0.990]
Epoch [69/120    avg_loss:0.015, val_acc:0.990]
Epoch [70/120    avg_loss:0.015, val_acc:0.990]
Epoch [71/120    avg_loss:0.014, val_acc:0.990]
Epoch [72/120    avg_loss:0.015, val_acc:0.989]
Epoch [73/120    avg_loss:0.023, val_acc:0.988]
Epoch [74/120    avg_loss:0.017, val_acc:0.991]
Epoch [75/120    avg_loss:0.015, val_acc:0.991]
Epoch [76/120    avg_loss:0.015, val_acc:0.991]
Epoch [77/120    avg_loss:0.017, val_acc:0.989]
Epoch [78/120    avg_loss:0.016, val_acc:0.991]
Epoch [79/120    avg_loss:0.017, val_acc:0.989]
Epoch [80/120    avg_loss:0.015, val_acc:0.988]
Epoch [81/120    avg_loss:0.013, val_acc:0.991]
Epoch [82/120    avg_loss:0.015, val_acc:0.990]
Epoch [83/120    avg_loss:0.015, val_acc:0.990]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.013, val_acc:0.989]
Epoch [86/120    avg_loss:0.014, val_acc:0.989]
Epoch [87/120    avg_loss:0.013, val_acc:0.990]
Epoch [88/120    avg_loss:0.014, val_acc:0.989]
Epoch [89/120    avg_loss:0.012, val_acc:0.989]
Epoch [90/120    avg_loss:0.015, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.990]
Epoch [92/120    avg_loss:0.015, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.010, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.014, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.990]
Epoch [100/120    avg_loss:0.014, val_acc:0.990]
Epoch [101/120    avg_loss:0.013, val_acc:0.990]
Epoch [102/120    avg_loss:0.011, val_acc:0.990]
Epoch [103/120    avg_loss:0.014, val_acc:0.990]
Epoch [104/120    avg_loss:0.011, val_acc:0.990]
Epoch [105/120    avg_loss:0.011, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.990]
Epoch [107/120    avg_loss:0.012, val_acc:0.990]
Epoch [108/120    avg_loss:0.015, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.013, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.013, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.013, val_acc:0.990]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.013, val_acc:0.990]
Epoch [119/120    avg_loss:0.014, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6345     0     0     0     0    22     0    65     0]
 [    0     0 18069     0    17     0     0     0     4     0]
 [    0     2     0  2001     0     0     0     0    23    10]
 [    0    59    25     0  2856     0    10     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    36     3     0     0  4834     0     0     5]
 [    0     0     0     0     0     0     0  1281     0     9]
 [    0    13     0     0    61     0     0     0  3487    10]
 [    0     0     3     0    14    44     0     0     0   858]]

Accuracy:
98.89860940399586

F1 scores:
[       nan 0.98747179 0.99765342 0.99059406 0.96486486 0.98342125
 0.99220033 0.99649942 0.97239264 0.94754279]

Kappa:
0.9853988592528963
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbb72cfe908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.073, val_acc:0.131]
Epoch [2/120    avg_loss:1.684, val_acc:0.352]
Epoch [3/120    avg_loss:1.409, val_acc:0.405]
Epoch [4/120    avg_loss:1.209, val_acc:0.443]
Epoch [5/120    avg_loss:1.061, val_acc:0.464]
Epoch [6/120    avg_loss:0.938, val_acc:0.668]
Epoch [7/120    avg_loss:0.781, val_acc:0.766]
Epoch [8/120    avg_loss:0.670, val_acc:0.771]
Epoch [9/120    avg_loss:0.551, val_acc:0.812]
Epoch [10/120    avg_loss:0.497, val_acc:0.782]
Epoch [11/120    avg_loss:0.449, val_acc:0.810]
Epoch [12/120    avg_loss:0.487, val_acc:0.795]
Epoch [13/120    avg_loss:0.439, val_acc:0.798]
Epoch [14/120    avg_loss:0.378, val_acc:0.780]
Epoch [15/120    avg_loss:0.329, val_acc:0.822]
Epoch [16/120    avg_loss:0.326, val_acc:0.848]
Epoch [17/120    avg_loss:0.265, val_acc:0.822]
Epoch [18/120    avg_loss:0.279, val_acc:0.824]
Epoch [19/120    avg_loss:0.235, val_acc:0.895]
Epoch [20/120    avg_loss:0.226, val_acc:0.929]
Epoch [21/120    avg_loss:0.236, val_acc:0.899]
Epoch [22/120    avg_loss:0.211, val_acc:0.910]
Epoch [23/120    avg_loss:0.169, val_acc:0.906]
Epoch [24/120    avg_loss:0.235, val_acc:0.889]
Epoch [25/120    avg_loss:0.208, val_acc:0.934]
Epoch [26/120    avg_loss:0.166, val_acc:0.951]
Epoch [27/120    avg_loss:0.151, val_acc:0.946]
Epoch [28/120    avg_loss:0.135, val_acc:0.935]
Epoch [29/120    avg_loss:0.135, val_acc:0.950]
Epoch [30/120    avg_loss:0.133, val_acc:0.934]
Epoch [31/120    avg_loss:0.118, val_acc:0.949]
Epoch [32/120    avg_loss:0.091, val_acc:0.957]
Epoch [33/120    avg_loss:0.085, val_acc:0.946]
Epoch [34/120    avg_loss:0.100, val_acc:0.949]
Epoch [35/120    avg_loss:0.095, val_acc:0.956]
Epoch [36/120    avg_loss:0.109, val_acc:0.962]
Epoch [37/120    avg_loss:0.065, val_acc:0.963]
Epoch [38/120    avg_loss:0.085, val_acc:0.958]
Epoch [39/120    avg_loss:0.053, val_acc:0.965]
Epoch [40/120    avg_loss:0.064, val_acc:0.964]
Epoch [41/120    avg_loss:0.092, val_acc:0.963]
Epoch [42/120    avg_loss:0.051, val_acc:0.963]
Epoch [43/120    avg_loss:0.050, val_acc:0.954]
Epoch [44/120    avg_loss:0.059, val_acc:0.972]
Epoch [45/120    avg_loss:0.043, val_acc:0.973]
Epoch [46/120    avg_loss:0.039, val_acc:0.965]
Epoch [47/120    avg_loss:0.051, val_acc:0.964]
Epoch [48/120    avg_loss:0.244, val_acc:0.844]
Epoch [49/120    avg_loss:0.105, val_acc:0.963]
Epoch [50/120    avg_loss:0.066, val_acc:0.967]
Epoch [51/120    avg_loss:0.060, val_acc:0.968]
Epoch [52/120    avg_loss:0.038, val_acc:0.972]
Epoch [53/120    avg_loss:0.038, val_acc:0.971]
Epoch [54/120    avg_loss:0.028, val_acc:0.962]
Epoch [55/120    avg_loss:0.032, val_acc:0.969]
Epoch [56/120    avg_loss:0.022, val_acc:0.974]
Epoch [57/120    avg_loss:0.024, val_acc:0.969]
Epoch [58/120    avg_loss:0.033, val_acc:0.977]
Epoch [59/120    avg_loss:0.036, val_acc:0.968]
Epoch [60/120    avg_loss:0.023, val_acc:0.970]
Epoch [61/120    avg_loss:0.031, val_acc:0.970]
Epoch [62/120    avg_loss:0.034, val_acc:0.961]
Epoch [63/120    avg_loss:0.020, val_acc:0.979]
Epoch [64/120    avg_loss:0.021, val_acc:0.967]
Epoch [65/120    avg_loss:0.025, val_acc:0.969]
Epoch [66/120    avg_loss:0.025, val_acc:0.968]
Epoch [67/120    avg_loss:0.019, val_acc:0.972]
Epoch [68/120    avg_loss:0.016, val_acc:0.971]
Epoch [69/120    avg_loss:0.021, val_acc:0.976]
Epoch [70/120    avg_loss:0.013, val_acc:0.974]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.972]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.010, val_acc:0.976]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.010, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.969]
Epoch [78/120    avg_loss:0.022, val_acc:0.977]
Epoch [79/120    avg_loss:0.018, val_acc:0.976]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.020, val_acc:0.975]
Epoch [83/120    avg_loss:0.009, val_acc:0.977]
Epoch [84/120    avg_loss:0.021, val_acc:0.962]
Epoch [85/120    avg_loss:0.019, val_acc:0.980]
Epoch [86/120    avg_loss:0.013, val_acc:0.976]
Epoch [87/120    avg_loss:0.011, val_acc:0.981]
Epoch [88/120    avg_loss:0.018, val_acc:0.974]
Epoch [89/120    avg_loss:0.025, val_acc:0.977]
Epoch [90/120    avg_loss:0.010, val_acc:0.977]
Epoch [91/120    avg_loss:0.008, val_acc:0.976]
Epoch [92/120    avg_loss:0.009, val_acc:0.972]
Epoch [93/120    avg_loss:0.014, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.982]
Epoch [95/120    avg_loss:0.010, val_acc:0.980]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.980]
Epoch [98/120    avg_loss:0.005, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.977]
Epoch [100/120    avg_loss:0.006, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.975]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.007, val_acc:0.974]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.976]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.976]
Epoch [110/120    avg_loss:0.007, val_acc:0.980]
Epoch [111/120    avg_loss:0.024, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.966]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.018, val_acc:0.955]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.023, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6302     0     0     0     0     0     0   130     0]
 [    0     5 17895     0   186     0     4     0     0     0]
 [    0     3     0  1925     0     0     0     0   101     7]
 [    0    40    21     2  2841     0     1     0    24    43]
 [    0     3     0     0     0  1275     0     0     0    27]
 [    0     0    38     0     0     0  4837     0     0     3]
 [    0     0     0     0     0     0     2  1277     0    11]
 [    0     3     0     0    67     0     0     0  3475    26]
 [    0     0     0     0     0     8     0     0     0   911]]

Accuracy:
98.18041597377871

F1 scores:
[       nan 0.98561151 0.99295306 0.97148625 0.93669634 0.98531685
 0.99506274 0.99493572 0.95192439 0.93579866]

Kappa:
0.9759329743520927
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf8eb4f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.045, val_acc:0.161]
Epoch [2/120    avg_loss:1.686, val_acc:0.299]
Epoch [3/120    avg_loss:1.384, val_acc:0.352]
Epoch [4/120    avg_loss:1.180, val_acc:0.430]
Epoch [5/120    avg_loss:0.968, val_acc:0.611]
Epoch [6/120    avg_loss:0.837, val_acc:0.688]
Epoch [7/120    avg_loss:0.695, val_acc:0.750]
Epoch [8/120    avg_loss:0.596, val_acc:0.659]
Epoch [9/120    avg_loss:0.538, val_acc:0.723]
Epoch [10/120    avg_loss:0.446, val_acc:0.780]
Epoch [11/120    avg_loss:0.384, val_acc:0.762]
Epoch [12/120    avg_loss:0.388, val_acc:0.807]
Epoch [13/120    avg_loss:0.315, val_acc:0.827]
Epoch [14/120    avg_loss:0.296, val_acc:0.863]
Epoch [15/120    avg_loss:0.267, val_acc:0.912]
Epoch [16/120    avg_loss:0.237, val_acc:0.863]
Epoch [17/120    avg_loss:0.212, val_acc:0.910]
Epoch [18/120    avg_loss:0.218, val_acc:0.932]
Epoch [19/120    avg_loss:0.209, val_acc:0.923]
Epoch [20/120    avg_loss:0.180, val_acc:0.899]
Epoch [21/120    avg_loss:0.153, val_acc:0.951]
Epoch [22/120    avg_loss:0.127, val_acc:0.932]
Epoch [23/120    avg_loss:0.138, val_acc:0.965]
Epoch [24/120    avg_loss:0.115, val_acc:0.963]
Epoch [25/120    avg_loss:0.097, val_acc:0.918]
Epoch [26/120    avg_loss:0.115, val_acc:0.948]
Epoch [27/120    avg_loss:0.084, val_acc:0.963]
Epoch [28/120    avg_loss:0.081, val_acc:0.967]
Epoch [29/120    avg_loss:0.077, val_acc:0.974]
Epoch [30/120    avg_loss:0.075, val_acc:0.958]
Epoch [31/120    avg_loss:0.071, val_acc:0.976]
Epoch [32/120    avg_loss:0.055, val_acc:0.978]
Epoch [33/120    avg_loss:0.041, val_acc:0.971]
Epoch [34/120    avg_loss:0.040, val_acc:0.978]
Epoch [35/120    avg_loss:0.037, val_acc:0.978]
Epoch [36/120    avg_loss:0.384, val_acc:0.406]
Epoch [37/120    avg_loss:1.460, val_acc:0.537]
Epoch [38/120    avg_loss:1.092, val_acc:0.591]
Epoch [39/120    avg_loss:0.934, val_acc:0.635]
Epoch [40/120    avg_loss:0.813, val_acc:0.686]
Epoch [41/120    avg_loss:0.736, val_acc:0.737]
Epoch [42/120    avg_loss:0.666, val_acc:0.752]
Epoch [43/120    avg_loss:0.572, val_acc:0.770]
Epoch [44/120    avg_loss:0.544, val_acc:0.761]
Epoch [45/120    avg_loss:0.506, val_acc:0.782]
Epoch [46/120    avg_loss:0.456, val_acc:0.783]
Epoch [47/120    avg_loss:0.387, val_acc:0.824]
Epoch [48/120    avg_loss:0.389, val_acc:0.812]
Epoch [49/120    avg_loss:0.347, val_acc:0.818]
Epoch [50/120    avg_loss:0.384, val_acc:0.817]
Epoch [51/120    avg_loss:0.345, val_acc:0.814]
Epoch [52/120    avg_loss:0.343, val_acc:0.819]
Epoch [53/120    avg_loss:0.335, val_acc:0.815]
Epoch [54/120    avg_loss:0.331, val_acc:0.824]
Epoch [55/120    avg_loss:0.310, val_acc:0.819]
Epoch [56/120    avg_loss:0.330, val_acc:0.824]
Epoch [57/120    avg_loss:0.340, val_acc:0.817]
Epoch [58/120    avg_loss:0.332, val_acc:0.823]
Epoch [59/120    avg_loss:0.304, val_acc:0.818]
Epoch [60/120    avg_loss:0.313, val_acc:0.829]
Epoch [61/120    avg_loss:0.311, val_acc:0.828]
Epoch [62/120    avg_loss:0.315, val_acc:0.828]
Epoch [63/120    avg_loss:0.298, val_acc:0.828]
Epoch [64/120    avg_loss:0.303, val_acc:0.828]
Epoch [65/120    avg_loss:0.320, val_acc:0.826]
Epoch [66/120    avg_loss:0.303, val_acc:0.826]
Epoch [67/120    avg_loss:0.321, val_acc:0.825]
Epoch [68/120    avg_loss:0.328, val_acc:0.827]
Epoch [69/120    avg_loss:0.322, val_acc:0.827]
Epoch [70/120    avg_loss:0.326, val_acc:0.826]
Epoch [71/120    avg_loss:0.314, val_acc:0.828]
Epoch [72/120    avg_loss:0.315, val_acc:0.828]
Epoch [73/120    avg_loss:0.316, val_acc:0.831]
Epoch [74/120    avg_loss:0.295, val_acc:0.832]
Epoch [75/120    avg_loss:0.342, val_acc:0.831]
Epoch [76/120    avg_loss:0.313, val_acc:0.831]
Epoch [77/120    avg_loss:0.318, val_acc:0.831]
Epoch [78/120    avg_loss:0.299, val_acc:0.832]
Epoch [79/120    avg_loss:0.308, val_acc:0.831]
Epoch [80/120    avg_loss:0.307, val_acc:0.831]
Epoch [81/120    avg_loss:0.310, val_acc:0.831]
Epoch [82/120    avg_loss:0.302, val_acc:0.831]
Epoch [83/120    avg_loss:0.321, val_acc:0.831]
Epoch [84/120    avg_loss:0.321, val_acc:0.830]
Epoch [85/120    avg_loss:0.324, val_acc:0.830]
Epoch [86/120    avg_loss:0.305, val_acc:0.831]
Epoch [87/120    avg_loss:0.313, val_acc:0.831]
Epoch [88/120    avg_loss:0.314, val_acc:0.831]
Epoch [89/120    avg_loss:0.310, val_acc:0.831]
Epoch [90/120    avg_loss:0.319, val_acc:0.831]
Epoch [91/120    avg_loss:0.303, val_acc:0.831]
Epoch [92/120    avg_loss:0.296, val_acc:0.831]
Epoch [93/120    avg_loss:0.308, val_acc:0.830]
Epoch [94/120    avg_loss:0.286, val_acc:0.830]
Epoch [95/120    avg_loss:0.305, val_acc:0.831]
Epoch [96/120    avg_loss:0.302, val_acc:0.831]
Epoch [97/120    avg_loss:0.316, val_acc:0.831]
Epoch [98/120    avg_loss:0.332, val_acc:0.831]
Epoch [99/120    avg_loss:0.311, val_acc:0.831]
Epoch [100/120    avg_loss:0.303, val_acc:0.831]
Epoch [101/120    avg_loss:0.312, val_acc:0.831]
Epoch [102/120    avg_loss:0.286, val_acc:0.831]
Epoch [103/120    avg_loss:0.307, val_acc:0.831]
Epoch [104/120    avg_loss:0.304, val_acc:0.831]
Epoch [105/120    avg_loss:0.301, val_acc:0.831]
Epoch [106/120    avg_loss:0.313, val_acc:0.831]
Epoch [107/120    avg_loss:0.309, val_acc:0.831]
Epoch [108/120    avg_loss:0.306, val_acc:0.831]
Epoch [109/120    avg_loss:0.300, val_acc:0.831]
Epoch [110/120    avg_loss:0.297, val_acc:0.831]
Epoch [111/120    avg_loss:0.296, val_acc:0.831]
Epoch [112/120    avg_loss:0.311, val_acc:0.831]
Epoch [113/120    avg_loss:0.322, val_acc:0.831]
Epoch [114/120    avg_loss:0.296, val_acc:0.831]
Epoch [115/120    avg_loss:0.317, val_acc:0.831]
Epoch [116/120    avg_loss:0.318, val_acc:0.831]
Epoch [117/120    avg_loss:0.298, val_acc:0.831]
Epoch [118/120    avg_loss:0.309, val_acc:0.831]
Epoch [119/120    avg_loss:0.311, val_acc:0.831]
Epoch [120/120    avg_loss:0.304, val_acc:0.831]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5405     3    53   176     0   167    77   388   163]
 [    0     1 14366     0   163     0  3560     0     0     0]
 [    0    47     0  1909     1     0     0     0    65    14]
 [    0   104   143     0  2614     0    43     0    35    33]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    26   318    84    21     0  4259     0   170     0]
 [    0    61     0     0     0     0     3  1225     0     1]
 [    0   146     0    40    48     0    99     0  3234     4]
 [    0    17     0     9    14    84     0     0     0   795]]

Accuracy:
84.62150242209529

F1 scores:
[       nan 0.88324209 0.8727825  0.92423142 0.87002829 0.9688196
 0.65477746 0.94521605 0.8666756  0.82426128]

Kappa:
0.8031137520962535
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67c3992978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.227]
Epoch [2/120    avg_loss:1.682, val_acc:0.256]
Epoch [3/120    avg_loss:1.462, val_acc:0.349]
Epoch [4/120    avg_loss:1.232, val_acc:0.434]
Epoch [5/120    avg_loss:1.027, val_acc:0.462]
Epoch [6/120    avg_loss:0.894, val_acc:0.493]
Epoch [7/120    avg_loss:0.750, val_acc:0.569]
Epoch [8/120    avg_loss:0.636, val_acc:0.672]
Epoch [9/120    avg_loss:0.546, val_acc:0.690]
Epoch [10/120    avg_loss:0.471, val_acc:0.750]
Epoch [11/120    avg_loss:0.435, val_acc:0.779]
Epoch [12/120    avg_loss:0.364, val_acc:0.790]
Epoch [13/120    avg_loss:0.375, val_acc:0.785]
Epoch [14/120    avg_loss:0.324, val_acc:0.798]
Epoch [15/120    avg_loss:0.347, val_acc:0.760]
Epoch [16/120    avg_loss:0.337, val_acc:0.812]
Epoch [17/120    avg_loss:0.271, val_acc:0.821]
Epoch [18/120    avg_loss:0.246, val_acc:0.849]
Epoch [19/120    avg_loss:0.235, val_acc:0.877]
Epoch [20/120    avg_loss:0.187, val_acc:0.857]
Epoch [21/120    avg_loss:0.199, val_acc:0.921]
Epoch [22/120    avg_loss:0.132, val_acc:0.855]
Epoch [23/120    avg_loss:0.149, val_acc:0.887]
Epoch [24/120    avg_loss:0.110, val_acc:0.929]
Epoch [25/120    avg_loss:0.116, val_acc:0.954]
Epoch [26/120    avg_loss:0.102, val_acc:0.964]
Epoch [27/120    avg_loss:0.094, val_acc:0.956]
Epoch [28/120    avg_loss:0.080, val_acc:0.949]
Epoch [29/120    avg_loss:0.118, val_acc:0.944]
Epoch [30/120    avg_loss:0.079, val_acc:0.966]
Epoch [31/120    avg_loss:0.057, val_acc:0.966]
Epoch [32/120    avg_loss:0.070, val_acc:0.955]
Epoch [33/120    avg_loss:0.069, val_acc:0.967]
Epoch [34/120    avg_loss:0.057, val_acc:0.966]
Epoch [35/120    avg_loss:0.088, val_acc:0.969]
Epoch [36/120    avg_loss:0.077, val_acc:0.732]
Epoch [37/120    avg_loss:0.430, val_acc:0.836]
Epoch [38/120    avg_loss:0.214, val_acc:0.913]
Epoch [39/120    avg_loss:0.153, val_acc:0.940]
Epoch [40/120    avg_loss:0.101, val_acc:0.959]
Epoch [41/120    avg_loss:0.070, val_acc:0.942]
Epoch [42/120    avg_loss:0.052, val_acc:0.939]
Epoch [43/120    avg_loss:0.081, val_acc:0.962]
Epoch [44/120    avg_loss:0.062, val_acc:0.966]
Epoch [45/120    avg_loss:0.085, val_acc:0.954]
Epoch [46/120    avg_loss:0.050, val_acc:0.924]
Epoch [47/120    avg_loss:0.034, val_acc:0.971]
Epoch [48/120    avg_loss:0.027, val_acc:0.977]
Epoch [49/120    avg_loss:0.029, val_acc:0.970]
Epoch [50/120    avg_loss:0.032, val_acc:0.964]
Epoch [51/120    avg_loss:0.031, val_acc:0.978]
Epoch [52/120    avg_loss:0.025, val_acc:0.984]
Epoch [53/120    avg_loss:0.029, val_acc:0.969]
Epoch [54/120    avg_loss:0.038, val_acc:0.964]
Epoch [55/120    avg_loss:0.029, val_acc:0.974]
Epoch [56/120    avg_loss:0.017, val_acc:0.980]
Epoch [57/120    avg_loss:0.017, val_acc:0.974]
Epoch [58/120    avg_loss:0.023, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.978]
Epoch [60/120    avg_loss:0.017, val_acc:0.974]
Epoch [61/120    avg_loss:0.018, val_acc:0.976]
Epoch [62/120    avg_loss:0.024, val_acc:0.967]
Epoch [63/120    avg_loss:0.023, val_acc:0.976]
Epoch [64/120    avg_loss:0.015, val_acc:0.980]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.011, val_acc:0.980]
Epoch [67/120    avg_loss:0.010, val_acc:0.980]
Epoch [68/120    avg_loss:0.011, val_acc:0.980]
Epoch [69/120    avg_loss:0.010, val_acc:0.980]
Epoch [70/120    avg_loss:0.009, val_acc:0.980]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.979]
Epoch [74/120    avg_loss:0.009, val_acc:0.980]
Epoch [75/120    avg_loss:0.010, val_acc:0.979]
Epoch [76/120    avg_loss:0.015, val_acc:0.979]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.011, val_acc:0.980]
Epoch [84/120    avg_loss:0.010, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.009, val_acc:0.980]
Epoch [87/120    avg_loss:0.014, val_acc:0.980]
Epoch [88/120    avg_loss:0.010, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.981]
Epoch [90/120    avg_loss:0.009, val_acc:0.981]
Epoch [91/120    avg_loss:0.010, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.011, val_acc:0.981]
Epoch [100/120    avg_loss:0.017, val_acc:0.981]
Epoch [101/120    avg_loss:0.015, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.009, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6342     0     0     3     0     0    11    76     0]
 [    0     2 18008     0    42     0    37     0     1     0]
 [    0     1     0  2015     1     0     0     0    17     2]
 [    0    26    19     0  2907     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     3     0     0  4840     0    27     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0    26     0     4    51     0     0     0  3489     1]
 [    0     0     0     0    14    71     0     0     1   833]]

Accuracy:
98.87450895331743

F1 scores:
[       nan 0.98869748 0.9969827  0.99310005 0.9706177  0.97351734
 0.99231164 0.99459042 0.96889753 0.9476678 ]

Kappa:
0.985096577583019
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1fbda37898>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.046, val_acc:0.105]
Epoch [2/120    avg_loss:1.685, val_acc:0.147]
Epoch [3/120    avg_loss:1.453, val_acc:0.554]
Epoch [4/120    avg_loss:1.273, val_acc:0.489]
Epoch [5/120    avg_loss:1.116, val_acc:0.675]
Epoch [6/120    avg_loss:0.918, val_acc:0.655]
Epoch [7/120    avg_loss:0.763, val_acc:0.770]
Epoch [8/120    avg_loss:0.632, val_acc:0.830]
Epoch [9/120    avg_loss:0.515, val_acc:0.819]
Epoch [10/120    avg_loss:0.396, val_acc:0.845]
Epoch [11/120    avg_loss:0.363, val_acc:0.814]
Epoch [12/120    avg_loss:0.334, val_acc:0.898]
Epoch [13/120    avg_loss:0.277, val_acc:0.921]
Epoch [14/120    avg_loss:0.268, val_acc:0.894]
Epoch [15/120    avg_loss:0.224, val_acc:0.943]
Epoch [16/120    avg_loss:0.197, val_acc:0.923]
Epoch [17/120    avg_loss:0.198, val_acc:0.929]
Epoch [18/120    avg_loss:0.189, val_acc:0.940]
Epoch [19/120    avg_loss:0.185, val_acc:0.891]
Epoch [20/120    avg_loss:0.189, val_acc:0.927]
Epoch [21/120    avg_loss:0.136, val_acc:0.950]
Epoch [22/120    avg_loss:0.115, val_acc:0.970]
Epoch [23/120    avg_loss:0.110, val_acc:0.954]
Epoch [24/120    avg_loss:0.122, val_acc:0.951]
Epoch [25/120    avg_loss:0.108, val_acc:0.936]
Epoch [26/120    avg_loss:0.088, val_acc:0.963]
Epoch [27/120    avg_loss:0.094, val_acc:0.951]
Epoch [28/120    avg_loss:0.100, val_acc:0.963]
Epoch [29/120    avg_loss:0.125, val_acc:0.970]
Epoch [30/120    avg_loss:0.127, val_acc:0.955]
Epoch [31/120    avg_loss:0.117, val_acc:0.918]
Epoch [32/120    avg_loss:0.094, val_acc:0.960]
Epoch [33/120    avg_loss:0.078, val_acc:0.954]
Epoch [34/120    avg_loss:0.072, val_acc:0.957]
Epoch [35/120    avg_loss:0.063, val_acc:0.957]
Epoch [36/120    avg_loss:0.079, val_acc:0.957]
Epoch [37/120    avg_loss:0.048, val_acc:0.984]
Epoch [38/120    avg_loss:0.069, val_acc:0.968]
Epoch [39/120    avg_loss:0.073, val_acc:0.970]
Epoch [40/120    avg_loss:0.050, val_acc:0.980]
Epoch [41/120    avg_loss:0.044, val_acc:0.980]
Epoch [42/120    avg_loss:0.033, val_acc:0.984]
Epoch [43/120    avg_loss:0.031, val_acc:0.985]
Epoch [44/120    avg_loss:0.055, val_acc:0.976]
Epoch [45/120    avg_loss:0.045, val_acc:0.970]
Epoch [46/120    avg_loss:0.047, val_acc:0.984]
Epoch [47/120    avg_loss:0.041, val_acc:0.983]
Epoch [48/120    avg_loss:0.030, val_acc:0.986]
Epoch [49/120    avg_loss:0.045, val_acc:0.962]
Epoch [50/120    avg_loss:0.034, val_acc:0.984]
Epoch [51/120    avg_loss:0.020, val_acc:0.984]
Epoch [52/120    avg_loss:0.021, val_acc:0.980]
Epoch [53/120    avg_loss:0.027, val_acc:0.976]
Epoch [54/120    avg_loss:0.019, val_acc:0.985]
Epoch [55/120    avg_loss:0.017, val_acc:0.985]
Epoch [56/120    avg_loss:0.019, val_acc:0.988]
Epoch [57/120    avg_loss:0.017, val_acc:0.990]
Epoch [58/120    avg_loss:0.033, val_acc:0.985]
Epoch [59/120    avg_loss:0.047, val_acc:0.986]
Epoch [60/120    avg_loss:0.027, val_acc:0.981]
Epoch [61/120    avg_loss:0.023, val_acc:0.970]
Epoch [62/120    avg_loss:0.932, val_acc:0.706]
Epoch [63/120    avg_loss:0.732, val_acc:0.787]
Epoch [64/120    avg_loss:0.595, val_acc:0.714]
Epoch [65/120    avg_loss:0.587, val_acc:0.833]
Epoch [66/120    avg_loss:0.483, val_acc:0.826]
Epoch [67/120    avg_loss:0.413, val_acc:0.840]
Epoch [68/120    avg_loss:0.390, val_acc:0.854]
Epoch [69/120    avg_loss:0.384, val_acc:0.859]
Epoch [70/120    avg_loss:0.360, val_acc:0.867]
Epoch [71/120    avg_loss:0.312, val_acc:0.872]
Epoch [72/120    avg_loss:0.266, val_acc:0.901]
Epoch [73/120    avg_loss:0.227, val_acc:0.901]
Epoch [74/120    avg_loss:0.226, val_acc:0.910]
Epoch [75/120    avg_loss:0.227, val_acc:0.911]
Epoch [76/120    avg_loss:0.219, val_acc:0.908]
Epoch [77/120    avg_loss:0.208, val_acc:0.918]
Epoch [78/120    avg_loss:0.204, val_acc:0.919]
Epoch [79/120    avg_loss:0.197, val_acc:0.932]
Epoch [80/120    avg_loss:0.182, val_acc:0.922]
Epoch [81/120    avg_loss:0.212, val_acc:0.932]
Epoch [82/120    avg_loss:0.180, val_acc:0.928]
Epoch [83/120    avg_loss:0.200, val_acc:0.919]
Epoch [84/120    avg_loss:0.176, val_acc:0.925]
Epoch [85/120    avg_loss:0.177, val_acc:0.932]
Epoch [86/120    avg_loss:0.171, val_acc:0.934]
Epoch [87/120    avg_loss:0.162, val_acc:0.932]
Epoch [88/120    avg_loss:0.171, val_acc:0.933]
Epoch [89/120    avg_loss:0.172, val_acc:0.931]
Epoch [90/120    avg_loss:0.181, val_acc:0.933]
Epoch [91/120    avg_loss:0.170, val_acc:0.933]
Epoch [92/120    avg_loss:0.189, val_acc:0.934]
Epoch [93/120    avg_loss:0.183, val_acc:0.933]
Epoch [94/120    avg_loss:0.166, val_acc:0.935]
Epoch [95/120    avg_loss:0.168, val_acc:0.935]
Epoch [96/120    avg_loss:0.163, val_acc:0.934]
Epoch [97/120    avg_loss:0.161, val_acc:0.934]
Epoch [98/120    avg_loss:0.165, val_acc:0.934]
Epoch [99/120    avg_loss:0.174, val_acc:0.934]
Epoch [100/120    avg_loss:0.164, val_acc:0.934]
Epoch [101/120    avg_loss:0.157, val_acc:0.934]
Epoch [102/120    avg_loss:0.168, val_acc:0.934]
Epoch [103/120    avg_loss:0.185, val_acc:0.934]
Epoch [104/120    avg_loss:0.174, val_acc:0.934]
Epoch [105/120    avg_loss:0.172, val_acc:0.934]
Epoch [106/120    avg_loss:0.156, val_acc:0.934]
Epoch [107/120    avg_loss:0.173, val_acc:0.934]
Epoch [108/120    avg_loss:0.157, val_acc:0.934]
Epoch [109/120    avg_loss:0.168, val_acc:0.934]
Epoch [110/120    avg_loss:0.171, val_acc:0.934]
Epoch [111/120    avg_loss:0.170, val_acc:0.934]
Epoch [112/120    avg_loss:0.165, val_acc:0.934]
Epoch [113/120    avg_loss:0.167, val_acc:0.934]
Epoch [114/120    avg_loss:0.161, val_acc:0.934]
Epoch [115/120    avg_loss:0.164, val_acc:0.934]
Epoch [116/120    avg_loss:0.165, val_acc:0.934]
Epoch [117/120    avg_loss:0.167, val_acc:0.934]
Epoch [118/120    avg_loss:0.175, val_acc:0.934]
Epoch [119/120    avg_loss:0.170, val_acc:0.934]
Epoch [120/120    avg_loss:0.164, val_acc:0.934]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5457     0   166   318     0     4    12   431    44]
 [    0     0 17896     0   158     0    35     0     1     0]
 [    0    15     0  1972     0     0     0     0    23    26]
 [    0    67    24     1  2776     0    54     0    42     8]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     5   111     6     1     0  4743     0    12     0]
 [    0    29     0     0     0     0     2  1239     0    20]
 [    0   111     0    61    58     0     3     0  3326    12]
 [    0     7     1     5    13    65     0     0     2   826]]

Accuracy:
95.29318198250307

F1 scores:
[       nan 0.90027221 0.99086429 0.92865552 0.88182973 0.97570093
 0.97602634 0.97520661 0.89794816 0.89056604]

Kappa:
0.9378038263704633
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd92e26e9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.034, val_acc:0.223]
Epoch [2/120    avg_loss:1.612, val_acc:0.289]
Epoch [3/120    avg_loss:1.399, val_acc:0.339]
Epoch [4/120    avg_loss:1.210, val_acc:0.412]
Epoch [5/120    avg_loss:1.041, val_acc:0.700]
Epoch [6/120    avg_loss:0.904, val_acc:0.726]
Epoch [7/120    avg_loss:0.811, val_acc:0.707]
Epoch [8/120    avg_loss:0.637, val_acc:0.752]
Epoch [9/120    avg_loss:0.566, val_acc:0.800]
Epoch [10/120    avg_loss:0.461, val_acc:0.824]
Epoch [11/120    avg_loss:0.409, val_acc:0.888]
Epoch [12/120    avg_loss:0.340, val_acc:0.901]
Epoch [13/120    avg_loss:0.282, val_acc:0.904]
Epoch [14/120    avg_loss:0.236, val_acc:0.935]
Epoch [15/120    avg_loss:0.237, val_acc:0.909]
Epoch [16/120    avg_loss:0.239, val_acc:0.931]
Epoch [17/120    avg_loss:0.226, val_acc:0.908]
Epoch [18/120    avg_loss:0.209, val_acc:0.896]
Epoch [19/120    avg_loss:0.208, val_acc:0.914]
Epoch [20/120    avg_loss:0.152, val_acc:0.946]
Epoch [21/120    avg_loss:0.154, val_acc:0.440]
Epoch [22/120    avg_loss:0.272, val_acc:0.915]
Epoch [23/120    avg_loss:0.333, val_acc:0.878]
Epoch [24/120    avg_loss:0.227, val_acc:0.940]
Epoch [25/120    avg_loss:0.144, val_acc:0.940]
Epoch [26/120    avg_loss:0.112, val_acc:0.965]
Epoch [27/120    avg_loss:0.144, val_acc:0.854]
Epoch [28/120    avg_loss:0.151, val_acc:0.912]
Epoch [29/120    avg_loss:0.106, val_acc:0.952]
Epoch [30/120    avg_loss:0.068, val_acc:0.967]
Epoch [31/120    avg_loss:0.195, val_acc:0.761]
Epoch [32/120    avg_loss:0.378, val_acc:0.840]
Epoch [33/120    avg_loss:0.252, val_acc:0.946]
Epoch [34/120    avg_loss:0.150, val_acc:0.897]
Epoch [35/120    avg_loss:0.121, val_acc:0.953]
Epoch [36/120    avg_loss:0.109, val_acc:0.952]
Epoch [37/120    avg_loss:0.111, val_acc:0.965]
Epoch [38/120    avg_loss:0.089, val_acc:0.963]
Epoch [39/120    avg_loss:0.085, val_acc:0.946]
Epoch [40/120    avg_loss:0.064, val_acc:0.946]
Epoch [41/120    avg_loss:0.055, val_acc:0.969]
Epoch [42/120    avg_loss:0.077, val_acc:0.962]
Epoch [43/120    avg_loss:0.055, val_acc:0.979]
Epoch [44/120    avg_loss:0.045, val_acc:0.973]
Epoch [45/120    avg_loss:0.048, val_acc:0.965]
Epoch [46/120    avg_loss:0.052, val_acc:0.969]
Epoch [47/120    avg_loss:0.056, val_acc:0.967]
Epoch [48/120    avg_loss:0.036, val_acc:0.971]
Epoch [49/120    avg_loss:0.038, val_acc:0.978]
Epoch [50/120    avg_loss:0.042, val_acc:0.974]
Epoch [51/120    avg_loss:0.035, val_acc:0.976]
Epoch [52/120    avg_loss:0.028, val_acc:0.982]
Epoch [53/120    avg_loss:0.036, val_acc:0.976]
Epoch [54/120    avg_loss:0.034, val_acc:0.980]
Epoch [55/120    avg_loss:0.038, val_acc:0.979]
Epoch [56/120    avg_loss:0.019, val_acc:0.980]
Epoch [57/120    avg_loss:0.018, val_acc:0.972]
Epoch [58/120    avg_loss:0.023, val_acc:0.980]
Epoch [59/120    avg_loss:0.035, val_acc:0.974]
Epoch [60/120    avg_loss:0.025, val_acc:0.980]
Epoch [61/120    avg_loss:0.019, val_acc:0.974]
Epoch [62/120    avg_loss:0.025, val_acc:0.979]
Epoch [63/120    avg_loss:0.029, val_acc:0.935]
Epoch [64/120    avg_loss:0.020, val_acc:0.977]
Epoch [65/120    avg_loss:0.019, val_acc:0.979]
Epoch [66/120    avg_loss:0.017, val_acc:0.981]
Epoch [67/120    avg_loss:0.011, val_acc:0.980]
Epoch [68/120    avg_loss:0.015, val_acc:0.983]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.011, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.014, val_acc:0.983]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.013, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.022, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.011, val_acc:0.985]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.012, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.013, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.009, val_acc:0.987]
Epoch [118/120    avg_loss:0.009, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.987]
Epoch [120/120    avg_loss:0.011, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     0     2     0     7     0    49     4]
 [    0     0 18030     0    50     0     8     0     2     0]
 [    0     5     0  1987     1     0     0     0    36     7]
 [    0    12    13     2  2908     0     8     0    26     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     4     0     0  4850     0     0     0]
 [    0     1     0     0     0     0     0  1288     0     1]
 [    0     1     0     7    61     0     0     0  3491    11]
 [    0     0     0     4    14    69     0     0     0   832]]

Accuracy:
98.95886053069192

F1 scores:
[       nan 0.99368224 0.99731726 0.98366337 0.96804261 0.97424412
 0.99476977 0.9992242  0.97310105 0.93640968]

Kappa:
0.9862089723253633
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b496ed9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.013, val_acc:0.158]
Epoch [2/120    avg_loss:1.703, val_acc:0.290]
Epoch [3/120    avg_loss:1.501, val_acc:0.348]
Epoch [4/120    avg_loss:1.372, val_acc:0.431]
Epoch [5/120    avg_loss:1.157, val_acc:0.424]
Epoch [6/120    avg_loss:1.059, val_acc:0.490]
Epoch [7/120    avg_loss:0.902, val_acc:0.641]
Epoch [8/120    avg_loss:0.780, val_acc:0.711]
Epoch [9/120    avg_loss:0.658, val_acc:0.751]
Epoch [10/120    avg_loss:0.595, val_acc:0.764]
Epoch [11/120    avg_loss:0.494, val_acc:0.834]
Epoch [12/120    avg_loss:0.433, val_acc:0.840]
Epoch [13/120    avg_loss:0.406, val_acc:0.861]
Epoch [14/120    avg_loss:0.305, val_acc:0.875]
Epoch [15/120    avg_loss:0.311, val_acc:0.866]
Epoch [16/120    avg_loss:0.330, val_acc:0.905]
Epoch [17/120    avg_loss:0.287, val_acc:0.907]
Epoch [18/120    avg_loss:0.230, val_acc:0.868]
Epoch [19/120    avg_loss:0.234, val_acc:0.917]
Epoch [20/120    avg_loss:0.238, val_acc:0.892]
Epoch [21/120    avg_loss:0.200, val_acc:0.933]
Epoch [22/120    avg_loss:0.143, val_acc:0.942]
Epoch [23/120    avg_loss:0.164, val_acc:0.934]
Epoch [24/120    avg_loss:0.151, val_acc:0.917]
Epoch [25/120    avg_loss:0.258, val_acc:0.550]
Epoch [26/120    avg_loss:0.451, val_acc:0.869]
Epoch [27/120    avg_loss:0.227, val_acc:0.921]
Epoch [28/120    avg_loss:0.182, val_acc:0.942]
Epoch [29/120    avg_loss:0.167, val_acc:0.936]
Epoch [30/120    avg_loss:0.148, val_acc:0.946]
Epoch [31/120    avg_loss:0.128, val_acc:0.902]
Epoch [32/120    avg_loss:0.141, val_acc:0.846]
Epoch [33/120    avg_loss:0.248, val_acc:0.878]
Epoch [34/120    avg_loss:0.230, val_acc:0.872]
Epoch [35/120    avg_loss:0.148, val_acc:0.949]
Epoch [36/120    avg_loss:0.128, val_acc:0.948]
Epoch [37/120    avg_loss:0.311, val_acc:0.905]
Epoch [38/120    avg_loss:0.165, val_acc:0.952]
Epoch [39/120    avg_loss:0.135, val_acc:0.955]
Epoch [40/120    avg_loss:0.092, val_acc:0.959]
Epoch [41/120    avg_loss:0.072, val_acc:0.964]
Epoch [42/120    avg_loss:0.080, val_acc:0.959]
Epoch [43/120    avg_loss:0.071, val_acc:0.966]
Epoch [44/120    avg_loss:0.056, val_acc:0.960]
Epoch [45/120    avg_loss:0.081, val_acc:0.964]
Epoch [46/120    avg_loss:0.049, val_acc:0.964]
Epoch [47/120    avg_loss:0.060, val_acc:0.965]
Epoch [48/120    avg_loss:0.052, val_acc:0.967]
Epoch [49/120    avg_loss:0.049, val_acc:0.970]
Epoch [50/120    avg_loss:0.045, val_acc:0.964]
Epoch [51/120    avg_loss:0.046, val_acc:0.982]
Epoch [52/120    avg_loss:0.032, val_acc:0.975]
Epoch [53/120    avg_loss:0.048, val_acc:0.979]
Epoch [54/120    avg_loss:0.060, val_acc:0.954]
Epoch [55/120    avg_loss:0.075, val_acc:0.924]
Epoch [56/120    avg_loss:0.063, val_acc:0.966]
Epoch [57/120    avg_loss:0.038, val_acc:0.975]
Epoch [58/120    avg_loss:0.033, val_acc:0.981]
Epoch [59/120    avg_loss:0.029, val_acc:0.966]
Epoch [60/120    avg_loss:0.034, val_acc:0.982]
Epoch [61/120    avg_loss:0.027, val_acc:0.985]
Epoch [62/120    avg_loss:0.038, val_acc:0.982]
Epoch [63/120    avg_loss:0.026, val_acc:0.984]
Epoch [64/120    avg_loss:0.024, val_acc:0.984]
Epoch [65/120    avg_loss:0.020, val_acc:0.980]
Epoch [66/120    avg_loss:0.021, val_acc:0.984]
Epoch [67/120    avg_loss:0.016, val_acc:0.984]
Epoch [68/120    avg_loss:0.017, val_acc:0.986]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.011, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.982]
Epoch [72/120    avg_loss:0.016, val_acc:0.987]
Epoch [73/120    avg_loss:0.011, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.990]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.019, val_acc:0.981]
Epoch [78/120    avg_loss:0.017, val_acc:0.986]
Epoch [79/120    avg_loss:0.016, val_acc:0.980]
Epoch [80/120    avg_loss:0.028, val_acc:0.987]
Epoch [81/120    avg_loss:0.939, val_acc:0.444]
Epoch [82/120    avg_loss:1.193, val_acc:0.612]
Epoch [83/120    avg_loss:0.942, val_acc:0.763]
Epoch [84/120    avg_loss:0.755, val_acc:0.588]
Epoch [85/120    avg_loss:0.690, val_acc:0.711]
Epoch [86/120    avg_loss:0.536, val_acc:0.812]
Epoch [87/120    avg_loss:0.524, val_acc:0.847]
Epoch [88/120    avg_loss:0.422, val_acc:0.817]
Epoch [89/120    avg_loss:0.361, val_acc:0.844]
Epoch [90/120    avg_loss:0.372, val_acc:0.819]
Epoch [91/120    avg_loss:0.331, val_acc:0.841]
Epoch [92/120    avg_loss:0.333, val_acc:0.832]
Epoch [93/120    avg_loss:0.330, val_acc:0.840]
Epoch [94/120    avg_loss:0.331, val_acc:0.826]
Epoch [95/120    avg_loss:0.322, val_acc:0.836]
Epoch [96/120    avg_loss:0.322, val_acc:0.827]
Epoch [97/120    avg_loss:0.299, val_acc:0.844]
Epoch [98/120    avg_loss:0.314, val_acc:0.845]
Epoch [99/120    avg_loss:0.319, val_acc:0.846]
Epoch [100/120    avg_loss:0.291, val_acc:0.840]
Epoch [101/120    avg_loss:0.296, val_acc:0.841]
Epoch [102/120    avg_loss:0.313, val_acc:0.844]
Epoch [103/120    avg_loss:0.287, val_acc:0.844]
Epoch [104/120    avg_loss:0.301, val_acc:0.845]
Epoch [105/120    avg_loss:0.286, val_acc:0.845]
Epoch [106/120    avg_loss:0.318, val_acc:0.844]
Epoch [107/120    avg_loss:0.308, val_acc:0.844]
Epoch [108/120    avg_loss:0.298, val_acc:0.843]
Epoch [109/120    avg_loss:0.289, val_acc:0.843]
Epoch [110/120    avg_loss:0.300, val_acc:0.844]
Epoch [111/120    avg_loss:0.278, val_acc:0.845]
Epoch [112/120    avg_loss:0.277, val_acc:0.843]
Epoch [113/120    avg_loss:0.291, val_acc:0.845]
Epoch [114/120    avg_loss:0.295, val_acc:0.845]
Epoch [115/120    avg_loss:0.292, val_acc:0.845]
Epoch [116/120    avg_loss:0.296, val_acc:0.844]
Epoch [117/120    avg_loss:0.294, val_acc:0.844]
Epoch [118/120    avg_loss:0.288, val_acc:0.845]
Epoch [119/120    avg_loss:0.305, val_acc:0.845]
Epoch [120/120    avg_loss:0.305, val_acc:0.845]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5167    21    68   196     0     1   137   653   189]
 [    0    17 15773     0   113     0  2186     0     1     0]
 [    0     9     0  1926     0     0     0    22    64    15]
 [    0   167    53     3  2649     0    57     0    41     2]
 [    0     0     0     0     0  1304     0     1     0     0]
 [    0     1   356    30    11     0  4222   123   135     0]
 [    0     1     0     0     0     0     0  1269     4    16]
 [    0   106     0    12    53     0    19     0  3362    19]
 [    0     2     0    10    18   124     0     2    15   748]]

Accuracy:
87.77384137083364

F1 scores:
[       nan 0.86825744 0.91989619 0.94296206 0.88123752 0.95426271
 0.74311361 0.89240506 0.8569972  0.78406709]

Kappa:
0.8415539707551117
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faef01f1978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.078, val_acc:0.194]
Epoch [2/120    avg_loss:1.725, val_acc:0.297]
Epoch [3/120    avg_loss:1.508, val_acc:0.365]
Epoch [4/120    avg_loss:1.313, val_acc:0.406]
Epoch [5/120    avg_loss:1.154, val_acc:0.505]
Epoch [6/120    avg_loss:1.007, val_acc:0.562]
Epoch [7/120    avg_loss:0.904, val_acc:0.611]
Epoch [8/120    avg_loss:0.766, val_acc:0.600]
Epoch [9/120    avg_loss:0.651, val_acc:0.624]
Epoch [10/120    avg_loss:0.568, val_acc:0.704]
Epoch [11/120    avg_loss:0.579, val_acc:0.769]
Epoch [12/120    avg_loss:0.501, val_acc:0.810]
Epoch [13/120    avg_loss:0.403, val_acc:0.849]
Epoch [14/120    avg_loss:0.315, val_acc:0.786]
Epoch [15/120    avg_loss:0.298, val_acc:0.848]
Epoch [16/120    avg_loss:0.282, val_acc:0.910]
Epoch [17/120    avg_loss:0.271, val_acc:0.917]
Epoch [18/120    avg_loss:0.217, val_acc:0.939]
Epoch [19/120    avg_loss:0.200, val_acc:0.901]
Epoch [20/120    avg_loss:0.225, val_acc:0.868]
Epoch [21/120    avg_loss:0.228, val_acc:0.924]
Epoch [22/120    avg_loss:0.205, val_acc:0.947]
Epoch [23/120    avg_loss:0.173, val_acc:0.922]
Epoch [24/120    avg_loss:0.188, val_acc:0.933]
Epoch [25/120    avg_loss:0.124, val_acc:0.935]
Epoch [26/120    avg_loss:0.127, val_acc:0.953]
Epoch [27/120    avg_loss:0.088, val_acc:0.941]
Epoch [28/120    avg_loss:0.402, val_acc:0.370]
Epoch [29/120    avg_loss:1.207, val_acc:0.505]
Epoch [30/120    avg_loss:1.088, val_acc:0.503]
Epoch [31/120    avg_loss:1.016, val_acc:0.587]
Epoch [32/120    avg_loss:0.993, val_acc:0.576]
Epoch [33/120    avg_loss:0.931, val_acc:0.635]
Epoch [34/120    avg_loss:0.827, val_acc:0.659]
Epoch [35/120    avg_loss:0.761, val_acc:0.682]
Epoch [36/120    avg_loss:0.792, val_acc:0.710]
Epoch [37/120    avg_loss:0.764, val_acc:0.714]
Epoch [38/120    avg_loss:0.669, val_acc:0.701]
Epoch [39/120    avg_loss:0.600, val_acc:0.752]
Epoch [40/120    avg_loss:0.584, val_acc:0.761]
Epoch [41/120    avg_loss:0.534, val_acc:0.762]
Epoch [42/120    avg_loss:0.532, val_acc:0.760]
Epoch [43/120    avg_loss:0.544, val_acc:0.766]
Epoch [44/120    avg_loss:0.543, val_acc:0.772]
Epoch [45/120    avg_loss:0.528, val_acc:0.766]
Epoch [46/120    avg_loss:0.510, val_acc:0.770]
Epoch [47/120    avg_loss:0.527, val_acc:0.769]
Epoch [48/120    avg_loss:0.532, val_acc:0.767]
Epoch [49/120    avg_loss:0.494, val_acc:0.771]
Epoch [50/120    avg_loss:0.499, val_acc:0.773]
Epoch [51/120    avg_loss:0.506, val_acc:0.774]
Epoch [52/120    avg_loss:0.516, val_acc:0.779]
Epoch [53/120    avg_loss:0.495, val_acc:0.779]
Epoch [54/120    avg_loss:0.497, val_acc:0.774]
Epoch [55/120    avg_loss:0.501, val_acc:0.780]
Epoch [56/120    avg_loss:0.485, val_acc:0.778]
Epoch [57/120    avg_loss:0.501, val_acc:0.780]
Epoch [58/120    avg_loss:0.476, val_acc:0.782]
Epoch [59/120    avg_loss:0.487, val_acc:0.781]
Epoch [60/120    avg_loss:0.490, val_acc:0.778]
Epoch [61/120    avg_loss:0.494, val_acc:0.785]
Epoch [62/120    avg_loss:0.495, val_acc:0.782]
Epoch [63/120    avg_loss:0.487, val_acc:0.783]
Epoch [64/120    avg_loss:0.480, val_acc:0.781]
Epoch [65/120    avg_loss:0.456, val_acc:0.782]
Epoch [66/120    avg_loss:0.508, val_acc:0.782]
Epoch [67/120    avg_loss:0.492, val_acc:0.782]
Epoch [68/120    avg_loss:0.496, val_acc:0.783]
Epoch [69/120    avg_loss:0.469, val_acc:0.782]
Epoch [70/120    avg_loss:0.501, val_acc:0.783]
Epoch [71/120    avg_loss:0.471, val_acc:0.783]
Epoch [72/120    avg_loss:0.495, val_acc:0.783]
Epoch [73/120    avg_loss:0.479, val_acc:0.782]
Epoch [74/120    avg_loss:0.490, val_acc:0.783]
Epoch [75/120    avg_loss:0.478, val_acc:0.783]
Epoch [76/120    avg_loss:0.480, val_acc:0.784]
Epoch [77/120    avg_loss:0.513, val_acc:0.785]
Epoch [78/120    avg_loss:0.481, val_acc:0.785]
Epoch [79/120    avg_loss:0.477, val_acc:0.785]
Epoch [80/120    avg_loss:0.493, val_acc:0.785]
Epoch [81/120    avg_loss:0.494, val_acc:0.785]
Epoch [82/120    avg_loss:0.504, val_acc:0.785]
Epoch [83/120    avg_loss:0.486, val_acc:0.785]
Epoch [84/120    avg_loss:0.487, val_acc:0.785]
Epoch [85/120    avg_loss:0.492, val_acc:0.785]
Epoch [86/120    avg_loss:0.493, val_acc:0.785]
Epoch [87/120    avg_loss:0.506, val_acc:0.785]
Epoch [88/120    avg_loss:0.482, val_acc:0.785]
Epoch [89/120    avg_loss:0.511, val_acc:0.785]
Epoch [90/120    avg_loss:0.465, val_acc:0.785]
Epoch [91/120    avg_loss:0.483, val_acc:0.785]
Epoch [92/120    avg_loss:0.468, val_acc:0.785]
Epoch [93/120    avg_loss:0.479, val_acc:0.785]
Epoch [94/120    avg_loss:0.508, val_acc:0.785]
Epoch [95/120    avg_loss:0.513, val_acc:0.785]
Epoch [96/120    avg_loss:0.484, val_acc:0.785]
Epoch [97/120    avg_loss:0.521, val_acc:0.785]
Epoch [98/120    avg_loss:0.488, val_acc:0.785]
Epoch [99/120    avg_loss:0.473, val_acc:0.785]
Epoch [100/120    avg_loss:0.479, val_acc:0.785]
Epoch [101/120    avg_loss:0.490, val_acc:0.785]
Epoch [102/120    avg_loss:0.486, val_acc:0.785]
Epoch [103/120    avg_loss:0.475, val_acc:0.785]
Epoch [104/120    avg_loss:0.484, val_acc:0.785]
Epoch [105/120    avg_loss:0.490, val_acc:0.785]
Epoch [106/120    avg_loss:0.491, val_acc:0.785]
Epoch [107/120    avg_loss:0.486, val_acc:0.785]
Epoch [108/120    avg_loss:0.478, val_acc:0.785]
Epoch [109/120    avg_loss:0.478, val_acc:0.785]
Epoch [110/120    avg_loss:0.514, val_acc:0.785]
Epoch [111/120    avg_loss:0.494, val_acc:0.785]
Epoch [112/120    avg_loss:0.464, val_acc:0.785]
Epoch [113/120    avg_loss:0.478, val_acc:0.785]
Epoch [114/120    avg_loss:0.491, val_acc:0.785]
Epoch [115/120    avg_loss:0.471, val_acc:0.785]
Epoch [116/120    avg_loss:0.496, val_acc:0.785]
Epoch [117/120    avg_loss:0.485, val_acc:0.785]
Epoch [118/120    avg_loss:0.478, val_acc:0.785]
Epoch [119/120    avg_loss:0.466, val_acc:0.785]
Epoch [120/120    avg_loss:0.508, val_acc:0.785]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4453    11   216   638     0   225    49   677   163]
 [    0     1 15133     0  1007     0  1949     0     0     0]
 [    0     8     0  1726    10     0     6     0   191    95]
 [    0   145    90     0  2536     0   160     0    37     4]
 [    0     0     0     0     0  1304     0     1     0     0]
 [    0     0   210     0   179     0  4489     0     0     0]
 [    0    56     1     0     4     0     1  1202     4    22]
 [    0   292    16   184   106     0     9     0  2964     0]
 [    0    29     2     6    17   132    16     0    12   705]]

Accuracy:
83.17547538138963

F1 scores:
[       nan 0.78013315 0.90203559 0.82821497 0.6790735  0.95147756
 0.76519219 0.94571204 0.79506438 0.73899371]

Kappa:
0.7839097260716325
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67f014f978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.049, val_acc:0.148]
Epoch [2/120    avg_loss:1.708, val_acc:0.265]
Epoch [3/120    avg_loss:1.479, val_acc:0.319]
Epoch [4/120    avg_loss:1.278, val_acc:0.354]
Epoch [5/120    avg_loss:1.169, val_acc:0.411]
Epoch [6/120    avg_loss:1.023, val_acc:0.555]
Epoch [7/120    avg_loss:0.916, val_acc:0.604]
Epoch [8/120    avg_loss:0.732, val_acc:0.761]
Epoch [9/120    avg_loss:0.630, val_acc:0.737]
Epoch [10/120    avg_loss:0.541, val_acc:0.787]
Epoch [11/120    avg_loss:0.504, val_acc:0.806]
Epoch [12/120    avg_loss:0.405, val_acc:0.851]
Epoch [13/120    avg_loss:0.308, val_acc:0.901]
Epoch [14/120    avg_loss:0.370, val_acc:0.891]
Epoch [15/120    avg_loss:0.307, val_acc:0.897]
Epoch [16/120    avg_loss:0.282, val_acc:0.911]
Epoch [17/120    avg_loss:0.272, val_acc:0.923]
Epoch [18/120    avg_loss:0.248, val_acc:0.924]
Epoch [19/120    avg_loss:0.259, val_acc:0.868]
Epoch [20/120    avg_loss:0.509, val_acc:0.859]
Epoch [21/120    avg_loss:0.319, val_acc:0.912]
Epoch [22/120    avg_loss:0.290, val_acc:0.903]
Epoch [23/120    avg_loss:0.214, val_acc:0.921]
Epoch [24/120    avg_loss:0.186, val_acc:0.925]
Epoch [25/120    avg_loss:0.129, val_acc:0.918]
Epoch [26/120    avg_loss:0.155, val_acc:0.921]
Epoch [27/120    avg_loss:0.114, val_acc:0.945]
Epoch [28/120    avg_loss:0.103, val_acc:0.934]
Epoch [29/120    avg_loss:0.107, val_acc:0.943]
Epoch [30/120    avg_loss:0.086, val_acc:0.940]
Epoch [31/120    avg_loss:0.111, val_acc:0.937]
Epoch [32/120    avg_loss:0.097, val_acc:0.942]
Epoch [33/120    avg_loss:0.087, val_acc:0.951]
Epoch [34/120    avg_loss:0.107, val_acc:0.920]
Epoch [35/120    avg_loss:0.099, val_acc:0.959]
Epoch [36/120    avg_loss:0.066, val_acc:0.956]
Epoch [37/120    avg_loss:0.066, val_acc:0.946]
Epoch [38/120    avg_loss:0.062, val_acc:0.957]
Epoch [39/120    avg_loss:0.051, val_acc:0.962]
Epoch [40/120    avg_loss:0.074, val_acc:0.938]
Epoch [41/120    avg_loss:0.095, val_acc:0.956]
Epoch [42/120    avg_loss:0.090, val_acc:0.939]
Epoch [43/120    avg_loss:0.100, val_acc:0.964]
Epoch [44/120    avg_loss:0.056, val_acc:0.970]
Epoch [45/120    avg_loss:0.064, val_acc:0.956]
Epoch [46/120    avg_loss:0.068, val_acc:0.944]
Epoch [47/120    avg_loss:0.053, val_acc:0.937]
Epoch [48/120    avg_loss:0.061, val_acc:0.963]
Epoch [49/120    avg_loss:0.050, val_acc:0.968]
Epoch [50/120    avg_loss:0.063, val_acc:0.963]
Epoch [51/120    avg_loss:0.043, val_acc:0.960]
Epoch [52/120    avg_loss:0.034, val_acc:0.966]
Epoch [53/120    avg_loss:0.028, val_acc:0.965]
Epoch [54/120    avg_loss:0.051, val_acc:0.961]
Epoch [55/120    avg_loss:0.032, val_acc:0.968]
Epoch [56/120    avg_loss:0.028, val_acc:0.968]
Epoch [57/120    avg_loss:0.051, val_acc:0.964]
Epoch [58/120    avg_loss:0.028, val_acc:0.973]
Epoch [59/120    avg_loss:0.020, val_acc:0.978]
Epoch [60/120    avg_loss:0.030, val_acc:0.977]
Epoch [61/120    avg_loss:0.018, val_acc:0.977]
Epoch [62/120    avg_loss:0.017, val_acc:0.978]
Epoch [63/120    avg_loss:0.020, val_acc:0.978]
Epoch [64/120    avg_loss:0.016, val_acc:0.979]
Epoch [65/120    avg_loss:0.020, val_acc:0.977]
Epoch [66/120    avg_loss:0.017, val_acc:0.978]
Epoch [67/120    avg_loss:0.019, val_acc:0.977]
Epoch [68/120    avg_loss:0.021, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.977]
Epoch [70/120    avg_loss:0.017, val_acc:0.977]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.979]
Epoch [73/120    avg_loss:0.019, val_acc:0.977]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.017, val_acc:0.977]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.979]
Epoch [78/120    avg_loss:0.019, val_acc:0.979]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.013, val_acc:0.982]
Epoch [81/120    avg_loss:0.012, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.976]
Epoch [85/120    avg_loss:0.015, val_acc:0.977]
Epoch [86/120    avg_loss:0.013, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.977]
Epoch [88/120    avg_loss:0.014, val_acc:0.979]
Epoch [89/120    avg_loss:0.015, val_acc:0.980]
Epoch [90/120    avg_loss:0.014, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.981]
Epoch [92/120    avg_loss:0.014, val_acc:0.980]
Epoch [93/120    avg_loss:0.016, val_acc:0.980]
Epoch [94/120    avg_loss:0.014, val_acc:0.981]
Epoch [95/120    avg_loss:0.013, val_acc:0.981]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.981]
Epoch [102/120    avg_loss:0.018, val_acc:0.981]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.982]
Epoch [106/120    avg_loss:0.014, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.014, val_acc:0.982]
Epoch [112/120    avg_loss:0.014, val_acc:0.982]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.011, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.015, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     0     8     0     0     0    65     4]
 [    0     4 18015     0    47     0    24     0     0     0]
 [    0     5     0  1988     1     0     0     0    40     2]
 [    0    56    20     0  2860     0    10     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    16    20     0     0  4842     0     0     0]
 [    0     0     0     0     0     0     0  1280     0    10]
 [    0    13     0    21    67     0     0     0  3464     6]
 [    0     0     0    10    17   100     0     0     0   792]]

Accuracy:
98.57325331983708

F1 scores:
[       nan 0.98795181 0.9969287  0.97570552 0.95780308 0.96309963
 0.99282346 0.99610895 0.96678761 0.91402193]

Kappa:
0.981102201626035
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fddc89e1978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.973, val_acc:0.293]
Epoch [2/120    avg_loss:1.614, val_acc:0.297]
Epoch [3/120    avg_loss:1.381, val_acc:0.407]
Epoch [4/120    avg_loss:1.207, val_acc:0.585]
Epoch [5/120    avg_loss:0.994, val_acc:0.705]
Epoch [6/120    avg_loss:0.845, val_acc:0.790]
Epoch [7/120    avg_loss:0.745, val_acc:0.764]
Epoch [8/120    avg_loss:0.635, val_acc:0.779]
Epoch [9/120    avg_loss:0.549, val_acc:0.815]
Epoch [10/120    avg_loss:0.462, val_acc:0.868]
Epoch [11/120    avg_loss:0.421, val_acc:0.873]
Epoch [12/120    avg_loss:0.379, val_acc:0.847]
Epoch [13/120    avg_loss:0.329, val_acc:0.814]
Epoch [14/120    avg_loss:0.366, val_acc:0.880]
Epoch [15/120    avg_loss:0.299, val_acc:0.864]
Epoch [16/120    avg_loss:0.269, val_acc:0.911]
Epoch [17/120    avg_loss:0.244, val_acc:0.925]
Epoch [18/120    avg_loss:0.203, val_acc:0.951]
Epoch [19/120    avg_loss:0.181, val_acc:0.924]
Epoch [20/120    avg_loss:0.179, val_acc:0.845]
Epoch [21/120    avg_loss:0.179, val_acc:0.938]
Epoch [22/120    avg_loss:0.160, val_acc:0.957]
Epoch [23/120    avg_loss:0.187, val_acc:0.884]
Epoch [24/120    avg_loss:0.173, val_acc:0.927]
Epoch [25/120    avg_loss:0.161, val_acc:0.944]
Epoch [26/120    avg_loss:0.123, val_acc:0.937]
Epoch [27/120    avg_loss:0.100, val_acc:0.950]
Epoch [28/120    avg_loss:0.135, val_acc:0.936]
Epoch [29/120    avg_loss:0.107, val_acc:0.969]
Epoch [30/120    avg_loss:0.091, val_acc:0.959]
Epoch [31/120    avg_loss:0.069, val_acc:0.967]
Epoch [32/120    avg_loss:0.070, val_acc:0.961]
Epoch [33/120    avg_loss:0.052, val_acc:0.970]
Epoch [34/120    avg_loss:0.055, val_acc:0.969]
Epoch [35/120    avg_loss:0.048, val_acc:0.970]
Epoch [36/120    avg_loss:0.047, val_acc:0.974]
Epoch [37/120    avg_loss:0.051, val_acc:0.964]
Epoch [38/120    avg_loss:0.052, val_acc:0.958]
Epoch [39/120    avg_loss:0.042, val_acc:0.976]
Epoch [40/120    avg_loss:0.040, val_acc:0.977]
Epoch [41/120    avg_loss:0.032, val_acc:0.966]
Epoch [42/120    avg_loss:0.031, val_acc:0.975]
Epoch [43/120    avg_loss:0.027, val_acc:0.979]
Epoch [44/120    avg_loss:0.023, val_acc:0.985]
Epoch [45/120    avg_loss:0.022, val_acc:0.987]
Epoch [46/120    avg_loss:0.020, val_acc:0.977]
Epoch [47/120    avg_loss:0.017, val_acc:0.983]
Epoch [48/120    avg_loss:0.018, val_acc:0.987]
Epoch [49/120    avg_loss:0.024, val_acc:0.980]
Epoch [50/120    avg_loss:0.017, val_acc:0.984]
Epoch [51/120    avg_loss:0.057, val_acc:0.976]
Epoch [52/120    avg_loss:0.032, val_acc:0.979]
Epoch [53/120    avg_loss:0.029, val_acc:0.959]
Epoch [54/120    avg_loss:0.021, val_acc:0.982]
Epoch [55/120    avg_loss:0.045, val_acc:0.975]
Epoch [56/120    avg_loss:0.038, val_acc:0.972]
Epoch [57/120    avg_loss:0.039, val_acc:0.978]
Epoch [58/120    avg_loss:0.025, val_acc:0.981]
Epoch [59/120    avg_loss:0.018, val_acc:0.977]
Epoch [60/120    avg_loss:0.087, val_acc:0.947]
Epoch [61/120    avg_loss:0.037, val_acc:0.969]
Epoch [62/120    avg_loss:0.024, val_acc:0.978]
Epoch [63/120    avg_loss:0.023, val_acc:0.982]
Epoch [64/120    avg_loss:0.016, val_acc:0.982]
Epoch [65/120    avg_loss:0.015, val_acc:0.982]
Epoch [66/120    avg_loss:0.013, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.983]
Epoch [68/120    avg_loss:0.017, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.985]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.014, val_acc:0.986]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.985]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.013, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.986]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.013, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.987]
Epoch [84/120    avg_loss:0.010, val_acc:0.987]
Epoch [85/120    avg_loss:0.011, val_acc:0.987]
Epoch [86/120    avg_loss:0.011, val_acc:0.987]
Epoch [87/120    avg_loss:0.010, val_acc:0.987]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.012, val_acc:0.988]
Epoch [90/120    avg_loss:0.012, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.987]
Epoch [98/120    avg_loss:0.011, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.012, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.012, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.987]
Epoch [114/120    avg_loss:0.014, val_acc:0.987]
Epoch [115/120    avg_loss:0.012, val_acc:0.987]
Epoch [116/120    avg_loss:0.013, val_acc:0.986]
Epoch [117/120    avg_loss:0.012, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.017, val_acc:0.987]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     0     0     0     1     0    83     1]
 [    0     0 17955     0   109     0    26     0     0     0]
 [    0     0     0  1989     2     0     0     0    40     5]
 [    0    27    18     0  2885     0    12     0    29     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     8     0     0  4867     0     0     1]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     6     0     0    54     0     0     0  3508     3]
 [    0     0     0     1    14    35     0     0     0   869]]

Accuracy:
98.84558841250332

F1 scores:
[       nan 0.99078988 0.9957022  0.986118   0.95593108 0.98676749
 0.99488962 0.99961225 0.97026691 0.96555556]

Kappa:
0.9847239814023449
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8fe1013b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.114, val_acc:0.390]
Epoch [2/120    avg_loss:1.722, val_acc:0.414]
Epoch [3/120    avg_loss:1.451, val_acc:0.419]
Epoch [4/120    avg_loss:1.183, val_acc:0.485]
Epoch [5/120    avg_loss:0.978, val_acc:0.615]
Epoch [6/120    avg_loss:0.794, val_acc:0.676]
Epoch [7/120    avg_loss:0.637, val_acc:0.747]
Epoch [8/120    avg_loss:0.635, val_acc:0.747]
Epoch [9/120    avg_loss:0.490, val_acc:0.747]
Epoch [10/120    avg_loss:0.480, val_acc:0.789]
Epoch [11/120    avg_loss:0.416, val_acc:0.819]
Epoch [12/120    avg_loss:0.401, val_acc:0.820]
Epoch [13/120    avg_loss:0.367, val_acc:0.840]
Epoch [14/120    avg_loss:0.343, val_acc:0.857]
Epoch [15/120    avg_loss:0.326, val_acc:0.805]
Epoch [16/120    avg_loss:0.250, val_acc:0.879]
Epoch [17/120    avg_loss:0.227, val_acc:0.855]
Epoch [18/120    avg_loss:0.235, val_acc:0.894]
Epoch [19/120    avg_loss:0.226, val_acc:0.916]
Epoch [20/120    avg_loss:0.205, val_acc:0.907]
Epoch [21/120    avg_loss:0.182, val_acc:0.915]
Epoch [22/120    avg_loss:0.197, val_acc:0.915]
Epoch [23/120    avg_loss:0.171, val_acc:0.936]
Epoch [24/120    avg_loss:0.149, val_acc:0.942]
Epoch [25/120    avg_loss:0.139, val_acc:0.912]
Epoch [26/120    avg_loss:0.127, val_acc:0.943]
Epoch [27/120    avg_loss:0.090, val_acc:0.954]
Epoch [28/120    avg_loss:0.098, val_acc:0.944]
Epoch [29/120    avg_loss:0.103, val_acc:0.944]
Epoch [30/120    avg_loss:0.087, val_acc:0.954]
Epoch [31/120    avg_loss:0.102, val_acc:0.957]
Epoch [32/120    avg_loss:0.094, val_acc:0.962]
Epoch [33/120    avg_loss:0.059, val_acc:0.964]
Epoch [34/120    avg_loss:0.077, val_acc:0.967]
Epoch [35/120    avg_loss:0.092, val_acc:0.957]
Epoch [36/120    avg_loss:0.072, val_acc:0.951]
Epoch [37/120    avg_loss:0.056, val_acc:0.957]
Epoch [38/120    avg_loss:0.088, val_acc:0.951]
Epoch [39/120    avg_loss:0.137, val_acc:0.948]
Epoch [40/120    avg_loss:0.090, val_acc:0.960]
Epoch [41/120    avg_loss:0.059, val_acc:0.937]
Epoch [42/120    avg_loss:0.067, val_acc:0.936]
Epoch [43/120    avg_loss:0.071, val_acc:0.963]
Epoch [44/120    avg_loss:0.042, val_acc:0.964]
Epoch [45/120    avg_loss:0.045, val_acc:0.969]
Epoch [46/120    avg_loss:0.033, val_acc:0.971]
Epoch [47/120    avg_loss:0.056, val_acc:0.961]
Epoch [48/120    avg_loss:0.045, val_acc:0.970]
Epoch [49/120    avg_loss:0.040, val_acc:0.944]
Epoch [50/120    avg_loss:0.042, val_acc:0.977]
Epoch [51/120    avg_loss:0.018, val_acc:0.973]
Epoch [52/120    avg_loss:0.020, val_acc:0.973]
Epoch [53/120    avg_loss:0.024, val_acc:0.979]
Epoch [54/120    avg_loss:0.034, val_acc:0.943]
Epoch [55/120    avg_loss:0.066, val_acc:0.948]
Epoch [56/120    avg_loss:0.056, val_acc:0.969]
Epoch [57/120    avg_loss:0.034, val_acc:0.974]
Epoch [58/120    avg_loss:0.067, val_acc:0.943]
Epoch [59/120    avg_loss:0.039, val_acc:0.972]
Epoch [60/120    avg_loss:0.028, val_acc:0.974]
Epoch [61/120    avg_loss:0.033, val_acc:0.977]
Epoch [62/120    avg_loss:0.035, val_acc:0.974]
Epoch [63/120    avg_loss:0.068, val_acc:0.968]
Epoch [64/120    avg_loss:0.065, val_acc:0.966]
Epoch [65/120    avg_loss:0.027, val_acc:0.978]
Epoch [66/120    avg_loss:0.034, val_acc:0.972]
Epoch [67/120    avg_loss:0.026, val_acc:0.978]
Epoch [68/120    avg_loss:0.024, val_acc:0.978]
Epoch [69/120    avg_loss:0.018, val_acc:0.977]
Epoch [70/120    avg_loss:0.015, val_acc:0.978]
Epoch [71/120    avg_loss:0.018, val_acc:0.978]
Epoch [72/120    avg_loss:0.018, val_acc:0.978]
Epoch [73/120    avg_loss:0.017, val_acc:0.978]
Epoch [74/120    avg_loss:0.018, val_acc:0.977]
Epoch [75/120    avg_loss:0.015, val_acc:0.980]
Epoch [76/120    avg_loss:0.014, val_acc:0.978]
Epoch [77/120    avg_loss:0.017, val_acc:0.977]
Epoch [78/120    avg_loss:0.016, val_acc:0.981]
Epoch [79/120    avg_loss:0.015, val_acc:0.979]
Epoch [80/120    avg_loss:0.018, val_acc:0.977]
Epoch [81/120    avg_loss:0.017, val_acc:0.979]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.015, val_acc:0.979]
Epoch [84/120    avg_loss:0.013, val_acc:0.982]
Epoch [85/120    avg_loss:0.016, val_acc:0.981]
Epoch [86/120    avg_loss:0.013, val_acc:0.981]
Epoch [87/120    avg_loss:0.013, val_acc:0.982]
Epoch [88/120    avg_loss:0.012, val_acc:0.980]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.013, val_acc:0.981]
Epoch [93/120    avg_loss:0.014, val_acc:0.982]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.979]
Epoch [101/120    avg_loss:0.012, val_acc:0.980]
Epoch [102/120    avg_loss:0.015, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.016, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.977]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.013, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.010, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.017, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     0     0     0     0     0    17     9]
 [    0    18 18001     0    70     0     1     0     0     0]
 [    0     8     0  2022     1     0     0     0     4     1]
 [    0    39    20     0  2881     0     4     0    27     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9    12     0     0  4851     0     0     6]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     4     0    19    50     0     0     0  3490     8]
 [    0     0     0     6    14    45     0     0     0   854]]

Accuracy:
99.05285228833779

F1 scores:
[       nan 0.99263965 0.99673311 0.98754579 0.96225785 0.98305085
 0.99671255 1.         0.98185399 0.94994438]

Kappa:
0.9874577753473948
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b6431f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.002, val_acc:0.099]
Epoch [2/120    avg_loss:1.634, val_acc:0.513]
Epoch [3/120    avg_loss:1.377, val_acc:0.570]
Epoch [4/120    avg_loss:1.164, val_acc:0.601]
Epoch [5/120    avg_loss:0.952, val_acc:0.682]
Epoch [6/120    avg_loss:0.813, val_acc:0.715]
Epoch [7/120    avg_loss:0.682, val_acc:0.732]
Epoch [8/120    avg_loss:0.602, val_acc:0.755]
Epoch [9/120    avg_loss:0.496, val_acc:0.780]
Epoch [10/120    avg_loss:0.432, val_acc:0.802]
Epoch [11/120    avg_loss:0.364, val_acc:0.818]
Epoch [12/120    avg_loss:0.327, val_acc:0.845]
Epoch [13/120    avg_loss:0.303, val_acc:0.842]
Epoch [14/120    avg_loss:0.299, val_acc:0.874]
Epoch [15/120    avg_loss:0.268, val_acc:0.896]
Epoch [16/120    avg_loss:0.252, val_acc:0.864]
Epoch [17/120    avg_loss:0.289, val_acc:0.893]
Epoch [18/120    avg_loss:0.231, val_acc:0.923]
Epoch [19/120    avg_loss:0.170, val_acc:0.910]
Epoch [20/120    avg_loss:0.192, val_acc:0.899]
Epoch [21/120    avg_loss:0.161, val_acc:0.935]
Epoch [22/120    avg_loss:0.194, val_acc:0.945]
Epoch [23/120    avg_loss:0.142, val_acc:0.930]
Epoch [24/120    avg_loss:0.129, val_acc:0.951]
Epoch [25/120    avg_loss:0.091, val_acc:0.948]
Epoch [26/120    avg_loss:0.091, val_acc:0.944]
Epoch [27/120    avg_loss:0.094, val_acc:0.939]
Epoch [28/120    avg_loss:0.120, val_acc:0.898]
Epoch [29/120    avg_loss:0.122, val_acc:0.944]
Epoch [30/120    avg_loss:0.073, val_acc:0.964]
Epoch [31/120    avg_loss:0.062, val_acc:0.964]
Epoch [32/120    avg_loss:0.048, val_acc:0.962]
Epoch [33/120    avg_loss:0.057, val_acc:0.959]
Epoch [34/120    avg_loss:0.042, val_acc:0.973]
Epoch [35/120    avg_loss:0.058, val_acc:0.961]
Epoch [36/120    avg_loss:0.324, val_acc:0.922]
Epoch [37/120    avg_loss:0.130, val_acc:0.923]
Epoch [38/120    avg_loss:0.119, val_acc:0.926]
Epoch [39/120    avg_loss:0.098, val_acc:0.954]
Epoch [40/120    avg_loss:0.082, val_acc:0.962]
Epoch [41/120    avg_loss:0.056, val_acc:0.967]
Epoch [42/120    avg_loss:0.063, val_acc:0.951]
Epoch [43/120    avg_loss:0.060, val_acc:0.965]
Epoch [44/120    avg_loss:0.054, val_acc:0.960]
Epoch [45/120    avg_loss:0.047, val_acc:0.967]
Epoch [46/120    avg_loss:0.046, val_acc:0.977]
Epoch [47/120    avg_loss:0.032, val_acc:0.977]
Epoch [48/120    avg_loss:0.025, val_acc:0.982]
Epoch [49/120    avg_loss:0.022, val_acc:0.984]
Epoch [50/120    avg_loss:0.027, val_acc:0.981]
Epoch [51/120    avg_loss:0.035, val_acc:0.945]
Epoch [52/120    avg_loss:0.030, val_acc:0.975]
Epoch [53/120    avg_loss:0.025, val_acc:0.978]
Epoch [54/120    avg_loss:0.020, val_acc:0.967]
Epoch [55/120    avg_loss:0.028, val_acc:0.984]
Epoch [56/120    avg_loss:0.863, val_acc:0.304]
Epoch [57/120    avg_loss:1.168, val_acc:0.404]
Epoch [58/120    avg_loss:1.047, val_acc:0.592]
Epoch [59/120    avg_loss:0.976, val_acc:0.629]
Epoch [60/120    avg_loss:0.850, val_acc:0.737]
Epoch [61/120    avg_loss:0.722, val_acc:0.670]
Epoch [62/120    avg_loss:0.698, val_acc:0.783]
Epoch [63/120    avg_loss:0.538, val_acc:0.772]
Epoch [64/120    avg_loss:0.544, val_acc:0.776]
Epoch [65/120    avg_loss:0.544, val_acc:0.785]
Epoch [66/120    avg_loss:0.543, val_acc:0.777]
Epoch [67/120    avg_loss:0.517, val_acc:0.798]
Epoch [68/120    avg_loss:0.499, val_acc:0.790]
Epoch [69/120    avg_loss:0.474, val_acc:0.802]
Epoch [70/120    avg_loss:0.489, val_acc:0.803]
Epoch [71/120    avg_loss:0.487, val_acc:0.812]
Epoch [72/120    avg_loss:0.503, val_acc:0.811]
Epoch [73/120    avg_loss:0.493, val_acc:0.803]
Epoch [74/120    avg_loss:0.450, val_acc:0.799]
Epoch [75/120    avg_loss:0.447, val_acc:0.808]
Epoch [76/120    avg_loss:0.460, val_acc:0.815]
Epoch [77/120    avg_loss:0.455, val_acc:0.820]
Epoch [78/120    avg_loss:0.450, val_acc:0.819]
Epoch [79/120    avg_loss:0.452, val_acc:0.816]
Epoch [80/120    avg_loss:0.446, val_acc:0.818]
Epoch [81/120    avg_loss:0.463, val_acc:0.815]
Epoch [82/120    avg_loss:0.451, val_acc:0.818]
Epoch [83/120    avg_loss:0.416, val_acc:0.819]
Epoch [84/120    avg_loss:0.443, val_acc:0.823]
Epoch [85/120    avg_loss:0.436, val_acc:0.819]
Epoch [86/120    avg_loss:0.425, val_acc:0.820]
Epoch [87/120    avg_loss:0.455, val_acc:0.820]
Epoch [88/120    avg_loss:0.423, val_acc:0.820]
Epoch [89/120    avg_loss:0.448, val_acc:0.822]
Epoch [90/120    avg_loss:0.430, val_acc:0.822]
Epoch [91/120    avg_loss:0.417, val_acc:0.821]
Epoch [92/120    avg_loss:0.439, val_acc:0.821]
Epoch [93/120    avg_loss:0.436, val_acc:0.823]
Epoch [94/120    avg_loss:0.432, val_acc:0.824]
Epoch [95/120    avg_loss:0.427, val_acc:0.819]
Epoch [96/120    avg_loss:0.457, val_acc:0.819]
Epoch [97/120    avg_loss:0.439, val_acc:0.819]
Epoch [98/120    avg_loss:0.469, val_acc:0.823]
Epoch [99/120    avg_loss:0.440, val_acc:0.824]
Epoch [100/120    avg_loss:0.451, val_acc:0.824]
Epoch [101/120    avg_loss:0.466, val_acc:0.824]
Epoch [102/120    avg_loss:0.415, val_acc:0.824]
Epoch [103/120    avg_loss:0.418, val_acc:0.824]
Epoch [104/120    avg_loss:0.420, val_acc:0.824]
Epoch [105/120    avg_loss:0.421, val_acc:0.824]
Epoch [106/120    avg_loss:0.447, val_acc:0.824]
Epoch [107/120    avg_loss:0.457, val_acc:0.824]
Epoch [108/120    avg_loss:0.416, val_acc:0.824]
Epoch [109/120    avg_loss:0.453, val_acc:0.824]
Epoch [110/120    avg_loss:0.407, val_acc:0.824]
Epoch [111/120    avg_loss:0.442, val_acc:0.824]
Epoch [112/120    avg_loss:0.455, val_acc:0.824]
Epoch [113/120    avg_loss:0.445, val_acc:0.824]
Epoch [114/120    avg_loss:0.425, val_acc:0.824]
Epoch [115/120    avg_loss:0.402, val_acc:0.824]
Epoch [116/120    avg_loss:0.418, val_acc:0.824]
Epoch [117/120    avg_loss:0.437, val_acc:0.824]
Epoch [118/120    avg_loss:0.460, val_acc:0.824]
Epoch [119/120    avg_loss:0.437, val_acc:0.824]
Epoch [120/120    avg_loss:0.437, val_acc:0.824]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5027     1    60   383     0     5    65   699   192]
 [    0     0 14715     0   733     0  2641     0     1     0]
 [    0    22     0  1838     3     0     0     0    80    93]
 [    0   112    86     0  2620     0    66     0    82     6]
 [    0     0     0     0     0  1301     0     4     0     0]
 [    0     1   227   129    32     0  4447     0    42     0]
 [    0    39     0     0     4     0     4  1214     1    28]
 [    0   131     0     0    70     0     1     0  3367     2]
 [    0    26     0    15    21   128     0     0     2   727]]

Accuracy:
84.96854891186466

F1 scores:
[       nan 0.85275657 0.88861379 0.90142227 0.76630594 0.95171909
 0.73858163 0.94364555 0.85838113 0.73919675]

Kappa:
0.8075887144772643
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ba2fbf978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.030, val_acc:0.444]
Epoch [2/120    avg_loss:1.697, val_acc:0.245]
Epoch [3/120    avg_loss:1.494, val_acc:0.381]
Epoch [4/120    avg_loss:1.305, val_acc:0.407]
Epoch [5/120    avg_loss:1.211, val_acc:0.399]
Epoch [6/120    avg_loss:1.022, val_acc:0.602]
Epoch [7/120    avg_loss:0.857, val_acc:0.606]
Epoch [8/120    avg_loss:0.739, val_acc:0.718]
Epoch [9/120    avg_loss:0.665, val_acc:0.722]
Epoch [10/120    avg_loss:0.571, val_acc:0.769]
Epoch [11/120    avg_loss:0.474, val_acc:0.813]
Epoch [12/120    avg_loss:0.437, val_acc:0.793]
Epoch [13/120    avg_loss:0.408, val_acc:0.832]
Epoch [14/120    avg_loss:0.364, val_acc:0.834]
Epoch [15/120    avg_loss:0.332, val_acc:0.851]
Epoch [16/120    avg_loss:0.350, val_acc:0.831]
Epoch [17/120    avg_loss:0.489, val_acc:0.811]
Epoch [18/120    avg_loss:0.343, val_acc:0.853]
Epoch [19/120    avg_loss:0.306, val_acc:0.832]
Epoch [20/120    avg_loss:0.220, val_acc:0.887]
Epoch [21/120    avg_loss:0.226, val_acc:0.874]
Epoch [22/120    avg_loss:0.247, val_acc:0.922]
Epoch [23/120    avg_loss:0.199, val_acc:0.923]
Epoch [24/120    avg_loss:0.163, val_acc:0.946]
Epoch [25/120    avg_loss:0.146, val_acc:0.947]
Epoch [26/120    avg_loss:0.141, val_acc:0.946]
Epoch [27/120    avg_loss:0.132, val_acc:0.963]
Epoch [28/120    avg_loss:0.138, val_acc:0.878]
Epoch [29/120    avg_loss:0.529, val_acc:0.778]
Epoch [30/120    avg_loss:0.427, val_acc:0.872]
Epoch [31/120    avg_loss:0.284, val_acc:0.917]
Epoch [32/120    avg_loss:0.193, val_acc:0.924]
Epoch [33/120    avg_loss:0.176, val_acc:0.932]
Epoch [34/120    avg_loss:0.176, val_acc:0.906]
Epoch [35/120    avg_loss:0.129, val_acc:0.964]
Epoch [36/120    avg_loss:0.115, val_acc:0.951]
Epoch [37/120    avg_loss:0.123, val_acc:0.949]
Epoch [38/120    avg_loss:0.080, val_acc:0.967]
Epoch [39/120    avg_loss:0.071, val_acc:0.965]
Epoch [40/120    avg_loss:0.073, val_acc:0.967]
Epoch [41/120    avg_loss:0.075, val_acc:0.962]
Epoch [42/120    avg_loss:0.059, val_acc:0.953]
Epoch [43/120    avg_loss:0.064, val_acc:0.970]
Epoch [44/120    avg_loss:0.055, val_acc:0.957]
Epoch [45/120    avg_loss:0.050, val_acc:0.958]
Epoch [46/120    avg_loss:0.045, val_acc:0.975]
Epoch [47/120    avg_loss:0.042, val_acc:0.979]
Epoch [48/120    avg_loss:0.042, val_acc:0.973]
Epoch [49/120    avg_loss:0.059, val_acc:0.958]
Epoch [50/120    avg_loss:0.044, val_acc:0.974]
Epoch [51/120    avg_loss:0.051, val_acc:0.952]
Epoch [52/120    avg_loss:0.053, val_acc:0.974]
Epoch [53/120    avg_loss:0.032, val_acc:0.975]
Epoch [54/120    avg_loss:0.026, val_acc:0.980]
Epoch [55/120    avg_loss:0.044, val_acc:0.976]
Epoch [56/120    avg_loss:0.027, val_acc:0.980]
Epoch [57/120    avg_loss:0.025, val_acc:0.977]
Epoch [58/120    avg_loss:0.025, val_acc:0.973]
Epoch [59/120    avg_loss:0.024, val_acc:0.977]
Epoch [60/120    avg_loss:0.018, val_acc:0.979]
Epoch [61/120    avg_loss:0.018, val_acc:0.972]
Epoch [62/120    avg_loss:0.014, val_acc:0.976]
Epoch [63/120    avg_loss:0.030, val_acc:0.974]
Epoch [64/120    avg_loss:0.023, val_acc:0.977]
Epoch [65/120    avg_loss:0.015, val_acc:0.979]
Epoch [66/120    avg_loss:0.019, val_acc:0.977]
Epoch [67/120    avg_loss:0.014, val_acc:0.978]
Epoch [68/120    avg_loss:0.014, val_acc:0.976]
Epoch [69/120    avg_loss:0.019, val_acc:0.977]
Epoch [70/120    avg_loss:0.015, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.979]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.979]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.983]
Epoch [76/120    avg_loss:0.010, val_acc:0.981]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.009, val_acc:0.983]
Epoch [79/120    avg_loss:0.010, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.984]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.985]
Epoch [83/120    avg_loss:0.010, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.985]
Epoch [95/120    avg_loss:0.009, val_acc:0.985]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     5     0     0     0    12     6]
 [    0     0 18038     0    49     0     3     0     0     0]
 [    0     1     0  2011     2     0     0     0    20     2]
 [    0    38    18     2  2881     0     7     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     8     0     0  4863     0     0     6]
 [    0     1     0     0     0     0     4  1283     0     2]
 [    0    12     0    12    51     0     0     0  3487     9]
 [    0     1     0     1    14    70     0     0     0   833]]

Accuracy:
99.07695273901622

F1 scores:
[       nan 0.99410579 0.9980358  0.98820639 0.96451289 0.9738806
 0.99702717 0.99727944 0.98004497 0.93753517]

Kappa:
0.9877727399718074
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbb3807b9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.087, val_acc:0.201]
Epoch [2/120    avg_loss:1.729, val_acc:0.332]
Epoch [3/120    avg_loss:1.534, val_acc:0.391]
Epoch [4/120    avg_loss:1.320, val_acc:0.431]
Epoch [5/120    avg_loss:1.131, val_acc:0.451]
Epoch [6/120    avg_loss:0.951, val_acc:0.461]
Epoch [7/120    avg_loss:0.832, val_acc:0.681]
Epoch [8/120    avg_loss:0.683, val_acc:0.634]
Epoch [9/120    avg_loss:0.610, val_acc:0.747]
Epoch [10/120    avg_loss:0.482, val_acc:0.756]
Epoch [11/120    avg_loss:0.523, val_acc:0.786]
Epoch [12/120    avg_loss:0.446, val_acc:0.812]
Epoch [13/120    avg_loss:0.364, val_acc:0.820]
Epoch [14/120    avg_loss:0.293, val_acc:0.847]
Epoch [15/120    avg_loss:0.275, val_acc:0.846]
Epoch [16/120    avg_loss:0.338, val_acc:0.740]
Epoch [17/120    avg_loss:0.297, val_acc:0.855]
Epoch [18/120    avg_loss:0.250, val_acc:0.911]
Epoch [19/120    avg_loss:0.222, val_acc:0.898]
Epoch [20/120    avg_loss:0.207, val_acc:0.894]
Epoch [21/120    avg_loss:0.203, val_acc:0.915]
Epoch [22/120    avg_loss:0.185, val_acc:0.881]
Epoch [23/120    avg_loss:0.207, val_acc:0.905]
Epoch [24/120    avg_loss:0.168, val_acc:0.919]
Epoch [25/120    avg_loss:0.157, val_acc:0.906]
Epoch [26/120    avg_loss:0.127, val_acc:0.937]
Epoch [27/120    avg_loss:0.139, val_acc:0.931]
Epoch [28/120    avg_loss:0.166, val_acc:0.880]
Epoch [29/120    avg_loss:0.167, val_acc:0.931]
Epoch [30/120    avg_loss:0.113, val_acc:0.948]
Epoch [31/120    avg_loss:0.123, val_acc:0.944]
Epoch [32/120    avg_loss:0.085, val_acc:0.946]
Epoch [33/120    avg_loss:0.079, val_acc:0.938]
Epoch [34/120    avg_loss:0.125, val_acc:0.935]
Epoch [35/120    avg_loss:0.079, val_acc:0.954]
Epoch [36/120    avg_loss:0.068, val_acc:0.953]
Epoch [37/120    avg_loss:0.073, val_acc:0.938]
Epoch [38/120    avg_loss:0.070, val_acc:0.945]
Epoch [39/120    avg_loss:0.062, val_acc:0.938]
Epoch [40/120    avg_loss:0.053, val_acc:0.956]
Epoch [41/120    avg_loss:0.049, val_acc:0.959]
Epoch [42/120    avg_loss:0.050, val_acc:0.889]
Epoch [43/120    avg_loss:0.048, val_acc:0.955]
Epoch [44/120    avg_loss:0.048, val_acc:0.963]
Epoch [45/120    avg_loss:0.045, val_acc:0.966]
Epoch [46/120    avg_loss:0.046, val_acc:0.965]
Epoch [47/120    avg_loss:0.059, val_acc:0.963]
Epoch [48/120    avg_loss:0.042, val_acc:0.962]
Epoch [49/120    avg_loss:0.036, val_acc:0.969]
Epoch [50/120    avg_loss:0.047, val_acc:0.962]
Epoch [51/120    avg_loss:0.033, val_acc:0.964]
Epoch [52/120    avg_loss:0.030, val_acc:0.973]
Epoch [53/120    avg_loss:0.021, val_acc:0.977]
Epoch [54/120    avg_loss:0.024, val_acc:0.973]
Epoch [55/120    avg_loss:0.020, val_acc:0.979]
Epoch [56/120    avg_loss:0.028, val_acc:0.951]
Epoch [57/120    avg_loss:0.081, val_acc:0.958]
Epoch [58/120    avg_loss:0.051, val_acc:0.965]
Epoch [59/120    avg_loss:0.068, val_acc:0.956]
Epoch [60/120    avg_loss:0.096, val_acc:0.945]
Epoch [61/120    avg_loss:0.041, val_acc:0.972]
Epoch [62/120    avg_loss:0.037, val_acc:0.968]
Epoch [63/120    avg_loss:0.030, val_acc:0.978]
Epoch [64/120    avg_loss:0.043, val_acc:0.973]
Epoch [65/120    avg_loss:0.022, val_acc:0.978]
Epoch [66/120    avg_loss:0.023, val_acc:0.979]
Epoch [67/120    avg_loss:0.022, val_acc:0.976]
Epoch [68/120    avg_loss:0.016, val_acc:0.975]
Epoch [69/120    avg_loss:0.013, val_acc:0.979]
Epoch [70/120    avg_loss:0.011, val_acc:0.981]
Epoch [71/120    avg_loss:0.013, val_acc:0.979]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.011, val_acc:0.970]
Epoch [74/120    avg_loss:0.017, val_acc:0.977]
Epoch [75/120    avg_loss:0.013, val_acc:0.970]
Epoch [76/120    avg_loss:0.017, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.979]
Epoch [78/120    avg_loss:0.014, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.982]
Epoch [80/120    avg_loss:0.013, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.977]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.007, val_acc:0.987]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.007, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.005, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.028, val_acc:0.942]
Epoch [98/120    avg_loss:0.028, val_acc:0.978]
Epoch [99/120    avg_loss:0.017, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.964]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.011, val_acc:0.976]
Epoch [105/120    avg_loss:0.009, val_acc:0.981]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     0     1     0     0    12    29     0]
 [    0     0 17965     0    82     0    42     0     1     0]
 [    0     8     0  2019     1     0     0     0     5     3]
 [    0    35    19     0  2906     0     0     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    16    11     0     0  4842     0     0     9]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    10     0     3    61     0     0     0  3464    33]
 [    0     0     0     2    14    41     0     0     0   862]]

Accuracy:
98.91547971947075

F1 scores:
[       nan 0.99262136 0.99556664 0.99189388 0.96272983 0.98453414
 0.99200983 0.99537037 0.97825473 0.9441402 ]

Kappa:
0.9856450292884925
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faaaca33978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.061, val_acc:0.139]
Epoch [2/120    avg_loss:1.724, val_acc:0.252]
Epoch [3/120    avg_loss:1.481, val_acc:0.405]
Epoch [4/120    avg_loss:1.261, val_acc:0.420]
Epoch [5/120    avg_loss:1.037, val_acc:0.482]
Epoch [6/120    avg_loss:0.876, val_acc:0.506]
Epoch [7/120    avg_loss:0.731, val_acc:0.680]
Epoch [8/120    avg_loss:0.643, val_acc:0.738]
Epoch [9/120    avg_loss:0.525, val_acc:0.791]
Epoch [10/120    avg_loss:0.456, val_acc:0.831]
Epoch [11/120    avg_loss:0.388, val_acc:0.872]
Epoch [12/120    avg_loss:0.465, val_acc:0.797]
Epoch [13/120    avg_loss:0.392, val_acc:0.847]
Epoch [14/120    avg_loss:0.429, val_acc:0.833]
Epoch [15/120    avg_loss:0.290, val_acc:0.907]
Epoch [16/120    avg_loss:0.266, val_acc:0.862]
Epoch [17/120    avg_loss:0.228, val_acc:0.913]
Epoch [18/120    avg_loss:0.201, val_acc:0.918]
Epoch [19/120    avg_loss:0.181, val_acc:0.933]
Epoch [20/120    avg_loss:0.192, val_acc:0.897]
Epoch [21/120    avg_loss:0.195, val_acc:0.955]
Epoch [22/120    avg_loss:0.156, val_acc:0.949]
Epoch [23/120    avg_loss:0.159, val_acc:0.931]
Epoch [24/120    avg_loss:0.150, val_acc:0.944]
Epoch [25/120    avg_loss:0.122, val_acc:0.939]
Epoch [26/120    avg_loss:0.212, val_acc:0.871]
Epoch [27/120    avg_loss:0.311, val_acc:0.921]
Epoch [28/120    avg_loss:0.183, val_acc:0.940]
Epoch [29/120    avg_loss:0.177, val_acc:0.948]
Epoch [30/120    avg_loss:0.153, val_acc:0.924]
Epoch [31/120    avg_loss:0.107, val_acc:0.969]
Epoch [32/120    avg_loss:0.096, val_acc:0.967]
Epoch [33/120    avg_loss:0.073, val_acc:0.970]
Epoch [34/120    avg_loss:0.062, val_acc:0.962]
Epoch [35/120    avg_loss:0.060, val_acc:0.953]
Epoch [36/120    avg_loss:0.059, val_acc:0.966]
Epoch [37/120    avg_loss:0.056, val_acc:0.970]
Epoch [38/120    avg_loss:0.065, val_acc:0.976]
Epoch [39/120    avg_loss:0.058, val_acc:0.977]
Epoch [40/120    avg_loss:0.056, val_acc:0.954]
Epoch [41/120    avg_loss:0.050, val_acc:0.973]
Epoch [42/120    avg_loss:0.047, val_acc:0.977]
Epoch [43/120    avg_loss:0.044, val_acc:0.977]
Epoch [44/120    avg_loss:0.053, val_acc:0.972]
Epoch [45/120    avg_loss:0.047, val_acc:0.972]
Epoch [46/120    avg_loss:0.075, val_acc:0.970]
Epoch [47/120    avg_loss:0.045, val_acc:0.974]
Epoch [48/120    avg_loss:0.046, val_acc:0.971]
Epoch [49/120    avg_loss:0.035, val_acc:0.972]
Epoch [50/120    avg_loss:0.041, val_acc:0.970]
Epoch [51/120    avg_loss:0.029, val_acc:0.981]
Epoch [52/120    avg_loss:0.035, val_acc:0.982]
Epoch [53/120    avg_loss:0.039, val_acc:0.959]
Epoch [54/120    avg_loss:0.033, val_acc:0.973]
Epoch [55/120    avg_loss:0.029, val_acc:0.984]
Epoch [56/120    avg_loss:0.034, val_acc:0.982]
Epoch [57/120    avg_loss:0.019, val_acc:0.977]
Epoch [58/120    avg_loss:0.028, val_acc:0.978]
Epoch [59/120    avg_loss:0.055, val_acc:0.970]
Epoch [60/120    avg_loss:0.057, val_acc:0.969]
Epoch [61/120    avg_loss:0.033, val_acc:0.981]
Epoch [62/120    avg_loss:0.023, val_acc:0.977]
Epoch [63/120    avg_loss:0.015, val_acc:0.982]
Epoch [64/120    avg_loss:0.034, val_acc:0.979]
Epoch [65/120    avg_loss:0.025, val_acc:0.960]
Epoch [66/120    avg_loss:0.025, val_acc:0.978]
Epoch [67/120    avg_loss:0.021, val_acc:0.970]
Epoch [68/120    avg_loss:0.023, val_acc:0.961]
Epoch [69/120    avg_loss:0.050, val_acc:0.985]
Epoch [70/120    avg_loss:0.014, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.986]
Epoch [72/120    avg_loss:0.013, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.013, val_acc:0.986]
Epoch [75/120    avg_loss:0.014, val_acc:0.987]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.987]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.012, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.011, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.987]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0     0     1     0     3     0    72     0]
 [    0     0 18020     0    61     0     6     0     1     2]
 [    0     0     0  1983     5     0     0     0    48     0]
 [    0    33    18     0  2882     0     7     0    29     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     1     0     0  4863     0     3     6]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     9     0     0    61     0     0     0  3501     0]
 [    0     0     0     3    14    49     0     1     0   852]]

Accuracy:
98.92993998987781

F1 scores:
[       nan 0.99080281 0.99742618 0.98583147 0.96130754 0.98157202
 0.99682279 0.99844841 0.96913495 0.95462185]

Kappa:
0.9858286955179754
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdca175d940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.041, val_acc:0.090]
Epoch [2/120    avg_loss:1.686, val_acc:0.293]
Epoch [3/120    avg_loss:1.465, val_acc:0.364]
Epoch [4/120    avg_loss:1.331, val_acc:0.417]
Epoch [5/120    avg_loss:1.066, val_acc:0.473]
Epoch [6/120    avg_loss:0.913, val_acc:0.589]
Epoch [7/120    avg_loss:0.798, val_acc:0.670]
Epoch [8/120    avg_loss:0.641, val_acc:0.722]
Epoch [9/120    avg_loss:0.558, val_acc:0.807]
Epoch [10/120    avg_loss:0.478, val_acc:0.752]
Epoch [11/120    avg_loss:0.415, val_acc:0.761]
Epoch [12/120    avg_loss:0.399, val_acc:0.831]
Epoch [13/120    avg_loss:0.337, val_acc:0.903]
Epoch [14/120    avg_loss:0.289, val_acc:0.859]
Epoch [15/120    avg_loss:0.280, val_acc:0.917]
Epoch [16/120    avg_loss:0.214, val_acc:0.922]
Epoch [17/120    avg_loss:0.180, val_acc:0.911]
Epoch [18/120    avg_loss:0.170, val_acc:0.935]
Epoch [19/120    avg_loss:0.157, val_acc:0.929]
Epoch [20/120    avg_loss:0.241, val_acc:0.915]
Epoch [21/120    avg_loss:0.160, val_acc:0.933]
Epoch [22/120    avg_loss:0.144, val_acc:0.949]
Epoch [23/120    avg_loss:0.145, val_acc:0.924]
Epoch [24/120    avg_loss:0.150, val_acc:0.919]
Epoch [25/120    avg_loss:0.114, val_acc:0.958]
Epoch [26/120    avg_loss:0.093, val_acc:0.951]
Epoch [27/120    avg_loss:0.108, val_acc:0.945]
Epoch [28/120    avg_loss:0.108, val_acc:0.952]
Epoch [29/120    avg_loss:0.072, val_acc:0.961]
Epoch [30/120    avg_loss:0.074, val_acc:0.965]
Epoch [31/120    avg_loss:0.051, val_acc:0.968]
Epoch [32/120    avg_loss:0.047, val_acc:0.971]
Epoch [33/120    avg_loss:0.103, val_acc:0.957]
Epoch [34/120    avg_loss:0.110, val_acc:0.964]
Epoch [35/120    avg_loss:0.073, val_acc:0.970]
Epoch [36/120    avg_loss:0.060, val_acc:0.948]
Epoch [37/120    avg_loss:0.090, val_acc:0.951]
Epoch [38/120    avg_loss:0.055, val_acc:0.956]
Epoch [39/120    avg_loss:0.034, val_acc:0.976]
Epoch [40/120    avg_loss:0.038, val_acc:0.977]
Epoch [41/120    avg_loss:0.040, val_acc:0.980]
Epoch [42/120    avg_loss:0.041, val_acc:0.970]
Epoch [43/120    avg_loss:0.049, val_acc:0.966]
Epoch [44/120    avg_loss:0.039, val_acc:0.970]
Epoch [45/120    avg_loss:0.046, val_acc:0.957]
Epoch [46/120    avg_loss:0.052, val_acc:0.966]
Epoch [47/120    avg_loss:0.044, val_acc:0.956]
Epoch [48/120    avg_loss:0.055, val_acc:0.971]
Epoch [49/120    avg_loss:0.043, val_acc:0.973]
Epoch [50/120    avg_loss:0.025, val_acc:0.983]
Epoch [51/120    avg_loss:0.022, val_acc:0.985]
Epoch [52/120    avg_loss:0.018, val_acc:0.979]
Epoch [53/120    avg_loss:0.022, val_acc:0.977]
Epoch [54/120    avg_loss:0.023, val_acc:0.980]
Epoch [55/120    avg_loss:0.028, val_acc:0.975]
Epoch [56/120    avg_loss:0.020, val_acc:0.981]
Epoch [57/120    avg_loss:0.026, val_acc:0.981]
Epoch [58/120    avg_loss:0.044, val_acc:0.977]
Epoch [59/120    avg_loss:0.025, val_acc:0.976]
Epoch [60/120    avg_loss:0.052, val_acc:0.980]
Epoch [61/120    avg_loss:0.032, val_acc:0.976]
Epoch [62/120    avg_loss:0.022, val_acc:0.984]
Epoch [63/120    avg_loss:0.025, val_acc:0.974]
Epoch [64/120    avg_loss:0.023, val_acc:0.986]
Epoch [65/120    avg_loss:0.020, val_acc:0.986]
Epoch [66/120    avg_loss:0.016, val_acc:0.987]
Epoch [67/120    avg_loss:0.019, val_acc:0.984]
Epoch [68/120    avg_loss:0.024, val_acc:0.968]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.013, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.978]
Epoch [73/120    avg_loss:0.020, val_acc:0.984]
Epoch [74/120    avg_loss:0.014, val_acc:0.977]
Epoch [75/120    avg_loss:0.026, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.985]
Epoch [77/120    avg_loss:0.012, val_acc:0.984]
Epoch [78/120    avg_loss:0.017, val_acc:0.989]
Epoch [79/120    avg_loss:0.008, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.017, val_acc:0.983]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.017, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.981]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.977]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.016, val_acc:0.984]
Epoch [92/120    avg_loss:0.021, val_acc:0.984]
Epoch [93/120    avg_loss:0.010, val_acc:0.986]
Epoch [94/120    avg_loss:0.011, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0     0     0     0     0     0    49     0]
 [    0     0 18032     0    53     0     5     0     0     0]
 [    0     3     0  2008     0     0     0     0    25     0]
 [    0    36    18     0  2883     0     7     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1     5     2     0     0  4870     0     0     0]
 [    0     4     0     0     0     0     0  1285     0     1]
 [    0    18     0    14    37     0     0     0  3502     0]
 [    0     0     0     2    14    64     0     0     0   839]]

Accuracy:
99.06972260381269

F1 scores:
[       nan 0.99137998 0.99775903 0.98867553 0.96761202 0.97606582
 0.99795082 0.99805825 0.97616725 0.95395111]

Kappa:
0.9876768997143675
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8701a099b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.025, val_acc:0.081]
Epoch [2/120    avg_loss:1.641, val_acc:0.531]
Epoch [3/120    avg_loss:1.424, val_acc:0.591]
Epoch [4/120    avg_loss:1.282, val_acc:0.698]
Epoch [5/120    avg_loss:1.110, val_acc:0.696]
Epoch [6/120    avg_loss:1.013, val_acc:0.748]
Epoch [7/120    avg_loss:0.890, val_acc:0.744]
Epoch [8/120    avg_loss:0.809, val_acc:0.771]
Epoch [9/120    avg_loss:0.696, val_acc:0.750]
Epoch [10/120    avg_loss:0.616, val_acc:0.731]
Epoch [11/120    avg_loss:0.519, val_acc:0.819]
Epoch [12/120    avg_loss:0.454, val_acc:0.811]
Epoch [13/120    avg_loss:0.396, val_acc:0.771]
Epoch [14/120    avg_loss:0.362, val_acc:0.803]
Epoch [15/120    avg_loss:0.364, val_acc:0.826]
Epoch [16/120    avg_loss:0.323, val_acc:0.793]
Epoch [17/120    avg_loss:0.325, val_acc:0.808]
Epoch [18/120    avg_loss:0.291, val_acc:0.830]
Epoch [19/120    avg_loss:0.288, val_acc:0.825]
Epoch [20/120    avg_loss:0.235, val_acc:0.871]
Epoch [21/120    avg_loss:0.205, val_acc:0.897]
Epoch [22/120    avg_loss:0.188, val_acc:0.920]
Epoch [23/120    avg_loss:0.195, val_acc:0.924]
Epoch [24/120    avg_loss:0.173, val_acc:0.911]
Epoch [25/120    avg_loss:0.145, val_acc:0.946]
Epoch [26/120    avg_loss:0.124, val_acc:0.948]
Epoch [27/120    avg_loss:0.141, val_acc:0.956]
Epoch [28/120    avg_loss:0.104, val_acc:0.961]
Epoch [29/120    avg_loss:0.109, val_acc:0.970]
Epoch [30/120    avg_loss:0.126, val_acc:0.947]
Epoch [31/120    avg_loss:0.116, val_acc:0.941]
Epoch [32/120    avg_loss:0.108, val_acc:0.961]
Epoch [33/120    avg_loss:0.096, val_acc:0.968]
Epoch [34/120    avg_loss:0.069, val_acc:0.975]
Epoch [35/120    avg_loss:0.069, val_acc:0.967]
Epoch [36/120    avg_loss:0.058, val_acc:0.954]
Epoch [37/120    avg_loss:0.083, val_acc:0.961]
Epoch [38/120    avg_loss:0.064, val_acc:0.972]
Epoch [39/120    avg_loss:0.049, val_acc:0.961]
Epoch [40/120    avg_loss:0.061, val_acc:0.977]
Epoch [41/120    avg_loss:0.309, val_acc:0.729]
Epoch [42/120    avg_loss:0.531, val_acc:0.840]
Epoch [43/120    avg_loss:0.357, val_acc:0.812]
Epoch [44/120    avg_loss:0.259, val_acc:0.919]
Epoch [45/120    avg_loss:0.202, val_acc:0.938]
Epoch [46/120    avg_loss:0.176, val_acc:0.909]
Epoch [47/120    avg_loss:0.133, val_acc:0.952]
Epoch [48/120    avg_loss:0.118, val_acc:0.908]
Epoch [49/120    avg_loss:0.110, val_acc:0.961]
Epoch [50/120    avg_loss:0.104, val_acc:0.932]
Epoch [51/120    avg_loss:0.120, val_acc:0.940]
Epoch [52/120    avg_loss:0.093, val_acc:0.954]
Epoch [53/120    avg_loss:0.078, val_acc:0.963]
Epoch [54/120    avg_loss:0.058, val_acc:0.962]
Epoch [55/120    avg_loss:0.044, val_acc:0.965]
Epoch [56/120    avg_loss:0.051, val_acc:0.951]
Epoch [57/120    avg_loss:0.044, val_acc:0.961]
Epoch [58/120    avg_loss:0.044, val_acc:0.967]
Epoch [59/120    avg_loss:0.045, val_acc:0.965]
Epoch [60/120    avg_loss:0.042, val_acc:0.970]
Epoch [61/120    avg_loss:0.038, val_acc:0.969]
Epoch [62/120    avg_loss:0.045, val_acc:0.971]
Epoch [63/120    avg_loss:0.038, val_acc:0.971]
Epoch [64/120    avg_loss:0.041, val_acc:0.972]
Epoch [65/120    avg_loss:0.039, val_acc:0.970]
Epoch [66/120    avg_loss:0.036, val_acc:0.971]
Epoch [67/120    avg_loss:0.041, val_acc:0.971]
Epoch [68/120    avg_loss:0.037, val_acc:0.971]
Epoch [69/120    avg_loss:0.038, val_acc:0.971]
Epoch [70/120    avg_loss:0.034, val_acc:0.971]
Epoch [71/120    avg_loss:0.032, val_acc:0.972]
Epoch [72/120    avg_loss:0.036, val_acc:0.971]
Epoch [73/120    avg_loss:0.040, val_acc:0.971]
Epoch [74/120    avg_loss:0.038, val_acc:0.973]
Epoch [75/120    avg_loss:0.041, val_acc:0.972]
Epoch [76/120    avg_loss:0.035, val_acc:0.973]
Epoch [77/120    avg_loss:0.038, val_acc:0.973]
Epoch [78/120    avg_loss:0.038, val_acc:0.973]
Epoch [79/120    avg_loss:0.038, val_acc:0.973]
Epoch [80/120    avg_loss:0.033, val_acc:0.973]
Epoch [81/120    avg_loss:0.037, val_acc:0.973]
Epoch [82/120    avg_loss:0.038, val_acc:0.973]
Epoch [83/120    avg_loss:0.035, val_acc:0.973]
Epoch [84/120    avg_loss:0.038, val_acc:0.973]
Epoch [85/120    avg_loss:0.042, val_acc:0.973]
Epoch [86/120    avg_loss:0.036, val_acc:0.973]
Epoch [87/120    avg_loss:0.039, val_acc:0.973]
Epoch [88/120    avg_loss:0.038, val_acc:0.973]
Epoch [89/120    avg_loss:0.037, val_acc:0.973]
Epoch [90/120    avg_loss:0.035, val_acc:0.973]
Epoch [91/120    avg_loss:0.037, val_acc:0.973]
Epoch [92/120    avg_loss:0.033, val_acc:0.973]
Epoch [93/120    avg_loss:0.034, val_acc:0.973]
Epoch [94/120    avg_loss:0.035, val_acc:0.973]
Epoch [95/120    avg_loss:0.032, val_acc:0.973]
Epoch [96/120    avg_loss:0.037, val_acc:0.973]
Epoch [97/120    avg_loss:0.036, val_acc:0.973]
Epoch [98/120    avg_loss:0.042, val_acc:0.973]
Epoch [99/120    avg_loss:0.038, val_acc:0.973]
Epoch [100/120    avg_loss:0.036, val_acc:0.973]
Epoch [101/120    avg_loss:0.034, val_acc:0.973]
Epoch [102/120    avg_loss:0.034, val_acc:0.973]
Epoch [103/120    avg_loss:0.037, val_acc:0.973]
Epoch [104/120    avg_loss:0.039, val_acc:0.973]
Epoch [105/120    avg_loss:0.036, val_acc:0.973]
Epoch [106/120    avg_loss:0.033, val_acc:0.973]
Epoch [107/120    avg_loss:0.039, val_acc:0.973]
Epoch [108/120    avg_loss:0.032, val_acc:0.973]
Epoch [109/120    avg_loss:0.036, val_acc:0.973]
Epoch [110/120    avg_loss:0.036, val_acc:0.973]
Epoch [111/120    avg_loss:0.037, val_acc:0.973]
Epoch [112/120    avg_loss:0.041, val_acc:0.973]
Epoch [113/120    avg_loss:0.038, val_acc:0.973]
Epoch [114/120    avg_loss:0.038, val_acc:0.973]
Epoch [115/120    avg_loss:0.037, val_acc:0.973]
Epoch [116/120    avg_loss:0.038, val_acc:0.973]
Epoch [117/120    avg_loss:0.040, val_acc:0.973]
Epoch [118/120    avg_loss:0.034, val_acc:0.973]
Epoch [119/120    avg_loss:0.038, val_acc:0.973]
Epoch [120/120    avg_loss:0.036, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6219     0     0     3     0     5    39   107    59]
 [    0     1 18024     0    13     0    46     0     6     0]
 [    0     4     0  1840     0     0     0     0   189     3]
 [    0    10    24     0  2916     0    17     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    63     0     0     0  4815     0     0     0]
 [    0    21     0     0     0     0     2  1264     3     0]
 [    0    82     3    59    36     0    19     0  3372     0]
 [    0     1     0     0    14     5     0     0     0   899]]

Accuracy:
97.97797218807992

F1 scores:
[       nan 0.97400157 0.99569108 0.93519695 0.97950957 0.99808795
 0.98446126 0.97493251 0.9302069  0.95485927]

Kappa:
0.9732036621975021
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb722198908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.002, val_acc:0.144]
Epoch [2/120    avg_loss:1.611, val_acc:0.574]
Epoch [3/120    avg_loss:1.361, val_acc:0.628]
Epoch [4/120    avg_loss:1.179, val_acc:0.665]
Epoch [5/120    avg_loss:0.971, val_acc:0.750]
Epoch [6/120    avg_loss:0.842, val_acc:0.757]
Epoch [7/120    avg_loss:0.695, val_acc:0.692]
Epoch [8/120    avg_loss:0.587, val_acc:0.689]
Epoch [9/120    avg_loss:0.575, val_acc:0.816]
Epoch [10/120    avg_loss:0.498, val_acc:0.715]
Epoch [11/120    avg_loss:0.463, val_acc:0.743]
Epoch [12/120    avg_loss:0.436, val_acc:0.785]
Epoch [13/120    avg_loss:0.391, val_acc:0.805]
Epoch [14/120    avg_loss:0.328, val_acc:0.809]
Epoch [15/120    avg_loss:0.318, val_acc:0.849]
Epoch [16/120    avg_loss:0.275, val_acc:0.836]
Epoch [17/120    avg_loss:0.268, val_acc:0.877]
Epoch [18/120    avg_loss:0.227, val_acc:0.862]
Epoch [19/120    avg_loss:0.217, val_acc:0.919]
Epoch [20/120    avg_loss:0.199, val_acc:0.933]
Epoch [21/120    avg_loss:0.154, val_acc:0.913]
Epoch [22/120    avg_loss:0.147, val_acc:0.873]
Epoch [23/120    avg_loss:0.146, val_acc:0.944]
Epoch [24/120    avg_loss:0.138, val_acc:0.932]
Epoch [25/120    avg_loss:0.148, val_acc:0.943]
Epoch [26/120    avg_loss:0.113, val_acc:0.947]
Epoch [27/120    avg_loss:0.110, val_acc:0.956]
Epoch [28/120    avg_loss:0.135, val_acc:0.945]
Epoch [29/120    avg_loss:0.126, val_acc:0.961]
Epoch [30/120    avg_loss:0.097, val_acc:0.961]
Epoch [31/120    avg_loss:0.096, val_acc:0.897]
Epoch [32/120    avg_loss:0.105, val_acc:0.952]
Epoch [33/120    avg_loss:0.081, val_acc:0.962]
Epoch [34/120    avg_loss:0.080, val_acc:0.945]
Epoch [35/120    avg_loss:0.089, val_acc:0.942]
Epoch [36/120    avg_loss:0.071, val_acc:0.957]
Epoch [37/120    avg_loss:0.088, val_acc:0.942]
Epoch [38/120    avg_loss:0.079, val_acc:0.970]
Epoch [39/120    avg_loss:0.056, val_acc:0.965]
Epoch [40/120    avg_loss:0.066, val_acc:0.965]
Epoch [41/120    avg_loss:0.072, val_acc:0.964]
Epoch [42/120    avg_loss:0.053, val_acc:0.953]
Epoch [43/120    avg_loss:0.085, val_acc:0.955]
Epoch [44/120    avg_loss:0.064, val_acc:0.917]
Epoch [45/120    avg_loss:0.072, val_acc:0.948]
Epoch [46/120    avg_loss:0.053, val_acc:0.970]
Epoch [47/120    avg_loss:0.046, val_acc:0.957]
Epoch [48/120    avg_loss:0.045, val_acc:0.975]
Epoch [49/120    avg_loss:0.067, val_acc:0.965]
Epoch [50/120    avg_loss:0.060, val_acc:0.938]
Epoch [51/120    avg_loss:0.058, val_acc:0.948]
Epoch [52/120    avg_loss:0.046, val_acc:0.964]
Epoch [53/120    avg_loss:0.045, val_acc:0.967]
Epoch [54/120    avg_loss:0.035, val_acc:0.965]
Epoch [55/120    avg_loss:0.034, val_acc:0.971]
Epoch [56/120    avg_loss:0.038, val_acc:0.965]
Epoch [57/120    avg_loss:0.031, val_acc:0.965]
Epoch [58/120    avg_loss:0.027, val_acc:0.969]
Epoch [59/120    avg_loss:0.037, val_acc:0.969]
Epoch [60/120    avg_loss:0.066, val_acc:0.960]
Epoch [61/120    avg_loss:0.050, val_acc:0.972]
Epoch [62/120    avg_loss:0.027, val_acc:0.973]
Epoch [63/120    avg_loss:0.023, val_acc:0.970]
Epoch [64/120    avg_loss:0.018, val_acc:0.974]
Epoch [65/120    avg_loss:0.021, val_acc:0.975]
Epoch [66/120    avg_loss:0.014, val_acc:0.974]
Epoch [67/120    avg_loss:0.016, val_acc:0.975]
Epoch [68/120    avg_loss:0.018, val_acc:0.974]
Epoch [69/120    avg_loss:0.019, val_acc:0.978]
Epoch [70/120    avg_loss:0.014, val_acc:0.978]
Epoch [71/120    avg_loss:0.014, val_acc:0.976]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.015, val_acc:0.975]
Epoch [74/120    avg_loss:0.016, val_acc:0.975]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.015, val_acc:0.977]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.015, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.975]
Epoch [82/120    avg_loss:0.014, val_acc:0.976]
Epoch [83/120    avg_loss:0.015, val_acc:0.976]
Epoch [84/120    avg_loss:0.012, val_acc:0.975]
Epoch [85/120    avg_loss:0.013, val_acc:0.976]
Epoch [86/120    avg_loss:0.013, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.975]
Epoch [88/120    avg_loss:0.014, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.975]
Epoch [90/120    avg_loss:0.010, val_acc:0.975]
Epoch [91/120    avg_loss:0.016, val_acc:0.976]
Epoch [92/120    avg_loss:0.015, val_acc:0.977]
Epoch [93/120    avg_loss:0.011, val_acc:0.976]
Epoch [94/120    avg_loss:0.011, val_acc:0.976]
Epoch [95/120    avg_loss:0.011, val_acc:0.975]
Epoch [96/120    avg_loss:0.012, val_acc:0.975]
Epoch [97/120    avg_loss:0.013, val_acc:0.976]
Epoch [98/120    avg_loss:0.010, val_acc:0.975]
Epoch [99/120    avg_loss:0.011, val_acc:0.977]
Epoch [100/120    avg_loss:0.010, val_acc:0.980]
Epoch [101/120    avg_loss:0.018, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.011, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.013, val_acc:0.979]
Epoch [108/120    avg_loss:0.012, val_acc:0.979]
Epoch [109/120    avg_loss:0.010, val_acc:0.975]
Epoch [110/120    avg_loss:0.010, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.979]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.015, val_acc:0.975]
Epoch [114/120    avg_loss:0.010, val_acc:0.975]
Epoch [115/120    avg_loss:0.009, val_acc:0.975]
Epoch [116/120    avg_loss:0.012, val_acc:0.975]
Epoch [117/120    avg_loss:0.012, val_acc:0.975]
Epoch [118/120    avg_loss:0.011, val_acc:0.975]
Epoch [119/120    avg_loss:0.012, val_acc:0.975]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6331     0     0     0     0     0    10    91     0]
 [    0     0 18003     0    61     0    19     0     7     0]
 [    0     6     0  1896     0     0     0     0   134     0]
 [    0     9    17     0  2922     0    13     0     9     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    49     0     0     0  4829     0     0     0]
 [    0     3     0     0     0     0     0  1287     0     0]
 [    0     7     3    62    34     0    12     1  3452     0]
 [    0     2     0     0     3    18     0     0     0   896]]

Accuracy:
98.62145422119394

F1 scores:
[       nan 0.98999218 0.99568608 0.94942414 0.9753004  0.99315068
 0.99046252 0.99459042 0.95044053 0.98624106]

Kappa:
0.981738944446843
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe8280a6f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.023, val_acc:0.527]
Epoch [2/120    avg_loss:1.634, val_acc:0.550]
Epoch [3/120    avg_loss:1.378, val_acc:0.628]
Epoch [4/120    avg_loss:1.150, val_acc:0.664]
Epoch [5/120    avg_loss:0.989, val_acc:0.721]
Epoch [6/120    avg_loss:0.833, val_acc:0.766]
Epoch [7/120    avg_loss:0.714, val_acc:0.796]
Epoch [8/120    avg_loss:0.613, val_acc:0.812]
Epoch [9/120    avg_loss:0.564, val_acc:0.815]
Epoch [10/120    avg_loss:0.489, val_acc:0.830]
Epoch [11/120    avg_loss:0.449, val_acc:0.831]
Epoch [12/120    avg_loss:0.405, val_acc:0.775]
Epoch [13/120    avg_loss:0.352, val_acc:0.809]
Epoch [14/120    avg_loss:0.323, val_acc:0.806]
Epoch [15/120    avg_loss:0.307, val_acc:0.845]
Epoch [16/120    avg_loss:0.282, val_acc:0.834]
Epoch [17/120    avg_loss:0.281, val_acc:0.835]
Epoch [18/120    avg_loss:0.243, val_acc:0.894]
Epoch [19/120    avg_loss:0.202, val_acc:0.910]
Epoch [20/120    avg_loss:0.188, val_acc:0.927]
Epoch [21/120    avg_loss:0.199, val_acc:0.950]
Epoch [22/120    avg_loss:0.165, val_acc:0.941]
Epoch [23/120    avg_loss:0.195, val_acc:0.888]
Epoch [24/120    avg_loss:0.159, val_acc:0.942]
Epoch [25/120    avg_loss:0.152, val_acc:0.950]
Epoch [26/120    avg_loss:0.117, val_acc:0.953]
Epoch [27/120    avg_loss:0.115, val_acc:0.961]
Epoch [28/120    avg_loss:0.083, val_acc:0.934]
Epoch [29/120    avg_loss:0.087, val_acc:0.946]
Epoch [30/120    avg_loss:0.085, val_acc:0.952]
Epoch [31/120    avg_loss:0.078, val_acc:0.965]
Epoch [32/120    avg_loss:0.081, val_acc:0.970]
Epoch [33/120    avg_loss:0.061, val_acc:0.979]
Epoch [34/120    avg_loss:0.065, val_acc:0.934]
Epoch [35/120    avg_loss:0.085, val_acc:0.938]
Epoch [36/120    avg_loss:0.080, val_acc:0.964]
Epoch [37/120    avg_loss:0.065, val_acc:0.969]
Epoch [38/120    avg_loss:0.053, val_acc:0.949]
Epoch [39/120    avg_loss:0.068, val_acc:0.976]
Epoch [40/120    avg_loss:0.044, val_acc:0.975]
Epoch [41/120    avg_loss:0.067, val_acc:0.976]
Epoch [42/120    avg_loss:0.043, val_acc:0.976]
Epoch [43/120    avg_loss:0.036, val_acc:0.979]
Epoch [44/120    avg_loss:0.034, val_acc:0.978]
Epoch [45/120    avg_loss:0.026, val_acc:0.982]
Epoch [46/120    avg_loss:0.026, val_acc:0.977]
Epoch [47/120    avg_loss:0.029, val_acc:0.967]
Epoch [48/120    avg_loss:0.033, val_acc:0.973]
Epoch [49/120    avg_loss:0.023, val_acc:0.982]
Epoch [50/120    avg_loss:0.029, val_acc:0.970]
Epoch [51/120    avg_loss:0.031, val_acc:0.975]
Epoch [52/120    avg_loss:0.036, val_acc:0.980]
Epoch [53/120    avg_loss:0.041, val_acc:0.974]
Epoch [54/120    avg_loss:0.041, val_acc:0.983]
Epoch [55/120    avg_loss:0.028, val_acc:0.970]
Epoch [56/120    avg_loss:0.045, val_acc:0.961]
Epoch [57/120    avg_loss:0.040, val_acc:0.984]
Epoch [58/120    avg_loss:0.022, val_acc:0.983]
Epoch [59/120    avg_loss:0.023, val_acc:0.980]
Epoch [60/120    avg_loss:0.022, val_acc:0.973]
Epoch [61/120    avg_loss:0.029, val_acc:0.977]
Epoch [62/120    avg_loss:0.020, val_acc:0.984]
Epoch [63/120    avg_loss:0.013, val_acc:0.983]
Epoch [64/120    avg_loss:0.017, val_acc:0.982]
Epoch [65/120    avg_loss:0.016, val_acc:0.989]
Epoch [66/120    avg_loss:0.021, val_acc:0.970]
Epoch [67/120    avg_loss:0.017, val_acc:0.984]
Epoch [68/120    avg_loss:0.019, val_acc:0.971]
Epoch [69/120    avg_loss:0.015, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.009, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.013, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.979]
Epoch [76/120    avg_loss:0.014, val_acc:0.981]
Epoch [77/120    avg_loss:0.021, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.008, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6295     0     7     1     0     2    60    59     8]
 [    0     0 18004     0    16     0    65     0     5     0]
 [    0    10     0  1931     0     0     0     0    94     1]
 [    0    14    18     0  2932     0     5     0     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    31     0     0     0  4847     0     0     0]
 [    0    19     0     0     0     0     0  1271     0     0]
 [    0    12     0    42    50     0     3     0  3464     0]
 [    0     1     0     0     0     7     0     0     0   911]]

Accuracy:
98.71544597883981

F1 scores:
[       nan 0.98490182 0.99626484 0.96165339 0.98208005 0.99732518
 0.98918367 0.96985883 0.9628909  0.99021739]

Kappa:
0.9829887855959014
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9c8635a908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.101, val_acc:0.132]
Epoch [2/120    avg_loss:1.674, val_acc:0.266]
Epoch [3/120    avg_loss:1.384, val_acc:0.498]
Epoch [4/120    avg_loss:1.184, val_acc:0.676]
Epoch [5/120    avg_loss:1.008, val_acc:0.766]
Epoch [6/120    avg_loss:0.873, val_acc:0.762]
Epoch [7/120    avg_loss:0.780, val_acc:0.747]
Epoch [8/120    avg_loss:0.670, val_acc:0.774]
Epoch [9/120    avg_loss:0.617, val_acc:0.775]
Epoch [10/120    avg_loss:0.531, val_acc:0.802]
Epoch [11/120    avg_loss:0.452, val_acc:0.808]
Epoch [12/120    avg_loss:0.392, val_acc:0.819]
Epoch [13/120    avg_loss:0.331, val_acc:0.839]
Epoch [14/120    avg_loss:0.321, val_acc:0.824]
Epoch [15/120    avg_loss:0.277, val_acc:0.873]
Epoch [16/120    avg_loss:0.287, val_acc:0.818]
Epoch [17/120    avg_loss:0.249, val_acc:0.924]
Epoch [18/120    avg_loss:0.196, val_acc:0.911]
Epoch [19/120    avg_loss:0.244, val_acc:0.886]
Epoch [20/120    avg_loss:0.209, val_acc:0.903]
Epoch [21/120    avg_loss:0.178, val_acc:0.938]
Epoch [22/120    avg_loss:0.149, val_acc:0.928]
Epoch [23/120    avg_loss:0.155, val_acc:0.904]
Epoch [24/120    avg_loss:0.127, val_acc:0.884]
Epoch [25/120    avg_loss:0.146, val_acc:0.928]
Epoch [26/120    avg_loss:0.128, val_acc:0.930]
Epoch [27/120    avg_loss:0.119, val_acc:0.959]
Epoch [28/120    avg_loss:0.100, val_acc:0.956]
Epoch [29/120    avg_loss:0.109, val_acc:0.957]
Epoch [30/120    avg_loss:0.104, val_acc:0.963]
Epoch [31/120    avg_loss:0.082, val_acc:0.956]
Epoch [32/120    avg_loss:0.118, val_acc:0.945]
Epoch [33/120    avg_loss:0.113, val_acc:0.965]
Epoch [34/120    avg_loss:0.079, val_acc:0.949]
Epoch [35/120    avg_loss:0.092, val_acc:0.953]
Epoch [36/120    avg_loss:0.072, val_acc:0.961]
Epoch [37/120    avg_loss:0.065, val_acc:0.968]
Epoch [38/120    avg_loss:0.079, val_acc:0.967]
Epoch [39/120    avg_loss:0.050, val_acc:0.924]
Epoch [40/120    avg_loss:0.074, val_acc:0.938]
Epoch [41/120    avg_loss:0.070, val_acc:0.974]
Epoch [42/120    avg_loss:0.067, val_acc:0.942]
Epoch [43/120    avg_loss:0.066, val_acc:0.971]
Epoch [44/120    avg_loss:0.051, val_acc:0.948]
Epoch [45/120    avg_loss:0.047, val_acc:0.966]
Epoch [46/120    avg_loss:0.041, val_acc:0.962]
Epoch [47/120    avg_loss:0.034, val_acc:0.961]
Epoch [48/120    avg_loss:0.038, val_acc:0.942]
Epoch [49/120    avg_loss:0.076, val_acc:0.938]
Epoch [50/120    avg_loss:0.108, val_acc:0.963]
Epoch [51/120    avg_loss:0.058, val_acc:0.971]
Epoch [52/120    avg_loss:0.049, val_acc:0.968]
Epoch [53/120    avg_loss:0.032, val_acc:0.965]
Epoch [54/120    avg_loss:0.035, val_acc:0.967]
Epoch [55/120    avg_loss:0.024, val_acc:0.978]
Epoch [56/120    avg_loss:0.022, val_acc:0.978]
Epoch [57/120    avg_loss:0.023, val_acc:0.976]
Epoch [58/120    avg_loss:0.020, val_acc:0.979]
Epoch [59/120    avg_loss:0.021, val_acc:0.978]
Epoch [60/120    avg_loss:0.017, val_acc:0.978]
Epoch [61/120    avg_loss:0.018, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.980]
Epoch [63/120    avg_loss:0.019, val_acc:0.981]
Epoch [64/120    avg_loss:0.016, val_acc:0.977]
Epoch [65/120    avg_loss:0.020, val_acc:0.980]
Epoch [66/120    avg_loss:0.017, val_acc:0.982]
Epoch [67/120    avg_loss:0.022, val_acc:0.979]
Epoch [68/120    avg_loss:0.016, val_acc:0.981]
Epoch [69/120    avg_loss:0.017, val_acc:0.980]
Epoch [70/120    avg_loss:0.019, val_acc:0.982]
Epoch [71/120    avg_loss:0.016, val_acc:0.980]
Epoch [72/120    avg_loss:0.021, val_acc:0.984]
Epoch [73/120    avg_loss:0.018, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.980]
Epoch [75/120    avg_loss:0.012, val_acc:0.979]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.017, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.982]
Epoch [83/120    avg_loss:0.016, val_acc:0.975]
Epoch [84/120    avg_loss:0.014, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.981]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.981]
Epoch [88/120    avg_loss:0.015, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.981]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.012, val_acc:0.981]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.013, val_acc:0.981]
Epoch [96/120    avg_loss:0.014, val_acc:0.981]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.015, val_acc:0.981]
Epoch [106/120    avg_loss:0.015, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.014, val_acc:0.980]
Epoch [110/120    avg_loss:0.014, val_acc:0.980]
Epoch [111/120    avg_loss:0.012, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.980]
Epoch [114/120    avg_loss:0.015, val_acc:0.980]
Epoch [115/120    avg_loss:0.013, val_acc:0.980]
Epoch [116/120    avg_loss:0.013, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.980]
Epoch [119/120    avg_loss:0.014, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6317     0     3     0     0     0    26    79     7]
 [    0     1 18021     0    31     0    30     0     7     0]
 [    0     4     0  1909     0     0     0     0   122     1]
 [    0    27     3     0  2918     0    14     3     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    25     0     0     0  4852     0     1     0]
 [    0    11     0     0     0     0     1  1274     3     1]
 [    0     3     3    50    43     0     0     0  3471     1]
 [    0     0     0     0     4    19     0     0     0   896]]

Accuracy:
98.72267611404334

F1 scores:
[       nan 0.98741696 0.99723314 0.95497749 0.97788204 0.99277292
 0.99273657 0.98264558 0.95633007 0.98084291]

Kappa:
0.9830833921392002
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe0f78d5940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.091, val_acc:0.084]
Epoch [2/120    avg_loss:1.707, val_acc:0.237]
Epoch [3/120    avg_loss:1.460, val_acc:0.321]
Epoch [4/120    avg_loss:1.213, val_acc:0.521]
Epoch [5/120    avg_loss:1.039, val_acc:0.693]
Epoch [6/120    avg_loss:0.856, val_acc:0.608]
Epoch [7/120    avg_loss:0.726, val_acc:0.614]
Epoch [8/120    avg_loss:0.662, val_acc:0.672]
Epoch [9/120    avg_loss:0.556, val_acc:0.678]
Epoch [10/120    avg_loss:0.501, val_acc:0.692]
Epoch [11/120    avg_loss:0.446, val_acc:0.706]
Epoch [12/120    avg_loss:0.410, val_acc:0.697]
Epoch [13/120    avg_loss:0.369, val_acc:0.741]
Epoch [14/120    avg_loss:0.332, val_acc:0.776]
Epoch [15/120    avg_loss:0.318, val_acc:0.728]
Epoch [16/120    avg_loss:0.303, val_acc:0.812]
Epoch [17/120    avg_loss:0.289, val_acc:0.780]
Epoch [18/120    avg_loss:0.289, val_acc:0.775]
Epoch [19/120    avg_loss:0.247, val_acc:0.896]
Epoch [20/120    avg_loss:0.218, val_acc:0.909]
Epoch [21/120    avg_loss:0.213, val_acc:0.899]
Epoch [22/120    avg_loss:0.195, val_acc:0.910]
Epoch [23/120    avg_loss:0.176, val_acc:0.931]
Epoch [24/120    avg_loss:0.184, val_acc:0.930]
Epoch [25/120    avg_loss:0.277, val_acc:0.850]
Epoch [26/120    avg_loss:0.199, val_acc:0.831]
Epoch [27/120    avg_loss:0.175, val_acc:0.933]
Epoch [28/120    avg_loss:0.193, val_acc:0.932]
Epoch [29/120    avg_loss:0.148, val_acc:0.928]
Epoch [30/120    avg_loss:0.139, val_acc:0.866]
Epoch [31/120    avg_loss:0.143, val_acc:0.955]
Epoch [32/120    avg_loss:0.109, val_acc:0.935]
Epoch [33/120    avg_loss:0.138, val_acc:0.960]
Epoch [34/120    avg_loss:0.097, val_acc:0.895]
Epoch [35/120    avg_loss:0.123, val_acc:0.957]
Epoch [36/120    avg_loss:0.111, val_acc:0.937]
Epoch [37/120    avg_loss:0.095, val_acc:0.945]
Epoch [38/120    avg_loss:0.089, val_acc:0.968]
Epoch [39/120    avg_loss:0.086, val_acc:0.938]
Epoch [40/120    avg_loss:0.081, val_acc:0.958]
Epoch [41/120    avg_loss:0.080, val_acc:0.932]
Epoch [42/120    avg_loss:0.068, val_acc:0.956]
Epoch [43/120    avg_loss:0.082, val_acc:0.948]
Epoch [44/120    avg_loss:0.079, val_acc:0.951]
Epoch [45/120    avg_loss:0.050, val_acc:0.975]
Epoch [46/120    avg_loss:0.053, val_acc:0.972]
Epoch [47/120    avg_loss:0.049, val_acc:0.973]
Epoch [48/120    avg_loss:0.055, val_acc:0.975]
Epoch [49/120    avg_loss:0.050, val_acc:0.975]
Epoch [50/120    avg_loss:0.042, val_acc:0.974]
Epoch [51/120    avg_loss:0.041, val_acc:0.968]
Epoch [52/120    avg_loss:0.044, val_acc:0.960]
Epoch [53/120    avg_loss:0.040, val_acc:0.975]
Epoch [54/120    avg_loss:0.045, val_acc:0.961]
Epoch [55/120    avg_loss:0.051, val_acc:0.975]
Epoch [56/120    avg_loss:0.037, val_acc:0.966]
Epoch [57/120    avg_loss:0.035, val_acc:0.975]
Epoch [58/120    avg_loss:0.031, val_acc:0.980]
Epoch [59/120    avg_loss:0.032, val_acc:0.973]
Epoch [60/120    avg_loss:0.055, val_acc:0.955]
Epoch [61/120    avg_loss:0.038, val_acc:0.970]
Epoch [62/120    avg_loss:0.046, val_acc:0.975]
Epoch [63/120    avg_loss:0.026, val_acc:0.971]
Epoch [64/120    avg_loss:0.022, val_acc:0.975]
Epoch [65/120    avg_loss:0.050, val_acc:0.951]
Epoch [66/120    avg_loss:0.046, val_acc:0.969]
Epoch [67/120    avg_loss:0.025, val_acc:0.979]
Epoch [68/120    avg_loss:0.028, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.975]
Epoch [71/120    avg_loss:0.017, val_acc:0.984]
Epoch [72/120    avg_loss:0.019, val_acc:0.986]
Epoch [73/120    avg_loss:0.018, val_acc:0.984]
Epoch [74/120    avg_loss:0.022, val_acc:0.978]
Epoch [75/120    avg_loss:0.017, val_acc:0.970]
Epoch [76/120    avg_loss:0.028, val_acc:0.969]
Epoch [77/120    avg_loss:0.012, val_acc:0.981]
Epoch [78/120    avg_loss:0.014, val_acc:0.984]
Epoch [79/120    avg_loss:0.022, val_acc:0.975]
Epoch [80/120    avg_loss:0.027, val_acc:0.983]
Epoch [81/120    avg_loss:0.020, val_acc:0.984]
Epoch [82/120    avg_loss:0.016, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.023, val_acc:0.979]
Epoch [85/120    avg_loss:0.025, val_acc:0.980]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0     6     1     0     0     3    48     3]
 [    0     0 17936     0    65     0    88     0     1     0]
 [    0     0     0  1915     0     0     0     0   120     1]
 [    0    29    16     0  2895     0     8     0    18     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0    10     0  4860     0     1     0]
 [    0     6     0     0     0     0     0  1283     0     1]
 [    0     8     0    41    38     0     3     0  3477     4]
 [    0     0     0     0     0    14     0     0     0   905]]

Accuracy:
98.68411539295785

F1 scores:
[       nan 0.99190409 0.99509002 0.95797899 0.96806554 0.99466463
 0.98810613 0.99611801 0.96102819 0.98423056]

Kappa:
0.9825876076501555
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fee76643940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.067, val_acc:0.170]
Epoch [2/120    avg_loss:1.608, val_acc:0.350]
Epoch [3/120    avg_loss:1.372, val_acc:0.451]
Epoch [4/120    avg_loss:1.165, val_acc:0.585]
Epoch [5/120    avg_loss:0.999, val_acc:0.732]
Epoch [6/120    avg_loss:0.863, val_acc:0.740]
Epoch [7/120    avg_loss:0.772, val_acc:0.678]
Epoch [8/120    avg_loss:0.697, val_acc:0.763]
Epoch [9/120    avg_loss:0.590, val_acc:0.744]
Epoch [10/120    avg_loss:0.540, val_acc:0.725]
Epoch [11/120    avg_loss:0.464, val_acc:0.742]
Epoch [12/120    avg_loss:0.407, val_acc:0.755]
Epoch [13/120    avg_loss:0.396, val_acc:0.711]
Epoch [14/120    avg_loss:0.372, val_acc:0.767]
Epoch [15/120    avg_loss:0.344, val_acc:0.791]
Epoch [16/120    avg_loss:0.305, val_acc:0.810]
Epoch [17/120    avg_loss:0.285, val_acc:0.752]
Epoch [18/120    avg_loss:0.286, val_acc:0.822]
Epoch [19/120    avg_loss:0.241, val_acc:0.828]
Epoch [20/120    avg_loss:0.223, val_acc:0.847]
Epoch [21/120    avg_loss:0.197, val_acc:0.885]
Epoch [22/120    avg_loss:0.190, val_acc:0.836]
Epoch [23/120    avg_loss:0.185, val_acc:0.937]
Epoch [24/120    avg_loss:0.180, val_acc:0.927]
Epoch [25/120    avg_loss:0.167, val_acc:0.931]
Epoch [26/120    avg_loss:0.134, val_acc:0.929]
Epoch [27/120    avg_loss:0.115, val_acc:0.942]
Epoch [28/120    avg_loss:0.102, val_acc:0.956]
Epoch [29/120    avg_loss:0.111, val_acc:0.928]
Epoch [30/120    avg_loss:0.129, val_acc:0.917]
Epoch [31/120    avg_loss:0.123, val_acc:0.955]
Epoch [32/120    avg_loss:0.088, val_acc:0.951]
Epoch [33/120    avg_loss:0.099, val_acc:0.952]
Epoch [34/120    avg_loss:0.083, val_acc:0.951]
Epoch [35/120    avg_loss:0.132, val_acc:0.891]
Epoch [36/120    avg_loss:0.104, val_acc:0.947]
Epoch [37/120    avg_loss:0.080, val_acc:0.956]
Epoch [38/120    avg_loss:0.069, val_acc:0.957]
Epoch [39/120    avg_loss:0.060, val_acc:0.964]
Epoch [40/120    avg_loss:0.053, val_acc:0.948]
Epoch [41/120    avg_loss:0.091, val_acc:0.953]
Epoch [42/120    avg_loss:0.079, val_acc:0.950]
Epoch [43/120    avg_loss:0.079, val_acc:0.957]
Epoch [44/120    avg_loss:0.075, val_acc:0.954]
Epoch [45/120    avg_loss:0.057, val_acc:0.960]
Epoch [46/120    avg_loss:0.049, val_acc:0.962]
Epoch [47/120    avg_loss:0.040, val_acc:0.951]
Epoch [48/120    avg_loss:0.043, val_acc:0.973]
Epoch [49/120    avg_loss:0.040, val_acc:0.960]
Epoch [50/120    avg_loss:0.071, val_acc:0.938]
Epoch [51/120    avg_loss:0.057, val_acc:0.966]
Epoch [52/120    avg_loss:0.028, val_acc:0.975]
Epoch [53/120    avg_loss:0.025, val_acc:0.975]
Epoch [54/120    avg_loss:0.034, val_acc:0.887]
Epoch [55/120    avg_loss:0.048, val_acc:0.964]
Epoch [56/120    avg_loss:0.047, val_acc:0.971]
Epoch [57/120    avg_loss:0.027, val_acc:0.974]
Epoch [58/120    avg_loss:0.024, val_acc:0.975]
Epoch [59/120    avg_loss:0.018, val_acc:0.981]
Epoch [60/120    avg_loss:0.026, val_acc:0.980]
Epoch [61/120    avg_loss:0.026, val_acc:0.975]
Epoch [62/120    avg_loss:0.024, val_acc:0.981]
Epoch [63/120    avg_loss:0.027, val_acc:0.975]
Epoch [64/120    avg_loss:0.029, val_acc:0.970]
Epoch [65/120    avg_loss:0.036, val_acc:0.970]
Epoch [66/120    avg_loss:0.034, val_acc:0.977]
Epoch [67/120    avg_loss:0.028, val_acc:0.982]
Epoch [68/120    avg_loss:0.018, val_acc:0.979]
Epoch [69/120    avg_loss:0.018, val_acc:0.979]
Epoch [70/120    avg_loss:0.018, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.019, val_acc:0.975]
Epoch [73/120    avg_loss:0.017, val_acc:0.981]
Epoch [74/120    avg_loss:0.016, val_acc:0.979]
Epoch [75/120    avg_loss:0.032, val_acc:0.978]
Epoch [76/120    avg_loss:0.025, val_acc:0.973]
Epoch [77/120    avg_loss:0.014, val_acc:0.978]
Epoch [78/120    avg_loss:0.011, val_acc:0.981]
Epoch [79/120    avg_loss:0.014, val_acc:0.975]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.014, val_acc:0.975]
Epoch [83/120    avg_loss:0.016, val_acc:0.980]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.965]
Epoch [91/120    avg_loss:0.011, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.983]
Epoch [93/120    avg_loss:0.018, val_acc:0.984]
Epoch [94/120    avg_loss:0.014, val_acc:0.979]
Epoch [95/120    avg_loss:0.014, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.980]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.005, val_acc:0.982]
Epoch [99/120    avg_loss:0.005, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6329     0     7     0     0     4    49    41     2]
 [    0     0 18014     0     7     0    65     0     4     0]
 [    0     1     0  1910     0     0     0     0   124     1]
 [    0    17     3     0  2942     0     3     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    10     0     4     0  4863     0     0     0]
 [    0    17     0     0     0     0     0  1266     5     2]
 [    0     2     0    25    33     0     0     0  3510     1]
 [    0     0     0     0     2    15     0     0     0   902]]

Accuracy:
98.91065962933507

F1 scores:
[       nan 0.98898351 0.99753579 0.96028155 0.98724832 0.99428571
 0.99113421 0.97197697 0.96667585 0.98741106]

Kappa:
0.9855761647784854
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ff88e8898>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.951, val_acc:0.088]
Epoch [2/120    avg_loss:1.583, val_acc:0.178]
Epoch [3/120    avg_loss:1.391, val_acc:0.302]
Epoch [4/120    avg_loss:1.241, val_acc:0.381]
Epoch [5/120    avg_loss:1.135, val_acc:0.453]
Epoch [6/120    avg_loss:1.013, val_acc:0.646]
Epoch [7/120    avg_loss:0.927, val_acc:0.566]
Epoch [8/120    avg_loss:0.815, val_acc:0.758]
Epoch [9/120    avg_loss:0.734, val_acc:0.766]
Epoch [10/120    avg_loss:0.650, val_acc:0.691]
Epoch [11/120    avg_loss:0.568, val_acc:0.712]
Epoch [12/120    avg_loss:0.497, val_acc:0.769]
Epoch [13/120    avg_loss:0.426, val_acc:0.803]
Epoch [14/120    avg_loss:0.373, val_acc:0.808]
Epoch [15/120    avg_loss:0.360, val_acc:0.863]
Epoch [16/120    avg_loss:0.296, val_acc:0.852]
Epoch [17/120    avg_loss:0.245, val_acc:0.891]
Epoch [18/120    avg_loss:0.223, val_acc:0.917]
Epoch [19/120    avg_loss:0.215, val_acc:0.909]
Epoch [20/120    avg_loss:0.200, val_acc:0.856]
Epoch [21/120    avg_loss:0.217, val_acc:0.848]
Epoch [22/120    avg_loss:0.197, val_acc:0.936]
Epoch [23/120    avg_loss:0.157, val_acc:0.919]
Epoch [24/120    avg_loss:0.125, val_acc:0.936]
Epoch [25/120    avg_loss:0.125, val_acc:0.948]
Epoch [26/120    avg_loss:0.119, val_acc:0.944]
Epoch [27/120    avg_loss:0.139, val_acc:0.946]
Epoch [28/120    avg_loss:0.109, val_acc:0.960]
Epoch [29/120    avg_loss:0.101, val_acc:0.956]
Epoch [30/120    avg_loss:0.092, val_acc:0.930]
Epoch [31/120    avg_loss:0.084, val_acc:0.947]
Epoch [32/120    avg_loss:0.122, val_acc:0.930]
Epoch [33/120    avg_loss:0.106, val_acc:0.932]
Epoch [34/120    avg_loss:0.100, val_acc:0.947]
Epoch [35/120    avg_loss:0.101, val_acc:0.923]
Epoch [36/120    avg_loss:0.081, val_acc:0.948]
Epoch [37/120    avg_loss:0.077, val_acc:0.962]
Epoch [38/120    avg_loss:0.087, val_acc:0.961]
Epoch [39/120    avg_loss:0.063, val_acc:0.963]
Epoch [40/120    avg_loss:0.060, val_acc:0.973]
Epoch [41/120    avg_loss:0.041, val_acc:0.961]
Epoch [42/120    avg_loss:0.051, val_acc:0.968]
Epoch [43/120    avg_loss:0.047, val_acc:0.974]
Epoch [44/120    avg_loss:0.042, val_acc:0.977]
Epoch [45/120    avg_loss:0.047, val_acc:0.966]
Epoch [46/120    avg_loss:0.042, val_acc:0.968]
Epoch [47/120    avg_loss:0.038, val_acc:0.969]
Epoch [48/120    avg_loss:0.030, val_acc:0.974]
Epoch [49/120    avg_loss:0.046, val_acc:0.968]
Epoch [50/120    avg_loss:0.060, val_acc:0.961]
Epoch [51/120    avg_loss:0.057, val_acc:0.950]
Epoch [52/120    avg_loss:0.059, val_acc:0.959]
Epoch [53/120    avg_loss:0.086, val_acc:0.961]
Epoch [54/120    avg_loss:0.042, val_acc:0.961]
Epoch [55/120    avg_loss:0.028, val_acc:0.975]
Epoch [56/120    avg_loss:0.029, val_acc:0.959]
Epoch [57/120    avg_loss:0.043, val_acc:0.951]
Epoch [58/120    avg_loss:0.028, val_acc:0.976]
Epoch [59/120    avg_loss:0.020, val_acc:0.979]
Epoch [60/120    avg_loss:0.018, val_acc:0.982]
Epoch [61/120    avg_loss:0.014, val_acc:0.982]
Epoch [62/120    avg_loss:0.016, val_acc:0.984]
Epoch [63/120    avg_loss:0.017, val_acc:0.984]
Epoch [64/120    avg_loss:0.015, val_acc:0.985]
Epoch [65/120    avg_loss:0.016, val_acc:0.984]
Epoch [66/120    avg_loss:0.015, val_acc:0.984]
Epoch [67/120    avg_loss:0.013, val_acc:0.984]
Epoch [68/120    avg_loss:0.016, val_acc:0.984]
Epoch [69/120    avg_loss:0.015, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.015, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.984]
Epoch [73/120    avg_loss:0.015, val_acc:0.983]
Epoch [74/120    avg_loss:0.015, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.013, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.983]
Epoch [79/120    avg_loss:0.014, val_acc:0.983]
Epoch [80/120    avg_loss:0.015, val_acc:0.983]
Epoch [81/120    avg_loss:0.012, val_acc:0.983]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.983]
Epoch [85/120    avg_loss:0.015, val_acc:0.983]
Epoch [86/120    avg_loss:0.015, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.014, val_acc:0.983]
Epoch [89/120    avg_loss:0.017, val_acc:0.983]
Epoch [90/120    avg_loss:0.012, val_acc:0.983]
Epoch [91/120    avg_loss:0.018, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.015, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.014, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.011, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.013, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.013, val_acc:0.983]
Epoch [103/120    avg_loss:0.016, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.014, val_acc:0.983]
Epoch [106/120    avg_loss:0.013, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.015, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.011, val_acc:0.983]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.012, val_acc:0.983]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.012, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     0     0     0     1    35    12]
 [    0     0 18046     0    26     0    10     0     8     0]
 [    0     0     0  1881     0     0     0     0   152     3]
 [    0    10    14     0  2933     0    10     0     1     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    70     0     0     0  4774     0    34     0]
 [    0    11     0     0     0     0     0  1278     1     0]
 [    0     5     0    50    48     0     5     0  3463     0]
 [    0     0     0     0     1    11     0     0     0   907]]

Accuracy:
98.74195647458608

F1 scores:
[       nan 0.99423766 0.99646604 0.94832367 0.98093645 0.99580313
 0.98666942 0.99493967 0.95333792 0.98319783]

Kappa:
0.9833232866704164
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbaf081e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.058, val_acc:0.068]
Epoch [2/120    avg_loss:1.718, val_acc:0.248]
Epoch [3/120    avg_loss:1.476, val_acc:0.289]
Epoch [4/120    avg_loss:1.281, val_acc:0.367]
Epoch [5/120    avg_loss:1.112, val_acc:0.447]
Epoch [6/120    avg_loss:0.959, val_acc:0.582]
Epoch [7/120    avg_loss:0.866, val_acc:0.670]
Epoch [8/120    avg_loss:0.767, val_acc:0.672]
Epoch [9/120    avg_loss:0.692, val_acc:0.603]
Epoch [10/120    avg_loss:0.599, val_acc:0.590]
Epoch [11/120    avg_loss:0.546, val_acc:0.670]
Epoch [12/120    avg_loss:0.492, val_acc:0.655]
Epoch [13/120    avg_loss:0.426, val_acc:0.674]
Epoch [14/120    avg_loss:0.400, val_acc:0.788]
Epoch [15/120    avg_loss:0.379, val_acc:0.758]
Epoch [16/120    avg_loss:0.386, val_acc:0.726]
Epoch [17/120    avg_loss:0.333, val_acc:0.777]
Epoch [18/120    avg_loss:0.312, val_acc:0.766]
Epoch [19/120    avg_loss:0.288, val_acc:0.738]
Epoch [20/120    avg_loss:0.276, val_acc:0.832]
Epoch [21/120    avg_loss:0.245, val_acc:0.824]
Epoch [22/120    avg_loss:0.224, val_acc:0.877]
Epoch [23/120    avg_loss:0.227, val_acc:0.889]
Epoch [24/120    avg_loss:0.245, val_acc:0.909]
Epoch [25/120    avg_loss:0.184, val_acc:0.893]
Epoch [26/120    avg_loss:0.157, val_acc:0.908]
Epoch [27/120    avg_loss:0.172, val_acc:0.908]
Epoch [28/120    avg_loss:0.152, val_acc:0.933]
Epoch [29/120    avg_loss:0.112, val_acc:0.931]
Epoch [30/120    avg_loss:0.102, val_acc:0.919]
Epoch [31/120    avg_loss:0.101, val_acc:0.911]
Epoch [32/120    avg_loss:0.087, val_acc:0.941]
Epoch [33/120    avg_loss:0.088, val_acc:0.942]
Epoch [34/120    avg_loss:0.108, val_acc:0.920]
Epoch [35/120    avg_loss:0.139, val_acc:0.937]
Epoch [36/120    avg_loss:0.095, val_acc:0.932]
Epoch [37/120    avg_loss:0.124, val_acc:0.840]
Epoch [38/120    avg_loss:0.108, val_acc:0.915]
Epoch [39/120    avg_loss:0.091, val_acc:0.918]
Epoch [40/120    avg_loss:0.169, val_acc:0.825]
Epoch [41/120    avg_loss:0.150, val_acc:0.933]
Epoch [42/120    avg_loss:0.099, val_acc:0.933]
Epoch [43/120    avg_loss:0.076, val_acc:0.947]
Epoch [44/120    avg_loss:0.064, val_acc:0.924]
Epoch [45/120    avg_loss:0.072, val_acc:0.946]
Epoch [46/120    avg_loss:0.069, val_acc:0.940]
Epoch [47/120    avg_loss:0.058, val_acc:0.947]
Epoch [48/120    avg_loss:0.053, val_acc:0.940]
Epoch [49/120    avg_loss:0.049, val_acc:0.949]
Epoch [50/120    avg_loss:0.041, val_acc:0.955]
Epoch [51/120    avg_loss:0.046, val_acc:0.960]
Epoch [52/120    avg_loss:0.054, val_acc:0.955]
Epoch [53/120    avg_loss:0.045, val_acc:0.956]
Epoch [54/120    avg_loss:0.055, val_acc:0.954]
Epoch [55/120    avg_loss:0.047, val_acc:0.942]
Epoch [56/120    avg_loss:0.061, val_acc:0.956]
Epoch [57/120    avg_loss:0.040, val_acc:0.942]
Epoch [58/120    avg_loss:0.037, val_acc:0.935]
Epoch [59/120    avg_loss:0.035, val_acc:0.961]
Epoch [60/120    avg_loss:0.035, val_acc:0.882]
Epoch [61/120    avg_loss:0.052, val_acc:0.967]
Epoch [62/120    avg_loss:0.031, val_acc:0.966]
Epoch [63/120    avg_loss:0.033, val_acc:0.963]
Epoch [64/120    avg_loss:0.026, val_acc:0.962]
Epoch [65/120    avg_loss:0.021, val_acc:0.946]
Epoch [66/120    avg_loss:0.029, val_acc:0.946]
Epoch [67/120    avg_loss:0.031, val_acc:0.962]
Epoch [68/120    avg_loss:0.024, val_acc:0.951]
Epoch [69/120    avg_loss:0.017, val_acc:0.965]
Epoch [70/120    avg_loss:0.026, val_acc:0.964]
Epoch [71/120    avg_loss:0.022, val_acc:0.970]
Epoch [72/120    avg_loss:0.022, val_acc:0.954]
Epoch [73/120    avg_loss:0.022, val_acc:0.967]
Epoch [74/120    avg_loss:0.025, val_acc:0.937]
Epoch [75/120    avg_loss:0.021, val_acc:0.963]
Epoch [76/120    avg_loss:0.016, val_acc:0.966]
Epoch [77/120    avg_loss:0.014, val_acc:0.969]
Epoch [78/120    avg_loss:0.032, val_acc:0.968]
Epoch [79/120    avg_loss:0.019, val_acc:0.965]
Epoch [80/120    avg_loss:0.019, val_acc:0.965]
Epoch [81/120    avg_loss:0.018, val_acc:0.973]
Epoch [82/120    avg_loss:0.016, val_acc:0.952]
Epoch [83/120    avg_loss:0.016, val_acc:0.970]
Epoch [84/120    avg_loss:0.017, val_acc:0.964]
Epoch [85/120    avg_loss:0.021, val_acc:0.966]
Epoch [86/120    avg_loss:0.022, val_acc:0.971]
Epoch [87/120    avg_loss:0.013, val_acc:0.972]
Epoch [88/120    avg_loss:0.010, val_acc:0.977]
Epoch [89/120    avg_loss:0.026, val_acc:0.970]
Epoch [90/120    avg_loss:0.025, val_acc:0.970]
Epoch [91/120    avg_loss:0.015, val_acc:0.972]
Epoch [92/120    avg_loss:0.016, val_acc:0.972]
Epoch [93/120    avg_loss:0.018, val_acc:0.972]
Epoch [94/120    avg_loss:0.009, val_acc:0.973]
Epoch [95/120    avg_loss:0.007, val_acc:0.975]
Epoch [96/120    avg_loss:0.010, val_acc:0.961]
Epoch [97/120    avg_loss:0.014, val_acc:0.938]
Epoch [98/120    avg_loss:0.021, val_acc:0.973]
Epoch [99/120    avg_loss:0.019, val_acc:0.972]
Epoch [100/120    avg_loss:0.014, val_acc:0.971]
Epoch [101/120    avg_loss:0.015, val_acc:0.975]
Epoch [102/120    avg_loss:0.015, val_acc:0.977]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.975]
Epoch [105/120    avg_loss:0.006, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.975]
Epoch [108/120    avg_loss:0.009, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.976]
Epoch [110/120    avg_loss:0.008, val_acc:0.976]
Epoch [111/120    avg_loss:0.007, val_acc:0.975]
Epoch [112/120    avg_loss:0.006, val_acc:0.977]
Epoch [113/120    avg_loss:0.006, val_acc:0.978]
Epoch [114/120    avg_loss:0.006, val_acc:0.977]
Epoch [115/120    avg_loss:0.006, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.976]
Epoch [117/120    avg_loss:0.005, val_acc:0.976]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.005, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     2     0     0     0     1    69     1]
 [    0     1 18053     0    14     0    19     0     3     0]
 [    0     5     0  1895     0     0     0     0   136     0]
 [    0    33     8     1  2914     0    13     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    57     0     0     0  4821     0     0     0]
 [    0    30     0     0     0     1     0  1252     7     0]
 [    0    26     1    51    38     0     2     0  3452     1]
 [    0     0     0     0     9    12     0     0     0   898]]

Accuracy:
98.68893548309353

F1 scores:
[       nan 0.9869626  0.9971554  0.9510665  0.97998991 0.99504384
 0.99065036 0.98466378 0.95345947 0.98735569]

Kappa:
0.9826183592286968
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc59b2ab940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.009, val_acc:0.123]
Epoch [2/120    avg_loss:1.629, val_acc:0.187]
Epoch [3/120    avg_loss:1.413, val_acc:0.310]
Epoch [4/120    avg_loss:1.251, val_acc:0.520]
Epoch [5/120    avg_loss:1.076, val_acc:0.601]
Epoch [6/120    avg_loss:0.927, val_acc:0.581]
Epoch [7/120    avg_loss:0.780, val_acc:0.600]
Epoch [8/120    avg_loss:0.637, val_acc:0.596]
Epoch [9/120    avg_loss:0.553, val_acc:0.728]
Epoch [10/120    avg_loss:0.495, val_acc:0.791]
Epoch [11/120    avg_loss:0.421, val_acc:0.820]
Epoch [12/120    avg_loss:0.422, val_acc:0.813]
Epoch [13/120    avg_loss:0.374, val_acc:0.772]
Epoch [14/120    avg_loss:0.320, val_acc:0.871]
Epoch [15/120    avg_loss:0.285, val_acc:0.822]
Epoch [16/120    avg_loss:0.267, val_acc:0.900]
Epoch [17/120    avg_loss:0.241, val_acc:0.889]
Epoch [18/120    avg_loss:0.246, val_acc:0.896]
Epoch [19/120    avg_loss:0.193, val_acc:0.915]
Epoch [20/120    avg_loss:0.220, val_acc:0.929]
Epoch [21/120    avg_loss:0.163, val_acc:0.873]
Epoch [22/120    avg_loss:0.176, val_acc:0.929]
Epoch [23/120    avg_loss:0.147, val_acc:0.925]
Epoch [24/120    avg_loss:0.123, val_acc:0.953]
Epoch [25/120    avg_loss:0.140, val_acc:0.927]
Epoch [26/120    avg_loss:0.110, val_acc:0.924]
Epoch [27/120    avg_loss:0.084, val_acc:0.954]
Epoch [28/120    avg_loss:0.080, val_acc:0.951]
Epoch [29/120    avg_loss:0.066, val_acc:0.962]
Epoch [30/120    avg_loss:0.119, val_acc:0.890]
Epoch [31/120    avg_loss:0.086, val_acc:0.947]
Epoch [32/120    avg_loss:0.108, val_acc:0.924]
Epoch [33/120    avg_loss:0.088, val_acc:0.962]
Epoch [34/120    avg_loss:0.061, val_acc:0.950]
Epoch [35/120    avg_loss:0.056, val_acc:0.968]
Epoch [36/120    avg_loss:0.051, val_acc:0.966]
Epoch [37/120    avg_loss:0.059, val_acc:0.960]
Epoch [38/120    avg_loss:0.061, val_acc:0.962]
Epoch [39/120    avg_loss:0.059, val_acc:0.964]
Epoch [40/120    avg_loss:0.054, val_acc:0.965]
Epoch [41/120    avg_loss:0.042, val_acc:0.975]
Epoch [42/120    avg_loss:0.035, val_acc:0.969]
Epoch [43/120    avg_loss:0.045, val_acc:0.970]
Epoch [44/120    avg_loss:0.026, val_acc:0.978]
Epoch [45/120    avg_loss:0.027, val_acc:0.974]
Epoch [46/120    avg_loss:0.045, val_acc:0.978]
Epoch [47/120    avg_loss:0.037, val_acc:0.964]
Epoch [48/120    avg_loss:0.041, val_acc:0.974]
Epoch [49/120    avg_loss:0.039, val_acc:0.965]
Epoch [50/120    avg_loss:0.043, val_acc:0.965]
Epoch [51/120    avg_loss:0.035, val_acc:0.971]
Epoch [52/120    avg_loss:0.047, val_acc:0.953]
Epoch [53/120    avg_loss:0.030, val_acc:0.972]
Epoch [54/120    avg_loss:0.031, val_acc:0.976]
Epoch [55/120    avg_loss:0.027, val_acc:0.951]
Epoch [56/120    avg_loss:0.022, val_acc:0.976]
Epoch [57/120    avg_loss:0.027, val_acc:0.975]
Epoch [58/120    avg_loss:0.022, val_acc:0.973]
Epoch [59/120    avg_loss:0.016, val_acc:0.975]
Epoch [60/120    avg_loss:0.013, val_acc:0.978]
Epoch [61/120    avg_loss:0.013, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.012, val_acc:0.980]
Epoch [64/120    avg_loss:0.016, val_acc:0.982]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.012, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.010, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.985]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.009, val_acc:0.983]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.983]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.009, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.983]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.982]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     0     0     3     4    63    23]
 [    0     0 18050     0    26     0    10     0     4     0]
 [    0     4     0  1971     0     0     0     0    61     0]
 [    0    11     0     0  2952     0     1     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     2     0  4868     0     3     0]
 [    0    11     0     0     0     0     1  1276     2     0]
 [    0    25     0    52    48     0     0     0  3446     0]
 [    0     2     0     0     3    17     0     0     0   897]]

Accuracy:
99.06249246860916

F1 scores:
[       nan 0.9886151  0.99875501 0.97117517 0.98350825 0.99352874
 0.99743879 0.99299611 0.96324249 0.97394137]

Kappa:
0.9875837446301993
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93db93b978>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.054, val_acc:0.419]
Epoch [2/120    avg_loss:1.686, val_acc:0.568]
Epoch [3/120    avg_loss:1.407, val_acc:0.558]
Epoch [4/120    avg_loss:1.195, val_acc:0.691]
Epoch [5/120    avg_loss:1.013, val_acc:0.770]
Epoch [6/120    avg_loss:0.885, val_acc:0.670]
Epoch [7/120    avg_loss:0.756, val_acc:0.729]
Epoch [8/120    avg_loss:0.660, val_acc:0.627]
Epoch [9/120    avg_loss:0.576, val_acc:0.703]
Epoch [10/120    avg_loss:0.485, val_acc:0.721]
Epoch [11/120    avg_loss:0.431, val_acc:0.750]
Epoch [12/120    avg_loss:0.398, val_acc:0.821]
Epoch [13/120    avg_loss:0.362, val_acc:0.838]
Epoch [14/120    avg_loss:0.313, val_acc:0.820]
Epoch [15/120    avg_loss:0.262, val_acc:0.831]
Epoch [16/120    avg_loss:0.231, val_acc:0.920]
Epoch [17/120    avg_loss:0.226, val_acc:0.873]
Epoch [18/120    avg_loss:0.212, val_acc:0.908]
Epoch [19/120    avg_loss:0.190, val_acc:0.909]
Epoch [20/120    avg_loss:0.172, val_acc:0.908]
Epoch [21/120    avg_loss:0.151, val_acc:0.937]
Epoch [22/120    avg_loss:0.126, val_acc:0.945]
Epoch [23/120    avg_loss:0.125, val_acc:0.891]
Epoch [24/120    avg_loss:0.122, val_acc:0.961]
Epoch [25/120    avg_loss:0.106, val_acc:0.956]
Epoch [26/120    avg_loss:0.098, val_acc:0.953]
Epoch [27/120    avg_loss:0.085, val_acc:0.970]
Epoch [28/120    avg_loss:0.077, val_acc:0.965]
Epoch [29/120    avg_loss:0.066, val_acc:0.968]
Epoch [30/120    avg_loss:0.061, val_acc:0.967]
Epoch [31/120    avg_loss:0.048, val_acc:0.968]
Epoch [32/120    avg_loss:0.062, val_acc:0.969]
Epoch [33/120    avg_loss:0.057, val_acc:0.978]
Epoch [34/120    avg_loss:0.082, val_acc:0.945]
Epoch [35/120    avg_loss:0.075, val_acc:0.966]
Epoch [36/120    avg_loss:0.044, val_acc:0.970]
Epoch [37/120    avg_loss:0.070, val_acc:0.966]
Epoch [38/120    avg_loss:0.043, val_acc:0.978]
Epoch [39/120    avg_loss:0.043, val_acc:0.955]
Epoch [40/120    avg_loss:0.065, val_acc:0.938]
Epoch [41/120    avg_loss:0.080, val_acc:0.919]
Epoch [42/120    avg_loss:0.072, val_acc:0.951]
Epoch [43/120    avg_loss:0.061, val_acc:0.953]
Epoch [44/120    avg_loss:0.051, val_acc:0.973]
Epoch [45/120    avg_loss:0.028, val_acc:0.979]
Epoch [46/120    avg_loss:0.022, val_acc:0.979]
Epoch [47/120    avg_loss:0.027, val_acc:0.969]
Epoch [48/120    avg_loss:0.027, val_acc:0.970]
Epoch [49/120    avg_loss:0.034, val_acc:0.979]
Epoch [50/120    avg_loss:0.021, val_acc:0.982]
Epoch [51/120    avg_loss:0.019, val_acc:0.980]
Epoch [52/120    avg_loss:0.017, val_acc:0.955]
Epoch [53/120    avg_loss:0.023, val_acc:0.976]
Epoch [54/120    avg_loss:0.024, val_acc:0.964]
Epoch [55/120    avg_loss:0.023, val_acc:0.979]
Epoch [56/120    avg_loss:0.020, val_acc:0.980]
Epoch [57/120    avg_loss:0.013, val_acc:0.982]
Epoch [58/120    avg_loss:0.014, val_acc:0.979]
Epoch [59/120    avg_loss:0.012, val_acc:0.984]
Epoch [60/120    avg_loss:0.010, val_acc:0.980]
Epoch [61/120    avg_loss:0.011, val_acc:0.985]
Epoch [62/120    avg_loss:0.017, val_acc:0.983]
Epoch [63/120    avg_loss:0.010, val_acc:0.982]
Epoch [64/120    avg_loss:0.016, val_acc:0.975]
Epoch [65/120    avg_loss:0.015, val_acc:0.978]
Epoch [66/120    avg_loss:0.027, val_acc:0.979]
Epoch [67/120    avg_loss:0.015, val_acc:0.979]
Epoch [68/120    avg_loss:0.015, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.974]
Epoch [71/120    avg_loss:0.019, val_acc:0.976]
Epoch [72/120    avg_loss:0.016, val_acc:0.979]
Epoch [73/120    avg_loss:0.014, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6327     4     4     2     0     2     0    83    10]
 [    0     0 18063     0    21     0     5     0     1     0]
 [    0     6     0  1928     0     0     0     0   101     1]
 [    0    11    12     0  2934     0    12     0     1     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    58     0     0     0  4806     0    14     0]
 [    0    23     4     0     0     0     0  1259     4     0]
 [    0    44     8    38    28     0     1     0  3452     0]
 [    0     0     2     0     6    16     0     0     0   895]]

Accuracy:
98.7371363844504

F1 scores:
[       nan 0.98528381 0.9968268  0.96255617 0.98406842 0.99390708
 0.99051937 0.98783837 0.95530649 0.97974822]

Kappa:
0.9832545511866804
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f30a01569b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.033, val_acc:0.073]
Epoch [2/120    avg_loss:1.704, val_acc:0.249]
Epoch [3/120    avg_loss:1.496, val_acc:0.319]
Epoch [4/120    avg_loss:1.330, val_acc:0.368]
Epoch [5/120    avg_loss:1.173, val_acc:0.418]
Epoch [6/120    avg_loss:1.012, val_acc:0.449]
Epoch [7/120    avg_loss:0.889, val_acc:0.625]
Epoch [8/120    avg_loss:0.807, val_acc:0.633]
Epoch [9/120    avg_loss:0.707, val_acc:0.655]
Epoch [10/120    avg_loss:0.631, val_acc:0.660]
Epoch [11/120    avg_loss:0.537, val_acc:0.703]
Epoch [12/120    avg_loss:0.479, val_acc:0.767]
Epoch [13/120    avg_loss:0.440, val_acc:0.726]
Epoch [14/120    avg_loss:0.387, val_acc:0.799]
Epoch [15/120    avg_loss:0.387, val_acc:0.821]
Epoch [16/120    avg_loss:0.333, val_acc:0.860]
Epoch [17/120    avg_loss:0.315, val_acc:0.803]
Epoch [18/120    avg_loss:0.297, val_acc:0.843]
Epoch [19/120    avg_loss:0.283, val_acc:0.814]
Epoch [20/120    avg_loss:0.262, val_acc:0.895]
Epoch [21/120    avg_loss:0.231, val_acc:0.905]
Epoch [22/120    avg_loss:0.191, val_acc:0.927]
Epoch [23/120    avg_loss:0.183, val_acc:0.921]
Epoch [24/120    avg_loss:0.202, val_acc:0.918]
Epoch [25/120    avg_loss:0.173, val_acc:0.907]
Epoch [26/120    avg_loss:0.138, val_acc:0.943]
Epoch [27/120    avg_loss:0.165, val_acc:0.951]
Epoch [28/120    avg_loss:0.124, val_acc:0.911]
Epoch [29/120    avg_loss:0.119, val_acc:0.950]
Epoch [30/120    avg_loss:0.099, val_acc:0.948]
Epoch [31/120    avg_loss:0.089, val_acc:0.961]
Epoch [32/120    avg_loss:0.086, val_acc:0.961]
Epoch [33/120    avg_loss:0.085, val_acc:0.960]
Epoch [34/120    avg_loss:0.077, val_acc:0.958]
Epoch [35/120    avg_loss:0.077, val_acc:0.971]
Epoch [36/120    avg_loss:0.069, val_acc:0.969]
Epoch [37/120    avg_loss:0.249, val_acc:0.537]
Epoch [38/120    avg_loss:1.185, val_acc:0.543]
Epoch [39/120    avg_loss:0.983, val_acc:0.595]
Epoch [40/120    avg_loss:0.897, val_acc:0.558]
Epoch [41/120    avg_loss:0.845, val_acc:0.594]
Epoch [42/120    avg_loss:0.818, val_acc:0.566]
Epoch [43/120    avg_loss:0.810, val_acc:0.629]
Epoch [44/120    avg_loss:0.802, val_acc:0.622]
Epoch [45/120    avg_loss:0.780, val_acc:0.655]
Epoch [46/120    avg_loss:0.728, val_acc:0.656]
Epoch [47/120    avg_loss:0.779, val_acc:0.636]
Epoch [48/120    avg_loss:0.728, val_acc:0.682]
Epoch [49/120    avg_loss:0.716, val_acc:0.675]
Epoch [50/120    avg_loss:0.658, val_acc:0.677]
Epoch [51/120    avg_loss:0.677, val_acc:0.681]
Epoch [52/120    avg_loss:0.697, val_acc:0.680]
Epoch [53/120    avg_loss:0.660, val_acc:0.684]
Epoch [54/120    avg_loss:0.679, val_acc:0.683]
Epoch [55/120    avg_loss:0.702, val_acc:0.679]
Epoch [56/120    avg_loss:0.667, val_acc:0.676]
Epoch [57/120    avg_loss:0.663, val_acc:0.674]
Epoch [58/120    avg_loss:0.655, val_acc:0.686]
Epoch [59/120    avg_loss:0.674, val_acc:0.688]
Epoch [60/120    avg_loss:0.670, val_acc:0.692]
Epoch [61/120    avg_loss:0.679, val_acc:0.688]
Epoch [62/120    avg_loss:0.670, val_acc:0.690]
Epoch [63/120    avg_loss:0.673, val_acc:0.695]
Epoch [64/120    avg_loss:0.668, val_acc:0.696]
Epoch [65/120    avg_loss:0.678, val_acc:0.692]
Epoch [66/120    avg_loss:0.663, val_acc:0.694]
Epoch [67/120    avg_loss:0.674, val_acc:0.690]
Epoch [68/120    avg_loss:0.679, val_acc:0.695]
Epoch [69/120    avg_loss:0.663, val_acc:0.693]
Epoch [70/120    avg_loss:0.683, val_acc:0.696]
Epoch [71/120    avg_loss:0.677, val_acc:0.696]
Epoch [72/120    avg_loss:0.650, val_acc:0.695]
Epoch [73/120    avg_loss:0.689, val_acc:0.694]
Epoch [74/120    avg_loss:0.669, val_acc:0.694]
Epoch [75/120    avg_loss:0.664, val_acc:0.694]
Epoch [76/120    avg_loss:0.655, val_acc:0.693]
Epoch [77/120    avg_loss:0.652, val_acc:0.693]
Epoch [78/120    avg_loss:0.695, val_acc:0.693]
Epoch [79/120    avg_loss:0.648, val_acc:0.693]
Epoch [80/120    avg_loss:0.677, val_acc:0.692]
Epoch [81/120    avg_loss:0.696, val_acc:0.693]
Epoch [82/120    avg_loss:0.663, val_acc:0.692]
Epoch [83/120    avg_loss:0.666, val_acc:0.693]
Epoch [84/120    avg_loss:0.650, val_acc:0.692]
Epoch [85/120    avg_loss:0.675, val_acc:0.692]
Epoch [86/120    avg_loss:0.641, val_acc:0.692]
Epoch [87/120    avg_loss:0.661, val_acc:0.692]
Epoch [88/120    avg_loss:0.661, val_acc:0.692]
Epoch [89/120    avg_loss:0.689, val_acc:0.692]
Epoch [90/120    avg_loss:0.647, val_acc:0.692]
Epoch [91/120    avg_loss:0.671, val_acc:0.692]
Epoch [92/120    avg_loss:0.656, val_acc:0.692]
Epoch [93/120    avg_loss:0.669, val_acc:0.692]
Epoch [94/120    avg_loss:0.674, val_acc:0.692]
Epoch [95/120    avg_loss:0.682, val_acc:0.692]
Epoch [96/120    avg_loss:0.661, val_acc:0.692]
Epoch [97/120    avg_loss:0.656, val_acc:0.692]
Epoch [98/120    avg_loss:0.652, val_acc:0.692]
Epoch [99/120    avg_loss:0.669, val_acc:0.692]
Epoch [100/120    avg_loss:0.688, val_acc:0.692]
Epoch [101/120    avg_loss:0.656, val_acc:0.692]
Epoch [102/120    avg_loss:0.675, val_acc:0.692]
Epoch [103/120    avg_loss:0.650, val_acc:0.692]
Epoch [104/120    avg_loss:0.662, val_acc:0.692]
Epoch [105/120    avg_loss:0.689, val_acc:0.692]
Epoch [106/120    avg_loss:0.670, val_acc:0.692]
Epoch [107/120    avg_loss:0.641, val_acc:0.692]
Epoch [108/120    avg_loss:0.671, val_acc:0.692]
Epoch [109/120    avg_loss:0.665, val_acc:0.692]
Epoch [110/120    avg_loss:0.712, val_acc:0.692]
Epoch [111/120    avg_loss:0.645, val_acc:0.692]
Epoch [112/120    avg_loss:0.648, val_acc:0.692]
Epoch [113/120    avg_loss:0.657, val_acc:0.692]
Epoch [114/120    avg_loss:0.661, val_acc:0.692]
Epoch [115/120    avg_loss:0.668, val_acc:0.692]
Epoch [116/120    avg_loss:0.683, val_acc:0.692]
Epoch [117/120    avg_loss:0.661, val_acc:0.692]
Epoch [118/120    avg_loss:0.675, val_acc:0.692]
Epoch [119/120    avg_loss:0.678, val_acc:0.692]
Epoch [120/120    avg_loss:0.671, val_acc:0.692]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4296   692    20    47     0   746    25   391   215]
 [    0   128 11668     0   825     0  4260     0  1209     0]
 [    0    23     9  1609     0     0    19     0   307    69]
 [    0    14   220     0  2476     0   222     0    38     2]
 [    0     0     0     0     0  1299     0     6     0     0]
 [    0     0   542   146    39     0  3917     0   234     0]
 [    0   141     1     0     0     0     4  1088    56     0]
 [    0   107   223    71    13     0   347     1  2809     0]
 [    0    29     0     0    14    54    22     0     0   800]]

Accuracy:
72.20977032270504

F1 scores:
[       nan 0.76920322 0.74212116 0.82895415 0.77544629 0.97742664
 0.54346167 0.90290456 0.6521184  0.79800499]

Kappa:
0.6499843268467935
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feb8797d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.075, val_acc:0.084]
Epoch [2/120    avg_loss:1.719, val_acc:0.160]
Epoch [3/120    avg_loss:1.490, val_acc:0.430]
Epoch [4/120    avg_loss:1.307, val_acc:0.460]
Epoch [5/120    avg_loss:1.175, val_acc:0.546]
Epoch [6/120    avg_loss:0.999, val_acc:0.791]
Epoch [7/120    avg_loss:0.879, val_acc:0.560]
Epoch [8/120    avg_loss:0.772, val_acc:0.520]
Epoch [9/120    avg_loss:0.651, val_acc:0.590]
Epoch [10/120    avg_loss:0.584, val_acc:0.657]
Epoch [11/120    avg_loss:0.491, val_acc:0.724]
Epoch [12/120    avg_loss:0.454, val_acc:0.691]
Epoch [13/120    avg_loss:0.397, val_acc:0.828]
Epoch [14/120    avg_loss:0.335, val_acc:0.815]
Epoch [15/120    avg_loss:0.348, val_acc:0.823]
Epoch [16/120    avg_loss:0.341, val_acc:0.842]
Epoch [17/120    avg_loss:0.287, val_acc:0.875]
Epoch [18/120    avg_loss:0.262, val_acc:0.849]
Epoch [19/120    avg_loss:0.263, val_acc:0.886]
Epoch [20/120    avg_loss:0.217, val_acc:0.918]
Epoch [21/120    avg_loss:0.219, val_acc:0.889]
Epoch [22/120    avg_loss:0.242, val_acc:0.920]
Epoch [23/120    avg_loss:0.229, val_acc:0.899]
Epoch [24/120    avg_loss:0.198, val_acc:0.935]
Epoch [25/120    avg_loss:0.161, val_acc:0.934]
Epoch [26/120    avg_loss:0.154, val_acc:0.876]
Epoch [27/120    avg_loss:0.147, val_acc:0.958]
Epoch [28/120    avg_loss:0.135, val_acc:0.964]
Epoch [29/120    avg_loss:0.119, val_acc:0.954]
Epoch [30/120    avg_loss:0.101, val_acc:0.927]
Epoch [31/120    avg_loss:0.138, val_acc:0.928]
Epoch [32/120    avg_loss:0.114, val_acc:0.949]
Epoch [33/120    avg_loss:0.095, val_acc:0.972]
Epoch [34/120    avg_loss:0.106, val_acc:0.959]
Epoch [35/120    avg_loss:0.119, val_acc:0.942]
Epoch [36/120    avg_loss:0.082, val_acc:0.973]
Epoch [37/120    avg_loss:0.081, val_acc:0.981]
Epoch [38/120    avg_loss:0.070, val_acc:0.970]
Epoch [39/120    avg_loss:0.091, val_acc:0.967]
Epoch [40/120    avg_loss:0.083, val_acc:0.966]
Epoch [41/120    avg_loss:0.088, val_acc:0.969]
Epoch [42/120    avg_loss:0.055, val_acc:0.978]
Epoch [43/120    avg_loss:0.056, val_acc:0.977]
Epoch [44/120    avg_loss:0.064, val_acc:0.964]
Epoch [45/120    avg_loss:0.065, val_acc:0.978]
Epoch [46/120    avg_loss:0.065, val_acc:0.974]
Epoch [47/120    avg_loss:0.053, val_acc:0.981]
Epoch [48/120    avg_loss:0.049, val_acc:0.973]
Epoch [49/120    avg_loss:0.042, val_acc:0.981]
Epoch [50/120    avg_loss:0.026, val_acc:0.977]
Epoch [51/120    avg_loss:0.035, val_acc:0.976]
Epoch [52/120    avg_loss:0.036, val_acc:0.985]
Epoch [53/120    avg_loss:0.089, val_acc:0.964]
Epoch [54/120    avg_loss:0.079, val_acc:0.962]
Epoch [55/120    avg_loss:0.130, val_acc:0.916]
Epoch [56/120    avg_loss:0.112, val_acc:0.962]
Epoch [57/120    avg_loss:0.107, val_acc:0.970]
Epoch [58/120    avg_loss:0.072, val_acc:0.976]
Epoch [59/120    avg_loss:0.072, val_acc:0.970]
Epoch [60/120    avg_loss:0.080, val_acc:0.968]
Epoch [61/120    avg_loss:0.061, val_acc:0.964]
Epoch [62/120    avg_loss:0.050, val_acc:0.980]
Epoch [63/120    avg_loss:0.037, val_acc:0.980]
Epoch [64/120    avg_loss:0.024, val_acc:0.980]
Epoch [65/120    avg_loss:0.038, val_acc:0.977]
Epoch [66/120    avg_loss:0.036, val_acc:0.978]
Epoch [67/120    avg_loss:0.025, val_acc:0.980]
Epoch [68/120    avg_loss:0.022, val_acc:0.981]
Epoch [69/120    avg_loss:0.021, val_acc:0.982]
Epoch [70/120    avg_loss:0.020, val_acc:0.982]
Epoch [71/120    avg_loss:0.023, val_acc:0.983]
Epoch [72/120    avg_loss:0.020, val_acc:0.982]
Epoch [73/120    avg_loss:0.022, val_acc:0.983]
Epoch [74/120    avg_loss:0.025, val_acc:0.986]
Epoch [75/120    avg_loss:0.015, val_acc:0.984]
Epoch [76/120    avg_loss:0.016, val_acc:0.983]
Epoch [77/120    avg_loss:0.018, val_acc:0.985]
Epoch [78/120    avg_loss:0.018, val_acc:0.985]
Epoch [79/120    avg_loss:0.021, val_acc:0.984]
Epoch [80/120    avg_loss:0.017, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.984]
Epoch [82/120    avg_loss:0.018, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.983]
Epoch [85/120    avg_loss:0.018, val_acc:0.984]
Epoch [86/120    avg_loss:0.018, val_acc:0.982]
Epoch [87/120    avg_loss:0.014, val_acc:0.985]
Epoch [88/120    avg_loss:0.016, val_acc:0.985]
Epoch [89/120    avg_loss:0.018, val_acc:0.985]
Epoch [90/120    avg_loss:0.023, val_acc:0.985]
Epoch [91/120    avg_loss:0.015, val_acc:0.985]
Epoch [92/120    avg_loss:0.016, val_acc:0.985]
Epoch [93/120    avg_loss:0.016, val_acc:0.985]
Epoch [94/120    avg_loss:0.015, val_acc:0.985]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.985]
Epoch [97/120    avg_loss:0.016, val_acc:0.985]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.016, val_acc:0.985]
Epoch [100/120    avg_loss:0.015, val_acc:0.985]
Epoch [101/120    avg_loss:0.016, val_acc:0.985]
Epoch [102/120    avg_loss:0.019, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.016, val_acc:0.985]
Epoch [105/120    avg_loss:0.017, val_acc:0.985]
Epoch [106/120    avg_loss:0.020, val_acc:0.985]
Epoch [107/120    avg_loss:0.018, val_acc:0.985]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.017, val_acc:0.985]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.018, val_acc:0.985]
Epoch [112/120    avg_loss:0.022, val_acc:0.985]
Epoch [113/120    avg_loss:0.016, val_acc:0.985]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.015, val_acc:0.985]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.017, val_acc:0.985]
Epoch [118/120    avg_loss:0.016, val_acc:0.985]
Epoch [119/120    avg_loss:0.015, val_acc:0.985]
Epoch [120/120    avg_loss:0.018, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6297     0     1     0     0     7     7   110    10]
 [    0     0 18047     0    33     0     3     0     7     0]
 [    0    15     0  1971     0     0     0     0    45     5]
 [    0    22     7     0  2912     0    19     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    25     4     1     0  4842     0     6     0]
 [    0    10     0     0     0     0     6  1274     0     0]
 [    0    12     0    56    53     0     0     0  3449     1]
 [    0     0     0     1    14    17     0     0     0   887]]

Accuracy:
98.77328706046804

F1 scores:
[       nan 0.98482953 0.9979264  0.9687884  0.97309942 0.99352874
 0.99272168 0.99105406 0.95805556 0.97365532]

Kappa:
0.9837499896253915
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f18aed499e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.003, val_acc:0.157]
Epoch [2/120    avg_loss:1.640, val_acc:0.394]
Epoch [3/120    avg_loss:1.387, val_acc:0.600]
Epoch [4/120    avg_loss:1.236, val_acc:0.710]
Epoch [5/120    avg_loss:1.043, val_acc:0.683]
Epoch [6/120    avg_loss:0.888, val_acc:0.702]
Epoch [7/120    avg_loss:0.838, val_acc:0.696]
Epoch [8/120    avg_loss:0.747, val_acc:0.635]
Epoch [9/120    avg_loss:0.664, val_acc:0.599]
Epoch [10/120    avg_loss:0.566, val_acc:0.624]
Epoch [11/120    avg_loss:0.510, val_acc:0.753]
Epoch [12/120    avg_loss:0.440, val_acc:0.725]
Epoch [13/120    avg_loss:0.394, val_acc:0.764]
Epoch [14/120    avg_loss:0.359, val_acc:0.780]
Epoch [15/120    avg_loss:0.391, val_acc:0.808]
Epoch [16/120    avg_loss:0.341, val_acc:0.804]
Epoch [17/120    avg_loss:0.281, val_acc:0.891]
Epoch [18/120    avg_loss:0.233, val_acc:0.880]
Epoch [19/120    avg_loss:0.280, val_acc:0.810]
Epoch [20/120    avg_loss:0.273, val_acc:0.825]
Epoch [21/120    avg_loss:0.255, val_acc:0.913]
Epoch [22/120    avg_loss:0.214, val_acc:0.918]
Epoch [23/120    avg_loss:0.173, val_acc:0.932]
Epoch [24/120    avg_loss:0.174, val_acc:0.931]
Epoch [25/120    avg_loss:0.156, val_acc:0.938]
Epoch [26/120    avg_loss:0.132, val_acc:0.932]
Epoch [27/120    avg_loss:0.171, val_acc:0.938]
Epoch [28/120    avg_loss:0.128, val_acc:0.932]
Epoch [29/120    avg_loss:0.150, val_acc:0.938]
Epoch [30/120    avg_loss:0.137, val_acc:0.948]
Epoch [31/120    avg_loss:0.116, val_acc:0.955]
Epoch [32/120    avg_loss:0.079, val_acc:0.911]
Epoch [33/120    avg_loss:0.084, val_acc:0.959]
Epoch [34/120    avg_loss:0.069, val_acc:0.958]
Epoch [35/120    avg_loss:0.068, val_acc:0.965]
Epoch [36/120    avg_loss:0.062, val_acc:0.961]
Epoch [37/120    avg_loss:0.086, val_acc:0.877]
Epoch [38/120    avg_loss:0.091, val_acc:0.956]
Epoch [39/120    avg_loss:0.093, val_acc:0.901]
Epoch [40/120    avg_loss:0.068, val_acc:0.958]
Epoch [41/120    avg_loss:0.057, val_acc:0.969]
Epoch [42/120    avg_loss:0.043, val_acc:0.974]
Epoch [43/120    avg_loss:0.054, val_acc:0.974]
Epoch [44/120    avg_loss:0.050, val_acc:0.973]
Epoch [45/120    avg_loss:0.058, val_acc:0.960]
Epoch [46/120    avg_loss:0.045, val_acc:0.976]
Epoch [47/120    avg_loss:0.044, val_acc:0.973]
Epoch [48/120    avg_loss:0.040, val_acc:0.959]
Epoch [49/120    avg_loss:0.033, val_acc:0.957]
Epoch [50/120    avg_loss:0.034, val_acc:0.976]
Epoch [51/120    avg_loss:0.028, val_acc:0.972]
Epoch [52/120    avg_loss:0.038, val_acc:0.974]
Epoch [53/120    avg_loss:0.048, val_acc:0.978]
Epoch [54/120    avg_loss:0.029, val_acc:0.976]
Epoch [55/120    avg_loss:0.024, val_acc:0.970]
Epoch [56/120    avg_loss:0.020, val_acc:0.972]
Epoch [57/120    avg_loss:0.037, val_acc:0.974]
Epoch [58/120    avg_loss:0.053, val_acc:0.968]
Epoch [59/120    avg_loss:0.029, val_acc:0.968]
Epoch [60/120    avg_loss:0.023, val_acc:0.980]
Epoch [61/120    avg_loss:0.019, val_acc:0.980]
Epoch [62/120    avg_loss:0.017, val_acc:0.969]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.023, val_acc:0.965]
Epoch [65/120    avg_loss:0.037, val_acc:0.947]
Epoch [66/120    avg_loss:0.056, val_acc:0.969]
Epoch [67/120    avg_loss:0.040, val_acc:0.973]
Epoch [68/120    avg_loss:0.032, val_acc:0.970]
Epoch [69/120    avg_loss:0.023, val_acc:0.975]
Epoch [70/120    avg_loss:0.030, val_acc:0.970]
Epoch [71/120    avg_loss:0.027, val_acc:0.981]
Epoch [72/120    avg_loss:0.014, val_acc:0.985]
Epoch [73/120    avg_loss:0.015, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.026, val_acc:0.985]
Epoch [76/120    avg_loss:0.014, val_acc:0.987]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.011, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.976]
Epoch [80/120    avg_loss:0.010, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.024, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.968]
Epoch [85/120    avg_loss:0.017, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.014, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.972]
Epoch [89/120    avg_loss:0.014, val_acc:0.981]
Epoch [90/120    avg_loss:0.015, val_acc:0.986]
Epoch [91/120    avg_loss:0.012, val_acc:0.978]
Epoch [92/120    avg_loss:0.012, val_acc:0.984]
Epoch [93/120    avg_loss:0.013, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     1     1     0     0     3    54     8]
 [    0     4 18037     0    40     0     4     0     5     0]
 [    0     3     0  1962     0     0     0     0    68     3]
 [    0    25     1     1  2916     0     0     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    48     0     0     0  4824     0     6     0]
 [    0    17     0     0     0     0     0  1263     0    10]
 [    0    16     0    71    44     0     0     0  3435     5]
 [    0     0     0     0     0    28     0     0     0   891]]

Accuracy:
98.80702769141783

F1 scores:
[       nan 0.98973721 0.99718045 0.96389094 0.97639377 0.9893859
 0.99402431 0.98826291 0.95842634 0.97058824]

Kappa:
0.9841942013575652
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8bf42b79e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.051, val_acc:0.117]
Epoch [2/120    avg_loss:1.691, val_acc:0.166]
Epoch [3/120    avg_loss:1.438, val_acc:0.316]
Epoch [4/120    avg_loss:1.290, val_acc:0.394]
Epoch [5/120    avg_loss:1.144, val_acc:0.435]
Epoch [6/120    avg_loss:1.057, val_acc:0.488]
Epoch [7/120    avg_loss:0.920, val_acc:0.524]
Epoch [8/120    avg_loss:0.842, val_acc:0.766]
Epoch [9/120    avg_loss:0.761, val_acc:0.603]
Epoch [10/120    avg_loss:0.667, val_acc:0.726]
Epoch [11/120    avg_loss:0.593, val_acc:0.659]
Epoch [12/120    avg_loss:0.558, val_acc:0.708]
Epoch [13/120    avg_loss:0.455, val_acc:0.728]
Epoch [14/120    avg_loss:0.443, val_acc:0.755]
Epoch [15/120    avg_loss:0.390, val_acc:0.736]
Epoch [16/120    avg_loss:0.362, val_acc:0.775]
Epoch [17/120    avg_loss:0.340, val_acc:0.758]
Epoch [18/120    avg_loss:0.285, val_acc:0.885]
Epoch [19/120    avg_loss:0.263, val_acc:0.885]
Epoch [20/120    avg_loss:0.260, val_acc:0.903]
Epoch [21/120    avg_loss:0.210, val_acc:0.876]
Epoch [22/120    avg_loss:0.218, val_acc:0.931]
Epoch [23/120    avg_loss:0.185, val_acc:0.932]
Epoch [24/120    avg_loss:0.177, val_acc:0.957]
Epoch [25/120    avg_loss:0.159, val_acc:0.956]
Epoch [26/120    avg_loss:0.132, val_acc:0.939]
Epoch [27/120    avg_loss:0.145, val_acc:0.971]
Epoch [28/120    avg_loss:0.132, val_acc:0.867]
Epoch [29/120    avg_loss:0.115, val_acc:0.910]
Epoch [30/120    avg_loss:0.129, val_acc:0.955]
Epoch [31/120    avg_loss:0.115, val_acc:0.946]
Epoch [32/120    avg_loss:0.110, val_acc:0.951]
Epoch [33/120    avg_loss:0.098, val_acc:0.961]
Epoch [34/120    avg_loss:0.087, val_acc:0.936]
Epoch [35/120    avg_loss:0.108, val_acc:0.977]
Epoch [36/120    avg_loss:0.056, val_acc:0.979]
Epoch [37/120    avg_loss:0.057, val_acc:0.961]
Epoch [38/120    avg_loss:0.082, val_acc:0.977]
Epoch [39/120    avg_loss:0.069, val_acc:0.959]
Epoch [40/120    avg_loss:0.075, val_acc:0.972]
Epoch [41/120    avg_loss:0.063, val_acc:0.982]
Epoch [42/120    avg_loss:0.041, val_acc:0.973]
Epoch [43/120    avg_loss:0.051, val_acc:0.981]
Epoch [44/120    avg_loss:0.043, val_acc:0.970]
Epoch [45/120    avg_loss:0.037, val_acc:0.983]
Epoch [46/120    avg_loss:0.040, val_acc:0.984]
Epoch [47/120    avg_loss:0.035, val_acc:0.982]
Epoch [48/120    avg_loss:0.035, val_acc:0.959]
Epoch [49/120    avg_loss:0.051, val_acc:0.974]
Epoch [50/120    avg_loss:0.063, val_acc:0.981]
Epoch [51/120    avg_loss:0.039, val_acc:0.977]
Epoch [52/120    avg_loss:0.058, val_acc:0.959]
Epoch [53/120    avg_loss:0.053, val_acc:0.969]
Epoch [54/120    avg_loss:0.066, val_acc:0.932]
Epoch [55/120    avg_loss:0.059, val_acc:0.983]
Epoch [56/120    avg_loss:0.047, val_acc:0.989]
Epoch [57/120    avg_loss:0.031, val_acc:0.967]
Epoch [58/120    avg_loss:0.031, val_acc:0.975]
Epoch [59/120    avg_loss:0.030, val_acc:0.981]
Epoch [60/120    avg_loss:0.032, val_acc:0.984]
Epoch [61/120    avg_loss:0.024, val_acc:0.944]
Epoch [62/120    avg_loss:0.019, val_acc:0.987]
Epoch [63/120    avg_loss:0.014, val_acc:0.988]
Epoch [64/120    avg_loss:0.013, val_acc:0.990]
Epoch [65/120    avg_loss:0.012, val_acc:0.991]
Epoch [66/120    avg_loss:0.015, val_acc:0.985]
Epoch [67/120    avg_loss:0.012, val_acc:0.988]
Epoch [68/120    avg_loss:0.015, val_acc:0.989]
Epoch [69/120    avg_loss:0.018, val_acc:0.987]
Epoch [70/120    avg_loss:0.021, val_acc:0.990]
Epoch [71/120    avg_loss:0.013, val_acc:0.988]
Epoch [72/120    avg_loss:0.018, val_acc:0.986]
Epoch [73/120    avg_loss:0.019, val_acc:0.986]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.991]
Epoch [80/120    avg_loss:0.009, val_acc:0.992]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.008, val_acc:0.992]
Epoch [83/120    avg_loss:0.007, val_acc:0.992]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.007, val_acc:0.992]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.006, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.006, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.007, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.006, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.008, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.993]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.007, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.993]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.006, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     0     0     1    15    37     0]
 [    0     0 18059     0    22     0     6     0     3     0]
 [    0     1     0  1943     0     0     0     0    89     3]
 [    0    35     8     1  2915     0     2     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    33     2     5     0  4835     0     3     0]
 [    0     3     0     0     0     6     0  1281     0     0]
 [    0     6     0    42    40     0     0     0  3482     1]
 [    0     0     0     0     4    10     0     0     0   905]]

Accuracy:
99.06249246860916

F1 scores:
[       nan 0.9923771  0.9980105  0.96570577 0.97851628 0.99390708
 0.99465131 0.99071926 0.96775987 0.99015317]

Kappa:
0.9875763140322262
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f343345b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.116, val_acc:0.653]
Epoch [2/120    avg_loss:1.698, val_acc:0.628]
Epoch [3/120    avg_loss:1.469, val_acc:0.503]
Epoch [4/120    avg_loss:1.251, val_acc:0.407]
Epoch [5/120    avg_loss:1.046, val_acc:0.555]
Epoch [6/120    avg_loss:0.881, val_acc:0.656]
Epoch [7/120    avg_loss:0.727, val_acc:0.702]
Epoch [8/120    avg_loss:0.593, val_acc:0.737]
Epoch [9/120    avg_loss:0.512, val_acc:0.767]
Epoch [10/120    avg_loss:0.424, val_acc:0.814]
Epoch [11/120    avg_loss:0.380, val_acc:0.785]
Epoch [12/120    avg_loss:0.341, val_acc:0.853]
Epoch [13/120    avg_loss:0.339, val_acc:0.852]
Epoch [14/120    avg_loss:0.300, val_acc:0.873]
Epoch [15/120    avg_loss:0.257, val_acc:0.867]
Epoch [16/120    avg_loss:0.248, val_acc:0.903]
Epoch [17/120    avg_loss:0.210, val_acc:0.935]
Epoch [18/120    avg_loss:0.200, val_acc:0.927]
Epoch [19/120    avg_loss:0.191, val_acc:0.936]
Epoch [20/120    avg_loss:0.166, val_acc:0.923]
Epoch [21/120    avg_loss:0.150, val_acc:0.950]
Epoch [22/120    avg_loss:0.132, val_acc:0.949]
Epoch [23/120    avg_loss:0.139, val_acc:0.934]
Epoch [24/120    avg_loss:0.116, val_acc:0.950]
Epoch [25/120    avg_loss:0.107, val_acc:0.948]
Epoch [26/120    avg_loss:0.127, val_acc:0.938]
Epoch [27/120    avg_loss:0.113, val_acc:0.959]
Epoch [28/120    avg_loss:0.078, val_acc:0.943]
Epoch [29/120    avg_loss:0.081, val_acc:0.948]
Epoch [30/120    avg_loss:0.092, val_acc:0.965]
Epoch [31/120    avg_loss:0.078, val_acc:0.967]
Epoch [32/120    avg_loss:0.081, val_acc:0.976]
Epoch [33/120    avg_loss:0.089, val_acc:0.944]
Epoch [34/120    avg_loss:0.133, val_acc:0.941]
Epoch [35/120    avg_loss:0.095, val_acc:0.952]
Epoch [36/120    avg_loss:0.067, val_acc:0.972]
Epoch [37/120    avg_loss:0.061, val_acc:0.979]
Epoch [38/120    avg_loss:0.050, val_acc:0.970]
Epoch [39/120    avg_loss:0.067, val_acc:0.965]
Epoch [40/120    avg_loss:0.053, val_acc:0.977]
Epoch [41/120    avg_loss:0.043, val_acc:0.972]
Epoch [42/120    avg_loss:0.057, val_acc:0.976]
Epoch [43/120    avg_loss:0.058, val_acc:0.981]
Epoch [44/120    avg_loss:0.031, val_acc:0.986]
Epoch [45/120    avg_loss:0.027, val_acc:0.986]
Epoch [46/120    avg_loss:0.025, val_acc:0.979]
Epoch [47/120    avg_loss:0.025, val_acc:0.985]
Epoch [48/120    avg_loss:0.025, val_acc:0.983]
Epoch [49/120    avg_loss:0.024, val_acc:0.985]
Epoch [50/120    avg_loss:0.017, val_acc:0.986]
Epoch [51/120    avg_loss:0.018, val_acc:0.977]
Epoch [52/120    avg_loss:0.021, val_acc:0.982]
Epoch [53/120    avg_loss:0.030, val_acc:0.978]
Epoch [54/120    avg_loss:0.022, val_acc:0.983]
Epoch [55/120    avg_loss:0.016, val_acc:0.979]
Epoch [56/120    avg_loss:0.025, val_acc:0.968]
Epoch [57/120    avg_loss:0.035, val_acc:0.978]
Epoch [58/120    avg_loss:0.022, val_acc:0.982]
Epoch [59/120    avg_loss:0.018, val_acc:0.984]
Epoch [60/120    avg_loss:0.018, val_acc:0.982]
Epoch [61/120    avg_loss:0.014, val_acc:0.984]
Epoch [62/120    avg_loss:0.012, val_acc:0.987]
Epoch [63/120    avg_loss:0.024, val_acc:0.982]
Epoch [64/120    avg_loss:0.024, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.014, val_acc:0.981]
Epoch [68/120    avg_loss:0.009, val_acc:0.985]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.988]
Epoch [71/120    avg_loss:0.010, val_acc:0.982]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.006, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.982]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.970]
Epoch [81/120    avg_loss:0.012, val_acc:0.989]
Epoch [82/120    avg_loss:0.009, val_acc:0.976]
Epoch [83/120    avg_loss:0.034, val_acc:0.983]
Epoch [84/120    avg_loss:0.028, val_acc:0.976]
Epoch [85/120    avg_loss:0.023, val_acc:0.976]
Epoch [86/120    avg_loss:0.029, val_acc:0.969]
Epoch [87/120    avg_loss:0.054, val_acc:0.971]
Epoch [88/120    avg_loss:0.024, val_acc:0.985]
Epoch [89/120    avg_loss:0.032, val_acc:0.938]
Epoch [90/120    avg_loss:0.040, val_acc:0.971]
Epoch [91/120    avg_loss:0.027, val_acc:0.986]
Epoch [92/120    avg_loss:0.029, val_acc:0.980]
Epoch [93/120    avg_loss:0.016, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.014, val_acc:0.981]
Epoch [97/120    avg_loss:0.036, val_acc:0.965]
Epoch [98/120    avg_loss:0.035, val_acc:0.974]
Epoch [99/120    avg_loss:0.027, val_acc:0.982]
Epoch [100/120    avg_loss:0.022, val_acc:0.980]
Epoch [101/120    avg_loss:0.062, val_acc:0.969]
Epoch [102/120    avg_loss:0.037, val_acc:0.972]
Epoch [103/120    avg_loss:0.056, val_acc:0.975]
Epoch [104/120    avg_loss:0.024, val_acc:0.983]
Epoch [105/120    avg_loss:0.014, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.975]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.991]
Epoch [120/120    avg_loss:0.007, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6330     0     1     0     0     0     3    89     9]
 [    0     0 18062     0    27     0     1     0     0     0]
 [    0     1     0  1910     0     0     0     0   117     8]
 [    0    26     9     0  2898     0    19     0    16     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     0     0     0  4853     0     3     0]
 [    0     5     0     0     0     0     3  1278     0     4]
 [    0     1     0    86    53     0     8     0  3407    16]
 [    0     0     0     0    15    28     0     0     0   876]]

Accuracy:
98.61663413105825

F1 scores:
[       nan 0.989449   0.9983694  0.94718572 0.97166806 0.9893859
 0.99426347 0.99416569 0.94599472 0.95424837]

Kappa:
0.9816714898650432
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f18e41f8940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.030, val_acc:0.171]
Epoch [2/120    avg_loss:1.695, val_acc:0.484]
Epoch [3/120    avg_loss:1.450, val_acc:0.492]
Epoch [4/120    avg_loss:1.235, val_acc:0.454]
Epoch [5/120    avg_loss:1.062, val_acc:0.606]
Epoch [6/120    avg_loss:0.968, val_acc:0.748]
Epoch [7/120    avg_loss:0.887, val_acc:0.729]
Epoch [8/120    avg_loss:0.795, val_acc:0.690]
Epoch [9/120    avg_loss:0.726, val_acc:0.766]
Epoch [10/120    avg_loss:0.655, val_acc:0.747]
Epoch [11/120    avg_loss:0.607, val_acc:0.715]
Epoch [12/120    avg_loss:0.562, val_acc:0.763]
Epoch [13/120    avg_loss:0.471, val_acc:0.773]
Epoch [14/120    avg_loss:0.436, val_acc:0.787]
Epoch [15/120    avg_loss:0.368, val_acc:0.859]
Epoch [16/120    avg_loss:0.334, val_acc:0.868]
Epoch [17/120    avg_loss:0.276, val_acc:0.860]
Epoch [18/120    avg_loss:0.283, val_acc:0.830]
Epoch [19/120    avg_loss:0.216, val_acc:0.918]
Epoch [20/120    avg_loss:0.197, val_acc:0.924]
Epoch [21/120    avg_loss:0.240, val_acc:0.939]
Epoch [22/120    avg_loss:0.183, val_acc:0.920]
Epoch [23/120    avg_loss:0.164, val_acc:0.887]
Epoch [24/120    avg_loss:0.151, val_acc:0.952]
Epoch [25/120    avg_loss:0.163, val_acc:0.933]
Epoch [26/120    avg_loss:0.145, val_acc:0.958]
Epoch [27/120    avg_loss:0.120, val_acc:0.952]
Epoch [28/120    avg_loss:0.142, val_acc:0.911]
Epoch [29/120    avg_loss:0.107, val_acc:0.951]
Epoch [30/120    avg_loss:0.133, val_acc:0.935]
Epoch [31/120    avg_loss:0.119, val_acc:0.956]
Epoch [32/120    avg_loss:0.099, val_acc:0.946]
Epoch [33/120    avg_loss:0.080, val_acc:0.966]
Epoch [34/120    avg_loss:0.072, val_acc:0.954]
Epoch [35/120    avg_loss:0.067, val_acc:0.970]
Epoch [36/120    avg_loss:0.054, val_acc:0.956]
Epoch [37/120    avg_loss:0.041, val_acc:0.970]
Epoch [38/120    avg_loss:0.059, val_acc:0.967]
Epoch [39/120    avg_loss:0.065, val_acc:0.949]
Epoch [40/120    avg_loss:0.097, val_acc:0.927]
Epoch [41/120    avg_loss:0.057, val_acc:0.964]
Epoch [42/120    avg_loss:0.054, val_acc:0.973]
Epoch [43/120    avg_loss:0.061, val_acc:0.933]
Epoch [44/120    avg_loss:0.051, val_acc:0.961]
Epoch [45/120    avg_loss:0.054, val_acc:0.955]
Epoch [46/120    avg_loss:0.086, val_acc:0.970]
Epoch [47/120    avg_loss:0.053, val_acc:0.968]
Epoch [48/120    avg_loss:0.697, val_acc:0.737]
Epoch [49/120    avg_loss:0.351, val_acc:0.818]
Epoch [50/120    avg_loss:0.172, val_acc:0.937]
Epoch [51/120    avg_loss:0.141, val_acc:0.949]
Epoch [52/120    avg_loss:0.122, val_acc:0.953]
Epoch [53/120    avg_loss:0.138, val_acc:0.944]
Epoch [54/120    avg_loss:0.095, val_acc:0.936]
Epoch [55/120    avg_loss:0.083, val_acc:0.973]
Epoch [56/120    avg_loss:0.073, val_acc:0.970]
Epoch [57/120    avg_loss:0.062, val_acc:0.969]
Epoch [58/120    avg_loss:0.063, val_acc:0.955]
Epoch [59/120    avg_loss:0.047, val_acc:0.975]
Epoch [60/120    avg_loss:0.035, val_acc:0.973]
Epoch [61/120    avg_loss:0.033, val_acc:0.975]
Epoch [62/120    avg_loss:0.033, val_acc:0.972]
Epoch [63/120    avg_loss:0.063, val_acc:0.956]
Epoch [64/120    avg_loss:0.046, val_acc:0.971]
Epoch [65/120    avg_loss:0.052, val_acc:0.970]
Epoch [66/120    avg_loss:0.041, val_acc:0.971]
Epoch [67/120    avg_loss:0.026, val_acc:0.976]
Epoch [68/120    avg_loss:0.045, val_acc:0.973]
Epoch [69/120    avg_loss:0.031, val_acc:0.981]
Epoch [70/120    avg_loss:0.049, val_acc:0.969]
Epoch [71/120    avg_loss:0.056, val_acc:0.959]
Epoch [72/120    avg_loss:0.040, val_acc:0.964]
Epoch [73/120    avg_loss:0.026, val_acc:0.976]
Epoch [74/120    avg_loss:0.022, val_acc:0.975]
Epoch [75/120    avg_loss:0.016, val_acc:0.979]
Epoch [76/120    avg_loss:0.019, val_acc:0.978]
Epoch [77/120    avg_loss:0.018, val_acc:0.978]
Epoch [78/120    avg_loss:0.023, val_acc:0.980]
Epoch [79/120    avg_loss:0.025, val_acc:0.975]
Epoch [80/120    avg_loss:0.025, val_acc:0.975]
Epoch [81/120    avg_loss:0.022, val_acc:0.977]
Epoch [82/120    avg_loss:0.027, val_acc:0.970]
Epoch [83/120    avg_loss:0.022, val_acc:0.973]
Epoch [84/120    avg_loss:0.015, val_acc:0.979]
Epoch [85/120    avg_loss:0.011, val_acc:0.980]
Epoch [86/120    avg_loss:0.013, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.979]
Epoch [88/120    avg_loss:0.009, val_acc:0.979]
Epoch [89/120    avg_loss:0.011, val_acc:0.979]
Epoch [90/120    avg_loss:0.009, val_acc:0.979]
Epoch [91/120    avg_loss:0.009, val_acc:0.980]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.980]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.009, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.979]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.982]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.007, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6376     0     0     0     0     0     2    51     3]
 [    0     0 18062     0    13     0    15     0     0     0]
 [    0     2     0  1954     0     0     0     0    79     1]
 [    0    30     1     0  2913     0    13     0    12     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    26     3     1     0  4845     0     3     0]
 [    0     4     0     0     0     1     2  1282     0     1]
 [    0     9     0    33    60     0     0     0  3466     3]
 [    0     0     0     0     6    17     0     0     0   896]]

Accuracy:
99.05044224326996

F1 scores:
[       nan 0.99214191 0.99847978 0.97069051 0.9766974  0.99315068
 0.99354045 0.996115   0.96519075 0.98138007]

Kappa:
0.9874176617911274
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f337e8978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.027, val_acc:0.122]
Epoch [2/120    avg_loss:1.650, val_acc:0.144]
Epoch [3/120    avg_loss:1.460, val_acc:0.274]
Epoch [4/120    avg_loss:1.306, val_acc:0.326]
Epoch [5/120    avg_loss:1.137, val_acc:0.416]
Epoch [6/120    avg_loss:1.024, val_acc:0.447]
Epoch [7/120    avg_loss:0.887, val_acc:0.476]
Epoch [8/120    avg_loss:0.814, val_acc:0.643]
Epoch [9/120    avg_loss:0.721, val_acc:0.573]
Epoch [10/120    avg_loss:0.661, val_acc:0.643]
Epoch [11/120    avg_loss:0.562, val_acc:0.641]
Epoch [12/120    avg_loss:0.489, val_acc:0.635]
Epoch [13/120    avg_loss:0.481, val_acc:0.693]
Epoch [14/120    avg_loss:0.420, val_acc:0.802]
Epoch [15/120    avg_loss:0.383, val_acc:0.789]
Epoch [16/120    avg_loss:0.314, val_acc:0.851]
Epoch [17/120    avg_loss:0.282, val_acc:0.833]
Epoch [18/120    avg_loss:0.239, val_acc:0.903]
Epoch [19/120    avg_loss:0.228, val_acc:0.883]
Epoch [20/120    avg_loss:0.195, val_acc:0.876]
Epoch [21/120    avg_loss:0.190, val_acc:0.894]
Epoch [22/120    avg_loss:0.180, val_acc:0.927]
Epoch [23/120    avg_loss:0.155, val_acc:0.908]
Epoch [24/120    avg_loss:0.182, val_acc:0.939]
Epoch [25/120    avg_loss:0.129, val_acc:0.947]
Epoch [26/120    avg_loss:0.113, val_acc:0.939]
Epoch [27/120    avg_loss:0.098, val_acc:0.956]
Epoch [28/120    avg_loss:0.103, val_acc:0.948]
Epoch [29/120    avg_loss:0.105, val_acc:0.927]
Epoch [30/120    avg_loss:0.116, val_acc:0.894]
Epoch [31/120    avg_loss:0.447, val_acc:0.831]
Epoch [32/120    avg_loss:0.275, val_acc:0.883]
Epoch [33/120    avg_loss:0.158, val_acc:0.945]
Epoch [34/120    avg_loss:0.133, val_acc:0.933]
Epoch [35/120    avg_loss:0.099, val_acc:0.953]
Epoch [36/120    avg_loss:0.078, val_acc:0.945]
Epoch [37/120    avg_loss:0.082, val_acc:0.954]
Epoch [38/120    avg_loss:0.071, val_acc:0.960]
Epoch [39/120    avg_loss:0.061, val_acc:0.954]
Epoch [40/120    avg_loss:0.080, val_acc:0.959]
Epoch [41/120    avg_loss:0.062, val_acc:0.943]
Epoch [42/120    avg_loss:0.068, val_acc:0.945]
Epoch [43/120    avg_loss:0.060, val_acc:0.970]
Epoch [44/120    avg_loss:0.047, val_acc:0.965]
Epoch [45/120    avg_loss:0.224, val_acc:0.773]
Epoch [46/120    avg_loss:0.314, val_acc:0.891]
Epoch [47/120    avg_loss:0.164, val_acc:0.888]
Epoch [48/120    avg_loss:0.121, val_acc:0.941]
Epoch [49/120    avg_loss:0.093, val_acc:0.951]
Epoch [50/120    avg_loss:0.117, val_acc:0.950]
Epoch [51/120    avg_loss:0.060, val_acc:0.956]
Epoch [52/120    avg_loss:0.065, val_acc:0.961]
Epoch [53/120    avg_loss:0.058, val_acc:0.963]
Epoch [54/120    avg_loss:0.038, val_acc:0.969]
Epoch [55/120    avg_loss:0.031, val_acc:0.975]
Epoch [56/120    avg_loss:0.057, val_acc:0.959]
Epoch [57/120    avg_loss:0.035, val_acc:0.976]
Epoch [58/120    avg_loss:0.032, val_acc:0.965]
Epoch [59/120    avg_loss:0.042, val_acc:0.976]
Epoch [60/120    avg_loss:0.030, val_acc:0.976]
Epoch [61/120    avg_loss:0.026, val_acc:0.976]
Epoch [62/120    avg_loss:0.034, val_acc:0.975]
Epoch [63/120    avg_loss:0.033, val_acc:0.979]
Epoch [64/120    avg_loss:0.018, val_acc:0.970]
Epoch [65/120    avg_loss:0.030, val_acc:0.965]
Epoch [66/120    avg_loss:0.022, val_acc:0.967]
Epoch [67/120    avg_loss:0.023, val_acc:0.970]
Epoch [68/120    avg_loss:0.027, val_acc:0.970]
Epoch [69/120    avg_loss:0.026, val_acc:0.965]
Epoch [70/120    avg_loss:0.027, val_acc:0.975]
Epoch [71/120    avg_loss:0.021, val_acc:0.972]
Epoch [72/120    avg_loss:0.017, val_acc:0.983]
Epoch [73/120    avg_loss:0.017, val_acc:0.975]
Epoch [74/120    avg_loss:0.027, val_acc:0.954]
Epoch [75/120    avg_loss:0.021, val_acc:0.981]
Epoch [76/120    avg_loss:0.017, val_acc:0.984]
Epoch [77/120    avg_loss:0.016, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.048, val_acc:0.962]
Epoch [81/120    avg_loss:0.056, val_acc:0.969]
Epoch [82/120    avg_loss:0.021, val_acc:0.980]
Epoch [83/120    avg_loss:0.015, val_acc:0.968]
Epoch [84/120    avg_loss:0.020, val_acc:0.971]
Epoch [85/120    avg_loss:0.023, val_acc:0.980]
Epoch [86/120    avg_loss:0.018, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.959]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.009, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     7     0     0     0     0    68     0]
 [    0     2 18034     0    24     0    30     0     0     0]
 [    0     3     0  1986     0     0     0     0    40     7]
 [    0    13     5     0  2930     0    11     0    11     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     0     0     0  4850     0    11     0]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    28     0    28    45     0     0     0  3470     0]
 [    0     0     0     0    12     8     0     0     0   899]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.99018692 0.99784208 0.97904856 0.97944175 0.99694423
 0.99293684 0.99805825 0.96778692 0.98412698]

Kappa:
0.9879655195801345
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c7c93d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.993, val_acc:0.091]
Epoch [2/120    avg_loss:1.648, val_acc:0.290]
Epoch [3/120    avg_loss:1.390, val_acc:0.340]
Epoch [4/120    avg_loss:1.197, val_acc:0.336]
Epoch [5/120    avg_loss:1.051, val_acc:0.481]
Epoch [6/120    avg_loss:0.947, val_acc:0.472]
Epoch [7/120    avg_loss:0.809, val_acc:0.504]
Epoch [8/120    avg_loss:0.719, val_acc:0.586]
Epoch [9/120    avg_loss:0.660, val_acc:0.600]
Epoch [10/120    avg_loss:0.566, val_acc:0.720]
Epoch [11/120    avg_loss:0.500, val_acc:0.703]
Epoch [12/120    avg_loss:0.444, val_acc:0.726]
Epoch [13/120    avg_loss:0.414, val_acc:0.754]
Epoch [14/120    avg_loss:0.416, val_acc:0.808]
Epoch [15/120    avg_loss:0.390, val_acc:0.715]
Epoch [16/120    avg_loss:0.337, val_acc:0.822]
Epoch [17/120    avg_loss:0.330, val_acc:0.820]
Epoch [18/120    avg_loss:0.294, val_acc:0.812]
Epoch [19/120    avg_loss:0.291, val_acc:0.727]
Epoch [20/120    avg_loss:0.251, val_acc:0.826]
Epoch [21/120    avg_loss:0.238, val_acc:0.803]
Epoch [22/120    avg_loss:0.295, val_acc:0.771]
Epoch [23/120    avg_loss:0.242, val_acc:0.840]
Epoch [24/120    avg_loss:0.203, val_acc:0.896]
Epoch [25/120    avg_loss:0.176, val_acc:0.928]
Epoch [26/120    avg_loss:0.169, val_acc:0.914]
Epoch [27/120    avg_loss:0.159, val_acc:0.911]
Epoch [28/120    avg_loss:0.167, val_acc:0.911]
Epoch [29/120    avg_loss:0.127, val_acc:0.936]
Epoch [30/120    avg_loss:0.112, val_acc:0.924]
Epoch [31/120    avg_loss:0.135, val_acc:0.910]
Epoch [32/120    avg_loss:0.161, val_acc:0.949]
Epoch [33/120    avg_loss:0.101, val_acc:0.950]
Epoch [34/120    avg_loss:0.087, val_acc:0.951]
Epoch [35/120    avg_loss:0.103, val_acc:0.953]
Epoch [36/120    avg_loss:0.081, val_acc:0.943]
Epoch [37/120    avg_loss:0.089, val_acc:0.929]
Epoch [38/120    avg_loss:0.078, val_acc:0.949]
Epoch [39/120    avg_loss:0.095, val_acc:0.939]
Epoch [40/120    avg_loss:0.077, val_acc:0.956]
Epoch [41/120    avg_loss:0.069, val_acc:0.959]
Epoch [42/120    avg_loss:0.121, val_acc:0.947]
Epoch [43/120    avg_loss:0.105, val_acc:0.945]
Epoch [44/120    avg_loss:0.090, val_acc:0.887]
Epoch [45/120    avg_loss:0.057, val_acc:0.965]
Epoch [46/120    avg_loss:0.051, val_acc:0.947]
Epoch [47/120    avg_loss:0.049, val_acc:0.961]
Epoch [48/120    avg_loss:0.037, val_acc:0.962]
Epoch [49/120    avg_loss:0.046, val_acc:0.940]
Epoch [50/120    avg_loss:0.051, val_acc:0.966]
Epoch [51/120    avg_loss:0.054, val_acc:0.936]
Epoch [52/120    avg_loss:0.053, val_acc:0.965]
Epoch [53/120    avg_loss:0.050, val_acc:0.964]
Epoch [54/120    avg_loss:0.048, val_acc:0.970]
Epoch [55/120    avg_loss:0.044, val_acc:0.950]
Epoch [56/120    avg_loss:0.055, val_acc:0.951]
Epoch [57/120    avg_loss:0.047, val_acc:0.970]
Epoch [58/120    avg_loss:0.042, val_acc:0.965]
Epoch [59/120    avg_loss:0.045, val_acc:0.962]
Epoch [60/120    avg_loss:0.026, val_acc:0.973]
Epoch [61/120    avg_loss:0.022, val_acc:0.974]
Epoch [62/120    avg_loss:0.023, val_acc:0.972]
Epoch [63/120    avg_loss:0.051, val_acc:0.912]
Epoch [64/120    avg_loss:0.053, val_acc:0.967]
Epoch [65/120    avg_loss:0.035, val_acc:0.963]
Epoch [66/120    avg_loss:0.027, val_acc:0.979]
Epoch [67/120    avg_loss:0.022, val_acc:0.973]
Epoch [68/120    avg_loss:0.016, val_acc:0.969]
Epoch [69/120    avg_loss:0.028, val_acc:0.970]
Epoch [70/120    avg_loss:0.021, val_acc:0.973]
Epoch [71/120    avg_loss:0.016, val_acc:0.951]
Epoch [72/120    avg_loss:0.023, val_acc:0.975]
Epoch [73/120    avg_loss:0.021, val_acc:0.964]
Epoch [74/120    avg_loss:0.026, val_acc:0.970]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.953]
Epoch [77/120    avg_loss:0.019, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.966]
Epoch [79/120    avg_loss:0.010, val_acc:0.966]
Epoch [80/120    avg_loss:0.012, val_acc:0.972]
Epoch [81/120    avg_loss:0.036, val_acc:0.933]
Epoch [82/120    avg_loss:0.025, val_acc:0.968]
Epoch [83/120    avg_loss:0.013, val_acc:0.972]
Epoch [84/120    avg_loss:0.013, val_acc:0.975]
Epoch [85/120    avg_loss:0.007, val_acc:0.976]
Epoch [86/120    avg_loss:0.024, val_acc:0.942]
Epoch [87/120    avg_loss:0.015, val_acc:0.969]
Epoch [88/120    avg_loss:0.010, val_acc:0.972]
Epoch [89/120    avg_loss:0.025, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.965]
Epoch [91/120    avg_loss:0.013, val_acc:0.975]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.979]
Epoch [95/120    avg_loss:0.011, val_acc:0.977]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.007, val_acc:0.977]
Epoch [98/120    avg_loss:0.010, val_acc:0.977]
Epoch [99/120    avg_loss:0.008, val_acc:0.978]
Epoch [100/120    avg_loss:0.007, val_acc:0.977]
Epoch [101/120    avg_loss:0.006, val_acc:0.978]
Epoch [102/120    avg_loss:0.007, val_acc:0.979]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.008, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.005, val_acc:0.978]
Epoch [109/120    avg_loss:0.005, val_acc:0.977]
Epoch [110/120    avg_loss:0.005, val_acc:0.977]
Epoch [111/120    avg_loss:0.008, val_acc:0.977]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.005, val_acc:0.977]
Epoch [114/120    avg_loss:0.006, val_acc:0.979]
Epoch [115/120    avg_loss:0.007, val_acc:0.978]
Epoch [116/120    avg_loss:0.005, val_acc:0.979]
Epoch [117/120    avg_loss:0.006, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.979]
Epoch [120/120    avg_loss:0.005, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6346     0     0     0     0     0     9    70     7]
 [    0     1 18051     0    25     0    11     0     2     0]
 [    0     4     0  1965     0     0     0     0    62     5]
 [    0    20     2     1  2938     0     7     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     5     0     0  4853     0     0     0]
 [    0     4     0     0     0    24     0  1257     0     5]
 [    0    44     0    28    35     0     0     0  3462     2]
 [    0     0     0     0     3    24     0     0     0   892]]

Accuracy:
98.97814089123467

F1 scores:
[       nan 0.98762742 0.99831319 0.9739777  0.98376025 0.98194131
 0.99558929 0.98356808 0.96555571 0.97486339]

Kappa:
0.986462735054878
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f2275c9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.049, val_acc:0.186]
Epoch [2/120    avg_loss:1.708, val_acc:0.233]
Epoch [3/120    avg_loss:1.440, val_acc:0.271]
Epoch [4/120    avg_loss:1.236, val_acc:0.340]
Epoch [5/120    avg_loss:1.071, val_acc:0.419]
Epoch [6/120    avg_loss:0.944, val_acc:0.711]
Epoch [7/120    avg_loss:0.806, val_acc:0.737]
Epoch [8/120    avg_loss:0.712, val_acc:0.697]
Epoch [9/120    avg_loss:0.646, val_acc:0.702]
Epoch [10/120    avg_loss:0.592, val_acc:0.733]
Epoch [11/120    avg_loss:0.521, val_acc:0.763]
Epoch [12/120    avg_loss:0.475, val_acc:0.784]
Epoch [13/120    avg_loss:0.384, val_acc:0.836]
Epoch [14/120    avg_loss:0.355, val_acc:0.868]
Epoch [15/120    avg_loss:0.336, val_acc:0.838]
Epoch [16/120    avg_loss:0.290, val_acc:0.886]
Epoch [17/120    avg_loss:0.244, val_acc:0.887]
Epoch [18/120    avg_loss:0.244, val_acc:0.898]
Epoch [19/120    avg_loss:0.196, val_acc:0.944]
Epoch [20/120    avg_loss:0.172, val_acc:0.886]
Epoch [21/120    avg_loss:0.185, val_acc:0.932]
Epoch [22/120    avg_loss:0.143, val_acc:0.956]
Epoch [23/120    avg_loss:0.165, val_acc:0.911]
Epoch [24/120    avg_loss:0.162, val_acc:0.919]
Epoch [25/120    avg_loss:0.147, val_acc:0.931]
Epoch [26/120    avg_loss:0.127, val_acc:0.949]
Epoch [27/120    avg_loss:0.119, val_acc:0.921]
Epoch [28/120    avg_loss:0.098, val_acc:0.951]
Epoch [29/120    avg_loss:0.101, val_acc:0.963]
Epoch [30/120    avg_loss:0.086, val_acc:0.964]
Epoch [31/120    avg_loss:0.078, val_acc:0.965]
Epoch [32/120    avg_loss:0.130, val_acc:0.942]
Epoch [33/120    avg_loss:0.118, val_acc:0.964]
Epoch [34/120    avg_loss:0.069, val_acc:0.966]
Epoch [35/120    avg_loss:0.071, val_acc:0.969]
Epoch [36/120    avg_loss:0.052, val_acc:0.969]
Epoch [37/120    avg_loss:0.058, val_acc:0.968]
Epoch [38/120    avg_loss:0.056, val_acc:0.969]
Epoch [39/120    avg_loss:0.042, val_acc:0.979]
Epoch [40/120    avg_loss:0.056, val_acc:0.955]
Epoch [41/120    avg_loss:0.067, val_acc:0.970]
Epoch [42/120    avg_loss:0.059, val_acc:0.961]
Epoch [43/120    avg_loss:0.065, val_acc:0.949]
Epoch [44/120    avg_loss:0.088, val_acc:0.953]
Epoch [45/120    avg_loss:0.064, val_acc:0.975]
Epoch [46/120    avg_loss:0.051, val_acc:0.976]
Epoch [47/120    avg_loss:0.050, val_acc:0.964]
Epoch [48/120    avg_loss:0.043, val_acc:0.963]
Epoch [49/120    avg_loss:0.030, val_acc:0.975]
Epoch [50/120    avg_loss:0.033, val_acc:0.973]
Epoch [51/120    avg_loss:0.030, val_acc:0.975]
Epoch [52/120    avg_loss:0.027, val_acc:0.976]
Epoch [53/120    avg_loss:0.022, val_acc:0.976]
Epoch [54/120    avg_loss:0.019, val_acc:0.977]
Epoch [55/120    avg_loss:0.013, val_acc:0.976]
Epoch [56/120    avg_loss:0.017, val_acc:0.977]
Epoch [57/120    avg_loss:0.015, val_acc:0.977]
Epoch [58/120    avg_loss:0.023, val_acc:0.979]
Epoch [59/120    avg_loss:0.014, val_acc:0.978]
Epoch [60/120    avg_loss:0.015, val_acc:0.981]
Epoch [61/120    avg_loss:0.014, val_acc:0.979]
Epoch [62/120    avg_loss:0.016, val_acc:0.978]
Epoch [63/120    avg_loss:0.015, val_acc:0.977]
Epoch [64/120    avg_loss:0.013, val_acc:0.979]
Epoch [65/120    avg_loss:0.013, val_acc:0.979]
Epoch [66/120    avg_loss:0.015, val_acc:0.979]
Epoch [67/120    avg_loss:0.012, val_acc:0.980]
Epoch [68/120    avg_loss:0.014, val_acc:0.979]
Epoch [69/120    avg_loss:0.018, val_acc:0.980]
Epoch [70/120    avg_loss:0.015, val_acc:0.980]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.979]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.977]
Epoch [76/120    avg_loss:0.018, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.978]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.979]
Epoch [80/120    avg_loss:0.014, val_acc:0.979]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.012, val_acc:0.979]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.015, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.979]
Epoch [87/120    avg_loss:0.011, val_acc:0.979]
Epoch [88/120    avg_loss:0.014, val_acc:0.979]
Epoch [89/120    avg_loss:0.014, val_acc:0.979]
Epoch [90/120    avg_loss:0.014, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.013, val_acc:0.979]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.013, val_acc:0.979]
Epoch [96/120    avg_loss:0.014, val_acc:0.979]
Epoch [97/120    avg_loss:0.012, val_acc:0.979]
Epoch [98/120    avg_loss:0.012, val_acc:0.979]
Epoch [99/120    avg_loss:0.012, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.979]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.013, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.979]
Epoch [106/120    avg_loss:0.015, val_acc:0.979]
Epoch [107/120    avg_loss:0.014, val_acc:0.979]
Epoch [108/120    avg_loss:0.012, val_acc:0.979]
Epoch [109/120    avg_loss:0.012, val_acc:0.979]
Epoch [110/120    avg_loss:0.012, val_acc:0.979]
Epoch [111/120    avg_loss:0.016, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.013, val_acc:0.979]
Epoch [114/120    avg_loss:0.013, val_acc:0.979]
Epoch [115/120    avg_loss:0.014, val_acc:0.979]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.014, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.014, val_acc:0.979]
Epoch [120/120    avg_loss:0.013, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6282     0     0     0     0    31    17   100     2]
 [    0     0 17938     0    79     0    66     0     7     0]
 [    0     6     0  1946     0     0     0     0    84     0]
 [    0    23     1     1  2915     0    21     0    10     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     2     1     0  4853     0    12     0]
 [    0    24     0     0     0     0     0  1266     0     0]
 [    0    10     0    47    58     0     2     0  3452     2]
 [    0     1     0     0    14    21     0     0     0   883]]

Accuracy:
98.42624057069868

F1 scores:
[       nan 0.98325247 0.99547712 0.96527778 0.96539162 0.99201824
 0.98528068 0.98406529 0.9541183  0.97731046]

Kappa:
0.9791806099124657
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f02102c4940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.097]
Epoch [2/120    avg_loss:1.678, val_acc:0.154]
Epoch [3/120    avg_loss:1.500, val_acc:0.329]
Epoch [4/120    avg_loss:1.331, val_acc:0.361]
Epoch [5/120    avg_loss:1.161, val_acc:0.394]
Epoch [6/120    avg_loss:1.022, val_acc:0.441]
Epoch [7/120    avg_loss:0.885, val_acc:0.515]
Epoch [8/120    avg_loss:0.779, val_acc:0.620]
Epoch [9/120    avg_loss:0.684, val_acc:0.597]
Epoch [10/120    avg_loss:0.623, val_acc:0.644]
Epoch [11/120    avg_loss:0.525, val_acc:0.613]
Epoch [12/120    avg_loss:0.481, val_acc:0.625]
Epoch [13/120    avg_loss:0.434, val_acc:0.625]
Epoch [14/120    avg_loss:0.387, val_acc:0.707]
Epoch [15/120    avg_loss:0.327, val_acc:0.702]
Epoch [16/120    avg_loss:0.333, val_acc:0.791]
Epoch [17/120    avg_loss:0.307, val_acc:0.755]
Epoch [18/120    avg_loss:0.316, val_acc:0.825]
Epoch [19/120    avg_loss:0.286, val_acc:0.805]
Epoch [20/120    avg_loss:0.259, val_acc:0.818]
Epoch [21/120    avg_loss:0.236, val_acc:0.882]
Epoch [22/120    avg_loss:0.224, val_acc:0.878]
Epoch [23/120    avg_loss:0.203, val_acc:0.917]
Epoch [24/120    avg_loss:0.163, val_acc:0.929]
Epoch [25/120    avg_loss:0.150, val_acc:0.847]
Epoch [26/120    avg_loss:0.150, val_acc:0.920]
Epoch [27/120    avg_loss:0.143, val_acc:0.921]
Epoch [28/120    avg_loss:0.115, val_acc:0.958]
Epoch [29/120    avg_loss:0.111, val_acc:0.959]
Epoch [30/120    avg_loss:0.098, val_acc:0.965]
Epoch [31/120    avg_loss:0.097, val_acc:0.959]
Epoch [32/120    avg_loss:0.106, val_acc:0.954]
Epoch [33/120    avg_loss:0.074, val_acc:0.951]
Epoch [34/120    avg_loss:0.070, val_acc:0.969]
Epoch [35/120    avg_loss:0.068, val_acc:0.971]
Epoch [36/120    avg_loss:0.095, val_acc:0.941]
Epoch [37/120    avg_loss:0.080, val_acc:0.968]
Epoch [38/120    avg_loss:0.068, val_acc:0.974]
Epoch [39/120    avg_loss:0.046, val_acc:0.963]
Epoch [40/120    avg_loss:0.060, val_acc:0.969]
Epoch [41/120    avg_loss:0.041, val_acc:0.970]
Epoch [42/120    avg_loss:0.042, val_acc:0.979]
Epoch [43/120    avg_loss:0.042, val_acc:0.968]
Epoch [44/120    avg_loss:0.034, val_acc:0.983]
Epoch [45/120    avg_loss:0.037, val_acc:0.947]
Epoch [46/120    avg_loss:0.090, val_acc:0.911]
Epoch [47/120    avg_loss:0.053, val_acc:0.975]
Epoch [48/120    avg_loss:0.040, val_acc:0.968]
Epoch [49/120    avg_loss:0.050, val_acc:0.959]
Epoch [50/120    avg_loss:0.058, val_acc:0.948]
Epoch [51/120    avg_loss:0.036, val_acc:0.970]
Epoch [52/120    avg_loss:0.047, val_acc:0.974]
Epoch [53/120    avg_loss:0.025, val_acc:0.979]
Epoch [54/120    avg_loss:0.023, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.984]
Epoch [56/120    avg_loss:0.021, val_acc:0.965]
Epoch [57/120    avg_loss:0.020, val_acc:0.981]
Epoch [58/120    avg_loss:0.030, val_acc:0.965]
Epoch [59/120    avg_loss:0.020, val_acc:0.981]
Epoch [60/120    avg_loss:0.023, val_acc:0.983]
Epoch [61/120    avg_loss:0.026, val_acc:0.980]
Epoch [62/120    avg_loss:0.020, val_acc:0.977]
Epoch [63/120    avg_loss:0.057, val_acc:0.938]
Epoch [64/120    avg_loss:1.324, val_acc:0.674]
Epoch [65/120    avg_loss:0.449, val_acc:0.755]
Epoch [66/120    avg_loss:0.361, val_acc:0.798]
Epoch [67/120    avg_loss:0.324, val_acc:0.845]
Epoch [68/120    avg_loss:0.274, val_acc:0.867]
Epoch [69/120    avg_loss:0.246, val_acc:0.875]
Epoch [70/120    avg_loss:0.226, val_acc:0.886]
Epoch [71/120    avg_loss:0.212, val_acc:0.911]
Epoch [72/120    avg_loss:0.200, val_acc:0.914]
Epoch [73/120    avg_loss:0.199, val_acc:0.916]
Epoch [74/120    avg_loss:0.200, val_acc:0.906]
Epoch [75/120    avg_loss:0.186, val_acc:0.921]
Epoch [76/120    avg_loss:0.177, val_acc:0.928]
Epoch [77/120    avg_loss:0.181, val_acc:0.929]
Epoch [78/120    avg_loss:0.172, val_acc:0.932]
Epoch [79/120    avg_loss:0.159, val_acc:0.942]
Epoch [80/120    avg_loss:0.162, val_acc:0.937]
Epoch [81/120    avg_loss:0.154, val_acc:0.931]
Epoch [82/120    avg_loss:0.153, val_acc:0.935]
Epoch [83/120    avg_loss:0.152, val_acc:0.939]
Epoch [84/120    avg_loss:0.140, val_acc:0.942]
Epoch [85/120    avg_loss:0.157, val_acc:0.943]
Epoch [86/120    avg_loss:0.136, val_acc:0.943]
Epoch [87/120    avg_loss:0.141, val_acc:0.946]
Epoch [88/120    avg_loss:0.147, val_acc:0.946]
Epoch [89/120    avg_loss:0.160, val_acc:0.944]
Epoch [90/120    avg_loss:0.136, val_acc:0.943]
Epoch [91/120    avg_loss:0.151, val_acc:0.943]
Epoch [92/120    avg_loss:0.153, val_acc:0.944]
Epoch [93/120    avg_loss:0.141, val_acc:0.946]
Epoch [94/120    avg_loss:0.146, val_acc:0.947]
Epoch [95/120    avg_loss:0.152, val_acc:0.947]
Epoch [96/120    avg_loss:0.144, val_acc:0.947]
Epoch [97/120    avg_loss:0.142, val_acc:0.947]
Epoch [98/120    avg_loss:0.149, val_acc:0.945]
Epoch [99/120    avg_loss:0.152, val_acc:0.947]
Epoch [100/120    avg_loss:0.158, val_acc:0.945]
Epoch [101/120    avg_loss:0.159, val_acc:0.945]
Epoch [102/120    avg_loss:0.134, val_acc:0.945]
Epoch [103/120    avg_loss:0.133, val_acc:0.945]
Epoch [104/120    avg_loss:0.155, val_acc:0.946]
Epoch [105/120    avg_loss:0.141, val_acc:0.946]
Epoch [106/120    avg_loss:0.142, val_acc:0.946]
Epoch [107/120    avg_loss:0.150, val_acc:0.946]
Epoch [108/120    avg_loss:0.137, val_acc:0.946]
Epoch [109/120    avg_loss:0.147, val_acc:0.946]
Epoch [110/120    avg_loss:0.146, val_acc:0.946]
Epoch [111/120    avg_loss:0.150, val_acc:0.946]
Epoch [112/120    avg_loss:0.144, val_acc:0.946]
Epoch [113/120    avg_loss:0.148, val_acc:0.946]
Epoch [114/120    avg_loss:0.143, val_acc:0.946]
Epoch [115/120    avg_loss:0.141, val_acc:0.946]
Epoch [116/120    avg_loss:0.150, val_acc:0.946]
Epoch [117/120    avg_loss:0.151, val_acc:0.946]
Epoch [118/120    avg_loss:0.133, val_acc:0.946]
Epoch [119/120    avg_loss:0.147, val_acc:0.946]
Epoch [120/120    avg_loss:0.138, val_acc:0.946]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5890     0    51    23     0    34   150   166   118]
 [    0     0 16972     0   646     0   463     0     9     0]
 [    0    12     0  1871     0     0     0     0   146     7]
 [    0    25    14     0  2917     0     9     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    12   104     0     0     0  4671    19    72     0]
 [    0    30     0     0     0     0    14  1246     0     0]
 [    0    35     1   298    65     0    11     0  3158     3]
 [    0     2     0     0    14    30     0     0     0   873]]

Accuracy:
93.75798327428723

F1 scores:
[       nan 0.9470976  0.96483898 0.87922932 0.8790116  0.98863636
 0.92678571 0.92125693 0.88620738 0.90842872]

Kappa:
0.918331222965718
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12d4c07908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.032, val_acc:0.143]
Epoch [2/120    avg_loss:1.697, val_acc:0.229]
Epoch [3/120    avg_loss:1.437, val_acc:0.291]
Epoch [4/120    avg_loss:1.260, val_acc:0.378]
Epoch [5/120    avg_loss:1.097, val_acc:0.432]
Epoch [6/120    avg_loss:0.935, val_acc:0.461]
Epoch [7/120    avg_loss:0.803, val_acc:0.508]
Epoch [8/120    avg_loss:0.705, val_acc:0.523]
Epoch [9/120    avg_loss:0.630, val_acc:0.546]
Epoch [10/120    avg_loss:0.587, val_acc:0.606]
Epoch [11/120    avg_loss:0.528, val_acc:0.632]
Epoch [12/120    avg_loss:0.456, val_acc:0.738]
Epoch [13/120    avg_loss:0.442, val_acc:0.774]
Epoch [14/120    avg_loss:0.515, val_acc:0.774]
Epoch [15/120    avg_loss:0.419, val_acc:0.807]
Epoch [16/120    avg_loss:0.366, val_acc:0.813]
Epoch [17/120    avg_loss:0.336, val_acc:0.829]
Epoch [18/120    avg_loss:0.306, val_acc:0.836]
Epoch [19/120    avg_loss:0.274, val_acc:0.838]
Epoch [20/120    avg_loss:0.242, val_acc:0.863]
Epoch [21/120    avg_loss:0.220, val_acc:0.861]
Epoch [22/120    avg_loss:0.232, val_acc:0.856]
Epoch [23/120    avg_loss:0.214, val_acc:0.900]
Epoch [24/120    avg_loss:0.179, val_acc:0.902]
Epoch [25/120    avg_loss:0.138, val_acc:0.929]
Epoch [26/120    avg_loss:0.144, val_acc:0.919]
Epoch [27/120    avg_loss:0.139, val_acc:0.969]
Epoch [28/120    avg_loss:0.126, val_acc:0.940]
Epoch [29/120    avg_loss:0.152, val_acc:0.949]
Epoch [30/120    avg_loss:0.120, val_acc:0.943]
Epoch [31/120    avg_loss:0.105, val_acc:0.973]
Epoch [32/120    avg_loss:0.107, val_acc:0.954]
Epoch [33/120    avg_loss:0.084, val_acc:0.961]
Epoch [34/120    avg_loss:0.071, val_acc:0.965]
Epoch [35/120    avg_loss:0.058, val_acc:0.970]
Epoch [36/120    avg_loss:0.056, val_acc:0.973]
Epoch [37/120    avg_loss:0.064, val_acc:0.972]
Epoch [38/120    avg_loss:0.050, val_acc:0.980]
Epoch [39/120    avg_loss:0.035, val_acc:0.970]
Epoch [40/120    avg_loss:0.044, val_acc:0.976]
Epoch [41/120    avg_loss:0.049, val_acc:0.975]
Epoch [42/120    avg_loss:0.049, val_acc:0.916]
Epoch [43/120    avg_loss:0.057, val_acc:0.977]
Epoch [44/120    avg_loss:0.067, val_acc:0.972]
Epoch [45/120    avg_loss:0.082, val_acc:0.951]
Epoch [46/120    avg_loss:0.042, val_acc:0.981]
Epoch [47/120    avg_loss:0.047, val_acc:0.942]
Epoch [48/120    avg_loss:0.073, val_acc:0.968]
Epoch [49/120    avg_loss:0.039, val_acc:0.981]
Epoch [50/120    avg_loss:0.031, val_acc:0.973]
Epoch [51/120    avg_loss:0.032, val_acc:0.977]
Epoch [52/120    avg_loss:0.023, val_acc:0.985]
Epoch [53/120    avg_loss:0.021, val_acc:0.981]
Epoch [54/120    avg_loss:0.040, val_acc:0.987]
Epoch [55/120    avg_loss:0.021, val_acc:0.982]
Epoch [56/120    avg_loss:0.015, val_acc:0.984]
Epoch [57/120    avg_loss:0.017, val_acc:0.981]
Epoch [58/120    avg_loss:0.021, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.986]
Epoch [60/120    avg_loss:0.021, val_acc:0.983]
Epoch [61/120    avg_loss:0.014, val_acc:0.988]
Epoch [62/120    avg_loss:0.013, val_acc:0.986]
Epoch [63/120    avg_loss:0.014, val_acc:0.986]
Epoch [64/120    avg_loss:0.017, val_acc:0.986]
Epoch [65/120    avg_loss:0.024, val_acc:0.979]
Epoch [66/120    avg_loss:0.024, val_acc:0.983]
Epoch [67/120    avg_loss:0.021, val_acc:0.983]
Epoch [68/120    avg_loss:0.020, val_acc:0.976]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.013, val_acc:0.983]
Epoch [71/120    avg_loss:0.025, val_acc:0.986]
Epoch [72/120    avg_loss:0.020, val_acc:0.985]
Epoch [73/120    avg_loss:0.035, val_acc:0.975]
Epoch [74/120    avg_loss:0.028, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.987]
Epoch [76/120    avg_loss:0.012, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.987]
Epoch [78/120    avg_loss:0.012, val_acc:0.987]
Epoch [79/120    avg_loss:0.011, val_acc:0.987]
Epoch [80/120    avg_loss:0.011, val_acc:0.987]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.014, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.016, val_acc:0.987]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.989]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.989]
Epoch [112/120    avg_loss:0.009, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     0     0     0     1     0    77     0]
 [    0     0 18024     0    52     0     8     0     4     2]
 [    0     5     0  1994     0     0     0     0    35     2]
 [    0     9    10     0  2931     0     8     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     4     0     0  4861     0     0     0]
 [    0     0     0     0     0     9     0  1281     0     0]
 [    0    15     0     4    53     0     0     0  3497     2]
 [    0     0     0     0    14    26     0     0     0   879]]

Accuracy:
99.11551346010171

F1 scores:
[       nan 0.99165041 0.99753715 0.98761763 0.97343075 0.98676749
 0.99651497 0.99649942 0.9717938  0.97396122]

Kappa:
0.988286602808871
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48a2d7e978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.029, val_acc:0.152]
Epoch [2/120    avg_loss:1.696, val_acc:0.284]
Epoch [3/120    avg_loss:1.492, val_acc:0.383]
Epoch [4/120    avg_loss:1.299, val_acc:0.450]
Epoch [5/120    avg_loss:1.172, val_acc:0.512]
Epoch [6/120    avg_loss:1.019, val_acc:0.575]
Epoch [7/120    avg_loss:0.888, val_acc:0.618]
Epoch [8/120    avg_loss:0.769, val_acc:0.701]
Epoch [9/120    avg_loss:0.700, val_acc:0.691]
Epoch [10/120    avg_loss:0.571, val_acc:0.739]
Epoch [11/120    avg_loss:0.495, val_acc:0.726]
Epoch [12/120    avg_loss:0.447, val_acc:0.754]
Epoch [13/120    avg_loss:0.425, val_acc:0.779]
Epoch [14/120    avg_loss:0.344, val_acc:0.782]
Epoch [15/120    avg_loss:0.331, val_acc:0.810]
Epoch [16/120    avg_loss:0.333, val_acc:0.802]
Epoch [17/120    avg_loss:0.306, val_acc:0.772]
Epoch [18/120    avg_loss:0.277, val_acc:0.860]
Epoch [19/120    avg_loss:0.230, val_acc:0.898]
Epoch [20/120    avg_loss:0.216, val_acc:0.872]
Epoch [21/120    avg_loss:0.183, val_acc:0.926]
Epoch [22/120    avg_loss:0.179, val_acc:0.883]
Epoch [23/120    avg_loss:0.173, val_acc:0.943]
Epoch [24/120    avg_loss:0.146, val_acc:0.944]
Epoch [25/120    avg_loss:0.130, val_acc:0.943]
Epoch [26/120    avg_loss:0.104, val_acc:0.944]
Epoch [27/120    avg_loss:0.099, val_acc:0.940]
Epoch [28/120    avg_loss:0.101, val_acc:0.951]
Epoch [29/120    avg_loss:0.085, val_acc:0.963]
Epoch [30/120    avg_loss:0.080, val_acc:0.956]
Epoch [31/120    avg_loss:0.088, val_acc:0.941]
Epoch [32/120    avg_loss:0.075, val_acc:0.961]
Epoch [33/120    avg_loss:0.064, val_acc:0.951]
Epoch [34/120    avg_loss:0.059, val_acc:0.967]
Epoch [35/120    avg_loss:0.067, val_acc:0.970]
Epoch [36/120    avg_loss:0.076, val_acc:0.958]
Epoch [37/120    avg_loss:0.073, val_acc:0.952]
Epoch [38/120    avg_loss:0.053, val_acc:0.946]
Epoch [39/120    avg_loss:0.076, val_acc:0.967]
Epoch [40/120    avg_loss:0.053, val_acc:0.967]
Epoch [41/120    avg_loss:0.041, val_acc:0.967]
Epoch [42/120    avg_loss:0.043, val_acc:0.954]
Epoch [43/120    avg_loss:0.045, val_acc:0.972]
Epoch [44/120    avg_loss:0.037, val_acc:0.968]
Epoch [45/120    avg_loss:0.033, val_acc:0.975]
Epoch [46/120    avg_loss:0.053, val_acc:0.938]
Epoch [47/120    avg_loss:0.079, val_acc:0.935]
Epoch [48/120    avg_loss:0.063, val_acc:0.972]
Epoch [49/120    avg_loss:0.044, val_acc:0.970]
Epoch [50/120    avg_loss:0.038, val_acc:0.960]
Epoch [51/120    avg_loss:0.043, val_acc:0.932]
Epoch [52/120    avg_loss:0.062, val_acc:0.962]
Epoch [53/120    avg_loss:0.044, val_acc:0.969]
Epoch [54/120    avg_loss:0.029, val_acc:0.975]
Epoch [55/120    avg_loss:0.031, val_acc:0.954]
Epoch [56/120    avg_loss:0.035, val_acc:0.977]
Epoch [57/120    avg_loss:0.026, val_acc:0.976]
Epoch [58/120    avg_loss:0.044, val_acc:0.945]
Epoch [59/120    avg_loss:0.045, val_acc:0.972]
Epoch [60/120    avg_loss:0.033, val_acc:0.973]
Epoch [61/120    avg_loss:0.032, val_acc:0.965]
Epoch [62/120    avg_loss:0.029, val_acc:0.976]
Epoch [63/120    avg_loss:0.021, val_acc:0.980]
Epoch [64/120    avg_loss:0.022, val_acc:0.976]
Epoch [65/120    avg_loss:0.026, val_acc:0.973]
Epoch [66/120    avg_loss:0.017, val_acc:0.981]
Epoch [67/120    avg_loss:0.019, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.975]
Epoch [70/120    avg_loss:0.018, val_acc:0.981]
Epoch [71/120    avg_loss:0.020, val_acc:0.976]
Epoch [72/120    avg_loss:0.017, val_acc:0.979]
Epoch [73/120    avg_loss:0.014, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.980]
Epoch [75/120    avg_loss:0.014, val_acc:0.981]
Epoch [76/120    avg_loss:0.021, val_acc:0.983]
Epoch [77/120    avg_loss:0.015, val_acc:0.981]
Epoch [78/120    avg_loss:0.022, val_acc:0.976]
Epoch [79/120    avg_loss:0.019, val_acc:0.981]
Epoch [80/120    avg_loss:0.008, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.025, val_acc:0.959]
Epoch [83/120    avg_loss:0.025, val_acc:0.981]
Epoch [84/120    avg_loss:0.034, val_acc:0.965]
Epoch [85/120    avg_loss:0.022, val_acc:0.986]
Epoch [86/120    avg_loss:0.017, val_acc:0.977]
Epoch [87/120    avg_loss:0.016, val_acc:0.978]
Epoch [88/120    avg_loss:0.014, val_acc:0.982]
Epoch [89/120    avg_loss:0.017, val_acc:0.969]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6341     0     0     0     0     0     4    87     0]
 [    0     0 18024     0    60     0     6     0     0     0]
 [    0     4     0  2008     0     0     0     0    23     1]
 [    0    58    14     0  2884     0     0     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     7     0     0  4850     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    15     0     9    50     0     0     0  3497     0]
 [    0     0     0     0    14    47     0     0     0   858]]

Accuracy:
98.94922035042056

F1 scores:
[       nan 0.98692607 0.99720601 0.98916256 0.96454849 0.98231088
 0.99650709 0.99845201 0.97219905 0.96512936]

Kappa:
0.9860816611574924
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0dcf9e7898>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.103, val_acc:0.114]
Epoch [2/120    avg_loss:1.742, val_acc:0.215]
Epoch [3/120    avg_loss:1.507, val_acc:0.319]
Epoch [4/120    avg_loss:1.304, val_acc:0.376]
Epoch [5/120    avg_loss:1.151, val_acc:0.438]
Epoch [6/120    avg_loss:0.987, val_acc:0.498]
Epoch [7/120    avg_loss:0.809, val_acc:0.522]
Epoch [8/120    avg_loss:0.674, val_acc:0.572]
Epoch [9/120    avg_loss:0.628, val_acc:0.622]
Epoch [10/120    avg_loss:0.525, val_acc:0.794]
Epoch [11/120    avg_loss:0.464, val_acc:0.785]
Epoch [12/120    avg_loss:0.440, val_acc:0.812]
Epoch [13/120    avg_loss:0.394, val_acc:0.827]
Epoch [14/120    avg_loss:0.329, val_acc:0.840]
Epoch [15/120    avg_loss:0.289, val_acc:0.865]
Epoch [16/120    avg_loss:0.286, val_acc:0.852]
Epoch [17/120    avg_loss:0.250, val_acc:0.891]
Epoch [18/120    avg_loss:0.242, val_acc:0.921]
Epoch [19/120    avg_loss:0.242, val_acc:0.905]
Epoch [20/120    avg_loss:0.240, val_acc:0.923]
Epoch [21/120    avg_loss:0.164, val_acc:0.934]
Epoch [22/120    avg_loss:0.176, val_acc:0.899]
Epoch [23/120    avg_loss:0.163, val_acc:0.948]
Epoch [24/120    avg_loss:0.132, val_acc:0.945]
Epoch [25/120    avg_loss:0.116, val_acc:0.964]
Epoch [26/120    avg_loss:0.112, val_acc:0.960]
Epoch [27/120    avg_loss:0.127, val_acc:0.932]
Epoch [28/120    avg_loss:0.146, val_acc:0.961]
Epoch [29/120    avg_loss:0.113, val_acc:0.944]
Epoch [30/120    avg_loss:0.141, val_acc:0.957]
Epoch [31/120    avg_loss:0.106, val_acc:0.948]
Epoch [32/120    avg_loss:0.078, val_acc:0.968]
Epoch [33/120    avg_loss:0.156, val_acc:0.925]
Epoch [34/120    avg_loss:0.114, val_acc:0.948]
Epoch [35/120    avg_loss:0.094, val_acc:0.956]
Epoch [36/120    avg_loss:0.101, val_acc:0.943]
Epoch [37/120    avg_loss:0.068, val_acc:0.970]
Epoch [38/120    avg_loss:0.054, val_acc:0.967]
Epoch [39/120    avg_loss:0.049, val_acc:0.973]
Epoch [40/120    avg_loss:0.050, val_acc:0.976]
Epoch [41/120    avg_loss:0.043, val_acc:0.986]
Epoch [42/120    avg_loss:0.040, val_acc:0.951]
Epoch [43/120    avg_loss:0.028, val_acc:0.978]
Epoch [44/120    avg_loss:0.032, val_acc:0.973]
Epoch [45/120    avg_loss:0.048, val_acc:0.975]
Epoch [46/120    avg_loss:0.045, val_acc:0.982]
Epoch [47/120    avg_loss:0.033, val_acc:0.977]
Epoch [48/120    avg_loss:0.037, val_acc:0.978]
Epoch [49/120    avg_loss:0.030, val_acc:0.979]
Epoch [50/120    avg_loss:0.020, val_acc:0.984]
Epoch [51/120    avg_loss:0.022, val_acc:0.985]
Epoch [52/120    avg_loss:0.020, val_acc:0.979]
Epoch [53/120    avg_loss:0.025, val_acc:0.985]
Epoch [54/120    avg_loss:0.022, val_acc:0.982]
Epoch [55/120    avg_loss:0.018, val_acc:0.986]
Epoch [56/120    avg_loss:0.012, val_acc:0.986]
Epoch [57/120    avg_loss:0.013, val_acc:0.985]
Epoch [58/120    avg_loss:0.019, val_acc:0.984]
Epoch [59/120    avg_loss:0.020, val_acc:0.986]
Epoch [60/120    avg_loss:0.015, val_acc:0.985]
Epoch [61/120    avg_loss:0.016, val_acc:0.986]
Epoch [62/120    avg_loss:0.012, val_acc:0.987]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.011, val_acc:0.986]
Epoch [65/120    avg_loss:0.014, val_acc:0.986]
Epoch [66/120    avg_loss:0.012, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.011, val_acc:0.987]
Epoch [71/120    avg_loss:0.013, val_acc:0.986]
Epoch [72/120    avg_loss:0.013, val_acc:0.986]
Epoch [73/120    avg_loss:0.011, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.016, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.985]
Epoch [78/120    avg_loss:0.015, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.987]
Epoch [82/120    avg_loss:0.013, val_acc:0.985]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.014, val_acc:0.986]
Epoch [85/120    avg_loss:0.014, val_acc:0.987]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.016, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.014, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.012, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.011, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.011, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.012, val_acc:0.987]
Epoch [117/120    avg_loss:0.011, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0     0     0    34     4]
 [    0     0 18064     0    16     0     9     0     1     0]
 [    0     5     0  1978     0     0     0     0    53     0]
 [    0    22    10     0  2910     0     8     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     0     0     0  4848     0     0     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0     4     0    23    51     0     0     0  3489     4]
 [    0     0     0     0    17    27     0     0     0   875]]

Accuracy:
99.17817463186562

F1 scores:
[       nan 0.99455592 0.99817649 0.9799356  0.97552799 0.98976109
 0.99517602 0.99961225 0.97322176 0.97114317]

Kappa:
0.9891082534102866
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd023739940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.106, val_acc:0.151]
Epoch [2/120    avg_loss:1.699, val_acc:0.209]
Epoch [3/120    avg_loss:1.473, val_acc:0.280]
Epoch [4/120    avg_loss:1.281, val_acc:0.323]
Epoch [5/120    avg_loss:1.169, val_acc:0.339]
Epoch [6/120    avg_loss:1.043, val_acc:0.416]
Epoch [7/120    avg_loss:0.949, val_acc:0.495]
Epoch [8/120    avg_loss:0.869, val_acc:0.563]
Epoch [9/120    avg_loss:0.776, val_acc:0.643]
Epoch [10/120    avg_loss:0.642, val_acc:0.615]
Epoch [11/120    avg_loss:0.559, val_acc:0.655]
Epoch [12/120    avg_loss:0.501, val_acc:0.732]
Epoch [13/120    avg_loss:0.467, val_acc:0.714]
Epoch [14/120    avg_loss:0.418, val_acc:0.785]
Epoch [15/120    avg_loss:0.369, val_acc:0.845]
Epoch [16/120    avg_loss:0.316, val_acc:0.862]
Epoch [17/120    avg_loss:0.293, val_acc:0.832]
Epoch [18/120    avg_loss:0.258, val_acc:0.870]
Epoch [19/120    avg_loss:0.245, val_acc:0.903]
Epoch [20/120    avg_loss:0.232, val_acc:0.907]
Epoch [21/120    avg_loss:0.203, val_acc:0.916]
Epoch [22/120    avg_loss:0.179, val_acc:0.935]
Epoch [23/120    avg_loss:0.175, val_acc:0.925]
Epoch [24/120    avg_loss:0.175, val_acc:0.904]
Epoch [25/120    avg_loss:0.144, val_acc:0.938]
Epoch [26/120    avg_loss:0.122, val_acc:0.948]
Epoch [27/120    avg_loss:0.114, val_acc:0.955]
Epoch [28/120    avg_loss:0.129, val_acc:0.948]
Epoch [29/120    avg_loss:0.149, val_acc:0.918]
Epoch [30/120    avg_loss:0.135, val_acc:0.938]
Epoch [31/120    avg_loss:0.111, val_acc:0.945]
Epoch [32/120    avg_loss:0.075, val_acc:0.965]
Epoch [33/120    avg_loss:0.073, val_acc:0.943]
Epoch [34/120    avg_loss:0.092, val_acc:0.945]
Epoch [35/120    avg_loss:0.066, val_acc:0.958]
Epoch [36/120    avg_loss:0.059, val_acc:0.953]
Epoch [37/120    avg_loss:0.063, val_acc:0.967]
Epoch [38/120    avg_loss:0.058, val_acc:0.971]
Epoch [39/120    avg_loss:0.049, val_acc:0.968]
Epoch [40/120    avg_loss:0.051, val_acc:0.965]
Epoch [41/120    avg_loss:0.068, val_acc:0.945]
Epoch [42/120    avg_loss:0.043, val_acc:0.968]
Epoch [43/120    avg_loss:0.039, val_acc:0.966]
Epoch [44/120    avg_loss:0.048, val_acc:0.962]
Epoch [45/120    avg_loss:0.043, val_acc:0.975]
Epoch [46/120    avg_loss:0.063, val_acc:0.970]
Epoch [47/120    avg_loss:0.048, val_acc:0.971]
Epoch [48/120    avg_loss:0.058, val_acc:0.973]
Epoch [49/120    avg_loss:0.056, val_acc:0.964]
Epoch [50/120    avg_loss:0.053, val_acc:0.965]
Epoch [51/120    avg_loss:0.044, val_acc:0.967]
Epoch [52/120    avg_loss:0.055, val_acc:0.966]
Epoch [53/120    avg_loss:0.037, val_acc:0.976]
Epoch [54/120    avg_loss:0.036, val_acc:0.976]
Epoch [55/120    avg_loss:0.034, val_acc:0.979]
Epoch [56/120    avg_loss:0.036, val_acc:0.968]
Epoch [57/120    avg_loss:0.044, val_acc:0.970]
Epoch [58/120    avg_loss:0.026, val_acc:0.973]
Epoch [59/120    avg_loss:0.019, val_acc:0.984]
Epoch [60/120    avg_loss:0.020, val_acc:0.977]
Epoch [61/120    avg_loss:0.015, val_acc:0.981]
Epoch [62/120    avg_loss:0.019, val_acc:0.960]
Epoch [63/120    avg_loss:0.017, val_acc:0.986]
Epoch [64/120    avg_loss:0.011, val_acc:0.979]
Epoch [65/120    avg_loss:0.013, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.984]
Epoch [69/120    avg_loss:0.014, val_acc:0.982]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.018, val_acc:0.983]
Epoch [75/120    avg_loss:0.015, val_acc:0.973]
Epoch [76/120    avg_loss:0.017, val_acc:0.988]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.987]
Epoch [79/120    avg_loss:0.014, val_acc:0.969]
Epoch [80/120    avg_loss:0.027, val_acc:0.983]
Epoch [81/120    avg_loss:0.026, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.985]
Epoch [83/120    avg_loss:0.013, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.005, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     0     3     0     3     5    32     4]
 [    0     2 18060     0    27     0     1     0     0     0]
 [    0     5     0  1976     0     0     0     0    54     1]
 [    0    23     5     0  2918     0     9     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     6     0     0  4861     0     0     0]
 [    0     7     0     0     0     0     0  1280     0     3]
 [    0    20     0    16    55     0     0     0  3477     3]
 [    0     1     0     0    14    37     0     0     0   867]]

Accuracy:
99.12274359530524

F1 scores:
[       nan 0.99184466 0.99872809 0.97967278 0.97445316 0.98602191
 0.99692371 0.99417476 0.97245141 0.96494157]

Kappa:
0.9883768009715839
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff731814908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.085, val_acc:0.476]
Epoch [2/120    avg_loss:1.709, val_acc:0.267]
Epoch [3/120    avg_loss:1.490, val_acc:0.341]
Epoch [4/120    avg_loss:1.313, val_acc:0.416]
Epoch [5/120    avg_loss:1.167, val_acc:0.399]
Epoch [6/120    avg_loss:1.062, val_acc:0.476]
Epoch [7/120    avg_loss:0.930, val_acc:0.481]
Epoch [8/120    avg_loss:0.832, val_acc:0.660]
Epoch [9/120    avg_loss:0.744, val_acc:0.675]
Epoch [10/120    avg_loss:0.640, val_acc:0.779]
Epoch [11/120    avg_loss:0.544, val_acc:0.757]
Epoch [12/120    avg_loss:0.459, val_acc:0.768]
Epoch [13/120    avg_loss:0.391, val_acc:0.833]
Epoch [14/120    avg_loss:0.366, val_acc:0.781]
Epoch [15/120    avg_loss:0.323, val_acc:0.889]
Epoch [16/120    avg_loss:0.321, val_acc:0.882]
Epoch [17/120    avg_loss:0.264, val_acc:0.821]
Epoch [18/120    avg_loss:0.231, val_acc:0.893]
Epoch [19/120    avg_loss:0.195, val_acc:0.889]
Epoch [20/120    avg_loss:0.180, val_acc:0.917]
Epoch [21/120    avg_loss:0.149, val_acc:0.838]
Epoch [22/120    avg_loss:0.145, val_acc:0.957]
Epoch [23/120    avg_loss:0.106, val_acc:0.964]
Epoch [24/120    avg_loss:0.095, val_acc:0.966]
Epoch [25/120    avg_loss:0.133, val_acc:0.950]
Epoch [26/120    avg_loss:0.110, val_acc:0.967]
Epoch [27/120    avg_loss:0.128, val_acc:0.942]
Epoch [28/120    avg_loss:0.129, val_acc:0.950]
Epoch [29/120    avg_loss:0.093, val_acc:0.965]
Epoch [30/120    avg_loss:0.077, val_acc:0.970]
Epoch [31/120    avg_loss:0.062, val_acc:0.945]
Epoch [32/120    avg_loss:0.065, val_acc:0.976]
Epoch [33/120    avg_loss:0.051, val_acc:0.982]
Epoch [34/120    avg_loss:0.055, val_acc:0.976]
Epoch [35/120    avg_loss:0.051, val_acc:0.974]
Epoch [36/120    avg_loss:0.067, val_acc:0.979]
Epoch [37/120    avg_loss:0.097, val_acc:0.961]
Epoch [38/120    avg_loss:0.058, val_acc:0.965]
Epoch [39/120    avg_loss:0.041, val_acc:0.979]
Epoch [40/120    avg_loss:0.036, val_acc:0.982]
Epoch [41/120    avg_loss:0.033, val_acc:0.952]
Epoch [42/120    avg_loss:0.033, val_acc:0.981]
Epoch [43/120    avg_loss:0.023, val_acc:0.986]
Epoch [44/120    avg_loss:0.028, val_acc:0.973]
Epoch [45/120    avg_loss:0.029, val_acc:0.988]
Epoch [46/120    avg_loss:0.026, val_acc:0.982]
Epoch [47/120    avg_loss:0.024, val_acc:0.987]
Epoch [48/120    avg_loss:0.016, val_acc:0.991]
Epoch [49/120    avg_loss:0.030, val_acc:0.946]
Epoch [50/120    avg_loss:0.091, val_acc:0.967]
Epoch [51/120    avg_loss:0.047, val_acc:0.976]
Epoch [52/120    avg_loss:0.028, val_acc:0.985]
Epoch [53/120    avg_loss:0.025, val_acc:0.983]
Epoch [54/120    avg_loss:0.021, val_acc:0.986]
Epoch [55/120    avg_loss:0.017, val_acc:0.987]
Epoch [56/120    avg_loss:0.018, val_acc:0.992]
Epoch [57/120    avg_loss:0.033, val_acc:0.984]
Epoch [58/120    avg_loss:0.029, val_acc:0.981]
Epoch [59/120    avg_loss:0.037, val_acc:0.968]
Epoch [60/120    avg_loss:0.076, val_acc:0.934]
Epoch [61/120    avg_loss:0.068, val_acc:0.982]
Epoch [62/120    avg_loss:0.031, val_acc:0.979]
Epoch [63/120    avg_loss:0.042, val_acc:0.934]
Epoch [64/120    avg_loss:0.020, val_acc:0.986]
Epoch [65/120    avg_loss:0.044, val_acc:0.976]
Epoch [66/120    avg_loss:0.035, val_acc:0.965]
Epoch [67/120    avg_loss:0.031, val_acc:0.983]
Epoch [68/120    avg_loss:0.028, val_acc:0.986]
Epoch [69/120    avg_loss:0.021, val_acc:0.990]
Epoch [70/120    avg_loss:0.015, val_acc:0.992]
Epoch [71/120    avg_loss:0.012, val_acc:0.991]
Epoch [72/120    avg_loss:0.009, val_acc:0.991]
Epoch [73/120    avg_loss:0.010, val_acc:0.992]
Epoch [74/120    avg_loss:0.013, val_acc:0.992]
Epoch [75/120    avg_loss:0.009, val_acc:0.991]
Epoch [76/120    avg_loss:0.010, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.990]
Epoch [79/120    avg_loss:0.011, val_acc:0.989]
Epoch [80/120    avg_loss:0.013, val_acc:0.990]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.992]
Epoch [83/120    avg_loss:0.008, val_acc:0.992]
Epoch [84/120    avg_loss:0.008, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.992]
Epoch [87/120    avg_loss:0.009, val_acc:0.992]
Epoch [88/120    avg_loss:0.008, val_acc:0.992]
Epoch [89/120    avg_loss:0.009, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.992]
Epoch [91/120    avg_loss:0.008, val_acc:0.992]
Epoch [92/120    avg_loss:0.008, val_acc:0.992]
Epoch [93/120    avg_loss:0.009, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.991]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.009, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.991]
Epoch [99/120    avg_loss:0.010, val_acc:0.992]
Epoch [100/120    avg_loss:0.008, val_acc:0.992]
Epoch [101/120    avg_loss:0.008, val_acc:0.992]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.008, val_acc:0.992]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.009, val_acc:0.991]
Epoch [106/120    avg_loss:0.011, val_acc:0.992]
Epoch [107/120    avg_loss:0.008, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.011, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.992]
Epoch [111/120    avg_loss:0.009, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.009, val_acc:0.992]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.011, val_acc:0.991]
Epoch [118/120    avg_loss:0.011, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6360     0     0     2     0     4     6    55     5]
 [    0     2 18052     0    21     0     9     0     6     0]
 [    0     1     0  2018     0     0     0     0    15     2]
 [    0    34    14     0  2894     0     8     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4873     0     1     0]
 [    0     4     0     0     0     0     1  1280     0     5]
 [    0    11     0    25    53     0     0     0  3478     4]
 [    0     0     0     0    14    13     0     0     0   892]]

Accuracy:
99.17817463186562

F1 scores:
[       nan 0.99034569 0.99856179 0.98848886 0.97179315 0.99504384
 0.99723729 0.99378882 0.97313934 0.97646415]

Kappa:
0.989113657909558
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb752cbf978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.031, val_acc:0.228]
Epoch [2/120    avg_loss:1.670, val_acc:0.251]
Epoch [3/120    avg_loss:1.462, val_acc:0.323]
Epoch [4/120    avg_loss:1.265, val_acc:0.389]
Epoch [5/120    avg_loss:1.118, val_acc:0.402]
Epoch [6/120    avg_loss:0.967, val_acc:0.419]
Epoch [7/120    avg_loss:0.865, val_acc:0.512]
Epoch [8/120    avg_loss:0.761, val_acc:0.702]
Epoch [9/120    avg_loss:0.655, val_acc:0.709]
Epoch [10/120    avg_loss:0.618, val_acc:0.766]
Epoch [11/120    avg_loss:0.531, val_acc:0.802]
Epoch [12/120    avg_loss:0.498, val_acc:0.810]
Epoch [13/120    avg_loss:0.461, val_acc:0.870]
Epoch [14/120    avg_loss:0.385, val_acc:0.871]
Epoch [15/120    avg_loss:0.337, val_acc:0.879]
Epoch [16/120    avg_loss:0.326, val_acc:0.898]
Epoch [17/120    avg_loss:0.294, val_acc:0.909]
Epoch [18/120    avg_loss:0.242, val_acc:0.926]
Epoch [19/120    avg_loss:0.245, val_acc:0.894]
Epoch [20/120    avg_loss:0.196, val_acc:0.884]
Epoch [21/120    avg_loss:0.187, val_acc:0.935]
Epoch [22/120    avg_loss:0.161, val_acc:0.956]
Epoch [23/120    avg_loss:0.156, val_acc:0.958]
Epoch [24/120    avg_loss:0.123, val_acc:0.969]
Epoch [25/120    avg_loss:0.105, val_acc:0.956]
Epoch [26/120    avg_loss:0.129, val_acc:0.956]
Epoch [27/120    avg_loss:0.116, val_acc:0.958]
Epoch [28/120    avg_loss:0.147, val_acc:0.955]
Epoch [29/120    avg_loss:0.105, val_acc:0.958]
Epoch [30/120    avg_loss:0.099, val_acc:0.970]
Epoch [31/120    avg_loss:0.089, val_acc:0.973]
Epoch [32/120    avg_loss:0.085, val_acc:0.967]
Epoch [33/120    avg_loss:0.074, val_acc:0.982]
Epoch [34/120    avg_loss:0.069, val_acc:0.973]
Epoch [35/120    avg_loss:0.079, val_acc:0.969]
Epoch [36/120    avg_loss:0.069, val_acc:0.965]
Epoch [37/120    avg_loss:0.061, val_acc:0.976]
Epoch [38/120    avg_loss:0.071, val_acc:0.975]
Epoch [39/120    avg_loss:0.055, val_acc:0.977]
Epoch [40/120    avg_loss:0.062, val_acc:0.971]
Epoch [41/120    avg_loss:0.038, val_acc:0.986]
Epoch [42/120    avg_loss:0.040, val_acc:0.980]
Epoch [43/120    avg_loss:0.053, val_acc:0.981]
Epoch [44/120    avg_loss:0.054, val_acc:0.967]
Epoch [45/120    avg_loss:0.050, val_acc:0.981]
Epoch [46/120    avg_loss:0.053, val_acc:0.983]
Epoch [47/120    avg_loss:0.044, val_acc:0.984]
Epoch [48/120    avg_loss:0.094, val_acc:0.950]
Epoch [49/120    avg_loss:0.070, val_acc:0.960]
Epoch [50/120    avg_loss:0.066, val_acc:0.981]
Epoch [51/120    avg_loss:0.056, val_acc:0.974]
Epoch [52/120    avg_loss:0.072, val_acc:0.966]
Epoch [53/120    avg_loss:0.059, val_acc:0.959]
Epoch [54/120    avg_loss:0.034, val_acc:0.978]
Epoch [55/120    avg_loss:0.030, val_acc:0.987]
Epoch [56/120    avg_loss:0.025, val_acc:0.988]
Epoch [57/120    avg_loss:0.029, val_acc:0.986]
Epoch [58/120    avg_loss:0.017, val_acc:0.986]
Epoch [59/120    avg_loss:0.019, val_acc:0.987]
Epoch [60/120    avg_loss:0.018, val_acc:0.988]
Epoch [61/120    avg_loss:0.021, val_acc:0.988]
Epoch [62/120    avg_loss:0.018, val_acc:0.989]
Epoch [63/120    avg_loss:0.020, val_acc:0.989]
Epoch [64/120    avg_loss:0.024, val_acc:0.989]
Epoch [65/120    avg_loss:0.017, val_acc:0.987]
Epoch [66/120    avg_loss:0.018, val_acc:0.988]
Epoch [67/120    avg_loss:0.020, val_acc:0.988]
Epoch [68/120    avg_loss:0.017, val_acc:0.988]
Epoch [69/120    avg_loss:0.016, val_acc:0.989]
Epoch [70/120    avg_loss:0.015, val_acc:0.989]
Epoch [71/120    avg_loss:0.019, val_acc:0.988]
Epoch [72/120    avg_loss:0.017, val_acc:0.988]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.018, val_acc:0.987]
Epoch [75/120    avg_loss:0.015, val_acc:0.986]
Epoch [76/120    avg_loss:0.023, val_acc:0.987]
Epoch [77/120    avg_loss:0.016, val_acc:0.988]
Epoch [78/120    avg_loss:0.015, val_acc:0.989]
Epoch [79/120    avg_loss:0.015, val_acc:0.989]
Epoch [80/120    avg_loss:0.014, val_acc:0.989]
Epoch [81/120    avg_loss:0.015, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.989]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.015, val_acc:0.987]
Epoch [85/120    avg_loss:0.015, val_acc:0.989]
Epoch [86/120    avg_loss:0.015, val_acc:0.989]
Epoch [87/120    avg_loss:0.014, val_acc:0.989]
Epoch [88/120    avg_loss:0.014, val_acc:0.991]
Epoch [89/120    avg_loss:0.013, val_acc:0.990]
Epoch [90/120    avg_loss:0.017, val_acc:0.991]
Epoch [91/120    avg_loss:0.015, val_acc:0.988]
Epoch [92/120    avg_loss:0.017, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.014, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.989]
Epoch [96/120    avg_loss:0.014, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.988]
Epoch [98/120    avg_loss:0.016, val_acc:0.986]
Epoch [99/120    avg_loss:0.014, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.017, val_acc:0.991]
Epoch [102/120    avg_loss:0.012, val_acc:0.989]
Epoch [103/120    avg_loss:0.016, val_acc:0.988]
Epoch [104/120    avg_loss:0.014, val_acc:0.989]
Epoch [105/120    avg_loss:0.014, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.991]
Epoch [107/120    avg_loss:0.015, val_acc:0.990]
Epoch [108/120    avg_loss:0.012, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.014, val_acc:0.990]
Epoch [112/120    avg_loss:0.012, val_acc:0.989]
Epoch [113/120    avg_loss:0.012, val_acc:0.990]
Epoch [114/120    avg_loss:0.011, val_acc:0.989]
Epoch [115/120    avg_loss:0.012, val_acc:0.989]
Epoch [116/120    avg_loss:0.011, val_acc:0.989]
Epoch [117/120    avg_loss:0.016, val_acc:0.991]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.013, val_acc:0.987]
Epoch [120/120    avg_loss:0.014, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6366     0     0     1     0     0    46    18     1]
 [    0     4 17997     0    64     0    25     0     0     0]
 [    0     0     0  1966     0     0     0     0    67     3]
 [    0    30    12     1  2900     0     9     0    19     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     4     0     0  4855     0     7     0]
 [    0     0     0     0     0     0     0  1280     0    10]
 [    0    32     0    27    59     0     0     0  3453     0]
 [    0     0     0     0    16    21     0     0     0   882]]

Accuracy:
98.82148796182489

F1 scores:
[       nan 0.98973881 0.99675999 0.97471492 0.96473719 0.99201824
 0.99416402 0.97859327 0.9679047  0.97136564]

Kappa:
0.9843965073508382
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94736c0978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.081]
Epoch [2/120    avg_loss:1.726, val_acc:0.194]
Epoch [3/120    avg_loss:1.516, val_acc:0.276]
Epoch [4/120    avg_loss:1.327, val_acc:0.405]
Epoch [5/120    avg_loss:1.142, val_acc:0.616]
Epoch [6/120    avg_loss:0.974, val_acc:0.742]
Epoch [7/120    avg_loss:0.826, val_acc:0.767]
Epoch [8/120    avg_loss:0.690, val_acc:0.803]
Epoch [9/120    avg_loss:0.596, val_acc:0.760]
Epoch [10/120    avg_loss:0.494, val_acc:0.726]
Epoch [11/120    avg_loss:0.451, val_acc:0.811]
Epoch [12/120    avg_loss:0.403, val_acc:0.793]
Epoch [13/120    avg_loss:0.366, val_acc:0.803]
Epoch [14/120    avg_loss:0.335, val_acc:0.792]
Epoch [15/120    avg_loss:0.326, val_acc:0.835]
Epoch [16/120    avg_loss:0.317, val_acc:0.839]
Epoch [17/120    avg_loss:0.253, val_acc:0.851]
Epoch [18/120    avg_loss:0.241, val_acc:0.892]
Epoch [19/120    avg_loss:0.202, val_acc:0.872]
Epoch [20/120    avg_loss:0.194, val_acc:0.944]
Epoch [21/120    avg_loss:0.161, val_acc:0.948]
Epoch [22/120    avg_loss:0.162, val_acc:0.933]
Epoch [23/120    avg_loss:0.159, val_acc:0.931]
Epoch [24/120    avg_loss:0.128, val_acc:0.948]
Epoch [25/120    avg_loss:0.133, val_acc:0.937]
Epoch [26/120    avg_loss:0.132, val_acc:0.962]
Epoch [27/120    avg_loss:0.118, val_acc:0.948]
Epoch [28/120    avg_loss:0.081, val_acc:0.974]
Epoch [29/120    avg_loss:0.086, val_acc:0.984]
Epoch [30/120    avg_loss:0.068, val_acc:0.965]
Epoch [31/120    avg_loss:0.057, val_acc:0.974]
Epoch [32/120    avg_loss:0.072, val_acc:0.946]
Epoch [33/120    avg_loss:0.080, val_acc:0.957]
Epoch [34/120    avg_loss:0.066, val_acc:0.981]
Epoch [35/120    avg_loss:0.051, val_acc:0.977]
Epoch [36/120    avg_loss:0.050, val_acc:0.982]
Epoch [37/120    avg_loss:0.043, val_acc:0.982]
Epoch [38/120    avg_loss:0.043, val_acc:0.978]
Epoch [39/120    avg_loss:0.051, val_acc:0.971]
Epoch [40/120    avg_loss:0.040, val_acc:0.983]
Epoch [41/120    avg_loss:0.033, val_acc:0.976]
Epoch [42/120    avg_loss:0.033, val_acc:0.973]
Epoch [43/120    avg_loss:0.032, val_acc:0.983]
Epoch [44/120    avg_loss:0.021, val_acc:0.988]
Epoch [45/120    avg_loss:0.021, val_acc:0.988]
Epoch [46/120    avg_loss:0.021, val_acc:0.989]
Epoch [47/120    avg_loss:0.020, val_acc:0.990]
Epoch [48/120    avg_loss:0.019, val_acc:0.987]
Epoch [49/120    avg_loss:0.017, val_acc:0.988]
Epoch [50/120    avg_loss:0.017, val_acc:0.987]
Epoch [51/120    avg_loss:0.017, val_acc:0.990]
Epoch [52/120    avg_loss:0.018, val_acc:0.986]
Epoch [53/120    avg_loss:0.014, val_acc:0.988]
Epoch [54/120    avg_loss:0.017, val_acc:0.990]
Epoch [55/120    avg_loss:0.018, val_acc:0.989]
Epoch [56/120    avg_loss:0.016, val_acc:0.988]
Epoch [57/120    avg_loss:0.015, val_acc:0.989]
Epoch [58/120    avg_loss:0.015, val_acc:0.988]
Epoch [59/120    avg_loss:0.015, val_acc:0.988]
Epoch [60/120    avg_loss:0.017, val_acc:0.988]
Epoch [61/120    avg_loss:0.016, val_acc:0.988]
Epoch [62/120    avg_loss:0.018, val_acc:0.988]
Epoch [63/120    avg_loss:0.017, val_acc:0.988]
Epoch [64/120    avg_loss:0.017, val_acc:0.988]
Epoch [65/120    avg_loss:0.015, val_acc:0.988]
Epoch [66/120    avg_loss:0.016, val_acc:0.988]
Epoch [67/120    avg_loss:0.013, val_acc:0.988]
Epoch [68/120    avg_loss:0.016, val_acc:0.988]
Epoch [69/120    avg_loss:0.016, val_acc:0.988]
Epoch [70/120    avg_loss:0.017, val_acc:0.988]
Epoch [71/120    avg_loss:0.015, val_acc:0.988]
Epoch [72/120    avg_loss:0.015, val_acc:0.988]
Epoch [73/120    avg_loss:0.012, val_acc:0.988]
Epoch [74/120    avg_loss:0.015, val_acc:0.988]
Epoch [75/120    avg_loss:0.016, val_acc:0.988]
Epoch [76/120    avg_loss:0.016, val_acc:0.988]
Epoch [77/120    avg_loss:0.015, val_acc:0.988]
Epoch [78/120    avg_loss:0.020, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.019, val_acc:0.988]
Epoch [82/120    avg_loss:0.016, val_acc:0.988]
Epoch [83/120    avg_loss:0.018, val_acc:0.988]
Epoch [84/120    avg_loss:0.015, val_acc:0.988]
Epoch [85/120    avg_loss:0.016, val_acc:0.988]
Epoch [86/120    avg_loss:0.016, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.988]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.016, val_acc:0.988]
Epoch [92/120    avg_loss:0.014, val_acc:0.988]
Epoch [93/120    avg_loss:0.017, val_acc:0.988]
Epoch [94/120    avg_loss:0.016, val_acc:0.988]
Epoch [95/120    avg_loss:0.014, val_acc:0.988]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.988]
Epoch [100/120    avg_loss:0.014, val_acc:0.988]
Epoch [101/120    avg_loss:0.014, val_acc:0.988]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.015, val_acc:0.988]
Epoch [104/120    avg_loss:0.019, val_acc:0.988]
Epoch [105/120    avg_loss:0.018, val_acc:0.988]
Epoch [106/120    avg_loss:0.019, val_acc:0.988]
Epoch [107/120    avg_loss:0.016, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.988]
Epoch [110/120    avg_loss:0.016, val_acc:0.988]
Epoch [111/120    avg_loss:0.017, val_acc:0.988]
Epoch [112/120    avg_loss:0.016, val_acc:0.988]
Epoch [113/120    avg_loss:0.014, val_acc:0.988]
Epoch [114/120    avg_loss:0.018, val_acc:0.988]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6341     0     0     1     0    15    24    48     3]
 [    0     2 18024     0    29     0    31     0     4     0]
 [    0     8     0  1950     0     0     0     0    70     8]
 [    0    31    18     0  2885     0    12     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4857     0    17     0]
 [    0     6     0     0     0     0     5  1275     0     4]
 [    0     6     0    11    60     0     0     0  3491     3]
 [    0     0     0     0    14    21     0     0     0   884]]

Accuracy:
98.84076832236764

F1 scores:
[       nan 0.98877281 0.99767519 0.97475631 0.9679584  0.99201824
 0.99142682 0.98493627 0.96609935 0.97089511]

Kappa:
0.9846474553581835
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa6e4220a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.066, val_acc:0.086]
Epoch [2/120    avg_loss:1.755, val_acc:0.244]
Epoch [3/120    avg_loss:1.521, val_acc:0.272]
Epoch [4/120    avg_loss:1.378, val_acc:0.319]
Epoch [5/120    avg_loss:1.207, val_acc:0.350]
Epoch [6/120    avg_loss:1.134, val_acc:0.431]
Epoch [7/120    avg_loss:1.007, val_acc:0.440]
Epoch [8/120    avg_loss:0.904, val_acc:0.461]
Epoch [9/120    avg_loss:0.782, val_acc:0.595]
Epoch [10/120    avg_loss:0.671, val_acc:0.590]
Epoch [11/120    avg_loss:0.592, val_acc:0.629]
Epoch [12/120    avg_loss:0.555, val_acc:0.644]
Epoch [13/120    avg_loss:0.486, val_acc:0.717]
Epoch [14/120    avg_loss:0.452, val_acc:0.807]
Epoch [15/120    avg_loss:0.377, val_acc:0.753]
Epoch [16/120    avg_loss:0.341, val_acc:0.810]
Epoch [17/120    avg_loss:0.316, val_acc:0.816]
Epoch [18/120    avg_loss:0.287, val_acc:0.830]
Epoch [19/120    avg_loss:0.296, val_acc:0.853]
Epoch [20/120    avg_loss:0.291, val_acc:0.855]
Epoch [21/120    avg_loss:0.243, val_acc:0.865]
Epoch [22/120    avg_loss:0.201, val_acc:0.885]
Epoch [23/120    avg_loss:0.206, val_acc:0.927]
Epoch [24/120    avg_loss:0.215, val_acc:0.940]
Epoch [25/120    avg_loss:0.155, val_acc:0.934]
Epoch [26/120    avg_loss:0.152, val_acc:0.927]
Epoch [27/120    avg_loss:0.125, val_acc:0.947]
Epoch [28/120    avg_loss:0.111, val_acc:0.957]
Epoch [29/120    avg_loss:0.113, val_acc:0.957]
Epoch [30/120    avg_loss:0.091, val_acc:0.959]
Epoch [31/120    avg_loss:0.107, val_acc:0.964]
Epoch [32/120    avg_loss:0.101, val_acc:0.966]
Epoch [33/120    avg_loss:0.083, val_acc:0.957]
Epoch [34/120    avg_loss:0.079, val_acc:0.973]
Epoch [35/120    avg_loss:0.084, val_acc:0.952]
Epoch [36/120    avg_loss:0.087, val_acc:0.961]
Epoch [37/120    avg_loss:0.080, val_acc:0.963]
Epoch [38/120    avg_loss:0.083, val_acc:0.963]
Epoch [39/120    avg_loss:0.050, val_acc:0.976]
Epoch [40/120    avg_loss:0.045, val_acc:0.973]
Epoch [41/120    avg_loss:0.060, val_acc:0.977]
Epoch [42/120    avg_loss:0.049, val_acc:0.967]
Epoch [43/120    avg_loss:0.031, val_acc:0.979]
Epoch [44/120    avg_loss:0.035, val_acc:0.971]
Epoch [45/120    avg_loss:0.054, val_acc:0.976]
Epoch [46/120    avg_loss:0.037, val_acc:0.977]
Epoch [47/120    avg_loss:0.035, val_acc:0.980]
Epoch [48/120    avg_loss:0.064, val_acc:0.950]
Epoch [49/120    avg_loss:0.137, val_acc:0.943]
Epoch [50/120    avg_loss:0.082, val_acc:0.971]
Epoch [51/120    avg_loss:0.057, val_acc:0.968]
Epoch [52/120    avg_loss:0.045, val_acc:0.971]
Epoch [53/120    avg_loss:0.042, val_acc:0.978]
Epoch [54/120    avg_loss:0.034, val_acc:0.981]
Epoch [55/120    avg_loss:0.027, val_acc:0.973]
Epoch [56/120    avg_loss:0.021, val_acc:0.980]
Epoch [57/120    avg_loss:0.031, val_acc:0.981]
Epoch [58/120    avg_loss:0.033, val_acc:0.979]
Epoch [59/120    avg_loss:0.019, val_acc:0.978]
Epoch [60/120    avg_loss:0.019, val_acc:0.981]
Epoch [61/120    avg_loss:0.022, val_acc:0.980]
Epoch [62/120    avg_loss:0.018, val_acc:0.977]
Epoch [63/120    avg_loss:0.014, val_acc:0.979]
Epoch [64/120    avg_loss:0.015, val_acc:0.979]
Epoch [65/120    avg_loss:0.012, val_acc:0.979]
Epoch [66/120    avg_loss:0.015, val_acc:0.980]
Epoch [67/120    avg_loss:0.019, val_acc:0.983]
Epoch [68/120    avg_loss:0.015, val_acc:0.974]
Epoch [69/120    avg_loss:0.031, val_acc:0.966]
Epoch [70/120    avg_loss:0.033, val_acc:0.968]
Epoch [71/120    avg_loss:0.039, val_acc:0.946]
Epoch [72/120    avg_loss:0.035, val_acc:0.983]
Epoch [73/120    avg_loss:0.014, val_acc:0.985]
Epoch [74/120    avg_loss:0.018, val_acc:0.975]
Epoch [75/120    avg_loss:0.023, val_acc:0.981]
Epoch [76/120    avg_loss:0.011, val_acc:0.973]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.021, val_acc:0.982]
Epoch [80/120    avg_loss:0.014, val_acc:0.976]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.017, val_acc:0.978]
Epoch [83/120    avg_loss:0.009, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.046, val_acc:0.981]
Epoch [88/120    avg_loss:0.016, val_acc:0.978]
Epoch [89/120    avg_loss:0.014, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.016, val_acc:0.985]
Epoch [93/120    avg_loss:0.028, val_acc:0.963]
Epoch [94/120    avg_loss:0.009, val_acc:0.979]
Epoch [95/120    avg_loss:0.007, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.977]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.012, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     0     0     0     0     1     0    67     1]
 [    0     0 18034     0    26     0    20     0    10     0]
 [    0     7     0  1991     1     0     0     0    36     1]
 [    0    26    20     0  2904     0     8     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     2     0  4864     0     1     0]
 [    0    11     0     0     0     0     0  1277     0     2]
 [    0    16     0    17    59     0     0     0  3479     0]
 [    0     0     0     0    14    22     0     0     0   883]]

Accuracy:
99.05285228833779

F1 scores:
[       nan 0.98996499 0.99759369 0.98466864 0.9715624  0.99164134
 0.99559922 0.99493572 0.96935079 0.97785161]

Kappa:
0.9874524424513347
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c928c3940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.039, val_acc:0.084]
Epoch [2/120    avg_loss:1.743, val_acc:0.206]
Epoch [3/120    avg_loss:1.505, val_acc:0.254]
Epoch [4/120    avg_loss:1.348, val_acc:0.286]
Epoch [5/120    avg_loss:1.207, val_acc:0.328]
Epoch [6/120    avg_loss:1.094, val_acc:0.389]
Epoch [7/120    avg_loss:0.981, val_acc:0.546]
Epoch [8/120    avg_loss:0.866, val_acc:0.729]
Epoch [9/120    avg_loss:0.784, val_acc:0.742]
Epoch [10/120    avg_loss:0.664, val_acc:0.742]
Epoch [11/120    avg_loss:0.575, val_acc:0.765]
Epoch [12/120    avg_loss:0.499, val_acc:0.761]
Epoch [13/120    avg_loss:0.458, val_acc:0.787]
Epoch [14/120    avg_loss:0.394, val_acc:0.785]
Epoch [15/120    avg_loss:0.352, val_acc:0.780]
Epoch [16/120    avg_loss:0.333, val_acc:0.878]
Epoch [17/120    avg_loss:0.304, val_acc:0.859]
Epoch [18/120    avg_loss:0.248, val_acc:0.845]
Epoch [19/120    avg_loss:0.230, val_acc:0.927]
Epoch [20/120    avg_loss:0.191, val_acc:0.907]
Epoch [21/120    avg_loss:0.186, val_acc:0.922]
Epoch [22/120    avg_loss:0.195, val_acc:0.932]
Epoch [23/120    avg_loss:0.181, val_acc:0.912]
Epoch [24/120    avg_loss:0.135, val_acc:0.948]
Epoch [25/120    avg_loss:0.107, val_acc:0.940]
Epoch [26/120    avg_loss:0.086, val_acc:0.957]
Epoch [27/120    avg_loss:0.092, val_acc:0.953]
Epoch [28/120    avg_loss:0.250, val_acc:0.597]
Epoch [29/120    avg_loss:0.774, val_acc:0.687]
Epoch [30/120    avg_loss:0.532, val_acc:0.816]
Epoch [31/120    avg_loss:0.444, val_acc:0.877]
Epoch [32/120    avg_loss:0.329, val_acc:0.918]
Epoch [33/120    avg_loss:0.234, val_acc:0.924]
Epoch [34/120    avg_loss:0.220, val_acc:0.930]
Epoch [35/120    avg_loss:0.220, val_acc:0.848]
Epoch [36/120    avg_loss:0.183, val_acc:0.939]
Epoch [37/120    avg_loss:0.143, val_acc:0.954]
Epoch [38/120    avg_loss:0.119, val_acc:0.953]
Epoch [39/120    avg_loss:0.086, val_acc:0.909]
Epoch [40/120    avg_loss:0.081, val_acc:0.942]
Epoch [41/120    avg_loss:0.075, val_acc:0.952]
Epoch [42/120    avg_loss:0.065, val_acc:0.959]
Epoch [43/120    avg_loss:0.069, val_acc:0.964]
Epoch [44/120    avg_loss:0.065, val_acc:0.965]
Epoch [45/120    avg_loss:0.068, val_acc:0.965]
Epoch [46/120    avg_loss:0.069, val_acc:0.969]
Epoch [47/120    avg_loss:0.064, val_acc:0.965]
Epoch [48/120    avg_loss:0.065, val_acc:0.966]
Epoch [49/120    avg_loss:0.053, val_acc:0.966]
Epoch [50/120    avg_loss:0.063, val_acc:0.966]
Epoch [51/120    avg_loss:0.056, val_acc:0.967]
Epoch [52/120    avg_loss:0.059, val_acc:0.968]
Epoch [53/120    avg_loss:0.055, val_acc:0.970]
Epoch [54/120    avg_loss:0.055, val_acc:0.970]
Epoch [55/120    avg_loss:0.052, val_acc:0.967]
Epoch [56/120    avg_loss:0.048, val_acc:0.968]
Epoch [57/120    avg_loss:0.058, val_acc:0.969]
Epoch [58/120    avg_loss:0.048, val_acc:0.970]
Epoch [59/120    avg_loss:0.051, val_acc:0.967]
Epoch [60/120    avg_loss:0.057, val_acc:0.969]
Epoch [61/120    avg_loss:0.056, val_acc:0.972]
Epoch [62/120    avg_loss:0.049, val_acc:0.970]
Epoch [63/120    avg_loss:0.056, val_acc:0.969]
Epoch [64/120    avg_loss:0.050, val_acc:0.970]
Epoch [65/120    avg_loss:0.047, val_acc:0.970]
Epoch [66/120    avg_loss:0.047, val_acc:0.970]
Epoch [67/120    avg_loss:0.046, val_acc:0.972]
Epoch [68/120    avg_loss:0.041, val_acc:0.972]
Epoch [69/120    avg_loss:0.044, val_acc:0.970]
Epoch [70/120    avg_loss:0.041, val_acc:0.971]
Epoch [71/120    avg_loss:0.044, val_acc:0.971]
Epoch [72/120    avg_loss:0.040, val_acc:0.970]
Epoch [73/120    avg_loss:0.043, val_acc:0.970]
Epoch [74/120    avg_loss:0.040, val_acc:0.972]
Epoch [75/120    avg_loss:0.049, val_acc:0.967]
Epoch [76/120    avg_loss:0.045, val_acc:0.971]
Epoch [77/120    avg_loss:0.042, val_acc:0.974]
Epoch [78/120    avg_loss:0.039, val_acc:0.971]
Epoch [79/120    avg_loss:0.039, val_acc:0.970]
Epoch [80/120    avg_loss:0.037, val_acc:0.972]
Epoch [81/120    avg_loss:0.041, val_acc:0.972]
Epoch [82/120    avg_loss:0.033, val_acc:0.975]
Epoch [83/120    avg_loss:0.040, val_acc:0.976]
Epoch [84/120    avg_loss:0.039, val_acc:0.979]
Epoch [85/120    avg_loss:0.038, val_acc:0.974]
Epoch [86/120    avg_loss:0.038, val_acc:0.975]
Epoch [87/120    avg_loss:0.036, val_acc:0.977]
Epoch [88/120    avg_loss:0.041, val_acc:0.977]
Epoch [89/120    avg_loss:0.035, val_acc:0.977]
Epoch [90/120    avg_loss:0.042, val_acc:0.976]
Epoch [91/120    avg_loss:0.038, val_acc:0.976]
Epoch [92/120    avg_loss:0.031, val_acc:0.976]
Epoch [93/120    avg_loss:0.029, val_acc:0.975]
Epoch [94/120    avg_loss:0.029, val_acc:0.978]
Epoch [95/120    avg_loss:0.032, val_acc:0.977]
Epoch [96/120    avg_loss:0.031, val_acc:0.973]
Epoch [97/120    avg_loss:0.036, val_acc:0.975]
Epoch [98/120    avg_loss:0.035, val_acc:0.974]
Epoch [99/120    avg_loss:0.039, val_acc:0.975]
Epoch [100/120    avg_loss:0.036, val_acc:0.974]
Epoch [101/120    avg_loss:0.029, val_acc:0.975]
Epoch [102/120    avg_loss:0.032, val_acc:0.975]
Epoch [103/120    avg_loss:0.027, val_acc:0.975]
Epoch [104/120    avg_loss:0.028, val_acc:0.975]
Epoch [105/120    avg_loss:0.034, val_acc:0.976]
Epoch [106/120    avg_loss:0.035, val_acc:0.976]
Epoch [107/120    avg_loss:0.031, val_acc:0.976]
Epoch [108/120    avg_loss:0.029, val_acc:0.976]
Epoch [109/120    avg_loss:0.034, val_acc:0.974]
Epoch [110/120    avg_loss:0.030, val_acc:0.975]
Epoch [111/120    avg_loss:0.033, val_acc:0.975]
Epoch [112/120    avg_loss:0.025, val_acc:0.975]
Epoch [113/120    avg_loss:0.033, val_acc:0.975]
Epoch [114/120    avg_loss:0.030, val_acc:0.975]
Epoch [115/120    avg_loss:0.030, val_acc:0.975]
Epoch [116/120    avg_loss:0.036, val_acc:0.975]
Epoch [117/120    avg_loss:0.030, val_acc:0.975]
Epoch [118/120    avg_loss:0.034, val_acc:0.976]
Epoch [119/120    avg_loss:0.032, val_acc:0.976]
Epoch [120/120    avg_loss:0.035, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6311     0     0     4     0     0    20    80    17]
 [    0     9 17931     0    55     0    93     0     2     0]
 [    0     8     0  2013     0     0     0     0    13     2]
 [    0    53    11     0  2881     0     4     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2    30     0     0     0  4826     0    20     0]
 [    0     6     0     0     0     0     3  1278     0     3]
 [    0    36     0    35    57     0     0     0  3437     6]
 [    0     0     0     1    14    27     0     0     0   877]]

Accuracy:
98.47203142698768

F1 scores:
[       nan 0.98172202 0.994454   0.98555692 0.96306201 0.98976109
 0.98449612 0.98763524 0.96193675 0.96162281]

Kappa:
0.9797808216720674
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f52e21859b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.045, val_acc:0.087]
Epoch [2/120    avg_loss:1.681, val_acc:0.203]
Epoch [3/120    avg_loss:1.486, val_acc:0.304]
Epoch [4/120    avg_loss:1.319, val_acc:0.372]
Epoch [5/120    avg_loss:1.160, val_acc:0.448]
Epoch [6/120    avg_loss:1.082, val_acc:0.692]
Epoch [7/120    avg_loss:0.949, val_acc:0.718]
Epoch [8/120    avg_loss:0.819, val_acc:0.764]
Epoch [9/120    avg_loss:0.753, val_acc:0.732]
Epoch [10/120    avg_loss:0.649, val_acc:0.649]
Epoch [11/120    avg_loss:0.577, val_acc:0.725]
Epoch [12/120    avg_loss:0.484, val_acc:0.728]
Epoch [13/120    avg_loss:0.425, val_acc:0.739]
Epoch [14/120    avg_loss:0.372, val_acc:0.715]
Epoch [15/120    avg_loss:0.337, val_acc:0.803]
Epoch [16/120    avg_loss:0.303, val_acc:0.834]
Epoch [17/120    avg_loss:0.270, val_acc:0.865]
Epoch [18/120    avg_loss:0.256, val_acc:0.867]
Epoch [19/120    avg_loss:0.246, val_acc:0.899]
Epoch [20/120    avg_loss:0.216, val_acc:0.872]
Epoch [21/120    avg_loss:0.200, val_acc:0.842]
Epoch [22/120    avg_loss:0.212, val_acc:0.835]
Epoch [23/120    avg_loss:0.182, val_acc:0.910]
Epoch [24/120    avg_loss:0.184, val_acc:0.851]
Epoch [25/120    avg_loss:0.160, val_acc:0.955]
Epoch [26/120    avg_loss:0.172, val_acc:0.933]
Epoch [27/120    avg_loss:0.140, val_acc:0.959]
Epoch [28/120    avg_loss:0.117, val_acc:0.945]
Epoch [29/120    avg_loss:0.117, val_acc:0.970]
Epoch [30/120    avg_loss:0.104, val_acc:0.979]
Epoch [31/120    avg_loss:0.096, val_acc:0.971]
Epoch [32/120    avg_loss:0.070, val_acc:0.978]
Epoch [33/120    avg_loss:0.090, val_acc:0.943]
Epoch [34/120    avg_loss:0.094, val_acc:0.980]
Epoch [35/120    avg_loss:0.104, val_acc:0.968]
Epoch [36/120    avg_loss:0.081, val_acc:0.981]
Epoch [37/120    avg_loss:0.064, val_acc:0.981]
Epoch [38/120    avg_loss:0.068, val_acc:0.948]
Epoch [39/120    avg_loss:0.074, val_acc:0.972]
Epoch [40/120    avg_loss:0.091, val_acc:0.965]
Epoch [41/120    avg_loss:0.063, val_acc:0.976]
Epoch [42/120    avg_loss:0.054, val_acc:0.982]
Epoch [43/120    avg_loss:0.048, val_acc:0.983]
Epoch [44/120    avg_loss:0.036, val_acc:0.983]
Epoch [45/120    avg_loss:0.037, val_acc:0.983]
Epoch [46/120    avg_loss:0.042, val_acc:0.937]
Epoch [47/120    avg_loss:1.084, val_acc:0.682]
Epoch [48/120    avg_loss:0.640, val_acc:0.731]
Epoch [49/120    avg_loss:0.510, val_acc:0.739]
Epoch [50/120    avg_loss:0.448, val_acc:0.833]
Epoch [51/120    avg_loss:0.398, val_acc:0.827]
Epoch [52/120    avg_loss:0.333, val_acc:0.826]
Epoch [53/120    avg_loss:0.327, val_acc:0.824]
Epoch [54/120    avg_loss:0.288, val_acc:0.821]
Epoch [55/120    avg_loss:0.234, val_acc:0.848]
Epoch [56/120    avg_loss:0.213, val_acc:0.897]
Epoch [57/120    avg_loss:0.157, val_acc:0.933]
Epoch [58/120    avg_loss:0.160, val_acc:0.917]
Epoch [59/120    avg_loss:0.135, val_acc:0.934]
Epoch [60/120    avg_loss:0.117, val_acc:0.933]
Epoch [61/120    avg_loss:0.121, val_acc:0.938]
Epoch [62/120    avg_loss:0.118, val_acc:0.936]
Epoch [63/120    avg_loss:0.113, val_acc:0.937]
Epoch [64/120    avg_loss:0.109, val_acc:0.944]
Epoch [65/120    avg_loss:0.105, val_acc:0.948]
Epoch [66/120    avg_loss:0.104, val_acc:0.946]
Epoch [67/120    avg_loss:0.106, val_acc:0.948]
Epoch [68/120    avg_loss:0.094, val_acc:0.948]
Epoch [69/120    avg_loss:0.101, val_acc:0.948]
Epoch [70/120    avg_loss:0.106, val_acc:0.940]
Epoch [71/120    avg_loss:0.089, val_acc:0.949]
Epoch [72/120    avg_loss:0.086, val_acc:0.952]
Epoch [73/120    avg_loss:0.098, val_acc:0.952]
Epoch [74/120    avg_loss:0.085, val_acc:0.952]
Epoch [75/120    avg_loss:0.096, val_acc:0.948]
Epoch [76/120    avg_loss:0.089, val_acc:0.950]
Epoch [77/120    avg_loss:0.088, val_acc:0.952]
Epoch [78/120    avg_loss:0.093, val_acc:0.950]
Epoch [79/120    avg_loss:0.091, val_acc:0.950]
Epoch [80/120    avg_loss:0.093, val_acc:0.952]
Epoch [81/120    avg_loss:0.089, val_acc:0.952]
Epoch [82/120    avg_loss:0.091, val_acc:0.950]
Epoch [83/120    avg_loss:0.089, val_acc:0.951]
Epoch [84/120    avg_loss:0.090, val_acc:0.949]
Epoch [85/120    avg_loss:0.086, val_acc:0.949]
Epoch [86/120    avg_loss:0.094, val_acc:0.950]
Epoch [87/120    avg_loss:0.091, val_acc:0.950]
Epoch [88/120    avg_loss:0.090, val_acc:0.951]
Epoch [89/120    avg_loss:0.082, val_acc:0.950]
Epoch [90/120    avg_loss:0.090, val_acc:0.952]
Epoch [91/120    avg_loss:0.089, val_acc:0.951]
Epoch [92/120    avg_loss:0.089, val_acc:0.951]
Epoch [93/120    avg_loss:0.077, val_acc:0.951]
Epoch [94/120    avg_loss:0.090, val_acc:0.950]
Epoch [95/120    avg_loss:0.094, val_acc:0.951]
Epoch [96/120    avg_loss:0.088, val_acc:0.951]
Epoch [97/120    avg_loss:0.082, val_acc:0.951]
Epoch [98/120    avg_loss:0.088, val_acc:0.951]
Epoch [99/120    avg_loss:0.094, val_acc:0.951]
Epoch [100/120    avg_loss:0.092, val_acc:0.951]
Epoch [101/120    avg_loss:0.091, val_acc:0.951]
Epoch [102/120    avg_loss:0.094, val_acc:0.951]
Epoch [103/120    avg_loss:0.095, val_acc:0.951]
Epoch [104/120    avg_loss:0.089, val_acc:0.951]
Epoch [105/120    avg_loss:0.090, val_acc:0.951]
Epoch [106/120    avg_loss:0.097, val_acc:0.951]
Epoch [107/120    avg_loss:0.086, val_acc:0.951]
Epoch [108/120    avg_loss:0.085, val_acc:0.951]
Epoch [109/120    avg_loss:0.086, val_acc:0.951]
Epoch [110/120    avg_loss:0.086, val_acc:0.951]
Epoch [111/120    avg_loss:0.092, val_acc:0.951]
Epoch [112/120    avg_loss:0.091, val_acc:0.951]
Epoch [113/120    avg_loss:0.091, val_acc:0.951]
Epoch [114/120    avg_loss:0.093, val_acc:0.951]
Epoch [115/120    avg_loss:0.087, val_acc:0.951]
Epoch [116/120    avg_loss:0.087, val_acc:0.951]
Epoch [117/120    avg_loss:0.085, val_acc:0.951]
Epoch [118/120    avg_loss:0.091, val_acc:0.951]
Epoch [119/120    avg_loss:0.085, val_acc:0.951]
Epoch [120/120    avg_loss:0.084, val_acc:0.951]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6228     0     0    21     0     0     3   112    68]
 [    0     0 17803     0    48     0   239     0     0     0]
 [    0     1     0  1858     0     0     0     0   157    20]
 [    0    27    14     0  2906     0     3     0    15     7]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0    78     0     0     0  4771     2    27     0]
 [    0     5     0     0     0     0     7  1257     0    21]
 [    0    32     0    74    69     0     0     0  3396     0]
 [    0     0     0     0    14    36     0     0     0   869]]

Accuracy:
97.34654038030511

F1 scores:
[       nan 0.97886051 0.98946783 0.93649194 0.96384743 0.98601134
 0.96403314 0.98510972 0.93322341 0.91233596]

Kappa:
0.9649245066838006
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b039db940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.006, val_acc:0.104]
Epoch [2/120    avg_loss:1.696, val_acc:0.201]
Epoch [3/120    avg_loss:1.526, val_acc:0.254]
Epoch [4/120    avg_loss:1.375, val_acc:0.315]
Epoch [5/120    avg_loss:1.252, val_acc:0.342]
Epoch [6/120    avg_loss:1.131, val_acc:0.371]
Epoch [7/120    avg_loss:1.049, val_acc:0.579]
Epoch [8/120    avg_loss:0.925, val_acc:0.684]
Epoch [9/120    avg_loss:0.802, val_acc:0.679]
Epoch [10/120    avg_loss:0.683, val_acc:0.642]
Epoch [11/120    avg_loss:0.611, val_acc:0.660]
Epoch [12/120    avg_loss:0.567, val_acc:0.674]
Epoch [13/120    avg_loss:0.478, val_acc:0.746]
Epoch [14/120    avg_loss:0.429, val_acc:0.791]
Epoch [15/120    avg_loss:0.399, val_acc:0.827]
Epoch [16/120    avg_loss:0.368, val_acc:0.799]
Epoch [17/120    avg_loss:0.318, val_acc:0.875]
Epoch [18/120    avg_loss:0.320, val_acc:0.915]
Epoch [19/120    avg_loss:0.297, val_acc:0.905]
Epoch [20/120    avg_loss:0.239, val_acc:0.887]
Epoch [21/120    avg_loss:0.224, val_acc:0.894]
Epoch [22/120    avg_loss:0.173, val_acc:0.946]
Epoch [23/120    avg_loss:0.175, val_acc:0.905]
Epoch [24/120    avg_loss:0.140, val_acc:0.956]
Epoch [25/120    avg_loss:0.242, val_acc:0.904]
Epoch [26/120    avg_loss:0.306, val_acc:0.750]
Epoch [27/120    avg_loss:0.215, val_acc:0.940]
Epoch [28/120    avg_loss:0.150, val_acc:0.933]
Epoch [29/120    avg_loss:0.320, val_acc:0.867]
Epoch [30/120    avg_loss:0.184, val_acc:0.924]
Epoch [31/120    avg_loss:0.137, val_acc:0.930]
Epoch [32/120    avg_loss:0.130, val_acc:0.961]
Epoch [33/120    avg_loss:0.098, val_acc:0.902]
Epoch [34/120    avg_loss:0.097, val_acc:0.959]
Epoch [35/120    avg_loss:0.097, val_acc:0.970]
Epoch [36/120    avg_loss:0.059, val_acc:0.967]
Epoch [37/120    avg_loss:0.069, val_acc:0.962]
Epoch [38/120    avg_loss:0.080, val_acc:0.965]
Epoch [39/120    avg_loss:0.066, val_acc:0.966]
Epoch [40/120    avg_loss:0.049, val_acc:0.970]
Epoch [41/120    avg_loss:0.047, val_acc:0.951]
Epoch [42/120    avg_loss:0.062, val_acc:0.968]
Epoch [43/120    avg_loss:0.039, val_acc:0.967]
Epoch [44/120    avg_loss:0.039, val_acc:0.977]
Epoch [45/120    avg_loss:0.029, val_acc:0.972]
Epoch [46/120    avg_loss:0.038, val_acc:0.976]
Epoch [47/120    avg_loss:0.046, val_acc:0.885]
Epoch [48/120    avg_loss:0.230, val_acc:0.943]
Epoch [49/120    avg_loss:0.095, val_acc:0.944]
Epoch [50/120    avg_loss:0.071, val_acc:0.958]
Epoch [51/120    avg_loss:0.059, val_acc:0.974]
Epoch [52/120    avg_loss:0.038, val_acc:0.973]
Epoch [53/120    avg_loss:0.036, val_acc:0.970]
Epoch [54/120    avg_loss:0.024, val_acc:0.978]
Epoch [55/120    avg_loss:0.039, val_acc:0.977]
Epoch [56/120    avg_loss:0.039, val_acc:0.983]
Epoch [57/120    avg_loss:0.073, val_acc:0.944]
Epoch [58/120    avg_loss:0.054, val_acc:0.958]
Epoch [59/120    avg_loss:0.041, val_acc:0.972]
Epoch [60/120    avg_loss:0.027, val_acc:0.967]
Epoch [61/120    avg_loss:0.022, val_acc:0.981]
Epoch [62/120    avg_loss:0.021, val_acc:0.979]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.023, val_acc:0.970]
Epoch [66/120    avg_loss:0.018, val_acc:0.977]
Epoch [67/120    avg_loss:0.028, val_acc:0.969]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.013, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.976]
Epoch [72/120    avg_loss:0.012, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.016, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.014, val_acc:0.960]
Epoch [78/120    avg_loss:0.011, val_acc:0.977]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.983]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.966]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.978]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.980]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.028, val_acc:0.970]
Epoch [101/120    avg_loss:0.034, val_acc:0.981]
Epoch [102/120    avg_loss:0.016, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.014, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.092, val_acc:0.977]
Epoch [117/120    avg_loss:0.038, val_acc:0.972]
Epoch [118/120    avg_loss:0.016, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     2     0     0     6     0    39     0]
 [    0     0 18028     0     2     0    58     0     2     0]
 [    0     1     0  2011     1     0     0     0    22     1]
 [    0    36    49     0  2860     0     8     0    18     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    10     0     0  4868     0     0     0]
 [    0     2     0     0     0     0     1  1287     0     0]
 [    0     5     0     0    46     0     0     0  3520     0]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.15166413611935

F1 scores:
[       nan 0.99292434 0.9969309  0.99088445 0.97031383 0.9893859
 0.991547   0.99883586 0.98159509 0.97552836]

Kappa:
0.988759177594568
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3920dab9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.047, val_acc:0.122]
Epoch [2/120    avg_loss:1.747, val_acc:0.194]
Epoch [3/120    avg_loss:1.548, val_acc:0.247]
Epoch [4/120    avg_loss:1.384, val_acc:0.332]
Epoch [5/120    avg_loss:1.247, val_acc:0.348]
Epoch [6/120    avg_loss:1.091, val_acc:0.470]
Epoch [7/120    avg_loss:0.983, val_acc:0.566]
Epoch [8/120    avg_loss:0.868, val_acc:0.644]
Epoch [9/120    avg_loss:0.731, val_acc:0.689]
Epoch [10/120    avg_loss:0.644, val_acc:0.713]
Epoch [11/120    avg_loss:0.554, val_acc:0.754]
Epoch [12/120    avg_loss:0.475, val_acc:0.812]
Epoch [13/120    avg_loss:0.426, val_acc:0.799]
Epoch [14/120    avg_loss:0.378, val_acc:0.791]
Epoch [15/120    avg_loss:0.349, val_acc:0.841]
Epoch [16/120    avg_loss:0.307, val_acc:0.827]
Epoch [17/120    avg_loss:0.303, val_acc:0.837]
Epoch [18/120    avg_loss:0.293, val_acc:0.839]
Epoch [19/120    avg_loss:0.252, val_acc:0.856]
Epoch [20/120    avg_loss:0.205, val_acc:0.889]
Epoch [21/120    avg_loss:0.191, val_acc:0.914]
Epoch [22/120    avg_loss:0.163, val_acc:0.891]
Epoch [23/120    avg_loss:0.152, val_acc:0.932]
Epoch [24/120    avg_loss:0.841, val_acc:0.334]
Epoch [25/120    avg_loss:1.179, val_acc:0.519]
Epoch [26/120    avg_loss:1.104, val_acc:0.497]
Epoch [27/120    avg_loss:1.033, val_acc:0.520]
Epoch [28/120    avg_loss:0.983, val_acc:0.532]
Epoch [29/120    avg_loss:0.946, val_acc:0.565]
Epoch [30/120    avg_loss:0.925, val_acc:0.566]
Epoch [31/120    avg_loss:0.926, val_acc:0.635]
Epoch [32/120    avg_loss:0.863, val_acc:0.685]
Epoch [33/120    avg_loss:0.821, val_acc:0.701]
Epoch [34/120    avg_loss:0.773, val_acc:0.731]
Epoch [35/120    avg_loss:0.769, val_acc:0.729]
Epoch [36/120    avg_loss:0.780, val_acc:0.671]
Epoch [37/120    avg_loss:0.703, val_acc:0.724]
Epoch [38/120    avg_loss:0.708, val_acc:0.740]
Epoch [39/120    avg_loss:0.708, val_acc:0.769]
Epoch [40/120    avg_loss:0.669, val_acc:0.761]
Epoch [41/120    avg_loss:0.659, val_acc:0.755]
Epoch [42/120    avg_loss:0.659, val_acc:0.765]
Epoch [43/120    avg_loss:0.658, val_acc:0.780]
Epoch [44/120    avg_loss:0.644, val_acc:0.767]
Epoch [45/120    avg_loss:0.661, val_acc:0.777]
Epoch [46/120    avg_loss:0.634, val_acc:0.782]
Epoch [47/120    avg_loss:0.656, val_acc:0.785]
Epoch [48/120    avg_loss:0.647, val_acc:0.770]
Epoch [49/120    avg_loss:0.632, val_acc:0.778]
Epoch [50/120    avg_loss:0.618, val_acc:0.780]
Epoch [51/120    avg_loss:0.617, val_acc:0.780]
Epoch [52/120    avg_loss:0.611, val_acc:0.781]
Epoch [53/120    avg_loss:0.621, val_acc:0.781]
Epoch [54/120    avg_loss:0.636, val_acc:0.780]
Epoch [55/120    avg_loss:0.608, val_acc:0.783]
Epoch [56/120    avg_loss:0.620, val_acc:0.783]
Epoch [57/120    avg_loss:0.623, val_acc:0.785]
Epoch [58/120    avg_loss:0.610, val_acc:0.782]
Epoch [59/120    avg_loss:0.614, val_acc:0.782]
Epoch [60/120    avg_loss:0.609, val_acc:0.782]
Epoch [61/120    avg_loss:0.618, val_acc:0.781]
Epoch [62/120    avg_loss:0.618, val_acc:0.782]
Epoch [63/120    avg_loss:0.615, val_acc:0.783]
Epoch [64/120    avg_loss:0.622, val_acc:0.782]
Epoch [65/120    avg_loss:0.632, val_acc:0.782]
Epoch [66/120    avg_loss:0.638, val_acc:0.782]
Epoch [67/120    avg_loss:0.602, val_acc:0.783]
Epoch [68/120    avg_loss:0.624, val_acc:0.782]
Epoch [69/120    avg_loss:0.617, val_acc:0.782]
Epoch [70/120    avg_loss:0.599, val_acc:0.782]
Epoch [71/120    avg_loss:0.618, val_acc:0.782]
Epoch [72/120    avg_loss:0.602, val_acc:0.782]
Epoch [73/120    avg_loss:0.633, val_acc:0.783]
Epoch [74/120    avg_loss:0.630, val_acc:0.782]
Epoch [75/120    avg_loss:0.616, val_acc:0.782]
Epoch [76/120    avg_loss:0.621, val_acc:0.782]
Epoch [77/120    avg_loss:0.622, val_acc:0.782]
Epoch [78/120    avg_loss:0.613, val_acc:0.782]
Epoch [79/120    avg_loss:0.613, val_acc:0.782]
Epoch [80/120    avg_loss:0.633, val_acc:0.782]
Epoch [81/120    avg_loss:0.611, val_acc:0.782]
Epoch [82/120    avg_loss:0.609, val_acc:0.782]
Epoch [83/120    avg_loss:0.612, val_acc:0.782]
Epoch [84/120    avg_loss:0.635, val_acc:0.782]
Epoch [85/120    avg_loss:0.596, val_acc:0.782]
Epoch [86/120    avg_loss:0.612, val_acc:0.782]
Epoch [87/120    avg_loss:0.630, val_acc:0.782]
Epoch [88/120    avg_loss:0.603, val_acc:0.782]
Epoch [89/120    avg_loss:0.619, val_acc:0.782]
Epoch [90/120    avg_loss:0.615, val_acc:0.782]
Epoch [91/120    avg_loss:0.614, val_acc:0.782]
Epoch [92/120    avg_loss:0.614, val_acc:0.782]
Epoch [93/120    avg_loss:0.630, val_acc:0.782]
Epoch [94/120    avg_loss:0.612, val_acc:0.782]
Epoch [95/120    avg_loss:0.627, val_acc:0.782]
Epoch [96/120    avg_loss:0.613, val_acc:0.782]
Epoch [97/120    avg_loss:0.617, val_acc:0.782]
Epoch [98/120    avg_loss:0.624, val_acc:0.782]
Epoch [99/120    avg_loss:0.603, val_acc:0.782]
Epoch [100/120    avg_loss:0.628, val_acc:0.782]
Epoch [101/120    avg_loss:0.640, val_acc:0.782]
Epoch [102/120    avg_loss:0.629, val_acc:0.782]
Epoch [103/120    avg_loss:0.613, val_acc:0.782]
Epoch [104/120    avg_loss:0.640, val_acc:0.782]
Epoch [105/120    avg_loss:0.618, val_acc:0.782]
Epoch [106/120    avg_loss:0.611, val_acc:0.782]
Epoch [107/120    avg_loss:0.614, val_acc:0.782]
Epoch [108/120    avg_loss:0.610, val_acc:0.782]
Epoch [109/120    avg_loss:0.640, val_acc:0.782]
Epoch [110/120    avg_loss:0.612, val_acc:0.782]
Epoch [111/120    avg_loss:0.620, val_acc:0.782]
Epoch [112/120    avg_loss:0.620, val_acc:0.782]
Epoch [113/120    avg_loss:0.631, val_acc:0.782]
Epoch [114/120    avg_loss:0.619, val_acc:0.782]
Epoch [115/120    avg_loss:0.623, val_acc:0.782]
Epoch [116/120    avg_loss:0.628, val_acc:0.782]
Epoch [117/120    avg_loss:0.608, val_acc:0.782]
Epoch [118/120    avg_loss:0.622, val_acc:0.782]
Epoch [119/120    avg_loss:0.636, val_acc:0.782]
Epoch [120/120    avg_loss:0.607, val_acc:0.782]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4032   341   144   263     0   942    59   362   289]
 [    0   151 14101     3   333     0  3470     0    32     0]
 [    0     0     0  1711     0     0    22     0   223    80]
 [    0    32   139     0  2495     0   269     0    30     7]
 [    0     0     0     0     0  1301     0     4     0     0]
 [    0     0   494   164   275     0  3926     0    19     0]
 [    0    96     0     0    21     0     0  1150     7    16]
 [    0    37   115   105    47     0   214     0  3053     0]
 [    0    18     0     8    21   115     8     0     4   745]]

Accuracy:
78.36020533583978

F1 scores:
[       nan 0.74680496 0.84741587 0.82042676 0.77641201 0.95626608
 0.57192804 0.91889732 0.83632379 0.72470817]

Kappa:
0.7226349552936105
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbb16c1c9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.091, val_acc:0.182]
Epoch [2/120    avg_loss:1.737, val_acc:0.245]
Epoch [3/120    avg_loss:1.513, val_acc:0.280]
Epoch [4/120    avg_loss:1.364, val_acc:0.372]
Epoch [5/120    avg_loss:1.204, val_acc:0.421]
Epoch [6/120    avg_loss:1.102, val_acc:0.430]
Epoch [7/120    avg_loss:0.925, val_acc:0.516]
Epoch [8/120    avg_loss:0.801, val_acc:0.514]
Epoch [9/120    avg_loss:0.740, val_acc:0.608]
Epoch [10/120    avg_loss:0.642, val_acc:0.598]
Epoch [11/120    avg_loss:0.515, val_acc:0.601]
Epoch [12/120    avg_loss:0.495, val_acc:0.712]
Epoch [13/120    avg_loss:0.450, val_acc:0.717]
Epoch [14/120    avg_loss:0.413, val_acc:0.755]
Epoch [15/120    avg_loss:0.379, val_acc:0.777]
Epoch [16/120    avg_loss:0.304, val_acc:0.822]
Epoch [17/120    avg_loss:0.275, val_acc:0.812]
Epoch [18/120    avg_loss:0.298, val_acc:0.833]
Epoch [19/120    avg_loss:0.266, val_acc:0.876]
Epoch [20/120    avg_loss:0.271, val_acc:0.838]
Epoch [21/120    avg_loss:0.259, val_acc:0.916]
Epoch [22/120    avg_loss:0.207, val_acc:0.928]
Epoch [23/120    avg_loss:0.165, val_acc:0.931]
Epoch [24/120    avg_loss:0.173, val_acc:0.938]
Epoch [25/120    avg_loss:0.200, val_acc:0.928]
Epoch [26/120    avg_loss:0.188, val_acc:0.912]
Epoch [27/120    avg_loss:0.213, val_acc:0.919]
Epoch [28/120    avg_loss:0.129, val_acc:0.951]
Epoch [29/120    avg_loss:0.120, val_acc:0.938]
Epoch [30/120    avg_loss:0.100, val_acc:0.958]
Epoch [31/120    avg_loss:0.096, val_acc:0.958]
Epoch [32/120    avg_loss:0.090, val_acc:0.963]
Epoch [33/120    avg_loss:0.069, val_acc:0.961]
Epoch [34/120    avg_loss:0.071, val_acc:0.964]
Epoch [35/120    avg_loss:0.076, val_acc:0.959]
Epoch [36/120    avg_loss:0.117, val_acc:0.948]
Epoch [37/120    avg_loss:0.124, val_acc:0.942]
Epoch [38/120    avg_loss:0.072, val_acc:0.962]
Epoch [39/120    avg_loss:0.072, val_acc:0.967]
Epoch [40/120    avg_loss:0.065, val_acc:0.966]
Epoch [41/120    avg_loss:0.053, val_acc:0.971]
Epoch [42/120    avg_loss:0.128, val_acc:0.948]
Epoch [43/120    avg_loss:0.068, val_acc:0.967]
Epoch [44/120    avg_loss:0.056, val_acc:0.958]
Epoch [45/120    avg_loss:0.072, val_acc:0.961]
Epoch [46/120    avg_loss:0.060, val_acc:0.967]
Epoch [47/120    avg_loss:0.059, val_acc:0.957]
Epoch [48/120    avg_loss:0.050, val_acc:0.977]
Epoch [49/120    avg_loss:0.039, val_acc:0.975]
Epoch [50/120    avg_loss:0.046, val_acc:0.964]
Epoch [51/120    avg_loss:0.062, val_acc:0.972]
Epoch [52/120    avg_loss:0.041, val_acc:0.978]
Epoch [53/120    avg_loss:0.035, val_acc:0.973]
Epoch [54/120    avg_loss:0.039, val_acc:0.974]
Epoch [55/120    avg_loss:0.029, val_acc:0.972]
Epoch [56/120    avg_loss:0.062, val_acc:0.970]
Epoch [57/120    avg_loss:0.063, val_acc:0.960]
Epoch [58/120    avg_loss:0.043, val_acc:0.967]
Epoch [59/120    avg_loss:0.045, val_acc:0.976]
Epoch [60/120    avg_loss:0.026, val_acc:0.979]
Epoch [61/120    avg_loss:0.017, val_acc:0.975]
Epoch [62/120    avg_loss:0.019, val_acc:0.978]
Epoch [63/120    avg_loss:0.020, val_acc:0.977]
Epoch [64/120    avg_loss:0.020, val_acc:0.980]
Epoch [65/120    avg_loss:0.019, val_acc:0.982]
Epoch [66/120    avg_loss:0.017, val_acc:0.973]
Epoch [67/120    avg_loss:0.027, val_acc:0.979]
Epoch [68/120    avg_loss:0.022, val_acc:0.984]
Epoch [69/120    avg_loss:0.014, val_acc:0.981]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.017, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.016, val_acc:0.972]
Epoch [74/120    avg_loss:0.017, val_acc:0.983]
Epoch [75/120    avg_loss:0.013, val_acc:0.982]
Epoch [76/120    avg_loss:0.012, val_acc:0.987]
Epoch [77/120    avg_loss:0.012, val_acc:0.981]
Epoch [78/120    avg_loss:0.013, val_acc:0.981]
Epoch [79/120    avg_loss:0.012, val_acc:0.977]
Epoch [80/120    avg_loss:0.025, val_acc:0.977]
Epoch [81/120    avg_loss:0.016, val_acc:0.984]
Epoch [82/120    avg_loss:0.021, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.977]
Epoch [84/120    avg_loss:0.018, val_acc:0.973]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.024, val_acc:0.969]
Epoch [87/120    avg_loss:0.019, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     4     0     0    15    15     0]
 [    0     0 18037     0    46     0     7     0     0     0]
 [    0     0     2  2021     0     0     0     0    10     3]
 [    0    39    23     0  2876     0     8     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     3     0     0  4860     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    17     0     5    79     0     0     0  3470     0]
 [    0     0     0     0    14    29     0     1     0   875]]

Accuracy:
99.12997373050877

F1 scores:
[       nan 0.99301568 0.9974286  0.99434194 0.96010683 0.98901099
 0.99661643 0.99383667 0.9785674  0.9738453 ]

Kappa:
0.9884728738652334
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a41a7c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.117, val_acc:0.137]
Epoch [2/120    avg_loss:1.780, val_acc:0.217]
Epoch [3/120    avg_loss:1.538, val_acc:0.272]
Epoch [4/120    avg_loss:1.391, val_acc:0.394]
Epoch [5/120    avg_loss:1.161, val_acc:0.375]
Epoch [6/120    avg_loss:1.014, val_acc:0.445]
Epoch [7/120    avg_loss:0.910, val_acc:0.539]
Epoch [8/120    avg_loss:0.809, val_acc:0.656]
Epoch [9/120    avg_loss:0.697, val_acc:0.708]
Epoch [10/120    avg_loss:0.640, val_acc:0.744]
Epoch [11/120    avg_loss:0.546, val_acc:0.772]
Epoch [12/120    avg_loss:0.486, val_acc:0.818]
Epoch [13/120    avg_loss:0.439, val_acc:0.811]
Epoch [14/120    avg_loss:0.373, val_acc:0.881]
Epoch [15/120    avg_loss:0.343, val_acc:0.855]
Epoch [16/120    avg_loss:0.306, val_acc:0.825]
Epoch [17/120    avg_loss:0.251, val_acc:0.934]
Epoch [18/120    avg_loss:0.244, val_acc:0.932]
Epoch [19/120    avg_loss:0.256, val_acc:0.921]
Epoch [20/120    avg_loss:0.193, val_acc:0.943]
Epoch [21/120    avg_loss:0.176, val_acc:0.942]
Epoch [22/120    avg_loss:0.158, val_acc:0.919]
Epoch [23/120    avg_loss:0.136, val_acc:0.939]
Epoch [24/120    avg_loss:0.165, val_acc:0.927]
Epoch [25/120    avg_loss:0.139, val_acc:0.952]
Epoch [26/120    avg_loss:0.095, val_acc:0.956]
Epoch [27/120    avg_loss:0.142, val_acc:0.920]
Epoch [28/120    avg_loss:0.151, val_acc:0.952]
Epoch [29/120    avg_loss:0.131, val_acc:0.961]
Epoch [30/120    avg_loss:0.157, val_acc:0.966]
Epoch [31/120    avg_loss:0.126, val_acc:0.950]
Epoch [32/120    avg_loss:0.091, val_acc:0.970]
Epoch [33/120    avg_loss:0.083, val_acc:0.956]
Epoch [34/120    avg_loss:0.071, val_acc:0.970]
Epoch [35/120    avg_loss:0.069, val_acc:0.976]
Epoch [36/120    avg_loss:0.055, val_acc:0.960]
Epoch [37/120    avg_loss:0.067, val_acc:0.978]
Epoch [38/120    avg_loss:0.073, val_acc:0.963]
Epoch [39/120    avg_loss:0.066, val_acc:0.971]
Epoch [40/120    avg_loss:0.064, val_acc:0.967]
Epoch [41/120    avg_loss:0.054, val_acc:0.981]
Epoch [42/120    avg_loss:0.046, val_acc:0.976]
Epoch [43/120    avg_loss:0.040, val_acc:0.972]
Epoch [44/120    avg_loss:0.034, val_acc:0.984]
Epoch [45/120    avg_loss:0.045, val_acc:0.981]
Epoch [46/120    avg_loss:0.027, val_acc:0.979]
Epoch [47/120    avg_loss:0.034, val_acc:0.974]
Epoch [48/120    avg_loss:0.039, val_acc:0.972]
Epoch [49/120    avg_loss:0.031, val_acc:0.981]
Epoch [50/120    avg_loss:0.038, val_acc:0.981]
Epoch [51/120    avg_loss:0.030, val_acc:0.984]
Epoch [52/120    avg_loss:0.022, val_acc:0.988]
Epoch [53/120    avg_loss:0.019, val_acc:0.986]
Epoch [54/120    avg_loss:0.019, val_acc:0.981]
Epoch [55/120    avg_loss:0.018, val_acc:0.989]
Epoch [56/120    avg_loss:0.023, val_acc:0.981]
Epoch [57/120    avg_loss:0.023, val_acc:0.983]
Epoch [58/120    avg_loss:0.018, val_acc:0.986]
Epoch [59/120    avg_loss:0.020, val_acc:0.985]
Epoch [60/120    avg_loss:0.018, val_acc:0.983]
Epoch [61/120    avg_loss:0.019, val_acc:0.987]
Epoch [62/120    avg_loss:0.024, val_acc:0.986]
Epoch [63/120    avg_loss:0.018, val_acc:0.983]
Epoch [64/120    avg_loss:0.019, val_acc:0.987]
Epoch [65/120    avg_loss:0.013, val_acc:0.980]
Epoch [66/120    avg_loss:0.014, val_acc:0.985]
Epoch [67/120    avg_loss:0.018, val_acc:0.979]
Epoch [68/120    avg_loss:0.030, val_acc:0.979]
Epoch [69/120    avg_loss:0.019, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.987]
Epoch [72/120    avg_loss:0.010, val_acc:0.987]
Epoch [73/120    avg_loss:0.009, val_acc:0.987]
Epoch [74/120    avg_loss:0.012, val_acc:0.987]
Epoch [75/120    avg_loss:0.013, val_acc:0.987]
Epoch [76/120    avg_loss:0.011, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.014, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6360     0     0     2     0     5     1    59     5]
 [    0     2 18063     0    24     0     1     0     0     0]
 [    0     7     0  1985     0     0     0     0    41     3]
 [    0    35    14     0  2890     0     7     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     6     0     0  4871     0     0     0]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0    21     0     4    53     0     0     0  3493     0]
 [    0     0     0     0    17    50     0     0     0   852]]

Accuracy:
99.06490251367701

F1 scores:
[       nan 0.98934433 0.99883875 0.98486728 0.9701242  0.98120301
 0.99795124 0.99805976 0.97162726 0.95569265]

Kappa:
0.9876102751388126
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72ceedd9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.070, val_acc:0.079]
Epoch [2/120    avg_loss:1.715, val_acc:0.132]
Epoch [3/120    avg_loss:1.529, val_acc:0.252]
Epoch [4/120    avg_loss:1.383, val_acc:0.362]
Epoch [5/120    avg_loss:1.251, val_acc:0.405]
Epoch [6/120    avg_loss:1.155, val_acc:0.449]
Epoch [7/120    avg_loss:1.039, val_acc:0.471]
Epoch [8/120    avg_loss:0.908, val_acc:0.505]
Epoch [9/120    avg_loss:0.758, val_acc:0.546]
Epoch [10/120    avg_loss:0.657, val_acc:0.566]
Epoch [11/120    avg_loss:0.604, val_acc:0.648]
Epoch [12/120    avg_loss:0.503, val_acc:0.752]
Epoch [13/120    avg_loss:0.440, val_acc:0.787]
Epoch [14/120    avg_loss:0.446, val_acc:0.776]
Epoch [15/120    avg_loss:0.380, val_acc:0.755]
Epoch [16/120    avg_loss:0.347, val_acc:0.795]
Epoch [17/120    avg_loss:0.319, val_acc:0.813]
Epoch [18/120    avg_loss:0.344, val_acc:0.740]
Epoch [19/120    avg_loss:0.368, val_acc:0.751]
Epoch [20/120    avg_loss:0.359, val_acc:0.797]
Epoch [21/120    avg_loss:0.293, val_acc:0.797]
Epoch [22/120    avg_loss:0.283, val_acc:0.782]
Epoch [23/120    avg_loss:0.255, val_acc:0.891]
Epoch [24/120    avg_loss:0.221, val_acc:0.880]
Epoch [25/120    avg_loss:0.209, val_acc:0.855]
Epoch [26/120    avg_loss:0.182, val_acc:0.905]
Epoch [27/120    avg_loss:0.191, val_acc:0.890]
Epoch [28/120    avg_loss:0.180, val_acc:0.835]
Epoch [29/120    avg_loss:0.167, val_acc:0.914]
Epoch [30/120    avg_loss:0.153, val_acc:0.927]
Epoch [31/120    avg_loss:0.109, val_acc:0.942]
Epoch [32/120    avg_loss:0.113, val_acc:0.898]
Epoch [33/120    avg_loss:0.115, val_acc:0.934]
Epoch [34/120    avg_loss:0.130, val_acc:0.955]
Epoch [35/120    avg_loss:0.108, val_acc:0.942]
Epoch [36/120    avg_loss:0.119, val_acc:0.957]
Epoch [37/120    avg_loss:0.080, val_acc:0.951]
Epoch [38/120    avg_loss:0.081, val_acc:0.957]
Epoch [39/120    avg_loss:0.109, val_acc:0.899]
Epoch [40/120    avg_loss:0.108, val_acc:0.947]
Epoch [41/120    avg_loss:0.106, val_acc:0.956]
Epoch [42/120    avg_loss:0.086, val_acc:0.912]
Epoch [43/120    avg_loss:0.072, val_acc:0.966]
Epoch [44/120    avg_loss:0.059, val_acc:0.974]
Epoch [45/120    avg_loss:0.057, val_acc:0.966]
Epoch [46/120    avg_loss:0.054, val_acc:0.970]
Epoch [47/120    avg_loss:0.040, val_acc:0.964]
Epoch [48/120    avg_loss:0.036, val_acc:0.980]
Epoch [49/120    avg_loss:0.032, val_acc:0.954]
Epoch [50/120    avg_loss:0.045, val_acc:0.972]
Epoch [51/120    avg_loss:0.036, val_acc:0.977]
Epoch [52/120    avg_loss:0.032, val_acc:0.977]
Epoch [53/120    avg_loss:0.030, val_acc:0.978]
Epoch [54/120    avg_loss:0.030, val_acc:0.957]
Epoch [55/120    avg_loss:0.050, val_acc:0.977]
Epoch [56/120    avg_loss:0.049, val_acc:0.953]
Epoch [57/120    avg_loss:0.091, val_acc:0.953]
Epoch [58/120    avg_loss:0.053, val_acc:0.971]
Epoch [59/120    avg_loss:0.036, val_acc:0.978]
Epoch [60/120    avg_loss:0.034, val_acc:0.974]
Epoch [61/120    avg_loss:0.034, val_acc:0.976]
Epoch [62/120    avg_loss:0.024, val_acc:0.983]
Epoch [63/120    avg_loss:0.016, val_acc:0.983]
Epoch [64/120    avg_loss:0.014, val_acc:0.983]
Epoch [65/120    avg_loss:0.018, val_acc:0.981]
Epoch [66/120    avg_loss:0.025, val_acc:0.981]
Epoch [67/120    avg_loss:0.014, val_acc:0.981]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.015, val_acc:0.984]
Epoch [71/120    avg_loss:0.017, val_acc:0.984]
Epoch [72/120    avg_loss:0.018, val_acc:0.984]
Epoch [73/120    avg_loss:0.015, val_acc:0.986]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.016, val_acc:0.986]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.013, val_acc:0.984]
Epoch [83/120    avg_loss:0.015, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.015, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.015, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.011, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.015, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.014, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.985]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     1     0     0     1    29     0]
 [    0     1 18061     0    16     0    12     0     0     0]
 [    0     1     0  2017     1     0     0     0    13     4]
 [    0    42    22     0  2880     0     2     0    25     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     5     0     0  4867     0     0     0]
 [    0     0     0     0     0     0     2  1284     0     4]
 [    0    37     0    24    60     0     0     0  3435    15]
 [    0     0     0     0    14    64     0     0     0   841]]

Accuracy:
99.0311618827272

F1 scores:
[       nan 0.99132724 0.9984245  0.98824106 0.96904441 0.97606582
 0.99723389 0.99728155 0.97129931 0.94282511]

Kappa:
0.9871610320201145
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d0aa019b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.071, val_acc:0.306]
Epoch [2/120    avg_loss:1.763, val_acc:0.283]
Epoch [3/120    avg_loss:1.549, val_acc:0.339]
Epoch [4/120    avg_loss:1.365, val_acc:0.326]
Epoch [5/120    avg_loss:1.230, val_acc:0.410]
Epoch [6/120    avg_loss:1.108, val_acc:0.470]
Epoch [7/120    avg_loss:0.990, val_acc:0.645]
Epoch [8/120    avg_loss:0.916, val_acc:0.679]
Epoch [9/120    avg_loss:0.803, val_acc:0.694]
Epoch [10/120    avg_loss:0.756, val_acc:0.635]
Epoch [11/120    avg_loss:0.670, val_acc:0.674]
Epoch [12/120    avg_loss:0.591, val_acc:0.711]
Epoch [13/120    avg_loss:0.533, val_acc:0.711]
Epoch [14/120    avg_loss:0.483, val_acc:0.745]
Epoch [15/120    avg_loss:0.448, val_acc:0.806]
Epoch [16/120    avg_loss:0.399, val_acc:0.764]
Epoch [17/120    avg_loss:0.380, val_acc:0.850]
Epoch [18/120    avg_loss:0.337, val_acc:0.871]
Epoch [19/120    avg_loss:0.304, val_acc:0.898]
Epoch [20/120    avg_loss:0.267, val_acc:0.872]
Epoch [21/120    avg_loss:0.240, val_acc:0.897]
Epoch [22/120    avg_loss:0.195, val_acc:0.919]
Epoch [23/120    avg_loss:0.196, val_acc:0.921]
Epoch [24/120    avg_loss:0.204, val_acc:0.892]
Epoch [25/120    avg_loss:0.160, val_acc:0.925]
Epoch [26/120    avg_loss:0.147, val_acc:0.930]
Epoch [27/120    avg_loss:0.124, val_acc:0.923]
Epoch [28/120    avg_loss:0.134, val_acc:0.940]
Epoch [29/120    avg_loss:0.144, val_acc:0.944]
Epoch [30/120    avg_loss:0.115, val_acc:0.951]
Epoch [31/120    avg_loss:0.094, val_acc:0.950]
Epoch [32/120    avg_loss:0.084, val_acc:0.940]
Epoch [33/120    avg_loss:0.087, val_acc:0.954]
Epoch [34/120    avg_loss:0.077, val_acc:0.956]
Epoch [35/120    avg_loss:0.078, val_acc:0.960]
Epoch [36/120    avg_loss:0.068, val_acc:0.956]
Epoch [37/120    avg_loss:0.049, val_acc:0.958]
Epoch [38/120    avg_loss:0.057, val_acc:0.958]
Epoch [39/120    avg_loss:0.057, val_acc:0.963]
Epoch [40/120    avg_loss:0.060, val_acc:0.969]
Epoch [41/120    avg_loss:0.046, val_acc:0.953]
Epoch [42/120    avg_loss:0.045, val_acc:0.968]
Epoch [43/120    avg_loss:0.054, val_acc:0.960]
Epoch [44/120    avg_loss:0.054, val_acc:0.964]
Epoch [45/120    avg_loss:0.043, val_acc:0.956]
Epoch [46/120    avg_loss:0.055, val_acc:0.953]
Epoch [47/120    avg_loss:0.042, val_acc:0.962]
Epoch [48/120    avg_loss:0.031, val_acc:0.970]
Epoch [49/120    avg_loss:0.033, val_acc:0.971]
Epoch [50/120    avg_loss:0.023, val_acc:0.970]
Epoch [51/120    avg_loss:0.026, val_acc:0.972]
Epoch [52/120    avg_loss:0.032, val_acc:0.977]
Epoch [53/120    avg_loss:0.036, val_acc:0.968]
Epoch [54/120    avg_loss:0.029, val_acc:0.974]
Epoch [55/120    avg_loss:0.025, val_acc:0.972]
Epoch [56/120    avg_loss:0.028, val_acc:0.970]
Epoch [57/120    avg_loss:0.027, val_acc:0.976]
Epoch [58/120    avg_loss:0.019, val_acc:0.978]
Epoch [59/120    avg_loss:0.021, val_acc:0.977]
Epoch [60/120    avg_loss:0.020, val_acc:0.965]
Epoch [61/120    avg_loss:0.033, val_acc:0.971]
Epoch [62/120    avg_loss:0.047, val_acc:0.963]
Epoch [63/120    avg_loss:0.037, val_acc:0.977]
Epoch [64/120    avg_loss:0.023, val_acc:0.975]
Epoch [65/120    avg_loss:0.018, val_acc:0.974]
Epoch [66/120    avg_loss:0.030, val_acc:0.970]
Epoch [67/120    avg_loss:0.024, val_acc:0.967]
Epoch [68/120    avg_loss:0.014, val_acc:0.971]
Epoch [69/120    avg_loss:0.019, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.978]
Epoch [71/120    avg_loss:0.016, val_acc:0.978]
Epoch [72/120    avg_loss:0.013, val_acc:0.982]
Epoch [73/120    avg_loss:0.011, val_acc:0.977]
Epoch [74/120    avg_loss:0.023, val_acc:0.968]
Epoch [75/120    avg_loss:0.025, val_acc:0.978]
Epoch [76/120    avg_loss:0.019, val_acc:0.977]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.011, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.011, val_acc:0.980]
Epoch [81/120    avg_loss:0.017, val_acc:0.982]
Epoch [82/120    avg_loss:0.027, val_acc:0.971]
Epoch [83/120    avg_loss:0.024, val_acc:0.980]
Epoch [84/120    avg_loss:0.023, val_acc:0.986]
Epoch [85/120    avg_loss:0.015, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.980]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.981]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.013, val_acc:0.983]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.977]
Epoch [110/120    avg_loss:0.036, val_acc:0.978]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     1     0     0     0     0    92     0]
 [    0     4 18062     0    21     0     3     0     0     0]
 [    0     2     0  2030     0     0     0     0     3     1]
 [    0    50    19     0  2867     0     8     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     0     0  4864     0     0     0]
 [    0     6     0     0     0     0     0  1283     0     1]
 [    0    20     0    12    45     0     0     0  3494     0]
 [    0     2     0     4    15    35     0     0     2   861]]

Accuracy:
99.06490251367701

F1 scores:
[       nan 0.98623104 0.99831422 0.99436689 0.96858108 0.98676749
 0.99743669 0.99727944 0.97190542 0.96632997]

Kappa:
0.9876081957401073
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a958c3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.074, val_acc:0.204]
Epoch [2/120    avg_loss:1.733, val_acc:0.321]
Epoch [3/120    avg_loss:1.521, val_acc:0.333]
Epoch [4/120    avg_loss:1.335, val_acc:0.436]
Epoch [5/120    avg_loss:1.242, val_acc:0.456]
Epoch [6/120    avg_loss:1.088, val_acc:0.457]
Epoch [7/120    avg_loss:0.956, val_acc:0.601]
Epoch [8/120    avg_loss:0.858, val_acc:0.665]
Epoch [9/120    avg_loss:0.778, val_acc:0.657]
Epoch [10/120    avg_loss:0.686, val_acc:0.723]
Epoch [11/120    avg_loss:0.585, val_acc:0.757]
Epoch [12/120    avg_loss:0.502, val_acc:0.756]
Epoch [13/120    avg_loss:0.464, val_acc:0.839]
Epoch [14/120    avg_loss:0.402, val_acc:0.801]
Epoch [15/120    avg_loss:0.347, val_acc:0.855]
Epoch [16/120    avg_loss:0.280, val_acc:0.883]
Epoch [17/120    avg_loss:0.255, val_acc:0.916]
Epoch [18/120    avg_loss:0.246, val_acc:0.906]
Epoch [19/120    avg_loss:0.219, val_acc:0.932]
Epoch [20/120    avg_loss:0.190, val_acc:0.928]
Epoch [21/120    avg_loss:0.234, val_acc:0.910]
Epoch [22/120    avg_loss:0.188, val_acc:0.918]
Epoch [23/120    avg_loss:0.174, val_acc:0.943]
Epoch [24/120    avg_loss:0.162, val_acc:0.930]
Epoch [25/120    avg_loss:0.150, val_acc:0.938]
Epoch [26/120    avg_loss:0.157, val_acc:0.919]
Epoch [27/120    avg_loss:0.125, val_acc:0.910]
Epoch [28/120    avg_loss:0.122, val_acc:0.952]
Epoch [29/120    avg_loss:0.101, val_acc:0.959]
Epoch [30/120    avg_loss:0.099, val_acc:0.945]
Epoch [31/120    avg_loss:0.111, val_acc:0.955]
Epoch [32/120    avg_loss:0.083, val_acc:0.971]
Epoch [33/120    avg_loss:0.079, val_acc:0.960]
Epoch [34/120    avg_loss:0.079, val_acc:0.968]
Epoch [35/120    avg_loss:0.060, val_acc:0.964]
Epoch [36/120    avg_loss:0.085, val_acc:0.957]
Epoch [37/120    avg_loss:0.055, val_acc:0.973]
Epoch [38/120    avg_loss:0.058, val_acc:0.969]
Epoch [39/120    avg_loss:0.065, val_acc:0.930]
Epoch [40/120    avg_loss:0.057, val_acc:0.965]
Epoch [41/120    avg_loss:0.062, val_acc:0.976]
Epoch [42/120    avg_loss:0.040, val_acc:0.975]
Epoch [43/120    avg_loss:0.049, val_acc:0.968]
Epoch [44/120    avg_loss:0.063, val_acc:0.931]
Epoch [45/120    avg_loss:0.068, val_acc:0.970]
Epoch [46/120    avg_loss:0.049, val_acc:0.975]
Epoch [47/120    avg_loss:0.056, val_acc:0.973]
Epoch [48/120    avg_loss:0.042, val_acc:0.974]
Epoch [49/120    avg_loss:0.053, val_acc:0.980]
Epoch [50/120    avg_loss:0.038, val_acc:0.970]
Epoch [51/120    avg_loss:0.042, val_acc:0.975]
Epoch [52/120    avg_loss:0.044, val_acc:0.970]
Epoch [53/120    avg_loss:0.053, val_acc:0.980]
Epoch [54/120    avg_loss:0.036, val_acc:0.977]
Epoch [55/120    avg_loss:0.030, val_acc:0.976]
Epoch [56/120    avg_loss:0.023, val_acc:0.978]
Epoch [57/120    avg_loss:0.030, val_acc:0.978]
Epoch [58/120    avg_loss:0.045, val_acc:0.951]
Epoch [59/120    avg_loss:0.048, val_acc:0.981]
Epoch [60/120    avg_loss:0.027, val_acc:0.984]
Epoch [61/120    avg_loss:0.022, val_acc:0.981]
Epoch [62/120    avg_loss:0.026, val_acc:0.979]
Epoch [63/120    avg_loss:0.018, val_acc:0.986]
Epoch [64/120    avg_loss:0.021, val_acc:0.981]
Epoch [65/120    avg_loss:0.014, val_acc:0.985]
Epoch [66/120    avg_loss:0.018, val_acc:0.980]
Epoch [67/120    avg_loss:0.024, val_acc:0.982]
Epoch [68/120    avg_loss:0.018, val_acc:0.984]
Epoch [69/120    avg_loss:0.061, val_acc:0.952]
Epoch [70/120    avg_loss:0.037, val_acc:0.980]
Epoch [71/120    avg_loss:0.021, val_acc:0.986]
Epoch [72/120    avg_loss:0.020, val_acc:0.981]
Epoch [73/120    avg_loss:0.016, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.988]
Epoch [75/120    avg_loss:0.017, val_acc:0.986]
Epoch [76/120    avg_loss:0.069, val_acc:0.967]
Epoch [77/120    avg_loss:0.082, val_acc:0.934]
Epoch [78/120    avg_loss:0.059, val_acc:0.980]
Epoch [79/120    avg_loss:0.026, val_acc:0.983]
Epoch [80/120    avg_loss:0.024, val_acc:0.981]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.017, val_acc:0.988]
Epoch [83/120    avg_loss:0.011, val_acc:0.989]
Epoch [84/120    avg_loss:0.018, val_acc:0.969]
Epoch [85/120    avg_loss:0.038, val_acc:0.978]
Epoch [86/120    avg_loss:0.021, val_acc:0.986]
Epoch [87/120    avg_loss:0.028, val_acc:0.986]
Epoch [88/120    avg_loss:0.019, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.026, val_acc:0.985]
Epoch [94/120    avg_loss:0.023, val_acc:0.982]
Epoch [95/120    avg_loss:0.015, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6360     0     0     0     0    17     0    52     3]
 [    0     2 18047     0    27     0    10     0     4     0]
 [    0     3     0  2020     2     0     0     0    11     0]
 [    0    39    21     0  2874     0    10     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     7     0     0  4870     0     0     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0    14     0    16    50     0     0     0  3485     6]
 [    0     0     0     0     8    34     0     0     0   877]]

Accuracy:
99.11551346010171

F1 scores:
[       nan 0.98988327 0.99820238 0.99043883 0.96881847 0.9871407
 0.99519771 0.9992242  0.97468885 0.97174515]

Kappa:
0.9882826664135604
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb6e297978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.056, val_acc:0.123]
Epoch [2/120    avg_loss:1.729, val_acc:0.138]
Epoch [3/120    avg_loss:1.554, val_acc:0.247]
Epoch [4/120    avg_loss:1.366, val_acc:0.381]
Epoch [5/120    avg_loss:1.238, val_acc:0.405]
Epoch [6/120    avg_loss:1.110, val_acc:0.432]
Epoch [7/120    avg_loss:0.950, val_acc:0.472]
Epoch [8/120    avg_loss:0.819, val_acc:0.542]
Epoch [9/120    avg_loss:0.699, val_acc:0.632]
Epoch [10/120    avg_loss:0.589, val_acc:0.722]
Epoch [11/120    avg_loss:0.508, val_acc:0.794]
Epoch [12/120    avg_loss:0.467, val_acc:0.792]
Epoch [13/120    avg_loss:0.387, val_acc:0.835]
Epoch [14/120    avg_loss:0.374, val_acc:0.859]
Epoch [15/120    avg_loss:0.309, val_acc:0.856]
Epoch [16/120    avg_loss:0.261, val_acc:0.883]
Epoch [17/120    avg_loss:0.246, val_acc:0.878]
Epoch [18/120    avg_loss:0.233, val_acc:0.885]
Epoch [19/120    avg_loss:0.201, val_acc:0.944]
Epoch [20/120    avg_loss:0.201, val_acc:0.936]
Epoch [21/120    avg_loss:0.175, val_acc:0.918]
Epoch [22/120    avg_loss:0.197, val_acc:0.937]
Epoch [23/120    avg_loss:0.121, val_acc:0.947]
Epoch [24/120    avg_loss:0.163, val_acc:0.872]
Epoch [25/120    avg_loss:0.159, val_acc:0.815]
Epoch [26/120    avg_loss:0.132, val_acc:0.958]
Epoch [27/120    avg_loss:0.111, val_acc:0.954]
Epoch [28/120    avg_loss:0.119, val_acc:0.942]
Epoch [29/120    avg_loss:0.115, val_acc:0.946]
Epoch [30/120    avg_loss:0.098, val_acc:0.932]
Epoch [31/120    avg_loss:0.101, val_acc:0.970]
Epoch [32/120    avg_loss:0.075, val_acc:0.974]
Epoch [33/120    avg_loss:0.061, val_acc:0.949]
Epoch [34/120    avg_loss:0.073, val_acc:0.932]
Epoch [35/120    avg_loss:0.073, val_acc:0.964]
Epoch [36/120    avg_loss:0.079, val_acc:0.976]
Epoch [37/120    avg_loss:0.069, val_acc:0.959]
Epoch [38/120    avg_loss:0.079, val_acc:0.959]
Epoch [39/120    avg_loss:0.065, val_acc:0.962]
Epoch [40/120    avg_loss:0.094, val_acc:0.973]
Epoch [41/120    avg_loss:0.066, val_acc:0.965]
Epoch [42/120    avg_loss:0.067, val_acc:0.965]
Epoch [43/120    avg_loss:0.074, val_acc:0.965]
Epoch [44/120    avg_loss:0.043, val_acc:0.962]
Epoch [45/120    avg_loss:0.049, val_acc:0.971]
Epoch [46/120    avg_loss:0.041, val_acc:0.973]
Epoch [47/120    avg_loss:0.044, val_acc:0.975]
Epoch [48/120    avg_loss:0.035, val_acc:0.969]
Epoch [49/120    avg_loss:0.032, val_acc:0.968]
Epoch [50/120    avg_loss:0.026, val_acc:0.983]
Epoch [51/120    avg_loss:0.025, val_acc:0.986]
Epoch [52/120    avg_loss:0.020, val_acc:0.985]
Epoch [53/120    avg_loss:0.018, val_acc:0.983]
Epoch [54/120    avg_loss:0.019, val_acc:0.983]
Epoch [55/120    avg_loss:0.021, val_acc:0.983]
Epoch [56/120    avg_loss:0.022, val_acc:0.981]
Epoch [57/120    avg_loss:0.017, val_acc:0.982]
Epoch [58/120    avg_loss:0.017, val_acc:0.983]
Epoch [59/120    avg_loss:0.015, val_acc:0.983]
Epoch [60/120    avg_loss:0.022, val_acc:0.984]
Epoch [61/120    avg_loss:0.023, val_acc:0.982]
Epoch [62/120    avg_loss:0.016, val_acc:0.983]
Epoch [63/120    avg_loss:0.016, val_acc:0.984]
Epoch [64/120    avg_loss:0.017, val_acc:0.986]
Epoch [65/120    avg_loss:0.019, val_acc:0.986]
Epoch [66/120    avg_loss:0.018, val_acc:0.983]
Epoch [67/120    avg_loss:0.015, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.985]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.016, val_acc:0.986]
Epoch [71/120    avg_loss:0.017, val_acc:0.986]
Epoch [72/120    avg_loss:0.016, val_acc:0.984]
Epoch [73/120    avg_loss:0.015, val_acc:0.985]
Epoch [74/120    avg_loss:0.020, val_acc:0.981]
Epoch [75/120    avg_loss:0.018, val_acc:0.984]
Epoch [76/120    avg_loss:0.016, val_acc:0.984]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.021, val_acc:0.986]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.015, val_acc:0.987]
Epoch [82/120    avg_loss:0.016, val_acc:0.987]
Epoch [83/120    avg_loss:0.020, val_acc:0.986]
Epoch [84/120    avg_loss:0.018, val_acc:0.986]
Epoch [85/120    avg_loss:0.017, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.983]
Epoch [87/120    avg_loss:0.018, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.986]
Epoch [89/120    avg_loss:0.017, val_acc:0.984]
Epoch [90/120    avg_loss:0.016, val_acc:0.983]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.984]
Epoch [94/120    avg_loss:0.013, val_acc:0.984]
Epoch [95/120    avg_loss:0.016, val_acc:0.986]
Epoch [96/120    avg_loss:0.019, val_acc:0.986]
Epoch [97/120    avg_loss:0.015, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.017, val_acc:0.986]
Epoch [100/120    avg_loss:0.016, val_acc:0.986]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.018, val_acc:0.985]
Epoch [103/120    avg_loss:0.018, val_acc:0.985]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.014, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.014, val_acc:0.985]
Epoch [108/120    avg_loss:0.015, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.017, val_acc:0.985]
Epoch [112/120    avg_loss:0.015, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.986]
Epoch [114/120    avg_loss:0.013, val_acc:0.986]
Epoch [115/120    avg_loss:0.016, val_acc:0.985]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.013, val_acc:0.985]
Epoch [119/120    avg_loss:0.014, val_acc:0.985]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     4     0    20     0    30     5]
 [    0     0 17990     0    28     0    60     0    12     0]
 [    0    11     0  2001     0     0     0     0    24     0]
 [    0    57    20     5  2860     0     4     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4869     0     3     0]
 [    0     1     0     0     0     2     3  1282     0     2]
 [    0    17     0    18    54     0     0     0  3477     5]
 [    0     0     0     2    14    39     0     0     0   864]]

Accuracy:
98.86245872797822

F1 scores:
[       nan 0.98875184 0.9966759  0.98377581 0.96426163 0.98453414
 0.99023795 0.99688958 0.97354053 0.96267409]

Kappa:
0.984937667682806
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48badf49b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.042, val_acc:0.212]
Epoch [2/120    avg_loss:1.713, val_acc:0.275]
Epoch [3/120    avg_loss:1.496, val_acc:0.296]
Epoch [4/120    avg_loss:1.327, val_acc:0.365]
Epoch [5/120    avg_loss:1.122, val_acc:0.409]
Epoch [6/120    avg_loss:0.967, val_acc:0.486]
Epoch [7/120    avg_loss:0.807, val_acc:0.492]
Epoch [8/120    avg_loss:0.740, val_acc:0.547]
Epoch [9/120    avg_loss:0.640, val_acc:0.618]
Epoch [10/120    avg_loss:0.601, val_acc:0.741]
Epoch [11/120    avg_loss:0.498, val_acc:0.700]
Epoch [12/120    avg_loss:0.452, val_acc:0.752]
Epoch [13/120    avg_loss:0.413, val_acc:0.714]
Epoch [14/120    avg_loss:0.362, val_acc:0.790]
Epoch [15/120    avg_loss:0.319, val_acc:0.800]
Epoch [16/120    avg_loss:0.311, val_acc:0.837]
Epoch [17/120    avg_loss:0.265, val_acc:0.829]
Epoch [18/120    avg_loss:0.316, val_acc:0.812]
Epoch [19/120    avg_loss:0.282, val_acc:0.836]
Epoch [20/120    avg_loss:0.226, val_acc:0.856]
Epoch [21/120    avg_loss:0.212, val_acc:0.876]
Epoch [22/120    avg_loss:0.184, val_acc:0.872]
Epoch [23/120    avg_loss:0.169, val_acc:0.910]
Epoch [24/120    avg_loss:0.167, val_acc:0.865]
Epoch [25/120    avg_loss:0.173, val_acc:0.921]
Epoch [26/120    avg_loss:0.124, val_acc:0.928]
Epoch [27/120    avg_loss:0.117, val_acc:0.916]
Epoch [28/120    avg_loss:0.106, val_acc:0.929]
Epoch [29/120    avg_loss:0.120, val_acc:0.922]
Epoch [30/120    avg_loss:0.133, val_acc:0.894]
Epoch [31/120    avg_loss:0.153, val_acc:0.930]
Epoch [32/120    avg_loss:0.104, val_acc:0.959]
Epoch [33/120    avg_loss:0.079, val_acc:0.956]
Epoch [34/120    avg_loss:0.075, val_acc:0.912]
Epoch [35/120    avg_loss:0.100, val_acc:0.933]
Epoch [36/120    avg_loss:0.102, val_acc:0.952]
Epoch [37/120    avg_loss:0.080, val_acc:0.965]
Epoch [38/120    avg_loss:0.060, val_acc:0.928]
Epoch [39/120    avg_loss:0.073, val_acc:0.944]
Epoch [40/120    avg_loss:0.066, val_acc:0.955]
Epoch [41/120    avg_loss:0.045, val_acc:0.973]
Epoch [42/120    avg_loss:0.056, val_acc:0.957]
Epoch [43/120    avg_loss:0.059, val_acc:0.967]
Epoch [44/120    avg_loss:0.067, val_acc:0.949]
Epoch [45/120    avg_loss:0.052, val_acc:0.967]
Epoch [46/120    avg_loss:0.045, val_acc:0.968]
Epoch [47/120    avg_loss:0.073, val_acc:0.947]
Epoch [48/120    avg_loss:0.046, val_acc:0.934]
Epoch [49/120    avg_loss:0.038, val_acc:0.975]
Epoch [50/120    avg_loss:0.032, val_acc:0.980]
Epoch [51/120    avg_loss:0.033, val_acc:0.972]
Epoch [52/120    avg_loss:0.027, val_acc:0.973]
Epoch [53/120    avg_loss:0.031, val_acc:0.979]
Epoch [54/120    avg_loss:0.046, val_acc:0.979]
Epoch [55/120    avg_loss:0.030, val_acc:0.978]
Epoch [56/120    avg_loss:0.026, val_acc:0.976]
Epoch [57/120    avg_loss:0.025, val_acc:0.984]
Epoch [58/120    avg_loss:0.022, val_acc:0.977]
Epoch [59/120    avg_loss:0.042, val_acc:0.963]
Epoch [60/120    avg_loss:0.028, val_acc:0.977]
Epoch [61/120    avg_loss:0.023, val_acc:0.979]
Epoch [62/120    avg_loss:0.049, val_acc:0.972]
Epoch [63/120    avg_loss:0.049, val_acc:0.965]
Epoch [64/120    avg_loss:0.031, val_acc:0.981]
Epoch [65/120    avg_loss:0.040, val_acc:0.975]
Epoch [66/120    avg_loss:0.025, val_acc:0.976]
Epoch [67/120    avg_loss:0.028, val_acc:0.982]
Epoch [68/120    avg_loss:0.019, val_acc:0.981]
Epoch [69/120    avg_loss:0.018, val_acc:0.976]
Epoch [70/120    avg_loss:0.016, val_acc:0.981]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.010, val_acc:0.980]
Epoch [73/120    avg_loss:0.012, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.009, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.981]
Epoch [79/120    avg_loss:0.009, val_acc:0.982]
Epoch [80/120    avg_loss:0.014, val_acc:0.982]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.011, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6410     0     0     0     0     0     0    22     0]
 [    0     0 17919     0    64     0   107     0     0     0]
 [    0    11     0  1945     0     0     0     0    75     5]
 [    0    31    24     0  2880     0     7     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     8     0     0  4866     0     1     1]
 [    0     3     0     0     0     0     2  1283     0     2]
 [    0    34     0     0    51     0     0     0  3486     0]
 [    0     0     0     0    16    49     0     0     0   854]]

Accuracy:
98.6865254380257

F1 scores:
[       nan 0.99218327 0.99453309 0.97518175 0.96272773 0.98157202
 0.98701826 0.99727944 0.97035491 0.95901179]

Kappa:
0.982617690423031
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70bf46f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.053, val_acc:0.141]
Epoch [2/120    avg_loss:1.737, val_acc:0.151]
Epoch [3/120    avg_loss:1.522, val_acc:0.149]
Epoch [4/120    avg_loss:1.392, val_acc:0.284]
Epoch [5/120    avg_loss:1.206, val_acc:0.500]
Epoch [6/120    avg_loss:1.067, val_acc:0.650]
Epoch [7/120    avg_loss:0.929, val_acc:0.656]
Epoch [8/120    avg_loss:0.876, val_acc:0.678]
Epoch [9/120    avg_loss:0.742, val_acc:0.634]
Epoch [10/120    avg_loss:0.667, val_acc:0.655]
Epoch [11/120    avg_loss:0.568, val_acc:0.694]
Epoch [12/120    avg_loss:0.525, val_acc:0.736]
Epoch [13/120    avg_loss:0.440, val_acc:0.741]
Epoch [14/120    avg_loss:0.402, val_acc:0.763]
Epoch [15/120    avg_loss:0.374, val_acc:0.805]
Epoch [16/120    avg_loss:0.331, val_acc:0.802]
Epoch [17/120    avg_loss:0.329, val_acc:0.838]
Epoch [18/120    avg_loss:0.283, val_acc:0.859]
Epoch [19/120    avg_loss:0.285, val_acc:0.858]
Epoch [20/120    avg_loss:0.231, val_acc:0.917]
Epoch [21/120    avg_loss:0.218, val_acc:0.923]
Epoch [22/120    avg_loss:0.202, val_acc:0.924]
Epoch [23/120    avg_loss:0.192, val_acc:0.927]
Epoch [24/120    avg_loss:0.173, val_acc:0.927]
Epoch [25/120    avg_loss:0.143, val_acc:0.942]
Epoch [26/120    avg_loss:0.122, val_acc:0.952]
Epoch [27/120    avg_loss:0.112, val_acc:0.969]
Epoch [28/120    avg_loss:0.103, val_acc:0.956]
Epoch [29/120    avg_loss:0.099, val_acc:0.962]
Epoch [30/120    avg_loss:0.138, val_acc:0.944]
Epoch [31/120    avg_loss:0.119, val_acc:0.957]
Epoch [32/120    avg_loss:0.085, val_acc:0.951]
Epoch [33/120    avg_loss:0.108, val_acc:0.965]
Epoch [34/120    avg_loss:0.073, val_acc:0.975]
Epoch [35/120    avg_loss:0.065, val_acc:0.957]
Epoch [36/120    avg_loss:0.060, val_acc:0.972]
Epoch [37/120    avg_loss:0.059, val_acc:0.962]
Epoch [38/120    avg_loss:0.055, val_acc:0.977]
Epoch [39/120    avg_loss:0.061, val_acc:0.939]
Epoch [40/120    avg_loss:0.079, val_acc:0.972]
Epoch [41/120    avg_loss:0.060, val_acc:0.965]
Epoch [42/120    avg_loss:0.045, val_acc:0.968]
Epoch [43/120    avg_loss:0.057, val_acc:0.969]
Epoch [44/120    avg_loss:0.041, val_acc:0.964]
Epoch [45/120    avg_loss:0.056, val_acc:0.976]
Epoch [46/120    avg_loss:0.048, val_acc:0.973]
Epoch [47/120    avg_loss:0.048, val_acc:0.978]
Epoch [48/120    avg_loss:0.035, val_acc:0.978]
Epoch [49/120    avg_loss:0.027, val_acc:0.983]
Epoch [50/120    avg_loss:0.038, val_acc:0.971]
Epoch [51/120    avg_loss:0.032, val_acc:0.973]
Epoch [52/120    avg_loss:0.036, val_acc:0.977]
Epoch [53/120    avg_loss:0.025, val_acc:0.981]
Epoch [54/120    avg_loss:0.076, val_acc:0.940]
Epoch [55/120    avg_loss:0.058, val_acc:0.975]
Epoch [56/120    avg_loss:0.040, val_acc:0.973]
Epoch [57/120    avg_loss:0.040, val_acc:0.982]
Epoch [58/120    avg_loss:0.018, val_acc:0.983]
Epoch [59/120    avg_loss:0.024, val_acc:0.982]
Epoch [60/120    avg_loss:0.048, val_acc:0.976]
Epoch [61/120    avg_loss:0.033, val_acc:0.966]
Epoch [62/120    avg_loss:0.031, val_acc:0.982]
Epoch [63/120    avg_loss:0.029, val_acc:0.972]
Epoch [64/120    avg_loss:0.020, val_acc:0.980]
Epoch [65/120    avg_loss:0.021, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.984]
Epoch [67/120    avg_loss:0.025, val_acc:0.984]
Epoch [68/120    avg_loss:0.021, val_acc:0.977]
Epoch [69/120    avg_loss:0.023, val_acc:0.980]
Epoch [70/120    avg_loss:0.022, val_acc:0.983]
Epoch [71/120    avg_loss:0.018, val_acc:0.983]
Epoch [72/120    avg_loss:0.016, val_acc:0.982]
Epoch [73/120    avg_loss:0.014, val_acc:0.985]
Epoch [74/120    avg_loss:0.014, val_acc:0.984]
Epoch [75/120    avg_loss:0.020, val_acc:0.974]
Epoch [76/120    avg_loss:0.026, val_acc:0.984]
Epoch [77/120    avg_loss:0.021, val_acc:0.979]
Epoch [78/120    avg_loss:0.013, val_acc:0.989]
Epoch [79/120    avg_loss:0.014, val_acc:0.984]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.009, val_acc:0.984]
Epoch [83/120    avg_loss:0.018, val_acc:0.979]
Epoch [84/120    avg_loss:0.018, val_acc:0.977]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.013, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.980]
Epoch [89/120    avg_loss:0.021, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.990]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     1     0     0    10    19     0]
 [    0     0 18014     0    58     0    14     0     4     0]
 [    0     9     1  2002     1     0     0     0    22     1]
 [    0    26    25     0  2894     0     8     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4871     0     0     2]
 [    0     1     0     0     0     0     0  1283     0     6]
 [    0    30     0     0    49     0     0     0  3492     0]
 [    0     0     0     0    15    50     0     0     0   854]]

Accuracy:
99.09382305449112

F1 scores:
[       nan 0.99255814 0.99703888 0.99157999 0.96627713 0.98120301
 0.99703203 0.99341851 0.97993546 0.95847363]

Kappa:
0.9879970215189524
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50dbb31908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.092, val_acc:0.155]
Epoch [2/120    avg_loss:1.770, val_acc:0.251]
Epoch [3/120    avg_loss:1.590, val_acc:0.344]
Epoch [4/120    avg_loss:1.382, val_acc:0.411]
Epoch [5/120    avg_loss:1.280, val_acc:0.449]
Epoch [6/120    avg_loss:1.090, val_acc:0.489]
Epoch [7/120    avg_loss:0.984, val_acc:0.503]
Epoch [8/120    avg_loss:0.842, val_acc:0.534]
Epoch [9/120    avg_loss:0.686, val_acc:0.615]
Epoch [10/120    avg_loss:0.641, val_acc:0.681]
Epoch [11/120    avg_loss:0.525, val_acc:0.756]
Epoch [12/120    avg_loss:0.467, val_acc:0.812]
Epoch [13/120    avg_loss:0.429, val_acc:0.812]
Epoch [14/120    avg_loss:0.402, val_acc:0.869]
Epoch [15/120    avg_loss:0.340, val_acc:0.861]
Epoch [16/120    avg_loss:0.345, val_acc:0.890]
Epoch [17/120    avg_loss:0.277, val_acc:0.889]
Epoch [18/120    avg_loss:0.236, val_acc:0.909]
Epoch [19/120    avg_loss:0.218, val_acc:0.862]
Epoch [20/120    avg_loss:0.281, val_acc:0.928]
Epoch [21/120    avg_loss:0.179, val_acc:0.942]
Epoch [22/120    avg_loss:0.174, val_acc:0.953]
Epoch [23/120    avg_loss:0.146, val_acc:0.943]
Epoch [24/120    avg_loss:0.173, val_acc:0.906]
Epoch [25/120    avg_loss:0.161, val_acc:0.959]
Epoch [26/120    avg_loss:0.137, val_acc:0.952]
Epoch [27/120    avg_loss:0.110, val_acc:0.968]
Epoch [28/120    avg_loss:0.092, val_acc:0.961]
Epoch [29/120    avg_loss:0.086, val_acc:0.967]
Epoch [30/120    avg_loss:0.100, val_acc:0.957]
Epoch [31/120    avg_loss:0.075, val_acc:0.964]
Epoch [32/120    avg_loss:0.066, val_acc:0.966]
Epoch [33/120    avg_loss:0.097, val_acc:0.963]
Epoch [34/120    avg_loss:0.102, val_acc:0.962]
Epoch [35/120    avg_loss:0.070, val_acc:0.967]
Epoch [36/120    avg_loss:0.072, val_acc:0.970]
Epoch [37/120    avg_loss:0.051, val_acc:0.978]
Epoch [38/120    avg_loss:0.054, val_acc:0.977]
Epoch [39/120    avg_loss:0.047, val_acc:0.977]
Epoch [40/120    avg_loss:0.043, val_acc:0.977]
Epoch [41/120    avg_loss:0.043, val_acc:0.977]
Epoch [42/120    avg_loss:0.049, val_acc:0.972]
Epoch [43/120    avg_loss:0.045, val_acc:0.966]
Epoch [44/120    avg_loss:0.038, val_acc:0.975]
Epoch [45/120    avg_loss:0.040, val_acc:0.969]
Epoch [46/120    avg_loss:0.030, val_acc:0.978]
Epoch [47/120    avg_loss:0.027, val_acc:0.971]
Epoch [48/120    avg_loss:0.027, val_acc:0.980]
Epoch [49/120    avg_loss:0.026, val_acc:0.971]
Epoch [50/120    avg_loss:0.033, val_acc:0.970]
Epoch [51/120    avg_loss:0.025, val_acc:0.986]
Epoch [52/120    avg_loss:0.025, val_acc:0.985]
Epoch [53/120    avg_loss:0.028, val_acc:0.978]
Epoch [54/120    avg_loss:0.027, val_acc:0.970]
Epoch [55/120    avg_loss:0.040, val_acc:0.981]
Epoch [56/120    avg_loss:0.021, val_acc:0.980]
Epoch [57/120    avg_loss:0.016, val_acc:0.985]
Epoch [58/120    avg_loss:0.021, val_acc:0.977]
Epoch [59/120    avg_loss:0.045, val_acc:0.968]
Epoch [60/120    avg_loss:0.033, val_acc:0.982]
Epoch [61/120    avg_loss:0.050, val_acc:0.955]
Epoch [62/120    avg_loss:0.040, val_acc:0.984]
Epoch [63/120    avg_loss:0.026, val_acc:0.979]
Epoch [64/120    avg_loss:0.015, val_acc:0.990]
Epoch [65/120    avg_loss:0.024, val_acc:0.980]
Epoch [66/120    avg_loss:0.030, val_acc:0.986]
Epoch [67/120    avg_loss:0.016, val_acc:0.983]
Epoch [68/120    avg_loss:0.022, val_acc:0.973]
Epoch [69/120    avg_loss:0.012, val_acc:0.986]
Epoch [70/120    avg_loss:0.014, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.990]
Epoch [73/120    avg_loss:0.013, val_acc:0.985]
Epoch [74/120    avg_loss:0.015, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.989]
Epoch [76/120    avg_loss:0.018, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.988]
Epoch [78/120    avg_loss:0.015, val_acc:0.988]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.037, val_acc:0.970]
Epoch [81/120    avg_loss:0.648, val_acc:0.355]
Epoch [82/120    avg_loss:1.258, val_acc:0.391]
Epoch [83/120    avg_loss:0.855, val_acc:0.617]
Epoch [84/120    avg_loss:0.662, val_acc:0.671]
Epoch [85/120    avg_loss:0.548, val_acc:0.707]
Epoch [86/120    avg_loss:0.494, val_acc:0.766]
Epoch [87/120    avg_loss:0.444, val_acc:0.780]
Epoch [88/120    avg_loss:0.446, val_acc:0.803]
Epoch [89/120    avg_loss:0.447, val_acc:0.769]
Epoch [90/120    avg_loss:0.420, val_acc:0.778]
Epoch [91/120    avg_loss:0.410, val_acc:0.806]
Epoch [92/120    avg_loss:0.390, val_acc:0.768]
Epoch [93/120    avg_loss:0.404, val_acc:0.770]
Epoch [94/120    avg_loss:0.401, val_acc:0.788]
Epoch [95/120    avg_loss:0.419, val_acc:0.799]
Epoch [96/120    avg_loss:0.378, val_acc:0.783]
Epoch [97/120    avg_loss:0.392, val_acc:0.781]
Epoch [98/120    avg_loss:0.376, val_acc:0.777]
Epoch [99/120    avg_loss:0.371, val_acc:0.794]
Epoch [100/120    avg_loss:0.373, val_acc:0.806]
Epoch [101/120    avg_loss:0.372, val_acc:0.806]
Epoch [102/120    avg_loss:0.374, val_acc:0.803]
Epoch [103/120    avg_loss:0.353, val_acc:0.799]
Epoch [104/120    avg_loss:0.388, val_acc:0.806]
Epoch [105/120    avg_loss:0.365, val_acc:0.806]
Epoch [106/120    avg_loss:0.368, val_acc:0.806]
Epoch [107/120    avg_loss:0.364, val_acc:0.806]
Epoch [108/120    avg_loss:0.347, val_acc:0.805]
Epoch [109/120    avg_loss:0.350, val_acc:0.807]
Epoch [110/120    avg_loss:0.361, val_acc:0.807]
Epoch [111/120    avg_loss:0.355, val_acc:0.806]
Epoch [112/120    avg_loss:0.350, val_acc:0.806]
Epoch [113/120    avg_loss:0.347, val_acc:0.806]
Epoch [114/120    avg_loss:0.348, val_acc:0.806]
Epoch [115/120    avg_loss:0.358, val_acc:0.806]
Epoch [116/120    avg_loss:0.353, val_acc:0.807]
Epoch [117/120    avg_loss:0.364, val_acc:0.807]
Epoch [118/120    avg_loss:0.347, val_acc:0.807]
Epoch [119/120    avg_loss:0.354, val_acc:0.809]
Epoch [120/120    avg_loss:0.340, val_acc:0.809]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4833    21    38   559     0    69   111   647   154]
 [    0     1 14594     0   194     0  3301     0     0     0]
 [    0    22     0  1733     3     0    11     5   151   111]
 [    0   158    35     4  2661     0    55     0    53     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0    51   394     0     0     0  4418     0    15     0]
 [    0   101     0     0     0     0    26  1132     3    28]
 [    0   168     7     0    54     0     0     0  3342     0]
 [    0     6     0     8    19   106     0     3    11   766]]

Accuracy:
83.83100763984287

F1 scores:
[       nan 0.82110092 0.88072176 0.90756743 0.82358403 0.96097202
 0.69258504 0.8909878  0.8576928  0.77217742]

Kappa:
0.7927354724359399
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca1028e9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.118, val_acc:0.470]
Epoch [2/120    avg_loss:1.829, val_acc:0.540]
Epoch [3/120    avg_loss:1.632, val_acc:0.562]
Epoch [4/120    avg_loss:1.487, val_acc:0.556]
Epoch [5/120    avg_loss:1.310, val_acc:0.494]
Epoch [6/120    avg_loss:1.158, val_acc:0.544]
Epoch [7/120    avg_loss:1.020, val_acc:0.570]
Epoch [8/120    avg_loss:0.935, val_acc:0.683]
Epoch [9/120    avg_loss:0.799, val_acc:0.605]
Epoch [10/120    avg_loss:0.770, val_acc:0.650]
Epoch [11/120    avg_loss:0.645, val_acc:0.721]
Epoch [12/120    avg_loss:0.542, val_acc:0.779]
Epoch [13/120    avg_loss:0.510, val_acc:0.801]
Epoch [14/120    avg_loss:0.461, val_acc:0.806]
Epoch [15/120    avg_loss:0.419, val_acc:0.834]
Epoch [16/120    avg_loss:0.349, val_acc:0.885]
Epoch [17/120    avg_loss:0.334, val_acc:0.897]
Epoch [18/120    avg_loss:0.290, val_acc:0.882]
Epoch [19/120    avg_loss:0.252, val_acc:0.946]
Epoch [20/120    avg_loss:0.824, val_acc:0.633]
Epoch [21/120    avg_loss:0.612, val_acc:0.768]
Epoch [22/120    avg_loss:0.538, val_acc:0.699]
Epoch [23/120    avg_loss:0.484, val_acc:0.843]
Epoch [24/120    avg_loss:0.310, val_acc:0.901]
Epoch [25/120    avg_loss:0.246, val_acc:0.918]
Epoch [26/120    avg_loss:0.215, val_acc:0.918]
Epoch [27/120    avg_loss:0.252, val_acc:0.930]
Epoch [28/120    avg_loss:0.177, val_acc:0.937]
Epoch [29/120    avg_loss:0.165, val_acc:0.964]
Epoch [30/120    avg_loss:0.137, val_acc:0.918]
Epoch [31/120    avg_loss:0.144, val_acc:0.950]
Epoch [32/120    avg_loss:0.176, val_acc:0.944]
Epoch [33/120    avg_loss:0.152, val_acc:0.957]
Epoch [34/120    avg_loss:0.114, val_acc:0.960]
Epoch [35/120    avg_loss:0.112, val_acc:0.951]
Epoch [36/120    avg_loss:0.476, val_acc:0.815]
Epoch [37/120    avg_loss:0.399, val_acc:0.893]
Epoch [38/120    avg_loss:0.276, val_acc:0.924]
Epoch [39/120    avg_loss:0.245, val_acc:0.929]
Epoch [40/120    avg_loss:0.277, val_acc:0.859]
Epoch [41/120    avg_loss:0.222, val_acc:0.941]
Epoch [42/120    avg_loss:0.176, val_acc:0.937]
Epoch [43/120    avg_loss:0.134, val_acc:0.951]
Epoch [44/120    avg_loss:0.130, val_acc:0.958]
Epoch [45/120    avg_loss:0.117, val_acc:0.958]
Epoch [46/120    avg_loss:0.107, val_acc:0.960]
Epoch [47/120    avg_loss:0.114, val_acc:0.963]
Epoch [48/120    avg_loss:0.102, val_acc:0.965]
Epoch [49/120    avg_loss:0.105, val_acc:0.966]
Epoch [50/120    avg_loss:0.112, val_acc:0.964]
Epoch [51/120    avg_loss:0.101, val_acc:0.965]
Epoch [52/120    avg_loss:0.099, val_acc:0.963]
Epoch [53/120    avg_loss:0.100, val_acc:0.963]
Epoch [54/120    avg_loss:0.102, val_acc:0.964]
Epoch [55/120    avg_loss:0.095, val_acc:0.965]
Epoch [56/120    avg_loss:0.102, val_acc:0.968]
Epoch [57/120    avg_loss:0.095, val_acc:0.965]
Epoch [58/120    avg_loss:0.099, val_acc:0.966]
Epoch [59/120    avg_loss:0.090, val_acc:0.968]
Epoch [60/120    avg_loss:0.089, val_acc:0.968]
Epoch [61/120    avg_loss:0.086, val_acc:0.967]
Epoch [62/120    avg_loss:0.086, val_acc:0.966]
Epoch [63/120    avg_loss:0.092, val_acc:0.968]
Epoch [64/120    avg_loss:0.084, val_acc:0.969]
Epoch [65/120    avg_loss:0.084, val_acc:0.968]
Epoch [66/120    avg_loss:0.083, val_acc:0.968]
Epoch [67/120    avg_loss:0.087, val_acc:0.969]
Epoch [68/120    avg_loss:0.087, val_acc:0.968]
Epoch [69/120    avg_loss:0.085, val_acc:0.967]
Epoch [70/120    avg_loss:0.084, val_acc:0.971]
Epoch [71/120    avg_loss:0.072, val_acc:0.969]
Epoch [72/120    avg_loss:0.075, val_acc:0.970]
Epoch [73/120    avg_loss:0.074, val_acc:0.970]
Epoch [74/120    avg_loss:0.074, val_acc:0.970]
Epoch [75/120    avg_loss:0.081, val_acc:0.969]
Epoch [76/120    avg_loss:0.071, val_acc:0.972]
Epoch [77/120    avg_loss:0.072, val_acc:0.970]
Epoch [78/120    avg_loss:0.075, val_acc:0.970]
Epoch [79/120    avg_loss:0.072, val_acc:0.972]
Epoch [80/120    avg_loss:0.075, val_acc:0.972]
Epoch [81/120    avg_loss:0.079, val_acc:0.972]
Epoch [82/120    avg_loss:0.072, val_acc:0.974]
Epoch [83/120    avg_loss:0.076, val_acc:0.972]
Epoch [84/120    avg_loss:0.069, val_acc:0.974]
Epoch [85/120    avg_loss:0.060, val_acc:0.975]
Epoch [86/120    avg_loss:0.060, val_acc:0.973]
Epoch [87/120    avg_loss:0.066, val_acc:0.973]
Epoch [88/120    avg_loss:0.066, val_acc:0.972]
Epoch [89/120    avg_loss:0.064, val_acc:0.972]
Epoch [90/120    avg_loss:0.068, val_acc:0.975]
Epoch [91/120    avg_loss:0.065, val_acc:0.975]
Epoch [92/120    avg_loss:0.066, val_acc:0.976]
Epoch [93/120    avg_loss:0.055, val_acc:0.972]
Epoch [94/120    avg_loss:0.059, val_acc:0.971]
Epoch [95/120    avg_loss:0.064, val_acc:0.975]
Epoch [96/120    avg_loss:0.062, val_acc:0.972]
Epoch [97/120    avg_loss:0.064, val_acc:0.970]
Epoch [98/120    avg_loss:0.056, val_acc:0.974]
Epoch [99/120    avg_loss:0.060, val_acc:0.974]
Epoch [100/120    avg_loss:0.055, val_acc:0.976]
Epoch [101/120    avg_loss:0.058, val_acc:0.975]
Epoch [102/120    avg_loss:0.053, val_acc:0.976]
Epoch [103/120    avg_loss:0.056, val_acc:0.976]
Epoch [104/120    avg_loss:0.056, val_acc:0.974]
Epoch [105/120    avg_loss:0.060, val_acc:0.975]
Epoch [106/120    avg_loss:0.066, val_acc:0.975]
Epoch [107/120    avg_loss:0.051, val_acc:0.976]
Epoch [108/120    avg_loss:0.058, val_acc:0.977]
Epoch [109/120    avg_loss:0.052, val_acc:0.976]
Epoch [110/120    avg_loss:0.054, val_acc:0.975]
Epoch [111/120    avg_loss:0.051, val_acc:0.977]
Epoch [112/120    avg_loss:0.046, val_acc:0.974]
Epoch [113/120    avg_loss:0.061, val_acc:0.974]
Epoch [114/120    avg_loss:0.046, val_acc:0.979]
Epoch [115/120    avg_loss:0.050, val_acc:0.976]
Epoch [116/120    avg_loss:0.051, val_acc:0.975]
Epoch [117/120    avg_loss:0.048, val_acc:0.977]
Epoch [118/120    avg_loss:0.048, val_acc:0.977]
Epoch [119/120    avg_loss:0.052, val_acc:0.974]
Epoch [120/120    avg_loss:0.044, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6234     0    19     6     0     0     0   163    10]
 [    0     1 18011     0    30     0    48     0     0     0]
 [    0     0     0  1988     6     0     0     0    35     7]
 [    0    82    21     0  2840     0     1     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4846     0    10     3]
 [    0     7     0     0     0     0     1  1281     0     1]
 [    0     2     0     9    79     0     0     0  3480     1]
 [    0     0     0     0    14    76     0     0     0   829]]

Accuracy:
98.36357939893476

F1 scores:
[       nan 0.97726916 0.99670734 0.98124383 0.95510341 0.97170514
 0.99161039 0.99649942 0.95512557 0.93672316]

Kappa:
0.9783295882671021
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96ed725978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.121, val_acc:0.077]
Epoch [2/120    avg_loss:1.846, val_acc:0.128]
Epoch [3/120    avg_loss:1.624, val_acc:0.169]
Epoch [4/120    avg_loss:1.481, val_acc:0.336]
Epoch [5/120    avg_loss:1.344, val_acc:0.366]
Epoch [6/120    avg_loss:1.196, val_acc:0.361]
Epoch [7/120    avg_loss:1.092, val_acc:0.453]
Epoch [8/120    avg_loss:0.975, val_acc:0.459]
Epoch [9/120    avg_loss:0.900, val_acc:0.533]
Epoch [10/120    avg_loss:0.752, val_acc:0.637]
Epoch [11/120    avg_loss:0.667, val_acc:0.738]
Epoch [12/120    avg_loss:0.568, val_acc:0.803]
Epoch [13/120    avg_loss:0.504, val_acc:0.776]
Epoch [14/120    avg_loss:0.490, val_acc:0.834]
Epoch [15/120    avg_loss:0.451, val_acc:0.844]
Epoch [16/120    avg_loss:0.411, val_acc:0.831]
Epoch [17/120    avg_loss:0.371, val_acc:0.823]
Epoch [18/120    avg_loss:0.406, val_acc:0.842]
Epoch [19/120    avg_loss:0.341, val_acc:0.853]
Epoch [20/120    avg_loss:0.280, val_acc:0.879]
Epoch [21/120    avg_loss:0.290, val_acc:0.914]
Epoch [22/120    avg_loss:0.257, val_acc:0.897]
Epoch [23/120    avg_loss:0.241, val_acc:0.901]
Epoch [24/120    avg_loss:0.241, val_acc:0.893]
Epoch [25/120    avg_loss:0.194, val_acc:0.896]
Epoch [26/120    avg_loss:0.167, val_acc:0.934]
Epoch [27/120    avg_loss:0.134, val_acc:0.938]
Epoch [28/120    avg_loss:0.182, val_acc:0.915]
Epoch [29/120    avg_loss:0.202, val_acc:0.874]
Epoch [30/120    avg_loss:0.169, val_acc:0.936]
Epoch [31/120    avg_loss:0.148, val_acc:0.945]
Epoch [32/120    avg_loss:0.135, val_acc:0.953]
Epoch [33/120    avg_loss:0.114, val_acc:0.944]
Epoch [34/120    avg_loss:0.116, val_acc:0.929]
Epoch [35/120    avg_loss:0.108, val_acc:0.960]
Epoch [36/120    avg_loss:0.109, val_acc:0.956]
Epoch [37/120    avg_loss:0.088, val_acc:0.943]
Epoch [38/120    avg_loss:0.076, val_acc:0.961]
Epoch [39/120    avg_loss:0.069, val_acc:0.955]
Epoch [40/120    avg_loss:0.074, val_acc:0.966]
Epoch [41/120    avg_loss:0.069, val_acc:0.963]
Epoch [42/120    avg_loss:0.061, val_acc:0.975]
Epoch [43/120    avg_loss:0.069, val_acc:0.963]
Epoch [44/120    avg_loss:0.130, val_acc:0.960]
Epoch [45/120    avg_loss:0.099, val_acc:0.959]
Epoch [46/120    avg_loss:0.099, val_acc:0.954]
Epoch [47/120    avg_loss:0.072, val_acc:0.971]
Epoch [48/120    avg_loss:0.047, val_acc:0.968]
Epoch [49/120    avg_loss:0.037, val_acc:0.977]
Epoch [50/120    avg_loss:0.043, val_acc:0.965]
Epoch [51/120    avg_loss:0.036, val_acc:0.970]
Epoch [52/120    avg_loss:0.041, val_acc:0.974]
Epoch [53/120    avg_loss:0.037, val_acc:0.954]
Epoch [54/120    avg_loss:0.069, val_acc:0.959]
Epoch [55/120    avg_loss:0.074, val_acc:0.971]
Epoch [56/120    avg_loss:0.035, val_acc:0.964]
Epoch [57/120    avg_loss:0.057, val_acc:0.976]
Epoch [58/120    avg_loss:0.033, val_acc:0.974]
Epoch [59/120    avg_loss:0.042, val_acc:0.978]
Epoch [60/120    avg_loss:0.033, val_acc:0.979]
Epoch [61/120    avg_loss:0.034, val_acc:0.980]
Epoch [62/120    avg_loss:0.043, val_acc:0.972]
Epoch [63/120    avg_loss:0.031, val_acc:0.983]
Epoch [64/120    avg_loss:0.023, val_acc:0.978]
Epoch [65/120    avg_loss:0.024, val_acc:0.985]
Epoch [66/120    avg_loss:0.031, val_acc:0.974]
Epoch [67/120    avg_loss:0.040, val_acc:0.967]
Epoch [68/120    avg_loss:0.023, val_acc:0.983]
Epoch [69/120    avg_loss:0.021, val_acc:0.984]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.021, val_acc:0.985]
Epoch [72/120    avg_loss:0.413, val_acc:0.840]
Epoch [73/120    avg_loss:0.207, val_acc:0.908]
Epoch [74/120    avg_loss:0.142, val_acc:0.952]
Epoch [75/120    avg_loss:0.098, val_acc:0.954]
Epoch [76/120    avg_loss:0.076, val_acc:0.958]
Epoch [77/120    avg_loss:0.107, val_acc:0.954]
Epoch [78/120    avg_loss:0.086, val_acc:0.956]
Epoch [79/120    avg_loss:0.125, val_acc:0.962]
Epoch [80/120    avg_loss:0.065, val_acc:0.956]
Epoch [81/120    avg_loss:0.057, val_acc:0.954]
Epoch [82/120    avg_loss:0.057, val_acc:0.964]
Epoch [83/120    avg_loss:0.043, val_acc:0.976]
Epoch [84/120    avg_loss:0.038, val_acc:0.967]
Epoch [85/120    avg_loss:0.032, val_acc:0.968]
Epoch [86/120    avg_loss:0.028, val_acc:0.971]
Epoch [87/120    avg_loss:0.029, val_acc:0.972]
Epoch [88/120    avg_loss:0.023, val_acc:0.972]
Epoch [89/120    avg_loss:0.028, val_acc:0.973]
Epoch [90/120    avg_loss:0.033, val_acc:0.973]
Epoch [91/120    avg_loss:0.023, val_acc:0.972]
Epoch [92/120    avg_loss:0.022, val_acc:0.973]
Epoch [93/120    avg_loss:0.029, val_acc:0.974]
Epoch [94/120    avg_loss:0.026, val_acc:0.975]
Epoch [95/120    avg_loss:0.029, val_acc:0.973]
Epoch [96/120    avg_loss:0.023, val_acc:0.974]
Epoch [97/120    avg_loss:0.025, val_acc:0.974]
Epoch [98/120    avg_loss:0.022, val_acc:0.974]
Epoch [99/120    avg_loss:0.026, val_acc:0.974]
Epoch [100/120    avg_loss:0.021, val_acc:0.974]
Epoch [101/120    avg_loss:0.023, val_acc:0.974]
Epoch [102/120    avg_loss:0.024, val_acc:0.974]
Epoch [103/120    avg_loss:0.029, val_acc:0.975]
Epoch [104/120    avg_loss:0.025, val_acc:0.976]
Epoch [105/120    avg_loss:0.029, val_acc:0.976]
Epoch [106/120    avg_loss:0.019, val_acc:0.976]
Epoch [107/120    avg_loss:0.020, val_acc:0.976]
Epoch [108/120    avg_loss:0.021, val_acc:0.977]
Epoch [109/120    avg_loss:0.021, val_acc:0.976]
Epoch [110/120    avg_loss:0.021, val_acc:0.976]
Epoch [111/120    avg_loss:0.028, val_acc:0.976]
Epoch [112/120    avg_loss:0.023, val_acc:0.976]
Epoch [113/120    avg_loss:0.018, val_acc:0.976]
Epoch [114/120    avg_loss:0.023, val_acc:0.976]
Epoch [115/120    avg_loss:0.025, val_acc:0.976]
Epoch [116/120    avg_loss:0.028, val_acc:0.976]
Epoch [117/120    avg_loss:0.022, val_acc:0.976]
Epoch [118/120    avg_loss:0.021, val_acc:0.976]
Epoch [119/120    avg_loss:0.022, val_acc:0.976]
Epoch [120/120    avg_loss:0.023, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6277     0     0    13     0     0     6   135     1]
 [    0     0 18030     0    24     0    36     0     0     0]
 [    0     6     0  1977     2     0     0     0    44     7]
 [    0    54    24     0  2867     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    25     4     0     0  4838     0     0    11]
 [    0     8     0     0     0     0     3  1279     0     0]
 [    0    30     0     2    72     0     0     0  3456    11]
 [    0     0     0     5    18    66     0     0     0   830]]

Accuracy:
98.47203142698768

F1 scores:
[       nan 0.98024518 0.99698637 0.98260437 0.96079088 0.97533632
 0.99190159 0.99339806 0.95562007 0.93310849]

Kappa:
0.9797573333633866
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ef2d4f9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.053, val_acc:0.188]
Epoch [2/120    avg_loss:1.766, val_acc:0.260]
Epoch [3/120    avg_loss:1.593, val_acc:0.335]
Epoch [4/120    avg_loss:1.374, val_acc:0.411]
Epoch [5/120    avg_loss:1.230, val_acc:0.470]
Epoch [6/120    avg_loss:1.118, val_acc:0.521]
Epoch [7/120    avg_loss:0.937, val_acc:0.576]
Epoch [8/120    avg_loss:0.811, val_acc:0.636]
Epoch [9/120    avg_loss:0.709, val_acc:0.680]
Epoch [10/120    avg_loss:0.623, val_acc:0.692]
Epoch [11/120    avg_loss:0.549, val_acc:0.748]
Epoch [12/120    avg_loss:0.520, val_acc:0.753]
Epoch [13/120    avg_loss:0.474, val_acc:0.799]
Epoch [14/120    avg_loss:0.422, val_acc:0.767]
Epoch [15/120    avg_loss:0.361, val_acc:0.804]
Epoch [16/120    avg_loss:0.341, val_acc:0.826]
Epoch [17/120    avg_loss:0.293, val_acc:0.829]
Epoch [18/120    avg_loss:0.332, val_acc:0.820]
Epoch [19/120    avg_loss:0.289, val_acc:0.866]
Epoch [20/120    avg_loss:0.228, val_acc:0.866]
Epoch [21/120    avg_loss:0.221, val_acc:0.882]
Epoch [22/120    avg_loss:0.206, val_acc:0.907]
Epoch [23/120    avg_loss:0.254, val_acc:0.898]
Epoch [24/120    avg_loss:0.198, val_acc:0.912]
Epoch [25/120    avg_loss:0.182, val_acc:0.872]
Epoch [26/120    avg_loss:0.165, val_acc:0.911]
Epoch [27/120    avg_loss:0.131, val_acc:0.934]
Epoch [28/120    avg_loss:0.110, val_acc:0.914]
Epoch [29/120    avg_loss:0.132, val_acc:0.947]
Epoch [30/120    avg_loss:0.129, val_acc:0.948]
Epoch [31/120    avg_loss:0.102, val_acc:0.953]
Epoch [32/120    avg_loss:0.091, val_acc:0.941]
Epoch [33/120    avg_loss:0.094, val_acc:0.951]
Epoch [34/120    avg_loss:0.094, val_acc:0.944]
Epoch [35/120    avg_loss:0.087, val_acc:0.924]
Epoch [36/120    avg_loss:0.152, val_acc:0.931]
Epoch [37/120    avg_loss:0.074, val_acc:0.960]
Epoch [38/120    avg_loss:0.057, val_acc:0.950]
Epoch [39/120    avg_loss:0.121, val_acc:0.944]
Epoch [40/120    avg_loss:0.097, val_acc:0.955]
Epoch [41/120    avg_loss:0.064, val_acc:0.960]
Epoch [42/120    avg_loss:0.059, val_acc:0.959]
Epoch [43/120    avg_loss:0.054, val_acc:0.962]
Epoch [44/120    avg_loss:0.045, val_acc:0.979]
Epoch [45/120    avg_loss:0.041, val_acc:0.969]
Epoch [46/120    avg_loss:0.034, val_acc:0.981]
Epoch [47/120    avg_loss:0.046, val_acc:0.977]
Epoch [48/120    avg_loss:0.038, val_acc:0.973]
Epoch [49/120    avg_loss:0.034, val_acc:0.977]
Epoch [50/120    avg_loss:0.045, val_acc:0.968]
Epoch [51/120    avg_loss:0.049, val_acc:0.949]
Epoch [52/120    avg_loss:0.026, val_acc:0.975]
Epoch [53/120    avg_loss:0.022, val_acc:0.976]
Epoch [54/120    avg_loss:0.025, val_acc:0.982]
Epoch [55/120    avg_loss:0.046, val_acc:0.959]
Epoch [56/120    avg_loss:0.056, val_acc:0.966]
Epoch [57/120    avg_loss:0.055, val_acc:0.971]
Epoch [58/120    avg_loss:0.048, val_acc:0.976]
Epoch [59/120    avg_loss:0.028, val_acc:0.984]
Epoch [60/120    avg_loss:0.024, val_acc:0.960]
Epoch [61/120    avg_loss:0.025, val_acc:0.977]
Epoch [62/120    avg_loss:0.021, val_acc:0.988]
Epoch [63/120    avg_loss:0.020, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.980]
Epoch [65/120    avg_loss:0.023, val_acc:0.970]
Epoch [66/120    avg_loss:0.026, val_acc:0.955]
Epoch [67/120    avg_loss:0.034, val_acc:0.981]
Epoch [68/120    avg_loss:0.020, val_acc:0.978]
Epoch [69/120    avg_loss:0.027, val_acc:0.978]
Epoch [70/120    avg_loss:0.019, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.985]
Epoch [72/120    avg_loss:0.014, val_acc:0.985]
Epoch [73/120    avg_loss:0.029, val_acc:0.964]
Epoch [74/120    avg_loss:0.019, val_acc:0.984]
Epoch [75/120    avg_loss:0.022, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.987]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.987]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.010, val_acc:0.989]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.011, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.989]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.989]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.009, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6413     0     0     1     0     0     0    18     0]
 [    0     4 18005     0    61     0    15     0     5     0]
 [    0     0     0  2013     2     0     0     0    20     1]
 [    0    35    21     0  2880     0     8     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     4     0     0  4869     0     0     0]
 [    0     0     0     0     0     0     3  1285     0     2]
 [    0     7     0     9    55     0     0     0  3500     0]
 [    0     0     0     7    14    51     0     0     0   847]]

Accuracy:
99.09382305449112

F1 scores:
[       nan 0.99495772 0.996927   0.98943229 0.96240602 0.98083427
 0.9964187  0.99805825 0.98011761 0.95760317]

Kappa:
0.9879990576947911
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feb66a89978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.094, val_acc:0.148]
Epoch [2/120    avg_loss:1.817, val_acc:0.273]
Epoch [3/120    avg_loss:1.635, val_acc:0.316]
Epoch [4/120    avg_loss:1.462, val_acc:0.359]
Epoch [5/120    avg_loss:1.328, val_acc:0.370]
Epoch [6/120    avg_loss:1.175, val_acc:0.411]
Epoch [7/120    avg_loss:1.083, val_acc:0.470]
Epoch [8/120    avg_loss:0.950, val_acc:0.603]
Epoch [9/120    avg_loss:0.830, val_acc:0.657]
Epoch [10/120    avg_loss:0.687, val_acc:0.694]
Epoch [11/120    avg_loss:0.577, val_acc:0.764]
Epoch [12/120    avg_loss:0.525, val_acc:0.835]
Epoch [13/120    avg_loss:0.441, val_acc:0.827]
Epoch [14/120    avg_loss:0.396, val_acc:0.801]
Epoch [15/120    avg_loss:0.391, val_acc:0.846]
Epoch [16/120    avg_loss:0.356, val_acc:0.916]
Epoch [17/120    avg_loss:0.281, val_acc:0.926]
Epoch [18/120    avg_loss:0.233, val_acc:0.923]
Epoch [19/120    avg_loss:0.241, val_acc:0.874]
Epoch [20/120    avg_loss:0.250, val_acc:0.931]
Epoch [21/120    avg_loss:0.204, val_acc:0.939]
Epoch [22/120    avg_loss:0.263, val_acc:0.163]
Epoch [23/120    avg_loss:1.504, val_acc:0.548]
Epoch [24/120    avg_loss:1.203, val_acc:0.351]
Epoch [25/120    avg_loss:1.106, val_acc:0.381]
Epoch [26/120    avg_loss:1.048, val_acc:0.398]
Epoch [27/120    avg_loss:0.959, val_acc:0.495]
Epoch [28/120    avg_loss:0.937, val_acc:0.612]
Epoch [29/120    avg_loss:0.923, val_acc:0.569]
Epoch [30/120    avg_loss:0.880, val_acc:0.398]
Epoch [31/120    avg_loss:0.799, val_acc:0.698]
Epoch [32/120    avg_loss:0.800, val_acc:0.698]
Epoch [33/120    avg_loss:0.743, val_acc:0.687]
Epoch [34/120    avg_loss:0.736, val_acc:0.702]
Epoch [35/120    avg_loss:0.703, val_acc:0.583]
Epoch [36/120    avg_loss:0.686, val_acc:0.643]
Epoch [37/120    avg_loss:0.702, val_acc:0.620]
Epoch [38/120    avg_loss:0.684, val_acc:0.668]
Epoch [39/120    avg_loss:0.689, val_acc:0.698]
Epoch [40/120    avg_loss:0.684, val_acc:0.688]
Epoch [41/120    avg_loss:0.693, val_acc:0.671]
Epoch [42/120    avg_loss:0.694, val_acc:0.693]
Epoch [43/120    avg_loss:0.666, val_acc:0.693]
Epoch [44/120    avg_loss:0.674, val_acc:0.675]
Epoch [45/120    avg_loss:0.696, val_acc:0.714]
Epoch [46/120    avg_loss:0.661, val_acc:0.699]
Epoch [47/120    avg_loss:0.670, val_acc:0.710]
Epoch [48/120    avg_loss:0.669, val_acc:0.707]
Epoch [49/120    avg_loss:0.662, val_acc:0.707]
Epoch [50/120    avg_loss:0.675, val_acc:0.711]
Epoch [51/120    avg_loss:0.675, val_acc:0.707]
Epoch [52/120    avg_loss:0.675, val_acc:0.707]
Epoch [53/120    avg_loss:0.668, val_acc:0.707]
Epoch [54/120    avg_loss:0.651, val_acc:0.707]
Epoch [55/120    avg_loss:0.661, val_acc:0.704]
Epoch [56/120    avg_loss:0.665, val_acc:0.707]
Epoch [57/120    avg_loss:0.673, val_acc:0.707]
Epoch [58/120    avg_loss:0.684, val_acc:0.705]
Epoch [59/120    avg_loss:0.646, val_acc:0.708]
Epoch [60/120    avg_loss:0.659, val_acc:0.711]
Epoch [61/120    avg_loss:0.668, val_acc:0.711]
Epoch [62/120    avg_loss:0.653, val_acc:0.711]
Epoch [63/120    avg_loss:0.691, val_acc:0.711]
Epoch [64/120    avg_loss:0.660, val_acc:0.711]
Epoch [65/120    avg_loss:0.681, val_acc:0.711]
Epoch [66/120    avg_loss:0.661, val_acc:0.711]
Epoch [67/120    avg_loss:0.667, val_acc:0.711]
Epoch [68/120    avg_loss:0.652, val_acc:0.710]
Epoch [69/120    avg_loss:0.647, val_acc:0.709]
Epoch [70/120    avg_loss:0.677, val_acc:0.709]
Epoch [71/120    avg_loss:0.672, val_acc:0.709]
Epoch [72/120    avg_loss:0.653, val_acc:0.710]
Epoch [73/120    avg_loss:0.641, val_acc:0.710]
Epoch [74/120    avg_loss:0.646, val_acc:0.710]
Epoch [75/120    avg_loss:0.667, val_acc:0.710]
Epoch [76/120    avg_loss:0.670, val_acc:0.710]
Epoch [77/120    avg_loss:0.676, val_acc:0.710]
Epoch [78/120    avg_loss:0.668, val_acc:0.710]
Epoch [79/120    avg_loss:0.677, val_acc:0.710]
Epoch [80/120    avg_loss:0.676, val_acc:0.710]
Epoch [81/120    avg_loss:0.675, val_acc:0.710]
Epoch [82/120    avg_loss:0.683, val_acc:0.710]
Epoch [83/120    avg_loss:0.674, val_acc:0.710]
Epoch [84/120    avg_loss:0.695, val_acc:0.710]
Epoch [85/120    avg_loss:0.651, val_acc:0.710]
Epoch [86/120    avg_loss:0.638, val_acc:0.710]
Epoch [87/120    avg_loss:0.668, val_acc:0.710]
Epoch [88/120    avg_loss:0.665, val_acc:0.710]
Epoch [89/120    avg_loss:0.654, val_acc:0.710]
Epoch [90/120    avg_loss:0.661, val_acc:0.710]
Epoch [91/120    avg_loss:0.675, val_acc:0.710]
Epoch [92/120    avg_loss:0.656, val_acc:0.710]
Epoch [93/120    avg_loss:0.676, val_acc:0.710]
Epoch [94/120    avg_loss:0.649, val_acc:0.710]
Epoch [95/120    avg_loss:0.640, val_acc:0.710]
Epoch [96/120    avg_loss:0.642, val_acc:0.710]
Epoch [97/120    avg_loss:0.663, val_acc:0.710]
Epoch [98/120    avg_loss:0.647, val_acc:0.710]
Epoch [99/120    avg_loss:0.643, val_acc:0.710]
Epoch [100/120    avg_loss:0.652, val_acc:0.710]
Epoch [101/120    avg_loss:0.666, val_acc:0.710]
Epoch [102/120    avg_loss:0.681, val_acc:0.710]
Epoch [103/120    avg_loss:0.650, val_acc:0.710]
Epoch [104/120    avg_loss:0.664, val_acc:0.710]
Epoch [105/120    avg_loss:0.638, val_acc:0.710]
Epoch [106/120    avg_loss:0.645, val_acc:0.710]
Epoch [107/120    avg_loss:0.664, val_acc:0.710]
Epoch [108/120    avg_loss:0.668, val_acc:0.710]
Epoch [109/120    avg_loss:0.657, val_acc:0.710]
Epoch [110/120    avg_loss:0.637, val_acc:0.710]
Epoch [111/120    avg_loss:0.683, val_acc:0.710]
Epoch [112/120    avg_loss:0.662, val_acc:0.710]
Epoch [113/120    avg_loss:0.648, val_acc:0.710]
Epoch [114/120    avg_loss:0.657, val_acc:0.710]
Epoch [115/120    avg_loss:0.645, val_acc:0.710]
Epoch [116/120    avg_loss:0.668, val_acc:0.710]
Epoch [117/120    avg_loss:0.672, val_acc:0.710]
Epoch [118/120    avg_loss:0.670, val_acc:0.710]
Epoch [119/120    avg_loss:0.670, val_acc:0.710]
Epoch [120/120    avg_loss:0.657, val_acc:0.710]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4898     1    61   564     0    63   135   670    40]
 [    0     0 11931     0   149     0  6006     0     4     0]
 [    0    19     0  1857    21     0     0     0   104    35]
 [    0   137   237     0  1832     0   606     0   120    40]
 [    0     0     0     1     0  1304     0     0     0     0]
 [    0     0   813     0   627     0  3255     0   183     0]
 [    0   108     0     0     0     0     0  1163     0    19]
 [    0   191     0    74   180     0    41     0  3076     9]
 [    0    18     0     8    17   145     0     0     4   727]]

Accuracy:
72.4049839732003

F1 scores:
[       nan 0.82995849 0.76795829 0.91999009 0.57591952 0.9469862
 0.43841336 0.89876352 0.79565442 0.81274455]

Kappa:
0.6532829888061875
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e1d43a978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.091, val_acc:0.222]
Epoch [2/120    avg_loss:1.774, val_acc:0.247]
Epoch [3/120    avg_loss:1.586, val_acc:0.281]
Epoch [4/120    avg_loss:1.422, val_acc:0.359]
Epoch [5/120    avg_loss:1.254, val_acc:0.405]
Epoch [6/120    avg_loss:1.128, val_acc:0.504]
Epoch [7/120    avg_loss:0.975, val_acc:0.686]
Epoch [8/120    avg_loss:0.852, val_acc:0.728]
Epoch [9/120    avg_loss:0.775, val_acc:0.748]
Epoch [10/120    avg_loss:0.681, val_acc:0.737]
Epoch [11/120    avg_loss:0.576, val_acc:0.726]
Epoch [12/120    avg_loss:0.515, val_acc:0.781]
Epoch [13/120    avg_loss:0.494, val_acc:0.806]
Epoch [14/120    avg_loss:0.451, val_acc:0.806]
Epoch [15/120    avg_loss:0.365, val_acc:0.861]
Epoch [16/120    avg_loss:0.380, val_acc:0.851]
Epoch [17/120    avg_loss:0.332, val_acc:0.891]
Epoch [18/120    avg_loss:0.251, val_acc:0.891]
Epoch [19/120    avg_loss:0.283, val_acc:0.898]
Epoch [20/120    avg_loss:0.288, val_acc:0.911]
Epoch [21/120    avg_loss:0.212, val_acc:0.926]
Epoch [22/120    avg_loss:0.197, val_acc:0.927]
Epoch [23/120    avg_loss:0.176, val_acc:0.939]
Epoch [24/120    avg_loss:0.174, val_acc:0.947]
Epoch [25/120    avg_loss:0.150, val_acc:0.928]
Epoch [26/120    avg_loss:0.361, val_acc:0.405]
Epoch [27/120    avg_loss:0.857, val_acc:0.779]
Epoch [28/120    avg_loss:0.450, val_acc:0.588]
Epoch [29/120    avg_loss:1.086, val_acc:0.622]
Epoch [30/120    avg_loss:0.877, val_acc:0.686]
Epoch [31/120    avg_loss:0.719, val_acc:0.734]
Epoch [32/120    avg_loss:0.634, val_acc:0.734]
Epoch [33/120    avg_loss:0.535, val_acc:0.806]
Epoch [34/120    avg_loss:0.496, val_acc:0.776]
Epoch [35/120    avg_loss:0.500, val_acc:0.856]
Epoch [36/120    avg_loss:0.388, val_acc:0.833]
Epoch [37/120    avg_loss:0.358, val_acc:0.898]
Epoch [38/120    avg_loss:0.323, val_acc:0.891]
Epoch [39/120    avg_loss:0.272, val_acc:0.900]
Epoch [40/120    avg_loss:0.259, val_acc:0.898]
Epoch [41/120    avg_loss:0.249, val_acc:0.918]
Epoch [42/120    avg_loss:0.251, val_acc:0.910]
Epoch [43/120    avg_loss:0.238, val_acc:0.905]
Epoch [44/120    avg_loss:0.234, val_acc:0.911]
Epoch [45/120    avg_loss:0.266, val_acc:0.925]
Epoch [46/120    avg_loss:0.251, val_acc:0.903]
Epoch [47/120    avg_loss:0.237, val_acc:0.917]
Epoch [48/120    avg_loss:0.232, val_acc:0.933]
Epoch [49/120    avg_loss:0.239, val_acc:0.918]
Epoch [50/120    avg_loss:0.237, val_acc:0.918]
Epoch [51/120    avg_loss:0.229, val_acc:0.922]
Epoch [52/120    avg_loss:0.218, val_acc:0.923]
Epoch [53/120    avg_loss:0.228, val_acc:0.928]
Epoch [54/120    avg_loss:0.205, val_acc:0.929]
Epoch [55/120    avg_loss:0.214, val_acc:0.931]
Epoch [56/120    avg_loss:0.206, val_acc:0.931]
Epoch [57/120    avg_loss:0.225, val_acc:0.929]
Epoch [58/120    avg_loss:0.214, val_acc:0.934]
Epoch [59/120    avg_loss:0.218, val_acc:0.937]
Epoch [60/120    avg_loss:0.216, val_acc:0.938]
Epoch [61/120    avg_loss:0.207, val_acc:0.937]
Epoch [62/120    avg_loss:0.214, val_acc:0.934]
Epoch [63/120    avg_loss:0.218, val_acc:0.931]
Epoch [64/120    avg_loss:0.205, val_acc:0.933]
Epoch [65/120    avg_loss:0.212, val_acc:0.934]
Epoch [66/120    avg_loss:0.198, val_acc:0.934]
Epoch [67/120    avg_loss:0.207, val_acc:0.934]
Epoch [68/120    avg_loss:0.208, val_acc:0.933]
Epoch [69/120    avg_loss:0.212, val_acc:0.933]
Epoch [70/120    avg_loss:0.210, val_acc:0.934]
Epoch [71/120    avg_loss:0.205, val_acc:0.934]
Epoch [72/120    avg_loss:0.219, val_acc:0.934]
Epoch [73/120    avg_loss:0.211, val_acc:0.933]
Epoch [74/120    avg_loss:0.208, val_acc:0.935]
Epoch [75/120    avg_loss:0.219, val_acc:0.935]
Epoch [76/120    avg_loss:0.220, val_acc:0.935]
Epoch [77/120    avg_loss:0.200, val_acc:0.935]
Epoch [78/120    avg_loss:0.225, val_acc:0.935]
Epoch [79/120    avg_loss:0.218, val_acc:0.935]
Epoch [80/120    avg_loss:0.206, val_acc:0.935]
Epoch [81/120    avg_loss:0.222, val_acc:0.935]
Epoch [82/120    avg_loss:0.208, val_acc:0.934]
Epoch [83/120    avg_loss:0.209, val_acc:0.935]
Epoch [84/120    avg_loss:0.216, val_acc:0.935]
Epoch [85/120    avg_loss:0.205, val_acc:0.935]
Epoch [86/120    avg_loss:0.209, val_acc:0.935]
Epoch [87/120    avg_loss:0.232, val_acc:0.935]
Epoch [88/120    avg_loss:0.220, val_acc:0.935]
Epoch [89/120    avg_loss:0.210, val_acc:0.935]
Epoch [90/120    avg_loss:0.214, val_acc:0.935]
Epoch [91/120    avg_loss:0.206, val_acc:0.935]
Epoch [92/120    avg_loss:0.223, val_acc:0.935]
Epoch [93/120    avg_loss:0.210, val_acc:0.935]
Epoch [94/120    avg_loss:0.236, val_acc:0.935]
Epoch [95/120    avg_loss:0.214, val_acc:0.935]
Epoch [96/120    avg_loss:0.226, val_acc:0.935]
Epoch [97/120    avg_loss:0.210, val_acc:0.935]
Epoch [98/120    avg_loss:0.216, val_acc:0.935]
Epoch [99/120    avg_loss:0.219, val_acc:0.935]
Epoch [100/120    avg_loss:0.228, val_acc:0.935]
Epoch [101/120    avg_loss:0.233, val_acc:0.935]
Epoch [102/120    avg_loss:0.214, val_acc:0.935]
Epoch [103/120    avg_loss:0.207, val_acc:0.935]
Epoch [104/120    avg_loss:0.207, val_acc:0.935]
Epoch [105/120    avg_loss:0.223, val_acc:0.935]
Epoch [106/120    avg_loss:0.205, val_acc:0.935]
Epoch [107/120    avg_loss:0.208, val_acc:0.935]
Epoch [108/120    avg_loss:0.206, val_acc:0.935]
Epoch [109/120    avg_loss:0.221, val_acc:0.935]
Epoch [110/120    avg_loss:0.224, val_acc:0.935]
Epoch [111/120    avg_loss:0.200, val_acc:0.935]
Epoch [112/120    avg_loss:0.214, val_acc:0.935]
Epoch [113/120    avg_loss:0.200, val_acc:0.935]
Epoch [114/120    avg_loss:0.206, val_acc:0.935]
Epoch [115/120    avg_loss:0.208, val_acc:0.935]
Epoch [116/120    avg_loss:0.195, val_acc:0.935]
Epoch [117/120    avg_loss:0.220, val_acc:0.935]
Epoch [118/120    avg_loss:0.215, val_acc:0.935]
Epoch [119/120    avg_loss:0.203, val_acc:0.935]
Epoch [120/120    avg_loss:0.232, val_acc:0.935]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5574     0   188   247     0     0    21   378    24]
 [    0     0 17535     0   374     0   181     0     0     0]
 [    0    18     0  1906     0     0     0     0    60    52]
 [    0   107    26     0  2726     0    52     0    57     4]
 [    0     0     0     0     0  1303     0     2     0     0]
 [    0     0   103     0     3     0  4762     0    10     0]
 [    0     8     0     0     0     0    10  1242     6    24]
 [    0    82     4    16    67     0     1     0  3400     1]
 [    0     4     0    13    27   119     0     0     5   751]]

Accuracy:
94.47135661436869

F1 scores:
[       nan 0.91190184 0.98075955 0.91656648 0.84975062 0.9556289
 0.9635775  0.97221135 0.90824095 0.84619718]

Kappa:
0.9272040845207739
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2712837940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.125, val_acc:0.098]
Epoch [2/120    avg_loss:1.814, val_acc:0.136]
Epoch [3/120    avg_loss:1.657, val_acc:0.213]
Epoch [4/120    avg_loss:1.462, val_acc:0.378]
Epoch [5/120    avg_loss:1.322, val_acc:0.516]
Epoch [6/120    avg_loss:1.198, val_acc:0.668]
Epoch [7/120    avg_loss:1.050, val_acc:0.650]
Epoch [8/120    avg_loss:0.972, val_acc:0.661]
Epoch [9/120    avg_loss:0.872, val_acc:0.695]
Epoch [10/120    avg_loss:0.767, val_acc:0.663]
Epoch [11/120    avg_loss:0.715, val_acc:0.668]
Epoch [12/120    avg_loss:0.607, val_acc:0.700]
Epoch [13/120    avg_loss:0.534, val_acc:0.745]
Epoch [14/120    avg_loss:0.479, val_acc:0.790]
Epoch [15/120    avg_loss:0.440, val_acc:0.795]
Epoch [16/120    avg_loss:0.379, val_acc:0.846]
Epoch [17/120    avg_loss:0.351, val_acc:0.845]
Epoch [18/120    avg_loss:0.320, val_acc:0.845]
Epoch [19/120    avg_loss:0.301, val_acc:0.896]
Epoch [20/120    avg_loss:0.261, val_acc:0.886]
Epoch [21/120    avg_loss:0.216, val_acc:0.898]
Epoch [22/120    avg_loss:0.179, val_acc:0.929]
Epoch [23/120    avg_loss:0.191, val_acc:0.920]
Epoch [24/120    avg_loss:0.221, val_acc:0.906]
Epoch [25/120    avg_loss:0.166, val_acc:0.959]
Epoch [26/120    avg_loss:0.152, val_acc:0.928]
Epoch [27/120    avg_loss:0.143, val_acc:0.944]
Epoch [28/120    avg_loss:0.152, val_acc:0.941]
Epoch [29/120    avg_loss:0.149, val_acc:0.955]
Epoch [30/120    avg_loss:0.116, val_acc:0.953]
Epoch [31/120    avg_loss:0.166, val_acc:0.927]
Epoch [32/120    avg_loss:0.126, val_acc:0.968]
Epoch [33/120    avg_loss:0.099, val_acc:0.954]
Epoch [34/120    avg_loss:0.117, val_acc:0.955]
Epoch [35/120    avg_loss:0.082, val_acc:0.937]
Epoch [36/120    avg_loss:0.082, val_acc:0.972]
Epoch [37/120    avg_loss:0.067, val_acc:0.968]
Epoch [38/120    avg_loss:0.084, val_acc:0.964]
Epoch [39/120    avg_loss:0.067, val_acc:0.963]
Epoch [40/120    avg_loss:0.055, val_acc:0.971]
Epoch [41/120    avg_loss:0.049, val_acc:0.973]
Epoch [42/120    avg_loss:0.056, val_acc:0.969]
Epoch [43/120    avg_loss:0.070, val_acc:0.974]
Epoch [44/120    avg_loss:0.101, val_acc:0.936]
Epoch [45/120    avg_loss:0.097, val_acc:0.970]
Epoch [46/120    avg_loss:0.070, val_acc:0.928]
Epoch [47/120    avg_loss:0.058, val_acc:0.972]
Epoch [48/120    avg_loss:0.043, val_acc:0.960]
Epoch [49/120    avg_loss:0.052, val_acc:0.968]
Epoch [50/120    avg_loss:0.049, val_acc:0.973]
Epoch [51/120    avg_loss:0.035, val_acc:0.977]
Epoch [52/120    avg_loss:0.051, val_acc:0.970]
Epoch [53/120    avg_loss:0.063, val_acc:0.975]
Epoch [54/120    avg_loss:0.044, val_acc:0.982]
Epoch [55/120    avg_loss:0.025, val_acc:0.975]
Epoch [56/120    avg_loss:0.028, val_acc:0.976]
Epoch [57/120    avg_loss:0.035, val_acc:0.978]
Epoch [58/120    avg_loss:0.029, val_acc:0.977]
Epoch [59/120    avg_loss:0.029, val_acc:0.962]
Epoch [60/120    avg_loss:0.051, val_acc:0.972]
Epoch [61/120    avg_loss:0.032, val_acc:0.974]
Epoch [62/120    avg_loss:0.026, val_acc:0.979]
Epoch [63/120    avg_loss:0.027, val_acc:0.966]
Epoch [64/120    avg_loss:0.097, val_acc:0.962]
Epoch [65/120    avg_loss:0.064, val_acc:0.967]
Epoch [66/120    avg_loss:0.044, val_acc:0.969]
Epoch [67/120    avg_loss:0.033, val_acc:0.981]
Epoch [68/120    avg_loss:0.025, val_acc:0.980]
Epoch [69/120    avg_loss:0.027, val_acc:0.982]
Epoch [70/120    avg_loss:0.017, val_acc:0.982]
Epoch [71/120    avg_loss:0.019, val_acc:0.982]
Epoch [72/120    avg_loss:0.020, val_acc:0.982]
Epoch [73/120    avg_loss:0.024, val_acc:0.981]
Epoch [74/120    avg_loss:0.020, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.982]
Epoch [77/120    avg_loss:0.017, val_acc:0.983]
Epoch [78/120    avg_loss:0.017, val_acc:0.983]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.014, val_acc:0.982]
Epoch [81/120    avg_loss:0.020, val_acc:0.983]
Epoch [82/120    avg_loss:0.015, val_acc:0.983]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.015, val_acc:0.983]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.983]
Epoch [87/120    avg_loss:0.017, val_acc:0.983]
Epoch [88/120    avg_loss:0.016, val_acc:0.983]
Epoch [89/120    avg_loss:0.019, val_acc:0.981]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.016, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.022, val_acc:0.982]
Epoch [94/120    avg_loss:0.016, val_acc:0.983]
Epoch [95/120    avg_loss:0.017, val_acc:0.984]
Epoch [96/120    avg_loss:0.016, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.984]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.018, val_acc:0.983]
Epoch [101/120    avg_loss:0.024, val_acc:0.982]
Epoch [102/120    avg_loss:0.015, val_acc:0.983]
Epoch [103/120    avg_loss:0.016, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.984]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.013, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.013, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.014, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.015, val_acc:0.984]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.013, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.014, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6348     0     0     2     0     0    77     4     1]
 [    0     3 18049     0    20     0    16     0     2     0]
 [    0     4     0  2008     0     0     0     0    24     0]
 [    0    64    24     2  2859     0     7     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     2     0     0  4871     0     0     3]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0    33     0    13    50     0     0     0  3475     0]
 [    0     0     0     2    16    34     0     0     0   867]]

Accuracy:
98.97814089123467

F1 scores:
[       nan 0.98540826 0.99814738 0.98843219 0.96604156 0.9871407
 0.99672601 0.96985682 0.97997744 0.9681742 ]

Kappa:
0.9864621401849845
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f823e5609b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.084, val_acc:0.097]
Epoch [2/120    avg_loss:1.801, val_acc:0.154]
Epoch [3/120    avg_loss:1.602, val_acc:0.340]
Epoch [4/120    avg_loss:1.478, val_acc:0.359]
Epoch [5/120    avg_loss:1.318, val_acc:0.402]
Epoch [6/120    avg_loss:1.190, val_acc:0.430]
Epoch [7/120    avg_loss:1.074, val_acc:0.456]
Epoch [8/120    avg_loss:0.930, val_acc:0.469]
Epoch [9/120    avg_loss:0.849, val_acc:0.636]
Epoch [10/120    avg_loss:0.737, val_acc:0.648]
Epoch [11/120    avg_loss:0.630, val_acc:0.728]
Epoch [12/120    avg_loss:0.580, val_acc:0.746]
Epoch [13/120    avg_loss:0.456, val_acc:0.777]
Epoch [14/120    avg_loss:0.437, val_acc:0.785]
Epoch [15/120    avg_loss:0.423, val_acc:0.794]
Epoch [16/120    avg_loss:0.352, val_acc:0.813]
Epoch [17/120    avg_loss:0.305, val_acc:0.864]
Epoch [18/120    avg_loss:0.365, val_acc:0.839]
Epoch [19/120    avg_loss:0.295, val_acc:0.867]
Epoch [20/120    avg_loss:0.248, val_acc:0.866]
Epoch [21/120    avg_loss:0.234, val_acc:0.916]
Epoch [22/120    avg_loss:0.208, val_acc:0.892]
Epoch [23/120    avg_loss:0.173, val_acc:0.931]
Epoch [24/120    avg_loss:0.155, val_acc:0.932]
Epoch [25/120    avg_loss:0.161, val_acc:0.942]
Epoch [26/120    avg_loss:0.159, val_acc:0.915]
Epoch [27/120    avg_loss:0.131, val_acc:0.951]
Epoch [28/120    avg_loss:0.129, val_acc:0.951]
Epoch [29/120    avg_loss:0.146, val_acc:0.948]
Epoch [30/120    avg_loss:0.136, val_acc:0.945]
Epoch [31/120    avg_loss:0.103, val_acc:0.944]
Epoch [32/120    avg_loss:0.113, val_acc:0.954]
Epoch [33/120    avg_loss:0.091, val_acc:0.964]
Epoch [34/120    avg_loss:0.080, val_acc:0.959]
Epoch [35/120    avg_loss:0.079, val_acc:0.960]
Epoch [36/120    avg_loss:1.011, val_acc:0.486]
Epoch [37/120    avg_loss:0.988, val_acc:0.475]
Epoch [38/120    avg_loss:0.803, val_acc:0.678]
Epoch [39/120    avg_loss:0.661, val_acc:0.714]
Epoch [40/120    avg_loss:0.582, val_acc:0.743]
Epoch [41/120    avg_loss:0.587, val_acc:0.745]
Epoch [42/120    avg_loss:0.520, val_acc:0.760]
Epoch [43/120    avg_loss:0.472, val_acc:0.795]
Epoch [44/120    avg_loss:0.442, val_acc:0.800]
Epoch [45/120    avg_loss:0.408, val_acc:0.806]
Epoch [46/120    avg_loss:0.355, val_acc:0.834]
Epoch [47/120    avg_loss:0.325, val_acc:0.832]
Epoch [48/120    avg_loss:0.325, val_acc:0.831]
Epoch [49/120    avg_loss:0.293, val_acc:0.843]
Epoch [50/120    avg_loss:0.309, val_acc:0.839]
Epoch [51/120    avg_loss:0.296, val_acc:0.836]
Epoch [52/120    avg_loss:0.294, val_acc:0.834]
Epoch [53/120    avg_loss:0.280, val_acc:0.842]
Epoch [54/120    avg_loss:0.279, val_acc:0.837]
Epoch [55/120    avg_loss:0.290, val_acc:0.843]
Epoch [56/120    avg_loss:0.274, val_acc:0.840]
Epoch [57/120    avg_loss:0.278, val_acc:0.840]
Epoch [58/120    avg_loss:0.269, val_acc:0.845]
Epoch [59/120    avg_loss:0.262, val_acc:0.849]
Epoch [60/120    avg_loss:0.250, val_acc:0.848]
Epoch [61/120    avg_loss:0.266, val_acc:0.845]
Epoch [62/120    avg_loss:0.260, val_acc:0.844]
Epoch [63/120    avg_loss:0.262, val_acc:0.842]
Epoch [64/120    avg_loss:0.249, val_acc:0.845]
Epoch [65/120    avg_loss:0.248, val_acc:0.845]
Epoch [66/120    avg_loss:0.263, val_acc:0.845]
Epoch [67/120    avg_loss:0.245, val_acc:0.846]
Epoch [68/120    avg_loss:0.255, val_acc:0.845]
Epoch [69/120    avg_loss:0.265, val_acc:0.845]
Epoch [70/120    avg_loss:0.251, val_acc:0.845]
Epoch [71/120    avg_loss:0.250, val_acc:0.845]
Epoch [72/120    avg_loss:0.253, val_acc:0.845]
Epoch [73/120    avg_loss:0.257, val_acc:0.845]
Epoch [74/120    avg_loss:0.243, val_acc:0.845]
Epoch [75/120    avg_loss:0.254, val_acc:0.845]
Epoch [76/120    avg_loss:0.259, val_acc:0.845]
Epoch [77/120    avg_loss:0.259, val_acc:0.845]
Epoch [78/120    avg_loss:0.262, val_acc:0.845]
Epoch [79/120    avg_loss:0.246, val_acc:0.845]
Epoch [80/120    avg_loss:0.262, val_acc:0.845]
Epoch [81/120    avg_loss:0.262, val_acc:0.845]
Epoch [82/120    avg_loss:0.262, val_acc:0.845]
Epoch [83/120    avg_loss:0.253, val_acc:0.845]
Epoch [84/120    avg_loss:0.244, val_acc:0.845]
Epoch [85/120    avg_loss:0.245, val_acc:0.845]
Epoch [86/120    avg_loss:0.252, val_acc:0.845]
Epoch [87/120    avg_loss:0.263, val_acc:0.845]
Epoch [88/120    avg_loss:0.262, val_acc:0.845]
Epoch [89/120    avg_loss:0.252, val_acc:0.845]
Epoch [90/120    avg_loss:0.240, val_acc:0.845]
Epoch [91/120    avg_loss:0.245, val_acc:0.845]
Epoch [92/120    avg_loss:0.245, val_acc:0.845]
Epoch [93/120    avg_loss:0.269, val_acc:0.845]
Epoch [94/120    avg_loss:0.262, val_acc:0.845]
Epoch [95/120    avg_loss:0.262, val_acc:0.845]
Epoch [96/120    avg_loss:0.271, val_acc:0.845]
Epoch [97/120    avg_loss:0.262, val_acc:0.845]
Epoch [98/120    avg_loss:0.238, val_acc:0.845]
Epoch [99/120    avg_loss:0.246, val_acc:0.845]
Epoch [100/120    avg_loss:0.265, val_acc:0.845]
Epoch [101/120    avg_loss:0.250, val_acc:0.845]
Epoch [102/120    avg_loss:0.259, val_acc:0.845]
Epoch [103/120    avg_loss:0.248, val_acc:0.845]
Epoch [104/120    avg_loss:0.244, val_acc:0.845]
Epoch [105/120    avg_loss:0.249, val_acc:0.845]
Epoch [106/120    avg_loss:0.246, val_acc:0.845]
Epoch [107/120    avg_loss:0.256, val_acc:0.845]
Epoch [108/120    avg_loss:0.255, val_acc:0.845]
Epoch [109/120    avg_loss:0.240, val_acc:0.845]
Epoch [110/120    avg_loss:0.238, val_acc:0.845]
Epoch [111/120    avg_loss:0.256, val_acc:0.845]
Epoch [112/120    avg_loss:0.250, val_acc:0.845]
Epoch [113/120    avg_loss:0.257, val_acc:0.845]
Epoch [114/120    avg_loss:0.249, val_acc:0.845]
Epoch [115/120    avg_loss:0.250, val_acc:0.845]
Epoch [116/120    avg_loss:0.259, val_acc:0.845]
Epoch [117/120    avg_loss:0.249, val_acc:0.845]
Epoch [118/120    avg_loss:0.243, val_acc:0.845]
Epoch [119/120    avg_loss:0.251, val_acc:0.845]
Epoch [120/120    avg_loss:0.249, val_acc:0.845]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5579     0   216   180     0     0    88   288    81]
 [    0     0 15065     0   371     0  2654     0     0     0]
 [    0    11     0  1944     2     0     0     0    55    24]
 [    0    95    28     1  2774     0    24     4    43     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   305    16     0     0  4377     0   180     0]
 [    0    24     0     0     0     0     7  1254     0     5]
 [    0   170     0    25    47     0     9     0  3320     0]
 [    0    14     0    10    21    80     0     3     2   789]]

Accuracy:
87.74251078495168

F1 scores:
[       nan 0.9053144  0.89972527 0.91525424 0.87136799 0.97026022
 0.73261361 0.95035998 0.89019976 0.86655684]

Kappa:
0.8422615291156303
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0922878a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.026, val_acc:0.120]
Epoch [2/120    avg_loss:1.700, val_acc:0.299]
Epoch [3/120    avg_loss:1.507, val_acc:0.351]
Epoch [4/120    avg_loss:1.362, val_acc:0.365]
Epoch [5/120    avg_loss:1.252, val_acc:0.446]
Epoch [6/120    avg_loss:1.091, val_acc:0.497]
Epoch [7/120    avg_loss:0.997, val_acc:0.582]
Epoch [8/120    avg_loss:0.864, val_acc:0.595]
Epoch [9/120    avg_loss:0.757, val_acc:0.655]
Epoch [10/120    avg_loss:0.697, val_acc:0.707]
Epoch [11/120    avg_loss:0.636, val_acc:0.626]
Epoch [12/120    avg_loss:0.589, val_acc:0.731]
Epoch [13/120    avg_loss:0.478, val_acc:0.826]
Epoch [14/120    avg_loss:0.454, val_acc:0.845]
Epoch [15/120    avg_loss:0.398, val_acc:0.858]
Epoch [16/120    avg_loss:0.375, val_acc:0.896]
Epoch [17/120    avg_loss:0.344, val_acc:0.872]
Epoch [18/120    avg_loss:0.994, val_acc:0.457]
Epoch [19/120    avg_loss:1.072, val_acc:0.554]
Epoch [20/120    avg_loss:0.924, val_acc:0.705]
Epoch [21/120    avg_loss:0.784, val_acc:0.718]
Epoch [22/120    avg_loss:0.754, val_acc:0.751]
Epoch [23/120    avg_loss:0.656, val_acc:0.653]
Epoch [24/120    avg_loss:0.623, val_acc:0.767]
Epoch [25/120    avg_loss:0.599, val_acc:0.785]
Epoch [26/120    avg_loss:0.503, val_acc:0.764]
Epoch [27/120    avg_loss:0.507, val_acc:0.783]
Epoch [28/120    avg_loss:0.528, val_acc:0.751]
Epoch [29/120    avg_loss:0.480, val_acc:0.793]
Epoch [30/120    avg_loss:0.436, val_acc:0.861]
Epoch [31/120    avg_loss:0.410, val_acc:0.877]
Epoch [32/120    avg_loss:0.391, val_acc:0.864]
Epoch [33/120    avg_loss:0.391, val_acc:0.866]
Epoch [34/120    avg_loss:0.379, val_acc:0.869]
Epoch [35/120    avg_loss:0.371, val_acc:0.866]
Epoch [36/120    avg_loss:0.362, val_acc:0.874]
Epoch [37/120    avg_loss:0.361, val_acc:0.873]
Epoch [38/120    avg_loss:0.356, val_acc:0.873]
Epoch [39/120    avg_loss:0.357, val_acc:0.883]
Epoch [40/120    avg_loss:0.348, val_acc:0.872]
Epoch [41/120    avg_loss:0.347, val_acc:0.878]
Epoch [42/120    avg_loss:0.342, val_acc:0.872]
Epoch [43/120    avg_loss:0.341, val_acc:0.874]
Epoch [44/120    avg_loss:0.345, val_acc:0.880]
Epoch [45/120    avg_loss:0.347, val_acc:0.880]
Epoch [46/120    avg_loss:0.342, val_acc:0.880]
Epoch [47/120    avg_loss:0.340, val_acc:0.883]
Epoch [48/120    avg_loss:0.330, val_acc:0.885]
Epoch [49/120    avg_loss:0.335, val_acc:0.883]
Epoch [50/120    avg_loss:0.345, val_acc:0.885]
Epoch [51/120    avg_loss:0.341, val_acc:0.881]
Epoch [52/120    avg_loss:0.334, val_acc:0.884]
Epoch [53/120    avg_loss:0.338, val_acc:0.882]
Epoch [54/120    avg_loss:0.341, val_acc:0.881]
Epoch [55/120    avg_loss:0.343, val_acc:0.885]
Epoch [56/120    avg_loss:0.334, val_acc:0.885]
Epoch [57/120    avg_loss:0.337, val_acc:0.885]
Epoch [58/120    avg_loss:0.330, val_acc:0.885]
Epoch [59/120    avg_loss:0.329, val_acc:0.885]
Epoch [60/120    avg_loss:0.327, val_acc:0.885]
Epoch [61/120    avg_loss:0.345, val_acc:0.885]
Epoch [62/120    avg_loss:0.333, val_acc:0.885]
Epoch [63/120    avg_loss:0.329, val_acc:0.885]
Epoch [64/120    avg_loss:0.361, val_acc:0.885]
Epoch [65/120    avg_loss:0.332, val_acc:0.885]
Epoch [66/120    avg_loss:0.330, val_acc:0.884]
Epoch [67/120    avg_loss:0.339, val_acc:0.884]
Epoch [68/120    avg_loss:0.350, val_acc:0.884]
Epoch [69/120    avg_loss:0.325, val_acc:0.884]
Epoch [70/120    avg_loss:0.341, val_acc:0.884]
Epoch [71/120    avg_loss:0.351, val_acc:0.884]
Epoch [72/120    avg_loss:0.347, val_acc:0.884]
Epoch [73/120    avg_loss:0.326, val_acc:0.884]
Epoch [74/120    avg_loss:0.341, val_acc:0.884]
Epoch [75/120    avg_loss:0.329, val_acc:0.884]
Epoch [76/120    avg_loss:0.337, val_acc:0.884]
Epoch [77/120    avg_loss:0.333, val_acc:0.884]
Epoch [78/120    avg_loss:0.334, val_acc:0.883]
Epoch [79/120    avg_loss:0.339, val_acc:0.884]
Epoch [80/120    avg_loss:0.331, val_acc:0.884]
Epoch [81/120    avg_loss:0.323, val_acc:0.884]
Epoch [82/120    avg_loss:0.341, val_acc:0.884]
Epoch [83/120    avg_loss:0.342, val_acc:0.884]
Epoch [84/120    avg_loss:0.325, val_acc:0.884]
Epoch [85/120    avg_loss:0.318, val_acc:0.884]
Epoch [86/120    avg_loss:0.340, val_acc:0.884]
Epoch [87/120    avg_loss:0.333, val_acc:0.884]
Epoch [88/120    avg_loss:0.348, val_acc:0.884]
Epoch [89/120    avg_loss:0.318, val_acc:0.884]
Epoch [90/120    avg_loss:0.320, val_acc:0.884]
Epoch [91/120    avg_loss:0.336, val_acc:0.884]
Epoch [92/120    avg_loss:0.326, val_acc:0.884]
Epoch [93/120    avg_loss:0.331, val_acc:0.884]
Epoch [94/120    avg_loss:0.336, val_acc:0.884]
Epoch [95/120    avg_loss:0.336, val_acc:0.884]
Epoch [96/120    avg_loss:0.317, val_acc:0.884]
Epoch [97/120    avg_loss:0.338, val_acc:0.884]
Epoch [98/120    avg_loss:0.334, val_acc:0.884]
Epoch [99/120    avg_loss:0.333, val_acc:0.884]
Epoch [100/120    avg_loss:0.336, val_acc:0.884]
Epoch [101/120    avg_loss:0.334, val_acc:0.884]
Epoch [102/120    avg_loss:0.336, val_acc:0.884]
Epoch [103/120    avg_loss:0.332, val_acc:0.884]
Epoch [104/120    avg_loss:0.327, val_acc:0.884]
Epoch [105/120    avg_loss:0.347, val_acc:0.884]
Epoch [106/120    avg_loss:0.323, val_acc:0.884]
Epoch [107/120    avg_loss:0.337, val_acc:0.884]
Epoch [108/120    avg_loss:0.331, val_acc:0.884]
Epoch [109/120    avg_loss:0.328, val_acc:0.884]
Epoch [110/120    avg_loss:0.342, val_acc:0.884]
Epoch [111/120    avg_loss:0.341, val_acc:0.884]
Epoch [112/120    avg_loss:0.333, val_acc:0.884]
Epoch [113/120    avg_loss:0.325, val_acc:0.884]
Epoch [114/120    avg_loss:0.327, val_acc:0.884]
Epoch [115/120    avg_loss:0.331, val_acc:0.884]
Epoch [116/120    avg_loss:0.333, val_acc:0.884]
Epoch [117/120    avg_loss:0.336, val_acc:0.884]
Epoch [118/120    avg_loss:0.335, val_acc:0.884]
Epoch [119/120    avg_loss:0.328, val_acc:0.884]
Epoch [120/120    avg_loss:0.330, val_acc:0.884]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5075     0    93   353     0     0   133   579   199]
 [    0     0 16455     0   180     0  1455     0     0     0]
 [    0     9     0  1890     0     0     0     0    83    54]
 [    0   154    79     0  2650     0    30     0    56     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   295     0    33     0  4519     0    31     0]
 [    0    49     0     0     0     0     5  1226     0    10]
 [    0   287     0   128    63     0     7     0  3084     2]
 [    0    21     0     7    18   113     0     0    10   750]]

Accuracy:
89.06080543706167

F1 scores:
[       nan 0.84393448 0.94246685 0.9099663  0.84542989 0.95850165
 0.82963099 0.92563231 0.83193957 0.77439339]

Kappa:
0.8572693998848635
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe69e71b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.099, val_acc:0.077]
Epoch [2/120    avg_loss:1.798, val_acc:0.220]
Epoch [3/120    avg_loss:1.541, val_acc:0.292]
Epoch [4/120    avg_loss:1.419, val_acc:0.372]
Epoch [5/120    avg_loss:1.262, val_acc:0.369]
Epoch [6/120    avg_loss:1.143, val_acc:0.420]
Epoch [7/120    avg_loss:0.983, val_acc:0.480]
Epoch [8/120    avg_loss:0.860, val_acc:0.577]
Epoch [9/120    avg_loss:0.739, val_acc:0.692]
Epoch [10/120    avg_loss:0.641, val_acc:0.676]
Epoch [11/120    avg_loss:0.549, val_acc:0.716]
Epoch [12/120    avg_loss:0.474, val_acc:0.792]
Epoch [13/120    avg_loss:0.410, val_acc:0.852]
Epoch [14/120    avg_loss:0.428, val_acc:0.760]
Epoch [15/120    avg_loss:0.405, val_acc:0.847]
Epoch [16/120    avg_loss:0.335, val_acc:0.832]
Epoch [17/120    avg_loss:0.299, val_acc:0.861]
Epoch [18/120    avg_loss:0.268, val_acc:0.828]
Epoch [19/120    avg_loss:0.232, val_acc:0.870]
Epoch [20/120    avg_loss:0.208, val_acc:0.932]
Epoch [21/120    avg_loss:0.207, val_acc:0.919]
Epoch [22/120    avg_loss:0.180, val_acc:0.939]
Epoch [23/120    avg_loss:0.168, val_acc:0.936]
Epoch [24/120    avg_loss:0.359, val_acc:0.829]
Epoch [25/120    avg_loss:0.276, val_acc:0.927]
Epoch [26/120    avg_loss:0.163, val_acc:0.939]
Epoch [27/120    avg_loss:0.150, val_acc:0.918]
Epoch [28/120    avg_loss:0.161, val_acc:0.895]
Epoch [29/120    avg_loss:0.123, val_acc:0.957]
Epoch [30/120    avg_loss:0.128, val_acc:0.937]
Epoch [31/120    avg_loss:0.113, val_acc:0.885]
Epoch [32/120    avg_loss:0.117, val_acc:0.889]
Epoch [33/120    avg_loss:0.152, val_acc:0.955]
Epoch [34/120    avg_loss:0.090, val_acc:0.956]
Epoch [35/120    avg_loss:0.094, val_acc:0.951]
Epoch [36/120    avg_loss:0.082, val_acc:0.966]
Epoch [37/120    avg_loss:0.074, val_acc:0.965]
Epoch [38/120    avg_loss:0.090, val_acc:0.960]
Epoch [39/120    avg_loss:0.077, val_acc:0.965]
Epoch [40/120    avg_loss:0.070, val_acc:0.968]
Epoch [41/120    avg_loss:0.063, val_acc:0.970]
Epoch [42/120    avg_loss:0.075, val_acc:0.960]
Epoch [43/120    avg_loss:0.069, val_acc:0.973]
Epoch [44/120    avg_loss:0.053, val_acc:0.957]
Epoch [45/120    avg_loss:0.041, val_acc:0.978]
Epoch [46/120    avg_loss:0.031, val_acc:0.977]
Epoch [47/120    avg_loss:0.043, val_acc:0.964]
Epoch [48/120    avg_loss:0.040, val_acc:0.978]
Epoch [49/120    avg_loss:0.033, val_acc:0.984]
Epoch [50/120    avg_loss:0.030, val_acc:0.977]
Epoch [51/120    avg_loss:0.029, val_acc:0.978]
Epoch [52/120    avg_loss:0.026, val_acc:0.965]
Epoch [53/120    avg_loss:0.022, val_acc:0.986]
Epoch [54/120    avg_loss:0.020, val_acc:0.973]
Epoch [55/120    avg_loss:0.035, val_acc:0.936]
Epoch [56/120    avg_loss:0.033, val_acc:0.974]
Epoch [57/120    avg_loss:0.029, val_acc:0.975]
Epoch [58/120    avg_loss:0.040, val_acc:0.980]
Epoch [59/120    avg_loss:0.033, val_acc:0.981]
Epoch [60/120    avg_loss:0.025, val_acc:0.984]
Epoch [61/120    avg_loss:0.046, val_acc:0.977]
Epoch [62/120    avg_loss:0.026, val_acc:0.980]
Epoch [63/120    avg_loss:0.017, val_acc:0.977]
Epoch [64/120    avg_loss:0.015, val_acc:0.985]
Epoch [65/120    avg_loss:0.017, val_acc:0.977]
Epoch [66/120    avg_loss:0.013, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.986]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.015, val_acc:0.976]
Epoch [70/120    avg_loss:0.024, val_acc:0.984]
Epoch [71/120    avg_loss:0.017, val_acc:0.982]
Epoch [72/120    avg_loss:0.019, val_acc:0.976]
Epoch [73/120    avg_loss:0.017, val_acc:0.977]
Epoch [74/120    avg_loss:0.014, val_acc:0.980]
Epoch [75/120    avg_loss:0.026, val_acc:0.977]
Epoch [76/120    avg_loss:0.044, val_acc:0.981]
Epoch [77/120    avg_loss:0.026, val_acc:0.972]
Epoch [78/120    avg_loss:0.039, val_acc:0.974]
Epoch [79/120    avg_loss:0.014, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.016, val_acc:0.977]
Epoch [83/120    avg_loss:0.018, val_acc:0.983]
Epoch [84/120    avg_loss:0.080, val_acc:0.951]
Epoch [85/120    avg_loss:0.291, val_acc:0.943]
Epoch [86/120    avg_loss:0.132, val_acc:0.933]
Epoch [87/120    avg_loss:0.083, val_acc:0.974]
Epoch [88/120    avg_loss:0.045, val_acc:0.954]
Epoch [89/120    avg_loss:0.054, val_acc:0.971]
Epoch [90/120    avg_loss:0.036, val_acc:0.971]
Epoch [91/120    avg_loss:0.030, val_acc:0.984]
Epoch [92/120    avg_loss:0.063, val_acc:0.966]
Epoch [93/120    avg_loss:0.045, val_acc:0.977]
Epoch [94/120    avg_loss:0.035, val_acc:0.981]
Epoch [95/120    avg_loss:0.024, val_acc:0.983]
Epoch [96/120    avg_loss:0.021, val_acc:0.980]
Epoch [97/120    avg_loss:0.023, val_acc:0.984]
Epoch [98/120    avg_loss:0.022, val_acc:0.984]
Epoch [99/120    avg_loss:0.018, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.984]
Epoch [101/120    avg_loss:0.016, val_acc:0.984]
Epoch [102/120    avg_loss:0.024, val_acc:0.983]
Epoch [103/120    avg_loss:0.017, val_acc:0.983]
Epoch [104/120    avg_loss:0.021, val_acc:0.984]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.016, val_acc:0.984]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.982]
Epoch [109/120    avg_loss:0.018, val_acc:0.983]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.988]
Epoch [112/120    avg_loss:0.014, val_acc:0.987]
Epoch [113/120    avg_loss:0.015, val_acc:0.986]
Epoch [114/120    avg_loss:0.013, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.985]
Epoch [116/120    avg_loss:0.013, val_acc:0.985]
Epoch [117/120    avg_loss:0.017, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6330     0     0    13     0    17     0    72     0]
 [    0     2 18028     0    49     0     7     0     4     0]
 [    0     4     0  2018     2     0     0     0    12     0]
 [    0    33    20     1  2873     0    12     0    33     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     3    21     0     0  4850     0     0     4]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0     0     0    25    55     0     0     0  3478    13]
 [    0     0     0     6    14    59     0     0     0   840]]

Accuracy:
98.82630805196058

F1 scores:
[       nan 0.98898524 0.9976481  0.98271244 0.96119103 0.97751124
 0.99324186 0.99805825 0.97015342 0.94382022]

Kappa:
0.9844571281597568
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03e2cfa978>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.148, val_acc:0.102]
Epoch [2/120    avg_loss:1.768, val_acc:0.153]
Epoch [3/120    avg_loss:1.551, val_acc:0.334]
Epoch [4/120    avg_loss:1.364, val_acc:0.406]
Epoch [5/120    avg_loss:1.160, val_acc:0.473]
Epoch [6/120    avg_loss:1.025, val_acc:0.460]
Epoch [7/120    avg_loss:0.882, val_acc:0.615]
Epoch [8/120    avg_loss:0.788, val_acc:0.652]
Epoch [9/120    avg_loss:0.695, val_acc:0.708]
Epoch [10/120    avg_loss:0.626, val_acc:0.716]
Epoch [11/120    avg_loss:0.536, val_acc:0.708]
Epoch [12/120    avg_loss:0.481, val_acc:0.787]
Epoch [13/120    avg_loss:0.440, val_acc:0.807]
Epoch [14/120    avg_loss:0.417, val_acc:0.783]
Epoch [15/120    avg_loss:0.383, val_acc:0.815]
Epoch [16/120    avg_loss:0.360, val_acc:0.851]
Epoch [17/120    avg_loss:0.325, val_acc:0.748]
Epoch [18/120    avg_loss:0.311, val_acc:0.873]
Epoch [19/120    avg_loss:0.304, val_acc:0.795]
Epoch [20/120    avg_loss:0.261, val_acc:0.839]
Epoch [21/120    avg_loss:0.252, val_acc:0.845]
Epoch [22/120    avg_loss:0.280, val_acc:0.887]
Epoch [23/120    avg_loss:0.221, val_acc:0.915]
Epoch [24/120    avg_loss:0.225, val_acc:0.863]
Epoch [25/120    avg_loss:0.207, val_acc:0.884]
Epoch [26/120    avg_loss:0.197, val_acc:0.897]
Epoch [27/120    avg_loss:0.166, val_acc:0.933]
Epoch [28/120    avg_loss:0.162, val_acc:0.932]
Epoch [29/120    avg_loss:0.155, val_acc:0.920]
Epoch [30/120    avg_loss:0.117, val_acc:0.951]
Epoch [31/120    avg_loss:0.126, val_acc:0.936]
Epoch [32/120    avg_loss:0.131, val_acc:0.953]
Epoch [33/120    avg_loss:0.094, val_acc:0.961]
Epoch [34/120    avg_loss:0.084, val_acc:0.965]
Epoch [35/120    avg_loss:0.094, val_acc:0.887]
Epoch [36/120    avg_loss:0.097, val_acc:0.956]
Epoch [37/120    avg_loss:0.089, val_acc:0.961]
Epoch [38/120    avg_loss:0.088, val_acc:0.954]
Epoch [39/120    avg_loss:0.091, val_acc:0.947]
Epoch [40/120    avg_loss:0.092, val_acc:0.934]
Epoch [41/120    avg_loss:0.070, val_acc:0.944]
Epoch [42/120    avg_loss:0.112, val_acc:0.949]
Epoch [43/120    avg_loss:0.070, val_acc:0.957]
Epoch [44/120    avg_loss:0.066, val_acc:0.965]
Epoch [45/120    avg_loss:0.070, val_acc:0.971]
Epoch [46/120    avg_loss:0.051, val_acc:0.964]
Epoch [47/120    avg_loss:0.058, val_acc:0.962]
Epoch [48/120    avg_loss:0.074, val_acc:0.965]
Epoch [49/120    avg_loss:0.056, val_acc:0.973]
Epoch [50/120    avg_loss:0.050, val_acc:0.956]
Epoch [51/120    avg_loss:0.038, val_acc:0.973]
Epoch [52/120    avg_loss:0.036, val_acc:0.969]
Epoch [53/120    avg_loss:0.039, val_acc:0.979]
Epoch [54/120    avg_loss:0.053, val_acc:0.968]
Epoch [55/120    avg_loss:0.044, val_acc:0.973]
Epoch [56/120    avg_loss:0.031, val_acc:0.978]
Epoch [57/120    avg_loss:0.042, val_acc:0.973]
Epoch [58/120    avg_loss:0.048, val_acc:0.963]
Epoch [59/120    avg_loss:0.044, val_acc:0.948]
Epoch [60/120    avg_loss:0.033, val_acc:0.970]
Epoch [61/120    avg_loss:0.031, val_acc:0.974]
Epoch [62/120    avg_loss:0.026, val_acc:0.978]
Epoch [63/120    avg_loss:0.020, val_acc:0.977]
Epoch [64/120    avg_loss:0.014, val_acc:0.982]
Epoch [65/120    avg_loss:0.014, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.983]
Epoch [67/120    avg_loss:0.021, val_acc:0.970]
Epoch [68/120    avg_loss:0.022, val_acc:0.982]
Epoch [69/120    avg_loss:0.020, val_acc:0.981]
Epoch [70/120    avg_loss:0.026, val_acc:0.967]
Epoch [71/120    avg_loss:0.025, val_acc:0.956]
Epoch [72/120    avg_loss:0.030, val_acc:0.971]
Epoch [73/120    avg_loss:0.032, val_acc:0.968]
Epoch [74/120    avg_loss:0.047, val_acc:0.978]
Epoch [75/120    avg_loss:0.019, val_acc:0.983]
Epoch [76/120    avg_loss:0.020, val_acc:0.977]
Epoch [77/120    avg_loss:0.026, val_acc:0.964]
Epoch [78/120    avg_loss:0.031, val_acc:0.967]
Epoch [79/120    avg_loss:0.019, val_acc:0.974]
Epoch [80/120    avg_loss:0.015, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.013, val_acc:0.981]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.012, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.010, val_acc:0.982]
Epoch [99/120    avg_loss:0.011, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.012, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.987]
Epoch [113/120    avg_loss:0.010, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     0     0     0     0    43    24     0]
 [    0     0 18064     0    14     0     5     0     7     0]
 [    0     4     0  1948     0     0     0     0    83     1]
 [    0    21     8     0  2930     0     6     0     4     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4870     0     8     0]
 [    0    24     0     0     0     0     0  1265     1     0]
 [    0    31     1    49    15     0    15     3  3457     0]
 [    0     0     0     0    12    16     0     0     0   891]]

Accuracy:
99.04080206299858

F1 scores:
[       nan 0.9885843  0.99903216 0.96603025 0.98603399 0.99390708
 0.99652138 0.97270281 0.96631726 0.98235943]

Kappa:
0.9872914774154269
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5fc7fa908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.119, val_acc:0.333]
Epoch [2/120    avg_loss:1.756, val_acc:0.517]
Epoch [3/120    avg_loss:1.503, val_acc:0.568]
Epoch [4/120    avg_loss:1.322, val_acc:0.556]
Epoch [5/120    avg_loss:1.201, val_acc:0.495]
Epoch [6/120    avg_loss:1.073, val_acc:0.487]
Epoch [7/120    avg_loss:0.959, val_acc:0.426]
Epoch [8/120    avg_loss:0.876, val_acc:0.571]
Epoch [9/120    avg_loss:0.773, val_acc:0.543]
Epoch [10/120    avg_loss:0.706, val_acc:0.537]
Epoch [11/120    avg_loss:0.619, val_acc:0.598]
Epoch [12/120    avg_loss:0.531, val_acc:0.613]
Epoch [13/120    avg_loss:0.478, val_acc:0.653]
Epoch [14/120    avg_loss:0.443, val_acc:0.682]
Epoch [15/120    avg_loss:0.412, val_acc:0.730]
Epoch [16/120    avg_loss:0.380, val_acc:0.769]
Epoch [17/120    avg_loss:0.367, val_acc:0.784]
Epoch [18/120    avg_loss:0.327, val_acc:0.805]
Epoch [19/120    avg_loss:0.300, val_acc:0.831]
Epoch [20/120    avg_loss:0.279, val_acc:0.863]
Epoch [21/120    avg_loss:0.286, val_acc:0.868]
Epoch [22/120    avg_loss:0.252, val_acc:0.847]
Epoch [23/120    avg_loss:0.229, val_acc:0.902]
Epoch [24/120    avg_loss:0.193, val_acc:0.926]
Epoch [25/120    avg_loss:0.207, val_acc:0.948]
Epoch [26/120    avg_loss:0.215, val_acc:0.905]
Epoch [27/120    avg_loss:0.210, val_acc:0.931]
Epoch [28/120    avg_loss:0.152, val_acc:0.915]
Epoch [29/120    avg_loss:0.149, val_acc:0.944]
Epoch [30/120    avg_loss:0.140, val_acc:0.921]
Epoch [31/120    avg_loss:0.122, val_acc:0.948]
Epoch [32/120    avg_loss:0.126, val_acc:0.940]
Epoch [33/120    avg_loss:0.110, val_acc:0.936]
Epoch [34/120    avg_loss:0.113, val_acc:0.953]
Epoch [35/120    avg_loss:0.100, val_acc:0.945]
Epoch [36/120    avg_loss:0.122, val_acc:0.956]
Epoch [37/120    avg_loss:0.093, val_acc:0.946]
Epoch [38/120    avg_loss:0.119, val_acc:0.916]
Epoch [39/120    avg_loss:0.087, val_acc:0.953]
Epoch [40/120    avg_loss:0.070, val_acc:0.961]
Epoch [41/120    avg_loss:0.071, val_acc:0.957]
Epoch [42/120    avg_loss:0.060, val_acc:0.949]
Epoch [43/120    avg_loss:0.056, val_acc:0.966]
Epoch [44/120    avg_loss:0.061, val_acc:0.953]
Epoch [45/120    avg_loss:0.075, val_acc:0.961]
Epoch [46/120    avg_loss:0.065, val_acc:0.958]
Epoch [47/120    avg_loss:0.062, val_acc:0.969]
Epoch [48/120    avg_loss:0.061, val_acc:0.958]
Epoch [49/120    avg_loss:0.053, val_acc:0.967]
Epoch [50/120    avg_loss:0.048, val_acc:0.971]
Epoch [51/120    avg_loss:0.049, val_acc:0.959]
Epoch [52/120    avg_loss:0.065, val_acc:0.968]
Epoch [53/120    avg_loss:0.041, val_acc:0.973]
Epoch [54/120    avg_loss:0.036, val_acc:0.963]
Epoch [55/120    avg_loss:0.041, val_acc:0.968]
Epoch [56/120    avg_loss:0.046, val_acc:0.966]
Epoch [57/120    avg_loss:0.042, val_acc:0.958]
Epoch [58/120    avg_loss:0.035, val_acc:0.963]
Epoch [59/120    avg_loss:0.054, val_acc:0.967]
Epoch [60/120    avg_loss:0.056, val_acc:0.956]
Epoch [61/120    avg_loss:0.042, val_acc:0.961]
Epoch [62/120    avg_loss:0.042, val_acc:0.957]
Epoch [63/120    avg_loss:0.042, val_acc:0.968]
Epoch [64/120    avg_loss:0.039, val_acc:0.958]
Epoch [65/120    avg_loss:0.043, val_acc:0.966]
Epoch [66/120    avg_loss:0.043, val_acc:0.965]
Epoch [67/120    avg_loss:0.040, val_acc:0.976]
Epoch [68/120    avg_loss:0.020, val_acc:0.974]
Epoch [69/120    avg_loss:0.025, val_acc:0.977]
Epoch [70/120    avg_loss:0.017, val_acc:0.973]
Epoch [71/120    avg_loss:0.017, val_acc:0.977]
Epoch [72/120    avg_loss:0.017, val_acc:0.978]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.018, val_acc:0.978]
Epoch [75/120    avg_loss:0.016, val_acc:0.977]
Epoch [76/120    avg_loss:0.020, val_acc:0.974]
Epoch [77/120    avg_loss:0.021, val_acc:0.978]
Epoch [78/120    avg_loss:0.018, val_acc:0.978]
Epoch [79/120    avg_loss:0.014, val_acc:0.977]
Epoch [80/120    avg_loss:0.013, val_acc:0.978]
Epoch [81/120    avg_loss:0.014, val_acc:0.980]
Epoch [82/120    avg_loss:0.021, val_acc:0.980]
Epoch [83/120    avg_loss:0.014, val_acc:0.981]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.012, val_acc:0.981]
Epoch [86/120    avg_loss:0.017, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.980]
Epoch [88/120    avg_loss:0.015, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.978]
Epoch [90/120    avg_loss:0.015, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.977]
Epoch [92/120    avg_loss:0.012, val_acc:0.980]
Epoch [93/120    avg_loss:0.016, val_acc:0.982]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.980]
Epoch [97/120    avg_loss:0.012, val_acc:0.980]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.982]
Epoch [101/120    avg_loss:0.013, val_acc:0.982]
Epoch [102/120    avg_loss:0.012, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.978]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.979]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6302     0     1     0     0     4    50    67     8]
 [    0     0 18020     0    59     0     7     0     4     0]
 [    0     1     0  1957     0     0     0     0    78     0]
 [    0    10    12     0  2923     0    20     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    42     0     0     0  4834     0     2     0]
 [    0    16     0     0     0     0     0  1272     1     1]
 [    0    21     4    68    30     0     3     0  3444     1]
 [    0     0     0     0     9    23     0     0     0   887]]

Accuracy:
98.67688525775432

F1 scores:
[       nan 0.98607417 0.99646096 0.96356475 0.97547138 0.99126472
 0.99199672 0.97396631 0.96040156 0.97579758]

Kappa:
0.9824747819913335
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcafb907978>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.123, val_acc:0.102]
Epoch [2/120    avg_loss:1.743, val_acc:0.174]
Epoch [3/120    avg_loss:1.500, val_acc:0.429]
Epoch [4/120    avg_loss:1.318, val_acc:0.573]
Epoch [5/120    avg_loss:1.177, val_acc:0.519]
Epoch [6/120    avg_loss:1.051, val_acc:0.614]
Epoch [7/120    avg_loss:0.889, val_acc:0.755]
Epoch [8/120    avg_loss:0.762, val_acc:0.730]
Epoch [9/120    avg_loss:0.670, val_acc:0.798]
Epoch [10/120    avg_loss:0.586, val_acc:0.757]
Epoch [11/120    avg_loss:0.500, val_acc:0.760]
Epoch [12/120    avg_loss:0.460, val_acc:0.834]
Epoch [13/120    avg_loss:0.410, val_acc:0.830]
Epoch [14/120    avg_loss:0.379, val_acc:0.806]
Epoch [15/120    avg_loss:0.339, val_acc:0.844]
Epoch [16/120    avg_loss:0.330, val_acc:0.865]
Epoch [17/120    avg_loss:0.278, val_acc:0.888]
Epoch [18/120    avg_loss:0.254, val_acc:0.845]
Epoch [19/120    avg_loss:0.238, val_acc:0.911]
Epoch [20/120    avg_loss:0.244, val_acc:0.866]
Epoch [21/120    avg_loss:0.229, val_acc:0.919]
Epoch [22/120    avg_loss:0.216, val_acc:0.911]
Epoch [23/120    avg_loss:0.182, val_acc:0.927]
Epoch [24/120    avg_loss:0.169, val_acc:0.895]
Epoch [25/120    avg_loss:0.151, val_acc:0.868]
Epoch [26/120    avg_loss:0.178, val_acc:0.897]
Epoch [27/120    avg_loss:0.154, val_acc:0.960]
Epoch [28/120    avg_loss:0.137, val_acc:0.937]
Epoch [29/120    avg_loss:0.129, val_acc:0.927]
Epoch [30/120    avg_loss:0.124, val_acc:0.942]
Epoch [31/120    avg_loss:0.145, val_acc:0.934]
Epoch [32/120    avg_loss:0.098, val_acc:0.938]
Epoch [33/120    avg_loss:0.087, val_acc:0.958]
Epoch [34/120    avg_loss:0.094, val_acc:0.953]
Epoch [35/120    avg_loss:0.078, val_acc:0.963]
Epoch [36/120    avg_loss:0.098, val_acc:0.955]
Epoch [37/120    avg_loss:0.093, val_acc:0.957]
Epoch [38/120    avg_loss:0.081, val_acc:0.964]
Epoch [39/120    avg_loss:0.091, val_acc:0.962]
Epoch [40/120    avg_loss:0.071, val_acc:0.961]
Epoch [41/120    avg_loss:0.068, val_acc:0.948]
Epoch [42/120    avg_loss:0.095, val_acc:0.953]
Epoch [43/120    avg_loss:0.075, val_acc:0.961]
Epoch [44/120    avg_loss:0.064, val_acc:0.960]
Epoch [45/120    avg_loss:0.063, val_acc:0.967]
Epoch [46/120    avg_loss:0.052, val_acc:0.969]
Epoch [47/120    avg_loss:0.047, val_acc:0.967]
Epoch [48/120    avg_loss:0.042, val_acc:0.971]
Epoch [49/120    avg_loss:0.079, val_acc:0.916]
Epoch [50/120    avg_loss:0.070, val_acc:0.948]
Epoch [51/120    avg_loss:0.040, val_acc:0.963]
Epoch [52/120    avg_loss:0.051, val_acc:0.965]
Epoch [53/120    avg_loss:0.031, val_acc:0.978]
Epoch [54/120    avg_loss:0.024, val_acc:0.978]
Epoch [55/120    avg_loss:0.029, val_acc:0.972]
Epoch [56/120    avg_loss:0.088, val_acc:0.942]
Epoch [57/120    avg_loss:0.096, val_acc:0.937]
Epoch [58/120    avg_loss:0.047, val_acc:0.974]
Epoch [59/120    avg_loss:0.035, val_acc:0.978]
Epoch [60/120    avg_loss:0.066, val_acc:0.948]
Epoch [61/120    avg_loss:0.053, val_acc:0.965]
Epoch [62/120    avg_loss:0.043, val_acc:0.958]
Epoch [63/120    avg_loss:0.033, val_acc:0.979]
Epoch [64/120    avg_loss:0.026, val_acc:0.973]
Epoch [65/120    avg_loss:0.024, val_acc:0.978]
Epoch [66/120    avg_loss:0.043, val_acc:0.971]
Epoch [67/120    avg_loss:0.039, val_acc:0.967]
Epoch [68/120    avg_loss:0.026, val_acc:0.972]
Epoch [69/120    avg_loss:0.026, val_acc:0.979]
Epoch [70/120    avg_loss:0.029, val_acc:0.980]
Epoch [71/120    avg_loss:0.030, val_acc:0.978]
Epoch [72/120    avg_loss:0.026, val_acc:0.976]
Epoch [73/120    avg_loss:0.027, val_acc:0.978]
Epoch [74/120    avg_loss:0.045, val_acc:0.958]
Epoch [75/120    avg_loss:0.028, val_acc:0.977]
Epoch [76/120    avg_loss:0.021, val_acc:0.979]
Epoch [77/120    avg_loss:0.014, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.025, val_acc:0.979]
Epoch [80/120    avg_loss:0.022, val_acc:0.969]
Epoch [81/120    avg_loss:0.016, val_acc:0.982]
Epoch [82/120    avg_loss:0.033, val_acc:0.968]
Epoch [83/120    avg_loss:0.023, val_acc:0.973]
Epoch [84/120    avg_loss:0.047, val_acc:0.973]
Epoch [85/120    avg_loss:0.024, val_acc:0.975]
Epoch [86/120    avg_loss:0.059, val_acc:0.949]
Epoch [87/120    avg_loss:0.027, val_acc:0.979]
Epoch [88/120    avg_loss:0.024, val_acc:0.980]
Epoch [89/120    avg_loss:0.017, val_acc:0.979]
Epoch [90/120    avg_loss:0.020, val_acc:0.978]
Epoch [91/120    avg_loss:0.017, val_acc:0.976]
Epoch [92/120    avg_loss:0.033, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.978]
Epoch [94/120    avg_loss:0.015, val_acc:0.969]
Epoch [95/120    avg_loss:0.017, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     0     0     0     3    16    30    26]
 [    0     0 18052     0    28     0     4     0     6     0]
 [    0     6     0  1910     0     0     0     0   119     1]
 [    0    16    10     0  2917     0     6     0    20     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    41     0     0     0  4833     0     4     0]
 [    0    19     0     0     0     0     0  1269     1     1]
 [    0    54     2    50    32     0     0     6  3427     0]
 [    0     0     0     0     8    10     0     0     0   901]]

Accuracy:
98.74195647458608

F1 scores:
[       nan 0.98680534 0.99748584 0.95595596 0.97935202 0.99618321
 0.99403538 0.98333979 0.95486208 0.97352782]

Kappa:
0.98332653643368
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2cab41a908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.142, val_acc:0.155]
Epoch [2/120    avg_loss:1.760, val_acc:0.178]
Epoch [3/120    avg_loss:1.493, val_acc:0.562]
Epoch [4/120    avg_loss:1.316, val_acc:0.621]
Epoch [5/120    avg_loss:1.172, val_acc:0.665]
Epoch [6/120    avg_loss:1.030, val_acc:0.739]
Epoch [7/120    avg_loss:0.901, val_acc:0.696]
Epoch [8/120    avg_loss:0.813, val_acc:0.706]
Epoch [9/120    avg_loss:0.666, val_acc:0.699]
Epoch [10/120    avg_loss:0.578, val_acc:0.717]
Epoch [11/120    avg_loss:0.503, val_acc:0.773]
Epoch [12/120    avg_loss:0.481, val_acc:0.780]
Epoch [13/120    avg_loss:0.415, val_acc:0.811]
Epoch [14/120    avg_loss:0.364, val_acc:0.802]
Epoch [15/120    avg_loss:0.330, val_acc:0.853]
Epoch [16/120    avg_loss:0.285, val_acc:0.912]
Epoch [17/120    avg_loss:0.264, val_acc:0.887]
Epoch [18/120    avg_loss:0.246, val_acc:0.898]
Epoch [19/120    avg_loss:0.247, val_acc:0.928]
Epoch [20/120    avg_loss:0.216, val_acc:0.857]
Epoch [21/120    avg_loss:0.198, val_acc:0.927]
Epoch [22/120    avg_loss:0.175, val_acc:0.940]
Epoch [23/120    avg_loss:0.156, val_acc:0.862]
Epoch [24/120    avg_loss:0.158, val_acc:0.945]
Epoch [25/120    avg_loss:0.141, val_acc:0.938]
Epoch [26/120    avg_loss:0.125, val_acc:0.956]
Epoch [27/120    avg_loss:0.116, val_acc:0.966]
Epoch [28/120    avg_loss:0.108, val_acc:0.949]
Epoch [29/120    avg_loss:0.093, val_acc:0.968]
Epoch [30/120    avg_loss:0.075, val_acc:0.972]
Epoch [31/120    avg_loss:0.069, val_acc:0.964]
Epoch [32/120    avg_loss:0.052, val_acc:0.953]
Epoch [33/120    avg_loss:0.078, val_acc:0.963]
Epoch [34/120    avg_loss:0.084, val_acc:0.973]
Epoch [35/120    avg_loss:0.060, val_acc:0.967]
Epoch [36/120    avg_loss:0.052, val_acc:0.969]
Epoch [37/120    avg_loss:0.065, val_acc:0.973]
Epoch [38/120    avg_loss:0.053, val_acc:0.977]
Epoch [39/120    avg_loss:0.057, val_acc:0.962]
Epoch [40/120    avg_loss:0.068, val_acc:0.968]
Epoch [41/120    avg_loss:0.053, val_acc:0.968]
Epoch [42/120    avg_loss:0.054, val_acc:0.979]
Epoch [43/120    avg_loss:0.048, val_acc:0.937]
Epoch [44/120    avg_loss:0.068, val_acc:0.971]
Epoch [45/120    avg_loss:0.041, val_acc:0.980]
Epoch [46/120    avg_loss:0.028, val_acc:0.978]
Epoch [47/120    avg_loss:0.056, val_acc:0.977]
Epoch [48/120    avg_loss:0.034, val_acc:0.982]
Epoch [49/120    avg_loss:0.031, val_acc:0.983]
Epoch [50/120    avg_loss:0.031, val_acc:0.980]
Epoch [51/120    avg_loss:0.029, val_acc:0.976]
Epoch [52/120    avg_loss:0.034, val_acc:0.963]
Epoch [53/120    avg_loss:0.028, val_acc:0.978]
Epoch [54/120    avg_loss:0.059, val_acc:0.967]
Epoch [55/120    avg_loss:0.058, val_acc:0.975]
Epoch [56/120    avg_loss:0.047, val_acc:0.980]
Epoch [57/120    avg_loss:0.050, val_acc:0.976]
Epoch [58/120    avg_loss:0.031, val_acc:0.986]
Epoch [59/120    avg_loss:0.028, val_acc:0.978]
Epoch [60/120    avg_loss:0.027, val_acc:0.974]
Epoch [61/120    avg_loss:0.018, val_acc:0.984]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.011, val_acc:0.986]
Epoch [64/120    avg_loss:0.013, val_acc:0.985]
Epoch [65/120    avg_loss:0.013, val_acc:0.984]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.017, val_acc:0.974]
Epoch [68/120    avg_loss:0.012, val_acc:0.978]
Epoch [69/120    avg_loss:0.019, val_acc:0.978]
Epoch [70/120    avg_loss:0.023, val_acc:0.984]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.021, val_acc:0.973]
Epoch [73/120    avg_loss:0.018, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.014, val_acc:0.981]
Epoch [76/120    avg_loss:0.037, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.981]
Epoch [78/120    avg_loss:0.011, val_acc:0.982]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.009, val_acc:0.982]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.007, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.006, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     0     0     0     5    20    12]
 [    0     0 18035     0    48     0     6     0     1     0]
 [    0    10     0  1914     0     0     0     0   111     1]
 [    0    24    18     0  2910     0    14     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    29     0     0     0  4839     0    10     0]
 [    0     5     0     0     0     0     0  1284     1     0]
 [    0     9     0    28    43     0     6     0  3485     0]
 [    0     0     0     0     8    11     0     0     0   900]]

Accuracy:
98.97332080109898

F1 scores:
[       nan 0.99339806 0.99718014 0.96229261 0.97308142 0.99580313
 0.99332854 0.99573478 0.96792112 0.98039216]

Kappa:
0.9863958478532137
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6f25c99b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.078, val_acc:0.137]
Epoch [2/120    avg_loss:1.718, val_acc:0.224]
Epoch [3/120    avg_loss:1.499, val_acc:0.271]
Epoch [4/120    avg_loss:1.343, val_acc:0.345]
Epoch [5/120    avg_loss:1.168, val_acc:0.403]
Epoch [6/120    avg_loss:1.049, val_acc:0.476]
Epoch [7/120    avg_loss:0.908, val_acc:0.634]
Epoch [8/120    avg_loss:0.806, val_acc:0.586]
Epoch [9/120    avg_loss:0.669, val_acc:0.593]
Epoch [10/120    avg_loss:0.553, val_acc:0.616]
Epoch [11/120    avg_loss:0.473, val_acc:0.675]
Epoch [12/120    avg_loss:0.445, val_acc:0.688]
Epoch [13/120    avg_loss:0.437, val_acc:0.742]
Epoch [14/120    avg_loss:0.370, val_acc:0.815]
Epoch [15/120    avg_loss:0.390, val_acc:0.807]
Epoch [16/120    avg_loss:0.364, val_acc:0.811]
Epoch [17/120    avg_loss:0.317, val_acc:0.772]
Epoch [18/120    avg_loss:0.278, val_acc:0.809]
Epoch [19/120    avg_loss:0.275, val_acc:0.887]
Epoch [20/120    avg_loss:0.243, val_acc:0.864]
Epoch [21/120    avg_loss:0.209, val_acc:0.892]
Epoch [22/120    avg_loss:0.231, val_acc:0.910]
Epoch [23/120    avg_loss:0.193, val_acc:0.932]
Epoch [24/120    avg_loss:0.182, val_acc:0.928]
Epoch [25/120    avg_loss:0.197, val_acc:0.939]
Epoch [26/120    avg_loss:0.157, val_acc:0.931]
Epoch [27/120    avg_loss:0.147, val_acc:0.947]
Epoch [28/120    avg_loss:0.128, val_acc:0.943]
Epoch [29/120    avg_loss:0.108, val_acc:0.944]
Epoch [30/120    avg_loss:0.115, val_acc:0.932]
Epoch [31/120    avg_loss:0.116, val_acc:0.954]
Epoch [32/120    avg_loss:0.092, val_acc:0.946]
Epoch [33/120    avg_loss:0.087, val_acc:0.943]
Epoch [34/120    avg_loss:0.076, val_acc:0.963]
Epoch [35/120    avg_loss:0.092, val_acc:0.959]
Epoch [36/120    avg_loss:0.084, val_acc:0.954]
Epoch [37/120    avg_loss:0.105, val_acc:0.949]
Epoch [38/120    avg_loss:0.085, val_acc:0.906]
Epoch [39/120    avg_loss:0.074, val_acc:0.969]
Epoch [40/120    avg_loss:0.059, val_acc:0.965]
Epoch [41/120    avg_loss:0.056, val_acc:0.947]
Epoch [42/120    avg_loss:0.075, val_acc:0.956]
Epoch [43/120    avg_loss:0.062, val_acc:0.963]
Epoch [44/120    avg_loss:0.040, val_acc:0.965]
Epoch [45/120    avg_loss:0.059, val_acc:0.930]
Epoch [46/120    avg_loss:0.045, val_acc:0.962]
Epoch [47/120    avg_loss:0.036, val_acc:0.963]
Epoch [48/120    avg_loss:0.034, val_acc:0.962]
Epoch [49/120    avg_loss:0.037, val_acc:0.981]
Epoch [50/120    avg_loss:0.048, val_acc:0.963]
Epoch [51/120    avg_loss:0.035, val_acc:0.967]
Epoch [52/120    avg_loss:0.033, val_acc:0.978]
Epoch [53/120    avg_loss:0.026, val_acc:0.980]
Epoch [54/120    avg_loss:0.023, val_acc:0.960]
Epoch [55/120    avg_loss:0.030, val_acc:0.968]
Epoch [56/120    avg_loss:0.028, val_acc:0.972]
Epoch [57/120    avg_loss:0.041, val_acc:0.971]
Epoch [58/120    avg_loss:0.057, val_acc:0.962]
Epoch [59/120    avg_loss:0.042, val_acc:0.977]
Epoch [60/120    avg_loss:0.038, val_acc:0.975]
Epoch [61/120    avg_loss:0.029, val_acc:0.978]
Epoch [62/120    avg_loss:0.024, val_acc:0.973]
Epoch [63/120    avg_loss:0.019, val_acc:0.978]
Epoch [64/120    avg_loss:0.015, val_acc:0.977]
Epoch [65/120    avg_loss:0.018, val_acc:0.978]
Epoch [66/120    avg_loss:0.013, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.978]
Epoch [68/120    avg_loss:0.016, val_acc:0.979]
Epoch [69/120    avg_loss:0.019, val_acc:0.978]
Epoch [70/120    avg_loss:0.015, val_acc:0.978]
Epoch [71/120    avg_loss:0.012, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.980]
Epoch [74/120    avg_loss:0.013, val_acc:0.980]
Epoch [75/120    avg_loss:0.014, val_acc:0.980]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.980]
Epoch [78/120    avg_loss:0.015, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.979]
Epoch [80/120    avg_loss:0.013, val_acc:0.980]
Epoch [81/120    avg_loss:0.014, val_acc:0.980]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.015, val_acc:0.980]
Epoch [84/120    avg_loss:0.017, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.982]
Epoch [86/120    avg_loss:0.013, val_acc:0.982]
Epoch [87/120    avg_loss:0.015, val_acc:0.981]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.013, val_acc:0.982]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.981]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.016, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.015, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.014, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.015, val_acc:0.982]
Epoch [104/120    avg_loss:0.016, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.983]
Epoch [109/120    avg_loss:0.015, val_acc:0.982]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.012, val_acc:0.983]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.015, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.013, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6308     0     6     2     0     2     2    76    36]
 [    0     0 17999     0    17     0    66     0     8     0]
 [    0     3     0  1951     0     0     0     0    82     0]
 [    0     5     8     0  2939     1     8     2     5     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    50     0     0     0  4810     0    18     0]
 [    0     7     1     0     0     0     0  1282     0     0]
 [    0    10     0    87    36     0     6     1  3431     0]
 [    0     0     0     0     0     4     0     0     0   915]]

Accuracy:
98.66724507748295

F1 scores:
[       nan 0.98832746 0.99585039 0.95637255 0.98524975 0.99808795
 0.98464688 0.99495537 0.95424837 0.97652081]

Kappa:
0.9823518049889837
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f321258d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.043, val_acc:0.083]
Epoch [2/120    avg_loss:1.671, val_acc:0.133]
Epoch [3/120    avg_loss:1.462, val_acc:0.424]
Epoch [4/120    avg_loss:1.316, val_acc:0.515]
Epoch [5/120    avg_loss:1.162, val_acc:0.554]
Epoch [6/120    avg_loss:1.022, val_acc:0.583]
Epoch [7/120    avg_loss:0.888, val_acc:0.657]
Epoch [8/120    avg_loss:0.772, val_acc:0.664]
Epoch [9/120    avg_loss:0.664, val_acc:0.642]
Epoch [10/120    avg_loss:0.538, val_acc:0.649]
Epoch [11/120    avg_loss:0.469, val_acc:0.685]
Epoch [12/120    avg_loss:0.434, val_acc:0.753]
Epoch [13/120    avg_loss:0.378, val_acc:0.763]
Epoch [14/120    avg_loss:0.349, val_acc:0.792]
Epoch [15/120    avg_loss:0.322, val_acc:0.811]
Epoch [16/120    avg_loss:0.292, val_acc:0.792]
Epoch [17/120    avg_loss:0.279, val_acc:0.818]
Epoch [18/120    avg_loss:0.271, val_acc:0.799]
Epoch [19/120    avg_loss:0.240, val_acc:0.843]
Epoch [20/120    avg_loss:0.245, val_acc:0.867]
Epoch [21/120    avg_loss:0.224, val_acc:0.885]
Epoch [22/120    avg_loss:0.176, val_acc:0.907]
Epoch [23/120    avg_loss:0.176, val_acc:0.718]
Epoch [24/120    avg_loss:0.178, val_acc:0.886]
Epoch [25/120    avg_loss:0.200, val_acc:0.872]
Epoch [26/120    avg_loss:0.181, val_acc:0.936]
Epoch [27/120    avg_loss:0.161, val_acc:0.878]
Epoch [28/120    avg_loss:0.209, val_acc:0.929]
Epoch [29/120    avg_loss:0.138, val_acc:0.943]
Epoch [30/120    avg_loss:0.123, val_acc:0.949]
Epoch [31/120    avg_loss:0.105, val_acc:0.951]
Epoch [32/120    avg_loss:0.087, val_acc:0.956]
Epoch [33/120    avg_loss:0.090, val_acc:0.952]
Epoch [34/120    avg_loss:0.073, val_acc:0.929]
Epoch [35/120    avg_loss:0.074, val_acc:0.959]
Epoch [36/120    avg_loss:0.068, val_acc:0.957]
Epoch [37/120    avg_loss:0.088, val_acc:0.938]
Epoch [38/120    avg_loss:0.065, val_acc:0.965]
Epoch [39/120    avg_loss:0.049, val_acc:0.967]
Epoch [40/120    avg_loss:0.047, val_acc:0.965]
Epoch [41/120    avg_loss:0.049, val_acc:0.964]
Epoch [42/120    avg_loss:0.046, val_acc:0.971]
Epoch [43/120    avg_loss:0.043, val_acc:0.958]
Epoch [44/120    avg_loss:0.057, val_acc:0.959]
Epoch [45/120    avg_loss:0.044, val_acc:0.968]
Epoch [46/120    avg_loss:0.039, val_acc:0.960]
Epoch [47/120    avg_loss:0.042, val_acc:0.973]
Epoch [48/120    avg_loss:0.036, val_acc:0.956]
Epoch [49/120    avg_loss:0.040, val_acc:0.956]
Epoch [50/120    avg_loss:0.057, val_acc:0.893]
Epoch [51/120    avg_loss:0.064, val_acc:0.966]
Epoch [52/120    avg_loss:0.048, val_acc:0.899]
Epoch [53/120    avg_loss:0.055, val_acc:0.950]
Epoch [54/120    avg_loss:0.038, val_acc:0.977]
Epoch [55/120    avg_loss:0.029, val_acc:0.963]
Epoch [56/120    avg_loss:0.044, val_acc:0.951]
Epoch [57/120    avg_loss:0.038, val_acc:0.970]
Epoch [58/120    avg_loss:0.041, val_acc:0.968]
Epoch [59/120    avg_loss:0.035, val_acc:0.968]
Epoch [60/120    avg_loss:0.026, val_acc:0.973]
Epoch [61/120    avg_loss:0.020, val_acc:0.970]
Epoch [62/120    avg_loss:0.018, val_acc:0.973]
Epoch [63/120    avg_loss:0.017, val_acc:0.972]
Epoch [64/120    avg_loss:0.020, val_acc:0.967]
Epoch [65/120    avg_loss:0.021, val_acc:0.977]
Epoch [66/120    avg_loss:0.022, val_acc:0.953]
Epoch [67/120    avg_loss:0.020, val_acc:0.952]
Epoch [68/120    avg_loss:0.016, val_acc:0.978]
Epoch [69/120    avg_loss:0.018, val_acc:0.980]
Epoch [70/120    avg_loss:0.013, val_acc:0.978]
Epoch [71/120    avg_loss:0.016, val_acc:0.978]
Epoch [72/120    avg_loss:0.016, val_acc:0.973]
Epoch [73/120    avg_loss:0.010, val_acc:0.976]
Epoch [74/120    avg_loss:0.011, val_acc:0.973]
Epoch [75/120    avg_loss:0.018, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.983]
Epoch [77/120    avg_loss:0.019, val_acc:0.977]
Epoch [78/120    avg_loss:0.021, val_acc:0.973]
Epoch [79/120    avg_loss:0.014, val_acc:0.967]
Epoch [80/120    avg_loss:0.016, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.945]
Epoch [82/120    avg_loss:0.032, val_acc:0.973]
Epoch [83/120    avg_loss:0.013, val_acc:0.978]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.981]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.980]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.980]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.007, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.979]
Epoch [94/120    avg_loss:0.008, val_acc:0.980]
Epoch [95/120    avg_loss:0.005, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.005, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.981]
Epoch [99/120    avg_loss:0.007, val_acc:0.979]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.006, val_acc:0.978]
Epoch [103/120    avg_loss:0.006, val_acc:0.978]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.005, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.006, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.006, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.980]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.005, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6298     0     0     0     0     0     6    93    35]
 [    0     1 18051     0     9     0    29     0     0     0]
 [    0     1     0  1905     0     0     0     0   130     0]
 [    0    32     9     0  2910     0    16     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    33     0    10     1  4834     0     0     0]
 [    0     7     0     0     0     2     0  1281     0     0]
 [    0    50     0    67    42     0    10     0  3401     1]
 [    0     0     0     0     0    15     0     0     0   904]]

Accuracy:
98.54433277902297

F1 scores:
[       nan 0.98245067 0.99776138 0.9505988  0.97930338 0.99315068
 0.98986383 0.99417928 0.94511602 0.97099893]

Kappa:
0.980712477749066
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb722e49940>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.113, val_acc:0.181]
Epoch [2/120    avg_loss:1.782, val_acc:0.175]
Epoch [3/120    avg_loss:1.534, val_acc:0.284]
Epoch [4/120    avg_loss:1.338, val_acc:0.323]
Epoch [5/120    avg_loss:1.210, val_acc:0.407]
Epoch [6/120    avg_loss:1.085, val_acc:0.492]
Epoch [7/120    avg_loss:0.969, val_acc:0.539]
Epoch [8/120    avg_loss:0.859, val_acc:0.618]
Epoch [9/120    avg_loss:0.761, val_acc:0.653]
Epoch [10/120    avg_loss:0.694, val_acc:0.667]
Epoch [11/120    avg_loss:0.600, val_acc:0.649]
Epoch [12/120    avg_loss:0.507, val_acc:0.760]
Epoch [13/120    avg_loss:0.453, val_acc:0.754]
Epoch [14/120    avg_loss:0.415, val_acc:0.809]
Epoch [15/120    avg_loss:0.353, val_acc:0.782]
Epoch [16/120    avg_loss:0.340, val_acc:0.828]
Epoch [17/120    avg_loss:0.295, val_acc:0.837]
Epoch [18/120    avg_loss:0.272, val_acc:0.817]
Epoch [19/120    avg_loss:0.288, val_acc:0.827]
Epoch [20/120    avg_loss:0.242, val_acc:0.907]
Epoch [21/120    avg_loss:0.179, val_acc:0.944]
Epoch [22/120    avg_loss:0.180, val_acc:0.808]
Epoch [23/120    avg_loss:0.207, val_acc:0.905]
Epoch [24/120    avg_loss:0.158, val_acc:0.946]
Epoch [25/120    avg_loss:0.157, val_acc:0.927]
Epoch [26/120    avg_loss:0.123, val_acc:0.942]
Epoch [27/120    avg_loss:0.120, val_acc:0.967]
Epoch [28/120    avg_loss:0.115, val_acc:0.953]
Epoch [29/120    avg_loss:0.100, val_acc:0.957]
Epoch [30/120    avg_loss:0.122, val_acc:0.952]
Epoch [31/120    avg_loss:0.094, val_acc:0.943]
Epoch [32/120    avg_loss:0.101, val_acc:0.939]
Epoch [33/120    avg_loss:0.116, val_acc:0.903]
Epoch [34/120    avg_loss:0.081, val_acc:0.958]
Epoch [35/120    avg_loss:0.100, val_acc:0.963]
Epoch [36/120    avg_loss:0.085, val_acc:0.967]
Epoch [37/120    avg_loss:0.078, val_acc:0.958]
Epoch [38/120    avg_loss:0.089, val_acc:0.962]
Epoch [39/120    avg_loss:0.078, val_acc:0.969]
Epoch [40/120    avg_loss:0.049, val_acc:0.975]
Epoch [41/120    avg_loss:0.058, val_acc:0.967]
Epoch [42/120    avg_loss:0.053, val_acc:0.964]
Epoch [43/120    avg_loss:0.060, val_acc:0.963]
Epoch [44/120    avg_loss:0.067, val_acc:0.968]
Epoch [45/120    avg_loss:0.054, val_acc:0.928]
Epoch [46/120    avg_loss:0.053, val_acc:0.973]
Epoch [47/120    avg_loss:0.043, val_acc:0.973]
Epoch [48/120    avg_loss:0.038, val_acc:0.974]
Epoch [49/120    avg_loss:0.035, val_acc:0.976]
Epoch [50/120    avg_loss:0.030, val_acc:0.978]
Epoch [51/120    avg_loss:0.032, val_acc:0.971]
Epoch [52/120    avg_loss:0.031, val_acc:0.975]
Epoch [53/120    avg_loss:0.035, val_acc:0.970]
Epoch [54/120    avg_loss:0.041, val_acc:0.981]
Epoch [55/120    avg_loss:0.038, val_acc:0.968]
Epoch [56/120    avg_loss:0.057, val_acc:0.968]
Epoch [57/120    avg_loss:0.031, val_acc:0.971]
Epoch [58/120    avg_loss:0.030, val_acc:0.973]
Epoch [59/120    avg_loss:0.029, val_acc:0.979]
Epoch [60/120    avg_loss:0.026, val_acc:0.979]
Epoch [61/120    avg_loss:0.017, val_acc:0.975]
Epoch [62/120    avg_loss:0.017, val_acc:0.980]
Epoch [63/120    avg_loss:0.019, val_acc:0.981]
Epoch [64/120    avg_loss:0.032, val_acc:0.977]
Epoch [65/120    avg_loss:0.026, val_acc:0.979]
Epoch [66/120    avg_loss:0.025, val_acc:0.980]
Epoch [67/120    avg_loss:0.018, val_acc:0.982]
Epoch [68/120    avg_loss:0.023, val_acc:0.975]
Epoch [69/120    avg_loss:0.019, val_acc:0.982]
Epoch [70/120    avg_loss:0.013, val_acc:0.981]
Epoch [71/120    avg_loss:0.030, val_acc:0.979]
Epoch [72/120    avg_loss:0.031, val_acc:0.978]
Epoch [73/120    avg_loss:0.022, val_acc:0.979]
Epoch [74/120    avg_loss:0.015, val_acc:0.980]
Epoch [75/120    avg_loss:0.023, val_acc:0.978]
Epoch [76/120    avg_loss:0.026, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.979]
Epoch [78/120    avg_loss:0.014, val_acc:0.976]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.012, val_acc:0.976]
Epoch [82/120    avg_loss:0.009, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.980]
Epoch [84/120    avg_loss:0.010, val_acc:0.980]
Epoch [85/120    avg_loss:0.035, val_acc:0.968]
Epoch [86/120    avg_loss:0.027, val_acc:0.977]
Epoch [87/120    avg_loss:0.027, val_acc:0.971]
Epoch [88/120    avg_loss:0.018, val_acc:0.976]
Epoch [89/120    avg_loss:0.021, val_acc:0.982]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.961]
Epoch [97/120    avg_loss:0.024, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.039, val_acc:0.968]
Epoch [104/120    avg_loss:0.027, val_acc:0.973]
Epoch [105/120    avg_loss:0.020, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.978]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.979]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.013, val_acc:0.976]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6337     0    10     0     0     1    11    68     5]
 [    0     0 18063     0    18     0     8     0     1     0]
 [    0     0     0  1908     0     0     0     0   127     1]
 [    0    16    20     0  2894     0    36     2     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     7     0  4864     0     0     0]
 [    0    23     0     0     0     1     0  1265     1     0]
 [    0    25     0    62    42     0     0     5  3437     0]
 [    0     1     0     0     3    14     0     0     0   901]]

Accuracy:
98.7491866097896

F1 scores:
[       nan 0.98753312 0.99850746 0.9501992  0.97506739 0.99428571
 0.99397159 0.98328799 0.95392728 0.98523783]

Kappa:
0.983425312832316
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4be8108d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.032, val_acc:0.149]
Epoch [2/120    avg_loss:1.675, val_acc:0.173]
Epoch [3/120    avg_loss:1.475, val_acc:0.348]
Epoch [4/120    avg_loss:1.301, val_acc:0.397]
Epoch [5/120    avg_loss:1.159, val_acc:0.428]
Epoch [6/120    avg_loss:1.040, val_acc:0.422]
Epoch [7/120    avg_loss:0.903, val_acc:0.473]
Epoch [8/120    avg_loss:0.823, val_acc:0.497]
Epoch [9/120    avg_loss:0.767, val_acc:0.558]
Epoch [10/120    avg_loss:0.673, val_acc:0.667]
Epoch [11/120    avg_loss:0.628, val_acc:0.694]
Epoch [12/120    avg_loss:0.556, val_acc:0.728]
Epoch [13/120    avg_loss:0.503, val_acc:0.777]
Epoch [14/120    avg_loss:0.430, val_acc:0.821]
Epoch [15/120    avg_loss:0.404, val_acc:0.852]
Epoch [16/120    avg_loss:0.334, val_acc:0.820]
Epoch [17/120    avg_loss:0.308, val_acc:0.852]
Epoch [18/120    avg_loss:0.301, val_acc:0.911]
Epoch [19/120    avg_loss:0.287, val_acc:0.899]
Epoch [20/120    avg_loss:0.268, val_acc:0.887]
Epoch [21/120    avg_loss:0.284, val_acc:0.875]
Epoch [22/120    avg_loss:0.223, val_acc:0.936]
Epoch [23/120    avg_loss:0.180, val_acc:0.892]
Epoch [24/120    avg_loss:0.170, val_acc:0.944]
Epoch [25/120    avg_loss:0.143, val_acc:0.942]
Epoch [26/120    avg_loss:0.133, val_acc:0.933]
Epoch [27/120    avg_loss:0.141, val_acc:0.902]
Epoch [28/120    avg_loss:0.145, val_acc:0.932]
Epoch [29/120    avg_loss:0.128, val_acc:0.947]
Epoch [30/120    avg_loss:0.115, val_acc:0.952]
Epoch [31/120    avg_loss:0.121, val_acc:0.951]
Epoch [32/120    avg_loss:0.089, val_acc:0.959]
Epoch [33/120    avg_loss:0.082, val_acc:0.957]
Epoch [34/120    avg_loss:0.082, val_acc:0.953]
Epoch [35/120    avg_loss:0.098, val_acc:0.941]
Epoch [36/120    avg_loss:0.095, val_acc:0.942]
Epoch [37/120    avg_loss:0.122, val_acc:0.922]
Epoch [38/120    avg_loss:0.091, val_acc:0.950]
Epoch [39/120    avg_loss:0.088, val_acc:0.951]
Epoch [40/120    avg_loss:0.075, val_acc:0.964]
Epoch [41/120    avg_loss:0.081, val_acc:0.958]
Epoch [42/120    avg_loss:0.059, val_acc:0.963]
Epoch [43/120    avg_loss:0.055, val_acc:0.963]
Epoch [44/120    avg_loss:0.052, val_acc:0.964]
Epoch [45/120    avg_loss:0.058, val_acc:0.956]
Epoch [46/120    avg_loss:0.063, val_acc:0.960]
Epoch [47/120    avg_loss:0.090, val_acc:0.868]
Epoch [48/120    avg_loss:0.088, val_acc:0.935]
Epoch [49/120    avg_loss:0.062, val_acc:0.948]
Epoch [50/120    avg_loss:0.061, val_acc:0.949]
Epoch [51/120    avg_loss:0.048, val_acc:0.968]
Epoch [52/120    avg_loss:0.052, val_acc:0.955]
Epoch [53/120    avg_loss:0.056, val_acc:0.968]
Epoch [54/120    avg_loss:0.029, val_acc:0.975]
Epoch [55/120    avg_loss:0.036, val_acc:0.972]
Epoch [56/120    avg_loss:0.034, val_acc:0.966]
Epoch [57/120    avg_loss:0.036, val_acc:0.971]
Epoch [58/120    avg_loss:0.031, val_acc:0.972]
Epoch [59/120    avg_loss:0.029, val_acc:0.975]
Epoch [60/120    avg_loss:0.061, val_acc:0.961]
Epoch [61/120    avg_loss:0.041, val_acc:0.959]
Epoch [62/120    avg_loss:0.045, val_acc:0.961]
Epoch [63/120    avg_loss:0.054, val_acc:0.960]
Epoch [64/120    avg_loss:0.032, val_acc:0.973]
Epoch [65/120    avg_loss:0.022, val_acc:0.978]
Epoch [66/120    avg_loss:0.027, val_acc:0.978]
Epoch [67/120    avg_loss:0.023, val_acc:0.973]
Epoch [68/120    avg_loss:0.023, val_acc:0.979]
Epoch [69/120    avg_loss:0.033, val_acc:0.972]
Epoch [70/120    avg_loss:0.028, val_acc:0.974]
Epoch [71/120    avg_loss:0.023, val_acc:0.975]
Epoch [72/120    avg_loss:0.021, val_acc:0.976]
Epoch [73/120    avg_loss:0.015, val_acc:0.974]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.978]
Epoch [77/120    avg_loss:0.011, val_acc:0.976]
Epoch [78/120    avg_loss:0.017, val_acc:0.964]
Epoch [79/120    avg_loss:0.015, val_acc:0.981]
Epoch [80/120    avg_loss:0.017, val_acc:0.980]
Epoch [81/120    avg_loss:0.029, val_acc:0.875]
Epoch [82/120    avg_loss:0.036, val_acc:0.973]
Epoch [83/120    avg_loss:0.036, val_acc:0.972]
Epoch [84/120    avg_loss:0.017, val_acc:0.978]
Epoch [85/120    avg_loss:0.019, val_acc:0.973]
Epoch [86/120    avg_loss:0.014, val_acc:0.973]
Epoch [87/120    avg_loss:0.012, val_acc:0.977]
Epoch [88/120    avg_loss:0.017, val_acc:0.978]
Epoch [89/120    avg_loss:0.026, val_acc:0.967]
Epoch [90/120    avg_loss:0.026, val_acc:0.978]
Epoch [91/120    avg_loss:0.019, val_acc:0.979]
Epoch [92/120    avg_loss:0.016, val_acc:0.981]
Epoch [93/120    avg_loss:0.010, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.013, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.978]
Epoch [97/120    avg_loss:0.015, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.980]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.052, val_acc:0.958]
Epoch [107/120    avg_loss:0.043, val_acc:0.964]
Epoch [108/120    avg_loss:0.025, val_acc:0.965]
Epoch [109/120    avg_loss:0.021, val_acc:0.971]
Epoch [110/120    avg_loss:0.017, val_acc:0.970]
Epoch [111/120    avg_loss:0.016, val_acc:0.970]
Epoch [112/120    avg_loss:0.015, val_acc:0.972]
Epoch [113/120    avg_loss:0.020, val_acc:0.972]
Epoch [114/120    avg_loss:0.015, val_acc:0.972]
Epoch [115/120    avg_loss:0.013, val_acc:0.972]
Epoch [116/120    avg_loss:0.016, val_acc:0.973]
Epoch [117/120    avg_loss:0.014, val_acc:0.972]
Epoch [118/120    avg_loss:0.011, val_acc:0.973]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.015, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6344     0     7     2     0     1    17    45    16]
 [    0     0 18011     0    58     0    16     0     5     0]
 [    0     3     0  1859     0     0     0     0   171     3]
 [    0    21    23     2  2902     0    16     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     2    29     0     0     0  4825     0    22     0]
 [    0    22     0     0     0     0     0  1265     2     1]
 [    0    43     0    36    12     0    52    10  3418     0]
 [    0     0     0     0     3     5     0     0     0   911]]

Accuracy:
98.42624057069868

F1 scores:
[       nan 0.98608844 0.99637651 0.94365482 0.97562616 0.99808795
 0.9859011  0.97986057 0.94432933 0.98327037]

Kappa:
0.9791499086840945
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1dd183710>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.023, val_acc:0.128]
Epoch [2/120    avg_loss:1.725, val_acc:0.140]
Epoch [3/120    avg_loss:1.513, val_acc:0.252]
Epoch [4/120    avg_loss:1.344, val_acc:0.362]
Epoch [5/120    avg_loss:1.201, val_acc:0.417]
Epoch [6/120    avg_loss:1.072, val_acc:0.449]
Epoch [7/120    avg_loss:0.966, val_acc:0.477]
Epoch [8/120    avg_loss:0.823, val_acc:0.476]
Epoch [9/120    avg_loss:0.718, val_acc:0.642]
Epoch [10/120    avg_loss:0.588, val_acc:0.670]
Epoch [11/120    avg_loss:0.519, val_acc:0.725]
Epoch [12/120    avg_loss:0.474, val_acc:0.762]
Epoch [13/120    avg_loss:0.425, val_acc:0.816]
Epoch [14/120    avg_loss:0.368, val_acc:0.797]
Epoch [15/120    avg_loss:0.366, val_acc:0.789]
Epoch [16/120    avg_loss:0.347, val_acc:0.807]
Epoch [17/120    avg_loss:0.330, val_acc:0.907]
Epoch [18/120    avg_loss:0.318, val_acc:0.847]
Epoch [19/120    avg_loss:0.292, val_acc:0.891]
Epoch [20/120    avg_loss:0.246, val_acc:0.938]
Epoch [21/120    avg_loss:0.235, val_acc:0.868]
Epoch [22/120    avg_loss:0.200, val_acc:0.921]
Epoch [23/120    avg_loss:0.211, val_acc:0.892]
Epoch [24/120    avg_loss:0.198, val_acc:0.932]
Epoch [25/120    avg_loss:0.158, val_acc:0.946]
Epoch [26/120    avg_loss:0.156, val_acc:0.927]
Epoch [27/120    avg_loss:0.141, val_acc:0.922]
Epoch [28/120    avg_loss:0.129, val_acc:0.954]
Epoch [29/120    avg_loss:0.130, val_acc:0.938]
Epoch [30/120    avg_loss:0.147, val_acc:0.953]
Epoch [31/120    avg_loss:0.111, val_acc:0.969]
Epoch [32/120    avg_loss:0.156, val_acc:0.947]
Epoch [33/120    avg_loss:0.124, val_acc:0.954]
Epoch [34/120    avg_loss:0.109, val_acc:0.957]
Epoch [35/120    avg_loss:0.080, val_acc:0.975]
Epoch [36/120    avg_loss:0.067, val_acc:0.981]
Epoch [37/120    avg_loss:0.059, val_acc:0.966]
Epoch [38/120    avg_loss:0.088, val_acc:0.972]
Epoch [39/120    avg_loss:0.064, val_acc:0.981]
Epoch [40/120    avg_loss:0.073, val_acc:0.978]
Epoch [41/120    avg_loss:0.061, val_acc:0.965]
Epoch [42/120    avg_loss:0.069, val_acc:0.938]
Epoch [43/120    avg_loss:0.056, val_acc:0.977]
Epoch [44/120    avg_loss:0.057, val_acc:0.963]
Epoch [45/120    avg_loss:0.084, val_acc:0.947]
Epoch [46/120    avg_loss:0.088, val_acc:0.963]
Epoch [47/120    avg_loss:0.049, val_acc:0.968]
Epoch [48/120    avg_loss:0.096, val_acc:0.958]
Epoch [49/120    avg_loss:0.101, val_acc:0.963]
Epoch [50/120    avg_loss:0.058, val_acc:0.983]
Epoch [51/120    avg_loss:0.031, val_acc:0.983]
Epoch [52/120    avg_loss:0.028, val_acc:0.980]
Epoch [53/120    avg_loss:0.027, val_acc:0.985]
Epoch [54/120    avg_loss:0.031, val_acc:0.984]
Epoch [55/120    avg_loss:0.023, val_acc:0.992]
Epoch [56/120    avg_loss:0.017, val_acc:0.990]
Epoch [57/120    avg_loss:0.022, val_acc:0.984]
Epoch [58/120    avg_loss:0.033, val_acc:0.986]
Epoch [59/120    avg_loss:0.021, val_acc:0.966]
Epoch [60/120    avg_loss:0.030, val_acc:0.988]
Epoch [61/120    avg_loss:0.017, val_acc:0.990]
Epoch [62/120    avg_loss:0.018, val_acc:0.988]
Epoch [63/120    avg_loss:0.038, val_acc:0.978]
Epoch [64/120    avg_loss:0.039, val_acc:0.963]
Epoch [65/120    avg_loss:0.192, val_acc:0.938]
Epoch [66/120    avg_loss:0.093, val_acc:0.961]
Epoch [67/120    avg_loss:0.088, val_acc:0.966]
Epoch [68/120    avg_loss:0.050, val_acc:0.979]
Epoch [69/120    avg_loss:0.031, val_acc:0.982]
Epoch [70/120    avg_loss:0.028, val_acc:0.983]
Epoch [71/120    avg_loss:0.022, val_acc:0.984]
Epoch [72/120    avg_loss:0.022, val_acc:0.985]
Epoch [73/120    avg_loss:0.023, val_acc:0.985]
Epoch [74/120    avg_loss:0.021, val_acc:0.986]
Epoch [75/120    avg_loss:0.021, val_acc:0.986]
Epoch [76/120    avg_loss:0.022, val_acc:0.983]
Epoch [77/120    avg_loss:0.023, val_acc:0.985]
Epoch [78/120    avg_loss:0.020, val_acc:0.987]
Epoch [79/120    avg_loss:0.019, val_acc:0.987]
Epoch [80/120    avg_loss:0.021, val_acc:0.988]
Epoch [81/120    avg_loss:0.020, val_acc:0.984]
Epoch [82/120    avg_loss:0.025, val_acc:0.984]
Epoch [83/120    avg_loss:0.023, val_acc:0.984]
Epoch [84/120    avg_loss:0.019, val_acc:0.985]
Epoch [85/120    avg_loss:0.022, val_acc:0.986]
Epoch [86/120    avg_loss:0.016, val_acc:0.986]
Epoch [87/120    avg_loss:0.021, val_acc:0.986]
Epoch [88/120    avg_loss:0.018, val_acc:0.987]
Epoch [89/120    avg_loss:0.019, val_acc:0.987]
Epoch [90/120    avg_loss:0.020, val_acc:0.987]
Epoch [91/120    avg_loss:0.019, val_acc:0.987]
Epoch [92/120    avg_loss:0.018, val_acc:0.987]
Epoch [93/120    avg_loss:0.017, val_acc:0.986]
Epoch [94/120    avg_loss:0.018, val_acc:0.986]
Epoch [95/120    avg_loss:0.019, val_acc:0.986]
Epoch [96/120    avg_loss:0.019, val_acc:0.986]
Epoch [97/120    avg_loss:0.019, val_acc:0.986]
Epoch [98/120    avg_loss:0.018, val_acc:0.986]
Epoch [99/120    avg_loss:0.018, val_acc:0.986]
Epoch [100/120    avg_loss:0.016, val_acc:0.986]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.020, val_acc:0.986]
Epoch [103/120    avg_loss:0.019, val_acc:0.986]
Epoch [104/120    avg_loss:0.021, val_acc:0.986]
Epoch [105/120    avg_loss:0.021, val_acc:0.987]
Epoch [106/120    avg_loss:0.019, val_acc:0.987]
Epoch [107/120    avg_loss:0.018, val_acc:0.987]
Epoch [108/120    avg_loss:0.022, val_acc:0.987]
Epoch [109/120    avg_loss:0.020, val_acc:0.987]
Epoch [110/120    avg_loss:0.016, val_acc:0.987]
Epoch [111/120    avg_loss:0.017, val_acc:0.987]
Epoch [112/120    avg_loss:0.016, val_acc:0.987]
Epoch [113/120    avg_loss:0.025, val_acc:0.987]
Epoch [114/120    avg_loss:0.019, val_acc:0.987]
Epoch [115/120    avg_loss:0.017, val_acc:0.987]
Epoch [116/120    avg_loss:0.023, val_acc:0.987]
Epoch [117/120    avg_loss:0.015, val_acc:0.987]
Epoch [118/120    avg_loss:0.018, val_acc:0.987]
Epoch [119/120    avg_loss:0.020, val_acc:0.987]
Epoch [120/120    avg_loss:0.014, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6305     0     0     5     0     6     3    46    67]
 [    0     0 18028     0    56     0     2     0     4     0]
 [    0     3     0  1913     0     0     0     0   119     1]
 [    0    29     6     0  2912     0    17     0     6     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    24     0     0     0  4840     0    13     0]
 [    0    21     0     0     0     0     0  1263     5     1]
 [    0    58     2    38    19     0    40     0  3414     0]
 [    0     0     0     0    22     9     0     0     0   888]]

Accuracy:
98.49372183259827

F1 scores:
[       nan 0.98139933 0.99739972 0.95961876 0.97293685 0.99656357
 0.98947153 0.98826291 0.9512399  0.9456869 ]

Kappa:
0.980047194554941
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0becb2c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 15399==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.142, val_acc:0.141]
Epoch [2/120    avg_loss:1.730, val_acc:0.214]
Epoch [3/120    avg_loss:1.490, val_acc:0.262]
Epoch [4/120    avg_loss:1.332, val_acc:0.376]
Epoch [5/120    avg_loss:1.148, val_acc:0.422]
Epoch [6/120    avg_loss:1.007, val_acc:0.448]
Epoch [7/120    avg_loss:0.889, val_acc:0.486]
Epoch [8/120    avg_loss:0.763, val_acc:0.553]
Epoch [9/120    avg_loss:0.686, val_acc:0.596]
Epoch [10/120    avg_loss:0.606, val_acc:0.657]
Epoch [11/120    avg_loss:0.522, val_acc:0.694]
Epoch [12/120    avg_loss:0.493, val_acc:0.696]
Epoch [13/120    avg_loss:0.468, val_acc:0.681]
Epoch [14/120    avg_loss:0.413, val_acc:0.769]
Epoch [15/120    avg_loss:0.367, val_acc:0.754]
Epoch [16/120    avg_loss:0.368, val_acc:0.752]
Epoch [17/120    avg_loss:0.352, val_acc:0.782]
Epoch [18/120    avg_loss:0.322, val_acc:0.785]
Epoch [19/120    avg_loss:0.280, val_acc:0.795]
Epoch [20/120    avg_loss:0.308, val_acc:0.799]
Epoch [21/120    avg_loss:0.246, val_acc:0.806]
Epoch [22/120    avg_loss:0.236, val_acc:0.822]
Epoch [23/120    avg_loss:0.240, val_acc:0.804]
Epoch [24/120    avg_loss:0.293, val_acc:0.833]
Epoch [25/120    avg_loss:0.273, val_acc:0.833]
Epoch [26/120    avg_loss:0.238, val_acc:0.802]
Epoch [27/120    avg_loss:0.237, val_acc:0.838]
Epoch [28/120    avg_loss:0.202, val_acc:0.868]
Epoch [29/120    avg_loss:0.191, val_acc:0.877]
Epoch [30/120    avg_loss:0.190, val_acc:0.843]
Epoch [31/120    avg_loss:0.176, val_acc:0.909]
Epoch [32/120    avg_loss:0.188, val_acc:0.891]
Epoch [33/120    avg_loss:0.154, val_acc:0.922]
Epoch [34/120    avg_loss:0.115, val_acc:0.943]
Epoch [35/120    avg_loss:0.129, val_acc:0.907]
Epoch [36/120    avg_loss:0.153, val_acc:0.916]
Epoch [37/120    avg_loss:0.124, val_acc:0.954]
Epoch [38/120    avg_loss:0.120, val_acc:0.958]
Epoch [39/120    avg_loss:0.107, val_acc:0.954]
Epoch [40/120    avg_loss:0.066, val_acc:0.953]
Epoch [41/120    avg_loss:0.072, val_acc:0.950]
Epoch [42/120    avg_loss:0.079, val_acc:0.958]
Epoch [43/120    avg_loss:0.093, val_acc:0.957]
Epoch [44/120    avg_loss:0.057, val_acc:0.966]
Epoch [45/120    avg_loss:0.073, val_acc:0.928]
Epoch [46/120    avg_loss:0.079, val_acc:0.958]
Epoch [47/120    avg_loss:0.051, val_acc:0.969]
Epoch [48/120    avg_loss:0.056, val_acc:0.967]
Epoch [49/120    avg_loss:0.045, val_acc:0.955]
Epoch [50/120    avg_loss:0.045, val_acc:0.974]
Epoch [51/120    avg_loss:0.048, val_acc:0.957]
Epoch [52/120    avg_loss:0.044, val_acc:0.972]
Epoch [53/120    avg_loss:0.040, val_acc:0.968]
Epoch [54/120    avg_loss:0.046, val_acc:0.974]
Epoch [55/120    avg_loss:0.050, val_acc:0.963]
Epoch [56/120    avg_loss:0.080, val_acc:0.951]
Epoch [57/120    avg_loss:0.070, val_acc:0.970]
Epoch [58/120    avg_loss:0.038, val_acc:0.977]
Epoch [59/120    avg_loss:0.040, val_acc:0.969]
Epoch [60/120    avg_loss:0.045, val_acc:0.965]
Epoch [61/120    avg_loss:0.039, val_acc:0.948]
Epoch [62/120    avg_loss:0.047, val_acc:0.954]
Epoch [63/120    avg_loss:0.054, val_acc:0.960]
Epoch [64/120    avg_loss:0.070, val_acc:0.930]
Epoch [65/120    avg_loss:0.060, val_acc:0.971]
Epoch [66/120    avg_loss:0.031, val_acc:0.971]
Epoch [67/120    avg_loss:0.046, val_acc:0.979]
Epoch [68/120    avg_loss:0.024, val_acc:0.974]
Epoch [69/120    avg_loss:0.029, val_acc:0.958]
Epoch [70/120    avg_loss:0.041, val_acc:0.956]
Epoch [71/120    avg_loss:0.037, val_acc:0.971]
Epoch [72/120    avg_loss:0.033, val_acc:0.965]
Epoch [73/120    avg_loss:0.055, val_acc:0.973]
Epoch [74/120    avg_loss:0.041, val_acc:0.969]
Epoch [75/120    avg_loss:0.059, val_acc:0.963]
Epoch [76/120    avg_loss:0.049, val_acc:0.968]
Epoch [77/120    avg_loss:0.027, val_acc:0.973]
Epoch [78/120    avg_loss:0.036, val_acc:0.972]
Epoch [79/120    avg_loss:0.024, val_acc:0.960]
Epoch [80/120    avg_loss:0.025, val_acc:0.973]
Epoch [81/120    avg_loss:0.017, val_acc:0.977]
Epoch [82/120    avg_loss:0.014, val_acc:0.978]
Epoch [83/120    avg_loss:0.018, val_acc:0.979]
Epoch [84/120    avg_loss:0.013, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.977]
Epoch [86/120    avg_loss:0.010, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.978]
Epoch [90/120    avg_loss:0.014, val_acc:0.976]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.979]
Epoch [93/120    avg_loss:0.015, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.977]
Epoch [96/120    avg_loss:0.012, val_acc:0.978]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.011, val_acc:0.977]
Epoch [99/120    avg_loss:0.013, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.978]
Epoch [103/120    avg_loss:0.013, val_acc:0.978]
Epoch [104/120    avg_loss:0.013, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.009, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.978]
Epoch [108/120    avg_loss:0.013, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.009, val_acc:0.978]
Epoch [113/120    avg_loss:0.011, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     2     0     0     0     0    29     9]
 [    0     0 18055     0    22     0    10     0     3     0]
 [    0     1     0  1953     0     0     0     0    79     3]
 [    0    25    16     0  2920     0     8     0     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     0     0     0  4857     0     0     0]
 [    0    31     0     0     0     0     0  1257     0     2]
 [    0    24     4    58    51     0     4     0  3427     3]
 [    0     0     0     0    14    15     0     0     0   890]]

Accuracy:
98.94681030535271

F1 scores:
[       nan 0.99062379 0.99789974 0.96468264 0.97675197 0.99428571
 0.99559291 0.98704358 0.96385881 0.97427477]

Kappa:
0.9860414793526726
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5eecc0978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.163, val_acc:0.115]
Epoch [2/120    avg_loss:1.831, val_acc:0.206]
Epoch [3/120    avg_loss:1.600, val_acc:0.408]
Epoch [4/120    avg_loss:1.369, val_acc:0.505]
Epoch [5/120    avg_loss:1.192, val_acc:0.578]
Epoch [6/120    avg_loss:1.059, val_acc:0.659]
Epoch [7/120    avg_loss:0.918, val_acc:0.693]
Epoch [8/120    avg_loss:0.830, val_acc:0.715]
Epoch [9/120    avg_loss:0.732, val_acc:0.708]
Epoch [10/120    avg_loss:0.659, val_acc:0.719]
Epoch [11/120    avg_loss:0.613, val_acc:0.721]
Epoch [12/120    avg_loss:0.545, val_acc:0.773]
Epoch [13/120    avg_loss:0.486, val_acc:0.763]
Epoch [14/120    avg_loss:0.465, val_acc:0.752]
Epoch [15/120    avg_loss:0.451, val_acc:0.799]
Epoch [16/120    avg_loss:0.410, val_acc:0.787]
Epoch [17/120    avg_loss:0.382, val_acc:0.821]
Epoch [18/120    avg_loss:0.306, val_acc:0.847]
Epoch [19/120    avg_loss:0.320, val_acc:0.881]
Epoch [20/120    avg_loss:0.296, val_acc:0.840]
Epoch [21/120    avg_loss:0.266, val_acc:0.880]
Epoch [22/120    avg_loss:0.224, val_acc:0.928]
Epoch [23/120    avg_loss:0.200, val_acc:0.924]
Epoch [24/120    avg_loss:0.286, val_acc:0.869]
Epoch [25/120    avg_loss:0.243, val_acc:0.932]
Epoch [26/120    avg_loss:0.184, val_acc:0.893]
Epoch [27/120    avg_loss:0.160, val_acc:0.932]
Epoch [28/120    avg_loss:0.157, val_acc:0.922]
Epoch [29/120    avg_loss:0.155, val_acc:0.958]
Epoch [30/120    avg_loss:0.150, val_acc:0.932]
Epoch [31/120    avg_loss:0.149, val_acc:0.926]
Epoch [32/120    avg_loss:0.126, val_acc:0.963]
Epoch [33/120    avg_loss:0.100, val_acc:0.953]
Epoch [34/120    avg_loss:0.097, val_acc:0.958]
Epoch [35/120    avg_loss:0.095, val_acc:0.963]
Epoch [36/120    avg_loss:0.087, val_acc:0.963]
Epoch [37/120    avg_loss:0.064, val_acc:0.944]
Epoch [38/120    avg_loss:0.082, val_acc:0.886]
Epoch [39/120    avg_loss:0.058, val_acc:0.966]
Epoch [40/120    avg_loss:0.085, val_acc:0.962]
Epoch [41/120    avg_loss:0.089, val_acc:0.939]
Epoch [42/120    avg_loss:0.082, val_acc:0.963]
Epoch [43/120    avg_loss:0.072, val_acc:0.956]
Epoch [44/120    avg_loss:0.059, val_acc:0.972]
Epoch [45/120    avg_loss:0.071, val_acc:0.972]
Epoch [46/120    avg_loss:0.039, val_acc:0.969]
Epoch [47/120    avg_loss:0.038, val_acc:0.977]
Epoch [48/120    avg_loss:0.042, val_acc:0.968]
Epoch [49/120    avg_loss:0.038, val_acc:0.972]
Epoch [50/120    avg_loss:0.039, val_acc:0.974]
Epoch [51/120    avg_loss:0.040, val_acc:0.978]
Epoch [52/120    avg_loss:0.033, val_acc:0.945]
Epoch [53/120    avg_loss:0.040, val_acc:0.953]
Epoch [54/120    avg_loss:0.040, val_acc:0.975]
Epoch [55/120    avg_loss:0.049, val_acc:0.974]
Epoch [56/120    avg_loss:0.038, val_acc:0.970]
Epoch [57/120    avg_loss:0.029, val_acc:0.974]
Epoch [58/120    avg_loss:0.025, val_acc:0.980]
Epoch [59/120    avg_loss:0.024, val_acc:0.976]
Epoch [60/120    avg_loss:0.042, val_acc:0.891]
Epoch [61/120    avg_loss:0.044, val_acc:0.978]
Epoch [62/120    avg_loss:0.031, val_acc:0.975]
Epoch [63/120    avg_loss:0.027, val_acc:0.984]
Epoch [64/120    avg_loss:0.025, val_acc:0.979]
Epoch [65/120    avg_loss:0.025, val_acc:0.984]
Epoch [66/120    avg_loss:0.066, val_acc:0.974]
Epoch [67/120    avg_loss:0.059, val_acc:0.967]
Epoch [68/120    avg_loss:0.058, val_acc:0.957]
Epoch [69/120    avg_loss:0.064, val_acc:0.917]
Epoch [70/120    avg_loss:0.033, val_acc:0.979]
Epoch [71/120    avg_loss:0.037, val_acc:0.943]
Epoch [72/120    avg_loss:0.030, val_acc:0.975]
Epoch [73/120    avg_loss:0.024, val_acc:0.985]
Epoch [74/120    avg_loss:0.016, val_acc:0.984]
Epoch [75/120    avg_loss:0.021, val_acc:0.979]
Epoch [76/120    avg_loss:0.017, val_acc:0.983]
Epoch [77/120    avg_loss:0.016, val_acc:0.983]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.017, val_acc:0.978]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.023, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.985]
Epoch [83/120    avg_loss:0.038, val_acc:0.978]
Epoch [84/120    avg_loss:0.021, val_acc:0.983]
Epoch [85/120    avg_loss:0.015, val_acc:0.981]
Epoch [86/120    avg_loss:0.023, val_acc:0.980]
Epoch [87/120    avg_loss:0.028, val_acc:0.974]
Epoch [88/120    avg_loss:0.040, val_acc:0.981]
Epoch [89/120    avg_loss:0.031, val_acc:0.979]
Epoch [90/120    avg_loss:0.022, val_acc:0.975]
Epoch [91/120    avg_loss:0.028, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.016, val_acc:0.982]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.982]
Epoch [96/120    avg_loss:0.014, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     1    35     0]
 [    0     5 18030     0    35     0    12     0     8     0]
 [    0     2     0  1997     0     0     0     0    35     2]
 [    0    36     2     2  2910     0    12     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     1    10     1     3     0  4863     0     0     0]
 [    0     2     0     0     0     0     0  1283     0     5]
 [    0     7     0    25    45     0     0     0  3492     2]
 [    0     0     0     0    14    11     0     0     0   894]]

Accuracy:
99.22155544308679

F1 scores:
[       nan 0.9930906  0.99800731 0.9835016  0.97340692 0.99580313
 0.99600614 0.996892   0.97664662 0.98133919]

Kappa:
0.9896899749835496
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16e6b7c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.035, val_acc:0.122]
Epoch [2/120    avg_loss:1.748, val_acc:0.134]
Epoch [3/120    avg_loss:1.558, val_acc:0.297]
Epoch [4/120    avg_loss:1.407, val_acc:0.409]
Epoch [5/120    avg_loss:1.256, val_acc:0.419]
Epoch [6/120    avg_loss:1.126, val_acc:0.430]
Epoch [7/120    avg_loss:0.972, val_acc:0.481]
Epoch [8/120    avg_loss:0.871, val_acc:0.517]
Epoch [9/120    avg_loss:0.765, val_acc:0.467]
Epoch [10/120    avg_loss:0.686, val_acc:0.502]
Epoch [11/120    avg_loss:0.643, val_acc:0.533]
Epoch [12/120    avg_loss:0.575, val_acc:0.578]
Epoch [13/120    avg_loss:0.538, val_acc:0.607]
Epoch [14/120    avg_loss:0.501, val_acc:0.623]
Epoch [15/120    avg_loss:0.460, val_acc:0.677]
Epoch [16/120    avg_loss:0.403, val_acc:0.736]
Epoch [17/120    avg_loss:0.353, val_acc:0.767]
Epoch [18/120    avg_loss:0.345, val_acc:0.802]
Epoch [19/120    avg_loss:0.304, val_acc:0.783]
Epoch [20/120    avg_loss:0.286, val_acc:0.780]
Epoch [21/120    avg_loss:0.266, val_acc:0.825]
Epoch [22/120    avg_loss:0.250, val_acc:0.864]
Epoch [23/120    avg_loss:0.216, val_acc:0.885]
Epoch [24/120    avg_loss:0.196, val_acc:0.921]
Epoch [25/120    avg_loss:0.185, val_acc:0.925]
Epoch [26/120    avg_loss:0.164, val_acc:0.905]
Epoch [27/120    avg_loss:0.155, val_acc:0.912]
Epoch [28/120    avg_loss:0.143, val_acc:0.943]
Epoch [29/120    avg_loss:0.136, val_acc:0.940]
Epoch [30/120    avg_loss:0.116, val_acc:0.949]
Epoch [31/120    avg_loss:0.146, val_acc:0.934]
Epoch [32/120    avg_loss:0.215, val_acc:0.836]
Epoch [33/120    avg_loss:0.197, val_acc:0.847]
Epoch [34/120    avg_loss:0.192, val_acc:0.921]
Epoch [35/120    avg_loss:0.162, val_acc:0.892]
Epoch [36/120    avg_loss:0.113, val_acc:0.946]
Epoch [37/120    avg_loss:0.092, val_acc:0.952]
Epoch [38/120    avg_loss:0.098, val_acc:0.954]
Epoch [39/120    avg_loss:0.074, val_acc:0.947]
Epoch [40/120    avg_loss:0.071, val_acc:0.943]
Epoch [41/120    avg_loss:0.070, val_acc:0.973]
Epoch [42/120    avg_loss:0.069, val_acc:0.955]
Epoch [43/120    avg_loss:0.120, val_acc:0.917]
Epoch [44/120    avg_loss:0.082, val_acc:0.962]
Epoch [45/120    avg_loss:0.066, val_acc:0.964]
Epoch [46/120    avg_loss:0.046, val_acc:0.967]
Epoch [47/120    avg_loss:0.041, val_acc:0.963]
Epoch [48/120    avg_loss:0.053, val_acc:0.968]
Epoch [49/120    avg_loss:0.043, val_acc:0.970]
Epoch [50/120    avg_loss:0.029, val_acc:0.968]
Epoch [51/120    avg_loss:0.032, val_acc:0.972]
Epoch [52/120    avg_loss:0.046, val_acc:0.956]
Epoch [53/120    avg_loss:0.036, val_acc:0.968]
Epoch [54/120    avg_loss:0.039, val_acc:0.969]
Epoch [55/120    avg_loss:0.028, val_acc:0.977]
Epoch [56/120    avg_loss:0.025, val_acc:0.976]
Epoch [57/120    avg_loss:0.020, val_acc:0.977]
Epoch [58/120    avg_loss:0.021, val_acc:0.978]
Epoch [59/120    avg_loss:0.023, val_acc:0.976]
Epoch [60/120    avg_loss:0.019, val_acc:0.977]
Epoch [61/120    avg_loss:0.020, val_acc:0.976]
Epoch [62/120    avg_loss:0.023, val_acc:0.976]
Epoch [63/120    avg_loss:0.020, val_acc:0.975]
Epoch [64/120    avg_loss:0.017, val_acc:0.977]
Epoch [65/120    avg_loss:0.019, val_acc:0.976]
Epoch [66/120    avg_loss:0.022, val_acc:0.977]
Epoch [67/120    avg_loss:0.025, val_acc:0.977]
Epoch [68/120    avg_loss:0.016, val_acc:0.978]
Epoch [69/120    avg_loss:0.018, val_acc:0.978]
Epoch [70/120    avg_loss:0.019, val_acc:0.978]
Epoch [71/120    avg_loss:0.019, val_acc:0.978]
Epoch [72/120    avg_loss:0.018, val_acc:0.978]
Epoch [73/120    avg_loss:0.019, val_acc:0.978]
Epoch [74/120    avg_loss:0.019, val_acc:0.978]
Epoch [75/120    avg_loss:0.017, val_acc:0.979]
Epoch [76/120    avg_loss:0.015, val_acc:0.979]
Epoch [77/120    avg_loss:0.018, val_acc:0.978]
Epoch [78/120    avg_loss:0.019, val_acc:0.978]
Epoch [79/120    avg_loss:0.019, val_acc:0.979]
Epoch [80/120    avg_loss:0.016, val_acc:0.981]
Epoch [81/120    avg_loss:0.015, val_acc:0.979]
Epoch [82/120    avg_loss:0.018, val_acc:0.979]
Epoch [83/120    avg_loss:0.017, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.979]
Epoch [85/120    avg_loss:0.016, val_acc:0.978]
Epoch [86/120    avg_loss:0.015, val_acc:0.978]
Epoch [87/120    avg_loss:0.018, val_acc:0.979]
Epoch [88/120    avg_loss:0.018, val_acc:0.978]
Epoch [89/120    avg_loss:0.017, val_acc:0.980]
Epoch [90/120    avg_loss:0.017, val_acc:0.978]
Epoch [91/120    avg_loss:0.015, val_acc:0.981]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.979]
Epoch [94/120    avg_loss:0.017, val_acc:0.979]
Epoch [95/120    avg_loss:0.016, val_acc:0.980]
Epoch [96/120    avg_loss:0.021, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.980]
Epoch [98/120    avg_loss:0.017, val_acc:0.979]
Epoch [99/120    avg_loss:0.020, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.979]
Epoch [101/120    avg_loss:0.014, val_acc:0.979]
Epoch [102/120    avg_loss:0.013, val_acc:0.978]
Epoch [103/120    avg_loss:0.016, val_acc:0.979]
Epoch [104/120    avg_loss:0.016, val_acc:0.978]
Epoch [105/120    avg_loss:0.016, val_acc:0.979]
Epoch [106/120    avg_loss:0.017, val_acc:0.979]
Epoch [107/120    avg_loss:0.016, val_acc:0.980]
Epoch [108/120    avg_loss:0.015, val_acc:0.980]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.016, val_acc:0.981]
Epoch [112/120    avg_loss:0.013, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.981]
Epoch [114/120    avg_loss:0.016, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.981]
Epoch [116/120    avg_loss:0.015, val_acc:0.981]
Epoch [117/120    avg_loss:0.014, val_acc:0.981]
Epoch [118/120    avg_loss:0.017, val_acc:0.980]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6353     0     0     2     0     0     0    76     1]
 [    0     0 18022     0    58     0     1     0     9     0]
 [    0     3     0  2006     0     0     0     0    23     4]
 [    0    20    18     1  2910     0    13     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     1     0     0  4861     0     3     0]
 [    0    37     0     0     0     0     0  1253     0     0]
 [    0    30     0    33    55     0     1     0  3451     1]
 [    0     0     0     0     5    36     0     0     0   878]]

Accuracy:
98.90583953919939

F1 scores:
[       nan 0.98687379 0.99726088 0.9840569  0.96967677 0.98639456
 0.99671929 0.98545026 0.96666667 0.97231451]

Kappa:
0.9855068783537337
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59b2259908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.117, val_acc:0.087]
Epoch [2/120    avg_loss:1.836, val_acc:0.136]
Epoch [3/120    avg_loss:1.621, val_acc:0.142]
Epoch [4/120    avg_loss:1.451, val_acc:0.267]
Epoch [5/120    avg_loss:1.278, val_acc:0.353]
Epoch [6/120    avg_loss:1.157, val_acc:0.435]
Epoch [7/120    avg_loss:1.052, val_acc:0.491]
Epoch [8/120    avg_loss:0.922, val_acc:0.540]
Epoch [9/120    avg_loss:0.800, val_acc:0.542]
Epoch [10/120    avg_loss:0.693, val_acc:0.629]
Epoch [11/120    avg_loss:0.581, val_acc:0.652]
Epoch [12/120    avg_loss:0.513, val_acc:0.741]
Epoch [13/120    avg_loss:0.474, val_acc:0.749]
Epoch [14/120    avg_loss:0.400, val_acc:0.763]
Epoch [15/120    avg_loss:0.421, val_acc:0.795]
Epoch [16/120    avg_loss:0.358, val_acc:0.858]
Epoch [17/120    avg_loss:0.318, val_acc:0.864]
Epoch [18/120    avg_loss:0.313, val_acc:0.901]
Epoch [19/120    avg_loss:0.271, val_acc:0.878]
Epoch [20/120    avg_loss:0.234, val_acc:0.909]
Epoch [21/120    avg_loss:0.215, val_acc:0.949]
Epoch [22/120    avg_loss:0.193, val_acc:0.952]
Epoch [23/120    avg_loss:0.172, val_acc:0.926]
Epoch [24/120    avg_loss:0.177, val_acc:0.943]
Epoch [25/120    avg_loss:0.153, val_acc:0.948]
Epoch [26/120    avg_loss:0.124, val_acc:0.946]
Epoch [27/120    avg_loss:0.118, val_acc:0.959]
Epoch [28/120    avg_loss:0.115, val_acc:0.970]
Epoch [29/120    avg_loss:0.090, val_acc:0.970]
Epoch [30/120    avg_loss:0.100, val_acc:0.962]
Epoch [31/120    avg_loss:0.138, val_acc:0.910]
Epoch [32/120    avg_loss:0.122, val_acc:0.905]
Epoch [33/120    avg_loss:0.104, val_acc:0.969]
Epoch [34/120    avg_loss:0.084, val_acc:0.959]
Epoch [35/120    avg_loss:0.082, val_acc:0.958]
Epoch [36/120    avg_loss:0.077, val_acc:0.958]
Epoch [37/120    avg_loss:0.072, val_acc:0.971]
Epoch [38/120    avg_loss:0.066, val_acc:0.978]
Epoch [39/120    avg_loss:0.060, val_acc:0.971]
Epoch [40/120    avg_loss:0.065, val_acc:0.972]
Epoch [41/120    avg_loss:0.054, val_acc:0.976]
Epoch [42/120    avg_loss:0.059, val_acc:0.984]
Epoch [43/120    avg_loss:0.048, val_acc:0.974]
Epoch [44/120    avg_loss:0.045, val_acc:0.984]
Epoch [45/120    avg_loss:0.035, val_acc:0.979]
Epoch [46/120    avg_loss:0.030, val_acc:0.985]
Epoch [47/120    avg_loss:0.032, val_acc:0.980]
Epoch [48/120    avg_loss:0.027, val_acc:0.969]
Epoch [49/120    avg_loss:0.047, val_acc:0.976]
Epoch [50/120    avg_loss:0.032, val_acc:0.986]
Epoch [51/120    avg_loss:0.028, val_acc:0.982]
Epoch [52/120    avg_loss:0.022, val_acc:0.987]
Epoch [53/120    avg_loss:0.032, val_acc:0.974]
Epoch [54/120    avg_loss:0.033, val_acc:0.971]
Epoch [55/120    avg_loss:0.023, val_acc:0.984]
Epoch [56/120    avg_loss:0.021, val_acc:0.969]
Epoch [57/120    avg_loss:0.025, val_acc:0.980]
Epoch [58/120    avg_loss:0.014, val_acc:0.989]
Epoch [59/120    avg_loss:0.016, val_acc:0.983]
Epoch [60/120    avg_loss:0.025, val_acc:0.978]
Epoch [61/120    avg_loss:0.023, val_acc:0.975]
Epoch [62/120    avg_loss:0.022, val_acc:0.983]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.016, val_acc:0.990]
Epoch [65/120    avg_loss:0.014, val_acc:0.990]
Epoch [66/120    avg_loss:0.013, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.990]
Epoch [68/120    avg_loss:0.021, val_acc:0.972]
Epoch [69/120    avg_loss:0.029, val_acc:0.976]
Epoch [70/120    avg_loss:0.033, val_acc:0.946]
Epoch [71/120    avg_loss:0.021, val_acc:0.990]
Epoch [72/120    avg_loss:0.014, val_acc:0.983]
Epoch [73/120    avg_loss:0.013, val_acc:0.981]
Epoch [74/120    avg_loss:0.016, val_acc:0.946]
Epoch [75/120    avg_loss:0.035, val_acc:0.975]
Epoch [76/120    avg_loss:0.019, val_acc:0.990]
Epoch [77/120    avg_loss:0.022, val_acc:0.989]
Epoch [78/120    avg_loss:0.032, val_acc:0.972]
Epoch [79/120    avg_loss:0.019, val_acc:0.983]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.039, val_acc:0.978]
Epoch [82/120    avg_loss:0.022, val_acc:0.980]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.026, val_acc:0.975]
Epoch [87/120    avg_loss:0.019, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.989]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.007, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     0     0     0     2    45     1]
 [    0     0 18066     0    18     0     5     0     1     0]
 [    0    11     0  1959     0     0     0     0    60     6]
 [    0    31     5     0  2907     0    12     0    10     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     4     0     0  4860     0     7     0]
 [    0     2     0     0     0     0     0  1288     0     0]
 [    0    24     0    23    47     0     0     0  3476     1]
 [    0     0     0     0    12    12     0     0     0   895]]

Accuracy:
99.1492540910515

F1 scores:
[       nan 0.99099658 0.99900464 0.97414222 0.9761585  0.99542334
 0.9964121  0.99844961 0.96959554 0.97867687]

Kappa:
0.9887275546245348
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74f0e40908>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.038, val_acc:0.167]
Epoch [2/120    avg_loss:1.716, val_acc:0.245]
Epoch [3/120    avg_loss:1.549, val_acc:0.313]
Epoch [4/120    avg_loss:1.362, val_acc:0.343]
Epoch [5/120    avg_loss:1.234, val_acc:0.363]
Epoch [6/120    avg_loss:1.094, val_acc:0.400]
Epoch [7/120    avg_loss:0.996, val_acc:0.483]
Epoch [8/120    avg_loss:0.883, val_acc:0.562]
Epoch [9/120    avg_loss:0.775, val_acc:0.652]
Epoch [10/120    avg_loss:0.723, val_acc:0.566]
Epoch [11/120    avg_loss:0.637, val_acc:0.572]
Epoch [12/120    avg_loss:0.567, val_acc:0.572]
Epoch [13/120    avg_loss:0.495, val_acc:0.597]
Epoch [14/120    avg_loss:0.467, val_acc:0.654]
Epoch [15/120    avg_loss:0.422, val_acc:0.731]
Epoch [16/120    avg_loss:0.390, val_acc:0.662]
Epoch [17/120    avg_loss:0.366, val_acc:0.752]
Epoch [18/120    avg_loss:0.320, val_acc:0.754]
Epoch [19/120    avg_loss:0.282, val_acc:0.878]
Epoch [20/120    avg_loss:0.246, val_acc:0.907]
Epoch [21/120    avg_loss:0.251, val_acc:0.833]
Epoch [22/120    avg_loss:0.242, val_acc:0.878]
Epoch [23/120    avg_loss:0.210, val_acc:0.878]
Epoch [24/120    avg_loss:0.177, val_acc:0.903]
Epoch [25/120    avg_loss:0.172, val_acc:0.899]
Epoch [26/120    avg_loss:0.163, val_acc:0.915]
Epoch [27/120    avg_loss:0.145, val_acc:0.941]
Epoch [28/120    avg_loss:0.147, val_acc:0.916]
Epoch [29/120    avg_loss:0.129, val_acc:0.928]
Epoch [30/120    avg_loss:0.112, val_acc:0.943]
Epoch [31/120    avg_loss:0.115, val_acc:0.947]
Epoch [32/120    avg_loss:0.117, val_acc:0.960]
Epoch [33/120    avg_loss:0.086, val_acc:0.964]
Epoch [34/120    avg_loss:0.110, val_acc:0.946]
Epoch [35/120    avg_loss:0.100, val_acc:0.948]
Epoch [36/120    avg_loss:0.084, val_acc:0.945]
Epoch [37/120    avg_loss:0.071, val_acc:0.967]
Epoch [38/120    avg_loss:0.069, val_acc:0.942]
Epoch [39/120    avg_loss:0.055, val_acc:0.966]
Epoch [40/120    avg_loss:0.059, val_acc:0.961]
Epoch [41/120    avg_loss:0.087, val_acc:0.955]
Epoch [42/120    avg_loss:0.063, val_acc:0.955]
Epoch [43/120    avg_loss:0.047, val_acc:0.974]
Epoch [44/120    avg_loss:0.042, val_acc:0.972]
Epoch [45/120    avg_loss:0.052, val_acc:0.972]
Epoch [46/120    avg_loss:0.048, val_acc:0.972]
Epoch [47/120    avg_loss:0.034, val_acc:0.975]
Epoch [48/120    avg_loss:0.034, val_acc:0.969]
Epoch [49/120    avg_loss:0.030, val_acc:0.969]
Epoch [50/120    avg_loss:0.035, val_acc:0.973]
Epoch [51/120    avg_loss:0.033, val_acc:0.969]
Epoch [52/120    avg_loss:0.042, val_acc:0.966]
Epoch [53/120    avg_loss:0.026, val_acc:0.979]
Epoch [54/120    avg_loss:0.040, val_acc:0.963]
Epoch [55/120    avg_loss:0.035, val_acc:0.972]
Epoch [56/120    avg_loss:0.025, val_acc:0.962]
Epoch [57/120    avg_loss:0.024, val_acc:0.980]
Epoch [58/120    avg_loss:0.020, val_acc:0.982]
Epoch [59/120    avg_loss:0.023, val_acc:0.978]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.020, val_acc:0.976]
Epoch [62/120    avg_loss:0.032, val_acc:0.968]
Epoch [63/120    avg_loss:0.079, val_acc:0.969]
Epoch [64/120    avg_loss:0.123, val_acc:0.950]
Epoch [65/120    avg_loss:0.061, val_acc:0.959]
Epoch [66/120    avg_loss:0.041, val_acc:0.962]
Epoch [67/120    avg_loss:0.036, val_acc:0.972]
Epoch [68/120    avg_loss:0.048, val_acc:0.972]
Epoch [69/120    avg_loss:0.028, val_acc:0.982]
Epoch [70/120    avg_loss:0.023, val_acc:0.981]
Epoch [71/120    avg_loss:0.022, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.019, val_acc:0.983]
Epoch [74/120    avg_loss:0.018, val_acc:0.973]
Epoch [75/120    avg_loss:0.018, val_acc:0.984]
Epoch [76/120    avg_loss:0.017, val_acc:0.981]
Epoch [77/120    avg_loss:0.018, val_acc:0.986]
Epoch [78/120    avg_loss:0.015, val_acc:0.979]
Epoch [79/120    avg_loss:0.059, val_acc:0.953]
Epoch [80/120    avg_loss:0.038, val_acc:0.972]
Epoch [81/120    avg_loss:0.024, val_acc:0.973]
Epoch [82/120    avg_loss:0.028, val_acc:0.970]
Epoch [83/120    avg_loss:0.019, val_acc:0.980]
Epoch [84/120    avg_loss:0.013, val_acc:0.980]
Epoch [85/120    avg_loss:0.010, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.008, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     0     0     0     0    24    46     4]
 [    0     0 18050     0    33     0     2     0     5     0]
 [    0     2     0  1974     2     0     0     0    54     4]
 [    0    30     6     0  2911     9     3     0    12     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     0     0  4854     0     0     0]
 [    0    20     0     0     0     0     0  1267     0     3]
 [    0    52     0    22    43     0     0     0  3453     1]
 [    0     0     0     0     6    24     0     0     0   889]]

Accuracy:
98.95886053069192

F1 scores:
[       nan 0.98619513 0.99806469 0.97916667 0.97569968 0.98751419
 0.99702167 0.98179    0.96709144 0.9763866 ]

Kappa:
0.986205000892197
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d64db5940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.146]
Epoch [2/120    avg_loss:1.683, val_acc:0.196]
Epoch [3/120    avg_loss:1.484, val_acc:0.302]
Epoch [4/120    avg_loss:1.328, val_acc:0.379]
Epoch [5/120    avg_loss:1.203, val_acc:0.370]
Epoch [6/120    avg_loss:1.091, val_acc:0.460]
Epoch [7/120    avg_loss:0.997, val_acc:0.462]
Epoch [8/120    avg_loss:0.898, val_acc:0.496]
Epoch [9/120    avg_loss:0.826, val_acc:0.506]
Epoch [10/120    avg_loss:0.760, val_acc:0.549]
Epoch [11/120    avg_loss:0.636, val_acc:0.583]
Epoch [12/120    avg_loss:0.565, val_acc:0.680]
Epoch [13/120    avg_loss:0.485, val_acc:0.641]
Epoch [14/120    avg_loss:0.446, val_acc:0.765]
Epoch [15/120    avg_loss:0.389, val_acc:0.787]
Epoch [16/120    avg_loss:0.372, val_acc:0.770]
Epoch [17/120    avg_loss:0.332, val_acc:0.828]
Epoch [18/120    avg_loss:0.307, val_acc:0.785]
Epoch [19/120    avg_loss:0.276, val_acc:0.833]
Epoch [20/120    avg_loss:0.243, val_acc:0.888]
Epoch [21/120    avg_loss:0.292, val_acc:0.876]
Epoch [22/120    avg_loss:0.247, val_acc:0.813]
Epoch [23/120    avg_loss:0.220, val_acc:0.931]
Epoch [24/120    avg_loss:0.198, val_acc:0.914]
Epoch [25/120    avg_loss:0.190, val_acc:0.914]
Epoch [26/120    avg_loss:0.179, val_acc:0.945]
Epoch [27/120    avg_loss:0.129, val_acc:0.938]
Epoch [28/120    avg_loss:0.121, val_acc:0.955]
Epoch [29/120    avg_loss:0.111, val_acc:0.958]
Epoch [30/120    avg_loss:0.110, val_acc:0.955]
Epoch [31/120    avg_loss:0.111, val_acc:0.899]
Epoch [32/120    avg_loss:0.096, val_acc:0.973]
Epoch [33/120    avg_loss:0.085, val_acc:0.966]
Epoch [34/120    avg_loss:0.069, val_acc:0.966]
Epoch [35/120    avg_loss:0.127, val_acc:0.960]
Epoch [36/120    avg_loss:0.077, val_acc:0.968]
Epoch [37/120    avg_loss:0.062, val_acc:0.968]
Epoch [38/120    avg_loss:0.058, val_acc:0.959]
Epoch [39/120    avg_loss:0.049, val_acc:0.974]
Epoch [40/120    avg_loss:0.041, val_acc:0.974]
Epoch [41/120    avg_loss:0.043, val_acc:0.953]
Epoch [42/120    avg_loss:0.049, val_acc:0.968]
Epoch [43/120    avg_loss:0.052, val_acc:0.970]
Epoch [44/120    avg_loss:0.039, val_acc:0.963]
Epoch [45/120    avg_loss:0.063, val_acc:0.961]
Epoch [46/120    avg_loss:0.047, val_acc:0.968]
Epoch [47/120    avg_loss:0.038, val_acc:0.977]
Epoch [48/120    avg_loss:0.036, val_acc:0.975]
Epoch [49/120    avg_loss:0.045, val_acc:0.971]
Epoch [50/120    avg_loss:0.031, val_acc:0.979]
Epoch [51/120    avg_loss:0.030, val_acc:0.980]
Epoch [52/120    avg_loss:0.033, val_acc:0.980]
Epoch [53/120    avg_loss:0.035, val_acc:0.978]
Epoch [54/120    avg_loss:0.030, val_acc:0.978]
Epoch [55/120    avg_loss:0.035, val_acc:0.976]
Epoch [56/120    avg_loss:0.024, val_acc:0.982]
Epoch [57/120    avg_loss:0.019, val_acc:0.982]
Epoch [58/120    avg_loss:0.020, val_acc:0.983]
Epoch [59/120    avg_loss:0.021, val_acc:0.982]
Epoch [60/120    avg_loss:0.036, val_acc:0.970]
Epoch [61/120    avg_loss:0.058, val_acc:0.972]
Epoch [62/120    avg_loss:0.030, val_acc:0.980]
Epoch [63/120    avg_loss:0.027, val_acc:0.971]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.017, val_acc:0.985]
Epoch [66/120    avg_loss:0.017, val_acc:0.974]
Epoch [67/120    avg_loss:0.032, val_acc:0.976]
Epoch [68/120    avg_loss:0.442, val_acc:0.704]
Epoch [69/120    avg_loss:0.684, val_acc:0.695]
Epoch [70/120    avg_loss:0.477, val_acc:0.794]
Epoch [71/120    avg_loss:0.403, val_acc:0.777]
Epoch [72/120    avg_loss:0.370, val_acc:0.905]
Epoch [73/120    avg_loss:0.352, val_acc:0.862]
Epoch [74/120    avg_loss:0.312, val_acc:0.809]
Epoch [75/120    avg_loss:0.324, val_acc:0.917]
Epoch [76/120    avg_loss:0.263, val_acc:0.893]
Epoch [77/120    avg_loss:0.203, val_acc:0.953]
Epoch [78/120    avg_loss:0.175, val_acc:0.935]
Epoch [79/120    avg_loss:0.138, val_acc:0.958]
Epoch [80/120    avg_loss:0.130, val_acc:0.963]
Epoch [81/120    avg_loss:0.118, val_acc:0.961]
Epoch [82/120    avg_loss:0.118, val_acc:0.961]
Epoch [83/120    avg_loss:0.114, val_acc:0.963]
Epoch [84/120    avg_loss:0.117, val_acc:0.962]
Epoch [85/120    avg_loss:0.113, val_acc:0.965]
Epoch [86/120    avg_loss:0.114, val_acc:0.966]
Epoch [87/120    avg_loss:0.117, val_acc:0.964]
Epoch [88/120    avg_loss:0.108, val_acc:0.960]
Epoch [89/120    avg_loss:0.115, val_acc:0.964]
Epoch [90/120    avg_loss:0.114, val_acc:0.962]
Epoch [91/120    avg_loss:0.107, val_acc:0.965]
Epoch [92/120    avg_loss:0.103, val_acc:0.965]
Epoch [93/120    avg_loss:0.096, val_acc:0.963]
Epoch [94/120    avg_loss:0.104, val_acc:0.962]
Epoch [95/120    avg_loss:0.096, val_acc:0.962]
Epoch [96/120    avg_loss:0.098, val_acc:0.962]
Epoch [97/120    avg_loss:0.099, val_acc:0.962]
Epoch [98/120    avg_loss:0.100, val_acc:0.963]
Epoch [99/120    avg_loss:0.099, val_acc:0.965]
Epoch [100/120    avg_loss:0.095, val_acc:0.965]
Epoch [101/120    avg_loss:0.102, val_acc:0.965]
Epoch [102/120    avg_loss:0.094, val_acc:0.964]
Epoch [103/120    avg_loss:0.098, val_acc:0.965]
Epoch [104/120    avg_loss:0.096, val_acc:0.964]
Epoch [105/120    avg_loss:0.092, val_acc:0.964]
Epoch [106/120    avg_loss:0.107, val_acc:0.964]
Epoch [107/120    avg_loss:0.094, val_acc:0.964]
Epoch [108/120    avg_loss:0.087, val_acc:0.964]
Epoch [109/120    avg_loss:0.098, val_acc:0.964]
Epoch [110/120    avg_loss:0.096, val_acc:0.964]
Epoch [111/120    avg_loss:0.101, val_acc:0.964]
Epoch [112/120    avg_loss:0.093, val_acc:0.963]
Epoch [113/120    avg_loss:0.100, val_acc:0.964]
Epoch [114/120    avg_loss:0.104, val_acc:0.964]
Epoch [115/120    avg_loss:0.089, val_acc:0.964]
Epoch [116/120    avg_loss:0.096, val_acc:0.964]
Epoch [117/120    avg_loss:0.096, val_acc:0.964]
Epoch [118/120    avg_loss:0.098, val_acc:0.964]
Epoch [119/120    avg_loss:0.100, val_acc:0.964]
Epoch [120/120    avg_loss:0.099, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6056     0    42    15     0     0    65   142   112]
 [    0     0 17777     0    88     0   225     0     0     0]
 [    0    12     0  1961     0     0     0     0    38    25]
 [    0    40    13     0  2888     0    25     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     5     0     0  4807     0    51     0]
 [    0    35     0     0     0     0     6  1240     6     3]
 [    0   118     0    30    41     0    77     0  3305     0]
 [    0     4     0     0    10    52     0     0     0   853]]

Accuracy:
96.86453136673656

F1 scores:
[       nan 0.95392612 0.99050007 0.96269023 0.96042567 0.98046582
 0.95967259 0.95568401 0.92850119 0.89225941]

Kappa:
0.9586018306687522
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8806049978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.223, val_acc:0.077]
Epoch [2/120    avg_loss:1.845, val_acc:0.214]
Epoch [3/120    avg_loss:1.602, val_acc:0.278]
Epoch [4/120    avg_loss:1.394, val_acc:0.298]
Epoch [5/120    avg_loss:1.238, val_acc:0.357]
Epoch [6/120    avg_loss:1.112, val_acc:0.446]
Epoch [7/120    avg_loss:1.000, val_acc:0.512]
Epoch [8/120    avg_loss:0.914, val_acc:0.612]
Epoch [9/120    avg_loss:0.814, val_acc:0.650]
Epoch [10/120    avg_loss:0.726, val_acc:0.623]
Epoch [11/120    avg_loss:0.625, val_acc:0.647]
Epoch [12/120    avg_loss:0.570, val_acc:0.647]
Epoch [13/120    avg_loss:0.489, val_acc:0.767]
Epoch [14/120    avg_loss:0.433, val_acc:0.725]
Epoch [15/120    avg_loss:0.380, val_acc:0.781]
Epoch [16/120    avg_loss:0.363, val_acc:0.824]
Epoch [17/120    avg_loss:0.317, val_acc:0.895]
Epoch [18/120    avg_loss:0.256, val_acc:0.829]
Epoch [19/120    avg_loss:0.244, val_acc:0.875]
Epoch [20/120    avg_loss:0.223, val_acc:0.923]
Epoch [21/120    avg_loss:0.194, val_acc:0.947]
Epoch [22/120    avg_loss:0.187, val_acc:0.939]
Epoch [23/120    avg_loss:0.176, val_acc:0.940]
Epoch [24/120    avg_loss:0.176, val_acc:0.930]
Epoch [25/120    avg_loss:0.148, val_acc:0.953]
Epoch [26/120    avg_loss:0.128, val_acc:0.956]
Epoch [27/120    avg_loss:0.118, val_acc:0.922]
Epoch [28/120    avg_loss:0.101, val_acc:0.960]
Epoch [29/120    avg_loss:0.101, val_acc:0.950]
Epoch [30/120    avg_loss:0.121, val_acc:0.968]
Epoch [31/120    avg_loss:0.095, val_acc:0.966]
Epoch [32/120    avg_loss:0.076, val_acc:0.962]
Epoch [33/120    avg_loss:0.094, val_acc:0.962]
Epoch [34/120    avg_loss:0.067, val_acc:0.964]
Epoch [35/120    avg_loss:0.071, val_acc:0.963]
Epoch [36/120    avg_loss:0.063, val_acc:0.957]
Epoch [37/120    avg_loss:0.088, val_acc:0.936]
Epoch [38/120    avg_loss:0.068, val_acc:0.964]
Epoch [39/120    avg_loss:0.071, val_acc:0.973]
Epoch [40/120    avg_loss:0.050, val_acc:0.975]
Epoch [41/120    avg_loss:0.046, val_acc:0.963]
Epoch [42/120    avg_loss:0.055, val_acc:0.968]
Epoch [43/120    avg_loss:0.046, val_acc:0.968]
Epoch [44/120    avg_loss:0.039, val_acc:0.971]
Epoch [45/120    avg_loss:0.039, val_acc:0.973]
Epoch [46/120    avg_loss:0.040, val_acc:0.973]
Epoch [47/120    avg_loss:0.042, val_acc:0.972]
Epoch [48/120    avg_loss:0.033, val_acc:0.971]
Epoch [49/120    avg_loss:0.028, val_acc:0.975]
Epoch [50/120    avg_loss:0.021, val_acc:0.981]
Epoch [51/120    avg_loss:0.021, val_acc:0.983]
Epoch [52/120    avg_loss:0.022, val_acc:0.983]
Epoch [53/120    avg_loss:0.018, val_acc:0.977]
Epoch [54/120    avg_loss:0.018, val_acc:0.977]
Epoch [55/120    avg_loss:0.021, val_acc:0.979]
Epoch [56/120    avg_loss:0.022, val_acc:0.969]
Epoch [57/120    avg_loss:0.019, val_acc:0.983]
Epoch [58/120    avg_loss:0.031, val_acc:0.979]
Epoch [59/120    avg_loss:0.018, val_acc:0.978]
Epoch [60/120    avg_loss:0.028, val_acc:0.971]
Epoch [61/120    avg_loss:0.020, val_acc:0.980]
Epoch [62/120    avg_loss:0.019, val_acc:0.982]
Epoch [63/120    avg_loss:0.014, val_acc:0.985]
Epoch [64/120    avg_loss:0.027, val_acc:0.966]
Epoch [65/120    avg_loss:0.018, val_acc:0.976]
Epoch [66/120    avg_loss:0.015, val_acc:0.978]
Epoch [67/120    avg_loss:0.015, val_acc:0.981]
Epoch [68/120    avg_loss:0.016, val_acc:0.981]
Epoch [69/120    avg_loss:0.014, val_acc:0.983]
Epoch [70/120    avg_loss:0.012, val_acc:0.980]
Epoch [71/120    avg_loss:0.012, val_acc:0.982]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.015, val_acc:0.977]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.974]
Epoch [80/120    avg_loss:0.015, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.983]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.027, val_acc:0.973]
Epoch [86/120    avg_loss:0.053, val_acc:0.966]
Epoch [87/120    avg_loss:0.020, val_acc:0.983]
Epoch [88/120    avg_loss:0.017, val_acc:0.979]
Epoch [89/120    avg_loss:0.013, val_acc:0.980]
Epoch [90/120    avg_loss:0.036, val_acc:0.985]
Epoch [91/120    avg_loss:0.031, val_acc:0.963]
Epoch [92/120    avg_loss:0.028, val_acc:0.981]
Epoch [93/120    avg_loss:0.013, val_acc:0.983]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.017, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.013, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     0     2     0     1     9    72    12]
 [    0     0 17950     0    38     0    94     0     8     0]
 [    0     0     0  1997     0     0     0     0    38     1]
 [    0    29     3     1  2920     0     4     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     2     0     0  4859     0     0     0]
 [    0     6     0     0     0     0     2  1282     0     0]
 [    0    15     0    36    54     0     0     0  3464     2]
 [    0     0     0     0     4    25     0     0     0   890]]

Accuracy:
98.81907791675705

F1 scores:
[       nan 0.98860977 0.99556295 0.98084479 0.97495826 0.99051233
 0.9878024  0.99341341 0.96651786 0.97587719]

Kappa:
0.9843739098776074
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05359af940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.083, val_acc:0.505]
Epoch [2/120    avg_loss:1.792, val_acc:0.584]
Epoch [3/120    avg_loss:1.582, val_acc:0.512]
Epoch [4/120    avg_loss:1.378, val_acc:0.469]
Epoch [5/120    avg_loss:1.231, val_acc:0.506]
Epoch [6/120    avg_loss:1.075, val_acc:0.678]
Epoch [7/120    avg_loss:0.989, val_acc:0.798]
Epoch [8/120    avg_loss:0.870, val_acc:0.785]
Epoch [9/120    avg_loss:0.766, val_acc:0.796]
Epoch [10/120    avg_loss:0.664, val_acc:0.843]
Epoch [11/120    avg_loss:0.590, val_acc:0.848]
Epoch [12/120    avg_loss:0.550, val_acc:0.777]
Epoch [13/120    avg_loss:0.512, val_acc:0.858]
Epoch [14/120    avg_loss:0.434, val_acc:0.854]
Epoch [15/120    avg_loss:0.387, val_acc:0.852]
Epoch [16/120    avg_loss:0.384, val_acc:0.834]
Epoch [17/120    avg_loss:0.381, val_acc:0.846]
Epoch [18/120    avg_loss:0.331, val_acc:0.895]
Epoch [19/120    avg_loss:0.299, val_acc:0.870]
Epoch [20/120    avg_loss:0.261, val_acc:0.927]
Epoch [21/120    avg_loss:0.242, val_acc:0.927]
Epoch [22/120    avg_loss:0.225, val_acc:0.806]
Epoch [23/120    avg_loss:0.219, val_acc:0.899]
Epoch [24/120    avg_loss:0.199, val_acc:0.933]
Epoch [25/120    avg_loss:0.157, val_acc:0.948]
Epoch [26/120    avg_loss:0.183, val_acc:0.949]
Epoch [27/120    avg_loss:0.153, val_acc:0.939]
Epoch [28/120    avg_loss:0.152, val_acc:0.942]
Epoch [29/120    avg_loss:0.154, val_acc:0.960]
Epoch [30/120    avg_loss:0.126, val_acc:0.962]
Epoch [31/120    avg_loss:0.111, val_acc:0.945]
Epoch [32/120    avg_loss:0.092, val_acc:0.965]
Epoch [33/120    avg_loss:0.087, val_acc:0.961]
Epoch [34/120    avg_loss:0.088, val_acc:0.964]
Epoch [35/120    avg_loss:0.089, val_acc:0.961]
Epoch [36/120    avg_loss:0.066, val_acc:0.964]
Epoch [37/120    avg_loss:0.065, val_acc:0.975]
Epoch [38/120    avg_loss:0.065, val_acc:0.965]
Epoch [39/120    avg_loss:0.061, val_acc:0.977]
Epoch [40/120    avg_loss:0.057, val_acc:0.969]
Epoch [41/120    avg_loss:0.055, val_acc:0.978]
Epoch [42/120    avg_loss:0.052, val_acc:0.974]
Epoch [43/120    avg_loss:0.056, val_acc:0.969]
Epoch [44/120    avg_loss:0.052, val_acc:0.968]
Epoch [45/120    avg_loss:0.043, val_acc:0.972]
Epoch [46/120    avg_loss:0.039, val_acc:0.975]
Epoch [47/120    avg_loss:0.052, val_acc:0.971]
Epoch [48/120    avg_loss:0.075, val_acc:0.953]
Epoch [49/120    avg_loss:0.083, val_acc:0.968]
Epoch [50/120    avg_loss:0.083, val_acc:0.975]
Epoch [51/120    avg_loss:0.043, val_acc:0.981]
Epoch [52/120    avg_loss:0.032, val_acc:0.973]
Epoch [53/120    avg_loss:0.031, val_acc:0.983]
Epoch [54/120    avg_loss:0.057, val_acc:0.971]
Epoch [55/120    avg_loss:0.061, val_acc:0.974]
Epoch [56/120    avg_loss:0.041, val_acc:0.971]
Epoch [57/120    avg_loss:0.030, val_acc:0.978]
Epoch [58/120    avg_loss:0.034, val_acc:0.945]
Epoch [59/120    avg_loss:0.026, val_acc:0.975]
Epoch [60/120    avg_loss:0.026, val_acc:0.969]
Epoch [61/120    avg_loss:0.031, val_acc:0.981]
Epoch [62/120    avg_loss:0.025, val_acc:0.976]
Epoch [63/120    avg_loss:0.015, val_acc:0.983]
Epoch [64/120    avg_loss:0.019, val_acc:0.982]
Epoch [65/120    avg_loss:0.013, val_acc:0.983]
Epoch [66/120    avg_loss:0.016, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.980]
Epoch [68/120    avg_loss:0.017, val_acc:0.988]
Epoch [69/120    avg_loss:0.019, val_acc:0.983]
Epoch [70/120    avg_loss:0.013, val_acc:0.983]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.012, val_acc:0.990]
Epoch [73/120    avg_loss:0.018, val_acc:0.987]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.013, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.018, val_acc:0.985]
Epoch [79/120    avg_loss:0.030, val_acc:0.983]
Epoch [80/120    avg_loss:0.016, val_acc:0.988]
Epoch [81/120    avg_loss:0.011, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.013, val_acc:0.988]
Epoch [84/120    avg_loss:0.010, val_acc:0.976]
Epoch [85/120    avg_loss:0.013, val_acc:0.988]
Epoch [86/120    avg_loss:0.026, val_acc:0.978]
Epoch [87/120    avg_loss:0.018, val_acc:0.987]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.018, val_acc:0.982]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.023, val_acc:0.971]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6352     0     1     0     0     0     0    77     2]
 [    0     1 18059     0    25     0     0     0     5     0]
 [    0    11     0  1945     0     0     0     0    79     1]
 [    0    26     9     0  2915     0     3     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     1     0     0  4872     0     0     0]
 [    0     7     0     0     0     7     2  1273     1     0]
 [    0     7     0    16    45     0     1     0  3502     0]
 [    0     0     0     0    13    27     0     0     0   879]]

Accuracy:
99.05767237847348

F1 scores:
[       nan 0.98971642 0.99875563 0.97274319 0.97654941 0.9871407
 0.99876999 0.99336715 0.96553626 0.97612438]

Kappa:
0.9875152289940031
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd4b87dd8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.069, val_acc:0.231]
Epoch [2/120    avg_loss:1.733, val_acc:0.237]
Epoch [3/120    avg_loss:1.520, val_acc:0.315]
Epoch [4/120    avg_loss:1.367, val_acc:0.366]
Epoch [5/120    avg_loss:1.236, val_acc:0.384]
Epoch [6/120    avg_loss:1.100, val_acc:0.392]
Epoch [7/120    avg_loss:0.972, val_acc:0.434]
Epoch [8/120    avg_loss:0.879, val_acc:0.482]
Epoch [9/120    avg_loss:0.824, val_acc:0.591]
Epoch [10/120    avg_loss:0.722, val_acc:0.536]
Epoch [11/120    avg_loss:0.637, val_acc:0.541]
Epoch [12/120    avg_loss:0.586, val_acc:0.597]
Epoch [13/120    avg_loss:0.498, val_acc:0.658]
Epoch [14/120    avg_loss:0.472, val_acc:0.673]
Epoch [15/120    avg_loss:0.427, val_acc:0.747]
Epoch [16/120    avg_loss:0.384, val_acc:0.728]
Epoch [17/120    avg_loss:0.327, val_acc:0.806]
Epoch [18/120    avg_loss:0.338, val_acc:0.782]
Epoch [19/120    avg_loss:0.310, val_acc:0.828]
Epoch [20/120    avg_loss:0.294, val_acc:0.860]
Epoch [21/120    avg_loss:0.250, val_acc:0.878]
Epoch [22/120    avg_loss:0.229, val_acc:0.917]
Epoch [23/120    avg_loss:0.216, val_acc:0.873]
Epoch [24/120    avg_loss:0.172, val_acc:0.884]
Epoch [25/120    avg_loss:0.172, val_acc:0.841]
Epoch [26/120    avg_loss:0.149, val_acc:0.943]
Epoch [27/120    avg_loss:0.155, val_acc:0.927]
Epoch [28/120    avg_loss:0.140, val_acc:0.956]
Epoch [29/120    avg_loss:0.118, val_acc:0.955]
Epoch [30/120    avg_loss:0.128, val_acc:0.940]
Epoch [31/120    avg_loss:0.114, val_acc:0.959]
Epoch [32/120    avg_loss:0.075, val_acc:0.954]
Epoch [33/120    avg_loss:0.095, val_acc:0.952]
Epoch [34/120    avg_loss:0.095, val_acc:0.969]
Epoch [35/120    avg_loss:0.088, val_acc:0.966]
Epoch [36/120    avg_loss:0.065, val_acc:0.961]
Epoch [37/120    avg_loss:0.081, val_acc:0.957]
Epoch [38/120    avg_loss:0.077, val_acc:0.942]
Epoch [39/120    avg_loss:0.061, val_acc:0.959]
Epoch [40/120    avg_loss:0.041, val_acc:0.969]
Epoch [41/120    avg_loss:0.048, val_acc:0.967]
Epoch [42/120    avg_loss:0.053, val_acc:0.976]
Epoch [43/120    avg_loss:0.041, val_acc:0.965]
Epoch [44/120    avg_loss:0.042, val_acc:0.970]
Epoch [45/120    avg_loss:0.053, val_acc:0.965]
Epoch [46/120    avg_loss:0.045, val_acc:0.963]
Epoch [47/120    avg_loss:0.043, val_acc:0.959]
Epoch [48/120    avg_loss:0.047, val_acc:0.972]
Epoch [49/120    avg_loss:0.042, val_acc:0.971]
Epoch [50/120    avg_loss:0.046, val_acc:0.969]
Epoch [51/120    avg_loss:0.031, val_acc:0.949]
Epoch [52/120    avg_loss:0.026, val_acc:0.969]
Epoch [53/120    avg_loss:0.022, val_acc:0.983]
Epoch [54/120    avg_loss:0.023, val_acc:0.977]
Epoch [55/120    avg_loss:0.034, val_acc:0.961]
Epoch [56/120    avg_loss:0.036, val_acc:0.947]
Epoch [57/120    avg_loss:0.030, val_acc:0.975]
Epoch [58/120    avg_loss:0.023, val_acc:0.978]
Epoch [59/120    avg_loss:0.021, val_acc:0.979]
Epoch [60/120    avg_loss:0.023, val_acc:0.977]
Epoch [61/120    avg_loss:0.025, val_acc:0.978]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.013, val_acc:0.982]
Epoch [64/120    avg_loss:0.018, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.021, val_acc:0.975]
Epoch [67/120    avg_loss:0.022, val_acc:0.982]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.012, val_acc:0.983]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.013, val_acc:0.984]
Epoch [74/120    avg_loss:0.013, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.984]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.012, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.982]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.013, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.983]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.982]
Epoch [93/120    avg_loss:0.009, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.984]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.010, val_acc:0.984]
Epoch [101/120    avg_loss:0.011, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.012, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.011, val_acc:0.983]
Epoch [109/120    avg_loss:0.013, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6260     0     0     0     0     1     2   168     1]
 [    0     2 18037     0    41     0     3     0     7     0]
 [    0     2     0  1980     0     0     0     0    47     7]
 [    0    31    18     4  2899     0     8     0     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     4     0  4848     0     2     0]
 [    0    16     0     0     0    14     0  1260     0     0]
 [    0    39     0    40    39     0     0     0  3452     1]
 [    0     0     0     0    10    15     0     0     0   894]]

Accuracy:
98.65519485214374

F1 scores:
[       nan 0.97950243 0.99737344 0.97536946 0.97200335 0.98901099
 0.995687   0.98746082 0.95148842 0.97972603]

Kappa:
0.9821857207817538
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7fa330978>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.986, val_acc:0.094]
Epoch [2/120    avg_loss:1.681, val_acc:0.143]
Epoch [3/120    avg_loss:1.458, val_acc:0.264]
Epoch [4/120    avg_loss:1.335, val_acc:0.555]
Epoch [5/120    avg_loss:1.218, val_acc:0.349]
Epoch [6/120    avg_loss:1.130, val_acc:0.583]
Epoch [7/120    avg_loss:1.029, val_acc:0.703]
Epoch [8/120    avg_loss:0.964, val_acc:0.719]
Epoch [9/120    avg_loss:0.849, val_acc:0.773]
Epoch [10/120    avg_loss:0.753, val_acc:0.754]
Epoch [11/120    avg_loss:0.676, val_acc:0.718]
Epoch [12/120    avg_loss:0.572, val_acc:0.738]
Epoch [13/120    avg_loss:0.492, val_acc:0.785]
Epoch [14/120    avg_loss:0.436, val_acc:0.804]
Epoch [15/120    avg_loss:0.408, val_acc:0.808]
Epoch [16/120    avg_loss:0.366, val_acc:0.798]
Epoch [17/120    avg_loss:0.351, val_acc:0.794]
Epoch [18/120    avg_loss:0.341, val_acc:0.820]
Epoch [19/120    avg_loss:0.281, val_acc:0.870]
Epoch [20/120    avg_loss:0.283, val_acc:0.858]
Epoch [21/120    avg_loss:0.291, val_acc:0.854]
Epoch [22/120    avg_loss:0.246, val_acc:0.819]
Epoch [23/120    avg_loss:0.216, val_acc:0.932]
Epoch [24/120    avg_loss:0.195, val_acc:0.828]
Epoch [25/120    avg_loss:0.187, val_acc:0.929]
Epoch [26/120    avg_loss:0.155, val_acc:0.930]
Epoch [27/120    avg_loss:0.131, val_acc:0.950]
Epoch [28/120    avg_loss:0.117, val_acc:0.923]
Epoch [29/120    avg_loss:0.109, val_acc:0.953]
Epoch [30/120    avg_loss:0.107, val_acc:0.928]
Epoch [31/120    avg_loss:0.151, val_acc:0.950]
Epoch [32/120    avg_loss:0.091, val_acc:0.936]
Epoch [33/120    avg_loss:0.085, val_acc:0.958]
Epoch [34/120    avg_loss:0.079, val_acc:0.947]
Epoch [35/120    avg_loss:0.080, val_acc:0.950]
Epoch [36/120    avg_loss:0.067, val_acc:0.961]
Epoch [37/120    avg_loss:0.094, val_acc:0.937]
Epoch [38/120    avg_loss:0.091, val_acc:0.963]
Epoch [39/120    avg_loss:0.077, val_acc:0.947]
Epoch [40/120    avg_loss:0.076, val_acc:0.943]
Epoch [41/120    avg_loss:0.066, val_acc:0.953]
Epoch [42/120    avg_loss:0.074, val_acc:0.961]
Epoch [43/120    avg_loss:0.067, val_acc:0.942]
Epoch [44/120    avg_loss:0.056, val_acc:0.972]
Epoch [45/120    avg_loss:0.043, val_acc:0.968]
Epoch [46/120    avg_loss:0.040, val_acc:0.969]
Epoch [47/120    avg_loss:0.040, val_acc:0.969]
Epoch [48/120    avg_loss:0.043, val_acc:0.949]
Epoch [49/120    avg_loss:0.063, val_acc:0.954]
Epoch [50/120    avg_loss:0.056, val_acc:0.961]
Epoch [51/120    avg_loss:0.052, val_acc:0.975]
Epoch [52/120    avg_loss:0.038, val_acc:0.968]
Epoch [53/120    avg_loss:0.035, val_acc:0.965]
Epoch [54/120    avg_loss:0.028, val_acc:0.973]
Epoch [55/120    avg_loss:0.024, val_acc:0.971]
Epoch [56/120    avg_loss:0.029, val_acc:0.978]
Epoch [57/120    avg_loss:0.048, val_acc:0.963]
Epoch [58/120    avg_loss:0.033, val_acc:0.972]
Epoch [59/120    avg_loss:0.027, val_acc:0.976]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.017, val_acc:0.975]
Epoch [62/120    avg_loss:0.014, val_acc:0.974]
Epoch [63/120    avg_loss:0.014, val_acc:0.979]
Epoch [64/120    avg_loss:0.016, val_acc:0.978]
Epoch [65/120    avg_loss:0.019, val_acc:0.977]
Epoch [66/120    avg_loss:0.016, val_acc:0.975]
Epoch [67/120    avg_loss:0.014, val_acc:0.978]
Epoch [68/120    avg_loss:0.024, val_acc:0.976]
Epoch [69/120    avg_loss:0.019, val_acc:0.970]
Epoch [70/120    avg_loss:0.016, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.978]
Epoch [72/120    avg_loss:0.010, val_acc:0.983]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.954]
Epoch [75/120    avg_loss:0.013, val_acc:0.977]
Epoch [76/120    avg_loss:0.019, val_acc:0.983]
Epoch [77/120    avg_loss:0.014, val_acc:0.979]
Epoch [78/120    avg_loss:0.013, val_acc:0.983]
Epoch [79/120    avg_loss:0.021, val_acc:0.980]
Epoch [80/120    avg_loss:0.024, val_acc:0.980]
Epoch [81/120    avg_loss:0.014, val_acc:0.983]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.014, val_acc:0.964]
Epoch [84/120    avg_loss:0.012, val_acc:0.980]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.012, val_acc:0.978]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.981]
Epoch [95/120    avg_loss:0.005, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.035, val_acc:0.963]
Epoch [102/120    avg_loss:0.025, val_acc:0.965]
Epoch [103/120    avg_loss:0.014, val_acc:0.971]
Epoch [104/120    avg_loss:0.014, val_acc:0.978]
Epoch [105/120    avg_loss:0.014, val_acc:0.978]
Epoch [106/120    avg_loss:0.013, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.979]
Epoch [108/120    avg_loss:0.007, val_acc:0.978]
Epoch [109/120    avg_loss:0.006, val_acc:0.978]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.980]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.005, val_acc:0.980]
Epoch [114/120    avg_loss:0.005, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.006, val_acc:0.980]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.005, val_acc:0.979]
Epoch [120/120    avg_loss:0.005, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6293     0     0     0     0    12    24    93    10]
 [    0     1 18064     0    25     0     0     0     0     0]
 [    0    10     0  1964     2     0     0     0    51     9]
 [    0    19     5     0  2903     0    19     2    23     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    22     5     0     0  4850     0     1     0]
 [    0    11     0     0     0     0     0  1270     0     9]
 [    0     2     0    18    55     0     0     0  3495     1]
 [    0     0     0     0     9    17     0     0     0   893]]

Accuracy:
98.9010194490637

F1 scores:
[       nan 0.98574561 0.99853514 0.97638578 0.97318136 0.99352874
 0.9939543  0.98221191 0.96627039 0.96959826]

Kappa:
0.985440723591484
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f85b1f40940>
supervision:full
center_pixel:True
Network :
Number of parameter: 16999==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.115, val_acc:0.194]
Epoch [2/120    avg_loss:1.746, val_acc:0.195]
Epoch [3/120    avg_loss:1.491, val_acc:0.218]
Epoch [4/120    avg_loss:1.344, val_acc:0.345]
Epoch [5/120    avg_loss:1.191, val_acc:0.448]
Epoch [6/120    avg_loss:1.045, val_acc:0.522]
Epoch [7/120    avg_loss:0.886, val_acc:0.572]
Epoch [8/120    avg_loss:0.776, val_acc:0.625]
Epoch [9/120    avg_loss:0.649, val_acc:0.701]
Epoch [10/120    avg_loss:0.561, val_acc:0.724]
Epoch [11/120    avg_loss:0.530, val_acc:0.733]
Epoch [12/120    avg_loss:0.446, val_acc:0.788]
Epoch [13/120    avg_loss:0.447, val_acc:0.776]
Epoch [14/120    avg_loss:0.395, val_acc:0.853]
Epoch [15/120    avg_loss:0.344, val_acc:0.845]
Epoch [16/120    avg_loss:0.298, val_acc:0.845]
Epoch [17/120    avg_loss:0.285, val_acc:0.845]
Epoch [18/120    avg_loss:0.271, val_acc:0.896]
Epoch [19/120    avg_loss:0.252, val_acc:0.905]
Epoch [20/120    avg_loss:0.206, val_acc:0.944]
Epoch [21/120    avg_loss:0.223, val_acc:0.896]
Epoch [22/120    avg_loss:0.220, val_acc:0.917]
Epoch [23/120    avg_loss:0.182, val_acc:0.949]
Epoch [24/120    avg_loss:0.149, val_acc:0.941]
Epoch [25/120    avg_loss:0.168, val_acc:0.951]
Epoch [26/120    avg_loss:0.153, val_acc:0.953]
Epoch [27/120    avg_loss:0.169, val_acc:0.946]
Epoch [28/120    avg_loss:0.132, val_acc:0.955]
Epoch [29/120    avg_loss:0.154, val_acc:0.953]
Epoch [30/120    avg_loss:0.136, val_acc:0.971]
Epoch [31/120    avg_loss:0.109, val_acc:0.934]
Epoch [32/120    avg_loss:0.108, val_acc:0.953]
Epoch [33/120    avg_loss:0.097, val_acc:0.959]
Epoch [34/120    avg_loss:0.093, val_acc:0.961]
Epoch [35/120    avg_loss:0.115, val_acc:0.958]
Epoch [36/120    avg_loss:0.084, val_acc:0.964]
Epoch [37/120    avg_loss:0.080, val_acc:0.975]
Epoch [38/120    avg_loss:0.091, val_acc:0.966]
Epoch [39/120    avg_loss:0.066, val_acc:0.968]
Epoch [40/120    avg_loss:0.058, val_acc:0.973]
Epoch [41/120    avg_loss:0.066, val_acc:0.971]
Epoch [42/120    avg_loss:0.045, val_acc:0.978]
Epoch [43/120    avg_loss:0.045, val_acc:0.975]
Epoch [44/120    avg_loss:0.039, val_acc:0.968]
Epoch [45/120    avg_loss:0.037, val_acc:0.978]
Epoch [46/120    avg_loss:0.054, val_acc:0.965]
Epoch [47/120    avg_loss:0.048, val_acc:0.975]
Epoch [48/120    avg_loss:0.057, val_acc:0.966]
Epoch [49/120    avg_loss:0.039, val_acc:0.980]
Epoch [50/120    avg_loss:0.028, val_acc:0.980]
Epoch [51/120    avg_loss:0.025, val_acc:0.979]
Epoch [52/120    avg_loss:0.026, val_acc:0.971]
Epoch [53/120    avg_loss:0.022, val_acc:0.985]
Epoch [54/120    avg_loss:0.022, val_acc:0.979]
Epoch [55/120    avg_loss:0.029, val_acc:0.981]
Epoch [56/120    avg_loss:0.021, val_acc:0.978]
Epoch [57/120    avg_loss:0.035, val_acc:0.965]
Epoch [58/120    avg_loss:0.058, val_acc:0.975]
Epoch [59/120    avg_loss:0.097, val_acc:0.979]
Epoch [60/120    avg_loss:0.042, val_acc:0.977]
Epoch [61/120    avg_loss:0.026, val_acc:0.976]
Epoch [62/120    avg_loss:0.025, val_acc:0.978]
Epoch [63/120    avg_loss:0.021, val_acc:0.978]
Epoch [64/120    avg_loss:0.019, val_acc:0.981]
Epoch [65/120    avg_loss:0.017, val_acc:0.971]
Epoch [66/120    avg_loss:0.021, val_acc:0.986]
Epoch [67/120    avg_loss:0.023, val_acc:0.976]
Epoch [68/120    avg_loss:0.021, val_acc:0.977]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.020, val_acc:0.976]
Epoch [71/120    avg_loss:0.024, val_acc:0.980]
Epoch [72/120    avg_loss:0.027, val_acc:0.972]
Epoch [73/120    avg_loss:0.057, val_acc:0.972]
Epoch [74/120    avg_loss:0.033, val_acc:0.979]
Epoch [75/120    avg_loss:0.024, val_acc:0.978]
Epoch [76/120    avg_loss:0.027, val_acc:0.971]
Epoch [77/120    avg_loss:0.022, val_acc:0.976]
Epoch [78/120    avg_loss:0.020, val_acc:0.975]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.014, val_acc:0.979]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.011, val_acc:0.983]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.981]
Epoch [89/120    avg_loss:0.011, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.010, val_acc:0.982]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.010, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     0     0     0     0     3    63     7]
 [    0     8 17905     0    76     0    87     0    14     0]
 [    0     6     0  1939     0     0     0     0    83     8]
 [    0    36     5     0  2898     0    14     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    23     0     0     0     2     0  1245     0    20]
 [    0     7     0    78    53     0     3     0  3399    31]
 [    0     0     0     0    14    19     0     0     0   886]]

Accuracy:
98.36357939893476

F1 scores:
[       nan 0.98811281 0.99472222 0.95682211 0.96391153 0.99201824
 0.98945233 0.98108747 0.95090222 0.94708712]

Kappa:
0.9783582281046113
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f277d2d99b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.069, val_acc:0.287]
Epoch [2/120    avg_loss:1.743, val_acc:0.460]
Epoch [3/120    avg_loss:1.522, val_acc:0.466]
Epoch [4/120    avg_loss:1.378, val_acc:0.517]
Epoch [5/120    avg_loss:1.264, val_acc:0.644]
Epoch [6/120    avg_loss:1.136, val_acc:0.618]
Epoch [7/120    avg_loss:1.018, val_acc:0.600]
Epoch [8/120    avg_loss:0.877, val_acc:0.616]
Epoch [9/120    avg_loss:0.756, val_acc:0.613]
Epoch [10/120    avg_loss:0.658, val_acc:0.732]
Epoch [11/120    avg_loss:0.577, val_acc:0.786]
Epoch [12/120    avg_loss:0.532, val_acc:0.802]
Epoch [13/120    avg_loss:0.476, val_acc:0.778]
Epoch [14/120    avg_loss:0.422, val_acc:0.786]
Epoch [15/120    avg_loss:0.391, val_acc:0.807]
Epoch [16/120    avg_loss:0.356, val_acc:0.851]
Epoch [17/120    avg_loss:0.335, val_acc:0.860]
Epoch [18/120    avg_loss:0.328, val_acc:0.875]
Epoch [19/120    avg_loss:0.294, val_acc:0.890]
Epoch [20/120    avg_loss:0.305, val_acc:0.901]
Epoch [21/120    avg_loss:0.205, val_acc:0.936]
Epoch [22/120    avg_loss:0.192, val_acc:0.921]
Epoch [23/120    avg_loss:0.170, val_acc:0.921]
Epoch [24/120    avg_loss:0.171, val_acc:0.935]
Epoch [25/120    avg_loss:0.162, val_acc:0.935]
Epoch [26/120    avg_loss:0.153, val_acc:0.912]
Epoch [27/120    avg_loss:0.122, val_acc:0.955]
Epoch [28/120    avg_loss:0.142, val_acc:0.945]
Epoch [29/120    avg_loss:0.124, val_acc:0.951]
Epoch [30/120    avg_loss:0.112, val_acc:0.951]
Epoch [31/120    avg_loss:0.091, val_acc:0.965]
Epoch [32/120    avg_loss:0.096, val_acc:0.957]
Epoch [33/120    avg_loss:0.099, val_acc:0.963]
Epoch [34/120    avg_loss:0.082, val_acc:0.960]
Epoch [35/120    avg_loss:0.089, val_acc:0.963]
Epoch [36/120    avg_loss:0.100, val_acc:0.949]
Epoch [37/120    avg_loss:0.121, val_acc:0.964]
Epoch [38/120    avg_loss:0.073, val_acc:0.973]
Epoch [39/120    avg_loss:0.068, val_acc:0.960]
Epoch [40/120    avg_loss:0.076, val_acc:0.965]
Epoch [41/120    avg_loss:0.068, val_acc:0.943]
Epoch [42/120    avg_loss:0.101, val_acc:0.973]
Epoch [43/120    avg_loss:0.054, val_acc:0.977]
Epoch [44/120    avg_loss:0.045, val_acc:0.960]
Epoch [45/120    avg_loss:0.061, val_acc:0.911]
Epoch [46/120    avg_loss:0.047, val_acc:0.981]
Epoch [47/120    avg_loss:0.049, val_acc:0.912]
Epoch [48/120    avg_loss:0.059, val_acc:0.967]
Epoch [49/120    avg_loss:0.041, val_acc:0.973]
Epoch [50/120    avg_loss:0.028, val_acc:0.981]
Epoch [51/120    avg_loss:0.026, val_acc:0.980]
Epoch [52/120    avg_loss:0.027, val_acc:0.973]
Epoch [53/120    avg_loss:0.027, val_acc:0.968]
Epoch [54/120    avg_loss:0.024, val_acc:0.975]
Epoch [55/120    avg_loss:0.046, val_acc:0.973]
Epoch [56/120    avg_loss:0.035, val_acc:0.981]
Epoch [57/120    avg_loss:0.028, val_acc:0.985]
Epoch [58/120    avg_loss:0.046, val_acc:0.978]
Epoch [59/120    avg_loss:0.037, val_acc:0.973]
Epoch [60/120    avg_loss:0.040, val_acc:0.977]
Epoch [61/120    avg_loss:0.033, val_acc:0.983]
Epoch [62/120    avg_loss:0.020, val_acc:0.978]
Epoch [63/120    avg_loss:0.015, val_acc:0.982]
Epoch [64/120    avg_loss:0.037, val_acc:0.966]
Epoch [65/120    avg_loss:0.036, val_acc:0.973]
Epoch [66/120    avg_loss:0.033, val_acc:0.976]
Epoch [67/120    avg_loss:0.029, val_acc:0.978]
Epoch [68/120    avg_loss:0.030, val_acc:0.978]
Epoch [69/120    avg_loss:0.019, val_acc:0.985]
Epoch [70/120    avg_loss:0.023, val_acc:0.973]
Epoch [71/120    avg_loss:0.028, val_acc:0.979]
Epoch [72/120    avg_loss:0.020, val_acc:0.982]
Epoch [73/120    avg_loss:0.023, val_acc:0.955]
Epoch [74/120    avg_loss:0.045, val_acc:0.973]
Epoch [75/120    avg_loss:0.044, val_acc:0.973]
Epoch [76/120    avg_loss:0.019, val_acc:0.979]
Epoch [77/120    avg_loss:0.026, val_acc:0.976]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.014, val_acc:0.983]
Epoch [81/120    avg_loss:0.014, val_acc:0.978]
Epoch [82/120    avg_loss:0.024, val_acc:0.980]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.981]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6415     0     0     1     0     1     1     7     7]
 [    0     1 17952     0   114     0    23     0     0     0]
 [    0    11     0  1953     0     0     0     0    66     6]
 [    0    33    16     0  2901     0     5     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    25     3     0     0  4850     0     0     0]
 [    0     0     0     0     0     0     1  1286     0     3]
 [    0    25     0    19    59     0     0     0  3467     1]
 [    0     0     0     1    14    39     0     0     0   865]]

Accuracy:
98.79738751114645

F1 scores:
[       nan 0.99326469 0.99503922 0.97357926 0.95726778 0.98527746
 0.99405616 0.99805976 0.97278339 0.96057746]

Kappa:
0.9840795514059694
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea65b96978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.054, val_acc:0.113]
Epoch [2/120    avg_loss:1.755, val_acc:0.152]
Epoch [3/120    avg_loss:1.570, val_acc:0.171]
Epoch [4/120    avg_loss:1.417, val_acc:0.309]
Epoch [5/120    avg_loss:1.258, val_acc:0.341]
Epoch [6/120    avg_loss:1.158, val_acc:0.405]
Epoch [7/120    avg_loss:1.030, val_acc:0.430]
Epoch [8/120    avg_loss:0.926, val_acc:0.455]
Epoch [9/120    avg_loss:0.846, val_acc:0.529]
Epoch [10/120    avg_loss:0.699, val_acc:0.571]
Epoch [11/120    avg_loss:0.633, val_acc:0.681]
Epoch [12/120    avg_loss:0.580, val_acc:0.740]
Epoch [13/120    avg_loss:0.531, val_acc:0.701]
Epoch [14/120    avg_loss:0.466, val_acc:0.741]
Epoch [15/120    avg_loss:0.430, val_acc:0.726]
Epoch [16/120    avg_loss:0.394, val_acc:0.725]
Epoch [17/120    avg_loss:0.381, val_acc:0.795]
Epoch [18/120    avg_loss:0.335, val_acc:0.828]
Epoch [19/120    avg_loss:0.327, val_acc:0.823]
Epoch [20/120    avg_loss:0.299, val_acc:0.819]
Epoch [21/120    avg_loss:0.278, val_acc:0.838]
Epoch [22/120    avg_loss:0.254, val_acc:0.882]
Epoch [23/120    avg_loss:0.247, val_acc:0.908]
Epoch [24/120    avg_loss:0.211, val_acc:0.935]
Epoch [25/120    avg_loss:0.207, val_acc:0.898]
Epoch [26/120    avg_loss:0.177, val_acc:0.947]
Epoch [27/120    avg_loss:0.184, val_acc:0.941]
Epoch [28/120    avg_loss:0.140, val_acc:0.872]
Epoch [29/120    avg_loss:0.141, val_acc:0.941]
Epoch [30/120    avg_loss:0.134, val_acc:0.913]
Epoch [31/120    avg_loss:0.115, val_acc:0.951]
Epoch [32/120    avg_loss:0.108, val_acc:0.950]
Epoch [33/120    avg_loss:0.116, val_acc:0.889]
Epoch [34/120    avg_loss:0.117, val_acc:0.967]
Epoch [35/120    avg_loss:0.071, val_acc:0.972]
Epoch [36/120    avg_loss:0.088, val_acc:0.946]
Epoch [37/120    avg_loss:0.071, val_acc:0.969]
Epoch [38/120    avg_loss:0.059, val_acc:0.964]
Epoch [39/120    avg_loss:0.073, val_acc:0.960]
Epoch [40/120    avg_loss:0.095, val_acc:0.944]
Epoch [41/120    avg_loss:0.088, val_acc:0.964]
Epoch [42/120    avg_loss:0.076, val_acc:0.962]
Epoch [43/120    avg_loss:0.054, val_acc:0.976]
Epoch [44/120    avg_loss:0.039, val_acc:0.980]
Epoch [45/120    avg_loss:0.048, val_acc:0.976]
Epoch [46/120    avg_loss:0.046, val_acc:0.979]
Epoch [47/120    avg_loss:0.078, val_acc:0.952]
Epoch [48/120    avg_loss:0.074, val_acc:0.968]
Epoch [49/120    avg_loss:0.063, val_acc:0.980]
Epoch [50/120    avg_loss:0.047, val_acc:0.979]
Epoch [51/120    avg_loss:0.040, val_acc:0.982]
Epoch [52/120    avg_loss:0.029, val_acc:0.979]
Epoch [53/120    avg_loss:0.028, val_acc:0.987]
Epoch [54/120    avg_loss:0.027, val_acc:0.973]
Epoch [55/120    avg_loss:0.025, val_acc:0.984]
Epoch [56/120    avg_loss:0.027, val_acc:0.981]
Epoch [57/120    avg_loss:0.036, val_acc:0.967]
Epoch [58/120    avg_loss:0.030, val_acc:0.986]
Epoch [59/120    avg_loss:0.024, val_acc:0.980]
Epoch [60/120    avg_loss:0.029, val_acc:0.981]
Epoch [61/120    avg_loss:0.025, val_acc:0.973]
Epoch [62/120    avg_loss:0.038, val_acc:0.976]
Epoch [63/120    avg_loss:0.022, val_acc:0.984]
Epoch [64/120    avg_loss:0.037, val_acc:0.979]
Epoch [65/120    avg_loss:0.037, val_acc:0.981]
Epoch [66/120    avg_loss:0.028, val_acc:0.979]
Epoch [67/120    avg_loss:0.020, val_acc:0.984]
Epoch [68/120    avg_loss:0.019, val_acc:0.985]
Epoch [69/120    avg_loss:0.016, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.986]
Epoch [72/120    avg_loss:0.019, val_acc:0.986]
Epoch [73/120    avg_loss:0.012, val_acc:0.985]
Epoch [74/120    avg_loss:0.015, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.011, val_acc:0.987]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.012, val_acc:0.987]
Epoch [82/120    avg_loss:0.015, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.012, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.987]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.987]
Epoch [90/120    avg_loss:0.012, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.012, val_acc:0.987]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.010, val_acc:0.987]
Epoch [96/120    avg_loss:0.014, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.009, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.011, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.987]
Epoch [106/120    avg_loss:0.010, val_acc:0.987]
Epoch [107/120    avg_loss:0.013, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.012, val_acc:0.987]
Epoch [110/120    avg_loss:0.013, val_acc:0.987]
Epoch [111/120    avg_loss:0.012, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.987]
Epoch [113/120    avg_loss:0.012, val_acc:0.987]
Epoch [114/120    avg_loss:0.010, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.987]
Epoch [117/120    avg_loss:0.009, val_acc:0.987]
Epoch [118/120    avg_loss:0.012, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     1     0     0     0     0     1    81     0]
 [    0     2 18047     0    27     0     5     0     9     0]
 [    0     6     0  1997     0     0     0     0    30     3]
 [    0    50    18     0  2883     0    16     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     8     0     0  4847     0    10     0]
 [    0    13     0     0     0     0     0  1277     0     0]
 [    0    11     0    17    51     0     0     0  3492     0]
 [    0     1     0     0    14    21     0     0     0   883]]

Accuracy:
99.00465138698094

F1 scores:
[       nan 0.98709577 0.9979264  0.98422868 0.96956449 0.99201824
 0.99466448 0.99454829 0.97026952 0.97839335]

Kappa:
0.9868119003574927
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f31a1710908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.094, val_acc:0.504]
Epoch [2/120    avg_loss:1.772, val_acc:0.548]
Epoch [3/120    avg_loss:1.539, val_acc:0.553]
Epoch [4/120    avg_loss:1.378, val_acc:0.465]
Epoch [5/120    avg_loss:1.210, val_acc:0.469]
Epoch [6/120    avg_loss:1.071, val_acc:0.565]
Epoch [7/120    avg_loss:0.957, val_acc:0.604]
Epoch [8/120    avg_loss:0.859, val_acc:0.712]
Epoch [9/120    avg_loss:0.741, val_acc:0.672]
Epoch [10/120    avg_loss:0.621, val_acc:0.717]
Epoch [11/120    avg_loss:0.547, val_acc:0.782]
Epoch [12/120    avg_loss:0.460, val_acc:0.792]
Epoch [13/120    avg_loss:0.404, val_acc:0.802]
Epoch [14/120    avg_loss:0.354, val_acc:0.828]
Epoch [15/120    avg_loss:0.308, val_acc:0.828]
Epoch [16/120    avg_loss:0.259, val_acc:0.865]
Epoch [17/120    avg_loss:0.231, val_acc:0.932]
Epoch [18/120    avg_loss:0.213, val_acc:0.922]
Epoch [19/120    avg_loss:0.224, val_acc:0.882]
Epoch [20/120    avg_loss:0.189, val_acc:0.938]
Epoch [21/120    avg_loss:0.148, val_acc:0.937]
Epoch [22/120    avg_loss:0.153, val_acc:0.917]
Epoch [23/120    avg_loss:0.161, val_acc:0.932]
Epoch [24/120    avg_loss:0.141, val_acc:0.936]
Epoch [25/120    avg_loss:0.146, val_acc:0.906]
Epoch [26/120    avg_loss:0.117, val_acc:0.935]
Epoch [27/120    avg_loss:0.106, val_acc:0.948]
Epoch [28/120    avg_loss:0.094, val_acc:0.951]
Epoch [29/120    avg_loss:0.091, val_acc:0.943]
Epoch [30/120    avg_loss:0.083, val_acc:0.962]
Epoch [31/120    avg_loss:0.086, val_acc:0.951]
Epoch [32/120    avg_loss:0.091, val_acc:0.950]
Epoch [33/120    avg_loss:0.070, val_acc:0.966]
Epoch [34/120    avg_loss:0.073, val_acc:0.963]
Epoch [35/120    avg_loss:0.064, val_acc:0.963]
Epoch [36/120    avg_loss:0.069, val_acc:0.965]
Epoch [37/120    avg_loss:0.047, val_acc:0.974]
Epoch [38/120    avg_loss:0.047, val_acc:0.972]
Epoch [39/120    avg_loss:0.043, val_acc:0.967]
Epoch [40/120    avg_loss:0.041, val_acc:0.922]
Epoch [41/120    avg_loss:0.060, val_acc:0.971]
Epoch [42/120    avg_loss:0.063, val_acc:0.953]
Epoch [43/120    avg_loss:0.053, val_acc:0.970]
Epoch [44/120    avg_loss:0.051, val_acc:0.966]
Epoch [45/120    avg_loss:0.044, val_acc:0.974]
Epoch [46/120    avg_loss:0.050, val_acc:0.952]
Epoch [47/120    avg_loss:0.042, val_acc:0.973]
Epoch [48/120    avg_loss:0.044, val_acc:0.978]
Epoch [49/120    avg_loss:0.036, val_acc:0.977]
Epoch [50/120    avg_loss:0.034, val_acc:0.976]
Epoch [51/120    avg_loss:0.031, val_acc:0.980]
Epoch [52/120    avg_loss:0.032, val_acc:0.973]
Epoch [53/120    avg_loss:0.034, val_acc:0.978]
Epoch [54/120    avg_loss:0.032, val_acc:0.967]
Epoch [55/120    avg_loss:0.023, val_acc:0.959]
Epoch [56/120    avg_loss:0.041, val_acc:0.960]
Epoch [57/120    avg_loss:0.024, val_acc:0.983]
Epoch [58/120    avg_loss:0.022, val_acc:0.978]
Epoch [59/120    avg_loss:0.027, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.984]
Epoch [61/120    avg_loss:0.034, val_acc:0.970]
Epoch [62/120    avg_loss:0.051, val_acc:0.964]
Epoch [63/120    avg_loss:0.032, val_acc:0.970]
Epoch [64/120    avg_loss:0.021, val_acc:0.978]
Epoch [65/120    avg_loss:0.016, val_acc:0.981]
Epoch [66/120    avg_loss:0.016, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.972]
Epoch [68/120    avg_loss:0.022, val_acc:0.977]
Epoch [69/120    avg_loss:0.022, val_acc:0.975]
Epoch [70/120    avg_loss:0.019, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.981]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.028, val_acc:0.970]
Epoch [74/120    avg_loss:0.025, val_acc:0.979]
Epoch [75/120    avg_loss:0.014, val_acc:0.981]
Epoch [76/120    avg_loss:0.025, val_acc:0.982]
Epoch [77/120    avg_loss:0.017, val_acc:0.984]
Epoch [78/120    avg_loss:0.013, val_acc:0.988]
Epoch [79/120    avg_loss:0.022, val_acc:0.977]
Epoch [80/120    avg_loss:0.028, val_acc:0.972]
Epoch [81/120    avg_loss:0.021, val_acc:0.979]
Epoch [82/120    avg_loss:0.018, val_acc:0.983]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.017, val_acc:0.983]
Epoch [90/120    avg_loss:0.012, val_acc:0.980]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.978]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     1     0     0     0    30     1]
 [    0     0 18053     0    29     0     4     0     4     0]
 [    0     9     0  2015     0     0     0     0    12     0]
 [    0    25     1     0  2917     0    11     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     9     0     0  4859     0     4     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     3     0    18    53     0     0     0  3485    12]
 [    0     0     0     0    13    31     0     0     0   875]]

Accuracy:
99.2842166148507

F1 scores:
[       nan 0.99463828 0.99878285 0.98822952 0.97477026 0.98826202
 0.99651354 0.99883586 0.97838293 0.96685083]

Kappa:
0.9905187171874373
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0967602978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.118, val_acc:0.133]
Epoch [2/120    avg_loss:1.811, val_acc:0.126]
Epoch [3/120    avg_loss:1.630, val_acc:0.264]
Epoch [4/120    avg_loss:1.417, val_acc:0.341]
Epoch [5/120    avg_loss:1.275, val_acc:0.416]
Epoch [6/120    avg_loss:1.237, val_acc:0.414]
Epoch [7/120    avg_loss:1.089, val_acc:0.441]
Epoch [8/120    avg_loss:0.960, val_acc:0.561]
Epoch [9/120    avg_loss:0.859, val_acc:0.609]
Epoch [10/120    avg_loss:0.746, val_acc:0.648]
Epoch [11/120    avg_loss:0.617, val_acc:0.648]
Epoch [12/120    avg_loss:0.531, val_acc:0.723]
Epoch [13/120    avg_loss:0.485, val_acc:0.713]
Epoch [14/120    avg_loss:0.445, val_acc:0.815]
Epoch [15/120    avg_loss:0.405, val_acc:0.775]
Epoch [16/120    avg_loss:0.394, val_acc:0.791]
Epoch [17/120    avg_loss:0.369, val_acc:0.812]
Epoch [18/120    avg_loss:0.342, val_acc:0.817]
Epoch [19/120    avg_loss:0.302, val_acc:0.784]
Epoch [20/120    avg_loss:0.276, val_acc:0.853]
Epoch [21/120    avg_loss:0.270, val_acc:0.828]
Epoch [22/120    avg_loss:0.224, val_acc:0.874]
Epoch [23/120    avg_loss:0.212, val_acc:0.935]
Epoch [24/120    avg_loss:0.179, val_acc:0.920]
Epoch [25/120    avg_loss:0.188, val_acc:0.936]
Epoch [26/120    avg_loss:0.169, val_acc:0.929]
Epoch [27/120    avg_loss:0.155, val_acc:0.957]
Epoch [28/120    avg_loss:0.126, val_acc:0.959]
Epoch [29/120    avg_loss:0.105, val_acc:0.943]
Epoch [30/120    avg_loss:0.129, val_acc:0.942]
Epoch [31/120    avg_loss:0.102, val_acc:0.922]
Epoch [32/120    avg_loss:0.092, val_acc:0.946]
Epoch [33/120    avg_loss:0.069, val_acc:0.939]
Epoch [34/120    avg_loss:0.084, val_acc:0.960]
Epoch [35/120    avg_loss:0.086, val_acc:0.966]
Epoch [36/120    avg_loss:0.107, val_acc:0.943]
Epoch [37/120    avg_loss:0.088, val_acc:0.959]
Epoch [38/120    avg_loss:0.080, val_acc:0.961]
Epoch [39/120    avg_loss:0.064, val_acc:0.972]
Epoch [40/120    avg_loss:0.051, val_acc:0.954]
Epoch [41/120    avg_loss:0.052, val_acc:0.970]
Epoch [42/120    avg_loss:0.051, val_acc:0.948]
Epoch [43/120    avg_loss:0.041, val_acc:0.972]
Epoch [44/120    avg_loss:0.043, val_acc:0.972]
Epoch [45/120    avg_loss:0.049, val_acc:0.970]
Epoch [46/120    avg_loss:0.035, val_acc:0.978]
Epoch [47/120    avg_loss:0.052, val_acc:0.952]
Epoch [48/120    avg_loss:0.056, val_acc:0.970]
Epoch [49/120    avg_loss:0.037, val_acc:0.961]
Epoch [50/120    avg_loss:0.028, val_acc:0.972]
Epoch [51/120    avg_loss:0.033, val_acc:0.977]
Epoch [52/120    avg_loss:0.033, val_acc:0.972]
Epoch [53/120    avg_loss:0.033, val_acc:0.969]
Epoch [54/120    avg_loss:0.036, val_acc:0.973]
Epoch [55/120    avg_loss:0.036, val_acc:0.978]
Epoch [56/120    avg_loss:0.024, val_acc:0.978]
Epoch [57/120    avg_loss:0.022, val_acc:0.971]
Epoch [58/120    avg_loss:0.021, val_acc:0.937]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.019, val_acc:0.978]
Epoch [61/120    avg_loss:0.015, val_acc:0.978]
Epoch [62/120    avg_loss:0.013, val_acc:0.978]
Epoch [63/120    avg_loss:0.012, val_acc:0.978]
Epoch [64/120    avg_loss:0.016, val_acc:0.976]
Epoch [65/120    avg_loss:0.013, val_acc:0.978]
Epoch [66/120    avg_loss:0.012, val_acc:0.978]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.013, val_acc:0.978]
Epoch [69/120    avg_loss:0.011, val_acc:0.978]
Epoch [70/120    avg_loss:0.011, val_acc:0.978]
Epoch [71/120    avg_loss:0.012, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.011, val_acc:0.977]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.010, val_acc:0.978]
Epoch [78/120    avg_loss:0.014, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.015, val_acc:0.979]
Epoch [81/120    avg_loss:0.015, val_acc:0.980]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.012, val_acc:0.979]
Epoch [85/120    avg_loss:0.013, val_acc:0.978]
Epoch [86/120    avg_loss:0.010, val_acc:0.978]
Epoch [87/120    avg_loss:0.010, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.013, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.011, val_acc:0.980]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.979]
Epoch [102/120    avg_loss:0.009, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.009, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.011, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     0     1     0     0     1    77     3]
 [    0     0 18021     0    60     0     9     0     0     0]
 [    0     3     0  2002     0     0     0     0    30     1]
 [    0    29    13     0  2893     0    11     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     3     0     0  4864     0     7     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    35     0    65    51     0     0     0  3416     4]
 [    0     0     0     0    14    31     0     0     0   874]]

Accuracy:
98.84317836743547

F1 scores:
[       nan 0.98840377 0.99761957 0.9751583  0.96578201 0.98826202
 0.99651711 0.99883676 0.95860811 0.96949529]

Kappa:
0.9846813834333391
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe5b420978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.086, val_acc:0.079]
Epoch [2/120    avg_loss:1.767, val_acc:0.147]
Epoch [3/120    avg_loss:1.551, val_acc:0.259]
Epoch [4/120    avg_loss:1.402, val_acc:0.297]
Epoch [5/120    avg_loss:1.252, val_acc:0.357]
Epoch [6/120    avg_loss:1.130, val_acc:0.401]
Epoch [7/120    avg_loss:1.020, val_acc:0.434]
Epoch [8/120    avg_loss:0.901, val_acc:0.701]
Epoch [9/120    avg_loss:0.846, val_acc:0.785]
Epoch [10/120    avg_loss:0.733, val_acc:0.740]
Epoch [11/120    avg_loss:0.636, val_acc:0.799]
Epoch [12/120    avg_loss:0.556, val_acc:0.766]
Epoch [13/120    avg_loss:0.485, val_acc:0.753]
Epoch [14/120    avg_loss:0.431, val_acc:0.803]
Epoch [15/120    avg_loss:0.378, val_acc:0.816]
Epoch [16/120    avg_loss:0.347, val_acc:0.838]
Epoch [17/120    avg_loss:0.324, val_acc:0.817]
Epoch [18/120    avg_loss:0.330, val_acc:0.820]
Epoch [19/120    avg_loss:0.282, val_acc:0.782]
Epoch [20/120    avg_loss:0.269, val_acc:0.900]
Epoch [21/120    avg_loss:0.256, val_acc:0.861]
Epoch [22/120    avg_loss:0.210, val_acc:0.896]
Epoch [23/120    avg_loss:0.253, val_acc:0.882]
Epoch [24/120    avg_loss:0.231, val_acc:0.906]
Epoch [25/120    avg_loss:0.201, val_acc:0.843]
Epoch [26/120    avg_loss:0.174, val_acc:0.919]
Epoch [27/120    avg_loss:0.144, val_acc:0.941]
Epoch [28/120    avg_loss:0.154, val_acc:0.944]
Epoch [29/120    avg_loss:0.133, val_acc:0.950]
Epoch [30/120    avg_loss:0.120, val_acc:0.952]
Epoch [31/120    avg_loss:0.118, val_acc:0.921]
Epoch [32/120    avg_loss:0.125, val_acc:0.947]
Epoch [33/120    avg_loss:0.114, val_acc:0.959]
Epoch [34/120    avg_loss:0.101, val_acc:0.959]
Epoch [35/120    avg_loss:0.098, val_acc:0.959]
Epoch [36/120    avg_loss:0.091, val_acc:0.957]
Epoch [37/120    avg_loss:0.094, val_acc:0.969]
Epoch [38/120    avg_loss:0.076, val_acc:0.969]
Epoch [39/120    avg_loss:0.084, val_acc:0.952]
Epoch [40/120    avg_loss:0.160, val_acc:0.851]
Epoch [41/120    avg_loss:0.101, val_acc:0.957]
Epoch [42/120    avg_loss:0.075, val_acc:0.955]
Epoch [43/120    avg_loss:0.062, val_acc:0.966]
Epoch [44/120    avg_loss:0.056, val_acc:0.961]
Epoch [45/120    avg_loss:0.050, val_acc:0.954]
Epoch [46/120    avg_loss:0.107, val_acc:0.929]
Epoch [47/120    avg_loss:0.106, val_acc:0.965]
Epoch [48/120    avg_loss:0.057, val_acc:0.972]
Epoch [49/120    avg_loss:0.056, val_acc:0.959]
Epoch [50/120    avg_loss:0.080, val_acc:0.962]
Epoch [51/120    avg_loss:0.063, val_acc:0.968]
Epoch [52/120    avg_loss:0.044, val_acc:0.967]
Epoch [53/120    avg_loss:0.044, val_acc:0.966]
Epoch [54/120    avg_loss:0.038, val_acc:0.979]
Epoch [55/120    avg_loss:0.041, val_acc:0.973]
Epoch [56/120    avg_loss:0.053, val_acc:0.971]
Epoch [57/120    avg_loss:0.032, val_acc:0.970]
Epoch [58/120    avg_loss:0.028, val_acc:0.972]
Epoch [59/120    avg_loss:0.046, val_acc:0.972]
Epoch [60/120    avg_loss:0.040, val_acc:0.968]
Epoch [61/120    avg_loss:0.036, val_acc:0.969]
Epoch [62/120    avg_loss:0.026, val_acc:0.976]
Epoch [63/120    avg_loss:0.030, val_acc:0.978]
Epoch [64/120    avg_loss:0.025, val_acc:0.972]
Epoch [65/120    avg_loss:0.032, val_acc:0.965]
Epoch [66/120    avg_loss:0.038, val_acc:0.973]
Epoch [67/120    avg_loss:0.034, val_acc:0.978]
Epoch [68/120    avg_loss:0.021, val_acc:0.979]
Epoch [69/120    avg_loss:0.021, val_acc:0.981]
Epoch [70/120    avg_loss:0.022, val_acc:0.981]
Epoch [71/120    avg_loss:0.019, val_acc:0.981]
Epoch [72/120    avg_loss:0.016, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.982]
Epoch [74/120    avg_loss:0.017, val_acc:0.981]
Epoch [75/120    avg_loss:0.018, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.982]
Epoch [77/120    avg_loss:0.015, val_acc:0.979]
Epoch [78/120    avg_loss:0.018, val_acc:0.979]
Epoch [79/120    avg_loss:0.016, val_acc:0.981]
Epoch [80/120    avg_loss:0.015, val_acc:0.982]
Epoch [81/120    avg_loss:0.013, val_acc:0.982]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.016, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.982]
Epoch [85/120    avg_loss:0.018, val_acc:0.979]
Epoch [86/120    avg_loss:0.014, val_acc:0.979]
Epoch [87/120    avg_loss:0.016, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.014, val_acc:0.981]
Epoch [91/120    avg_loss:0.012, val_acc:0.982]
Epoch [92/120    avg_loss:0.015, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.981]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.981]
Epoch [97/120    avg_loss:0.013, val_acc:0.980]
Epoch [98/120    avg_loss:0.015, val_acc:0.982]
Epoch [99/120    avg_loss:0.014, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.982]
Epoch [101/120    avg_loss:0.013, val_acc:0.981]
Epoch [102/120    avg_loss:0.016, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.015, val_acc:0.981]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.981]
Epoch [107/120    avg_loss:0.011, val_acc:0.981]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.016, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.012, val_acc:0.981]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     1     0     0    17    18     2]
 [    0     0 17929     0    42     0   114     0     5     0]
 [    0     2     0  1981     0     0     0     0    40    13]
 [    0    41    14     0  2896     0     7     0    14     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     3     7     0     0  4868     0     0     0]
 [    0     0     0     0     0     0     0  1283     0     7]
 [    0    34     0    28    52     0     0     0  3453     4]
 [    0     0     0     0    16    26     0     0     0   877]]

Accuracy:
98.77569710553587

F1 scores:
[       nan 0.99108734 0.9950605  0.97778875 0.96872387 0.98975332
 0.98672342 0.99073359 0.97253908 0.9621503 ]

Kappa:
0.9838004873091661
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f333f76d978>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.111, val_acc:0.139]
Epoch [2/120    avg_loss:1.809, val_acc:0.164]
Epoch [3/120    avg_loss:1.628, val_acc:0.303]
Epoch [4/120    avg_loss:1.422, val_acc:0.367]
Epoch [5/120    avg_loss:1.280, val_acc:0.404]
Epoch [6/120    avg_loss:1.135, val_acc:0.451]
Epoch [7/120    avg_loss:1.011, val_acc:0.491]
Epoch [8/120    avg_loss:0.847, val_acc:0.616]
Epoch [9/120    avg_loss:0.748, val_acc:0.702]
Epoch [10/120    avg_loss:0.621, val_acc:0.703]
Epoch [11/120    avg_loss:0.550, val_acc:0.789]
Epoch [12/120    avg_loss:0.504, val_acc:0.781]
Epoch [13/120    avg_loss:0.448, val_acc:0.798]
Epoch [14/120    avg_loss:0.413, val_acc:0.816]
Epoch [15/120    avg_loss:0.387, val_acc:0.826]
Epoch [16/120    avg_loss:0.339, val_acc:0.816]
Epoch [17/120    avg_loss:0.300, val_acc:0.836]
Epoch [18/120    avg_loss:0.285, val_acc:0.831]
Epoch [19/120    avg_loss:0.253, val_acc:0.882]
Epoch [20/120    avg_loss:0.227, val_acc:0.891]
Epoch [21/120    avg_loss:0.256, val_acc:0.855]
Epoch [22/120    avg_loss:0.233, val_acc:0.866]
Epoch [23/120    avg_loss:0.221, val_acc:0.917]
Epoch [24/120    avg_loss:0.161, val_acc:0.922]
Epoch [25/120    avg_loss:0.176, val_acc:0.918]
Epoch [26/120    avg_loss:0.167, val_acc:0.934]
Epoch [27/120    avg_loss:0.132, val_acc:0.944]
Epoch [28/120    avg_loss:0.114, val_acc:0.954]
Epoch [29/120    avg_loss:0.102, val_acc:0.953]
Epoch [30/120    avg_loss:0.143, val_acc:0.928]
Epoch [31/120    avg_loss:0.101, val_acc:0.959]
Epoch [32/120    avg_loss:0.093, val_acc:0.950]
Epoch [33/120    avg_loss:0.104, val_acc:0.960]
Epoch [34/120    avg_loss:0.067, val_acc:0.969]
Epoch [35/120    avg_loss:0.087, val_acc:0.948]
Epoch [36/120    avg_loss:0.096, val_acc:0.959]
Epoch [37/120    avg_loss:0.069, val_acc:0.966]
Epoch [38/120    avg_loss:0.050, val_acc:0.959]
Epoch [39/120    avg_loss:0.058, val_acc:0.960]
Epoch [40/120    avg_loss:0.119, val_acc:0.946]
Epoch [41/120    avg_loss:0.088, val_acc:0.965]
Epoch [42/120    avg_loss:0.061, val_acc:0.972]
Epoch [43/120    avg_loss:0.060, val_acc:0.971]
Epoch [44/120    avg_loss:0.049, val_acc:0.965]
Epoch [45/120    avg_loss:0.038, val_acc:0.974]
Epoch [46/120    avg_loss:0.032, val_acc:0.973]
Epoch [47/120    avg_loss:0.029, val_acc:0.975]
Epoch [48/120    avg_loss:0.044, val_acc:0.966]
Epoch [49/120    avg_loss:0.048, val_acc:0.969]
Epoch [50/120    avg_loss:0.041, val_acc:0.967]
Epoch [51/120    avg_loss:0.031, val_acc:0.967]
Epoch [52/120    avg_loss:0.040, val_acc:0.979]
Epoch [53/120    avg_loss:0.020, val_acc:0.983]
Epoch [54/120    avg_loss:0.019, val_acc:0.982]
Epoch [55/120    avg_loss:0.019, val_acc:0.984]
Epoch [56/120    avg_loss:0.021, val_acc:0.984]
Epoch [57/120    avg_loss:0.032, val_acc:0.962]
Epoch [58/120    avg_loss:0.041, val_acc:0.978]
Epoch [59/120    avg_loss:0.039, val_acc:0.982]
Epoch [60/120    avg_loss:0.021, val_acc:0.979]
Epoch [61/120    avg_loss:0.016, val_acc:0.984]
Epoch [62/120    avg_loss:0.015, val_acc:0.986]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.021, val_acc:0.982]
Epoch [65/120    avg_loss:0.030, val_acc:0.980]
Epoch [66/120    avg_loss:0.022, val_acc:0.987]
Epoch [67/120    avg_loss:0.027, val_acc:0.969]
Epoch [68/120    avg_loss:0.025, val_acc:0.978]
Epoch [69/120    avg_loss:0.025, val_acc:0.981]
Epoch [70/120    avg_loss:0.020, val_acc:0.984]
Epoch [71/120    avg_loss:0.016, val_acc:0.983]
Epoch [72/120    avg_loss:0.012, val_acc:0.984]
Epoch [73/120    avg_loss:0.013, val_acc:0.986]
Epoch [74/120    avg_loss:0.010, val_acc:0.985]
Epoch [75/120    avg_loss:0.014, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.015, val_acc:0.980]
Epoch [78/120    avg_loss:0.016, val_acc:0.987]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.014, val_acc:0.991]
Epoch [82/120    avg_loss:0.012, val_acc:0.966]
Epoch [83/120    avg_loss:0.016, val_acc:0.956]
Epoch [84/120    avg_loss:0.012, val_acc:0.989]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.982]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.015, val_acc:0.984]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.020, val_acc:0.987]
Epoch [96/120    avg_loss:0.008, val_acc:0.991]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     0     0     0     1    47     5]
 [    0     5 17995     0    75     0     3     0    12     0]
 [    0     3     0  2001     0     0     0     0    27     5]
 [    0    28    14     0  2911     0     9     0     9     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     4     0     0  4849     0     6     0]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    11     0    10    50     0     0     0  3488    12]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
99.02875183765937

F1 scores:
[       nan 0.99222274 0.99645606 0.98790422 0.96678844 0.98788796
 0.99579012 0.99922481 0.97430168 0.96145374]

Kappa:
0.9871399581757757
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f95afc69940>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.101, val_acc:0.066]
Epoch [2/120    avg_loss:1.778, val_acc:0.254]
Epoch [3/120    avg_loss:1.589, val_acc:0.299]
Epoch [4/120    avg_loss:1.446, val_acc:0.347]
Epoch [5/120    avg_loss:1.352, val_acc:0.390]
Epoch [6/120    avg_loss:1.205, val_acc:0.414]
Epoch [7/120    avg_loss:1.112, val_acc:0.488]
Epoch [8/120    avg_loss:0.988, val_acc:0.461]
Epoch [9/120    avg_loss:0.897, val_acc:0.463]
Epoch [10/120    avg_loss:0.794, val_acc:0.553]
Epoch [11/120    avg_loss:0.669, val_acc:0.549]
Epoch [12/120    avg_loss:0.647, val_acc:0.722]
Epoch [13/120    avg_loss:0.558, val_acc:0.768]
Epoch [14/120    avg_loss:0.472, val_acc:0.782]
Epoch [15/120    avg_loss:0.423, val_acc:0.794]
Epoch [16/120    avg_loss:0.376, val_acc:0.833]
Epoch [17/120    avg_loss:0.343, val_acc:0.870]
Epoch [18/120    avg_loss:0.296, val_acc:0.860]
Epoch [19/120    avg_loss:0.288, val_acc:0.884]
Epoch [20/120    avg_loss:0.274, val_acc:0.898]
Epoch [21/120    avg_loss:0.237, val_acc:0.901]
Epoch [22/120    avg_loss:0.205, val_acc:0.946]
Epoch [23/120    avg_loss:0.207, val_acc:0.943]
Epoch [24/120    avg_loss:0.165, val_acc:0.947]
Epoch [25/120    avg_loss:0.147, val_acc:0.956]
Epoch [26/120    avg_loss:0.150, val_acc:0.934]
Epoch [27/120    avg_loss:0.135, val_acc:0.954]
Epoch [28/120    avg_loss:0.109, val_acc:0.970]
Epoch [29/120    avg_loss:0.124, val_acc:0.959]
Epoch [30/120    avg_loss:0.112, val_acc:0.971]
Epoch [31/120    avg_loss:0.126, val_acc:0.972]
Epoch [32/120    avg_loss:0.087, val_acc:0.961]
Epoch [33/120    avg_loss:0.096, val_acc:0.971]
Epoch [34/120    avg_loss:0.101, val_acc:0.960]
Epoch [35/120    avg_loss:0.093, val_acc:0.952]
Epoch [36/120    avg_loss:0.072, val_acc:0.977]
Epoch [37/120    avg_loss:0.078, val_acc:0.982]
Epoch [38/120    avg_loss:0.060, val_acc:0.979]
Epoch [39/120    avg_loss:0.078, val_acc:0.977]
Epoch [40/120    avg_loss:0.091, val_acc:0.972]
Epoch [41/120    avg_loss:0.066, val_acc:0.979]
Epoch [42/120    avg_loss:0.062, val_acc:0.968]
Epoch [43/120    avg_loss:0.063, val_acc:0.978]
Epoch [44/120    avg_loss:0.051, val_acc:0.984]
Epoch [45/120    avg_loss:0.049, val_acc:0.980]
Epoch [46/120    avg_loss:0.048, val_acc:0.984]
Epoch [47/120    avg_loss:0.042, val_acc:0.987]
Epoch [48/120    avg_loss:0.047, val_acc:0.970]
Epoch [49/120    avg_loss:0.044, val_acc:0.982]
Epoch [50/120    avg_loss:0.031, val_acc:0.987]
Epoch [51/120    avg_loss:0.049, val_acc:0.985]
Epoch [52/120    avg_loss:0.049, val_acc:0.988]
Epoch [53/120    avg_loss:0.033, val_acc:0.988]
Epoch [54/120    avg_loss:0.028, val_acc:0.984]
Epoch [55/120    avg_loss:0.031, val_acc:0.986]
Epoch [56/120    avg_loss:0.033, val_acc:0.987]
Epoch [57/120    avg_loss:0.031, val_acc:0.991]
Epoch [58/120    avg_loss:0.026, val_acc:0.987]
Epoch [59/120    avg_loss:0.029, val_acc:0.986]
Epoch [60/120    avg_loss:0.023, val_acc:0.979]
Epoch [61/120    avg_loss:0.015, val_acc:0.991]
Epoch [62/120    avg_loss:0.031, val_acc:0.987]
Epoch [63/120    avg_loss:0.025, val_acc:0.981]
Epoch [64/120    avg_loss:0.026, val_acc:0.989]
Epoch [65/120    avg_loss:0.035, val_acc:0.981]
Epoch [66/120    avg_loss:0.029, val_acc:0.985]
Epoch [67/120    avg_loss:0.021, val_acc:0.990]
Epoch [68/120    avg_loss:0.013, val_acc:0.991]
Epoch [69/120    avg_loss:0.013, val_acc:0.992]
Epoch [70/120    avg_loss:0.013, val_acc:0.990]
Epoch [71/120    avg_loss:0.018, val_acc:0.992]
Epoch [72/120    avg_loss:0.018, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.989]
Epoch [74/120    avg_loss:0.014, val_acc:0.964]
Epoch [75/120    avg_loss:0.023, val_acc:0.991]
Epoch [76/120    avg_loss:0.022, val_acc:0.991]
Epoch [77/120    avg_loss:0.019, val_acc:0.993]
Epoch [78/120    avg_loss:0.013, val_acc:0.991]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.990]
Epoch [81/120    avg_loss:0.020, val_acc:0.991]
Epoch [82/120    avg_loss:0.013, val_acc:0.990]
Epoch [83/120    avg_loss:0.029, val_acc:0.986]
Epoch [84/120    avg_loss:0.016, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.989]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.989]
Epoch [90/120    avg_loss:0.047, val_acc:0.950]
Epoch [91/120    avg_loss:0.054, val_acc:0.977]
Epoch [92/120    avg_loss:0.024, val_acc:0.984]
Epoch [93/120    avg_loss:0.017, val_acc:0.987]
Epoch [94/120    avg_loss:0.013, val_acc:0.988]
Epoch [95/120    avg_loss:0.016, val_acc:0.988]
Epoch [96/120    avg_loss:0.012, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.989]
Epoch [98/120    avg_loss:0.012, val_acc:0.991]
Epoch [99/120    avg_loss:0.012, val_acc:0.991]
Epoch [100/120    avg_loss:0.013, val_acc:0.991]
Epoch [101/120    avg_loss:0.012, val_acc:0.991]
Epoch [102/120    avg_loss:0.009, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.992]
Epoch [105/120    avg_loss:0.009, val_acc:0.992]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.008, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.991]
Epoch [109/120    avg_loss:0.009, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.010, val_acc:0.991]
Epoch [112/120    avg_loss:0.009, val_acc:0.991]
Epoch [113/120    avg_loss:0.008, val_acc:0.991]
Epoch [114/120    avg_loss:0.010, val_acc:0.991]
Epoch [115/120    avg_loss:0.010, val_acc:0.991]
Epoch [116/120    avg_loss:0.010, val_acc:0.991]
Epoch [117/120    avg_loss:0.010, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.008, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6377     0     0     0     0     3     0    52     0]
 [    0     1 18053     0    36     0     0     0     0     0]
 [    0     8     0  1990     0     0     0     0    34     4]
 [    0    30    13     0  2898     0    13     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4861     0    17     0]
 [    0    10     0     0     0     0     3  1277     0     0]
 [    0    24     0     9    55     0     2     0  3481     0]
 [    0     0     0     0    14    14     0     0     0   891]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.99006365 0.9986171  0.98636927 0.97004184 0.99466463
 0.99610656 0.99493572 0.97058413 0.98235943]

Kappa:
0.9885052588859218
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f376cac5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.006, val_acc:0.098]
Epoch [2/120    avg_loss:1.711, val_acc:0.191]
Epoch [3/120    avg_loss:1.489, val_acc:0.322]
Epoch [4/120    avg_loss:1.300, val_acc:0.375]
Epoch [5/120    avg_loss:1.162, val_acc:0.408]
Epoch [6/120    avg_loss:1.006, val_acc:0.425]
Epoch [7/120    avg_loss:0.929, val_acc:0.465]
Epoch [8/120    avg_loss:0.800, val_acc:0.635]
Epoch [9/120    avg_loss:0.682, val_acc:0.639]
Epoch [10/120    avg_loss:0.585, val_acc:0.696]
Epoch [11/120    avg_loss:0.527, val_acc:0.707]
Epoch [12/120    avg_loss:0.434, val_acc:0.764]
Epoch [13/120    avg_loss:0.395, val_acc:0.768]
Epoch [14/120    avg_loss:0.389, val_acc:0.802]
Epoch [15/120    avg_loss:0.349, val_acc:0.810]
Epoch [16/120    avg_loss:0.307, val_acc:0.838]
Epoch [17/120    avg_loss:0.267, val_acc:0.832]
Epoch [18/120    avg_loss:0.265, val_acc:0.868]
Epoch [19/120    avg_loss:0.252, val_acc:0.828]
Epoch [20/120    avg_loss:0.242, val_acc:0.818]
Epoch [21/120    avg_loss:0.223, val_acc:0.876]
Epoch [22/120    avg_loss:0.202, val_acc:0.884]
Epoch [23/120    avg_loss:0.191, val_acc:0.879]
Epoch [24/120    avg_loss:0.160, val_acc:0.925]
Epoch [25/120    avg_loss:0.177, val_acc:0.945]
Epoch [26/120    avg_loss:0.165, val_acc:0.937]
Epoch [27/120    avg_loss:0.134, val_acc:0.960]
Epoch [28/120    avg_loss:0.108, val_acc:0.953]
Epoch [29/120    avg_loss:0.116, val_acc:0.914]
Epoch [30/120    avg_loss:0.116, val_acc:0.943]
Epoch [31/120    avg_loss:0.111, val_acc:0.956]
Epoch [32/120    avg_loss:0.111, val_acc:0.966]
Epoch [33/120    avg_loss:0.101, val_acc:0.910]
Epoch [34/120    avg_loss:0.080, val_acc:0.959]
Epoch [35/120    avg_loss:0.077, val_acc:0.965]
Epoch [36/120    avg_loss:0.074, val_acc:0.966]
Epoch [37/120    avg_loss:0.055, val_acc:0.955]
Epoch [38/120    avg_loss:0.070, val_acc:0.957]
Epoch [39/120    avg_loss:0.091, val_acc:0.953]
Epoch [40/120    avg_loss:0.062, val_acc:0.942]
Epoch [41/120    avg_loss:0.048, val_acc:0.971]
Epoch [42/120    avg_loss:0.044, val_acc:0.972]
Epoch [43/120    avg_loss:0.058, val_acc:0.960]
Epoch [44/120    avg_loss:0.043, val_acc:0.970]
Epoch [45/120    avg_loss:0.041, val_acc:0.975]
Epoch [46/120    avg_loss:0.034, val_acc:0.979]
Epoch [47/120    avg_loss:0.025, val_acc:0.974]
Epoch [48/120    avg_loss:0.038, val_acc:0.975]
Epoch [49/120    avg_loss:0.029, val_acc:0.977]
Epoch [50/120    avg_loss:0.043, val_acc:0.975]
Epoch [51/120    avg_loss:0.044, val_acc:0.966]
Epoch [52/120    avg_loss:0.064, val_acc:0.964]
Epoch [53/120    avg_loss:0.045, val_acc:0.975]
Epoch [54/120    avg_loss:0.030, val_acc:0.978]
Epoch [55/120    avg_loss:0.029, val_acc:0.978]
Epoch [56/120    avg_loss:0.024, val_acc:0.981]
Epoch [57/120    avg_loss:0.026, val_acc:0.972]
Epoch [58/120    avg_loss:0.020, val_acc:0.982]
Epoch [59/120    avg_loss:0.022, val_acc:0.983]
Epoch [60/120    avg_loss:0.019, val_acc:0.973]
Epoch [61/120    avg_loss:0.025, val_acc:0.966]
Epoch [62/120    avg_loss:0.030, val_acc:0.979]
Epoch [63/120    avg_loss:0.018, val_acc:0.972]
Epoch [64/120    avg_loss:0.020, val_acc:0.982]
Epoch [65/120    avg_loss:0.020, val_acc:0.977]
Epoch [66/120    avg_loss:0.036, val_acc:0.983]
Epoch [67/120    avg_loss:0.043, val_acc:0.953]
Epoch [68/120    avg_loss:0.025, val_acc:0.981]
Epoch [69/120    avg_loss:0.021, val_acc:0.977]
Epoch [70/120    avg_loss:0.026, val_acc:0.975]
Epoch [71/120    avg_loss:0.019, val_acc:0.983]
Epoch [72/120    avg_loss:0.020, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.986]
Epoch [75/120    avg_loss:0.023, val_acc:0.973]
Epoch [76/120    avg_loss:0.046, val_acc:0.972]
Epoch [77/120    avg_loss:0.031, val_acc:0.977]
Epoch [78/120    avg_loss:0.037, val_acc:0.978]
Epoch [79/120    avg_loss:0.028, val_acc:0.973]
Epoch [80/120    avg_loss:0.021, val_acc:0.979]
Epoch [81/120    avg_loss:0.017, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.978]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.015, val_acc:0.981]
Epoch [87/120    avg_loss:0.013, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     0     0     0     0     1    46     0]
 [    0     0 18036     0    33     0    19     0     2     0]
 [    0     0     0  1986     3     0     0     0    47     0]
 [    0    27     9     0  2902     0     9     0    23     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     4     0     0  4865     0     0     0]
 [    0     9     0     0     0     0     0  1277     0     4]
 [    0     3     0    47    49     0     0     0  3460    12]
 [    0     0     0     0    10    29     0     0     0   880]]

Accuracy:
99.04321210806643

F1 scores:
[       nan 0.99331052 0.99800797 0.97520255 0.97235718 0.98901099
 0.99580391 0.99454829 0.96796755 0.96862961]

Kappa:
0.987327104334597
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7299d9d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.062, val_acc:0.253]
Epoch [2/120    avg_loss:1.766, val_acc:0.380]
Epoch [3/120    avg_loss:1.597, val_acc:0.529]
Epoch [4/120    avg_loss:1.402, val_acc:0.604]
Epoch [5/120    avg_loss:1.265, val_acc:0.628]
Epoch [6/120    avg_loss:1.115, val_acc:0.559]
Epoch [7/120    avg_loss:0.990, val_acc:0.540]
Epoch [8/120    avg_loss:0.883, val_acc:0.666]
Epoch [9/120    avg_loss:0.753, val_acc:0.738]
Epoch [10/120    avg_loss:0.641, val_acc:0.718]
Epoch [11/120    avg_loss:0.550, val_acc:0.768]
Epoch [12/120    avg_loss:0.467, val_acc:0.809]
Epoch [13/120    avg_loss:0.412, val_acc:0.834]
Epoch [14/120    avg_loss:0.365, val_acc:0.809]
Epoch [15/120    avg_loss:0.349, val_acc:0.849]
Epoch [16/120    avg_loss:0.311, val_acc:0.866]
Epoch [17/120    avg_loss:0.297, val_acc:0.919]
Epoch [18/120    avg_loss:0.247, val_acc:0.905]
Epoch [19/120    avg_loss:0.242, val_acc:0.928]
Epoch [20/120    avg_loss:0.217, val_acc:0.910]
Epoch [21/120    avg_loss:0.191, val_acc:0.936]
Epoch [22/120    avg_loss:0.157, val_acc:0.950]
Epoch [23/120    avg_loss:0.158, val_acc:0.944]
Epoch [24/120    avg_loss:0.189, val_acc:0.941]
Epoch [25/120    avg_loss:0.140, val_acc:0.947]
Epoch [26/120    avg_loss:0.126, val_acc:0.938]
Epoch [27/120    avg_loss:0.142, val_acc:0.947]
Epoch [28/120    avg_loss:0.088, val_acc:0.953]
Epoch [29/120    avg_loss:0.105, val_acc:0.958]
Epoch [30/120    avg_loss:0.092, val_acc:0.955]
Epoch [31/120    avg_loss:0.067, val_acc:0.967]
Epoch [32/120    avg_loss:0.066, val_acc:0.966]
Epoch [33/120    avg_loss:0.082, val_acc:0.948]
Epoch [34/120    avg_loss:0.082, val_acc:0.963]
Epoch [35/120    avg_loss:0.060, val_acc:0.962]
Epoch [36/120    avg_loss:0.071, val_acc:0.969]
Epoch [37/120    avg_loss:0.051, val_acc:0.972]
Epoch [38/120    avg_loss:0.039, val_acc:0.978]
Epoch [39/120    avg_loss:0.045, val_acc:0.972]
Epoch [40/120    avg_loss:0.061, val_acc:0.967]
Epoch [41/120    avg_loss:0.056, val_acc:0.972]
Epoch [42/120    avg_loss:0.052, val_acc:0.973]
Epoch [43/120    avg_loss:0.061, val_acc:0.966]
Epoch [44/120    avg_loss:0.064, val_acc:0.966]
Epoch [45/120    avg_loss:0.044, val_acc:0.972]
Epoch [46/120    avg_loss:0.029, val_acc:0.983]
Epoch [47/120    avg_loss:0.028, val_acc:0.970]
Epoch [48/120    avg_loss:0.033, val_acc:0.968]
Epoch [49/120    avg_loss:0.071, val_acc:0.965]
Epoch [50/120    avg_loss:0.039, val_acc:0.981]
Epoch [51/120    avg_loss:0.024, val_acc:0.976]
Epoch [52/120    avg_loss:0.027, val_acc:0.983]
Epoch [53/120    avg_loss:0.022, val_acc:0.975]
Epoch [54/120    avg_loss:0.024, val_acc:0.972]
Epoch [55/120    avg_loss:0.030, val_acc:0.951]
Epoch [56/120    avg_loss:0.027, val_acc:0.979]
Epoch [57/120    avg_loss:0.020, val_acc:0.975]
Epoch [58/120    avg_loss:0.020, val_acc:0.981]
Epoch [59/120    avg_loss:0.014, val_acc:0.980]
Epoch [60/120    avg_loss:0.022, val_acc:0.975]
Epoch [61/120    avg_loss:0.028, val_acc:0.978]
Epoch [62/120    avg_loss:0.020, val_acc:0.982]
Epoch [63/120    avg_loss:0.017, val_acc:0.979]
Epoch [64/120    avg_loss:0.047, val_acc:0.963]
Epoch [65/120    avg_loss:0.039, val_acc:0.975]
Epoch [66/120    avg_loss:0.027, val_acc:0.980]
Epoch [67/120    avg_loss:0.022, val_acc:0.983]
Epoch [68/120    avg_loss:0.020, val_acc:0.982]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.014, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.981]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.016, val_acc:0.982]
Epoch [74/120    avg_loss:0.017, val_acc:0.982]
Epoch [75/120    avg_loss:0.015, val_acc:0.983]
Epoch [76/120    avg_loss:0.015, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.014, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.013, val_acc:0.982]
Epoch [82/120    avg_loss:0.011, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.012, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.985]
Epoch [92/120    avg_loss:0.016, val_acc:0.984]
Epoch [93/120    avg_loss:0.011, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.013, val_acc:0.985]
Epoch [99/120    avg_loss:0.014, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.010, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     0     0     0     0     0    70     0]
 [    0     0 18017     0    44     0    27     0     2     0]
 [    0    11     0  1966     0     0     0     0    56     3]
 [    0    33     8     0  2911     0     0     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4869     0     0     2]
 [    0     1     0     0     0    10     2  1275     0     2]
 [    0    22     0    31    56     0     0     0  3449    13]
 [    0     0     0     0    17    23     0     0     0   879]]

Accuracy:
98.89137926879233

F1 scores:
[       nan 0.98934764 0.99756381 0.97495661 0.97033333 0.98751419
 0.99611293 0.99415205 0.96233259 0.9669967 ]

Kappa:
0.9853189838569733
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc938ec9908>
supervision:full
center_pixel:True
Network :
Number of parameter: 18919==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.119, val_acc:0.083]
Epoch [2/120    avg_loss:1.774, val_acc:0.116]
Epoch [3/120    avg_loss:1.601, val_acc:0.134]
Epoch [4/120    avg_loss:1.447, val_acc:0.279]
Epoch [5/120    avg_loss:1.318, val_acc:0.324]
Epoch [6/120    avg_loss:1.187, val_acc:0.376]
Epoch [7/120    avg_loss:1.059, val_acc:0.390]
Epoch [8/120    avg_loss:0.970, val_acc:0.455]
Epoch [9/120    avg_loss:0.876, val_acc:0.475]
Epoch [10/120    avg_loss:0.812, val_acc:0.557]
Epoch [11/120    avg_loss:0.694, val_acc:0.608]
Epoch [12/120    avg_loss:0.629, val_acc:0.625]
Epoch [13/120    avg_loss:0.554, val_acc:0.653]
Epoch [14/120    avg_loss:0.532, val_acc:0.661]
Epoch [15/120    avg_loss:0.439, val_acc:0.737]
Epoch [16/120    avg_loss:0.386, val_acc:0.754]
Epoch [17/120    avg_loss:0.370, val_acc:0.797]
Epoch [18/120    avg_loss:0.335, val_acc:0.747]
Epoch [19/120    avg_loss:0.305, val_acc:0.801]
Epoch [20/120    avg_loss:0.305, val_acc:0.829]
Epoch [21/120    avg_loss:0.257, val_acc:0.815]
Epoch [22/120    avg_loss:0.281, val_acc:0.791]
Epoch [23/120    avg_loss:0.253, val_acc:0.872]
Epoch [24/120    avg_loss:0.203, val_acc:0.936]
Epoch [25/120    avg_loss:0.218, val_acc:0.891]
Epoch [26/120    avg_loss:0.212, val_acc:0.942]
Epoch [27/120    avg_loss:0.160, val_acc:0.937]
Epoch [28/120    avg_loss:0.136, val_acc:0.951]
Epoch [29/120    avg_loss:0.133, val_acc:0.959]
Epoch [30/120    avg_loss:0.129, val_acc:0.955]
Epoch [31/120    avg_loss:0.111, val_acc:0.960]
Epoch [32/120    avg_loss:0.118, val_acc:0.960]
Epoch [33/120    avg_loss:0.103, val_acc:0.964]
Epoch [34/120    avg_loss:0.107, val_acc:0.956]
Epoch [35/120    avg_loss:0.114, val_acc:0.953]
Epoch [36/120    avg_loss:0.113, val_acc:0.951]
Epoch [37/120    avg_loss:0.081, val_acc:0.958]
Epoch [38/120    avg_loss:0.073, val_acc:0.971]
Epoch [39/120    avg_loss:0.084, val_acc:0.947]
Epoch [40/120    avg_loss:0.080, val_acc:0.929]
Epoch [41/120    avg_loss:0.112, val_acc:0.943]
Epoch [42/120    avg_loss:0.063, val_acc:0.974]
Epoch [43/120    avg_loss:0.045, val_acc:0.978]
Epoch [44/120    avg_loss:0.043, val_acc:0.982]
Epoch [45/120    avg_loss:0.044, val_acc:0.975]
Epoch [46/120    avg_loss:0.033, val_acc:0.979]
Epoch [47/120    avg_loss:0.041, val_acc:0.981]
Epoch [48/120    avg_loss:0.056, val_acc:0.909]
Epoch [49/120    avg_loss:0.071, val_acc:0.966]
Epoch [50/120    avg_loss:0.043, val_acc:0.972]
Epoch [51/120    avg_loss:0.041, val_acc:0.972]
Epoch [52/120    avg_loss:0.041, val_acc:0.984]
Epoch [53/120    avg_loss:0.033, val_acc:0.979]
Epoch [54/120    avg_loss:0.025, val_acc:0.984]
Epoch [55/120    avg_loss:0.028, val_acc:0.984]
Epoch [56/120    avg_loss:0.031, val_acc:0.979]
Epoch [57/120    avg_loss:0.027, val_acc:0.987]
Epoch [58/120    avg_loss:0.022, val_acc:0.986]
Epoch [59/120    avg_loss:0.054, val_acc:0.953]
Epoch [60/120    avg_loss:0.066, val_acc:0.981]
Epoch [61/120    avg_loss:0.027, val_acc:0.979]
Epoch [62/120    avg_loss:0.037, val_acc:0.969]
Epoch [63/120    avg_loss:0.029, val_acc:0.983]
Epoch [64/120    avg_loss:0.028, val_acc:0.965]
Epoch [65/120    avg_loss:0.022, val_acc:0.984]
Epoch [66/120    avg_loss:0.023, val_acc:0.978]
Epoch [67/120    avg_loss:0.042, val_acc:0.983]
Epoch [68/120    avg_loss:0.032, val_acc:0.979]
Epoch [69/120    avg_loss:0.028, val_acc:0.970]
Epoch [70/120    avg_loss:0.030, val_acc:0.965]
Epoch [71/120    avg_loss:0.020, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.986]
Epoch [73/120    avg_loss:0.016, val_acc:0.984]
Epoch [74/120    avg_loss:0.013, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.987]
Epoch [77/120    avg_loss:0.012, val_acc:0.986]
Epoch [78/120    avg_loss:0.013, val_acc:0.987]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.013, val_acc:0.986]
Epoch [82/120    avg_loss:0.018, val_acc:0.987]
Epoch [83/120    avg_loss:0.014, val_acc:0.989]
Epoch [84/120    avg_loss:0.012, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.989]
Epoch [86/120    avg_loss:0.013, val_acc:0.988]
Epoch [87/120    avg_loss:0.012, val_acc:0.988]
Epoch [88/120    avg_loss:0.012, val_acc:0.989]
Epoch [89/120    avg_loss:0.011, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.989]
Epoch [91/120    avg_loss:0.011, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.989]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.011, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.989]
Epoch [96/120    avg_loss:0.011, val_acc:0.989]
Epoch [97/120    avg_loss:0.010, val_acc:0.989]
Epoch [98/120    avg_loss:0.012, val_acc:0.989]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.011, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.989]
Epoch [103/120    avg_loss:0.011, val_acc:0.989]
Epoch [104/120    avg_loss:0.010, val_acc:0.989]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.989]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.013, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     4     0     8     3    33     2]
 [    0     0 18019     0    58     0     8     0     5     0]
 [    0     4     0  1975     2     0     0     0    53     2]
 [    0    22    18     0  2894     0    12     0    23     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     2  1281     0     7]
 [    0     8     0     7    63     0     0     0  3491     2]
 [    0     0     0     0    15    26     0     0     0   878]]

Accuracy:
99.06008242354132

F1 scores:
[       nan 0.99346202 0.99753647 0.98307616 0.96338216 0.99013657
 0.9969344  0.995338   0.97296544 0.9685604 ]

Kappa:
0.9875523305978632
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a2f93d978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.177, val_acc:0.115]
Epoch [2/120    avg_loss:1.801, val_acc:0.191]
Epoch [3/120    avg_loss:1.630, val_acc:0.241]
Epoch [4/120    avg_loss:1.479, val_acc:0.266]
Epoch [5/120    avg_loss:1.344, val_acc:0.328]
Epoch [6/120    avg_loss:1.205, val_acc:0.358]
Epoch [7/120    avg_loss:1.096, val_acc:0.384]
Epoch [8/120    avg_loss:0.976, val_acc:0.577]
Epoch [9/120    avg_loss:0.859, val_acc:0.604]
Epoch [10/120    avg_loss:0.782, val_acc:0.696]
Epoch [11/120    avg_loss:0.695, val_acc:0.747]
Epoch [12/120    avg_loss:0.676, val_acc:0.795]
Epoch [13/120    avg_loss:0.559, val_acc:0.816]
Epoch [14/120    avg_loss:0.524, val_acc:0.848]
Epoch [15/120    avg_loss:0.443, val_acc:0.787]
Epoch [16/120    avg_loss:0.439, val_acc:0.833]
Epoch [17/120    avg_loss:0.400, val_acc:0.723]
Epoch [18/120    avg_loss:0.381, val_acc:0.834]
Epoch [19/120    avg_loss:0.347, val_acc:0.914]
Epoch [20/120    avg_loss:0.270, val_acc:0.905]
Epoch [21/120    avg_loss:0.227, val_acc:0.924]
Epoch [22/120    avg_loss:0.236, val_acc:0.916]
Epoch [23/120    avg_loss:0.226, val_acc:0.921]
Epoch [24/120    avg_loss:0.183, val_acc:0.899]
Epoch [25/120    avg_loss:0.189, val_acc:0.910]
Epoch [26/120    avg_loss:0.179, val_acc:0.953]
Epoch [27/120    avg_loss:0.133, val_acc:0.926]
Epoch [28/120    avg_loss:0.125, val_acc:0.936]
Epoch [29/120    avg_loss:0.114, val_acc:0.929]
Epoch [30/120    avg_loss:0.102, val_acc:0.930]
Epoch [31/120    avg_loss:0.126, val_acc:0.952]
Epoch [32/120    avg_loss:0.116, val_acc:0.958]
Epoch [33/120    avg_loss:0.103, val_acc:0.959]
Epoch [34/120    avg_loss:0.075, val_acc:0.969]
Epoch [35/120    avg_loss:0.107, val_acc:0.951]
Epoch [36/120    avg_loss:0.112, val_acc:0.922]
Epoch [37/120    avg_loss:0.076, val_acc:0.962]
Epoch [38/120    avg_loss:0.064, val_acc:0.966]
Epoch [39/120    avg_loss:0.056, val_acc:0.959]
Epoch [40/120    avg_loss:0.073, val_acc:0.971]
Epoch [41/120    avg_loss:0.058, val_acc:0.968]
Epoch [42/120    avg_loss:0.064, val_acc:0.976]
Epoch [43/120    avg_loss:0.058, val_acc:0.960]
Epoch [44/120    avg_loss:0.044, val_acc:0.976]
Epoch [45/120    avg_loss:0.035, val_acc:0.972]
Epoch [46/120    avg_loss:0.045, val_acc:0.978]
Epoch [47/120    avg_loss:0.051, val_acc:0.967]
Epoch [48/120    avg_loss:0.047, val_acc:0.977]
Epoch [49/120    avg_loss:0.042, val_acc:0.977]
Epoch [50/120    avg_loss:0.044, val_acc:0.979]
Epoch [51/120    avg_loss:0.033, val_acc:0.969]
Epoch [52/120    avg_loss:0.107, val_acc:0.945]
Epoch [53/120    avg_loss:0.112, val_acc:0.964]
Epoch [54/120    avg_loss:0.055, val_acc:0.962]
Epoch [55/120    avg_loss:0.048, val_acc:0.973]
Epoch [56/120    avg_loss:0.039, val_acc:0.973]
Epoch [57/120    avg_loss:0.032, val_acc:0.975]
Epoch [58/120    avg_loss:0.035, val_acc:0.973]
Epoch [59/120    avg_loss:1.153, val_acc:0.479]
Epoch [60/120    avg_loss:1.224, val_acc:0.492]
Epoch [61/120    avg_loss:1.136, val_acc:0.489]
Epoch [62/120    avg_loss:1.057, val_acc:0.507]
Epoch [63/120    avg_loss:0.986, val_acc:0.535]
Epoch [64/120    avg_loss:0.955, val_acc:0.550]
Epoch [65/120    avg_loss:0.911, val_acc:0.551]
Epoch [66/120    avg_loss:0.915, val_acc:0.541]
Epoch [67/120    avg_loss:0.937, val_acc:0.563]
Epoch [68/120    avg_loss:0.882, val_acc:0.546]
Epoch [69/120    avg_loss:0.912, val_acc:0.555]
Epoch [70/120    avg_loss:0.914, val_acc:0.560]
Epoch [71/120    avg_loss:0.922, val_acc:0.567]
Epoch [72/120    avg_loss:0.881, val_acc:0.572]
Epoch [73/120    avg_loss:0.904, val_acc:0.574]
Epoch [74/120    avg_loss:0.844, val_acc:0.579]
Epoch [75/120    avg_loss:0.876, val_acc:0.575]
Epoch [76/120    avg_loss:0.876, val_acc:0.589]
Epoch [77/120    avg_loss:0.861, val_acc:0.588]
Epoch [78/120    avg_loss:0.883, val_acc:0.586]
Epoch [79/120    avg_loss:0.868, val_acc:0.589]
Epoch [80/120    avg_loss:0.882, val_acc:0.589]
Epoch [81/120    avg_loss:0.849, val_acc:0.590]
Epoch [82/120    avg_loss:0.860, val_acc:0.588]
Epoch [83/120    avg_loss:0.841, val_acc:0.587]
Epoch [84/120    avg_loss:0.848, val_acc:0.590]
Epoch [85/120    avg_loss:0.859, val_acc:0.590]
Epoch [86/120    avg_loss:0.848, val_acc:0.590]
Epoch [87/120    avg_loss:0.840, val_acc:0.592]
Epoch [88/120    avg_loss:0.862, val_acc:0.590]
Epoch [89/120    avg_loss:0.854, val_acc:0.591]
Epoch [90/120    avg_loss:0.863, val_acc:0.591]
Epoch [91/120    avg_loss:0.878, val_acc:0.591]
Epoch [92/120    avg_loss:0.850, val_acc:0.591]
Epoch [93/120    avg_loss:0.843, val_acc:0.591]
Epoch [94/120    avg_loss:0.862, val_acc:0.591]
Epoch [95/120    avg_loss:0.852, val_acc:0.591]
Epoch [96/120    avg_loss:0.888, val_acc:0.591]
Epoch [97/120    avg_loss:0.865, val_acc:0.591]
Epoch [98/120    avg_loss:0.870, val_acc:0.591]
Epoch [99/120    avg_loss:0.849, val_acc:0.591]
Epoch [100/120    avg_loss:0.853, val_acc:0.592]
Epoch [101/120    avg_loss:0.846, val_acc:0.591]
Epoch [102/120    avg_loss:0.844, val_acc:0.591]
Epoch [103/120    avg_loss:0.876, val_acc:0.591]
Epoch [104/120    avg_loss:0.846, val_acc:0.591]
Epoch [105/120    avg_loss:0.842, val_acc:0.591]
Epoch [106/120    avg_loss:0.867, val_acc:0.591]
Epoch [107/120    avg_loss:0.882, val_acc:0.591]
Epoch [108/120    avg_loss:0.857, val_acc:0.591]
Epoch [109/120    avg_loss:0.863, val_acc:0.591]
Epoch [110/120    avg_loss:0.846, val_acc:0.591]
Epoch [111/120    avg_loss:0.842, val_acc:0.591]
Epoch [112/120    avg_loss:0.862, val_acc:0.591]
Epoch [113/120    avg_loss:0.841, val_acc:0.591]
Epoch [114/120    avg_loss:0.875, val_acc:0.591]
Epoch [115/120    avg_loss:0.848, val_acc:0.591]
Epoch [116/120    avg_loss:0.847, val_acc:0.591]
Epoch [117/120    avg_loss:0.854, val_acc:0.591]
Epoch [118/120    avg_loss:0.870, val_acc:0.591]
Epoch [119/120    avg_loss:0.846, val_acc:0.591]
Epoch [120/120    avg_loss:0.893, val_acc:0.591]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0]
 [   0 3558  716  160  475    0  625  176  515  207]
 [   0 2695 9600  417  604    0 4602    0  172    0]
 [   0    5    0 1494   26    0   19    0  386  106]
 [   0   41  499    0 2269    0  102    0   52    9]
 [   0    0    0    0    0 1298    0    7    0    0]
 [   0    0  387  252  172    0 4067    0    0    0]
 [   0  165    0   11   13    0    0 1062    0   39]
 [   0  161  278   41   99    0   59    0 2929    4]
 [   0   19    3    3   23  125   30    0    2  714]]

Accuracy:
65.04952642614417

F1 scores:
[       nan 0.54420312 0.64924086 0.67693702 0.6820983  0.9516129
 0.56556807 0.83786982 0.76806084 0.71471471]

Kappa:
0.5671390989355137
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd6fb658940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.080, val_acc:0.079]
Epoch [2/120    avg_loss:1.832, val_acc:0.136]
Epoch [3/120    avg_loss:1.683, val_acc:0.159]
Epoch [4/120    avg_loss:1.536, val_acc:0.306]
Epoch [5/120    avg_loss:1.400, val_acc:0.424]
Epoch [6/120    avg_loss:1.263, val_acc:0.370]
Epoch [7/120    avg_loss:1.208, val_acc:0.517]
Epoch [8/120    avg_loss:1.082, val_acc:0.537]
Epoch [9/120    avg_loss:0.984, val_acc:0.591]
Epoch [10/120    avg_loss:0.856, val_acc:0.565]
Epoch [11/120    avg_loss:0.760, val_acc:0.618]
Epoch [12/120    avg_loss:0.692, val_acc:0.623]
Epoch [13/120    avg_loss:0.561, val_acc:0.677]
Epoch [14/120    avg_loss:0.505, val_acc:0.728]
Epoch [15/120    avg_loss:0.473, val_acc:0.779]
Epoch [16/120    avg_loss:0.406, val_acc:0.796]
Epoch [17/120    avg_loss:0.373, val_acc:0.804]
Epoch [18/120    avg_loss:0.354, val_acc:0.817]
Epoch [19/120    avg_loss:0.291, val_acc:0.854]
Epoch [20/120    avg_loss:0.320, val_acc:0.862]
Epoch [21/120    avg_loss:0.268, val_acc:0.875]
Epoch [22/120    avg_loss:0.280, val_acc:0.873]
Epoch [23/120    avg_loss:0.226, val_acc:0.902]
Epoch [24/120    avg_loss:0.202, val_acc:0.928]
Epoch [25/120    avg_loss:0.183, val_acc:0.928]
Epoch [26/120    avg_loss:0.146, val_acc:0.960]
Epoch [27/120    avg_loss:0.137, val_acc:0.953]
Epoch [28/120    avg_loss:0.135, val_acc:0.921]
Epoch [29/120    avg_loss:0.163, val_acc:0.945]
Epoch [30/120    avg_loss:0.132, val_acc:0.958]
Epoch [31/120    avg_loss:0.117, val_acc:0.947]
Epoch [32/120    avg_loss:0.109, val_acc:0.954]
Epoch [33/120    avg_loss:0.111, val_acc:0.950]
Epoch [34/120    avg_loss:0.097, val_acc:0.957]
Epoch [35/120    avg_loss:0.099, val_acc:0.967]
Epoch [36/120    avg_loss:0.075, val_acc:0.959]
Epoch [37/120    avg_loss:0.076, val_acc:0.962]
Epoch [38/120    avg_loss:0.077, val_acc:0.960]
Epoch [39/120    avg_loss:0.086, val_acc:0.975]
Epoch [40/120    avg_loss:0.138, val_acc:0.951]
Epoch [41/120    avg_loss:0.139, val_acc:0.928]
Epoch [42/120    avg_loss:0.105, val_acc:0.970]
Epoch [43/120    avg_loss:0.081, val_acc:0.970]
Epoch [44/120    avg_loss:0.069, val_acc:0.970]
Epoch [45/120    avg_loss:0.058, val_acc:0.950]
Epoch [46/120    avg_loss:0.061, val_acc:0.977]
Epoch [47/120    avg_loss:0.045, val_acc:0.978]
Epoch [48/120    avg_loss:0.045, val_acc:0.966]
Epoch [49/120    avg_loss:0.063, val_acc:0.974]
Epoch [50/120    avg_loss:0.055, val_acc:0.971]
Epoch [51/120    avg_loss:0.077, val_acc:0.978]
Epoch [52/120    avg_loss:0.070, val_acc:0.978]
Epoch [53/120    avg_loss:0.042, val_acc:0.984]
Epoch [54/120    avg_loss:0.035, val_acc:0.984]
Epoch [55/120    avg_loss:0.032, val_acc:0.978]
Epoch [56/120    avg_loss:0.029, val_acc:0.980]
Epoch [57/120    avg_loss:0.027, val_acc:0.977]
Epoch [58/120    avg_loss:0.031, val_acc:0.983]
Epoch [59/120    avg_loss:0.026, val_acc:0.982]
Epoch [60/120    avg_loss:0.032, val_acc:0.978]
Epoch [61/120    avg_loss:0.031, val_acc:0.982]
Epoch [62/120    avg_loss:0.020, val_acc:0.979]
Epoch [63/120    avg_loss:0.024, val_acc:0.979]
Epoch [64/120    avg_loss:0.024, val_acc:0.984]
Epoch [65/120    avg_loss:0.027, val_acc:0.983]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.014, val_acc:0.983]
Epoch [68/120    avg_loss:0.019, val_acc:0.982]
Epoch [69/120    avg_loss:0.025, val_acc:0.981]
Epoch [70/120    avg_loss:0.033, val_acc:0.984]
Epoch [71/120    avg_loss:0.021, val_acc:0.983]
Epoch [72/120    avg_loss:0.017, val_acc:0.983]
Epoch [73/120    avg_loss:0.021, val_acc:0.988]
Epoch [74/120    avg_loss:0.037, val_acc:0.973]
Epoch [75/120    avg_loss:0.023, val_acc:0.985]
Epoch [76/120    avg_loss:0.016, val_acc:0.988]
Epoch [77/120    avg_loss:0.014, val_acc:0.986]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.989]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.011, val_acc:0.990]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.987]
Epoch [90/120    avg_loss:0.013, val_acc:0.989]
Epoch [91/120    avg_loss:0.024, val_acc:0.985]
Epoch [92/120    avg_loss:0.021, val_acc:0.981]
Epoch [93/120    avg_loss:0.013, val_acc:0.982]
Epoch [94/120    avg_loss:0.016, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.991]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.989]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.018, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     1     0     0     0    34     0]
 [    0     0 18058     0    22     0     8     0     0     2]
 [    0     4     0  2028     0     0     0     0     3     1]
 [    0    23    20     0  2911     0     7     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     7     0     0  4852     0     0     0]
 [    0    17     0     0     0     0     0  1271     0     2]
 [    0    45     0    26    76     0     0     0  3422     2]
 [    0     2     0     2    15    30     0     0     0   870]]

Accuracy:
99.0865929192876

F1 scores:
[       nan 0.99024768 0.99803797 0.98950964 0.97081874 0.98863636
 0.99579271 0.99258102 0.97202102 0.9688196 ]

Kappa:
0.9878943725089822
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0815530978>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.129, val_acc:0.484]
Epoch [2/120    avg_loss:1.864, val_acc:0.487]
Epoch [3/120    avg_loss:1.692, val_acc:0.325]
Epoch [4/120    avg_loss:1.529, val_acc:0.356]
Epoch [5/120    avg_loss:1.374, val_acc:0.362]
Epoch [6/120    avg_loss:1.222, val_acc:0.378]
Epoch [7/120    avg_loss:1.118, val_acc:0.412]
Epoch [8/120    avg_loss:1.028, val_acc:0.441]
Epoch [9/120    avg_loss:0.921, val_acc:0.524]
Epoch [10/120    avg_loss:0.850, val_acc:0.557]
Epoch [11/120    avg_loss:0.746, val_acc:0.654]
Epoch [12/120    avg_loss:0.647, val_acc:0.718]
Epoch [13/120    avg_loss:0.594, val_acc:0.730]
Epoch [14/120    avg_loss:0.507, val_acc:0.759]
Epoch [15/120    avg_loss:0.428, val_acc:0.855]
Epoch [16/120    avg_loss:0.366, val_acc:0.873]
Epoch [17/120    avg_loss:0.381, val_acc:0.863]
Epoch [18/120    avg_loss:0.298, val_acc:0.909]
Epoch [19/120    avg_loss:0.273, val_acc:0.889]
Epoch [20/120    avg_loss:0.237, val_acc:0.896]
Epoch [21/120    avg_loss:0.232, val_acc:0.927]
Epoch [22/120    avg_loss:0.189, val_acc:0.925]
Epoch [23/120    avg_loss:0.195, val_acc:0.918]
Epoch [24/120    avg_loss:0.198, val_acc:0.915]
Epoch [25/120    avg_loss:0.192, val_acc:0.934]
Epoch [26/120    avg_loss:0.135, val_acc:0.941]
Epoch [27/120    avg_loss:0.130, val_acc:0.928]
Epoch [28/120    avg_loss:0.130, val_acc:0.927]
Epoch [29/120    avg_loss:0.138, val_acc:0.918]
Epoch [30/120    avg_loss:0.127, val_acc:0.949]
Epoch [31/120    avg_loss:0.104, val_acc:0.953]
Epoch [32/120    avg_loss:0.099, val_acc:0.928]
Epoch [33/120    avg_loss:0.087, val_acc:0.954]
Epoch [34/120    avg_loss:0.093, val_acc:0.955]
Epoch [35/120    avg_loss:0.080, val_acc:0.953]
Epoch [36/120    avg_loss:0.103, val_acc:0.951]
Epoch [37/120    avg_loss:0.107, val_acc:0.961]
Epoch [38/120    avg_loss:0.099, val_acc:0.954]
Epoch [39/120    avg_loss:0.077, val_acc:0.953]
Epoch [40/120    avg_loss:0.092, val_acc:0.957]
Epoch [41/120    avg_loss:0.065, val_acc:0.951]
Epoch [42/120    avg_loss:0.072, val_acc:0.966]
Epoch [43/120    avg_loss:0.053, val_acc:0.961]
Epoch [44/120    avg_loss:0.083, val_acc:0.957]
Epoch [45/120    avg_loss:0.063, val_acc:0.959]
Epoch [46/120    avg_loss:0.056, val_acc:0.968]
Epoch [47/120    avg_loss:0.053, val_acc:0.964]
Epoch [48/120    avg_loss:0.040, val_acc:0.970]
Epoch [49/120    avg_loss:0.033, val_acc:0.976]
Epoch [50/120    avg_loss:0.039, val_acc:0.976]
Epoch [51/120    avg_loss:0.037, val_acc:0.968]
Epoch [52/120    avg_loss:0.064, val_acc:0.951]
Epoch [53/120    avg_loss:0.088, val_acc:0.958]
Epoch [54/120    avg_loss:0.071, val_acc:0.918]
Epoch [55/120    avg_loss:0.077, val_acc:0.966]
Epoch [56/120    avg_loss:0.090, val_acc:0.969]
Epoch [57/120    avg_loss:0.054, val_acc:0.967]
Epoch [58/120    avg_loss:0.072, val_acc:0.946]
Epoch [59/120    avg_loss:0.064, val_acc:0.966]
Epoch [60/120    avg_loss:0.056, val_acc:0.972]
Epoch [61/120    avg_loss:0.032, val_acc:0.971]
Epoch [62/120    avg_loss:0.024, val_acc:0.974]
Epoch [63/120    avg_loss:0.031, val_acc:0.972]
Epoch [64/120    avg_loss:0.028, val_acc:0.980]
Epoch [65/120    avg_loss:0.024, val_acc:0.982]
Epoch [66/120    avg_loss:0.017, val_acc:0.983]
Epoch [67/120    avg_loss:0.017, val_acc:0.981]
Epoch [68/120    avg_loss:0.018, val_acc:0.983]
Epoch [69/120    avg_loss:0.018, val_acc:0.983]
Epoch [70/120    avg_loss:0.019, val_acc:0.981]
Epoch [71/120    avg_loss:0.016, val_acc:0.981]
Epoch [72/120    avg_loss:0.017, val_acc:0.980]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.019, val_acc:0.980]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.017, val_acc:0.982]
Epoch [78/120    avg_loss:0.016, val_acc:0.983]
Epoch [79/120    avg_loss:0.016, val_acc:0.983]
Epoch [80/120    avg_loss:0.017, val_acc:0.983]
Epoch [81/120    avg_loss:0.020, val_acc:0.984]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.981]
Epoch [85/120    avg_loss:0.015, val_acc:0.982]
Epoch [86/120    avg_loss:0.013, val_acc:0.982]
Epoch [87/120    avg_loss:0.020, val_acc:0.982]
Epoch [88/120    avg_loss:0.017, val_acc:0.981]
Epoch [89/120    avg_loss:0.025, val_acc:0.983]
Epoch [90/120    avg_loss:0.014, val_acc:0.981]
Epoch [91/120    avg_loss:0.015, val_acc:0.984]
Epoch [92/120    avg_loss:0.014, val_acc:0.984]
Epoch [93/120    avg_loss:0.015, val_acc:0.984]
Epoch [94/120    avg_loss:0.023, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.984]
Epoch [96/120    avg_loss:0.017, val_acc:0.982]
Epoch [97/120    avg_loss:0.014, val_acc:0.982]
Epoch [98/120    avg_loss:0.017, val_acc:0.984]
Epoch [99/120    avg_loss:0.015, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.984]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.015, val_acc:0.985]
Epoch [103/120    avg_loss:0.015, val_acc:0.986]
Epoch [104/120    avg_loss:0.016, val_acc:0.984]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.015, val_acc:0.984]
Epoch [112/120    avg_loss:0.017, val_acc:0.981]
Epoch [113/120    avg_loss:0.017, val_acc:0.984]
Epoch [114/120    avg_loss:0.014, val_acc:0.984]
Epoch [115/120    avg_loss:0.016, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.013, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.014, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     2     0     0     0    85     6]
 [    0     3 18052     0    26     0     9     0     0     0]
 [    0     2     0  2000     3     0     0     0    31     0]
 [    0    36    19     0  2886     0    14     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     6     0     0  4859     0     0     4]
 [    0     4     0     0     0     0     0  1280     0     6]
 [    0     8     0    33    55     0     0     0  3475     0]
 [    0     0     0     1    14    47     0     0     0   857]]

Accuracy:
98.93958017014918

F1 scores:
[       nan 0.9886151  0.99817528 0.98135427 0.96878147 0.98231088
 0.99569672 0.99610895 0.96810141 0.95647321]

Kappa:
0.9859512903608972
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4b8075908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.115, val_acc:0.095]
Epoch [2/120    avg_loss:1.840, val_acc:0.156]
Epoch [3/120    avg_loss:1.663, val_acc:0.263]
Epoch [4/120    avg_loss:1.510, val_acc:0.309]
Epoch [5/120    avg_loss:1.355, val_acc:0.369]
Epoch [6/120    avg_loss:1.239, val_acc:0.420]
Epoch [7/120    avg_loss:1.111, val_acc:0.454]
Epoch [8/120    avg_loss:1.017, val_acc:0.494]
Epoch [9/120    avg_loss:0.878, val_acc:0.490]
Epoch [10/120    avg_loss:0.797, val_acc:0.578]
Epoch [11/120    avg_loss:0.681, val_acc:0.666]
Epoch [12/120    avg_loss:0.613, val_acc:0.677]
Epoch [13/120    avg_loss:0.529, val_acc:0.772]
Epoch [14/120    avg_loss:0.462, val_acc:0.778]
Epoch [15/120    avg_loss:0.398, val_acc:0.754]
Epoch [16/120    avg_loss:0.377, val_acc:0.826]
Epoch [17/120    avg_loss:0.337, val_acc:0.854]
Epoch [18/120    avg_loss:0.333, val_acc:0.882]
Epoch [19/120    avg_loss:0.286, val_acc:0.888]
Epoch [20/120    avg_loss:0.256, val_acc:0.896]
Epoch [21/120    avg_loss:0.249, val_acc:0.923]
Epoch [22/120    avg_loss:0.230, val_acc:0.926]
Epoch [23/120    avg_loss:0.206, val_acc:0.937]
Epoch [24/120    avg_loss:0.189, val_acc:0.943]
Epoch [25/120    avg_loss:0.160, val_acc:0.908]
Epoch [26/120    avg_loss:0.145, val_acc:0.972]
Epoch [27/120    avg_loss:0.133, val_acc:0.966]
Epoch [28/120    avg_loss:0.136, val_acc:0.927]
Epoch [29/120    avg_loss:0.107, val_acc:0.962]
Epoch [30/120    avg_loss:0.103, val_acc:0.974]
Epoch [31/120    avg_loss:0.115, val_acc:0.966]
Epoch [32/120    avg_loss:0.090, val_acc:0.977]
Epoch [33/120    avg_loss:0.089, val_acc:0.953]
Epoch [34/120    avg_loss:0.076, val_acc:0.981]
Epoch [35/120    avg_loss:0.065, val_acc:0.974]
Epoch [36/120    avg_loss:0.066, val_acc:0.972]
Epoch [37/120    avg_loss:0.064, val_acc:0.978]
Epoch [38/120    avg_loss:0.069, val_acc:0.974]
Epoch [39/120    avg_loss:0.056, val_acc:0.976]
Epoch [40/120    avg_loss:0.065, val_acc:0.959]
Epoch [41/120    avg_loss:0.069, val_acc:0.974]
Epoch [42/120    avg_loss:0.053, val_acc:0.983]
Epoch [43/120    avg_loss:0.060, val_acc:0.973]
Epoch [44/120    avg_loss:0.042, val_acc:0.985]
Epoch [45/120    avg_loss:0.051, val_acc:0.985]
Epoch [46/120    avg_loss:0.061, val_acc:0.983]
Epoch [47/120    avg_loss:0.043, val_acc:0.989]
Epoch [48/120    avg_loss:0.041, val_acc:0.986]
Epoch [49/120    avg_loss:0.043, val_acc:0.981]
Epoch [50/120    avg_loss:0.041, val_acc:0.980]
Epoch [51/120    avg_loss:0.024, val_acc:0.991]
Epoch [52/120    avg_loss:0.024, val_acc:0.978]
Epoch [53/120    avg_loss:0.034, val_acc:0.981]
Epoch [54/120    avg_loss:0.032, val_acc:0.981]
Epoch [55/120    avg_loss:0.034, val_acc:0.985]
Epoch [56/120    avg_loss:0.024, val_acc:0.984]
Epoch [57/120    avg_loss:0.025, val_acc:0.987]
Epoch [58/120    avg_loss:0.028, val_acc:0.986]
Epoch [59/120    avg_loss:0.015, val_acc:0.988]
Epoch [60/120    avg_loss:0.024, val_acc:0.976]
Epoch [61/120    avg_loss:0.038, val_acc:0.984]
Epoch [62/120    avg_loss:0.055, val_acc:0.965]
Epoch [63/120    avg_loss:0.052, val_acc:0.974]
Epoch [64/120    avg_loss:0.031, val_acc:0.978]
Epoch [65/120    avg_loss:0.025, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.988]
Epoch [67/120    avg_loss:0.016, val_acc:0.989]
Epoch [68/120    avg_loss:0.014, val_acc:0.989]
Epoch [69/120    avg_loss:0.017, val_acc:0.989]
Epoch [70/120    avg_loss:0.014, val_acc:0.990]
Epoch [71/120    avg_loss:0.019, val_acc:0.989]
Epoch [72/120    avg_loss:0.014, val_acc:0.990]
Epoch [73/120    avg_loss:0.017, val_acc:0.990]
Epoch [74/120    avg_loss:0.015, val_acc:0.990]
Epoch [75/120    avg_loss:0.015, val_acc:0.989]
Epoch [76/120    avg_loss:0.016, val_acc:0.990]
Epoch [77/120    avg_loss:0.012, val_acc:0.991]
Epoch [78/120    avg_loss:0.015, val_acc:0.990]
Epoch [79/120    avg_loss:0.015, val_acc:0.991]
Epoch [80/120    avg_loss:0.014, val_acc:0.991]
Epoch [81/120    avg_loss:0.014, val_acc:0.990]
Epoch [82/120    avg_loss:0.015, val_acc:0.990]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.015, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.013, val_acc:0.991]
Epoch [87/120    avg_loss:0.010, val_acc:0.991]
Epoch [88/120    avg_loss:0.009, val_acc:0.991]
Epoch [89/120    avg_loss:0.011, val_acc:0.991]
Epoch [90/120    avg_loss:0.014, val_acc:0.990]
Epoch [91/120    avg_loss:0.010, val_acc:0.991]
Epoch [92/120    avg_loss:0.013, val_acc:0.991]
Epoch [93/120    avg_loss:0.010, val_acc:0.991]
Epoch [94/120    avg_loss:0.012, val_acc:0.991]
Epoch [95/120    avg_loss:0.012, val_acc:0.991]
Epoch [96/120    avg_loss:0.011, val_acc:0.991]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.013, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.991]
Epoch [100/120    avg_loss:0.013, val_acc:0.991]
Epoch [101/120    avg_loss:0.015, val_acc:0.991]
Epoch [102/120    avg_loss:0.015, val_acc:0.991]
Epoch [103/120    avg_loss:0.013, val_acc:0.991]
Epoch [104/120    avg_loss:0.010, val_acc:0.991]
Epoch [105/120    avg_loss:0.013, val_acc:0.991]
Epoch [106/120    avg_loss:0.011, val_acc:0.991]
Epoch [107/120    avg_loss:0.013, val_acc:0.991]
Epoch [108/120    avg_loss:0.015, val_acc:0.991]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.011, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.011, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.010, val_acc:0.991]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.011, val_acc:0.991]
Epoch [119/120    avg_loss:0.010, val_acc:0.991]
Epoch [120/120    avg_loss:0.011, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     0     0     0     2     0    57     1]
 [    0     0 18044     0    28     0    18     0     0     0]
 [    0     2     0  2001     2     0     0     0    31     0]
 [    0    15    18     0  2915     0     6     0    16     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4869     0     0     0]
 [    0     0     0     0     0     0     4  1286     0     0]
 [    0     2     0    24    69     0     0     0  3476     0]
 [    0     0     0     3    16    41     0     1     0   858]]

Accuracy:
99.11551346010171

F1 scores:
[       nan 0.9938392  0.99798125 0.98474409 0.97134289 0.98453414
 0.99601105 0.99805976 0.97217172 0.96404494]

Kappa:
0.9882829086339625
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c530f19e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.077, val_acc:0.066]
Epoch [2/120    avg_loss:1.824, val_acc:0.164]
Epoch [3/120    avg_loss:1.690, val_acc:0.222]
Epoch [4/120    avg_loss:1.558, val_acc:0.252]
Epoch [5/120    avg_loss:1.431, val_acc:0.328]
Epoch [6/120    avg_loss:1.320, val_acc:0.409]
Epoch [7/120    avg_loss:1.187, val_acc:0.436]
Epoch [8/120    avg_loss:1.107, val_acc:0.511]
Epoch [9/120    avg_loss:1.000, val_acc:0.504]
Epoch [10/120    avg_loss:0.903, val_acc:0.619]
Epoch [11/120    avg_loss:0.812, val_acc:0.619]
Epoch [12/120    avg_loss:0.718, val_acc:0.649]
Epoch [13/120    avg_loss:0.616, val_acc:0.725]
Epoch [14/120    avg_loss:0.553, val_acc:0.725]
Epoch [15/120    avg_loss:0.530, val_acc:0.752]
Epoch [16/120    avg_loss:0.463, val_acc:0.784]
Epoch [17/120    avg_loss:0.404, val_acc:0.794]
Epoch [18/120    avg_loss:0.359, val_acc:0.776]
Epoch [19/120    avg_loss:0.342, val_acc:0.792]
Epoch [20/120    avg_loss:0.328, val_acc:0.809]
Epoch [21/120    avg_loss:0.323, val_acc:0.807]
Epoch [22/120    avg_loss:0.283, val_acc:0.798]
Epoch [23/120    avg_loss:0.267, val_acc:0.817]
Epoch [24/120    avg_loss:0.267, val_acc:0.870]
Epoch [25/120    avg_loss:0.235, val_acc:0.900]
Epoch [26/120    avg_loss:0.217, val_acc:0.878]
Epoch [27/120    avg_loss:0.235, val_acc:0.849]
Epoch [28/120    avg_loss:0.190, val_acc:0.920]
Epoch [29/120    avg_loss:0.176, val_acc:0.947]
Epoch [30/120    avg_loss:0.137, val_acc:0.958]
Epoch [31/120    avg_loss:0.135, val_acc:0.964]
Epoch [32/120    avg_loss:0.137, val_acc:0.954]
Epoch [33/120    avg_loss:0.123, val_acc:0.935]
Epoch [34/120    avg_loss:0.110, val_acc:0.956]
Epoch [35/120    avg_loss:0.093, val_acc:0.959]
Epoch [36/120    avg_loss:0.076, val_acc:0.963]
Epoch [37/120    avg_loss:0.074, val_acc:0.966]
Epoch [38/120    avg_loss:0.073, val_acc:0.966]
Epoch [39/120    avg_loss:0.064, val_acc:0.952]
Epoch [40/120    avg_loss:0.068, val_acc:0.978]
Epoch [41/120    avg_loss:0.068, val_acc:0.974]
Epoch [42/120    avg_loss:0.053, val_acc:0.963]
Epoch [43/120    avg_loss:0.052, val_acc:0.969]
Epoch [44/120    avg_loss:0.051, val_acc:0.976]
Epoch [45/120    avg_loss:0.047, val_acc:0.973]
Epoch [46/120    avg_loss:0.036, val_acc:0.984]
Epoch [47/120    avg_loss:0.035, val_acc:0.978]
Epoch [48/120    avg_loss:0.036, val_acc:0.969]
Epoch [49/120    avg_loss:0.038, val_acc:0.976]
Epoch [50/120    avg_loss:0.042, val_acc:0.976]
Epoch [51/120    avg_loss:0.048, val_acc:0.962]
Epoch [52/120    avg_loss:0.035, val_acc:0.978]
Epoch [53/120    avg_loss:0.032, val_acc:0.975]
Epoch [54/120    avg_loss:0.030, val_acc:0.981]
Epoch [55/120    avg_loss:0.029, val_acc:0.984]
Epoch [56/120    avg_loss:0.033, val_acc:0.972]
Epoch [57/120    avg_loss:0.038, val_acc:0.980]
Epoch [58/120    avg_loss:0.032, val_acc:0.972]
Epoch [59/120    avg_loss:0.121, val_acc:0.954]
Epoch [60/120    avg_loss:0.089, val_acc:0.972]
Epoch [61/120    avg_loss:0.086, val_acc:0.928]
Epoch [62/120    avg_loss:0.088, val_acc:0.919]
Epoch [63/120    avg_loss:0.057, val_acc:0.972]
Epoch [64/120    avg_loss:0.046, val_acc:0.972]
Epoch [65/120    avg_loss:0.032, val_acc:0.978]
Epoch [66/120    avg_loss:0.021, val_acc:0.982]
Epoch [67/120    avg_loss:0.018, val_acc:0.982]
Epoch [68/120    avg_loss:0.022, val_acc:0.978]
Epoch [69/120    avg_loss:0.023, val_acc:0.978]
Epoch [70/120    avg_loss:0.016, val_acc:0.983]
Epoch [71/120    avg_loss:0.017, val_acc:0.982]
Epoch [72/120    avg_loss:0.016, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.984]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.019, val_acc:0.984]
Epoch [76/120    avg_loss:0.016, val_acc:0.984]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.014, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.984]
Epoch [83/120    avg_loss:0.013, val_acc:0.985]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.012, val_acc:0.985]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.014, val_acc:0.985]
Epoch [90/120    avg_loss:0.013, val_acc:0.985]
Epoch [91/120    avg_loss:0.015, val_acc:0.984]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.012, val_acc:0.984]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.013, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.984]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.011, val_acc:0.986]
Epoch [106/120    avg_loss:0.011, val_acc:0.985]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.012, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.012, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.987]
Epoch [113/120    avg_loss:0.013, val_acc:0.987]
Epoch [114/120    avg_loss:0.011, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     2     0     0    17    10     3]
 [    0     1 18059     0    19     0    10     0     0     1]
 [    0     9     0  1988     1     0     0     0    37     1]
 [    0    34    25     0  2883     0     5     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9    11     0     0  4858     0     0     0]
 [    0     0     0     0     0     0     0  1278     0    12]
 [    0    25     0     2    64     0     0     0  3480     0]
 [    0     0     0     1    14    24     0     0     0   880]]

Accuracy:
99.12756368544092

F1 scores:
[       nan 0.99217115 0.99820358 0.98464586 0.96826196 0.99088838
 0.99641062 0.98878143 0.97711638 0.969163  ]

Kappa:
0.9884380809075518
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6df70f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.164, val_acc:0.236]
Epoch [2/120    avg_loss:1.849, val_acc:0.510]
Epoch [3/120    avg_loss:1.646, val_acc:0.550]
Epoch [4/120    avg_loss:1.500, val_acc:0.426]
Epoch [5/120    avg_loss:1.347, val_acc:0.347]
Epoch [6/120    avg_loss:1.206, val_acc:0.361]
Epoch [7/120    avg_loss:1.056, val_acc:0.419]
Epoch [8/120    avg_loss:0.944, val_acc:0.511]
Epoch [9/120    avg_loss:0.864, val_acc:0.542]
Epoch [10/120    avg_loss:0.771, val_acc:0.554]
Epoch [11/120    avg_loss:0.672, val_acc:0.572]
Epoch [12/120    avg_loss:0.639, val_acc:0.658]
Epoch [13/120    avg_loss:0.562, val_acc:0.674]
Epoch [14/120    avg_loss:0.510, val_acc:0.690]
Epoch [15/120    avg_loss:0.469, val_acc:0.761]
Epoch [16/120    avg_loss:0.421, val_acc:0.722]
Epoch [17/120    avg_loss:0.391, val_acc:0.756]
Epoch [18/120    avg_loss:0.355, val_acc:0.851]
Epoch [19/120    avg_loss:0.307, val_acc:0.905]
Epoch [20/120    avg_loss:0.256, val_acc:0.918]
Epoch [21/120    avg_loss:0.251, val_acc:0.872]
Epoch [22/120    avg_loss:0.301, val_acc:0.837]
Epoch [23/120    avg_loss:0.221, val_acc:0.909]
Epoch [24/120    avg_loss:0.190, val_acc:0.922]
Epoch [25/120    avg_loss:0.186, val_acc:0.924]
Epoch [26/120    avg_loss:0.186, val_acc:0.922]
Epoch [27/120    avg_loss:0.174, val_acc:0.925]
Epoch [28/120    avg_loss:0.151, val_acc:0.954]
Epoch [29/120    avg_loss:0.179, val_acc:0.906]
Epoch [30/120    avg_loss:0.142, val_acc:0.951]
Epoch [31/120    avg_loss:0.097, val_acc:0.956]
Epoch [32/120    avg_loss:0.658, val_acc:0.686]
Epoch [33/120    avg_loss:0.605, val_acc:0.787]
Epoch [34/120    avg_loss:0.405, val_acc:0.850]
Epoch [35/120    avg_loss:0.316, val_acc:0.890]
Epoch [36/120    avg_loss:0.280, val_acc:0.856]
Epoch [37/120    avg_loss:0.244, val_acc:0.871]
Epoch [38/120    avg_loss:0.218, val_acc:0.914]
Epoch [39/120    avg_loss:0.180, val_acc:0.922]
Epoch [40/120    avg_loss:0.174, val_acc:0.930]
Epoch [41/120    avg_loss:0.165, val_acc:0.940]
Epoch [42/120    avg_loss:0.154, val_acc:0.934]
Epoch [43/120    avg_loss:0.122, val_acc:0.942]
Epoch [44/120    avg_loss:0.125, val_acc:0.930]
Epoch [45/120    avg_loss:0.099, val_acc:0.960]
Epoch [46/120    avg_loss:0.085, val_acc:0.961]
Epoch [47/120    avg_loss:0.088, val_acc:0.962]
Epoch [48/120    avg_loss:0.089, val_acc:0.962]
Epoch [49/120    avg_loss:0.086, val_acc:0.960]
Epoch [50/120    avg_loss:0.087, val_acc:0.959]
Epoch [51/120    avg_loss:0.079, val_acc:0.960]
Epoch [52/120    avg_loss:0.087, val_acc:0.963]
Epoch [53/120    avg_loss:0.080, val_acc:0.960]
Epoch [54/120    avg_loss:0.071, val_acc:0.965]
Epoch [55/120    avg_loss:0.080, val_acc:0.964]
Epoch [56/120    avg_loss:0.070, val_acc:0.966]
Epoch [57/120    avg_loss:0.083, val_acc:0.965]
Epoch [58/120    avg_loss:0.076, val_acc:0.964]
Epoch [59/120    avg_loss:0.081, val_acc:0.963]
Epoch [60/120    avg_loss:0.066, val_acc:0.962]
Epoch [61/120    avg_loss:0.069, val_acc:0.965]
Epoch [62/120    avg_loss:0.073, val_acc:0.968]
Epoch [63/120    avg_loss:0.072, val_acc:0.968]
Epoch [64/120    avg_loss:0.074, val_acc:0.966]
Epoch [65/120    avg_loss:0.080, val_acc:0.970]
Epoch [66/120    avg_loss:0.070, val_acc:0.966]
Epoch [67/120    avg_loss:0.062, val_acc:0.969]
Epoch [68/120    avg_loss:0.063, val_acc:0.970]
Epoch [69/120    avg_loss:0.062, val_acc:0.969]
Epoch [70/120    avg_loss:0.072, val_acc:0.969]
Epoch [71/120    avg_loss:0.064, val_acc:0.966]
Epoch [72/120    avg_loss:0.067, val_acc:0.972]
Epoch [73/120    avg_loss:0.063, val_acc:0.970]
Epoch [74/120    avg_loss:0.070, val_acc:0.970]
Epoch [75/120    avg_loss:0.055, val_acc:0.970]
Epoch [76/120    avg_loss:0.054, val_acc:0.972]
Epoch [77/120    avg_loss:0.069, val_acc:0.970]
Epoch [78/120    avg_loss:0.058, val_acc:0.972]
Epoch [79/120    avg_loss:0.063, val_acc:0.968]
Epoch [80/120    avg_loss:0.059, val_acc:0.971]
Epoch [81/120    avg_loss:0.057, val_acc:0.972]
Epoch [82/120    avg_loss:0.058, val_acc:0.971]
Epoch [83/120    avg_loss:0.055, val_acc:0.972]
Epoch [84/120    avg_loss:0.060, val_acc:0.970]
Epoch [85/120    avg_loss:0.056, val_acc:0.972]
Epoch [86/120    avg_loss:0.055, val_acc:0.970]
Epoch [87/120    avg_loss:0.056, val_acc:0.972]
Epoch [88/120    avg_loss:0.051, val_acc:0.972]
Epoch [89/120    avg_loss:0.059, val_acc:0.969]
Epoch [90/120    avg_loss:0.052, val_acc:0.973]
Epoch [91/120    avg_loss:0.052, val_acc:0.973]
Epoch [92/120    avg_loss:0.061, val_acc:0.972]
Epoch [93/120    avg_loss:0.053, val_acc:0.972]
Epoch [94/120    avg_loss:0.053, val_acc:0.972]
Epoch [95/120    avg_loss:0.052, val_acc:0.974]
Epoch [96/120    avg_loss:0.051, val_acc:0.974]
Epoch [97/120    avg_loss:0.055, val_acc:0.972]
Epoch [98/120    avg_loss:0.054, val_acc:0.973]
Epoch [99/120    avg_loss:0.051, val_acc:0.973]
Epoch [100/120    avg_loss:0.046, val_acc:0.972]
Epoch [101/120    avg_loss:0.049, val_acc:0.974]
Epoch [102/120    avg_loss:0.046, val_acc:0.972]
Epoch [103/120    avg_loss:0.042, val_acc:0.969]
Epoch [104/120    avg_loss:0.041, val_acc:0.975]
Epoch [105/120    avg_loss:0.042, val_acc:0.972]
Epoch [106/120    avg_loss:0.047, val_acc:0.975]
Epoch [107/120    avg_loss:0.043, val_acc:0.973]
Epoch [108/120    avg_loss:0.047, val_acc:0.973]
Epoch [109/120    avg_loss:0.046, val_acc:0.973]
Epoch [110/120    avg_loss:0.055, val_acc:0.972]
Epoch [111/120    avg_loss:0.048, val_acc:0.973]
Epoch [112/120    avg_loss:0.041, val_acc:0.973]
Epoch [113/120    avg_loss:0.047, val_acc:0.973]
Epoch [114/120    avg_loss:0.040, val_acc:0.973]
Epoch [115/120    avg_loss:0.045, val_acc:0.974]
Epoch [116/120    avg_loss:0.044, val_acc:0.972]
Epoch [117/120    avg_loss:0.042, val_acc:0.977]
Epoch [118/120    avg_loss:0.038, val_acc:0.977]
Epoch [119/120    avg_loss:0.036, val_acc:0.977]
Epoch [120/120    avg_loss:0.044, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6229     0    11    61     0     0    10   121     0]
 [    0     0 17815     0   154     0   115     0     6     0]
 [    0    12     0  1968     0     0     0     0    50     6]
 [    0    40    18     0  2875     0    10     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    18     0     0  4834     0    22     4]
 [    0     0     0     0     0     0     7  1273     0    10]
 [    0    11     0    38    76     0     0     0  3408    38]
 [    0     0     0     0    16    33     0     0     2   868]]

Accuracy:
97.78757862772034

F1 scores:
[       nan 0.97909462 0.99184367 0.96683861 0.93435164 0.98751419
 0.98212109 0.98950641 0.94548481 0.94092141]

Kappa:
0.9707789536404161
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa32b4079b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.094, val_acc:0.116]
Epoch [2/120    avg_loss:1.781, val_acc:0.172]
Epoch [3/120    avg_loss:1.603, val_acc:0.275]
Epoch [4/120    avg_loss:1.472, val_acc:0.313]
Epoch [5/120    avg_loss:1.369, val_acc:0.396]
Epoch [6/120    avg_loss:1.244, val_acc:0.439]
Epoch [7/120    avg_loss:1.161, val_acc:0.453]
Epoch [8/120    avg_loss:1.043, val_acc:0.478]
Epoch [9/120    avg_loss:0.946, val_acc:0.495]
Epoch [10/120    avg_loss:0.820, val_acc:0.535]
Epoch [11/120    avg_loss:0.732, val_acc:0.668]
Epoch [12/120    avg_loss:0.638, val_acc:0.721]
Epoch [13/120    avg_loss:0.558, val_acc:0.783]
Epoch [14/120    avg_loss:0.493, val_acc:0.756]
Epoch [15/120    avg_loss:0.435, val_acc:0.860]
Epoch [16/120    avg_loss:0.376, val_acc:0.850]
Epoch [17/120    avg_loss:0.344, val_acc:0.834]
Epoch [18/120    avg_loss:0.366, val_acc:0.884]
Epoch [19/120    avg_loss:0.294, val_acc:0.917]
Epoch [20/120    avg_loss:0.271, val_acc:0.867]
Epoch [21/120    avg_loss:0.269, val_acc:0.758]
Epoch [22/120    avg_loss:1.029, val_acc:0.664]
Epoch [23/120    avg_loss:0.860, val_acc:0.747]
Epoch [24/120    avg_loss:0.745, val_acc:0.720]
Epoch [25/120    avg_loss:0.624, val_acc:0.809]
Epoch [26/120    avg_loss:0.540, val_acc:0.888]
Epoch [27/120    avg_loss:0.501, val_acc:0.817]
Epoch [28/120    avg_loss:0.474, val_acc:0.854]
Epoch [29/120    avg_loss:0.444, val_acc:0.901]
Epoch [30/120    avg_loss:0.419, val_acc:0.884]
Epoch [31/120    avg_loss:0.331, val_acc:0.895]
Epoch [32/120    avg_loss:0.354, val_acc:0.915]
Epoch [33/120    avg_loss:0.314, val_acc:0.904]
Epoch [34/120    avg_loss:0.292, val_acc:0.923]
Epoch [35/120    avg_loss:0.272, val_acc:0.919]
Epoch [36/120    avg_loss:0.248, val_acc:0.930]
Epoch [37/120    avg_loss:0.251, val_acc:0.926]
Epoch [38/120    avg_loss:0.248, val_acc:0.924]
Epoch [39/120    avg_loss:0.235, val_acc:0.928]
Epoch [40/120    avg_loss:0.252, val_acc:0.936]
Epoch [41/120    avg_loss:0.228, val_acc:0.933]
Epoch [42/120    avg_loss:0.233, val_acc:0.932]
Epoch [43/120    avg_loss:0.222, val_acc:0.936]
Epoch [44/120    avg_loss:0.225, val_acc:0.937]
Epoch [45/120    avg_loss:0.219, val_acc:0.941]
Epoch [46/120    avg_loss:0.212, val_acc:0.942]
Epoch [47/120    avg_loss:0.200, val_acc:0.945]
Epoch [48/120    avg_loss:0.225, val_acc:0.922]
Epoch [49/120    avg_loss:0.200, val_acc:0.943]
Epoch [50/120    avg_loss:0.217, val_acc:0.943]
Epoch [51/120    avg_loss:0.205, val_acc:0.945]
Epoch [52/120    avg_loss:0.192, val_acc:0.942]
Epoch [53/120    avg_loss:0.208, val_acc:0.942]
Epoch [54/120    avg_loss:0.194, val_acc:0.944]
Epoch [55/120    avg_loss:0.197, val_acc:0.945]
Epoch [56/120    avg_loss:0.202, val_acc:0.938]
Epoch [57/120    avg_loss:0.183, val_acc:0.947]
Epoch [58/120    avg_loss:0.203, val_acc:0.947]
Epoch [59/120    avg_loss:0.184, val_acc:0.941]
Epoch [60/120    avg_loss:0.186, val_acc:0.946]
Epoch [61/120    avg_loss:0.183, val_acc:0.945]
Epoch [62/120    avg_loss:0.170, val_acc:0.949]
Epoch [63/120    avg_loss:0.176, val_acc:0.947]
Epoch [64/120    avg_loss:0.182, val_acc:0.945]
Epoch [65/120    avg_loss:0.181, val_acc:0.950]
Epoch [66/120    avg_loss:0.169, val_acc:0.948]
Epoch [67/120    avg_loss:0.167, val_acc:0.942]
Epoch [68/120    avg_loss:0.169, val_acc:0.944]
Epoch [69/120    avg_loss:0.165, val_acc:0.953]
Epoch [70/120    avg_loss:0.182, val_acc:0.947]
Epoch [71/120    avg_loss:0.156, val_acc:0.949]
Epoch [72/120    avg_loss:0.159, val_acc:0.954]
Epoch [73/120    avg_loss:0.170, val_acc:0.943]
Epoch [74/120    avg_loss:0.157, val_acc:0.943]
Epoch [75/120    avg_loss:0.150, val_acc:0.941]
Epoch [76/120    avg_loss:0.159, val_acc:0.953]
Epoch [77/120    avg_loss:0.151, val_acc:0.948]
Epoch [78/120    avg_loss:0.170, val_acc:0.955]
Epoch [79/120    avg_loss:0.159, val_acc:0.956]
Epoch [80/120    avg_loss:0.154, val_acc:0.953]
Epoch [81/120    avg_loss:0.149, val_acc:0.953]
Epoch [82/120    avg_loss:0.146, val_acc:0.958]
Epoch [83/120    avg_loss:0.148, val_acc:0.953]
Epoch [84/120    avg_loss:0.148, val_acc:0.954]
Epoch [85/120    avg_loss:0.161, val_acc:0.955]
Epoch [86/120    avg_loss:0.135, val_acc:0.956]
Epoch [87/120    avg_loss:0.139, val_acc:0.946]
Epoch [88/120    avg_loss:0.146, val_acc:0.953]
Epoch [89/120    avg_loss:0.144, val_acc:0.958]
Epoch [90/120    avg_loss:0.158, val_acc:0.954]
Epoch [91/120    avg_loss:0.143, val_acc:0.959]
Epoch [92/120    avg_loss:0.149, val_acc:0.955]
Epoch [93/120    avg_loss:0.131, val_acc:0.953]
Epoch [94/120    avg_loss:0.144, val_acc:0.957]
Epoch [95/120    avg_loss:0.131, val_acc:0.955]
Epoch [96/120    avg_loss:0.124, val_acc:0.953]
Epoch [97/120    avg_loss:0.129, val_acc:0.957]
Epoch [98/120    avg_loss:0.144, val_acc:0.959]
Epoch [99/120    avg_loss:0.124, val_acc:0.953]
Epoch [100/120    avg_loss:0.135, val_acc:0.957]
Epoch [101/120    avg_loss:0.133, val_acc:0.959]
Epoch [102/120    avg_loss:0.128, val_acc:0.960]
Epoch [103/120    avg_loss:0.134, val_acc:0.953]
Epoch [104/120    avg_loss:0.124, val_acc:0.957]
Epoch [105/120    avg_loss:0.122, val_acc:0.956]
Epoch [106/120    avg_loss:0.127, val_acc:0.957]
Epoch [107/120    avg_loss:0.121, val_acc:0.960]
Epoch [108/120    avg_loss:0.113, val_acc:0.958]
Epoch [109/120    avg_loss:0.120, val_acc:0.958]
Epoch [110/120    avg_loss:0.122, val_acc:0.957]
Epoch [111/120    avg_loss:0.107, val_acc:0.962]
Epoch [112/120    avg_loss:0.129, val_acc:0.960]
Epoch [113/120    avg_loss:0.125, val_acc:0.958]
Epoch [114/120    avg_loss:0.105, val_acc:0.964]
Epoch [115/120    avg_loss:0.111, val_acc:0.958]
Epoch [116/120    avg_loss:0.127, val_acc:0.944]
Epoch [117/120    avg_loss:0.109, val_acc:0.964]
Epoch [118/120    avg_loss:0.115, val_acc:0.959]
Epoch [119/120    avg_loss:0.105, val_acc:0.961]
Epoch [120/120    avg_loss:0.107, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5844     7    75   128     0    14    54   218    92]
 [    0     8 17997     0    55     0    30     0     0     0]
 [    0    15     0  1926     0     0     0     0    38    57]
 [    0   162    40     0  2704     0    27     0    37     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    38     0     0     0  4840     0     0     0]
 [    0    15     0     0     0     0     6  1252     0    17]
 [    0    94     0    60    45     0     0     0  3372     0]
 [    0    16     0     0    14    50     0     0     0   839]]

Accuracy:
96.59219627407033

F1 scores:
[       nan 0.92865088 0.99507907 0.94020015 0.91382224 0.98120301
 0.98825932 0.96456086 0.93200663 0.87123572]

Kappa:
0.9548885147365437
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a830858d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.142, val_acc:0.138]
Epoch [2/120    avg_loss:1.856, val_acc:0.179]
Epoch [3/120    avg_loss:1.682, val_acc:0.196]
Epoch [4/120    avg_loss:1.532, val_acc:0.210]
Epoch [5/120    avg_loss:1.386, val_acc:0.274]
Epoch [6/120    avg_loss:1.300, val_acc:0.304]
Epoch [7/120    avg_loss:1.174, val_acc:0.352]
Epoch [8/120    avg_loss:1.072, val_acc:0.481]
Epoch [9/120    avg_loss:1.001, val_acc:0.534]
Epoch [10/120    avg_loss:0.937, val_acc:0.745]
Epoch [11/120    avg_loss:0.837, val_acc:0.716]
Epoch [12/120    avg_loss:0.791, val_acc:0.740]
Epoch [13/120    avg_loss:0.691, val_acc:0.691]
Epoch [14/120    avg_loss:0.586, val_acc:0.713]
Epoch [15/120    avg_loss:0.501, val_acc:0.817]
Epoch [16/120    avg_loss:0.426, val_acc:0.830]
Epoch [17/120    avg_loss:0.390, val_acc:0.864]
Epoch [18/120    avg_loss:0.398, val_acc:0.880]
Epoch [19/120    avg_loss:0.339, val_acc:0.884]
Epoch [20/120    avg_loss:0.272, val_acc:0.922]
Epoch [21/120    avg_loss:0.234, val_acc:0.933]
Epoch [22/120    avg_loss:0.218, val_acc:0.921]
Epoch [23/120    avg_loss:0.192, val_acc:0.913]
Epoch [24/120    avg_loss:0.166, val_acc:0.940]
Epoch [25/120    avg_loss:0.188, val_acc:0.905]
Epoch [26/120    avg_loss:0.208, val_acc:0.899]
Epoch [27/120    avg_loss:0.211, val_acc:0.913]
Epoch [28/120    avg_loss:0.155, val_acc:0.953]
Epoch [29/120    avg_loss:0.135, val_acc:0.943]
Epoch [30/120    avg_loss:0.120, val_acc:0.960]
Epoch [31/120    avg_loss:0.108, val_acc:0.942]
Epoch [32/120    avg_loss:0.114, val_acc:0.952]
Epoch [33/120    avg_loss:0.107, val_acc:0.958]
Epoch [34/120    avg_loss:0.093, val_acc:0.972]
Epoch [35/120    avg_loss:0.096, val_acc:0.941]
Epoch [36/120    avg_loss:0.100, val_acc:0.965]
Epoch [37/120    avg_loss:0.074, val_acc:0.953]
Epoch [38/120    avg_loss:0.064, val_acc:0.959]
Epoch [39/120    avg_loss:0.064, val_acc:0.966]
Epoch [40/120    avg_loss:0.083, val_acc:0.967]
Epoch [41/120    avg_loss:0.057, val_acc:0.965]
Epoch [42/120    avg_loss:0.069, val_acc:0.966]
Epoch [43/120    avg_loss:0.074, val_acc:0.965]
Epoch [44/120    avg_loss:0.057, val_acc:0.967]
Epoch [45/120    avg_loss:0.064, val_acc:0.970]
Epoch [46/120    avg_loss:0.075, val_acc:0.964]
Epoch [47/120    avg_loss:0.055, val_acc:0.968]
Epoch [48/120    avg_loss:0.038, val_acc:0.975]
Epoch [49/120    avg_loss:0.035, val_acc:0.977]
Epoch [50/120    avg_loss:0.029, val_acc:0.977]
Epoch [51/120    avg_loss:0.036, val_acc:0.977]
Epoch [52/120    avg_loss:0.029, val_acc:0.976]
Epoch [53/120    avg_loss:0.031, val_acc:0.977]
Epoch [54/120    avg_loss:0.028, val_acc:0.978]
Epoch [55/120    avg_loss:0.027, val_acc:0.978]
Epoch [56/120    avg_loss:0.027, val_acc:0.978]
Epoch [57/120    avg_loss:0.028, val_acc:0.979]
Epoch [58/120    avg_loss:0.025, val_acc:0.978]
Epoch [59/120    avg_loss:0.024, val_acc:0.980]
Epoch [60/120    avg_loss:0.028, val_acc:0.980]
Epoch [61/120    avg_loss:0.027, val_acc:0.980]
Epoch [62/120    avg_loss:0.030, val_acc:0.980]
Epoch [63/120    avg_loss:0.029, val_acc:0.979]
Epoch [64/120    avg_loss:0.032, val_acc:0.979]
Epoch [65/120    avg_loss:0.028, val_acc:0.979]
Epoch [66/120    avg_loss:0.024, val_acc:0.981]
Epoch [67/120    avg_loss:0.027, val_acc:0.983]
Epoch [68/120    avg_loss:0.027, val_acc:0.982]
Epoch [69/120    avg_loss:0.026, val_acc:0.978]
Epoch [70/120    avg_loss:0.021, val_acc:0.982]
Epoch [71/120    avg_loss:0.023, val_acc:0.981]
Epoch [72/120    avg_loss:0.024, val_acc:0.981]
Epoch [73/120    avg_loss:0.024, val_acc:0.983]
Epoch [74/120    avg_loss:0.025, val_acc:0.981]
Epoch [75/120    avg_loss:0.021, val_acc:0.979]
Epoch [76/120    avg_loss:0.023, val_acc:0.979]
Epoch [77/120    avg_loss:0.025, val_acc:0.979]
Epoch [78/120    avg_loss:0.026, val_acc:0.981]
Epoch [79/120    avg_loss:0.024, val_acc:0.981]
Epoch [80/120    avg_loss:0.027, val_acc:0.979]
Epoch [81/120    avg_loss:0.023, val_acc:0.981]
Epoch [82/120    avg_loss:0.023, val_acc:0.983]
Epoch [83/120    avg_loss:0.023, val_acc:0.982]
Epoch [84/120    avg_loss:0.026, val_acc:0.980]
Epoch [85/120    avg_loss:0.023, val_acc:0.983]
Epoch [86/120    avg_loss:0.023, val_acc:0.984]
Epoch [87/120    avg_loss:0.021, val_acc:0.982]
Epoch [88/120    avg_loss:0.023, val_acc:0.979]
Epoch [89/120    avg_loss:0.023, val_acc:0.981]
Epoch [90/120    avg_loss:0.020, val_acc:0.982]
Epoch [91/120    avg_loss:0.018, val_acc:0.982]
Epoch [92/120    avg_loss:0.021, val_acc:0.982]
Epoch [93/120    avg_loss:0.024, val_acc:0.984]
Epoch [94/120    avg_loss:0.022, val_acc:0.980]
Epoch [95/120    avg_loss:0.025, val_acc:0.982]
Epoch [96/120    avg_loss:0.019, val_acc:0.981]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.019, val_acc:0.983]
Epoch [99/120    avg_loss:0.023, val_acc:0.982]
Epoch [100/120    avg_loss:0.018, val_acc:0.982]
Epoch [101/120    avg_loss:0.022, val_acc:0.984]
Epoch [102/120    avg_loss:0.020, val_acc:0.983]
Epoch [103/120    avg_loss:0.021, val_acc:0.984]
Epoch [104/120    avg_loss:0.020, val_acc:0.983]
Epoch [105/120    avg_loss:0.021, val_acc:0.982]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.020, val_acc:0.979]
Epoch [108/120    avg_loss:0.017, val_acc:0.981]
Epoch [109/120    avg_loss:0.019, val_acc:0.982]
Epoch [110/120    avg_loss:0.020, val_acc:0.984]
Epoch [111/120    avg_loss:0.019, val_acc:0.981]
Epoch [112/120    avg_loss:0.020, val_acc:0.984]
Epoch [113/120    avg_loss:0.016, val_acc:0.981]
Epoch [114/120    avg_loss:0.017, val_acc:0.982]
Epoch [115/120    avg_loss:0.019, val_acc:0.984]
Epoch [116/120    avg_loss:0.019, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.982]
Epoch [118/120    avg_loss:0.021, val_acc:0.983]
Epoch [119/120    avg_loss:0.018, val_acc:0.984]
Epoch [120/120    avg_loss:0.017, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     4     0     2     1    57     0]
 [    0     0 17952     0    89     0    49     0     0     0]
 [    0     4     0  1984     3     0     0     0    40     5]
 [    0    25    18     0  2888     0    12     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4858     0     5     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    20     0    33    61     0     0     0  3457     0]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
98.75159665485745

F1 scores:
[       nan 0.99120554 0.99525988 0.97902788 0.95771845 0.98863636
 0.99152975 0.99883676 0.96577734 0.9716824 ]

Kappa:
0.9834769986872987
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d230609e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.112, val_acc:0.135]
Epoch [2/120    avg_loss:1.843, val_acc:0.216]
Epoch [3/120    avg_loss:1.634, val_acc:0.236]
Epoch [4/120    avg_loss:1.485, val_acc:0.276]
Epoch [5/120    avg_loss:1.345, val_acc:0.438]
Epoch [6/120    avg_loss:1.215, val_acc:0.500]
Epoch [7/120    avg_loss:1.113, val_acc:0.620]
Epoch [8/120    avg_loss:1.015, val_acc:0.673]
Epoch [9/120    avg_loss:0.898, val_acc:0.698]
Epoch [10/120    avg_loss:0.817, val_acc:0.727]
Epoch [11/120    avg_loss:0.682, val_acc:0.744]
Epoch [12/120    avg_loss:0.622, val_acc:0.770]
Epoch [13/120    avg_loss:0.551, val_acc:0.748]
Epoch [14/120    avg_loss:0.483, val_acc:0.812]
Epoch [15/120    avg_loss:0.440, val_acc:0.816]
Epoch [16/120    avg_loss:0.402, val_acc:0.796]
Epoch [17/120    avg_loss:0.394, val_acc:0.804]
Epoch [18/120    avg_loss:0.341, val_acc:0.836]
Epoch [19/120    avg_loss:0.319, val_acc:0.866]
Epoch [20/120    avg_loss:0.281, val_acc:0.863]
Epoch [21/120    avg_loss:0.244, val_acc:0.897]
Epoch [22/120    avg_loss:0.237, val_acc:0.919]
Epoch [23/120    avg_loss:0.241, val_acc:0.914]
Epoch [24/120    avg_loss:0.222, val_acc:0.957]
Epoch [25/120    avg_loss:0.189, val_acc:0.890]
Epoch [26/120    avg_loss:0.171, val_acc:0.921]
Epoch [27/120    avg_loss:0.168, val_acc:0.911]
Epoch [28/120    avg_loss:0.153, val_acc:0.960]
Epoch [29/120    avg_loss:0.126, val_acc:0.965]
Epoch [30/120    avg_loss:0.114, val_acc:0.952]
Epoch [31/120    avg_loss:0.113, val_acc:0.932]
Epoch [32/120    avg_loss:0.093, val_acc:0.974]
Epoch [33/120    avg_loss:0.113, val_acc:0.968]
Epoch [34/120    avg_loss:0.082, val_acc:0.964]
Epoch [35/120    avg_loss:0.095, val_acc:0.968]
Epoch [36/120    avg_loss:0.103, val_acc:0.940]
Epoch [37/120    avg_loss:0.122, val_acc:0.966]
Epoch [38/120    avg_loss:0.080, val_acc:0.972]
Epoch [39/120    avg_loss:0.079, val_acc:0.961]
Epoch [40/120    avg_loss:0.105, val_acc:0.947]
Epoch [41/120    avg_loss:0.074, val_acc:0.970]
Epoch [42/120    avg_loss:0.056, val_acc:0.970]
Epoch [43/120    avg_loss:0.061, val_acc:0.958]
Epoch [44/120    avg_loss:0.084, val_acc:0.973]
Epoch [45/120    avg_loss:0.053, val_acc:0.977]
Epoch [46/120    avg_loss:0.048, val_acc:0.973]
Epoch [47/120    avg_loss:0.051, val_acc:0.975]
Epoch [48/120    avg_loss:0.082, val_acc:0.949]
Epoch [49/120    avg_loss:0.066, val_acc:0.942]
Epoch [50/120    avg_loss:0.074, val_acc:0.971]
Epoch [51/120    avg_loss:0.057, val_acc:0.963]
Epoch [52/120    avg_loss:0.072, val_acc:0.978]
Epoch [53/120    avg_loss:0.041, val_acc:0.980]
Epoch [54/120    avg_loss:0.043, val_acc:0.983]
Epoch [55/120    avg_loss:0.052, val_acc:0.978]
Epoch [56/120    avg_loss:0.033, val_acc:0.984]
Epoch [57/120    avg_loss:0.065, val_acc:0.961]
Epoch [58/120    avg_loss:0.055, val_acc:0.963]
Epoch [59/120    avg_loss:0.047, val_acc:0.973]
Epoch [60/120    avg_loss:0.037, val_acc:0.980]
Epoch [61/120    avg_loss:0.033, val_acc:0.983]
Epoch [62/120    avg_loss:0.051, val_acc:0.977]
Epoch [63/120    avg_loss:0.036, val_acc:0.978]
Epoch [64/120    avg_loss:0.024, val_acc:0.985]
Epoch [65/120    avg_loss:0.030, val_acc:0.982]
Epoch [66/120    avg_loss:0.032, val_acc:0.976]
Epoch [67/120    avg_loss:0.023, val_acc:0.981]
Epoch [68/120    avg_loss:0.028, val_acc:0.964]
Epoch [69/120    avg_loss:0.043, val_acc:0.982]
Epoch [70/120    avg_loss:0.019, val_acc:0.987]
Epoch [71/120    avg_loss:0.027, val_acc:0.973]
Epoch [72/120    avg_loss:0.021, val_acc:0.983]
Epoch [73/120    avg_loss:0.031, val_acc:0.980]
Epoch [74/120    avg_loss:0.022, val_acc:0.984]
Epoch [75/120    avg_loss:0.019, val_acc:0.986]
Epoch [76/120    avg_loss:0.015, val_acc:0.988]
Epoch [77/120    avg_loss:0.022, val_acc:0.972]
Epoch [78/120    avg_loss:0.018, val_acc:0.982]
Epoch [79/120    avg_loss:0.016, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.984]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.989]
Epoch [84/120    avg_loss:0.025, val_acc:0.973]
Epoch [85/120    avg_loss:0.014, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.991]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.991]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.977]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.011, val_acc:0.991]
Epoch [97/120    avg_loss:0.012, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.991]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.019, val_acc:0.985]
Epoch [110/120    avg_loss:0.013, val_acc:0.975]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.036, val_acc:0.973]
Epoch [113/120    avg_loss:0.046, val_acc:0.962]
Epoch [114/120    avg_loss:0.029, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.988]
Epoch [116/120    avg_loss:0.011, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.989]
Epoch [118/120    avg_loss:0.012, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     0     1     0     0     0    70     0]
 [    0     7 18041     0    36     0     3     0     3     0]
 [    0     2     0  2007     0     0     0     0    24     3]
 [    0    38    28     0  2870     0     8     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    25     0     0     0  4851     0     0     2]
 [    0     0     0     0     0     0     0  1283     0     7]
 [    0     4     0     0    53     0     0     0  3487    27]
 [    0     0     0    16    14    42     0     0     0   847]]

Accuracy:
98.93717012508134

F1 scores:
[       nan 0.9905014  0.99718107 0.98891353 0.96535486 0.9841629
 0.99609856 0.99727944 0.97090352 0.93850416]

Kappa:
0.9859167861610422
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22e3164940>
supervision:full
center_pixel:True
Network :
Number of parameter: 21159==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.094, val_acc:0.086]
Epoch [2/120    avg_loss:1.820, val_acc:0.228]
Epoch [3/120    avg_loss:1.557, val_acc:0.229]
Epoch [4/120    avg_loss:1.431, val_acc:0.234]
Epoch [5/120    avg_loss:1.302, val_acc:0.284]
Epoch [6/120    avg_loss:1.215, val_acc:0.357]
Epoch [7/120    avg_loss:1.066, val_acc:0.362]
Epoch [8/120    avg_loss:1.005, val_acc:0.390]
Epoch [9/120    avg_loss:0.993, val_acc:0.519]
Epoch [10/120    avg_loss:0.844, val_acc:0.641]
Epoch [11/120    avg_loss:0.726, val_acc:0.645]
Epoch [12/120    avg_loss:0.618, val_acc:0.685]
Epoch [13/120    avg_loss:0.553, val_acc:0.728]
Epoch [14/120    avg_loss:0.498, val_acc:0.780]
Epoch [15/120    avg_loss:0.447, val_acc:0.796]
Epoch [16/120    avg_loss:0.379, val_acc:0.827]
Epoch [17/120    avg_loss:0.345, val_acc:0.853]
Epoch [18/120    avg_loss:0.315, val_acc:0.894]
Epoch [19/120    avg_loss:0.284, val_acc:0.864]
Epoch [20/120    avg_loss:0.284, val_acc:0.839]
Epoch [21/120    avg_loss:0.248, val_acc:0.919]
Epoch [22/120    avg_loss:0.205, val_acc:0.896]
Epoch [23/120    avg_loss:0.206, val_acc:0.913]
Epoch [24/120    avg_loss:0.181, val_acc:0.956]
Epoch [25/120    avg_loss:0.176, val_acc:0.955]
Epoch [26/120    avg_loss:0.140, val_acc:0.964]
Epoch [27/120    avg_loss:0.150, val_acc:0.956]
Epoch [28/120    avg_loss:0.134, val_acc:0.947]
Epoch [29/120    avg_loss:0.130, val_acc:0.965]
Epoch [30/120    avg_loss:0.108, val_acc:0.966]
Epoch [31/120    avg_loss:0.080, val_acc:0.953]
Epoch [32/120    avg_loss:0.100, val_acc:0.966]
Epoch [33/120    avg_loss:0.071, val_acc:0.978]
Epoch [34/120    avg_loss:0.074, val_acc:0.971]
Epoch [35/120    avg_loss:0.064, val_acc:0.978]
Epoch [36/120    avg_loss:0.062, val_acc:0.974]
Epoch [37/120    avg_loss:0.052, val_acc:0.969]
Epoch [38/120    avg_loss:0.050, val_acc:0.978]
Epoch [39/120    avg_loss:0.054, val_acc:0.972]
Epoch [40/120    avg_loss:0.052, val_acc:0.976]
Epoch [41/120    avg_loss:0.060, val_acc:0.972]
Epoch [42/120    avg_loss:0.058, val_acc:0.978]
Epoch [43/120    avg_loss:0.046, val_acc:0.972]
Epoch [44/120    avg_loss:0.056, val_acc:0.968]
Epoch [45/120    avg_loss:0.056, val_acc:0.959]
Epoch [46/120    avg_loss:0.045, val_acc:0.976]
Epoch [47/120    avg_loss:0.054, val_acc:0.979]
Epoch [48/120    avg_loss:0.047, val_acc:0.978]
Epoch [49/120    avg_loss:0.042, val_acc:0.978]
Epoch [50/120    avg_loss:0.036, val_acc:0.978]
Epoch [51/120    avg_loss:0.055, val_acc:0.967]
Epoch [52/120    avg_loss:0.048, val_acc:0.976]
Epoch [53/120    avg_loss:0.031, val_acc:0.978]
Epoch [54/120    avg_loss:0.032, val_acc:0.967]
Epoch [55/120    avg_loss:0.034, val_acc:0.975]
Epoch [56/120    avg_loss:0.035, val_acc:0.977]
Epoch [57/120    avg_loss:0.026, val_acc:0.983]
Epoch [58/120    avg_loss:0.022, val_acc:0.976]
Epoch [59/120    avg_loss:0.023, val_acc:0.972]
Epoch [60/120    avg_loss:0.027, val_acc:0.984]
Epoch [61/120    avg_loss:0.024, val_acc:0.978]
Epoch [62/120    avg_loss:0.019, val_acc:0.978]
Epoch [63/120    avg_loss:0.024, val_acc:0.983]
Epoch [64/120    avg_loss:0.023, val_acc:0.983]
Epoch [65/120    avg_loss:0.024, val_acc:0.984]
Epoch [66/120    avg_loss:0.028, val_acc:0.977]
Epoch [67/120    avg_loss:0.037, val_acc:0.974]
Epoch [68/120    avg_loss:0.024, val_acc:0.978]
Epoch [69/120    avg_loss:0.025, val_acc:0.975]
Epoch [70/120    avg_loss:0.018, val_acc:0.979]
Epoch [71/120    avg_loss:0.022, val_acc:0.979]
Epoch [72/120    avg_loss:0.014, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.984]
Epoch [74/120    avg_loss:0.020, val_acc:0.972]
Epoch [75/120    avg_loss:0.020, val_acc:0.978]
Epoch [76/120    avg_loss:0.011, val_acc:0.987]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.012, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.980]
Epoch [86/120    avg_loss:0.011, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0     0    27     0    11]
 [    0     0 18039     0    39     0    12     0     0     0]
 [    0     6     0  2012     0     0     0     0    18     0]
 [    0    16    19     0  2899     0    14     0    21     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     1     0     0  4857     0     0     1]
 [    0     0     0     0     0     0     0  1284     0     6]
 [    0    12     0    12    55     0     0     0  3492     0]
 [    0     0     0     1    14    39     0     0     0   865]]

Accuracy:
99.16612440652641

F1 scores:
[       nan 0.99440124 0.99753919 0.990645   0.96972738 0.98527746
 0.99518492 0.98731257 0.98338496 0.95844875]

Kappa:
0.9889527535339793
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f76a3f79a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.144, val_acc:0.085]
Epoch [2/120    avg_loss:1.877, val_acc:0.115]
Epoch [3/120    avg_loss:1.692, val_acc:0.128]
Epoch [4/120    avg_loss:1.552, val_acc:0.271]
Epoch [5/120    avg_loss:1.451, val_acc:0.371]
Epoch [6/120    avg_loss:1.309, val_acc:0.423]
Epoch [7/120    avg_loss:1.183, val_acc:0.430]
Epoch [8/120    avg_loss:1.053, val_acc:0.498]
Epoch [9/120    avg_loss:0.966, val_acc:0.479]
Epoch [10/120    avg_loss:0.869, val_acc:0.565]
Epoch [11/120    avg_loss:0.793, val_acc:0.647]
Epoch [12/120    avg_loss:0.693, val_acc:0.709]
Epoch [13/120    avg_loss:0.647, val_acc:0.713]
Epoch [14/120    avg_loss:0.563, val_acc:0.773]
Epoch [15/120    avg_loss:0.502, val_acc:0.805]
Epoch [16/120    avg_loss:0.448, val_acc:0.841]
Epoch [17/120    avg_loss:0.407, val_acc:0.836]
Epoch [18/120    avg_loss:0.365, val_acc:0.833]
Epoch [19/120    avg_loss:0.329, val_acc:0.857]
Epoch [20/120    avg_loss:0.325, val_acc:0.862]
Epoch [21/120    avg_loss:0.323, val_acc:0.882]
Epoch [22/120    avg_loss:0.276, val_acc:0.894]
Epoch [23/120    avg_loss:0.251, val_acc:0.878]
Epoch [24/120    avg_loss:0.259, val_acc:0.906]
Epoch [25/120    avg_loss:0.205, val_acc:0.927]
Epoch [26/120    avg_loss:0.179, val_acc:0.931]
Epoch [27/120    avg_loss:0.183, val_acc:0.938]
Epoch [28/120    avg_loss:0.161, val_acc:0.958]
Epoch [29/120    avg_loss:0.151, val_acc:0.962]
Epoch [30/120    avg_loss:0.128, val_acc:0.970]
Epoch [31/120    avg_loss:0.147, val_acc:0.954]
Epoch [32/120    avg_loss:0.153, val_acc:0.957]
Epoch [33/120    avg_loss:0.136, val_acc:0.948]
Epoch [34/120    avg_loss:0.101, val_acc:0.959]
Epoch [35/120    avg_loss:0.091, val_acc:0.959]
Epoch [36/120    avg_loss:0.103, val_acc:0.947]
Epoch [37/120    avg_loss:0.098, val_acc:0.972]
Epoch [38/120    avg_loss:0.076, val_acc:0.978]
Epoch [39/120    avg_loss:0.082, val_acc:0.983]
Epoch [40/120    avg_loss:0.074, val_acc:0.965]
Epoch [41/120    avg_loss:0.069, val_acc:0.969]
Epoch [42/120    avg_loss:0.094, val_acc:0.962]
Epoch [43/120    avg_loss:1.027, val_acc:0.669]
Epoch [44/120    avg_loss:0.607, val_acc:0.761]
Epoch [45/120    avg_loss:0.456, val_acc:0.797]
Epoch [46/120    avg_loss:0.389, val_acc:0.795]
Epoch [47/120    avg_loss:0.338, val_acc:0.854]
Epoch [48/120    avg_loss:0.314, val_acc:0.860]
Epoch [49/120    avg_loss:0.350, val_acc:0.833]
Epoch [50/120    avg_loss:0.263, val_acc:0.915]
Epoch [51/120    avg_loss:0.211, val_acc:0.925]
Epoch [52/120    avg_loss:0.212, val_acc:0.922]
Epoch [53/120    avg_loss:0.154, val_acc:0.934]
Epoch [54/120    avg_loss:0.155, val_acc:0.936]
Epoch [55/120    avg_loss:0.143, val_acc:0.943]
Epoch [56/120    avg_loss:0.145, val_acc:0.928]
Epoch [57/120    avg_loss:0.134, val_acc:0.944]
Epoch [58/120    avg_loss:0.125, val_acc:0.949]
Epoch [59/120    avg_loss:0.147, val_acc:0.947]
Epoch [60/120    avg_loss:0.130, val_acc:0.952]
Epoch [61/120    avg_loss:0.130, val_acc:0.953]
Epoch [62/120    avg_loss:0.126, val_acc:0.956]
Epoch [63/120    avg_loss:0.124, val_acc:0.955]
Epoch [64/120    avg_loss:0.111, val_acc:0.955]
Epoch [65/120    avg_loss:0.121, val_acc:0.947]
Epoch [66/120    avg_loss:0.125, val_acc:0.950]
Epoch [67/120    avg_loss:0.121, val_acc:0.953]
Epoch [68/120    avg_loss:0.114, val_acc:0.956]
Epoch [69/120    avg_loss:0.105, val_acc:0.955]
Epoch [70/120    avg_loss:0.096, val_acc:0.958]
Epoch [71/120    avg_loss:0.117, val_acc:0.958]
Epoch [72/120    avg_loss:0.114, val_acc:0.958]
Epoch [73/120    avg_loss:0.101, val_acc:0.957]
Epoch [74/120    avg_loss:0.110, val_acc:0.958]
Epoch [75/120    avg_loss:0.104, val_acc:0.954]
Epoch [76/120    avg_loss:0.107, val_acc:0.955]
Epoch [77/120    avg_loss:0.108, val_acc:0.956]
Epoch [78/120    avg_loss:0.113, val_acc:0.956]
Epoch [79/120    avg_loss:0.116, val_acc:0.956]
Epoch [80/120    avg_loss:0.103, val_acc:0.956]
Epoch [81/120    avg_loss:0.102, val_acc:0.956]
Epoch [82/120    avg_loss:0.104, val_acc:0.956]
Epoch [83/120    avg_loss:0.107, val_acc:0.956]
Epoch [84/120    avg_loss:0.113, val_acc:0.957]
Epoch [85/120    avg_loss:0.113, val_acc:0.956]
Epoch [86/120    avg_loss:0.105, val_acc:0.957]
Epoch [87/120    avg_loss:0.122, val_acc:0.956]
Epoch [88/120    avg_loss:0.102, val_acc:0.957]
Epoch [89/120    avg_loss:0.102, val_acc:0.956]
Epoch [90/120    avg_loss:0.111, val_acc:0.956]
Epoch [91/120    avg_loss:0.118, val_acc:0.956]
Epoch [92/120    avg_loss:0.101, val_acc:0.956]
Epoch [93/120    avg_loss:0.104, val_acc:0.956]
Epoch [94/120    avg_loss:0.114, val_acc:0.956]
Epoch [95/120    avg_loss:0.111, val_acc:0.956]
Epoch [96/120    avg_loss:0.112, val_acc:0.956]
Epoch [97/120    avg_loss:0.103, val_acc:0.956]
Epoch [98/120    avg_loss:0.099, val_acc:0.956]
Epoch [99/120    avg_loss:0.106, val_acc:0.956]
Epoch [100/120    avg_loss:0.107, val_acc:0.956]
Epoch [101/120    avg_loss:0.116, val_acc:0.956]
Epoch [102/120    avg_loss:0.099, val_acc:0.956]
Epoch [103/120    avg_loss:0.104, val_acc:0.956]
Epoch [104/120    avg_loss:0.099, val_acc:0.956]
Epoch [105/120    avg_loss:0.106, val_acc:0.956]
Epoch [106/120    avg_loss:0.116, val_acc:0.956]
Epoch [107/120    avg_loss:0.097, val_acc:0.956]
Epoch [108/120    avg_loss:0.106, val_acc:0.956]
Epoch [109/120    avg_loss:0.108, val_acc:0.956]
Epoch [110/120    avg_loss:0.120, val_acc:0.956]
Epoch [111/120    avg_loss:0.109, val_acc:0.956]
Epoch [112/120    avg_loss:0.112, val_acc:0.956]
Epoch [113/120    avg_loss:0.097, val_acc:0.956]
Epoch [114/120    avg_loss:0.104, val_acc:0.956]
Epoch [115/120    avg_loss:0.095, val_acc:0.956]
Epoch [116/120    avg_loss:0.095, val_acc:0.956]
Epoch [117/120    avg_loss:0.103, val_acc:0.956]
Epoch [118/120    avg_loss:0.113, val_acc:0.956]
Epoch [119/120    avg_loss:0.101, val_acc:0.956]
Epoch [120/120    avg_loss:0.121, val_acc:0.956]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6137     0    70    56     0     7    34    59    69]
 [    0     0 17791     0    52     0   237     0    10     0]
 [    0    18     0  1930     2     0     0     0    69    17]
 [    0    77    21     0  2825     0    15     0    34     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    47     0     0     0  4828     0     1     2]
 [    0    26     0     0     0     0     3  1251     0    10]
 [    0     0     0    10    88     0     5     0  3439    29]
 [    0     0     0     8    19    62     0     0     0   830]]

Accuracy:
97.21157785650591

F1 scores:
[       nan 0.96721828 0.98979109 0.95214603 0.93947456 0.97679641
 0.96821418 0.97165049 0.95753863 0.88486141]

Kappa:
0.9631598131078789
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc7ad2b978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.064, val_acc:0.131]
Epoch [2/120    avg_loss:1.869, val_acc:0.173]
Epoch [3/120    avg_loss:1.684, val_acc:0.256]
Epoch [4/120    avg_loss:1.554, val_acc:0.318]
Epoch [5/120    avg_loss:1.401, val_acc:0.336]
Epoch [6/120    avg_loss:1.256, val_acc:0.398]
Epoch [7/120    avg_loss:1.128, val_acc:0.392]
Epoch [8/120    avg_loss:0.978, val_acc:0.509]
Epoch [9/120    avg_loss:0.849, val_acc:0.538]
Epoch [10/120    avg_loss:0.762, val_acc:0.657]
Epoch [11/120    avg_loss:0.670, val_acc:0.531]
Epoch [12/120    avg_loss:0.609, val_acc:0.718]
Epoch [13/120    avg_loss:0.520, val_acc:0.762]
Epoch [14/120    avg_loss:0.451, val_acc:0.817]
Epoch [15/120    avg_loss:0.441, val_acc:0.828]
Epoch [16/120    avg_loss:0.397, val_acc:0.826]
Epoch [17/120    avg_loss:0.354, val_acc:0.895]
Epoch [18/120    avg_loss:0.359, val_acc:0.844]
Epoch [19/120    avg_loss:0.316, val_acc:0.829]
Epoch [20/120    avg_loss:0.264, val_acc:0.915]
Epoch [21/120    avg_loss:0.238, val_acc:0.932]
Epoch [22/120    avg_loss:0.214, val_acc:0.938]
Epoch [23/120    avg_loss:0.288, val_acc:0.916]
Epoch [24/120    avg_loss:0.211, val_acc:0.935]
Epoch [25/120    avg_loss:0.181, val_acc:0.923]
Epoch [26/120    avg_loss:0.156, val_acc:0.947]
Epoch [27/120    avg_loss:0.150, val_acc:0.937]
Epoch [28/120    avg_loss:0.139, val_acc:0.949]
Epoch [29/120    avg_loss:0.119, val_acc:0.946]
Epoch [30/120    avg_loss:0.159, val_acc:0.922]
Epoch [31/120    avg_loss:0.116, val_acc:0.954]
Epoch [32/120    avg_loss:0.096, val_acc:0.934]
Epoch [33/120    avg_loss:0.101, val_acc:0.953]
Epoch [34/120    avg_loss:0.103, val_acc:0.959]
Epoch [35/120    avg_loss:0.088, val_acc:0.968]
Epoch [36/120    avg_loss:0.079, val_acc:0.969]
Epoch [37/120    avg_loss:0.062, val_acc:0.972]
Epoch [38/120    avg_loss:0.068, val_acc:0.957]
Epoch [39/120    avg_loss:0.067, val_acc:0.973]
Epoch [40/120    avg_loss:0.055, val_acc:0.964]
Epoch [41/120    avg_loss:0.072, val_acc:0.969]
Epoch [42/120    avg_loss:0.073, val_acc:0.965]
Epoch [43/120    avg_loss:0.051, val_acc:0.973]
Epoch [44/120    avg_loss:0.050, val_acc:0.972]
Epoch [45/120    avg_loss:0.047, val_acc:0.971]
Epoch [46/120    avg_loss:0.036, val_acc:0.974]
Epoch [47/120    avg_loss:0.040, val_acc:0.979]
Epoch [48/120    avg_loss:0.041, val_acc:0.975]
Epoch [49/120    avg_loss:0.029, val_acc:0.971]
Epoch [50/120    avg_loss:0.033, val_acc:0.979]
Epoch [51/120    avg_loss:0.030, val_acc:0.977]
Epoch [52/120    avg_loss:0.023, val_acc:0.978]
Epoch [53/120    avg_loss:0.031, val_acc:0.980]
Epoch [54/120    avg_loss:0.050, val_acc:0.955]
Epoch [55/120    avg_loss:0.040, val_acc:0.972]
Epoch [56/120    avg_loss:0.036, val_acc:0.974]
Epoch [57/120    avg_loss:0.035, val_acc:0.975]
Epoch [58/120    avg_loss:0.025, val_acc:0.974]
Epoch [59/120    avg_loss:0.029, val_acc:0.979]
Epoch [60/120    avg_loss:0.026, val_acc:0.968]
Epoch [61/120    avg_loss:0.030, val_acc:0.962]
Epoch [62/120    avg_loss:0.022, val_acc:0.971]
Epoch [63/120    avg_loss:0.019, val_acc:0.982]
Epoch [64/120    avg_loss:0.024, val_acc:0.976]
Epoch [65/120    avg_loss:0.020, val_acc:0.981]
Epoch [66/120    avg_loss:0.023, val_acc:0.981]
Epoch [67/120    avg_loss:0.030, val_acc:0.975]
Epoch [68/120    avg_loss:0.038, val_acc:0.980]
Epoch [69/120    avg_loss:0.025, val_acc:0.982]
Epoch [70/120    avg_loss:0.018, val_acc:0.977]
Epoch [71/120    avg_loss:0.023, val_acc:0.980]
Epoch [72/120    avg_loss:0.034, val_acc:0.977]
Epoch [73/120    avg_loss:0.019, val_acc:0.983]
Epoch [74/120    avg_loss:0.011, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.022, val_acc:0.978]
Epoch [78/120    avg_loss:0.020, val_acc:0.977]
Epoch [79/120    avg_loss:0.014, val_acc:0.979]
Epoch [80/120    avg_loss:0.012, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.018, val_acc:0.966]
Epoch [83/120    avg_loss:0.014, val_acc:0.982]
Epoch [84/120    avg_loss:0.026, val_acc:0.982]
Epoch [85/120    avg_loss:0.022, val_acc:0.980]
Epoch [86/120    avg_loss:0.020, val_acc:0.972]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.022, val_acc:0.978]
Epoch [89/120    avg_loss:0.021, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.009, val_acc:0.980]
Epoch [92/120    avg_loss:0.011, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.987]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.015, val_acc:0.984]
Epoch [97/120    avg_loss:0.024, val_acc:0.978]
Epoch [98/120    avg_loss:0.015, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.009, val_acc:0.982]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.953]
Epoch [117/120    avg_loss:0.017, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     9     0     0     0    21     0]
 [    0     0 18037     0    39     0    13     0     1     0]
 [    0     3     0  1982     6     0     0     0    40     5]
 [    0    18    20     0  2897     0     6     0    31     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4847     0     1    23]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    25     0     0    61     0     0     0  3483     2]
 [    0     0     0     1    25    65     0     1     0   827]]

Accuracy:
98.97814089123467

F1 scores:
[       nan 0.99409938 0.99778724 0.986315   0.96422034 0.97570093
 0.99486864 0.99922481 0.97453833 0.93078222]

Kappa:
0.9864627762102826
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f598d5978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.202, val_acc:0.131]
Epoch [2/120    avg_loss:1.884, val_acc:0.101]
Epoch [3/120    avg_loss:1.705, val_acc:0.156]
Epoch [4/120    avg_loss:1.563, val_acc:0.225]
Epoch [5/120    avg_loss:1.466, val_acc:0.360]
Epoch [6/120    avg_loss:1.314, val_acc:0.544]
Epoch [7/120    avg_loss:1.337, val_acc:0.504]
Epoch [8/120    avg_loss:1.165, val_acc:0.517]
Epoch [9/120    avg_loss:1.048, val_acc:0.536]
Epoch [10/120    avg_loss:0.956, val_acc:0.534]
Epoch [11/120    avg_loss:0.839, val_acc:0.560]
Epoch [12/120    avg_loss:0.788, val_acc:0.562]
Epoch [13/120    avg_loss:0.671, val_acc:0.578]
Epoch [14/120    avg_loss:0.628, val_acc:0.647]
Epoch [15/120    avg_loss:0.565, val_acc:0.626]
Epoch [16/120    avg_loss:0.528, val_acc:0.695]
Epoch [17/120    avg_loss:0.461, val_acc:0.694]
Epoch [18/120    avg_loss:0.426, val_acc:0.703]
Epoch [19/120    avg_loss:0.384, val_acc:0.766]
Epoch [20/120    avg_loss:0.361, val_acc:0.809]
Epoch [21/120    avg_loss:0.312, val_acc:0.788]
Epoch [22/120    avg_loss:0.296, val_acc:0.836]
Epoch [23/120    avg_loss:0.252, val_acc:0.861]
Epoch [24/120    avg_loss:0.261, val_acc:0.907]
Epoch [25/120    avg_loss:0.248, val_acc:0.884]
Epoch [26/120    avg_loss:0.233, val_acc:0.860]
Epoch [27/120    avg_loss:0.226, val_acc:0.886]
Epoch [28/120    avg_loss:0.231, val_acc:0.928]
Epoch [29/120    avg_loss:0.189, val_acc:0.911]
Epoch [30/120    avg_loss:0.181, val_acc:0.918]
Epoch [31/120    avg_loss:0.204, val_acc:0.929]
Epoch [32/120    avg_loss:0.180, val_acc:0.945]
Epoch [33/120    avg_loss:0.156, val_acc:0.917]
Epoch [34/120    avg_loss:0.219, val_acc:0.948]
Epoch [35/120    avg_loss:0.134, val_acc:0.956]
Epoch [36/120    avg_loss:0.132, val_acc:0.940]
Epoch [37/120    avg_loss:0.112, val_acc:0.950]
Epoch [38/120    avg_loss:0.095, val_acc:0.964]
Epoch [39/120    avg_loss:0.081, val_acc:0.943]
Epoch [40/120    avg_loss:0.084, val_acc:0.962]
Epoch [41/120    avg_loss:0.078, val_acc:0.922]
Epoch [42/120    avg_loss:0.067, val_acc:0.967]
Epoch [43/120    avg_loss:0.119, val_acc:0.959]
Epoch [44/120    avg_loss:0.077, val_acc:0.962]
Epoch [45/120    avg_loss:0.075, val_acc:0.969]
Epoch [46/120    avg_loss:0.063, val_acc:0.976]
Epoch [47/120    avg_loss:0.090, val_acc:0.958]
Epoch [48/120    avg_loss:0.076, val_acc:0.979]
Epoch [49/120    avg_loss:0.050, val_acc:0.984]
Epoch [50/120    avg_loss:0.038, val_acc:0.978]
Epoch [51/120    avg_loss:0.041, val_acc:0.968]
Epoch [52/120    avg_loss:0.059, val_acc:0.972]
Epoch [53/120    avg_loss:0.060, val_acc:0.986]
Epoch [54/120    avg_loss:0.048, val_acc:0.918]
Epoch [55/120    avg_loss:0.099, val_acc:0.971]
Epoch [56/120    avg_loss:0.054, val_acc:0.982]
Epoch [57/120    avg_loss:0.039, val_acc:0.982]
Epoch [58/120    avg_loss:0.029, val_acc:0.980]
Epoch [59/120    avg_loss:0.033, val_acc:0.978]
Epoch [60/120    avg_loss:0.033, val_acc:0.982]
Epoch [61/120    avg_loss:0.024, val_acc:0.988]
Epoch [62/120    avg_loss:0.023, val_acc:0.984]
Epoch [63/120    avg_loss:0.022, val_acc:0.984]
Epoch [64/120    avg_loss:0.019, val_acc:0.984]
Epoch [65/120    avg_loss:0.021, val_acc:0.982]
Epoch [66/120    avg_loss:0.025, val_acc:0.978]
Epoch [67/120    avg_loss:0.019, val_acc:0.985]
Epoch [68/120    avg_loss:0.023, val_acc:0.977]
Epoch [69/120    avg_loss:0.022, val_acc:0.984]
Epoch [70/120    avg_loss:0.019, val_acc:0.988]
Epoch [71/120    avg_loss:0.015, val_acc:0.972]
Epoch [72/120    avg_loss:0.047, val_acc:0.973]
Epoch [73/120    avg_loss:0.033, val_acc:0.976]
Epoch [74/120    avg_loss:0.028, val_acc:0.981]
Epoch [75/120    avg_loss:0.028, val_acc:0.981]
Epoch [76/120    avg_loss:0.019, val_acc:0.991]
Epoch [77/120    avg_loss:0.024, val_acc:0.988]
Epoch [78/120    avg_loss:0.019, val_acc:0.987]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.016, val_acc:0.989]
Epoch [81/120    avg_loss:0.013, val_acc:0.991]
Epoch [82/120    avg_loss:0.014, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.990]
Epoch [84/120    avg_loss:0.013, val_acc:0.988]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.017, val_acc:0.982]
Epoch [88/120    avg_loss:0.024, val_acc:0.966]
Epoch [89/120    avg_loss:0.035, val_acc:0.984]
Epoch [90/120    avg_loss:0.019, val_acc:0.985]
Epoch [91/120    avg_loss:0.023, val_acc:0.982]
Epoch [92/120    avg_loss:0.015, val_acc:0.987]
Epoch [93/120    avg_loss:0.017, val_acc:0.987]
Epoch [94/120    avg_loss:0.086, val_acc:0.966]
Epoch [95/120    avg_loss:0.052, val_acc:0.980]
Epoch [96/120    avg_loss:0.026, val_acc:0.984]
Epoch [97/120    avg_loss:0.018, val_acc:0.984]
Epoch [98/120    avg_loss:0.018, val_acc:0.986]
Epoch [99/120    avg_loss:0.020, val_acc:0.989]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.012, val_acc:0.989]
Epoch [102/120    avg_loss:0.012, val_acc:0.988]
Epoch [103/120    avg_loss:0.014, val_acc:0.988]
Epoch [104/120    avg_loss:0.014, val_acc:0.991]
Epoch [105/120    avg_loss:0.013, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.991]
Epoch [107/120    avg_loss:0.015, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.991]
Epoch [109/120    avg_loss:0.012, val_acc:0.991]
Epoch [110/120    avg_loss:0.010, val_acc:0.991]
Epoch [111/120    avg_loss:0.008, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.011, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.991]
Epoch [116/120    avg_loss:0.010, val_acc:0.991]
Epoch [117/120    avg_loss:0.010, val_acc:0.991]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     0     3     0     0    10    25    24]
 [    0     0 18031     0    49     0    10     0     0     0]
 [    0     2     0  2015     2     0     0     0    16     1]
 [    0    28    19     0  2894     0     3     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4861     0     2    14]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     0     0    27    72     0     0     0  3466     6]
 [    0     0     0     6    14    34     0     0     0   865]]

Accuracy:
99.04080206299858

F1 scores:
[       nan 0.99283042 0.99781412 0.98677767 0.96370296 0.9871407
 0.99692371 0.99536321 0.97523917 0.94483889]

Kappa:
0.9872978229137492
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf87640908>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.153, val_acc:0.484]
Epoch [2/120    avg_loss:1.857, val_acc:0.378]
Epoch [3/120    avg_loss:1.730, val_acc:0.255]
Epoch [4/120    avg_loss:1.606, val_acc:0.317]
Epoch [5/120    avg_loss:1.478, val_acc:0.317]
Epoch [6/120    avg_loss:1.354, val_acc:0.352]
Epoch [7/120    avg_loss:1.267, val_acc:0.415]
Epoch [8/120    avg_loss:1.161, val_acc:0.413]
Epoch [9/120    avg_loss:1.072, val_acc:0.473]
Epoch [10/120    avg_loss:0.990, val_acc:0.518]
Epoch [11/120    avg_loss:0.915, val_acc:0.544]
Epoch [12/120    avg_loss:0.844, val_acc:0.627]
Epoch [13/120    avg_loss:0.771, val_acc:0.718]
Epoch [14/120    avg_loss:0.691, val_acc:0.700]
Epoch [15/120    avg_loss:0.650, val_acc:0.636]
Epoch [16/120    avg_loss:0.598, val_acc:0.728]
Epoch [17/120    avg_loss:0.524, val_acc:0.729]
Epoch [18/120    avg_loss:0.474, val_acc:0.758]
Epoch [19/120    avg_loss:0.455, val_acc:0.760]
Epoch [20/120    avg_loss:0.399, val_acc:0.806]
Epoch [21/120    avg_loss:0.360, val_acc:0.808]
Epoch [22/120    avg_loss:0.349, val_acc:0.794]
Epoch [23/120    avg_loss:0.318, val_acc:0.819]
Epoch [24/120    avg_loss:0.301, val_acc:0.896]
Epoch [25/120    avg_loss:0.280, val_acc:0.891]
Epoch [26/120    avg_loss:0.273, val_acc:0.903]
Epoch [27/120    avg_loss:0.214, val_acc:0.909]
Epoch [28/120    avg_loss:0.198, val_acc:0.906]
Epoch [29/120    avg_loss:0.229, val_acc:0.926]
Epoch [30/120    avg_loss:0.165, val_acc:0.930]
Epoch [31/120    avg_loss:0.145, val_acc:0.934]
Epoch [32/120    avg_loss:0.152, val_acc:0.952]
Epoch [33/120    avg_loss:0.141, val_acc:0.953]
Epoch [34/120    avg_loss:0.132, val_acc:0.927]
Epoch [35/120    avg_loss:0.142, val_acc:0.954]
Epoch [36/120    avg_loss:0.112, val_acc:0.948]
Epoch [37/120    avg_loss:0.099, val_acc:0.957]
Epoch [38/120    avg_loss:0.105, val_acc:0.952]
Epoch [39/120    avg_loss:0.128, val_acc:0.972]
Epoch [40/120    avg_loss:0.106, val_acc:0.953]
Epoch [41/120    avg_loss:0.085, val_acc:0.928]
Epoch [42/120    avg_loss:0.099, val_acc:0.966]
Epoch [43/120    avg_loss:0.075, val_acc:0.967]
Epoch [44/120    avg_loss:0.119, val_acc:0.934]
Epoch [45/120    avg_loss:0.099, val_acc:0.964]
Epoch [46/120    avg_loss:0.063, val_acc:0.972]
Epoch [47/120    avg_loss:0.063, val_acc:0.968]
Epoch [48/120    avg_loss:0.046, val_acc:0.973]
Epoch [49/120    avg_loss:0.045, val_acc:0.972]
Epoch [50/120    avg_loss:0.042, val_acc:0.972]
Epoch [51/120    avg_loss:0.045, val_acc:0.966]
Epoch [52/120    avg_loss:0.061, val_acc:0.963]
Epoch [53/120    avg_loss:0.034, val_acc:0.974]
Epoch [54/120    avg_loss:0.036, val_acc:0.979]
Epoch [55/120    avg_loss:0.029, val_acc:0.979]
Epoch [56/120    avg_loss:0.035, val_acc:0.973]
Epoch [57/120    avg_loss:0.041, val_acc:0.966]
Epoch [58/120    avg_loss:0.047, val_acc:0.956]
Epoch [59/120    avg_loss:0.046, val_acc:0.958]
Epoch [60/120    avg_loss:0.034, val_acc:0.976]
Epoch [61/120    avg_loss:0.046, val_acc:0.975]
Epoch [62/120    avg_loss:0.037, val_acc:0.977]
Epoch [63/120    avg_loss:0.025, val_acc:0.978]
Epoch [64/120    avg_loss:0.022, val_acc:0.984]
Epoch [65/120    avg_loss:0.021, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.985]
Epoch [68/120    avg_loss:0.018, val_acc:0.984]
Epoch [69/120    avg_loss:0.016, val_acc:0.984]
Epoch [70/120    avg_loss:0.024, val_acc:0.983]
Epoch [71/120    avg_loss:0.020, val_acc:0.971]
Epoch [72/120    avg_loss:0.019, val_acc:0.975]
Epoch [73/120    avg_loss:0.016, val_acc:0.983]
Epoch [74/120    avg_loss:0.016, val_acc:0.978]
Epoch [75/120    avg_loss:0.016, val_acc:0.968]
Epoch [76/120    avg_loss:0.016, val_acc:0.978]
Epoch [77/120    avg_loss:0.018, val_acc:0.976]
Epoch [78/120    avg_loss:0.015, val_acc:0.974]
Epoch [79/120    avg_loss:0.017, val_acc:0.982]
Epoch [80/120    avg_loss:0.013, val_acc:0.984]
Epoch [81/120    avg_loss:0.018, val_acc:0.984]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.012, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.983]
Epoch [104/120    avg_loss:0.011, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     0     0     0     1     0    37     0]
 [    0     3 18049     0    23     0    15     0     0     0]
 [    0     6     0  2012     3     0     0     0    14     1]
 [    0    28    23     0  2878     0     7     0    36     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4845     0     0    20]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     2     0     7    84     0     0     0  3478     0]
 [    0     0     0     5    23   107     0     0     0   784]]

Accuracy:
98.88896922372449

F1 scores:
[       nan 0.99401477 0.99787146 0.991133   0.96205917 0.96061833
 0.99425405 0.99883586 0.97477578 0.90793283]

Kappa:
0.9852784995910693
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd4dd1ca940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.094, val_acc:0.475]
Epoch [2/120    avg_loss:1.780, val_acc:0.516]
Epoch [3/120    avg_loss:1.613, val_acc:0.522]
Epoch [4/120    avg_loss:1.473, val_acc:0.551]
Epoch [5/120    avg_loss:1.356, val_acc:0.584]
Epoch [6/120    avg_loss:1.234, val_acc:0.616]
Epoch [7/120    avg_loss:1.161, val_acc:0.582]
Epoch [8/120    avg_loss:1.036, val_acc:0.601]
Epoch [9/120    avg_loss:0.904, val_acc:0.652]
Epoch [10/120    avg_loss:0.841, val_acc:0.687]
Epoch [11/120    avg_loss:0.755, val_acc:0.716]
Epoch [12/120    avg_loss:0.673, val_acc:0.707]
Epoch [13/120    avg_loss:0.612, val_acc:0.746]
Epoch [14/120    avg_loss:0.541, val_acc:0.769]
Epoch [15/120    avg_loss:0.491, val_acc:0.811]
Epoch [16/120    avg_loss:0.401, val_acc:0.861]
Epoch [17/120    avg_loss:0.382, val_acc:0.890]
Epoch [18/120    avg_loss:0.332, val_acc:0.891]
Epoch [19/120    avg_loss:0.320, val_acc:0.870]
Epoch [20/120    avg_loss:0.249, val_acc:0.920]
Epoch [21/120    avg_loss:0.263, val_acc:0.933]
Epoch [22/120    avg_loss:0.257, val_acc:0.910]
Epoch [23/120    avg_loss:0.213, val_acc:0.917]
Epoch [24/120    avg_loss:0.191, val_acc:0.934]
Epoch [25/120    avg_loss:0.257, val_acc:0.922]
Epoch [26/120    avg_loss:0.226, val_acc:0.914]
Epoch [27/120    avg_loss:0.189, val_acc:0.934]
Epoch [28/120    avg_loss:0.172, val_acc:0.939]
Epoch [29/120    avg_loss:0.174, val_acc:0.930]
Epoch [30/120    avg_loss:0.152, val_acc:0.961]
Epoch [31/120    avg_loss:0.123, val_acc:0.954]
Epoch [32/120    avg_loss:0.098, val_acc:0.963]
Epoch [33/120    avg_loss:0.125, val_acc:0.922]
Epoch [34/120    avg_loss:0.111, val_acc:0.961]
Epoch [35/120    avg_loss:0.101, val_acc:0.959]
Epoch [36/120    avg_loss:0.089, val_acc:0.972]
Epoch [37/120    avg_loss:0.079, val_acc:0.968]
Epoch [38/120    avg_loss:0.091, val_acc:0.968]
Epoch [39/120    avg_loss:0.114, val_acc:0.968]
Epoch [40/120    avg_loss:0.084, val_acc:0.966]
Epoch [41/120    avg_loss:0.061, val_acc:0.977]
Epoch [42/120    avg_loss:0.064, val_acc:0.973]
Epoch [43/120    avg_loss:0.049, val_acc:0.974]
Epoch [44/120    avg_loss:0.078, val_acc:0.953]
Epoch [45/120    avg_loss:0.080, val_acc:0.961]
Epoch [46/120    avg_loss:0.087, val_acc:0.976]
Epoch [47/120    avg_loss:0.073, val_acc:0.974]
Epoch [48/120    avg_loss:0.055, val_acc:0.957]
Epoch [49/120    avg_loss:0.063, val_acc:0.945]
Epoch [50/120    avg_loss:0.055, val_acc:0.976]
Epoch [51/120    avg_loss:0.051, val_acc:0.959]
Epoch [52/120    avg_loss:0.492, val_acc:0.578]
Epoch [53/120    avg_loss:1.025, val_acc:0.723]
Epoch [54/120    avg_loss:0.710, val_acc:0.766]
Epoch [55/120    avg_loss:0.588, val_acc:0.811]
Epoch [56/120    avg_loss:0.579, val_acc:0.806]
Epoch [57/120    avg_loss:0.561, val_acc:0.816]
Epoch [58/120    avg_loss:0.544, val_acc:0.821]
Epoch [59/120    avg_loss:0.509, val_acc:0.818]
Epoch [60/120    avg_loss:0.525, val_acc:0.809]
Epoch [61/120    avg_loss:0.512, val_acc:0.817]
Epoch [62/120    avg_loss:0.473, val_acc:0.819]
Epoch [63/120    avg_loss:0.491, val_acc:0.823]
Epoch [64/120    avg_loss:0.472, val_acc:0.841]
Epoch [65/120    avg_loss:0.444, val_acc:0.848]
Epoch [66/120    avg_loss:0.460, val_acc:0.840]
Epoch [67/120    avg_loss:0.455, val_acc:0.845]
Epoch [68/120    avg_loss:0.432, val_acc:0.847]
Epoch [69/120    avg_loss:0.430, val_acc:0.846]
Epoch [70/120    avg_loss:0.427, val_acc:0.845]
Epoch [71/120    avg_loss:0.424, val_acc:0.847]
Epoch [72/120    avg_loss:0.418, val_acc:0.847]
Epoch [73/120    avg_loss:0.432, val_acc:0.847]
Epoch [74/120    avg_loss:0.412, val_acc:0.848]
Epoch [75/120    avg_loss:0.430, val_acc:0.846]
Epoch [76/120    avg_loss:0.417, val_acc:0.849]
Epoch [77/120    avg_loss:0.440, val_acc:0.845]
Epoch [78/120    avg_loss:0.411, val_acc:0.850]
Epoch [79/120    avg_loss:0.415, val_acc:0.853]
Epoch [80/120    avg_loss:0.431, val_acc:0.847]
Epoch [81/120    avg_loss:0.428, val_acc:0.847]
Epoch [82/120    avg_loss:0.424, val_acc:0.847]
Epoch [83/120    avg_loss:0.423, val_acc:0.847]
Epoch [84/120    avg_loss:0.416, val_acc:0.847]
Epoch [85/120    avg_loss:0.411, val_acc:0.847]
Epoch [86/120    avg_loss:0.411, val_acc:0.847]
Epoch [87/120    avg_loss:0.427, val_acc:0.847]
Epoch [88/120    avg_loss:0.422, val_acc:0.847]
Epoch [89/120    avg_loss:0.428, val_acc:0.847]
Epoch [90/120    avg_loss:0.435, val_acc:0.846]
Epoch [91/120    avg_loss:0.437, val_acc:0.847]
Epoch [92/120    avg_loss:0.412, val_acc:0.848]
Epoch [93/120    avg_loss:0.424, val_acc:0.847]
Epoch [94/120    avg_loss:0.413, val_acc:0.847]
Epoch [95/120    avg_loss:0.425, val_acc:0.847]
Epoch [96/120    avg_loss:0.434, val_acc:0.847]
Epoch [97/120    avg_loss:0.431, val_acc:0.847]
Epoch [98/120    avg_loss:0.424, val_acc:0.847]
Epoch [99/120    avg_loss:0.421, val_acc:0.847]
Epoch [100/120    avg_loss:0.398, val_acc:0.847]
Epoch [101/120    avg_loss:0.414, val_acc:0.847]
Epoch [102/120    avg_loss:0.418, val_acc:0.847]
Epoch [103/120    avg_loss:0.430, val_acc:0.847]
Epoch [104/120    avg_loss:0.436, val_acc:0.847]
Epoch [105/120    avg_loss:0.416, val_acc:0.847]
Epoch [106/120    avg_loss:0.420, val_acc:0.847]
Epoch [107/120    avg_loss:0.400, val_acc:0.847]
Epoch [108/120    avg_loss:0.440, val_acc:0.847]
Epoch [109/120    avg_loss:0.440, val_acc:0.847]
Epoch [110/120    avg_loss:0.415, val_acc:0.847]
Epoch [111/120    avg_loss:0.401, val_acc:0.847]
Epoch [112/120    avg_loss:0.411, val_acc:0.847]
Epoch [113/120    avg_loss:0.427, val_acc:0.847]
Epoch [114/120    avg_loss:0.413, val_acc:0.847]
Epoch [115/120    avg_loss:0.407, val_acc:0.847]
Epoch [116/120    avg_loss:0.406, val_acc:0.847]
Epoch [117/120    avg_loss:0.420, val_acc:0.847]
Epoch [118/120    avg_loss:0.413, val_acc:0.847]
Epoch [119/120    avg_loss:0.426, val_acc:0.847]
Epoch [120/120    avg_loss:0.408, val_acc:0.847]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4746     1   187   607     0    50     3   681   157]
 [    0     0 15472     0   751     0  1858     0     9     0]
 [    0     1     0  1720    21     0     0     0   200    94]
 [    0   172   743     0  1922     0    71     0    55     9]
 [    0     0     0     0     0  1300     0     0     5     0]
 [    0     3    63     0    86     0  4664     0    59     3]
 [    0   115     0     0     0     0     1  1161     0    13]
 [    0   104     2    88    74     0     0     0  3298     5]
 [    0    21     0     5    15   154     0     1    15   708]]

Accuracy:
84.32988696888631

F1 scores:
[       nan 0.81869933 0.90029385 0.85232904 0.59615385 0.94237042
 0.80958167 0.94582485 0.83567718 0.74213836]

Kappa:
0.7967531991712948
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f26efde7940>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.092, val_acc:0.381]
Epoch [2/120    avg_loss:1.808, val_acc:0.416]
Epoch [3/120    avg_loss:1.666, val_acc:0.496]
Epoch [4/120    avg_loss:1.528, val_acc:0.537]
Epoch [5/120    avg_loss:1.427, val_acc:0.559]
Epoch [6/120    avg_loss:1.331, val_acc:0.632]
Epoch [7/120    avg_loss:1.217, val_acc:0.669]
Epoch [8/120    avg_loss:1.107, val_acc:0.703]
Epoch [9/120    avg_loss:1.020, val_acc:0.700]
Epoch [10/120    avg_loss:0.960, val_acc:0.691]
Epoch [11/120    avg_loss:0.855, val_acc:0.681]
Epoch [12/120    avg_loss:0.774, val_acc:0.725]
Epoch [13/120    avg_loss:0.708, val_acc:0.701]
Epoch [14/120    avg_loss:0.648, val_acc:0.680]
Epoch [15/120    avg_loss:0.558, val_acc:0.663]
Epoch [16/120    avg_loss:0.555, val_acc:0.713]
Epoch [17/120    avg_loss:0.534, val_acc:0.703]
Epoch [18/120    avg_loss:0.462, val_acc:0.722]
Epoch [19/120    avg_loss:0.507, val_acc:0.733]
Epoch [20/120    avg_loss:0.426, val_acc:0.794]
Epoch [21/120    avg_loss:0.397, val_acc:0.775]
Epoch [22/120    avg_loss:0.378, val_acc:0.861]
Epoch [23/120    avg_loss:0.345, val_acc:0.858]
Epoch [24/120    avg_loss:0.297, val_acc:0.858]
Epoch [25/120    avg_loss:0.288, val_acc:0.879]
Epoch [26/120    avg_loss:0.290, val_acc:0.857]
Epoch [27/120    avg_loss:0.259, val_acc:0.907]
Epoch [28/120    avg_loss:0.244, val_acc:0.890]
Epoch [29/120    avg_loss:0.207, val_acc:0.923]
Epoch [30/120    avg_loss:0.174, val_acc:0.914]
Epoch [31/120    avg_loss:0.152, val_acc:0.935]
Epoch [32/120    avg_loss:0.128, val_acc:0.951]
Epoch [33/120    avg_loss:0.141, val_acc:0.948]
Epoch [34/120    avg_loss:0.147, val_acc:0.952]
Epoch [35/120    avg_loss:0.133, val_acc:0.962]
Epoch [36/120    avg_loss:0.102, val_acc:0.946]
Epoch [37/120    avg_loss:0.094, val_acc:0.967]
Epoch [38/120    avg_loss:0.103, val_acc:0.946]
Epoch [39/120    avg_loss:0.091, val_acc:0.963]
Epoch [40/120    avg_loss:0.086, val_acc:0.959]
Epoch [41/120    avg_loss:0.088, val_acc:0.962]
Epoch [42/120    avg_loss:0.101, val_acc:0.955]
Epoch [43/120    avg_loss:0.098, val_acc:0.961]
Epoch [44/120    avg_loss:0.145, val_acc:0.928]
Epoch [45/120    avg_loss:0.114, val_acc:0.970]
Epoch [46/120    avg_loss:0.075, val_acc:0.971]
Epoch [47/120    avg_loss:0.056, val_acc:0.969]
Epoch [48/120    avg_loss:0.051, val_acc:0.966]
Epoch [49/120    avg_loss:0.064, val_acc:0.970]
Epoch [50/120    avg_loss:0.061, val_acc:0.951]
Epoch [51/120    avg_loss:0.055, val_acc:0.976]
Epoch [52/120    avg_loss:0.045, val_acc:0.971]
Epoch [53/120    avg_loss:0.052, val_acc:0.968]
Epoch [54/120    avg_loss:0.036, val_acc:0.969]
Epoch [55/120    avg_loss:0.030, val_acc:0.974]
Epoch [56/120    avg_loss:0.035, val_acc:0.974]
Epoch [57/120    avg_loss:0.034, val_acc:0.977]
Epoch [58/120    avg_loss:0.049, val_acc:0.965]
Epoch [59/120    avg_loss:0.060, val_acc:0.973]
Epoch [60/120    avg_loss:0.037, val_acc:0.978]
Epoch [61/120    avg_loss:0.038, val_acc:0.980]
Epoch [62/120    avg_loss:0.036, val_acc:0.980]
Epoch [63/120    avg_loss:0.032, val_acc:0.976]
Epoch [64/120    avg_loss:0.029, val_acc:0.969]
Epoch [65/120    avg_loss:0.026, val_acc:0.980]
Epoch [66/120    avg_loss:0.030, val_acc:0.977]
Epoch [67/120    avg_loss:0.026, val_acc:0.978]
Epoch [68/120    avg_loss:0.037, val_acc:0.980]
Epoch [69/120    avg_loss:0.051, val_acc:0.975]
Epoch [70/120    avg_loss:0.043, val_acc:0.973]
Epoch [71/120    avg_loss:0.037, val_acc:0.974]
Epoch [72/120    avg_loss:0.030, val_acc:0.984]
Epoch [73/120    avg_loss:0.017, val_acc:0.980]
Epoch [74/120    avg_loss:0.020, val_acc:0.979]
Epoch [75/120    avg_loss:0.041, val_acc:0.962]
Epoch [76/120    avg_loss:0.024, val_acc:0.986]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.016, val_acc:0.982]
Epoch [79/120    avg_loss:0.046, val_acc:0.981]
Epoch [80/120    avg_loss:0.038, val_acc:0.983]
Epoch [81/120    avg_loss:0.022, val_acc:0.972]
Epoch [82/120    avg_loss:0.023, val_acc:0.983]
Epoch [83/120    avg_loss:0.026, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.019, val_acc:0.984]
Epoch [87/120    avg_loss:0.015, val_acc:0.982]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.015, val_acc:0.986]
Epoch [91/120    avg_loss:0.017, val_acc:0.977]
Epoch [92/120    avg_loss:0.014, val_acc:0.985]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.012, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.982]
Epoch [99/120    avg_loss:0.013, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.976]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.019, val_acc:0.981]
Epoch [114/120    avg_loss:0.010, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6351     0     6     0     0     0     0    75     0]
 [    0     0 18047     0    38     0     5     0     0     0]
 [    0     1     0  2020     2     0     0     0     9     4]
 [    0    40    19     0  2876     0     7     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4849     0     0    28]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     4     0     6    54     0     0     0  3507     0]
 [    0     0     0     6    15    61     0     5     0   832]]

Accuracy:
98.99501120670956

F1 scores:
[       nan 0.99017774 0.9982576  0.99165439 0.9655867  0.97716211
 0.99579012 0.99767802 0.97525028 0.93273543]

Kappa:
0.9866881492504562
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f23f01c99b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.127, val_acc:0.081]
Epoch [2/120    avg_loss:1.836, val_acc:0.099]
Epoch [3/120    avg_loss:1.662, val_acc:0.146]
Epoch [4/120    avg_loss:1.549, val_acc:0.234]
Epoch [5/120    avg_loss:1.448, val_acc:0.342]
Epoch [6/120    avg_loss:1.361, val_acc:0.362]
Epoch [7/120    avg_loss:1.256, val_acc:0.429]
Epoch [8/120    avg_loss:1.126, val_acc:0.464]
Epoch [9/120    avg_loss:1.023, val_acc:0.464]
Epoch [10/120    avg_loss:0.920, val_acc:0.594]
Epoch [11/120    avg_loss:0.787, val_acc:0.663]
Epoch [12/120    avg_loss:0.715, val_acc:0.678]
Epoch [13/120    avg_loss:0.624, val_acc:0.698]
Epoch [14/120    avg_loss:0.567, val_acc:0.779]
Epoch [15/120    avg_loss:0.477, val_acc:0.809]
Epoch [16/120    avg_loss:0.413, val_acc:0.793]
Epoch [17/120    avg_loss:0.374, val_acc:0.868]
Epoch [18/120    avg_loss:0.377, val_acc:0.862]
Epoch [19/120    avg_loss:0.282, val_acc:0.896]
Epoch [20/120    avg_loss:0.267, val_acc:0.922]
Epoch [21/120    avg_loss:0.309, val_acc:0.903]
Epoch [22/120    avg_loss:0.248, val_acc:0.931]
Epoch [23/120    avg_loss:0.220, val_acc:0.944]
Epoch [24/120    avg_loss:0.167, val_acc:0.929]
Epoch [25/120    avg_loss:0.191, val_acc:0.934]
Epoch [26/120    avg_loss:0.147, val_acc:0.897]
Epoch [27/120    avg_loss:0.145, val_acc:0.947]
Epoch [28/120    avg_loss:0.134, val_acc:0.959]
Epoch [29/120    avg_loss:0.130, val_acc:0.960]
Epoch [30/120    avg_loss:0.149, val_acc:0.964]
Epoch [31/120    avg_loss:0.099, val_acc:0.959]
Epoch [32/120    avg_loss:0.089, val_acc:0.955]
Epoch [33/120    avg_loss:0.084, val_acc:0.965]
Epoch [34/120    avg_loss:0.096, val_acc:0.965]
Epoch [35/120    avg_loss:0.066, val_acc:0.947]
Epoch [36/120    avg_loss:0.088, val_acc:0.968]
Epoch [37/120    avg_loss:0.063, val_acc:0.951]
Epoch [38/120    avg_loss:0.063, val_acc:0.968]
Epoch [39/120    avg_loss:0.062, val_acc:0.948]
Epoch [40/120    avg_loss:0.082, val_acc:0.967]
Epoch [41/120    avg_loss:0.074, val_acc:0.962]
Epoch [42/120    avg_loss:0.052, val_acc:0.968]
Epoch [43/120    avg_loss:0.063, val_acc:0.948]
Epoch [44/120    avg_loss:0.058, val_acc:0.960]
Epoch [45/120    avg_loss:0.049, val_acc:0.973]
Epoch [46/120    avg_loss:0.055, val_acc:0.979]
Epoch [47/120    avg_loss:0.058, val_acc:0.972]
Epoch [48/120    avg_loss:0.053, val_acc:0.972]
Epoch [49/120    avg_loss:0.042, val_acc:0.978]
Epoch [50/120    avg_loss:0.040, val_acc:0.971]
Epoch [51/120    avg_loss:0.089, val_acc:0.958]
Epoch [52/120    avg_loss:0.060, val_acc:0.960]
Epoch [53/120    avg_loss:0.056, val_acc:0.973]
Epoch [54/120    avg_loss:0.032, val_acc:0.981]
Epoch [55/120    avg_loss:0.033, val_acc:0.973]
Epoch [56/120    avg_loss:0.028, val_acc:0.975]
Epoch [57/120    avg_loss:0.034, val_acc:0.972]
Epoch [58/120    avg_loss:0.026, val_acc:0.980]
Epoch [59/120    avg_loss:0.020, val_acc:0.977]
Epoch [60/120    avg_loss:0.035, val_acc:0.972]
Epoch [61/120    avg_loss:0.040, val_acc:0.973]
Epoch [62/120    avg_loss:0.023, val_acc:0.972]
Epoch [63/120    avg_loss:0.021, val_acc:0.977]
Epoch [64/120    avg_loss:0.029, val_acc:0.963]
Epoch [65/120    avg_loss:0.025, val_acc:0.959]
Epoch [66/120    avg_loss:0.022, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.968]
Epoch [68/120    avg_loss:0.019, val_acc:0.971]
Epoch [69/120    avg_loss:0.017, val_acc:0.977]
Epoch [70/120    avg_loss:0.015, val_acc:0.976]
Epoch [71/120    avg_loss:0.015, val_acc:0.978]
Epoch [72/120    avg_loss:0.009, val_acc:0.979]
Epoch [73/120    avg_loss:0.013, val_acc:0.979]
Epoch [74/120    avg_loss:0.012, val_acc:0.979]
Epoch [75/120    avg_loss:0.012, val_acc:0.979]
Epoch [76/120    avg_loss:0.010, val_acc:0.979]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.012, val_acc:0.981]
Epoch [79/120    avg_loss:0.010, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.981]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.982]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.011, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.981]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.011, val_acc:0.981]
Epoch [88/120    avg_loss:0.011, val_acc:0.983]
Epoch [89/120    avg_loss:0.013, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.012, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.984]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.983]
Epoch [106/120    avg_loss:0.015, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     0    36     0]
 [    0     0 18052     0    24     0    14     0     0     0]
 [    0     4     0  1988     0     0     0     0    43     1]
 [    0    25    20     0  2865     0     9     0    52     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    14     0     0  4845     0     0    18]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0    17     0     4    48     0     0     0  3498     4]
 [    0     0     0     8    14    44     0     0     0   853]]

Accuracy:
99.02152170245584

F1 scores:
[       nan 0.99363057 0.9983685  0.9817284  0.96741516 0.98342125
 0.99425405 0.99805825 0.97166667 0.94725153]

Kappa:
0.9870363186362747
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9336cd9978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.175, val_acc:0.138]
Epoch [2/120    avg_loss:1.881, val_acc:0.201]
Epoch [3/120    avg_loss:1.703, val_acc:0.271]
Epoch [4/120    avg_loss:1.564, val_acc:0.303]
Epoch [5/120    avg_loss:1.433, val_acc:0.321]
Epoch [6/120    avg_loss:1.285, val_acc:0.358]
Epoch [7/120    avg_loss:1.144, val_acc:0.436]
Epoch [8/120    avg_loss:1.047, val_acc:0.475]
Epoch [9/120    avg_loss:0.951, val_acc:0.654]
Epoch [10/120    avg_loss:0.859, val_acc:0.683]
Epoch [11/120    avg_loss:0.762, val_acc:0.717]
Epoch [12/120    avg_loss:0.642, val_acc:0.752]
Epoch [13/120    avg_loss:0.594, val_acc:0.775]
Epoch [14/120    avg_loss:0.531, val_acc:0.735]
Epoch [15/120    avg_loss:0.453, val_acc:0.810]
Epoch [16/120    avg_loss:0.430, val_acc:0.820]
Epoch [17/120    avg_loss:0.372, val_acc:0.846]
Epoch [18/120    avg_loss:0.332, val_acc:0.828]
Epoch [19/120    avg_loss:0.287, val_acc:0.913]
Epoch [20/120    avg_loss:0.284, val_acc:0.815]
Epoch [21/120    avg_loss:0.273, val_acc:0.943]
Epoch [22/120    avg_loss:0.229, val_acc:0.864]
Epoch [23/120    avg_loss:0.246, val_acc:0.934]
Epoch [24/120    avg_loss:0.198, val_acc:0.939]
Epoch [25/120    avg_loss:0.190, val_acc:0.881]
Epoch [26/120    avg_loss:0.178, val_acc:0.942]
Epoch [27/120    avg_loss:0.178, val_acc:0.940]
Epoch [28/120    avg_loss:0.160, val_acc:0.934]
Epoch [29/120    avg_loss:0.138, val_acc:0.934]
Epoch [30/120    avg_loss:0.145, val_acc:0.953]
Epoch [31/120    avg_loss:0.158, val_acc:0.951]
Epoch [32/120    avg_loss:0.156, val_acc:0.954]
Epoch [33/120    avg_loss:0.144, val_acc:0.962]
Epoch [34/120    avg_loss:0.110, val_acc:0.946]
Epoch [35/120    avg_loss:0.092, val_acc:0.970]
Epoch [36/120    avg_loss:0.102, val_acc:0.967]
Epoch [37/120    avg_loss:0.083, val_acc:0.970]
Epoch [38/120    avg_loss:0.078, val_acc:0.962]
Epoch [39/120    avg_loss:0.077, val_acc:0.969]
Epoch [40/120    avg_loss:0.086, val_acc:0.936]
Epoch [41/120    avg_loss:0.090, val_acc:0.965]
Epoch [42/120    avg_loss:0.088, val_acc:0.971]
Epoch [43/120    avg_loss:0.059, val_acc:0.967]
Epoch [44/120    avg_loss:0.070, val_acc:0.964]
Epoch [45/120    avg_loss:0.060, val_acc:0.941]
Epoch [46/120    avg_loss:0.074, val_acc:0.971]
Epoch [47/120    avg_loss:0.051, val_acc:0.963]
Epoch [48/120    avg_loss:0.055, val_acc:0.971]
Epoch [49/120    avg_loss:0.062, val_acc:0.969]
Epoch [50/120    avg_loss:0.062, val_acc:0.970]
Epoch [51/120    avg_loss:0.051, val_acc:0.969]
Epoch [52/120    avg_loss:0.039, val_acc:0.969]
Epoch [53/120    avg_loss:0.040, val_acc:0.978]
Epoch [54/120    avg_loss:0.034, val_acc:0.971]
Epoch [55/120    avg_loss:0.054, val_acc:0.969]
Epoch [56/120    avg_loss:0.053, val_acc:0.966]
Epoch [57/120    avg_loss:0.032, val_acc:0.970]
Epoch [58/120    avg_loss:0.033, val_acc:0.968]
Epoch [59/120    avg_loss:0.024, val_acc:0.963]
Epoch [60/120    avg_loss:0.023, val_acc:0.974]
Epoch [61/120    avg_loss:0.035, val_acc:0.967]
Epoch [62/120    avg_loss:0.028, val_acc:0.973]
Epoch [63/120    avg_loss:0.029, val_acc:0.979]
Epoch [64/120    avg_loss:0.029, val_acc:0.960]
Epoch [65/120    avg_loss:0.040, val_acc:0.963]
Epoch [66/120    avg_loss:0.041, val_acc:0.972]
Epoch [67/120    avg_loss:0.030, val_acc:0.979]
Epoch [68/120    avg_loss:0.024, val_acc:0.979]
Epoch [69/120    avg_loss:0.019, val_acc:0.976]
Epoch [70/120    avg_loss:0.024, val_acc:0.975]
Epoch [71/120    avg_loss:0.017, val_acc:0.975]
Epoch [72/120    avg_loss:0.023, val_acc:0.969]
Epoch [73/120    avg_loss:0.038, val_acc:0.960]
Epoch [74/120    avg_loss:0.027, val_acc:0.979]
Epoch [75/120    avg_loss:0.016, val_acc:0.975]
Epoch [76/120    avg_loss:0.018, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.979]
Epoch [78/120    avg_loss:0.023, val_acc:0.954]
Epoch [79/120    avg_loss:0.023, val_acc:0.978]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.017, val_acc:0.971]
Epoch [82/120    avg_loss:0.013, val_acc:0.975]
Epoch [83/120    avg_loss:0.011, val_acc:0.975]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.024, val_acc:0.975]
Epoch [86/120    avg_loss:0.018, val_acc:0.970]
Epoch [87/120    avg_loss:0.013, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.976]
Epoch [89/120    avg_loss:0.014, val_acc:0.970]
Epoch [90/120    avg_loss:0.016, val_acc:0.972]
Epoch [91/120    avg_loss:0.011, val_acc:0.978]
Epoch [92/120    avg_loss:0.023, val_acc:0.978]
Epoch [93/120    avg_loss:0.019, val_acc:0.979]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.980]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.979]
Epoch [101/120    avg_loss:0.008, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.979]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.009, val_acc:0.979]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.008, val_acc:0.979]
Epoch [110/120    avg_loss:0.006, val_acc:0.979]
Epoch [111/120    avg_loss:0.009, val_acc:0.979]
Epoch [112/120    avg_loss:0.006, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.979]
Epoch [114/120    avg_loss:0.007, val_acc:0.979]
Epoch [115/120    avg_loss:0.007, val_acc:0.979]
Epoch [116/120    avg_loss:0.007, val_acc:0.979]
Epoch [117/120    avg_loss:0.007, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.006, val_acc:0.979]
Epoch [120/120    avg_loss:0.007, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     0     0     0     0    33     1]
 [    0     4 18018     0    59     0     9     0     0     0]
 [    0     2     0  1989     4     0     0     0    38     3]
 [    0    24    19     0  2879     0    11     0    39     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     2     0     0  4870     0     0     5]
 [    0     0     0     0     0     0     5  1284     0     1]
 [    0    14     0     3    74     0     0     0  3480     0]
 [    0     0     0     2    17    51     0     0     0   849]]

Accuracy:
98.9853710264382

F1 scores:
[       nan 0.99394128 0.9974535  0.98660714 0.95886761 0.98083427
 0.99662335 0.997669   0.97193129 0.95500562]

Kappa:
0.9865619867909589
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff055a148d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.115, val_acc:0.148]
Epoch [2/120    avg_loss:1.833, val_acc:0.155]
Epoch [3/120    avg_loss:1.680, val_acc:0.225]
Epoch [4/120    avg_loss:1.537, val_acc:0.258]
Epoch [5/120    avg_loss:1.435, val_acc:0.316]
Epoch [6/120    avg_loss:1.332, val_acc:0.380]
Epoch [7/120    avg_loss:1.197, val_acc:0.405]
Epoch [8/120    avg_loss:1.106, val_acc:0.468]
Epoch [9/120    avg_loss:1.008, val_acc:0.563]
Epoch [10/120    avg_loss:0.936, val_acc:0.655]
Epoch [11/120    avg_loss:0.841, val_acc:0.672]
Epoch [12/120    avg_loss:0.737, val_acc:0.704]
Epoch [13/120    avg_loss:0.669, val_acc:0.713]
Epoch [14/120    avg_loss:0.587, val_acc:0.736]
Epoch [15/120    avg_loss:0.554, val_acc:0.765]
Epoch [16/120    avg_loss:0.478, val_acc:0.728]
Epoch [17/120    avg_loss:0.428, val_acc:0.802]
Epoch [18/120    avg_loss:0.395, val_acc:0.792]
Epoch [19/120    avg_loss:0.391, val_acc:0.807]
Epoch [20/120    avg_loss:0.366, val_acc:0.830]
Epoch [21/120    avg_loss:0.305, val_acc:0.803]
Epoch [22/120    avg_loss:0.273, val_acc:0.832]
Epoch [23/120    avg_loss:0.272, val_acc:0.853]
Epoch [24/120    avg_loss:0.277, val_acc:0.889]
Epoch [25/120    avg_loss:0.240, val_acc:0.903]
Epoch [26/120    avg_loss:0.214, val_acc:0.931]
Epoch [27/120    avg_loss:0.174, val_acc:0.909]
Epoch [28/120    avg_loss:0.192, val_acc:0.939]
Epoch [29/120    avg_loss:0.150, val_acc:0.922]
Epoch [30/120    avg_loss:0.129, val_acc:0.946]
Epoch [31/120    avg_loss:0.157, val_acc:0.951]
Epoch [32/120    avg_loss:0.133, val_acc:0.933]
Epoch [33/120    avg_loss:0.136, val_acc:0.938]
Epoch [34/120    avg_loss:0.124, val_acc:0.930]
Epoch [35/120    avg_loss:0.099, val_acc:0.935]
Epoch [36/120    avg_loss:0.104, val_acc:0.954]
Epoch [37/120    avg_loss:0.108, val_acc:0.943]
Epoch [38/120    avg_loss:0.075, val_acc:0.959]
Epoch [39/120    avg_loss:0.073, val_acc:0.954]
Epoch [40/120    avg_loss:0.082, val_acc:0.949]
Epoch [41/120    avg_loss:0.080, val_acc:0.937]
Epoch [42/120    avg_loss:0.065, val_acc:0.943]
Epoch [43/120    avg_loss:0.069, val_acc:0.955]
Epoch [44/120    avg_loss:0.071, val_acc:0.962]
Epoch [45/120    avg_loss:1.005, val_acc:0.575]
Epoch [46/120    avg_loss:1.428, val_acc:0.578]
Epoch [47/120    avg_loss:1.236, val_acc:0.517]
Epoch [48/120    avg_loss:1.131, val_acc:0.650]
Epoch [49/120    avg_loss:1.041, val_acc:0.676]
Epoch [50/120    avg_loss:0.971, val_acc:0.446]
Epoch [51/120    avg_loss:0.952, val_acc:0.459]
Epoch [52/120    avg_loss:0.938, val_acc:0.461]
Epoch [53/120    avg_loss:0.909, val_acc:0.724]
Epoch [54/120    avg_loss:0.831, val_acc:0.707]
Epoch [55/120    avg_loss:0.818, val_acc:0.718]
Epoch [56/120    avg_loss:0.779, val_acc:0.566]
Epoch [57/120    avg_loss:0.759, val_acc:0.701]
Epoch [58/120    avg_loss:0.718, val_acc:0.716]
Epoch [59/120    avg_loss:0.720, val_acc:0.739]
Epoch [60/120    avg_loss:0.742, val_acc:0.733]
Epoch [61/120    avg_loss:0.702, val_acc:0.720]
Epoch [62/120    avg_loss:0.700, val_acc:0.737]
Epoch [63/120    avg_loss:0.688, val_acc:0.739]
Epoch [64/120    avg_loss:0.699, val_acc:0.733]
Epoch [65/120    avg_loss:0.695, val_acc:0.741]
Epoch [66/120    avg_loss:0.718, val_acc:0.733]
Epoch [67/120    avg_loss:0.695, val_acc:0.733]
Epoch [68/120    avg_loss:0.702, val_acc:0.734]
Epoch [69/120    avg_loss:0.661, val_acc:0.741]
Epoch [70/120    avg_loss:0.686, val_acc:0.740]
Epoch [71/120    avg_loss:0.712, val_acc:0.740]
Epoch [72/120    avg_loss:0.654, val_acc:0.741]
Epoch [73/120    avg_loss:0.674, val_acc:0.740]
Epoch [74/120    avg_loss:0.683, val_acc:0.741]
Epoch [75/120    avg_loss:0.660, val_acc:0.741]
Epoch [76/120    avg_loss:0.674, val_acc:0.743]
Epoch [77/120    avg_loss:0.671, val_acc:0.742]
Epoch [78/120    avg_loss:0.659, val_acc:0.744]
Epoch [79/120    avg_loss:0.679, val_acc:0.744]
Epoch [80/120    avg_loss:0.673, val_acc:0.745]
Epoch [81/120    avg_loss:0.669, val_acc:0.745]
Epoch [82/120    avg_loss:0.680, val_acc:0.745]
Epoch [83/120    avg_loss:0.679, val_acc:0.743]
Epoch [84/120    avg_loss:0.657, val_acc:0.743]
Epoch [85/120    avg_loss:0.680, val_acc:0.744]
Epoch [86/120    avg_loss:0.679, val_acc:0.744]
Epoch [87/120    avg_loss:0.689, val_acc:0.745]
Epoch [88/120    avg_loss:0.662, val_acc:0.745]
Epoch [89/120    avg_loss:0.664, val_acc:0.745]
Epoch [90/120    avg_loss:0.665, val_acc:0.745]
Epoch [91/120    avg_loss:0.671, val_acc:0.745]
Epoch [92/120    avg_loss:0.692, val_acc:0.746]
Epoch [93/120    avg_loss:0.682, val_acc:0.746]
Epoch [94/120    avg_loss:0.659, val_acc:0.746]
Epoch [95/120    avg_loss:0.671, val_acc:0.746]
Epoch [96/120    avg_loss:0.688, val_acc:0.746]
Epoch [97/120    avg_loss:0.674, val_acc:0.746]
Epoch [98/120    avg_loss:0.657, val_acc:0.746]
Epoch [99/120    avg_loss:0.660, val_acc:0.746]
Epoch [100/120    avg_loss:0.680, val_acc:0.746]
Epoch [101/120    avg_loss:0.679, val_acc:0.746]
Epoch [102/120    avg_loss:0.665, val_acc:0.745]
Epoch [103/120    avg_loss:0.666, val_acc:0.746]
Epoch [104/120    avg_loss:0.669, val_acc:0.746]
Epoch [105/120    avg_loss:0.655, val_acc:0.746]
Epoch [106/120    avg_loss:0.669, val_acc:0.746]
Epoch [107/120    avg_loss:0.670, val_acc:0.746]
Epoch [108/120    avg_loss:0.640, val_acc:0.746]
Epoch [109/120    avg_loss:0.662, val_acc:0.746]
Epoch [110/120    avg_loss:0.667, val_acc:0.746]
Epoch [111/120    avg_loss:0.648, val_acc:0.746]
Epoch [112/120    avg_loss:0.659, val_acc:0.746]
Epoch [113/120    avg_loss:0.657, val_acc:0.746]
Epoch [114/120    avg_loss:0.659, val_acc:0.746]
Epoch [115/120    avg_loss:0.661, val_acc:0.746]
Epoch [116/120    avg_loss:0.657, val_acc:0.746]
Epoch [117/120    avg_loss:0.676, val_acc:0.746]
Epoch [118/120    avg_loss:0.653, val_acc:0.746]
Epoch [119/120    avg_loss:0.655, val_acc:0.746]
Epoch [120/120    avg_loss:0.672, val_acc:0.746]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5207     0     1   355     0     0   151   536   182]
 [    0    16 14263     0   154     0  3657     0     0     0]
 [    0    53     0  1669    15     0     0     0   219    80]
 [    0   238   268    19  2028     0   297     0    90    32]
 [    0     0     0     4     0  1299     0     2     0     0]
 [    0    17  1262    91   295     0  3031     0   182     0]
 [    0    78     0     0    21     0     0  1158     1    32]
 [    0   312     0    91    60     0    22     0  3062    24]
 [    0    12     0     7    11   159     0     7     1   722]]

Accuracy:
78.17945195575157

F1 scores:
[       nan 0.84221593 0.84189712 0.85196529 0.68617831 0.9402823
 0.51005469 0.88803681 0.79926912 0.72526369]

Kappa:
0.7179319976623106
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f66dc3ce978>
supervision:full
center_pixel:True
Network :
Number of parameter: 23719==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.182, val_acc:0.151]
Epoch [2/120    avg_loss:1.833, val_acc:0.141]
Epoch [3/120    avg_loss:1.657, val_acc:0.184]
Epoch [4/120    avg_loss:1.511, val_acc:0.286]
Epoch [5/120    avg_loss:1.357, val_acc:0.321]
Epoch [6/120    avg_loss:1.260, val_acc:0.329]
Epoch [7/120    avg_loss:1.146, val_acc:0.397]
Epoch [8/120    avg_loss:1.012, val_acc:0.451]
Epoch [9/120    avg_loss:0.917, val_acc:0.560]
Epoch [10/120    avg_loss:0.821, val_acc:0.581]
Epoch [11/120    avg_loss:0.751, val_acc:0.678]
Epoch [12/120    avg_loss:0.664, val_acc:0.666]
Epoch [13/120    avg_loss:0.595, val_acc:0.727]
Epoch [14/120    avg_loss:0.503, val_acc:0.744]
Epoch [15/120    avg_loss:0.446, val_acc:0.773]
Epoch [16/120    avg_loss:0.509, val_acc:0.759]
Epoch [17/120    avg_loss:0.384, val_acc:0.793]
Epoch [18/120    avg_loss:0.365, val_acc:0.800]
Epoch [19/120    avg_loss:0.330, val_acc:0.799]
Epoch [20/120    avg_loss:0.319, val_acc:0.820]
Epoch [21/120    avg_loss:0.293, val_acc:0.827]
Epoch [22/120    avg_loss:0.260, val_acc:0.904]
Epoch [23/120    avg_loss:0.227, val_acc:0.905]
Epoch [24/120    avg_loss:0.245, val_acc:0.917]
Epoch [25/120    avg_loss:0.235, val_acc:0.911]
Epoch [26/120    avg_loss:0.192, val_acc:0.943]
Epoch [27/120    avg_loss:0.181, val_acc:0.946]
Epoch [28/120    avg_loss:0.172, val_acc:0.926]
Epoch [29/120    avg_loss:0.144, val_acc:0.962]
Epoch [30/120    avg_loss:0.187, val_acc:0.933]
Epoch [31/120    avg_loss:0.154, val_acc:0.954]
Epoch [32/120    avg_loss:0.123, val_acc:0.951]
Epoch [33/120    avg_loss:0.108, val_acc:0.964]
Epoch [34/120    avg_loss:0.119, val_acc:0.961]
Epoch [35/120    avg_loss:0.544, val_acc:0.552]
Epoch [36/120    avg_loss:1.495, val_acc:0.566]
Epoch [37/120    avg_loss:1.336, val_acc:0.589]
Epoch [38/120    avg_loss:1.273, val_acc:0.608]
Epoch [39/120    avg_loss:1.223, val_acc:0.621]
Epoch [40/120    avg_loss:1.132, val_acc:0.621]
Epoch [41/120    avg_loss:1.086, val_acc:0.637]
Epoch [42/120    avg_loss:1.051, val_acc:0.671]
Epoch [43/120    avg_loss:1.029, val_acc:0.608]
Epoch [44/120    avg_loss:0.982, val_acc:0.632]
Epoch [45/120    avg_loss:0.936, val_acc:0.716]
Epoch [46/120    avg_loss:0.912, val_acc:0.675]
Epoch [47/120    avg_loss:0.877, val_acc:0.685]
Epoch [48/120    avg_loss:0.905, val_acc:0.687]
Epoch [49/120    avg_loss:0.883, val_acc:0.688]
Epoch [50/120    avg_loss:0.879, val_acc:0.696]
Epoch [51/120    avg_loss:0.896, val_acc:0.704]
Epoch [52/120    avg_loss:0.873, val_acc:0.718]
Epoch [53/120    avg_loss:0.874, val_acc:0.720]
Epoch [54/120    avg_loss:0.880, val_acc:0.715]
Epoch [55/120    avg_loss:0.874, val_acc:0.728]
Epoch [56/120    avg_loss:0.871, val_acc:0.722]
Epoch [57/120    avg_loss:0.880, val_acc:0.720]
Epoch [58/120    avg_loss:0.863, val_acc:0.730]
Epoch [59/120    avg_loss:0.876, val_acc:0.729]
Epoch [60/120    avg_loss:0.835, val_acc:0.729]
Epoch [61/120    avg_loss:0.860, val_acc:0.728]
Epoch [62/120    avg_loss:0.862, val_acc:0.731]
Epoch [63/120    avg_loss:0.870, val_acc:0.729]
Epoch [64/120    avg_loss:0.855, val_acc:0.729]
Epoch [65/120    avg_loss:0.869, val_acc:0.729]
Epoch [66/120    avg_loss:0.845, val_acc:0.730]
Epoch [67/120    avg_loss:0.865, val_acc:0.731]
Epoch [68/120    avg_loss:0.841, val_acc:0.731]
Epoch [69/120    avg_loss:0.856, val_acc:0.731]
Epoch [70/120    avg_loss:0.851, val_acc:0.730]
Epoch [71/120    avg_loss:0.861, val_acc:0.730]
Epoch [72/120    avg_loss:0.874, val_acc:0.730]
Epoch [73/120    avg_loss:0.874, val_acc:0.730]
Epoch [74/120    avg_loss:0.853, val_acc:0.730]
Epoch [75/120    avg_loss:0.842, val_acc:0.730]
Epoch [76/120    avg_loss:0.863, val_acc:0.731]
Epoch [77/120    avg_loss:0.855, val_acc:0.731]
Epoch [78/120    avg_loss:0.878, val_acc:0.731]
Epoch [79/120    avg_loss:0.860, val_acc:0.731]
Epoch [80/120    avg_loss:0.858, val_acc:0.730]
Epoch [81/120    avg_loss:0.850, val_acc:0.731]
Epoch [82/120    avg_loss:0.906, val_acc:0.731]
Epoch [83/120    avg_loss:0.849, val_acc:0.731]
Epoch [84/120    avg_loss:0.863, val_acc:0.731]
Epoch [85/120    avg_loss:0.844, val_acc:0.731]
Epoch [86/120    avg_loss:0.864, val_acc:0.731]
Epoch [87/120    avg_loss:0.867, val_acc:0.731]
Epoch [88/120    avg_loss:0.851, val_acc:0.731]
Epoch [89/120    avg_loss:0.849, val_acc:0.731]
Epoch [90/120    avg_loss:0.866, val_acc:0.731]
Epoch [91/120    avg_loss:0.857, val_acc:0.731]
Epoch [92/120    avg_loss:0.861, val_acc:0.731]
Epoch [93/120    avg_loss:0.860, val_acc:0.731]
Epoch [94/120    avg_loss:0.857, val_acc:0.731]
Epoch [95/120    avg_loss:0.854, val_acc:0.731]
Epoch [96/120    avg_loss:0.856, val_acc:0.731]
Epoch [97/120    avg_loss:0.879, val_acc:0.731]
Epoch [98/120    avg_loss:0.846, val_acc:0.731]
Epoch [99/120    avg_loss:0.851, val_acc:0.731]
Epoch [100/120    avg_loss:0.840, val_acc:0.731]
Epoch [101/120    avg_loss:0.829, val_acc:0.731]
Epoch [102/120    avg_loss:0.839, val_acc:0.731]
Epoch [103/120    avg_loss:0.853, val_acc:0.731]
Epoch [104/120    avg_loss:0.856, val_acc:0.731]
Epoch [105/120    avg_loss:0.865, val_acc:0.731]
Epoch [106/120    avg_loss:0.861, val_acc:0.731]
Epoch [107/120    avg_loss:0.875, val_acc:0.731]
Epoch [108/120    avg_loss:0.857, val_acc:0.731]
Epoch [109/120    avg_loss:0.844, val_acc:0.731]
Epoch [110/120    avg_loss:0.843, val_acc:0.731]
Epoch [111/120    avg_loss:0.846, val_acc:0.731]
Epoch [112/120    avg_loss:0.869, val_acc:0.731]
Epoch [113/120    avg_loss:0.856, val_acc:0.731]
Epoch [114/120    avg_loss:0.852, val_acc:0.731]
Epoch [115/120    avg_loss:0.840, val_acc:0.731]
Epoch [116/120    avg_loss:0.848, val_acc:0.731]
Epoch [117/120    avg_loss:0.870, val_acc:0.731]
Epoch [118/120    avg_loss:0.853, val_acc:0.731]
Epoch [119/120    avg_loss:0.847, val_acc:0.731]
Epoch [120/120    avg_loss:0.867, val_acc:0.731]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  3840    10    75   617     0   141   524   646   579]
 [    0    23 13313     0   253     0  4495     0     6     0]
 [    0    42     0  1482   100     0    90     0   194   128]
 [    0    89    81    53  1906     0   632     7   174    30]
 [    0     0     0     2     0  1303     0     0     0     0]
 [    0    15   546    18   356     0  3704     0   239     0]
 [    0    73     0     2     0     0     6  1181     0    28]
 [    0   232     0   313    90     0    58     0  2840    38]
 [    0     8     0    23    11   183     0    13     0   681]]

Accuracy:
72.90386330224375

F1 scores:
[       nan 0.71415287 0.83102372 0.74025974 0.60459952 0.93371551
 0.52899172 0.78341625 0.74054759 0.56679151]

Kappa:
0.6577158055868961
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff50e2b9908>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.572, val_acc:0.660]
Epoch [2/120    avg_loss:0.890, val_acc:0.723]
Epoch [3/120    avg_loss:0.588, val_acc:0.664]
Epoch [4/120    avg_loss:0.516, val_acc:0.790]
Epoch [5/120    avg_loss:0.444, val_acc:0.796]
Epoch [6/120    avg_loss:0.416, val_acc:0.782]
Epoch [7/120    avg_loss:0.371, val_acc:0.766]
Epoch [8/120    avg_loss:0.307, val_acc:0.882]
Epoch [9/120    avg_loss:0.246, val_acc:0.882]
Epoch [10/120    avg_loss:0.340, val_acc:0.750]
Epoch [11/120    avg_loss:0.268, val_acc:0.903]
Epoch [12/120    avg_loss:0.272, val_acc:0.882]
Epoch [13/120    avg_loss:0.211, val_acc:0.914]
Epoch [14/120    avg_loss:0.190, val_acc:0.914]
Epoch [15/120    avg_loss:0.208, val_acc:0.869]
Epoch [16/120    avg_loss:0.159, val_acc:0.898]
Epoch [17/120    avg_loss:0.185, val_acc:0.928]
Epoch [18/120    avg_loss:0.148, val_acc:0.922]
Epoch [19/120    avg_loss:0.157, val_acc:0.829]
Epoch [20/120    avg_loss:0.120, val_acc:0.929]
Epoch [21/120    avg_loss:0.125, val_acc:0.894]
Epoch [22/120    avg_loss:0.155, val_acc:0.932]
Epoch [23/120    avg_loss:0.112, val_acc:0.914]
Epoch [24/120    avg_loss:0.112, val_acc:0.863]
Epoch [25/120    avg_loss:0.103, val_acc:0.921]
Epoch [26/120    avg_loss:0.093, val_acc:0.941]
Epoch [27/120    avg_loss:0.138, val_acc:0.927]
Epoch [28/120    avg_loss:0.108, val_acc:0.921]
Epoch [29/120    avg_loss:0.080, val_acc:0.947]
Epoch [30/120    avg_loss:0.085, val_acc:0.911]
Epoch [31/120    avg_loss:0.075, val_acc:0.895]
Epoch [32/120    avg_loss:0.078, val_acc:0.955]
Epoch [33/120    avg_loss:0.072, val_acc:0.913]
Epoch [34/120    avg_loss:0.120, val_acc:0.955]
Epoch [35/120    avg_loss:0.113, val_acc:0.905]
Epoch [36/120    avg_loss:0.074, val_acc:0.935]
Epoch [37/120    avg_loss:0.091, val_acc:0.936]
Epoch [38/120    avg_loss:0.079, val_acc:0.942]
Epoch [39/120    avg_loss:0.068, val_acc:0.959]
Epoch [40/120    avg_loss:0.100, val_acc:0.948]
Epoch [41/120    avg_loss:0.083, val_acc:0.945]
Epoch [42/120    avg_loss:0.076, val_acc:0.948]
Epoch [43/120    avg_loss:0.043, val_acc:0.965]
Epoch [44/120    avg_loss:0.048, val_acc:0.951]
Epoch [45/120    avg_loss:0.057, val_acc:0.934]
Epoch [46/120    avg_loss:0.047, val_acc:0.963]
Epoch [47/120    avg_loss:0.041, val_acc:0.933]
Epoch [48/120    avg_loss:0.051, val_acc:0.966]
Epoch [49/120    avg_loss:0.026, val_acc:0.969]
Epoch [50/120    avg_loss:0.049, val_acc:0.965]
Epoch [51/120    avg_loss:0.040, val_acc:0.965]
Epoch [52/120    avg_loss:0.031, val_acc:0.967]
Epoch [53/120    avg_loss:0.018, val_acc:0.969]
Epoch [54/120    avg_loss:0.022, val_acc:0.969]
Epoch [55/120    avg_loss:0.031, val_acc:0.961]
Epoch [56/120    avg_loss:0.025, val_acc:0.965]
Epoch [57/120    avg_loss:0.048, val_acc:0.957]
Epoch [58/120    avg_loss:0.030, val_acc:0.961]
Epoch [59/120    avg_loss:0.039, val_acc:0.961]
Epoch [60/120    avg_loss:0.037, val_acc:0.968]
Epoch [61/120    avg_loss:0.022, val_acc:0.933]
Epoch [62/120    avg_loss:0.040, val_acc:0.974]
Epoch [63/120    avg_loss:0.012, val_acc:0.972]
Epoch [64/120    avg_loss:0.034, val_acc:0.952]
Epoch [65/120    avg_loss:0.045, val_acc:0.969]
Epoch [66/120    avg_loss:0.023, val_acc:0.967]
Epoch [67/120    avg_loss:0.019, val_acc:0.974]
Epoch [68/120    avg_loss:0.009, val_acc:0.977]
Epoch [69/120    avg_loss:0.024, val_acc:0.974]
Epoch [70/120    avg_loss:0.011, val_acc:0.970]
Epoch [71/120    avg_loss:0.011, val_acc:0.974]
Epoch [72/120    avg_loss:0.008, val_acc:0.978]
Epoch [73/120    avg_loss:0.012, val_acc:0.974]
Epoch [74/120    avg_loss:0.011, val_acc:0.976]
Epoch [75/120    avg_loss:0.017, val_acc:0.979]
Epoch [76/120    avg_loss:0.007, val_acc:0.974]
Epoch [77/120    avg_loss:0.012, val_acc:0.978]
Epoch [78/120    avg_loss:0.008, val_acc:0.979]
Epoch [79/120    avg_loss:0.021, val_acc:0.976]
Epoch [80/120    avg_loss:0.008, val_acc:0.977]
Epoch [81/120    avg_loss:0.010, val_acc:0.969]
Epoch [82/120    avg_loss:0.031, val_acc:0.970]
Epoch [83/120    avg_loss:0.016, val_acc:0.978]
Epoch [84/120    avg_loss:0.012, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.972]
Epoch [87/120    avg_loss:0.010, val_acc:0.975]
Epoch [88/120    avg_loss:0.005, val_acc:0.975]
Epoch [89/120    avg_loss:0.081, val_acc:0.974]
Epoch [90/120    avg_loss:0.018, val_acc:0.942]
Epoch [91/120    avg_loss:0.023, val_acc:0.972]
Epoch [92/120    avg_loss:0.019, val_acc:0.971]
Epoch [93/120    avg_loss:0.050, val_acc:0.967]
Epoch [94/120    avg_loss:0.032, val_acc:0.974]
Epoch [95/120    avg_loss:0.012, val_acc:0.970]
Epoch [96/120    avg_loss:0.025, val_acc:0.970]
Epoch [97/120    avg_loss:0.025, val_acc:0.958]
Epoch [98/120    avg_loss:0.012, val_acc:0.972]
Epoch [99/120    avg_loss:0.009, val_acc:0.975]
Epoch [100/120    avg_loss:0.006, val_acc:0.974]
Epoch [101/120    avg_loss:0.008, val_acc:0.975]
Epoch [102/120    avg_loss:0.007, val_acc:0.973]
Epoch [103/120    avg_loss:0.005, val_acc:0.974]
Epoch [104/120    avg_loss:0.005, val_acc:0.974]
Epoch [105/120    avg_loss:0.006, val_acc:0.973]
Epoch [106/120    avg_loss:0.007, val_acc:0.974]
Epoch [107/120    avg_loss:0.006, val_acc:0.974]
Epoch [108/120    avg_loss:0.008, val_acc:0.973]
Epoch [109/120    avg_loss:0.008, val_acc:0.975]
Epoch [110/120    avg_loss:0.007, val_acc:0.976]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.015, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.975]
Epoch [114/120    avg_loss:0.005, val_acc:0.975]
Epoch [115/120    avg_loss:0.006, val_acc:0.975]
Epoch [116/120    avg_loss:0.008, val_acc:0.975]
Epoch [117/120    avg_loss:0.008, val_acc:0.975]
Epoch [118/120    avg_loss:0.007, val_acc:0.975]
Epoch [119/120    avg_loss:0.006, val_acc:0.975]
Epoch [120/120    avg_loss:0.005, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6358     0     0     0     0     0    21    53     0]
 [    0     0 17981     0    16     0    90     0     3     0]
 [    0     0     0  1871     0     0     0     0   159     6]
 [    0    16     6     0  2929     0    12     1     8     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0    18     0     0     1     0     0  1270     1     0]
 [    0     2     0   102    34     0     0     0  3431     2]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.65519485214374

F1 scores:
[       nan 0.99142367 0.99675712 0.93339985 0.98420699 0.99808795
 0.98944805 0.98373354 0.94962635 0.99293862]

Kappa:
0.9821993757980793
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff454b20898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.520, val_acc:0.400]
Epoch [2/120    avg_loss:0.898, val_acc:0.683]
Epoch [3/120    avg_loss:0.621, val_acc:0.736]
Epoch [4/120    avg_loss:0.486, val_acc:0.761]
Epoch [5/120    avg_loss:0.410, val_acc:0.823]
Epoch [6/120    avg_loss:0.419, val_acc:0.666]
Epoch [7/120    avg_loss:0.377, val_acc:0.834]
Epoch [8/120    avg_loss:0.357, val_acc:0.845]
Epoch [9/120    avg_loss:0.341, val_acc:0.864]
Epoch [10/120    avg_loss:0.277, val_acc:0.841]
Epoch [11/120    avg_loss:0.237, val_acc:0.880]
Epoch [12/120    avg_loss:0.253, val_acc:0.885]
Epoch [13/120    avg_loss:0.489, val_acc:0.688]
Epoch [14/120    avg_loss:0.494, val_acc:0.811]
Epoch [15/120    avg_loss:0.436, val_acc:0.659]
Epoch [16/120    avg_loss:0.333, val_acc:0.815]
Epoch [17/120    avg_loss:0.341, val_acc:0.801]
Epoch [18/120    avg_loss:0.274, val_acc:0.801]
Epoch [19/120    avg_loss:0.233, val_acc:0.863]
Epoch [20/120    avg_loss:0.216, val_acc:0.821]
Epoch [21/120    avg_loss:0.197, val_acc:0.914]
Epoch [22/120    avg_loss:0.208, val_acc:0.864]
Epoch [23/120    avg_loss:0.192, val_acc:0.868]
Epoch [24/120    avg_loss:0.177, val_acc:0.907]
Epoch [25/120    avg_loss:0.236, val_acc:0.837]
Epoch [26/120    avg_loss:0.143, val_acc:0.937]
Epoch [27/120    avg_loss:0.145, val_acc:0.930]
Epoch [28/120    avg_loss:0.103, val_acc:0.954]
Epoch [29/120    avg_loss:0.086, val_acc:0.951]
Epoch [30/120    avg_loss:0.100, val_acc:0.910]
Epoch [31/120    avg_loss:0.103, val_acc:0.870]
Epoch [32/120    avg_loss:0.097, val_acc:0.935]
Epoch [33/120    avg_loss:0.072, val_acc:0.962]
Epoch [34/120    avg_loss:0.097, val_acc:0.868]
Epoch [35/120    avg_loss:0.093, val_acc:0.961]
Epoch [36/120    avg_loss:0.073, val_acc:0.927]
Epoch [37/120    avg_loss:0.103, val_acc:0.965]
Epoch [38/120    avg_loss:0.080, val_acc:0.946]
Epoch [39/120    avg_loss:0.062, val_acc:0.960]
Epoch [40/120    avg_loss:0.056, val_acc:0.965]
Epoch [41/120    avg_loss:0.060, val_acc:0.959]
Epoch [42/120    avg_loss:0.071, val_acc:0.965]
Epoch [43/120    avg_loss:0.057, val_acc:0.974]
Epoch [44/120    avg_loss:0.051, val_acc:0.958]
Epoch [45/120    avg_loss:0.040, val_acc:0.968]
Epoch [46/120    avg_loss:0.046, val_acc:0.937]
Epoch [47/120    avg_loss:0.048, val_acc:0.966]
Epoch [48/120    avg_loss:0.047, val_acc:0.952]
Epoch [49/120    avg_loss:0.040, val_acc:0.976]
Epoch [50/120    avg_loss:0.032, val_acc:0.971]
Epoch [51/120    avg_loss:0.053, val_acc:0.969]
Epoch [52/120    avg_loss:0.033, val_acc:0.968]
Epoch [53/120    avg_loss:0.050, val_acc:0.965]
Epoch [54/120    avg_loss:0.058, val_acc:0.969]
Epoch [55/120    avg_loss:0.035, val_acc:0.974]
Epoch [56/120    avg_loss:0.037, val_acc:0.974]
Epoch [57/120    avg_loss:0.027, val_acc:0.958]
Epoch [58/120    avg_loss:0.049, val_acc:0.974]
Epoch [59/120    avg_loss:0.036, val_acc:0.972]
Epoch [60/120    avg_loss:0.029, val_acc:0.973]
Epoch [61/120    avg_loss:0.018, val_acc:0.973]
Epoch [62/120    avg_loss:0.022, val_acc:0.972]
Epoch [63/120    avg_loss:0.018, val_acc:0.982]
Epoch [64/120    avg_loss:0.015, val_acc:0.984]
Epoch [65/120    avg_loss:0.012, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.983]
Epoch [67/120    avg_loss:0.015, val_acc:0.981]
Epoch [68/120    avg_loss:0.013, val_acc:0.982]
Epoch [69/120    avg_loss:0.014, val_acc:0.984]
Epoch [70/120    avg_loss:0.012, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.011, val_acc:0.983]
Epoch [74/120    avg_loss:0.015, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.983]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.010, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6298     0     0     0     0    16     7    80    31]
 [    0     0 17999     0    15     0    65     0    11     0]
 [    0     8     0  1958     0     0     0     0    65     5]
 [    0    28     8     0  2921     0     7     0     7     1]
 [    0     0     1     0     0  1304     0     0     0     0]
 [    0     0    18     0     0     0  4857     0     3     0]
 [    0    57     0     0     0     0     0  1231     0     2]
 [    0     5     0    56    41     0     0     0  3469     0]
 [    0     0     0     0     2     4     0     0     0   913]]

Accuracy:
98.69134552816138

F1 scores:
[       nan 0.98191456 0.99673275 0.96691358 0.98168375 0.99808649
 0.98890359 0.97389241 0.96280877 0.97594869]

Kappa:
0.9826714176899088
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbddda008d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.646, val_acc:0.559]
Epoch [2/120    avg_loss:1.004, val_acc:0.571]
Epoch [3/120    avg_loss:0.682, val_acc:0.651]
Epoch [4/120    avg_loss:0.526, val_acc:0.698]
Epoch [5/120    avg_loss:0.454, val_acc:0.809]
Epoch [6/120    avg_loss:0.400, val_acc:0.789]
Epoch [7/120    avg_loss:0.339, val_acc:0.797]
Epoch [8/120    avg_loss:0.318, val_acc:0.770]
Epoch [9/120    avg_loss:0.333, val_acc:0.794]
Epoch [10/120    avg_loss:0.281, val_acc:0.871]
Epoch [11/120    avg_loss:0.235, val_acc:0.874]
Epoch [12/120    avg_loss:0.206, val_acc:0.859]
Epoch [13/120    avg_loss:0.240, val_acc:0.841]
Epoch [14/120    avg_loss:0.245, val_acc:0.892]
Epoch [15/120    avg_loss:0.180, val_acc:0.877]
Epoch [16/120    avg_loss:0.174, val_acc:0.904]
Epoch [17/120    avg_loss:0.166, val_acc:0.907]
Epoch [18/120    avg_loss:0.152, val_acc:0.916]
Epoch [19/120    avg_loss:0.145, val_acc:0.895]
Epoch [20/120    avg_loss:0.142, val_acc:0.924]
Epoch [21/120    avg_loss:0.124, val_acc:0.911]
Epoch [22/120    avg_loss:0.090, val_acc:0.940]
Epoch [23/120    avg_loss:0.128, val_acc:0.891]
Epoch [24/120    avg_loss:0.131, val_acc:0.878]
Epoch [25/120    avg_loss:0.113, val_acc:0.859]
Epoch [26/120    avg_loss:0.093, val_acc:0.935]
Epoch [27/120    avg_loss:0.096, val_acc:0.940]
Epoch [28/120    avg_loss:0.108, val_acc:0.800]
Epoch [29/120    avg_loss:0.121, val_acc:0.935]
Epoch [30/120    avg_loss:0.054, val_acc:0.928]
Epoch [31/120    avg_loss:0.064, val_acc:0.957]
Epoch [32/120    avg_loss:0.045, val_acc:0.940]
Epoch [33/120    avg_loss:0.060, val_acc:0.949]
Epoch [34/120    avg_loss:0.057, val_acc:0.917]
Epoch [35/120    avg_loss:0.076, val_acc:0.865]
Epoch [36/120    avg_loss:0.095, val_acc:0.944]
Epoch [37/120    avg_loss:0.064, val_acc:0.940]
Epoch [38/120    avg_loss:0.073, val_acc:0.948]
Epoch [39/120    avg_loss:0.049, val_acc:0.958]
Epoch [40/120    avg_loss:0.055, val_acc:0.960]
Epoch [41/120    avg_loss:0.077, val_acc:0.923]
Epoch [42/120    avg_loss:0.062, val_acc:0.946]
Epoch [43/120    avg_loss:0.053, val_acc:0.953]
Epoch [44/120    avg_loss:0.042, val_acc:0.962]
Epoch [45/120    avg_loss:0.038, val_acc:0.959]
Epoch [46/120    avg_loss:0.057, val_acc:0.955]
Epoch [47/120    avg_loss:0.039, val_acc:0.965]
Epoch [48/120    avg_loss:0.023, val_acc:0.963]
Epoch [49/120    avg_loss:0.020, val_acc:0.963]
Epoch [50/120    avg_loss:0.060, val_acc:0.952]
Epoch [51/120    avg_loss:0.031, val_acc:0.964]
Epoch [52/120    avg_loss:0.064, val_acc:0.960]
Epoch [53/120    avg_loss:0.066, val_acc:0.960]
Epoch [54/120    avg_loss:0.033, val_acc:0.962]
Epoch [55/120    avg_loss:0.035, val_acc:0.938]
Epoch [56/120    avg_loss:0.056, val_acc:0.968]
Epoch [57/120    avg_loss:0.027, val_acc:0.972]
Epoch [58/120    avg_loss:0.033, val_acc:0.967]
Epoch [59/120    avg_loss:0.044, val_acc:0.962]
Epoch [60/120    avg_loss:0.042, val_acc:0.959]
Epoch [61/120    avg_loss:0.020, val_acc:0.962]
Epoch [62/120    avg_loss:0.026, val_acc:0.933]
Epoch [63/120    avg_loss:0.022, val_acc:0.971]
Epoch [64/120    avg_loss:0.017, val_acc:0.967]
Epoch [65/120    avg_loss:0.113, val_acc:0.886]
Epoch [66/120    avg_loss:0.134, val_acc:0.949]
Epoch [67/120    avg_loss:0.063, val_acc:0.949]
Epoch [68/120    avg_loss:0.046, val_acc:0.962]
Epoch [69/120    avg_loss:0.069, val_acc:0.956]
Epoch [70/120    avg_loss:0.055, val_acc:0.954]
Epoch [71/120    avg_loss:0.033, val_acc:0.968]
Epoch [72/120    avg_loss:0.021, val_acc:0.967]
Epoch [73/120    avg_loss:0.023, val_acc:0.971]
Epoch [74/120    avg_loss:0.018, val_acc:0.972]
Epoch [75/120    avg_loss:0.022, val_acc:0.975]
Epoch [76/120    avg_loss:0.017, val_acc:0.971]
Epoch [77/120    avg_loss:0.015, val_acc:0.971]
Epoch [78/120    avg_loss:0.013, val_acc:0.971]
Epoch [79/120    avg_loss:0.017, val_acc:0.971]
Epoch [80/120    avg_loss:0.017, val_acc:0.969]
Epoch [81/120    avg_loss:0.018, val_acc:0.971]
Epoch [82/120    avg_loss:0.014, val_acc:0.971]
Epoch [83/120    avg_loss:0.017, val_acc:0.973]
Epoch [84/120    avg_loss:0.018, val_acc:0.971]
Epoch [85/120    avg_loss:0.013, val_acc:0.972]
Epoch [86/120    avg_loss:0.015, val_acc:0.971]
Epoch [87/120    avg_loss:0.013, val_acc:0.971]
Epoch [88/120    avg_loss:0.019, val_acc:0.971]
Epoch [89/120    avg_loss:0.013, val_acc:0.973]
Epoch [90/120    avg_loss:0.013, val_acc:0.973]
Epoch [91/120    avg_loss:0.013, val_acc:0.973]
Epoch [92/120    avg_loss:0.016, val_acc:0.971]
Epoch [93/120    avg_loss:0.015, val_acc:0.971]
Epoch [94/120    avg_loss:0.013, val_acc:0.971]
Epoch [95/120    avg_loss:0.009, val_acc:0.971]
Epoch [96/120    avg_loss:0.018, val_acc:0.971]
Epoch [97/120    avg_loss:0.015, val_acc:0.971]
Epoch [98/120    avg_loss:0.016, val_acc:0.971]
Epoch [99/120    avg_loss:0.013, val_acc:0.971]
Epoch [100/120    avg_loss:0.014, val_acc:0.971]
Epoch [101/120    avg_loss:0.011, val_acc:0.971]
Epoch [102/120    avg_loss:0.014, val_acc:0.971]
Epoch [103/120    avg_loss:0.011, val_acc:0.971]
Epoch [104/120    avg_loss:0.019, val_acc:0.971]
Epoch [105/120    avg_loss:0.011, val_acc:0.971]
Epoch [106/120    avg_loss:0.013, val_acc:0.971]
Epoch [107/120    avg_loss:0.017, val_acc:0.971]
Epoch [108/120    avg_loss:0.015, val_acc:0.971]
Epoch [109/120    avg_loss:0.013, val_acc:0.971]
Epoch [110/120    avg_loss:0.012, val_acc:0.971]
Epoch [111/120    avg_loss:0.016, val_acc:0.971]
Epoch [112/120    avg_loss:0.013, val_acc:0.971]
Epoch [113/120    avg_loss:0.014, val_acc:0.971]
Epoch [114/120    avg_loss:0.016, val_acc:0.971]
Epoch [115/120    avg_loss:0.014, val_acc:0.971]
Epoch [116/120    avg_loss:0.023, val_acc:0.971]
Epoch [117/120    avg_loss:0.011, val_acc:0.971]
Epoch [118/120    avg_loss:0.010, val_acc:0.971]
Epoch [119/120    avg_loss:0.016, val_acc:0.971]
Epoch [120/120    avg_loss:0.012, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6272     0     1     0     0    44    39    76     0]
 [    0     0 18078     0     0     0     8     0     4     0]
 [    0     9     0  1862     0     0     0     0   165     0]
 [    0    19    21     0  2920     0     8     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4863     0     0     0]
 [    0     9     0     0     0     0     0  1281     0     0]
 [    0    16    10    83    25     0     2     0  3435     0]
 [    0     0     0     0     1    37     0     0     0   881]]

Accuracy:
98.56361313956572

F1 scores:
[       nan 0.98330328 0.99839841 0.93520844 0.98681987 0.98602191
 0.99214526 0.9816092  0.94732488 0.97726012]

Kappa:
0.980961977475801
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f162c8de828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.596, val_acc:0.647]
Epoch [2/120    avg_loss:0.813, val_acc:0.752]
Epoch [3/120    avg_loss:0.585, val_acc:0.733]
Epoch [4/120    avg_loss:0.504, val_acc:0.787]
Epoch [5/120    avg_loss:0.398, val_acc:0.809]
Epoch [6/120    avg_loss:0.377, val_acc:0.810]
Epoch [7/120    avg_loss:0.331, val_acc:0.877]
Epoch [8/120    avg_loss:0.250, val_acc:0.787]
Epoch [9/120    avg_loss:0.282, val_acc:0.844]
Epoch [10/120    avg_loss:0.309, val_acc:0.842]
Epoch [11/120    avg_loss:0.235, val_acc:0.876]
Epoch [12/120    avg_loss:0.167, val_acc:0.775]
Epoch [13/120    avg_loss:0.212, val_acc:0.903]
Epoch [14/120    avg_loss:0.136, val_acc:0.921]
Epoch [15/120    avg_loss:0.219, val_acc:0.918]
Epoch [16/120    avg_loss:0.149, val_acc:0.895]
Epoch [17/120    avg_loss:0.147, val_acc:0.895]
Epoch [18/120    avg_loss:0.123, val_acc:0.908]
Epoch [19/120    avg_loss:0.143, val_acc:0.895]
Epoch [20/120    avg_loss:0.138, val_acc:0.924]
Epoch [21/120    avg_loss:0.154, val_acc:0.858]
Epoch [22/120    avg_loss:0.126, val_acc:0.929]
Epoch [23/120    avg_loss:0.094, val_acc:0.949]
Epoch [24/120    avg_loss:0.078, val_acc:0.890]
Epoch [25/120    avg_loss:0.095, val_acc:0.926]
Epoch [26/120    avg_loss:0.140, val_acc:0.852]
Epoch [27/120    avg_loss:0.173, val_acc:0.932]
Epoch [28/120    avg_loss:0.104, val_acc:0.922]
Epoch [29/120    avg_loss:0.088, val_acc:0.947]
Epoch [30/120    avg_loss:0.102, val_acc:0.924]
Epoch [31/120    avg_loss:0.129, val_acc:0.873]
Epoch [32/120    avg_loss:0.097, val_acc:0.960]
Epoch [33/120    avg_loss:0.137, val_acc:0.942]
Epoch [34/120    avg_loss:0.088, val_acc:0.949]
Epoch [35/120    avg_loss:0.089, val_acc:0.933]
Epoch [36/120    avg_loss:0.075, val_acc:0.940]
Epoch [37/120    avg_loss:0.080, val_acc:0.942]
Epoch [38/120    avg_loss:0.060, val_acc:0.916]
Epoch [39/120    avg_loss:0.061, val_acc:0.962]
Epoch [40/120    avg_loss:0.052, val_acc:0.935]
Epoch [41/120    avg_loss:0.057, val_acc:0.946]
Epoch [42/120    avg_loss:0.049, val_acc:0.956]
Epoch [43/120    avg_loss:0.042, val_acc:0.964]
Epoch [44/120    avg_loss:0.040, val_acc:0.949]
Epoch [45/120    avg_loss:0.068, val_acc:0.956]
Epoch [46/120    avg_loss:0.056, val_acc:0.951]
Epoch [47/120    avg_loss:0.029, val_acc:0.970]
Epoch [48/120    avg_loss:0.033, val_acc:0.970]
Epoch [49/120    avg_loss:0.019, val_acc:0.967]
Epoch [50/120    avg_loss:0.033, val_acc:0.972]
Epoch [51/120    avg_loss:0.025, val_acc:0.975]
Epoch [52/120    avg_loss:0.015, val_acc:0.977]
Epoch [53/120    avg_loss:0.048, val_acc:0.964]
Epoch [54/120    avg_loss:0.025, val_acc:0.971]
Epoch [55/120    avg_loss:0.037, val_acc:0.975]
Epoch [56/120    avg_loss:0.013, val_acc:0.969]
Epoch [57/120    avg_loss:0.028, val_acc:0.977]
Epoch [58/120    avg_loss:0.039, val_acc:0.975]
Epoch [59/120    avg_loss:0.032, val_acc:0.972]
Epoch [60/120    avg_loss:0.021, val_acc:0.978]
Epoch [61/120    avg_loss:0.018, val_acc:0.981]
Epoch [62/120    avg_loss:0.025, val_acc:0.980]
Epoch [63/120    avg_loss:0.016, val_acc:0.975]
Epoch [64/120    avg_loss:0.013, val_acc:0.976]
Epoch [65/120    avg_loss:0.022, val_acc:0.978]
Epoch [66/120    avg_loss:0.023, val_acc:0.975]
Epoch [67/120    avg_loss:0.017, val_acc:0.971]
Epoch [68/120    avg_loss:0.012, val_acc:0.979]
Epoch [69/120    avg_loss:0.010, val_acc:0.980]
Epoch [70/120    avg_loss:0.018, val_acc:0.984]
Epoch [71/120    avg_loss:0.019, val_acc:0.984]
Epoch [72/120    avg_loss:0.033, val_acc:0.975]
Epoch [73/120    avg_loss:0.015, val_acc:0.980]
Epoch [74/120    avg_loss:0.013, val_acc:0.981]
Epoch [75/120    avg_loss:0.018, val_acc:0.977]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.016, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.017, val_acc:0.973]
Epoch [81/120    avg_loss:0.025, val_acc:0.977]
Epoch [82/120    avg_loss:0.025, val_acc:0.969]
Epoch [83/120    avg_loss:0.011, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.979]
Epoch [87/120    avg_loss:0.006, val_acc:0.981]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.022, val_acc:0.975]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.982]
Epoch [95/120    avg_loss:0.027, val_acc:0.977]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6279     0     5     0     0    33    26    88     1]
 [    0     0 17976     0    14     0    98     0     2     0]
 [    0     2     0  1879     0     0     0     0   153     2]
 [    0     8     1     0  2935     0    14     0     7     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4863     0     0     0]
 [    0    27     0     0     0     0     0  1260     3     0]
 [    0    19     0    53    39     0     0     0  3460     0]
 [    0     0     0     0     3     8     0     0     0   908]]

Accuracy:
98.48649169739474

F1 scores:
[       nan 0.98362967 0.9963971  0.94588472 0.98440382 0.99694423
 0.9838155  0.97826087 0.95002746 0.98856832]

Kappa:
0.9799666285457136
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22f306b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.545, val_acc:0.676]
Epoch [2/120    avg_loss:0.960, val_acc:0.708]
Epoch [3/120    avg_loss:0.657, val_acc:0.764]
Epoch [4/120    avg_loss:0.522, val_acc:0.824]
Epoch [5/120    avg_loss:0.424, val_acc:0.831]
Epoch [6/120    avg_loss:0.339, val_acc:0.858]
Epoch [7/120    avg_loss:0.338, val_acc:0.863]
Epoch [8/120    avg_loss:0.276, val_acc:0.854]
Epoch [9/120    avg_loss:0.257, val_acc:0.883]
Epoch [10/120    avg_loss:0.228, val_acc:0.877]
Epoch [11/120    avg_loss:0.222, val_acc:0.911]
Epoch [12/120    avg_loss:0.225, val_acc:0.895]
Epoch [13/120    avg_loss:0.205, val_acc:0.911]
Epoch [14/120    avg_loss:0.213, val_acc:0.875]
Epoch [15/120    avg_loss:0.173, val_acc:0.935]
Epoch [16/120    avg_loss:0.207, val_acc:0.929]
Epoch [17/120    avg_loss:0.161, val_acc:0.926]
Epoch [18/120    avg_loss:0.132, val_acc:0.938]
Epoch [19/120    avg_loss:0.111, val_acc:0.943]
Epoch [20/120    avg_loss:0.108, val_acc:0.917]
Epoch [21/120    avg_loss:0.118, val_acc:0.943]
Epoch [22/120    avg_loss:0.112, val_acc:0.945]
Epoch [23/120    avg_loss:0.119, val_acc:0.943]
Epoch [24/120    avg_loss:0.094, val_acc:0.943]
Epoch [25/120    avg_loss:0.074, val_acc:0.770]
Epoch [26/120    avg_loss:0.251, val_acc:0.894]
Epoch [27/120    avg_loss:0.120, val_acc:0.941]
Epoch [28/120    avg_loss:0.099, val_acc:0.944]
Epoch [29/120    avg_loss:0.097, val_acc:0.932]
Epoch [30/120    avg_loss:0.073, val_acc:0.954]
Epoch [31/120    avg_loss:0.080, val_acc:0.963]
Epoch [32/120    avg_loss:0.087, val_acc:0.953]
Epoch [33/120    avg_loss:0.084, val_acc:0.949]
Epoch [34/120    avg_loss:0.074, val_acc:0.950]
Epoch [35/120    avg_loss:0.048, val_acc:0.940]
Epoch [36/120    avg_loss:0.062, val_acc:0.957]
Epoch [37/120    avg_loss:0.049, val_acc:0.952]
Epoch [38/120    avg_loss:0.062, val_acc:0.933]
Epoch [39/120    avg_loss:0.067, val_acc:0.947]
Epoch [40/120    avg_loss:0.076, val_acc:0.958]
Epoch [41/120    avg_loss:0.043, val_acc:0.972]
Epoch [42/120    avg_loss:0.042, val_acc:0.955]
Epoch [43/120    avg_loss:0.127, val_acc:0.959]
Epoch [44/120    avg_loss:0.055, val_acc:0.958]
Epoch [45/120    avg_loss:0.049, val_acc:0.966]
Epoch [46/120    avg_loss:0.047, val_acc:0.966]
Epoch [47/120    avg_loss:0.058, val_acc:0.958]
Epoch [48/120    avg_loss:0.060, val_acc:0.951]
Epoch [49/120    avg_loss:0.042, val_acc:0.967]
Epoch [50/120    avg_loss:0.036, val_acc:0.969]
Epoch [51/120    avg_loss:0.031, val_acc:0.957]
Epoch [52/120    avg_loss:0.032, val_acc:0.966]
Epoch [53/120    avg_loss:0.027, val_acc:0.969]
Epoch [54/120    avg_loss:0.030, val_acc:0.971]
Epoch [55/120    avg_loss:0.018, val_acc:0.977]
Epoch [56/120    avg_loss:0.014, val_acc:0.977]
Epoch [57/120    avg_loss:0.017, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.976]
Epoch [59/120    avg_loss:0.015, val_acc:0.977]
Epoch [60/120    avg_loss:0.014, val_acc:0.977]
Epoch [61/120    avg_loss:0.020, val_acc:0.973]
Epoch [62/120    avg_loss:0.021, val_acc:0.977]
Epoch [63/120    avg_loss:0.016, val_acc:0.975]
Epoch [64/120    avg_loss:0.013, val_acc:0.978]
Epoch [65/120    avg_loss:0.016, val_acc:0.977]
Epoch [66/120    avg_loss:0.012, val_acc:0.978]
Epoch [67/120    avg_loss:0.021, val_acc:0.980]
Epoch [68/120    avg_loss:0.012, val_acc:0.979]
Epoch [69/120    avg_loss:0.014, val_acc:0.978]
Epoch [70/120    avg_loss:0.013, val_acc:0.979]
Epoch [71/120    avg_loss:0.012, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.979]
Epoch [73/120    avg_loss:0.009, val_acc:0.978]
Epoch [74/120    avg_loss:0.016, val_acc:0.978]
Epoch [75/120    avg_loss:0.013, val_acc:0.979]
Epoch [76/120    avg_loss:0.011, val_acc:0.980]
Epoch [77/120    avg_loss:0.013, val_acc:0.979]
Epoch [78/120    avg_loss:0.013, val_acc:0.979]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.012, val_acc:0.979]
Epoch [81/120    avg_loss:0.012, val_acc:0.976]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.973]
Epoch [85/120    avg_loss:0.013, val_acc:0.978]
Epoch [86/120    avg_loss:0.016, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.979]
Epoch [89/120    avg_loss:0.014, val_acc:0.980]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.978]
Epoch [92/120    avg_loss:0.016, val_acc:0.978]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.010, val_acc:0.982]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.977]
Epoch [97/120    avg_loss:0.015, val_acc:0.978]
Epoch [98/120    avg_loss:0.008, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.009, val_acc:0.979]
Epoch [105/120    avg_loss:0.007, val_acc:0.978]
Epoch [106/120    avg_loss:0.019, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.978]
Epoch [108/120    avg_loss:0.009, val_acc:0.974]
Epoch [109/120    avg_loss:0.010, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.013, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6300     0     2     0     0     2    25   102     1]
 [    0     0 18020     0    30     0    32     0     8     0]
 [    0     5     0  1877     0     0     0     0   151     3]
 [    0     9     6     0  2940     3     3     0     3     8]
 [    0     1     0     0     0  1304     0     0     0     0]
 [    0     0    26     0     5     0  4825     0    22     0]
 [    0    19     0     0     0     0     0  1267     4     0]
 [    0    17     0    52    44     0     0     1  3457     0]
 [    0     0     0     0     0    21     0     0     0   898]]

Accuracy:
98.54192273395513

F1 scores:
[       nan 0.98568411 0.9971778  0.94630703 0.98147221 0.99050513
 0.99075975 0.98102981 0.94479366 0.98195735]

Kappa:
0.9806900944176185
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fec184898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.606, val_acc:0.708]
Epoch [2/120    avg_loss:0.777, val_acc:0.773]
Epoch [3/120    avg_loss:0.591, val_acc:0.757]
Epoch [4/120    avg_loss:0.486, val_acc:0.813]
Epoch [5/120    avg_loss:0.406, val_acc:0.829]
Epoch [6/120    avg_loss:0.355, val_acc:0.859]
Epoch [7/120    avg_loss:0.308, val_acc:0.779]
Epoch [8/120    avg_loss:0.298, val_acc:0.875]
Epoch [9/120    avg_loss:0.259, val_acc:0.894]
Epoch [10/120    avg_loss:0.246, val_acc:0.867]
Epoch [11/120    avg_loss:0.238, val_acc:0.910]
Epoch [12/120    avg_loss:0.208, val_acc:0.895]
Epoch [13/120    avg_loss:0.237, val_acc:0.928]
Epoch [14/120    avg_loss:0.227, val_acc:0.904]
Epoch [15/120    avg_loss:0.175, val_acc:0.932]
Epoch [16/120    avg_loss:0.158, val_acc:0.917]
Epoch [17/120    avg_loss:0.175, val_acc:0.882]
Epoch [18/120    avg_loss:0.170, val_acc:0.943]
Epoch [19/120    avg_loss:0.117, val_acc:0.925]
Epoch [20/120    avg_loss:0.102, val_acc:0.949]
Epoch [21/120    avg_loss:0.112, val_acc:0.943]
Epoch [22/120    avg_loss:0.079, val_acc:0.946]
Epoch [23/120    avg_loss:0.080, val_acc:0.956]
Epoch [24/120    avg_loss:0.072, val_acc:0.953]
Epoch [25/120    avg_loss:0.068, val_acc:0.930]
Epoch [26/120    avg_loss:0.097, val_acc:0.948]
Epoch [27/120    avg_loss:0.073, val_acc:0.958]
Epoch [28/120    avg_loss:0.069, val_acc:0.963]
Epoch [29/120    avg_loss:0.051, val_acc:0.949]
Epoch [30/120    avg_loss:0.055, val_acc:0.964]
Epoch [31/120    avg_loss:0.038, val_acc:0.971]
Epoch [32/120    avg_loss:0.048, val_acc:0.969]
Epoch [33/120    avg_loss:0.115, val_acc:0.934]
Epoch [34/120    avg_loss:0.080, val_acc:0.966]
Epoch [35/120    avg_loss:0.054, val_acc:0.970]
Epoch [36/120    avg_loss:0.069, val_acc:0.949]
Epoch [37/120    avg_loss:0.043, val_acc:0.967]
Epoch [38/120    avg_loss:0.064, val_acc:0.920]
Epoch [39/120    avg_loss:0.052, val_acc:0.962]
Epoch [40/120    avg_loss:0.040, val_acc:0.970]
Epoch [41/120    avg_loss:0.046, val_acc:0.954]
Epoch [42/120    avg_loss:0.049, val_acc:0.974]
Epoch [43/120    avg_loss:0.049, val_acc:0.961]
Epoch [44/120    avg_loss:0.025, val_acc:0.971]
Epoch [45/120    avg_loss:0.030, val_acc:0.972]
Epoch [46/120    avg_loss:0.049, val_acc:0.964]
Epoch [47/120    avg_loss:0.045, val_acc:0.968]
Epoch [48/120    avg_loss:0.016, val_acc:0.974]
Epoch [49/120    avg_loss:0.044, val_acc:0.971]
Epoch [50/120    avg_loss:0.083, val_acc:0.967]
Epoch [51/120    avg_loss:0.068, val_acc:0.963]
Epoch [52/120    avg_loss:0.025, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.979]
Epoch [54/120    avg_loss:0.030, val_acc:0.944]
Epoch [55/120    avg_loss:0.025, val_acc:0.974]
Epoch [56/120    avg_loss:0.021, val_acc:0.980]
Epoch [57/120    avg_loss:0.033, val_acc:0.978]
Epoch [58/120    avg_loss:0.054, val_acc:0.930]
Epoch [59/120    avg_loss:0.059, val_acc:0.972]
Epoch [60/120    avg_loss:0.028, val_acc:0.956]
Epoch [61/120    avg_loss:0.034, val_acc:0.970]
Epoch [62/120    avg_loss:0.021, val_acc:0.980]
Epoch [63/120    avg_loss:0.042, val_acc:0.972]
Epoch [64/120    avg_loss:0.039, val_acc:0.970]
Epoch [65/120    avg_loss:0.025, val_acc:0.975]
Epoch [66/120    avg_loss:0.047, val_acc:0.955]
Epoch [67/120    avg_loss:0.027, val_acc:0.975]
Epoch [68/120    avg_loss:0.039, val_acc:0.977]
Epoch [69/120    avg_loss:0.026, val_acc:0.979]
Epoch [70/120    avg_loss:0.014, val_acc:0.976]
Epoch [71/120    avg_loss:0.011, val_acc:0.977]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.024, val_acc:0.962]
Epoch [74/120    avg_loss:0.012, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.972]
Epoch [76/120    avg_loss:0.015, val_acc:0.981]
Epoch [77/120    avg_loss:0.007, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.980]
Epoch [79/120    avg_loss:0.007, val_acc:0.980]
Epoch [80/120    avg_loss:0.006, val_acc:0.980]
Epoch [81/120    avg_loss:0.007, val_acc:0.980]
Epoch [82/120    avg_loss:0.006, val_acc:0.980]
Epoch [83/120    avg_loss:0.005, val_acc:0.980]
Epoch [84/120    avg_loss:0.005, val_acc:0.978]
Epoch [85/120    avg_loss:0.005, val_acc:0.980]
Epoch [86/120    avg_loss:0.007, val_acc:0.981]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.981]
Epoch [90/120    avg_loss:0.006, val_acc:0.981]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.004, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.007, val_acc:0.981]
Epoch [95/120    avg_loss:0.006, val_acc:0.981]
Epoch [96/120    avg_loss:0.006, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.981]
Epoch [99/120    avg_loss:0.004, val_acc:0.981]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.004, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.981]
Epoch [106/120    avg_loss:0.005, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.005, val_acc:0.981]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.005, val_acc:0.981]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.005, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6320     0     0     0     0     1    30    73     8]
 [    0     0 17926     0    15     0   148     0     1     0]
 [    0     7     0  1901     0     0     0     0   128     0]
 [    0    11    16     0  2930     0     9     1     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    18     0     0     0  4850     0    10     0]
 [    0    21     0     0     0     0     1  1264     4     0]
 [    0    13     1    44    39     0     8     0  3464     2]
 [    0     0     0     0    18     4     0     0     0   897]]

Accuracy:
98.467211336852

F1 scores:
[       nan 0.9871915  0.99448004 0.95503642 0.98091731 0.99846978
 0.98029308 0.97794971 0.95519096 0.98086386]

Kappa:
0.9797167530632751
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f593d2ab828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.600, val_acc:0.674]
Epoch [2/120    avg_loss:0.917, val_acc:0.706]
Epoch [3/120    avg_loss:0.639, val_acc:0.771]
Epoch [4/120    avg_loss:0.532, val_acc:0.698]
Epoch [5/120    avg_loss:0.469, val_acc:0.710]
Epoch [6/120    avg_loss:0.451, val_acc:0.733]
Epoch [7/120    avg_loss:0.337, val_acc:0.875]
Epoch [8/120    avg_loss:0.335, val_acc:0.840]
Epoch [9/120    avg_loss:0.309, val_acc:0.855]
Epoch [10/120    avg_loss:0.256, val_acc:0.872]
Epoch [11/120    avg_loss:0.238, val_acc:0.841]
Epoch [12/120    avg_loss:0.229, val_acc:0.876]
Epoch [13/120    avg_loss:0.237, val_acc:0.896]
Epoch [14/120    avg_loss:0.186, val_acc:0.868]
Epoch [15/120    avg_loss:0.217, val_acc:0.880]
Epoch [16/120    avg_loss:0.167, val_acc:0.897]
Epoch [17/120    avg_loss:0.172, val_acc:0.931]
Epoch [18/120    avg_loss:0.174, val_acc:0.887]
Epoch [19/120    avg_loss:0.121, val_acc:0.946]
Epoch [20/120    avg_loss:0.130, val_acc:0.930]
Epoch [21/120    avg_loss:0.138, val_acc:0.932]
Epoch [22/120    avg_loss:0.114, val_acc:0.885]
Epoch [23/120    avg_loss:0.115, val_acc:0.902]
Epoch [24/120    avg_loss:0.105, val_acc:0.941]
Epoch [25/120    avg_loss:0.102, val_acc:0.953]
Epoch [26/120    avg_loss:0.107, val_acc:0.854]
Epoch [27/120    avg_loss:0.122, val_acc:0.896]
Epoch [28/120    avg_loss:0.093, val_acc:0.951]
Epoch [29/120    avg_loss:0.085, val_acc:0.945]
Epoch [30/120    avg_loss:0.087, val_acc:0.946]
Epoch [31/120    avg_loss:0.079, val_acc:0.928]
Epoch [32/120    avg_loss:0.084, val_acc:0.960]
Epoch [33/120    avg_loss:0.064, val_acc:0.937]
Epoch [34/120    avg_loss:0.061, val_acc:0.947]
Epoch [35/120    avg_loss:0.071, val_acc:0.964]
Epoch [36/120    avg_loss:0.042, val_acc:0.968]
Epoch [37/120    avg_loss:0.045, val_acc:0.948]
Epoch [38/120    avg_loss:0.045, val_acc:0.961]
Epoch [39/120    avg_loss:0.047, val_acc:0.970]
Epoch [40/120    avg_loss:0.065, val_acc:0.953]
Epoch [41/120    avg_loss:0.055, val_acc:0.947]
Epoch [42/120    avg_loss:0.046, val_acc:0.961]
Epoch [43/120    avg_loss:0.054, val_acc:0.959]
Epoch [44/120    avg_loss:0.029, val_acc:0.957]
Epoch [45/120    avg_loss:0.057, val_acc:0.965]
Epoch [46/120    avg_loss:0.048, val_acc:0.961]
Epoch [47/120    avg_loss:0.030, val_acc:0.965]
Epoch [48/120    avg_loss:0.029, val_acc:0.970]
Epoch [49/120    avg_loss:0.031, val_acc:0.952]
Epoch [50/120    avg_loss:0.032, val_acc:0.966]
Epoch [51/120    avg_loss:0.060, val_acc:0.960]
Epoch [52/120    avg_loss:0.051, val_acc:0.954]
Epoch [53/120    avg_loss:0.034, val_acc:0.968]
Epoch [54/120    avg_loss:0.025, val_acc:0.968]
Epoch [55/120    avg_loss:0.019, val_acc:0.968]
Epoch [56/120    avg_loss:0.022, val_acc:0.970]
Epoch [57/120    avg_loss:0.019, val_acc:0.973]
Epoch [58/120    avg_loss:0.021, val_acc:0.970]
Epoch [59/120    avg_loss:0.025, val_acc:0.970]
Epoch [60/120    avg_loss:0.018, val_acc:0.972]
Epoch [61/120    avg_loss:0.018, val_acc:0.974]
Epoch [62/120    avg_loss:0.021, val_acc:0.970]
Epoch [63/120    avg_loss:0.017, val_acc:0.973]
Epoch [64/120    avg_loss:0.017, val_acc:0.973]
Epoch [65/120    avg_loss:0.015, val_acc:0.973]
Epoch [66/120    avg_loss:0.014, val_acc:0.973]
Epoch [67/120    avg_loss:0.015, val_acc:0.972]
Epoch [68/120    avg_loss:0.017, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.972]
Epoch [70/120    avg_loss:0.021, val_acc:0.974]
Epoch [71/120    avg_loss:0.016, val_acc:0.972]
Epoch [72/120    avg_loss:0.016, val_acc:0.975]
Epoch [73/120    avg_loss:0.015, val_acc:0.975]
Epoch [74/120    avg_loss:0.013, val_acc:0.977]
Epoch [75/120    avg_loss:0.015, val_acc:0.975]
Epoch [76/120    avg_loss:0.014, val_acc:0.975]
Epoch [77/120    avg_loss:0.020, val_acc:0.975]
Epoch [78/120    avg_loss:0.010, val_acc:0.976]
Epoch [79/120    avg_loss:0.014, val_acc:0.973]
Epoch [80/120    avg_loss:0.012, val_acc:0.975]
Epoch [81/120    avg_loss:0.014, val_acc:0.977]
Epoch [82/120    avg_loss:0.017, val_acc:0.979]
Epoch [83/120    avg_loss:0.009, val_acc:0.980]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.980]
Epoch [86/120    avg_loss:0.017, val_acc:0.981]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.014, val_acc:0.976]
Epoch [89/120    avg_loss:0.017, val_acc:0.981]
Epoch [90/120    avg_loss:0.012, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.978]
Epoch [93/120    avg_loss:0.016, val_acc:0.979]
Epoch [94/120    avg_loss:0.010, val_acc:0.978]
Epoch [95/120    avg_loss:0.014, val_acc:0.979]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.979]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.015, val_acc:0.977]
Epoch [100/120    avg_loss:0.011, val_acc:0.979]
Epoch [101/120    avg_loss:0.012, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.980]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.016, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.015, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.013, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.010, val_acc:0.980]
Epoch [120/120    avg_loss:0.013, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0     0     0     0     1     7    43     1]
 [    0     0 17721     0   104     0   261     0     4     0]
 [    0     8     0  1952     0     0     0     0    76     0]
 [    0    15     5     1  2934     0    13     0     1     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0    13     0     0     0     2     0  1272     3     0]
 [    0    49    21    60    24     0     7     0  3410     0]
 [    0     0     2     0     3     7     0     0     0   907]]

Accuracy:
98.22861687513557

F1 scores:
[       nan 0.98937737 0.98889509 0.96418869 0.97200596 0.99656357
 0.97180432 0.99026859 0.95948227 0.99125683]

Kappa:
0.9766085634887329
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04cf185898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.595, val_acc:0.495]
Epoch [2/120    avg_loss:0.871, val_acc:0.603]
Epoch [3/120    avg_loss:0.590, val_acc:0.715]
Epoch [4/120    avg_loss:0.510, val_acc:0.752]
Epoch [5/120    avg_loss:0.450, val_acc:0.768]
Epoch [6/120    avg_loss:0.401, val_acc:0.740]
Epoch [7/120    avg_loss:0.360, val_acc:0.826]
Epoch [8/120    avg_loss:0.306, val_acc:0.872]
Epoch [9/120    avg_loss:0.254, val_acc:0.863]
Epoch [10/120    avg_loss:0.247, val_acc:0.875]
Epoch [11/120    avg_loss:0.242, val_acc:0.821]
Epoch [12/120    avg_loss:0.214, val_acc:0.923]
Epoch [13/120    avg_loss:0.210, val_acc:0.895]
Epoch [14/120    avg_loss:0.153, val_acc:0.925]
Epoch [15/120    avg_loss:0.186, val_acc:0.853]
Epoch [16/120    avg_loss:0.136, val_acc:0.939]
Epoch [17/120    avg_loss:0.109, val_acc:0.921]
Epoch [18/120    avg_loss:0.119, val_acc:0.911]
Epoch [19/120    avg_loss:0.125, val_acc:0.957]
Epoch [20/120    avg_loss:0.091, val_acc:0.927]
Epoch [21/120    avg_loss:0.120, val_acc:0.912]
Epoch [22/120    avg_loss:0.130, val_acc:0.897]
Epoch [23/120    avg_loss:0.106, val_acc:0.966]
Epoch [24/120    avg_loss:0.076, val_acc:0.957]
Epoch [25/120    avg_loss:0.081, val_acc:0.967]
Epoch [26/120    avg_loss:0.071, val_acc:0.956]
Epoch [27/120    avg_loss:0.075, val_acc:0.934]
Epoch [28/120    avg_loss:0.066, val_acc:0.958]
Epoch [29/120    avg_loss:0.060, val_acc:0.964]
Epoch [30/120    avg_loss:0.032, val_acc:0.969]
Epoch [31/120    avg_loss:0.046, val_acc:0.971]
Epoch [32/120    avg_loss:0.055, val_acc:0.938]
Epoch [33/120    avg_loss:0.065, val_acc:0.959]
Epoch [34/120    avg_loss:0.074, val_acc:0.960]
Epoch [35/120    avg_loss:0.040, val_acc:0.976]
Epoch [36/120    avg_loss:0.027, val_acc:0.974]
Epoch [37/120    avg_loss:0.020, val_acc:0.968]
Epoch [38/120    avg_loss:0.041, val_acc:0.970]
Epoch [39/120    avg_loss:0.046, val_acc:0.925]
Epoch [40/120    avg_loss:0.098, val_acc:0.970]
Epoch [41/120    avg_loss:0.032, val_acc:0.931]
Epoch [42/120    avg_loss:0.037, val_acc:0.973]
Epoch [43/120    avg_loss:0.059, val_acc:0.954]
Epoch [44/120    avg_loss:0.041, val_acc:0.964]
Epoch [45/120    avg_loss:0.021, val_acc:0.970]
Epoch [46/120    avg_loss:0.030, val_acc:0.921]
Epoch [47/120    avg_loss:0.027, val_acc:0.974]
Epoch [48/120    avg_loss:0.016, val_acc:0.974]
Epoch [49/120    avg_loss:0.015, val_acc:0.978]
Epoch [50/120    avg_loss:0.013, val_acc:0.978]
Epoch [51/120    avg_loss:0.012, val_acc:0.978]
Epoch [52/120    avg_loss:0.013, val_acc:0.978]
Epoch [53/120    avg_loss:0.011, val_acc:0.980]
Epoch [54/120    avg_loss:0.010, val_acc:0.979]
Epoch [55/120    avg_loss:0.009, val_acc:0.981]
Epoch [56/120    avg_loss:0.010, val_acc:0.980]
Epoch [57/120    avg_loss:0.008, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.979]
Epoch [59/120    avg_loss:0.009, val_acc:0.981]
Epoch [60/120    avg_loss:0.014, val_acc:0.983]
Epoch [61/120    avg_loss:0.011, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.983]
Epoch [63/120    avg_loss:0.011, val_acc:0.980]
Epoch [64/120    avg_loss:0.008, val_acc:0.980]
Epoch [65/120    avg_loss:0.008, val_acc:0.981]
Epoch [66/120    avg_loss:0.009, val_acc:0.983]
Epoch [67/120    avg_loss:0.010, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.981]
Epoch [70/120    avg_loss:0.009, val_acc:0.980]
Epoch [71/120    avg_loss:0.009, val_acc:0.979]
Epoch [72/120    avg_loss:0.007, val_acc:0.980]
Epoch [73/120    avg_loss:0.010, val_acc:0.981]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.006, val_acc:0.981]
Epoch [76/120    avg_loss:0.008, val_acc:0.981]
Epoch [77/120    avg_loss:0.008, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.980]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.009, val_acc:0.979]
Epoch [82/120    avg_loss:0.009, val_acc:0.980]
Epoch [83/120    avg_loss:0.008, val_acc:0.980]
Epoch [84/120    avg_loss:0.007, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.980]
Epoch [90/120    avg_loss:0.007, val_acc:0.980]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.980]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.011, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.006, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.007, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6282     0     1     0     0    22    13   105     9]
 [    0     0 17974     0    13     0    98     0     5     0]
 [    0     9     0  1867     0     0     0     0   159     1]
 [    0    22     7     0  2937     0     1     0     1     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0    29     0     0     0     0     0  1260     0     1]
 [    0    24     0    50    48     0     1     0  3448     0]
 [    0     0     0     0     2     3     0     0     0   914]]

Accuracy:
98.48167160725906

F1 scores:
[       nan 0.98171589 0.99659006 0.94436014 0.98359009 0.99885189
 0.98744431 0.98322279 0.94582362 0.98917749]

Kappa:
0.9799035773880015
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7b112d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.622, val_acc:0.550]
Epoch [2/120    avg_loss:0.942, val_acc:0.694]
Epoch [3/120    avg_loss:0.629, val_acc:0.716]
Epoch [4/120    avg_loss:0.500, val_acc:0.781]
Epoch [5/120    avg_loss:0.440, val_acc:0.796]
Epoch [6/120    avg_loss:0.411, val_acc:0.832]
Epoch [7/120    avg_loss:0.417, val_acc:0.789]
Epoch [8/120    avg_loss:0.315, val_acc:0.840]
Epoch [9/120    avg_loss:0.285, val_acc:0.820]
Epoch [10/120    avg_loss:0.257, val_acc:0.859]
Epoch [11/120    avg_loss:0.244, val_acc:0.878]
Epoch [12/120    avg_loss:0.242, val_acc:0.849]
Epoch [13/120    avg_loss:0.197, val_acc:0.881]
Epoch [14/120    avg_loss:0.157, val_acc:0.865]
Epoch [15/120    avg_loss:0.186, val_acc:0.914]
Epoch [16/120    avg_loss:0.153, val_acc:0.921]
Epoch [17/120    avg_loss:0.116, val_acc:0.905]
Epoch [18/120    avg_loss:0.115, val_acc:0.949]
Epoch [19/120    avg_loss:0.104, val_acc:0.916]
Epoch [20/120    avg_loss:0.103, val_acc:0.945]
Epoch [21/120    avg_loss:0.116, val_acc:0.905]
Epoch [22/120    avg_loss:0.123, val_acc:0.919]
Epoch [23/120    avg_loss:0.096, val_acc:0.942]
Epoch [24/120    avg_loss:0.094, val_acc:0.904]
Epoch [25/120    avg_loss:0.155, val_acc:0.913]
Epoch [26/120    avg_loss:0.140, val_acc:0.951]
Epoch [27/120    avg_loss:0.099, val_acc:0.885]
Epoch [28/120    avg_loss:0.190, val_acc:0.920]
Epoch [29/120    avg_loss:0.096, val_acc:0.924]
Epoch [30/120    avg_loss:0.117, val_acc:0.935]
Epoch [31/120    avg_loss:0.073, val_acc:0.940]
Epoch [32/120    avg_loss:0.070, val_acc:0.955]
Epoch [33/120    avg_loss:0.060, val_acc:0.918]
Epoch [34/120    avg_loss:0.085, val_acc:0.950]
Epoch [35/120    avg_loss:0.070, val_acc:0.952]
Epoch [36/120    avg_loss:0.043, val_acc:0.962]
Epoch [37/120    avg_loss:0.042, val_acc:0.959]
Epoch [38/120    avg_loss:0.036, val_acc:0.942]
Epoch [39/120    avg_loss:0.038, val_acc:0.966]
Epoch [40/120    avg_loss:0.044, val_acc:0.898]
Epoch [41/120    avg_loss:0.042, val_acc:0.962]
Epoch [42/120    avg_loss:0.050, val_acc:0.964]
Epoch [43/120    avg_loss:0.027, val_acc:0.960]
Epoch [44/120    avg_loss:0.029, val_acc:0.949]
Epoch [45/120    avg_loss:0.035, val_acc:0.933]
Epoch [46/120    avg_loss:0.043, val_acc:0.921]
Epoch [47/120    avg_loss:0.067, val_acc:0.910]
Epoch [48/120    avg_loss:0.041, val_acc:0.959]
Epoch [49/120    avg_loss:0.037, val_acc:0.952]
Epoch [50/120    avg_loss:0.025, val_acc:0.965]
Epoch [51/120    avg_loss:0.031, val_acc:0.961]
Epoch [52/120    avg_loss:0.064, val_acc:0.954]
Epoch [53/120    avg_loss:0.044, val_acc:0.961]
Epoch [54/120    avg_loss:0.030, val_acc:0.964]
Epoch [55/120    avg_loss:0.019, val_acc:0.968]
Epoch [56/120    avg_loss:0.024, val_acc:0.967]
Epoch [57/120    avg_loss:0.026, val_acc:0.969]
Epoch [58/120    avg_loss:0.024, val_acc:0.963]
Epoch [59/120    avg_loss:0.025, val_acc:0.966]
Epoch [60/120    avg_loss:0.029, val_acc:0.968]
Epoch [61/120    avg_loss:0.015, val_acc:0.972]
Epoch [62/120    avg_loss:0.017, val_acc:0.971]
Epoch [63/120    avg_loss:0.021, val_acc:0.969]
Epoch [64/120    avg_loss:0.017, val_acc:0.969]
Epoch [65/120    avg_loss:0.021, val_acc:0.967]
Epoch [66/120    avg_loss:0.015, val_acc:0.968]
Epoch [67/120    avg_loss:0.018, val_acc:0.970]
Epoch [68/120    avg_loss:0.021, val_acc:0.971]
Epoch [69/120    avg_loss:0.022, val_acc:0.973]
Epoch [70/120    avg_loss:0.016, val_acc:0.971]
Epoch [71/120    avg_loss:0.021, val_acc:0.972]
Epoch [72/120    avg_loss:0.020, val_acc:0.970]
Epoch [73/120    avg_loss:0.016, val_acc:0.971]
Epoch [74/120    avg_loss:0.015, val_acc:0.971]
Epoch [75/120    avg_loss:0.016, val_acc:0.973]
Epoch [76/120    avg_loss:0.019, val_acc:0.972]
Epoch [77/120    avg_loss:0.017, val_acc:0.969]
Epoch [78/120    avg_loss:0.014, val_acc:0.969]
Epoch [79/120    avg_loss:0.017, val_acc:0.970]
Epoch [80/120    avg_loss:0.014, val_acc:0.972]
Epoch [81/120    avg_loss:0.014, val_acc:0.970]
Epoch [82/120    avg_loss:0.010, val_acc:0.969]
Epoch [83/120    avg_loss:0.014, val_acc:0.971]
Epoch [84/120    avg_loss:0.012, val_acc:0.969]
Epoch [85/120    avg_loss:0.018, val_acc:0.971]
Epoch [86/120    avg_loss:0.017, val_acc:0.970]
Epoch [87/120    avg_loss:0.011, val_acc:0.970]
Epoch [88/120    avg_loss:0.013, val_acc:0.972]
Epoch [89/120    avg_loss:0.012, val_acc:0.972]
Epoch [90/120    avg_loss:0.013, val_acc:0.971]
Epoch [91/120    avg_loss:0.017, val_acc:0.971]
Epoch [92/120    avg_loss:0.016, val_acc:0.973]
Epoch [93/120    avg_loss:0.020, val_acc:0.971]
Epoch [94/120    avg_loss:0.011, val_acc:0.972]
Epoch [95/120    avg_loss:0.012, val_acc:0.972]
Epoch [96/120    avg_loss:0.010, val_acc:0.972]
Epoch [97/120    avg_loss:0.011, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.972]
Epoch [99/120    avg_loss:0.016, val_acc:0.972]
Epoch [100/120    avg_loss:0.014, val_acc:0.972]
Epoch [101/120    avg_loss:0.015, val_acc:0.972]
Epoch [102/120    avg_loss:0.016, val_acc:0.972]
Epoch [103/120    avg_loss:0.010, val_acc:0.972]
Epoch [104/120    avg_loss:0.012, val_acc:0.971]
Epoch [105/120    avg_loss:0.015, val_acc:0.972]
Epoch [106/120    avg_loss:0.015, val_acc:0.972]
Epoch [107/120    avg_loss:0.019, val_acc:0.972]
Epoch [108/120    avg_loss:0.014, val_acc:0.972]
Epoch [109/120    avg_loss:0.011, val_acc:0.972]
Epoch [110/120    avg_loss:0.015, val_acc:0.972]
Epoch [111/120    avg_loss:0.015, val_acc:0.971]
Epoch [112/120    avg_loss:0.011, val_acc:0.971]
Epoch [113/120    avg_loss:0.020, val_acc:0.971]
Epoch [114/120    avg_loss:0.015, val_acc:0.971]
Epoch [115/120    avg_loss:0.012, val_acc:0.971]
Epoch [116/120    avg_loss:0.014, val_acc:0.971]
Epoch [117/120    avg_loss:0.016, val_acc:0.972]
Epoch [118/120    avg_loss:0.012, val_acc:0.972]
Epoch [119/120    avg_loss:0.013, val_acc:0.972]
Epoch [120/120    avg_loss:0.021, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6310     0     1     0     0    21    21    74     5]
 [    0     0 17763     0    93     0   230     0     4     0]
 [    0     6     0  1916     0     0     0     0   111     3]
 [    0    18    11     0  2937     0     1     0     1     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0    13     0     0     0     0     1  1270     6     0]
 [    0    13     0    74    45     0     0     3  3436     0]
 [    0     0     0     0     0     2     0     0     0   917]]

Accuracy:
98.16595570337165

F1 scores:
[       nan 0.9865541  0.99057551 0.95157686 0.97139077 0.9992343
 0.97472275 0.98297214 0.95404692 0.99242424]

Kappa:
0.9757816101386945
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:153
Validation dataloader:153
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd843024828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.558, val_acc:0.706]
Epoch [2/120    avg_loss:0.870, val_acc:0.780]
Epoch [3/120    avg_loss:0.632, val_acc:0.762]
Epoch [4/120    avg_loss:0.529, val_acc:0.700]
Epoch [5/120    avg_loss:0.430, val_acc:0.776]
Epoch [6/120    avg_loss:0.382, val_acc:0.856]
Epoch [7/120    avg_loss:0.369, val_acc:0.772]
Epoch [8/120    avg_loss:0.327, val_acc:0.877]
Epoch [9/120    avg_loss:0.250, val_acc:0.806]
Epoch [10/120    avg_loss:0.235, val_acc:0.911]
Epoch [11/120    avg_loss:0.221, val_acc:0.918]
Epoch [12/120    avg_loss:0.172, val_acc:0.895]
Epoch [13/120    avg_loss:0.227, val_acc:0.859]
Epoch [14/120    avg_loss:0.155, val_acc:0.905]
Epoch [15/120    avg_loss:0.141, val_acc:0.927]
Epoch [16/120    avg_loss:0.125, val_acc:0.929]
Epoch [17/120    avg_loss:0.124, val_acc:0.939]
Epoch [18/120    avg_loss:0.119, val_acc:0.933]
Epoch [19/120    avg_loss:0.116, val_acc:0.930]
Epoch [20/120    avg_loss:0.117, val_acc:0.958]
Epoch [21/120    avg_loss:0.150, val_acc:0.949]
Epoch [22/120    avg_loss:0.138, val_acc:0.952]
Epoch [23/120    avg_loss:0.082, val_acc:0.960]
Epoch [24/120    avg_loss:0.116, val_acc:0.944]
Epoch [25/120    avg_loss:0.084, val_acc:0.934]
Epoch [26/120    avg_loss:0.094, val_acc:0.933]
Epoch [27/120    avg_loss:0.064, val_acc:0.970]
Epoch [28/120    avg_loss:0.081, val_acc:0.968]
Epoch [29/120    avg_loss:0.122, val_acc:0.942]
Epoch [30/120    avg_loss:0.086, val_acc:0.928]
Epoch [31/120    avg_loss:0.075, val_acc:0.960]
Epoch [32/120    avg_loss:0.065, val_acc:0.955]
Epoch [33/120    avg_loss:0.077, val_acc:0.956]
Epoch [34/120    avg_loss:0.085, val_acc:0.951]
Epoch [35/120    avg_loss:0.080, val_acc:0.965]
Epoch [36/120    avg_loss:0.053, val_acc:0.968]
Epoch [37/120    avg_loss:0.038, val_acc:0.975]
Epoch [38/120    avg_loss:0.027, val_acc:0.979]
Epoch [39/120    avg_loss:0.036, val_acc:0.970]
Epoch [40/120    avg_loss:0.040, val_acc:0.964]
Epoch [41/120    avg_loss:0.034, val_acc:0.969]
Epoch [42/120    avg_loss:0.050, val_acc:0.970]
Epoch [43/120    avg_loss:0.026, val_acc:0.967]
Epoch [44/120    avg_loss:0.057, val_acc:0.957]
Epoch [45/120    avg_loss:0.037, val_acc:0.973]
Epoch [46/120    avg_loss:0.052, val_acc:0.977]
Epoch [47/120    avg_loss:0.038, val_acc:0.968]
Epoch [48/120    avg_loss:0.046, val_acc:0.975]
Epoch [49/120    avg_loss:0.043, val_acc:0.966]
Epoch [50/120    avg_loss:0.052, val_acc:0.954]
Epoch [51/120    avg_loss:0.043, val_acc:0.975]
Epoch [52/120    avg_loss:0.025, val_acc:0.976]
Epoch [53/120    avg_loss:0.022, val_acc:0.975]
Epoch [54/120    avg_loss:0.016, val_acc:0.975]
Epoch [55/120    avg_loss:0.025, val_acc:0.976]
Epoch [56/120    avg_loss:0.024, val_acc:0.975]
Epoch [57/120    avg_loss:0.017, val_acc:0.975]
Epoch [58/120    avg_loss:0.014, val_acc:0.977]
Epoch [59/120    avg_loss:0.019, val_acc:0.976]
Epoch [60/120    avg_loss:0.016, val_acc:0.977]
Epoch [61/120    avg_loss:0.014, val_acc:0.976]
Epoch [62/120    avg_loss:0.012, val_acc:0.979]
Epoch [63/120    avg_loss:0.013, val_acc:0.978]
Epoch [64/120    avg_loss:0.018, val_acc:0.977]
Epoch [65/120    avg_loss:0.019, val_acc:0.977]
Epoch [66/120    avg_loss:0.012, val_acc:0.979]
Epoch [67/120    avg_loss:0.017, val_acc:0.979]
Epoch [68/120    avg_loss:0.014, val_acc:0.975]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.012, val_acc:0.979]
Epoch [71/120    avg_loss:0.014, val_acc:0.979]
Epoch [72/120    avg_loss:0.017, val_acc:0.978]
Epoch [73/120    avg_loss:0.013, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.979]
Epoch [75/120    avg_loss:0.015, val_acc:0.981]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.978]
Epoch [80/120    avg_loss:0.017, val_acc:0.979]
Epoch [81/120    avg_loss:0.010, val_acc:0.976]
Epoch [82/120    avg_loss:0.015, val_acc:0.979]
Epoch [83/120    avg_loss:0.011, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.980]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.012, val_acc:0.979]
Epoch [87/120    avg_loss:0.012, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.979]
Epoch [89/120    avg_loss:0.012, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.012, val_acc:0.979]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.011, val_acc:0.979]
Epoch [97/120    avg_loss:0.014, val_acc:0.979]
Epoch [98/120    avg_loss:0.016, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.011, val_acc:0.980]
Epoch [101/120    avg_loss:0.011, val_acc:0.979]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.010, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.979]
Epoch [105/120    avg_loss:0.007, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.017, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.015, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.979]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6286     0     5     1     0    21    18    98     3]
 [    0     0 17898     0    41     0   151     0     0     0]
 [    0     6     0  1913     0     0     0     0   115     2]
 [    0    28     9     0  2918     0     9     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4864     0    13     0]
 [    0    54     0     0     0     0     0  1234     1     1]
 [    0    14     0    49    33     0     1     0  3474     0]
 [    0     0     0     0    14     5     0     0     0   900]]

Accuracy:
98.31055840744222

F1 scores:
[       nan 0.98065523 0.99438858 0.95578316 0.97608296 0.99808795
 0.9802499  0.97088906 0.95478906 0.98468271]

Kappa:
0.9776544990147938
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3df0704860>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.584, val_acc:0.708]
Epoch [2/120    avg_loss:0.866, val_acc:0.596]
Epoch [3/120    avg_loss:0.620, val_acc:0.762]
Epoch [4/120    avg_loss:0.437, val_acc:0.791]
Epoch [5/120    avg_loss:0.381, val_acc:0.865]
Epoch [6/120    avg_loss:0.404, val_acc:0.864]
Epoch [7/120    avg_loss:0.286, val_acc:0.893]
Epoch [8/120    avg_loss:0.247, val_acc:0.894]
Epoch [9/120    avg_loss:0.275, val_acc:0.907]
Epoch [10/120    avg_loss:0.216, val_acc:0.867]
Epoch [11/120    avg_loss:0.209, val_acc:0.883]
Epoch [12/120    avg_loss:0.248, val_acc:0.921]
Epoch [13/120    avg_loss:0.210, val_acc:0.876]
Epoch [14/120    avg_loss:0.261, val_acc:0.908]
Epoch [15/120    avg_loss:0.162, val_acc:0.934]
Epoch [16/120    avg_loss:0.142, val_acc:0.922]
Epoch [17/120    avg_loss:0.186, val_acc:0.895]
Epoch [18/120    avg_loss:0.143, val_acc:0.887]
Epoch [19/120    avg_loss:0.107, val_acc:0.928]
Epoch [20/120    avg_loss:0.122, val_acc:0.907]
Epoch [21/120    avg_loss:0.172, val_acc:0.949]
Epoch [22/120    avg_loss:0.105, val_acc:0.964]
Epoch [23/120    avg_loss:0.089, val_acc:0.920]
Epoch [24/120    avg_loss:0.085, val_acc:0.957]
Epoch [25/120    avg_loss:0.075, val_acc:0.958]
Epoch [26/120    avg_loss:0.066, val_acc:0.966]
Epoch [27/120    avg_loss:0.081, val_acc:0.952]
Epoch [28/120    avg_loss:0.077, val_acc:0.973]
Epoch [29/120    avg_loss:0.067, val_acc:0.937]
Epoch [30/120    avg_loss:0.107, val_acc:0.948]
Epoch [31/120    avg_loss:0.062, val_acc:0.974]
Epoch [32/120    avg_loss:0.041, val_acc:0.976]
Epoch [33/120    avg_loss:0.077, val_acc:0.965]
Epoch [34/120    avg_loss:0.039, val_acc:0.973]
Epoch [35/120    avg_loss:0.060, val_acc:0.972]
Epoch [36/120    avg_loss:0.046, val_acc:0.974]
Epoch [37/120    avg_loss:0.037, val_acc:0.983]
Epoch [38/120    avg_loss:0.107, val_acc:0.964]
Epoch [39/120    avg_loss:0.071, val_acc:0.970]
Epoch [40/120    avg_loss:0.035, val_acc:0.961]
Epoch [41/120    avg_loss:0.055, val_acc:0.972]
Epoch [42/120    avg_loss:0.037, val_acc:0.978]
Epoch [43/120    avg_loss:0.052, val_acc:0.974]
Epoch [44/120    avg_loss:0.036, val_acc:0.973]
Epoch [45/120    avg_loss:0.018, val_acc:0.978]
Epoch [46/120    avg_loss:0.028, val_acc:0.978]
Epoch [47/120    avg_loss:0.149, val_acc:0.958]
Epoch [48/120    avg_loss:0.105, val_acc:0.963]
Epoch [49/120    avg_loss:0.053, val_acc:0.970]
Epoch [50/120    avg_loss:0.079, val_acc:0.962]
Epoch [51/120    avg_loss:0.067, val_acc:0.973]
Epoch [52/120    avg_loss:0.041, val_acc:0.977]
Epoch [53/120    avg_loss:0.028, val_acc:0.981]
Epoch [54/120    avg_loss:0.036, val_acc:0.984]
Epoch [55/120    avg_loss:0.033, val_acc:0.981]
Epoch [56/120    avg_loss:0.030, val_acc:0.983]
Epoch [57/120    avg_loss:0.025, val_acc:0.984]
Epoch [58/120    avg_loss:0.025, val_acc:0.982]
Epoch [59/120    avg_loss:0.028, val_acc:0.979]
Epoch [60/120    avg_loss:0.020, val_acc:0.981]
Epoch [61/120    avg_loss:0.022, val_acc:0.982]
Epoch [62/120    avg_loss:0.022, val_acc:0.980]
Epoch [63/120    avg_loss:0.019, val_acc:0.980]
Epoch [64/120    avg_loss:0.019, val_acc:0.983]
Epoch [65/120    avg_loss:0.023, val_acc:0.981]
Epoch [66/120    avg_loss:0.019, val_acc:0.980]
Epoch [67/120    avg_loss:0.021, val_acc:0.981]
Epoch [68/120    avg_loss:0.016, val_acc:0.982]
Epoch [69/120    avg_loss:0.020, val_acc:0.983]
Epoch [70/120    avg_loss:0.023, val_acc:0.983]
Epoch [71/120    avg_loss:0.018, val_acc:0.983]
Epoch [72/120    avg_loss:0.018, val_acc:0.983]
Epoch [73/120    avg_loss:0.015, val_acc:0.983]
Epoch [74/120    avg_loss:0.017, val_acc:0.984]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.019, val_acc:0.984]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.018, val_acc:0.984]
Epoch [79/120    avg_loss:0.016, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.984]
Epoch [81/120    avg_loss:0.017, val_acc:0.983]
Epoch [82/120    avg_loss:0.014, val_acc:0.983]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.022, val_acc:0.983]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.985]
Epoch [87/120    avg_loss:0.013, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.019, val_acc:0.984]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.015, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.984]
Epoch [93/120    avg_loss:0.021, val_acc:0.983]
Epoch [94/120    avg_loss:0.021, val_acc:0.983]
Epoch [95/120    avg_loss:0.020, val_acc:0.983]
Epoch [96/120    avg_loss:0.020, val_acc:0.983]
Epoch [97/120    avg_loss:0.015, val_acc:0.984]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.015, val_acc:0.983]
Epoch [100/120    avg_loss:0.017, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.983]
Epoch [102/120    avg_loss:0.018, val_acc:0.983]
Epoch [103/120    avg_loss:0.018, val_acc:0.983]
Epoch [104/120    avg_loss:0.015, val_acc:0.983]
Epoch [105/120    avg_loss:0.025, val_acc:0.983]
Epoch [106/120    avg_loss:0.014, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.016, val_acc:0.983]
Epoch [109/120    avg_loss:0.020, val_acc:0.983]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.983]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.012, val_acc:0.983]
Epoch [114/120    avg_loss:0.017, val_acc:0.983]
Epoch [115/120    avg_loss:0.015, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.017, val_acc:0.983]
Epoch [118/120    avg_loss:0.018, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.014, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6294     0     0     2     0    27    32    74     3]
 [    0     0 18033     0     4     0    53     0     0     0]
 [    0     0     0  1932     0     0     0     0    98     6]
 [    0    25    10     0  2918     0     4     0    12     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     1     0  4858     0     5     0]
 [    0     8     0     0     0     0     2  1277     1     2]
 [    0     3     0    88    62     0     0     0  3404    14]
 [    0     0     0     0    13    59     0     0     0   847]]

Accuracy:
98.49372183259827

F1 scores:
[       nan 0.98636577 0.99775915 0.95266272 0.97722706 0.97789434
 0.9892079  0.98268565 0.95017446 0.94425864]

Kappa:
0.9800522230023061
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5672682898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.639, val_acc:0.590]
Epoch [2/120    avg_loss:0.836, val_acc:0.618]
Epoch [3/120    avg_loss:0.583, val_acc:0.797]
Epoch [4/120    avg_loss:0.496, val_acc:0.764]
Epoch [5/120    avg_loss:0.453, val_acc:0.848]
Epoch [6/120    avg_loss:0.390, val_acc:0.857]
Epoch [7/120    avg_loss:0.360, val_acc:0.834]
Epoch [8/120    avg_loss:0.305, val_acc:0.843]
Epoch [9/120    avg_loss:0.296, val_acc:0.876]
Epoch [10/120    avg_loss:0.241, val_acc:0.891]
Epoch [11/120    avg_loss:0.225, val_acc:0.944]
Epoch [12/120    avg_loss:0.197, val_acc:0.891]
Epoch [13/120    avg_loss:0.177, val_acc:0.887]
Epoch [14/120    avg_loss:0.159, val_acc:0.931]
Epoch [15/120    avg_loss:0.126, val_acc:0.933]
Epoch [16/120    avg_loss:0.126, val_acc:0.895]
Epoch [17/120    avg_loss:0.187, val_acc:0.928]
Epoch [18/120    avg_loss:0.140, val_acc:0.927]
Epoch [19/120    avg_loss:0.121, val_acc:0.905]
Epoch [20/120    avg_loss:0.082, val_acc:0.958]
Epoch [21/120    avg_loss:0.087, val_acc:0.957]
Epoch [22/120    avg_loss:0.080, val_acc:0.959]
Epoch [23/120    avg_loss:0.090, val_acc:0.950]
Epoch [24/120    avg_loss:0.084, val_acc:0.942]
Epoch [25/120    avg_loss:0.071, val_acc:0.978]
Epoch [26/120    avg_loss:0.051, val_acc:0.951]
Epoch [27/120    avg_loss:0.054, val_acc:0.957]
Epoch [28/120    avg_loss:0.043, val_acc:0.937]
Epoch [29/120    avg_loss:0.083, val_acc:0.955]
Epoch [30/120    avg_loss:0.040, val_acc:0.968]
Epoch [31/120    avg_loss:0.040, val_acc:0.970]
Epoch [32/120    avg_loss:0.046, val_acc:0.979]
Epoch [33/120    avg_loss:0.026, val_acc:0.978]
Epoch [34/120    avg_loss:0.034, val_acc:0.977]
Epoch [35/120    avg_loss:0.037, val_acc:0.985]
Epoch [36/120    avg_loss:0.063, val_acc:0.916]
Epoch [37/120    avg_loss:0.156, val_acc:0.934]
Epoch [38/120    avg_loss:0.123, val_acc:0.818]
Epoch [39/120    avg_loss:0.078, val_acc:0.947]
Epoch [40/120    avg_loss:0.056, val_acc:0.970]
Epoch [41/120    avg_loss:0.051, val_acc:0.959]
Epoch [42/120    avg_loss:0.043, val_acc:0.975]
Epoch [43/120    avg_loss:0.036, val_acc:0.970]
Epoch [44/120    avg_loss:0.043, val_acc:0.964]
Epoch [45/120    avg_loss:0.075, val_acc:0.957]
Epoch [46/120    avg_loss:0.053, val_acc:0.977]
Epoch [47/120    avg_loss:0.025, val_acc:0.983]
Epoch [48/120    avg_loss:0.027, val_acc:0.974]
Epoch [49/120    avg_loss:0.027, val_acc:0.977]
Epoch [50/120    avg_loss:0.015, val_acc:0.981]
Epoch [51/120    avg_loss:0.017, val_acc:0.982]
Epoch [52/120    avg_loss:0.012, val_acc:0.982]
Epoch [53/120    avg_loss:0.012, val_acc:0.983]
Epoch [54/120    avg_loss:0.013, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.017, val_acc:0.982]
Epoch [57/120    avg_loss:0.014, val_acc:0.980]
Epoch [58/120    avg_loss:0.014, val_acc:0.982]
Epoch [59/120    avg_loss:0.014, val_acc:0.980]
Epoch [60/120    avg_loss:0.010, val_acc:0.982]
Epoch [61/120    avg_loss:0.012, val_acc:0.981]
Epoch [62/120    avg_loss:0.012, val_acc:0.981]
Epoch [63/120    avg_loss:0.012, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.981]
Epoch [65/120    avg_loss:0.015, val_acc:0.981]
Epoch [66/120    avg_loss:0.013, val_acc:0.981]
Epoch [67/120    avg_loss:0.014, val_acc:0.981]
Epoch [68/120    avg_loss:0.012, val_acc:0.981]
Epoch [69/120    avg_loss:0.013, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.981]
Epoch [71/120    avg_loss:0.012, val_acc:0.981]
Epoch [72/120    avg_loss:0.012, val_acc:0.981]
Epoch [73/120    avg_loss:0.012, val_acc:0.981]
Epoch [74/120    avg_loss:0.013, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.010, val_acc:0.981]
Epoch [77/120    avg_loss:0.017, val_acc:0.981]
Epoch [78/120    avg_loss:0.015, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.014, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.981]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.012, val_acc:0.981]
Epoch [86/120    avg_loss:0.013, val_acc:0.981]
Epoch [87/120    avg_loss:0.013, val_acc:0.981]
Epoch [88/120    avg_loss:0.014, val_acc:0.981]
Epoch [89/120    avg_loss:0.014, val_acc:0.981]
Epoch [90/120    avg_loss:0.012, val_acc:0.981]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.017, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.012, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.014, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.017, val_acc:0.981]
Epoch [112/120    avg_loss:0.014, val_acc:0.981]
Epoch [113/120    avg_loss:0.012, val_acc:0.981]
Epoch [114/120    avg_loss:0.012, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.011, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.013, val_acc:0.981]
Epoch [120/120    avg_loss:0.011, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6318     0     0     1     0    19     3    91     0]
 [    0     0 17780     0    51     0   255     0     4     0]
 [    0     3     0  1933     0     0     0     0    94     6]
 [    0    31    21     0  2903     0     4     0    13     0]
 [    0     0     0     0     0  1301     0     0     0     4]
 [    0     0     0     0     0     0  4877     0     1     0]
 [    0    11     0     0     0     0     7  1271     1     0]
 [    0     0     0    37    56     0     0     0  3476     2]
 [    0     0     0     0    14    11     0     0     0   894]]

Accuracy:
98.21656664979635

F1 scores:
[       nan 0.98757327 0.99077763 0.96505242 0.96815074 0.99426825
 0.97151394 0.99141966 0.95876431 0.97972603]

Kappa:
0.9764391062353176
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f88951898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.614, val_acc:0.539]
Epoch [2/120    avg_loss:0.957, val_acc:0.649]
Epoch [3/120    avg_loss:0.704, val_acc:0.759]
Epoch [4/120    avg_loss:0.507, val_acc:0.786]
Epoch [5/120    avg_loss:0.484, val_acc:0.758]
Epoch [6/120    avg_loss:0.349, val_acc:0.893]
Epoch [7/120    avg_loss:0.330, val_acc:0.775]
Epoch [8/120    avg_loss:0.294, val_acc:0.872]
Epoch [9/120    avg_loss:0.280, val_acc:0.748]
Epoch [10/120    avg_loss:0.259, val_acc:0.934]
Epoch [11/120    avg_loss:0.190, val_acc:0.908]
Epoch [12/120    avg_loss:0.222, val_acc:0.896]
Epoch [13/120    avg_loss:0.224, val_acc:0.883]
Epoch [14/120    avg_loss:0.189, val_acc:0.893]
Epoch [15/120    avg_loss:0.120, val_acc:0.956]
Epoch [16/120    avg_loss:0.121, val_acc:0.927]
Epoch [17/120    avg_loss:0.122, val_acc:0.940]
Epoch [18/120    avg_loss:0.106, val_acc:0.963]
Epoch [19/120    avg_loss:0.085, val_acc:0.931]
Epoch [20/120    avg_loss:0.090, val_acc:0.936]
Epoch [21/120    avg_loss:0.076, val_acc:0.957]
Epoch [22/120    avg_loss:0.058, val_acc:0.856]
Epoch [23/120    avg_loss:0.117, val_acc:0.966]
Epoch [24/120    avg_loss:0.068, val_acc:0.966]
Epoch [25/120    avg_loss:0.058, val_acc:0.961]
Epoch [26/120    avg_loss:0.068, val_acc:0.958]
Epoch [27/120    avg_loss:0.061, val_acc:0.957]
Epoch [28/120    avg_loss:0.100, val_acc:0.927]
Epoch [29/120    avg_loss:0.064, val_acc:0.969]
Epoch [30/120    avg_loss:0.057, val_acc:0.935]
Epoch [31/120    avg_loss:0.055, val_acc:0.975]
Epoch [32/120    avg_loss:0.057, val_acc:0.961]
Epoch [33/120    avg_loss:0.055, val_acc:0.964]
Epoch [34/120    avg_loss:0.046, val_acc:0.935]
Epoch [35/120    avg_loss:0.038, val_acc:0.978]
Epoch [36/120    avg_loss:0.048, val_acc:0.976]
Epoch [37/120    avg_loss:0.036, val_acc:0.967]
Epoch [38/120    avg_loss:0.063, val_acc:0.971]
Epoch [39/120    avg_loss:0.053, val_acc:0.971]
Epoch [40/120    avg_loss:0.034, val_acc:0.977]
Epoch [41/120    avg_loss:0.032, val_acc:0.979]
Epoch [42/120    avg_loss:0.032, val_acc:0.965]
Epoch [43/120    avg_loss:0.031, val_acc:0.979]
Epoch [44/120    avg_loss:0.028, val_acc:0.977]
Epoch [45/120    avg_loss:0.025, val_acc:0.948]
Epoch [46/120    avg_loss:0.026, val_acc:0.937]
Epoch [47/120    avg_loss:0.024, val_acc:0.987]
Epoch [48/120    avg_loss:0.008, val_acc:0.987]
Epoch [49/120    avg_loss:0.025, val_acc:0.976]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.014, val_acc:0.984]
Epoch [52/120    avg_loss:0.016, val_acc:0.984]
Epoch [53/120    avg_loss:0.016, val_acc:0.966]
Epoch [54/120    avg_loss:0.038, val_acc:0.972]
Epoch [55/120    avg_loss:0.021, val_acc:0.935]
Epoch [56/120    avg_loss:0.023, val_acc:0.984]
Epoch [57/120    avg_loss:0.009, val_acc:0.985]
Epoch [58/120    avg_loss:0.009, val_acc:0.980]
Epoch [59/120    avg_loss:0.012, val_acc:0.981]
Epoch [60/120    avg_loss:0.020, val_acc:0.977]
Epoch [61/120    avg_loss:0.017, val_acc:0.980]
Epoch [62/120    avg_loss:0.010, val_acc:0.982]
Epoch [63/120    avg_loss:0.010, val_acc:0.982]
Epoch [64/120    avg_loss:0.007, val_acc:0.983]
Epoch [65/120    avg_loss:0.008, val_acc:0.982]
Epoch [66/120    avg_loss:0.009, val_acc:0.984]
Epoch [67/120    avg_loss:0.006, val_acc:0.983]
Epoch [68/120    avg_loss:0.013, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.005, val_acc:0.984]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.005, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.007, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6313     0    17     0     0    23    25    54     0]
 [    0     0 18014     0    15     0    57     0     4     0]
 [    0    21     0  1943     0     0     0     0    63     9]
 [    0    32     4     1  2916     0    10     0     9     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     2     0]
 [    0     4     0     0     0     0     2  1281     0     3]
 [    0    10     0    39    47     0     5     3  3466     1]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
98.81425782662136

F1 scores:
[       nan 0.98548236 0.99778442 0.96283449 0.9778672  0.99315068
 0.98995026 0.98576376 0.966941   0.97526113]

Kappa:
0.9843010126836826
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fec4a436898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.623, val_acc:0.375]
Epoch [2/120    avg_loss:0.868, val_acc:0.648]
Epoch [3/120    avg_loss:0.583, val_acc:0.736]
Epoch [4/120    avg_loss:0.488, val_acc:0.641]
Epoch [5/120    avg_loss:0.403, val_acc:0.811]
Epoch [6/120    avg_loss:0.326, val_acc:0.761]
Epoch [7/120    avg_loss:0.317, val_acc:0.805]
Epoch [8/120    avg_loss:0.281, val_acc:0.898]
Epoch [9/120    avg_loss:0.308, val_acc:0.867]
Epoch [10/120    avg_loss:0.266, val_acc:0.891]
Epoch [11/120    avg_loss:0.217, val_acc:0.907]
Epoch [12/120    avg_loss:0.184, val_acc:0.900]
Epoch [13/120    avg_loss:0.213, val_acc:0.922]
Epoch [14/120    avg_loss:0.200, val_acc:0.907]
Epoch [15/120    avg_loss:0.145, val_acc:0.903]
Epoch [16/120    avg_loss:0.169, val_acc:0.911]
Epoch [17/120    avg_loss:0.173, val_acc:0.932]
Epoch [18/120    avg_loss:0.129, val_acc:0.922]
Epoch [19/120    avg_loss:0.114, val_acc:0.915]
Epoch [20/120    avg_loss:0.134, val_acc:0.957]
Epoch [21/120    avg_loss:0.100, val_acc:0.932]
Epoch [22/120    avg_loss:0.102, val_acc:0.947]
Epoch [23/120    avg_loss:0.112, val_acc:0.914]
Epoch [24/120    avg_loss:0.121, val_acc:0.895]
Epoch [25/120    avg_loss:0.130, val_acc:0.953]
Epoch [26/120    avg_loss:0.126, val_acc:0.929]
Epoch [27/120    avg_loss:0.079, val_acc:0.957]
Epoch [28/120    avg_loss:0.090, val_acc:0.964]
Epoch [29/120    avg_loss:0.088, val_acc:0.958]
Epoch [30/120    avg_loss:0.040, val_acc:0.969]
Epoch [31/120    avg_loss:0.048, val_acc:0.945]
Epoch [32/120    avg_loss:0.041, val_acc:0.957]
Epoch [33/120    avg_loss:0.046, val_acc:0.948]
Epoch [34/120    avg_loss:0.055, val_acc:0.952]
Epoch [35/120    avg_loss:0.057, val_acc:0.939]
Epoch [36/120    avg_loss:0.040, val_acc:0.971]
Epoch [37/120    avg_loss:0.030, val_acc:0.970]
Epoch [38/120    avg_loss:0.029, val_acc:0.978]
Epoch [39/120    avg_loss:0.031, val_acc:0.976]
Epoch [40/120    avg_loss:0.018, val_acc:0.978]
Epoch [41/120    avg_loss:0.021, val_acc:0.980]
Epoch [42/120    avg_loss:0.018, val_acc:0.977]
Epoch [43/120    avg_loss:0.022, val_acc:0.979]
Epoch [44/120    avg_loss:0.035, val_acc:0.969]
Epoch [45/120    avg_loss:0.021, val_acc:0.973]
Epoch [46/120    avg_loss:0.019, val_acc:0.971]
Epoch [47/120    avg_loss:0.012, val_acc:0.975]
Epoch [48/120    avg_loss:0.014, val_acc:0.977]
Epoch [49/120    avg_loss:0.017, val_acc:0.976]
Epoch [50/120    avg_loss:0.017, val_acc:0.977]
Epoch [51/120    avg_loss:0.015, val_acc:0.945]
Epoch [52/120    avg_loss:0.016, val_acc:0.979]
Epoch [53/120    avg_loss:0.016, val_acc:0.982]
Epoch [54/120    avg_loss:0.022, val_acc:0.978]
Epoch [55/120    avg_loss:0.012, val_acc:0.984]
Epoch [56/120    avg_loss:0.019, val_acc:0.974]
Epoch [57/120    avg_loss:0.032, val_acc:0.980]
Epoch [58/120    avg_loss:0.014, val_acc:0.975]
Epoch [59/120    avg_loss:0.012, val_acc:0.974]
Epoch [60/120    avg_loss:0.035, val_acc:0.955]
Epoch [61/120    avg_loss:0.065, val_acc:0.977]
Epoch [62/120    avg_loss:0.038, val_acc:0.974]
Epoch [63/120    avg_loss:0.021, val_acc:0.976]
Epoch [64/120    avg_loss:0.010, val_acc:0.979]
Epoch [65/120    avg_loss:0.008, val_acc:0.982]
Epoch [66/120    avg_loss:0.016, val_acc:0.977]
Epoch [67/120    avg_loss:0.012, val_acc:0.980]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.008, val_acc:0.983]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.007, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.007, val_acc:0.983]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.007, val_acc:0.983]
Epoch [78/120    avg_loss:0.006, val_acc:0.982]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.006, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.983]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.005, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.981]
Epoch [91/120    avg_loss:0.005, val_acc:0.982]
Epoch [92/120    avg_loss:0.005, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.979]
Epoch [96/120    avg_loss:0.006, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.981]
Epoch [99/120    avg_loss:0.005, val_acc:0.981]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.006, val_acc:0.981]
Epoch [102/120    avg_loss:0.006, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.004, val_acc:0.982]
Epoch [105/120    avg_loss:0.005, val_acc:0.982]
Epoch [106/120    avg_loss:0.005, val_acc:0.982]
Epoch [107/120    avg_loss:0.004, val_acc:0.982]
Epoch [108/120    avg_loss:0.004, val_acc:0.982]
Epoch [109/120    avg_loss:0.004, val_acc:0.982]
Epoch [110/120    avg_loss:0.005, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.004, val_acc:0.982]
Epoch [119/120    avg_loss:0.006, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     0     0     0     8     7    56     2]
 [    0     0 17854     0    81     0   154     0     1     0]
 [    0     0     0  1916     0     0     0     0   116     4]
 [    0    22     3     0  2931     0     4     0     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4877     0     1     0]
 [    0    11     0     0     0     0     0  1277     0     2]
 [    0     6     0    19    52     0     0     0  3492     2]
 [    0     0     0     0     8    19     0     0     0   892]]

Accuracy:
98.57807340997277

F1 scores:
[       nan 0.99127046 0.99335132 0.96499622 0.96988749 0.99277292
 0.98316702 0.99222999 0.96384212 0.97807018]

Kappa:
0.9812043496856065
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f919cc98860>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.643, val_acc:0.635]
Epoch [2/120    avg_loss:0.933, val_acc:0.650]
Epoch [3/120    avg_loss:0.671, val_acc:0.678]
Epoch [4/120    avg_loss:0.545, val_acc:0.705]
Epoch [5/120    avg_loss:0.448, val_acc:0.762]
Epoch [6/120    avg_loss:0.384, val_acc:0.812]
Epoch [7/120    avg_loss:0.358, val_acc:0.778]
Epoch [8/120    avg_loss:0.296, val_acc:0.859]
Epoch [9/120    avg_loss:0.251, val_acc:0.783]
Epoch [10/120    avg_loss:0.263, val_acc:0.897]
Epoch [11/120    avg_loss:0.222, val_acc:0.897]
Epoch [12/120    avg_loss:0.239, val_acc:0.883]
Epoch [13/120    avg_loss:0.199, val_acc:0.888]
Epoch [14/120    avg_loss:0.192, val_acc:0.907]
Epoch [15/120    avg_loss:0.182, val_acc:0.912]
Epoch [16/120    avg_loss:0.142, val_acc:0.921]
Epoch [17/120    avg_loss:0.129, val_acc:0.950]
Epoch [18/120    avg_loss:0.130, val_acc:0.923]
Epoch [19/120    avg_loss:0.134, val_acc:0.943]
Epoch [20/120    avg_loss:0.105, val_acc:0.932]
Epoch [21/120    avg_loss:0.090, val_acc:0.833]
Epoch [22/120    avg_loss:0.096, val_acc:0.933]
Epoch [23/120    avg_loss:0.095, val_acc:0.960]
Epoch [24/120    avg_loss:0.165, val_acc:0.935]
Epoch [25/120    avg_loss:0.100, val_acc:0.962]
Epoch [26/120    avg_loss:0.081, val_acc:0.964]
Epoch [27/120    avg_loss:0.056, val_acc:0.966]
Epoch [28/120    avg_loss:0.094, val_acc:0.971]
Epoch [29/120    avg_loss:0.048, val_acc:0.973]
Epoch [30/120    avg_loss:0.069, val_acc:0.960]
Epoch [31/120    avg_loss:0.037, val_acc:0.972]
Epoch [32/120    avg_loss:0.082, val_acc:0.974]
Epoch [33/120    avg_loss:0.039, val_acc:0.972]
Epoch [34/120    avg_loss:0.107, val_acc:0.921]
Epoch [35/120    avg_loss:0.098, val_acc:0.959]
Epoch [36/120    avg_loss:0.043, val_acc:0.970]
Epoch [37/120    avg_loss:0.032, val_acc:0.975]
Epoch [38/120    avg_loss:0.035, val_acc:0.980]
Epoch [39/120    avg_loss:0.026, val_acc:0.983]
Epoch [40/120    avg_loss:0.024, val_acc:0.974]
Epoch [41/120    avg_loss:0.026, val_acc:0.975]
Epoch [42/120    avg_loss:0.022, val_acc:0.982]
Epoch [43/120    avg_loss:0.028, val_acc:0.970]
Epoch [44/120    avg_loss:0.017, val_acc:0.983]
Epoch [45/120    avg_loss:0.014, val_acc:0.983]
Epoch [46/120    avg_loss:0.034, val_acc:0.976]
Epoch [47/120    avg_loss:0.047, val_acc:0.983]
Epoch [48/120    avg_loss:0.018, val_acc:0.981]
Epoch [49/120    avg_loss:0.024, val_acc:0.982]
Epoch [50/120    avg_loss:0.013, val_acc:0.986]
Epoch [51/120    avg_loss:0.011, val_acc:0.989]
Epoch [52/120    avg_loss:0.013, val_acc:0.980]
Epoch [53/120    avg_loss:0.010, val_acc:0.988]
Epoch [54/120    avg_loss:0.013, val_acc:0.984]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.010, val_acc:0.985]
Epoch [57/120    avg_loss:0.007, val_acc:0.985]
Epoch [58/120    avg_loss:0.013, val_acc:0.980]
Epoch [59/120    avg_loss:0.012, val_acc:0.982]
Epoch [60/120    avg_loss:0.016, val_acc:0.984]
Epoch [61/120    avg_loss:0.009, val_acc:0.978]
Epoch [62/120    avg_loss:0.008, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.987]
Epoch [64/120    avg_loss:0.036, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.984]
Epoch [66/120    avg_loss:0.009, val_acc:0.986]
Epoch [67/120    avg_loss:0.009, val_acc:0.987]
Epoch [68/120    avg_loss:0.006, val_acc:0.986]
Epoch [69/120    avg_loss:0.005, val_acc:0.988]
Epoch [70/120    avg_loss:0.007, val_acc:0.987]
Epoch [71/120    avg_loss:0.007, val_acc:0.988]
Epoch [72/120    avg_loss:0.008, val_acc:0.988]
Epoch [73/120    avg_loss:0.005, val_acc:0.990]
Epoch [74/120    avg_loss:0.007, val_acc:0.989]
Epoch [75/120    avg_loss:0.005, val_acc:0.989]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.006, val_acc:0.987]
Epoch [78/120    avg_loss:0.005, val_acc:0.988]
Epoch [79/120    avg_loss:0.004, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.989]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.991]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.991]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.013, val_acc:0.987]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.004, val_acc:0.990]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6345     0     0     0     0     8     1    75     3]
 [    0     0 17989     0    12     0    88     0     1     0]
 [    0     4     0  1921     0     0     0     0   103     8]
 [    0    28     1     1  2931     0     7     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     4     0  4863     0     0     0]
 [    0    25     0     0     0     0     0  1265     0     0]
 [    0    26     0    79    32     0     0     0  3426     8]
 [    0     0     0     0    14    23     0     0     0   882]]

Accuracy:
98.635914491601

F1 scores:
[       nan 0.98678072 0.99686903 0.9516968  0.98273261 0.99126472
 0.988013   0.98982786 0.95431755 0.96923077]

Kappa:
0.9819402922574283
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe2638fd898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.611, val_acc:0.682]
Epoch [2/120    avg_loss:0.891, val_acc:0.788]
Epoch [3/120    avg_loss:0.608, val_acc:0.743]
Epoch [4/120    avg_loss:0.498, val_acc:0.810]
Epoch [5/120    avg_loss:0.427, val_acc:0.838]
Epoch [6/120    avg_loss:0.391, val_acc:0.858]
Epoch [7/120    avg_loss:0.337, val_acc:0.852]
Epoch [8/120    avg_loss:0.255, val_acc:0.874]
Epoch [9/120    avg_loss:0.269, val_acc:0.856]
Epoch [10/120    avg_loss:0.253, val_acc:0.886]
Epoch [11/120    avg_loss:0.221, val_acc:0.953]
Epoch [12/120    avg_loss:0.213, val_acc:0.939]
Epoch [13/120    avg_loss:0.182, val_acc:0.916]
Epoch [14/120    avg_loss:0.153, val_acc:0.968]
Epoch [15/120    avg_loss:0.163, val_acc:0.911]
Epoch [16/120    avg_loss:0.145, val_acc:0.918]
Epoch [17/120    avg_loss:0.133, val_acc:0.956]
Epoch [18/120    avg_loss:0.109, val_acc:0.958]
Epoch [19/120    avg_loss:0.119, val_acc:0.958]
Epoch [20/120    avg_loss:0.120, val_acc:0.951]
Epoch [21/120    avg_loss:0.114, val_acc:0.963]
Epoch [22/120    avg_loss:0.103, val_acc:0.971]
Epoch [23/120    avg_loss:0.099, val_acc:0.977]
Epoch [24/120    avg_loss:0.093, val_acc:0.953]
Epoch [25/120    avg_loss:0.054, val_acc:0.982]
Epoch [26/120    avg_loss:0.076, val_acc:0.978]
Epoch [27/120    avg_loss:0.094, val_acc:0.903]
Epoch [28/120    avg_loss:0.090, val_acc:0.980]
Epoch [29/120    avg_loss:0.041, val_acc:0.980]
Epoch [30/120    avg_loss:0.028, val_acc:0.985]
Epoch [31/120    avg_loss:0.066, val_acc:0.964]
Epoch [32/120    avg_loss:0.074, val_acc:0.974]
Epoch [33/120    avg_loss:0.036, val_acc:0.980]
Epoch [34/120    avg_loss:0.045, val_acc:0.987]
Epoch [35/120    avg_loss:0.055, val_acc:0.943]
Epoch [36/120    avg_loss:0.074, val_acc:0.954]
Epoch [37/120    avg_loss:0.135, val_acc:0.944]
Epoch [38/120    avg_loss:0.106, val_acc:0.968]
Epoch [39/120    avg_loss:0.042, val_acc:0.978]
Epoch [40/120    avg_loss:0.024, val_acc:0.989]
Epoch [41/120    avg_loss:0.026, val_acc:0.974]
Epoch [42/120    avg_loss:0.038, val_acc:0.964]
Epoch [43/120    avg_loss:0.035, val_acc:0.984]
Epoch [44/120    avg_loss:0.024, val_acc:0.987]
Epoch [45/120    avg_loss:0.017, val_acc:0.988]
Epoch [46/120    avg_loss:0.020, val_acc:0.990]
Epoch [47/120    avg_loss:0.009, val_acc:0.990]
Epoch [48/120    avg_loss:0.018, val_acc:0.986]
Epoch [49/120    avg_loss:0.059, val_acc:0.988]
Epoch [50/120    avg_loss:0.021, val_acc:0.988]
Epoch [51/120    avg_loss:0.029, val_acc:0.984]
Epoch [52/120    avg_loss:0.022, val_acc:0.983]
Epoch [53/120    avg_loss:0.061, val_acc:0.920]
Epoch [54/120    avg_loss:0.076, val_acc:0.978]
Epoch [55/120    avg_loss:0.030, val_acc:0.985]
Epoch [56/120    avg_loss:0.131, val_acc:0.929]
Epoch [57/120    avg_loss:0.098, val_acc:0.927]
Epoch [58/120    avg_loss:0.062, val_acc:0.973]
Epoch [59/120    avg_loss:0.045, val_acc:0.972]
Epoch [60/120    avg_loss:0.042, val_acc:0.978]
Epoch [61/120    avg_loss:0.023, val_acc:0.978]
Epoch [62/120    avg_loss:0.021, val_acc:0.982]
Epoch [63/120    avg_loss:0.022, val_acc:0.981]
Epoch [64/120    avg_loss:0.017, val_acc:0.983]
Epoch [65/120    avg_loss:0.015, val_acc:0.986]
Epoch [66/120    avg_loss:0.012, val_acc:0.986]
Epoch [67/120    avg_loss:0.012, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.987]
Epoch [71/120    avg_loss:0.012, val_acc:0.987]
Epoch [72/120    avg_loss:0.012, val_acc:0.987]
Epoch [73/120    avg_loss:0.012, val_acc:0.988]
Epoch [74/120    avg_loss:0.014, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.987]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.014, val_acc:0.988]
Epoch [84/120    avg_loss:0.010, val_acc:0.987]
Epoch [85/120    avg_loss:0.011, val_acc:0.987]
Epoch [86/120    avg_loss:0.015, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.012, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.012, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.011, val_acc:0.988]
Epoch [95/120    avg_loss:0.011, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.012, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.010, val_acc:0.988]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.012, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.014, val_acc:0.988]
Epoch [115/120    avg_loss:0.010, val_acc:0.988]
Epoch [116/120    avg_loss:0.011, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6302     0    11     4     0    34     0    75     6]
 [    0     0 18026     0     6     0    57     0     1     0]
 [    0     9     0  1941     0     0     0     0    75    11]
 [    0    20     6     1  2934     0     3     0     7     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0    12     0     0     0     2     0  1266     0    10]
 [    0    37     0    54    43     0     0     0  3433     4]
 [    0     0     0     0    14    19     0     0     0   886]]

Accuracy:
98.73472633938255

F1 scores:
[       nan 0.98376522 0.99806212 0.96017809 0.98242089 0.99201824
 0.99014928 0.99061033 0.95826936 0.96461622]

Kappa:
0.9832451269684349
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdddf4098d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.655, val_acc:0.621]
Epoch [2/120    avg_loss:0.953, val_acc:0.577]
Epoch [3/120    avg_loss:0.655, val_acc:0.734]
Epoch [4/120    avg_loss:0.552, val_acc:0.783]
Epoch [5/120    avg_loss:0.454, val_acc:0.736]
Epoch [6/120    avg_loss:0.381, val_acc:0.781]
Epoch [7/120    avg_loss:0.327, val_acc:0.869]
Epoch [8/120    avg_loss:0.298, val_acc:0.860]
Epoch [9/120    avg_loss:0.260, val_acc:0.872]
Epoch [10/120    avg_loss:0.222, val_acc:0.866]
Epoch [11/120    avg_loss:0.208, val_acc:0.912]
Epoch [12/120    avg_loss:0.174, val_acc:0.891]
Epoch [13/120    avg_loss:0.167, val_acc:0.876]
Epoch [14/120    avg_loss:0.116, val_acc:0.903]
Epoch [15/120    avg_loss:0.179, val_acc:0.831]
Epoch [16/120    avg_loss:0.296, val_acc:0.932]
Epoch [17/120    avg_loss:0.145, val_acc:0.915]
Epoch [18/120    avg_loss:0.141, val_acc:0.928]
Epoch [19/120    avg_loss:0.120, val_acc:0.930]
Epoch [20/120    avg_loss:0.109, val_acc:0.947]
Epoch [21/120    avg_loss:0.116, val_acc:0.891]
Epoch [22/120    avg_loss:0.093, val_acc:0.932]
Epoch [23/120    avg_loss:0.094, val_acc:0.934]
Epoch [24/120    avg_loss:0.069, val_acc:0.953]
Epoch [25/120    avg_loss:0.088, val_acc:0.937]
Epoch [26/120    avg_loss:0.088, val_acc:0.959]
Epoch [27/120    avg_loss:0.074, val_acc:0.959]
Epoch [28/120    avg_loss:0.078, val_acc:0.942]
Epoch [29/120    avg_loss:0.099, val_acc:0.931]
Epoch [30/120    avg_loss:0.065, val_acc:0.953]
Epoch [31/120    avg_loss:0.051, val_acc:0.971]
Epoch [32/120    avg_loss:0.054, val_acc:0.952]
Epoch [33/120    avg_loss:0.044, val_acc:0.965]
Epoch [34/120    avg_loss:0.041, val_acc:0.963]
Epoch [35/120    avg_loss:0.036, val_acc:0.963]
Epoch [36/120    avg_loss:0.052, val_acc:0.960]
Epoch [37/120    avg_loss:0.046, val_acc:0.946]
Epoch [38/120    avg_loss:0.059, val_acc:0.965]
Epoch [39/120    avg_loss:0.039, val_acc:0.945]
Epoch [40/120    avg_loss:0.025, val_acc:0.979]
Epoch [41/120    avg_loss:0.016, val_acc:0.982]
Epoch [42/120    avg_loss:0.019, val_acc:0.974]
Epoch [43/120    avg_loss:0.032, val_acc:0.970]
Epoch [44/120    avg_loss:0.020, val_acc:0.969]
Epoch [45/120    avg_loss:0.038, val_acc:0.913]
Epoch [46/120    avg_loss:0.048, val_acc:0.973]
Epoch [47/120    avg_loss:0.152, val_acc:0.799]
Epoch [48/120    avg_loss:0.179, val_acc:0.939]
Epoch [49/120    avg_loss:0.071, val_acc:0.942]
Epoch [50/120    avg_loss:0.062, val_acc:0.964]
Epoch [51/120    avg_loss:0.065, val_acc:0.956]
Epoch [52/120    avg_loss:0.036, val_acc:0.966]
Epoch [53/120    avg_loss:0.029, val_acc:0.971]
Epoch [54/120    avg_loss:0.042, val_acc:0.965]
Epoch [55/120    avg_loss:0.049, val_acc:0.965]
Epoch [56/120    avg_loss:0.027, val_acc:0.965]
Epoch [57/120    avg_loss:0.026, val_acc:0.965]
Epoch [58/120    avg_loss:0.021, val_acc:0.968]
Epoch [59/120    avg_loss:0.020, val_acc:0.969]
Epoch [60/120    avg_loss:0.023, val_acc:0.970]
Epoch [61/120    avg_loss:0.021, val_acc:0.970]
Epoch [62/120    avg_loss:0.025, val_acc:0.970]
Epoch [63/120    avg_loss:0.026, val_acc:0.970]
Epoch [64/120    avg_loss:0.022, val_acc:0.970]
Epoch [65/120    avg_loss:0.015, val_acc:0.970]
Epoch [66/120    avg_loss:0.018, val_acc:0.968]
Epoch [67/120    avg_loss:0.015, val_acc:0.970]
Epoch [68/120    avg_loss:0.014, val_acc:0.970]
Epoch [69/120    avg_loss:0.016, val_acc:0.970]
Epoch [70/120    avg_loss:0.016, val_acc:0.970]
Epoch [71/120    avg_loss:0.014, val_acc:0.970]
Epoch [72/120    avg_loss:0.015, val_acc:0.971]
Epoch [73/120    avg_loss:0.020, val_acc:0.970]
Epoch [74/120    avg_loss:0.013, val_acc:0.970]
Epoch [75/120    avg_loss:0.017, val_acc:0.970]
Epoch [76/120    avg_loss:0.017, val_acc:0.970]
Epoch [77/120    avg_loss:0.014, val_acc:0.970]
Epoch [78/120    avg_loss:0.015, val_acc:0.970]
Epoch [79/120    avg_loss:0.012, val_acc:0.971]
Epoch [80/120    avg_loss:0.014, val_acc:0.970]
Epoch [81/120    avg_loss:0.019, val_acc:0.970]
Epoch [82/120    avg_loss:0.014, val_acc:0.970]
Epoch [83/120    avg_loss:0.016, val_acc:0.970]
Epoch [84/120    avg_loss:0.017, val_acc:0.970]
Epoch [85/120    avg_loss:0.019, val_acc:0.970]
Epoch [86/120    avg_loss:0.022, val_acc:0.970]
Epoch [87/120    avg_loss:0.019, val_acc:0.970]
Epoch [88/120    avg_loss:0.015, val_acc:0.970]
Epoch [89/120    avg_loss:0.015, val_acc:0.970]
Epoch [90/120    avg_loss:0.013, val_acc:0.970]
Epoch [91/120    avg_loss:0.014, val_acc:0.971]
Epoch [92/120    avg_loss:0.017, val_acc:0.971]
Epoch [93/120    avg_loss:0.016, val_acc:0.971]
Epoch [94/120    avg_loss:0.013, val_acc:0.971]
Epoch [95/120    avg_loss:0.016, val_acc:0.971]
Epoch [96/120    avg_loss:0.014, val_acc:0.971]
Epoch [97/120    avg_loss:0.017, val_acc:0.971]
Epoch [98/120    avg_loss:0.015, val_acc:0.971]
Epoch [99/120    avg_loss:0.016, val_acc:0.971]
Epoch [100/120    avg_loss:0.015, val_acc:0.971]
Epoch [101/120    avg_loss:0.013, val_acc:0.971]
Epoch [102/120    avg_loss:0.016, val_acc:0.971]
Epoch [103/120    avg_loss:0.013, val_acc:0.971]
Epoch [104/120    avg_loss:0.017, val_acc:0.971]
Epoch [105/120    avg_loss:0.012, val_acc:0.971]
Epoch [106/120    avg_loss:0.016, val_acc:0.971]
Epoch [107/120    avg_loss:0.012, val_acc:0.971]
Epoch [108/120    avg_loss:0.017, val_acc:0.971]
Epoch [109/120    avg_loss:0.018, val_acc:0.971]
Epoch [110/120    avg_loss:0.016, val_acc:0.971]
Epoch [111/120    avg_loss:0.013, val_acc:0.971]
Epoch [112/120    avg_loss:0.015, val_acc:0.971]
Epoch [113/120    avg_loss:0.015, val_acc:0.971]
Epoch [114/120    avg_loss:0.012, val_acc:0.971]
Epoch [115/120    avg_loss:0.018, val_acc:0.971]
Epoch [116/120    avg_loss:0.014, val_acc:0.971]
Epoch [117/120    avg_loss:0.016, val_acc:0.971]
Epoch [118/120    avg_loss:0.017, val_acc:0.971]
Epoch [119/120    avg_loss:0.019, val_acc:0.971]
Epoch [120/120    avg_loss:0.016, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6274     0     3     0     0    75     1    78     1]
 [    0     0 17922     0    16     0   151     0     1     0]
 [    0     0     0  1976     0     0     0     0    57     3]
 [    0    28    15     0  2918     0     4     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4873     0     4     0]
 [    0    25     0     0     0     0     4  1260     0     1]
 [    0    13     0     9    52     0     9     0  3472    16]
 [    0     0     0     0    13    28     0     0     0   878]]

Accuracy:
98.5178222832767

F1 scores:
[       nan 0.98246163 0.99489286 0.98210736 0.97739072 0.9893859
 0.97518511 0.9878479  0.96578581 0.96589659]

Kappa:
0.9803911243828397
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7908c5b8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.625, val_acc:0.554]
Epoch [2/120    avg_loss:0.863, val_acc:0.693]
Epoch [3/120    avg_loss:0.595, val_acc:0.757]
Epoch [4/120    avg_loss:0.531, val_acc:0.792]
Epoch [5/120    avg_loss:0.439, val_acc:0.805]
Epoch [6/120    avg_loss:0.450, val_acc:0.728]
Epoch [7/120    avg_loss:0.318, val_acc:0.825]
Epoch [8/120    avg_loss:0.300, val_acc:0.853]
Epoch [9/120    avg_loss:0.317, val_acc:0.821]
Epoch [10/120    avg_loss:0.248, val_acc:0.806]
Epoch [11/120    avg_loss:0.227, val_acc:0.924]
Epoch [12/120    avg_loss:0.211, val_acc:0.908]
Epoch [13/120    avg_loss:0.180, val_acc:0.930]
Epoch [14/120    avg_loss:0.167, val_acc:0.804]
Epoch [15/120    avg_loss:0.164, val_acc:0.923]
Epoch [16/120    avg_loss:0.152, val_acc:0.924]
Epoch [17/120    avg_loss:0.145, val_acc:0.947]
Epoch [18/120    avg_loss:0.145, val_acc:0.924]
Epoch [19/120    avg_loss:0.140, val_acc:0.941]
Epoch [20/120    avg_loss:0.118, val_acc:0.873]
Epoch [21/120    avg_loss:0.128, val_acc:0.872]
Epoch [22/120    avg_loss:0.145, val_acc:0.916]
Epoch [23/120    avg_loss:0.168, val_acc:0.912]
Epoch [24/120    avg_loss:0.117, val_acc:0.918]
Epoch [25/120    avg_loss:0.101, val_acc:0.943]
Epoch [26/120    avg_loss:0.083, val_acc:0.951]
Epoch [27/120    avg_loss:0.074, val_acc:0.949]
Epoch [28/120    avg_loss:0.061, val_acc:0.930]
Epoch [29/120    avg_loss:0.090, val_acc:0.943]
Epoch [30/120    avg_loss:0.077, val_acc:0.943]
Epoch [31/120    avg_loss:0.065, val_acc:0.944]
Epoch [32/120    avg_loss:0.112, val_acc:0.907]
Epoch [33/120    avg_loss:0.098, val_acc:0.953]
Epoch [34/120    avg_loss:0.059, val_acc:0.914]
Epoch [35/120    avg_loss:0.052, val_acc:0.956]
Epoch [36/120    avg_loss:0.054, val_acc:0.917]
Epoch [37/120    avg_loss:0.050, val_acc:0.973]
Epoch [38/120    avg_loss:0.071, val_acc:0.923]
Epoch [39/120    avg_loss:0.071, val_acc:0.914]
Epoch [40/120    avg_loss:0.056, val_acc:0.960]
Epoch [41/120    avg_loss:0.054, val_acc:0.915]
Epoch [42/120    avg_loss:0.071, val_acc:0.936]
Epoch [43/120    avg_loss:0.055, val_acc:0.967]
Epoch [44/120    avg_loss:0.045, val_acc:0.949]
Epoch [45/120    avg_loss:0.041, val_acc:0.953]
Epoch [46/120    avg_loss:0.051, val_acc:0.976]
Epoch [47/120    avg_loss:0.023, val_acc:0.973]
Epoch [48/120    avg_loss:0.070, val_acc:0.917]
Epoch [49/120    avg_loss:0.048, val_acc:0.933]
Epoch [50/120    avg_loss:0.041, val_acc:0.961]
Epoch [51/120    avg_loss:0.042, val_acc:0.968]
Epoch [52/120    avg_loss:0.033, val_acc:0.958]
Epoch [53/120    avg_loss:0.041, val_acc:0.963]
Epoch [54/120    avg_loss:0.028, val_acc:0.975]
Epoch [55/120    avg_loss:0.055, val_acc:0.867]
Epoch [56/120    avg_loss:0.043, val_acc:0.942]
Epoch [57/120    avg_loss:0.034, val_acc:0.972]
Epoch [58/120    avg_loss:0.032, val_acc:0.966]
Epoch [59/120    avg_loss:0.024, val_acc:0.973]
Epoch [60/120    avg_loss:0.018, val_acc:0.977]
Epoch [61/120    avg_loss:0.019, val_acc:0.978]
Epoch [62/120    avg_loss:0.018, val_acc:0.976]
Epoch [63/120    avg_loss:0.024, val_acc:0.972]
Epoch [64/120    avg_loss:0.024, val_acc:0.980]
Epoch [65/120    avg_loss:0.013, val_acc:0.982]
Epoch [66/120    avg_loss:0.018, val_acc:0.981]
Epoch [67/120    avg_loss:0.019, val_acc:0.980]
Epoch [68/120    avg_loss:0.022, val_acc:0.978]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.017, val_acc:0.981]
Epoch [71/120    avg_loss:0.018, val_acc:0.978]
Epoch [72/120    avg_loss:0.015, val_acc:0.978]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.014, val_acc:0.982]
Epoch [75/120    avg_loss:0.015, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.981]
Epoch [77/120    avg_loss:0.016, val_acc:0.981]
Epoch [78/120    avg_loss:0.014, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.018, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.983]
Epoch [82/120    avg_loss:0.017, val_acc:0.983]
Epoch [83/120    avg_loss:0.012, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.020, val_acc:0.984]
Epoch [87/120    avg_loss:0.022, val_acc:0.980]
Epoch [88/120    avg_loss:0.012, val_acc:0.981]
Epoch [89/120    avg_loss:0.016, val_acc:0.983]
Epoch [90/120    avg_loss:0.020, val_acc:0.982]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.013, val_acc:0.980]
Epoch [93/120    avg_loss:0.014, val_acc:0.981]
Epoch [94/120    avg_loss:0.013, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.015, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.982]
Epoch [101/120    avg_loss:0.012, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.011, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.019, val_acc:0.984]
Epoch [111/120    avg_loss:0.015, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.017, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.986]
Epoch [120/120    avg_loss:0.017, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6322     0    11     3     0    25    12    56     3]
 [    0     0 18068     0    14     0     4     0     4     0]
 [    0     2     0  1953     1     0     0     0    65    15]
 [    0    16    13     0  2929     0     5     0     7     2]
 [    0     0     0     0     0  1299     0     0     0     6]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0    10     0     0     0     0     0  1277     1     2]
 [    0     0     0    29    61     0     2     0  3471     8]
 [    0     0     0     0    14    12     0     0     0   893]]

Accuracy:
99.02634179259152

F1 scores:
[       nan 0.98920357 0.99900476 0.96947133 0.97731064 0.99311927
 0.99622102 0.99030632 0.96752613 0.96645022]

Kappa:
0.9871014861698584
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f144a676898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.634, val_acc:0.696]
Epoch [2/120    avg_loss:0.955, val_acc:0.681]
Epoch [3/120    avg_loss:0.659, val_acc:0.762]
Epoch [4/120    avg_loss:0.522, val_acc:0.760]
Epoch [5/120    avg_loss:0.462, val_acc:0.717]
Epoch [6/120    avg_loss:0.426, val_acc:0.773]
Epoch [7/120    avg_loss:0.325, val_acc:0.838]
Epoch [8/120    avg_loss:0.371, val_acc:0.812]
Epoch [9/120    avg_loss:0.300, val_acc:0.823]
Epoch [10/120    avg_loss:0.294, val_acc:0.844]
Epoch [11/120    avg_loss:0.246, val_acc:0.872]
Epoch [12/120    avg_loss:0.194, val_acc:0.848]
Epoch [13/120    avg_loss:0.211, val_acc:0.865]
Epoch [14/120    avg_loss:0.206, val_acc:0.906]
Epoch [15/120    avg_loss:0.162, val_acc:0.933]
Epoch [16/120    avg_loss:0.172, val_acc:0.936]
Epoch [17/120    avg_loss:0.132, val_acc:0.918]
Epoch [18/120    avg_loss:0.165, val_acc:0.911]
Epoch [19/120    avg_loss:0.234, val_acc:0.863]
Epoch [20/120    avg_loss:0.172, val_acc:0.933]
Epoch [21/120    avg_loss:0.140, val_acc:0.922]
Epoch [22/120    avg_loss:0.140, val_acc:0.948]
Epoch [23/120    avg_loss:0.150, val_acc:0.948]
Epoch [24/120    avg_loss:0.141, val_acc:0.889]
Epoch [25/120    avg_loss:0.125, val_acc:0.948]
Epoch [26/120    avg_loss:0.087, val_acc:0.948]
Epoch [27/120    avg_loss:0.066, val_acc:0.959]
Epoch [28/120    avg_loss:0.074, val_acc:0.947]
Epoch [29/120    avg_loss:0.119, val_acc:0.966]
Epoch [30/120    avg_loss:0.061, val_acc:0.932]
Epoch [31/120    avg_loss:0.087, val_acc:0.958]
Epoch [32/120    avg_loss:0.067, val_acc:0.946]
Epoch [33/120    avg_loss:0.146, val_acc:0.922]
Epoch [34/120    avg_loss:0.076, val_acc:0.953]
Epoch [35/120    avg_loss:0.087, val_acc:0.912]
Epoch [36/120    avg_loss:0.063, val_acc:0.969]
Epoch [37/120    avg_loss:0.059, val_acc:0.963]
Epoch [38/120    avg_loss:0.060, val_acc:0.973]
Epoch [39/120    avg_loss:0.061, val_acc:0.941]
Epoch [40/120    avg_loss:0.057, val_acc:0.971]
Epoch [41/120    avg_loss:0.067, val_acc:0.975]
Epoch [42/120    avg_loss:0.028, val_acc:0.962]
Epoch [43/120    avg_loss:0.040, val_acc:0.953]
Epoch [44/120    avg_loss:0.049, val_acc:0.962]
Epoch [45/120    avg_loss:0.038, val_acc:0.975]
Epoch [46/120    avg_loss:0.032, val_acc:0.973]
Epoch [47/120    avg_loss:0.023, val_acc:0.970]
Epoch [48/120    avg_loss:0.030, val_acc:0.972]
Epoch [49/120    avg_loss:0.023, val_acc:0.973]
Epoch [50/120    avg_loss:0.018, val_acc:0.979]
Epoch [51/120    avg_loss:0.077, val_acc:0.910]
Epoch [52/120    avg_loss:0.092, val_acc:0.964]
Epoch [53/120    avg_loss:0.033, val_acc:0.964]
Epoch [54/120    avg_loss:0.064, val_acc:0.974]
Epoch [55/120    avg_loss:0.025, val_acc:0.972]
Epoch [56/120    avg_loss:0.023, val_acc:0.982]
Epoch [57/120    avg_loss:0.021, val_acc:0.975]
Epoch [58/120    avg_loss:0.039, val_acc:0.978]
Epoch [59/120    avg_loss:0.049, val_acc:0.958]
Epoch [60/120    avg_loss:0.072, val_acc:0.951]
Epoch [61/120    avg_loss:0.028, val_acc:0.972]
Epoch [62/120    avg_loss:0.024, val_acc:0.977]
Epoch [63/120    avg_loss:0.016, val_acc:0.985]
Epoch [64/120    avg_loss:0.038, val_acc:0.969]
Epoch [65/120    avg_loss:0.021, val_acc:0.983]
Epoch [66/120    avg_loss:0.015, val_acc:0.981]
Epoch [67/120    avg_loss:0.025, val_acc:0.978]
Epoch [68/120    avg_loss:0.015, val_acc:0.979]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.013, val_acc:0.975]
Epoch [71/120    avg_loss:0.020, val_acc:0.938]
Epoch [72/120    avg_loss:0.017, val_acc:0.983]
Epoch [73/120    avg_loss:0.019, val_acc:0.982]
Epoch [74/120    avg_loss:0.016, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.968]
Epoch [77/120    avg_loss:0.025, val_acc:0.976]
Epoch [78/120    avg_loss:0.015, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.017, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.989]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6343     0     7     0     0    19     4    59     0]
 [    0     0 17754     0    98     0   234     0     3     1]
 [    0     0     0  1934     0     0     0     0   100     2]
 [    0    19     5     0  2938     0     3     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0     2     0     0     0    19     2  1263     0     4]
 [    0     4     0    31    58     0     2     0  3473     3]
 [    0     0     0     0     6    22     0     0     0   891]]

Accuracy:
98.27199768635674

F1 scores:
[       nan 0.99109375 0.99048788 0.96506986 0.96772069 0.98453414
 0.97373415 0.98787642 0.96285001 0.9780461 ]

Kappa:
0.9771842178836962
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:151
Validation dataloader:151
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffb019b1898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.637, val_acc:0.639]
Epoch [2/120    avg_loss:0.934, val_acc:0.708]
Epoch [3/120    avg_loss:0.646, val_acc:0.735]
Epoch [4/120    avg_loss:0.514, val_acc:0.653]
Epoch [5/120    avg_loss:0.463, val_acc:0.821]
Epoch [6/120    avg_loss:0.429, val_acc:0.778]
Epoch [7/120    avg_loss:0.322, val_acc:0.743]
Epoch [8/120    avg_loss:0.266, val_acc:0.873]
Epoch [9/120    avg_loss:0.291, val_acc:0.882]
Epoch [10/120    avg_loss:0.268, val_acc:0.824]
Epoch [11/120    avg_loss:0.218, val_acc:0.737]
Epoch [12/120    avg_loss:0.213, val_acc:0.775]
Epoch [13/120    avg_loss:0.198, val_acc:0.872]
Epoch [14/120    avg_loss:0.182, val_acc:0.898]
Epoch [15/120    avg_loss:0.187, val_acc:0.870]
Epoch [16/120    avg_loss:0.196, val_acc:0.895]
Epoch [17/120    avg_loss:0.151, val_acc:0.925]
Epoch [18/120    avg_loss:0.110, val_acc:0.944]
Epoch [19/120    avg_loss:0.096, val_acc:0.945]
Epoch [20/120    avg_loss:0.097, val_acc:0.937]
Epoch [21/120    avg_loss:0.087, val_acc:0.922]
Epoch [22/120    avg_loss:0.120, val_acc:0.943]
Epoch [23/120    avg_loss:0.090, val_acc:0.922]
Epoch [24/120    avg_loss:0.071, val_acc:0.956]
Epoch [25/120    avg_loss:0.055, val_acc:0.953]
Epoch [26/120    avg_loss:0.055, val_acc:0.951]
Epoch [27/120    avg_loss:0.069, val_acc:0.919]
Epoch [28/120    avg_loss:0.065, val_acc:0.946]
Epoch [29/120    avg_loss:0.051, val_acc:0.959]
Epoch [30/120    avg_loss:0.081, val_acc:0.928]
Epoch [31/120    avg_loss:0.046, val_acc:0.962]
Epoch [32/120    avg_loss:0.058, val_acc:0.961]
Epoch [33/120    avg_loss:0.043, val_acc:0.955]
Epoch [34/120    avg_loss:0.040, val_acc:0.963]
Epoch [35/120    avg_loss:0.034, val_acc:0.967]
Epoch [36/120    avg_loss:0.038, val_acc:0.961]
Epoch [37/120    avg_loss:0.027, val_acc:0.961]
Epoch [38/120    avg_loss:0.037, val_acc:0.970]
Epoch [39/120    avg_loss:0.079, val_acc:0.951]
Epoch [40/120    avg_loss:0.105, val_acc:0.953]
Epoch [41/120    avg_loss:0.066, val_acc:0.950]
Epoch [42/120    avg_loss:0.104, val_acc:0.945]
Epoch [43/120    avg_loss:0.054, val_acc:0.968]
Epoch [44/120    avg_loss:0.031, val_acc:0.965]
Epoch [45/120    avg_loss:0.030, val_acc:0.958]
Epoch [46/120    avg_loss:0.033, val_acc:0.966]
Epoch [47/120    avg_loss:0.021, val_acc:0.968]
Epoch [48/120    avg_loss:0.025, val_acc:0.961]
Epoch [49/120    avg_loss:0.031, val_acc:0.953]
Epoch [50/120    avg_loss:0.028, val_acc:0.968]
Epoch [51/120    avg_loss:0.037, val_acc:0.955]
Epoch [52/120    avg_loss:0.024, val_acc:0.978]
Epoch [53/120    avg_loss:0.023, val_acc:0.976]
Epoch [54/120    avg_loss:0.017, val_acc:0.981]
Epoch [55/120    avg_loss:0.014, val_acc:0.978]
Epoch [56/120    avg_loss:0.014, val_acc:0.977]
Epoch [57/120    avg_loss:0.016, val_acc:0.977]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.014, val_acc:0.980]
Epoch [60/120    avg_loss:0.012, val_acc:0.978]
Epoch [61/120    avg_loss:0.013, val_acc:0.979]
Epoch [62/120    avg_loss:0.012, val_acc:0.978]
Epoch [63/120    avg_loss:0.011, val_acc:0.978]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.011, val_acc:0.978]
Epoch [66/120    avg_loss:0.012, val_acc:0.978]
Epoch [67/120    avg_loss:0.012, val_acc:0.977]
Epoch [68/120    avg_loss:0.011, val_acc:0.977]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.009, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.978]
Epoch [72/120    avg_loss:0.013, val_acc:0.978]
Epoch [73/120    avg_loss:0.015, val_acc:0.977]
Epoch [74/120    avg_loss:0.012, val_acc:0.978]
Epoch [75/120    avg_loss:0.010, val_acc:0.977]
Epoch [76/120    avg_loss:0.013, val_acc:0.977]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.010, val_acc:0.976]
Epoch [79/120    avg_loss:0.013, val_acc:0.976]
Epoch [80/120    avg_loss:0.013, val_acc:0.976]
Epoch [81/120    avg_loss:0.011, val_acc:0.976]
Epoch [82/120    avg_loss:0.010, val_acc:0.976]
Epoch [83/120    avg_loss:0.012, val_acc:0.976]
Epoch [84/120    avg_loss:0.011, val_acc:0.976]
Epoch [85/120    avg_loss:0.010, val_acc:0.976]
Epoch [86/120    avg_loss:0.010, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.976]
Epoch [88/120    avg_loss:0.012, val_acc:0.976]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.011, val_acc:0.976]
Epoch [91/120    avg_loss:0.009, val_acc:0.976]
Epoch [92/120    avg_loss:0.011, val_acc:0.976]
Epoch [93/120    avg_loss:0.016, val_acc:0.976]
Epoch [94/120    avg_loss:0.012, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.012, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.976]
Epoch [100/120    avg_loss:0.011, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.010, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.976]
Epoch [105/120    avg_loss:0.012, val_acc:0.976]
Epoch [106/120    avg_loss:0.010, val_acc:0.976]
Epoch [107/120    avg_loss:0.009, val_acc:0.976]
Epoch [108/120    avg_loss:0.015, val_acc:0.976]
Epoch [109/120    avg_loss:0.014, val_acc:0.976]
Epoch [110/120    avg_loss:0.012, val_acc:0.976]
Epoch [111/120    avg_loss:0.013, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.976]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.011, val_acc:0.976]
Epoch [116/120    avg_loss:0.012, val_acc:0.976]
Epoch [117/120    avg_loss:0.017, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6302     0     6     0     0    14     2   108     0]
 [    0     0 17983     0    17     0    85     0     5     0]
 [    0     2     0  1928     0     0     0     0    98     8]
 [    0    23     9     0  2921     0     3     0    13     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4871     0     3     0]
 [    0    11     0     0     0     0     3  1275     0     1]
 [    0     2     0    19    45     0     1     0  3500     4]
 [    0     0     0     0    14    14     0     0     0   891]]

Accuracy:
98.75400669992528

F1 scores:
[       nan 0.98684623 0.99667461 0.96665831 0.9787234  0.99466463
 0.98853374 0.99337748 0.9591669  0.97590361]

Kappa:
0.9835070539980203
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7cbd8d2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.627, val_acc:0.473]
Epoch [2/120    avg_loss:0.973, val_acc:0.663]
Epoch [3/120    avg_loss:0.677, val_acc:0.775]
Epoch [4/120    avg_loss:0.550, val_acc:0.702]
Epoch [5/120    avg_loss:0.487, val_acc:0.827]
Epoch [6/120    avg_loss:0.383, val_acc:0.842]
Epoch [7/120    avg_loss:0.340, val_acc:0.754]
Epoch [8/120    avg_loss:0.322, val_acc:0.849]
Epoch [9/120    avg_loss:0.257, val_acc:0.883]
Epoch [10/120    avg_loss:0.209, val_acc:0.876]
Epoch [11/120    avg_loss:0.249, val_acc:0.910]
Epoch [12/120    avg_loss:0.163, val_acc:0.927]
Epoch [13/120    avg_loss:0.187, val_acc:0.913]
Epoch [14/120    avg_loss:0.174, val_acc:0.920]
Epoch [15/120    avg_loss:0.276, val_acc:0.904]
Epoch [16/120    avg_loss:0.186, val_acc:0.924]
Epoch [17/120    avg_loss:0.178, val_acc:0.912]
Epoch [18/120    avg_loss:0.130, val_acc:0.947]
Epoch [19/120    avg_loss:0.137, val_acc:0.945]
Epoch [20/120    avg_loss:0.102, val_acc:0.949]
Epoch [21/120    avg_loss:0.125, val_acc:0.919]
Epoch [22/120    avg_loss:0.100, val_acc:0.881]
Epoch [23/120    avg_loss:0.106, val_acc:0.946]
Epoch [24/120    avg_loss:0.120, val_acc:0.951]
Epoch [25/120    avg_loss:0.073, val_acc:0.953]
Epoch [26/120    avg_loss:0.091, val_acc:0.938]
Epoch [27/120    avg_loss:0.071, val_acc:0.934]
Epoch [28/120    avg_loss:0.085, val_acc:0.967]
Epoch [29/120    avg_loss:0.059, val_acc:0.960]
Epoch [30/120    avg_loss:0.053, val_acc:0.972]
Epoch [31/120    avg_loss:0.051, val_acc:0.961]
Epoch [32/120    avg_loss:0.062, val_acc:0.955]
Epoch [33/120    avg_loss:0.065, val_acc:0.956]
Epoch [34/120    avg_loss:0.094, val_acc:0.968]
Epoch [35/120    avg_loss:0.051, val_acc:0.976]
Epoch [36/120    avg_loss:0.070, val_acc:0.973]
Epoch [37/120    avg_loss:0.061, val_acc:0.942]
Epoch [38/120    avg_loss:0.037, val_acc:0.971]
Epoch [39/120    avg_loss:0.026, val_acc:0.971]
Epoch [40/120    avg_loss:0.023, val_acc:0.971]
Epoch [41/120    avg_loss:0.045, val_acc:0.953]
Epoch [42/120    avg_loss:0.053, val_acc:0.975]
Epoch [43/120    avg_loss:0.021, val_acc:0.971]
Epoch [44/120    avg_loss:0.030, val_acc:0.981]
Epoch [45/120    avg_loss:0.022, val_acc:0.982]
Epoch [46/120    avg_loss:0.022, val_acc:0.943]
Epoch [47/120    avg_loss:0.023, val_acc:0.972]
Epoch [48/120    avg_loss:0.029, val_acc:0.974]
Epoch [49/120    avg_loss:0.022, val_acc:0.976]
Epoch [50/120    avg_loss:0.014, val_acc:0.979]
Epoch [51/120    avg_loss:0.017, val_acc:0.979]
Epoch [52/120    avg_loss:0.011, val_acc:0.982]
Epoch [53/120    avg_loss:0.012, val_acc:0.982]
Epoch [54/120    avg_loss:0.065, val_acc:0.533]
Epoch [55/120    avg_loss:0.093, val_acc:0.977]
Epoch [56/120    avg_loss:0.028, val_acc:0.976]
Epoch [57/120    avg_loss:0.034, val_acc:0.952]
Epoch [58/120    avg_loss:0.024, val_acc:0.979]
Epoch [59/120    avg_loss:0.015, val_acc:0.982]
Epoch [60/120    avg_loss:0.009, val_acc:0.987]
Epoch [61/120    avg_loss:0.014, val_acc:0.978]
Epoch [62/120    avg_loss:0.010, val_acc:0.987]
Epoch [63/120    avg_loss:0.009, val_acc:0.989]
Epoch [64/120    avg_loss:0.008, val_acc:0.984]
Epoch [65/120    avg_loss:0.011, val_acc:0.989]
Epoch [66/120    avg_loss:0.008, val_acc:0.988]
Epoch [67/120    avg_loss:0.007, val_acc:0.986]
Epoch [68/120    avg_loss:0.004, val_acc:0.990]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.017, val_acc:0.980]
Epoch [71/120    avg_loss:0.012, val_acc:0.972]
Epoch [72/120    avg_loss:0.010, val_acc:0.987]
Epoch [73/120    avg_loss:0.008, val_acc:0.987]
Epoch [74/120    avg_loss:0.004, val_acc:0.987]
Epoch [75/120    avg_loss:0.009, val_acc:0.989]
Epoch [76/120    avg_loss:0.007, val_acc:0.990]
Epoch [77/120    avg_loss:0.019, val_acc:0.979]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.989]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.047, val_acc:0.890]
Epoch [82/120    avg_loss:0.031, val_acc:0.988]
Epoch [83/120    avg_loss:0.016, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.989]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.012, val_acc:0.987]
Epoch [88/120    avg_loss:0.017, val_acc:0.978]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.003, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.131, val_acc:0.869]
Epoch [104/120    avg_loss:0.117, val_acc:0.975]
Epoch [105/120    avg_loss:0.050, val_acc:0.980]
Epoch [106/120    avg_loss:0.025, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.982]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.016, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.030, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     0     3     0     0     1    71     2]
 [    0     5 17869     0    67     0   146     0     3     0]
 [    0     0     0  1984     1     0     0     0    44     7]
 [    0    25    12     0  2911     0     0     1    10    13]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4872     0     2     0]
 [    0     8     0     0     0     0     2  1273     0     7]
 [    0    13     0    10    51     0     0     0  3493     4]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
98.66483503241511

F1 scores:
[       nan 0.9900296  0.99352256 0.98363907 0.96727031 0.9893859
 0.9844413  0.99259259 0.97108702 0.95899399]

Kappa:
0.982347863145226
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fedbc4e68d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.614, val_acc:0.509]
Epoch [2/120    avg_loss:0.936, val_acc:0.703]
Epoch [3/120    avg_loss:0.659, val_acc:0.634]
Epoch [4/120    avg_loss:0.535, val_acc:0.791]
Epoch [5/120    avg_loss:0.419, val_acc:0.784]
Epoch [6/120    avg_loss:0.407, val_acc:0.803]
Epoch [7/120    avg_loss:0.341, val_acc:0.797]
Epoch [8/120    avg_loss:0.294, val_acc:0.851]
Epoch [9/120    avg_loss:0.268, val_acc:0.877]
Epoch [10/120    avg_loss:0.288, val_acc:0.868]
Epoch [11/120    avg_loss:0.195, val_acc:0.944]
Epoch [12/120    avg_loss:0.158, val_acc:0.821]
Epoch [13/120    avg_loss:0.192, val_acc:0.886]
Epoch [14/120    avg_loss:0.228, val_acc:0.874]
Epoch [15/120    avg_loss:0.152, val_acc:0.929]
Epoch [16/120    avg_loss:0.109, val_acc:0.964]
Epoch [17/120    avg_loss:0.126, val_acc:0.953]
Epoch [18/120    avg_loss:0.118, val_acc:0.942]
Epoch [19/120    avg_loss:0.159, val_acc:0.919]
Epoch [20/120    avg_loss:0.114, val_acc:0.906]
Epoch [21/120    avg_loss:0.077, val_acc:0.961]
Epoch [22/120    avg_loss:0.076, val_acc:0.953]
Epoch [23/120    avg_loss:0.072, val_acc:0.925]
Epoch [24/120    avg_loss:0.091, val_acc:0.968]
Epoch [25/120    avg_loss:0.089, val_acc:0.963]
Epoch [26/120    avg_loss:0.079, val_acc:0.962]
Epoch [27/120    avg_loss:0.049, val_acc:0.956]
Epoch [28/120    avg_loss:0.042, val_acc:0.981]
Epoch [29/120    avg_loss:0.031, val_acc:0.968]
Epoch [30/120    avg_loss:0.062, val_acc:0.950]
Epoch [31/120    avg_loss:0.066, val_acc:0.977]
Epoch [32/120    avg_loss:0.038, val_acc:0.978]
Epoch [33/120    avg_loss:0.036, val_acc:0.973]
Epoch [34/120    avg_loss:0.058, val_acc:0.949]
Epoch [35/120    avg_loss:0.128, val_acc:0.942]
Epoch [36/120    avg_loss:0.047, val_acc:0.973]
Epoch [37/120    avg_loss:0.041, val_acc:0.974]
Epoch [38/120    avg_loss:0.058, val_acc:0.960]
Epoch [39/120    avg_loss:0.040, val_acc:0.975]
Epoch [40/120    avg_loss:0.045, val_acc:0.938]
Epoch [41/120    avg_loss:0.047, val_acc:0.972]
Epoch [42/120    avg_loss:0.022, val_acc:0.979]
Epoch [43/120    avg_loss:0.018, val_acc:0.979]
Epoch [44/120    avg_loss:0.019, val_acc:0.980]
Epoch [45/120    avg_loss:0.015, val_acc:0.981]
Epoch [46/120    avg_loss:0.012, val_acc:0.983]
Epoch [47/120    avg_loss:0.013, val_acc:0.983]
Epoch [48/120    avg_loss:0.019, val_acc:0.983]
Epoch [49/120    avg_loss:0.015, val_acc:0.984]
Epoch [50/120    avg_loss:0.016, val_acc:0.984]
Epoch [51/120    avg_loss:0.017, val_acc:0.984]
Epoch [52/120    avg_loss:0.015, val_acc:0.983]
Epoch [53/120    avg_loss:0.019, val_acc:0.983]
Epoch [54/120    avg_loss:0.016, val_acc:0.984]
Epoch [55/120    avg_loss:0.018, val_acc:0.984]
Epoch [56/120    avg_loss:0.012, val_acc:0.986]
Epoch [57/120    avg_loss:0.013, val_acc:0.985]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.012, val_acc:0.985]
Epoch [60/120    avg_loss:0.014, val_acc:0.987]
Epoch [61/120    avg_loss:0.013, val_acc:0.988]
Epoch [62/120    avg_loss:0.014, val_acc:0.988]
Epoch [63/120    avg_loss:0.012, val_acc:0.987]
Epoch [64/120    avg_loss:0.012, val_acc:0.988]
Epoch [65/120    avg_loss:0.009, val_acc:0.988]
Epoch [66/120    avg_loss:0.018, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.987]
Epoch [68/120    avg_loss:0.009, val_acc:0.988]
Epoch [69/120    avg_loss:0.012, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.011, val_acc:0.988]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.012, val_acc:0.985]
Epoch [74/120    avg_loss:0.010, val_acc:0.988]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.014, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6309     0     0     0     0    32     0    91     0]
 [    0     0 17972     0    70     0    32     0    16     0]
 [    0     9     0  1950     2     0     0     0    65    10]
 [    0    43    15     0  2892     0     6     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    31     0     0     0  4839     0     8     0]
 [    0     4     0     0     0     0     0  1286     0     0]
 [    0    10     0     5    61     0     0     0  3490     5]
 [    0     0     0     0    14    15     0     0     0   890]]

Accuracy:
98.65037476200806

F1 scores:
[       nan 0.98524245 0.99545807 0.9771987  0.9622359  0.99428571
 0.98886278 0.9984472  0.96182996 0.97587719]

Kappa:
0.9821313855136166
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e74c6f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.667, val_acc:0.486]
Epoch [2/120    avg_loss:0.999, val_acc:0.640]
Epoch [3/120    avg_loss:0.685, val_acc:0.760]
Epoch [4/120    avg_loss:0.566, val_acc:0.769]
Epoch [5/120    avg_loss:0.459, val_acc:0.756]
Epoch [6/120    avg_loss:0.425, val_acc:0.813]
Epoch [7/120    avg_loss:0.358, val_acc:0.880]
Epoch [8/120    avg_loss:0.301, val_acc:0.857]
Epoch [9/120    avg_loss:0.246, val_acc:0.841]
Epoch [10/120    avg_loss:0.244, val_acc:0.879]
Epoch [11/120    avg_loss:0.233, val_acc:0.923]
Epoch [12/120    avg_loss:0.201, val_acc:0.899]
Epoch [13/120    avg_loss:0.185, val_acc:0.930]
Epoch [14/120    avg_loss:0.172, val_acc:0.922]
Epoch [15/120    avg_loss:0.197, val_acc:0.872]
Epoch [16/120    avg_loss:0.159, val_acc:0.899]
Epoch [17/120    avg_loss:0.142, val_acc:0.902]
Epoch [18/120    avg_loss:0.131, val_acc:0.887]
Epoch [19/120    avg_loss:0.127, val_acc:0.899]
Epoch [20/120    avg_loss:0.130, val_acc:0.932]
Epoch [21/120    avg_loss:0.147, val_acc:0.951]
Epoch [22/120    avg_loss:0.112, val_acc:0.900]
Epoch [23/120    avg_loss:0.110, val_acc:0.940]
Epoch [24/120    avg_loss:0.077, val_acc:0.964]
Epoch [25/120    avg_loss:0.085, val_acc:0.913]
Epoch [26/120    avg_loss:0.082, val_acc:0.937]
Epoch [27/120    avg_loss:0.165, val_acc:0.913]
Epoch [28/120    avg_loss:0.110, val_acc:0.944]
Epoch [29/120    avg_loss:0.075, val_acc:0.948]
Epoch [30/120    avg_loss:0.116, val_acc:0.937]
Epoch [31/120    avg_loss:0.077, val_acc:0.963]
Epoch [32/120    avg_loss:0.063, val_acc:0.952]
Epoch [33/120    avg_loss:0.081, val_acc:0.949]
Epoch [34/120    avg_loss:0.064, val_acc:0.934]
Epoch [35/120    avg_loss:0.050, val_acc:0.951]
Epoch [36/120    avg_loss:0.058, val_acc:0.935]
Epoch [37/120    avg_loss:0.083, val_acc:0.963]
Epoch [38/120    avg_loss:0.044, val_acc:0.971]
Epoch [39/120    avg_loss:0.036, val_acc:0.969]
Epoch [40/120    avg_loss:0.035, val_acc:0.974]
Epoch [41/120    avg_loss:0.029, val_acc:0.974]
Epoch [42/120    avg_loss:0.031, val_acc:0.974]
Epoch [43/120    avg_loss:0.031, val_acc:0.974]
Epoch [44/120    avg_loss:0.028, val_acc:0.971]
Epoch [45/120    avg_loss:0.028, val_acc:0.972]
Epoch [46/120    avg_loss:0.025, val_acc:0.973]
Epoch [47/120    avg_loss:0.025, val_acc:0.973]
Epoch [48/120    avg_loss:0.025, val_acc:0.974]
Epoch [49/120    avg_loss:0.028, val_acc:0.973]
Epoch [50/120    avg_loss:0.023, val_acc:0.972]
Epoch [51/120    avg_loss:0.022, val_acc:0.971]
Epoch [52/120    avg_loss:0.021, val_acc:0.974]
Epoch [53/120    avg_loss:0.020, val_acc:0.976]
Epoch [54/120    avg_loss:0.023, val_acc:0.972]
Epoch [55/120    avg_loss:0.025, val_acc:0.974]
Epoch [56/120    avg_loss:0.022, val_acc:0.973]
Epoch [57/120    avg_loss:0.018, val_acc:0.975]
Epoch [58/120    avg_loss:0.021, val_acc:0.976]
Epoch [59/120    avg_loss:0.024, val_acc:0.977]
Epoch [60/120    avg_loss:0.019, val_acc:0.977]
Epoch [61/120    avg_loss:0.020, val_acc:0.976]
Epoch [62/120    avg_loss:0.023, val_acc:0.969]
Epoch [63/120    avg_loss:0.024, val_acc:0.974]
Epoch [64/120    avg_loss:0.025, val_acc:0.971]
Epoch [65/120    avg_loss:0.017, val_acc:0.974]
Epoch [66/120    avg_loss:0.018, val_acc:0.977]
Epoch [67/120    avg_loss:0.017, val_acc:0.977]
Epoch [68/120    avg_loss:0.016, val_acc:0.976]
Epoch [69/120    avg_loss:0.019, val_acc:0.974]
Epoch [70/120    avg_loss:0.020, val_acc:0.977]
Epoch [71/120    avg_loss:0.024, val_acc:0.972]
Epoch [72/120    avg_loss:0.016, val_acc:0.974]
Epoch [73/120    avg_loss:0.016, val_acc:0.977]
Epoch [74/120    avg_loss:0.016, val_acc:0.976]
Epoch [75/120    avg_loss:0.017, val_acc:0.975]
Epoch [76/120    avg_loss:0.018, val_acc:0.972]
Epoch [77/120    avg_loss:0.019, val_acc:0.972]
Epoch [78/120    avg_loss:0.016, val_acc:0.977]
Epoch [79/120    avg_loss:0.016, val_acc:0.976]
Epoch [80/120    avg_loss:0.016, val_acc:0.977]
Epoch [81/120    avg_loss:0.017, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.977]
Epoch [83/120    avg_loss:0.025, val_acc:0.977]
Epoch [84/120    avg_loss:0.014, val_acc:0.977]
Epoch [85/120    avg_loss:0.020, val_acc:0.978]
Epoch [86/120    avg_loss:0.016, val_acc:0.977]
Epoch [87/120    avg_loss:0.016, val_acc:0.977]
Epoch [88/120    avg_loss:0.014, val_acc:0.977]
Epoch [89/120    avg_loss:0.016, val_acc:0.976]
Epoch [90/120    avg_loss:0.014, val_acc:0.976]
Epoch [91/120    avg_loss:0.018, val_acc:0.979]
Epoch [92/120    avg_loss:0.016, val_acc:0.980]
Epoch [93/120    avg_loss:0.013, val_acc:0.975]
Epoch [94/120    avg_loss:0.016, val_acc:0.977]
Epoch [95/120    avg_loss:0.016, val_acc:0.979]
Epoch [96/120    avg_loss:0.014, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.978]
Epoch [98/120    avg_loss:0.011, val_acc:0.977]
Epoch [99/120    avg_loss:0.014, val_acc:0.980]
Epoch [100/120    avg_loss:0.016, val_acc:0.977]
Epoch [101/120    avg_loss:0.009, val_acc:0.977]
Epoch [102/120    avg_loss:0.016, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.979]
Epoch [104/120    avg_loss:0.016, val_acc:0.978]
Epoch [105/120    avg_loss:0.014, val_acc:0.979]
Epoch [106/120    avg_loss:0.015, val_acc:0.977]
Epoch [107/120    avg_loss:0.013, val_acc:0.980]
Epoch [108/120    avg_loss:0.018, val_acc:0.977]
Epoch [109/120    avg_loss:0.014, val_acc:0.978]
Epoch [110/120    avg_loss:0.013, val_acc:0.977]
Epoch [111/120    avg_loss:0.013, val_acc:0.979]
Epoch [112/120    avg_loss:0.014, val_acc:0.976]
Epoch [113/120    avg_loss:0.012, val_acc:0.975]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.013, val_acc:0.976]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.013, val_acc:0.979]
Epoch [118/120    avg_loss:0.013, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.977]
Epoch [120/120    avg_loss:0.012, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6314     0    10     5     0     3    28    72     0]
 [    0     1 17868     0    29     0   183     0     5     4]
 [    0     3     0  1943     3     0     0     0    78     9]
 [    0    14     8     0  2923     0     1     1    19     6]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     3     0     0  4870     0     5     0]
 [    0    15     0     0     0     0     2  1261     0    12]
 [    0     1     0    28    54     0     0     0  3483     5]
 [    0     0     0     0    14    19     0     0     0   886]]

Accuracy:
98.45275106644495

F1 scores:
[       nan 0.98810642 0.99360507 0.96666667 0.97433333 0.99200609
 0.9801751  0.97751938 0.96308586 0.96147585]

Kappa:
0.9795470721983075
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f8233a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.680, val_acc:0.619]
Epoch [2/120    avg_loss:0.965, val_acc:0.589]
Epoch [3/120    avg_loss:0.677, val_acc:0.673]
Epoch [4/120    avg_loss:0.507, val_acc:0.794]
Epoch [5/120    avg_loss:0.413, val_acc:0.738]
Epoch [6/120    avg_loss:0.336, val_acc:0.861]
Epoch [7/120    avg_loss:0.296, val_acc:0.874]
Epoch [8/120    avg_loss:0.282, val_acc:0.839]
Epoch [9/120    avg_loss:0.225, val_acc:0.892]
Epoch [10/120    avg_loss:0.237, val_acc:0.903]
Epoch [11/120    avg_loss:0.203, val_acc:0.869]
Epoch [12/120    avg_loss:0.193, val_acc:0.843]
Epoch [13/120    avg_loss:0.179, val_acc:0.815]
Epoch [14/120    avg_loss:0.201, val_acc:0.912]
Epoch [15/120    avg_loss:0.194, val_acc:0.908]
Epoch [16/120    avg_loss:0.157, val_acc:0.919]
Epoch [17/120    avg_loss:0.163, val_acc:0.910]
Epoch [18/120    avg_loss:0.105, val_acc:0.940]
Epoch [19/120    avg_loss:0.116, val_acc:0.865]
Epoch [20/120    avg_loss:0.113, val_acc:0.942]
Epoch [21/120    avg_loss:0.103, val_acc:0.935]
Epoch [22/120    avg_loss:0.118, val_acc:0.937]
Epoch [23/120    avg_loss:0.111, val_acc:0.953]
Epoch [24/120    avg_loss:0.121, val_acc:0.942]
Epoch [25/120    avg_loss:0.099, val_acc:0.946]
Epoch [26/120    avg_loss:0.129, val_acc:0.895]
Epoch [27/120    avg_loss:0.118, val_acc:0.926]
Epoch [28/120    avg_loss:0.111, val_acc:0.944]
Epoch [29/120    avg_loss:0.087, val_acc:0.937]
Epoch [30/120    avg_loss:0.104, val_acc:0.910]
Epoch [31/120    avg_loss:0.150, val_acc:0.933]
Epoch [32/120    avg_loss:0.069, val_acc:0.943]
Epoch [33/120    avg_loss:0.101, val_acc:0.918]
Epoch [34/120    avg_loss:0.058, val_acc:0.957]
Epoch [35/120    avg_loss:0.072, val_acc:0.911]
Epoch [36/120    avg_loss:0.075, val_acc:0.968]
Epoch [37/120    avg_loss:0.061, val_acc:0.952]
Epoch [38/120    avg_loss:0.073, val_acc:0.963]
Epoch [39/120    avg_loss:0.071, val_acc:0.967]
Epoch [40/120    avg_loss:0.060, val_acc:0.953]
Epoch [41/120    avg_loss:0.046, val_acc:0.974]
Epoch [42/120    avg_loss:0.048, val_acc:0.968]
Epoch [43/120    avg_loss:0.027, val_acc:0.970]
Epoch [44/120    avg_loss:0.026, val_acc:0.974]
Epoch [45/120    avg_loss:0.018, val_acc:0.973]
Epoch [46/120    avg_loss:0.027, val_acc:0.974]
Epoch [47/120    avg_loss:0.028, val_acc:0.960]
Epoch [48/120    avg_loss:0.035, val_acc:0.955]
Epoch [49/120    avg_loss:0.023, val_acc:0.973]
Epoch [50/120    avg_loss:0.016, val_acc:0.968]
Epoch [51/120    avg_loss:0.023, val_acc:0.966]
Epoch [52/120    avg_loss:0.023, val_acc:0.974]
Epoch [53/120    avg_loss:0.015, val_acc:0.979]
Epoch [54/120    avg_loss:0.023, val_acc:0.983]
Epoch [55/120    avg_loss:0.026, val_acc:0.966]
Epoch [56/120    avg_loss:0.037, val_acc:0.963]
Epoch [57/120    avg_loss:0.021, val_acc:0.977]
Epoch [58/120    avg_loss:0.031, val_acc:0.966]
Epoch [59/120    avg_loss:0.030, val_acc:0.977]
Epoch [60/120    avg_loss:0.018, val_acc:0.977]
Epoch [61/120    avg_loss:0.016, val_acc:0.975]
Epoch [62/120    avg_loss:0.019, val_acc:0.977]
Epoch [63/120    avg_loss:0.017, val_acc:0.980]
Epoch [64/120    avg_loss:0.015, val_acc:0.976]
Epoch [65/120    avg_loss:0.019, val_acc:0.971]
Epoch [66/120    avg_loss:0.040, val_acc:0.940]
Epoch [67/120    avg_loss:0.041, val_acc:0.973]
Epoch [68/120    avg_loss:0.023, val_acc:0.975]
Epoch [69/120    avg_loss:0.012, val_acc:0.976]
Epoch [70/120    avg_loss:0.013, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.009, val_acc:0.979]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.010, val_acc:0.980]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.009, val_acc:0.981]
Epoch [79/120    avg_loss:0.009, val_acc:0.982]
Epoch [80/120    avg_loss:0.014, val_acc:0.981]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.980]
Epoch [84/120    avg_loss:0.011, val_acc:0.980]
Epoch [85/120    avg_loss:0.008, val_acc:0.980]
Epoch [86/120    avg_loss:0.009, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.980]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.013, val_acc:0.981]
Epoch [90/120    avg_loss:0.009, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.010, val_acc:0.981]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.008, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     0     0    18     1    74     0]
 [    0     0 18005     0    26     0    47     0    12     0]
 [    0     4     0  1940     1     0     0     0    89     2]
 [    0    17    11     0  2924     0     1     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0     4     0     0     0     0     5  1280     0     1]
 [    0     0     0    18    64     0     0     0  3482     7]
 [    0     0     0     0    13    29     0     0     0   877]]

Accuracy:
98.87691899838528

F1 scores:
[       nan 0.99077837 0.99734116 0.97145719 0.97466667 0.98901099
 0.99246743 0.99572151 0.96055172 0.97120709]

Kappa:
0.9851305628291191
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a8a22e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.615, val_acc:0.595]
Epoch [2/120    avg_loss:0.918, val_acc:0.674]
Epoch [3/120    avg_loss:0.665, val_acc:0.774]
Epoch [4/120    avg_loss:0.490, val_acc:0.732]
Epoch [5/120    avg_loss:0.468, val_acc:0.803]
Epoch [6/120    avg_loss:0.345, val_acc:0.863]
Epoch [7/120    avg_loss:0.308, val_acc:0.858]
Epoch [8/120    avg_loss:0.321, val_acc:0.892]
Epoch [9/120    avg_loss:0.250, val_acc:0.836]
Epoch [10/120    avg_loss:0.233, val_acc:0.855]
Epoch [11/120    avg_loss:0.234, val_acc:0.893]
Epoch [12/120    avg_loss:0.205, val_acc:0.925]
Epoch [13/120    avg_loss:0.169, val_acc:0.917]
Epoch [14/120    avg_loss:0.149, val_acc:0.906]
Epoch [15/120    avg_loss:0.160, val_acc:0.931]
Epoch [16/120    avg_loss:0.142, val_acc:0.884]
Epoch [17/120    avg_loss:0.208, val_acc:0.898]
Epoch [18/120    avg_loss:0.106, val_acc:0.914]
Epoch [19/120    avg_loss:0.139, val_acc:0.934]
Epoch [20/120    avg_loss:0.196, val_acc:0.951]
Epoch [21/120    avg_loss:0.093, val_acc:0.931]
Epoch [22/120    avg_loss:0.092, val_acc:0.945]
Epoch [23/120    avg_loss:0.071, val_acc:0.959]
Epoch [24/120    avg_loss:0.103, val_acc:0.869]
Epoch [25/120    avg_loss:0.178, val_acc:0.924]
Epoch [26/120    avg_loss:0.099, val_acc:0.938]
Epoch [27/120    avg_loss:0.103, val_acc:0.953]
Epoch [28/120    avg_loss:0.089, val_acc:0.961]
Epoch [29/120    avg_loss:0.077, val_acc:0.956]
Epoch [30/120    avg_loss:0.065, val_acc:0.962]
Epoch [31/120    avg_loss:0.084, val_acc:0.945]
Epoch [32/120    avg_loss:0.074, val_acc:0.974]
Epoch [33/120    avg_loss:0.055, val_acc:0.968]
Epoch [34/120    avg_loss:0.073, val_acc:0.933]
Epoch [35/120    avg_loss:0.077, val_acc:0.963]
Epoch [36/120    avg_loss:0.048, val_acc:0.966]
Epoch [37/120    avg_loss:0.069, val_acc:0.968]
Epoch [38/120    avg_loss:0.073, val_acc:0.946]
Epoch [39/120    avg_loss:0.051, val_acc:0.961]
Epoch [40/120    avg_loss:0.036, val_acc:0.975]
Epoch [41/120    avg_loss:0.044, val_acc:0.975]
Epoch [42/120    avg_loss:0.026, val_acc:0.975]
Epoch [43/120    avg_loss:0.063, val_acc:0.950]
Epoch [44/120    avg_loss:0.041, val_acc:0.976]
Epoch [45/120    avg_loss:0.027, val_acc:0.977]
Epoch [46/120    avg_loss:0.032, val_acc:0.971]
Epoch [47/120    avg_loss:0.029, val_acc:0.962]
Epoch [48/120    avg_loss:0.034, val_acc:0.972]
Epoch [49/120    avg_loss:0.018, val_acc:0.971]
Epoch [50/120    avg_loss:0.013, val_acc:0.978]
Epoch [51/120    avg_loss:0.017, val_acc:0.982]
Epoch [52/120    avg_loss:0.014, val_acc:0.983]
Epoch [53/120    avg_loss:0.012, val_acc:0.976]
Epoch [54/120    avg_loss:0.017, val_acc:0.982]
Epoch [55/120    avg_loss:0.018, val_acc:0.979]
Epoch [56/120    avg_loss:0.012, val_acc:0.983]
Epoch [57/120    avg_loss:0.011, val_acc:0.980]
Epoch [58/120    avg_loss:0.011, val_acc:0.980]
Epoch [59/120    avg_loss:0.015, val_acc:0.982]
Epoch [60/120    avg_loss:0.016, val_acc:0.940]
Epoch [61/120    avg_loss:0.055, val_acc:0.976]
Epoch [62/120    avg_loss:0.017, val_acc:0.980]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.978]
Epoch [65/120    avg_loss:0.017, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.982]
Epoch [67/120    avg_loss:0.015, val_acc:0.976]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.984]
Epoch [70/120    avg_loss:0.008, val_acc:0.984]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.982]
Epoch [75/120    avg_loss:0.009, val_acc:0.982]
Epoch [76/120    avg_loss:0.008, val_acc:0.987]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.004, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.013, val_acc:0.972]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.014, val_acc:0.977]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.976]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.982]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.987]
Epoch [100/120    avg_loss:0.002, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.003, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.003, val_acc:0.987]
Epoch [107/120    avg_loss:0.003, val_acc:0.987]
Epoch [108/120    avg_loss:0.002, val_acc:0.987]
Epoch [109/120    avg_loss:0.003, val_acc:0.987]
Epoch [110/120    avg_loss:0.003, val_acc:0.987]
Epoch [111/120    avg_loss:0.003, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.003, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.003, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6272     0    22     4     0    25    21    87     1]
 [    0     0 18083     0     4     0     0     0     3     0]
 [    0     1     0  1960     0     0     0     0    68     7]
 [    0    24    16     0  2914     0     2     5    11     0]
 [    0     0     0     0     0  1302     0     0     0     3]
 [    0     0    10     0     0     0  4868     0     0     0]
 [    0     8     0     0     0     0     5  1271     0     6]
 [    0     1     0    39    59     0     0     0  3461    11]
 [    0     0     0     0    14    17     0     0     0   888]]

Accuracy:
98.85763863784253

F1 scores:
[       nan 0.98476998 0.99908837 0.96623121 0.97670521 0.99237805
 0.99570464 0.98260533 0.96125538 0.96784741]

Kappa:
0.9848640279197416
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f492a1a58d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.647, val_acc:0.704]
Epoch [2/120    avg_loss:0.968, val_acc:0.751]
Epoch [3/120    avg_loss:0.731, val_acc:0.781]
Epoch [4/120    avg_loss:0.550, val_acc:0.826]
Epoch [5/120    avg_loss:0.434, val_acc:0.802]
Epoch [6/120    avg_loss:0.348, val_acc:0.837]
Epoch [7/120    avg_loss:0.330, val_acc:0.827]
Epoch [8/120    avg_loss:0.325, val_acc:0.899]
Epoch [9/120    avg_loss:0.256, val_acc:0.915]
Epoch [10/120    avg_loss:0.206, val_acc:0.907]
Epoch [11/120    avg_loss:0.191, val_acc:0.924]
Epoch [12/120    avg_loss:0.195, val_acc:0.917]
Epoch [13/120    avg_loss:0.198, val_acc:0.941]
Epoch [14/120    avg_loss:0.159, val_acc:0.922]
Epoch [15/120    avg_loss:0.135, val_acc:0.940]
Epoch [16/120    avg_loss:0.137, val_acc:0.938]
Epoch [17/120    avg_loss:0.115, val_acc:0.951]
Epoch [18/120    avg_loss:0.136, val_acc:0.941]
Epoch [19/120    avg_loss:0.096, val_acc:0.951]
Epoch [20/120    avg_loss:0.093, val_acc:0.966]
Epoch [21/120    avg_loss:0.068, val_acc:0.965]
Epoch [22/120    avg_loss:0.077, val_acc:0.967]
Epoch [23/120    avg_loss:0.091, val_acc:0.920]
Epoch [24/120    avg_loss:0.081, val_acc:0.964]
Epoch [25/120    avg_loss:0.107, val_acc:0.940]
Epoch [26/120    avg_loss:0.070, val_acc:0.976]
Epoch [27/120    avg_loss:0.076, val_acc:0.967]
Epoch [28/120    avg_loss:0.066, val_acc:0.908]
Epoch [29/120    avg_loss:0.064, val_acc:0.967]
Epoch [30/120    avg_loss:0.066, val_acc:0.950]
Epoch [31/120    avg_loss:0.072, val_acc:0.971]
Epoch [32/120    avg_loss:0.082, val_acc:0.965]
Epoch [33/120    avg_loss:0.040, val_acc:0.983]
Epoch [34/120    avg_loss:0.076, val_acc:0.959]
Epoch [35/120    avg_loss:0.083, val_acc:0.971]
Epoch [36/120    avg_loss:0.034, val_acc:0.975]
Epoch [37/120    avg_loss:0.041, val_acc:0.961]
Epoch [38/120    avg_loss:0.054, val_acc:0.973]
Epoch [39/120    avg_loss:0.021, val_acc:0.976]
Epoch [40/120    avg_loss:0.049, val_acc:0.897]
Epoch [41/120    avg_loss:0.069, val_acc:0.968]
Epoch [42/120    avg_loss:0.031, val_acc:0.981]
Epoch [43/120    avg_loss:0.032, val_acc:0.975]
Epoch [44/120    avg_loss:0.044, val_acc:0.971]
Epoch [45/120    avg_loss:0.033, val_acc:0.979]
Epoch [46/120    avg_loss:0.025, val_acc:0.982]
Epoch [47/120    avg_loss:0.017, val_acc:0.985]
Epoch [48/120    avg_loss:0.015, val_acc:0.982]
Epoch [49/120    avg_loss:0.009, val_acc:0.983]
Epoch [50/120    avg_loss:0.010, val_acc:0.984]
Epoch [51/120    avg_loss:0.014, val_acc:0.985]
Epoch [52/120    avg_loss:0.009, val_acc:0.985]
Epoch [53/120    avg_loss:0.009, val_acc:0.986]
Epoch [54/120    avg_loss:0.011, val_acc:0.986]
Epoch [55/120    avg_loss:0.009, val_acc:0.986]
Epoch [56/120    avg_loss:0.012, val_acc:0.985]
Epoch [57/120    avg_loss:0.010, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.986]
Epoch [59/120    avg_loss:0.007, val_acc:0.987]
Epoch [60/120    avg_loss:0.009, val_acc:0.987]
Epoch [61/120    avg_loss:0.011, val_acc:0.987]
Epoch [62/120    avg_loss:0.011, val_acc:0.987]
Epoch [63/120    avg_loss:0.012, val_acc:0.987]
Epoch [64/120    avg_loss:0.007, val_acc:0.987]
Epoch [65/120    avg_loss:0.014, val_acc:0.986]
Epoch [66/120    avg_loss:0.016, val_acc:0.986]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.009, val_acc:0.987]
Epoch [69/120    avg_loss:0.008, val_acc:0.987]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.011, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.012, val_acc:0.987]
Epoch [74/120    avg_loss:0.010, val_acc:0.988]
Epoch [75/120    avg_loss:0.011, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.013, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.012, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6292     0     0     0     0    20    11   106     3]
 [    0     0 17997     0    34     0    59     0     0     0]
 [    0     8     0  2004     0     0     0     0    15     9]
 [    0    27    16     0  2922     0     0     0     5     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4864     0    14     0]
 [    0     0     0     0     0     0     0  1285     0     5]
 [    0    18     0     8    79     0     0     0  3458     8]
 [    0     0     0     0    14    26     0     0     0   879]]

Accuracy:
98.82630805196058

F1 scores:
[       nan 0.98489473 0.99698086 0.99011858 0.97060289 0.99013657
 0.9905305  0.99381284 0.96470916 0.96328767]

Kappa:
0.9844637678036774
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97bd525828>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.684, val_acc:0.687]
Epoch [2/120    avg_loss:0.910, val_acc:0.623]
Epoch [3/120    avg_loss:0.672, val_acc:0.787]
Epoch [4/120    avg_loss:0.483, val_acc:0.792]
Epoch [5/120    avg_loss:0.441, val_acc:0.870]
Epoch [6/120    avg_loss:0.415, val_acc:0.862]
Epoch [7/120    avg_loss:0.325, val_acc:0.798]
Epoch [8/120    avg_loss:0.293, val_acc:0.897]
Epoch [9/120    avg_loss:0.251, val_acc:0.743]
Epoch [10/120    avg_loss:0.272, val_acc:0.943]
Epoch [11/120    avg_loss:0.201, val_acc:0.951]
Epoch [12/120    avg_loss:0.193, val_acc:0.941]
Epoch [13/120    avg_loss:0.166, val_acc:0.898]
Epoch [14/120    avg_loss:0.174, val_acc:0.952]
Epoch [15/120    avg_loss:0.105, val_acc:0.916]
Epoch [16/120    avg_loss:0.164, val_acc:0.958]
Epoch [17/120    avg_loss:0.124, val_acc:0.954]
Epoch [18/120    avg_loss:0.157, val_acc:0.853]
Epoch [19/120    avg_loss:0.139, val_acc:0.964]
Epoch [20/120    avg_loss:0.133, val_acc:0.960]
Epoch [21/120    avg_loss:0.122, val_acc:0.942]
Epoch [22/120    avg_loss:0.072, val_acc:0.946]
Epoch [23/120    avg_loss:0.111, val_acc:0.962]
Epoch [24/120    avg_loss:0.097, val_acc:0.930]
Epoch [25/120    avg_loss:0.089, val_acc:0.972]
Epoch [26/120    avg_loss:0.097, val_acc:0.920]
Epoch [27/120    avg_loss:0.146, val_acc:0.914]
Epoch [28/120    avg_loss:0.064, val_acc:0.983]
Epoch [29/120    avg_loss:0.038, val_acc:0.980]
Epoch [30/120    avg_loss:0.031, val_acc:0.973]
Epoch [31/120    avg_loss:0.037, val_acc:0.973]
Epoch [32/120    avg_loss:0.029, val_acc:0.985]
Epoch [33/120    avg_loss:0.056, val_acc:0.963]
Epoch [34/120    avg_loss:0.041, val_acc:0.978]
Epoch [35/120    avg_loss:0.028, val_acc:0.980]
Epoch [36/120    avg_loss:0.032, val_acc:0.978]
Epoch [37/120    avg_loss:0.047, val_acc:0.980]
Epoch [38/120    avg_loss:0.033, val_acc:0.972]
Epoch [39/120    avg_loss:0.120, val_acc:0.945]
Epoch [40/120    avg_loss:0.058, val_acc:0.977]
Epoch [41/120    avg_loss:0.033, val_acc:0.985]
Epoch [42/120    avg_loss:0.028, val_acc:0.979]
Epoch [43/120    avg_loss:0.022, val_acc:0.982]
Epoch [44/120    avg_loss:0.014, val_acc:0.982]
Epoch [45/120    avg_loss:0.036, val_acc:0.975]
Epoch [46/120    avg_loss:0.027, val_acc:0.971]
Epoch [47/120    avg_loss:0.271, val_acc:0.935]
Epoch [48/120    avg_loss:0.107, val_acc:0.955]
Epoch [49/120    avg_loss:0.071, val_acc:0.963]
Epoch [50/120    avg_loss:0.070, val_acc:0.971]
Epoch [51/120    avg_loss:0.051, val_acc:0.973]
Epoch [52/120    avg_loss:0.052, val_acc:0.980]
Epoch [53/120    avg_loss:0.382, val_acc:0.891]
Epoch [54/120    avg_loss:0.143, val_acc:0.932]
Epoch [55/120    avg_loss:0.115, val_acc:0.948]
Epoch [56/120    avg_loss:0.091, val_acc:0.955]
Epoch [57/120    avg_loss:0.081, val_acc:0.958]
Epoch [58/120    avg_loss:0.084, val_acc:0.955]
Epoch [59/120    avg_loss:0.073, val_acc:0.958]
Epoch [60/120    avg_loss:0.063, val_acc:0.962]
Epoch [61/120    avg_loss:0.062, val_acc:0.964]
Epoch [62/120    avg_loss:0.062, val_acc:0.968]
Epoch [63/120    avg_loss:0.050, val_acc:0.966]
Epoch [64/120    avg_loss:0.060, val_acc:0.969]
Epoch [65/120    avg_loss:0.050, val_acc:0.969]
Epoch [66/120    avg_loss:0.050, val_acc:0.973]
Epoch [67/120    avg_loss:0.045, val_acc:0.973]
Epoch [68/120    avg_loss:0.040, val_acc:0.973]
Epoch [69/120    avg_loss:0.043, val_acc:0.974]
Epoch [70/120    avg_loss:0.043, val_acc:0.976]
Epoch [71/120    avg_loss:0.043, val_acc:0.977]
Epoch [72/120    avg_loss:0.044, val_acc:0.977]
Epoch [73/120    avg_loss:0.038, val_acc:0.978]
Epoch [74/120    avg_loss:0.045, val_acc:0.978]
Epoch [75/120    avg_loss:0.040, val_acc:0.978]
Epoch [76/120    avg_loss:0.041, val_acc:0.978]
Epoch [77/120    avg_loss:0.041, val_acc:0.978]
Epoch [78/120    avg_loss:0.049, val_acc:0.979]
Epoch [79/120    avg_loss:0.047, val_acc:0.978]
Epoch [80/120    avg_loss:0.041, val_acc:0.978]
Epoch [81/120    avg_loss:0.039, val_acc:0.978]
Epoch [82/120    avg_loss:0.042, val_acc:0.978]
Epoch [83/120    avg_loss:0.037, val_acc:0.978]
Epoch [84/120    avg_loss:0.038, val_acc:0.978]
Epoch [85/120    avg_loss:0.038, val_acc:0.978]
Epoch [86/120    avg_loss:0.042, val_acc:0.978]
Epoch [87/120    avg_loss:0.041, val_acc:0.978]
Epoch [88/120    avg_loss:0.039, val_acc:0.978]
Epoch [89/120    avg_loss:0.050, val_acc:0.978]
Epoch [90/120    avg_loss:0.033, val_acc:0.978]
Epoch [91/120    avg_loss:0.042, val_acc:0.978]
Epoch [92/120    avg_loss:0.047, val_acc:0.978]
Epoch [93/120    avg_loss:0.046, val_acc:0.978]
Epoch [94/120    avg_loss:0.036, val_acc:0.978]
Epoch [95/120    avg_loss:0.039, val_acc:0.978]
Epoch [96/120    avg_loss:0.047, val_acc:0.978]
Epoch [97/120    avg_loss:0.046, val_acc:0.978]
Epoch [98/120    avg_loss:0.043, val_acc:0.978]
Epoch [99/120    avg_loss:0.040, val_acc:0.978]
Epoch [100/120    avg_loss:0.050, val_acc:0.978]
Epoch [101/120    avg_loss:0.045, val_acc:0.978]
Epoch [102/120    avg_loss:0.042, val_acc:0.978]
Epoch [103/120    avg_loss:0.046, val_acc:0.978]
Epoch [104/120    avg_loss:0.047, val_acc:0.978]
Epoch [105/120    avg_loss:0.049, val_acc:0.978]
Epoch [106/120    avg_loss:0.040, val_acc:0.978]
Epoch [107/120    avg_loss:0.043, val_acc:0.978]
Epoch [108/120    avg_loss:0.042, val_acc:0.978]
Epoch [109/120    avg_loss:0.044, val_acc:0.978]
Epoch [110/120    avg_loss:0.039, val_acc:0.978]
Epoch [111/120    avg_loss:0.041, val_acc:0.978]
Epoch [112/120    avg_loss:0.041, val_acc:0.978]
Epoch [113/120    avg_loss:0.039, val_acc:0.978]
Epoch [114/120    avg_loss:0.044, val_acc:0.978]
Epoch [115/120    avg_loss:0.044, val_acc:0.978]
Epoch [116/120    avg_loss:0.045, val_acc:0.978]
Epoch [117/120    avg_loss:0.045, val_acc:0.978]
Epoch [118/120    avg_loss:0.041, val_acc:0.978]
Epoch [119/120    avg_loss:0.042, val_acc:0.978]
Epoch [120/120    avg_loss:0.045, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6211     0    11     3     0    33     3   124    47]
 [    0     0 17809     0    98     0   183     0     0     0]
 [    0    12     0  1943     0     0     0     0    75     6]
 [    0    45    15     0  2880     0     4     0    26     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4877     0     0     0]
 [    0     3     0     0     0     0     4  1266     0    17]
 [    0    28     0    16    54     0     2     0  3464     7]
 [    0     0     0     0    15    32     0     0     0   872]]

Accuracy:
97.91290097124816

F1 scores:
[       nan 0.97572854 0.99175809 0.96980285 0.95649286 0.98788796
 0.97725679 0.989449   0.95426997 0.93262032]

Kappa:
0.9724296580792618
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22930ee898>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.640, val_acc:0.663]
Epoch [2/120    avg_loss:0.929, val_acc:0.764]
Epoch [3/120    avg_loss:0.621, val_acc:0.705]
Epoch [4/120    avg_loss:0.537, val_acc:0.789]
Epoch [5/120    avg_loss:0.437, val_acc:0.724]
Epoch [6/120    avg_loss:0.350, val_acc:0.819]
Epoch [7/120    avg_loss:0.350, val_acc:0.830]
Epoch [8/120    avg_loss:0.292, val_acc:0.836]
Epoch [9/120    avg_loss:0.303, val_acc:0.858]
Epoch [10/120    avg_loss:0.277, val_acc:0.785]
Epoch [11/120    avg_loss:0.275, val_acc:0.886]
Epoch [12/120    avg_loss:0.216, val_acc:0.869]
Epoch [13/120    avg_loss:0.247, val_acc:0.843]
Epoch [14/120    avg_loss:0.196, val_acc:0.885]
Epoch [15/120    avg_loss:0.214, val_acc:0.882]
Epoch [16/120    avg_loss:0.141, val_acc:0.897]
Epoch [17/120    avg_loss:0.162, val_acc:0.919]
Epoch [18/120    avg_loss:0.161, val_acc:0.910]
Epoch [19/120    avg_loss:0.190, val_acc:0.889]
Epoch [20/120    avg_loss:0.129, val_acc:0.935]
Epoch [21/120    avg_loss:0.134, val_acc:0.919]
Epoch [22/120    avg_loss:0.107, val_acc:0.886]
Epoch [23/120    avg_loss:0.084, val_acc:0.887]
Epoch [24/120    avg_loss:0.154, val_acc:0.920]
Epoch [25/120    avg_loss:0.104, val_acc:0.944]
Epoch [26/120    avg_loss:0.088, val_acc:0.950]
Epoch [27/120    avg_loss:0.081, val_acc:0.928]
Epoch [28/120    avg_loss:0.064, val_acc:0.956]
Epoch [29/120    avg_loss:0.048, val_acc:0.960]
Epoch [30/120    avg_loss:0.083, val_acc:0.896]
Epoch [31/120    avg_loss:0.077, val_acc:0.940]
Epoch [32/120    avg_loss:0.053, val_acc:0.892]
Epoch [33/120    avg_loss:0.084, val_acc:0.959]
Epoch [34/120    avg_loss:0.052, val_acc:0.952]
Epoch [35/120    avg_loss:0.051, val_acc:0.940]
Epoch [36/120    avg_loss:0.046, val_acc:0.969]
Epoch [37/120    avg_loss:0.044, val_acc:0.963]
Epoch [38/120    avg_loss:0.045, val_acc:0.936]
Epoch [39/120    avg_loss:0.109, val_acc:0.954]
Epoch [40/120    avg_loss:0.059, val_acc:0.949]
Epoch [41/120    avg_loss:0.050, val_acc:0.948]
Epoch [42/120    avg_loss:0.046, val_acc:0.960]
Epoch [43/120    avg_loss:0.051, val_acc:0.967]
Epoch [44/120    avg_loss:0.059, val_acc:0.950]
Epoch [45/120    avg_loss:0.064, val_acc:0.963]
Epoch [46/120    avg_loss:0.371, val_acc:0.799]
Epoch [47/120    avg_loss:0.198, val_acc:0.934]
Epoch [48/120    avg_loss:0.076, val_acc:0.919]
Epoch [49/120    avg_loss:0.081, val_acc:0.941]
Epoch [50/120    avg_loss:0.052, val_acc:0.953]
Epoch [51/120    avg_loss:0.050, val_acc:0.961]
Epoch [52/120    avg_loss:0.045, val_acc:0.946]
Epoch [53/120    avg_loss:0.037, val_acc:0.964]
Epoch [54/120    avg_loss:0.035, val_acc:0.969]
Epoch [55/120    avg_loss:0.030, val_acc:0.969]
Epoch [56/120    avg_loss:0.037, val_acc:0.969]
Epoch [57/120    avg_loss:0.026, val_acc:0.966]
Epoch [58/120    avg_loss:0.043, val_acc:0.966]
Epoch [59/120    avg_loss:0.028, val_acc:0.970]
Epoch [60/120    avg_loss:0.025, val_acc:0.966]
Epoch [61/120    avg_loss:0.029, val_acc:0.965]
Epoch [62/120    avg_loss:0.027, val_acc:0.955]
Epoch [63/120    avg_loss:0.026, val_acc:0.970]
Epoch [64/120    avg_loss:0.025, val_acc:0.974]
Epoch [65/120    avg_loss:0.022, val_acc:0.966]
Epoch [66/120    avg_loss:0.020, val_acc:0.972]
Epoch [67/120    avg_loss:0.022, val_acc:0.971]
Epoch [68/120    avg_loss:0.026, val_acc:0.973]
Epoch [69/120    avg_loss:0.023, val_acc:0.969]
Epoch [70/120    avg_loss:0.018, val_acc:0.974]
Epoch [71/120    avg_loss:0.028, val_acc:0.968]
Epoch [72/120    avg_loss:0.023, val_acc:0.973]
Epoch [73/120    avg_loss:0.017, val_acc:0.970]
Epoch [74/120    avg_loss:0.019, val_acc:0.968]
Epoch [75/120    avg_loss:0.023, val_acc:0.970]
Epoch [76/120    avg_loss:0.019, val_acc:0.975]
Epoch [77/120    avg_loss:0.016, val_acc:0.969]
Epoch [78/120    avg_loss:0.017, val_acc:0.978]
Epoch [79/120    avg_loss:0.018, val_acc:0.979]
Epoch [80/120    avg_loss:0.019, val_acc:0.970]
Epoch [81/120    avg_loss:0.017, val_acc:0.973]
Epoch [82/120    avg_loss:0.017, val_acc:0.977]
Epoch [83/120    avg_loss:0.016, val_acc:0.979]
Epoch [84/120    avg_loss:0.019, val_acc:0.974]
Epoch [85/120    avg_loss:0.019, val_acc:0.980]
Epoch [86/120    avg_loss:0.021, val_acc:0.974]
Epoch [87/120    avg_loss:0.015, val_acc:0.979]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.013, val_acc:0.978]
Epoch [90/120    avg_loss:0.015, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.978]
Epoch [92/120    avg_loss:0.016, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.978]
Epoch [94/120    avg_loss:0.018, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.974]
Epoch [96/120    avg_loss:0.017, val_acc:0.979]
Epoch [97/120    avg_loss:0.012, val_acc:0.979]
Epoch [98/120    avg_loss:0.021, val_acc:0.979]
Epoch [99/120    avg_loss:0.028, val_acc:0.971]
Epoch [100/120    avg_loss:0.017, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.018, val_acc:0.980]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.014, val_acc:0.981]
Epoch [107/120    avg_loss:0.017, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.985]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.014, val_acc:0.976]
Epoch [114/120    avg_loss:0.013, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.014, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     2     0     0     0     0    75     0]
 [    0     7 18058     0    24     0     1     0     0     0]
 [    0     2     0  2014     1     0     0     0    13     6]
 [    0    44    12     0  2894     0     8     0    13     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     7  1282     0     1]
 [    0    10     0    14    55     0     0     0  3481    11]
 [    0     0     0     0    14    21     0     0     0   884]]

Accuracy:
99.17576458679777

F1 scores:
[       nan 0.98910506 0.99878319 0.99065421 0.97114094 0.99201824
 0.99836267 0.99688958 0.97329792 0.97036224]

Kappa:
0.9890808771560219
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f184f4d7908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.659, val_acc:0.643]
Epoch [2/120    avg_loss:0.963, val_acc:0.613]
Epoch [3/120    avg_loss:0.665, val_acc:0.725]
Epoch [4/120    avg_loss:0.516, val_acc:0.699]
Epoch [5/120    avg_loss:0.403, val_acc:0.817]
Epoch [6/120    avg_loss:0.412, val_acc:0.831]
Epoch [7/120    avg_loss:0.321, val_acc:0.787]
Epoch [8/120    avg_loss:0.349, val_acc:0.883]
Epoch [9/120    avg_loss:0.318, val_acc:0.848]
Epoch [10/120    avg_loss:0.224, val_acc:0.840]
Epoch [11/120    avg_loss:0.231, val_acc:0.904]
Epoch [12/120    avg_loss:0.230, val_acc:0.918]
Epoch [13/120    avg_loss:0.180, val_acc:0.933]
Epoch [14/120    avg_loss:0.172, val_acc:0.938]
Epoch [15/120    avg_loss:0.129, val_acc:0.929]
Epoch [16/120    avg_loss:0.137, val_acc:0.903]
Epoch [17/120    avg_loss:0.140, val_acc:0.899]
Epoch [18/120    avg_loss:0.156, val_acc:0.958]
Epoch [19/120    avg_loss:0.151, val_acc:0.934]
Epoch [20/120    avg_loss:0.137, val_acc:0.921]
Epoch [21/120    avg_loss:0.101, val_acc:0.922]
Epoch [22/120    avg_loss:0.093, val_acc:0.951]
Epoch [23/120    avg_loss:0.105, val_acc:0.956]
Epoch [24/120    avg_loss:0.087, val_acc:0.948]
Epoch [25/120    avg_loss:0.072, val_acc:0.946]
Epoch [26/120    avg_loss:0.092, val_acc:0.948]
Epoch [27/120    avg_loss:0.094, val_acc:0.946]
Epoch [28/120    avg_loss:0.059, val_acc:0.965]
Epoch [29/120    avg_loss:0.062, val_acc:0.953]
Epoch [30/120    avg_loss:0.053, val_acc:0.946]
Epoch [31/120    avg_loss:0.062, val_acc:0.953]
Epoch [32/120    avg_loss:0.058, val_acc:0.965]
Epoch [33/120    avg_loss:0.044, val_acc:0.951]
Epoch [34/120    avg_loss:0.098, val_acc:0.963]
Epoch [35/120    avg_loss:0.064, val_acc:0.975]
Epoch [36/120    avg_loss:0.050, val_acc:0.978]
Epoch [37/120    avg_loss:0.048, val_acc:0.948]
Epoch [38/120    avg_loss:0.076, val_acc:0.959]
Epoch [39/120    avg_loss:0.037, val_acc:0.965]
Epoch [40/120    avg_loss:0.037, val_acc:0.968]
Epoch [41/120    avg_loss:0.022, val_acc:0.978]
Epoch [42/120    avg_loss:0.026, val_acc:0.964]
Epoch [43/120    avg_loss:0.028, val_acc:0.977]
Epoch [44/120    avg_loss:0.025, val_acc:0.980]
Epoch [45/120    avg_loss:0.030, val_acc:0.973]
Epoch [46/120    avg_loss:0.059, val_acc:0.948]
Epoch [47/120    avg_loss:0.044, val_acc:0.979]
Epoch [48/120    avg_loss:0.031, val_acc:0.979]
Epoch [49/120    avg_loss:0.024, val_acc:0.966]
Epoch [50/120    avg_loss:0.021, val_acc:0.973]
Epoch [51/120    avg_loss:0.021, val_acc:0.976]
Epoch [52/120    avg_loss:0.016, val_acc:0.978]
Epoch [53/120    avg_loss:0.015, val_acc:0.971]
Epoch [54/120    avg_loss:0.017, val_acc:0.984]
Epoch [55/120    avg_loss:0.018, val_acc:0.976]
Epoch [56/120    avg_loss:0.039, val_acc:0.863]
Epoch [57/120    avg_loss:0.043, val_acc:0.948]
Epoch [58/120    avg_loss:0.016, val_acc:0.988]
Epoch [59/120    avg_loss:0.011, val_acc:0.983]
Epoch [60/120    avg_loss:0.014, val_acc:0.958]
Epoch [61/120    avg_loss:0.023, val_acc:0.973]
Epoch [62/120    avg_loss:0.023, val_acc:0.982]
Epoch [63/120    avg_loss:0.010, val_acc:0.983]
Epoch [64/120    avg_loss:0.008, val_acc:0.989]
Epoch [65/120    avg_loss:0.007, val_acc:0.988]
Epoch [66/120    avg_loss:0.011, val_acc:0.974]
Epoch [67/120    avg_loss:0.010, val_acc:0.987]
Epoch [68/120    avg_loss:0.007, val_acc:0.988]
Epoch [69/120    avg_loss:0.017, val_acc:0.973]
Epoch [70/120    avg_loss:0.013, val_acc:0.979]
Epoch [71/120    avg_loss:0.027, val_acc:0.979]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.011, val_acc:0.988]
Epoch [74/120    avg_loss:0.014, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.990]
Epoch [76/120    avg_loss:0.008, val_acc:0.993]
Epoch [77/120    avg_loss:0.008, val_acc:0.991]
Epoch [78/120    avg_loss:0.006, val_acc:0.991]
Epoch [79/120    avg_loss:0.003, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.091, val_acc:0.603]
Epoch [82/120    avg_loss:0.394, val_acc:0.895]
Epoch [83/120    avg_loss:0.148, val_acc:0.950]
Epoch [84/120    avg_loss:0.094, val_acc:0.958]
Epoch [85/120    avg_loss:0.045, val_acc:0.968]
Epoch [86/120    avg_loss:0.071, val_acc:0.967]
Epoch [87/120    avg_loss:0.045, val_acc:0.975]
Epoch [88/120    avg_loss:0.096, val_acc:0.965]
Epoch [89/120    avg_loss:0.121, val_acc:0.949]
Epoch [90/120    avg_loss:0.060, val_acc:0.966]
Epoch [91/120    avg_loss:0.044, val_acc:0.966]
Epoch [92/120    avg_loss:0.030, val_acc:0.969]
Epoch [93/120    avg_loss:0.032, val_acc:0.971]
Epoch [94/120    avg_loss:0.026, val_acc:0.972]
Epoch [95/120    avg_loss:0.034, val_acc:0.970]
Epoch [96/120    avg_loss:0.025, val_acc:0.970]
Epoch [97/120    avg_loss:0.025, val_acc:0.972]
Epoch [98/120    avg_loss:0.025, val_acc:0.973]
Epoch [99/120    avg_loss:0.027, val_acc:0.973]
Epoch [100/120    avg_loss:0.029, val_acc:0.973]
Epoch [101/120    avg_loss:0.021, val_acc:0.974]
Epoch [102/120    avg_loss:0.019, val_acc:0.972]
Epoch [103/120    avg_loss:0.022, val_acc:0.971]
Epoch [104/120    avg_loss:0.030, val_acc:0.971]
Epoch [105/120    avg_loss:0.019, val_acc:0.972]
Epoch [106/120    avg_loss:0.018, val_acc:0.972]
Epoch [107/120    avg_loss:0.018, val_acc:0.971]
Epoch [108/120    avg_loss:0.021, val_acc:0.972]
Epoch [109/120    avg_loss:0.018, val_acc:0.972]
Epoch [110/120    avg_loss:0.024, val_acc:0.972]
Epoch [111/120    avg_loss:0.018, val_acc:0.973]
Epoch [112/120    avg_loss:0.020, val_acc:0.973]
Epoch [113/120    avg_loss:0.020, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.973]
Epoch [115/120    avg_loss:0.019, val_acc:0.973]
Epoch [116/120    avg_loss:0.017, val_acc:0.973]
Epoch [117/120    avg_loss:0.015, val_acc:0.973]
Epoch [118/120    avg_loss:0.021, val_acc:0.973]
Epoch [119/120    avg_loss:0.022, val_acc:0.973]
Epoch [120/120    avg_loss:0.022, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6268     0     0     2     0    30     0   132     0]
 [    0     4 17920     0   106     0    55     0     5     0]
 [    0     0     0  1940     0     0     0     0    88     8]
 [    0    42     3     0  2913     0     2     0    12     0]
 [    0     0     0     0     0  1300     0     0     0     5]
 [    0     0     3     1     0     0  4872     0     2     0]
 [    0     2     0     0     0     0     1  1284     0     3]
 [    0     1     0    27    73     0     0     0  3466     4]
 [    0     0     0     0    20    14     0     0     0   885]]

Accuracy:
98.44552093124142

F1 scores:
[       nan 0.98329281 0.99511328 0.96903097 0.957279   0.99274532
 0.99044521 0.997669   0.95272128 0.97039474]

Kappa:
0.979442609648938
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:150
Validation dataloader:150
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b2e0be898>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.701, val_acc:0.636]
Epoch [2/120    avg_loss:0.946, val_acc:0.725]
Epoch [3/120    avg_loss:0.627, val_acc:0.753]
Epoch [4/120    avg_loss:0.483, val_acc:0.732]
Epoch [5/120    avg_loss:0.411, val_acc:0.787]
Epoch [6/120    avg_loss:0.333, val_acc:0.872]
Epoch [7/120    avg_loss:0.300, val_acc:0.857]
Epoch [8/120    avg_loss:0.268, val_acc:0.816]
Epoch [9/120    avg_loss:0.213, val_acc:0.843]
Epoch [10/120    avg_loss:0.217, val_acc:0.820]
Epoch [11/120    avg_loss:0.204, val_acc:0.804]
Epoch [12/120    avg_loss:0.190, val_acc:0.910]
Epoch [13/120    avg_loss:0.178, val_acc:0.919]
Epoch [14/120    avg_loss:0.167, val_acc:0.921]
Epoch [15/120    avg_loss:0.157, val_acc:0.902]
Epoch [16/120    avg_loss:0.149, val_acc:0.939]
Epoch [17/120    avg_loss:0.135, val_acc:0.928]
Epoch [18/120    avg_loss:0.107, val_acc:0.896]
Epoch [19/120    avg_loss:0.118, val_acc:0.924]
Epoch [20/120    avg_loss:0.118, val_acc:0.939]
Epoch [21/120    avg_loss:0.079, val_acc:0.952]
Epoch [22/120    avg_loss:0.063, val_acc:0.952]
Epoch [23/120    avg_loss:0.075, val_acc:0.951]
Epoch [24/120    avg_loss:0.084, val_acc:0.928]
Epoch [25/120    avg_loss:0.114, val_acc:0.949]
Epoch [26/120    avg_loss:0.095, val_acc:0.929]
Epoch [27/120    avg_loss:0.107, val_acc:0.955]
Epoch [28/120    avg_loss:0.070, val_acc:0.969]
Epoch [29/120    avg_loss:0.064, val_acc:0.974]
Epoch [30/120    avg_loss:0.082, val_acc:0.946]
Epoch [31/120    avg_loss:0.068, val_acc:0.956]
Epoch [32/120    avg_loss:0.047, val_acc:0.969]
Epoch [33/120    avg_loss:0.061, val_acc:0.965]
Epoch [34/120    avg_loss:0.120, val_acc:0.878]
Epoch [35/120    avg_loss:0.123, val_acc:0.965]
Epoch [36/120    avg_loss:0.053, val_acc:0.964]
Epoch [37/120    avg_loss:0.036, val_acc:0.928]
Epoch [38/120    avg_loss:0.042, val_acc:0.966]
Epoch [39/120    avg_loss:0.042, val_acc:0.967]
Epoch [40/120    avg_loss:0.079, val_acc:0.955]
Epoch [41/120    avg_loss:0.047, val_acc:0.937]
Epoch [42/120    avg_loss:0.039, val_acc:0.978]
Epoch [43/120    avg_loss:0.029, val_acc:0.972]
Epoch [44/120    avg_loss:0.022, val_acc:0.967]
Epoch [45/120    avg_loss:0.026, val_acc:0.978]
Epoch [46/120    avg_loss:0.016, val_acc:0.979]
Epoch [47/120    avg_loss:0.023, val_acc:0.983]
Epoch [48/120    avg_loss:0.015, val_acc:0.984]
Epoch [49/120    avg_loss:0.023, val_acc:0.966]
Epoch [50/120    avg_loss:0.019, val_acc:0.983]
Epoch [51/120    avg_loss:0.011, val_acc:0.983]
Epoch [52/120    avg_loss:0.049, val_acc:0.971]
Epoch [53/120    avg_loss:0.038, val_acc:0.976]
Epoch [54/120    avg_loss:0.034, val_acc:0.975]
Epoch [55/120    avg_loss:0.023, val_acc:0.971]
Epoch [56/120    avg_loss:0.015, val_acc:0.971]
Epoch [57/120    avg_loss:0.016, val_acc:0.984]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.028, val_acc:0.978]
Epoch [60/120    avg_loss:0.027, val_acc:0.969]
Epoch [61/120    avg_loss:0.025, val_acc:0.983]
Epoch [62/120    avg_loss:0.010, val_acc:0.979]
Epoch [63/120    avg_loss:0.007, val_acc:0.985]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.011, val_acc:0.985]
Epoch [66/120    avg_loss:0.009, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.972]
Epoch [68/120    avg_loss:0.023, val_acc:0.979]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.015, val_acc:0.980]
Epoch [71/120    avg_loss:0.019, val_acc:0.971]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.980]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.985]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.978]
Epoch [89/120    avg_loss:0.014, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.074, val_acc:0.943]
Epoch [97/120    avg_loss:0.075, val_acc:0.938]
Epoch [98/120    avg_loss:0.026, val_acc:0.972]
Epoch [99/120    avg_loss:0.017, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.014, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6411     0     0     0     0     8     5     8     0]
 [    0     0 18001     0    29     0    57     0     3     0]
 [    0     3     0  1989     0     0     0     0    29    15]
 [    0    46    10     0  2881     0    20     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     2     0     0  4861     0    14     0]
 [    0     8     0     0     0     0     3  1274     0     5]
 [    0    26     0    12    42     0     0     0  3485     6]
 [    0     0     0     0    14    27     0     0     0   878]]

Accuracy:
99.01670161232015

F1 scores:
[       nan 0.9919542  0.99723007 0.98489725 0.97036039 0.98976109
 0.98931515 0.99182561 0.97824561 0.96324739]

Kappa:
0.9869787634560699
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe1e3b07b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.740, val_acc:0.399]
Epoch [2/120    avg_loss:0.993, val_acc:0.739]
Epoch [3/120    avg_loss:0.719, val_acc:0.749]
Epoch [4/120    avg_loss:0.580, val_acc:0.760]
Epoch [5/120    avg_loss:0.429, val_acc:0.839]
Epoch [6/120    avg_loss:0.447, val_acc:0.844]
Epoch [7/120    avg_loss:0.387, val_acc:0.886]
Epoch [8/120    avg_loss:0.308, val_acc:0.838]
Epoch [9/120    avg_loss:0.280, val_acc:0.917]
Epoch [10/120    avg_loss:0.227, val_acc:0.925]
Epoch [11/120    avg_loss:0.260, val_acc:0.896]
Epoch [12/120    avg_loss:0.190, val_acc:0.868]
Epoch [13/120    avg_loss:0.219, val_acc:0.895]
Epoch [14/120    avg_loss:0.265, val_acc:0.912]
Epoch [15/120    avg_loss:0.183, val_acc:0.943]
Epoch [16/120    avg_loss:0.142, val_acc:0.912]
Epoch [17/120    avg_loss:0.125, val_acc:0.943]
Epoch [18/120    avg_loss:0.109, val_acc:0.943]
Epoch [19/120    avg_loss:0.138, val_acc:0.940]
Epoch [20/120    avg_loss:0.131, val_acc:0.942]
Epoch [21/120    avg_loss:0.090, val_acc:0.946]
Epoch [22/120    avg_loss:0.221, val_acc:0.890]
Epoch [23/120    avg_loss:0.149, val_acc:0.892]
Epoch [24/120    avg_loss:0.068, val_acc:0.957]
Epoch [25/120    avg_loss:0.124, val_acc:0.965]
Epoch [26/120    avg_loss:0.093, val_acc:0.922]
Epoch [27/120    avg_loss:0.078, val_acc:0.931]
Epoch [28/120    avg_loss:0.062, val_acc:0.951]
Epoch [29/120    avg_loss:0.061, val_acc:0.939]
Epoch [30/120    avg_loss:0.047, val_acc:0.971]
Epoch [31/120    avg_loss:0.044, val_acc:0.969]
Epoch [32/120    avg_loss:0.038, val_acc:0.969]
Epoch [33/120    avg_loss:0.072, val_acc:0.964]
Epoch [34/120    avg_loss:0.043, val_acc:0.941]
Epoch [35/120    avg_loss:0.036, val_acc:0.971]
Epoch [36/120    avg_loss:0.027, val_acc:0.975]
Epoch [37/120    avg_loss:0.026, val_acc:0.960]
Epoch [38/120    avg_loss:0.258, val_acc:0.899]
Epoch [39/120    avg_loss:0.237, val_acc:0.912]
Epoch [40/120    avg_loss:0.083, val_acc:0.953]
Epoch [41/120    avg_loss:0.084, val_acc:0.968]
Epoch [42/120    avg_loss:0.054, val_acc:0.975]
Epoch [43/120    avg_loss:0.035, val_acc:0.968]
Epoch [44/120    avg_loss:0.071, val_acc:0.955]
Epoch [45/120    avg_loss:0.046, val_acc:0.972]
Epoch [46/120    avg_loss:0.051, val_acc:0.979]
Epoch [47/120    avg_loss:0.039, val_acc:0.977]
Epoch [48/120    avg_loss:0.029, val_acc:0.986]
Epoch [49/120    avg_loss:0.040, val_acc:0.980]
Epoch [50/120    avg_loss:0.020, val_acc:0.979]
Epoch [51/120    avg_loss:0.022, val_acc:0.979]
Epoch [52/120    avg_loss:0.022, val_acc:0.985]
Epoch [53/120    avg_loss:0.595, val_acc:0.278]
Epoch [54/120    avg_loss:1.255, val_acc:0.641]
Epoch [55/120    avg_loss:0.984, val_acc:0.668]
Epoch [56/120    avg_loss:0.912, val_acc:0.710]
Epoch [57/120    avg_loss:0.794, val_acc:0.713]
Epoch [58/120    avg_loss:0.734, val_acc:0.705]
Epoch [59/120    avg_loss:0.655, val_acc:0.774]
Epoch [60/120    avg_loss:0.675, val_acc:0.759]
Epoch [61/120    avg_loss:0.533, val_acc:0.784]
Epoch [62/120    avg_loss:0.522, val_acc:0.781]
Epoch [63/120    avg_loss:0.502, val_acc:0.779]
Epoch [64/120    avg_loss:0.504, val_acc:0.792]
Epoch [65/120    avg_loss:0.508, val_acc:0.795]
Epoch [66/120    avg_loss:0.474, val_acc:0.792]
Epoch [67/120    avg_loss:0.485, val_acc:0.796]
Epoch [68/120    avg_loss:0.445, val_acc:0.797]
Epoch [69/120    avg_loss:0.479, val_acc:0.787]
Epoch [70/120    avg_loss:0.444, val_acc:0.796]
Epoch [71/120    avg_loss:0.457, val_acc:0.808]
Epoch [72/120    avg_loss:0.457, val_acc:0.805]
Epoch [73/120    avg_loss:0.417, val_acc:0.811]
Epoch [74/120    avg_loss:0.437, val_acc:0.809]
Epoch [75/120    avg_loss:0.434, val_acc:0.812]
Epoch [76/120    avg_loss:0.400, val_acc:0.813]
Epoch [77/120    avg_loss:0.437, val_acc:0.813]
Epoch [78/120    avg_loss:0.442, val_acc:0.812]
Epoch [79/120    avg_loss:0.412, val_acc:0.812]
Epoch [80/120    avg_loss:0.413, val_acc:0.815]
Epoch [81/120    avg_loss:0.439, val_acc:0.817]
Epoch [82/120    avg_loss:0.392, val_acc:0.818]
Epoch [83/120    avg_loss:0.403, val_acc:0.817]
Epoch [84/120    avg_loss:0.430, val_acc:0.817]
Epoch [85/120    avg_loss:0.401, val_acc:0.817]
Epoch [86/120    avg_loss:0.413, val_acc:0.818]
Epoch [87/120    avg_loss:0.452, val_acc:0.818]
Epoch [88/120    avg_loss:0.394, val_acc:0.818]
Epoch [89/120    avg_loss:0.401, val_acc:0.818]
Epoch [90/120    avg_loss:0.411, val_acc:0.818]
Epoch [91/120    avg_loss:0.421, val_acc:0.818]
Epoch [92/120    avg_loss:0.440, val_acc:0.818]
Epoch [93/120    avg_loss:0.419, val_acc:0.818]
Epoch [94/120    avg_loss:0.418, val_acc:0.818]
Epoch [95/120    avg_loss:0.414, val_acc:0.818]
Epoch [96/120    avg_loss:0.405, val_acc:0.818]
Epoch [97/120    avg_loss:0.451, val_acc:0.818]
Epoch [98/120    avg_loss:0.417, val_acc:0.818]
Epoch [99/120    avg_loss:0.449, val_acc:0.818]
Epoch [100/120    avg_loss:0.423, val_acc:0.818]
Epoch [101/120    avg_loss:0.423, val_acc:0.818]
Epoch [102/120    avg_loss:0.390, val_acc:0.818]
Epoch [103/120    avg_loss:0.396, val_acc:0.818]
Epoch [104/120    avg_loss:0.382, val_acc:0.818]
Epoch [105/120    avg_loss:0.450, val_acc:0.818]
Epoch [106/120    avg_loss:0.412, val_acc:0.818]
Epoch [107/120    avg_loss:0.430, val_acc:0.818]
Epoch [108/120    avg_loss:0.411, val_acc:0.818]
Epoch [109/120    avg_loss:0.405, val_acc:0.818]
Epoch [110/120    avg_loss:0.412, val_acc:0.818]
Epoch [111/120    avg_loss:0.402, val_acc:0.818]
Epoch [112/120    avg_loss:0.423, val_acc:0.818]
Epoch [113/120    avg_loss:0.386, val_acc:0.818]
Epoch [114/120    avg_loss:0.412, val_acc:0.818]
Epoch [115/120    avg_loss:0.451, val_acc:0.818]
Epoch [116/120    avg_loss:0.404, val_acc:0.818]
Epoch [117/120    avg_loss:0.424, val_acc:0.818]
Epoch [118/120    avg_loss:0.397, val_acc:0.818]
Epoch [119/120    avg_loss:0.422, val_acc:0.818]
Epoch [120/120    avg_loss:0.397, val_acc:0.818]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5296     0     0   674     0    54     1   358    49]
 [    0   103 16100     0    60     0  1801     0    26     0]
 [    0    23     0  1714     6     0     0     0   243    50]
 [    0    99    83     0  2561     0   123     1    99     6]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   722    18    25     0  3972     0   141     0]
 [    0    72     0     0     1     0     1  1205     1    10]
 [    0    84     0    12    51     0    76     0  3326    22]
 [    0    16     0     4    30   117     0     0     2   750]]

Accuracy:
87.31352276287566

F1 scores:
[       nan 0.87356701 0.92013145 0.90591966 0.80282132 0.95709571
 0.72847318 0.96515819 0.85644393 0.83056478]

Kappa:
0.8341466184393147
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fad5008e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.775, val_acc:0.666]
Epoch [2/120    avg_loss:1.099, val_acc:0.687]
Epoch [3/120    avg_loss:0.807, val_acc:0.628]
Epoch [4/120    avg_loss:0.597, val_acc:0.731]
Epoch [5/120    avg_loss:0.490, val_acc:0.816]
Epoch [6/120    avg_loss:0.452, val_acc:0.822]
Epoch [7/120    avg_loss:0.350, val_acc:0.856]
Epoch [8/120    avg_loss:0.295, val_acc:0.890]
Epoch [9/120    avg_loss:0.337, val_acc:0.921]
Epoch [10/120    avg_loss:0.270, val_acc:0.782]
Epoch [11/120    avg_loss:0.269, val_acc:0.866]
Epoch [12/120    avg_loss:0.254, val_acc:0.905]
Epoch [13/120    avg_loss:0.227, val_acc:0.918]
Epoch [14/120    avg_loss:0.193, val_acc:0.848]
Epoch [15/120    avg_loss:0.138, val_acc:0.949]
Epoch [16/120    avg_loss:0.168, val_acc:0.943]
Epoch [17/120    avg_loss:0.281, val_acc:0.748]
Epoch [18/120    avg_loss:0.254, val_acc:0.943]
Epoch [19/120    avg_loss:0.199, val_acc:0.869]
Epoch [20/120    avg_loss:0.157, val_acc:0.959]
Epoch [21/120    avg_loss:0.096, val_acc:0.963]
Epoch [22/120    avg_loss:0.094, val_acc:0.966]
Epoch [23/120    avg_loss:0.107, val_acc:0.965]
Epoch [24/120    avg_loss:0.075, val_acc:0.976]
Epoch [25/120    avg_loss:0.145, val_acc:0.965]
Epoch [26/120    avg_loss:0.068, val_acc:0.965]
Epoch [27/120    avg_loss:0.085, val_acc:0.963]
Epoch [28/120    avg_loss:0.062, val_acc:0.954]
Epoch [29/120    avg_loss:0.085, val_acc:0.965]
Epoch [30/120    avg_loss:0.097, val_acc:0.917]
Epoch [31/120    avg_loss:0.087, val_acc:0.931]
Epoch [32/120    avg_loss:0.071, val_acc:0.970]
Epoch [33/120    avg_loss:0.064, val_acc:0.984]
Epoch [34/120    avg_loss:0.052, val_acc:0.978]
Epoch [35/120    avg_loss:0.070, val_acc:0.975]
Epoch [36/120    avg_loss:0.050, val_acc:0.973]
Epoch [37/120    avg_loss:0.065, val_acc:0.975]
Epoch [38/120    avg_loss:0.043, val_acc:0.984]
Epoch [39/120    avg_loss:0.050, val_acc:0.956]
Epoch [40/120    avg_loss:0.057, val_acc:0.981]
Epoch [41/120    avg_loss:0.051, val_acc:0.973]
Epoch [42/120    avg_loss:0.036, val_acc:0.977]
Epoch [43/120    avg_loss:0.045, val_acc:0.977]
Epoch [44/120    avg_loss:0.080, val_acc:0.973]
Epoch [45/120    avg_loss:0.043, val_acc:0.981]
Epoch [46/120    avg_loss:0.028, val_acc:0.982]
Epoch [47/120    avg_loss:0.028, val_acc:0.965]
Epoch [48/120    avg_loss:0.033, val_acc:0.970]
Epoch [49/120    avg_loss:0.027, val_acc:0.864]
Epoch [50/120    avg_loss:0.035, val_acc:0.973]
Epoch [51/120    avg_loss:0.030, val_acc:0.984]
Epoch [52/120    avg_loss:0.015, val_acc:0.986]
Epoch [53/120    avg_loss:0.031, val_acc:0.986]
Epoch [54/120    avg_loss:0.016, val_acc:0.987]
Epoch [55/120    avg_loss:0.018, val_acc:0.980]
Epoch [56/120    avg_loss:0.016, val_acc:0.984]
Epoch [57/120    avg_loss:0.022, val_acc:0.984]
Epoch [58/120    avg_loss:0.013, val_acc:0.989]
Epoch [59/120    avg_loss:0.021, val_acc:0.989]
Epoch [60/120    avg_loss:0.021, val_acc:0.983]
Epoch [61/120    avg_loss:0.030, val_acc:0.986]
Epoch [62/120    avg_loss:0.020, val_acc:0.982]
Epoch [63/120    avg_loss:0.018, val_acc:0.991]
Epoch [64/120    avg_loss:0.022, val_acc:0.987]
Epoch [65/120    avg_loss:0.022, val_acc:0.988]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.022, val_acc:0.938]
Epoch [68/120    avg_loss:0.020, val_acc:0.987]
Epoch [69/120    avg_loss:0.024, val_acc:0.986]
Epoch [70/120    avg_loss:0.021, val_acc:0.988]
Epoch [71/120    avg_loss:0.032, val_acc:0.985]
Epoch [72/120    avg_loss:0.017, val_acc:0.989]
Epoch [73/120    avg_loss:0.029, val_acc:0.989]
Epoch [74/120    avg_loss:0.008, val_acc:0.987]
Epoch [75/120    avg_loss:0.011, val_acc:0.992]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.017, val_acc:0.974]
Epoch [78/120    avg_loss:0.014, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.990]
Epoch [80/120    avg_loss:0.006, val_acc:0.991]
Epoch [81/120    avg_loss:0.007, val_acc:0.991]
Epoch [82/120    avg_loss:0.009, val_acc:0.991]
Epoch [83/120    avg_loss:0.018, val_acc:0.975]
Epoch [84/120    avg_loss:0.069, val_acc:0.948]
Epoch [85/120    avg_loss:0.089, val_acc:0.974]
Epoch [86/120    avg_loss:0.028, val_acc:0.984]
Epoch [87/120    avg_loss:0.019, val_acc:0.981]
Epoch [88/120    avg_loss:0.019, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.012, val_acc:0.987]
Epoch [91/120    avg_loss:0.009, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.010, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.008, val_acc:0.989]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.010, val_acc:0.989]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.009, val_acc:0.989]
Epoch [113/120    avg_loss:0.008, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.010, val_acc:0.989]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6365     0     2     0     0    11     0    53     1]
 [    0     1 18043     0    46     0     0     0     0     0]
 [    0     0     0  2010     1     0     0     0    16     9]
 [    0    39    19     0  2894     0     6     2    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     4     0     0  4848     0    16     0]
 [    0     2     0     0     0     0     1  1283     0     4]
 [    0     3     0     2    89     0     0     0  3467    10]
 [    0     0     0     0    16    31     0     0     0   872]]

Accuracy:
99.02152170245584

F1 scores:
[       nan 0.99127862 0.99789835 0.99161322 0.96178132 0.98826202
 0.99507389 0.99650485 0.97182901 0.96088154]

Kappa:
0.9870381080811027
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ec0a3f860>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.697, val_acc:0.652]
Epoch [2/120    avg_loss:1.078, val_acc:0.743]
Epoch [3/120    avg_loss:0.742, val_acc:0.713]
Epoch [4/120    avg_loss:0.603, val_acc:0.722]
Epoch [5/120    avg_loss:0.490, val_acc:0.817]
Epoch [6/120    avg_loss:0.412, val_acc:0.815]
Epoch [7/120    avg_loss:0.360, val_acc:0.798]
Epoch [8/120    avg_loss:0.323, val_acc:0.867]
Epoch [9/120    avg_loss:0.297, val_acc:0.884]
Epoch [10/120    avg_loss:0.234, val_acc:0.904]
Epoch [11/120    avg_loss:0.215, val_acc:0.894]
Epoch [12/120    avg_loss:0.194, val_acc:0.891]
Epoch [13/120    avg_loss:0.170, val_acc:0.919]
Epoch [14/120    avg_loss:0.150, val_acc:0.863]
Epoch [15/120    avg_loss:0.138, val_acc:0.888]
Epoch [16/120    avg_loss:0.157, val_acc:0.891]
Epoch [17/120    avg_loss:0.127, val_acc:0.952]
Epoch [18/120    avg_loss:0.134, val_acc:0.953]
Epoch [19/120    avg_loss:0.214, val_acc:0.952]
Epoch [20/120    avg_loss:0.111, val_acc:0.932]
Epoch [21/120    avg_loss:0.081, val_acc:0.966]
Epoch [22/120    avg_loss:0.061, val_acc:0.963]
Epoch [23/120    avg_loss:0.066, val_acc:0.964]
Epoch [24/120    avg_loss:0.064, val_acc:0.978]
Epoch [25/120    avg_loss:0.060, val_acc:0.972]
Epoch [26/120    avg_loss:0.062, val_acc:0.950]
Epoch [27/120    avg_loss:0.043, val_acc:0.984]
Epoch [28/120    avg_loss:0.039, val_acc:0.980]
Epoch [29/120    avg_loss:0.050, val_acc:0.974]
Epoch [30/120    avg_loss:0.047, val_acc:0.968]
Epoch [31/120    avg_loss:0.059, val_acc:0.905]
Epoch [32/120    avg_loss:0.035, val_acc:0.984]
Epoch [33/120    avg_loss:0.028, val_acc:0.952]
Epoch [34/120    avg_loss:0.029, val_acc:0.952]
Epoch [35/120    avg_loss:0.079, val_acc:0.950]
Epoch [36/120    avg_loss:0.060, val_acc:0.950]
Epoch [37/120    avg_loss:0.024, val_acc:0.986]
Epoch [38/120    avg_loss:0.023, val_acc:0.982]
Epoch [39/120    avg_loss:0.027, val_acc:0.978]
Epoch [40/120    avg_loss:0.026, val_acc:0.976]
Epoch [41/120    avg_loss:0.018, val_acc:0.987]
Epoch [42/120    avg_loss:0.018, val_acc:0.973]
Epoch [43/120    avg_loss:0.040, val_acc:0.981]
Epoch [44/120    avg_loss:0.020, val_acc:0.980]
Epoch [45/120    avg_loss:0.077, val_acc:0.918]
Epoch [46/120    avg_loss:0.077, val_acc:0.972]
Epoch [47/120    avg_loss:0.088, val_acc:0.974]
Epoch [48/120    avg_loss:0.027, val_acc:0.975]
Epoch [49/120    avg_loss:0.088, val_acc:0.963]
Epoch [50/120    avg_loss:0.026, val_acc:0.986]
Epoch [51/120    avg_loss:0.019, val_acc:0.981]
Epoch [52/120    avg_loss:0.012, val_acc:0.985]
Epoch [53/120    avg_loss:0.015, val_acc:0.986]
Epoch [54/120    avg_loss:0.009, val_acc:0.983]
Epoch [55/120    avg_loss:0.010, val_acc:0.986]
Epoch [56/120    avg_loss:0.006, val_acc:0.986]
Epoch [57/120    avg_loss:0.009, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.987]
Epoch [59/120    avg_loss:0.007, val_acc:0.986]
Epoch [60/120    avg_loss:0.008, val_acc:0.987]
Epoch [61/120    avg_loss:0.007, val_acc:0.988]
Epoch [62/120    avg_loss:0.008, val_acc:0.987]
Epoch [63/120    avg_loss:0.008, val_acc:0.987]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.008, val_acc:0.987]
Epoch [66/120    avg_loss:0.007, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.987]
Epoch [68/120    avg_loss:0.012, val_acc:0.986]
Epoch [69/120    avg_loss:0.006, val_acc:0.988]
Epoch [70/120    avg_loss:0.009, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.006, val_acc:0.989]
Epoch [74/120    avg_loss:0.006, val_acc:0.989]
Epoch [75/120    avg_loss:0.006, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.989]
Epoch [77/120    avg_loss:0.006, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.987]
Epoch [80/120    avg_loss:0.009, val_acc:0.989]
Epoch [81/120    avg_loss:0.007, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.005, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.010, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6297     0     2     0     0    17     0    93    23]
 [    0     0 18060     0    27     0     3     0     0     0]
 [    0     0     0  2024     0     0     0     0     8     4]
 [    0    46    15     0  2872     0     9     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     8     0     0  4864     0     6     0]
 [    0     1     0     0     0     0     5  1278     0     6]
 [    0    30     0     0    59     0     0     0  3482     0]
 [    0     0     0     0    15    30     0     0     0   874]]

Accuracy:
98.94681030535271

F1 scores:
[       nan 0.98344526 0.9987557  0.99459459 0.96619008 0.98863636
 0.99509002 0.9953271  0.96856745 0.95728368]

Kappa:
0.9860485077402238
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f173ec979b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.685, val_acc:0.524]
Epoch [2/120    avg_loss:1.047, val_acc:0.666]
Epoch [3/120    avg_loss:0.762, val_acc:0.597]
Epoch [4/120    avg_loss:0.667, val_acc:0.750]
Epoch [5/120    avg_loss:0.565, val_acc:0.805]
Epoch [6/120    avg_loss:0.486, val_acc:0.784]
Epoch [7/120    avg_loss:0.401, val_acc:0.781]
Epoch [8/120    avg_loss:0.350, val_acc:0.813]
Epoch [9/120    avg_loss:0.352, val_acc:0.890]
Epoch [10/120    avg_loss:0.324, val_acc:0.840]
Epoch [11/120    avg_loss:0.221, val_acc:0.905]
Epoch [12/120    avg_loss:0.241, val_acc:0.935]
Epoch [13/120    avg_loss:0.227, val_acc:0.837]
Epoch [14/120    avg_loss:0.196, val_acc:0.928]
Epoch [15/120    avg_loss:0.253, val_acc:0.808]
Epoch [16/120    avg_loss:0.210, val_acc:0.911]
Epoch [17/120    avg_loss:0.172, val_acc:0.914]
Epoch [18/120    avg_loss:0.187, val_acc:0.935]
Epoch [19/120    avg_loss:0.131, val_acc:0.920]
Epoch [20/120    avg_loss:0.112, val_acc:0.949]
Epoch [21/120    avg_loss:0.120, val_acc:0.917]
Epoch [22/120    avg_loss:0.314, val_acc:0.892]
Epoch [23/120    avg_loss:0.127, val_acc:0.947]
Epoch [24/120    avg_loss:0.112, val_acc:0.941]
Epoch [25/120    avg_loss:0.115, val_acc:0.944]
Epoch [26/120    avg_loss:0.086, val_acc:0.955]
Epoch [27/120    avg_loss:0.092, val_acc:0.932]
Epoch [28/120    avg_loss:0.147, val_acc:0.951]
Epoch [29/120    avg_loss:0.081, val_acc:0.966]
Epoch [30/120    avg_loss:0.118, val_acc:0.947]
Epoch [31/120    avg_loss:0.088, val_acc:0.944]
Epoch [32/120    avg_loss:0.072, val_acc:0.954]
Epoch [33/120    avg_loss:0.037, val_acc:0.975]
Epoch [34/120    avg_loss:0.059, val_acc:0.969]
Epoch [35/120    avg_loss:0.039, val_acc:0.963]
Epoch [36/120    avg_loss:0.051, val_acc:0.948]
Epoch [37/120    avg_loss:0.043, val_acc:0.980]
Epoch [38/120    avg_loss:0.053, val_acc:0.983]
Epoch [39/120    avg_loss:0.033, val_acc:0.978]
Epoch [40/120    avg_loss:0.034, val_acc:0.981]
Epoch [41/120    avg_loss:0.023, val_acc:0.959]
Epoch [42/120    avg_loss:0.047, val_acc:0.968]
Epoch [43/120    avg_loss:0.031, val_acc:0.972]
Epoch [44/120    avg_loss:0.031, val_acc:0.974]
Epoch [45/120    avg_loss:0.027, val_acc:0.979]
Epoch [46/120    avg_loss:0.061, val_acc:0.948]
Epoch [47/120    avg_loss:0.077, val_acc:0.969]
Epoch [48/120    avg_loss:0.040, val_acc:0.978]
Epoch [49/120    avg_loss:0.041, val_acc:0.972]
Epoch [50/120    avg_loss:0.015, val_acc:0.978]
Epoch [51/120    avg_loss:0.036, val_acc:0.975]
Epoch [52/120    avg_loss:0.022, val_acc:0.982]
Epoch [53/120    avg_loss:0.023, val_acc:0.978]
Epoch [54/120    avg_loss:0.017, val_acc:0.984]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.015, val_acc:0.988]
Epoch [57/120    avg_loss:0.017, val_acc:0.986]
Epoch [58/120    avg_loss:0.014, val_acc:0.986]
Epoch [59/120    avg_loss:0.015, val_acc:0.986]
Epoch [60/120    avg_loss:0.015, val_acc:0.987]
Epoch [61/120    avg_loss:0.012, val_acc:0.987]
Epoch [62/120    avg_loss:0.015, val_acc:0.987]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.010, val_acc:0.988]
Epoch [66/120    avg_loss:0.012, val_acc:0.988]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.012, val_acc:0.989]
Epoch [69/120    avg_loss:0.011, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.990]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.013, val_acc:0.989]
Epoch [73/120    avg_loss:0.010, val_acc:0.989]
Epoch [74/120    avg_loss:0.011, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.991]
Epoch [76/120    avg_loss:0.012, val_acc:0.989]
Epoch [77/120    avg_loss:0.013, val_acc:0.987]
Epoch [78/120    avg_loss:0.011, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.989]
Epoch [82/120    avg_loss:0.016, val_acc:0.991]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.010, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.008, val_acc:0.991]
Epoch [89/120    avg_loss:0.010, val_acc:0.990]
Epoch [90/120    avg_loss:0.008, val_acc:0.991]
Epoch [91/120    avg_loss:0.009, val_acc:0.987]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.015, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.012, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.008, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.012, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.989]
Epoch [108/120    avg_loss:0.011, val_acc:0.989]
Epoch [109/120    avg_loss:0.010, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6286     0     0     0     0    14    11   121     0]
 [    0     0 17983     0    59     0    48     0     0     0]
 [    0     0     0  2011     3     0     0     0    21     1]
 [    0    36    22     0  2889     0     8     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4859     0    10     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0     3    61     0     0     0  3500     3]
 [    0     0     0     0    14    80     0     0     0   825]]

Accuracy:
98.68170534789

F1 scores:
[       nan 0.98542091 0.99617771 0.99308642 0.96332111 0.97026022
 0.99072281 0.99497876 0.96685083 0.94393593]

Kappa:
0.9825492480283782
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc203108940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.671, val_acc:0.597]
Epoch [2/120    avg_loss:1.103, val_acc:0.698]
Epoch [3/120    avg_loss:0.776, val_acc:0.692]
Epoch [4/120    avg_loss:0.567, val_acc:0.745]
Epoch [5/120    avg_loss:0.479, val_acc:0.776]
Epoch [6/120    avg_loss:0.417, val_acc:0.821]
Epoch [7/120    avg_loss:0.402, val_acc:0.815]
Epoch [8/120    avg_loss:0.310, val_acc:0.866]
Epoch [9/120    avg_loss:0.306, val_acc:0.844]
Epoch [10/120    avg_loss:0.265, val_acc:0.837]
Epoch [11/120    avg_loss:0.204, val_acc:0.901]
Epoch [12/120    avg_loss:0.235, val_acc:0.917]
Epoch [13/120    avg_loss:0.170, val_acc:0.935]
Epoch [14/120    avg_loss:0.199, val_acc:0.952]
Epoch [15/120    avg_loss:0.112, val_acc:0.949]
Epoch [16/120    avg_loss:0.164, val_acc:0.945]
Epoch [17/120    avg_loss:0.174, val_acc:0.908]
Epoch [18/120    avg_loss:0.141, val_acc:0.941]
Epoch [19/120    avg_loss:0.153, val_acc:0.869]
Epoch [20/120    avg_loss:0.130, val_acc:0.943]
Epoch [21/120    avg_loss:0.092, val_acc:0.952]
Epoch [22/120    avg_loss:0.093, val_acc:0.941]
Epoch [23/120    avg_loss:0.140, val_acc:0.930]
Epoch [24/120    avg_loss:0.072, val_acc:0.969]
Epoch [25/120    avg_loss:0.070, val_acc:0.966]
Epoch [26/120    avg_loss:0.066, val_acc:0.970]
Epoch [27/120    avg_loss:0.050, val_acc:0.969]
Epoch [28/120    avg_loss:0.052, val_acc:0.980]
Epoch [29/120    avg_loss:0.042, val_acc:0.972]
Epoch [30/120    avg_loss:0.281, val_acc:0.937]
Epoch [31/120    avg_loss:0.107, val_acc:0.894]
Epoch [32/120    avg_loss:0.081, val_acc:0.969]
Epoch [33/120    avg_loss:0.048, val_acc:0.964]
Epoch [34/120    avg_loss:0.093, val_acc:0.943]
Epoch [35/120    avg_loss:0.104, val_acc:0.950]
Epoch [36/120    avg_loss:0.105, val_acc:0.966]
Epoch [37/120    avg_loss:0.062, val_acc:0.960]
Epoch [38/120    avg_loss:0.061, val_acc:0.947]
Epoch [39/120    avg_loss:0.047, val_acc:0.979]
Epoch [40/120    avg_loss:0.037, val_acc:0.929]
Epoch [41/120    avg_loss:0.052, val_acc:0.974]
Epoch [42/120    avg_loss:0.028, val_acc:0.978]
Epoch [43/120    avg_loss:0.021, val_acc:0.978]
Epoch [44/120    avg_loss:0.021, val_acc:0.977]
Epoch [45/120    avg_loss:0.026, val_acc:0.980]
Epoch [46/120    avg_loss:0.023, val_acc:0.977]
Epoch [47/120    avg_loss:0.018, val_acc:0.979]
Epoch [48/120    avg_loss:0.020, val_acc:0.980]
Epoch [49/120    avg_loss:0.018, val_acc:0.978]
Epoch [50/120    avg_loss:0.018, val_acc:0.979]
Epoch [51/120    avg_loss:0.019, val_acc:0.980]
Epoch [52/120    avg_loss:0.017, val_acc:0.980]
Epoch [53/120    avg_loss:0.017, val_acc:0.979]
Epoch [54/120    avg_loss:0.016, val_acc:0.980]
Epoch [55/120    avg_loss:0.018, val_acc:0.981]
Epoch [56/120    avg_loss:0.018, val_acc:0.980]
Epoch [57/120    avg_loss:0.016, val_acc:0.979]
Epoch [58/120    avg_loss:0.019, val_acc:0.980]
Epoch [59/120    avg_loss:0.014, val_acc:0.980]
Epoch [60/120    avg_loss:0.015, val_acc:0.980]
Epoch [61/120    avg_loss:0.015, val_acc:0.979]
Epoch [62/120    avg_loss:0.018, val_acc:0.980]
Epoch [63/120    avg_loss:0.016, val_acc:0.980]
Epoch [64/120    avg_loss:0.013, val_acc:0.981]
Epoch [65/120    avg_loss:0.014, val_acc:0.983]
Epoch [66/120    avg_loss:0.012, val_acc:0.981]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.019, val_acc:0.983]
Epoch [69/120    avg_loss:0.014, val_acc:0.984]
Epoch [70/120    avg_loss:0.016, val_acc:0.984]
Epoch [71/120    avg_loss:0.016, val_acc:0.983]
Epoch [72/120    avg_loss:0.013, val_acc:0.985]
Epoch [73/120    avg_loss:0.014, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.982]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.017, val_acc:0.984]
Epoch [78/120    avg_loss:0.013, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.011, val_acc:0.982]
Epoch [81/120    avg_loss:0.017, val_acc:0.981]
Epoch [82/120    avg_loss:0.011, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.981]
Epoch [85/120    avg_loss:0.012, val_acc:0.982]
Epoch [86/120    avg_loss:0.017, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.980]
Epoch [88/120    avg_loss:0.014, val_acc:0.981]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.018, val_acc:0.980]
Epoch [91/120    avg_loss:0.013, val_acc:0.980]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.013, val_acc:0.980]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.016, val_acc:0.981]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.022, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.981]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.010, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.015, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     0     0     0     2     0    73     0]
 [    0     0 17830     0   113     0   147     0     0     0]
 [    0     5     0  1999     0     0     0     0    30     2]
 [    0    32    15     0  2910     0     0     0    15     0]
 [    0     0     0     0     0  1136   166     0     0     3]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     0  1283     0     7]
 [    0    10     0     0    81     0     0     0  3480     0]
 [    0     0     0     0    28    16     0     0     0   875]]

Accuracy:
98.20451642445714

F1 scores:
[       nan 0.99049548 0.99234729 0.99083024 0.95347313 0.92470492
 0.96872207 0.99727944 0.9708467  0.96899225]

Kappa:
0.9762602288617557
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff36fd1c898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.683, val_acc:0.598]
Epoch [2/120    avg_loss:1.022, val_acc:0.688]
Epoch [3/120    avg_loss:0.757, val_acc:0.714]
Epoch [4/120    avg_loss:0.592, val_acc:0.802]
Epoch [5/120    avg_loss:0.467, val_acc:0.814]
Epoch [6/120    avg_loss:0.395, val_acc:0.887]
Epoch [7/120    avg_loss:0.323, val_acc:0.889]
Epoch [8/120    avg_loss:0.370, val_acc:0.882]
Epoch [9/120    avg_loss:0.318, val_acc:0.859]
Epoch [10/120    avg_loss:0.239, val_acc:0.845]
Epoch [11/120    avg_loss:0.245, val_acc:0.913]
Epoch [12/120    avg_loss:0.214, val_acc:0.936]
Epoch [13/120    avg_loss:0.173, val_acc:0.834]
Epoch [14/120    avg_loss:0.160, val_acc:0.934]
Epoch [15/120    avg_loss:0.135, val_acc:0.904]
Epoch [16/120    avg_loss:0.163, val_acc:0.955]
Epoch [17/120    avg_loss:0.133, val_acc:0.948]
Epoch [18/120    avg_loss:0.095, val_acc:0.938]
Epoch [19/120    avg_loss:0.133, val_acc:0.947]
Epoch [20/120    avg_loss:0.100, val_acc:0.944]
Epoch [21/120    avg_loss:0.079, val_acc:0.946]
Epoch [22/120    avg_loss:0.091, val_acc:0.954]
Epoch [23/120    avg_loss:0.075, val_acc:0.964]
Epoch [24/120    avg_loss:0.061, val_acc:0.966]
Epoch [25/120    avg_loss:0.069, val_acc:0.948]
Epoch [26/120    avg_loss:0.092, val_acc:0.963]
Epoch [27/120    avg_loss:0.099, val_acc:0.875]
Epoch [28/120    avg_loss:0.094, val_acc:0.962]
Epoch [29/120    avg_loss:0.037, val_acc:0.975]
Epoch [30/120    avg_loss:0.064, val_acc:0.968]
Epoch [31/120    avg_loss:0.127, val_acc:0.937]
Epoch [32/120    avg_loss:0.057, val_acc:0.955]
Epoch [33/120    avg_loss:0.049, val_acc:0.970]
Epoch [34/120    avg_loss:0.033, val_acc:0.973]
Epoch [35/120    avg_loss:0.031, val_acc:0.976]
Epoch [36/120    avg_loss:0.034, val_acc:0.982]
Epoch [37/120    avg_loss:0.065, val_acc:0.969]
Epoch [38/120    avg_loss:0.037, val_acc:0.975]
Epoch [39/120    avg_loss:0.050, val_acc:0.964]
Epoch [40/120    avg_loss:0.062, val_acc:0.944]
Epoch [41/120    avg_loss:0.070, val_acc:0.963]
Epoch [42/120    avg_loss:0.062, val_acc:0.967]
Epoch [43/120    avg_loss:0.021, val_acc:0.978]
Epoch [44/120    avg_loss:0.116, val_acc:0.952]
Epoch [45/120    avg_loss:0.038, val_acc:0.971]
Epoch [46/120    avg_loss:0.025, val_acc:0.974]
Epoch [47/120    avg_loss:0.028, val_acc:0.974]
Epoch [48/120    avg_loss:0.139, val_acc:0.962]
Epoch [49/120    avg_loss:0.046, val_acc:0.978]
Epoch [50/120    avg_loss:0.026, val_acc:0.983]
Epoch [51/120    avg_loss:0.022, val_acc:0.982]
Epoch [52/120    avg_loss:0.017, val_acc:0.984]
Epoch [53/120    avg_loss:0.018, val_acc:0.983]
Epoch [54/120    avg_loss:0.017, val_acc:0.983]
Epoch [55/120    avg_loss:0.019, val_acc:0.981]
Epoch [56/120    avg_loss:0.016, val_acc:0.985]
Epoch [57/120    avg_loss:0.016, val_acc:0.983]
Epoch [58/120    avg_loss:0.017, val_acc:0.984]
Epoch [59/120    avg_loss:0.012, val_acc:0.986]
Epoch [60/120    avg_loss:0.013, val_acc:0.986]
Epoch [61/120    avg_loss:0.012, val_acc:0.985]
Epoch [62/120    avg_loss:0.015, val_acc:0.981]
Epoch [63/120    avg_loss:0.012, val_acc:0.986]
Epoch [64/120    avg_loss:0.014, val_acc:0.983]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.014, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.986]
Epoch [68/120    avg_loss:0.012, val_acc:0.986]
Epoch [69/120    avg_loss:0.012, val_acc:0.986]
Epoch [70/120    avg_loss:0.013, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.013, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.011, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6285     0     0    16     0    30    12    76    13]
 [    0     0 18060     0    29     0     1     0     0     0]
 [    0     0     0  1999     2     0     0     0    27     8]
 [    0    30    10     0  2902     0     8     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     0     0    20    69     0     0     0  3468    14]
 [    0     0     0     0    16    87     0     0     0   816]]

Accuracy:
98.81425782662136

F1 scores:
[       nan 0.98611438 0.99889381 0.98594328 0.96636697 0.96774194
 0.99601838 0.99459459 0.9681742  0.92099323]

Kappa:
0.9842958157679225
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3bd8958860>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.752, val_acc:0.509]
Epoch [2/120    avg_loss:1.015, val_acc:0.673]
Epoch [3/120    avg_loss:0.706, val_acc:0.715]
Epoch [4/120    avg_loss:0.548, val_acc:0.779]
Epoch [5/120    avg_loss:0.529, val_acc:0.795]
Epoch [6/120    avg_loss:0.398, val_acc:0.867]
Epoch [7/120    avg_loss:0.375, val_acc:0.863]
Epoch [8/120    avg_loss:0.311, val_acc:0.878]
Epoch [9/120    avg_loss:0.234, val_acc:0.802]
Epoch [10/120    avg_loss:0.307, val_acc:0.888]
Epoch [11/120    avg_loss:0.190, val_acc:0.845]
Epoch [12/120    avg_loss:0.259, val_acc:0.875]
Epoch [13/120    avg_loss:0.203, val_acc:0.922]
Epoch [14/120    avg_loss:0.172, val_acc:0.899]
Epoch [15/120    avg_loss:0.141, val_acc:0.896]
Epoch [16/120    avg_loss:0.171, val_acc:0.917]
Epoch [17/120    avg_loss:0.112, val_acc:0.881]
Epoch [18/120    avg_loss:0.135, val_acc:0.926]
Epoch [19/120    avg_loss:0.163, val_acc:0.913]
Epoch [20/120    avg_loss:0.092, val_acc:0.944]
Epoch [21/120    avg_loss:0.120, val_acc:0.893]
Epoch [22/120    avg_loss:0.141, val_acc:0.940]
Epoch [23/120    avg_loss:0.074, val_acc:0.941]
Epoch [24/120    avg_loss:0.103, val_acc:0.955]
Epoch [25/120    avg_loss:0.085, val_acc:0.918]
Epoch [26/120    avg_loss:0.072, val_acc:0.970]
Epoch [27/120    avg_loss:0.064, val_acc:0.939]
Epoch [28/120    avg_loss:0.060, val_acc:0.958]
Epoch [29/120    avg_loss:0.058, val_acc:0.952]
Epoch [30/120    avg_loss:0.037, val_acc:0.968]
Epoch [31/120    avg_loss:0.040, val_acc:0.969]
Epoch [32/120    avg_loss:0.057, val_acc:0.962]
Epoch [33/120    avg_loss:0.054, val_acc:0.958]
Epoch [34/120    avg_loss:0.042, val_acc:0.951]
Epoch [35/120    avg_loss:0.063, val_acc:0.971]
Epoch [36/120    avg_loss:0.038, val_acc:0.900]
Epoch [37/120    avg_loss:0.026, val_acc:0.965]
Epoch [38/120    avg_loss:0.038, val_acc:0.970]
Epoch [39/120    avg_loss:0.025, val_acc:0.976]
Epoch [40/120    avg_loss:0.021, val_acc:0.975]
Epoch [41/120    avg_loss:0.030, val_acc:0.973]
Epoch [42/120    avg_loss:0.043, val_acc:0.970]
Epoch [43/120    avg_loss:0.022, val_acc:0.972]
Epoch [44/120    avg_loss:0.021, val_acc:0.971]
Epoch [45/120    avg_loss:0.038, val_acc:0.959]
Epoch [46/120    avg_loss:0.149, val_acc:0.870]
Epoch [47/120    avg_loss:0.059, val_acc:0.967]
Epoch [48/120    avg_loss:0.086, val_acc:0.955]
Epoch [49/120    avg_loss:0.057, val_acc:0.964]
Epoch [50/120    avg_loss:0.030, val_acc:0.968]
Epoch [51/120    avg_loss:0.041, val_acc:0.970]
Epoch [52/120    avg_loss:0.031, val_acc:0.974]
Epoch [53/120    avg_loss:0.018, val_acc:0.978]
Epoch [54/120    avg_loss:0.016, val_acc:0.981]
Epoch [55/120    avg_loss:0.013, val_acc:0.980]
Epoch [56/120    avg_loss:0.017, val_acc:0.981]
Epoch [57/120    avg_loss:0.014, val_acc:0.981]
Epoch [58/120    avg_loss:0.010, val_acc:0.979]
Epoch [59/120    avg_loss:0.010, val_acc:0.981]
Epoch [60/120    avg_loss:0.010, val_acc:0.981]
Epoch [61/120    avg_loss:0.012, val_acc:0.981]
Epoch [62/120    avg_loss:0.014, val_acc:0.981]
Epoch [63/120    avg_loss:0.013, val_acc:0.981]
Epoch [64/120    avg_loss:0.011, val_acc:0.981]
Epoch [65/120    avg_loss:0.011, val_acc:0.982]
Epoch [66/120    avg_loss:0.018, val_acc:0.982]
Epoch [67/120    avg_loss:0.010, val_acc:0.982]
Epoch [68/120    avg_loss:0.010, val_acc:0.982]
Epoch [69/120    avg_loss:0.011, val_acc:0.982]
Epoch [70/120    avg_loss:0.010, val_acc:0.983]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.009, val_acc:0.982]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.016, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.982]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6394     0     2     1     0     9    17     9     0]
 [    0     0 18057     0    33     0     0     0     0     0]
 [    0     1     0  2018     3     0     0     0     9     5]
 [    0    13    18     0  2920     0     1     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2    11     0     0  4862     0     1     2]
 [    0     0     0     0     0     0     2  1280     0     8]
 [    0     2     0     0    83     0     0     0  3458    28]
 [    0     0     0     0    15    29     0     0     0   875]]

Accuracy:
99.21914539801895

F1 scores:
[       nan 0.99579505 0.99853458 0.99237767 0.96897296 0.98901099
 0.99712879 0.9895632  0.97849462 0.95264017]

Kappa:
0.9896562140030475
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3cd129c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.621, val_acc:0.482]
Epoch [2/120    avg_loss:1.006, val_acc:0.657]
Epoch [3/120    avg_loss:0.711, val_acc:0.720]
Epoch [4/120    avg_loss:0.617, val_acc:0.770]
Epoch [5/120    avg_loss:0.514, val_acc:0.737]
Epoch [6/120    avg_loss:0.491, val_acc:0.862]
Epoch [7/120    avg_loss:0.397, val_acc:0.878]
Epoch [8/120    avg_loss:0.313, val_acc:0.865]
Epoch [9/120    avg_loss:0.289, val_acc:0.830]
Epoch [10/120    avg_loss:0.262, val_acc:0.853]
Epoch [11/120    avg_loss:0.227, val_acc:0.909]
Epoch [12/120    avg_loss:0.204, val_acc:0.916]
Epoch [13/120    avg_loss:0.188, val_acc:0.886]
Epoch [14/120    avg_loss:0.169, val_acc:0.926]
Epoch [15/120    avg_loss:0.162, val_acc:0.878]
Epoch [16/120    avg_loss:0.126, val_acc:0.920]
Epoch [17/120    avg_loss:0.098, val_acc:0.946]
Epoch [18/120    avg_loss:0.125, val_acc:0.927]
Epoch [19/120    avg_loss:0.114, val_acc:0.940]
Epoch [20/120    avg_loss:0.084, val_acc:0.957]
Epoch [21/120    avg_loss:0.105, val_acc:0.919]
Epoch [22/120    avg_loss:0.231, val_acc:0.869]
Epoch [23/120    avg_loss:0.134, val_acc:0.951]
Epoch [24/120    avg_loss:0.112, val_acc:0.953]
Epoch [25/120    avg_loss:0.095, val_acc:0.899]
Epoch [26/120    avg_loss:0.084, val_acc:0.952]
Epoch [27/120    avg_loss:0.082, val_acc:0.952]
Epoch [28/120    avg_loss:0.064, val_acc:0.967]
Epoch [29/120    avg_loss:0.055, val_acc:0.966]
Epoch [30/120    avg_loss:0.093, val_acc:0.891]
Epoch [31/120    avg_loss:0.079, val_acc:0.962]
Epoch [32/120    avg_loss:0.044, val_acc:0.970]
Epoch [33/120    avg_loss:0.034, val_acc:0.910]
Epoch [34/120    avg_loss:0.057, val_acc:0.890]
Epoch [35/120    avg_loss:0.099, val_acc:0.965]
Epoch [36/120    avg_loss:0.050, val_acc:0.955]
Epoch [37/120    avg_loss:0.033, val_acc:0.975]
Epoch [38/120    avg_loss:0.037, val_acc:0.976]
Epoch [39/120    avg_loss:0.043, val_acc:0.959]
Epoch [40/120    avg_loss:0.036, val_acc:0.963]
Epoch [41/120    avg_loss:0.042, val_acc:0.972]
Epoch [42/120    avg_loss:0.040, val_acc:0.960]
Epoch [43/120    avg_loss:0.043, val_acc:0.973]
Epoch [44/120    avg_loss:0.026, val_acc:0.973]
Epoch [45/120    avg_loss:0.040, val_acc:0.966]
Epoch [46/120    avg_loss:0.044, val_acc:0.967]
Epoch [47/120    avg_loss:0.052, val_acc:0.921]
Epoch [48/120    avg_loss:0.086, val_acc:0.973]
Epoch [49/120    avg_loss:0.027, val_acc:0.978]
Epoch [50/120    avg_loss:0.046, val_acc:0.976]
Epoch [51/120    avg_loss:0.027, val_acc:0.971]
Epoch [52/120    avg_loss:0.027, val_acc:0.975]
Epoch [53/120    avg_loss:0.026, val_acc:0.980]
Epoch [54/120    avg_loss:0.015, val_acc:0.983]
Epoch [55/120    avg_loss:0.012, val_acc:0.983]
Epoch [56/120    avg_loss:0.013, val_acc:0.983]
Epoch [57/120    avg_loss:0.025, val_acc:0.978]
Epoch [58/120    avg_loss:0.040, val_acc:0.955]
Epoch [59/120    avg_loss:0.046, val_acc:0.978]
Epoch [60/120    avg_loss:0.026, val_acc:0.957]
Epoch [61/120    avg_loss:0.016, val_acc:0.977]
Epoch [62/120    avg_loss:0.013, val_acc:0.979]
Epoch [63/120    avg_loss:0.014, val_acc:0.975]
Epoch [64/120    avg_loss:0.023, val_acc:0.978]
Epoch [65/120    avg_loss:0.013, val_acc:0.969]
Epoch [66/120    avg_loss:0.021, val_acc:0.976]
Epoch [67/120    avg_loss:0.016, val_acc:0.984]
Epoch [68/120    avg_loss:0.013, val_acc:0.982]
Epoch [69/120    avg_loss:0.017, val_acc:0.975]
Epoch [70/120    avg_loss:0.020, val_acc:0.976]
Epoch [71/120    avg_loss:0.017, val_acc:0.970]
Epoch [72/120    avg_loss:0.029, val_acc:0.981]
Epoch [73/120    avg_loss:0.009, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.973]
Epoch [75/120    avg_loss:0.009, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.979]
Epoch [77/120    avg_loss:0.006, val_acc:0.974]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.004, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.004, val_acc:0.982]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.004, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.013, val_acc:0.976]
Epoch [99/120    avg_loss:0.027, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.973]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.003, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.979]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6353     0     0     1     0    16     0    62     0]
 [    0     0 18077     0    11     0     0     0     1     1]
 [    0     3     0  2017     3     0     0     0     9     4]
 [    0    27    18     0  2906     0     5     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     0     0     0  4854     0    15     0]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     0     0     5    59     0     0     0  3507     0]
 [    0     0     0     0    14    48     0     0     0   857]]

Accuracy:
99.19504494734052

F1 scores:
[       nan 0.99149434 0.99889484 0.99408576 0.97418706 0.98194131
 0.99518196 0.99727944 0.97674419 0.95968645]

Kappa:
0.989332913681765
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f523e6e4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.722, val_acc:0.538]
Epoch [2/120    avg_loss:1.108, val_acc:0.678]
Epoch [3/120    avg_loss:0.751, val_acc:0.658]
Epoch [4/120    avg_loss:0.584, val_acc:0.737]
Epoch [5/120    avg_loss:0.557, val_acc:0.740]
Epoch [6/120    avg_loss:0.457, val_acc:0.782]
Epoch [7/120    avg_loss:0.347, val_acc:0.819]
Epoch [8/120    avg_loss:0.332, val_acc:0.818]
Epoch [9/120    avg_loss:0.350, val_acc:0.835]
Epoch [10/120    avg_loss:0.278, val_acc:0.815]
Epoch [11/120    avg_loss:0.248, val_acc:0.903]
Epoch [12/120    avg_loss:0.207, val_acc:0.909]
Epoch [13/120    avg_loss:0.210, val_acc:0.873]
Epoch [14/120    avg_loss:0.189, val_acc:0.859]
Epoch [15/120    avg_loss:0.202, val_acc:0.856]
Epoch [16/120    avg_loss:0.171, val_acc:0.860]
Epoch [17/120    avg_loss:0.211, val_acc:0.911]
Epoch [18/120    avg_loss:0.191, val_acc:0.896]
Epoch [19/120    avg_loss:0.184, val_acc:0.883]
Epoch [20/120    avg_loss:0.153, val_acc:0.927]
Epoch [21/120    avg_loss:0.121, val_acc:0.921]
Epoch [22/120    avg_loss:0.127, val_acc:0.847]
Epoch [23/120    avg_loss:0.125, val_acc:0.944]
Epoch [24/120    avg_loss:0.113, val_acc:0.932]
Epoch [25/120    avg_loss:0.105, val_acc:0.943]
Epoch [26/120    avg_loss:0.079, val_acc:0.947]
Epoch [27/120    avg_loss:0.093, val_acc:0.933]
Epoch [28/120    avg_loss:0.110, val_acc:0.937]
Epoch [29/120    avg_loss:0.109, val_acc:0.954]
Epoch [30/120    avg_loss:0.082, val_acc:0.941]
Epoch [31/120    avg_loss:0.052, val_acc:0.946]
Epoch [32/120    avg_loss:0.086, val_acc:0.956]
Epoch [33/120    avg_loss:0.084, val_acc:0.954]
Epoch [34/120    avg_loss:0.043, val_acc:0.946]
Epoch [35/120    avg_loss:0.049, val_acc:0.962]
Epoch [36/120    avg_loss:0.064, val_acc:0.943]
Epoch [37/120    avg_loss:0.043, val_acc:0.966]
Epoch [38/120    avg_loss:0.054, val_acc:0.968]
Epoch [39/120    avg_loss:0.042, val_acc:0.954]
Epoch [40/120    avg_loss:0.036, val_acc:0.963]
Epoch [41/120    avg_loss:0.058, val_acc:0.958]
Epoch [42/120    avg_loss:0.069, val_acc:0.954]
Epoch [43/120    avg_loss:0.055, val_acc:0.940]
Epoch [44/120    avg_loss:0.101, val_acc:0.948]
Epoch [45/120    avg_loss:0.098, val_acc:0.965]
Epoch [46/120    avg_loss:0.033, val_acc:0.917]
Epoch [47/120    avg_loss:0.036, val_acc:0.964]
Epoch [48/120    avg_loss:0.046, val_acc:0.971]
Epoch [49/120    avg_loss:0.022, val_acc:0.970]
Epoch [50/120    avg_loss:0.051, val_acc:0.954]
Epoch [51/120    avg_loss:0.026, val_acc:0.962]
Epoch [52/120    avg_loss:0.052, val_acc:0.964]
Epoch [53/120    avg_loss:0.023, val_acc:0.969]
Epoch [54/120    avg_loss:0.024, val_acc:0.957]
Epoch [55/120    avg_loss:0.026, val_acc:0.974]
Epoch [56/120    avg_loss:0.032, val_acc:0.967]
Epoch [57/120    avg_loss:0.028, val_acc:0.956]
Epoch [58/120    avg_loss:0.022, val_acc:0.979]
Epoch [59/120    avg_loss:0.017, val_acc:0.971]
Epoch [60/120    avg_loss:0.011, val_acc:0.972]
Epoch [61/120    avg_loss:0.021, val_acc:0.972]
Epoch [62/120    avg_loss:0.016, val_acc:0.971]
Epoch [63/120    avg_loss:0.019, val_acc:0.967]
Epoch [64/120    avg_loss:0.021, val_acc:0.969]
Epoch [65/120    avg_loss:0.028, val_acc:0.970]
Epoch [66/120    avg_loss:0.016, val_acc:0.978]
Epoch [67/120    avg_loss:0.008, val_acc:0.981]
Epoch [68/120    avg_loss:0.019, val_acc:0.978]
Epoch [69/120    avg_loss:0.038, val_acc:0.967]
Epoch [70/120    avg_loss:0.017, val_acc:0.977]
Epoch [71/120    avg_loss:0.011, val_acc:0.981]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.005, val_acc:0.981]
Epoch [76/120    avg_loss:0.008, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.970]
Epoch [78/120    avg_loss:0.081, val_acc:0.947]
Epoch [79/120    avg_loss:0.045, val_acc:0.955]
Epoch [80/120    avg_loss:0.026, val_acc:0.981]
Epoch [81/120    avg_loss:0.014, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.978]
Epoch [83/120    avg_loss:0.010, val_acc:0.979]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.027, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.036, val_acc:0.974]
Epoch [89/120    avg_loss:0.014, val_acc:0.980]
Epoch [90/120    avg_loss:0.010, val_acc:0.979]
Epoch [91/120    avg_loss:0.025, val_acc:0.974]
Epoch [92/120    avg_loss:0.023, val_acc:0.954]
Epoch [93/120    avg_loss:0.013, val_acc:0.968]
Epoch [94/120    avg_loss:0.011, val_acc:0.980]
Epoch [95/120    avg_loss:0.020, val_acc:0.981]
Epoch [96/120    avg_loss:0.006, val_acc:0.981]
Epoch [97/120    avg_loss:0.006, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.003, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.003, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.004, val_acc:0.982]
Epoch [116/120    avg_loss:0.003, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.982]
Epoch [118/120    avg_loss:0.004, val_acc:0.982]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     0     0     0     0    36     0    47     0]
 [    0     0 17850     0    75     0   165     0     0     0]
 [    0     3     0  1965     0     0     0     0    64     4]
 [    0    20     8     0  2922     0     1     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4876     0     0     0]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     2     0    10    57     0     0     0  3495     7]
 [    0     1     0     0    23    35     0     0     0   860]]

Accuracy:
98.59012363531198

F1 scores:
[       nan 0.99148903 0.99310115 0.97931722 0.9661101  0.98676749
 0.97931312 0.9984472  0.97110308 0.95982143]

Kappa:
0.9813640117911897
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:148
Validation dataloader:148
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83d0033908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.712, val_acc:0.432]
Epoch [2/120    avg_loss:1.084, val_acc:0.686]
Epoch [3/120    avg_loss:0.767, val_acc:0.764]
Epoch [4/120    avg_loss:0.575, val_acc:0.751]
Epoch [5/120    avg_loss:0.461, val_acc:0.664]
Epoch [6/120    avg_loss:0.432, val_acc:0.812]
Epoch [7/120    avg_loss:0.376, val_acc:0.856]
Epoch [8/120    avg_loss:0.355, val_acc:0.838]
Epoch [9/120    avg_loss:0.291, val_acc:0.853]
Epoch [10/120    avg_loss:0.239, val_acc:0.875]
Epoch [11/120    avg_loss:0.255, val_acc:0.861]
Epoch [12/120    avg_loss:0.226, val_acc:0.917]
Epoch [13/120    avg_loss:0.254, val_acc:0.892]
Epoch [14/120    avg_loss:0.223, val_acc:0.934]
Epoch [15/120    avg_loss:0.169, val_acc:0.915]
Epoch [16/120    avg_loss:0.208, val_acc:0.938]
Epoch [17/120    avg_loss:0.161, val_acc:0.943]
Epoch [18/120    avg_loss:0.137, val_acc:0.950]
Epoch [19/120    avg_loss:0.112, val_acc:0.950]
Epoch [20/120    avg_loss:0.126, val_acc:0.912]
Epoch [21/120    avg_loss:0.095, val_acc:0.962]
Epoch [22/120    avg_loss:0.075, val_acc:0.968]
Epoch [23/120    avg_loss:0.080, val_acc:0.937]
Epoch [24/120    avg_loss:0.153, val_acc:0.946]
Epoch [25/120    avg_loss:0.076, val_acc:0.942]
Epoch [26/120    avg_loss:0.054, val_acc:0.967]
Epoch [27/120    avg_loss:0.049, val_acc:0.969]
Epoch [28/120    avg_loss:0.078, val_acc:0.841]
Epoch [29/120    avg_loss:0.225, val_acc:0.954]
Epoch [30/120    avg_loss:0.093, val_acc:0.953]
Epoch [31/120    avg_loss:0.072, val_acc:0.955]
Epoch [32/120    avg_loss:0.068, val_acc:0.960]
Epoch [33/120    avg_loss:0.041, val_acc:0.981]
Epoch [34/120    avg_loss:0.036, val_acc:0.978]
Epoch [35/120    avg_loss:0.032, val_acc:0.958]
Epoch [36/120    avg_loss:0.039, val_acc:0.970]
Epoch [37/120    avg_loss:0.036, val_acc:0.983]
Epoch [38/120    avg_loss:0.046, val_acc:0.973]
Epoch [39/120    avg_loss:0.033, val_acc:0.969]
Epoch [40/120    avg_loss:0.027, val_acc:0.954]
Epoch [41/120    avg_loss:0.073, val_acc:0.902]
Epoch [42/120    avg_loss:0.057, val_acc:0.973]
Epoch [43/120    avg_loss:0.029, val_acc:0.962]
Epoch [44/120    avg_loss:0.024, val_acc:0.982]
Epoch [45/120    avg_loss:0.026, val_acc:0.985]
Epoch [46/120    avg_loss:0.018, val_acc:0.987]
Epoch [47/120    avg_loss:0.024, val_acc:0.989]
Epoch [48/120    avg_loss:0.024, val_acc:0.969]
Epoch [49/120    avg_loss:0.080, val_acc:0.984]
Epoch [50/120    avg_loss:0.030, val_acc:0.981]
Epoch [51/120    avg_loss:0.012, val_acc:0.982]
Epoch [52/120    avg_loss:0.020, val_acc:0.986]
Epoch [53/120    avg_loss:0.012, val_acc:0.987]
Epoch [54/120    avg_loss:0.009, val_acc:0.987]
Epoch [55/120    avg_loss:0.009, val_acc:0.988]
Epoch [56/120    avg_loss:0.010, val_acc:0.989]
Epoch [57/120    avg_loss:0.007, val_acc:0.992]
Epoch [58/120    avg_loss:0.008, val_acc:0.991]
Epoch [59/120    avg_loss:0.012, val_acc:0.988]
Epoch [60/120    avg_loss:0.016, val_acc:0.993]
Epoch [61/120    avg_loss:0.010, val_acc:0.983]
Epoch [62/120    avg_loss:0.010, val_acc:0.992]
Epoch [63/120    avg_loss:0.012, val_acc:0.985]
Epoch [64/120    avg_loss:0.013, val_acc:0.990]
Epoch [65/120    avg_loss:0.008, val_acc:0.990]
Epoch [66/120    avg_loss:0.009, val_acc:0.984]
Epoch [67/120    avg_loss:0.017, val_acc:0.991]
Epoch [68/120    avg_loss:0.019, val_acc:0.980]
Epoch [69/120    avg_loss:0.013, val_acc:0.977]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.015, val_acc:0.965]
Epoch [72/120    avg_loss:0.013, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.990]
Epoch [75/120    avg_loss:0.005, val_acc:0.992]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.006, val_acc:0.992]
Epoch [78/120    avg_loss:0.005, val_acc:0.991]
Epoch [79/120    avg_loss:0.004, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.992]
Epoch [81/120    avg_loss:0.005, val_acc:0.992]
Epoch [82/120    avg_loss:0.006, val_acc:0.992]
Epoch [83/120    avg_loss:0.004, val_acc:0.992]
Epoch [84/120    avg_loss:0.005, val_acc:0.992]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.004, val_acc:0.992]
Epoch [87/120    avg_loss:0.005, val_acc:0.992]
Epoch [88/120    avg_loss:0.008, val_acc:0.992]
Epoch [89/120    avg_loss:0.004, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.005, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.004, val_acc:0.992]
Epoch [97/120    avg_loss:0.004, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.004, val_acc:0.992]
Epoch [100/120    avg_loss:0.006, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.006, val_acc:0.992]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.004, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.004, val_acc:0.992]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     0     0    18    13    17     0]
 [    0     0 18021     0    28     0    41     0     0     0]
 [    0     6     0  2005     2     0     0     0    17     6]
 [    0    37    12     1  2902     0     0     1    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4874     0     0     0]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0    14     0     0    49     0     0     0  3508     0]
 [    0     0     1     0    25    36     0     0     0   857]]

Accuracy:
99.15166413611935

F1 scores:
[       nan 0.99184339 0.99773004 0.99110232 0.97089328 0.98639456
 0.99337613 0.99266126 0.98373528 0.96022409]

Kappa:
0.9887646996788088
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29dc5bb828>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.809, val_acc:0.564]
Epoch [2/120    avg_loss:1.113, val_acc:0.674]
Epoch [3/120    avg_loss:0.782, val_acc:0.697]
Epoch [4/120    avg_loss:0.628, val_acc:0.773]
Epoch [5/120    avg_loss:0.545, val_acc:0.822]
Epoch [6/120    avg_loss:0.416, val_acc:0.847]
Epoch [7/120    avg_loss:0.393, val_acc:0.866]
Epoch [8/120    avg_loss:0.331, val_acc:0.804]
Epoch [9/120    avg_loss:0.410, val_acc:0.882]
Epoch [10/120    avg_loss:0.271, val_acc:0.881]
Epoch [11/120    avg_loss:0.272, val_acc:0.866]
Epoch [12/120    avg_loss:0.267, val_acc:0.909]
Epoch [13/120    avg_loss:0.222, val_acc:0.858]
Epoch [14/120    avg_loss:0.178, val_acc:0.872]
Epoch [15/120    avg_loss:0.162, val_acc:0.923]
Epoch [16/120    avg_loss:0.183, val_acc:0.934]
Epoch [17/120    avg_loss:0.189, val_acc:0.943]
Epoch [18/120    avg_loss:0.192, val_acc:0.938]
Epoch [19/120    avg_loss:0.138, val_acc:0.938]
Epoch [20/120    avg_loss:0.185, val_acc:0.925]
Epoch [21/120    avg_loss:0.144, val_acc:0.948]
Epoch [22/120    avg_loss:0.162, val_acc:0.934]
Epoch [23/120    avg_loss:0.099, val_acc:0.962]
Epoch [24/120    avg_loss:0.122, val_acc:0.936]
Epoch [25/120    avg_loss:0.131, val_acc:0.923]
Epoch [26/120    avg_loss:0.090, val_acc:0.940]
Epoch [27/120    avg_loss:0.082, val_acc:0.946]
Epoch [28/120    avg_loss:0.292, val_acc:0.820]
Epoch [29/120    avg_loss:0.112, val_acc:0.949]
Epoch [30/120    avg_loss:0.073, val_acc:0.957]
Epoch [31/120    avg_loss:0.080, val_acc:0.945]
Epoch [32/120    avg_loss:0.094, val_acc:0.964]
Epoch [33/120    avg_loss:0.094, val_acc:0.959]
Epoch [34/120    avg_loss:0.080, val_acc:0.895]
Epoch [35/120    avg_loss:0.075, val_acc:0.961]
Epoch [36/120    avg_loss:0.054, val_acc:0.972]
Epoch [37/120    avg_loss:0.052, val_acc:0.973]
Epoch [38/120    avg_loss:0.045, val_acc:0.965]
Epoch [39/120    avg_loss:0.115, val_acc:0.968]
Epoch [40/120    avg_loss:0.090, val_acc:0.916]
Epoch [41/120    avg_loss:0.066, val_acc:0.949]
Epoch [42/120    avg_loss:0.056, val_acc:0.942]
Epoch [43/120    avg_loss:0.079, val_acc:0.964]
Epoch [44/120    avg_loss:0.108, val_acc:0.972]
Epoch [45/120    avg_loss:0.050, val_acc:0.978]
Epoch [46/120    avg_loss:0.048, val_acc:0.958]
Epoch [47/120    avg_loss:0.071, val_acc:0.971]
Epoch [48/120    avg_loss:0.039, val_acc:0.977]
Epoch [49/120    avg_loss:0.025, val_acc:0.977]
Epoch [50/120    avg_loss:0.027, val_acc:0.975]
Epoch [51/120    avg_loss:0.022, val_acc:0.979]
Epoch [52/120    avg_loss:0.030, val_acc:0.982]
Epoch [53/120    avg_loss:0.017, val_acc:0.983]
Epoch [54/120    avg_loss:0.011, val_acc:0.980]
Epoch [55/120    avg_loss:0.037, val_acc:0.979]
Epoch [56/120    avg_loss:0.107, val_acc:0.973]
Epoch [57/120    avg_loss:0.031, val_acc:0.978]
Epoch [58/120    avg_loss:0.022, val_acc:0.981]
Epoch [59/120    avg_loss:0.063, val_acc:0.972]
Epoch [60/120    avg_loss:0.024, val_acc:0.975]
Epoch [61/120    avg_loss:0.084, val_acc:0.969]
Epoch [62/120    avg_loss:0.030, val_acc:0.981]
Epoch [63/120    avg_loss:0.026, val_acc:0.969]
Epoch [64/120    avg_loss:0.023, val_acc:0.975]
Epoch [65/120    avg_loss:0.015, val_acc:0.985]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.046, val_acc:0.978]
Epoch [69/120    avg_loss:0.025, val_acc:0.972]
Epoch [70/120    avg_loss:0.016, val_acc:0.984]
Epoch [71/120    avg_loss:0.023, val_acc:0.974]
Epoch [72/120    avg_loss:0.018, val_acc:0.966]
Epoch [73/120    avg_loss:0.033, val_acc:0.979]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.987]
Epoch [76/120    avg_loss:0.010, val_acc:0.982]
Epoch [77/120    avg_loss:0.027, val_acc:0.966]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.008, val_acc:0.978]
Epoch [80/120    avg_loss:0.011, val_acc:0.988]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.016, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.979]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.092, val_acc:0.954]
Epoch [87/120    avg_loss:0.078, val_acc:0.975]
Epoch [88/120    avg_loss:0.016, val_acc:0.982]
Epoch [89/120    avg_loss:0.014, val_acc:0.966]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.009, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.004, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6340     0     0     4     0    36     0    51     1]
 [    0     2 18054     0    16     0    13     0     5     0]
 [    0     5     0  2025     2     0     0     0     3     1]
 [    0    42    23     0  2880     0     5     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4869     0     0     8]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     5     0     0    44     0     0     0  3501    21]
 [    0     0     0     0    14    77     0     0     0   828]]

Accuracy:
99.02634179259152

F1 scores:
[       nan 0.98861687 0.99836868 0.99704579 0.97100472 0.97134351
 0.99326805 0.99883586 0.97888998 0.93138358]

Kappa:
0.9871006317936911
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc925a29908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.778, val_acc:0.503]
Epoch [2/120    avg_loss:1.155, val_acc:0.726]
Epoch [3/120    avg_loss:0.818, val_acc:0.780]
Epoch [4/120    avg_loss:0.586, val_acc:0.816]
Epoch [5/120    avg_loss:0.487, val_acc:0.720]
Epoch [6/120    avg_loss:0.467, val_acc:0.853]
Epoch [7/120    avg_loss:0.384, val_acc:0.815]
Epoch [8/120    avg_loss:0.362, val_acc:0.851]
Epoch [9/120    avg_loss:1.380, val_acc:0.628]
Epoch [10/120    avg_loss:1.308, val_acc:0.700]
Epoch [11/120    avg_loss:1.121, val_acc:0.604]
Epoch [12/120    avg_loss:1.011, val_acc:0.715]
Epoch [13/120    avg_loss:0.930, val_acc:0.712]
Epoch [14/120    avg_loss:0.833, val_acc:0.662]
Epoch [15/120    avg_loss:0.820, val_acc:0.744]
Epoch [16/120    avg_loss:0.747, val_acc:0.751]
Epoch [17/120    avg_loss:0.730, val_acc:0.549]
Epoch [18/120    avg_loss:0.706, val_acc:0.791]
Epoch [19/120    avg_loss:0.636, val_acc:0.776]
Epoch [20/120    avg_loss:0.542, val_acc:0.797]
Epoch [21/120    avg_loss:0.492, val_acc:0.802]
Epoch [22/120    avg_loss:0.528, val_acc:0.802]
Epoch [23/120    avg_loss:0.532, val_acc:0.790]
Epoch [24/120    avg_loss:0.497, val_acc:0.797]
Epoch [25/120    avg_loss:0.504, val_acc:0.799]
Epoch [26/120    avg_loss:0.493, val_acc:0.798]
Epoch [27/120    avg_loss:0.504, val_acc:0.800]
Epoch [28/120    avg_loss:0.480, val_acc:0.807]
Epoch [29/120    avg_loss:0.459, val_acc:0.805]
Epoch [30/120    avg_loss:0.470, val_acc:0.807]
Epoch [31/120    avg_loss:0.486, val_acc:0.812]
Epoch [32/120    avg_loss:0.494, val_acc:0.810]
Epoch [33/120    avg_loss:0.441, val_acc:0.812]
Epoch [34/120    avg_loss:0.516, val_acc:0.811]
Epoch [35/120    avg_loss:0.438, val_acc:0.813]
Epoch [36/120    avg_loss:0.464, val_acc:0.812]
Epoch [37/120    avg_loss:0.426, val_acc:0.812]
Epoch [38/120    avg_loss:0.469, val_acc:0.812]
Epoch [39/120    avg_loss:0.418, val_acc:0.808]
Epoch [40/120    avg_loss:0.465, val_acc:0.810]
Epoch [41/120    avg_loss:0.492, val_acc:0.811]
Epoch [42/120    avg_loss:0.448, val_acc:0.812]
Epoch [43/120    avg_loss:0.446, val_acc:0.811]
Epoch [44/120    avg_loss:0.439, val_acc:0.810]
Epoch [45/120    avg_loss:0.420, val_acc:0.810]
Epoch [46/120    avg_loss:0.458, val_acc:0.809]
Epoch [47/120    avg_loss:0.473, val_acc:0.809]
Epoch [48/120    avg_loss:0.432, val_acc:0.808]
Epoch [49/120    avg_loss:0.456, val_acc:0.809]
Epoch [50/120    avg_loss:0.443, val_acc:0.807]
Epoch [51/120    avg_loss:0.467, val_acc:0.809]
Epoch [52/120    avg_loss:0.446, val_acc:0.808]
Epoch [53/120    avg_loss:0.422, val_acc:0.808]
Epoch [54/120    avg_loss:0.441, val_acc:0.808]
Epoch [55/120    avg_loss:0.468, val_acc:0.809]
Epoch [56/120    avg_loss:0.456, val_acc:0.809]
Epoch [57/120    avg_loss:0.488, val_acc:0.810]
Epoch [58/120    avg_loss:0.448, val_acc:0.809]
Epoch [59/120    avg_loss:0.472, val_acc:0.809]
Epoch [60/120    avg_loss:0.459, val_acc:0.809]
Epoch [61/120    avg_loss:0.435, val_acc:0.809]
Epoch [62/120    avg_loss:0.420, val_acc:0.809]
Epoch [63/120    avg_loss:0.424, val_acc:0.809]
Epoch [64/120    avg_loss:0.444, val_acc:0.809]
Epoch [65/120    avg_loss:0.451, val_acc:0.809]
Epoch [66/120    avg_loss:0.423, val_acc:0.809]
Epoch [67/120    avg_loss:0.434, val_acc:0.809]
Epoch [68/120    avg_loss:0.480, val_acc:0.809]
Epoch [69/120    avg_loss:0.481, val_acc:0.809]
Epoch [70/120    avg_loss:0.436, val_acc:0.809]
Epoch [71/120    avg_loss:0.408, val_acc:0.809]
Epoch [72/120    avg_loss:0.431, val_acc:0.809]
Epoch [73/120    avg_loss:0.423, val_acc:0.809]
Epoch [74/120    avg_loss:0.411, val_acc:0.809]
Epoch [75/120    avg_loss:0.449, val_acc:0.809]
Epoch [76/120    avg_loss:0.461, val_acc:0.809]
Epoch [77/120    avg_loss:0.446, val_acc:0.809]
Epoch [78/120    avg_loss:0.431, val_acc:0.809]
Epoch [79/120    avg_loss:0.469, val_acc:0.809]
Epoch [80/120    avg_loss:0.460, val_acc:0.809]
Epoch [81/120    avg_loss:0.458, val_acc:0.809]
Epoch [82/120    avg_loss:0.410, val_acc:0.809]
Epoch [83/120    avg_loss:0.447, val_acc:0.809]
Epoch [84/120    avg_loss:0.446, val_acc:0.809]
Epoch [85/120    avg_loss:0.462, val_acc:0.809]
Epoch [86/120    avg_loss:0.441, val_acc:0.809]
Epoch [87/120    avg_loss:0.448, val_acc:0.809]
Epoch [88/120    avg_loss:0.451, val_acc:0.809]
Epoch [89/120    avg_loss:0.459, val_acc:0.809]
Epoch [90/120    avg_loss:0.469, val_acc:0.809]
Epoch [91/120    avg_loss:0.472, val_acc:0.809]
Epoch [92/120    avg_loss:0.479, val_acc:0.809]
Epoch [93/120    avg_loss:0.441, val_acc:0.809]
Epoch [94/120    avg_loss:0.460, val_acc:0.809]
Epoch [95/120    avg_loss:0.441, val_acc:0.809]
Epoch [96/120    avg_loss:0.460, val_acc:0.809]
Epoch [97/120    avg_loss:0.472, val_acc:0.809]
Epoch [98/120    avg_loss:0.426, val_acc:0.809]
Epoch [99/120    avg_loss:0.452, val_acc:0.809]
Epoch [100/120    avg_loss:0.443, val_acc:0.809]
Epoch [101/120    avg_loss:0.436, val_acc:0.809]
Epoch [102/120    avg_loss:0.440, val_acc:0.809]
Epoch [103/120    avg_loss:0.441, val_acc:0.809]
Epoch [104/120    avg_loss:0.434, val_acc:0.809]
Epoch [105/120    avg_loss:0.445, val_acc:0.809]
Epoch [106/120    avg_loss:0.451, val_acc:0.809]
Epoch [107/120    avg_loss:0.413, val_acc:0.809]
Epoch [108/120    avg_loss:0.451, val_acc:0.809]
Epoch [109/120    avg_loss:0.465, val_acc:0.809]
Epoch [110/120    avg_loss:0.431, val_acc:0.809]
Epoch [111/120    avg_loss:0.441, val_acc:0.809]
Epoch [112/120    avg_loss:0.440, val_acc:0.809]
Epoch [113/120    avg_loss:0.439, val_acc:0.809]
Epoch [114/120    avg_loss:0.449, val_acc:0.809]
Epoch [115/120    avg_loss:0.484, val_acc:0.809]
Epoch [116/120    avg_loss:0.445, val_acc:0.809]
Epoch [117/120    avg_loss:0.392, val_acc:0.809]
Epoch [118/120    avg_loss:0.435, val_acc:0.809]
Epoch [119/120    avg_loss:0.484, val_acc:0.809]
Epoch [120/120    avg_loss:0.440, val_acc:0.809]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5238     0     0   428     0     9    28   696    33]
 [    0     8 15703     0    15     0  2362     0     2     0]
 [    0    29     0  1767    16     0     0     0   183    41]
 [    0   179   423    10  1916     8   318     0   107    11]
 [    0     0     0     2     0  1303     0     0     0     0]
 [    0     0  1330    20    40     0  3321     0   167     0]
 [    0     2     0     0     0     0     0  1274     5     9]
 [    0    84     0    32    31     0    64     0  3343    17]
 [    0     0     0    17    15   107     0     7     1   772]]

Accuracy:
83.47673101486998

F1 scores:
[       nan 0.87504176 0.8835312  0.90988671 0.70531934 0.95703268
 0.60646457 0.98037707 0.82798762 0.85682575]

Kappa:
0.782696232135062
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d10c42978>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.745, val_acc:0.612]
Epoch [2/120    avg_loss:1.019, val_acc:0.604]
Epoch [3/120    avg_loss:0.741, val_acc:0.783]
Epoch [4/120    avg_loss:0.616, val_acc:0.753]
Epoch [5/120    avg_loss:0.528, val_acc:0.817]
Epoch [6/120    avg_loss:0.420, val_acc:0.774]
Epoch [7/120    avg_loss:0.366, val_acc:0.820]
Epoch [8/120    avg_loss:0.374, val_acc:0.844]
Epoch [9/120    avg_loss:0.321, val_acc:0.895]
Epoch [10/120    avg_loss:0.246, val_acc:0.907]
Epoch [11/120    avg_loss:0.270, val_acc:0.895]
Epoch [12/120    avg_loss:0.220, val_acc:0.877]
Epoch [13/120    avg_loss:0.200, val_acc:0.902]
Epoch [14/120    avg_loss:0.138, val_acc:0.884]
Epoch [15/120    avg_loss:0.154, val_acc:0.898]
Epoch [16/120    avg_loss:0.116, val_acc:0.878]
Epoch [17/120    avg_loss:0.140, val_acc:0.938]
Epoch [18/120    avg_loss:0.120, val_acc:0.931]
Epoch [19/120    avg_loss:0.095, val_acc:0.960]
Epoch [20/120    avg_loss:0.110, val_acc:0.958]
Epoch [21/120    avg_loss:0.091, val_acc:0.936]
Epoch [22/120    avg_loss:0.103, val_acc:0.943]
Epoch [23/120    avg_loss:0.079, val_acc:0.944]
Epoch [24/120    avg_loss:0.053, val_acc:0.947]
Epoch [25/120    avg_loss:0.058, val_acc:0.964]
Epoch [26/120    avg_loss:0.087, val_acc:0.961]
Epoch [27/120    avg_loss:0.057, val_acc:0.964]
Epoch [28/120    avg_loss:0.038, val_acc:0.973]
Epoch [29/120    avg_loss:0.032, val_acc:0.973]
Epoch [30/120    avg_loss:0.029, val_acc:0.965]
Epoch [31/120    avg_loss:0.038, val_acc:0.978]
Epoch [32/120    avg_loss:0.028, val_acc:0.973]
Epoch [33/120    avg_loss:0.044, val_acc:0.980]
Epoch [34/120    avg_loss:0.024, val_acc:0.980]
Epoch [35/120    avg_loss:0.038, val_acc:0.965]
Epoch [36/120    avg_loss:0.055, val_acc:0.965]
Epoch [37/120    avg_loss:0.037, val_acc:0.972]
Epoch [38/120    avg_loss:0.036, val_acc:0.963]
Epoch [39/120    avg_loss:0.041, val_acc:0.942]
Epoch [40/120    avg_loss:0.095, val_acc:0.930]
Epoch [41/120    avg_loss:0.150, val_acc:0.942]
Epoch [42/120    avg_loss:0.045, val_acc:0.968]
Epoch [43/120    avg_loss:0.026, val_acc:0.977]
Epoch [44/120    avg_loss:0.023, val_acc:0.978]
Epoch [45/120    avg_loss:0.022, val_acc:0.983]
Epoch [46/120    avg_loss:0.019, val_acc:0.979]
Epoch [47/120    avg_loss:0.028, val_acc:0.981]
Epoch [48/120    avg_loss:0.025, val_acc:0.972]
Epoch [49/120    avg_loss:0.053, val_acc:0.967]
Epoch [50/120    avg_loss:0.056, val_acc:0.950]
Epoch [51/120    avg_loss:0.110, val_acc:0.955]
Epoch [52/120    avg_loss:0.067, val_acc:0.918]
Epoch [53/120    avg_loss:0.073, val_acc:0.976]
Epoch [54/120    avg_loss:0.032, val_acc:0.978]
Epoch [55/120    avg_loss:0.013, val_acc:0.977]
Epoch [56/120    avg_loss:0.017, val_acc:0.971]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.016, val_acc:0.968]
Epoch [59/120    avg_loss:0.027, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.981]
Epoch [61/120    avg_loss:0.009, val_acc:0.980]
Epoch [62/120    avg_loss:0.009, val_acc:0.981]
Epoch [63/120    avg_loss:0.009, val_acc:0.981]
Epoch [64/120    avg_loss:0.007, val_acc:0.981]
Epoch [65/120    avg_loss:0.008, val_acc:0.981]
Epoch [66/120    avg_loss:0.008, val_acc:0.982]
Epoch [67/120    avg_loss:0.007, val_acc:0.983]
Epoch [68/120    avg_loss:0.009, val_acc:0.982]
Epoch [69/120    avg_loss:0.007, val_acc:0.982]
Epoch [70/120    avg_loss:0.010, val_acc:0.983]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.006, val_acc:0.982]
Epoch [73/120    avg_loss:0.007, val_acc:0.982]
Epoch [74/120    avg_loss:0.007, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.007, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.982]
Epoch [79/120    avg_loss:0.006, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.979]
Epoch [87/120    avg_loss:0.008, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.007, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6287     0     0     0     0    35     4   106     0]
 [    0     2 18052     0    28     0     2     0     6     0]
 [    0     0     0  2005     3     0     0     0    24     4]
 [    0    48    21     0  2867     0     9     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     2     1]
 [    0     0     0     0     0     0     4  1285     0     1]
 [    0     3     0     0    47     0     0     0  3494    27]
 [    0     0     0     1    14    42     0     0     0   862]]

Accuracy:
98.88896922372449

F1 scores:
[       nan 0.98449734 0.9983685  0.99208313 0.96678469 0.9841629
 0.99459349 0.99651028 0.96652835 0.95038589]

Kappa:
0.985282973791774
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcdbb5e1908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.789, val_acc:0.537]
Epoch [2/120    avg_loss:1.144, val_acc:0.647]
Epoch [3/120    avg_loss:0.801, val_acc:0.632]
Epoch [4/120    avg_loss:0.649, val_acc:0.725]
Epoch [5/120    avg_loss:0.506, val_acc:0.821]
Epoch [6/120    avg_loss:0.438, val_acc:0.775]
Epoch [7/120    avg_loss:0.389, val_acc:0.823]
Epoch [8/120    avg_loss:0.383, val_acc:0.818]
Epoch [9/120    avg_loss:0.301, val_acc:0.851]
Epoch [10/120    avg_loss:0.292, val_acc:0.836]
Epoch [11/120    avg_loss:0.224, val_acc:0.866]
Epoch [12/120    avg_loss:0.199, val_acc:0.929]
Epoch [13/120    avg_loss:0.227, val_acc:0.869]
Epoch [14/120    avg_loss:0.156, val_acc:0.894]
Epoch [15/120    avg_loss:0.139, val_acc:0.948]
Epoch [16/120    avg_loss:0.127, val_acc:0.917]
Epoch [17/120    avg_loss:0.125, val_acc:0.932]
Epoch [18/120    avg_loss:0.126, val_acc:0.952]
Epoch [19/120    avg_loss:0.146, val_acc:0.924]
Epoch [20/120    avg_loss:0.130, val_acc:0.932]
Epoch [21/120    avg_loss:0.087, val_acc:0.943]
Epoch [22/120    avg_loss:0.766, val_acc:0.597]
Epoch [23/120    avg_loss:1.221, val_acc:0.690]
Epoch [24/120    avg_loss:0.994, val_acc:0.645]
Epoch [25/120    avg_loss:0.843, val_acc:0.748]
Epoch [26/120    avg_loss:0.770, val_acc:0.770]
Epoch [27/120    avg_loss:0.762, val_acc:0.766]
Epoch [28/120    avg_loss:0.676, val_acc:0.785]
Epoch [29/120    avg_loss:0.689, val_acc:0.758]
Epoch [30/120    avg_loss:0.593, val_acc:0.797]
Epoch [31/120    avg_loss:0.560, val_acc:0.814]
Epoch [32/120    avg_loss:0.529, val_acc:0.805]
Epoch [33/120    avg_loss:0.511, val_acc:0.811]
Epoch [34/120    avg_loss:0.506, val_acc:0.808]
Epoch [35/120    avg_loss:0.468, val_acc:0.801]
Epoch [36/120    avg_loss:0.483, val_acc:0.813]
Epoch [37/120    avg_loss:0.484, val_acc:0.804]
Epoch [38/120    avg_loss:0.502, val_acc:0.812]
Epoch [39/120    avg_loss:0.447, val_acc:0.810]
Epoch [40/120    avg_loss:0.472, val_acc:0.812]
Epoch [41/120    avg_loss:0.476, val_acc:0.810]
Epoch [42/120    avg_loss:0.451, val_acc:0.811]
Epoch [43/120    avg_loss:0.440, val_acc:0.817]
Epoch [44/120    avg_loss:0.441, val_acc:0.818]
Epoch [45/120    avg_loss:0.426, val_acc:0.818]
Epoch [46/120    avg_loss:0.435, val_acc:0.816]
Epoch [47/120    avg_loss:0.442, val_acc:0.819]
Epoch [48/120    avg_loss:0.465, val_acc:0.815]
Epoch [49/120    avg_loss:0.447, val_acc:0.814]
Epoch [50/120    avg_loss:0.471, val_acc:0.816]
Epoch [51/120    avg_loss:0.430, val_acc:0.818]
Epoch [52/120    avg_loss:0.431, val_acc:0.819]
Epoch [53/120    avg_loss:0.479, val_acc:0.816]
Epoch [54/120    avg_loss:0.402, val_acc:0.817]
Epoch [55/120    avg_loss:0.435, val_acc:0.814]
Epoch [56/120    avg_loss:0.461, val_acc:0.815]
Epoch [57/120    avg_loss:0.479, val_acc:0.817]
Epoch [58/120    avg_loss:0.410, val_acc:0.818]
Epoch [59/120    avg_loss:0.464, val_acc:0.819]
Epoch [60/120    avg_loss:0.410, val_acc:0.818]
Epoch [61/120    avg_loss:0.495, val_acc:0.818]
Epoch [62/120    avg_loss:0.465, val_acc:0.818]
Epoch [63/120    avg_loss:0.446, val_acc:0.818]
Epoch [64/120    avg_loss:0.441, val_acc:0.818]
Epoch [65/120    avg_loss:0.475, val_acc:0.817]
Epoch [66/120    avg_loss:0.430, val_acc:0.816]
Epoch [67/120    avg_loss:0.433, val_acc:0.816]
Epoch [68/120    avg_loss:0.437, val_acc:0.816]
Epoch [69/120    avg_loss:0.446, val_acc:0.816]
Epoch [70/120    avg_loss:0.444, val_acc:0.816]
Epoch [71/120    avg_loss:0.467, val_acc:0.816]
Epoch [72/120    avg_loss:0.478, val_acc:0.816]
Epoch [73/120    avg_loss:0.432, val_acc:0.816]
Epoch [74/120    avg_loss:0.424, val_acc:0.816]
Epoch [75/120    avg_loss:0.458, val_acc:0.816]
Epoch [76/120    avg_loss:0.426, val_acc:0.816]
Epoch [77/120    avg_loss:0.423, val_acc:0.816]
Epoch [78/120    avg_loss:0.463, val_acc:0.816]
Epoch [79/120    avg_loss:0.453, val_acc:0.816]
Epoch [80/120    avg_loss:0.411, val_acc:0.816]
Epoch [81/120    avg_loss:0.418, val_acc:0.816]
Epoch [82/120    avg_loss:0.426, val_acc:0.816]
Epoch [83/120    avg_loss:0.442, val_acc:0.816]
Epoch [84/120    avg_loss:0.411, val_acc:0.816]
Epoch [85/120    avg_loss:0.460, val_acc:0.816]
Epoch [86/120    avg_loss:0.475, val_acc:0.816]
Epoch [87/120    avg_loss:0.453, val_acc:0.816]
Epoch [88/120    avg_loss:0.444, val_acc:0.816]
Epoch [89/120    avg_loss:0.428, val_acc:0.816]
Epoch [90/120    avg_loss:0.413, val_acc:0.816]
Epoch [91/120    avg_loss:0.428, val_acc:0.816]
Epoch [92/120    avg_loss:0.441, val_acc:0.816]
Epoch [93/120    avg_loss:0.459, val_acc:0.816]
Epoch [94/120    avg_loss:0.432, val_acc:0.816]
Epoch [95/120    avg_loss:0.458, val_acc:0.816]
Epoch [96/120    avg_loss:0.456, val_acc:0.816]
Epoch [97/120    avg_loss:0.470, val_acc:0.816]
Epoch [98/120    avg_loss:0.423, val_acc:0.816]
Epoch [99/120    avg_loss:0.431, val_acc:0.816]
Epoch [100/120    avg_loss:0.450, val_acc:0.816]
Epoch [101/120    avg_loss:0.426, val_acc:0.816]
Epoch [102/120    avg_loss:0.453, val_acc:0.816]
Epoch [103/120    avg_loss:0.421, val_acc:0.816]
Epoch [104/120    avg_loss:0.470, val_acc:0.816]
Epoch [105/120    avg_loss:0.443, val_acc:0.816]
Epoch [106/120    avg_loss:0.500, val_acc:0.816]
Epoch [107/120    avg_loss:0.433, val_acc:0.816]
Epoch [108/120    avg_loss:0.458, val_acc:0.816]
Epoch [109/120    avg_loss:0.456, val_acc:0.816]
Epoch [110/120    avg_loss:0.437, val_acc:0.816]
Epoch [111/120    avg_loss:0.430, val_acc:0.816]
Epoch [112/120    avg_loss:0.434, val_acc:0.816]
Epoch [113/120    avg_loss:0.414, val_acc:0.816]
Epoch [114/120    avg_loss:0.426, val_acc:0.816]
Epoch [115/120    avg_loss:0.457, val_acc:0.816]
Epoch [116/120    avg_loss:0.460, val_acc:0.816]
Epoch [117/120    avg_loss:0.435, val_acc:0.816]
Epoch [118/120    avg_loss:0.442, val_acc:0.816]
Epoch [119/120    avg_loss:0.473, val_acc:0.816]
Epoch [120/120    avg_loss:0.431, val_acc:0.816]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5693     0     0   337     0     9    10   324    59]
 [    0     0 15649     0    39     0  2398     0     4     0]
 [    0    12     0  1847     0     0     0     0   116    61]
 [    0   175   126     0  2290     0   286     0    79    16]
 [    0     0     0     0     0  1293     0     0     4     8]
 [    0     0   925    25    44     0  3691     0   193     0]
 [    0     3     0     0     0     0    10  1267     2     8]
 [    0    91     0     2    38     0    70     0  3348    22]
 [    0    10     0     5    17   149     0     1     0   737]]

Accuracy:
86.31576410478876

F1 scores:
[       nan 0.91704253 0.89962633 0.94355045 0.79832665 0.94139061
 0.65085523 0.98676012 0.87632509 0.80546448]

Kappa:
0.821327830655784
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f472b184898>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.763, val_acc:0.469]
Epoch [2/120    avg_loss:1.039, val_acc:0.748]
Epoch [3/120    avg_loss:0.747, val_acc:0.661]
Epoch [4/120    avg_loss:0.607, val_acc:0.700]
Epoch [5/120    avg_loss:0.484, val_acc:0.741]
Epoch [6/120    avg_loss:0.441, val_acc:0.798]
Epoch [7/120    avg_loss:0.428, val_acc:0.776]
Epoch [8/120    avg_loss:0.387, val_acc:0.852]
Epoch [9/120    avg_loss:0.342, val_acc:0.864]
Epoch [10/120    avg_loss:0.313, val_acc:0.786]
Epoch [11/120    avg_loss:0.287, val_acc:0.861]
Epoch [12/120    avg_loss:0.253, val_acc:0.891]
Epoch [13/120    avg_loss:0.189, val_acc:0.861]
Epoch [14/120    avg_loss:0.161, val_acc:0.889]
Epoch [15/120    avg_loss:0.143, val_acc:0.856]
Epoch [16/120    avg_loss:0.160, val_acc:0.911]
Epoch [17/120    avg_loss:0.166, val_acc:0.931]
Epoch [18/120    avg_loss:0.109, val_acc:0.951]
Epoch [19/120    avg_loss:0.095, val_acc:0.946]
Epoch [20/120    avg_loss:0.095, val_acc:0.937]
Epoch [21/120    avg_loss:0.076, val_acc:0.957]
Epoch [22/120    avg_loss:0.072, val_acc:0.933]
Epoch [23/120    avg_loss:0.068, val_acc:0.937]
Epoch [24/120    avg_loss:0.126, val_acc:0.896]
Epoch [25/120    avg_loss:0.151, val_acc:0.926]
Epoch [26/120    avg_loss:0.076, val_acc:0.955]
Epoch [27/120    avg_loss:0.053, val_acc:0.928]
Epoch [28/120    avg_loss:0.050, val_acc:0.957]
Epoch [29/120    avg_loss:0.053, val_acc:0.974]
Epoch [30/120    avg_loss:0.043, val_acc:0.930]
Epoch [31/120    avg_loss:0.045, val_acc:0.970]
Epoch [32/120    avg_loss:0.103, val_acc:0.931]
Epoch [33/120    avg_loss:0.055, val_acc:0.966]
Epoch [34/120    avg_loss:0.050, val_acc:0.948]
Epoch [35/120    avg_loss:0.030, val_acc:0.959]
Epoch [36/120    avg_loss:0.022, val_acc:0.971]
Epoch [37/120    avg_loss:0.020, val_acc:0.971]
Epoch [38/120    avg_loss:0.019, val_acc:0.969]
Epoch [39/120    avg_loss:0.027, val_acc:0.960]
Epoch [40/120    avg_loss:0.016, val_acc:0.979]
Epoch [41/120    avg_loss:0.030, val_acc:0.913]
Epoch [42/120    avg_loss:0.037, val_acc:0.948]
Epoch [43/120    avg_loss:0.020, val_acc:0.980]
Epoch [44/120    avg_loss:0.024, val_acc:0.974]
Epoch [45/120    avg_loss:0.028, val_acc:0.954]
Epoch [46/120    avg_loss:0.068, val_acc:0.951]
Epoch [47/120    avg_loss:0.027, val_acc:0.952]
Epoch [48/120    avg_loss:0.045, val_acc:0.977]
Epoch [49/120    avg_loss:0.034, val_acc:0.944]
Epoch [50/120    avg_loss:0.081, val_acc:0.965]
Epoch [51/120    avg_loss:0.154, val_acc:0.953]
Epoch [52/120    avg_loss:0.056, val_acc:0.967]
Epoch [53/120    avg_loss:0.040, val_acc:0.978]
Epoch [54/120    avg_loss:0.018, val_acc:0.975]
Epoch [55/120    avg_loss:0.017, val_acc:0.977]
Epoch [56/120    avg_loss:0.015, val_acc:0.973]
Epoch [57/120    avg_loss:0.016, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.979]
Epoch [59/120    avg_loss:0.010, val_acc:0.982]
Epoch [60/120    avg_loss:0.009, val_acc:0.982]
Epoch [61/120    avg_loss:0.014, val_acc:0.984]
Epoch [62/120    avg_loss:0.009, val_acc:0.982]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.008, val_acc:0.983]
Epoch [65/120    avg_loss:0.007, val_acc:0.982]
Epoch [66/120    avg_loss:0.008, val_acc:0.984]
Epoch [67/120    avg_loss:0.008, val_acc:0.979]
Epoch [68/120    avg_loss:0.009, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.983]
Epoch [70/120    avg_loss:0.008, val_acc:0.984]
Epoch [71/120    avg_loss:0.014, val_acc:0.984]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.986]
Epoch [77/120    avg_loss:0.007, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.005, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0    31     6     0     5     1     2     0]
 [    0     2 18041     0    44     0     1     0     2     0]
 [    0     0     0  2027     1     0     0     0     7     1]
 [    0    69    19     0  2855     0     0     0    29     0]
 [    0     0     0     0     0  1300     0     0     0     5]
 [    0     0     0     3     0     0  4863     0     0    12]
 [    0     1     0     0     0     0     3  1282     0     4]
 [    0    11     0     0    57     0     0     0  3478    25]
 [    0     1     0     3    14    42     0     0     0   859]]

Accuracy:
99.03357192779505

F1 scores:
[       nan 0.99000233 0.99811895 0.98878049 0.95982518 0.98224405
 0.99753846 0.99650214 0.98123854 0.94136986]

Kappa:
0.9871982957660498
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f235beb1908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.738, val_acc:0.476]
Epoch [2/120    avg_loss:1.122, val_acc:0.521]
Epoch [3/120    avg_loss:0.784, val_acc:0.783]
Epoch [4/120    avg_loss:0.635, val_acc:0.708]
Epoch [5/120    avg_loss:0.487, val_acc:0.821]
Epoch [6/120    avg_loss:0.471, val_acc:0.736]
Epoch [7/120    avg_loss:0.456, val_acc:0.814]
Epoch [8/120    avg_loss:1.094, val_acc:0.541]
Epoch [9/120    avg_loss:1.309, val_acc:0.650]
Epoch [10/120    avg_loss:0.972, val_acc:0.684]
Epoch [11/120    avg_loss:0.800, val_acc:0.669]
Epoch [12/120    avg_loss:0.714, val_acc:0.739]
Epoch [13/120    avg_loss:0.605, val_acc:0.765]
Epoch [14/120    avg_loss:0.561, val_acc:0.770]
Epoch [15/120    avg_loss:0.521, val_acc:0.753]
Epoch [16/120    avg_loss:0.483, val_acc:0.747]
Epoch [17/120    avg_loss:0.406, val_acc:0.832]
Epoch [18/120    avg_loss:0.391, val_acc:0.821]
Epoch [19/120    avg_loss:0.349, val_acc:0.794]
Epoch [20/120    avg_loss:0.315, val_acc:0.849]
Epoch [21/120    avg_loss:0.272, val_acc:0.857]
Epoch [22/120    avg_loss:0.282, val_acc:0.840]
Epoch [23/120    avg_loss:0.276, val_acc:0.867]
Epoch [24/120    avg_loss:0.304, val_acc:0.867]
Epoch [25/120    avg_loss:0.230, val_acc:0.854]
Epoch [26/120    avg_loss:0.268, val_acc:0.865]
Epoch [27/120    avg_loss:0.242, val_acc:0.873]
Epoch [28/120    avg_loss:0.206, val_acc:0.872]
Epoch [29/120    avg_loss:0.208, val_acc:0.889]
Epoch [30/120    avg_loss:0.202, val_acc:0.880]
Epoch [31/120    avg_loss:0.177, val_acc:0.905]
Epoch [32/120    avg_loss:0.175, val_acc:0.895]
Epoch [33/120    avg_loss:0.166, val_acc:0.833]
Epoch [34/120    avg_loss:0.234, val_acc:0.894]
Epoch [35/120    avg_loss:0.149, val_acc:0.912]
Epoch [36/120    avg_loss:0.151, val_acc:0.890]
Epoch [37/120    avg_loss:0.163, val_acc:0.912]
Epoch [38/120    avg_loss:0.140, val_acc:0.941]
Epoch [39/120    avg_loss:0.117, val_acc:0.940]
Epoch [40/120    avg_loss:0.128, val_acc:0.888]
Epoch [41/120    avg_loss:0.181, val_acc:0.867]
Epoch [42/120    avg_loss:0.141, val_acc:0.930]
Epoch [43/120    avg_loss:0.125, val_acc:0.917]
Epoch [44/120    avg_loss:0.098, val_acc:0.929]
Epoch [45/120    avg_loss:0.111, val_acc:0.890]
Epoch [46/120    avg_loss:0.098, val_acc:0.917]
Epoch [47/120    avg_loss:0.094, val_acc:0.937]
Epoch [48/120    avg_loss:0.094, val_acc:0.923]
Epoch [49/120    avg_loss:0.103, val_acc:0.942]
Epoch [50/120    avg_loss:0.083, val_acc:0.922]
Epoch [51/120    avg_loss:0.081, val_acc:0.918]
Epoch [52/120    avg_loss:0.120, val_acc:0.928]
Epoch [53/120    avg_loss:0.109, val_acc:0.923]
Epoch [54/120    avg_loss:0.090, val_acc:0.890]
Epoch [55/120    avg_loss:0.086, val_acc:0.949]
Epoch [56/120    avg_loss:0.080, val_acc:0.941]
Epoch [57/120    avg_loss:0.062, val_acc:0.949]
Epoch [58/120    avg_loss:0.055, val_acc:0.955]
Epoch [59/120    avg_loss:0.053, val_acc:0.952]
Epoch [60/120    avg_loss:0.062, val_acc:0.947]
Epoch [61/120    avg_loss:0.053, val_acc:0.952]
Epoch [62/120    avg_loss:0.047, val_acc:0.957]
Epoch [63/120    avg_loss:0.046, val_acc:0.959]
Epoch [64/120    avg_loss:0.040, val_acc:0.955]
Epoch [65/120    avg_loss:0.047, val_acc:0.934]
Epoch [66/120    avg_loss:0.039, val_acc:0.963]
Epoch [67/120    avg_loss:0.043, val_acc:0.966]
Epoch [68/120    avg_loss:0.029, val_acc:0.965]
Epoch [69/120    avg_loss:0.035, val_acc:0.948]
Epoch [70/120    avg_loss:0.038, val_acc:0.960]
Epoch [71/120    avg_loss:0.086, val_acc:0.955]
Epoch [72/120    avg_loss:0.055, val_acc:0.941]
Epoch [73/120    avg_loss:0.056, val_acc:0.934]
Epoch [74/120    avg_loss:0.071, val_acc:0.915]
Epoch [75/120    avg_loss:0.042, val_acc:0.952]
Epoch [76/120    avg_loss:0.097, val_acc:0.928]
Epoch [77/120    avg_loss:0.064, val_acc:0.920]
Epoch [78/120    avg_loss:0.041, val_acc:0.955]
Epoch [79/120    avg_loss:0.034, val_acc:0.964]
Epoch [80/120    avg_loss:0.041, val_acc:0.952]
Epoch [81/120    avg_loss:0.040, val_acc:0.958]
Epoch [82/120    avg_loss:0.028, val_acc:0.965]
Epoch [83/120    avg_loss:0.027, val_acc:0.972]
Epoch [84/120    avg_loss:0.021, val_acc:0.973]
Epoch [85/120    avg_loss:0.023, val_acc:0.973]
Epoch [86/120    avg_loss:0.022, val_acc:0.969]
Epoch [87/120    avg_loss:0.019, val_acc:0.969]
Epoch [88/120    avg_loss:0.016, val_acc:0.968]
Epoch [89/120    avg_loss:0.019, val_acc:0.969]
Epoch [90/120    avg_loss:0.016, val_acc:0.973]
Epoch [91/120    avg_loss:0.023, val_acc:0.971]
Epoch [92/120    avg_loss:0.024, val_acc:0.969]
Epoch [93/120    avg_loss:0.015, val_acc:0.970]
Epoch [94/120    avg_loss:0.021, val_acc:0.967]
Epoch [95/120    avg_loss:0.021, val_acc:0.974]
Epoch [96/120    avg_loss:0.022, val_acc:0.971]
Epoch [97/120    avg_loss:0.022, val_acc:0.970]
Epoch [98/120    avg_loss:0.020, val_acc:0.972]
Epoch [99/120    avg_loss:0.015, val_acc:0.973]
Epoch [100/120    avg_loss:0.019, val_acc:0.971]
Epoch [101/120    avg_loss:0.015, val_acc:0.972]
Epoch [102/120    avg_loss:0.024, val_acc:0.971]
Epoch [103/120    avg_loss:0.016, val_acc:0.974]
Epoch [104/120    avg_loss:0.017, val_acc:0.970]
Epoch [105/120    avg_loss:0.015, val_acc:0.972]
Epoch [106/120    avg_loss:0.014, val_acc:0.973]
Epoch [107/120    avg_loss:0.016, val_acc:0.976]
Epoch [108/120    avg_loss:0.017, val_acc:0.970]
Epoch [109/120    avg_loss:0.015, val_acc:0.974]
Epoch [110/120    avg_loss:0.016, val_acc:0.974]
Epoch [111/120    avg_loss:0.019, val_acc:0.973]
Epoch [112/120    avg_loss:0.012, val_acc:0.973]
Epoch [113/120    avg_loss:0.016, val_acc:0.973]
Epoch [114/120    avg_loss:0.014, val_acc:0.972]
Epoch [115/120    avg_loss:0.015, val_acc:0.974]
Epoch [116/120    avg_loss:0.015, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.974]
Epoch [118/120    avg_loss:0.019, val_acc:0.974]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0     0    18     0     0     2    25     0]
 [    0     0 17684     0   399     0     0     0     7     0]
 [    0    38     0  1958     0     0     0     0    30    10]
 [    0    61    18     0  2835     0    16     3    28    11]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     7     5     0  4864     0     2     0]
 [    0     0     0     0     0     0     7  1278     0     5]
 [    0    64     0     0    49     0     0     0  3458     0]
 [    0     1     0     2    21    63     0     0     0   832]]

Accuracy:
97.85023979948426

F1 scores:
[       nan 0.98390203 0.98815378 0.9782663  0.90014288 0.97643098
 0.99621096 0.99339293 0.97121191 0.93640968]

Kappa:
0.9716312520756597
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f713e2fa898>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.762, val_acc:0.294]
Epoch [2/120    avg_loss:1.102, val_acc:0.670]
Epoch [3/120    avg_loss:0.806, val_acc:0.781]
Epoch [4/120    avg_loss:0.673, val_acc:0.754]
Epoch [5/120    avg_loss:0.565, val_acc:0.811]
Epoch [6/120    avg_loss:0.458, val_acc:0.821]
Epoch [7/120    avg_loss:0.375, val_acc:0.771]
Epoch [8/120    avg_loss:0.324, val_acc:0.865]
Epoch [9/120    avg_loss:0.279, val_acc:0.877]
Epoch [10/120    avg_loss:0.282, val_acc:0.811]
Epoch [11/120    avg_loss:0.282, val_acc:0.918]
Epoch [12/120    avg_loss:0.178, val_acc:0.852]
Epoch [13/120    avg_loss:0.184, val_acc:0.913]
Epoch [14/120    avg_loss:0.199, val_acc:0.946]
Epoch [15/120    avg_loss:0.173, val_acc:0.920]
Epoch [16/120    avg_loss:0.194, val_acc:0.954]
Epoch [17/120    avg_loss:0.231, val_acc:0.927]
Epoch [18/120    avg_loss:0.153, val_acc:0.964]
Epoch [19/120    avg_loss:0.142, val_acc:0.930]
Epoch [20/120    avg_loss:0.209, val_acc:0.840]
Epoch [21/120    avg_loss:0.174, val_acc:0.875]
Epoch [22/120    avg_loss:0.122, val_acc:0.960]
Epoch [23/120    avg_loss:0.087, val_acc:0.957]
Epoch [24/120    avg_loss:0.103, val_acc:0.919]
Epoch [25/120    avg_loss:0.116, val_acc:0.946]
Epoch [26/120    avg_loss:0.096, val_acc:0.946]
Epoch [27/120    avg_loss:0.097, val_acc:0.969]
Epoch [28/120    avg_loss:0.080, val_acc:0.966]
Epoch [29/120    avg_loss:0.080, val_acc:0.966]
Epoch [30/120    avg_loss:0.071, val_acc:0.948]
Epoch [31/120    avg_loss:0.081, val_acc:0.963]
Epoch [32/120    avg_loss:0.070, val_acc:0.954]
Epoch [33/120    avg_loss:0.055, val_acc:0.963]
Epoch [34/120    avg_loss:0.051, val_acc:0.969]
Epoch [35/120    avg_loss:0.064, val_acc:0.981]
Epoch [36/120    avg_loss:0.051, val_acc:0.979]
Epoch [37/120    avg_loss:0.038, val_acc:0.966]
Epoch [38/120    avg_loss:0.040, val_acc:0.974]
Epoch [39/120    avg_loss:0.055, val_acc:0.962]
Epoch [40/120    avg_loss:0.036, val_acc:0.971]
Epoch [41/120    avg_loss:0.052, val_acc:0.883]
Epoch [42/120    avg_loss:0.172, val_acc:0.909]
Epoch [43/120    avg_loss:0.086, val_acc:0.973]
Epoch [44/120    avg_loss:0.065, val_acc:0.980]
Epoch [45/120    avg_loss:0.054, val_acc:0.980]
Epoch [46/120    avg_loss:0.064, val_acc:0.972]
Epoch [47/120    avg_loss:0.046, val_acc:0.975]
Epoch [48/120    avg_loss:0.065, val_acc:0.970]
Epoch [49/120    avg_loss:0.029, val_acc:0.980]
Epoch [50/120    avg_loss:0.023, val_acc:0.980]
Epoch [51/120    avg_loss:0.032, val_acc:0.980]
Epoch [52/120    avg_loss:0.026, val_acc:0.982]
Epoch [53/120    avg_loss:0.021, val_acc:0.985]
Epoch [54/120    avg_loss:0.031, val_acc:0.983]
Epoch [55/120    avg_loss:0.021, val_acc:0.986]
Epoch [56/120    avg_loss:0.022, val_acc:0.982]
Epoch [57/120    avg_loss:0.024, val_acc:0.982]
Epoch [58/120    avg_loss:0.022, val_acc:0.980]
Epoch [59/120    avg_loss:0.021, val_acc:0.985]
Epoch [60/120    avg_loss:0.015, val_acc:0.984]
Epoch [61/120    avg_loss:0.017, val_acc:0.985]
Epoch [62/120    avg_loss:0.018, val_acc:0.983]
Epoch [63/120    avg_loss:0.027, val_acc:0.981]
Epoch [64/120    avg_loss:0.019, val_acc:0.984]
Epoch [65/120    avg_loss:0.023, val_acc:0.986]
Epoch [66/120    avg_loss:0.017, val_acc:0.985]
Epoch [67/120    avg_loss:0.022, val_acc:0.985]
Epoch [68/120    avg_loss:0.022, val_acc:0.984]
Epoch [69/120    avg_loss:0.023, val_acc:0.982]
Epoch [70/120    avg_loss:0.017, val_acc:0.982]
Epoch [71/120    avg_loss:0.016, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.022, val_acc:0.983]
Epoch [74/120    avg_loss:0.016, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.984]
Epoch [76/120    avg_loss:0.019, val_acc:0.984]
Epoch [77/120    avg_loss:0.019, val_acc:0.985]
Epoch [78/120    avg_loss:0.022, val_acc:0.983]
Epoch [79/120    avg_loss:0.024, val_acc:0.983]
Epoch [80/120    avg_loss:0.017, val_acc:0.983]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.017, val_acc:0.984]
Epoch [83/120    avg_loss:0.023, val_acc:0.984]
Epoch [84/120    avg_loss:0.018, val_acc:0.984]
Epoch [85/120    avg_loss:0.015, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.018, val_acc:0.984]
Epoch [88/120    avg_loss:0.025, val_acc:0.983]
Epoch [89/120    avg_loss:0.021, val_acc:0.982]
Epoch [90/120    avg_loss:0.019, val_acc:0.982]
Epoch [91/120    avg_loss:0.017, val_acc:0.983]
Epoch [92/120    avg_loss:0.014, val_acc:0.983]
Epoch [93/120    avg_loss:0.020, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.983]
Epoch [95/120    avg_loss:0.019, val_acc:0.983]
Epoch [96/120    avg_loss:0.015, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.983]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.983]
Epoch [100/120    avg_loss:0.017, val_acc:0.983]
Epoch [101/120    avg_loss:0.017, val_acc:0.983]
Epoch [102/120    avg_loss:0.018, val_acc:0.983]
Epoch [103/120    avg_loss:0.016, val_acc:0.983]
Epoch [104/120    avg_loss:0.015, val_acc:0.983]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.014, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.020, val_acc:0.983]
Epoch [109/120    avg_loss:0.021, val_acc:0.983]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.015, val_acc:0.983]
Epoch [112/120    avg_loss:0.018, val_acc:0.983]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.019, val_acc:0.983]
Epoch [115/120    avg_loss:0.015, val_acc:0.983]
Epoch [116/120    avg_loss:0.017, val_acc:0.983]
Epoch [117/120    avg_loss:0.020, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.015, val_acc:0.983]
Epoch [120/120    avg_loss:0.016, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6391     0     0     2     0    21     0    15     3]
 [    0     0 17878     0    29     0   182     0     1     0]
 [    0     0     0  2023     2     0     0     0    10     1]
 [    0    34    22     4  2873     0     6     0    33     0]
 [    0     0     0     0     0  1170   135     0     0     0]
 [    0     0     0     0     0     0  4871     0     0     7]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     1     0     0    46     0     0     0  3520     4]
 [    0     0     0     0    19    22    19     0     0   859]]

Accuracy:
98.50577205793749

F1 scores:
[       nan 0.99408928 0.99349819 0.9958159  0.96685176 0.93712455
 0.9632193  0.9992242  0.98461538 0.95817066]

Kappa:
0.980229692874602
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5a3c2b438>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.770, val_acc:0.683]
Epoch [2/120    avg_loss:1.061, val_acc:0.783]
Epoch [3/120    avg_loss:0.735, val_acc:0.777]
Epoch [4/120    avg_loss:0.562, val_acc:0.860]
Epoch [5/120    avg_loss:0.584, val_acc:0.830]
Epoch [6/120    avg_loss:0.386, val_acc:0.861]
Epoch [7/120    avg_loss:0.373, val_acc:0.794]
Epoch [8/120    avg_loss:0.304, val_acc:0.910]
Epoch [9/120    avg_loss:0.269, val_acc:0.835]
Epoch [10/120    avg_loss:0.260, val_acc:0.901]
Epoch [11/120    avg_loss:0.198, val_acc:0.902]
Epoch [12/120    avg_loss:0.256, val_acc:0.892]
Epoch [13/120    avg_loss:0.243, val_acc:0.894]
Epoch [14/120    avg_loss:0.212, val_acc:0.937]
Epoch [15/120    avg_loss:0.230, val_acc:0.937]
Epoch [16/120    avg_loss:0.138, val_acc:0.904]
Epoch [17/120    avg_loss:0.181, val_acc:0.835]
Epoch [18/120    avg_loss:0.202, val_acc:0.940]
Epoch [19/120    avg_loss:0.122, val_acc:0.948]
Epoch [20/120    avg_loss:0.096, val_acc:0.938]
Epoch [21/120    avg_loss:0.092, val_acc:0.921]
Epoch [22/120    avg_loss:0.090, val_acc:0.960]
Epoch [23/120    avg_loss:0.141, val_acc:0.932]
Epoch [24/120    avg_loss:0.086, val_acc:0.966]
Epoch [25/120    avg_loss:0.208, val_acc:0.887]
Epoch [26/120    avg_loss:0.148, val_acc:0.943]
Epoch [27/120    avg_loss:0.128, val_acc:0.938]
Epoch [28/120    avg_loss:0.071, val_acc:0.973]
Epoch [29/120    avg_loss:0.059, val_acc:0.960]
Epoch [30/120    avg_loss:0.073, val_acc:0.966]
Epoch [31/120    avg_loss:0.091, val_acc:0.961]
Epoch [32/120    avg_loss:0.103, val_acc:0.960]
Epoch [33/120    avg_loss:0.052, val_acc:0.975]
Epoch [34/120    avg_loss:0.076, val_acc:0.961]
Epoch [35/120    avg_loss:0.057, val_acc:0.978]
Epoch [36/120    avg_loss:0.045, val_acc:0.976]
Epoch [37/120    avg_loss:0.070, val_acc:0.942]
Epoch [38/120    avg_loss:0.052, val_acc:0.979]
Epoch [39/120    avg_loss:0.028, val_acc:0.979]
Epoch [40/120    avg_loss:0.025, val_acc:0.980]
Epoch [41/120    avg_loss:0.022, val_acc:0.981]
Epoch [42/120    avg_loss:0.063, val_acc:0.973]
Epoch [43/120    avg_loss:0.090, val_acc:0.968]
Epoch [44/120    avg_loss:0.042, val_acc:0.964]
Epoch [45/120    avg_loss:0.100, val_acc:0.970]
Epoch [46/120    avg_loss:0.089, val_acc:0.959]
Epoch [47/120    avg_loss:0.047, val_acc:0.985]
Epoch [48/120    avg_loss:0.029, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.982]
Epoch [50/120    avg_loss:0.022, val_acc:0.982]
Epoch [51/120    avg_loss:0.027, val_acc:0.983]
Epoch [52/120    avg_loss:0.023, val_acc:0.987]
Epoch [53/120    avg_loss:0.017, val_acc:0.979]
Epoch [54/120    avg_loss:0.017, val_acc:0.987]
Epoch [55/120    avg_loss:0.024, val_acc:0.981]
Epoch [56/120    avg_loss:0.011, val_acc:0.980]
Epoch [57/120    avg_loss:0.015, val_acc:0.987]
Epoch [58/120    avg_loss:0.011, val_acc:0.980]
Epoch [59/120    avg_loss:0.015, val_acc:0.981]
Epoch [60/120    avg_loss:0.030, val_acc:0.979]
Epoch [61/120    avg_loss:0.019, val_acc:0.984]
Epoch [62/120    avg_loss:0.012, val_acc:0.981]
Epoch [63/120    avg_loss:0.014, val_acc:0.984]
Epoch [64/120    avg_loss:0.006, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.019, val_acc:0.975]
Epoch [67/120    avg_loss:0.012, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.982]
Epoch [69/120    avg_loss:0.016, val_acc:0.981]
Epoch [70/120    avg_loss:0.012, val_acc:0.984]
Epoch [71/120    avg_loss:0.008, val_acc:0.985]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.006, val_acc:0.987]
Epoch [74/120    avg_loss:0.007, val_acc:0.987]
Epoch [75/120    avg_loss:0.008, val_acc:0.987]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.011, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.005, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     2     0    20     3    12     2]
 [    0     0 18062     0    28     0     0     0     0     0]
 [    0     0     0  1992     2     0     0     0    38     4]
 [    0    20    13     0  2923     0     0     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4869     0     0     8]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0     5     0     1    63     0     0     0  3486    16]
 [    0     0     0     2    14    26     0     1     0   876]]

Accuracy:
99.2673462993758

F1 scores:
[       nan 0.99501946 0.99886631 0.98809524 0.97368421 0.99013657
 0.9968267  0.99573147 0.97880107 0.95737705]

Kappa:
0.9902937354377959
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5344de58d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.690, val_acc:0.592]
Epoch [2/120    avg_loss:1.023, val_acc:0.622]
Epoch [3/120    avg_loss:0.769, val_acc:0.683]
Epoch [4/120    avg_loss:0.637, val_acc:0.757]
Epoch [5/120    avg_loss:0.495, val_acc:0.751]
Epoch [6/120    avg_loss:0.390, val_acc:0.827]
Epoch [7/120    avg_loss:0.312, val_acc:0.854]
Epoch [8/120    avg_loss:0.326, val_acc:0.890]
Epoch [9/120    avg_loss:0.273, val_acc:0.812]
Epoch [10/120    avg_loss:0.249, val_acc:0.901]
Epoch [11/120    avg_loss:0.253, val_acc:0.894]
Epoch [12/120    avg_loss:0.223, val_acc:0.913]
Epoch [13/120    avg_loss:0.175, val_acc:0.917]
Epoch [14/120    avg_loss:0.175, val_acc:0.914]
Epoch [15/120    avg_loss:0.177, val_acc:0.878]
Epoch [16/120    avg_loss:0.171, val_acc:0.926]
Epoch [17/120    avg_loss:0.160, val_acc:0.901]
Epoch [18/120    avg_loss:0.151, val_acc:0.940]
Epoch [19/120    avg_loss:0.121, val_acc:0.944]
Epoch [20/120    avg_loss:0.121, val_acc:0.898]
Epoch [21/120    avg_loss:0.134, val_acc:0.902]
Epoch [22/120    avg_loss:0.146, val_acc:0.950]
Epoch [23/120    avg_loss:0.097, val_acc:0.942]
Epoch [24/120    avg_loss:0.124, val_acc:0.883]
Epoch [25/120    avg_loss:0.087, val_acc:0.938]
Epoch [26/120    avg_loss:0.124, val_acc:0.935]
Epoch [27/120    avg_loss:0.083, val_acc:0.968]
Epoch [28/120    avg_loss:0.082, val_acc:0.963]
Epoch [29/120    avg_loss:0.071, val_acc:0.964]
Epoch [30/120    avg_loss:0.185, val_acc:0.903]
Epoch [31/120    avg_loss:0.127, val_acc:0.958]
Epoch [32/120    avg_loss:0.099, val_acc:0.935]
Epoch [33/120    avg_loss:0.115, val_acc:0.954]
Epoch [34/120    avg_loss:0.079, val_acc:0.968]
Epoch [35/120    avg_loss:0.112, val_acc:0.946]
Epoch [36/120    avg_loss:0.087, val_acc:0.935]
Epoch [37/120    avg_loss:0.065, val_acc:0.967]
Epoch [38/120    avg_loss:0.051, val_acc:0.972]
Epoch [39/120    avg_loss:0.034, val_acc:0.957]
Epoch [40/120    avg_loss:0.051, val_acc:0.963]
Epoch [41/120    avg_loss:0.049, val_acc:0.962]
Epoch [42/120    avg_loss:0.095, val_acc:0.963]
Epoch [43/120    avg_loss:0.041, val_acc:0.978]
Epoch [44/120    avg_loss:0.033, val_acc:0.974]
Epoch [45/120    avg_loss:0.028, val_acc:0.950]
Epoch [46/120    avg_loss:0.037, val_acc:0.975]
Epoch [47/120    avg_loss:0.053, val_acc:0.973]
Epoch [48/120    avg_loss:0.020, val_acc:0.977]
Epoch [49/120    avg_loss:0.018, val_acc:0.971]
Epoch [50/120    avg_loss:0.031, val_acc:0.977]
Epoch [51/120    avg_loss:0.025, val_acc:0.960]
Epoch [52/120    avg_loss:0.017, val_acc:0.978]
Epoch [53/120    avg_loss:0.021, val_acc:0.977]
Epoch [54/120    avg_loss:0.019, val_acc:0.983]
Epoch [55/120    avg_loss:0.017, val_acc:0.985]
Epoch [56/120    avg_loss:0.016, val_acc:0.980]
Epoch [57/120    avg_loss:0.013, val_acc:0.985]
Epoch [58/120    avg_loss:0.198, val_acc:0.927]
Epoch [59/120    avg_loss:0.080, val_acc:0.921]
Epoch [60/120    avg_loss:0.063, val_acc:0.967]
Epoch [61/120    avg_loss:0.062, val_acc:0.961]
Epoch [62/120    avg_loss:0.047, val_acc:0.929]
Epoch [63/120    avg_loss:0.079, val_acc:0.932]
Epoch [64/120    avg_loss:0.035, val_acc:0.970]
Epoch [65/120    avg_loss:0.034, val_acc:0.977]
Epoch [66/120    avg_loss:0.019, val_acc:0.985]
Epoch [67/120    avg_loss:0.026, val_acc:0.982]
Epoch [68/120    avg_loss:0.021, val_acc:0.971]
Epoch [69/120    avg_loss:0.021, val_acc:0.985]
Epoch [70/120    avg_loss:0.016, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.984]
Epoch [72/120    avg_loss:0.015, val_acc:0.985]
Epoch [73/120    avg_loss:0.018, val_acc:0.973]
Epoch [74/120    avg_loss:0.011, val_acc:0.979]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.020, val_acc:0.979]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.010, val_acc:0.989]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.990]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.027, val_acc:0.985]
Epoch [91/120    avg_loss:0.032, val_acc:0.980]
Epoch [92/120    avg_loss:0.029, val_acc:0.985]
Epoch [93/120    avg_loss:0.056, val_acc:0.979]
Epoch [94/120    avg_loss:0.016, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     0    14     0    15     8     6     0]
 [    0     0 18060     0    21     0     2     0     6     1]
 [    0     2     0  2017     2     0     0     0    10     5]
 [    0    36    22     0  2879     0     0     0    35     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     8  1282     0     0]
 [    0     5     0     1    54     0     0     0  3507     4]
 [    0     0     0     0    15    39     0     0     0   865]]

Accuracy:
99.24806593883307

F1 scores:
[       nan 0.99331468 0.99856242 0.9950666  0.96659392 0.98489426
 0.99744402 0.99379845 0.98304135 0.9637883 ]

Kappa:
0.9900366932883498
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:146
Validation dataloader:146
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7cc49c8908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.781, val_acc:0.507]
Epoch [2/120    avg_loss:1.101, val_acc:0.640]
Epoch [3/120    avg_loss:0.727, val_acc:0.748]
Epoch [4/120    avg_loss:0.513, val_acc:0.817]
Epoch [5/120    avg_loss:0.437, val_acc:0.829]
Epoch [6/120    avg_loss:0.338, val_acc:0.873]
Epoch [7/120    avg_loss:0.327, val_acc:0.869]
Epoch [8/120    avg_loss:0.447, val_acc:0.856]
Epoch [9/120    avg_loss:0.278, val_acc:0.920]
Epoch [10/120    avg_loss:0.244, val_acc:0.871]
Epoch [11/120    avg_loss:0.232, val_acc:0.893]
Epoch [12/120    avg_loss:0.205, val_acc:0.878]
Epoch [13/120    avg_loss:0.214, val_acc:0.949]
Epoch [14/120    avg_loss:0.197, val_acc:0.896]
Epoch [15/120    avg_loss:0.201, val_acc:0.896]
Epoch [16/120    avg_loss:0.196, val_acc:0.902]
Epoch [17/120    avg_loss:0.198, val_acc:0.926]
Epoch [18/120    avg_loss:0.128, val_acc:0.938]
Epoch [19/120    avg_loss:0.134, val_acc:0.949]
Epoch [20/120    avg_loss:0.106, val_acc:0.942]
Epoch [21/120    avg_loss:0.209, val_acc:0.948]
Epoch [22/120    avg_loss:0.202, val_acc:0.914]
Epoch [23/120    avg_loss:0.163, val_acc:0.933]
Epoch [24/120    avg_loss:0.128, val_acc:0.946]
Epoch [25/120    avg_loss:0.128, val_acc:0.878]
Epoch [26/120    avg_loss:0.177, val_acc:0.949]
Epoch [27/120    avg_loss:0.094, val_acc:0.964]
Epoch [28/120    avg_loss:0.101, val_acc:0.959]
Epoch [29/120    avg_loss:0.130, val_acc:0.943]
Epoch [30/120    avg_loss:0.144, val_acc:0.952]
Epoch [31/120    avg_loss:0.100, val_acc:0.931]
Epoch [32/120    avg_loss:0.071, val_acc:0.955]
Epoch [33/120    avg_loss:0.092, val_acc:0.947]
Epoch [34/120    avg_loss:0.064, val_acc:0.967]
Epoch [35/120    avg_loss:0.098, val_acc:0.963]
Epoch [36/120    avg_loss:0.041, val_acc:0.957]
Epoch [37/120    avg_loss:0.043, val_acc:0.970]
Epoch [38/120    avg_loss:0.038, val_acc:0.961]
Epoch [39/120    avg_loss:0.041, val_acc:0.973]
Epoch [40/120    avg_loss:0.044, val_acc:0.970]
Epoch [41/120    avg_loss:0.036, val_acc:0.973]
Epoch [42/120    avg_loss:0.039, val_acc:0.967]
Epoch [43/120    avg_loss:0.090, val_acc:0.963]
Epoch [44/120    avg_loss:0.055, val_acc:0.958]
Epoch [45/120    avg_loss:0.041, val_acc:0.930]
Epoch [46/120    avg_loss:0.046, val_acc:0.968]
Epoch [47/120    avg_loss:0.027, val_acc:0.965]
Epoch [48/120    avg_loss:0.037, val_acc:0.969]
Epoch [49/120    avg_loss:0.024, val_acc:0.977]
Epoch [50/120    avg_loss:0.022, val_acc:0.979]
Epoch [51/120    avg_loss:0.032, val_acc:0.943]
Epoch [52/120    avg_loss:0.024, val_acc:0.973]
Epoch [53/120    avg_loss:0.022, val_acc:0.972]
Epoch [54/120    avg_loss:0.019, val_acc:0.982]
Epoch [55/120    avg_loss:0.040, val_acc:0.933]
Epoch [56/120    avg_loss:0.124, val_acc:0.970]
Epoch [57/120    avg_loss:0.035, val_acc:0.979]
Epoch [58/120    avg_loss:0.024, val_acc:0.970]
Epoch [59/120    avg_loss:0.038, val_acc:0.974]
Epoch [60/120    avg_loss:0.022, val_acc:0.983]
Epoch [61/120    avg_loss:0.022, val_acc:0.984]
Epoch [62/120    avg_loss:0.022, val_acc:0.982]
Epoch [63/120    avg_loss:0.017, val_acc:0.981]
Epoch [64/120    avg_loss:0.010, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.985]
Epoch [66/120    avg_loss:0.018, val_acc:0.979]
Epoch [67/120    avg_loss:0.015, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.196, val_acc:0.910]
Epoch [70/120    avg_loss:0.074, val_acc:0.965]
Epoch [71/120    avg_loss:0.052, val_acc:0.972]
Epoch [72/120    avg_loss:0.028, val_acc:0.962]
Epoch [73/120    avg_loss:0.021, val_acc:0.974]
Epoch [74/120    avg_loss:0.027, val_acc:0.975]
Epoch [75/120    avg_loss:0.018, val_acc:0.985]
Epoch [76/120    avg_loss:0.019, val_acc:0.969]
Epoch [77/120    avg_loss:0.025, val_acc:0.984]
Epoch [78/120    avg_loss:0.186, val_acc:0.898]
Epoch [79/120    avg_loss:0.092, val_acc:0.958]
Epoch [80/120    avg_loss:0.064, val_acc:0.960]
Epoch [81/120    avg_loss:0.057, val_acc:0.962]
Epoch [82/120    avg_loss:0.054, val_acc:0.966]
Epoch [83/120    avg_loss:0.049, val_acc:0.967]
Epoch [84/120    avg_loss:0.041, val_acc:0.968]
Epoch [85/120    avg_loss:0.037, val_acc:0.969]
Epoch [86/120    avg_loss:0.039, val_acc:0.971]
Epoch [87/120    avg_loss:0.033, val_acc:0.971]
Epoch [88/120    avg_loss:0.039, val_acc:0.972]
Epoch [89/120    avg_loss:0.034, val_acc:0.970]
Epoch [90/120    avg_loss:0.027, val_acc:0.973]
Epoch [91/120    avg_loss:0.028, val_acc:0.972]
Epoch [92/120    avg_loss:0.029, val_acc:0.972]
Epoch [93/120    avg_loss:0.028, val_acc:0.973]
Epoch [94/120    avg_loss:0.028, val_acc:0.973]
Epoch [95/120    avg_loss:0.033, val_acc:0.973]
Epoch [96/120    avg_loss:0.026, val_acc:0.973]
Epoch [97/120    avg_loss:0.034, val_acc:0.973]
Epoch [98/120    avg_loss:0.028, val_acc:0.973]
Epoch [99/120    avg_loss:0.035, val_acc:0.974]
Epoch [100/120    avg_loss:0.031, val_acc:0.974]
Epoch [101/120    avg_loss:0.023, val_acc:0.975]
Epoch [102/120    avg_loss:0.030, val_acc:0.975]
Epoch [103/120    avg_loss:0.028, val_acc:0.973]
Epoch [104/120    avg_loss:0.032, val_acc:0.974]
Epoch [105/120    avg_loss:0.028, val_acc:0.974]
Epoch [106/120    avg_loss:0.024, val_acc:0.974]
Epoch [107/120    avg_loss:0.030, val_acc:0.974]
Epoch [108/120    avg_loss:0.024, val_acc:0.974]
Epoch [109/120    avg_loss:0.026, val_acc:0.974]
Epoch [110/120    avg_loss:0.032, val_acc:0.974]
Epoch [111/120    avg_loss:0.025, val_acc:0.974]
Epoch [112/120    avg_loss:0.025, val_acc:0.974]
Epoch [113/120    avg_loss:0.028, val_acc:0.974]
Epoch [114/120    avg_loss:0.026, val_acc:0.974]
Epoch [115/120    avg_loss:0.025, val_acc:0.973]
Epoch [116/120    avg_loss:0.035, val_acc:0.974]
Epoch [117/120    avg_loss:0.029, val_acc:0.974]
Epoch [118/120    avg_loss:0.032, val_acc:0.974]
Epoch [119/120    avg_loss:0.024, val_acc:0.974]
Epoch [120/120    avg_loss:0.031, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6250     0     0    34     0    35    14    99     0]
 [    0     0 17956     0   133     0     1     0     0     0]
 [    0     5     0  2008     5     0     0     0     3    15]
 [    0    63    21     0  2851     0     7     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     3  1283     0     4]
 [    0     0     0    15   100     0     0     0  3453     3]
 [    0     0     0     1    19    41     0     1     1   856]]

Accuracy:
98.42624057069868

F1 scores:
[       nan 0.98039216 0.99570244 0.98916256 0.93261367 0.98453414
 0.99530708 0.99149923 0.96492944 0.95269894]

Kappa:
0.9791786666772407
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f944732e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.802, val_acc:0.629]
Epoch [2/120    avg_loss:1.221, val_acc:0.710]
Epoch [3/120    avg_loss:0.898, val_acc:0.652]
Epoch [4/120    avg_loss:0.644, val_acc:0.738]
Epoch [5/120    avg_loss:0.451, val_acc:0.821]
Epoch [6/120    avg_loss:0.382, val_acc:0.828]
Epoch [7/120    avg_loss:0.351, val_acc:0.850]
Epoch [8/120    avg_loss:0.284, val_acc:0.889]
Epoch [9/120    avg_loss:0.258, val_acc:0.775]
Epoch [10/120    avg_loss:0.221, val_acc:0.868]
Epoch [11/120    avg_loss:0.229, val_acc:0.800]
Epoch [12/120    avg_loss:0.187, val_acc:0.910]
Epoch [13/120    avg_loss:0.170, val_acc:0.914]
Epoch [14/120    avg_loss:0.177, val_acc:0.811]
Epoch [15/120    avg_loss:0.126, val_acc:0.954]
Epoch [16/120    avg_loss:0.134, val_acc:0.834]
Epoch [17/120    avg_loss:0.136, val_acc:0.886]
Epoch [18/120    avg_loss:0.131, val_acc:0.938]
Epoch [19/120    avg_loss:0.104, val_acc:0.965]
Epoch [20/120    avg_loss:0.118, val_acc:0.957]
Epoch [21/120    avg_loss:0.119, val_acc:0.919]
Epoch [22/120    avg_loss:0.111, val_acc:0.931]
Epoch [23/120    avg_loss:0.082, val_acc:0.958]
Epoch [24/120    avg_loss:0.110, val_acc:0.953]
Epoch [25/120    avg_loss:0.065, val_acc:0.972]
Epoch [26/120    avg_loss:0.060, val_acc:0.964]
Epoch [27/120    avg_loss:0.083, val_acc:0.960]
Epoch [28/120    avg_loss:0.068, val_acc:0.941]
Epoch [29/120    avg_loss:0.082, val_acc:0.972]
Epoch [30/120    avg_loss:0.064, val_acc:0.948]
Epoch [31/120    avg_loss:0.066, val_acc:0.967]
Epoch [32/120    avg_loss:0.067, val_acc:0.967]
Epoch [33/120    avg_loss:0.067, val_acc:0.965]
Epoch [34/120    avg_loss:0.063, val_acc:0.971]
Epoch [35/120    avg_loss:0.033, val_acc:0.928]
Epoch [36/120    avg_loss:0.053, val_acc:0.971]
Epoch [37/120    avg_loss:0.046, val_acc:0.923]
Epoch [38/120    avg_loss:0.049, val_acc:0.964]
Epoch [39/120    avg_loss:0.054, val_acc:0.918]
Epoch [40/120    avg_loss:0.043, val_acc:0.966]
Epoch [41/120    avg_loss:0.042, val_acc:0.984]
Epoch [42/120    avg_loss:0.018, val_acc:0.951]
Epoch [43/120    avg_loss:0.043, val_acc:0.979]
Epoch [44/120    avg_loss:0.043, val_acc:0.953]
Epoch [45/120    avg_loss:0.034, val_acc:0.973]
Epoch [46/120    avg_loss:0.022, val_acc:0.978]
Epoch [47/120    avg_loss:0.017, val_acc:0.951]
Epoch [48/120    avg_loss:0.017, val_acc:0.983]
Epoch [49/120    avg_loss:0.029, val_acc:0.972]
Epoch [50/120    avg_loss:0.048, val_acc:0.957]
Epoch [51/120    avg_loss:0.016, val_acc:0.962]
Epoch [52/120    avg_loss:0.016, val_acc:0.980]
Epoch [53/120    avg_loss:0.014, val_acc:0.965]
Epoch [54/120    avg_loss:0.019, val_acc:0.979]
Epoch [55/120    avg_loss:0.016, val_acc:0.980]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.984]
Epoch [58/120    avg_loss:0.007, val_acc:0.983]
Epoch [59/120    avg_loss:0.011, val_acc:0.981]
Epoch [60/120    avg_loss:0.007, val_acc:0.983]
Epoch [61/120    avg_loss:0.007, val_acc:0.984]
Epoch [62/120    avg_loss:0.007, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.983]
Epoch [64/120    avg_loss:0.009, val_acc:0.982]
Epoch [65/120    avg_loss:0.008, val_acc:0.982]
Epoch [66/120    avg_loss:0.008, val_acc:0.982]
Epoch [67/120    avg_loss:0.006, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.982]
Epoch [69/120    avg_loss:0.006, val_acc:0.982]
Epoch [70/120    avg_loss:0.007, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.982]
Epoch [72/120    avg_loss:0.007, val_acc:0.983]
Epoch [73/120    avg_loss:0.013, val_acc:0.981]
Epoch [74/120    avg_loss:0.007, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.010, val_acc:0.984]
Epoch [78/120    avg_loss:0.006, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.982]
Epoch [83/120    avg_loss:0.006, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.985]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     2     0     0     1    23    49     3]
 [    0     0 18065     0    10     0    14     0     1     0]
 [    0     2     0  1959     0     0     0     0    73     2]
 [    0    28     2     0  2928     0     9     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4868     0     0     0]
 [    0    23     0     0     0     0     0  1265     1     1]
 [    0     7     0    55    38     0     0     0  3470     1]
 [    0     0     0     0     0     9     0     0     0   910]]

Accuracy:
99.11069336996601

F1 scores:
[       nan 0.98925736 0.99897697 0.96692991 0.98453262 0.99656357
 0.99651996 0.98138092 0.9679219  0.9912854 ]

Kappa:
0.9882180013685923
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1635be898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.702, val_acc:0.577]
Epoch [2/120    avg_loss:1.223, val_acc:0.683]
Epoch [3/120    avg_loss:0.925, val_acc:0.683]
Epoch [4/120    avg_loss:0.694, val_acc:0.609]
Epoch [5/120    avg_loss:0.512, val_acc:0.775]
Epoch [6/120    avg_loss:0.418, val_acc:0.798]
Epoch [7/120    avg_loss:0.318, val_acc:0.815]
Epoch [8/120    avg_loss:0.345, val_acc:0.861]
Epoch [9/120    avg_loss:0.272, val_acc:0.884]
Epoch [10/120    avg_loss:0.262, val_acc:0.766]
Epoch [11/120    avg_loss:0.275, val_acc:0.903]
Epoch [12/120    avg_loss:0.246, val_acc:0.875]
Epoch [13/120    avg_loss:0.191, val_acc:0.907]
Epoch [14/120    avg_loss:0.144, val_acc:0.864]
Epoch [15/120    avg_loss:0.191, val_acc:0.938]
Epoch [16/120    avg_loss:0.143, val_acc:0.936]
Epoch [17/120    avg_loss:0.157, val_acc:0.942]
Epoch [18/120    avg_loss:0.109, val_acc:0.953]
Epoch [19/120    avg_loss:0.124, val_acc:0.940]
Epoch [20/120    avg_loss:0.100, val_acc:0.948]
Epoch [21/120    avg_loss:0.098, val_acc:0.873]
Epoch [22/120    avg_loss:0.133, val_acc:0.963]
Epoch [23/120    avg_loss:0.067, val_acc:0.955]
Epoch [24/120    avg_loss:0.057, val_acc:0.956]
Epoch [25/120    avg_loss:0.065, val_acc:0.965]
Epoch [26/120    avg_loss:0.117, val_acc:0.930]
Epoch [27/120    avg_loss:0.110, val_acc:0.950]
Epoch [28/120    avg_loss:0.067, val_acc:0.959]
Epoch [29/120    avg_loss:0.117, val_acc:0.938]
Epoch [30/120    avg_loss:0.092, val_acc:0.906]
Epoch [31/120    avg_loss:0.116, val_acc:0.947]
Epoch [32/120    avg_loss:0.069, val_acc:0.952]
Epoch [33/120    avg_loss:0.069, val_acc:0.963]
Epoch [34/120    avg_loss:0.048, val_acc:0.962]
Epoch [35/120    avg_loss:0.125, val_acc:0.913]
Epoch [36/120    avg_loss:0.106, val_acc:0.947]
Epoch [37/120    avg_loss:0.074, val_acc:0.954]
Epoch [38/120    avg_loss:0.051, val_acc:0.962]
Epoch [39/120    avg_loss:0.049, val_acc:0.967]
Epoch [40/120    avg_loss:0.039, val_acc:0.973]
Epoch [41/120    avg_loss:0.040, val_acc:0.970]
Epoch [42/120    avg_loss:0.031, val_acc:0.971]
Epoch [43/120    avg_loss:0.032, val_acc:0.969]
Epoch [44/120    avg_loss:0.025, val_acc:0.969]
Epoch [45/120    avg_loss:0.025, val_acc:0.971]
Epoch [46/120    avg_loss:0.029, val_acc:0.968]
Epoch [47/120    avg_loss:0.028, val_acc:0.970]
Epoch [48/120    avg_loss:0.022, val_acc:0.971]
Epoch [49/120    avg_loss:0.026, val_acc:0.971]
Epoch [50/120    avg_loss:0.031, val_acc:0.974]
Epoch [51/120    avg_loss:0.029, val_acc:0.971]
Epoch [52/120    avg_loss:0.024, val_acc:0.976]
Epoch [53/120    avg_loss:0.023, val_acc:0.977]
Epoch [54/120    avg_loss:0.020, val_acc:0.976]
Epoch [55/120    avg_loss:0.022, val_acc:0.975]
Epoch [56/120    avg_loss:0.024, val_acc:0.978]
Epoch [57/120    avg_loss:0.022, val_acc:0.976]
Epoch [58/120    avg_loss:0.023, val_acc:0.978]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.024, val_acc:0.971]
Epoch [61/120    avg_loss:0.022, val_acc:0.972]
Epoch [62/120    avg_loss:0.020, val_acc:0.972]
Epoch [63/120    avg_loss:0.019, val_acc:0.978]
Epoch [64/120    avg_loss:0.023, val_acc:0.975]
Epoch [65/120    avg_loss:0.028, val_acc:0.977]
Epoch [66/120    avg_loss:0.022, val_acc:0.976]
Epoch [67/120    avg_loss:0.019, val_acc:0.979]
Epoch [68/120    avg_loss:0.024, val_acc:0.979]
Epoch [69/120    avg_loss:0.022, val_acc:0.979]
Epoch [70/120    avg_loss:0.020, val_acc:0.978]
Epoch [71/120    avg_loss:0.022, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.019, val_acc:0.978]
Epoch [74/120    avg_loss:0.018, val_acc:0.979]
Epoch [75/120    avg_loss:0.021, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.980]
Epoch [77/120    avg_loss:0.020, val_acc:0.979]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.024, val_acc:0.979]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.016, val_acc:0.978]
Epoch [82/120    avg_loss:0.017, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.980]
Epoch [84/120    avg_loss:0.021, val_acc:0.980]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.983]
Epoch [87/120    avg_loss:0.018, val_acc:0.982]
Epoch [88/120    avg_loss:0.015, val_acc:0.980]
Epoch [89/120    avg_loss:0.018, val_acc:0.980]
Epoch [90/120    avg_loss:0.017, val_acc:0.979]
Epoch [91/120    avg_loss:0.017, val_acc:0.982]
Epoch [92/120    avg_loss:0.016, val_acc:0.982]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.015, val_acc:0.982]
Epoch [95/120    avg_loss:0.019, val_acc:0.982]
Epoch [96/120    avg_loss:0.017, val_acc:0.979]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.017, val_acc:0.981]
Epoch [99/120    avg_loss:0.016, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.980]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.015, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.015, val_acc:0.981]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.015, val_acc:0.982]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.984]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.021, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6305     0     5     0     0     7    47    68     0]
 [    0     0 18065     0    13     0    11     0     1     0]
 [    0     7     0  1921     0     0     0     0   106     2]
 [    0    26     4     2  2935     0     2     1     0     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     0     0  4828     0    36     0]
 [    0    46     0     0     0     0     0  1243     1     0]
 [    0    33     0    59    50     0     8     0  3421     0]
 [    0     0     0     0     0    10     0     0     0   909]]

Accuracy:
98.64796471694021

F1 scores:
[       nan 0.98139933 0.99881127 0.9550087  0.98324958 0.99618321
 0.99198685 0.96319256 0.94975014 0.99235808]

Kappa:
0.9820864073367246
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a6fadba20>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.781, val_acc:0.581]
Epoch [2/120    avg_loss:1.235, val_acc:0.721]
Epoch [3/120    avg_loss:0.892, val_acc:0.734]
Epoch [4/120    avg_loss:0.714, val_acc:0.672]
Epoch [5/120    avg_loss:0.581, val_acc:0.782]
Epoch [6/120    avg_loss:0.510, val_acc:0.831]
Epoch [7/120    avg_loss:0.416, val_acc:0.813]
Epoch [8/120    avg_loss:0.441, val_acc:0.801]
Epoch [9/120    avg_loss:0.356, val_acc:0.816]
Epoch [10/120    avg_loss:0.303, val_acc:0.836]
Epoch [11/120    avg_loss:0.292, val_acc:0.878]
Epoch [12/120    avg_loss:0.275, val_acc:0.887]
Epoch [13/120    avg_loss:0.226, val_acc:0.910]
Epoch [14/120    avg_loss:0.210, val_acc:0.876]
Epoch [15/120    avg_loss:0.208, val_acc:0.916]
Epoch [16/120    avg_loss:0.200, val_acc:0.917]
Epoch [17/120    avg_loss:0.222, val_acc:0.888]
Epoch [18/120    avg_loss:0.208, val_acc:0.807]
Epoch [19/120    avg_loss:0.188, val_acc:0.900]
Epoch [20/120    avg_loss:0.178, val_acc:0.921]
Epoch [21/120    avg_loss:0.175, val_acc:0.919]
Epoch [22/120    avg_loss:0.201, val_acc:0.892]
Epoch [23/120    avg_loss:0.143, val_acc:0.927]
Epoch [24/120    avg_loss:0.132, val_acc:0.942]
Epoch [25/120    avg_loss:0.118, val_acc:0.956]
Epoch [26/120    avg_loss:0.165, val_acc:0.880]
Epoch [27/120    avg_loss:0.103, val_acc:0.954]
Epoch [28/120    avg_loss:0.093, val_acc:0.897]
Epoch [29/120    avg_loss:0.085, val_acc:0.939]
Epoch [30/120    avg_loss:0.088, val_acc:0.931]
Epoch [31/120    avg_loss:0.079, val_acc:0.943]
Epoch [32/120    avg_loss:0.080, val_acc:0.955]
Epoch [33/120    avg_loss:0.065, val_acc:0.938]
Epoch [34/120    avg_loss:0.123, val_acc:0.951]
Epoch [35/120    avg_loss:0.069, val_acc:0.949]
Epoch [36/120    avg_loss:0.055, val_acc:0.958]
Epoch [37/120    avg_loss:0.040, val_acc:0.957]
Epoch [38/120    avg_loss:0.064, val_acc:0.968]
Epoch [39/120    avg_loss:0.042, val_acc:0.944]
Epoch [40/120    avg_loss:0.047, val_acc:0.967]
Epoch [41/120    avg_loss:0.044, val_acc:0.964]
Epoch [42/120    avg_loss:0.044, val_acc:0.960]
Epoch [43/120    avg_loss:0.052, val_acc:0.963]
Epoch [44/120    avg_loss:0.044, val_acc:0.973]
Epoch [45/120    avg_loss:0.258, val_acc:0.934]
Epoch [46/120    avg_loss:0.096, val_acc:0.942]
Epoch [47/120    avg_loss:0.069, val_acc:0.929]
Epoch [48/120    avg_loss:0.051, val_acc:0.958]
Epoch [49/120    avg_loss:0.044, val_acc:0.959]
Epoch [50/120    avg_loss:0.052, val_acc:0.970]
Epoch [51/120    avg_loss:0.047, val_acc:0.973]
Epoch [52/120    avg_loss:0.046, val_acc:0.956]
Epoch [53/120    avg_loss:0.040, val_acc:0.938]
Epoch [54/120    avg_loss:0.030, val_acc:0.971]
Epoch [55/120    avg_loss:0.033, val_acc:0.973]
Epoch [56/120    avg_loss:0.051, val_acc:0.970]
Epoch [57/120    avg_loss:0.033, val_acc:0.973]
Epoch [58/120    avg_loss:0.029, val_acc:0.973]
Epoch [59/120    avg_loss:0.033, val_acc:0.963]
Epoch [60/120    avg_loss:0.022, val_acc:0.975]
Epoch [61/120    avg_loss:0.033, val_acc:0.972]
Epoch [62/120    avg_loss:0.026, val_acc:0.976]
Epoch [63/120    avg_loss:0.036, val_acc:0.977]
Epoch [64/120    avg_loss:0.017, val_acc:0.977]
Epoch [65/120    avg_loss:0.018, val_acc:0.978]
Epoch [66/120    avg_loss:0.018, val_acc:0.981]
Epoch [67/120    avg_loss:0.023, val_acc:0.968]
Epoch [68/120    avg_loss:0.022, val_acc:0.968]
Epoch [69/120    avg_loss:0.015, val_acc:0.980]
Epoch [70/120    avg_loss:0.023, val_acc:0.976]
Epoch [71/120    avg_loss:0.017, val_acc:0.974]
Epoch [72/120    avg_loss:0.014, val_acc:0.972]
Epoch [73/120    avg_loss:0.017, val_acc:0.983]
Epoch [74/120    avg_loss:0.022, val_acc:0.963]
Epoch [75/120    avg_loss:0.018, val_acc:0.980]
Epoch [76/120    avg_loss:0.022, val_acc:0.966]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.017, val_acc:0.966]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.022, val_acc:0.983]
Epoch [81/120    avg_loss:0.064, val_acc:0.965]
Epoch [82/120    avg_loss:0.052, val_acc:0.978]
Epoch [83/120    avg_loss:0.027, val_acc:0.977]
Epoch [84/120    avg_loss:0.022, val_acc:0.975]
Epoch [85/120    avg_loss:0.027, val_acc:0.978]
Epoch [86/120    avg_loss:0.019, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.978]
Epoch [89/120    avg_loss:0.030, val_acc:0.978]
Epoch [90/120    avg_loss:0.032, val_acc:0.962]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.021, val_acc:0.958]
Epoch [93/120    avg_loss:0.025, val_acc:0.974]
Epoch [94/120    avg_loss:0.023, val_acc:0.975]
Epoch [95/120    avg_loss:0.015, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.013, val_acc:0.974]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.983]
Epoch [100/120    avg_loss:0.014, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.015, val_acc:0.978]
Epoch [103/120    avg_loss:0.015, val_acc:0.978]
Epoch [104/120    avg_loss:0.015, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.972]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.018, val_acc:0.948]
Epoch [110/120    avg_loss:0.026, val_acc:0.959]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.979]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     1     0     0    10     4    42     0]
 [    0     0 18058     0    30     0     1     0     1     0]
 [    0     0     0  1994     0     0     0     0    41     1]
 [    0    22     5     0  2935     0     4     0     3     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    38     3     7     0  4830     0     0     0]
 [    0    12     0     0     0     0     0  1278     0     0]
 [    0    16     1    36    38     0    12     0  3468     0]
 [    0     0     0     0     2    14     0     0     0   903]]

Accuracy:
99.16371436145856

F1 scores:
[       nan 0.99167769 0.99790009 0.97985258 0.9809492  0.99466463
 0.99229584 0.99377916 0.97333708 0.9890471 ]

Kappa:
0.9889176506857366
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b5367b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.725, val_acc:0.616]
Epoch [2/120    avg_loss:1.244, val_acc:0.718]
Epoch [3/120    avg_loss:0.921, val_acc:0.738]
Epoch [4/120    avg_loss:0.703, val_acc:0.686]
Epoch [5/120    avg_loss:0.540, val_acc:0.764]
Epoch [6/120    avg_loss:0.479, val_acc:0.757]
Epoch [7/120    avg_loss:0.416, val_acc:0.719]
Epoch [8/120    avg_loss:0.402, val_acc:0.790]
Epoch [9/120    avg_loss:0.403, val_acc:0.769]
Epoch [10/120    avg_loss:0.357, val_acc:0.799]
Epoch [11/120    avg_loss:0.310, val_acc:0.790]
Epoch [12/120    avg_loss:0.330, val_acc:0.747]
Epoch [13/120    avg_loss:0.298, val_acc:0.833]
Epoch [14/120    avg_loss:0.253, val_acc:0.853]
Epoch [15/120    avg_loss:0.225, val_acc:0.857]
Epoch [16/120    avg_loss:0.203, val_acc:0.837]
Epoch [17/120    avg_loss:0.210, val_acc:0.840]
Epoch [18/120    avg_loss:0.188, val_acc:0.905]
Epoch [19/120    avg_loss:0.178, val_acc:0.909]
Epoch [20/120    avg_loss:0.164, val_acc:0.895]
Epoch [21/120    avg_loss:0.146, val_acc:0.909]
Epoch [22/120    avg_loss:0.119, val_acc:0.927]
Epoch [23/120    avg_loss:0.155, val_acc:0.922]
Epoch [24/120    avg_loss:0.133, val_acc:0.928]
Epoch [25/120    avg_loss:0.119, val_acc:0.933]
Epoch [26/120    avg_loss:0.103, val_acc:0.934]
Epoch [27/120    avg_loss:0.109, val_acc:0.925]
Epoch [28/120    avg_loss:0.125, val_acc:0.900]
Epoch [29/120    avg_loss:0.098, val_acc:0.953]
Epoch [30/120    avg_loss:0.100, val_acc:0.937]
Epoch [31/120    avg_loss:0.117, val_acc:0.953]
Epoch [32/120    avg_loss:0.077, val_acc:0.950]
Epoch [33/120    avg_loss:0.091, val_acc:0.953]
Epoch [34/120    avg_loss:0.061, val_acc:0.951]
Epoch [35/120    avg_loss:0.065, val_acc:0.952]
Epoch [36/120    avg_loss:0.063, val_acc:0.953]
Epoch [37/120    avg_loss:0.074, val_acc:0.899]
Epoch [38/120    avg_loss:0.061, val_acc:0.946]
Epoch [39/120    avg_loss:0.087, val_acc:0.953]
Epoch [40/120    avg_loss:0.056, val_acc:0.956]
Epoch [41/120    avg_loss:0.062, val_acc:0.967]
Epoch [42/120    avg_loss:0.080, val_acc:0.937]
Epoch [43/120    avg_loss:0.071, val_acc:0.953]
Epoch [44/120    avg_loss:0.049, val_acc:0.959]
Epoch [45/120    avg_loss:0.038, val_acc:0.958]
Epoch [46/120    avg_loss:0.029, val_acc:0.966]
Epoch [47/120    avg_loss:0.040, val_acc:0.963]
Epoch [48/120    avg_loss:0.042, val_acc:0.960]
Epoch [49/120    avg_loss:0.027, val_acc:0.963]
Epoch [50/120    avg_loss:0.050, val_acc:0.952]
Epoch [51/120    avg_loss:0.084, val_acc:0.938]
Epoch [52/120    avg_loss:0.063, val_acc:0.945]
Epoch [53/120    avg_loss:0.034, val_acc:0.964]
Epoch [54/120    avg_loss:0.042, val_acc:0.962]
Epoch [55/120    avg_loss:0.028, val_acc:0.967]
Epoch [56/120    avg_loss:0.022, val_acc:0.968]
Epoch [57/120    avg_loss:0.022, val_acc:0.969]
Epoch [58/120    avg_loss:0.018, val_acc:0.972]
Epoch [59/120    avg_loss:0.024, val_acc:0.975]
Epoch [60/120    avg_loss:0.017, val_acc:0.975]
Epoch [61/120    avg_loss:0.019, val_acc:0.977]
Epoch [62/120    avg_loss:0.019, val_acc:0.973]
Epoch [63/120    avg_loss:0.018, val_acc:0.976]
Epoch [64/120    avg_loss:0.019, val_acc:0.976]
Epoch [65/120    avg_loss:0.016, val_acc:0.975]
Epoch [66/120    avg_loss:0.018, val_acc:0.973]
Epoch [67/120    avg_loss:0.015, val_acc:0.976]
Epoch [68/120    avg_loss:0.017, val_acc:0.977]
Epoch [69/120    avg_loss:0.019, val_acc:0.975]
Epoch [70/120    avg_loss:0.015, val_acc:0.976]
Epoch [71/120    avg_loss:0.018, val_acc:0.975]
Epoch [72/120    avg_loss:0.017, val_acc:0.977]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.012, val_acc:0.977]
Epoch [76/120    avg_loss:0.014, val_acc:0.977]
Epoch [77/120    avg_loss:0.017, val_acc:0.977]
Epoch [78/120    avg_loss:0.016, val_acc:0.978]
Epoch [79/120    avg_loss:0.015, val_acc:0.978]
Epoch [80/120    avg_loss:0.014, val_acc:0.977]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.016, val_acc:0.978]
Epoch [83/120    avg_loss:0.017, val_acc:0.978]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.011, val_acc:0.978]
Epoch [86/120    avg_loss:0.012, val_acc:0.977]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.021, val_acc:0.976]
Epoch [89/120    avg_loss:0.016, val_acc:0.978]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.013, val_acc:0.978]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.976]
Epoch [97/120    avg_loss:0.013, val_acc:0.977]
Epoch [98/120    avg_loss:0.017, val_acc:0.978]
Epoch [99/120    avg_loss:0.014, val_acc:0.978]
Epoch [100/120    avg_loss:0.016, val_acc:0.978]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.012, val_acc:0.978]
Epoch [104/120    avg_loss:0.014, val_acc:0.976]
Epoch [105/120    avg_loss:0.012, val_acc:0.976]
Epoch [106/120    avg_loss:0.012, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.977]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.009, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.014, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.980]
Epoch [116/120    avg_loss:0.013, val_acc:0.976]
Epoch [117/120    avg_loss:0.015, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.979]
Epoch [119/120    avg_loss:0.011, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6275     0     0     0     0     5    28   124     0]
 [    0     1 17744     0   112     0   233     0     0     0]
 [    0    10     0  1906     0     0     0     0   119     1]
 [    0    23     4     0  2922     0    11     0     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4867     0     9     0]
 [    0     9     0     0     0     0     0  1276     5     0]
 [    0    20     0    93     2     0     4     0  3452     0]
 [    0     0     0     0     7    12     0     0     0   900]]

Accuracy:
97.96110187260501

F1 scores:
[       nan 0.98277212 0.99017857 0.94473358 0.97157107 0.99542334
 0.97359472 0.98380879 0.94718068 0.98738343]

Kappa:
0.9730847510471977
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa378d97860>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.775, val_acc:0.274]
Epoch [2/120    avg_loss:1.255, val_acc:0.391]
Epoch [3/120    avg_loss:0.968, val_acc:0.680]
Epoch [4/120    avg_loss:0.733, val_acc:0.627]
Epoch [5/120    avg_loss:0.553, val_acc:0.692]
Epoch [6/120    avg_loss:0.498, val_acc:0.725]
Epoch [7/120    avg_loss:0.465, val_acc:0.757]
Epoch [8/120    avg_loss:0.389, val_acc:0.803]
Epoch [9/120    avg_loss:0.326, val_acc:0.757]
Epoch [10/120    avg_loss:0.327, val_acc:0.789]
Epoch [11/120    avg_loss:0.310, val_acc:0.821]
Epoch [12/120    avg_loss:0.262, val_acc:0.860]
Epoch [13/120    avg_loss:0.290, val_acc:0.850]
Epoch [14/120    avg_loss:0.299, val_acc:0.848]
Epoch [15/120    avg_loss:0.228, val_acc:0.860]
Epoch [16/120    avg_loss:0.214, val_acc:0.910]
Epoch [17/120    avg_loss:0.213, val_acc:0.818]
Epoch [18/120    avg_loss:0.180, val_acc:0.924]
Epoch [19/120    avg_loss:0.140, val_acc:0.859]
Epoch [20/120    avg_loss:0.135, val_acc:0.938]
Epoch [21/120    avg_loss:0.127, val_acc:0.944]
Epoch [22/120    avg_loss:0.135, val_acc:0.921]
Epoch [23/120    avg_loss:0.155, val_acc:0.933]
Epoch [24/120    avg_loss:0.122, val_acc:0.920]
Epoch [25/120    avg_loss:0.248, val_acc:0.785]
Epoch [26/120    avg_loss:0.281, val_acc:0.921]
Epoch [27/120    avg_loss:0.152, val_acc:0.946]
Epoch [28/120    avg_loss:0.117, val_acc:0.943]
Epoch [29/120    avg_loss:0.111, val_acc:0.947]
Epoch [30/120    avg_loss:0.083, val_acc:0.951]
Epoch [31/120    avg_loss:0.119, val_acc:0.934]
Epoch [32/120    avg_loss:0.121, val_acc:0.938]
Epoch [33/120    avg_loss:0.073, val_acc:0.931]
Epoch [34/120    avg_loss:0.104, val_acc:0.919]
Epoch [35/120    avg_loss:0.064, val_acc:0.959]
Epoch [36/120    avg_loss:0.044, val_acc:0.961]
Epoch [37/120    avg_loss:0.064, val_acc:0.968]
Epoch [38/120    avg_loss:0.053, val_acc:0.954]
Epoch [39/120    avg_loss:0.047, val_acc:0.964]
Epoch [40/120    avg_loss:0.072, val_acc:0.934]
Epoch [41/120    avg_loss:0.056, val_acc:0.964]
Epoch [42/120    avg_loss:0.060, val_acc:0.970]
Epoch [43/120    avg_loss:0.053, val_acc:0.965]
Epoch [44/120    avg_loss:0.066, val_acc:0.933]
Epoch [45/120    avg_loss:0.055, val_acc:0.964]
Epoch [46/120    avg_loss:0.081, val_acc:0.945]
Epoch [47/120    avg_loss:0.085, val_acc:0.963]
Epoch [48/120    avg_loss:0.039, val_acc:0.969]
Epoch [49/120    avg_loss:0.047, val_acc:0.961]
Epoch [50/120    avg_loss:0.046, val_acc:0.963]
Epoch [51/120    avg_loss:0.037, val_acc:0.971]
Epoch [52/120    avg_loss:0.032, val_acc:0.975]
Epoch [53/120    avg_loss:0.037, val_acc:0.956]
Epoch [54/120    avg_loss:0.029, val_acc:0.978]
Epoch [55/120    avg_loss:0.032, val_acc:0.969]
Epoch [56/120    avg_loss:0.039, val_acc:0.975]
Epoch [57/120    avg_loss:0.022, val_acc:0.972]
Epoch [58/120    avg_loss:0.019, val_acc:0.975]
Epoch [59/120    avg_loss:0.020, val_acc:0.974]
Epoch [60/120    avg_loss:0.021, val_acc:0.962]
Epoch [61/120    avg_loss:0.036, val_acc:0.966]
Epoch [62/120    avg_loss:0.030, val_acc:0.956]
Epoch [63/120    avg_loss:0.027, val_acc:0.965]
Epoch [64/120    avg_loss:0.016, val_acc:0.979]
Epoch [65/120    avg_loss:0.022, val_acc:0.975]
Epoch [66/120    avg_loss:0.018, val_acc:0.974]
Epoch [67/120    avg_loss:0.041, val_acc:0.965]
Epoch [68/120    avg_loss:0.052, val_acc:0.922]
Epoch [69/120    avg_loss:0.050, val_acc:0.965]
Epoch [70/120    avg_loss:0.032, val_acc:0.960]
Epoch [71/120    avg_loss:0.023, val_acc:0.979]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.009, val_acc:0.978]
Epoch [74/120    avg_loss:0.040, val_acc:0.975]
Epoch [75/120    avg_loss:0.029, val_acc:0.975]
Epoch [76/120    avg_loss:0.026, val_acc:0.975]
Epoch [77/120    avg_loss:0.023, val_acc:0.979]
Epoch [78/120    avg_loss:0.012, val_acc:0.979]
Epoch [79/120    avg_loss:0.021, val_acc:0.967]
Epoch [80/120    avg_loss:0.012, val_acc:0.974]
Epoch [81/120    avg_loss:0.007, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.977]
Epoch [83/120    avg_loss:0.010, val_acc:0.975]
Epoch [84/120    avg_loss:0.007, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.018, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.015, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.013, val_acc:0.967]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.978]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.042, val_acc:0.978]
Epoch [97/120    avg_loss:0.024, val_acc:0.970]
Epoch [98/120    avg_loss:0.016, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.018, val_acc:0.960]
Epoch [102/120    avg_loss:0.070, val_acc:0.975]
Epoch [103/120    avg_loss:0.024, val_acc:0.975]
Epoch [104/120    avg_loss:0.028, val_acc:0.976]
Epoch [105/120    avg_loss:0.018, val_acc:0.980]
Epoch [106/120    avg_loss:0.018, val_acc:0.979]
Epoch [107/120    avg_loss:0.020, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.014, val_acc:0.980]
Epoch [111/120    avg_loss:0.013, val_acc:0.981]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.012, val_acc:0.982]
Epoch [114/120    avg_loss:0.017, val_acc:0.980]
Epoch [115/120    avg_loss:0.013, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.009, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6304     0    43     0     0     0    30    52     3]
 [    0     0 17974     0    27     0    88     0     1     0]
 [    0     7     0  1912     0     0     0     0   117     0]
 [    0    12     7     0  2934     0     3     9     4     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    17     0     0     0  4858     0     3     0]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    18     0    60    40     0     0     0  3453     0]
 [    0     0     0     0     2    29     0     0     0   888]]

Accuracy:
98.60217386065119

F1 scores:
[       nan 0.98669588 0.99612059 0.94396445 0.98209205 0.98901099
 0.98870459 0.98316756 0.95903347 0.97959184]

Kappa:
0.9815000613133898
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c127218d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.823, val_acc:0.275]
Epoch [2/120    avg_loss:1.281, val_acc:0.671]
Epoch [3/120    avg_loss:0.967, val_acc:0.740]
Epoch [4/120    avg_loss:0.828, val_acc:0.737]
Epoch [5/120    avg_loss:0.681, val_acc:0.725]
Epoch [6/120    avg_loss:0.578, val_acc:0.785]
Epoch [7/120    avg_loss:0.490, val_acc:0.753]
Epoch [8/120    avg_loss:0.403, val_acc:0.789]
Epoch [9/120    avg_loss:0.351, val_acc:0.801]
Epoch [10/120    avg_loss:0.308, val_acc:0.791]
Epoch [11/120    avg_loss:0.272, val_acc:0.877]
Epoch [12/120    avg_loss:0.236, val_acc:0.817]
Epoch [13/120    avg_loss:0.242, val_acc:0.877]
Epoch [14/120    avg_loss:0.196, val_acc:0.864]
Epoch [15/120    avg_loss:0.210, val_acc:0.881]
Epoch [16/120    avg_loss:0.184, val_acc:0.927]
Epoch [17/120    avg_loss:0.191, val_acc:0.916]
Epoch [18/120    avg_loss:0.151, val_acc:0.899]
Epoch [19/120    avg_loss:0.151, val_acc:0.912]
Epoch [20/120    avg_loss:0.126, val_acc:0.939]
Epoch [21/120    avg_loss:0.122, val_acc:0.939]
Epoch [22/120    avg_loss:0.131, val_acc:0.946]
Epoch [23/120    avg_loss:0.135, val_acc:0.864]
Epoch [24/120    avg_loss:0.116, val_acc:0.891]
Epoch [25/120    avg_loss:0.103, val_acc:0.944]
Epoch [26/120    avg_loss:0.098, val_acc:0.949]
Epoch [27/120    avg_loss:0.083, val_acc:0.947]
Epoch [28/120    avg_loss:0.094, val_acc:0.924]
Epoch [29/120    avg_loss:0.059, val_acc:0.938]
Epoch [30/120    avg_loss:0.110, val_acc:0.946]
Epoch [31/120    avg_loss:0.095, val_acc:0.951]
Epoch [32/120    avg_loss:0.068, val_acc:0.961]
Epoch [33/120    avg_loss:0.060, val_acc:0.972]
Epoch [34/120    avg_loss:0.044, val_acc:0.971]
Epoch [35/120    avg_loss:0.066, val_acc:0.932]
Epoch [36/120    avg_loss:0.055, val_acc:0.968]
Epoch [37/120    avg_loss:0.077, val_acc:0.882]
Epoch [38/120    avg_loss:0.090, val_acc:0.950]
Epoch [39/120    avg_loss:0.072, val_acc:0.925]
Epoch [40/120    avg_loss:0.088, val_acc:0.963]
Epoch [41/120    avg_loss:0.046, val_acc:0.970]
Epoch [42/120    avg_loss:0.033, val_acc:0.965]
Epoch [43/120    avg_loss:0.056, val_acc:0.955]
Epoch [44/120    avg_loss:0.033, val_acc:0.976]
Epoch [45/120    avg_loss:0.038, val_acc:0.942]
Epoch [46/120    avg_loss:0.034, val_acc:0.951]
Epoch [47/120    avg_loss:0.027, val_acc:0.967]
Epoch [48/120    avg_loss:0.027, val_acc:0.962]
Epoch [49/120    avg_loss:0.056, val_acc:0.969]
Epoch [50/120    avg_loss:0.051, val_acc:0.952]
Epoch [51/120    avg_loss:0.036, val_acc:0.981]
Epoch [52/120    avg_loss:0.025, val_acc:0.979]
Epoch [53/120    avg_loss:0.023, val_acc:0.966]
Epoch [54/120    avg_loss:0.022, val_acc:0.985]
Epoch [55/120    avg_loss:0.021, val_acc:0.964]
Epoch [56/120    avg_loss:0.021, val_acc:0.975]
Epoch [57/120    avg_loss:0.031, val_acc:0.972]
Epoch [58/120    avg_loss:0.027, val_acc:0.975]
Epoch [59/120    avg_loss:0.026, val_acc:0.976]
Epoch [60/120    avg_loss:0.020, val_acc:0.965]
Epoch [61/120    avg_loss:0.045, val_acc:0.961]
Epoch [62/120    avg_loss:0.032, val_acc:0.949]
Epoch [63/120    avg_loss:0.073, val_acc:0.965]
Epoch [64/120    avg_loss:0.041, val_acc:0.976]
Epoch [65/120    avg_loss:0.038, val_acc:0.947]
Epoch [66/120    avg_loss:0.053, val_acc:0.920]
Epoch [67/120    avg_loss:0.050, val_acc:0.965]
Epoch [68/120    avg_loss:0.026, val_acc:0.972]
Epoch [69/120    avg_loss:0.019, val_acc:0.972]
Epoch [70/120    avg_loss:0.016, val_acc:0.975]
Epoch [71/120    avg_loss:0.021, val_acc:0.976]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.013, val_acc:0.977]
Epoch [74/120    avg_loss:0.018, val_acc:0.976]
Epoch [75/120    avg_loss:0.015, val_acc:0.975]
Epoch [76/120    avg_loss:0.014, val_acc:0.976]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.017, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.013, val_acc:0.983]
Epoch [81/120    avg_loss:0.008, val_acc:0.983]
Epoch [82/120    avg_loss:0.015, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.015, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.985]
Epoch [93/120    avg_loss:0.017, val_acc:0.985]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.014, val_acc:0.985]
Epoch [103/120    avg_loss:0.016, val_acc:0.985]
Epoch [104/120    avg_loss:0.016, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.010, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.012, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6327     0     0     0     0     0     1   102     2]
 [    0     0 17911     0    34     0   142     0     3     0]
 [    0    10     0  1909     0     0     0     0   117     0]
 [    0    17     8     0  2930     0    10     2     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    13     0     0     0  4864     0     1     0]
 [    0    25     0     0     0     0     0  1264     1     0]
 [    0    43     0    33    40     0     0     0  3454     1]
 [    0     0     0     0     8     7     0     0     0   904]]

Accuracy:
98.49372183259827

F1 scores:
[       nan 0.98444064 0.99444784 0.95977878 0.97927807 0.99732518
 0.98322215 0.98865858 0.95269618 0.98851832]

Kappa:
0.9800709893099074
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba5b106898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.694, val_acc:0.584]
Epoch [2/120    avg_loss:1.224, val_acc:0.686]
Epoch [3/120    avg_loss:0.995, val_acc:0.494]
Epoch [4/120    avg_loss:0.738, val_acc:0.702]
Epoch [5/120    avg_loss:0.622, val_acc:0.702]
Epoch [6/120    avg_loss:0.481, val_acc:0.713]
Epoch [7/120    avg_loss:0.404, val_acc:0.725]
Epoch [8/120    avg_loss:0.396, val_acc:0.685]
Epoch [9/120    avg_loss:0.327, val_acc:0.831]
Epoch [10/120    avg_loss:0.351, val_acc:0.862]
Epoch [11/120    avg_loss:0.277, val_acc:0.887]
Epoch [12/120    avg_loss:0.244, val_acc:0.890]
Epoch [13/120    avg_loss:0.202, val_acc:0.924]
Epoch [14/120    avg_loss:0.212, val_acc:0.851]
Epoch [15/120    avg_loss:0.186, val_acc:0.852]
Epoch [16/120    avg_loss:0.217, val_acc:0.917]
Epoch [17/120    avg_loss:0.196, val_acc:0.894]
Epoch [18/120    avg_loss:0.156, val_acc:0.938]
Epoch [19/120    avg_loss:0.127, val_acc:0.921]
Epoch [20/120    avg_loss:0.130, val_acc:0.933]
Epoch [21/120    avg_loss:0.112, val_acc:0.948]
Epoch [22/120    avg_loss:0.106, val_acc:0.909]
Epoch [23/120    avg_loss:0.119, val_acc:0.946]
Epoch [24/120    avg_loss:0.078, val_acc:0.962]
Epoch [25/120    avg_loss:0.109, val_acc:0.900]
Epoch [26/120    avg_loss:0.099, val_acc:0.903]
Epoch [27/120    avg_loss:0.090, val_acc:0.926]
Epoch [28/120    avg_loss:0.077, val_acc:0.946]
Epoch [29/120    avg_loss:0.086, val_acc:0.912]
Epoch [30/120    avg_loss:0.075, val_acc:0.955]
Epoch [31/120    avg_loss:0.044, val_acc:0.978]
Epoch [32/120    avg_loss:0.040, val_acc:0.966]
Epoch [33/120    avg_loss:0.045, val_acc:0.949]
Epoch [34/120    avg_loss:0.041, val_acc:0.960]
Epoch [35/120    avg_loss:0.059, val_acc:0.953]
Epoch [36/120    avg_loss:0.039, val_acc:0.979]
Epoch [37/120    avg_loss:0.028, val_acc:0.974]
Epoch [38/120    avg_loss:0.039, val_acc:0.946]
Epoch [39/120    avg_loss:0.029, val_acc:0.973]
Epoch [40/120    avg_loss:0.036, val_acc:0.972]
Epoch [41/120    avg_loss:0.041, val_acc:0.970]
Epoch [42/120    avg_loss:0.055, val_acc:0.968]
Epoch [43/120    avg_loss:0.043, val_acc:0.967]
Epoch [44/120    avg_loss:0.047, val_acc:0.967]
Epoch [45/120    avg_loss:0.052, val_acc:0.965]
Epoch [46/120    avg_loss:0.035, val_acc:0.947]
Epoch [47/120    avg_loss:0.037, val_acc:0.972]
Epoch [48/120    avg_loss:0.024, val_acc:0.982]
Epoch [49/120    avg_loss:0.021, val_acc:0.972]
Epoch [50/120    avg_loss:0.021, val_acc:0.968]
Epoch [51/120    avg_loss:0.021, val_acc:0.984]
Epoch [52/120    avg_loss:0.032, val_acc:0.932]
Epoch [53/120    avg_loss:0.025, val_acc:0.980]
Epoch [54/120    avg_loss:0.019, val_acc:0.977]
Epoch [55/120    avg_loss:0.028, val_acc:0.960]
Epoch [56/120    avg_loss:0.025, val_acc:0.981]
Epoch [57/120    avg_loss:0.022, val_acc:0.969]
Epoch [58/120    avg_loss:0.012, val_acc:0.928]
Epoch [59/120    avg_loss:0.030, val_acc:0.958]
Epoch [60/120    avg_loss:0.025, val_acc:0.981]
Epoch [61/120    avg_loss:0.010, val_acc:0.984]
Epoch [62/120    avg_loss:0.017, val_acc:0.971]
Epoch [63/120    avg_loss:0.035, val_acc:0.969]
Epoch [64/120    avg_loss:0.031, val_acc:0.975]
Epoch [65/120    avg_loss:0.015, val_acc:0.981]
Epoch [66/120    avg_loss:0.014, val_acc:0.979]
Epoch [67/120    avg_loss:0.016, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.985]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.008, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.980]
Epoch [73/120    avg_loss:0.011, val_acc:0.984]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.012, val_acc:0.982]
Epoch [77/120    avg_loss:0.006, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.019, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     1     3     0     7     8    60     6]
 [    0     1 17942     0    35     0   112     0     0     0]
 [    0     1     0  1939     0     0     0     0    96     0]
 [    0    21     8     0  2930     0     5     0     5     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     1     0  4862     0     0     0]
 [    0    30     0     0     0     0     1  1259     0     0]
 [    0    26     0    55    37     0     0     0  3452     1]
 [    0     1     0     0     1    40     0     0     0   877]]

Accuracy:
98.60217386065119

F1 scores:
[       nan 0.98716852 0.99525725 0.96204416 0.98009701 0.98490566
 0.98570705 0.98474775 0.9610245  0.97120709]

Kappa:
0.9815007120690854
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd60091828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.859, val_acc:0.449]
Epoch [2/120    avg_loss:1.250, val_acc:0.704]
Epoch [3/120    avg_loss:0.900, val_acc:0.656]
Epoch [4/120    avg_loss:0.647, val_acc:0.687]
Epoch [5/120    avg_loss:0.503, val_acc:0.718]
Epoch [6/120    avg_loss:0.446, val_acc:0.650]
Epoch [7/120    avg_loss:0.379, val_acc:0.804]
Epoch [8/120    avg_loss:0.319, val_acc:0.822]
Epoch [9/120    avg_loss:0.259, val_acc:0.896]
Epoch [10/120    avg_loss:0.268, val_acc:0.908]
Epoch [11/120    avg_loss:0.230, val_acc:0.898]
Epoch [12/120    avg_loss:0.236, val_acc:0.932]
Epoch [13/120    avg_loss:0.165, val_acc:0.936]
Epoch [14/120    avg_loss:0.135, val_acc:0.926]
Epoch [15/120    avg_loss:0.120, val_acc:0.953]
Epoch [16/120    avg_loss:0.097, val_acc:0.945]
Epoch [17/120    avg_loss:0.112, val_acc:0.951]
Epoch [18/120    avg_loss:0.136, val_acc:0.939]
Epoch [19/120    avg_loss:0.136, val_acc:0.946]
Epoch [20/120    avg_loss:0.074, val_acc:0.953]
Epoch [21/120    avg_loss:0.088, val_acc:0.967]
Epoch [22/120    avg_loss:0.108, val_acc:0.961]
Epoch [23/120    avg_loss:0.063, val_acc:0.951]
Epoch [24/120    avg_loss:0.076, val_acc:0.939]
Epoch [25/120    avg_loss:0.088, val_acc:0.963]
Epoch [26/120    avg_loss:0.057, val_acc:0.962]
Epoch [27/120    avg_loss:0.054, val_acc:0.955]
Epoch [28/120    avg_loss:0.050, val_acc:0.961]
Epoch [29/120    avg_loss:0.092, val_acc:0.956]
Epoch [30/120    avg_loss:0.051, val_acc:0.970]
Epoch [31/120    avg_loss:0.041, val_acc:0.954]
Epoch [32/120    avg_loss:0.051, val_acc:0.951]
Epoch [33/120    avg_loss:0.070, val_acc:0.960]
Epoch [34/120    avg_loss:0.049, val_acc:0.965]
Epoch [35/120    avg_loss:0.039, val_acc:0.972]
Epoch [36/120    avg_loss:0.053, val_acc:0.968]
Epoch [37/120    avg_loss:0.032, val_acc:0.978]
Epoch [38/120    avg_loss:0.033, val_acc:0.968]
Epoch [39/120    avg_loss:0.033, val_acc:0.953]
Epoch [40/120    avg_loss:0.071, val_acc:0.956]
Epoch [41/120    avg_loss:0.036, val_acc:0.964]
Epoch [42/120    avg_loss:0.038, val_acc:0.902]
Epoch [43/120    avg_loss:0.034, val_acc:0.968]
Epoch [44/120    avg_loss:0.021, val_acc:0.980]
Epoch [45/120    avg_loss:0.022, val_acc:0.973]
Epoch [46/120    avg_loss:0.020, val_acc:0.968]
Epoch [47/120    avg_loss:0.014, val_acc:0.971]
Epoch [48/120    avg_loss:0.018, val_acc:0.956]
Epoch [49/120    avg_loss:0.046, val_acc:0.955]
Epoch [50/120    avg_loss:0.031, val_acc:0.966]
Epoch [51/120    avg_loss:0.024, val_acc:0.975]
Epoch [52/120    avg_loss:0.028, val_acc:0.971]
Epoch [53/120    avg_loss:0.048, val_acc:0.966]
Epoch [54/120    avg_loss:0.038, val_acc:0.976]
Epoch [55/120    avg_loss:0.021, val_acc:0.980]
Epoch [56/120    avg_loss:0.018, val_acc:0.974]
Epoch [57/120    avg_loss:0.015, val_acc:0.982]
Epoch [58/120    avg_loss:0.014, val_acc:0.972]
Epoch [59/120    avg_loss:0.016, val_acc:0.980]
Epoch [60/120    avg_loss:0.027, val_acc:0.973]
Epoch [61/120    avg_loss:0.020, val_acc:0.981]
Epoch [62/120    avg_loss:0.014, val_acc:0.982]
Epoch [63/120    avg_loss:0.015, val_acc:0.962]
Epoch [64/120    avg_loss:0.062, val_acc:0.970]
Epoch [65/120    avg_loss:0.032, val_acc:0.983]
Epoch [66/120    avg_loss:0.016, val_acc:0.969]
Epoch [67/120    avg_loss:0.017, val_acc:0.979]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.005, val_acc:0.986]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.006, val_acc:0.986]
Epoch [75/120    avg_loss:0.005, val_acc:0.990]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.988]
Epoch [78/120    avg_loss:0.008, val_acc:0.980]
Epoch [79/120    avg_loss:0.005, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.022, val_acc:0.971]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.006, val_acc:0.980]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.003, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.004, val_acc:0.991]
Epoch [105/120    avg_loss:0.004, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.003, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.002, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.002, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6427     0     1     0     0     0     1     3     0]
 [    0     0 18010     0    19     0    57     0     4     0]
 [    0     0     0  1987     0     0     0     0    49     0]
 [    0    26     6     0  2931     0     3     0     2     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4871     0     0     0]
 [    0    19     0     0     0     0     0  1271     0     0]
 [    0     9     0    26    38     0     0     0  3496     2]
 [    0     0     0     0     0     6     0     0     0   913]]

Accuracy:
99.32036729086835

F1 scores:
[       nan 0.99543096 0.99742475 0.98123457 0.98355705 0.99770642
 0.99316954 0.9921936  0.98133333 0.99347116]

Kappa:
0.9909993859281041
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ecd4f1898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.883, val_acc:0.587]
Epoch [2/120    avg_loss:1.253, val_acc:0.443]
Epoch [3/120    avg_loss:0.889, val_acc:0.635]
Epoch [4/120    avg_loss:0.605, val_acc:0.687]
Epoch [5/120    avg_loss:0.519, val_acc:0.751]
Epoch [6/120    avg_loss:0.459, val_acc:0.759]
Epoch [7/120    avg_loss:0.410, val_acc:0.808]
Epoch [8/120    avg_loss:0.377, val_acc:0.844]
Epoch [9/120    avg_loss:0.286, val_acc:0.815]
Epoch [10/120    avg_loss:0.261, val_acc:0.894]
Epoch [11/120    avg_loss:0.284, val_acc:0.829]
Epoch [12/120    avg_loss:0.309, val_acc:0.883]
Epoch [13/120    avg_loss:0.231, val_acc:0.891]
Epoch [14/120    avg_loss:0.239, val_acc:0.828]
Epoch [15/120    avg_loss:0.235, val_acc:0.866]
Epoch [16/120    avg_loss:0.202, val_acc:0.889]
Epoch [17/120    avg_loss:0.202, val_acc:0.864]
Epoch [18/120    avg_loss:0.152, val_acc:0.865]
Epoch [19/120    avg_loss:0.133, val_acc:0.914]
Epoch [20/120    avg_loss:0.117, val_acc:0.934]
Epoch [21/120    avg_loss:0.099, val_acc:0.938]
Epoch [22/120    avg_loss:0.116, val_acc:0.907]
Epoch [23/120    avg_loss:0.109, val_acc:0.951]
Epoch [24/120    avg_loss:0.103, val_acc:0.947]
Epoch [25/120    avg_loss:0.089, val_acc:0.957]
Epoch [26/120    avg_loss:0.062, val_acc:0.909]
Epoch [27/120    avg_loss:0.075, val_acc:0.922]
Epoch [28/120    avg_loss:0.078, val_acc:0.956]
Epoch [29/120    avg_loss:0.074, val_acc:0.946]
Epoch [30/120    avg_loss:0.059, val_acc:0.955]
Epoch [31/120    avg_loss:0.048, val_acc:0.951]
Epoch [32/120    avg_loss:0.067, val_acc:0.935]
Epoch [33/120    avg_loss:0.081, val_acc:0.937]
Epoch [34/120    avg_loss:0.076, val_acc:0.947]
Epoch [35/120    avg_loss:0.069, val_acc:0.944]
Epoch [36/120    avg_loss:0.044, val_acc:0.963]
Epoch [37/120    avg_loss:0.043, val_acc:0.972]
Epoch [38/120    avg_loss:0.044, val_acc:0.947]
Epoch [39/120    avg_loss:0.037, val_acc:0.942]
Epoch [40/120    avg_loss:0.036, val_acc:0.970]
Epoch [41/120    avg_loss:0.045, val_acc:0.948]
Epoch [42/120    avg_loss:0.060, val_acc:0.961]
Epoch [43/120    avg_loss:0.059, val_acc:0.961]
Epoch [44/120    avg_loss:0.037, val_acc:0.966]
Epoch [45/120    avg_loss:0.054, val_acc:0.943]
Epoch [46/120    avg_loss:0.042, val_acc:0.952]
Epoch [47/120    avg_loss:0.041, val_acc:0.953]
Epoch [48/120    avg_loss:0.036, val_acc:0.974]
Epoch [49/120    avg_loss:0.019, val_acc:0.975]
Epoch [50/120    avg_loss:0.033, val_acc:0.971]
Epoch [51/120    avg_loss:0.025, val_acc:0.975]
Epoch [52/120    avg_loss:0.018, val_acc:0.971]
Epoch [53/120    avg_loss:0.101, val_acc:0.961]
Epoch [54/120    avg_loss:0.048, val_acc:0.964]
Epoch [55/120    avg_loss:0.035, val_acc:0.961]
Epoch [56/120    avg_loss:0.032, val_acc:0.970]
Epoch [57/120    avg_loss:0.023, val_acc:0.970]
Epoch [58/120    avg_loss:0.019, val_acc:0.951]
Epoch [59/120    avg_loss:0.040, val_acc:0.951]
Epoch [60/120    avg_loss:0.029, val_acc:0.914]
Epoch [61/120    avg_loss:0.028, val_acc:0.975]
Epoch [62/120    avg_loss:0.019, val_acc:0.977]
Epoch [63/120    avg_loss:0.033, val_acc:0.948]
Epoch [64/120    avg_loss:0.024, val_acc:0.979]
Epoch [65/120    avg_loss:0.032, val_acc:0.975]
Epoch [66/120    avg_loss:0.019, val_acc:0.965]
Epoch [67/120    avg_loss:0.014, val_acc:0.982]
Epoch [68/120    avg_loss:0.015, val_acc:0.980]
Epoch [69/120    avg_loss:0.012, val_acc:0.980]
Epoch [70/120    avg_loss:0.019, val_acc:0.979]
Epoch [71/120    avg_loss:0.011, val_acc:0.976]
Epoch [72/120    avg_loss:0.009, val_acc:0.979]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.030, val_acc:0.970]
Epoch [75/120    avg_loss:0.016, val_acc:0.981]
Epoch [76/120    avg_loss:0.017, val_acc:0.975]
Epoch [77/120    avg_loss:0.019, val_acc:0.979]
Epoch [78/120    avg_loss:0.016, val_acc:0.981]
Epoch [79/120    avg_loss:0.015, val_acc:0.981]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.015, val_acc:0.978]
Epoch [82/120    avg_loss:0.027, val_acc:0.974]
Epoch [83/120    avg_loss:0.091, val_acc:0.965]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.021, val_acc:0.965]
Epoch [86/120    avg_loss:0.016, val_acc:0.975]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.028, val_acc:0.967]
Epoch [90/120    avg_loss:0.033, val_acc:0.972]
Epoch [91/120    avg_loss:0.020, val_acc:0.978]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.010, val_acc:0.973]
Epoch [94/120    avg_loss:0.013, val_acc:0.979]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6389     0     1     1     0    24     1     5    11]
 [    0     0 18037     0    26     0    25     0     2     0]
 [    0     6     0  1913     0     0     0     0   117     0]
 [    0    17     4     0  2940     0     8     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    38     0     0     0  4839     0     1     0]
 [    0    19     1     0     0     0     2  1268     0     0]
 [    0    40     1    31    18     0     2     0  3478     1]
 [    0     0     0     0     2     4     0     0     0   913]]

Accuracy:
99.00947147711662

F1 scores:
[       nan 0.99031233 0.99731829 0.96106506 0.98674274 0.99846978
 0.98977296 0.99101211 0.96920719 0.99023861]

Kappa:
0.9868732490897758
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:76
Validation dataloader:76
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1b019d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.798, val_acc:0.332]
Epoch [2/120    avg_loss:1.194, val_acc:0.695]
Epoch [3/120    avg_loss:0.907, val_acc:0.731]
Epoch [4/120    avg_loss:0.686, val_acc:0.768]
Epoch [5/120    avg_loss:0.545, val_acc:0.747]
Epoch [6/120    avg_loss:0.440, val_acc:0.828]
Epoch [7/120    avg_loss:0.394, val_acc:0.780]
Epoch [8/120    avg_loss:0.323, val_acc:0.795]
Epoch [9/120    avg_loss:0.316, val_acc:0.864]
Epoch [10/120    avg_loss:0.341, val_acc:0.886]
Epoch [11/120    avg_loss:0.236, val_acc:0.885]
Epoch [12/120    avg_loss:0.239, val_acc:0.894]
Epoch [13/120    avg_loss:0.202, val_acc:0.904]
Epoch [14/120    avg_loss:0.198, val_acc:0.862]
Epoch [15/120    avg_loss:0.180, val_acc:0.920]
Epoch [16/120    avg_loss:0.162, val_acc:0.923]
Epoch [17/120    avg_loss:0.190, val_acc:0.884]
Epoch [18/120    avg_loss:0.164, val_acc:0.856]
Epoch [19/120    avg_loss:0.189, val_acc:0.873]
Epoch [20/120    avg_loss:0.140, val_acc:0.928]
Epoch [21/120    avg_loss:0.155, val_acc:0.925]
Epoch [22/120    avg_loss:0.129, val_acc:0.893]
Epoch [23/120    avg_loss:0.126, val_acc:0.948]
Epoch [24/120    avg_loss:0.068, val_acc:0.950]
Epoch [25/120    avg_loss:0.096, val_acc:0.946]
Epoch [26/120    avg_loss:0.064, val_acc:0.961]
Epoch [27/120    avg_loss:0.109, val_acc:0.880]
Epoch [28/120    avg_loss:0.122, val_acc:0.912]
Epoch [29/120    avg_loss:0.096, val_acc:0.955]
Epoch [30/120    avg_loss:0.071, val_acc:0.935]
Epoch [31/120    avg_loss:0.084, val_acc:0.953]
Epoch [32/120    avg_loss:0.101, val_acc:0.933]
Epoch [33/120    avg_loss:0.123, val_acc:0.948]
Epoch [34/120    avg_loss:0.070, val_acc:0.960]
Epoch [35/120    avg_loss:0.084, val_acc:0.877]
Epoch [36/120    avg_loss:0.103, val_acc:0.920]
Epoch [37/120    avg_loss:0.051, val_acc:0.937]
Epoch [38/120    avg_loss:0.055, val_acc:0.963]
Epoch [39/120    avg_loss:0.044, val_acc:0.943]
Epoch [40/120    avg_loss:0.080, val_acc:0.942]
Epoch [41/120    avg_loss:0.109, val_acc:0.942]
Epoch [42/120    avg_loss:0.062, val_acc:0.959]
Epoch [43/120    avg_loss:0.026, val_acc:0.966]
Epoch [44/120    avg_loss:0.041, val_acc:0.965]
Epoch [45/120    avg_loss:0.077, val_acc:0.968]
Epoch [46/120    avg_loss:0.038, val_acc:0.955]
Epoch [47/120    avg_loss:0.031, val_acc:0.945]
Epoch [48/120    avg_loss:0.036, val_acc:0.961]
Epoch [49/120    avg_loss:0.025, val_acc:0.972]
Epoch [50/120    avg_loss:0.044, val_acc:0.972]
Epoch [51/120    avg_loss:0.033, val_acc:0.949]
Epoch [52/120    avg_loss:0.023, val_acc:0.964]
Epoch [53/120    avg_loss:0.063, val_acc:0.957]
Epoch [54/120    avg_loss:0.033, val_acc:0.968]
Epoch [55/120    avg_loss:0.025, val_acc:0.920]
Epoch [56/120    avg_loss:0.041, val_acc:0.961]
Epoch [57/120    avg_loss:0.053, val_acc:0.973]
Epoch [58/120    avg_loss:0.075, val_acc:0.899]
Epoch [59/120    avg_loss:0.040, val_acc:0.958]
Epoch [60/120    avg_loss:0.032, val_acc:0.960]
Epoch [61/120    avg_loss:0.056, val_acc:0.953]
Epoch [62/120    avg_loss:0.023, val_acc:0.976]
Epoch [63/120    avg_loss:0.017, val_acc:0.972]
Epoch [64/120    avg_loss:0.040, val_acc:0.967]
Epoch [65/120    avg_loss:0.040, val_acc:0.973]
Epoch [66/120    avg_loss:0.017, val_acc:0.963]
Epoch [67/120    avg_loss:0.021, val_acc:0.963]
Epoch [68/120    avg_loss:0.023, val_acc:0.969]
Epoch [69/120    avg_loss:0.017, val_acc:0.967]
Epoch [70/120    avg_loss:0.010, val_acc:0.976]
Epoch [71/120    avg_loss:0.014, val_acc:0.977]
Epoch [72/120    avg_loss:0.026, val_acc:0.973]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.015, val_acc:0.955]
Epoch [75/120    avg_loss:0.024, val_acc:0.963]
Epoch [76/120    avg_loss:0.016, val_acc:0.974]
Epoch [77/120    avg_loss:0.014, val_acc:0.976]
Epoch [78/120    avg_loss:0.009, val_acc:0.977]
Epoch [79/120    avg_loss:0.006, val_acc:0.971]
Epoch [80/120    avg_loss:0.012, val_acc:0.973]
Epoch [81/120    avg_loss:0.015, val_acc:0.963]
Epoch [82/120    avg_loss:0.021, val_acc:0.974]
Epoch [83/120    avg_loss:0.010, val_acc:0.976]
Epoch [84/120    avg_loss:0.035, val_acc:0.970]
Epoch [85/120    avg_loss:0.017, val_acc:0.972]
Epoch [86/120    avg_loss:0.015, val_acc:0.968]
Epoch [87/120    avg_loss:0.013, val_acc:0.979]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.006, val_acc:0.977]
Epoch [91/120    avg_loss:0.008, val_acc:0.977]
Epoch [92/120    avg_loss:0.007, val_acc:0.977]
Epoch [93/120    avg_loss:0.006, val_acc:0.979]
Epoch [94/120    avg_loss:0.006, val_acc:0.980]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.005, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.980]
Epoch [99/120    avg_loss:0.005, val_acc:0.977]
Epoch [100/120    avg_loss:0.005, val_acc:0.979]
Epoch [101/120    avg_loss:0.004, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.005, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.004, val_acc:0.979]
Epoch [106/120    avg_loss:0.005, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.978]
Epoch [109/120    avg_loss:0.005, val_acc:0.978]
Epoch [110/120    avg_loss:0.005, val_acc:0.979]
Epoch [111/120    avg_loss:0.005, val_acc:0.979]
Epoch [112/120    avg_loss:0.004, val_acc:0.979]
Epoch [113/120    avg_loss:0.004, val_acc:0.979]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6406     0     1     0     0     2     9     9     5]
 [    0     2 18075     0     2     0    11     0     0     0]
 [    0     7     0  1947     0     0     0     0    82     0]
 [    0    31     4     0  2929     0     5     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    37     0     0     0  4841     0     0     0]
 [    0    34     0     0     0     0     0  1256     0     0]
 [    0    29     1    57    50     0     0     0  3433     1]
 [    0     0     0     0     1    18     0     0     0   900]]

Accuracy:
99.03357192779505

F1 scores:
[       nan 0.99003168 0.99842572 0.96362287 0.98387639 0.99315068
 0.99435144 0.98317025 0.96731474 0.98630137]

Kappa:
0.9871875409586679
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f177c93c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.862, val_acc:0.580]
Epoch [2/120    avg_loss:1.264, val_acc:0.714]
Epoch [3/120    avg_loss:0.933, val_acc:0.713]
Epoch [4/120    avg_loss:0.701, val_acc:0.566]
Epoch [5/120    avg_loss:0.571, val_acc:0.732]
Epoch [6/120    avg_loss:0.555, val_acc:0.698]
Epoch [7/120    avg_loss:0.431, val_acc:0.723]
Epoch [8/120    avg_loss:0.365, val_acc:0.814]
Epoch [9/120    avg_loss:0.345, val_acc:0.793]
Epoch [10/120    avg_loss:0.308, val_acc:0.799]
Epoch [11/120    avg_loss:0.307, val_acc:0.729]
Epoch [12/120    avg_loss:0.245, val_acc:0.821]
Epoch [13/120    avg_loss:0.230, val_acc:0.879]
Epoch [14/120    avg_loss:0.264, val_acc:0.834]
Epoch [15/120    avg_loss:0.180, val_acc:0.912]
Epoch [16/120    avg_loss:0.224, val_acc:0.930]
Epoch [17/120    avg_loss:0.159, val_acc:0.873]
Epoch [18/120    avg_loss:0.181, val_acc:0.897]
Epoch [19/120    avg_loss:0.124, val_acc:0.911]
Epoch [20/120    avg_loss:0.138, val_acc:0.939]
Epoch [21/120    avg_loss:0.135, val_acc:0.939]
Epoch [22/120    avg_loss:0.115, val_acc:0.927]
Epoch [23/120    avg_loss:0.108, val_acc:0.932]
Epoch [24/120    avg_loss:0.092, val_acc:0.958]
Epoch [25/120    avg_loss:0.132, val_acc:0.865]
Epoch [26/120    avg_loss:0.130, val_acc:0.947]
Epoch [27/120    avg_loss:0.085, val_acc:0.963]
Epoch [28/120    avg_loss:0.078, val_acc:0.974]
Epoch [29/120    avg_loss:0.061, val_acc:0.970]
Epoch [30/120    avg_loss:0.060, val_acc:0.949]
Epoch [31/120    avg_loss:0.073, val_acc:0.957]
Epoch [32/120    avg_loss:0.093, val_acc:0.943]
Epoch [33/120    avg_loss:0.102, val_acc:0.958]
Epoch [34/120    avg_loss:0.075, val_acc:0.893]
Epoch [35/120    avg_loss:0.156, val_acc:0.948]
Epoch [36/120    avg_loss:0.059, val_acc:0.970]
Epoch [37/120    avg_loss:0.044, val_acc:0.978]
Epoch [38/120    avg_loss:0.037, val_acc:0.978]
Epoch [39/120    avg_loss:0.064, val_acc:0.929]
Epoch [40/120    avg_loss:0.043, val_acc:0.973]
Epoch [41/120    avg_loss:0.027, val_acc:0.972]
Epoch [42/120    avg_loss:0.036, val_acc:0.980]
Epoch [43/120    avg_loss:0.029, val_acc:0.979]
Epoch [44/120    avg_loss:0.029, val_acc:0.974]
Epoch [45/120    avg_loss:0.026, val_acc:0.978]
Epoch [46/120    avg_loss:0.041, val_acc:0.978]
Epoch [47/120    avg_loss:0.034, val_acc:0.980]
Epoch [48/120    avg_loss:0.018, val_acc:0.981]
Epoch [49/120    avg_loss:0.030, val_acc:0.981]
Epoch [50/120    avg_loss:0.029, val_acc:0.983]
Epoch [51/120    avg_loss:0.035, val_acc:0.974]
Epoch [52/120    avg_loss:0.025, val_acc:0.985]
Epoch [53/120    avg_loss:0.028, val_acc:0.983]
Epoch [54/120    avg_loss:0.033, val_acc:0.977]
Epoch [55/120    avg_loss:0.024, val_acc:0.982]
Epoch [56/120    avg_loss:0.026, val_acc:0.984]
Epoch [57/120    avg_loss:0.023, val_acc:0.978]
Epoch [58/120    avg_loss:0.045, val_acc:0.980]
Epoch [59/120    avg_loss:0.021, val_acc:0.979]
Epoch [60/120    avg_loss:0.022, val_acc:0.965]
Epoch [61/120    avg_loss:0.020, val_acc:0.983]
Epoch [62/120    avg_loss:0.026, val_acc:0.941]
Epoch [63/120    avg_loss:0.037, val_acc:0.966]
Epoch [64/120    avg_loss:0.026, val_acc:0.977]
Epoch [65/120    avg_loss:0.034, val_acc:0.979]
Epoch [66/120    avg_loss:0.017, val_acc:0.984]
Epoch [67/120    avg_loss:0.015, val_acc:0.988]
Epoch [68/120    avg_loss:0.017, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.986]
Epoch [72/120    avg_loss:0.011, val_acc:0.988]
Epoch [73/120    avg_loss:0.009, val_acc:0.989]
Epoch [74/120    avg_loss:0.012, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.988]
Epoch [80/120    avg_loss:0.011, val_acc:0.986]
Epoch [81/120    avg_loss:0.014, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.014, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6367     0     0     2     0     0     1    62     0]
 [    0     2 17973     0    74     0    37     0     4     0]
 [    0     7     0  1968     0     0     0     0    58     3]
 [    0    28     7     1  2921     0     3     0    11     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    55     0     0     0  4818     0     5     0]
 [    0     2     0     0     0     0     2  1284     2     0]
 [    0    47     0    32    42     0     0     0  3450     0]
 [    0     0     0     0     2    10     0     0     0   907]]

Accuracy:
98.79497746607862

F1 scores:
[       nan 0.98828095 0.99504498 0.97498142 0.97156162 0.99618321
 0.98952557 0.99728155 0.96328354 0.99125683]

Kappa:
0.9840418077514945
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe9058b49e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.821, val_acc:0.311]
Epoch [2/120    avg_loss:1.278, val_acc:0.471]
Epoch [3/120    avg_loss:0.962, val_acc:0.482]
Epoch [4/120    avg_loss:0.686, val_acc:0.731]
Epoch [5/120    avg_loss:0.590, val_acc:0.689]
Epoch [6/120    avg_loss:0.484, val_acc:0.639]
Epoch [7/120    avg_loss:0.487, val_acc:0.805]
Epoch [8/120    avg_loss:0.386, val_acc:0.770]
Epoch [9/120    avg_loss:0.333, val_acc:0.785]
Epoch [10/120    avg_loss:0.328, val_acc:0.815]
Epoch [11/120    avg_loss:0.240, val_acc:0.799]
Epoch [12/120    avg_loss:0.234, val_acc:0.867]
Epoch [13/120    avg_loss:0.239, val_acc:0.905]
Epoch [14/120    avg_loss:0.157, val_acc:0.889]
Epoch [15/120    avg_loss:0.162, val_acc:0.936]
Epoch [16/120    avg_loss:0.168, val_acc:0.923]
Epoch [17/120    avg_loss:0.159, val_acc:0.905]
Epoch [18/120    avg_loss:0.123, val_acc:0.953]
Epoch [19/120    avg_loss:0.125, val_acc:0.932]
Epoch [20/120    avg_loss:0.109, val_acc:0.955]
Epoch [21/120    avg_loss:0.135, val_acc:0.932]
Epoch [22/120    avg_loss:0.122, val_acc:0.952]
Epoch [23/120    avg_loss:0.108, val_acc:0.951]
Epoch [24/120    avg_loss:0.081, val_acc:0.960]
Epoch [25/120    avg_loss:0.075, val_acc:0.942]
Epoch [26/120    avg_loss:0.065, val_acc:0.969]
Epoch [27/120    avg_loss:0.086, val_acc:0.960]
Epoch [28/120    avg_loss:0.098, val_acc:0.937]
Epoch [29/120    avg_loss:0.118, val_acc:0.953]
Epoch [30/120    avg_loss:0.094, val_acc:0.955]
Epoch [31/120    avg_loss:0.076, val_acc:0.948]
Epoch [32/120    avg_loss:0.055, val_acc:0.973]
Epoch [33/120    avg_loss:0.044, val_acc:0.977]
Epoch [34/120    avg_loss:0.056, val_acc:0.965]
Epoch [35/120    avg_loss:0.044, val_acc:0.978]
Epoch [36/120    avg_loss:0.022, val_acc:0.976]
Epoch [37/120    avg_loss:0.033, val_acc:0.977]
Epoch [38/120    avg_loss:0.030, val_acc:0.974]
Epoch [39/120    avg_loss:0.036, val_acc:0.980]
Epoch [40/120    avg_loss:0.024, val_acc:0.982]
Epoch [41/120    avg_loss:0.026, val_acc:0.968]
Epoch [42/120    avg_loss:0.028, val_acc:0.972]
Epoch [43/120    avg_loss:0.024, val_acc:0.975]
Epoch [44/120    avg_loss:0.018, val_acc:0.979]
Epoch [45/120    avg_loss:0.020, val_acc:0.987]
Epoch [46/120    avg_loss:0.024, val_acc:0.981]
Epoch [47/120    avg_loss:0.030, val_acc:0.970]
Epoch [48/120    avg_loss:0.040, val_acc:0.973]
Epoch [49/120    avg_loss:0.029, val_acc:0.981]
Epoch [50/120    avg_loss:0.018, val_acc:0.981]
Epoch [51/120    avg_loss:0.026, val_acc:0.972]
Epoch [52/120    avg_loss:0.015, val_acc:0.986]
Epoch [53/120    avg_loss:0.013, val_acc:0.982]
Epoch [54/120    avg_loss:0.009, val_acc:0.988]
Epoch [55/120    avg_loss:0.015, val_acc:0.969]
Epoch [56/120    avg_loss:0.024, val_acc:0.980]
Epoch [57/120    avg_loss:0.009, val_acc:0.981]
Epoch [58/120    avg_loss:0.010, val_acc:0.982]
Epoch [59/120    avg_loss:0.021, val_acc:0.980]
Epoch [60/120    avg_loss:0.016, val_acc:0.988]
Epoch [61/120    avg_loss:0.010, val_acc:0.989]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.013, val_acc:0.996]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.016, val_acc:0.992]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.009, val_acc:0.992]
Epoch [68/120    avg_loss:0.006, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.991]
Epoch [70/120    avg_loss:0.033, val_acc:0.976]
Epoch [71/120    avg_loss:0.015, val_acc:0.987]
Epoch [72/120    avg_loss:0.024, val_acc:0.970]
Epoch [73/120    avg_loss:0.018, val_acc:0.983]
Epoch [74/120    avg_loss:0.014, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.006, val_acc:0.990]
Epoch [77/120    avg_loss:0.007, val_acc:0.992]
Epoch [78/120    avg_loss:0.005, val_acc:0.991]
Epoch [79/120    avg_loss:0.005, val_acc:0.990]
Epoch [80/120    avg_loss:0.005, val_acc:0.991]
Epoch [81/120    avg_loss:0.006, val_acc:0.992]
Epoch [82/120    avg_loss:0.005, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.994]
Epoch [84/120    avg_loss:0.005, val_acc:0.994]
Epoch [85/120    avg_loss:0.004, val_acc:0.992]
Epoch [86/120    avg_loss:0.004, val_acc:0.995]
Epoch [87/120    avg_loss:0.005, val_acc:0.995]
Epoch [88/120    avg_loss:0.004, val_acc:0.994]
Epoch [89/120    avg_loss:0.004, val_acc:0.994]
Epoch [90/120    avg_loss:0.004, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.994]
Epoch [92/120    avg_loss:0.004, val_acc:0.994]
Epoch [93/120    avg_loss:0.006, val_acc:0.994]
Epoch [94/120    avg_loss:0.006, val_acc:0.994]
Epoch [95/120    avg_loss:0.005, val_acc:0.994]
Epoch [96/120    avg_loss:0.003, val_acc:0.994]
Epoch [97/120    avg_loss:0.004, val_acc:0.994]
Epoch [98/120    avg_loss:0.005, val_acc:0.994]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.004, val_acc:0.994]
Epoch [102/120    avg_loss:0.005, val_acc:0.994]
Epoch [103/120    avg_loss:0.005, val_acc:0.994]
Epoch [104/120    avg_loss:0.004, val_acc:0.994]
Epoch [105/120    avg_loss:0.006, val_acc:0.994]
Epoch [106/120    avg_loss:0.005, val_acc:0.994]
Epoch [107/120    avg_loss:0.005, val_acc:0.994]
Epoch [108/120    avg_loss:0.005, val_acc:0.994]
Epoch [109/120    avg_loss:0.004, val_acc:0.994]
Epoch [110/120    avg_loss:0.005, val_acc:0.994]
Epoch [111/120    avg_loss:0.007, val_acc:0.994]
Epoch [112/120    avg_loss:0.004, val_acc:0.994]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.003, val_acc:0.994]
Epoch [116/120    avg_loss:0.005, val_acc:0.994]
Epoch [117/120    avg_loss:0.004, val_acc:0.994]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.004, val_acc:0.994]
Epoch [120/120    avg_loss:0.004, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     0     0     0     2     4    72     0]
 [    0     0 18059     0    16     0    14     0     1     0]
 [    0     2     0  1975     0     0     0     0    53     6]
 [    0    22     3     0  2934     0     4     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     3     0]
 [    0     7     0     0     0     0     0  1280     0     3]
 [    0     8     3    36    30     0     0     0  3494     0]
 [    0     0     0     0     9    32     0     0     0   878]]

Accuracy:
99.1829947220013

F1 scores:
[       nan 0.99087719 0.99897663 0.97603163 0.98439859 0.98788796
 0.99764658 0.99456099 0.97028603 0.97177643]

Kappa:
0.989177524618176
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3182aca908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.754, val_acc:0.264]
Epoch [2/120    avg_loss:1.243, val_acc:0.414]
Epoch [3/120    avg_loss:0.952, val_acc:0.714]
Epoch [4/120    avg_loss:0.705, val_acc:0.638]
Epoch [5/120    avg_loss:0.567, val_acc:0.759]
Epoch [6/120    avg_loss:0.476, val_acc:0.829]
Epoch [7/120    avg_loss:0.373, val_acc:0.753]
Epoch [8/120    avg_loss:0.354, val_acc:0.844]
Epoch [9/120    avg_loss:0.282, val_acc:0.791]
Epoch [10/120    avg_loss:0.278, val_acc:0.886]
Epoch [11/120    avg_loss:0.238, val_acc:0.820]
Epoch [12/120    avg_loss:0.222, val_acc:0.899]
Epoch [13/120    avg_loss:0.199, val_acc:0.822]
Epoch [14/120    avg_loss:0.214, val_acc:0.895]
Epoch [15/120    avg_loss:0.156, val_acc:0.928]
Epoch [16/120    avg_loss:0.177, val_acc:0.888]
Epoch [17/120    avg_loss:0.136, val_acc:0.929]
Epoch [18/120    avg_loss:0.141, val_acc:0.883]
Epoch [19/120    avg_loss:0.251, val_acc:0.845]
Epoch [20/120    avg_loss:0.215, val_acc:0.904]
Epoch [21/120    avg_loss:0.154, val_acc:0.957]
Epoch [22/120    avg_loss:0.093, val_acc:0.923]
Epoch [23/120    avg_loss:0.096, val_acc:0.905]
Epoch [24/120    avg_loss:0.089, val_acc:0.959]
Epoch [25/120    avg_loss:0.120, val_acc:0.916]
Epoch [26/120    avg_loss:0.085, val_acc:0.928]
Epoch [27/120    avg_loss:0.072, val_acc:0.970]
Epoch [28/120    avg_loss:0.053, val_acc:0.948]
Epoch [29/120    avg_loss:0.050, val_acc:0.966]
Epoch [30/120    avg_loss:0.042, val_acc:0.965]
Epoch [31/120    avg_loss:0.058, val_acc:0.937]
Epoch [32/120    avg_loss:0.076, val_acc:0.940]
Epoch [33/120    avg_loss:0.049, val_acc:0.965]
Epoch [34/120    avg_loss:0.038, val_acc:0.965]
Epoch [35/120    avg_loss:0.030, val_acc:0.971]
Epoch [36/120    avg_loss:0.029, val_acc:0.964]
Epoch [37/120    avg_loss:0.026, val_acc:0.965]
Epoch [38/120    avg_loss:0.072, val_acc:0.959]
Epoch [39/120    avg_loss:0.054, val_acc:0.968]
Epoch [40/120    avg_loss:0.047, val_acc:0.971]
Epoch [41/120    avg_loss:0.027, val_acc:0.976]
Epoch [42/120    avg_loss:0.017, val_acc:0.976]
Epoch [43/120    avg_loss:0.026, val_acc:0.966]
Epoch [44/120    avg_loss:0.021, val_acc:0.976]
Epoch [45/120    avg_loss:0.020, val_acc:0.963]
Epoch [46/120    avg_loss:0.013, val_acc:0.974]
Epoch [47/120    avg_loss:0.014, val_acc:0.971]
Epoch [48/120    avg_loss:0.021, val_acc:0.975]
Epoch [49/120    avg_loss:0.014, val_acc:0.978]
Epoch [50/120    avg_loss:0.036, val_acc:0.968]
Epoch [51/120    avg_loss:0.029, val_acc:0.972]
Epoch [52/120    avg_loss:0.012, val_acc:0.976]
Epoch [53/120    avg_loss:0.013, val_acc:0.975]
Epoch [54/120    avg_loss:0.011, val_acc:0.977]
Epoch [55/120    avg_loss:0.012, val_acc:0.975]
Epoch [56/120    avg_loss:0.019, val_acc:0.974]
Epoch [57/120    avg_loss:0.018, val_acc:0.976]
Epoch [58/120    avg_loss:0.009, val_acc:0.980]
Epoch [59/120    avg_loss:0.010, val_acc:0.971]
Epoch [60/120    avg_loss:0.054, val_acc:0.956]
Epoch [61/120    avg_loss:0.018, val_acc:0.976]
Epoch [62/120    avg_loss:0.021, val_acc:0.951]
Epoch [63/120    avg_loss:0.014, val_acc:0.978]
Epoch [64/120    avg_loss:0.014, val_acc:0.977]
Epoch [65/120    avg_loss:0.026, val_acc:0.977]
Epoch [66/120    avg_loss:0.021, val_acc:0.976]
Epoch [67/120    avg_loss:0.063, val_acc:0.953]
Epoch [68/120    avg_loss:0.060, val_acc:0.951]
Epoch [69/120    avg_loss:0.026, val_acc:0.978]
Epoch [70/120    avg_loss:0.028, val_acc:0.976]
Epoch [71/120    avg_loss:0.014, val_acc:0.977]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.010, val_acc:0.982]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.981]
Epoch [78/120    avg_loss:0.007, val_acc:0.981]
Epoch [79/120    avg_loss:0.009, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.007, val_acc:0.982]
Epoch [83/120    avg_loss:0.006, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.980]
Epoch [85/120    avg_loss:0.007, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.981]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.007, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     0     0     0     3     0    61     0]
 [    0     0 18050     0    17     0    19     0     4     0]
 [    0     0     0  1966     0     0     0     0    67     3]
 [    0    21     0     0  2944     0     0     0     7     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0    13     0     0     0  4852     0    13     0]
 [    0     8     0     0     0     0     0  1280     1     1]
 [    0     0     0    37    63     0     0     0  3461    10]
 [    0     0     0     0     0     6     0     0     0   913]]

Accuracy:
99.14443400091582

F1 scores:
[       nan 0.9927508  0.99853401 0.97350829 0.98198799 0.99732314
 0.99507793 0.99610895 0.96339596 0.98863021]

Kappa:
0.9886675096897367
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f26a365f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.869, val_acc:0.291]
Epoch [2/120    avg_loss:1.323, val_acc:0.624]
Epoch [3/120    avg_loss:1.030, val_acc:0.692]
Epoch [4/120    avg_loss:0.781, val_acc:0.733]
Epoch [5/120    avg_loss:0.573, val_acc:0.727]
Epoch [6/120    avg_loss:0.512, val_acc:0.805]
Epoch [7/120    avg_loss:0.405, val_acc:0.826]
Epoch [8/120    avg_loss:0.385, val_acc:0.825]
Epoch [9/120    avg_loss:0.310, val_acc:0.845]
Epoch [10/120    avg_loss:0.276, val_acc:0.874]
Epoch [11/120    avg_loss:0.217, val_acc:0.906]
Epoch [12/120    avg_loss:0.200, val_acc:0.901]
Epoch [13/120    avg_loss:0.230, val_acc:0.861]
Epoch [14/120    avg_loss:0.176, val_acc:0.887]
Epoch [15/120    avg_loss:0.162, val_acc:0.933]
Epoch [16/120    avg_loss:0.168, val_acc:0.935]
Epoch [17/120    avg_loss:0.154, val_acc:0.930]
Epoch [18/120    avg_loss:0.145, val_acc:0.921]
Epoch [19/120    avg_loss:0.123, val_acc:0.938]
Epoch [20/120    avg_loss:0.142, val_acc:0.928]
Epoch [21/120    avg_loss:0.101, val_acc:0.927]
Epoch [22/120    avg_loss:0.108, val_acc:0.947]
Epoch [23/120    avg_loss:0.143, val_acc:0.933]
Epoch [24/120    avg_loss:0.116, val_acc:0.953]
Epoch [25/120    avg_loss:0.085, val_acc:0.947]
Epoch [26/120    avg_loss:0.109, val_acc:0.945]
Epoch [27/120    avg_loss:0.089, val_acc:0.963]
Epoch [28/120    avg_loss:0.069, val_acc:0.965]
Epoch [29/120    avg_loss:0.092, val_acc:0.950]
Epoch [30/120    avg_loss:0.106, val_acc:0.945]
Epoch [31/120    avg_loss:0.055, val_acc:0.948]
Epoch [32/120    avg_loss:0.050, val_acc:0.940]
Epoch [33/120    avg_loss:0.038, val_acc:0.978]
Epoch [34/120    avg_loss:0.036, val_acc:0.967]
Epoch [35/120    avg_loss:0.038, val_acc:0.961]
Epoch [36/120    avg_loss:0.044, val_acc:0.975]
Epoch [37/120    avg_loss:0.030, val_acc:0.965]
Epoch [38/120    avg_loss:0.039, val_acc:0.968]
Epoch [39/120    avg_loss:0.031, val_acc:0.965]
Epoch [40/120    avg_loss:0.034, val_acc:0.972]
Epoch [41/120    avg_loss:0.028, val_acc:0.966]
Epoch [42/120    avg_loss:0.029, val_acc:0.970]
Epoch [43/120    avg_loss:0.033, val_acc:0.975]
Epoch [44/120    avg_loss:0.025, val_acc:0.974]
Epoch [45/120    avg_loss:0.039, val_acc:0.936]
Epoch [46/120    avg_loss:0.041, val_acc:0.962]
Epoch [47/120    avg_loss:0.026, val_acc:0.972]
Epoch [48/120    avg_loss:0.015, val_acc:0.972]
Epoch [49/120    avg_loss:0.017, val_acc:0.973]
Epoch [50/120    avg_loss:0.024, val_acc:0.976]
Epoch [51/120    avg_loss:0.018, val_acc:0.975]
Epoch [52/120    avg_loss:0.014, val_acc:0.976]
Epoch [53/120    avg_loss:0.014, val_acc:0.978]
Epoch [54/120    avg_loss:0.015, val_acc:0.977]
Epoch [55/120    avg_loss:0.014, val_acc:0.978]
Epoch [56/120    avg_loss:0.019, val_acc:0.978]
Epoch [57/120    avg_loss:0.013, val_acc:0.979]
Epoch [58/120    avg_loss:0.015, val_acc:0.978]
Epoch [59/120    avg_loss:0.019, val_acc:0.980]
Epoch [60/120    avg_loss:0.016, val_acc:0.978]
Epoch [61/120    avg_loss:0.014, val_acc:0.979]
Epoch [62/120    avg_loss:0.014, val_acc:0.982]
Epoch [63/120    avg_loss:0.017, val_acc:0.979]
Epoch [64/120    avg_loss:0.017, val_acc:0.977]
Epoch [65/120    avg_loss:0.013, val_acc:0.979]
Epoch [66/120    avg_loss:0.020, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.977]
Epoch [69/120    avg_loss:0.010, val_acc:0.978]
Epoch [70/120    avg_loss:0.010, val_acc:0.979]
Epoch [71/120    avg_loss:0.016, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.017, val_acc:0.978]
Epoch [74/120    avg_loss:0.010, val_acc:0.980]
Epoch [75/120    avg_loss:0.011, val_acc:0.979]
Epoch [76/120    avg_loss:0.015, val_acc:0.979]
Epoch [77/120    avg_loss:0.010, val_acc:0.979]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.010, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.979]
Epoch [81/120    avg_loss:0.012, val_acc:0.979]
Epoch [82/120    avg_loss:0.010, val_acc:0.979]
Epoch [83/120    avg_loss:0.012, val_acc:0.979]
Epoch [84/120    avg_loss:0.011, val_acc:0.980]
Epoch [85/120    avg_loss:0.013, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.979]
Epoch [87/120    avg_loss:0.015, val_acc:0.979]
Epoch [88/120    avg_loss:0.013, val_acc:0.979]
Epoch [89/120    avg_loss:0.016, val_acc:0.979]
Epoch [90/120    avg_loss:0.011, val_acc:0.979]
Epoch [91/120    avg_loss:0.011, val_acc:0.979]
Epoch [92/120    avg_loss:0.014, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.979]
Epoch [94/120    avg_loss:0.009, val_acc:0.979]
Epoch [95/120    avg_loss:0.011, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.011, val_acc:0.979]
Epoch [98/120    avg_loss:0.013, val_acc:0.979]
Epoch [99/120    avg_loss:0.015, val_acc:0.979]
Epoch [100/120    avg_loss:0.016, val_acc:0.979]
Epoch [101/120    avg_loss:0.011, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.979]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.012, val_acc:0.979]
Epoch [106/120    avg_loss:0.012, val_acc:0.979]
Epoch [107/120    avg_loss:0.014, val_acc:0.979]
Epoch [108/120    avg_loss:0.011, val_acc:0.979]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.014, val_acc:0.979]
Epoch [113/120    avg_loss:0.011, val_acc:0.979]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.010, val_acc:0.979]
Epoch [116/120    avg_loss:0.014, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.017, val_acc:0.979]
Epoch [119/120    avg_loss:0.014, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6326     0     0     0     0    24     1    81     0]
 [    0     0 17966     0    17     0    97     0    10     0]
 [    0     0     0  1991     0     0     0     0    35    10]
 [    0    30     5     0  2924     0     5     1     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4870     0     5     0]
 [    0    16     0     0     0     0     3  1269     0     2]
 [    0    19     0    17    49     0     3     0  3467    16]
 [    0     0     0     0     0    15     0     0     0   904]]

Accuracy:
98.86486877304606

F1 scores:
[       nan 0.98666459 0.99633984 0.98466864 0.9808789  0.99428571
 0.98582996 0.99101913 0.96627648 0.97676931]

Kappa:
0.9849776985362959
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8dfdeed908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.843, val_acc:0.278]
Epoch [2/120    avg_loss:1.353, val_acc:0.387]
Epoch [3/120    avg_loss:1.026, val_acc:0.451]
Epoch [4/120    avg_loss:0.841, val_acc:0.622]
Epoch [5/120    avg_loss:0.624, val_acc:0.710]
Epoch [6/120    avg_loss:0.550, val_acc:0.732]
Epoch [7/120    avg_loss:0.442, val_acc:0.783]
Epoch [8/120    avg_loss:0.407, val_acc:0.712]
Epoch [9/120    avg_loss:0.384, val_acc:0.701]
Epoch [10/120    avg_loss:0.339, val_acc:0.839]
Epoch [11/120    avg_loss:0.347, val_acc:0.831]
Epoch [12/120    avg_loss:0.282, val_acc:0.848]
Epoch [13/120    avg_loss:0.270, val_acc:0.816]
Epoch [14/120    avg_loss:0.243, val_acc:0.819]
Epoch [15/120    avg_loss:0.210, val_acc:0.858]
Epoch [16/120    avg_loss:0.194, val_acc:0.892]
Epoch [17/120    avg_loss:0.206, val_acc:0.882]
Epoch [18/120    avg_loss:0.209, val_acc:0.907]
Epoch [19/120    avg_loss:0.161, val_acc:0.834]
Epoch [20/120    avg_loss:0.202, val_acc:0.920]
Epoch [21/120    avg_loss:0.139, val_acc:0.920]
Epoch [22/120    avg_loss:0.145, val_acc:0.932]
Epoch [23/120    avg_loss:0.116, val_acc:0.902]
Epoch [24/120    avg_loss:0.112, val_acc:0.949]
Epoch [25/120    avg_loss:0.129, val_acc:0.938]
Epoch [26/120    avg_loss:0.114, val_acc:0.913]
Epoch [27/120    avg_loss:0.099, val_acc:0.954]
Epoch [28/120    avg_loss:0.094, val_acc:0.932]
Epoch [29/120    avg_loss:0.133, val_acc:0.952]
Epoch [30/120    avg_loss:0.089, val_acc:0.953]
Epoch [31/120    avg_loss:0.062, val_acc:0.973]
Epoch [32/120    avg_loss:0.052, val_acc:0.958]
Epoch [33/120    avg_loss:0.064, val_acc:0.970]
Epoch [34/120    avg_loss:0.057, val_acc:0.963]
Epoch [35/120    avg_loss:0.055, val_acc:0.897]
Epoch [36/120    avg_loss:0.056, val_acc:0.966]
Epoch [37/120    avg_loss:0.055, val_acc:0.947]
Epoch [38/120    avg_loss:0.072, val_acc:0.949]
Epoch [39/120    avg_loss:0.038, val_acc:0.965]
Epoch [40/120    avg_loss:0.032, val_acc:0.978]
Epoch [41/120    avg_loss:0.036, val_acc:0.977]
Epoch [42/120    avg_loss:0.054, val_acc:0.974]
Epoch [43/120    avg_loss:0.054, val_acc:0.942]
Epoch [44/120    avg_loss:0.064, val_acc:0.967]
Epoch [45/120    avg_loss:0.049, val_acc:0.920]
Epoch [46/120    avg_loss:0.047, val_acc:0.975]
Epoch [47/120    avg_loss:0.034, val_acc:0.981]
Epoch [48/120    avg_loss:0.043, val_acc:0.980]
Epoch [49/120    avg_loss:0.022, val_acc:0.981]
Epoch [50/120    avg_loss:0.014, val_acc:0.979]
Epoch [51/120    avg_loss:0.019, val_acc:0.983]
Epoch [52/120    avg_loss:0.014, val_acc:0.953]
Epoch [53/120    avg_loss:0.018, val_acc:0.974]
Epoch [54/120    avg_loss:0.049, val_acc:0.973]
Epoch [55/120    avg_loss:0.026, val_acc:0.973]
Epoch [56/120    avg_loss:0.029, val_acc:0.958]
Epoch [57/120    avg_loss:0.024, val_acc:0.983]
Epoch [58/120    avg_loss:0.047, val_acc:0.974]
Epoch [59/120    avg_loss:0.050, val_acc:0.975]
Epoch [60/120    avg_loss:0.028, val_acc:0.982]
Epoch [61/120    avg_loss:0.022, val_acc:0.982]
Epoch [62/120    avg_loss:0.016, val_acc:0.986]
Epoch [63/120    avg_loss:0.037, val_acc:0.964]
Epoch [64/120    avg_loss:0.026, val_acc:0.973]
Epoch [65/120    avg_loss:0.012, val_acc:0.975]
Epoch [66/120    avg_loss:0.015, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.968]
Epoch [68/120    avg_loss:0.038, val_acc:0.980]
Epoch [69/120    avg_loss:0.032, val_acc:0.979]
Epoch [70/120    avg_loss:0.016, val_acc:0.983]
Epoch [71/120    avg_loss:0.021, val_acc:0.981]
Epoch [72/120    avg_loss:0.018, val_acc:0.982]
Epoch [73/120    avg_loss:0.015, val_acc:0.980]
Epoch [74/120    avg_loss:0.017, val_acc:0.985]
Epoch [75/120    avg_loss:0.013, val_acc:0.960]
Epoch [76/120    avg_loss:0.013, val_acc:0.989]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.991]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.990]
Epoch [83/120    avg_loss:0.011, val_acc:0.990]
Epoch [84/120    avg_loss:0.007, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.005, val_acc:0.991]
Epoch [90/120    avg_loss:0.008, val_acc:0.991]
Epoch [91/120    avg_loss:0.005, val_acc:0.991]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.991]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     4     0     0     3     9    77     0]
 [    0     2 18036     0    47     0     1     0     4     0]
 [    0     0     0  1985     2     0     0     0    47     2]
 [    0    24     6     0  2917     0     9     6     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     0     3     0  4853     0     1     0]
 [    0    15     0     0     0     0     4  1269     0     2]
 [    0     0     0     9    36     0     0     0  3523     3]
 [    0     0     0     0    13    11     0     0     0   895]]

Accuracy:
99.10587327983033

F1 scores:
[       nan 0.98954106 0.99775952 0.98413485 0.97395659 0.99580313
 0.99569142 0.98601399 0.97455048 0.98135965]

Kappa:
0.9881567889249163
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6283495898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.815, val_acc:0.625]
Epoch [2/120    avg_loss:1.264, val_acc:0.660]
Epoch [3/120    avg_loss:0.943, val_acc:0.690]
Epoch [4/120    avg_loss:0.684, val_acc:0.573]
Epoch [5/120    avg_loss:0.537, val_acc:0.803]
Epoch [6/120    avg_loss:0.500, val_acc:0.823]
Epoch [7/120    avg_loss:0.410, val_acc:0.806]
Epoch [8/120    avg_loss:0.327, val_acc:0.855]
Epoch [9/120    avg_loss:0.327, val_acc:0.737]
Epoch [10/120    avg_loss:0.354, val_acc:0.846]
Epoch [11/120    avg_loss:0.280, val_acc:0.849]
Epoch [12/120    avg_loss:0.268, val_acc:0.854]
Epoch [13/120    avg_loss:0.195, val_acc:0.847]
Epoch [14/120    avg_loss:0.236, val_acc:0.891]
Epoch [15/120    avg_loss:0.165, val_acc:0.942]
Epoch [16/120    avg_loss:0.160, val_acc:0.894]
Epoch [17/120    avg_loss:0.167, val_acc:0.952]
Epoch [18/120    avg_loss:0.134, val_acc:0.944]
Epoch [19/120    avg_loss:0.127, val_acc:0.952]
Epoch [20/120    avg_loss:0.142, val_acc:0.933]
Epoch [21/120    avg_loss:0.212, val_acc:0.922]
Epoch [22/120    avg_loss:0.115, val_acc:0.965]
Epoch [23/120    avg_loss:0.105, val_acc:0.962]
Epoch [24/120    avg_loss:0.116, val_acc:0.959]
Epoch [25/120    avg_loss:0.077, val_acc:0.951]
Epoch [26/120    avg_loss:0.084, val_acc:0.941]
Epoch [27/120    avg_loss:0.064, val_acc:0.968]
Epoch [28/120    avg_loss:0.060, val_acc:0.959]
Epoch [29/120    avg_loss:0.052, val_acc:0.969]
Epoch [30/120    avg_loss:0.053, val_acc:0.961]
Epoch [31/120    avg_loss:0.050, val_acc:0.970]
Epoch [32/120    avg_loss:0.072, val_acc:0.924]
Epoch [33/120    avg_loss:0.074, val_acc:0.907]
Epoch [34/120    avg_loss:0.102, val_acc:0.900]
Epoch [35/120    avg_loss:0.112, val_acc:0.930]
Epoch [36/120    avg_loss:0.070, val_acc:0.956]
Epoch [37/120    avg_loss:0.083, val_acc:0.941]
Epoch [38/120    avg_loss:0.053, val_acc:0.963]
Epoch [39/120    avg_loss:0.044, val_acc:0.974]
Epoch [40/120    avg_loss:0.069, val_acc:0.967]
Epoch [41/120    avg_loss:0.069, val_acc:0.907]
Epoch [42/120    avg_loss:0.050, val_acc:0.964]
Epoch [43/120    avg_loss:0.024, val_acc:0.981]
Epoch [44/120    avg_loss:0.032, val_acc:0.976]
Epoch [45/120    avg_loss:0.020, val_acc:0.975]
Epoch [46/120    avg_loss:0.021, val_acc:0.974]
Epoch [47/120    avg_loss:0.036, val_acc:0.961]
Epoch [48/120    avg_loss:0.034, val_acc:0.972]
Epoch [49/120    avg_loss:0.027, val_acc:0.959]
Epoch [50/120    avg_loss:0.016, val_acc:0.981]
Epoch [51/120    avg_loss:0.030, val_acc:0.977]
Epoch [52/120    avg_loss:0.031, val_acc:0.977]
Epoch [53/120    avg_loss:0.031, val_acc:0.968]
Epoch [54/120    avg_loss:0.039, val_acc:0.970]
Epoch [55/120    avg_loss:0.037, val_acc:0.976]
Epoch [56/120    avg_loss:0.026, val_acc:0.975]
Epoch [57/120    avg_loss:0.017, val_acc:0.973]
Epoch [58/120    avg_loss:0.025, val_acc:0.974]
Epoch [59/120    avg_loss:0.029, val_acc:0.916]
Epoch [60/120    avg_loss:0.094, val_acc:0.960]
Epoch [61/120    avg_loss:0.050, val_acc:0.969]
Epoch [62/120    avg_loss:0.023, val_acc:0.973]
Epoch [63/120    avg_loss:0.026, val_acc:0.981]
Epoch [64/120    avg_loss:0.019, val_acc:0.983]
Epoch [65/120    avg_loss:0.017, val_acc:0.984]
Epoch [66/120    avg_loss:0.014, val_acc:0.979]
Epoch [67/120    avg_loss:0.013, val_acc:0.984]
Epoch [68/120    avg_loss:0.017, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.013, val_acc:0.956]
Epoch [71/120    avg_loss:0.059, val_acc:0.969]
Epoch [72/120    avg_loss:0.025, val_acc:0.986]
Epoch [73/120    avg_loss:0.016, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.981]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.028, val_acc:0.978]
Epoch [80/120    avg_loss:0.019, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.113, val_acc:0.952]
Epoch [83/120    avg_loss:0.052, val_acc:0.984]
Epoch [84/120    avg_loss:0.028, val_acc:0.974]
Epoch [85/120    avg_loss:0.042, val_acc:0.978]
Epoch [86/120    avg_loss:0.034, val_acc:0.982]
Epoch [87/120    avg_loss:0.049, val_acc:0.965]
Epoch [88/120    avg_loss:0.025, val_acc:0.984]
Epoch [89/120    avg_loss:0.019, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.984]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.013, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.011, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6299     0     1     0     0    34     0    98     0]
 [    0     0 18057     0    25     0     6     0     2     0]
 [    0     7     0  1957     0     0     0     0    66     6]
 [    0    28     2     0  2917    14     6     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     3     0     0  4866     0     4     0]
 [    0     2     0     0     0     0     1  1287     0     0]
 [    0    12     0    17    47     0     0     0  3492     3]
 [    0     0     0     0    14    10     0     0     0   895]]

Accuracy:
98.99260116164173

F1 scores:
[       nan 0.985759   0.99889362 0.97508719 0.97640167 0.99088838
 0.99397406 0.99883586 0.96490743 0.98189797]

Kappa:
0.9866565879303597
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd753019908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.879, val_acc:0.322]
Epoch [2/120    avg_loss:1.307, val_acc:0.593]
Epoch [3/120    avg_loss:1.009, val_acc:0.527]
Epoch [4/120    avg_loss:0.740, val_acc:0.708]
Epoch [5/120    avg_loss:0.546, val_acc:0.767]
Epoch [6/120    avg_loss:0.444, val_acc:0.819]
Epoch [7/120    avg_loss:0.371, val_acc:0.847]
Epoch [8/120    avg_loss:0.335, val_acc:0.728]
Epoch [9/120    avg_loss:0.312, val_acc:0.826]
Epoch [10/120    avg_loss:0.277, val_acc:0.824]
Epoch [11/120    avg_loss:0.254, val_acc:0.911]
Epoch [12/120    avg_loss:0.200, val_acc:0.903]
Epoch [13/120    avg_loss:0.212, val_acc:0.907]
Epoch [14/120    avg_loss:0.183, val_acc:0.915]
Epoch [15/120    avg_loss:0.176, val_acc:0.886]
Epoch [16/120    avg_loss:0.202, val_acc:0.863]
Epoch [17/120    avg_loss:0.190, val_acc:0.883]
Epoch [18/120    avg_loss:0.176, val_acc:0.877]
Epoch [19/120    avg_loss:0.132, val_acc:0.927]
Epoch [20/120    avg_loss:0.118, val_acc:0.935]
Epoch [21/120    avg_loss:0.104, val_acc:0.934]
Epoch [22/120    avg_loss:0.140, val_acc:0.907]
Epoch [23/120    avg_loss:0.114, val_acc:0.943]
Epoch [24/120    avg_loss:0.094, val_acc:0.948]
Epoch [25/120    avg_loss:0.100, val_acc:0.910]
Epoch [26/120    avg_loss:0.099, val_acc:0.926]
Epoch [27/120    avg_loss:0.077, val_acc:0.942]
Epoch [28/120    avg_loss:0.063, val_acc:0.930]
Epoch [29/120    avg_loss:0.097, val_acc:0.948]
Epoch [30/120    avg_loss:0.154, val_acc:0.926]
Epoch [31/120    avg_loss:0.085, val_acc:0.950]
Epoch [32/120    avg_loss:0.070, val_acc:0.873]
Epoch [33/120    avg_loss:0.074, val_acc:0.957]
Epoch [34/120    avg_loss:0.078, val_acc:0.955]
Epoch [35/120    avg_loss:0.055, val_acc:0.961]
Epoch [36/120    avg_loss:0.046, val_acc:0.948]
Epoch [37/120    avg_loss:0.044, val_acc:0.935]
Epoch [38/120    avg_loss:0.046, val_acc:0.963]
Epoch [39/120    avg_loss:0.048, val_acc:0.961]
Epoch [40/120    avg_loss:0.060, val_acc:0.969]
Epoch [41/120    avg_loss:0.023, val_acc:0.968]
Epoch [42/120    avg_loss:0.037, val_acc:0.953]
Epoch [43/120    avg_loss:0.047, val_acc:0.968]
Epoch [44/120    avg_loss:0.030, val_acc:0.966]
Epoch [45/120    avg_loss:0.063, val_acc:0.965]
Epoch [46/120    avg_loss:0.046, val_acc:0.957]
Epoch [47/120    avg_loss:0.048, val_acc:0.953]
Epoch [48/120    avg_loss:0.056, val_acc:0.957]
Epoch [49/120    avg_loss:0.042, val_acc:0.962]
Epoch [50/120    avg_loss:0.034, val_acc:0.963]
Epoch [51/120    avg_loss:0.026, val_acc:0.973]
Epoch [52/120    avg_loss:0.018, val_acc:0.973]
Epoch [53/120    avg_loss:0.025, val_acc:0.968]
Epoch [54/120    avg_loss:0.021, val_acc:0.978]
Epoch [55/120    avg_loss:0.027, val_acc:0.967]
Epoch [56/120    avg_loss:0.039, val_acc:0.973]
Epoch [57/120    avg_loss:0.023, val_acc:0.978]
Epoch [58/120    avg_loss:0.014, val_acc:0.969]
Epoch [59/120    avg_loss:0.014, val_acc:0.978]
Epoch [60/120    avg_loss:0.014, val_acc:0.983]
Epoch [61/120    avg_loss:0.015, val_acc:0.979]
Epoch [62/120    avg_loss:0.019, val_acc:0.960]
Epoch [63/120    avg_loss:0.030, val_acc:0.971]
Epoch [64/120    avg_loss:0.018, val_acc:0.976]
Epoch [65/120    avg_loss:0.019, val_acc:0.981]
Epoch [66/120    avg_loss:0.021, val_acc:0.973]
Epoch [67/120    avg_loss:0.012, val_acc:0.978]
Epoch [68/120    avg_loss:0.012, val_acc:0.979]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.005, val_acc:0.982]
Epoch [71/120    avg_loss:0.011, val_acc:0.978]
Epoch [72/120    avg_loss:0.023, val_acc:0.973]
Epoch [73/120    avg_loss:0.024, val_acc:0.968]
Epoch [74/120    avg_loss:0.020, val_acc:0.977]
Epoch [75/120    avg_loss:0.035, val_acc:0.976]
Epoch [76/120    avg_loss:0.012, val_acc:0.977]
Epoch [77/120    avg_loss:0.021, val_acc:0.978]
Epoch [78/120    avg_loss:0.007, val_acc:0.981]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.005, val_acc:0.982]
Epoch [81/120    avg_loss:0.004, val_acc:0.978]
Epoch [82/120    avg_loss:0.031, val_acc:0.948]
Epoch [83/120    avg_loss:0.023, val_acc:0.972]
Epoch [84/120    avg_loss:0.011, val_acc:0.975]
Epoch [85/120    avg_loss:0.007, val_acc:0.977]
Epoch [86/120    avg_loss:0.010, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.980]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.006, val_acc:0.981]
Epoch [90/120    avg_loss:0.007, val_acc:0.981]
Epoch [91/120    avg_loss:0.007, val_acc:0.979]
Epoch [92/120    avg_loss:0.006, val_acc:0.979]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.980]
Epoch [95/120    avg_loss:0.005, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.979]
Epoch [97/120    avg_loss:0.006, val_acc:0.979]
Epoch [98/120    avg_loss:0.006, val_acc:0.979]
Epoch [99/120    avg_loss:0.005, val_acc:0.979]
Epoch [100/120    avg_loss:0.007, val_acc:0.979]
Epoch [101/120    avg_loss:0.006, val_acc:0.979]
Epoch [102/120    avg_loss:0.005, val_acc:0.979]
Epoch [103/120    avg_loss:0.005, val_acc:0.980]
Epoch [104/120    avg_loss:0.005, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.980]
Epoch [106/120    avg_loss:0.004, val_acc:0.980]
Epoch [107/120    avg_loss:0.006, val_acc:0.980]
Epoch [108/120    avg_loss:0.004, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.005, val_acc:0.980]
Epoch [111/120    avg_loss:0.005, val_acc:0.980]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.005, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.980]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.005, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6331     0     0     0     0    24     4    73     0]
 [    0     1 18054     0    18     0    13     0     3     1]
 [    0     1     0  1948     0     0     0     0    82     5]
 [    0    33     0     0  2932     0     2     0     4     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     6     0  4869     0     0     0]
 [    0    14     0     0     0     0     0  1274     1     1]
 [    0    17     0    51    27     0     0     0  3460    16]
 [    0     0     0     0     4    24     0     0     0   891]]

Accuracy:
98.96609066589545

F1 scores:
[       nan 0.98698262 0.99892107 0.96555143 0.98405773 0.99088838
 0.99509503 0.99221184 0.96191271 0.97164667]

Kappa:
0.9863054378480111
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6bb06b1898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.810, val_acc:0.643]
Epoch [2/120    avg_loss:1.282, val_acc:0.667]
Epoch [3/120    avg_loss:0.982, val_acc:0.583]
Epoch [4/120    avg_loss:0.758, val_acc:0.789]
Epoch [5/120    avg_loss:0.581, val_acc:0.777]
Epoch [6/120    avg_loss:0.471, val_acc:0.772]
Epoch [7/120    avg_loss:0.405, val_acc:0.814]
Epoch [8/120    avg_loss:0.342, val_acc:0.824]
Epoch [9/120    avg_loss:0.325, val_acc:0.817]
Epoch [10/120    avg_loss:0.296, val_acc:0.870]
Epoch [11/120    avg_loss:0.226, val_acc:0.900]
Epoch [12/120    avg_loss:0.263, val_acc:0.903]
Epoch [13/120    avg_loss:0.214, val_acc:0.925]
Epoch [14/120    avg_loss:0.182, val_acc:0.922]
Epoch [15/120    avg_loss:0.159, val_acc:0.947]
Epoch [16/120    avg_loss:0.206, val_acc:0.894]
Epoch [17/120    avg_loss:0.215, val_acc:0.932]
Epoch [18/120    avg_loss:0.154, val_acc:0.924]
Epoch [19/120    avg_loss:0.174, val_acc:0.929]
Epoch [20/120    avg_loss:0.102, val_acc:0.966]
Epoch [21/120    avg_loss:0.114, val_acc:0.954]
Epoch [22/120    avg_loss:0.094, val_acc:0.934]
Epoch [23/120    avg_loss:0.116, val_acc:0.925]
Epoch [24/120    avg_loss:0.081, val_acc:0.963]
Epoch [25/120    avg_loss:0.082, val_acc:0.957]
Epoch [26/120    avg_loss:0.071, val_acc:0.960]
Epoch [27/120    avg_loss:0.079, val_acc:0.929]
Epoch [28/120    avg_loss:0.080, val_acc:0.958]
Epoch [29/120    avg_loss:0.070, val_acc:0.932]
Epoch [30/120    avg_loss:0.057, val_acc:0.977]
Epoch [31/120    avg_loss:0.054, val_acc:0.978]
Epoch [32/120    avg_loss:0.044, val_acc:0.964]
Epoch [33/120    avg_loss:0.048, val_acc:0.977]
Epoch [34/120    avg_loss:0.039, val_acc:0.967]
Epoch [35/120    avg_loss:0.036, val_acc:0.972]
Epoch [36/120    avg_loss:0.066, val_acc:0.971]
Epoch [37/120    avg_loss:0.050, val_acc:0.922]
Epoch [38/120    avg_loss:0.041, val_acc:0.983]
Epoch [39/120    avg_loss:0.057, val_acc:0.966]
Epoch [40/120    avg_loss:0.036, val_acc:0.983]
Epoch [41/120    avg_loss:0.036, val_acc:0.982]
Epoch [42/120    avg_loss:0.042, val_acc:0.958]
Epoch [43/120    avg_loss:0.050, val_acc:0.963]
Epoch [44/120    avg_loss:0.027, val_acc:0.978]
Epoch [45/120    avg_loss:0.019, val_acc:0.963]
Epoch [46/120    avg_loss:0.021, val_acc:0.979]
Epoch [47/120    avg_loss:0.024, val_acc:0.966]
Epoch [48/120    avg_loss:0.032, val_acc:0.980]
Epoch [49/120    avg_loss:0.027, val_acc:0.973]
Epoch [50/120    avg_loss:0.028, val_acc:0.977]
Epoch [51/120    avg_loss:0.107, val_acc:0.957]
Epoch [52/120    avg_loss:0.041, val_acc:0.926]
Epoch [53/120    avg_loss:0.066, val_acc:0.960]
Epoch [54/120    avg_loss:0.047, val_acc:0.970]
Epoch [55/120    avg_loss:0.029, val_acc:0.975]
Epoch [56/120    avg_loss:0.025, val_acc:0.977]
Epoch [57/120    avg_loss:0.027, val_acc:0.979]
Epoch [58/120    avg_loss:0.025, val_acc:0.978]
Epoch [59/120    avg_loss:0.020, val_acc:0.977]
Epoch [60/120    avg_loss:0.022, val_acc:0.978]
Epoch [61/120    avg_loss:0.022, val_acc:0.979]
Epoch [62/120    avg_loss:0.017, val_acc:0.980]
Epoch [63/120    avg_loss:0.024, val_acc:0.976]
Epoch [64/120    avg_loss:0.016, val_acc:0.980]
Epoch [65/120    avg_loss:0.016, val_acc:0.978]
Epoch [66/120    avg_loss:0.017, val_acc:0.983]
Epoch [67/120    avg_loss:0.023, val_acc:0.983]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.015, val_acc:0.981]
Epoch [70/120    avg_loss:0.019, val_acc:0.980]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.016, val_acc:0.983]
Epoch [73/120    avg_loss:0.017, val_acc:0.983]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.018, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.983]
Epoch [77/120    avg_loss:0.016, val_acc:0.982]
Epoch [78/120    avg_loss:0.014, val_acc:0.984]
Epoch [79/120    avg_loss:0.018, val_acc:0.983]
Epoch [80/120    avg_loss:0.016, val_acc:0.982]
Epoch [81/120    avg_loss:0.013, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.016, val_acc:0.981]
Epoch [84/120    avg_loss:0.015, val_acc:0.983]
Epoch [85/120    avg_loss:0.013, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.983]
Epoch [87/120    avg_loss:0.016, val_acc:0.980]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.014, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.985]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.014, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.987]
Epoch [99/120    avg_loss:0.013, val_acc:0.986]
Epoch [100/120    avg_loss:0.013, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.987]
Epoch [102/120    avg_loss:0.012, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.019, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.012, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.017, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6239     0    40     2     0    27     1   118     5]
 [    0     0 17879     0    46     0   161     0     4     0]
 [    0     1     0  1996     0     0     0     0    35     4]
 [    0    17     8     0  2932     0     1     0    12     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0     2     0     0     0     0     2  1276     5     5]
 [    0     7     0    13    61     0     0     0  3489     1]
 [    0     0     0     0    14    13     0     0     0   892]]

Accuracy:
98.52505241848023

F1 scores:
[       nan 0.98267444 0.99391278 0.97723378 0.97295504 0.99504384
 0.98028566 0.9941566  0.9639453  0.97592998]

Kappa:
0.9805044003044855
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb7de9c6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.838, val_acc:0.355]
Epoch [2/120    avg_loss:1.345, val_acc:0.677]
Epoch [3/120    avg_loss:1.000, val_acc:0.658]
Epoch [4/120    avg_loss:0.746, val_acc:0.744]
Epoch [5/120    avg_loss:0.597, val_acc:0.768]
Epoch [6/120    avg_loss:0.502, val_acc:0.782]
Epoch [7/120    avg_loss:0.431, val_acc:0.816]
Epoch [8/120    avg_loss:0.369, val_acc:0.793]
Epoch [9/120    avg_loss:0.287, val_acc:0.827]
Epoch [10/120    avg_loss:0.336, val_acc:0.855]
Epoch [11/120    avg_loss:0.338, val_acc:0.849]
Epoch [12/120    avg_loss:0.293, val_acc:0.838]
Epoch [13/120    avg_loss:0.245, val_acc:0.877]
Epoch [14/120    avg_loss:0.212, val_acc:0.875]
Epoch [15/120    avg_loss:0.191, val_acc:0.901]
Epoch [16/120    avg_loss:0.149, val_acc:0.915]
Epoch [17/120    avg_loss:0.197, val_acc:0.883]
Epoch [18/120    avg_loss:0.225, val_acc:0.910]
Epoch [19/120    avg_loss:0.243, val_acc:0.842]
Epoch [20/120    avg_loss:0.235, val_acc:0.915]
Epoch [21/120    avg_loss:0.199, val_acc:0.928]
Epoch [22/120    avg_loss:0.165, val_acc:0.904]
Epoch [23/120    avg_loss:0.119, val_acc:0.915]
Epoch [24/120    avg_loss:0.102, val_acc:0.954]
Epoch [25/120    avg_loss:0.108, val_acc:0.941]
Epoch [26/120    avg_loss:0.115, val_acc:0.952]
Epoch [27/120    avg_loss:0.092, val_acc:0.923]
Epoch [28/120    avg_loss:0.085, val_acc:0.964]
Epoch [29/120    avg_loss:0.074, val_acc:0.968]
Epoch [30/120    avg_loss:0.078, val_acc:0.957]
Epoch [31/120    avg_loss:0.064, val_acc:0.955]
Epoch [32/120    avg_loss:0.077, val_acc:0.943]
Epoch [33/120    avg_loss:0.064, val_acc:0.967]
Epoch [34/120    avg_loss:0.040, val_acc:0.968]
Epoch [35/120    avg_loss:0.032, val_acc:0.978]
Epoch [36/120    avg_loss:0.073, val_acc:0.965]
Epoch [37/120    avg_loss:0.050, val_acc:0.976]
Epoch [38/120    avg_loss:0.043, val_acc:0.970]
Epoch [39/120    avg_loss:0.035, val_acc:0.955]
Epoch [40/120    avg_loss:0.031, val_acc:0.978]
Epoch [41/120    avg_loss:0.047, val_acc:0.950]
Epoch [42/120    avg_loss:0.062, val_acc:0.974]
Epoch [43/120    avg_loss:0.033, val_acc:0.982]
Epoch [44/120    avg_loss:0.063, val_acc:0.898]
Epoch [45/120    avg_loss:0.104, val_acc:0.930]
Epoch [46/120    avg_loss:0.063, val_acc:0.976]
Epoch [47/120    avg_loss:0.037, val_acc:0.983]
Epoch [48/120    avg_loss:0.028, val_acc:0.976]
Epoch [49/120    avg_loss:0.046, val_acc:0.981]
Epoch [50/120    avg_loss:0.047, val_acc:0.964]
Epoch [51/120    avg_loss:0.027, val_acc:0.976]
Epoch [52/120    avg_loss:0.030, val_acc:0.976]
Epoch [53/120    avg_loss:0.130, val_acc:0.953]
Epoch [54/120    avg_loss:0.047, val_acc:0.951]
Epoch [55/120    avg_loss:0.038, val_acc:0.976]
Epoch [56/120    avg_loss:0.041, val_acc:0.967]
Epoch [57/120    avg_loss:0.041, val_acc:0.976]
Epoch [58/120    avg_loss:0.024, val_acc:0.981]
Epoch [59/120    avg_loss:0.026, val_acc:0.972]
Epoch [60/120    avg_loss:0.033, val_acc:0.978]
Epoch [61/120    avg_loss:0.014, val_acc:0.981]
Epoch [62/120    avg_loss:0.014, val_acc:0.981]
Epoch [63/120    avg_loss:0.019, val_acc:0.979]
Epoch [64/120    avg_loss:0.015, val_acc:0.984]
Epoch [65/120    avg_loss:0.017, val_acc:0.980]
Epoch [66/120    avg_loss:0.014, val_acc:0.981]
Epoch [67/120    avg_loss:0.012, val_acc:0.982]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.009, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.014, val_acc:0.986]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.011, val_acc:0.983]
Epoch [77/120    avg_loss:0.010, val_acc:0.983]
Epoch [78/120    avg_loss:0.010, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.016, val_acc:0.981]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.012, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.007, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.987]
Epoch [117/120    avg_loss:0.012, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6305     0     3     0     0     2     4   111     7]
 [    0     0 17985     0    79     0    21     0     5     0]
 [    0     4     0  1934     1     0     0     0    93     4]
 [    0    24     7     8  2918     0     9     1     0     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     2     0  4869     0     0     0]
 [    0     5     0     0     0     0     0  1285     0     0]
 [    0    45     0    45    52     0     0     0  3429     0]
 [    0     0     0     0    14    18     0     0     0   887]]

Accuracy:
98.61181404092257

F1 scores:
[       nan 0.98400312 0.9967026  0.96075509 0.96654521 0.99315068
 0.99580734 0.99612403 0.95131086 0.97365532]

Kappa:
0.9816257135729997
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f00ee387898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.830, val_acc:0.562]
Epoch [2/120    avg_loss:1.248, val_acc:0.491]
Epoch [3/120    avg_loss:0.915, val_acc:0.537]
Epoch [4/120    avg_loss:0.672, val_acc:0.761]
Epoch [5/120    avg_loss:0.541, val_acc:0.800]
Epoch [6/120    avg_loss:0.399, val_acc:0.820]
Epoch [7/120    avg_loss:0.366, val_acc:0.778]
Epoch [8/120    avg_loss:0.309, val_acc:0.867]
Epoch [9/120    avg_loss:0.258, val_acc:0.893]
Epoch [10/120    avg_loss:0.234, val_acc:0.882]
Epoch [11/120    avg_loss:0.210, val_acc:0.897]
Epoch [12/120    avg_loss:0.222, val_acc:0.871]
Epoch [13/120    avg_loss:0.254, val_acc:0.763]
Epoch [14/120    avg_loss:0.221, val_acc:0.869]
Epoch [15/120    avg_loss:0.227, val_acc:0.903]
Epoch [16/120    avg_loss:0.186, val_acc:0.918]
Epoch [17/120    avg_loss:0.122, val_acc:0.918]
Epoch [18/120    avg_loss:0.129, val_acc:0.937]
Epoch [19/120    avg_loss:0.136, val_acc:0.922]
Epoch [20/120    avg_loss:0.150, val_acc:0.892]
Epoch [21/120    avg_loss:0.125, val_acc:0.948]
Epoch [22/120    avg_loss:0.142, val_acc:0.915]
Epoch [23/120    avg_loss:0.111, val_acc:0.941]
Epoch [24/120    avg_loss:0.099, val_acc:0.955]
Epoch [25/120    avg_loss:0.098, val_acc:0.952]
Epoch [26/120    avg_loss:0.140, val_acc:0.942]
Epoch [27/120    avg_loss:0.108, val_acc:0.860]
Epoch [28/120    avg_loss:0.098, val_acc:0.953]
Epoch [29/120    avg_loss:0.091, val_acc:0.949]
Epoch [30/120    avg_loss:0.139, val_acc:0.883]
Epoch [31/120    avg_loss:0.088, val_acc:0.962]
Epoch [32/120    avg_loss:0.059, val_acc:0.965]
Epoch [33/120    avg_loss:0.046, val_acc:0.953]
Epoch [34/120    avg_loss:0.078, val_acc:0.955]
Epoch [35/120    avg_loss:0.045, val_acc:0.959]
Epoch [36/120    avg_loss:0.062, val_acc:0.938]
Epoch [37/120    avg_loss:0.046, val_acc:0.969]
Epoch [38/120    avg_loss:0.038, val_acc:0.970]
Epoch [39/120    avg_loss:0.045, val_acc:0.963]
Epoch [40/120    avg_loss:0.063, val_acc:0.973]
Epoch [41/120    avg_loss:0.034, val_acc:0.968]
Epoch [42/120    avg_loss:0.026, val_acc:0.973]
Epoch [43/120    avg_loss:0.034, val_acc:0.968]
Epoch [44/120    avg_loss:0.057, val_acc:0.970]
Epoch [45/120    avg_loss:0.030, val_acc:0.972]
Epoch [46/120    avg_loss:0.019, val_acc:0.975]
Epoch [47/120    avg_loss:0.021, val_acc:0.973]
Epoch [48/120    avg_loss:0.028, val_acc:0.978]
Epoch [49/120    avg_loss:0.029, val_acc:0.978]
Epoch [50/120    avg_loss:0.030, val_acc:0.968]
Epoch [51/120    avg_loss:0.030, val_acc:0.976]
Epoch [52/120    avg_loss:0.038, val_acc:0.973]
Epoch [53/120    avg_loss:0.066, val_acc:0.868]
Epoch [54/120    avg_loss:0.131, val_acc:0.963]
Epoch [55/120    avg_loss:0.085, val_acc:0.963]
Epoch [56/120    avg_loss:0.033, val_acc:0.976]
Epoch [57/120    avg_loss:0.025, val_acc:0.979]
Epoch [58/120    avg_loss:0.030, val_acc:0.977]
Epoch [59/120    avg_loss:0.017, val_acc:0.975]
Epoch [60/120    avg_loss:0.039, val_acc:0.963]
Epoch [61/120    avg_loss:0.052, val_acc:0.968]
Epoch [62/120    avg_loss:0.028, val_acc:0.977]
Epoch [63/120    avg_loss:0.033, val_acc:0.967]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.016, val_acc:0.980]
Epoch [66/120    avg_loss:0.012, val_acc:0.981]
Epoch [67/120    avg_loss:0.015, val_acc:0.962]
Epoch [68/120    avg_loss:0.012, val_acc:0.975]
Epoch [69/120    avg_loss:0.020, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.981]
Epoch [71/120    avg_loss:0.020, val_acc:0.972]
Epoch [72/120    avg_loss:0.026, val_acc:0.978]
Epoch [73/120    avg_loss:0.014, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.981]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.983]
Epoch [77/120    avg_loss:0.027, val_acc:0.946]
Epoch [78/120    avg_loss:0.034, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.011, val_acc:0.982]
Epoch [81/120    avg_loss:0.015, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.011, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.003, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.005, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.948]
Epoch [98/120    avg_loss:0.020, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.170, val_acc:0.912]
Epoch [102/120    avg_loss:0.078, val_acc:0.964]
Epoch [103/120    avg_loss:0.046, val_acc:0.980]
Epoch [104/120    avg_loss:0.063, val_acc:0.964]
Epoch [105/120    avg_loss:0.031, val_acc:0.983]
Epoch [106/120    avg_loss:0.021, val_acc:0.983]
Epoch [107/120    avg_loss:0.014, val_acc:0.980]
Epoch [108/120    avg_loss:0.014, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.018, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6336     0     7     1     0    26     4    58     0]
 [    0     0 18069     0     3     0    18     0     0     0]
 [    0     1     0  1990     0     0     0     0    38     7]
 [    0    22     2     0  2932     0     9     0     7     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4863     0     8     0]
 [    0     8     0     0     0     0     2  1273     0     7]
 [    0     2    18    75    49     0     4     0  3420     3]
 [    0     0     0     0     6    16     0     0     0   897]]

Accuracy:
99.01670161232015

F1 scores:
[       nan 0.98992266 0.99867352 0.96884129 0.98339762 0.99390708
 0.99244898 0.99181924 0.96310898 0.9787234 ]

Kappa:
0.986971661050349
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f857fca8898>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.808, val_acc:0.283]
Epoch [2/120    avg_loss:1.285, val_acc:0.391]
Epoch [3/120    avg_loss:0.973, val_acc:0.624]
Epoch [4/120    avg_loss:0.744, val_acc:0.698]
Epoch [5/120    avg_loss:0.569, val_acc:0.702]
Epoch [6/120    avg_loss:0.476, val_acc:0.765]
Epoch [7/120    avg_loss:0.382, val_acc:0.801]
Epoch [8/120    avg_loss:0.369, val_acc:0.854]
Epoch [9/120    avg_loss:0.363, val_acc:0.868]
Epoch [10/120    avg_loss:0.291, val_acc:0.896]
Epoch [11/120    avg_loss:0.291, val_acc:0.902]
Epoch [12/120    avg_loss:0.228, val_acc:0.926]
Epoch [13/120    avg_loss:0.214, val_acc:0.851]
Epoch [14/120    avg_loss:0.221, val_acc:0.842]
Epoch [15/120    avg_loss:0.194, val_acc:0.924]
Epoch [16/120    avg_loss:0.152, val_acc:0.938]
Epoch [17/120    avg_loss:0.195, val_acc:0.823]
Epoch [18/120    avg_loss:0.199, val_acc:0.925]
Epoch [19/120    avg_loss:0.105, val_acc:0.939]
Epoch [20/120    avg_loss:0.119, val_acc:0.957]
Epoch [21/120    avg_loss:0.094, val_acc:0.923]
Epoch [22/120    avg_loss:0.091, val_acc:0.925]
Epoch [23/120    avg_loss:0.149, val_acc:0.926]
Epoch [24/120    avg_loss:0.108, val_acc:0.954]
Epoch [25/120    avg_loss:0.072, val_acc:0.912]
Epoch [26/120    avg_loss:0.083, val_acc:0.966]
Epoch [27/120    avg_loss:0.133, val_acc:0.951]
Epoch [28/120    avg_loss:0.076, val_acc:0.962]
Epoch [29/120    avg_loss:0.058, val_acc:0.961]
Epoch [30/120    avg_loss:0.048, val_acc:0.964]
Epoch [31/120    avg_loss:0.113, val_acc:0.956]
Epoch [32/120    avg_loss:0.064, val_acc:0.969]
Epoch [33/120    avg_loss:0.043, val_acc:0.964]
Epoch [34/120    avg_loss:0.046, val_acc:0.940]
Epoch [35/120    avg_loss:0.060, val_acc:0.960]
Epoch [36/120    avg_loss:0.043, val_acc:0.961]
Epoch [37/120    avg_loss:0.037, val_acc:0.972]
Epoch [38/120    avg_loss:0.034, val_acc:0.967]
Epoch [39/120    avg_loss:0.061, val_acc:0.959]
Epoch [40/120    avg_loss:0.048, val_acc:0.971]
Epoch [41/120    avg_loss:0.035, val_acc:0.975]
Epoch [42/120    avg_loss:0.055, val_acc:0.914]
Epoch [43/120    avg_loss:0.052, val_acc:0.973]
Epoch [44/120    avg_loss:0.029, val_acc:0.974]
Epoch [45/120    avg_loss:0.022, val_acc:0.972]
Epoch [46/120    avg_loss:0.061, val_acc:0.951]
Epoch [47/120    avg_loss:0.047, val_acc:0.962]
Epoch [48/120    avg_loss:0.020, val_acc:0.979]
Epoch [49/120    avg_loss:0.030, val_acc:0.969]
Epoch [50/120    avg_loss:0.027, val_acc:0.976]
Epoch [51/120    avg_loss:0.023, val_acc:0.967]
Epoch [52/120    avg_loss:0.020, val_acc:0.974]
Epoch [53/120    avg_loss:0.027, val_acc:0.971]
Epoch [54/120    avg_loss:0.027, val_acc:0.961]
Epoch [55/120    avg_loss:0.027, val_acc:0.976]
Epoch [56/120    avg_loss:0.013, val_acc:0.983]
Epoch [57/120    avg_loss:0.028, val_acc:0.977]
Epoch [58/120    avg_loss:0.035, val_acc:0.981]
Epoch [59/120    avg_loss:0.016, val_acc:0.980]
Epoch [60/120    avg_loss:0.015, val_acc:0.984]
Epoch [61/120    avg_loss:0.016, val_acc:0.964]
Epoch [62/120    avg_loss:0.026, val_acc:0.973]
Epoch [63/120    avg_loss:0.021, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.985]
Epoch [65/120    avg_loss:0.013, val_acc:0.980]
Epoch [66/120    avg_loss:0.013, val_acc:0.980]
Epoch [67/120    avg_loss:0.012, val_acc:0.979]
Epoch [68/120    avg_loss:0.018, val_acc:0.981]
Epoch [69/120    avg_loss:0.009, val_acc:0.979]
Epoch [70/120    avg_loss:0.010, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.981]
Epoch [72/120    avg_loss:0.016, val_acc:0.967]
Epoch [73/120    avg_loss:0.123, val_acc:0.914]
Epoch [74/120    avg_loss:0.063, val_acc:0.954]
Epoch [75/120    avg_loss:0.037, val_acc:0.961]
Epoch [76/120    avg_loss:0.031, val_acc:0.967]
Epoch [77/120    avg_loss:0.023, val_acc:0.980]
Epoch [78/120    avg_loss:0.045, val_acc:0.982]
Epoch [79/120    avg_loss:0.016, val_acc:0.979]
Epoch [80/120    avg_loss:0.015, val_acc:0.975]
Epoch [81/120    avg_loss:0.019, val_acc:0.983]
Epoch [82/120    avg_loss:0.015, val_acc:0.979]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.014, val_acc:0.982]
Epoch [88/120    avg_loss:0.014, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.981]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.014, val_acc:0.982]
Epoch [92/120    avg_loss:0.013, val_acc:0.982]
Epoch [93/120    avg_loss:0.011, val_acc:0.982]
Epoch [94/120    avg_loss:0.017, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.014, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.011, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.982]
Epoch [112/120    avg_loss:0.017, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.015, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6340     0     3     1     0    20     3    65     0]
 [    0     0 17844     0    77     0   168     0     1     0]
 [    0     1     0  1969     0     0     0     0    63     3]
 [    0    25    16     0  2897     0     6     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4875     0     0     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0     7     0     4    73     0     0     0  3478     9]
 [    0     0     0     0    11    21     0     0     0   887]]

Accuracy:
98.53228255368376

F1 scores:
[       nan 0.99016086 0.99262927 0.98155533 0.96070303 0.99201824
 0.98019503 0.99845081 0.96530669 0.97579758]

Kappa:
0.9805992087881983
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71d43b3908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.920, val_acc:0.334]
Epoch [2/120    avg_loss:1.378, val_acc:0.396]
Epoch [3/120    avg_loss:1.081, val_acc:0.497]
Epoch [4/120    avg_loss:0.865, val_acc:0.616]
Epoch [5/120    avg_loss:0.675, val_acc:0.656]
Epoch [6/120    avg_loss:0.509, val_acc:0.787]
Epoch [7/120    avg_loss:0.456, val_acc:0.889]
Epoch [8/120    avg_loss:0.326, val_acc:0.888]
Epoch [9/120    avg_loss:0.322, val_acc:0.863]
Epoch [10/120    avg_loss:0.270, val_acc:0.897]
Epoch [11/120    avg_loss:0.232, val_acc:0.913]
Epoch [12/120    avg_loss:0.226, val_acc:0.905]
Epoch [13/120    avg_loss:0.249, val_acc:0.908]
Epoch [14/120    avg_loss:0.227, val_acc:0.820]
Epoch [15/120    avg_loss:0.182, val_acc:0.876]
Epoch [16/120    avg_loss:0.178, val_acc:0.904]
Epoch [17/120    avg_loss:0.150, val_acc:0.940]
Epoch [18/120    avg_loss:0.130, val_acc:0.932]
Epoch [19/120    avg_loss:0.103, val_acc:0.964]
Epoch [20/120    avg_loss:0.103, val_acc:0.850]
Epoch [21/120    avg_loss:0.104, val_acc:0.968]
Epoch [22/120    avg_loss:0.106, val_acc:0.948]
Epoch [23/120    avg_loss:0.106, val_acc:0.939]
Epoch [24/120    avg_loss:0.090, val_acc:0.964]
Epoch [25/120    avg_loss:0.073, val_acc:0.926]
Epoch [26/120    avg_loss:0.074, val_acc:0.951]
Epoch [27/120    avg_loss:0.080, val_acc:0.957]
Epoch [28/120    avg_loss:0.071, val_acc:0.978]
Epoch [29/120    avg_loss:0.057, val_acc:0.961]
Epoch [30/120    avg_loss:0.062, val_acc:0.975]
Epoch [31/120    avg_loss:0.057, val_acc:0.953]
Epoch [32/120    avg_loss:0.043, val_acc:0.972]
Epoch [33/120    avg_loss:0.043, val_acc:0.972]
Epoch [34/120    avg_loss:0.044, val_acc:0.963]
Epoch [35/120    avg_loss:0.034, val_acc:0.969]
Epoch [36/120    avg_loss:0.042, val_acc:0.977]
Epoch [37/120    avg_loss:0.034, val_acc:0.948]
Epoch [38/120    avg_loss:0.027, val_acc:0.966]
Epoch [39/120    avg_loss:0.030, val_acc:0.970]
Epoch [40/120    avg_loss:0.020, val_acc:0.979]
Epoch [41/120    avg_loss:0.037, val_acc:0.970]
Epoch [42/120    avg_loss:0.121, val_acc:0.902]
Epoch [43/120    avg_loss:0.111, val_acc:0.970]
Epoch [44/120    avg_loss:0.093, val_acc:0.937]
Epoch [45/120    avg_loss:0.077, val_acc:0.964]
Epoch [46/120    avg_loss:0.030, val_acc:0.981]
Epoch [47/120    avg_loss:0.047, val_acc:0.949]
Epoch [48/120    avg_loss:0.054, val_acc:0.958]
Epoch [49/120    avg_loss:0.037, val_acc:0.977]
Epoch [50/120    avg_loss:0.039, val_acc:0.944]
Epoch [51/120    avg_loss:0.032, val_acc:0.977]
Epoch [52/120    avg_loss:0.017, val_acc:0.981]
Epoch [53/120    avg_loss:0.016, val_acc:0.984]
Epoch [54/120    avg_loss:0.017, val_acc:0.981]
Epoch [55/120    avg_loss:0.025, val_acc:0.972]
Epoch [56/120    avg_loss:0.029, val_acc:0.984]
Epoch [57/120    avg_loss:0.014, val_acc:0.981]
Epoch [58/120    avg_loss:0.014, val_acc:0.985]
Epoch [59/120    avg_loss:0.013, val_acc:0.982]
Epoch [60/120    avg_loss:0.011, val_acc:0.984]
Epoch [61/120    avg_loss:0.015, val_acc:0.986]
Epoch [62/120    avg_loss:0.033, val_acc:0.976]
Epoch [63/120    avg_loss:0.016, val_acc:0.986]
Epoch [64/120    avg_loss:0.023, val_acc:0.946]
Epoch [65/120    avg_loss:0.015, val_acc:0.986]
Epoch [66/120    avg_loss:0.018, val_acc:0.986]
Epoch [67/120    avg_loss:0.019, val_acc:0.982]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.021, val_acc:0.982]
Epoch [70/120    avg_loss:0.103, val_acc:0.962]
Epoch [71/120    avg_loss:0.083, val_acc:0.916]
Epoch [72/120    avg_loss:0.086, val_acc:0.963]
Epoch [73/120    avg_loss:0.102, val_acc:0.965]
Epoch [74/120    avg_loss:0.039, val_acc:0.981]
Epoch [75/120    avg_loss:0.020, val_acc:0.985]
Epoch [76/120    avg_loss:0.031, val_acc:0.980]
Epoch [77/120    avg_loss:0.018, val_acc:0.986]
Epoch [78/120    avg_loss:0.028, val_acc:0.981]
Epoch [79/120    avg_loss:0.020, val_acc:0.984]
Epoch [80/120    avg_loss:0.019, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.012, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.009, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6331     0     0     5     0    12     0    82     2]
 [    0     0 18026     0    33     0    28     0     3     0]
 [    0     3     0  1991     0     0     0     0    34     8]
 [    0    38    13     0  2906     0     0     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4865     0     2     0]
 [    0     1     0     0     0     0     2  1284     0     3]
 [    0     4     0    26    68     0     0     0  3467     6]
 [    0     0     0     0    14    13     0     0     0   892]]

Accuracy:
98.97332080109898

F1 scores:
[       nan 0.98852369 0.99756502 0.98248211 0.96898966 0.99504384
 0.99437915 0.997669   0.96654586 0.97486339]

Kappa:
0.9864031842245515
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81c12636d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.850, val_acc:0.479]
Epoch [2/120    avg_loss:1.320, val_acc:0.683]
Epoch [3/120    avg_loss:0.959, val_acc:0.606]
Epoch [4/120    avg_loss:0.749, val_acc:0.712]
Epoch [5/120    avg_loss:0.517, val_acc:0.741]
Epoch [6/120    avg_loss:0.458, val_acc:0.824]
Epoch [7/120    avg_loss:0.473, val_acc:0.846]
Epoch [8/120    avg_loss:0.378, val_acc:0.833]
Epoch [9/120    avg_loss:0.302, val_acc:0.838]
Epoch [10/120    avg_loss:0.330, val_acc:0.871]
Epoch [11/120    avg_loss:0.240, val_acc:0.939]
Epoch [12/120    avg_loss:0.211, val_acc:0.939]
Epoch [13/120    avg_loss:0.186, val_acc:0.930]
Epoch [14/120    avg_loss:0.203, val_acc:0.855]
Epoch [15/120    avg_loss:0.196, val_acc:0.908]
Epoch [16/120    avg_loss:0.172, val_acc:0.948]
Epoch [17/120    avg_loss:0.176, val_acc:0.935]
Epoch [18/120    avg_loss:0.118, val_acc:0.936]
Epoch [19/120    avg_loss:0.111, val_acc:0.949]
Epoch [20/120    avg_loss:0.113, val_acc:0.947]
Epoch [21/120    avg_loss:0.111, val_acc:0.948]
Epoch [22/120    avg_loss:0.091, val_acc:0.958]
Epoch [23/120    avg_loss:0.078, val_acc:0.960]
Epoch [24/120    avg_loss:0.078, val_acc:0.951]
Epoch [25/120    avg_loss:0.088, val_acc:0.963]
Epoch [26/120    avg_loss:0.078, val_acc:0.964]
Epoch [27/120    avg_loss:0.067, val_acc:0.922]
Epoch [28/120    avg_loss:0.064, val_acc:0.973]
Epoch [29/120    avg_loss:0.065, val_acc:0.968]
Epoch [30/120    avg_loss:0.052, val_acc:0.968]
Epoch [31/120    avg_loss:0.077, val_acc:0.953]
Epoch [32/120    avg_loss:0.097, val_acc:0.881]
Epoch [33/120    avg_loss:0.068, val_acc:0.974]
Epoch [34/120    avg_loss:0.053, val_acc:0.963]
Epoch [35/120    avg_loss:0.048, val_acc:0.971]
Epoch [36/120    avg_loss:0.032, val_acc:0.981]
Epoch [37/120    avg_loss:0.040, val_acc:0.979]
Epoch [38/120    avg_loss:0.045, val_acc:0.972]
Epoch [39/120    avg_loss:0.031, val_acc:0.978]
Epoch [40/120    avg_loss:0.029, val_acc:0.980]
Epoch [41/120    avg_loss:0.030, val_acc:0.965]
Epoch [42/120    avg_loss:0.022, val_acc:0.977]
Epoch [43/120    avg_loss:0.019, val_acc:0.982]
Epoch [44/120    avg_loss:0.022, val_acc:0.979]
Epoch [45/120    avg_loss:0.026, val_acc:0.975]
Epoch [46/120    avg_loss:0.031, val_acc:0.976]
Epoch [47/120    avg_loss:0.037, val_acc:0.975]
Epoch [48/120    avg_loss:0.053, val_acc:0.952]
Epoch [49/120    avg_loss:0.043, val_acc:0.983]
Epoch [50/120    avg_loss:0.030, val_acc:0.978]
Epoch [51/120    avg_loss:0.021, val_acc:0.981]
Epoch [52/120    avg_loss:0.030, val_acc:0.982]
Epoch [53/120    avg_loss:0.018, val_acc:0.981]
Epoch [54/120    avg_loss:0.018, val_acc:0.974]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.019, val_acc:0.972]
Epoch [57/120    avg_loss:0.018, val_acc:0.983]
Epoch [58/120    avg_loss:0.020, val_acc:0.982]
Epoch [59/120    avg_loss:0.020, val_acc:0.967]
Epoch [60/120    avg_loss:0.039, val_acc:0.983]
Epoch [61/120    avg_loss:0.023, val_acc:0.978]
Epoch [62/120    avg_loss:0.011, val_acc:0.983]
Epoch [63/120    avg_loss:0.010, val_acc:0.986]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.986]
Epoch [66/120    avg_loss:0.014, val_acc:0.986]
Epoch [67/120    avg_loss:0.019, val_acc:0.954]
Epoch [68/120    avg_loss:0.031, val_acc:0.978]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.006, val_acc:0.987]
Epoch [73/120    avg_loss:0.015, val_acc:0.961]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.034, val_acc:0.983]
Epoch [84/120    avg_loss:0.037, val_acc:0.973]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.971]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.004, val_acc:0.991]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.977]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.005, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.990]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.003, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.003, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.002, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.002, val_acc:0.990]
Epoch [119/120    avg_loss:0.002, val_acc:0.990]
Epoch [120/120    avg_loss:0.002, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     2     0     3     1    34     0]
 [    0     0 18079     0    10     0     1     0     0     0]
 [    0     7     0  1991     0     0     0     0    37     1]
 [    0    13     7     0  2935     0     4     0    12     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4875     0     2     0]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     7     0     0    40     0     0     0  3516     8]
 [    0     0     1     0    16    27     0     0     0   875]]

Accuracy:
99.42640927385342

F1 scores:
[       nan 0.9947864  0.9994748  0.98857994 0.98242678 0.98976109
 0.99856616 0.99844841 0.98047964 0.97006652]

Kappa:
0.9923995048942489
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9762509908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.827, val_acc:0.305]
Epoch [2/120    avg_loss:1.326, val_acc:0.429]
Epoch [3/120    avg_loss:0.986, val_acc:0.714]
Epoch [4/120    avg_loss:0.823, val_acc:0.692]
Epoch [5/120    avg_loss:0.600, val_acc:0.759]
Epoch [6/120    avg_loss:0.510, val_acc:0.753]
Epoch [7/120    avg_loss:0.429, val_acc:0.818]
Epoch [8/120    avg_loss:0.365, val_acc:0.819]
Epoch [9/120    avg_loss:0.345, val_acc:0.836]
Epoch [10/120    avg_loss:0.301, val_acc:0.874]
Epoch [11/120    avg_loss:0.231, val_acc:0.929]
Epoch [12/120    avg_loss:0.201, val_acc:0.870]
Epoch [13/120    avg_loss:0.225, val_acc:0.889]
Epoch [14/120    avg_loss:0.228, val_acc:0.904]
Epoch [15/120    avg_loss:0.219, val_acc:0.856]
Epoch [16/120    avg_loss:0.183, val_acc:0.917]
Epoch [17/120    avg_loss:0.176, val_acc:0.933]
Epoch [18/120    avg_loss:0.123, val_acc:0.886]
Epoch [19/120    avg_loss:0.112, val_acc:0.942]
Epoch [20/120    avg_loss:0.191, val_acc:0.889]
Epoch [21/120    avg_loss:0.164, val_acc:0.933]
Epoch [22/120    avg_loss:0.121, val_acc:0.915]
Epoch [23/120    avg_loss:0.106, val_acc:0.893]
Epoch [24/120    avg_loss:0.097, val_acc:0.957]
Epoch [25/120    avg_loss:0.109, val_acc:0.915]
Epoch [26/120    avg_loss:0.083, val_acc:0.923]
Epoch [27/120    avg_loss:0.068, val_acc:0.952]
Epoch [28/120    avg_loss:0.067, val_acc:0.937]
Epoch [29/120    avg_loss:0.067, val_acc:0.951]
Epoch [30/120    avg_loss:0.067, val_acc:0.960]
Epoch [31/120    avg_loss:0.057, val_acc:0.975]
Epoch [32/120    avg_loss:0.061, val_acc:0.955]
Epoch [33/120    avg_loss:0.052, val_acc:0.955]
Epoch [34/120    avg_loss:0.116, val_acc:0.961]
Epoch [35/120    avg_loss:0.047, val_acc:0.946]
Epoch [36/120    avg_loss:0.039, val_acc:0.977]
Epoch [37/120    avg_loss:0.041, val_acc:0.975]
Epoch [38/120    avg_loss:0.060, val_acc:0.971]
Epoch [39/120    avg_loss:0.041, val_acc:0.974]
Epoch [40/120    avg_loss:0.039, val_acc:0.972]
Epoch [41/120    avg_loss:0.041, val_acc:0.979]
Epoch [42/120    avg_loss:0.024, val_acc:0.959]
Epoch [43/120    avg_loss:0.023, val_acc:0.973]
Epoch [44/120    avg_loss:0.024, val_acc:0.975]
Epoch [45/120    avg_loss:0.029, val_acc:0.973]
Epoch [46/120    avg_loss:0.028, val_acc:0.966]
Epoch [47/120    avg_loss:0.033, val_acc:0.965]
Epoch [48/120    avg_loss:0.111, val_acc:0.967]
Epoch [49/120    avg_loss:0.043, val_acc:0.969]
Epoch [50/120    avg_loss:0.028, val_acc:0.938]
Epoch [51/120    avg_loss:0.037, val_acc:0.943]
Epoch [52/120    avg_loss:0.036, val_acc:0.940]
Epoch [53/120    avg_loss:0.021, val_acc:0.974]
Epoch [54/120    avg_loss:0.016, val_acc:0.981]
Epoch [55/120    avg_loss:0.012, val_acc:0.977]
Epoch [56/120    avg_loss:0.015, val_acc:0.979]
Epoch [57/120    avg_loss:0.019, val_acc:0.979]
Epoch [58/120    avg_loss:0.012, val_acc:0.980]
Epoch [59/120    avg_loss:0.023, val_acc:0.978]
Epoch [60/120    avg_loss:0.033, val_acc:0.970]
Epoch [61/120    avg_loss:0.016, val_acc:0.983]
Epoch [62/120    avg_loss:0.020, val_acc:0.980]
Epoch [63/120    avg_loss:0.021, val_acc:0.982]
Epoch [64/120    avg_loss:0.016, val_acc:0.974]
Epoch [65/120    avg_loss:0.013, val_acc:0.981]
Epoch [66/120    avg_loss:0.012, val_acc:0.977]
Epoch [67/120    avg_loss:0.012, val_acc:0.959]
Epoch [68/120    avg_loss:0.012, val_acc:0.982]
Epoch [69/120    avg_loss:0.008, val_acc:0.980]
Epoch [70/120    avg_loss:0.015, val_acc:0.978]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.007, val_acc:0.979]
Epoch [73/120    avg_loss:0.018, val_acc:0.982]
Epoch [74/120    avg_loss:0.023, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.977]
Epoch [76/120    avg_loss:0.011, val_acc:0.987]
Epoch [77/120    avg_loss:0.016, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.073, val_acc:0.968]
Epoch [80/120    avg_loss:0.106, val_acc:0.934]
Epoch [81/120    avg_loss:0.039, val_acc:0.980]
Epoch [82/120    avg_loss:0.019, val_acc:0.982]
Epoch [83/120    avg_loss:0.022, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.015, val_acc:0.979]
Epoch [88/120    avg_loss:0.010, val_acc:0.975]
Epoch [89/120    avg_loss:0.009, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.989]
Epoch [91/120    avg_loss:0.018, val_acc:0.979]
Epoch [92/120    avg_loss:0.009, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.977]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.007, val_acc:0.981]
Epoch [98/120    avg_loss:0.004, val_acc:0.968]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.980]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.003, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.003, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6345     0     0     1     0     6    10    69     1]
 [    0     0 18068     0    14     0     2     0     3     3]
 [    0     0     0  1995     0     0     0     0    40     1]
 [    0    14     9     0  2924     0     7     1    16     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     1     0     0  4873     0     0     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0    19     0     5    54     0     0     0  3470    23]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
99.15648422625503

F1 scores:
[       nan 0.99063232 0.99903237 0.98835769 0.97808998 0.98863636
 0.99774775 0.99497876 0.96805691 0.95995612]

Kappa:
0.9888253614681679
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8d3e2d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.894, val_acc:0.342]
Epoch [2/120    avg_loss:1.339, val_acc:0.663]
Epoch [3/120    avg_loss:0.998, val_acc:0.634]
Epoch [4/120    avg_loss:0.720, val_acc:0.736]
Epoch [5/120    avg_loss:0.554, val_acc:0.767]
Epoch [6/120    avg_loss:0.497, val_acc:0.809]
Epoch [7/120    avg_loss:0.375, val_acc:0.849]
Epoch [8/120    avg_loss:0.327, val_acc:0.811]
Epoch [9/120    avg_loss:0.311, val_acc:0.843]
Epoch [10/120    avg_loss:0.243, val_acc:0.833]
Epoch [11/120    avg_loss:0.228, val_acc:0.895]
Epoch [12/120    avg_loss:0.229, val_acc:0.917]
Epoch [13/120    avg_loss:0.222, val_acc:0.911]
Epoch [14/120    avg_loss:0.192, val_acc:0.926]
Epoch [15/120    avg_loss:0.157, val_acc:0.918]
Epoch [16/120    avg_loss:0.294, val_acc:0.895]
Epoch [17/120    avg_loss:0.139, val_acc:0.946]
Epoch [18/120    avg_loss:0.145, val_acc:0.941]
Epoch [19/120    avg_loss:0.102, val_acc:0.943]
Epoch [20/120    avg_loss:0.097, val_acc:0.963]
Epoch [21/120    avg_loss:0.087, val_acc:0.956]
Epoch [22/120    avg_loss:0.061, val_acc:0.973]
Epoch [23/120    avg_loss:0.054, val_acc:0.962]
Epoch [24/120    avg_loss:0.064, val_acc:0.960]
Epoch [25/120    avg_loss:0.054, val_acc:0.971]
Epoch [26/120    avg_loss:0.052, val_acc:0.971]
Epoch [27/120    avg_loss:0.036, val_acc:0.973]
Epoch [28/120    avg_loss:0.038, val_acc:0.967]
Epoch [29/120    avg_loss:0.023, val_acc:0.968]
Epoch [30/120    avg_loss:0.036, val_acc:0.972]
Epoch [31/120    avg_loss:0.028, val_acc:0.977]
Epoch [32/120    avg_loss:0.023, val_acc:0.980]
Epoch [33/120    avg_loss:0.032, val_acc:0.941]
Epoch [34/120    avg_loss:0.071, val_acc:0.964]
Epoch [35/120    avg_loss:0.043, val_acc:0.973]
Epoch [36/120    avg_loss:0.017, val_acc:0.975]
Epoch [37/120    avg_loss:0.018, val_acc:0.968]
Epoch [38/120    avg_loss:0.016, val_acc:0.973]
Epoch [39/120    avg_loss:0.022, val_acc:0.971]
Epoch [40/120    avg_loss:0.021, val_acc:0.967]
Epoch [41/120    avg_loss:0.025, val_acc:0.953]
Epoch [42/120    avg_loss:0.017, val_acc:0.983]
Epoch [43/120    avg_loss:0.017, val_acc:0.979]
Epoch [44/120    avg_loss:0.015, val_acc:0.976]
Epoch [45/120    avg_loss:0.012, val_acc:0.982]
Epoch [46/120    avg_loss:0.011, val_acc:0.975]
Epoch [47/120    avg_loss:0.011, val_acc:0.964]
Epoch [48/120    avg_loss:0.024, val_acc:0.975]
Epoch [49/120    avg_loss:0.012, val_acc:0.982]
Epoch [50/120    avg_loss:0.008, val_acc:0.979]
Epoch [51/120    avg_loss:0.014, val_acc:0.983]
Epoch [52/120    avg_loss:0.010, val_acc:0.978]
Epoch [53/120    avg_loss:0.015, val_acc:0.970]
Epoch [54/120    avg_loss:0.018, val_acc:0.982]
Epoch [55/120    avg_loss:0.015, val_acc:0.982]
Epoch [56/120    avg_loss:0.009, val_acc:0.980]
Epoch [57/120    avg_loss:0.008, val_acc:0.981]
Epoch [58/120    avg_loss:0.008, val_acc:0.981]
Epoch [59/120    avg_loss:0.007, val_acc:0.981]
Epoch [60/120    avg_loss:0.006, val_acc:0.980]
Epoch [61/120    avg_loss:0.008, val_acc:0.982]
Epoch [62/120    avg_loss:0.006, val_acc:0.983]
Epoch [63/120    avg_loss:0.007, val_acc:0.983]
Epoch [64/120    avg_loss:0.007, val_acc:0.986]
Epoch [65/120    avg_loss:0.006, val_acc:0.983]
Epoch [66/120    avg_loss:0.006, val_acc:0.986]
Epoch [67/120    avg_loss:0.006, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.983]
Epoch [70/120    avg_loss:0.006, val_acc:0.983]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.983]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.987]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.004, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.985]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6342     0     0     0     0     1     1    87     1]
 [    0     0 18016     0    27     0    40     0     7     0]
 [    0     0     0  2021     1     0     0     0     6     8]
 [    0    27    10     0  2914     0     2     2    16     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     1     0  4876     0     1     0]
 [    0     0     0     0     0     0     3  1285     0     2]
 [    0     2     0    18    62     0     0     0  3474    15]
 [    0     0     0     0    14    26     0     0     0   879]]

Accuracy:
99.0817728291519

F1 scores:
[       nan 0.9907053  0.99767416 0.99190184 0.97279252 0.99013657
 0.99510204 0.99689682 0.97012008 0.96328767]

Kappa:
0.9878432166622846
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f371325f978>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.794, val_acc:0.363]
Epoch [2/120    avg_loss:1.310, val_acc:0.655]
Epoch [3/120    avg_loss:1.014, val_acc:0.695]
Epoch [4/120    avg_loss:0.794, val_acc:0.750]
Epoch [5/120    avg_loss:0.656, val_acc:0.762]
Epoch [6/120    avg_loss:0.514, val_acc:0.817]
Epoch [7/120    avg_loss:0.420, val_acc:0.853]
Epoch [8/120    avg_loss:0.325, val_acc:0.892]
Epoch [9/120    avg_loss:0.278, val_acc:0.868]
Epoch [10/120    avg_loss:0.243, val_acc:0.803]
Epoch [11/120    avg_loss:0.206, val_acc:0.915]
Epoch [12/120    avg_loss:0.175, val_acc:0.906]
Epoch [13/120    avg_loss:0.166, val_acc:0.833]
Epoch [14/120    avg_loss:0.160, val_acc:0.949]
Epoch [15/120    avg_loss:0.144, val_acc:0.919]
Epoch [16/120    avg_loss:0.119, val_acc:0.922]
Epoch [17/120    avg_loss:0.156, val_acc:0.962]
Epoch [18/120    avg_loss:0.146, val_acc:0.933]
Epoch [19/120    avg_loss:0.110, val_acc:0.954]
Epoch [20/120    avg_loss:0.082, val_acc:0.965]
Epoch [21/120    avg_loss:0.079, val_acc:0.929]
Epoch [22/120    avg_loss:0.172, val_acc:0.904]
Epoch [23/120    avg_loss:0.135, val_acc:0.918]
Epoch [24/120    avg_loss:0.124, val_acc:0.938]
Epoch [25/120    avg_loss:0.112, val_acc:0.958]
Epoch [26/120    avg_loss:0.081, val_acc:0.941]
Epoch [27/120    avg_loss:0.083, val_acc:0.962]
Epoch [28/120    avg_loss:0.057, val_acc:0.951]
Epoch [29/120    avg_loss:0.068, val_acc:0.938]
Epoch [30/120    avg_loss:0.088, val_acc:0.967]
Epoch [31/120    avg_loss:0.048, val_acc:0.969]
Epoch [32/120    avg_loss:0.048, val_acc:0.972]
Epoch [33/120    avg_loss:0.065, val_acc:0.917]
Epoch [34/120    avg_loss:0.073, val_acc:0.974]
Epoch [35/120    avg_loss:0.060, val_acc:0.934]
Epoch [36/120    avg_loss:0.043, val_acc:0.975]
Epoch [37/120    avg_loss:0.040, val_acc:0.984]
Epoch [38/120    avg_loss:0.042, val_acc:0.939]
Epoch [39/120    avg_loss:0.044, val_acc:0.976]
Epoch [40/120    avg_loss:0.031, val_acc:0.981]
Epoch [41/120    avg_loss:0.035, val_acc:0.972]
Epoch [42/120    avg_loss:0.034, val_acc:0.978]
Epoch [43/120    avg_loss:0.020, val_acc:0.988]
Epoch [44/120    avg_loss:0.023, val_acc:0.984]
Epoch [45/120    avg_loss:0.075, val_acc:0.922]
Epoch [46/120    avg_loss:0.053, val_acc:0.953]
Epoch [47/120    avg_loss:0.031, val_acc:0.985]
Epoch [48/120    avg_loss:0.018, val_acc:0.983]
Epoch [49/120    avg_loss:0.016, val_acc:0.987]
Epoch [50/120    avg_loss:0.027, val_acc:0.984]
Epoch [51/120    avg_loss:0.035, val_acc:0.984]
Epoch [52/120    avg_loss:0.017, val_acc:0.986]
Epoch [53/120    avg_loss:0.018, val_acc:0.975]
Epoch [54/120    avg_loss:0.016, val_acc:0.985]
Epoch [55/120    avg_loss:0.008, val_acc:0.988]
Epoch [56/120    avg_loss:0.010, val_acc:0.981]
Epoch [57/120    avg_loss:0.014, val_acc:0.982]
Epoch [58/120    avg_loss:0.039, val_acc:0.975]
Epoch [59/120    avg_loss:0.022, val_acc:0.987]
Epoch [60/120    avg_loss:0.031, val_acc:0.982]
Epoch [61/120    avg_loss:0.065, val_acc:0.970]
Epoch [62/120    avg_loss:0.020, val_acc:0.987]
Epoch [63/120    avg_loss:0.014, val_acc:0.984]
Epoch [64/120    avg_loss:0.010, val_acc:0.988]
Epoch [65/120    avg_loss:0.014, val_acc:0.963]
Epoch [66/120    avg_loss:0.025, val_acc:0.982]
Epoch [67/120    avg_loss:0.032, val_acc:0.979]
Epoch [68/120    avg_loss:0.035, val_acc:0.981]
Epoch [69/120    avg_loss:0.014, val_acc:0.983]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.018, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.010, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.010, val_acc:0.989]
Epoch [78/120    avg_loss:0.010, val_acc:0.989]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.007, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.008, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.992]
Epoch [96/120    avg_loss:0.009, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.008, val_acc:0.992]
Epoch [101/120    avg_loss:0.006, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0    10     0     0     9     0    63     0]
 [    0     0 18045     0    28     0     7     0    10     0]
 [    0     0     0  2011     0     0     0     0    20     5]
 [    0    32    11     0  2908     0     7     0    14     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4855     0    22     0]
 [    0     1     0     0     0     0     1  1279     0     9]
 [    0     1     0    22    54     0     0     0  3473    21]
 [    0     0     0     0    14    28     0     0     0   877]]

Accuracy:
99.06008242354132

F1 scores:
[       nan 0.99094881 0.99842311 0.98602599 0.97322624 0.9893859
 0.99518295 0.99571818 0.96835355 0.95794648]

Kappa:
0.9875518940125723
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa1ed1c0978>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.839, val_acc:0.315]
Epoch [2/120    avg_loss:1.280, val_acc:0.428]
Epoch [3/120    avg_loss:0.973, val_acc:0.527]
Epoch [4/120    avg_loss:0.715, val_acc:0.765]
Epoch [5/120    avg_loss:0.580, val_acc:0.779]
Epoch [6/120    avg_loss:0.432, val_acc:0.848]
Epoch [7/120    avg_loss:0.387, val_acc:0.849]
Epoch [8/120    avg_loss:0.367, val_acc:0.875]
Epoch [9/120    avg_loss:0.373, val_acc:0.883]
Epoch [10/120    avg_loss:0.311, val_acc:0.893]
Epoch [11/120    avg_loss:0.272, val_acc:0.912]
Epoch [12/120    avg_loss:0.223, val_acc:0.870]
Epoch [13/120    avg_loss:0.239, val_acc:0.934]
Epoch [14/120    avg_loss:0.220, val_acc:0.906]
Epoch [15/120    avg_loss:0.247, val_acc:0.895]
Epoch [16/120    avg_loss:0.179, val_acc:0.919]
Epoch [17/120    avg_loss:0.144, val_acc:0.926]
Epoch [18/120    avg_loss:0.140, val_acc:0.954]
Epoch [19/120    avg_loss:0.117, val_acc:0.953]
Epoch [20/120    avg_loss:0.115, val_acc:0.961]
Epoch [21/120    avg_loss:0.091, val_acc:0.963]
Epoch [22/120    avg_loss:0.093, val_acc:0.856]
Epoch [23/120    avg_loss:0.196, val_acc:0.943]
Epoch [24/120    avg_loss:0.117, val_acc:0.945]
Epoch [25/120    avg_loss:0.100, val_acc:0.963]
Epoch [26/120    avg_loss:0.098, val_acc:0.937]
Epoch [27/120    avg_loss:0.105, val_acc:0.968]
Epoch [28/120    avg_loss:0.066, val_acc:0.970]
Epoch [29/120    avg_loss:0.053, val_acc:0.957]
Epoch [30/120    avg_loss:0.052, val_acc:0.957]
Epoch [31/120    avg_loss:0.051, val_acc:0.957]
Epoch [32/120    avg_loss:0.055, val_acc:0.973]
Epoch [33/120    avg_loss:0.036, val_acc:0.959]
Epoch [34/120    avg_loss:0.038, val_acc:0.971]
Epoch [35/120    avg_loss:0.026, val_acc:0.971]
Epoch [36/120    avg_loss:0.046, val_acc:0.967]
Epoch [37/120    avg_loss:0.033, val_acc:0.982]
Epoch [38/120    avg_loss:0.031, val_acc:0.979]
Epoch [39/120    avg_loss:0.038, val_acc:0.974]
Epoch [40/120    avg_loss:0.048, val_acc:0.973]
Epoch [41/120    avg_loss:0.035, val_acc:0.975]
Epoch [42/120    avg_loss:0.031, val_acc:0.977]
Epoch [43/120    avg_loss:0.023, val_acc:0.980]
Epoch [44/120    avg_loss:0.032, val_acc:0.978]
Epoch [45/120    avg_loss:0.031, val_acc:0.970]
Epoch [46/120    avg_loss:0.022, val_acc:0.977]
Epoch [47/120    avg_loss:0.024, val_acc:0.955]
Epoch [48/120    avg_loss:0.024, val_acc:0.975]
Epoch [49/120    avg_loss:0.062, val_acc:0.959]
Epoch [50/120    avg_loss:0.039, val_acc:0.976]
Epoch [51/120    avg_loss:0.027, val_acc:0.981]
Epoch [52/120    avg_loss:0.019, val_acc:0.982]
Epoch [53/120    avg_loss:0.014, val_acc:0.983]
Epoch [54/120    avg_loss:0.014, val_acc:0.983]
Epoch [55/120    avg_loss:0.021, val_acc:0.981]
Epoch [56/120    avg_loss:0.020, val_acc:0.983]
Epoch [57/120    avg_loss:0.017, val_acc:0.982]
Epoch [58/120    avg_loss:0.013, val_acc:0.983]
Epoch [59/120    avg_loss:0.013, val_acc:0.984]
Epoch [60/120    avg_loss:0.018, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.015, val_acc:0.981]
Epoch [65/120    avg_loss:0.016, val_acc:0.981]
Epoch [66/120    avg_loss:0.012, val_acc:0.982]
Epoch [67/120    avg_loss:0.010, val_acc:0.983]
Epoch [68/120    avg_loss:0.013, val_acc:0.982]
Epoch [69/120    avg_loss:0.015, val_acc:0.981]
Epoch [70/120    avg_loss:0.016, val_acc:0.980]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.013, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.983]
Epoch [75/120    avg_loss:0.014, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.016, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.010, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.981]
Epoch [85/120    avg_loss:0.012, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.981]
Epoch [87/120    avg_loss:0.028, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.013, val_acc:0.981]
Epoch [98/120    avg_loss:0.010, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0    11     0    33     6]
 [    0     0 17970     0    26     0    77     0    16     1]
 [    0     5     0  2010     0     0     0     0    17     4]
 [    0    31    13     0  2906     0     5     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     3     0     0  4867     0     8     0]
 [    0     2     0     0     0     0     0  1287     1     0]
 [    0    15     0    21    67     0     0     0  3464     4]
 [    0     0     0     0    14     9     0     0     0   896]]

Accuracy:
99.02152170245584

F1 scores:
[       nan 0.99199503 0.99631303 0.98771499 0.9710944  0.99656357
 0.98942875 0.99883586 0.97207801 0.97923497]

Kappa:
0.9870490403958629
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff808da98d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.770, val_acc:0.263]
Epoch [2/120    avg_loss:1.254, val_acc:0.391]
Epoch [3/120    avg_loss:0.955, val_acc:0.573]
Epoch [4/120    avg_loss:0.763, val_acc:0.658]
Epoch [5/120    avg_loss:0.590, val_acc:0.641]
Epoch [6/120    avg_loss:0.542, val_acc:0.706]
Epoch [7/120    avg_loss:0.455, val_acc:0.808]
Epoch [8/120    avg_loss:0.374, val_acc:0.787]
Epoch [9/120    avg_loss:0.314, val_acc:0.859]
Epoch [10/120    avg_loss:0.279, val_acc:0.899]
Epoch [11/120    avg_loss:0.423, val_acc:0.800]
Epoch [12/120    avg_loss:0.263, val_acc:0.900]
Epoch [13/120    avg_loss:0.211, val_acc:0.885]
Epoch [14/120    avg_loss:0.209, val_acc:0.853]
Epoch [15/120    avg_loss:0.165, val_acc:0.894]
Epoch [16/120    avg_loss:0.186, val_acc:0.903]
Epoch [17/120    avg_loss:0.164, val_acc:0.891]
Epoch [18/120    avg_loss:0.151, val_acc:0.941]
Epoch [19/120    avg_loss:0.129, val_acc:0.921]
Epoch [20/120    avg_loss:0.097, val_acc:0.932]
Epoch [21/120    avg_loss:0.117, val_acc:0.902]
Epoch [22/120    avg_loss:0.096, val_acc:0.948]
Epoch [23/120    avg_loss:0.107, val_acc:0.958]
Epoch [24/120    avg_loss:0.080, val_acc:0.927]
Epoch [25/120    avg_loss:0.120, val_acc:0.852]
Epoch [26/120    avg_loss:0.241, val_acc:0.937]
Epoch [27/120    avg_loss:0.101, val_acc:0.944]
Epoch [28/120    avg_loss:0.114, val_acc:0.965]
Epoch [29/120    avg_loss:0.083, val_acc:0.974]
Epoch [30/120    avg_loss:0.063, val_acc:0.955]
Epoch [31/120    avg_loss:0.100, val_acc:0.963]
Epoch [32/120    avg_loss:0.060, val_acc:0.969]
Epoch [33/120    avg_loss:0.062, val_acc:0.935]
Epoch [34/120    avg_loss:0.059, val_acc:0.967]
Epoch [35/120    avg_loss:0.059, val_acc:0.959]
Epoch [36/120    avg_loss:0.064, val_acc:0.968]
Epoch [37/120    avg_loss:0.061, val_acc:0.981]
Epoch [38/120    avg_loss:0.042, val_acc:0.967]
Epoch [39/120    avg_loss:0.040, val_acc:0.939]
Epoch [40/120    avg_loss:0.046, val_acc:0.976]
Epoch [41/120    avg_loss:0.044, val_acc:0.978]
Epoch [42/120    avg_loss:0.031, val_acc:0.969]
Epoch [43/120    avg_loss:0.033, val_acc:0.980]
Epoch [44/120    avg_loss:0.029, val_acc:0.963]
Epoch [45/120    avg_loss:0.023, val_acc:0.982]
Epoch [46/120    avg_loss:0.038, val_acc:0.987]
Epoch [47/120    avg_loss:0.023, val_acc:0.983]
Epoch [48/120    avg_loss:0.025, val_acc:0.966]
Epoch [49/120    avg_loss:0.035, val_acc:0.978]
Epoch [50/120    avg_loss:0.032, val_acc:0.973]
Epoch [51/120    avg_loss:0.022, val_acc:0.982]
Epoch [52/120    avg_loss:0.030, val_acc:0.979]
Epoch [53/120    avg_loss:0.039, val_acc:0.973]
Epoch [54/120    avg_loss:0.024, val_acc:0.984]
Epoch [55/120    avg_loss:0.015, val_acc:0.979]
Epoch [56/120    avg_loss:0.020, val_acc:0.985]
Epoch [57/120    avg_loss:0.010, val_acc:0.985]
Epoch [58/120    avg_loss:0.026, val_acc:0.981]
Epoch [59/120    avg_loss:0.020, val_acc:0.991]
Epoch [60/120    avg_loss:0.013, val_acc:0.986]
Epoch [61/120    avg_loss:0.029, val_acc:0.978]
Epoch [62/120    avg_loss:0.025, val_acc:0.986]
Epoch [63/120    avg_loss:0.013, val_acc:0.986]
Epoch [64/120    avg_loss:0.019, val_acc:0.973]
Epoch [65/120    avg_loss:0.013, val_acc:0.990]
Epoch [66/120    avg_loss:0.013, val_acc:0.973]
Epoch [67/120    avg_loss:0.012, val_acc:0.943]
Epoch [68/120    avg_loss:0.017, val_acc:0.982]
Epoch [69/120    avg_loss:0.045, val_acc:0.991]
Epoch [70/120    avg_loss:0.037, val_acc:0.979]
Epoch [71/120    avg_loss:0.019, val_acc:0.979]
Epoch [72/120    avg_loss:0.040, val_acc:0.978]
Epoch [73/120    avg_loss:0.017, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.026, val_acc:0.988]
Epoch [77/120    avg_loss:0.023, val_acc:0.977]
Epoch [78/120    avg_loss:0.025, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.987]
Epoch [80/120    avg_loss:0.012, val_acc:0.987]
Epoch [81/120    avg_loss:0.014, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.989]
Epoch [84/120    avg_loss:0.013, val_acc:0.970]
Epoch [85/120    avg_loss:0.011, val_acc:0.988]
Epoch [86/120    avg_loss:0.011, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.986]
Epoch [88/120    avg_loss:0.011, val_acc:0.990]
Epoch [89/120    avg_loss:0.010, val_acc:0.975]
Epoch [90/120    avg_loss:0.011, val_acc:0.993]
Epoch [91/120    avg_loss:0.006, val_acc:0.993]
Epoch [92/120    avg_loss:0.004, val_acc:0.993]
Epoch [93/120    avg_loss:0.008, val_acc:0.989]
Epoch [94/120    avg_loss:0.011, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.989]
Epoch [97/120    avg_loss:0.004, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.993]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.992]
Epoch [111/120    avg_loss:0.004, val_acc:0.992]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.002, val_acc:0.993]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.004, val_acc:0.993]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.993]
Epoch [118/120    avg_loss:0.003, val_acc:0.993]
Epoch [119/120    avg_loss:0.003, val_acc:0.992]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     1     1     0     0     4    62     2]
 [    0     0 17979     0    96     0    15     0     0     0]
 [    0     3     0  1999     1     0     0     0    26     7]
 [    0    25     5     0  2930     0     1     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     0     0     0     0     0     0  1283     0     7]
 [    0    19     0     8    53     0     0     0  3483     8]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
99.0311618827272

F1 scores:
[       nan 0.99088856 0.99675676 0.98862512 0.965881   0.98788796
 0.99826016 0.99573147 0.97385712 0.96145374]

Kappa:
0.9871781847603572
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff526d38908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.783, val_acc:0.172]
Epoch [2/120    avg_loss:1.283, val_acc:0.726]
Epoch [3/120    avg_loss:0.960, val_acc:0.776]
Epoch [4/120    avg_loss:0.724, val_acc:0.774]
Epoch [5/120    avg_loss:0.554, val_acc:0.764]
Epoch [6/120    avg_loss:0.450, val_acc:0.834]
Epoch [7/120    avg_loss:0.377, val_acc:0.876]
Epoch [8/120    avg_loss:0.260, val_acc:0.919]
Epoch [9/120    avg_loss:0.234, val_acc:0.907]
Epoch [10/120    avg_loss:0.197, val_acc:0.919]
Epoch [11/120    avg_loss:0.206, val_acc:0.941]
Epoch [12/120    avg_loss:0.185, val_acc:0.932]
Epoch [13/120    avg_loss:0.150, val_acc:0.951]
Epoch [14/120    avg_loss:0.125, val_acc:0.959]
Epoch [15/120    avg_loss:0.148, val_acc:0.950]
Epoch [16/120    avg_loss:0.110, val_acc:0.958]
Epoch [17/120    avg_loss:0.103, val_acc:0.955]
Epoch [18/120    avg_loss:0.080, val_acc:0.941]
Epoch [19/120    avg_loss:0.089, val_acc:0.954]
Epoch [20/120    avg_loss:0.087, val_acc:0.941]
Epoch [21/120    avg_loss:0.096, val_acc:0.961]
Epoch [22/120    avg_loss:0.079, val_acc:0.807]
Epoch [23/120    avg_loss:0.076, val_acc:0.967]
Epoch [24/120    avg_loss:0.076, val_acc:0.976]
Epoch [25/120    avg_loss:0.054, val_acc:0.964]
Epoch [26/120    avg_loss:0.045, val_acc:0.963]
Epoch [27/120    avg_loss:0.097, val_acc:0.946]
Epoch [28/120    avg_loss:0.100, val_acc:0.938]
Epoch [29/120    avg_loss:0.094, val_acc:0.947]
Epoch [30/120    avg_loss:0.067, val_acc:0.970]
Epoch [31/120    avg_loss:0.058, val_acc:0.969]
Epoch [32/120    avg_loss:0.038, val_acc:0.978]
Epoch [33/120    avg_loss:0.051, val_acc:0.979]
Epoch [34/120    avg_loss:0.047, val_acc:0.975]
Epoch [35/120    avg_loss:0.037, val_acc:0.973]
Epoch [36/120    avg_loss:0.066, val_acc:0.958]
Epoch [37/120    avg_loss:0.085, val_acc:0.935]
Epoch [38/120    avg_loss:0.041, val_acc:0.948]
Epoch [39/120    avg_loss:0.043, val_acc:0.974]
Epoch [40/120    avg_loss:0.047, val_acc:0.970]
Epoch [41/120    avg_loss:0.039, val_acc:0.959]
Epoch [42/120    avg_loss:0.030, val_acc:0.981]
Epoch [43/120    avg_loss:0.022, val_acc:0.984]
Epoch [44/120    avg_loss:0.019, val_acc:0.983]
Epoch [45/120    avg_loss:0.026, val_acc:0.972]
Epoch [46/120    avg_loss:0.060, val_acc:0.970]
Epoch [47/120    avg_loss:0.061, val_acc:0.956]
Epoch [48/120    avg_loss:0.117, val_acc:0.871]
Epoch [49/120    avg_loss:0.052, val_acc:0.963]
Epoch [50/120    avg_loss:0.032, val_acc:0.982]
Epoch [51/120    avg_loss:0.042, val_acc:0.979]
Epoch [52/120    avg_loss:0.022, val_acc:0.986]
Epoch [53/120    avg_loss:0.018, val_acc:0.981]
Epoch [54/120    avg_loss:0.021, val_acc:0.983]
Epoch [55/120    avg_loss:0.045, val_acc:0.978]
Epoch [56/120    avg_loss:0.033, val_acc:0.977]
Epoch [57/120    avg_loss:0.022, val_acc:0.986]
Epoch [58/120    avg_loss:0.023, val_acc:0.976]
Epoch [59/120    avg_loss:0.027, val_acc:0.971]
Epoch [60/120    avg_loss:0.039, val_acc:0.982]
Epoch [61/120    avg_loss:0.020, val_acc:0.982]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.017, val_acc:0.982]
Epoch [64/120    avg_loss:0.019, val_acc:0.986]
Epoch [65/120    avg_loss:0.042, val_acc:0.953]
Epoch [66/120    avg_loss:0.023, val_acc:0.982]
Epoch [67/120    avg_loss:0.013, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.986]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.014, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.986]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.010, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.016, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.989]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.008, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.016, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6383     0     0     0     0     1     0    47     1]
 [    0     3 18058     0    13     0    12     0     4     0]
 [    0     5     0  2007     0     0     0     0    20     4]
 [    0    38    11     0  2901     0     6     0    16     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     5     0]
 [    0    10     0     0     0     0     2  1274     0     4]
 [    0     1     0    11    53     0     0     0  3489    17]
 [    0     0     0     0     9    29     0     0     0   881]]

Accuracy:
99.22396548815463

F1 scores:
[       nan 0.99176507 0.99881081 0.9901332  0.97545393 0.98901099
 0.99733934 0.99375975 0.97567114 0.96495071]

Kappa:
0.9897189196696423
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:75
Validation dataloader:75
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feb264fa8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.830, val_acc:0.247]
Epoch [2/120    avg_loss:1.279, val_acc:0.414]
Epoch [3/120    avg_loss:1.009, val_acc:0.656]
Epoch [4/120    avg_loss:0.778, val_acc:0.754]
Epoch [5/120    avg_loss:0.613, val_acc:0.789]
Epoch [6/120    avg_loss:0.511, val_acc:0.805]
Epoch [7/120    avg_loss:0.421, val_acc:0.796]
Epoch [8/120    avg_loss:0.411, val_acc:0.799]
Epoch [9/120    avg_loss:0.384, val_acc:0.762]
Epoch [10/120    avg_loss:0.327, val_acc:0.814]
Epoch [11/120    avg_loss:0.315, val_acc:0.835]
Epoch [12/120    avg_loss:0.271, val_acc:0.838]
Epoch [13/120    avg_loss:0.270, val_acc:0.842]
Epoch [14/120    avg_loss:0.254, val_acc:0.824]
Epoch [15/120    avg_loss:0.245, val_acc:0.862]
Epoch [16/120    avg_loss:0.205, val_acc:0.852]
Epoch [17/120    avg_loss:0.176, val_acc:0.909]
Epoch [18/120    avg_loss:0.155, val_acc:0.807]
Epoch [19/120    avg_loss:0.144, val_acc:0.931]
Epoch [20/120    avg_loss:0.235, val_acc:0.908]
Epoch [21/120    avg_loss:0.198, val_acc:0.920]
Epoch [22/120    avg_loss:0.160, val_acc:0.955]
Epoch [23/120    avg_loss:0.102, val_acc:0.948]
Epoch [24/120    avg_loss:0.096, val_acc:0.957]
Epoch [25/120    avg_loss:0.106, val_acc:0.945]
Epoch [26/120    avg_loss:0.071, val_acc:0.958]
Epoch [27/120    avg_loss:0.099, val_acc:0.925]
Epoch [28/120    avg_loss:0.078, val_acc:0.970]
Epoch [29/120    avg_loss:0.092, val_acc:0.928]
Epoch [30/120    avg_loss:0.079, val_acc:0.960]
Epoch [31/120    avg_loss:0.078, val_acc:0.960]
Epoch [32/120    avg_loss:0.057, val_acc:0.951]
Epoch [33/120    avg_loss:0.043, val_acc:0.965]
Epoch [34/120    avg_loss:0.043, val_acc:0.972]
Epoch [35/120    avg_loss:0.039, val_acc:0.967]
Epoch [36/120    avg_loss:0.050, val_acc:0.964]
Epoch [37/120    avg_loss:0.028, val_acc:0.964]
Epoch [38/120    avg_loss:0.029, val_acc:0.970]
Epoch [39/120    avg_loss:0.151, val_acc:0.949]
Epoch [40/120    avg_loss:0.083, val_acc:0.917]
Epoch [41/120    avg_loss:0.094, val_acc:0.944]
Epoch [42/120    avg_loss:0.062, val_acc:0.974]
Epoch [43/120    avg_loss:0.050, val_acc:0.974]
Epoch [44/120    avg_loss:0.041, val_acc:0.954]
Epoch [45/120    avg_loss:0.043, val_acc:0.980]
Epoch [46/120    avg_loss:0.032, val_acc:0.979]
Epoch [47/120    avg_loss:0.024, val_acc:0.972]
Epoch [48/120    avg_loss:0.026, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.979]
Epoch [50/120    avg_loss:0.023, val_acc:0.971]
Epoch [51/120    avg_loss:0.022, val_acc:0.977]
Epoch [52/120    avg_loss:0.026, val_acc:0.978]
Epoch [53/120    avg_loss:0.015, val_acc:0.981]
Epoch [54/120    avg_loss:0.010, val_acc:0.991]
Epoch [55/120    avg_loss:0.023, val_acc:0.976]
Epoch [56/120    avg_loss:0.017, val_acc:0.950]
Epoch [57/120    avg_loss:0.040, val_acc:0.927]
Epoch [58/120    avg_loss:0.043, val_acc:0.976]
Epoch [59/120    avg_loss:0.021, val_acc:0.967]
Epoch [60/120    avg_loss:0.028, val_acc:0.983]
Epoch [61/120    avg_loss:0.014, val_acc:0.983]
Epoch [62/120    avg_loss:0.013, val_acc:0.991]
Epoch [63/120    avg_loss:0.015, val_acc:0.985]
Epoch [64/120    avg_loss:0.017, val_acc:0.956]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.015, val_acc:0.973]
Epoch [67/120    avg_loss:0.014, val_acc:0.988]
Epoch [68/120    avg_loss:0.008, val_acc:0.991]
Epoch [69/120    avg_loss:0.014, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.008, val_acc:0.986]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.982]
Epoch [76/120    avg_loss:0.010, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.981]
Epoch [79/120    avg_loss:0.017, val_acc:0.979]
Epoch [80/120    avg_loss:0.008, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.004, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.989]
Epoch [85/120    avg_loss:0.005, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.005, val_acc:0.990]
Epoch [88/120    avg_loss:0.005, val_acc:0.990]
Epoch [89/120    avg_loss:0.004, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.009, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.004, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6393     0     0     0     0     8     0    30     1]
 [    0     3 18046     0    29     0     9     0     0     3]
 [    0     7     0  1989     0     0     0     0    34     6]
 [    0    43    18     0  2898     0     2     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     9     0     0  4864     0     0     0]
 [    0    19     0     0     0     0     2  1269     0     0]
 [    0     8     0     3    53     0     0     0  3507     0]
 [    0     0     0     0    15    27     0     0     0   877]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.99077877 0.99814707 0.98538519 0.97134238 0.98976109
 0.99641504 0.99179367 0.98056759 0.97120709]

Kappa:
0.9889831466401693
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f62bf8ba908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.866, val_acc:0.308]
Epoch [2/120    avg_loss:1.397, val_acc:0.568]
Epoch [3/120    avg_loss:1.011, val_acc:0.677]
Epoch [4/120    avg_loss:0.693, val_acc:0.743]
Epoch [5/120    avg_loss:0.580, val_acc:0.793]
Epoch [6/120    avg_loss:0.516, val_acc:0.782]
Epoch [7/120    avg_loss:0.408, val_acc:0.839]
Epoch [8/120    avg_loss:0.394, val_acc:0.830]
Epoch [9/120    avg_loss:0.390, val_acc:0.818]
Epoch [10/120    avg_loss:0.309, val_acc:0.801]
Epoch [11/120    avg_loss:0.281, val_acc:0.891]
Epoch [12/120    avg_loss:0.244, val_acc:0.896]
Epoch [13/120    avg_loss:0.257, val_acc:0.934]
Epoch [14/120    avg_loss:0.226, val_acc:0.882]
Epoch [15/120    avg_loss:0.194, val_acc:0.921]
Epoch [16/120    avg_loss:0.184, val_acc:0.896]
Epoch [17/120    avg_loss:0.139, val_acc:0.905]
Epoch [18/120    avg_loss:0.173, val_acc:0.908]
Epoch [19/120    avg_loss:0.174, val_acc:0.923]
Epoch [20/120    avg_loss:0.139, val_acc:0.935]
Epoch [21/120    avg_loss:0.100, val_acc:0.940]
Epoch [22/120    avg_loss:0.084, val_acc:0.949]
Epoch [23/120    avg_loss:0.106, val_acc:0.953]
Epoch [24/120    avg_loss:0.156, val_acc:0.859]
Epoch [25/120    avg_loss:0.161, val_acc:0.833]
Epoch [26/120    avg_loss:0.109, val_acc:0.964]
Epoch [27/120    avg_loss:0.081, val_acc:0.920]
Epoch [28/120    avg_loss:0.176, val_acc:0.932]
Epoch [29/120    avg_loss:0.090, val_acc:0.924]
Epoch [30/120    avg_loss:0.105, val_acc:0.938]
Epoch [31/120    avg_loss:0.076, val_acc:0.967]
Epoch [32/120    avg_loss:0.055, val_acc:0.977]
Epoch [33/120    avg_loss:0.054, val_acc:0.967]
Epoch [34/120    avg_loss:0.047, val_acc:0.984]
Epoch [35/120    avg_loss:0.058, val_acc:0.960]
Epoch [36/120    avg_loss:0.056, val_acc:0.961]
Epoch [37/120    avg_loss:0.052, val_acc:0.957]
Epoch [38/120    avg_loss:0.046, val_acc:0.974]
Epoch [39/120    avg_loss:0.038, val_acc:0.967]
Epoch [40/120    avg_loss:0.077, val_acc:0.932]
Epoch [41/120    avg_loss:0.084, val_acc:0.971]
Epoch [42/120    avg_loss:0.057, val_acc:0.970]
Epoch [43/120    avg_loss:0.039, val_acc:0.961]
Epoch [44/120    avg_loss:0.030, val_acc:0.975]
Epoch [45/120    avg_loss:0.042, val_acc:0.960]
Epoch [46/120    avg_loss:0.042, val_acc:0.978]
Epoch [47/120    avg_loss:0.037, val_acc:0.981]
Epoch [48/120    avg_loss:0.025, val_acc:0.982]
Epoch [49/120    avg_loss:0.017, val_acc:0.981]
Epoch [50/120    avg_loss:0.028, val_acc:0.981]
Epoch [51/120    avg_loss:0.016, val_acc:0.982]
Epoch [52/120    avg_loss:0.020, val_acc:0.982]
Epoch [53/120    avg_loss:0.017, val_acc:0.980]
Epoch [54/120    avg_loss:0.017, val_acc:0.984]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.018, val_acc:0.981]
Epoch [57/120    avg_loss:0.016, val_acc:0.983]
Epoch [58/120    avg_loss:0.016, val_acc:0.984]
Epoch [59/120    avg_loss:0.014, val_acc:0.985]
Epoch [60/120    avg_loss:0.017, val_acc:0.982]
Epoch [61/120    avg_loss:0.015, val_acc:0.982]
Epoch [62/120    avg_loss:0.016, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.980]
Epoch [64/120    avg_loss:0.013, val_acc:0.985]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.018, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.014, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.985]
Epoch [70/120    avg_loss:0.015, val_acc:0.985]
Epoch [71/120    avg_loss:0.025, val_acc:0.985]
Epoch [72/120    avg_loss:0.015, val_acc:0.985]
Epoch [73/120    avg_loss:0.020, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.985]
Epoch [75/120    avg_loss:0.017, val_acc:0.985]
Epoch [76/120    avg_loss:0.015, val_acc:0.985]
Epoch [77/120    avg_loss:0.015, val_acc:0.982]
Epoch [78/120    avg_loss:0.015, val_acc:0.985]
Epoch [79/120    avg_loss:0.015, val_acc:0.985]
Epoch [80/120    avg_loss:0.022, val_acc:0.984]
Epoch [81/120    avg_loss:0.013, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.013, val_acc:0.985]
Epoch [84/120    avg_loss:0.017, val_acc:0.982]
Epoch [85/120    avg_loss:0.013, val_acc:0.985]
Epoch [86/120    avg_loss:0.016, val_acc:0.985]
Epoch [87/120    avg_loss:0.014, val_acc:0.985]
Epoch [88/120    avg_loss:0.019, val_acc:0.985]
Epoch [89/120    avg_loss:0.016, val_acc:0.985]
Epoch [90/120    avg_loss:0.015, val_acc:0.985]
Epoch [91/120    avg_loss:0.013, val_acc:0.985]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.012, val_acc:0.985]
Epoch [94/120    avg_loss:0.013, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.011, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.985]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.013, val_acc:0.986]
Epoch [106/120    avg_loss:0.011, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.984]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.013, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.012, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.983]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6359     0     2     2     0    30     0    39     0]
 [    0     3 17877     0   100     0    87     0    23     0]
 [    0     3     0  2022     0     0     0     0     8     3]
 [    0    27     8     0  2896     0    11     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4877     0     0     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     1     0     1    56     0     0     0  3491    22]
 [    0     0     0     0    14    31     0     0     0   874]]

Accuracy:
98.7829272407394

F1 scores:
[       nan 0.99165692 0.99385685 0.99556869 0.9589404  0.98826202
 0.9867476  0.99883586 0.97486736 0.96096756]

Kappa:
0.9839098861386113
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbf2e617908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.860, val_acc:0.232]
Epoch [2/120    avg_loss:1.333, val_acc:0.411]
Epoch [3/120    avg_loss:1.034, val_acc:0.686]
Epoch [4/120    avg_loss:0.775, val_acc:0.709]
Epoch [5/120    avg_loss:0.572, val_acc:0.744]
Epoch [6/120    avg_loss:0.513, val_acc:0.813]
Epoch [7/120    avg_loss:0.438, val_acc:0.812]
Epoch [8/120    avg_loss:0.395, val_acc:0.855]
Epoch [9/120    avg_loss:0.309, val_acc:0.888]
Epoch [10/120    avg_loss:0.301, val_acc:0.902]
Epoch [11/120    avg_loss:0.246, val_acc:0.890]
Epoch [12/120    avg_loss:0.230, val_acc:0.863]
Epoch [13/120    avg_loss:0.238, val_acc:0.890]
Epoch [14/120    avg_loss:0.199, val_acc:0.917]
Epoch [15/120    avg_loss:0.155, val_acc:0.934]
Epoch [16/120    avg_loss:0.130, val_acc:0.874]
Epoch [17/120    avg_loss:0.165, val_acc:0.833]
Epoch [18/120    avg_loss:0.119, val_acc:0.927]
Epoch [19/120    avg_loss:0.104, val_acc:0.939]
Epoch [20/120    avg_loss:0.107, val_acc:0.836]
Epoch [21/120    avg_loss:0.111, val_acc:0.486]
Epoch [22/120    avg_loss:1.477, val_acc:0.630]
Epoch [23/120    avg_loss:0.932, val_acc:0.762]
Epoch [24/120    avg_loss:0.792, val_acc:0.745]
Epoch [25/120    avg_loss:0.672, val_acc:0.744]
Epoch [26/120    avg_loss:0.555, val_acc:0.760]
Epoch [27/120    avg_loss:0.545, val_acc:0.784]
Epoch [28/120    avg_loss:0.444, val_acc:0.740]
Epoch [29/120    avg_loss:0.435, val_acc:0.844]
Epoch [30/120    avg_loss:0.373, val_acc:0.825]
Epoch [31/120    avg_loss:0.397, val_acc:0.809]
Epoch [32/120    avg_loss:0.371, val_acc:0.755]
Epoch [33/120    avg_loss:0.316, val_acc:0.797]
Epoch [34/120    avg_loss:0.297, val_acc:0.820]
Epoch [35/120    avg_loss:0.273, val_acc:0.824]
Epoch [36/120    avg_loss:0.277, val_acc:0.833]
Epoch [37/120    avg_loss:0.259, val_acc:0.837]
Epoch [38/120    avg_loss:0.236, val_acc:0.847]
Epoch [39/120    avg_loss:0.263, val_acc:0.845]
Epoch [40/120    avg_loss:0.236, val_acc:0.849]
Epoch [41/120    avg_loss:0.246, val_acc:0.845]
Epoch [42/120    avg_loss:0.248, val_acc:0.854]
Epoch [43/120    avg_loss:0.262, val_acc:0.848]
Epoch [44/120    avg_loss:0.228, val_acc:0.850]
Epoch [45/120    avg_loss:0.234, val_acc:0.852]
Epoch [46/120    avg_loss:0.227, val_acc:0.853]
Epoch [47/120    avg_loss:0.245, val_acc:0.852]
Epoch [48/120    avg_loss:0.230, val_acc:0.852]
Epoch [49/120    avg_loss:0.237, val_acc:0.853]
Epoch [50/120    avg_loss:0.223, val_acc:0.853]
Epoch [51/120    avg_loss:0.245, val_acc:0.854]
Epoch [52/120    avg_loss:0.229, val_acc:0.853]
Epoch [53/120    avg_loss:0.215, val_acc:0.854]
Epoch [54/120    avg_loss:0.228, val_acc:0.854]
Epoch [55/120    avg_loss:0.255, val_acc:0.854]
Epoch [56/120    avg_loss:0.244, val_acc:0.854]
Epoch [57/120    avg_loss:0.228, val_acc:0.854]
Epoch [58/120    avg_loss:0.218, val_acc:0.853]
Epoch [59/120    avg_loss:0.229, val_acc:0.854]
Epoch [60/120    avg_loss:0.234, val_acc:0.853]
Epoch [61/120    avg_loss:0.213, val_acc:0.853]
Epoch [62/120    avg_loss:0.215, val_acc:0.853]
Epoch [63/120    avg_loss:0.231, val_acc:0.853]
Epoch [64/120    avg_loss:0.219, val_acc:0.853]
Epoch [65/120    avg_loss:0.226, val_acc:0.854]
Epoch [66/120    avg_loss:0.225, val_acc:0.854]
Epoch [67/120    avg_loss:0.240, val_acc:0.854]
Epoch [68/120    avg_loss:0.236, val_acc:0.853]
Epoch [69/120    avg_loss:0.226, val_acc:0.853]
Epoch [70/120    avg_loss:0.221, val_acc:0.853]
Epoch [71/120    avg_loss:0.220, val_acc:0.853]
Epoch [72/120    avg_loss:0.231, val_acc:0.854]
Epoch [73/120    avg_loss:0.232, val_acc:0.854]
Epoch [74/120    avg_loss:0.225, val_acc:0.854]
Epoch [75/120    avg_loss:0.221, val_acc:0.854]
Epoch [76/120    avg_loss:0.223, val_acc:0.854]
Epoch [77/120    avg_loss:0.208, val_acc:0.854]
Epoch [78/120    avg_loss:0.220, val_acc:0.854]
Epoch [79/120    avg_loss:0.224, val_acc:0.854]
Epoch [80/120    avg_loss:0.237, val_acc:0.854]
Epoch [81/120    avg_loss:0.234, val_acc:0.854]
Epoch [82/120    avg_loss:0.218, val_acc:0.854]
Epoch [83/120    avg_loss:0.221, val_acc:0.854]
Epoch [84/120    avg_loss:0.205, val_acc:0.854]
Epoch [85/120    avg_loss:0.225, val_acc:0.854]
Epoch [86/120    avg_loss:0.238, val_acc:0.854]
Epoch [87/120    avg_loss:0.230, val_acc:0.854]
Epoch [88/120    avg_loss:0.227, val_acc:0.854]
Epoch [89/120    avg_loss:0.235, val_acc:0.854]
Epoch [90/120    avg_loss:0.234, val_acc:0.854]
Epoch [91/120    avg_loss:0.250, val_acc:0.854]
Epoch [92/120    avg_loss:0.226, val_acc:0.854]
Epoch [93/120    avg_loss:0.222, val_acc:0.854]
Epoch [94/120    avg_loss:0.218, val_acc:0.854]
Epoch [95/120    avg_loss:0.233, val_acc:0.854]
Epoch [96/120    avg_loss:0.221, val_acc:0.854]
Epoch [97/120    avg_loss:0.222, val_acc:0.854]
Epoch [98/120    avg_loss:0.237, val_acc:0.854]
Epoch [99/120    avg_loss:0.217, val_acc:0.854]
Epoch [100/120    avg_loss:0.207, val_acc:0.854]
Epoch [101/120    avg_loss:0.224, val_acc:0.854]
Epoch [102/120    avg_loss:0.227, val_acc:0.854]
Epoch [103/120    avg_loss:0.241, val_acc:0.854]
Epoch [104/120    avg_loss:0.209, val_acc:0.854]
Epoch [105/120    avg_loss:0.238, val_acc:0.854]
Epoch [106/120    avg_loss:0.223, val_acc:0.854]
Epoch [107/120    avg_loss:0.224, val_acc:0.854]
Epoch [108/120    avg_loss:0.205, val_acc:0.854]
Epoch [109/120    avg_loss:0.210, val_acc:0.854]
Epoch [110/120    avg_loss:0.245, val_acc:0.854]
Epoch [111/120    avg_loss:0.215, val_acc:0.854]
Epoch [112/120    avg_loss:0.227, val_acc:0.854]
Epoch [113/120    avg_loss:0.254, val_acc:0.854]
Epoch [114/120    avg_loss:0.225, val_acc:0.854]
Epoch [115/120    avg_loss:0.255, val_acc:0.854]
Epoch [116/120    avg_loss:0.252, val_acc:0.854]
Epoch [117/120    avg_loss:0.227, val_acc:0.854]
Epoch [118/120    avg_loss:0.222, val_acc:0.854]
Epoch [119/120    avg_loss:0.229, val_acc:0.854]
Epoch [120/120    avg_loss:0.227, val_acc:0.854]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5704     0     5   188     0    20     0   431    84]
 [    0     0 15554     0   264     0  2270     0     2     0]
 [    0    10     0  1895     6     0     0     0   113    12]
 [    0   209    23     0  2634     0    56     0    50     0]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     1   416     0    14     0  4257     0   190     0]
 [    0     5     0     0     0     0     0  1271     0    14]
 [    0   143     0    32    44     0    22     0  3330     0]
 [    0     8     0     4    17    82     0     0     0   808]]

Accuracy:
88.58602655869664

F1 scores:
[       nan 0.91176471 0.91271308 0.95417925 0.85812022 0.96915645
 0.74015474 0.99258102 0.86639781 0.87921654]

Kappa:
0.8520903684252779
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f2f6bb978>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.892, val_acc:0.298]
Epoch [2/120    avg_loss:1.320, val_acc:0.483]
Epoch [3/120    avg_loss:1.003, val_acc:0.537]
Epoch [4/120    avg_loss:0.760, val_acc:0.677]
Epoch [5/120    avg_loss:0.626, val_acc:0.699]
Epoch [6/120    avg_loss:0.535, val_acc:0.822]
Epoch [7/120    avg_loss:0.471, val_acc:0.761]
Epoch [8/120    avg_loss:0.423, val_acc:0.807]
Epoch [9/120    avg_loss:0.375, val_acc:0.870]
Epoch [10/120    avg_loss:0.319, val_acc:0.764]
Epoch [11/120    avg_loss:0.388, val_acc:0.797]
Epoch [12/120    avg_loss:0.320, val_acc:0.832]
Epoch [13/120    avg_loss:0.282, val_acc:0.847]
Epoch [14/120    avg_loss:0.255, val_acc:0.907]
Epoch [15/120    avg_loss:0.201, val_acc:0.899]
Epoch [16/120    avg_loss:0.211, val_acc:0.836]
Epoch [17/120    avg_loss:0.199, val_acc:0.886]
Epoch [18/120    avg_loss:0.188, val_acc:0.947]
Epoch [19/120    avg_loss:0.175, val_acc:0.929]
Epoch [20/120    avg_loss:0.138, val_acc:0.939]
Epoch [21/120    avg_loss:0.110, val_acc:0.960]
Epoch [22/120    avg_loss:0.117, val_acc:0.911]
Epoch [23/120    avg_loss:0.139, val_acc:0.944]
Epoch [24/120    avg_loss:0.109, val_acc:0.962]
Epoch [25/120    avg_loss:0.087, val_acc:0.941]
Epoch [26/120    avg_loss:0.078, val_acc:0.938]
Epoch [27/120    avg_loss:0.087, val_acc:0.955]
Epoch [28/120    avg_loss:0.099, val_acc:0.943]
Epoch [29/120    avg_loss:0.104, val_acc:0.931]
Epoch [30/120    avg_loss:0.106, val_acc:0.956]
Epoch [31/120    avg_loss:0.064, val_acc:0.969]
Epoch [32/120    avg_loss:0.062, val_acc:0.976]
Epoch [33/120    avg_loss:0.082, val_acc:0.945]
Epoch [34/120    avg_loss:0.047, val_acc:0.959]
Epoch [35/120    avg_loss:0.062, val_acc:0.933]
Epoch [36/120    avg_loss:0.074, val_acc:0.949]
Epoch [37/120    avg_loss:0.047, val_acc:0.975]
Epoch [38/120    avg_loss:0.042, val_acc:0.975]
Epoch [39/120    avg_loss:0.062, val_acc:0.902]
Epoch [40/120    avg_loss:0.046, val_acc:0.982]
Epoch [41/120    avg_loss:0.075, val_acc:0.964]
Epoch [42/120    avg_loss:0.054, val_acc:0.961]
Epoch [43/120    avg_loss:0.041, val_acc:0.986]
Epoch [44/120    avg_loss:0.033, val_acc:0.976]
Epoch [45/120    avg_loss:0.023, val_acc:0.986]
Epoch [46/120    avg_loss:0.022, val_acc:0.980]
Epoch [47/120    avg_loss:0.020, val_acc:0.985]
Epoch [48/120    avg_loss:0.015, val_acc:0.983]
Epoch [49/120    avg_loss:0.017, val_acc:0.987]
Epoch [50/120    avg_loss:0.014, val_acc:0.985]
Epoch [51/120    avg_loss:0.019, val_acc:0.986]
Epoch [52/120    avg_loss:0.018, val_acc:0.989]
Epoch [53/120    avg_loss:0.030, val_acc:0.991]
Epoch [54/120    avg_loss:0.039, val_acc:0.963]
Epoch [55/120    avg_loss:0.020, val_acc:0.983]
Epoch [56/120    avg_loss:0.045, val_acc:0.973]
Epoch [57/120    avg_loss:0.037, val_acc:0.965]
Epoch [58/120    avg_loss:0.039, val_acc:0.985]
Epoch [59/120    avg_loss:0.026, val_acc:0.976]
Epoch [60/120    avg_loss:0.022, val_acc:0.988]
Epoch [61/120    avg_loss:0.060, val_acc:0.955]
Epoch [62/120    avg_loss:0.038, val_acc:0.982]
Epoch [63/120    avg_loss:0.044, val_acc:0.968]
Epoch [64/120    avg_loss:0.053, val_acc:0.981]
Epoch [65/120    avg_loss:0.040, val_acc:0.976]
Epoch [66/120    avg_loss:0.039, val_acc:0.978]
Epoch [67/120    avg_loss:0.019, val_acc:0.983]
Epoch [68/120    avg_loss:0.017, val_acc:0.980]
Epoch [69/120    avg_loss:0.012, val_acc:0.982]
Epoch [70/120    avg_loss:0.015, val_acc:0.985]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.985]
Epoch [73/120    avg_loss:0.013, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.011, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.987]
Epoch [77/120    avg_loss:0.012, val_acc:0.985]
Epoch [78/120    avg_loss:0.011, val_acc:0.987]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.988]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.009, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.987]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.009, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.012, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6416     0     2     0     0     3     5     6     0]
 [    0     1 17956     0    88     0    45     0     0     0]
 [    0    10     0  1974     0     0     0     0    47     5]
 [    0    38    24     0  2890     0     1     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    18     0     0  4837     0    23     0]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     1     0     0    52     0     0     0  3509     9]
 [    0     0     0     0    14    43     0     0     0   862]]

Accuracy:
98.89860940399586

F1 scores:
[       nan 0.99488293 0.99561963 0.97965261 0.96077128 0.98379193
 0.99057956 0.99690163 0.97811847 0.95991091]

Kappa:
0.985422245251733
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3fb4432940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.865, val_acc:0.439]
Epoch [2/120    avg_loss:1.358, val_acc:0.723]
Epoch [3/120    avg_loss:1.021, val_acc:0.481]
Epoch [4/120    avg_loss:0.768, val_acc:0.754]
Epoch [5/120    avg_loss:0.596, val_acc:0.789]
Epoch [6/120    avg_loss:0.448, val_acc:0.822]
Epoch [7/120    avg_loss:0.405, val_acc:0.798]
Epoch [8/120    avg_loss:0.326, val_acc:0.826]
Epoch [9/120    avg_loss:0.280, val_acc:0.901]
Epoch [10/120    avg_loss:0.289, val_acc:0.833]
Epoch [11/120    avg_loss:0.223, val_acc:0.892]
Epoch [12/120    avg_loss:0.219, val_acc:0.927]
Epoch [13/120    avg_loss:0.189, val_acc:0.920]
Epoch [14/120    avg_loss:0.204, val_acc:0.930]
Epoch [15/120    avg_loss:0.134, val_acc:0.921]
Epoch [16/120    avg_loss:0.153, val_acc:0.902]
Epoch [17/120    avg_loss:0.147, val_acc:0.942]
Epoch [18/120    avg_loss:0.123, val_acc:0.938]
Epoch [19/120    avg_loss:0.118, val_acc:0.903]
Epoch [20/120    avg_loss:0.100, val_acc:0.925]
Epoch [21/120    avg_loss:0.221, val_acc:0.876]
Epoch [22/120    avg_loss:0.144, val_acc:0.892]
Epoch [23/120    avg_loss:0.114, val_acc:0.924]
Epoch [24/120    avg_loss:0.078, val_acc:0.951]
Epoch [25/120    avg_loss:0.052, val_acc:0.938]
Epoch [26/120    avg_loss:0.073, val_acc:0.924]
Epoch [27/120    avg_loss:0.109, val_acc:0.950]
Epoch [28/120    avg_loss:0.072, val_acc:0.949]
Epoch [29/120    avg_loss:0.286, val_acc:0.884]
Epoch [30/120    avg_loss:0.289, val_acc:0.879]
Epoch [31/120    avg_loss:0.134, val_acc:0.934]
Epoch [32/120    avg_loss:0.116, val_acc:0.955]
Epoch [33/120    avg_loss:0.090, val_acc:0.923]
Epoch [34/120    avg_loss:0.066, val_acc:0.949]
Epoch [35/120    avg_loss:0.071, val_acc:0.955]
Epoch [36/120    avg_loss:0.161, val_acc:0.925]
Epoch [37/120    avg_loss:0.103, val_acc:0.949]
Epoch [38/120    avg_loss:0.060, val_acc:0.946]
Epoch [39/120    avg_loss:0.056, val_acc:0.937]
Epoch [40/120    avg_loss:0.045, val_acc:0.903]
Epoch [41/120    avg_loss:0.103, val_acc:0.928]
Epoch [42/120    avg_loss:0.059, val_acc:0.963]
Epoch [43/120    avg_loss:0.046, val_acc:0.966]
Epoch [44/120    avg_loss:0.040, val_acc:0.973]
Epoch [45/120    avg_loss:0.041, val_acc:0.958]
Epoch [46/120    avg_loss:0.044, val_acc:0.966]
Epoch [47/120    avg_loss:0.044, val_acc:0.935]
Epoch [48/120    avg_loss:0.053, val_acc:0.974]
Epoch [49/120    avg_loss:0.035, val_acc:0.955]
Epoch [50/120    avg_loss:0.036, val_acc:0.962]
Epoch [51/120    avg_loss:0.048, val_acc:0.923]
Epoch [52/120    avg_loss:0.063, val_acc:0.969]
Epoch [53/120    avg_loss:0.025, val_acc:0.970]
Epoch [54/120    avg_loss:0.025, val_acc:0.974]
Epoch [55/120    avg_loss:0.024, val_acc:0.972]
Epoch [56/120    avg_loss:0.024, val_acc:0.984]
Epoch [57/120    avg_loss:0.026, val_acc:0.969]
Epoch [58/120    avg_loss:0.018, val_acc:0.979]
Epoch [59/120    avg_loss:0.021, val_acc:0.976]
Epoch [60/120    avg_loss:0.020, val_acc:0.977]
Epoch [61/120    avg_loss:0.026, val_acc:0.961]
Epoch [62/120    avg_loss:0.028, val_acc:0.978]
Epoch [63/120    avg_loss:0.012, val_acc:0.977]
Epoch [64/120    avg_loss:0.035, val_acc:0.974]
Epoch [65/120    avg_loss:0.019, val_acc:0.978]
Epoch [66/120    avg_loss:0.026, val_acc:0.975]
Epoch [67/120    avg_loss:0.018, val_acc:0.979]
Epoch [68/120    avg_loss:0.019, val_acc:0.978]
Epoch [69/120    avg_loss:0.012, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.011, val_acc:0.979]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.982]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.985]
Epoch [82/120    avg_loss:0.008, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.981]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     3     0     0    11     0    49     0]
 [    0     0 17989     0    41     0    60     0     0     0]
 [    0     2     0  2025     0     0     0     0     6     3]
 [    0    32     8     0  2895     0     6     4    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4870     0     2     0]
 [    0     5     0     0     0     0     4  1279     0     2]
 [    0     7     0     0    56     0     0     0  3479    29]
 [    0     0     0     0    25    46     0     0     0   848]]

Accuracy:
98.95404044055624

F1 scores:
[       nan 0.99151553 0.99697952 0.995086   0.96677242 0.98268072
 0.99094516 0.99417023 0.97532941 0.94169906]

Kappa:
0.9861544436306318
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f360a791748>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.946, val_acc:0.372]
Epoch [2/120    avg_loss:1.339, val_acc:0.657]
Epoch [3/120    avg_loss:0.974, val_acc:0.749]
Epoch [4/120    avg_loss:0.728, val_acc:0.711]
Epoch [5/120    avg_loss:0.617, val_acc:0.754]
Epoch [6/120    avg_loss:0.498, val_acc:0.821]
Epoch [7/120    avg_loss:0.422, val_acc:0.833]
Epoch [8/120    avg_loss:0.350, val_acc:0.822]
Epoch [9/120    avg_loss:0.326, val_acc:0.881]
Epoch [10/120    avg_loss:0.288, val_acc:0.869]
Epoch [11/120    avg_loss:0.280, val_acc:0.801]
Epoch [12/120    avg_loss:0.231, val_acc:0.872]
Epoch [13/120    avg_loss:0.215, val_acc:0.931]
Epoch [14/120    avg_loss:0.203, val_acc:0.901]
Epoch [15/120    avg_loss:0.184, val_acc:0.928]
Epoch [16/120    avg_loss:0.153, val_acc:0.915]
Epoch [17/120    avg_loss:0.122, val_acc:0.937]
Epoch [18/120    avg_loss:0.114, val_acc:0.972]
Epoch [19/120    avg_loss:0.108, val_acc:0.958]
Epoch [20/120    avg_loss:0.109, val_acc:0.942]
Epoch [21/120    avg_loss:0.112, val_acc:0.949]
Epoch [22/120    avg_loss:0.096, val_acc:0.915]
Epoch [23/120    avg_loss:0.064, val_acc:0.939]
Epoch [24/120    avg_loss:0.098, val_acc:0.933]
Epoch [25/120    avg_loss:0.098, val_acc:0.941]
Epoch [26/120    avg_loss:0.104, val_acc:0.907]
Epoch [27/120    avg_loss:0.094, val_acc:0.942]
Epoch [28/120    avg_loss:0.072, val_acc:0.970]
Epoch [29/120    avg_loss:0.061, val_acc:0.976]
Epoch [30/120    avg_loss:0.054, val_acc:0.973]
Epoch [31/120    avg_loss:0.050, val_acc:0.978]
Epoch [32/120    avg_loss:0.048, val_acc:0.959]
Epoch [33/120    avg_loss:0.057, val_acc:0.978]
Epoch [34/120    avg_loss:0.038, val_acc:0.968]
Epoch [35/120    avg_loss:0.042, val_acc:0.955]
Epoch [36/120    avg_loss:0.032, val_acc:0.979]
Epoch [37/120    avg_loss:0.041, val_acc:0.969]
Epoch [38/120    avg_loss:0.037, val_acc:0.959]
Epoch [39/120    avg_loss:0.048, val_acc:0.974]
Epoch [40/120    avg_loss:0.024, val_acc:0.984]
Epoch [41/120    avg_loss:0.029, val_acc:0.982]
Epoch [42/120    avg_loss:0.028, val_acc:0.975]
Epoch [43/120    avg_loss:0.185, val_acc:0.951]
Epoch [44/120    avg_loss:0.103, val_acc:0.935]
Epoch [45/120    avg_loss:0.086, val_acc:0.960]
Epoch [46/120    avg_loss:0.038, val_acc:0.973]
Epoch [47/120    avg_loss:0.030, val_acc:0.982]
Epoch [48/120    avg_loss:0.090, val_acc:0.970]
Epoch [49/120    avg_loss:0.055, val_acc:0.968]
Epoch [50/120    avg_loss:0.033, val_acc:0.986]
Epoch [51/120    avg_loss:0.025, val_acc:0.983]
Epoch [52/120    avg_loss:0.018, val_acc:0.973]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.014, val_acc:0.983]
Epoch [55/120    avg_loss:0.016, val_acc:0.981]
Epoch [56/120    avg_loss:0.029, val_acc:0.979]
Epoch [57/120    avg_loss:0.024, val_acc:0.967]
Epoch [58/120    avg_loss:0.022, val_acc:0.991]
Epoch [59/120    avg_loss:0.011, val_acc:0.982]
Epoch [60/120    avg_loss:0.017, val_acc:0.984]
Epoch [61/120    avg_loss:0.020, val_acc:0.981]
Epoch [62/120    avg_loss:0.018, val_acc:0.980]
Epoch [63/120    avg_loss:0.012, val_acc:0.987]
Epoch [64/120    avg_loss:0.015, val_acc:0.979]
Epoch [65/120    avg_loss:0.010, val_acc:0.987]
Epoch [66/120    avg_loss:0.012, val_acc:0.973]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.020, val_acc:0.988]
Epoch [69/120    avg_loss:0.008, val_acc:0.987]
Epoch [70/120    avg_loss:0.010, val_acc:0.987]
Epoch [71/120    avg_loss:0.011, val_acc:0.985]
Epoch [72/120    avg_loss:0.013, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.989]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.006, val_acc:0.989]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.987]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.010, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.011, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6305     0    24     1     0    26     5    71     0]
 [    0     0 18030     0    42     0    17     0     1     0]
 [    0     0     0  2028     0     0     0     0     8     0]
 [    0    30    17     0  2884     0    14     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    10     0     0  4868     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0     0    58     0     0     0  3491    20]
 [    0     0     0     0    14    46     0     0     0   859]]

Accuracy:
98.95645048562409

F1 scores:
[       nan 0.98754797 0.99786922 0.9897511  0.96600234 0.98268072
 0.99316536 0.99806576 0.97391547 0.95550612]

Kappa:
0.9861819134858528
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe25edaa898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.866, val_acc:0.321]
Epoch [2/120    avg_loss:1.368, val_acc:0.370]
Epoch [3/120    avg_loss:1.117, val_acc:0.527]
Epoch [4/120    avg_loss:0.895, val_acc:0.608]
Epoch [5/120    avg_loss:0.787, val_acc:0.644]
Epoch [6/120    avg_loss:0.639, val_acc:0.726]
Epoch [7/120    avg_loss:0.575, val_acc:0.768]
Epoch [8/120    avg_loss:0.473, val_acc:0.753]
Epoch [9/120    avg_loss:0.398, val_acc:0.861]
Epoch [10/120    avg_loss:0.409, val_acc:0.794]
Epoch [11/120    avg_loss:0.367, val_acc:0.825]
Epoch [12/120    avg_loss:0.354, val_acc:0.888]
Epoch [13/120    avg_loss:0.245, val_acc:0.858]
Epoch [14/120    avg_loss:0.219, val_acc:0.926]
Epoch [15/120    avg_loss:0.187, val_acc:0.851]
Epoch [16/120    avg_loss:0.190, val_acc:0.920]
Epoch [17/120    avg_loss:0.179, val_acc:0.817]
Epoch [18/120    avg_loss:0.151, val_acc:0.911]
Epoch [19/120    avg_loss:0.153, val_acc:0.892]
Epoch [20/120    avg_loss:0.124, val_acc:0.924]
Epoch [21/120    avg_loss:0.112, val_acc:0.963]
Epoch [22/120    avg_loss:0.089, val_acc:0.965]
Epoch [23/120    avg_loss:0.065, val_acc:0.948]
Epoch [24/120    avg_loss:0.093, val_acc:0.959]
Epoch [25/120    avg_loss:0.080, val_acc:0.949]
Epoch [26/120    avg_loss:0.085, val_acc:0.951]
Epoch [27/120    avg_loss:0.070, val_acc:0.976]
Epoch [28/120    avg_loss:0.054, val_acc:0.982]
Epoch [29/120    avg_loss:0.040, val_acc:0.980]
Epoch [30/120    avg_loss:0.042, val_acc:0.954]
Epoch [31/120    avg_loss:0.081, val_acc:0.938]
Epoch [32/120    avg_loss:0.096, val_acc:0.903]
Epoch [33/120    avg_loss:0.160, val_acc:0.939]
Epoch [34/120    avg_loss:0.141, val_acc:0.939]
Epoch [35/120    avg_loss:0.118, val_acc:0.928]
Epoch [36/120    avg_loss:0.088, val_acc:0.935]
Epoch [37/120    avg_loss:0.046, val_acc:0.979]
Epoch [38/120    avg_loss:0.054, val_acc:0.986]
Epoch [39/120    avg_loss:0.050, val_acc:0.973]
Epoch [40/120    avg_loss:0.046, val_acc:0.982]
Epoch [41/120    avg_loss:0.044, val_acc:0.975]
Epoch [42/120    avg_loss:0.040, val_acc:0.971]
Epoch [43/120    avg_loss:0.022, val_acc:0.985]
Epoch [44/120    avg_loss:0.024, val_acc:0.971]
Epoch [45/120    avg_loss:0.031, val_acc:0.988]
Epoch [46/120    avg_loss:0.021, val_acc:0.988]
Epoch [47/120    avg_loss:0.041, val_acc:0.973]
Epoch [48/120    avg_loss:0.037, val_acc:0.981]
Epoch [49/120    avg_loss:0.017, val_acc:0.990]
Epoch [50/120    avg_loss:0.021, val_acc:0.986]
Epoch [51/120    avg_loss:0.043, val_acc:0.957]
Epoch [52/120    avg_loss:0.028, val_acc:0.987]
Epoch [53/120    avg_loss:0.018, val_acc:0.985]
Epoch [54/120    avg_loss:0.015, val_acc:0.970]
Epoch [55/120    avg_loss:0.022, val_acc:0.982]
Epoch [56/120    avg_loss:0.010, val_acc:0.992]
Epoch [57/120    avg_loss:0.012, val_acc:0.982]
Epoch [58/120    avg_loss:0.009, val_acc:0.990]
Epoch [59/120    avg_loss:0.017, val_acc:0.970]
Epoch [60/120    avg_loss:0.023, val_acc:0.987]
Epoch [61/120    avg_loss:0.016, val_acc:0.987]
Epoch [62/120    avg_loss:0.014, val_acc:0.989]
Epoch [63/120    avg_loss:0.140, val_acc:0.889]
Epoch [64/120    avg_loss:0.159, val_acc:0.938]
Epoch [65/120    avg_loss:0.100, val_acc:0.893]
Epoch [66/120    avg_loss:0.068, val_acc:0.965]
Epoch [67/120    avg_loss:0.047, val_acc:0.969]
Epoch [68/120    avg_loss:0.053, val_acc:0.977]
Epoch [69/120    avg_loss:0.052, val_acc:0.981]
Epoch [70/120    avg_loss:0.025, val_acc:0.985]
Epoch [71/120    avg_loss:0.026, val_acc:0.986]
Epoch [72/120    avg_loss:0.019, val_acc:0.985]
Epoch [73/120    avg_loss:0.021, val_acc:0.986]
Epoch [74/120    avg_loss:0.021, val_acc:0.985]
Epoch [75/120    avg_loss:0.019, val_acc:0.986]
Epoch [76/120    avg_loss:0.022, val_acc:0.986]
Epoch [77/120    avg_loss:0.021, val_acc:0.987]
Epoch [78/120    avg_loss:0.021, val_acc:0.987]
Epoch [79/120    avg_loss:0.020, val_acc:0.987]
Epoch [80/120    avg_loss:0.021, val_acc:0.987]
Epoch [81/120    avg_loss:0.018, val_acc:0.987]
Epoch [82/120    avg_loss:0.017, val_acc:0.988]
Epoch [83/120    avg_loss:0.016, val_acc:0.988]
Epoch [84/120    avg_loss:0.015, val_acc:0.989]
Epoch [85/120    avg_loss:0.016, val_acc:0.989]
Epoch [86/120    avg_loss:0.014, val_acc:0.989]
Epoch [87/120    avg_loss:0.013, val_acc:0.989]
Epoch [88/120    avg_loss:0.017, val_acc:0.989]
Epoch [89/120    avg_loss:0.015, val_acc:0.989]
Epoch [90/120    avg_loss:0.020, val_acc:0.989]
Epoch [91/120    avg_loss:0.019, val_acc:0.989]
Epoch [92/120    avg_loss:0.016, val_acc:0.990]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.990]
Epoch [95/120    avg_loss:0.016, val_acc:0.990]
Epoch [96/120    avg_loss:0.012, val_acc:0.990]
Epoch [97/120    avg_loss:0.014, val_acc:0.990]
Epoch [98/120    avg_loss:0.017, val_acc:0.990]
Epoch [99/120    avg_loss:0.016, val_acc:0.990]
Epoch [100/120    avg_loss:0.014, val_acc:0.990]
Epoch [101/120    avg_loss:0.015, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.990]
Epoch [103/120    avg_loss:0.013, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.990]
Epoch [105/120    avg_loss:0.016, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.990]
Epoch [107/120    avg_loss:0.017, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.990]
Epoch [109/120    avg_loss:0.015, val_acc:0.990]
Epoch [110/120    avg_loss:0.012, val_acc:0.990]
Epoch [111/120    avg_loss:0.014, val_acc:0.990]
Epoch [112/120    avg_loss:0.015, val_acc:0.990]
Epoch [113/120    avg_loss:0.014, val_acc:0.990]
Epoch [114/120    avg_loss:0.016, val_acc:0.990]
Epoch [115/120    avg_loss:0.013, val_acc:0.990]
Epoch [116/120    avg_loss:0.015, val_acc:0.990]
Epoch [117/120    avg_loss:0.014, val_acc:0.990]
Epoch [118/120    avg_loss:0.016, val_acc:0.990]
Epoch [119/120    avg_loss:0.014, val_acc:0.990]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0     0     0     0     0    16    52    14]
 [    0     5 17938     0   108     0    22     0    17     0]
 [    0    14     0  2013     0     0     0     0     5     4]
 [    0    26    16     0  2899     0     8     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     5     0     0  4841     0     2     0]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0    22     1    12    56     0     0     0  3480     0]
 [    0     0     0     0    18    30     0     0     0   871]]

Accuracy:
98.76846697033234

F1 scores:
[       nan 0.98840377 0.99448371 0.99016232 0.95787213 0.98863636
 0.9929238  0.99189502 0.97342657 0.9618995 ]

Kappa:
0.9837021588358071
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e451a7898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.901, val_acc:0.307]
Epoch [2/120    avg_loss:1.355, val_acc:0.557]
Epoch [3/120    avg_loss:1.053, val_acc:0.579]
Epoch [4/120    avg_loss:0.789, val_acc:0.676]
Epoch [5/120    avg_loss:0.611, val_acc:0.715]
Epoch [6/120    avg_loss:0.478, val_acc:0.726]
Epoch [7/120    avg_loss:0.448, val_acc:0.755]
Epoch [8/120    avg_loss:0.345, val_acc:0.831]
Epoch [9/120    avg_loss:0.410, val_acc:0.841]
Epoch [10/120    avg_loss:0.307, val_acc:0.894]
Epoch [11/120    avg_loss:0.269, val_acc:0.901]
Epoch [12/120    avg_loss:0.238, val_acc:0.864]
Epoch [13/120    avg_loss:0.222, val_acc:0.927]
Epoch [14/120    avg_loss:0.187, val_acc:0.941]
Epoch [15/120    avg_loss:0.150, val_acc:0.903]
Epoch [16/120    avg_loss:0.148, val_acc:0.949]
Epoch [17/120    avg_loss:0.124, val_acc:0.938]
Epoch [18/120    avg_loss:0.178, val_acc:0.905]
Epoch [19/120    avg_loss:0.133, val_acc:0.911]
Epoch [20/120    avg_loss:0.112, val_acc:0.948]
Epoch [21/120    avg_loss:0.092, val_acc:0.938]
Epoch [22/120    avg_loss:0.073, val_acc:0.950]
Epoch [23/120    avg_loss:0.103, val_acc:0.962]
Epoch [24/120    avg_loss:0.064, val_acc:0.961]
Epoch [25/120    avg_loss:0.078, val_acc:0.953]
Epoch [26/120    avg_loss:0.057, val_acc:0.965]
Epoch [27/120    avg_loss:0.061, val_acc:0.971]
Epoch [28/120    avg_loss:0.049, val_acc:0.973]
Epoch [29/120    avg_loss:0.044, val_acc:0.970]
Epoch [30/120    avg_loss:0.065, val_acc:0.967]
Epoch [31/120    avg_loss:0.053, val_acc:0.965]
Epoch [32/120    avg_loss:0.048, val_acc:0.928]
Epoch [33/120    avg_loss:0.036, val_acc:0.970]
Epoch [34/120    avg_loss:0.039, val_acc:0.972]
Epoch [35/120    avg_loss:0.020, val_acc:0.980]
Epoch [36/120    avg_loss:0.023, val_acc:0.979]
Epoch [37/120    avg_loss:0.030, val_acc:0.959]
Epoch [38/120    avg_loss:0.043, val_acc:0.963]
Epoch [39/120    avg_loss:0.025, val_acc:0.973]
Epoch [40/120    avg_loss:0.042, val_acc:0.968]
Epoch [41/120    avg_loss:0.027, val_acc:0.971]
Epoch [42/120    avg_loss:0.030, val_acc:0.977]
Epoch [43/120    avg_loss:0.021, val_acc:0.976]
Epoch [44/120    avg_loss:0.047, val_acc:0.946]
Epoch [45/120    avg_loss:0.053, val_acc:0.977]
Epoch [46/120    avg_loss:0.039, val_acc:0.964]
Epoch [47/120    avg_loss:0.048, val_acc:0.975]
Epoch [48/120    avg_loss:0.024, val_acc:0.977]
Epoch [49/120    avg_loss:0.015, val_acc:0.983]
Epoch [50/120    avg_loss:0.014, val_acc:0.985]
Epoch [51/120    avg_loss:0.012, val_acc:0.985]
Epoch [52/120    avg_loss:0.013, val_acc:0.986]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.012, val_acc:0.986]
Epoch [55/120    avg_loss:0.012, val_acc:0.986]
Epoch [56/120    avg_loss:0.012, val_acc:0.983]
Epoch [57/120    avg_loss:0.010, val_acc:0.986]
Epoch [58/120    avg_loss:0.011, val_acc:0.985]
Epoch [59/120    avg_loss:0.010, val_acc:0.981]
Epoch [60/120    avg_loss:0.015, val_acc:0.981]
Epoch [61/120    avg_loss:0.012, val_acc:0.983]
Epoch [62/120    avg_loss:0.009, val_acc:0.984]
Epoch [63/120    avg_loss:0.009, val_acc:0.982]
Epoch [64/120    avg_loss:0.009, val_acc:0.982]
Epoch [65/120    avg_loss:0.011, val_acc:0.981]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.981]
Epoch [68/120    avg_loss:0.009, val_acc:0.982]
Epoch [69/120    avg_loss:0.011, val_acc:0.982]
Epoch [70/120    avg_loss:0.009, val_acc:0.981]
Epoch [71/120    avg_loss:0.010, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.011, val_acc:0.981]
Epoch [74/120    avg_loss:0.011, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.981]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.010, val_acc:0.982]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.013, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.014, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.982]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.013, val_acc:0.982]
Epoch [97/120    avg_loss:0.010, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.012, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.982]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6338     0     0     1     0     8     0    84     1]
 [    0     0 18032     0    47     0    11     0     0     0]
 [    0     0     0  2016     0     0     0     0    17     3]
 [    0    37    21     0  2882     0     2     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10    11     0     0  4851     0     2     4]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     1     0     0    61     0     0     0  3503     6]
 [    0     0     0     0    14    37     0     0     0   868]]

Accuracy:
99.01429156725231

F1 scores:
[       nan 0.98969394 0.99753824 0.99237017 0.96436339 0.98602191
 0.99507692 0.99961225 0.97211045 0.96337403]

Kappa:
0.9869443155554927
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f817de54940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.894, val_acc:0.393]
Epoch [2/120    avg_loss:1.364, val_acc:0.518]
Epoch [3/120    avg_loss:1.053, val_acc:0.640]
Epoch [4/120    avg_loss:0.838, val_acc:0.629]
Epoch [5/120    avg_loss:0.617, val_acc:0.681]
Epoch [6/120    avg_loss:0.528, val_acc:0.755]
Epoch [7/120    avg_loss:0.400, val_acc:0.855]
Epoch [8/120    avg_loss:0.418, val_acc:0.855]
Epoch [9/120    avg_loss:0.377, val_acc:0.854]
Epoch [10/120    avg_loss:0.283, val_acc:0.894]
Epoch [11/120    avg_loss:0.254, val_acc:0.926]
Epoch [12/120    avg_loss:0.207, val_acc:0.939]
Epoch [13/120    avg_loss:0.186, val_acc:0.903]
Epoch [14/120    avg_loss:0.200, val_acc:0.908]
Epoch [15/120    avg_loss:0.157, val_acc:0.937]
Epoch [16/120    avg_loss:0.167, val_acc:0.941]
Epoch [17/120    avg_loss:0.131, val_acc:0.935]
Epoch [18/120    avg_loss:0.103, val_acc:0.953]
Epoch [19/120    avg_loss:0.110, val_acc:0.938]
Epoch [20/120    avg_loss:0.102, val_acc:0.948]
Epoch [21/120    avg_loss:0.156, val_acc:0.941]
Epoch [22/120    avg_loss:0.175, val_acc:0.908]
Epoch [23/120    avg_loss:0.192, val_acc:0.941]
Epoch [24/120    avg_loss:0.257, val_acc:0.921]
Epoch [25/120    avg_loss:0.112, val_acc:0.955]
Epoch [26/120    avg_loss:0.088, val_acc:0.961]
Epoch [27/120    avg_loss:0.099, val_acc:0.943]
Epoch [28/120    avg_loss:0.079, val_acc:0.974]
Epoch [29/120    avg_loss:0.057, val_acc:0.965]
Epoch [30/120    avg_loss:0.056, val_acc:0.975]
Epoch [31/120    avg_loss:0.060, val_acc:0.932]
Epoch [32/120    avg_loss:0.055, val_acc:0.932]
Epoch [33/120    avg_loss:0.064, val_acc:0.979]
Epoch [34/120    avg_loss:0.033, val_acc:0.971]
Epoch [35/120    avg_loss:0.037, val_acc:0.976]
Epoch [36/120    avg_loss:0.063, val_acc:0.978]
Epoch [37/120    avg_loss:0.037, val_acc:0.976]
Epoch [38/120    avg_loss:0.038, val_acc:0.981]
Epoch [39/120    avg_loss:0.034, val_acc:0.982]
Epoch [40/120    avg_loss:0.027, val_acc:0.981]
Epoch [41/120    avg_loss:0.044, val_acc:0.955]
Epoch [42/120    avg_loss:0.042, val_acc:0.968]
Epoch [43/120    avg_loss:0.043, val_acc:0.981]
Epoch [44/120    avg_loss:0.024, val_acc:0.976]
Epoch [45/120    avg_loss:0.035, val_acc:0.958]
Epoch [46/120    avg_loss:0.030, val_acc:0.973]
Epoch [47/120    avg_loss:0.039, val_acc:0.966]
Epoch [48/120    avg_loss:0.024, val_acc:0.981]
Epoch [49/120    avg_loss:0.018, val_acc:0.973]
Epoch [50/120    avg_loss:0.022, val_acc:0.980]
Epoch [51/120    avg_loss:0.022, val_acc:0.980]
Epoch [52/120    avg_loss:0.027, val_acc:0.955]
Epoch [53/120    avg_loss:0.036, val_acc:0.977]
Epoch [54/120    avg_loss:0.023, val_acc:0.983]
Epoch [55/120    avg_loss:0.019, val_acc:0.984]
Epoch [56/120    avg_loss:0.014, val_acc:0.984]
Epoch [57/120    avg_loss:0.013, val_acc:0.985]
Epoch [58/120    avg_loss:0.012, val_acc:0.985]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.015, val_acc:0.986]
Epoch [61/120    avg_loss:0.012, val_acc:0.986]
Epoch [62/120    avg_loss:0.012, val_acc:0.987]
Epoch [63/120    avg_loss:0.009, val_acc:0.987]
Epoch [64/120    avg_loss:0.015, val_acc:0.985]
Epoch [65/120    avg_loss:0.013, val_acc:0.984]
Epoch [66/120    avg_loss:0.012, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.985]
Epoch [68/120    avg_loss:0.011, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.985]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.012, val_acc:0.985]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.011, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.012, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.012, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.010, val_acc:0.985]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6315     0     0     3     0    20    38    56     0]
 [    0     2 17957     0    59     0    67     0     5     0]
 [    0     0     0  2001     1     0     0     0    33     1]
 [    0    37    13     0  2899     0     2     1    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4867     0    10     0]
 [    0     0     0     0     0     0     3  1284     0     3]
 [    0     3     0    18    37     0     0     0  3497    16]
 [    0     0     0     0    13    31     0     0     0   875]]

Accuracy:
98.81184778155351

F1 scores:
[       nan 0.98756744 0.99592357 0.98692972 0.96891711 0.98826202
 0.98952933 0.98277842 0.97246941 0.96471885]

Kappa:
0.9842794498044093
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d508e5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.876, val_acc:0.407]
Epoch [2/120    avg_loss:1.361, val_acc:0.593]
Epoch [3/120    avg_loss:0.973, val_acc:0.592]
Epoch [4/120    avg_loss:0.713, val_acc:0.709]
Epoch [5/120    avg_loss:0.582, val_acc:0.842]
Epoch [6/120    avg_loss:0.553, val_acc:0.817]
Epoch [7/120    avg_loss:0.433, val_acc:0.827]
Epoch [8/120    avg_loss:0.365, val_acc:0.888]
Epoch [9/120    avg_loss:0.303, val_acc:0.877]
Epoch [10/120    avg_loss:0.317, val_acc:0.916]
Epoch [11/120    avg_loss:0.284, val_acc:0.915]
Epoch [12/120    avg_loss:0.206, val_acc:0.932]
Epoch [13/120    avg_loss:0.175, val_acc:0.930]
Epoch [14/120    avg_loss:0.175, val_acc:0.942]
Epoch [15/120    avg_loss:0.160, val_acc:0.926]
Epoch [16/120    avg_loss:0.219, val_acc:0.878]
Epoch [17/120    avg_loss:0.128, val_acc:0.960]
Epoch [18/120    avg_loss:0.097, val_acc:0.966]
Epoch [19/120    avg_loss:0.091, val_acc:0.959]
Epoch [20/120    avg_loss:0.097, val_acc:0.934]
Epoch [21/120    avg_loss:0.098, val_acc:0.958]
Epoch [22/120    avg_loss:0.093, val_acc:0.939]
Epoch [23/120    avg_loss:0.099, val_acc:0.962]
Epoch [24/120    avg_loss:0.060, val_acc:0.971]
Epoch [25/120    avg_loss:0.068, val_acc:0.966]
Epoch [26/120    avg_loss:0.057, val_acc:0.979]
Epoch [27/120    avg_loss:0.048, val_acc:0.963]
Epoch [28/120    avg_loss:0.051, val_acc:0.970]
Epoch [29/120    avg_loss:0.039, val_acc:0.978]
Epoch [30/120    avg_loss:0.026, val_acc:0.977]
Epoch [31/120    avg_loss:0.032, val_acc:0.976]
Epoch [32/120    avg_loss:0.038, val_acc:0.960]
Epoch [33/120    avg_loss:0.082, val_acc:0.958]
Epoch [34/120    avg_loss:0.055, val_acc:0.960]
Epoch [35/120    avg_loss:0.050, val_acc:0.976]
Epoch [36/120    avg_loss:0.042, val_acc:0.953]
Epoch [37/120    avg_loss:0.045, val_acc:0.968]
Epoch [38/120    avg_loss:0.031, val_acc:0.975]
Epoch [39/120    avg_loss:0.022, val_acc:0.978]
Epoch [40/120    avg_loss:0.014, val_acc:0.978]
Epoch [41/120    avg_loss:0.014, val_acc:0.980]
Epoch [42/120    avg_loss:0.014, val_acc:0.985]
Epoch [43/120    avg_loss:0.010, val_acc:0.985]
Epoch [44/120    avg_loss:0.013, val_acc:0.985]
Epoch [45/120    avg_loss:0.013, val_acc:0.985]
Epoch [46/120    avg_loss:0.013, val_acc:0.985]
Epoch [47/120    avg_loss:0.012, val_acc:0.985]
Epoch [48/120    avg_loss:0.011, val_acc:0.985]
Epoch [49/120    avg_loss:0.012, val_acc:0.984]
Epoch [50/120    avg_loss:0.010, val_acc:0.985]
Epoch [51/120    avg_loss:0.014, val_acc:0.985]
Epoch [52/120    avg_loss:0.012, val_acc:0.984]
Epoch [53/120    avg_loss:0.009, val_acc:0.985]
Epoch [54/120    avg_loss:0.012, val_acc:0.984]
Epoch [55/120    avg_loss:0.014, val_acc:0.985]
Epoch [56/120    avg_loss:0.011, val_acc:0.985]
Epoch [57/120    avg_loss:0.010, val_acc:0.985]
Epoch [58/120    avg_loss:0.014, val_acc:0.985]
Epoch [59/120    avg_loss:0.012, val_acc:0.985]
Epoch [60/120    avg_loss:0.013, val_acc:0.985]
Epoch [61/120    avg_loss:0.009, val_acc:0.985]
Epoch [62/120    avg_loss:0.009, val_acc:0.985]
Epoch [63/120    avg_loss:0.010, val_acc:0.985]
Epoch [64/120    avg_loss:0.010, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.985]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.986]
Epoch [68/120    avg_loss:0.008, val_acc:0.985]
Epoch [69/120    avg_loss:0.010, val_acc:0.985]
Epoch [70/120    avg_loss:0.009, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.009, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.011, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6332     0     1     0     0     0     9    82     8]
 [    0     0 17921     0   109     0    53     0     7     0]
 [    0     8     0  1993     0     0     0     0    28     7]
 [    0    28    22     0  2906     0     1     0    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     0     0  4876     0     0     0]
 [    0     0     0     0     0     0     4  1283     0     3]
 [    0    26     0     1    58     0     0     0  3486     0]
 [    0     0     0     0    22    41     0     0     0   856]]

Accuracy:
98.71062588870412

F1 scores:
[       nan 0.98736941 0.9946441  0.98883652 0.95796934 0.98453414
 0.99388504 0.99380325 0.969815   0.95482432]

Kappa:
0.9829429716562715
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:74
Validation dataloader:74
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92224978d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.898, val_acc:0.253]
Epoch [2/120    avg_loss:1.418, val_acc:0.396]
Epoch [3/120    avg_loss:1.101, val_acc:0.643]
Epoch [4/120    avg_loss:0.778, val_acc:0.726]
Epoch [5/120    avg_loss:0.601, val_acc:0.860]
Epoch [6/120    avg_loss:0.506, val_acc:0.839]
Epoch [7/120    avg_loss:0.409, val_acc:0.888]
Epoch [8/120    avg_loss:0.282, val_acc:0.872]
Epoch [9/120    avg_loss:0.289, val_acc:0.910]
Epoch [10/120    avg_loss:0.234, val_acc:0.925]
Epoch [11/120    avg_loss:0.222, val_acc:0.837]
Epoch [12/120    avg_loss:0.216, val_acc:0.929]
Epoch [13/120    avg_loss:0.193, val_acc:0.894]
Epoch [14/120    avg_loss:0.179, val_acc:0.946]
Epoch [15/120    avg_loss:0.176, val_acc:0.840]
Epoch [16/120    avg_loss:0.153, val_acc:0.933]
Epoch [17/120    avg_loss:0.122, val_acc:0.942]
Epoch [18/120    avg_loss:0.118, val_acc:0.943]
Epoch [19/120    avg_loss:0.104, val_acc:0.928]
Epoch [20/120    avg_loss:0.096, val_acc:0.958]
Epoch [21/120    avg_loss:0.120, val_acc:0.957]
Epoch [22/120    avg_loss:0.096, val_acc:0.940]
Epoch [23/120    avg_loss:0.127, val_acc:0.959]
Epoch [24/120    avg_loss:0.107, val_acc:0.966]
Epoch [25/120    avg_loss:0.094, val_acc:0.948]
Epoch [26/120    avg_loss:0.141, val_acc:0.966]
Epoch [27/120    avg_loss:0.099, val_acc:0.958]
Epoch [28/120    avg_loss:0.093, val_acc:0.953]
Epoch [29/120    avg_loss:0.125, val_acc:0.944]
Epoch [30/120    avg_loss:0.135, val_acc:0.966]
Epoch [31/120    avg_loss:0.073, val_acc:0.966]
Epoch [32/120    avg_loss:0.048, val_acc:0.970]
Epoch [33/120    avg_loss:0.053, val_acc:0.966]
Epoch [34/120    avg_loss:0.048, val_acc:0.970]
Epoch [35/120    avg_loss:0.046, val_acc:0.965]
Epoch [36/120    avg_loss:0.045, val_acc:0.975]
Epoch [37/120    avg_loss:0.041, val_acc:0.970]
Epoch [38/120    avg_loss:0.031, val_acc:0.981]
Epoch [39/120    avg_loss:0.035, val_acc:0.978]
Epoch [40/120    avg_loss:0.038, val_acc:0.975]
Epoch [41/120    avg_loss:0.026, val_acc:0.975]
Epoch [42/120    avg_loss:0.039, val_acc:0.976]
Epoch [43/120    avg_loss:0.022, val_acc:0.981]
Epoch [44/120    avg_loss:0.034, val_acc:0.953]
Epoch [45/120    avg_loss:0.027, val_acc:0.981]
Epoch [46/120    avg_loss:0.016, val_acc:0.985]
Epoch [47/120    avg_loss:0.018, val_acc:0.983]
Epoch [48/120    avg_loss:0.028, val_acc:0.969]
Epoch [49/120    avg_loss:0.035, val_acc:0.981]
Epoch [50/120    avg_loss:0.022, val_acc:0.982]
Epoch [51/120    avg_loss:0.026, val_acc:0.981]
Epoch [52/120    avg_loss:0.019, val_acc:0.932]
Epoch [53/120    avg_loss:0.079, val_acc:0.968]
Epoch [54/120    avg_loss:0.045, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.977]
Epoch [57/120    avg_loss:0.013, val_acc:0.978]
Epoch [58/120    avg_loss:0.012, val_acc:0.983]
Epoch [59/120    avg_loss:0.011, val_acc:0.984]
Epoch [60/120    avg_loss:0.008, val_acc:0.986]
Epoch [61/120    avg_loss:0.012, val_acc:0.984]
Epoch [62/120    avg_loss:0.009, val_acc:0.985]
Epoch [63/120    avg_loss:0.010, val_acc:0.986]
Epoch [64/120    avg_loss:0.011, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.985]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.007, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.008, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.009, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.985]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.987]
Epoch [77/120    avg_loss:0.007, val_acc:0.987]
Epoch [78/120    avg_loss:0.007, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.007, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0    14     0     0     9     3    23     1]
 [    0     0 18062     0    20     0     2     0     6     0]
 [    0     0     0  2036     0     0     0     0     0     0]
 [    0    33    16     0  2917     0     0     0     6     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    18     0     0  4855     0     4     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0    18     0     0    51     0     0     0  3501     1]
 [    0     0     0     0    10    44     0     0     0   865]]

Accuracy:
99.32036729086835

F1 scores:
[       nan 0.99214924 0.99875584 0.99220273 0.97721943 0.98342125
 0.99630618 0.99806277 0.98467164 0.96864502]

Kappa:
0.990995833008362
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2efaacb908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.949, val_acc:0.286]
Epoch [2/120    avg_loss:1.461, val_acc:0.347]
Epoch [3/120    avg_loss:1.135, val_acc:0.401]
Epoch [4/120    avg_loss:0.925, val_acc:0.673]
Epoch [5/120    avg_loss:0.739, val_acc:0.681]
Epoch [6/120    avg_loss:0.678, val_acc:0.700]
Epoch [7/120    avg_loss:0.528, val_acc:0.798]
Epoch [8/120    avg_loss:0.463, val_acc:0.829]
Epoch [9/120    avg_loss:0.380, val_acc:0.780]
Epoch [10/120    avg_loss:0.338, val_acc:0.778]
Epoch [11/120    avg_loss:0.300, val_acc:0.907]
Epoch [12/120    avg_loss:0.289, val_acc:0.887]
Epoch [13/120    avg_loss:0.237, val_acc:0.897]
Epoch [14/120    avg_loss:0.209, val_acc:0.895]
Epoch [15/120    avg_loss:0.197, val_acc:0.888]
Epoch [16/120    avg_loss:0.173, val_acc:0.902]
Epoch [17/120    avg_loss:0.198, val_acc:0.924]
Epoch [18/120    avg_loss:0.193, val_acc:0.946]
Epoch [19/120    avg_loss:0.147, val_acc:0.938]
Epoch [20/120    avg_loss:0.146, val_acc:0.640]
Epoch [21/120    avg_loss:0.494, val_acc:0.682]
Epoch [22/120    avg_loss:0.315, val_acc:0.769]
Epoch [23/120    avg_loss:0.266, val_acc:0.899]
Epoch [24/120    avg_loss:0.230, val_acc:0.865]
Epoch [25/120    avg_loss:0.223, val_acc:0.923]
Epoch [26/120    avg_loss:0.154, val_acc:0.940]
Epoch [27/120    avg_loss:0.108, val_acc:0.953]
Epoch [28/120    avg_loss:0.122, val_acc:0.958]
Epoch [29/120    avg_loss:0.077, val_acc:0.961]
Epoch [30/120    avg_loss:0.076, val_acc:0.940]
Epoch [31/120    avg_loss:0.083, val_acc:0.975]
Epoch [32/120    avg_loss:0.054, val_acc:0.969]
Epoch [33/120    avg_loss:0.062, val_acc:0.976]
Epoch [34/120    avg_loss:0.062, val_acc:0.974]
Epoch [35/120    avg_loss:0.046, val_acc:0.976]
Epoch [36/120    avg_loss:0.041, val_acc:0.977]
Epoch [37/120    avg_loss:0.035, val_acc:0.970]
Epoch [38/120    avg_loss:0.069, val_acc:0.947]
Epoch [39/120    avg_loss:0.061, val_acc:0.965]
Epoch [40/120    avg_loss:0.066, val_acc:0.971]
Epoch [41/120    avg_loss:0.042, val_acc:0.960]
Epoch [42/120    avg_loss:0.037, val_acc:0.973]
Epoch [43/120    avg_loss:0.031, val_acc:0.949]
Epoch [44/120    avg_loss:0.033, val_acc:0.985]
Epoch [45/120    avg_loss:0.035, val_acc:0.986]
Epoch [46/120    avg_loss:0.025, val_acc:0.984]
Epoch [47/120    avg_loss:0.041, val_acc:0.971]
Epoch [48/120    avg_loss:0.039, val_acc:0.923]
Epoch [49/120    avg_loss:0.043, val_acc:0.969]
Epoch [50/120    avg_loss:0.083, val_acc:0.949]
Epoch [51/120    avg_loss:0.075, val_acc:0.944]
Epoch [52/120    avg_loss:0.068, val_acc:0.975]
Epoch [53/120    avg_loss:0.036, val_acc:0.970]
Epoch [54/120    avg_loss:0.034, val_acc:0.977]
Epoch [55/120    avg_loss:0.030, val_acc:0.983]
Epoch [56/120    avg_loss:0.024, val_acc:0.984]
Epoch [57/120    avg_loss:0.025, val_acc:0.977]
Epoch [58/120    avg_loss:0.033, val_acc:0.978]
Epoch [59/120    avg_loss:0.018, val_acc:0.982]
Epoch [60/120    avg_loss:0.018, val_acc:0.986]
Epoch [61/120    avg_loss:0.017, val_acc:0.989]
Epoch [62/120    avg_loss:0.015, val_acc:0.989]
Epoch [63/120    avg_loss:0.018, val_acc:0.989]
Epoch [64/120    avg_loss:0.016, val_acc:0.991]
Epoch [65/120    avg_loss:0.013, val_acc:0.991]
Epoch [66/120    avg_loss:0.015, val_acc:0.990]
Epoch [67/120    avg_loss:0.015, val_acc:0.989]
Epoch [68/120    avg_loss:0.013, val_acc:0.986]
Epoch [69/120    avg_loss:0.014, val_acc:0.989]
Epoch [70/120    avg_loss:0.012, val_acc:0.989]
Epoch [71/120    avg_loss:0.014, val_acc:0.989]
Epoch [72/120    avg_loss:0.013, val_acc:0.988]
Epoch [73/120    avg_loss:0.012, val_acc:0.990]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.016, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.987]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.013, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.012, val_acc:0.986]
Epoch [90/120    avg_loss:0.012, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.012, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.015, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.020, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.986]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.010, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.015, val_acc:0.986]
Epoch [108/120    avg_loss:0.014, val_acc:0.986]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.015, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.017, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6379     0     0     0     0     7     0    42     4]
 [    0     8 17954     0    93     0    23     0    12     0]
 [    0     2     0  1993     3     0     0     0    38     0]
 [    0    44    18     0  2869     0    11     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     0     2]
 [    0     0     0     0     0     0     3  1283     0     4]
 [    0    23     0     0    45     0     0     0  3501     2]
 [    0     0     0     1    18    33     0     4     0   863]]

Accuracy:
98.8672788181139

F1 scores:
[       nan 0.9899131  0.99572958 0.98908189 0.95633333 0.98751419
 0.99530516 0.99573147 0.97331109 0.96209588]

Kappa:
0.9850081829578438
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f519b605940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.867, val_acc:0.310]
Epoch [2/120    avg_loss:1.432, val_acc:0.375]
Epoch [3/120    avg_loss:1.152, val_acc:0.482]
Epoch [4/120    avg_loss:0.893, val_acc:0.633]
Epoch [5/120    avg_loss:0.697, val_acc:0.728]
Epoch [6/120    avg_loss:0.627, val_acc:0.719]
Epoch [7/120    avg_loss:0.530, val_acc:0.732]
Epoch [8/120    avg_loss:0.440, val_acc:0.839]
Epoch [9/120    avg_loss:0.370, val_acc:0.862]
Epoch [10/120    avg_loss:0.340, val_acc:0.860]
Epoch [11/120    avg_loss:0.326, val_acc:0.874]
Epoch [12/120    avg_loss:0.275, val_acc:0.861]
Epoch [13/120    avg_loss:0.239, val_acc:0.866]
Epoch [14/120    avg_loss:0.230, val_acc:0.862]
Epoch [15/120    avg_loss:0.273, val_acc:0.899]
Epoch [16/120    avg_loss:0.193, val_acc:0.934]
Epoch [17/120    avg_loss:0.177, val_acc:0.896]
Epoch [18/120    avg_loss:0.160, val_acc:0.889]
Epoch [19/120    avg_loss:0.199, val_acc:0.939]
Epoch [20/120    avg_loss:0.156, val_acc:0.926]
Epoch [21/120    avg_loss:0.112, val_acc:0.960]
Epoch [22/120    avg_loss:0.098, val_acc:0.959]
Epoch [23/120    avg_loss:0.083, val_acc:0.966]
Epoch [24/120    avg_loss:0.097, val_acc:0.961]
Epoch [25/120    avg_loss:0.118, val_acc:0.951]
Epoch [26/120    avg_loss:0.120, val_acc:0.946]
Epoch [27/120    avg_loss:0.101, val_acc:0.957]
Epoch [28/120    avg_loss:0.087, val_acc:0.954]
Epoch [29/120    avg_loss:0.075, val_acc:0.962]
Epoch [30/120    avg_loss:0.090, val_acc:0.923]
Epoch [31/120    avg_loss:0.069, val_acc:0.966]
Epoch [32/120    avg_loss:0.088, val_acc:0.946]
Epoch [33/120    avg_loss:0.080, val_acc:0.966]
Epoch [34/120    avg_loss:0.092, val_acc:0.883]
Epoch [35/120    avg_loss:0.114, val_acc:0.926]
Epoch [36/120    avg_loss:0.146, val_acc:0.947]
Epoch [37/120    avg_loss:0.095, val_acc:0.907]
Epoch [38/120    avg_loss:0.070, val_acc:0.971]
Epoch [39/120    avg_loss:0.057, val_acc:0.949]
Epoch [40/120    avg_loss:0.048, val_acc:0.926]
Epoch [41/120    avg_loss:0.071, val_acc:0.926]
Epoch [42/120    avg_loss:0.066, val_acc:0.956]
Epoch [43/120    avg_loss:0.071, val_acc:0.957]
Epoch [44/120    avg_loss:0.086, val_acc:0.972]
Epoch [45/120    avg_loss:0.036, val_acc:0.976]
Epoch [46/120    avg_loss:0.032, val_acc:0.985]
Epoch [47/120    avg_loss:0.034, val_acc:0.976]
Epoch [48/120    avg_loss:0.027, val_acc:0.977]
Epoch [49/120    avg_loss:0.021, val_acc:0.969]
Epoch [50/120    avg_loss:0.015, val_acc:0.982]
Epoch [51/120    avg_loss:0.023, val_acc:0.983]
Epoch [52/120    avg_loss:0.030, val_acc:0.968]
Epoch [53/120    avg_loss:0.084, val_acc:0.955]
Epoch [54/120    avg_loss:0.074, val_acc:0.962]
Epoch [55/120    avg_loss:0.033, val_acc:0.983]
Epoch [56/120    avg_loss:0.032, val_acc:0.983]
Epoch [57/120    avg_loss:0.020, val_acc:0.982]
Epoch [58/120    avg_loss:0.027, val_acc:0.926]
Epoch [59/120    avg_loss:0.041, val_acc:0.947]
Epoch [60/120    avg_loss:0.026, val_acc:0.974]
Epoch [61/120    avg_loss:0.018, val_acc:0.983]
Epoch [62/120    avg_loss:0.017, val_acc:0.980]
Epoch [63/120    avg_loss:0.013, val_acc:0.985]
Epoch [64/120    avg_loss:0.016, val_acc:0.980]
Epoch [65/120    avg_loss:0.013, val_acc:0.980]
Epoch [66/120    avg_loss:0.012, val_acc:0.985]
Epoch [67/120    avg_loss:0.013, val_acc:0.985]
Epoch [68/120    avg_loss:0.012, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.982]
Epoch [71/120    avg_loss:0.011, val_acc:0.986]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.986]
Epoch [75/120    avg_loss:0.017, val_acc:0.983]
Epoch [76/120    avg_loss:0.014, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.983]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.985]
Epoch [84/120    avg_loss:0.012, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.987]
Epoch [88/120    avg_loss:0.015, val_acc:0.986]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.985]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.985]
Epoch [94/120    avg_loss:0.013, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6412     0     0     0     0     1     0    19     0]
 [    0     0 18015     0    67     0     7     0     0     1]
 [    0     8     0  2001     2     0     0     0    22     3]
 [    0    45    19     0  2870     0     7     0    31     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4876     0     0     1]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0     6    54     0     0     0  3499     9]
 [    0     0     0     2    17    65     0     1     0   834]]

Accuracy:
99.05767237847348

F1 scores:
[       nan 0.99410853 0.99739785 0.98912506 0.9595453  0.97570093
 0.9982598  0.99961255 0.97983758 0.94397284]

Kappa:
0.9875197103799407
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f852af7f8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.913, val_acc:0.271]
Epoch [2/120    avg_loss:1.398, val_acc:0.350]
Epoch [3/120    avg_loss:1.134, val_acc:0.485]
Epoch [4/120    avg_loss:0.929, val_acc:0.627]
Epoch [5/120    avg_loss:0.731, val_acc:0.699]
Epoch [6/120    avg_loss:0.575, val_acc:0.719]
Epoch [7/120    avg_loss:0.470, val_acc:0.831]
Epoch [8/120    avg_loss:0.444, val_acc:0.875]
Epoch [9/120    avg_loss:0.436, val_acc:0.807]
Epoch [10/120    avg_loss:0.340, val_acc:0.824]
Epoch [11/120    avg_loss:0.254, val_acc:0.873]
Epoch [12/120    avg_loss:0.263, val_acc:0.930]
Epoch [13/120    avg_loss:0.218, val_acc:0.884]
Epoch [14/120    avg_loss:0.226, val_acc:0.897]
Epoch [15/120    avg_loss:0.195, val_acc:0.934]
Epoch [16/120    avg_loss:0.211, val_acc:0.917]
Epoch [17/120    avg_loss:0.199, val_acc:0.880]
Epoch [18/120    avg_loss:0.198, val_acc:0.907]
Epoch [19/120    avg_loss:0.171, val_acc:0.943]
Epoch [20/120    avg_loss:0.135, val_acc:0.952]
Epoch [21/120    avg_loss:0.126, val_acc:0.932]
Epoch [22/120    avg_loss:0.090, val_acc:0.956]
Epoch [23/120    avg_loss:0.150, val_acc:0.956]
Epoch [24/120    avg_loss:0.121, val_acc:0.900]
Epoch [25/120    avg_loss:0.097, val_acc:0.948]
Epoch [26/120    avg_loss:0.089, val_acc:0.968]
Epoch [27/120    avg_loss:0.093, val_acc:0.964]
Epoch [28/120    avg_loss:0.121, val_acc:0.944]
Epoch [29/120    avg_loss:0.116, val_acc:0.945]
Epoch [30/120    avg_loss:0.100, val_acc:0.958]
Epoch [31/120    avg_loss:0.071, val_acc:0.951]
Epoch [32/120    avg_loss:0.082, val_acc:0.961]
Epoch [33/120    avg_loss:0.077, val_acc:0.973]
Epoch [34/120    avg_loss:0.051, val_acc:0.937]
Epoch [35/120    avg_loss:0.091, val_acc:0.973]
Epoch [36/120    avg_loss:0.055, val_acc:0.952]
Epoch [37/120    avg_loss:0.056, val_acc:0.947]
Epoch [38/120    avg_loss:0.053, val_acc:0.973]
Epoch [39/120    avg_loss:0.032, val_acc:0.967]
Epoch [40/120    avg_loss:0.047, val_acc:0.980]
Epoch [41/120    avg_loss:0.067, val_acc:0.958]
Epoch [42/120    avg_loss:0.054, val_acc:0.955]
Epoch [43/120    avg_loss:0.037, val_acc:0.967]
Epoch [44/120    avg_loss:0.033, val_acc:0.976]
Epoch [45/120    avg_loss:0.024, val_acc:0.974]
Epoch [46/120    avg_loss:0.032, val_acc:0.980]
Epoch [47/120    avg_loss:0.022, val_acc:0.980]
Epoch [48/120    avg_loss:0.034, val_acc:0.962]
Epoch [49/120    avg_loss:0.025, val_acc:0.980]
Epoch [50/120    avg_loss:0.031, val_acc:0.969]
Epoch [51/120    avg_loss:0.053, val_acc:0.965]
Epoch [52/120    avg_loss:0.064, val_acc:0.961]
Epoch [53/120    avg_loss:0.047, val_acc:0.972]
Epoch [54/120    avg_loss:0.026, val_acc:0.984]
Epoch [55/120    avg_loss:0.020, val_acc:0.980]
Epoch [56/120    avg_loss:0.022, val_acc:0.986]
Epoch [57/120    avg_loss:0.020, val_acc:0.972]
Epoch [58/120    avg_loss:0.017, val_acc:0.972]
Epoch [59/120    avg_loss:0.029, val_acc:0.985]
Epoch [60/120    avg_loss:0.033, val_acc:0.975]
Epoch [61/120    avg_loss:0.021, val_acc:0.979]
Epoch [62/120    avg_loss:0.025, val_acc:0.949]
Epoch [63/120    avg_loss:0.068, val_acc:0.945]
Epoch [64/120    avg_loss:0.022, val_acc:0.985]
Epoch [65/120    avg_loss:0.033, val_acc:0.974]
Epoch [66/120    avg_loss:0.025, val_acc:0.975]
Epoch [67/120    avg_loss:0.032, val_acc:0.913]
Epoch [68/120    avg_loss:0.024, val_acc:0.980]
Epoch [69/120    avg_loss:0.024, val_acc:0.984]
Epoch [70/120    avg_loss:0.013, val_acc:0.982]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.013, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.986]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.987]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.990]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.987]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     0     0     5     0    22     3]
 [    0     0 18073     0    14     0     2     0     1     0]
 [    0     0     0  1997     1     0     0     0    37     1]
 [    0    29    17     4  2888     0     1     3    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4867     0     0    11]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     2     0     0    79     0     0     0  3482     8]
 [    0     0     0     0    14    75     0     0     0   830]]

Accuracy:
99.12515364037307

F1 scores:
[       nan 0.99525845 0.99906025 0.98934853 0.96782842 0.97206704
 0.99784726 0.99728577 0.9749405  0.93573844]

Kappa:
0.988407373979423
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f606d2c59b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.920, val_acc:0.241]
Epoch [2/120    avg_loss:1.452, val_acc:0.424]
Epoch [3/120    avg_loss:1.150, val_acc:0.595]
Epoch [4/120    avg_loss:0.940, val_acc:0.654]
Epoch [5/120    avg_loss:0.737, val_acc:0.677]
Epoch [6/120    avg_loss:0.605, val_acc:0.756]
Epoch [7/120    avg_loss:0.485, val_acc:0.792]
Epoch [8/120    avg_loss:0.439, val_acc:0.756]
Epoch [9/120    avg_loss:0.441, val_acc:0.805]
Epoch [10/120    avg_loss:0.338, val_acc:0.860]
Epoch [11/120    avg_loss:0.384, val_acc:0.809]
Epoch [12/120    avg_loss:0.325, val_acc:0.891]
Epoch [13/120    avg_loss:0.284, val_acc:0.817]
Epoch [14/120    avg_loss:0.262, val_acc:0.875]
Epoch [15/120    avg_loss:0.246, val_acc:0.887]
Epoch [16/120    avg_loss:0.167, val_acc:0.935]
Epoch [17/120    avg_loss:0.179, val_acc:0.869]
Epoch [18/120    avg_loss:0.165, val_acc:0.912]
Epoch [19/120    avg_loss:0.154, val_acc:0.938]
Epoch [20/120    avg_loss:0.117, val_acc:0.948]
Epoch [21/120    avg_loss:0.159, val_acc:0.948]
Epoch [22/120    avg_loss:0.115, val_acc:0.924]
Epoch [23/120    avg_loss:0.142, val_acc:0.953]
Epoch [24/120    avg_loss:0.151, val_acc:0.839]
Epoch [25/120    avg_loss:0.115, val_acc:0.943]
Epoch [26/120    avg_loss:0.089, val_acc:0.967]
Epoch [27/120    avg_loss:0.090, val_acc:0.965]
Epoch [28/120    avg_loss:0.061, val_acc:0.977]
Epoch [29/120    avg_loss:0.058, val_acc:0.973]
Epoch [30/120    avg_loss:0.081, val_acc:0.873]
Epoch [31/120    avg_loss:0.080, val_acc:0.977]
Epoch [32/120    avg_loss:0.087, val_acc:0.957]
Epoch [33/120    avg_loss:0.044, val_acc:0.970]
Epoch [34/120    avg_loss:0.046, val_acc:0.972]
Epoch [35/120    avg_loss:0.052, val_acc:0.965]
Epoch [36/120    avg_loss:0.051, val_acc:0.989]
Epoch [37/120    avg_loss:0.047, val_acc:0.977]
Epoch [38/120    avg_loss:0.061, val_acc:0.894]
Epoch [39/120    avg_loss:0.104, val_acc:0.970]
Epoch [40/120    avg_loss:0.041, val_acc:0.970]
Epoch [41/120    avg_loss:0.055, val_acc:0.974]
Epoch [42/120    avg_loss:0.079, val_acc:0.969]
Epoch [43/120    avg_loss:0.031, val_acc:0.977]
Epoch [44/120    avg_loss:0.030, val_acc:0.979]
Epoch [45/120    avg_loss:0.028, val_acc:0.984]
Epoch [46/120    avg_loss:0.029, val_acc:0.980]
Epoch [47/120    avg_loss:0.026, val_acc:0.990]
Epoch [48/120    avg_loss:0.035, val_acc:0.969]
Epoch [49/120    avg_loss:0.026, val_acc:0.983]
Epoch [50/120    avg_loss:0.032, val_acc:0.981]
Epoch [51/120    avg_loss:0.037, val_acc:0.988]
Epoch [52/120    avg_loss:0.016, val_acc:0.987]
Epoch [53/120    avg_loss:0.014, val_acc:0.987]
Epoch [54/120    avg_loss:0.014, val_acc:0.988]
Epoch [55/120    avg_loss:0.014, val_acc:0.984]
Epoch [56/120    avg_loss:0.023, val_acc:0.988]
Epoch [57/120    avg_loss:0.061, val_acc:0.978]
Epoch [58/120    avg_loss:0.045, val_acc:0.983]
Epoch [59/120    avg_loss:0.027, val_acc:0.985]
Epoch [60/120    avg_loss:0.026, val_acc:0.981]
Epoch [61/120    avg_loss:0.017, val_acc:0.986]
Epoch [62/120    avg_loss:0.016, val_acc:0.990]
Epoch [63/120    avg_loss:0.017, val_acc:0.989]
Epoch [64/120    avg_loss:0.019, val_acc:0.990]
Epoch [65/120    avg_loss:0.015, val_acc:0.990]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.014, val_acc:0.990]
Epoch [68/120    avg_loss:0.012, val_acc:0.991]
Epoch [69/120    avg_loss:0.010, val_acc:0.990]
Epoch [70/120    avg_loss:0.009, val_acc:0.991]
Epoch [71/120    avg_loss:0.012, val_acc:0.990]
Epoch [72/120    avg_loss:0.012, val_acc:0.990]
Epoch [73/120    avg_loss:0.010, val_acc:0.990]
Epoch [74/120    avg_loss:0.008, val_acc:0.991]
Epoch [75/120    avg_loss:0.010, val_acc:0.991]
Epoch [76/120    avg_loss:0.011, val_acc:0.990]
Epoch [77/120    avg_loss:0.010, val_acc:0.990]
Epoch [78/120    avg_loss:0.010, val_acc:0.990]
Epoch [79/120    avg_loss:0.012, val_acc:0.991]
Epoch [80/120    avg_loss:0.011, val_acc:0.990]
Epoch [81/120    avg_loss:0.010, val_acc:0.990]
Epoch [82/120    avg_loss:0.013, val_acc:0.990]
Epoch [83/120    avg_loss:0.009, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.009, val_acc:0.990]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.012, val_acc:0.990]
Epoch [90/120    avg_loss:0.010, val_acc:0.992]
Epoch [91/120    avg_loss:0.009, val_acc:0.991]
Epoch [92/120    avg_loss:0.008, val_acc:0.991]
Epoch [93/120    avg_loss:0.009, val_acc:0.991]
Epoch [94/120    avg_loss:0.009, val_acc:0.991]
Epoch [95/120    avg_loss:0.010, val_acc:0.992]
Epoch [96/120    avg_loss:0.010, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.991]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.008, val_acc:0.990]
Epoch [101/120    avg_loss:0.009, val_acc:0.991]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.009, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.009, val_acc:0.991]
Epoch [107/120    avg_loss:0.008, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.993]
Epoch [112/120    avg_loss:0.008, val_acc:0.993]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.006, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.008, val_acc:0.992]
Epoch [117/120    avg_loss:0.010, val_acc:0.991]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6398     0     0     2     0    24     1     7     0]
 [    0     0 17907     0   117     0    65     0     0     1]
 [    0     0     0  2027     2     0     0     0     3     4]
 [    0    32    18     2  2893     0     0     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     9     0     0  4861     0     2     4]
 [    0     0     0     0     0     0     2  1283     0     5]
 [    0    18     0     3    78     0     0     0  3472     0]
 [    0     0     0     0    17    70     0     0     0   832]]

Accuracy:
98.75882679006098

F1 scores:
[       nan 0.99347826 0.99436377 0.9943586  0.95148824 0.9738806
 0.98901322 0.996892   0.98051398 0.9427762 ]

Kappa:
0.9835823272578116
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3a194f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.899, val_acc:0.301]
Epoch [2/120    avg_loss:1.408, val_acc:0.406]
Epoch [3/120    avg_loss:1.104, val_acc:0.513]
Epoch [4/120    avg_loss:0.870, val_acc:0.644]
Epoch [5/120    avg_loss:0.681, val_acc:0.694]
Epoch [6/120    avg_loss:0.643, val_acc:0.625]
Epoch [7/120    avg_loss:0.573, val_acc:0.788]
Epoch [8/120    avg_loss:0.527, val_acc:0.789]
Epoch [9/120    avg_loss:0.460, val_acc:0.854]
Epoch [10/120    avg_loss:0.428, val_acc:0.847]
Epoch [11/120    avg_loss:0.369, val_acc:0.868]
Epoch [12/120    avg_loss:0.289, val_acc:0.891]
Epoch [13/120    avg_loss:0.234, val_acc:0.923]
Epoch [14/120    avg_loss:0.220, val_acc:0.926]
Epoch [15/120    avg_loss:0.243, val_acc:0.899]
Epoch [16/120    avg_loss:0.223, val_acc:0.932]
Epoch [17/120    avg_loss:0.239, val_acc:0.932]
Epoch [18/120    avg_loss:0.199, val_acc:0.934]
Epoch [19/120    avg_loss:0.168, val_acc:0.946]
Epoch [20/120    avg_loss:0.173, val_acc:0.939]
Epoch [21/120    avg_loss:0.182, val_acc:0.932]
Epoch [22/120    avg_loss:0.144, val_acc:0.932]
Epoch [23/120    avg_loss:0.158, val_acc:0.960]
Epoch [24/120    avg_loss:0.096, val_acc:0.965]
Epoch [25/120    avg_loss:0.122, val_acc:0.952]
Epoch [26/120    avg_loss:0.094, val_acc:0.971]
Epoch [27/120    avg_loss:0.086, val_acc:0.955]
Epoch [28/120    avg_loss:0.083, val_acc:0.961]
Epoch [29/120    avg_loss:0.077, val_acc:0.960]
Epoch [30/120    avg_loss:0.060, val_acc:0.974]
Epoch [31/120    avg_loss:0.069, val_acc:0.981]
Epoch [32/120    avg_loss:0.075, val_acc:0.963]
Epoch [33/120    avg_loss:0.056, val_acc:0.973]
Epoch [34/120    avg_loss:0.073, val_acc:0.929]
Epoch [35/120    avg_loss:0.076, val_acc:0.973]
Epoch [36/120    avg_loss:0.048, val_acc:0.975]
Epoch [37/120    avg_loss:0.055, val_acc:0.953]
Epoch [38/120    avg_loss:0.043, val_acc:0.976]
Epoch [39/120    avg_loss:0.040, val_acc:0.985]
Epoch [40/120    avg_loss:0.039, val_acc:0.968]
Epoch [41/120    avg_loss:0.035, val_acc:0.983]
Epoch [42/120    avg_loss:0.034, val_acc:0.967]
Epoch [43/120    avg_loss:0.039, val_acc:0.980]
Epoch [44/120    avg_loss:0.038, val_acc:0.976]
Epoch [45/120    avg_loss:0.051, val_acc:0.979]
Epoch [46/120    avg_loss:0.059, val_acc:0.984]
Epoch [47/120    avg_loss:0.058, val_acc:0.979]
Epoch [48/120    avg_loss:0.032, val_acc:0.986]
Epoch [49/120    avg_loss:0.026, val_acc:0.980]
Epoch [50/120    avg_loss:0.024, val_acc:0.977]
Epoch [51/120    avg_loss:0.029, val_acc:0.978]
Epoch [52/120    avg_loss:0.028, val_acc:0.979]
Epoch [53/120    avg_loss:0.024, val_acc:0.980]
Epoch [54/120    avg_loss:0.040, val_acc:0.984]
Epoch [55/120    avg_loss:0.086, val_acc:0.964]
Epoch [56/120    avg_loss:0.063, val_acc:0.956]
Epoch [57/120    avg_loss:0.137, val_acc:0.986]
Epoch [58/120    avg_loss:0.062, val_acc:0.955]
Epoch [59/120    avg_loss:0.046, val_acc:0.988]
Epoch [60/120    avg_loss:0.020, val_acc:0.986]
Epoch [61/120    avg_loss:0.039, val_acc:0.986]
Epoch [62/120    avg_loss:0.020, val_acc:0.975]
Epoch [63/120    avg_loss:0.024, val_acc:0.987]
Epoch [64/120    avg_loss:0.019, val_acc:0.984]
Epoch [65/120    avg_loss:0.025, val_acc:0.986]
Epoch [66/120    avg_loss:0.020, val_acc:0.973]
Epoch [67/120    avg_loss:0.024, val_acc:0.972]
Epoch [68/120    avg_loss:0.017, val_acc:0.980]
Epoch [69/120    avg_loss:0.066, val_acc:0.971]
Epoch [70/120    avg_loss:0.062, val_acc:0.974]
Epoch [71/120    avg_loss:0.036, val_acc:0.988]
Epoch [72/120    avg_loss:0.041, val_acc:0.976]
Epoch [73/120    avg_loss:0.078, val_acc:0.961]
Epoch [74/120    avg_loss:0.034, val_acc:0.985]
Epoch [75/120    avg_loss:0.020, val_acc:0.985]
Epoch [76/120    avg_loss:0.030, val_acc:0.980]
Epoch [77/120    avg_loss:0.032, val_acc:0.980]
Epoch [78/120    avg_loss:0.032, val_acc:0.988]
Epoch [79/120    avg_loss:0.016, val_acc:0.986]
Epoch [80/120    avg_loss:0.022, val_acc:0.983]
Epoch [81/120    avg_loss:0.019, val_acc:0.991]
Epoch [82/120    avg_loss:0.012, val_acc:0.989]
Epoch [83/120    avg_loss:0.012, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.992]
Epoch [85/120    avg_loss:0.011, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.991]
Epoch [88/120    avg_loss:0.008, val_acc:0.991]
Epoch [89/120    avg_loss:0.026, val_acc:0.979]
Epoch [90/120    avg_loss:0.016, val_acc:0.987]
Epoch [91/120    avg_loss:0.020, val_acc:0.971]
Epoch [92/120    avg_loss:0.018, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.021, val_acc:0.986]
Epoch [96/120    avg_loss:0.014, val_acc:0.991]
Epoch [97/120    avg_loss:0.010, val_acc:0.991]
Epoch [98/120    avg_loss:0.008, val_acc:0.993]
Epoch [99/120    avg_loss:0.015, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.994]
Epoch [101/120    avg_loss:0.005, val_acc:0.995]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.029, val_acc:0.982]
Epoch [107/120    avg_loss:0.030, val_acc:0.987]
Epoch [108/120    avg_loss:0.018, val_acc:0.989]
Epoch [109/120    avg_loss:0.016, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.991]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.995]
Epoch [116/120    avg_loss:0.006, val_acc:0.996]
Epoch [117/120    avg_loss:0.005, val_acc:0.996]
Epoch [118/120    avg_loss:0.005, val_acc:0.996]
Epoch [119/120    avg_loss:0.003, val_acc:0.996]
Epoch [120/120    avg_loss:0.007, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6351     0     0     5     0    29     0    47     0]
 [    0     8 18054     0    22     0     6     0     0     0]
 [    0     3     0  2000     3     0     0     0    27     3]
 [    0    55    16     0  2872     0     0     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4854     0    10    14]
 [    0     0     0     0     0     0     1  1289     0     0]
 [    0     6     0     0    51     0     0     0  3504    10]
 [    0     0     0     1    14    77     0     0     0   827]]

Accuracy:
98.94681030535271

F1 scores:
[       nan 0.98809802 0.99856195 0.99083478 0.96716619 0.97134351
 0.99385749 0.99961225 0.97495826 0.93288212]

Kappa:
0.9860472157138941
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f98dc85e908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.911, val_acc:0.258]
Epoch [2/120    avg_loss:1.446, val_acc:0.323]
Epoch [3/120    avg_loss:1.194, val_acc:0.464]
Epoch [4/120    avg_loss:0.953, val_acc:0.706]
Epoch [5/120    avg_loss:0.746, val_acc:0.779]
Epoch [6/120    avg_loss:0.619, val_acc:0.727]
Epoch [7/120    avg_loss:0.532, val_acc:0.801]
Epoch [8/120    avg_loss:0.444, val_acc:0.807]
Epoch [9/120    avg_loss:0.407, val_acc:0.798]
Epoch [10/120    avg_loss:0.394, val_acc:0.800]
Epoch [11/120    avg_loss:0.326, val_acc:0.884]
Epoch [12/120    avg_loss:0.286, val_acc:0.873]
Epoch [13/120    avg_loss:0.425, val_acc:0.729]
Epoch [14/120    avg_loss:0.342, val_acc:0.858]
Epoch [15/120    avg_loss:0.297, val_acc:0.826]
Epoch [16/120    avg_loss:0.236, val_acc:0.920]
Epoch [17/120    avg_loss:0.217, val_acc:0.931]
Epoch [18/120    avg_loss:0.165, val_acc:0.937]
Epoch [19/120    avg_loss:0.179, val_acc:0.924]
Epoch [20/120    avg_loss:0.202, val_acc:0.926]
Epoch [21/120    avg_loss:0.154, val_acc:0.950]
Epoch [22/120    avg_loss:0.121, val_acc:0.935]
Epoch [23/120    avg_loss:0.099, val_acc:0.943]
Epoch [24/120    avg_loss:0.118, val_acc:0.924]
Epoch [25/120    avg_loss:0.097, val_acc:0.950]
Epoch [26/120    avg_loss:0.084, val_acc:0.958]
Epoch [27/120    avg_loss:0.089, val_acc:0.964]
Epoch [28/120    avg_loss:0.087, val_acc:0.964]
Epoch [29/120    avg_loss:0.113, val_acc:0.904]
Epoch [30/120    avg_loss:0.107, val_acc:0.944]
Epoch [31/120    avg_loss:0.068, val_acc:0.951]
Epoch [32/120    avg_loss:0.117, val_acc:0.936]
Epoch [33/120    avg_loss:0.066, val_acc:0.966]
Epoch [34/120    avg_loss:0.053, val_acc:0.960]
Epoch [35/120    avg_loss:0.063, val_acc:0.959]
Epoch [36/120    avg_loss:0.050, val_acc:0.976]
Epoch [37/120    avg_loss:0.048, val_acc:0.954]
Epoch [38/120    avg_loss:0.057, val_acc:0.960]
Epoch [39/120    avg_loss:0.287, val_acc:0.957]
Epoch [40/120    avg_loss:0.080, val_acc:0.960]
Epoch [41/120    avg_loss:0.069, val_acc:0.951]
Epoch [42/120    avg_loss:0.057, val_acc:0.976]
Epoch [43/120    avg_loss:0.045, val_acc:0.924]
Epoch [44/120    avg_loss:0.619, val_acc:0.799]
Epoch [45/120    avg_loss:0.341, val_acc:0.832]
Epoch [46/120    avg_loss:0.258, val_acc:0.943]
Epoch [47/120    avg_loss:0.166, val_acc:0.908]
Epoch [48/120    avg_loss:0.166, val_acc:0.944]
Epoch [49/120    avg_loss:0.134, val_acc:0.938]
Epoch [50/120    avg_loss:0.096, val_acc:0.946]
Epoch [51/120    avg_loss:0.097, val_acc:0.955]
Epoch [52/120    avg_loss:0.090, val_acc:0.963]
Epoch [53/120    avg_loss:0.091, val_acc:0.965]
Epoch [54/120    avg_loss:0.059, val_acc:0.960]
Epoch [55/120    avg_loss:0.049, val_acc:0.974]
Epoch [56/120    avg_loss:0.030, val_acc:0.977]
Epoch [57/120    avg_loss:0.032, val_acc:0.977]
Epoch [58/120    avg_loss:0.025, val_acc:0.977]
Epoch [59/120    avg_loss:0.026, val_acc:0.977]
Epoch [60/120    avg_loss:0.032, val_acc:0.977]
Epoch [61/120    avg_loss:0.024, val_acc:0.978]
Epoch [62/120    avg_loss:0.020, val_acc:0.977]
Epoch [63/120    avg_loss:0.023, val_acc:0.978]
Epoch [64/120    avg_loss:0.023, val_acc:0.979]
Epoch [65/120    avg_loss:0.023, val_acc:0.979]
Epoch [66/120    avg_loss:0.025, val_acc:0.978]
Epoch [67/120    avg_loss:0.020, val_acc:0.979]
Epoch [68/120    avg_loss:0.020, val_acc:0.978]
Epoch [69/120    avg_loss:0.025, val_acc:0.979]
Epoch [70/120    avg_loss:0.024, val_acc:0.978]
Epoch [71/120    avg_loss:0.019, val_acc:0.980]
Epoch [72/120    avg_loss:0.025, val_acc:0.979]
Epoch [73/120    avg_loss:0.019, val_acc:0.978]
Epoch [74/120    avg_loss:0.022, val_acc:0.978]
Epoch [75/120    avg_loss:0.023, val_acc:0.978]
Epoch [76/120    avg_loss:0.019, val_acc:0.979]
Epoch [77/120    avg_loss:0.020, val_acc:0.978]
Epoch [78/120    avg_loss:0.020, val_acc:0.979]
Epoch [79/120    avg_loss:0.018, val_acc:0.980]
Epoch [80/120    avg_loss:0.019, val_acc:0.977]
Epoch [81/120    avg_loss:0.018, val_acc:0.978]
Epoch [82/120    avg_loss:0.026, val_acc:0.977]
Epoch [83/120    avg_loss:0.019, val_acc:0.977]
Epoch [84/120    avg_loss:0.019, val_acc:0.977]
Epoch [85/120    avg_loss:0.020, val_acc:0.977]
Epoch [86/120    avg_loss:0.020, val_acc:0.978]
Epoch [87/120    avg_loss:0.019, val_acc:0.979]
Epoch [88/120    avg_loss:0.020, val_acc:0.980]
Epoch [89/120    avg_loss:0.023, val_acc:0.979]
Epoch [90/120    avg_loss:0.018, val_acc:0.978]
Epoch [91/120    avg_loss:0.020, val_acc:0.979]
Epoch [92/120    avg_loss:0.020, val_acc:0.978]
Epoch [93/120    avg_loss:0.020, val_acc:0.979]
Epoch [94/120    avg_loss:0.028, val_acc:0.978]
Epoch [95/120    avg_loss:0.020, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.977]
Epoch [97/120    avg_loss:0.022, val_acc:0.976]
Epoch [98/120    avg_loss:0.015, val_acc:0.978]
Epoch [99/120    avg_loss:0.017, val_acc:0.978]
Epoch [100/120    avg_loss:0.019, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.017, val_acc:0.979]
Epoch [104/120    avg_loss:0.019, val_acc:0.979]
Epoch [105/120    avg_loss:0.016, val_acc:0.979]
Epoch [106/120    avg_loss:0.016, val_acc:0.979]
Epoch [107/120    avg_loss:0.018, val_acc:0.979]
Epoch [108/120    avg_loss:0.021, val_acc:0.979]
Epoch [109/120    avg_loss:0.019, val_acc:0.979]
Epoch [110/120    avg_loss:0.016, val_acc:0.980]
Epoch [111/120    avg_loss:0.017, val_acc:0.978]
Epoch [112/120    avg_loss:0.020, val_acc:0.978]
Epoch [113/120    avg_loss:0.017, val_acc:0.978]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.014, val_acc:0.978]
Epoch [116/120    avg_loss:0.015, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.979]
Epoch [118/120    avg_loss:0.015, val_acc:0.979]
Epoch [119/120    avg_loss:0.018, val_acc:0.980]
Epoch [120/120    avg_loss:0.017, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6321     0     0    10     0     0    23    73     5]
 [    0     4 17978     0    97     0     5     0     6     0]
 [    0    19     0  1997     3     0     0     0    14     3]
 [    0    45    19     0  2874     0     3     0    28     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4875     0     0     3]
 [    0     2     0     0     0     0     0  1287     0     1]
 [    0     9     0    16    62     0     0     0  3465    19]
 [    0     0     0     0    15    78     0     0     0   826]]

Accuracy:
98.63832453666883

F1 scores:
[       nan 0.98519327 0.99636988 0.9864164  0.95275982 0.97098214
 0.99887307 0.99       0.9682828  0.92861158]

Kappa:
0.9819780493177346
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f300ff238d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.919, val_acc:0.408]
Epoch [2/120    avg_loss:1.424, val_acc:0.641]
Epoch [3/120    avg_loss:1.158, val_acc:0.658]
Epoch [4/120    avg_loss:0.874, val_acc:0.731]
Epoch [5/120    avg_loss:0.673, val_acc:0.786]
Epoch [6/120    avg_loss:0.555, val_acc:0.727]
Epoch [7/120    avg_loss:0.519, val_acc:0.806]
Epoch [8/120    avg_loss:0.428, val_acc:0.844]
Epoch [9/120    avg_loss:0.374, val_acc:0.884]
Epoch [10/120    avg_loss:0.307, val_acc:0.842]
Epoch [11/120    avg_loss:0.284, val_acc:0.874]
Epoch [12/120    avg_loss:0.241, val_acc:0.916]
Epoch [13/120    avg_loss:0.203, val_acc:0.908]
Epoch [14/120    avg_loss:0.225, val_acc:0.923]
Epoch [15/120    avg_loss:0.179, val_acc:0.930]
Epoch [16/120    avg_loss:0.154, val_acc:0.909]
Epoch [17/120    avg_loss:0.201, val_acc:0.908]
Epoch [18/120    avg_loss:0.140, val_acc:0.927]
Epoch [19/120    avg_loss:0.125, val_acc:0.941]
Epoch [20/120    avg_loss:0.110, val_acc:0.952]
Epoch [21/120    avg_loss:0.120, val_acc:0.906]
Epoch [22/120    avg_loss:0.092, val_acc:0.938]
Epoch [23/120    avg_loss:0.105, val_acc:0.959]
Epoch [24/120    avg_loss:0.088, val_acc:0.907]
Epoch [25/120    avg_loss:0.114, val_acc:0.951]
Epoch [26/120    avg_loss:0.115, val_acc:0.957]
Epoch [27/120    avg_loss:0.107, val_acc:0.950]
Epoch [28/120    avg_loss:0.109, val_acc:0.888]
Epoch [29/120    avg_loss:0.197, val_acc:0.930]
Epoch [30/120    avg_loss:0.139, val_acc:0.954]
Epoch [31/120    avg_loss:0.105, val_acc:0.965]
Epoch [32/120    avg_loss:0.067, val_acc:0.961]
Epoch [33/120    avg_loss:0.073, val_acc:0.967]
Epoch [34/120    avg_loss:0.060, val_acc:0.937]
Epoch [35/120    avg_loss:0.090, val_acc:0.973]
Epoch [36/120    avg_loss:0.041, val_acc:0.966]
Epoch [37/120    avg_loss:0.051, val_acc:0.927]
Epoch [38/120    avg_loss:0.056, val_acc:0.977]
Epoch [39/120    avg_loss:0.029, val_acc:0.979]
Epoch [40/120    avg_loss:0.046, val_acc:0.984]
Epoch [41/120    avg_loss:0.051, val_acc:0.978]
Epoch [42/120    avg_loss:0.087, val_acc:0.963]
Epoch [43/120    avg_loss:0.059, val_acc:0.969]
Epoch [44/120    avg_loss:0.031, val_acc:0.977]
Epoch [45/120    avg_loss:0.028, val_acc:0.981]
Epoch [46/120    avg_loss:0.031, val_acc:0.979]
Epoch [47/120    avg_loss:0.034, val_acc:0.935]
Epoch [48/120    avg_loss:0.081, val_acc:0.969]
Epoch [49/120    avg_loss:0.046, val_acc:0.977]
Epoch [50/120    avg_loss:0.031, val_acc:0.974]
Epoch [51/120    avg_loss:0.021, val_acc:0.971]
Epoch [52/120    avg_loss:0.026, val_acc:0.978]
Epoch [53/120    avg_loss:0.027, val_acc:0.974]
Epoch [54/120    avg_loss:0.020, val_acc:0.982]
Epoch [55/120    avg_loss:0.016, val_acc:0.983]
Epoch [56/120    avg_loss:0.016, val_acc:0.982]
Epoch [57/120    avg_loss:0.013, val_acc:0.984]
Epoch [58/120    avg_loss:0.011, val_acc:0.985]
Epoch [59/120    avg_loss:0.019, val_acc:0.984]
Epoch [60/120    avg_loss:0.012, val_acc:0.985]
Epoch [61/120    avg_loss:0.013, val_acc:0.986]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.012, val_acc:0.988]
Epoch [64/120    avg_loss:0.012, val_acc:0.985]
Epoch [65/120    avg_loss:0.011, val_acc:0.986]
Epoch [66/120    avg_loss:0.015, val_acc:0.985]
Epoch [67/120    avg_loss:0.015, val_acc:0.985]
Epoch [68/120    avg_loss:0.012, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.014, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.984]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.015, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.012, val_acc:0.986]
Epoch [91/120    avg_loss:0.013, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.986]
Epoch [93/120    avg_loss:0.016, val_acc:0.986]
Epoch [94/120    avg_loss:0.014, val_acc:0.986]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.013, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.986]
Epoch [104/120    avg_loss:0.011, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.986]
Epoch [108/120    avg_loss:0.012, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.986]
Epoch [110/120    avg_loss:0.013, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.986]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.012, val_acc:0.986]
Epoch [120/120    avg_loss:0.013, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6378     0     0     2     0    25     0    27     0]
 [    0     0 17950     0    78     0    62     0     0     0]
 [    0     0     0  2032     2     0     0     0     1     1]
 [    0    37    18     0  2879     0     9     0    28     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4857     0    10    11]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     1     0     1    65     0     0     0  3480    24]
 [    0     0     0     0    18    49     0     0     0   852]]

Accuracy:
98.86004868291037

F1 scores:
[       nan 0.99283935 0.99561817 0.9987712  0.95711436 0.98157202
 0.98789789 0.99883586 0.97794014 0.94195688]

Kappa:
0.9849156475895269
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f305414f8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.928, val_acc:0.275]
Epoch [2/120    avg_loss:1.405, val_acc:0.525]
Epoch [3/120    avg_loss:1.094, val_acc:0.671]
Epoch [4/120    avg_loss:0.861, val_acc:0.701]
Epoch [5/120    avg_loss:0.655, val_acc:0.694]
Epoch [6/120    avg_loss:0.523, val_acc:0.755]
Epoch [7/120    avg_loss:0.515, val_acc:0.787]
Epoch [8/120    avg_loss:0.430, val_acc:0.798]
Epoch [9/120    avg_loss:0.406, val_acc:0.857]
Epoch [10/120    avg_loss:0.333, val_acc:0.829]
Epoch [11/120    avg_loss:0.239, val_acc:0.871]
Epoch [12/120    avg_loss:0.254, val_acc:0.878]
Epoch [13/120    avg_loss:0.233, val_acc:0.931]
Epoch [14/120    avg_loss:0.211, val_acc:0.918]
Epoch [15/120    avg_loss:0.221, val_acc:0.932]
Epoch [16/120    avg_loss:0.129, val_acc:0.903]
Epoch [17/120    avg_loss:0.158, val_acc:0.937]
Epoch [18/120    avg_loss:0.131, val_acc:0.931]
Epoch [19/120    avg_loss:0.125, val_acc:0.950]
Epoch [20/120    avg_loss:0.139, val_acc:0.924]
Epoch [21/120    avg_loss:0.130, val_acc:0.946]
Epoch [22/120    avg_loss:0.106, val_acc:0.930]
Epoch [23/120    avg_loss:0.105, val_acc:0.950]
Epoch [24/120    avg_loss:0.109, val_acc:0.953]
Epoch [25/120    avg_loss:0.091, val_acc:0.957]
Epoch [26/120    avg_loss:0.114, val_acc:0.887]
Epoch [27/120    avg_loss:0.222, val_acc:0.894]
Epoch [28/120    avg_loss:0.128, val_acc:0.960]
Epoch [29/120    avg_loss:0.073, val_acc:0.948]
Epoch [30/120    avg_loss:0.121, val_acc:0.958]
Epoch [31/120    avg_loss:0.072, val_acc:0.972]
Epoch [32/120    avg_loss:0.065, val_acc:0.970]
Epoch [33/120    avg_loss:0.050, val_acc:0.964]
Epoch [34/120    avg_loss:0.054, val_acc:0.965]
Epoch [35/120    avg_loss:0.046, val_acc:0.968]
Epoch [36/120    avg_loss:0.037, val_acc:0.972]
Epoch [37/120    avg_loss:0.051, val_acc:0.967]
Epoch [38/120    avg_loss:0.038, val_acc:0.976]
Epoch [39/120    avg_loss:0.053, val_acc:0.968]
Epoch [40/120    avg_loss:0.055, val_acc:0.966]
Epoch [41/120    avg_loss:0.189, val_acc:0.905]
Epoch [42/120    avg_loss:0.085, val_acc:0.972]
Epoch [43/120    avg_loss:0.063, val_acc:0.981]
Epoch [44/120    avg_loss:0.039, val_acc:0.966]
Epoch [45/120    avg_loss:0.029, val_acc:0.982]
Epoch [46/120    avg_loss:0.028, val_acc:0.982]
Epoch [47/120    avg_loss:0.031, val_acc:0.970]
Epoch [48/120    avg_loss:0.030, val_acc:0.985]
Epoch [49/120    avg_loss:0.024, val_acc:0.982]
Epoch [50/120    avg_loss:0.025, val_acc:0.982]
Epoch [51/120    avg_loss:0.018, val_acc:0.990]
Epoch [52/120    avg_loss:0.034, val_acc:0.980]
Epoch [53/120    avg_loss:0.026, val_acc:0.986]
Epoch [54/120    avg_loss:0.033, val_acc:0.966]
Epoch [55/120    avg_loss:0.621, val_acc:0.624]
Epoch [56/120    avg_loss:0.669, val_acc:0.755]
Epoch [57/120    avg_loss:0.524, val_acc:0.835]
Epoch [58/120    avg_loss:0.405, val_acc:0.804]
Epoch [59/120    avg_loss:0.442, val_acc:0.873]
Epoch [60/120    avg_loss:0.337, val_acc:0.880]
Epoch [61/120    avg_loss:0.264, val_acc:0.860]
Epoch [62/120    avg_loss:0.215, val_acc:0.888]
Epoch [63/120    avg_loss:0.238, val_acc:0.804]
Epoch [64/120    avg_loss:0.268, val_acc:0.911]
Epoch [65/120    avg_loss:0.179, val_acc:0.936]
Epoch [66/120    avg_loss:0.121, val_acc:0.939]
Epoch [67/120    avg_loss:0.125, val_acc:0.944]
Epoch [68/120    avg_loss:0.128, val_acc:0.946]
Epoch [69/120    avg_loss:0.121, val_acc:0.944]
Epoch [70/120    avg_loss:0.110, val_acc:0.949]
Epoch [71/120    avg_loss:0.110, val_acc:0.946]
Epoch [72/120    avg_loss:0.097, val_acc:0.947]
Epoch [73/120    avg_loss:0.104, val_acc:0.951]
Epoch [74/120    avg_loss:0.082, val_acc:0.947]
Epoch [75/120    avg_loss:0.091, val_acc:0.948]
Epoch [76/120    avg_loss:0.072, val_acc:0.944]
Epoch [77/120    avg_loss:0.096, val_acc:0.949]
Epoch [78/120    avg_loss:0.084, val_acc:0.950]
Epoch [79/120    avg_loss:0.084, val_acc:0.948]
Epoch [80/120    avg_loss:0.080, val_acc:0.948]
Epoch [81/120    avg_loss:0.070, val_acc:0.951]
Epoch [82/120    avg_loss:0.087, val_acc:0.951]
Epoch [83/120    avg_loss:0.081, val_acc:0.949]
Epoch [84/120    avg_loss:0.075, val_acc:0.950]
Epoch [85/120    avg_loss:0.076, val_acc:0.951]
Epoch [86/120    avg_loss:0.078, val_acc:0.951]
Epoch [87/120    avg_loss:0.079, val_acc:0.951]
Epoch [88/120    avg_loss:0.082, val_acc:0.951]
Epoch [89/120    avg_loss:0.087, val_acc:0.951]
Epoch [90/120    avg_loss:0.069, val_acc:0.951]
Epoch [91/120    avg_loss:0.081, val_acc:0.951]
Epoch [92/120    avg_loss:0.077, val_acc:0.951]
Epoch [93/120    avg_loss:0.073, val_acc:0.951]
Epoch [94/120    avg_loss:0.077, val_acc:0.951]
Epoch [95/120    avg_loss:0.083, val_acc:0.951]
Epoch [96/120    avg_loss:0.071, val_acc:0.951]
Epoch [97/120    avg_loss:0.078, val_acc:0.951]
Epoch [98/120    avg_loss:0.078, val_acc:0.951]
Epoch [99/120    avg_loss:0.077, val_acc:0.951]
Epoch [100/120    avg_loss:0.072, val_acc:0.951]
Epoch [101/120    avg_loss:0.080, val_acc:0.951]
Epoch [102/120    avg_loss:0.079, val_acc:0.950]
Epoch [103/120    avg_loss:0.073, val_acc:0.950]
Epoch [104/120    avg_loss:0.076, val_acc:0.950]
Epoch [105/120    avg_loss:0.078, val_acc:0.950]
Epoch [106/120    avg_loss:0.079, val_acc:0.950]
Epoch [107/120    avg_loss:0.078, val_acc:0.950]
Epoch [108/120    avg_loss:0.077, val_acc:0.950]
Epoch [109/120    avg_loss:0.081, val_acc:0.950]
Epoch [110/120    avg_loss:0.079, val_acc:0.950]
Epoch [111/120    avg_loss:0.075, val_acc:0.950]
Epoch [112/120    avg_loss:0.069, val_acc:0.950]
Epoch [113/120    avg_loss:0.077, val_acc:0.950]
Epoch [114/120    avg_loss:0.077, val_acc:0.950]
Epoch [115/120    avg_loss:0.078, val_acc:0.950]
Epoch [116/120    avg_loss:0.075, val_acc:0.950]
Epoch [117/120    avg_loss:0.082, val_acc:0.950]
Epoch [118/120    avg_loss:0.079, val_acc:0.950]
Epoch [119/120    avg_loss:0.084, val_acc:0.950]
Epoch [120/120    avg_loss:0.080, val_acc:0.950]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5987     0    40    83     0     0     6   316     0]
 [    0     5 17696     0   226     0   145     0    18     0]
 [    0    15     0  1948     1     0     0     0    52    20]
 [    0    38    44     0  2804     0     4     0    81     1]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     0     1     0  4866     0     1    10]
 [    0     6     0     0     0     0    10  1267     0     7]
 [    0    39     0     0    50     0     0     0  3467    15]
 [    0     0     0     1    31    44     0     0     2   841]]

Accuracy:
96.83320078085461

F1 scores:
[       nan 0.95623702 0.98777561 0.96795031 0.90920882 0.9826546
 0.98273251 0.98868513 0.92354822 0.92672176]

Kappa:
0.9582284929911387
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa339247940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.900, val_acc:0.368]
Epoch [2/120    avg_loss:1.390, val_acc:0.457]
Epoch [3/120    avg_loss:1.136, val_acc:0.607]
Epoch [4/120    avg_loss:0.844, val_acc:0.658]
Epoch [5/120    avg_loss:0.639, val_acc:0.789]
Epoch [6/120    avg_loss:0.527, val_acc:0.803]
Epoch [7/120    avg_loss:0.513, val_acc:0.694]
Epoch [8/120    avg_loss:0.435, val_acc:0.877]
Epoch [9/120    avg_loss:0.341, val_acc:0.877]
Epoch [10/120    avg_loss:0.321, val_acc:0.869]
Epoch [11/120    avg_loss:0.271, val_acc:0.918]
Epoch [12/120    avg_loss:0.258, val_acc:0.908]
Epoch [13/120    avg_loss:0.188, val_acc:0.923]
Epoch [14/120    avg_loss:0.244, val_acc:0.900]
Epoch [15/120    avg_loss:0.215, val_acc:0.922]
Epoch [16/120    avg_loss:0.176, val_acc:0.925]
Epoch [17/120    avg_loss:0.142, val_acc:0.949]
Epoch [18/120    avg_loss:0.157, val_acc:0.926]
Epoch [19/120    avg_loss:0.118, val_acc:0.933]
Epoch [20/120    avg_loss:0.162, val_acc:0.955]
Epoch [21/120    avg_loss:0.101, val_acc:0.962]
Epoch [22/120    avg_loss:0.099, val_acc:0.956]
Epoch [23/120    avg_loss:0.107, val_acc:0.946]
Epoch [24/120    avg_loss:0.096, val_acc:0.938]
Epoch [25/120    avg_loss:0.094, val_acc:0.968]
Epoch [26/120    avg_loss:0.089, val_acc:0.971]
Epoch [27/120    avg_loss:0.121, val_acc:0.952]
Epoch [28/120    avg_loss:0.203, val_acc:0.925]
Epoch [29/120    avg_loss:0.083, val_acc:0.967]
Epoch [30/120    avg_loss:0.086, val_acc:0.969]
Epoch [31/120    avg_loss:0.065, val_acc:0.954]
Epoch [32/120    avg_loss:0.059, val_acc:0.969]
Epoch [33/120    avg_loss:0.063, val_acc:0.961]
Epoch [34/120    avg_loss:0.092, val_acc:0.954]
Epoch [35/120    avg_loss:0.079, val_acc:0.972]
Epoch [36/120    avg_loss:0.049, val_acc:0.974]
Epoch [37/120    avg_loss:0.037, val_acc:0.957]
Epoch [38/120    avg_loss:0.061, val_acc:0.973]
Epoch [39/120    avg_loss:0.035, val_acc:0.979]
Epoch [40/120    avg_loss:0.068, val_acc:0.966]
Epoch [41/120    avg_loss:0.064, val_acc:0.928]
Epoch [42/120    avg_loss:0.049, val_acc:0.980]
Epoch [43/120    avg_loss:0.032, val_acc:0.985]
Epoch [44/120    avg_loss:0.037, val_acc:0.983]
Epoch [45/120    avg_loss:0.066, val_acc:0.967]
Epoch [46/120    avg_loss:0.039, val_acc:0.985]
Epoch [47/120    avg_loss:0.037, val_acc:0.984]
Epoch [48/120    avg_loss:0.035, val_acc:0.979]
Epoch [49/120    avg_loss:0.024, val_acc:0.978]
Epoch [50/120    avg_loss:0.035, val_acc:0.951]
Epoch [51/120    avg_loss:0.052, val_acc:0.978]
Epoch [52/120    avg_loss:0.046, val_acc:0.980]
Epoch [53/120    avg_loss:0.021, val_acc:0.985]
Epoch [54/120    avg_loss:0.027, val_acc:0.988]
Epoch [55/120    avg_loss:0.042, val_acc:0.979]
Epoch [56/120    avg_loss:0.029, val_acc:0.985]
Epoch [57/120    avg_loss:0.018, val_acc:0.985]
Epoch [58/120    avg_loss:0.015, val_acc:0.972]
Epoch [59/120    avg_loss:0.019, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.986]
Epoch [61/120    avg_loss:0.014, val_acc:0.987]
Epoch [62/120    avg_loss:0.014, val_acc:0.988]
Epoch [63/120    avg_loss:0.068, val_acc:0.976]
Epoch [64/120    avg_loss:0.028, val_acc:0.988]
Epoch [65/120    avg_loss:0.034, val_acc:0.981]
Epoch [66/120    avg_loss:0.026, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.983]
Epoch [68/120    avg_loss:0.022, val_acc:0.979]
Epoch [69/120    avg_loss:0.042, val_acc:0.987]
Epoch [70/120    avg_loss:0.059, val_acc:0.941]
Epoch [71/120    avg_loss:0.096, val_acc:0.952]
Epoch [72/120    avg_loss:0.038, val_acc:0.985]
Epoch [73/120    avg_loss:0.050, val_acc:0.974]
Epoch [74/120    avg_loss:0.081, val_acc:0.969]
Epoch [75/120    avg_loss:0.068, val_acc:0.985]
Epoch [76/120    avg_loss:0.024, val_acc:0.981]
Epoch [77/120    avg_loss:0.025, val_acc:0.985]
Epoch [78/120    avg_loss:0.020, val_acc:0.990]
Epoch [79/120    avg_loss:0.011, val_acc:0.990]
Epoch [80/120    avg_loss:0.010, val_acc:0.991]
Epoch [81/120    avg_loss:0.010, val_acc:0.991]
Epoch [82/120    avg_loss:0.010, val_acc:0.991]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.012, val_acc:0.992]
Epoch [85/120    avg_loss:0.010, val_acc:0.991]
Epoch [86/120    avg_loss:0.009, val_acc:0.991]
Epoch [87/120    avg_loss:0.009, val_acc:0.991]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.006, val_acc:0.992]
Epoch [91/120    avg_loss:0.013, val_acc:0.992]
Epoch [92/120    avg_loss:0.008, val_acc:0.994]
Epoch [93/120    avg_loss:0.010, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.992]
Epoch [95/120    avg_loss:0.010, val_acc:0.993]
Epoch [96/120    avg_loss:0.011, val_acc:0.991]
Epoch [97/120    avg_loss:0.011, val_acc:0.991]
Epoch [98/120    avg_loss:0.009, val_acc:0.992]
Epoch [99/120    avg_loss:0.008, val_acc:0.992]
Epoch [100/120    avg_loss:0.008, val_acc:0.992]
Epoch [101/120    avg_loss:0.009, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.991]
Epoch [103/120    avg_loss:0.011, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.991]
Epoch [105/120    avg_loss:0.008, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.009, val_acc:0.991]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.006, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.010, val_acc:0.991]
Epoch [112/120    avg_loss:0.009, val_acc:0.991]
Epoch [113/120    avg_loss:0.008, val_acc:0.991]
Epoch [114/120    avg_loss:0.008, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.011, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.008, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6424     0     0     0     0     7     0     0     1]
 [    0     4 18006     0    65     0    15     0     0     0]
 [    0    14     0  1988     0     0     0     0    30     4]
 [    0    39    18     0  2881     0     2     0    32     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4873     0     0     5]
 [    0     0     0     0     0     0     2  1286     0     2]
 [    0     2     0     0    41     0     0     0  3526     2]
 [    0     0     0     0    18    26     0     0     0   875]]

Accuracy:
99.20709517267973

F1 scores:
[       nan 0.99481223 0.99717561 0.98807157 0.96402878 0.99013657
 0.99682929 0.9984472  0.98505378 0.96792035]

Kappa:
0.9894991991741473
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:73
Validation dataloader:73
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f020dfc4940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.913, val_acc:0.274]
Epoch [2/120    avg_loss:1.485, val_acc:0.397]
Epoch [3/120    avg_loss:1.209, val_acc:0.391]
Epoch [4/120    avg_loss:0.919, val_acc:0.775]
Epoch [5/120    avg_loss:0.690, val_acc:0.807]
Epoch [6/120    avg_loss:0.567, val_acc:0.622]
Epoch [7/120    avg_loss:0.591, val_acc:0.763]
Epoch [8/120    avg_loss:0.439, val_acc:0.789]
Epoch [9/120    avg_loss:0.395, val_acc:0.875]
Epoch [10/120    avg_loss:0.310, val_acc:0.913]
Epoch [11/120    avg_loss:0.314, val_acc:0.868]
Epoch [12/120    avg_loss:0.243, val_acc:0.913]
Epoch [13/120    avg_loss:0.242, val_acc:0.874]
Epoch [14/120    avg_loss:0.200, val_acc:0.943]
Epoch [15/120    avg_loss:0.190, val_acc:0.920]
Epoch [16/120    avg_loss:0.206, val_acc:0.932]
Epoch [17/120    avg_loss:0.163, val_acc:0.941]
Epoch [18/120    avg_loss:0.132, val_acc:0.930]
Epoch [19/120    avg_loss:0.125, val_acc:0.925]
Epoch [20/120    avg_loss:0.115, val_acc:0.895]
Epoch [21/120    avg_loss:0.102, val_acc:0.909]
Epoch [22/120    avg_loss:0.135, val_acc:0.949]
Epoch [23/120    avg_loss:0.151, val_acc:0.960]
Epoch [24/120    avg_loss:0.106, val_acc:0.946]
Epoch [25/120    avg_loss:0.094, val_acc:0.933]
Epoch [26/120    avg_loss:0.067, val_acc:0.948]
Epoch [27/120    avg_loss:0.063, val_acc:0.973]
Epoch [28/120    avg_loss:0.045, val_acc:0.971]
Epoch [29/120    avg_loss:0.075, val_acc:0.926]
Epoch [30/120    avg_loss:0.096, val_acc:0.950]
Epoch [31/120    avg_loss:0.064, val_acc:0.974]
Epoch [32/120    avg_loss:0.057, val_acc:0.976]
Epoch [33/120    avg_loss:0.042, val_acc:0.965]
Epoch [34/120    avg_loss:0.043, val_acc:0.974]
Epoch [35/120    avg_loss:0.057, val_acc:0.974]
Epoch [36/120    avg_loss:0.047, val_acc:0.982]
Epoch [37/120    avg_loss:0.028, val_acc:0.981]
Epoch [38/120    avg_loss:0.054, val_acc:0.967]
Epoch [39/120    avg_loss:0.028, val_acc:0.973]
Epoch [40/120    avg_loss:0.026, val_acc:0.981]
Epoch [41/120    avg_loss:0.036, val_acc:0.981]
Epoch [42/120    avg_loss:0.043, val_acc:0.978]
Epoch [43/120    avg_loss:0.024, val_acc:0.978]
Epoch [44/120    avg_loss:0.021, val_acc:0.979]
Epoch [45/120    avg_loss:0.018, val_acc:0.956]
Epoch [46/120    avg_loss:0.021, val_acc:0.949]
Epoch [47/120    avg_loss:0.045, val_acc:0.974]
Epoch [48/120    avg_loss:0.021, val_acc:0.982]
Epoch [49/120    avg_loss:0.035, val_acc:0.978]
Epoch [50/120    avg_loss:0.049, val_acc:0.977]
Epoch [51/120    avg_loss:0.025, val_acc:0.987]
Epoch [52/120    avg_loss:0.024, val_acc:0.988]
Epoch [53/120    avg_loss:0.019, val_acc:0.973]
Epoch [54/120    avg_loss:0.014, val_acc:0.985]
Epoch [55/120    avg_loss:0.013, val_acc:0.987]
Epoch [56/120    avg_loss:0.009, val_acc:0.986]
Epoch [57/120    avg_loss:0.017, val_acc:0.936]
Epoch [58/120    avg_loss:0.035, val_acc:0.962]
Epoch [59/120    avg_loss:0.028, val_acc:0.981]
Epoch [60/120    avg_loss:0.027, val_acc:0.986]
Epoch [61/120    avg_loss:0.063, val_acc:0.934]
Epoch [62/120    avg_loss:0.058, val_acc:0.967]
Epoch [63/120    avg_loss:0.035, val_acc:0.975]
Epoch [64/120    avg_loss:0.028, val_acc:0.990]
Epoch [65/120    avg_loss:0.015, val_acc:0.989]
Epoch [66/120    avg_loss:0.017, val_acc:0.991]
Epoch [67/120    avg_loss:0.018, val_acc:0.990]
Epoch [68/120    avg_loss:0.008, val_acc:0.991]
Epoch [69/120    avg_loss:0.007, val_acc:0.987]
Epoch [70/120    avg_loss:0.015, val_acc:0.987]
Epoch [71/120    avg_loss:0.048, val_acc:0.847]
Epoch [72/120    avg_loss:0.055, val_acc:0.979]
Epoch [73/120    avg_loss:0.032, val_acc:0.987]
Epoch [74/120    avg_loss:0.015, val_acc:0.982]
Epoch [75/120    avg_loss:0.017, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.985]
Epoch [77/120    avg_loss:0.012, val_acc:0.991]
Epoch [78/120    avg_loss:0.009, val_acc:0.988]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.963]
Epoch [81/120    avg_loss:0.019, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.012, val_acc:0.979]
Epoch [85/120    avg_loss:0.018, val_acc:0.981]
Epoch [86/120    avg_loss:0.027, val_acc:0.978]
Epoch [87/120    avg_loss:0.030, val_acc:0.967]
Epoch [88/120    avg_loss:0.032, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.991]
Epoch [90/120    avg_loss:0.007, val_acc:0.992]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.991]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.991]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.007, val_acc:0.963]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.013, val_acc:0.987]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.009, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.003, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6392     0     0     0     0     2     0    38     0]
 [    0     1 18026     0    43     0    20     0     0     0]
 [    0     5     0  2022     0     0     0     0     9     0]
 [    0    24    17     0  2907     0     0     0    23     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    32     0     0  4846     0     0     0]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0    32     0     0    29     0     0     0  3510     0]
 [    0     0     0     0    14    49     0     0     0   856]]

Accuracy:
99.17576458679777

F1 scores:
[       nan 0.99208443 0.99775828 0.98875306 0.97468567 0.98157202
 0.99415325 0.99883586 0.98168088 0.96396396]

Kappa:
0.9890834960039647
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdec481b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.929, val_acc:0.512]
Epoch [2/120    avg_loss:1.419, val_acc:0.604]
Epoch [3/120    avg_loss:1.114, val_acc:0.617]
Epoch [4/120    avg_loss:0.919, val_acc:0.640]
Epoch [5/120    avg_loss:0.715, val_acc:0.660]
Epoch [6/120    avg_loss:0.588, val_acc:0.707]
Epoch [7/120    avg_loss:0.507, val_acc:0.714]
Epoch [8/120    avg_loss:0.432, val_acc:0.749]
Epoch [9/120    avg_loss:0.405, val_acc:0.734]
Epoch [10/120    avg_loss:0.384, val_acc:0.772]
Epoch [11/120    avg_loss:0.346, val_acc:0.818]
Epoch [12/120    avg_loss:0.291, val_acc:0.794]
Epoch [13/120    avg_loss:0.263, val_acc:0.806]
Epoch [14/120    avg_loss:0.243, val_acc:0.838]
Epoch [15/120    avg_loss:0.198, val_acc:0.881]
Epoch [16/120    avg_loss:0.197, val_acc:0.921]
Epoch [17/120    avg_loss:0.186, val_acc:0.871]
Epoch [18/120    avg_loss:0.180, val_acc:0.937]
Epoch [19/120    avg_loss:0.170, val_acc:0.938]
Epoch [20/120    avg_loss:0.160, val_acc:0.882]
Epoch [21/120    avg_loss:0.184, val_acc:0.930]
Epoch [22/120    avg_loss:0.131, val_acc:0.909]
Epoch [23/120    avg_loss:0.158, val_acc:0.944]
Epoch [24/120    avg_loss:0.095, val_acc:0.958]
Epoch [25/120    avg_loss:0.094, val_acc:0.850]
Epoch [26/120    avg_loss:0.105, val_acc:0.925]
Epoch [27/120    avg_loss:0.078, val_acc:0.956]
Epoch [28/120    avg_loss:0.063, val_acc:0.963]
Epoch [29/120    avg_loss:0.049, val_acc:0.966]
Epoch [30/120    avg_loss:0.061, val_acc:0.963]
Epoch [31/120    avg_loss:0.076, val_acc:0.953]
Epoch [32/120    avg_loss:0.057, val_acc:0.948]
Epoch [33/120    avg_loss:0.079, val_acc:0.948]
Epoch [34/120    avg_loss:0.045, val_acc:0.953]
Epoch [35/120    avg_loss:0.071, val_acc:0.954]
Epoch [36/120    avg_loss:0.058, val_acc:0.967]
Epoch [37/120    avg_loss:0.054, val_acc:0.972]
Epoch [38/120    avg_loss:0.030, val_acc:0.953]
Epoch [39/120    avg_loss:0.063, val_acc:0.952]
Epoch [40/120    avg_loss:0.039, val_acc:0.968]
Epoch [41/120    avg_loss:0.036, val_acc:0.973]
Epoch [42/120    avg_loss:0.035, val_acc:0.976]
Epoch [43/120    avg_loss:0.030, val_acc:0.969]
Epoch [44/120    avg_loss:0.035, val_acc:0.980]
Epoch [45/120    avg_loss:0.034, val_acc:0.968]
Epoch [46/120    avg_loss:0.044, val_acc:0.958]
Epoch [47/120    avg_loss:0.039, val_acc:0.945]
Epoch [48/120    avg_loss:0.056, val_acc:0.968]
Epoch [49/120    avg_loss:0.036, val_acc:0.969]
Epoch [50/120    avg_loss:0.042, val_acc:0.957]
Epoch [51/120    avg_loss:0.031, val_acc:0.969]
Epoch [52/120    avg_loss:0.030, val_acc:0.973]
Epoch [53/120    avg_loss:0.032, val_acc:0.973]
Epoch [54/120    avg_loss:0.027, val_acc:0.899]
Epoch [55/120    avg_loss:0.026, val_acc:0.971]
Epoch [56/120    avg_loss:0.028, val_acc:0.966]
Epoch [57/120    avg_loss:0.087, val_acc:0.957]
Epoch [58/120    avg_loss:0.050, val_acc:0.975]
Epoch [59/120    avg_loss:0.039, val_acc:0.975]
Epoch [60/120    avg_loss:0.030, val_acc:0.977]
Epoch [61/120    avg_loss:0.024, val_acc:0.977]
Epoch [62/120    avg_loss:0.022, val_acc:0.975]
Epoch [63/120    avg_loss:0.025, val_acc:0.975]
Epoch [64/120    avg_loss:0.020, val_acc:0.972]
Epoch [65/120    avg_loss:0.021, val_acc:0.976]
Epoch [66/120    avg_loss:0.018, val_acc:0.976]
Epoch [67/120    avg_loss:0.018, val_acc:0.977]
Epoch [68/120    avg_loss:0.020, val_acc:0.977]
Epoch [69/120    avg_loss:0.022, val_acc:0.976]
Epoch [70/120    avg_loss:0.020, val_acc:0.978]
Epoch [71/120    avg_loss:0.017, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.978]
Epoch [76/120    avg_loss:0.019, val_acc:0.978]
Epoch [77/120    avg_loss:0.021, val_acc:0.978]
Epoch [78/120    avg_loss:0.015, val_acc:0.978]
Epoch [79/120    avg_loss:0.014, val_acc:0.979]
Epoch [80/120    avg_loss:0.019, val_acc:0.979]
Epoch [81/120    avg_loss:0.014, val_acc:0.977]
Epoch [82/120    avg_loss:0.018, val_acc:0.977]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.017, val_acc:0.977]
Epoch [85/120    avg_loss:0.019, val_acc:0.978]
Epoch [86/120    avg_loss:0.016, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.978]
Epoch [88/120    avg_loss:0.014, val_acc:0.978]
Epoch [89/120    avg_loss:0.015, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.978]
Epoch [92/120    avg_loss:0.015, val_acc:0.978]
Epoch [93/120    avg_loss:0.014, val_acc:0.978]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.016, val_acc:0.978]
Epoch [96/120    avg_loss:0.016, val_acc:0.978]
Epoch [97/120    avg_loss:0.016, val_acc:0.978]
Epoch [98/120    avg_loss:0.017, val_acc:0.978]
Epoch [99/120    avg_loss:0.015, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.014, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.978]
Epoch [103/120    avg_loss:0.017, val_acc:0.978]
Epoch [104/120    avg_loss:0.016, val_acc:0.978]
Epoch [105/120    avg_loss:0.014, val_acc:0.978]
Epoch [106/120    avg_loss:0.016, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.015, val_acc:0.978]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.015, val_acc:0.978]
Epoch [111/120    avg_loss:0.015, val_acc:0.978]
Epoch [112/120    avg_loss:0.017, val_acc:0.978]
Epoch [113/120    avg_loss:0.020, val_acc:0.978]
Epoch [114/120    avg_loss:0.017, val_acc:0.978]
Epoch [115/120    avg_loss:0.019, val_acc:0.978]
Epoch [116/120    avg_loss:0.015, val_acc:0.978]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.017, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6288     0     2     3     0    12     9   107    11]
 [    0     5 17875     0   113     0    90     0     7     0]
 [    0     7     0  1905     0     0     0     0   123     1]
 [    0    25     0     0  2919     0    19     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     0     4     0  4851     0    11     0]
 [    0    30     0     0     0     0     0  1259     1     0]
 [    0    16    32    62    18     0     4     1  3438     0]
 [    0     0     0     0     0     6     0     0     0   913]]

Accuracy:
98.21656664979635

F1 scores:
[       nan 0.98226978 0.99280735 0.95131086 0.96831979 0.99770642
 0.98457479 0.98397812 0.94632535 0.9897019 ]

Kappa:
0.9764131155286221
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc26d16898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.858, val_acc:0.417]
Epoch [2/120    avg_loss:1.370, val_acc:0.456]
Epoch [3/120    avg_loss:1.029, val_acc:0.564]
Epoch [4/120    avg_loss:0.791, val_acc:0.596]
Epoch [5/120    avg_loss:0.609, val_acc:0.714]
Epoch [6/120    avg_loss:0.500, val_acc:0.729]
Epoch [7/120    avg_loss:0.431, val_acc:0.810]
Epoch [8/120    avg_loss:0.379, val_acc:0.733]
Epoch [9/120    avg_loss:0.345, val_acc:0.802]
Epoch [10/120    avg_loss:0.300, val_acc:0.871]
Epoch [11/120    avg_loss:0.250, val_acc:0.916]
Epoch [12/120    avg_loss:0.244, val_acc:0.923]
Epoch [13/120    avg_loss:0.231, val_acc:0.884]
Epoch [14/120    avg_loss:0.177, val_acc:0.917]
Epoch [15/120    avg_loss:0.191, val_acc:0.903]
Epoch [16/120    avg_loss:0.222, val_acc:0.897]
Epoch [17/120    avg_loss:0.167, val_acc:0.931]
Epoch [18/120    avg_loss:0.129, val_acc:0.929]
Epoch [19/120    avg_loss:0.127, val_acc:0.892]
Epoch [20/120    avg_loss:0.114, val_acc:0.925]
Epoch [21/120    avg_loss:0.133, val_acc:0.941]
Epoch [22/120    avg_loss:0.103, val_acc:0.907]
Epoch [23/120    avg_loss:0.101, val_acc:0.956]
Epoch [24/120    avg_loss:0.112, val_acc:0.964]
Epoch [25/120    avg_loss:0.080, val_acc:0.963]
Epoch [26/120    avg_loss:0.074, val_acc:0.912]
Epoch [27/120    avg_loss:0.076, val_acc:0.958]
Epoch [28/120    avg_loss:0.070, val_acc:0.963]
Epoch [29/120    avg_loss:0.052, val_acc:0.971]
Epoch [30/120    avg_loss:0.062, val_acc:0.969]
Epoch [31/120    avg_loss:0.072, val_acc:0.956]
Epoch [32/120    avg_loss:0.051, val_acc:0.959]
Epoch [33/120    avg_loss:0.056, val_acc:0.972]
Epoch [34/120    avg_loss:0.051, val_acc:0.958]
Epoch [35/120    avg_loss:0.056, val_acc:0.962]
Epoch [36/120    avg_loss:0.069, val_acc:0.963]
Epoch [37/120    avg_loss:0.042, val_acc:0.974]
Epoch [38/120    avg_loss:0.035, val_acc:0.964]
Epoch [39/120    avg_loss:0.045, val_acc:0.951]
Epoch [40/120    avg_loss:0.066, val_acc:0.961]
Epoch [41/120    avg_loss:0.039, val_acc:0.956]
Epoch [42/120    avg_loss:0.031, val_acc:0.959]
Epoch [43/120    avg_loss:0.029, val_acc:0.978]
Epoch [44/120    avg_loss:0.031, val_acc:0.963]
Epoch [45/120    avg_loss:0.021, val_acc:0.980]
Epoch [46/120    avg_loss:0.018, val_acc:0.976]
Epoch [47/120    avg_loss:0.018, val_acc:0.983]
Epoch [48/120    avg_loss:0.021, val_acc:0.974]
Epoch [49/120    avg_loss:0.016, val_acc:0.969]
Epoch [50/120    avg_loss:0.023, val_acc:0.967]
Epoch [51/120    avg_loss:0.032, val_acc:0.967]
Epoch [52/120    avg_loss:0.030, val_acc:0.967]
Epoch [53/120    avg_loss:0.023, val_acc:0.971]
Epoch [54/120    avg_loss:0.018, val_acc:0.973]
Epoch [55/120    avg_loss:0.025, val_acc:0.974]
Epoch [56/120    avg_loss:0.016, val_acc:0.978]
Epoch [57/120    avg_loss:0.031, val_acc:0.973]
Epoch [58/120    avg_loss:0.026, val_acc:0.970]
Epoch [59/120    avg_loss:0.018, val_acc:0.982]
Epoch [60/120    avg_loss:0.016, val_acc:0.967]
Epoch [61/120    avg_loss:0.014, val_acc:0.976]
Epoch [62/120    avg_loss:0.012, val_acc:0.977]
Epoch [63/120    avg_loss:0.011, val_acc:0.979]
Epoch [64/120    avg_loss:0.011, val_acc:0.978]
Epoch [65/120    avg_loss:0.010, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.980]
Epoch [67/120    avg_loss:0.011, val_acc:0.980]
Epoch [68/120    avg_loss:0.010, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.010, val_acc:0.982]
Epoch [71/120    avg_loss:0.008, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.981]
Epoch [73/120    avg_loss:0.008, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.008, val_acc:0.982]
Epoch [76/120    avg_loss:0.011, val_acc:0.982]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.008, val_acc:0.982]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.982]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.015, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.007, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6361     0     3     0     0     0     3    60     5]
 [    0     0 18011     0    12     0    58     0     9     0]
 [    0     6     0  1973     0     0     0     0    57     0]
 [    0    15    14     3  2917     0    20     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     0     0     0  4846     0     2     0]
 [    0     7     0     0     0     0     1  1281     1     0]
 [    0    18     0    67    27     0    10     0  3449     0]
 [    0     0     0     0    16     3     0     0     0   900]]

Accuracy:
98.91547971947075

F1 scores:
[       nan 0.99088714 0.99659704 0.966683   0.98149394 0.99885189
 0.98766942 0.995338   0.96448546 0.98684211]

Kappa:
0.9856347196732391
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0030bc860>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.918, val_acc:0.208]
Epoch [2/120    avg_loss:1.411, val_acc:0.330]
Epoch [3/120    avg_loss:1.101, val_acc:0.413]
Epoch [4/120    avg_loss:0.862, val_acc:0.520]
Epoch [5/120    avg_loss:0.671, val_acc:0.662]
Epoch [6/120    avg_loss:0.568, val_acc:0.682]
Epoch [7/120    avg_loss:0.470, val_acc:0.761]
Epoch [8/120    avg_loss:0.406, val_acc:0.783]
Epoch [9/120    avg_loss:0.360, val_acc:0.823]
Epoch [10/120    avg_loss:0.339, val_acc:0.825]
Epoch [11/120    avg_loss:0.286, val_acc:0.856]
Epoch [12/120    avg_loss:0.266, val_acc:0.917]
Epoch [13/120    avg_loss:0.241, val_acc:0.915]
Epoch [14/120    avg_loss:0.213, val_acc:0.940]
Epoch [15/120    avg_loss:0.184, val_acc:0.926]
Epoch [16/120    avg_loss:0.190, val_acc:0.905]
Epoch [17/120    avg_loss:0.170, val_acc:0.920]
Epoch [18/120    avg_loss:0.165, val_acc:0.900]
Epoch [19/120    avg_loss:0.139, val_acc:0.944]
Epoch [20/120    avg_loss:0.123, val_acc:0.917]
Epoch [21/120    avg_loss:0.102, val_acc:0.956]
Epoch [22/120    avg_loss:0.130, val_acc:0.817]
Epoch [23/120    avg_loss:0.109, val_acc:0.933]
Epoch [24/120    avg_loss:0.073, val_acc:0.973]
Epoch [25/120    avg_loss:0.080, val_acc:0.956]
Epoch [26/120    avg_loss:0.083, val_acc:0.980]
Epoch [27/120    avg_loss:0.093, val_acc:0.956]
Epoch [28/120    avg_loss:0.069, val_acc:0.965]
Epoch [29/120    avg_loss:0.069, val_acc:0.956]
Epoch [30/120    avg_loss:0.049, val_acc:0.953]
Epoch [31/120    avg_loss:0.043, val_acc:0.967]
Epoch [32/120    avg_loss:0.073, val_acc:0.875]
Epoch [33/120    avg_loss:0.098, val_acc:0.947]
Epoch [34/120    avg_loss:0.057, val_acc:0.961]
Epoch [35/120    avg_loss:0.041, val_acc:0.971]
Epoch [36/120    avg_loss:0.047, val_acc:0.962]
Epoch [37/120    avg_loss:0.059, val_acc:0.963]
Epoch [38/120    avg_loss:0.050, val_acc:0.971]
Epoch [39/120    avg_loss:0.027, val_acc:0.976]
Epoch [40/120    avg_loss:0.025, val_acc:0.978]
Epoch [41/120    avg_loss:0.021, val_acc:0.981]
Epoch [42/120    avg_loss:0.017, val_acc:0.980]
Epoch [43/120    avg_loss:0.019, val_acc:0.981]
Epoch [44/120    avg_loss:0.025, val_acc:0.983]
Epoch [45/120    avg_loss:0.019, val_acc:0.980]
Epoch [46/120    avg_loss:0.021, val_acc:0.981]
Epoch [47/120    avg_loss:0.017, val_acc:0.981]
Epoch [48/120    avg_loss:0.016, val_acc:0.982]
Epoch [49/120    avg_loss:0.018, val_acc:0.981]
Epoch [50/120    avg_loss:0.022, val_acc:0.981]
Epoch [51/120    avg_loss:0.017, val_acc:0.983]
Epoch [52/120    avg_loss:0.016, val_acc:0.980]
Epoch [53/120    avg_loss:0.016, val_acc:0.981]
Epoch [54/120    avg_loss:0.019, val_acc:0.981]
Epoch [55/120    avg_loss:0.013, val_acc:0.982]
Epoch [56/120    avg_loss:0.015, val_acc:0.982]
Epoch [57/120    avg_loss:0.017, val_acc:0.982]
Epoch [58/120    avg_loss:0.015, val_acc:0.982]
Epoch [59/120    avg_loss:0.016, val_acc:0.982]
Epoch [60/120    avg_loss:0.015, val_acc:0.983]
Epoch [61/120    avg_loss:0.016, val_acc:0.982]
Epoch [62/120    avg_loss:0.012, val_acc:0.984]
Epoch [63/120    avg_loss:0.022, val_acc:0.980]
Epoch [64/120    avg_loss:0.015, val_acc:0.984]
Epoch [65/120    avg_loss:0.014, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.012, val_acc:0.984]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.983]
Epoch [73/120    avg_loss:0.016, val_acc:0.983]
Epoch [74/120    avg_loss:0.014, val_acc:0.983]
Epoch [75/120    avg_loss:0.013, val_acc:0.982]
Epoch [76/120    avg_loss:0.012, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.982]
Epoch [78/120    avg_loss:0.013, val_acc:0.983]
Epoch [79/120    avg_loss:0.015, val_acc:0.983]
Epoch [80/120    avg_loss:0.013, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.009, val_acc:0.983]
Epoch [83/120    avg_loss:0.012, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.012, val_acc:0.983]
Epoch [89/120    avg_loss:0.013, val_acc:0.983]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.015, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.018, val_acc:0.983]
Epoch [94/120    avg_loss:0.015, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.012, val_acc:0.983]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.014, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.013, val_acc:0.983]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.983]
Epoch [113/120    avg_loss:0.012, val_acc:0.983]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.013, val_acc:0.983]
Epoch [119/120    avg_loss:0.012, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6331     0     0     1     0    18     0    81     1]
 [    0     1 18046     0    29     0    10     0     4     0]
 [    0     1     0  1974     0     0     0     0    58     3]
 [    0    14     1     0  2944     0     6     0     4     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2     0     4     0  4872     0     0     0]
 [    0     8     0     0     0     0     0  1282     0     0]
 [    0    41     0    64    34     0     5     0  3427     0]
 [    0     0     0     0     0     4     0     0     0   915]]

Accuracy:
99.04321210806643

F1 scores:
[       nan 0.98705956 0.99869947 0.96907216 0.98395722 0.99846978
 0.995403   0.99688958 0.95927222 0.99402499]

Kappa:
0.9873286233239527
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f142cddb860>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.813, val_acc:0.163]
Epoch [2/120    avg_loss:1.378, val_acc:0.383]
Epoch [3/120    avg_loss:1.125, val_acc:0.366]
Epoch [4/120    avg_loss:0.958, val_acc:0.461]
Epoch [5/120    avg_loss:0.873, val_acc:0.518]
Epoch [6/120    avg_loss:0.742, val_acc:0.667]
Epoch [7/120    avg_loss:0.637, val_acc:0.765]
Epoch [8/120    avg_loss:0.549, val_acc:0.770]
Epoch [9/120    avg_loss:0.442, val_acc:0.770]
Epoch [10/120    avg_loss:0.418, val_acc:0.806]
Epoch [11/120    avg_loss:0.362, val_acc:0.779]
Epoch [12/120    avg_loss:0.326, val_acc:0.779]
Epoch [13/120    avg_loss:0.279, val_acc:0.843]
Epoch [14/120    avg_loss:0.289, val_acc:0.871]
Epoch [15/120    avg_loss:0.235, val_acc:0.885]
Epoch [16/120    avg_loss:0.215, val_acc:0.932]
Epoch [17/120    avg_loss:0.193, val_acc:0.925]
Epoch [18/120    avg_loss:0.160, val_acc:0.926]
Epoch [19/120    avg_loss:0.210, val_acc:0.902]
Epoch [20/120    avg_loss:0.160, val_acc:0.944]
Epoch [21/120    avg_loss:0.143, val_acc:0.821]
Epoch [22/120    avg_loss:0.163, val_acc:0.928]
Epoch [23/120    avg_loss:0.136, val_acc:0.940]
Epoch [24/120    avg_loss:0.155, val_acc:0.939]
Epoch [25/120    avg_loss:0.138, val_acc:0.936]
Epoch [26/120    avg_loss:0.127, val_acc:0.931]
Epoch [27/120    avg_loss:0.112, val_acc:0.948]
Epoch [28/120    avg_loss:0.119, val_acc:0.956]
Epoch [29/120    avg_loss:0.112, val_acc:0.930]
Epoch [30/120    avg_loss:0.076, val_acc:0.940]
Epoch [31/120    avg_loss:0.077, val_acc:0.953]
Epoch [32/120    avg_loss:0.087, val_acc:0.919]
Epoch [33/120    avg_loss:0.077, val_acc:0.932]
Epoch [34/120    avg_loss:0.101, val_acc:0.950]
Epoch [35/120    avg_loss:0.079, val_acc:0.974]
Epoch [36/120    avg_loss:0.055, val_acc:0.947]
Epoch [37/120    avg_loss:0.070, val_acc:0.970]
Epoch [38/120    avg_loss:0.095, val_acc:0.958]
Epoch [39/120    avg_loss:0.056, val_acc:0.960]
Epoch [40/120    avg_loss:0.042, val_acc:0.967]
Epoch [41/120    avg_loss:0.067, val_acc:0.962]
Epoch [42/120    avg_loss:0.047, val_acc:0.975]
Epoch [43/120    avg_loss:0.032, val_acc:0.972]
Epoch [44/120    avg_loss:0.036, val_acc:0.956]
Epoch [45/120    avg_loss:0.033, val_acc:0.960]
Epoch [46/120    avg_loss:0.045, val_acc:0.921]
Epoch [47/120    avg_loss:0.066, val_acc:0.961]
Epoch [48/120    avg_loss:0.035, val_acc:0.976]
Epoch [49/120    avg_loss:0.029, val_acc:0.929]
Epoch [50/120    avg_loss:0.044, val_acc:0.971]
Epoch [51/120    avg_loss:0.026, val_acc:0.974]
Epoch [52/120    avg_loss:0.041, val_acc:0.968]
Epoch [53/120    avg_loss:0.037, val_acc:0.971]
Epoch [54/120    avg_loss:0.023, val_acc:0.968]
Epoch [55/120    avg_loss:0.033, val_acc:0.927]
Epoch [56/120    avg_loss:0.030, val_acc:0.946]
Epoch [57/120    avg_loss:0.045, val_acc:0.967]
Epoch [58/120    avg_loss:0.063, val_acc:0.904]
Epoch [59/120    avg_loss:0.036, val_acc:0.974]
Epoch [60/120    avg_loss:0.038, val_acc:0.962]
Epoch [61/120    avg_loss:0.041, val_acc:0.976]
Epoch [62/120    avg_loss:0.039, val_acc:0.968]
Epoch [63/120    avg_loss:0.024, val_acc:0.970]
Epoch [64/120    avg_loss:0.018, val_acc:0.984]
Epoch [65/120    avg_loss:0.015, val_acc:0.978]
Epoch [66/120    avg_loss:0.014, val_acc:0.983]
Epoch [67/120    avg_loss:0.030, val_acc:0.972]
Epoch [68/120    avg_loss:0.026, val_acc:0.976]
Epoch [69/120    avg_loss:0.025, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.978]
Epoch [71/120    avg_loss:0.039, val_acc:0.951]
Epoch [72/120    avg_loss:0.033, val_acc:0.984]
Epoch [73/120    avg_loss:0.018, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.980]
Epoch [75/120    avg_loss:0.022, val_acc:0.982]
Epoch [76/120    avg_loss:0.019, val_acc:0.961]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.013, val_acc:0.971]
Epoch [79/120    avg_loss:0.015, val_acc:0.984]
Epoch [80/120    avg_loss:0.015, val_acc:0.975]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.026, val_acc:0.969]
Epoch [83/120    avg_loss:0.026, val_acc:0.975]
Epoch [84/120    avg_loss:0.026, val_acc:0.970]
Epoch [85/120    avg_loss:0.018, val_acc:0.972]
Epoch [86/120    avg_loss:0.018, val_acc:0.971]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.015, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.978]
Epoch [90/120    avg_loss:0.031, val_acc:0.974]
Epoch [91/120    avg_loss:0.017, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.007, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     6     0     0    17    14    36     2]
 [    0     0 18063     0    15     0     9     0     3     0]
 [    0     2     0  1939     0     0     0     0    94     1]
 [    0    35     8     0  2917     0     5     0     6     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    27     0     0     0  4840     0    11     0]
 [    0     4     0     0     0     0     0  1286     0     0]
 [    0    16     0    42    38     0     2     0  3472     1]
 [    0     0     0     0     1     7     0     0     0   911]]

Accuracy:
99.02875183765937

F1 scores:
[       nan 0.98972443 0.99828672 0.96395725 0.98165909 0.99732518
 0.9927187  0.99305019 0.96538301 0.99291553]

Kappa:
0.9871293527351698
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a00155898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.970, val_acc:0.178]
Epoch [2/120    avg_loss:1.500, val_acc:0.412]
Epoch [3/120    avg_loss:1.206, val_acc:0.516]
Epoch [4/120    avg_loss:0.964, val_acc:0.642]
Epoch [5/120    avg_loss:0.722, val_acc:0.659]
Epoch [6/120    avg_loss:0.583, val_acc:0.709]
Epoch [7/120    avg_loss:0.478, val_acc:0.713]
Epoch [8/120    avg_loss:0.451, val_acc:0.786]
Epoch [9/120    avg_loss:0.370, val_acc:0.785]
Epoch [10/120    avg_loss:0.331, val_acc:0.821]
Epoch [11/120    avg_loss:0.314, val_acc:0.810]
Epoch [12/120    avg_loss:0.291, val_acc:0.853]
Epoch [13/120    avg_loss:0.243, val_acc:0.869]
Epoch [14/120    avg_loss:0.213, val_acc:0.868]
Epoch [15/120    avg_loss:0.220, val_acc:0.888]
Epoch [16/120    avg_loss:0.209, val_acc:0.873]
Epoch [17/120    avg_loss:0.164, val_acc:0.926]
Epoch [18/120    avg_loss:0.165, val_acc:0.898]
Epoch [19/120    avg_loss:0.179, val_acc:0.929]
Epoch [20/120    avg_loss:0.177, val_acc:0.937]
Epoch [21/120    avg_loss:0.191, val_acc:0.920]
Epoch [22/120    avg_loss:0.161, val_acc:0.939]
Epoch [23/120    avg_loss:0.151, val_acc:0.911]
Epoch [24/120    avg_loss:0.140, val_acc:0.949]
Epoch [25/120    avg_loss:0.114, val_acc:0.957]
Epoch [26/120    avg_loss:0.115, val_acc:0.965]
Epoch [27/120    avg_loss:0.103, val_acc:0.945]
Epoch [28/120    avg_loss:0.081, val_acc:0.958]
Epoch [29/120    avg_loss:0.073, val_acc:0.940]
Epoch [30/120    avg_loss:0.090, val_acc:0.961]
Epoch [31/120    avg_loss:0.069, val_acc:0.958]
Epoch [32/120    avg_loss:0.067, val_acc:0.959]
Epoch [33/120    avg_loss:0.074, val_acc:0.872]
Epoch [34/120    avg_loss:0.080, val_acc:0.948]
Epoch [35/120    avg_loss:0.048, val_acc:0.965]
Epoch [36/120    avg_loss:0.065, val_acc:0.952]
Epoch [37/120    avg_loss:0.053, val_acc:0.961]
Epoch [38/120    avg_loss:0.046, val_acc:0.956]
Epoch [39/120    avg_loss:0.056, val_acc:0.966]
Epoch [40/120    avg_loss:0.048, val_acc:0.907]
Epoch [41/120    avg_loss:0.050, val_acc:0.959]
Epoch [42/120    avg_loss:0.036, val_acc:0.966]
Epoch [43/120    avg_loss:0.027, val_acc:0.953]
Epoch [44/120    avg_loss:0.036, val_acc:0.963]
Epoch [45/120    avg_loss:0.041, val_acc:0.973]
Epoch [46/120    avg_loss:0.029, val_acc:0.978]
Epoch [47/120    avg_loss:0.031, val_acc:0.973]
Epoch [48/120    avg_loss:0.034, val_acc:0.958]
Epoch [49/120    avg_loss:0.052, val_acc:0.961]
Epoch [50/120    avg_loss:0.025, val_acc:0.964]
Epoch [51/120    avg_loss:0.019, val_acc:0.970]
Epoch [52/120    avg_loss:0.017, val_acc:0.973]
Epoch [53/120    avg_loss:0.031, val_acc:0.955]
Epoch [54/120    avg_loss:0.081, val_acc:0.968]
Epoch [55/120    avg_loss:0.054, val_acc:0.940]
Epoch [56/120    avg_loss:0.032, val_acc:0.971]
Epoch [57/120    avg_loss:0.022, val_acc:0.974]
Epoch [58/120    avg_loss:0.024, val_acc:0.965]
Epoch [59/120    avg_loss:0.033, val_acc:0.963]
Epoch [60/120    avg_loss:0.034, val_acc:0.975]
Epoch [61/120    avg_loss:0.020, val_acc:0.976]
Epoch [62/120    avg_loss:0.017, val_acc:0.977]
Epoch [63/120    avg_loss:0.013, val_acc:0.981]
Epoch [64/120    avg_loss:0.018, val_acc:0.980]
Epoch [65/120    avg_loss:0.011, val_acc:0.980]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.013, val_acc:0.981]
Epoch [68/120    avg_loss:0.011, val_acc:0.981]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.980]
Epoch [73/120    avg_loss:0.011, val_acc:0.982]
Epoch [74/120    avg_loss:0.009, val_acc:0.982]
Epoch [75/120    avg_loss:0.012, val_acc:0.980]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.981]
Epoch [81/120    avg_loss:0.010, val_acc:0.981]
Epoch [82/120    avg_loss:0.011, val_acc:0.981]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.010, val_acc:0.981]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.016, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.016, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.010, val_acc:0.982]
Epoch [95/120    avg_loss:0.012, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.982]
Epoch [97/120    avg_loss:0.007, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.013, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.017, val_acc:0.978]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.012, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.010, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     2     0     0     2     0    81     0]
 [    0     0 18049     0    30     0     7     0     4     0]
 [    0     3     0  1900     0     0     0     0   130     3]
 [    0    24    12     0  2923     0     3     0     8     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4828     0    27     0]
 [    0    21     0     0     0     0     0  1269     0     0]
 [    0     8     1    43    44     0     0     0  3474     1]
 [    0     0     0     0     2    14     0     0     0   903]]

Accuracy:
98.80702769141783

F1 scores:
[       nan 0.98901441 0.99787146 0.95453404 0.97906548 0.99466463
 0.99362009 0.99179367 0.95243317 0.98796499]

Kappa:
0.9841928710262264
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff65170e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.887, val_acc:0.328]
Epoch [2/120    avg_loss:1.344, val_acc:0.413]
Epoch [3/120    avg_loss:1.077, val_acc:0.659]
Epoch [4/120    avg_loss:0.865, val_acc:0.761]
Epoch [5/120    avg_loss:0.720, val_acc:0.755]
Epoch [6/120    avg_loss:0.549, val_acc:0.764]
Epoch [7/120    avg_loss:0.449, val_acc:0.793]
Epoch [8/120    avg_loss:0.396, val_acc:0.786]
Epoch [9/120    avg_loss:0.414, val_acc:0.827]
Epoch [10/120    avg_loss:0.351, val_acc:0.796]
Epoch [11/120    avg_loss:0.329, val_acc:0.843]
Epoch [12/120    avg_loss:0.269, val_acc:0.854]
Epoch [13/120    avg_loss:0.255, val_acc:0.877]
Epoch [14/120    avg_loss:0.256, val_acc:0.900]
Epoch [15/120    avg_loss:0.218, val_acc:0.855]
Epoch [16/120    avg_loss:0.186, val_acc:0.886]
Epoch [17/120    avg_loss:0.182, val_acc:0.873]
Epoch [18/120    avg_loss:0.177, val_acc:0.890]
Epoch [19/120    avg_loss:0.161, val_acc:0.929]
Epoch [20/120    avg_loss:0.145, val_acc:0.930]
Epoch [21/120    avg_loss:0.121, val_acc:0.830]
Epoch [22/120    avg_loss:0.133, val_acc:0.924]
Epoch [23/120    avg_loss:0.124, val_acc:0.924]
Epoch [24/120    avg_loss:0.148, val_acc:0.943]
Epoch [25/120    avg_loss:0.114, val_acc:0.951]
Epoch [26/120    avg_loss:0.104, val_acc:0.947]
Epoch [27/120    avg_loss:0.132, val_acc:0.914]
Epoch [28/120    avg_loss:0.112, val_acc:0.935]
Epoch [29/120    avg_loss:0.077, val_acc:0.959]
Epoch [30/120    avg_loss:0.065, val_acc:0.956]
Epoch [31/120    avg_loss:0.055, val_acc:0.954]
Epoch [32/120    avg_loss:0.063, val_acc:0.953]
Epoch [33/120    avg_loss:0.076, val_acc:0.941]
Epoch [34/120    avg_loss:0.055, val_acc:0.950]
Epoch [35/120    avg_loss:0.049, val_acc:0.956]
Epoch [36/120    avg_loss:0.060, val_acc:0.962]
Epoch [37/120    avg_loss:0.075, val_acc:0.927]
Epoch [38/120    avg_loss:0.081, val_acc:0.908]
Epoch [39/120    avg_loss:0.063, val_acc:0.932]
Epoch [40/120    avg_loss:0.073, val_acc:0.953]
Epoch [41/120    avg_loss:0.070, val_acc:0.901]
Epoch [42/120    avg_loss:0.068, val_acc:0.958]
Epoch [43/120    avg_loss:0.053, val_acc:0.943]
Epoch [44/120    avg_loss:0.059, val_acc:0.968]
Epoch [45/120    avg_loss:0.034, val_acc:0.955]
Epoch [46/120    avg_loss:0.037, val_acc:0.946]
Epoch [47/120    avg_loss:0.032, val_acc:0.966]
Epoch [48/120    avg_loss:0.054, val_acc:0.956]
Epoch [49/120    avg_loss:0.057, val_acc:0.964]
Epoch [50/120    avg_loss:0.042, val_acc:0.969]
Epoch [51/120    avg_loss:0.027, val_acc:0.975]
Epoch [52/120    avg_loss:0.035, val_acc:0.970]
Epoch [53/120    avg_loss:0.016, val_acc:0.970]
Epoch [54/120    avg_loss:0.028, val_acc:0.971]
Epoch [55/120    avg_loss:0.024, val_acc:0.965]
Epoch [56/120    avg_loss:0.036, val_acc:0.962]
Epoch [57/120    avg_loss:0.041, val_acc:0.966]
Epoch [58/120    avg_loss:0.020, val_acc:0.973]
Epoch [59/120    avg_loss:0.034, val_acc:0.962]
Epoch [60/120    avg_loss:0.054, val_acc:0.969]
Epoch [61/120    avg_loss:0.041, val_acc:0.944]
Epoch [62/120    avg_loss:0.048, val_acc:0.974]
Epoch [63/120    avg_loss:0.065, val_acc:0.978]
Epoch [64/120    avg_loss:0.022, val_acc:0.974]
Epoch [65/120    avg_loss:0.034, val_acc:0.963]
Epoch [66/120    avg_loss:0.042, val_acc:0.971]
Epoch [67/120    avg_loss:0.030, val_acc:0.958]
Epoch [68/120    avg_loss:0.035, val_acc:0.966]
Epoch [69/120    avg_loss:0.024, val_acc:0.964]
Epoch [70/120    avg_loss:0.017, val_acc:0.971]
Epoch [71/120    avg_loss:0.017, val_acc:0.972]
Epoch [72/120    avg_loss:0.024, val_acc:0.935]
Epoch [73/120    avg_loss:0.058, val_acc:0.969]
Epoch [74/120    avg_loss:0.035, val_acc:0.971]
Epoch [75/120    avg_loss:0.017, val_acc:0.980]
Epoch [76/120    avg_loss:0.011, val_acc:0.978]
Epoch [77/120    avg_loss:0.010, val_acc:0.978]
Epoch [78/120    avg_loss:0.021, val_acc:0.970]
Epoch [79/120    avg_loss:0.012, val_acc:0.974]
Epoch [80/120    avg_loss:0.013, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.017, val_acc:0.959]
Epoch [83/120    avg_loss:0.013, val_acc:0.978]
Epoch [84/120    avg_loss:0.017, val_acc:0.976]
Epoch [85/120    avg_loss:0.016, val_acc:0.953]
Epoch [86/120    avg_loss:0.014, val_acc:0.976]
Epoch [87/120    avg_loss:0.007, val_acc:0.973]
Epoch [88/120    avg_loss:0.011, val_acc:0.979]
Epoch [89/120    avg_loss:0.006, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.028, val_acc:0.966]
Epoch [94/120    avg_loss:0.022, val_acc:0.977]
Epoch [95/120    avg_loss:0.007, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.970]
Epoch [97/120    avg_loss:0.010, val_acc:0.979]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.005, val_acc:0.982]
Epoch [100/120    avg_loss:0.004, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.978]
Epoch [107/120    avg_loss:0.004, val_acc:0.977]
Epoch [108/120    avg_loss:0.005, val_acc:0.979]
Epoch [109/120    avg_loss:0.004, val_acc:0.981]
Epoch [110/120    avg_loss:0.004, val_acc:0.970]
Epoch [111/120    avg_loss:0.026, val_acc:0.973]
Epoch [112/120    avg_loss:0.029, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.977]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.006, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.977]
Epoch [117/120    avg_loss:0.008, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.973]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6332     0     1     0     0     9     5    76     9]
 [    0     0 18058     0    19     0     8     0     5     0]
 [    0     8     0  1909     0     0     0     0   119     0]
 [    0    20    15     0  2910     0    18     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4845     0    10     0]
 [    0     6     0     0     0     0     0  1284     0     0]
 [    0    21    14    40    37     0     3     0  3456     0]
 [    0     4     0     0     6     8     0     0     0   901]]

Accuracy:
98.81184778155351

F1 scores:
[       nan 0.98760041 0.99767956 0.95785248 0.97913863 0.99694423
 0.99272616 0.99573478 0.9543007  0.98362445]

Kappa:
0.9842524877820719
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8db52d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.882, val_acc:0.248]
Epoch [2/120    avg_loss:1.303, val_acc:0.481]
Epoch [3/120    avg_loss:0.973, val_acc:0.675]
Epoch [4/120    avg_loss:0.735, val_acc:0.627]
Epoch [5/120    avg_loss:0.563, val_acc:0.677]
Epoch [6/120    avg_loss:0.502, val_acc:0.743]
Epoch [7/120    avg_loss:0.448, val_acc:0.703]
Epoch [8/120    avg_loss:0.386, val_acc:0.777]
Epoch [9/120    avg_loss:0.399, val_acc:0.737]
Epoch [10/120    avg_loss:0.381, val_acc:0.726]
Epoch [11/120    avg_loss:0.317, val_acc:0.750]
Epoch [12/120    avg_loss:0.282, val_acc:0.812]
Epoch [13/120    avg_loss:0.275, val_acc:0.754]
Epoch [14/120    avg_loss:0.245, val_acc:0.841]
Epoch [15/120    avg_loss:0.219, val_acc:0.878]
Epoch [16/120    avg_loss:0.222, val_acc:0.748]
Epoch [17/120    avg_loss:0.208, val_acc:0.762]
Epoch [18/120    avg_loss:0.211, val_acc:0.895]
Epoch [19/120    avg_loss:0.168, val_acc:0.897]
Epoch [20/120    avg_loss:0.159, val_acc:0.913]
Epoch [21/120    avg_loss:0.127, val_acc:0.931]
Epoch [22/120    avg_loss:0.216, val_acc:0.912]
Epoch [23/120    avg_loss:0.155, val_acc:0.842]
Epoch [24/120    avg_loss:0.181, val_acc:0.913]
Epoch [25/120    avg_loss:0.156, val_acc:0.930]
Epoch [26/120    avg_loss:0.129, val_acc:0.937]
Epoch [27/120    avg_loss:0.091, val_acc:0.938]
Epoch [28/120    avg_loss:0.089, val_acc:0.904]
Epoch [29/120    avg_loss:0.098, val_acc:0.935]
Epoch [30/120    avg_loss:0.094, val_acc:0.936]
Epoch [31/120    avg_loss:0.108, val_acc:0.932]
Epoch [32/120    avg_loss:0.111, val_acc:0.939]
Epoch [33/120    avg_loss:0.075, val_acc:0.957]
Epoch [34/120    avg_loss:0.081, val_acc:0.953]
Epoch [35/120    avg_loss:0.061, val_acc:0.948]
Epoch [36/120    avg_loss:0.091, val_acc:0.949]
Epoch [37/120    avg_loss:0.087, val_acc:0.907]
Epoch [38/120    avg_loss:0.070, val_acc:0.964]
Epoch [39/120    avg_loss:0.047, val_acc:0.851]
Epoch [40/120    avg_loss:0.077, val_acc:0.959]
Epoch [41/120    avg_loss:0.041, val_acc:0.961]
Epoch [42/120    avg_loss:0.038, val_acc:0.969]
Epoch [43/120    avg_loss:0.030, val_acc:0.969]
Epoch [44/120    avg_loss:0.037, val_acc:0.972]
Epoch [45/120    avg_loss:0.081, val_acc:0.963]
Epoch [46/120    avg_loss:0.058, val_acc:0.967]
Epoch [47/120    avg_loss:0.059, val_acc:0.968]
Epoch [48/120    avg_loss:0.047, val_acc:0.965]
Epoch [49/120    avg_loss:0.046, val_acc:0.950]
Epoch [50/120    avg_loss:0.030, val_acc:0.975]
Epoch [51/120    avg_loss:0.019, val_acc:0.973]
Epoch [52/120    avg_loss:0.020, val_acc:0.978]
Epoch [53/120    avg_loss:0.018, val_acc:0.973]
Epoch [54/120    avg_loss:0.019, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.960]
Epoch [56/120    avg_loss:0.021, val_acc:0.974]
Epoch [57/120    avg_loss:0.025, val_acc:0.973]
Epoch [58/120    avg_loss:0.033, val_acc:0.968]
Epoch [59/120    avg_loss:0.038, val_acc:0.976]
Epoch [60/120    avg_loss:0.025, val_acc:0.969]
Epoch [61/120    avg_loss:0.021, val_acc:0.977]
Epoch [62/120    avg_loss:0.012, val_acc:0.977]
Epoch [63/120    avg_loss:0.016, val_acc:0.977]
Epoch [64/120    avg_loss:0.017, val_acc:0.973]
Epoch [65/120    avg_loss:0.026, val_acc:0.948]
Epoch [66/120    avg_loss:0.024, val_acc:0.972]
Epoch [67/120    avg_loss:0.013, val_acc:0.974]
Epoch [68/120    avg_loss:0.010, val_acc:0.977]
Epoch [69/120    avg_loss:0.008, val_acc:0.978]
Epoch [70/120    avg_loss:0.009, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.980]
Epoch [72/120    avg_loss:0.008, val_acc:0.980]
Epoch [73/120    avg_loss:0.007, val_acc:0.980]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.008, val_acc:0.983]
Epoch [76/120    avg_loss:0.008, val_acc:0.983]
Epoch [77/120    avg_loss:0.007, val_acc:0.982]
Epoch [78/120    avg_loss:0.009, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.010, val_acc:0.981]
Epoch [82/120    avg_loss:0.011, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.007, val_acc:0.980]
Epoch [85/120    avg_loss:0.007, val_acc:0.981]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.006, val_acc:0.982]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.983]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.005, val_acc:0.983]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.983]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.013, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6397     0     0     4     0     0    13    13     5]
 [    0     2 18058     0    14     0    16     0     0     0]
 [    0     9     0  1896     1     0     0     0   129     1]
 [    0    12     7     5  2944     0     2     0     1     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    36     1     0     0  4840     0     1     0]
 [    0    41     0     0     0     0     0  1243     1     5]
 [    0    25     0    59    54     0     2     0  3431     0]
 [    0     0     0     0     0    13     0     0     0   906]]

Accuracy:
98.86004868291037

F1 scores:
[       nan 0.99040099 0.99792766 0.94871153 0.98313575 0.99504384
 0.99404395 0.97643362 0.96012313 0.98639085]

Kappa:
0.9848899606040967
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d6b6db8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.891, val_acc:0.297]
Epoch [2/120    avg_loss:1.426, val_acc:0.439]
Epoch [3/120    avg_loss:1.150, val_acc:0.708]
Epoch [4/120    avg_loss:0.899, val_acc:0.614]
Epoch [5/120    avg_loss:0.692, val_acc:0.594]
Epoch [6/120    avg_loss:0.559, val_acc:0.742]
Epoch [7/120    avg_loss:0.468, val_acc:0.783]
Epoch [8/120    avg_loss:0.445, val_acc:0.812]
Epoch [9/120    avg_loss:0.345, val_acc:0.771]
Epoch [10/120    avg_loss:0.352, val_acc:0.819]
Epoch [11/120    avg_loss:0.291, val_acc:0.835]
Epoch [12/120    avg_loss:0.280, val_acc:0.831]
Epoch [13/120    avg_loss:0.241, val_acc:0.873]
Epoch [14/120    avg_loss:0.242, val_acc:0.891]
Epoch [15/120    avg_loss:0.201, val_acc:0.920]
Epoch [16/120    avg_loss:0.195, val_acc:0.888]
Epoch [17/120    avg_loss:0.187, val_acc:0.891]
Epoch [18/120    avg_loss:0.165, val_acc:0.895]
Epoch [19/120    avg_loss:0.152, val_acc:0.913]
Epoch [20/120    avg_loss:0.159, val_acc:0.892]
Epoch [21/120    avg_loss:0.145, val_acc:0.932]
Epoch [22/120    avg_loss:0.118, val_acc:0.920]
Epoch [23/120    avg_loss:0.111, val_acc:0.935]
Epoch [24/120    avg_loss:0.098, val_acc:0.946]
Epoch [25/120    avg_loss:0.067, val_acc:0.921]
Epoch [26/120    avg_loss:0.082, val_acc:0.946]
Epoch [27/120    avg_loss:0.113, val_acc:0.923]
Epoch [28/120    avg_loss:0.085, val_acc:0.954]
Epoch [29/120    avg_loss:0.097, val_acc:0.931]
Epoch [30/120    avg_loss:0.104, val_acc:0.913]
Epoch [31/120    avg_loss:0.128, val_acc:0.940]
Epoch [32/120    avg_loss:0.108, val_acc:0.873]
Epoch [33/120    avg_loss:0.071, val_acc:0.963]
Epoch [34/120    avg_loss:0.062, val_acc:0.935]
Epoch [35/120    avg_loss:0.071, val_acc:0.960]
Epoch [36/120    avg_loss:0.063, val_acc:0.947]
Epoch [37/120    avg_loss:0.061, val_acc:0.957]
Epoch [38/120    avg_loss:0.047, val_acc:0.940]
Epoch [39/120    avg_loss:0.034, val_acc:0.967]
Epoch [40/120    avg_loss:0.054, val_acc:0.965]
Epoch [41/120    avg_loss:0.041, val_acc:0.950]
Epoch [42/120    avg_loss:0.042, val_acc:0.935]
Epoch [43/120    avg_loss:0.077, val_acc:0.961]
Epoch [44/120    avg_loss:0.081, val_acc:0.961]
Epoch [45/120    avg_loss:0.039, val_acc:0.969]
Epoch [46/120    avg_loss:0.030, val_acc:0.971]
Epoch [47/120    avg_loss:0.037, val_acc:0.952]
Epoch [48/120    avg_loss:0.043, val_acc:0.934]
Epoch [49/120    avg_loss:0.036, val_acc:0.964]
Epoch [50/120    avg_loss:0.021, val_acc:0.973]
Epoch [51/120    avg_loss:0.020, val_acc:0.975]
Epoch [52/120    avg_loss:0.025, val_acc:0.967]
Epoch [53/120    avg_loss:0.029, val_acc:0.976]
Epoch [54/120    avg_loss:0.016, val_acc:0.977]
Epoch [55/120    avg_loss:0.015, val_acc:0.969]
Epoch [56/120    avg_loss:0.014, val_acc:0.968]
Epoch [57/120    avg_loss:0.012, val_acc:0.979]
Epoch [58/120    avg_loss:0.024, val_acc:0.958]
Epoch [59/120    avg_loss:0.028, val_acc:0.970]
Epoch [60/120    avg_loss:0.015, val_acc:0.973]
Epoch [61/120    avg_loss:0.016, val_acc:0.978]
Epoch [62/120    avg_loss:0.023, val_acc:0.968]
Epoch [63/120    avg_loss:0.016, val_acc:0.977]
Epoch [64/120    avg_loss:0.025, val_acc:0.971]
Epoch [65/120    avg_loss:0.013, val_acc:0.972]
Epoch [66/120    avg_loss:0.010, val_acc:0.969]
Epoch [67/120    avg_loss:0.042, val_acc:0.940]
Epoch [68/120    avg_loss:0.022, val_acc:0.959]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.011, val_acc:0.980]
Epoch [71/120    avg_loss:0.009, val_acc:0.976]
Epoch [72/120    avg_loss:0.021, val_acc:0.975]
Epoch [73/120    avg_loss:0.020, val_acc:0.975]
Epoch [74/120    avg_loss:0.009, val_acc:0.978]
Epoch [75/120    avg_loss:0.007, val_acc:0.976]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.007, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.962]
Epoch [79/120    avg_loss:0.016, val_acc:0.975]
Epoch [80/120    avg_loss:0.010, val_acc:0.978]
Epoch [81/120    avg_loss:0.007, val_acc:0.973]
Epoch [82/120    avg_loss:0.006, val_acc:0.975]
Epoch [83/120    avg_loss:0.069, val_acc:0.845]
Epoch [84/120    avg_loss:0.035, val_acc:0.964]
Epoch [85/120    avg_loss:0.036, val_acc:0.904]
Epoch [86/120    avg_loss:0.034, val_acc:0.972]
Epoch [87/120    avg_loss:0.030, val_acc:0.971]
Epoch [88/120    avg_loss:0.019, val_acc:0.977]
Epoch [89/120    avg_loss:0.026, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.982]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.007, val_acc:0.980]
Epoch [99/120    avg_loss:0.006, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.977]
Epoch [101/120    avg_loss:0.006, val_acc:0.980]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.980]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.005, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6401     0     0     0     0     0     0    16    15]
 [    0     0 18023     0    44     0    19     0     4     0]
 [    0     6     1  1862     0     0     0     0   167     0]
 [    0    25    10     0  2909     0    15     0    12     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     9     0     0     0     0     2  1263    14     2]
 [    0     9     4    63    28     0     0     0  3459     8]
 [    0     2     0     0     0    14     0     0     0   903]]

Accuracy:
98.8166678716892

F1 scores:
[       nan 0.99363552 0.99770268 0.94016662 0.97732236 0.99466463
 0.99622102 0.98942421 0.95512909 0.97727273]

Kappa:
0.9843257942909186
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feba37668d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.879, val_acc:0.213]
Epoch [2/120    avg_loss:1.384, val_acc:0.396]
Epoch [3/120    avg_loss:1.043, val_acc:0.475]
Epoch [4/120    avg_loss:0.850, val_acc:0.521]
Epoch [5/120    avg_loss:0.633, val_acc:0.622]
Epoch [6/120    avg_loss:0.515, val_acc:0.718]
Epoch [7/120    avg_loss:0.418, val_acc:0.770]
Epoch [8/120    avg_loss:0.412, val_acc:0.789]
Epoch [9/120    avg_loss:0.341, val_acc:0.795]
Epoch [10/120    avg_loss:0.303, val_acc:0.853]
Epoch [11/120    avg_loss:0.250, val_acc:0.852]
Epoch [12/120    avg_loss:0.302, val_acc:0.830]
Epoch [13/120    avg_loss:0.236, val_acc:0.868]
Epoch [14/120    avg_loss:0.223, val_acc:0.864]
Epoch [15/120    avg_loss:0.183, val_acc:0.936]
Epoch [16/120    avg_loss:0.217, val_acc:0.880]
Epoch [17/120    avg_loss:0.184, val_acc:0.917]
Epoch [18/120    avg_loss:0.149, val_acc:0.818]
Epoch [19/120    avg_loss:0.125, val_acc:0.924]
Epoch [20/120    avg_loss:0.114, val_acc:0.935]
Epoch [21/120    avg_loss:0.157, val_acc:0.920]
Epoch [22/120    avg_loss:0.114, val_acc:0.942]
Epoch [23/120    avg_loss:0.112, val_acc:0.943]
Epoch [24/120    avg_loss:0.078, val_acc:0.952]
Epoch [25/120    avg_loss:0.148, val_acc:0.912]
Epoch [26/120    avg_loss:0.082, val_acc:0.947]
Epoch [27/120    avg_loss:0.091, val_acc:0.936]
Epoch [28/120    avg_loss:0.109, val_acc:0.938]
Epoch [29/120    avg_loss:0.082, val_acc:0.953]
Epoch [30/120    avg_loss:0.066, val_acc:0.960]
Epoch [31/120    avg_loss:0.072, val_acc:0.951]
Epoch [32/120    avg_loss:0.076, val_acc:0.952]
Epoch [33/120    avg_loss:0.062, val_acc:0.952]
Epoch [34/120    avg_loss:0.073, val_acc:0.954]
Epoch [35/120    avg_loss:0.060, val_acc:0.958]
Epoch [36/120    avg_loss:0.054, val_acc:0.967]
Epoch [37/120    avg_loss:0.048, val_acc:0.963]
Epoch [38/120    avg_loss:0.033, val_acc:0.958]
Epoch [39/120    avg_loss:0.040, val_acc:0.953]
Epoch [40/120    avg_loss:0.053, val_acc:0.946]
Epoch [41/120    avg_loss:0.036, val_acc:0.956]
Epoch [42/120    avg_loss:0.034, val_acc:0.963]
Epoch [43/120    avg_loss:0.024, val_acc:0.968]
Epoch [44/120    avg_loss:0.058, val_acc:0.918]
Epoch [45/120    avg_loss:0.037, val_acc:0.959]
Epoch [46/120    avg_loss:0.075, val_acc:0.962]
Epoch [47/120    avg_loss:0.070, val_acc:0.938]
Epoch [48/120    avg_loss:0.068, val_acc:0.952]
Epoch [49/120    avg_loss:0.076, val_acc:0.963]
Epoch [50/120    avg_loss:0.040, val_acc:0.971]
Epoch [51/120    avg_loss:0.026, val_acc:0.954]
Epoch [52/120    avg_loss:0.032, val_acc:0.965]
Epoch [53/120    avg_loss:0.040, val_acc:0.973]
Epoch [54/120    avg_loss:0.051, val_acc:0.963]
Epoch [55/120    avg_loss:0.034, val_acc:0.954]
Epoch [56/120    avg_loss:0.035, val_acc:0.951]
Epoch [57/120    avg_loss:0.022, val_acc:0.969]
Epoch [58/120    avg_loss:0.016, val_acc:0.963]
Epoch [59/120    avg_loss:0.013, val_acc:0.967]
Epoch [60/120    avg_loss:0.014, val_acc:0.976]
Epoch [61/120    avg_loss:0.012, val_acc:0.972]
Epoch [62/120    avg_loss:0.014, val_acc:0.975]
Epoch [63/120    avg_loss:0.015, val_acc:0.975]
Epoch [64/120    avg_loss:0.018, val_acc:0.951]
Epoch [65/120    avg_loss:0.017, val_acc:0.966]
Epoch [66/120    avg_loss:0.015, val_acc:0.981]
Epoch [67/120    avg_loss:0.011, val_acc:0.978]
Epoch [68/120    avg_loss:0.022, val_acc:0.971]
Epoch [69/120    avg_loss:0.013, val_acc:0.977]
Epoch [70/120    avg_loss:0.011, val_acc:0.973]
Epoch [71/120    avg_loss:0.016, val_acc:0.958]
Epoch [72/120    avg_loss:0.010, val_acc:0.974]
Epoch [73/120    avg_loss:0.014, val_acc:0.978]
Epoch [74/120    avg_loss:0.012, val_acc:0.968]
Epoch [75/120    avg_loss:0.015, val_acc:0.967]
Epoch [76/120    avg_loss:0.013, val_acc:0.971]
Epoch [77/120    avg_loss:0.013, val_acc:0.974]
Epoch [78/120    avg_loss:0.012, val_acc:0.940]
Epoch [79/120    avg_loss:0.025, val_acc:0.967]
Epoch [80/120    avg_loss:0.021, val_acc:0.979]
Epoch [81/120    avg_loss:0.010, val_acc:0.975]
Epoch [82/120    avg_loss:0.007, val_acc:0.978]
Epoch [83/120    avg_loss:0.007, val_acc:0.976]
Epoch [84/120    avg_loss:0.007, val_acc:0.978]
Epoch [85/120    avg_loss:0.007, val_acc:0.978]
Epoch [86/120    avg_loss:0.007, val_acc:0.978]
Epoch [87/120    avg_loss:0.007, val_acc:0.978]
Epoch [88/120    avg_loss:0.007, val_acc:0.978]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.005, val_acc:0.978]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.006, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.978]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.005, val_acc:0.978]
Epoch [98/120    avg_loss:0.005, val_acc:0.978]
Epoch [99/120    avg_loss:0.006, val_acc:0.978]
Epoch [100/120    avg_loss:0.008, val_acc:0.979]
Epoch [101/120    avg_loss:0.006, val_acc:0.978]
Epoch [102/120    avg_loss:0.004, val_acc:0.978]
Epoch [103/120    avg_loss:0.006, val_acc:0.978]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.006, val_acc:0.978]
Epoch [106/120    avg_loss:0.005, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.978]
Epoch [109/120    avg_loss:0.005, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.005, val_acc:0.978]
Epoch [112/120    avg_loss:0.005, val_acc:0.978]
Epoch [113/120    avg_loss:0.004, val_acc:0.978]
Epoch [114/120    avg_loss:0.006, val_acc:0.978]
Epoch [115/120    avg_loss:0.004, val_acc:0.978]
Epoch [116/120    avg_loss:0.005, val_acc:0.978]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6314     0     2     0     0    25    14    77     0]
 [    0     2 18070     0     9     0     5     0     4     0]
 [    0     2     0  1945     0     0     0     0    89     0]
 [    0    34    10     0  2919     0     6     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     0     0     0  4854     0     1     0]
 [    0     9     0     0     0     0     2  1274     5     0]
 [    0    12     0    77    46     0     0     0  3436     0]
 [    0     1     0     0     9    23     0     0     0   886]]

Accuracy:
98.81907791675705

F1 scores:
[       nan 0.98610027 0.99853563 0.95812808 0.98035264 0.99126472
 0.99365404 0.98836307 0.95630392 0.98171745]

Kappa:
0.98435095019912
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:51
Validation dataloader:51
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c79633860>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.951, val_acc:0.217]
Epoch [2/120    avg_loss:1.412, val_acc:0.439]
Epoch [3/120    avg_loss:1.118, val_acc:0.717]
Epoch [4/120    avg_loss:0.856, val_acc:0.730]
Epoch [5/120    avg_loss:0.702, val_acc:0.777]
Epoch [6/120    avg_loss:0.583, val_acc:0.723]
Epoch [7/120    avg_loss:0.470, val_acc:0.725]
Epoch [8/120    avg_loss:0.408, val_acc:0.752]
Epoch [9/120    avg_loss:0.385, val_acc:0.814]
Epoch [10/120    avg_loss:0.361, val_acc:0.769]
Epoch [11/120    avg_loss:0.332, val_acc:0.793]
Epoch [12/120    avg_loss:0.336, val_acc:0.757]
Epoch [13/120    avg_loss:0.314, val_acc:0.776]
Epoch [14/120    avg_loss:0.296, val_acc:0.742]
Epoch [15/120    avg_loss:0.304, val_acc:0.792]
Epoch [16/120    avg_loss:0.265, val_acc:0.851]
Epoch [17/120    avg_loss:0.250, val_acc:0.816]
Epoch [18/120    avg_loss:0.204, val_acc:0.855]
Epoch [19/120    avg_loss:0.176, val_acc:0.877]
Epoch [20/120    avg_loss:0.169, val_acc:0.879]
Epoch [21/120    avg_loss:0.141, val_acc:0.931]
Epoch [22/120    avg_loss:0.150, val_acc:0.922]
Epoch [23/120    avg_loss:0.120, val_acc:0.913]
Epoch [24/120    avg_loss:0.118, val_acc:0.943]
Epoch [25/120    avg_loss:0.102, val_acc:0.924]
Epoch [26/120    avg_loss:0.125, val_acc:0.839]
Epoch [27/120    avg_loss:0.189, val_acc:0.922]
Epoch [28/120    avg_loss:0.179, val_acc:0.897]
Epoch [29/120    avg_loss:0.126, val_acc:0.927]
Epoch [30/120    avg_loss:0.115, val_acc:0.955]
Epoch [31/120    avg_loss:0.126, val_acc:0.928]
Epoch [32/120    avg_loss:0.092, val_acc:0.950]
Epoch [33/120    avg_loss:0.083, val_acc:0.948]
Epoch [34/120    avg_loss:0.090, val_acc:0.967]
Epoch [35/120    avg_loss:0.063, val_acc:0.904]
Epoch [36/120    avg_loss:0.058, val_acc:0.977]
Epoch [37/120    avg_loss:0.074, val_acc:0.951]
Epoch [38/120    avg_loss:0.095, val_acc:0.931]
Epoch [39/120    avg_loss:0.085, val_acc:0.963]
Epoch [40/120    avg_loss:0.050, val_acc:0.927]
Epoch [41/120    avg_loss:0.055, val_acc:0.977]
Epoch [42/120    avg_loss:0.041, val_acc:0.975]
Epoch [43/120    avg_loss:0.036, val_acc:0.963]
Epoch [44/120    avg_loss:0.047, val_acc:0.966]
Epoch [45/120    avg_loss:0.060, val_acc:0.946]
Epoch [46/120    avg_loss:0.063, val_acc:0.967]
Epoch [47/120    avg_loss:0.039, val_acc:0.970]
Epoch [48/120    avg_loss:0.041, val_acc:0.957]
Epoch [49/120    avg_loss:0.045, val_acc:0.948]
Epoch [50/120    avg_loss:0.035, val_acc:0.963]
Epoch [51/120    avg_loss:0.031, val_acc:0.971]
Epoch [52/120    avg_loss:0.058, val_acc:0.951]
Epoch [53/120    avg_loss:0.037, val_acc:0.971]
Epoch [54/120    avg_loss:0.030, val_acc:0.976]
Epoch [55/120    avg_loss:0.021, val_acc:0.978]
Epoch [56/120    avg_loss:0.017, val_acc:0.979]
Epoch [57/120    avg_loss:0.018, val_acc:0.978]
Epoch [58/120    avg_loss:0.015, val_acc:0.978]
Epoch [59/120    avg_loss:0.017, val_acc:0.978]
Epoch [60/120    avg_loss:0.015, val_acc:0.981]
Epoch [61/120    avg_loss:0.019, val_acc:0.980]
Epoch [62/120    avg_loss:0.017, val_acc:0.980]
Epoch [63/120    avg_loss:0.019, val_acc:0.981]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.015, val_acc:0.982]
Epoch [66/120    avg_loss:0.013, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.983]
Epoch [68/120    avg_loss:0.017, val_acc:0.978]
Epoch [69/120    avg_loss:0.014, val_acc:0.980]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.013, val_acc:0.983]
Epoch [72/120    avg_loss:0.013, val_acc:0.982]
Epoch [73/120    avg_loss:0.011, val_acc:0.984]
Epoch [74/120    avg_loss:0.014, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.011, val_acc:0.983]
Epoch [77/120    avg_loss:0.013, val_acc:0.983]
Epoch [78/120    avg_loss:0.016, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.010, val_acc:0.985]
Epoch [82/120    avg_loss:0.017, val_acc:0.984]
Epoch [83/120    avg_loss:0.013, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.014, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.015, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.014, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6310     0     1     2     0     0    22    87    10]
 [    0     0 18020     0    47     0    17     0     6     0]
 [    0     5     0  1948     0     0     0     0    83     0]
 [    0     7     9     1  2947     0     7     0     0     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0   134     0     0     0  4727     0    17     0]
 [    0     1     0     0     0     0     0  1280     6     3]
 [    0    30     0    39    44     0     0     0  3457     1]
 [    0     0     0     0     1     5     0     0     0   913]]

Accuracy:
98.58771359024414

F1 scores:
[       nan 0.98709425 0.99412462 0.96795031 0.98020955 0.99808795
 0.98182573 0.98765432 0.95669019 0.98863021]

Kappa:
0.9812771856030085
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff9c135e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.946, val_acc:0.281]
Epoch [2/120    avg_loss:1.501, val_acc:0.372]
Epoch [3/120    avg_loss:1.268, val_acc:0.464]
Epoch [4/120    avg_loss:1.041, val_acc:0.582]
Epoch [5/120    avg_loss:0.885, val_acc:0.594]
Epoch [6/120    avg_loss:0.734, val_acc:0.626]
Epoch [7/120    avg_loss:0.636, val_acc:0.753]
Epoch [8/120    avg_loss:0.504, val_acc:0.761]
Epoch [9/120    avg_loss:0.450, val_acc:0.776]
Epoch [10/120    avg_loss:0.374, val_acc:0.809]
Epoch [11/120    avg_loss:0.314, val_acc:0.836]
Epoch [12/120    avg_loss:0.310, val_acc:0.871]
Epoch [13/120    avg_loss:0.247, val_acc:0.797]
Epoch [14/120    avg_loss:0.225, val_acc:0.891]
Epoch [15/120    avg_loss:0.213, val_acc:0.924]
Epoch [16/120    avg_loss:0.191, val_acc:0.923]
Epoch [17/120    avg_loss:0.204, val_acc:0.913]
Epoch [18/120    avg_loss:0.167, val_acc:0.922]
Epoch [19/120    avg_loss:0.170, val_acc:0.928]
Epoch [20/120    avg_loss:0.130, val_acc:0.952]
Epoch [21/120    avg_loss:0.103, val_acc:0.949]
Epoch [22/120    avg_loss:0.107, val_acc:0.842]
Epoch [23/120    avg_loss:0.139, val_acc:0.904]
Epoch [24/120    avg_loss:0.135, val_acc:0.953]
Epoch [25/120    avg_loss:0.077, val_acc:0.945]
Epoch [26/120    avg_loss:0.088, val_acc:0.945]
Epoch [27/120    avg_loss:0.077, val_acc:0.968]
Epoch [28/120    avg_loss:0.076, val_acc:0.974]
Epoch [29/120    avg_loss:0.068, val_acc:0.954]
Epoch [30/120    avg_loss:0.049, val_acc:0.950]
Epoch [31/120    avg_loss:0.064, val_acc:0.965]
Epoch [32/120    avg_loss:0.059, val_acc:0.867]
Epoch [33/120    avg_loss:0.062, val_acc:0.974]
Epoch [34/120    avg_loss:0.032, val_acc:0.971]
Epoch [35/120    avg_loss:0.031, val_acc:0.988]
Epoch [36/120    avg_loss:0.054, val_acc:0.974]
Epoch [37/120    avg_loss:0.036, val_acc:0.980]
Epoch [38/120    avg_loss:0.042, val_acc:0.957]
Epoch [39/120    avg_loss:0.034, val_acc:0.975]
Epoch [40/120    avg_loss:0.040, val_acc:0.977]
Epoch [41/120    avg_loss:0.037, val_acc:0.957]
Epoch [42/120    avg_loss:0.026, val_acc:0.984]
Epoch [43/120    avg_loss:0.020, val_acc:0.982]
Epoch [44/120    avg_loss:0.029, val_acc:0.981]
Epoch [45/120    avg_loss:0.029, val_acc:0.969]
Epoch [46/120    avg_loss:0.024, val_acc:0.985]
Epoch [47/120    avg_loss:0.027, val_acc:0.988]
Epoch [48/120    avg_loss:0.030, val_acc:0.979]
Epoch [49/120    avg_loss:0.015, val_acc:0.990]
Epoch [50/120    avg_loss:0.013, val_acc:0.983]
Epoch [51/120    avg_loss:0.049, val_acc:0.977]
Epoch [52/120    avg_loss:0.040, val_acc:0.969]
Epoch [53/120    avg_loss:0.062, val_acc:0.970]
Epoch [54/120    avg_loss:0.024, val_acc:0.982]
Epoch [55/120    avg_loss:0.045, val_acc:0.981]
Epoch [56/120    avg_loss:0.020, val_acc:0.984]
Epoch [57/120    avg_loss:0.029, val_acc:0.991]
Epoch [58/120    avg_loss:0.019, val_acc:0.992]
Epoch [59/120    avg_loss:0.014, val_acc:0.989]
Epoch [60/120    avg_loss:0.017, val_acc:0.986]
Epoch [61/120    avg_loss:0.027, val_acc:0.988]
Epoch [62/120    avg_loss:0.048, val_acc:0.980]
Epoch [63/120    avg_loss:0.028, val_acc:0.981]
Epoch [64/120    avg_loss:0.016, val_acc:0.985]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.026, val_acc:0.985]
Epoch [67/120    avg_loss:0.059, val_acc:0.974]
Epoch [68/120    avg_loss:0.053, val_acc:0.935]
Epoch [69/120    avg_loss:0.035, val_acc:0.985]
Epoch [70/120    avg_loss:0.016, val_acc:0.987]
Epoch [71/120    avg_loss:0.014, val_acc:0.987]
Epoch [72/120    avg_loss:0.012, val_acc:0.990]
Epoch [73/120    avg_loss:0.009, val_acc:0.991]
Epoch [74/120    avg_loss:0.008, val_acc:0.991]
Epoch [75/120    avg_loss:0.010, val_acc:0.991]
Epoch [76/120    avg_loss:0.010, val_acc:0.992]
Epoch [77/120    avg_loss:0.009, val_acc:0.991]
Epoch [78/120    avg_loss:0.008, val_acc:0.992]
Epoch [79/120    avg_loss:0.006, val_acc:0.992]
Epoch [80/120    avg_loss:0.007, val_acc:0.991]
Epoch [81/120    avg_loss:0.009, val_acc:0.992]
Epoch [82/120    avg_loss:0.009, val_acc:0.992]
Epoch [83/120    avg_loss:0.006, val_acc:0.992]
Epoch [84/120    avg_loss:0.007, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.993]
Epoch [86/120    avg_loss:0.007, val_acc:0.993]
Epoch [87/120    avg_loss:0.007, val_acc:0.994]
Epoch [88/120    avg_loss:0.008, val_acc:0.993]
Epoch [89/120    avg_loss:0.006, val_acc:0.994]
Epoch [90/120    avg_loss:0.006, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.994]
Epoch [92/120    avg_loss:0.006, val_acc:0.994]
Epoch [93/120    avg_loss:0.007, val_acc:0.993]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.008, val_acc:0.994]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.994]
Epoch [98/120    avg_loss:0.005, val_acc:0.994]
Epoch [99/120    avg_loss:0.009, val_acc:0.994]
Epoch [100/120    avg_loss:0.006, val_acc:0.994]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.993]
Epoch [103/120    avg_loss:0.006, val_acc:0.993]
Epoch [104/120    avg_loss:0.010, val_acc:0.994]
Epoch [105/120    avg_loss:0.006, val_acc:0.994]
Epoch [106/120    avg_loss:0.007, val_acc:0.994]
Epoch [107/120    avg_loss:0.006, val_acc:0.994]
Epoch [108/120    avg_loss:0.005, val_acc:0.994]
Epoch [109/120    avg_loss:0.006, val_acc:0.994]
Epoch [110/120    avg_loss:0.007, val_acc:0.993]
Epoch [111/120    avg_loss:0.007, val_acc:0.994]
Epoch [112/120    avg_loss:0.006, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.005, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.994]
Epoch [117/120    avg_loss:0.005, val_acc:0.994]
Epoch [118/120    avg_loss:0.006, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6312     0     0     0     0    14     8    93     5]
 [    0     0 18065     0    19     0     1     0     5     0]
 [    0     5     0  1958     0     0     0     0    69     4]
 [    0    18    14     0  2928     0     2     0     7     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4872     0     3     0]
 [    0    10     0     0     0     0     1  1279     0     0]
 [    0     1     0     9    54     0     0     0  3506     1]
 [    0     0     0     0     1    10     0     0     0   908]]

Accuracy:
99.1323837755766

F1 scores:
[       nan 0.98794804 0.99883888 0.9782663  0.98024774 0.99618321
 0.997543   0.99262709 0.9666391  0.98695652]

Kappa:
0.9885060351387278
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3633bf8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.007, val_acc:0.575]
Epoch [2/120    avg_loss:1.488, val_acc:0.450]
Epoch [3/120    avg_loss:1.191, val_acc:0.584]
Epoch [4/120    avg_loss:0.971, val_acc:0.605]
Epoch [5/120    avg_loss:0.851, val_acc:0.694]
Epoch [6/120    avg_loss:0.710, val_acc:0.698]
Epoch [7/120    avg_loss:0.568, val_acc:0.647]
Epoch [8/120    avg_loss:0.497, val_acc:0.655]
Epoch [9/120    avg_loss:0.424, val_acc:0.774]
Epoch [10/120    avg_loss:0.390, val_acc:0.788]
Epoch [11/120    avg_loss:0.352, val_acc:0.787]
Epoch [12/120    avg_loss:0.312, val_acc:0.823]
Epoch [13/120    avg_loss:0.278, val_acc:0.817]
Epoch [14/120    avg_loss:0.226, val_acc:0.873]
Epoch [15/120    avg_loss:0.217, val_acc:0.895]
Epoch [16/120    avg_loss:0.200, val_acc:0.866]
Epoch [17/120    avg_loss:0.156, val_acc:0.902]
Epoch [18/120    avg_loss:0.177, val_acc:0.902]
Epoch [19/120    avg_loss:0.207, val_acc:0.918]
Epoch [20/120    avg_loss:0.157, val_acc:0.925]
Epoch [21/120    avg_loss:0.139, val_acc:0.918]
Epoch [22/120    avg_loss:0.128, val_acc:0.900]
Epoch [23/120    avg_loss:0.092, val_acc:0.897]
Epoch [24/120    avg_loss:0.107, val_acc:0.943]
Epoch [25/120    avg_loss:0.074, val_acc:0.963]
Epoch [26/120    avg_loss:0.087, val_acc:0.966]
Epoch [27/120    avg_loss:0.073, val_acc:0.960]
Epoch [28/120    avg_loss:0.063, val_acc:0.961]
Epoch [29/120    avg_loss:0.052, val_acc:0.973]
Epoch [30/120    avg_loss:0.061, val_acc:0.949]
Epoch [31/120    avg_loss:0.055, val_acc:0.976]
Epoch [32/120    avg_loss:0.064, val_acc:0.944]
Epoch [33/120    avg_loss:0.063, val_acc:0.970]
Epoch [34/120    avg_loss:0.059, val_acc:0.954]
Epoch [35/120    avg_loss:0.050, val_acc:0.978]
Epoch [36/120    avg_loss:0.043, val_acc:0.968]
Epoch [37/120    avg_loss:0.036, val_acc:0.962]
Epoch [38/120    avg_loss:0.043, val_acc:0.974]
Epoch [39/120    avg_loss:0.036, val_acc:0.983]
Epoch [40/120    avg_loss:0.030, val_acc:0.965]
Epoch [41/120    avg_loss:0.033, val_acc:0.972]
Epoch [42/120    avg_loss:0.018, val_acc:0.984]
Epoch [43/120    avg_loss:0.018, val_acc:0.974]
Epoch [44/120    avg_loss:0.019, val_acc:0.982]
Epoch [45/120    avg_loss:0.026, val_acc:0.973]
Epoch [46/120    avg_loss:0.033, val_acc:0.964]
Epoch [47/120    avg_loss:0.045, val_acc:0.968]
Epoch [48/120    avg_loss:0.050, val_acc:0.968]
Epoch [49/120    avg_loss:0.038, val_acc:0.978]
Epoch [50/120    avg_loss:0.047, val_acc:0.927]
Epoch [51/120    avg_loss:0.052, val_acc:0.977]
Epoch [52/120    avg_loss:0.027, val_acc:0.975]
Epoch [53/120    avg_loss:0.027, val_acc:0.974]
Epoch [54/120    avg_loss:0.021, val_acc:0.976]
Epoch [55/120    avg_loss:0.025, val_acc:0.979]
Epoch [56/120    avg_loss:0.016, val_acc:0.980]
Epoch [57/120    avg_loss:0.010, val_acc:0.983]
Epoch [58/120    avg_loss:0.013, val_acc:0.984]
Epoch [59/120    avg_loss:0.012, val_acc:0.983]
Epoch [60/120    avg_loss:0.009, val_acc:0.983]
Epoch [61/120    avg_loss:0.011, val_acc:0.984]
Epoch [62/120    avg_loss:0.009, val_acc:0.981]
Epoch [63/120    avg_loss:0.009, val_acc:0.981]
Epoch [64/120    avg_loss:0.010, val_acc:0.983]
Epoch [65/120    avg_loss:0.009, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.983]
Epoch [67/120    avg_loss:0.009, val_acc:0.983]
Epoch [68/120    avg_loss:0.008, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.980]
Epoch [70/120    avg_loss:0.007, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.981]
Epoch [72/120    avg_loss:0.010, val_acc:0.983]
Epoch [73/120    avg_loss:0.007, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.009, val_acc:0.982]
Epoch [76/120    avg_loss:0.008, val_acc:0.982]
Epoch [77/120    avg_loss:0.009, val_acc:0.982]
Epoch [78/120    avg_loss:0.009, val_acc:0.982]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.013, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.020, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     0     0     0     0     0    68     0]
 [    0     4 18024     0    46     0    13     0     3     0]
 [    0     9     0  1894     0     0     0     0   130     3]
 [    0    29     4    12  2922     0     1     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0    14     0  4837     0     3     0]
 [    0     0     0     0     0     0     0  1286     4     0]
 [    0    18     0    19    52     0     0     0  3482     0]
 [    0     0     0     0    13    13     0     0     0   893]]

Accuracy:
98.82871809702841

F1 scores:
[       nan 0.99004356 0.99739915 0.95632416 0.9709254  0.99504384
 0.9943468  0.9984472  0.95856848 0.98402204]

Kappa:
0.984485106351896
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f049ffec8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.881, val_acc:0.113]
Epoch [2/120    avg_loss:1.440, val_acc:0.400]
Epoch [3/120    avg_loss:1.174, val_acc:0.528]
Epoch [4/120    avg_loss:0.937, val_acc:0.665]
Epoch [5/120    avg_loss:0.745, val_acc:0.667]
Epoch [6/120    avg_loss:0.615, val_acc:0.770]
Epoch [7/120    avg_loss:0.561, val_acc:0.782]
Epoch [8/120    avg_loss:0.556, val_acc:0.682]
Epoch [9/120    avg_loss:0.438, val_acc:0.717]
Epoch [10/120    avg_loss:0.376, val_acc:0.753]
Epoch [11/120    avg_loss:0.338, val_acc:0.781]
Epoch [12/120    avg_loss:0.310, val_acc:0.854]
Epoch [13/120    avg_loss:0.285, val_acc:0.860]
Epoch [14/120    avg_loss:0.246, val_acc:0.923]
Epoch [15/120    avg_loss:0.250, val_acc:0.900]
Epoch [16/120    avg_loss:0.232, val_acc:0.912]
Epoch [17/120    avg_loss:0.170, val_acc:0.860]
Epoch [18/120    avg_loss:0.150, val_acc:0.931]
Epoch [19/120    avg_loss:0.155, val_acc:0.911]
Epoch [20/120    avg_loss:0.152, val_acc:0.927]
Epoch [21/120    avg_loss:0.160, val_acc:0.930]
Epoch [22/120    avg_loss:0.163, val_acc:0.966]
Epoch [23/120    avg_loss:0.110, val_acc:0.971]
Epoch [24/120    avg_loss:0.220, val_acc:0.916]
Epoch [25/120    avg_loss:0.177, val_acc:0.922]
Epoch [26/120    avg_loss:0.170, val_acc:0.926]
Epoch [27/120    avg_loss:0.124, val_acc:0.952]
Epoch [28/120    avg_loss:0.081, val_acc:0.960]
Epoch [29/120    avg_loss:0.063, val_acc:0.969]
Epoch [30/120    avg_loss:0.109, val_acc:0.891]
Epoch [31/120    avg_loss:0.136, val_acc:0.965]
Epoch [32/120    avg_loss:0.068, val_acc:0.974]
Epoch [33/120    avg_loss:0.053, val_acc:0.975]
Epoch [34/120    avg_loss:0.051, val_acc:0.965]
Epoch [35/120    avg_loss:0.076, val_acc:0.963]
Epoch [36/120    avg_loss:0.082, val_acc:0.944]
Epoch [37/120    avg_loss:0.073, val_acc:0.917]
Epoch [38/120    avg_loss:0.060, val_acc:0.949]
Epoch [39/120    avg_loss:0.045, val_acc:0.967]
Epoch [40/120    avg_loss:0.355, val_acc:0.823]
Epoch [41/120    avg_loss:0.246, val_acc:0.879]
Epoch [42/120    avg_loss:0.144, val_acc:0.950]
Epoch [43/120    avg_loss:0.100, val_acc:0.902]
Epoch [44/120    avg_loss:0.107, val_acc:0.953]
Epoch [45/120    avg_loss:0.066, val_acc:0.953]
Epoch [46/120    avg_loss:0.067, val_acc:0.968]
Epoch [47/120    avg_loss:0.040, val_acc:0.969]
Epoch [48/120    avg_loss:0.037, val_acc:0.972]
Epoch [49/120    avg_loss:0.041, val_acc:0.971]
Epoch [50/120    avg_loss:0.036, val_acc:0.968]
Epoch [51/120    avg_loss:0.045, val_acc:0.970]
Epoch [52/120    avg_loss:0.029, val_acc:0.975]
Epoch [53/120    avg_loss:0.038, val_acc:0.972]
Epoch [54/120    avg_loss:0.047, val_acc:0.968]
Epoch [55/120    avg_loss:0.039, val_acc:0.972]
Epoch [56/120    avg_loss:0.034, val_acc:0.972]
Epoch [57/120    avg_loss:0.028, val_acc:0.974]
Epoch [58/120    avg_loss:0.024, val_acc:0.972]
Epoch [59/120    avg_loss:0.024, val_acc:0.974]
Epoch [60/120    avg_loss:0.033, val_acc:0.972]
Epoch [61/120    avg_loss:0.033, val_acc:0.976]
Epoch [62/120    avg_loss:0.033, val_acc:0.973]
Epoch [63/120    avg_loss:0.035, val_acc:0.974]
Epoch [64/120    avg_loss:0.029, val_acc:0.975]
Epoch [65/120    avg_loss:0.024, val_acc:0.972]
Epoch [66/120    avg_loss:0.038, val_acc:0.969]
Epoch [67/120    avg_loss:0.029, val_acc:0.974]
Epoch [68/120    avg_loss:0.019, val_acc:0.976]
Epoch [69/120    avg_loss:0.029, val_acc:0.975]
Epoch [70/120    avg_loss:0.023, val_acc:0.976]
Epoch [71/120    avg_loss:0.021, val_acc:0.975]
Epoch [72/120    avg_loss:0.029, val_acc:0.973]
Epoch [73/120    avg_loss:0.023, val_acc:0.976]
Epoch [74/120    avg_loss:0.032, val_acc:0.977]
Epoch [75/120    avg_loss:0.025, val_acc:0.977]
Epoch [76/120    avg_loss:0.024, val_acc:0.973]
Epoch [77/120    avg_loss:0.035, val_acc:0.970]
Epoch [78/120    avg_loss:0.027, val_acc:0.976]
Epoch [79/120    avg_loss:0.026, val_acc:0.975]
Epoch [80/120    avg_loss:0.024, val_acc:0.973]
Epoch [81/120    avg_loss:0.026, val_acc:0.974]
Epoch [82/120    avg_loss:0.022, val_acc:0.976]
Epoch [83/120    avg_loss:0.023, val_acc:0.973]
Epoch [84/120    avg_loss:0.020, val_acc:0.974]
Epoch [85/120    avg_loss:0.021, val_acc:0.976]
Epoch [86/120    avg_loss:0.021, val_acc:0.976]
Epoch [87/120    avg_loss:0.018, val_acc:0.978]
Epoch [88/120    avg_loss:0.025, val_acc:0.973]
Epoch [89/120    avg_loss:0.020, val_acc:0.978]
Epoch [90/120    avg_loss:0.030, val_acc:0.978]
Epoch [91/120    avg_loss:0.023, val_acc:0.977]
Epoch [92/120    avg_loss:0.028, val_acc:0.972]
Epoch [93/120    avg_loss:0.032, val_acc:0.978]
Epoch [94/120    avg_loss:0.024, val_acc:0.978]
Epoch [95/120    avg_loss:0.021, val_acc:0.978]
Epoch [96/120    avg_loss:0.018, val_acc:0.977]
Epoch [97/120    avg_loss:0.019, val_acc:0.977]
Epoch [98/120    avg_loss:0.018, val_acc:0.979]
Epoch [99/120    avg_loss:0.022, val_acc:0.979]
Epoch [100/120    avg_loss:0.020, val_acc:0.978]
Epoch [101/120    avg_loss:0.021, val_acc:0.980]
Epoch [102/120    avg_loss:0.022, val_acc:0.978]
Epoch [103/120    avg_loss:0.017, val_acc:0.977]
Epoch [104/120    avg_loss:0.024, val_acc:0.977]
Epoch [105/120    avg_loss:0.023, val_acc:0.977]
Epoch [106/120    avg_loss:0.020, val_acc:0.979]
Epoch [107/120    avg_loss:0.019, val_acc:0.973]
Epoch [108/120    avg_loss:0.017, val_acc:0.978]
Epoch [109/120    avg_loss:0.017, val_acc:0.979]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.014, val_acc:0.979]
Epoch [112/120    avg_loss:0.020, val_acc:0.978]
Epoch [113/120    avg_loss:0.024, val_acc:0.976]
Epoch [114/120    avg_loss:0.014, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.977]
Epoch [116/120    avg_loss:0.016, val_acc:0.976]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.019, val_acc:0.975]
Epoch [119/120    avg_loss:0.015, val_acc:0.977]
Epoch [120/120    avg_loss:0.013, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6343     0     7     3     0    36     7    36     0]
 [    0     0 17966     0    31     0    90     0     3     0]
 [    0     1     0  1992     0     0     0     0    39     4]
 [    0    24     7     0  2920     0     3     0    17     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    44     0     1     0  4833     0     0     0]
 [    0     5     0     0     0     0     6  1271     2     6]
 [    0     9     0    35    55     0     2     0  3469     1]
 [    0     0     0     0     4    11     0     0     0   904]]

Accuracy:
98.81907791675705

F1 scores:
[       nan 0.99001093 0.99515329 0.97886978 0.97560976 0.99580313
 0.98151909 0.98987539 0.97211714 0.9852861 ]

Kappa:
0.9843652378355501
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9807950898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.929, val_acc:0.235]
Epoch [2/120    avg_loss:1.436, val_acc:0.352]
Epoch [3/120    avg_loss:1.175, val_acc:0.422]
Epoch [4/120    avg_loss:0.950, val_acc:0.522]
Epoch [5/120    avg_loss:0.742, val_acc:0.709]
Epoch [6/120    avg_loss:0.594, val_acc:0.676]
Epoch [7/120    avg_loss:0.467, val_acc:0.723]
Epoch [8/120    avg_loss:0.398, val_acc:0.785]
Epoch [9/120    avg_loss:0.364, val_acc:0.797]
Epoch [10/120    avg_loss:0.333, val_acc:0.818]
Epoch [11/120    avg_loss:0.293, val_acc:0.817]
Epoch [12/120    avg_loss:0.247, val_acc:0.838]
Epoch [13/120    avg_loss:0.241, val_acc:0.883]
Epoch [14/120    avg_loss:0.210, val_acc:0.917]
Epoch [15/120    avg_loss:0.205, val_acc:0.917]
Epoch [16/120    avg_loss:0.203, val_acc:0.930]
Epoch [17/120    avg_loss:0.194, val_acc:0.945]
Epoch [18/120    avg_loss:0.156, val_acc:0.951]
Epoch [19/120    avg_loss:0.132, val_acc:0.959]
Epoch [20/120    avg_loss:0.146, val_acc:0.923]
Epoch [21/120    avg_loss:0.124, val_acc:0.964]
Epoch [22/120    avg_loss:0.139, val_acc:0.905]
Epoch [23/120    avg_loss:0.135, val_acc:0.958]
Epoch [24/120    avg_loss:0.109, val_acc:0.967]
Epoch [25/120    avg_loss:0.094, val_acc:0.947]
Epoch [26/120    avg_loss:0.110, val_acc:0.944]
Epoch [27/120    avg_loss:0.094, val_acc:0.919]
Epoch [28/120    avg_loss:0.098, val_acc:0.953]
Epoch [29/120    avg_loss:0.085, val_acc:0.975]
Epoch [30/120    avg_loss:0.068, val_acc:0.974]
Epoch [31/120    avg_loss:0.056, val_acc:0.967]
Epoch [32/120    avg_loss:0.065, val_acc:0.963]
Epoch [33/120    avg_loss:0.049, val_acc:0.981]
Epoch [34/120    avg_loss:0.054, val_acc:0.956]
Epoch [35/120    avg_loss:0.064, val_acc:0.977]
Epoch [36/120    avg_loss:0.051, val_acc:0.973]
Epoch [37/120    avg_loss:0.043, val_acc:0.940]
Epoch [38/120    avg_loss:0.063, val_acc:0.968]
Epoch [39/120    avg_loss:0.047, val_acc:0.973]
Epoch [40/120    avg_loss:0.057, val_acc:0.963]
Epoch [41/120    avg_loss:0.055, val_acc:0.983]
Epoch [42/120    avg_loss:0.029, val_acc:0.978]
Epoch [43/120    avg_loss:0.030, val_acc:0.962]
Epoch [44/120    avg_loss:0.042, val_acc:0.970]
Epoch [45/120    avg_loss:0.053, val_acc:0.983]
Epoch [46/120    avg_loss:0.062, val_acc:0.978]
Epoch [47/120    avg_loss:0.033, val_acc:0.978]
Epoch [48/120    avg_loss:0.038, val_acc:0.981]
Epoch [49/120    avg_loss:0.032, val_acc:0.985]
Epoch [50/120    avg_loss:0.031, val_acc:0.968]
Epoch [51/120    avg_loss:0.057, val_acc:0.965]
Epoch [52/120    avg_loss:0.045, val_acc:0.978]
Epoch [53/120    avg_loss:0.026, val_acc:0.979]
Epoch [54/120    avg_loss:0.021, val_acc:0.967]
Epoch [55/120    avg_loss:0.049, val_acc:0.972]
Epoch [56/120    avg_loss:0.034, val_acc:0.979]
Epoch [57/120    avg_loss:0.019, val_acc:0.987]
Epoch [58/120    avg_loss:0.024, val_acc:0.980]
Epoch [59/120    avg_loss:0.023, val_acc:0.989]
Epoch [60/120    avg_loss:0.017, val_acc:0.986]
Epoch [61/120    avg_loss:0.015, val_acc:0.988]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.020, val_acc:0.983]
Epoch [64/120    avg_loss:0.050, val_acc:0.974]
Epoch [65/120    avg_loss:0.042, val_acc:0.968]
Epoch [66/120    avg_loss:0.028, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.986]
Epoch [68/120    avg_loss:0.015, val_acc:0.980]
Epoch [69/120    avg_loss:0.033, val_acc:0.973]
Epoch [70/120    avg_loss:0.024, val_acc:0.985]
Epoch [71/120    avg_loss:0.016, val_acc:0.986]
Epoch [72/120    avg_loss:0.008, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.010, val_acc:0.986]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.986]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6328     0     6     1     0     0     3    94     0]
 [    0     1 18025     0    28     0    33     0     2     1]
 [    0     0     0  1954     0     0     0     0    81     1]
 [    0    33     8     0  2923     0     3     0     5     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15     0     0     0  4859     0     4     0]
 [    0    12     0     0     0     6     0  1269     1     2]
 [    0    44     0    41    54     0     0     0  3420    12]
 [    0     0     0     0     6     7     0     0     0   906]]

Accuracy:
98.78533728580724

F1 scores:
[       nan 0.98490272 0.99756489 0.96804558 0.9769385  0.99504384
 0.99437225 0.99063232 0.95291167 0.98424769]

Kappa:
0.9839123428085316
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7532bb7898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.891, val_acc:0.526]
Epoch [2/120    avg_loss:1.444, val_acc:0.643]
Epoch [3/120    avg_loss:1.171, val_acc:0.701]
Epoch [4/120    avg_loss:1.007, val_acc:0.693]
Epoch [5/120    avg_loss:0.890, val_acc:0.736]
Epoch [6/120    avg_loss:0.775, val_acc:0.767]
Epoch [7/120    avg_loss:0.682, val_acc:0.749]
Epoch [8/120    avg_loss:0.576, val_acc:0.747]
Epoch [9/120    avg_loss:0.482, val_acc:0.779]
Epoch [10/120    avg_loss:0.421, val_acc:0.800]
Epoch [11/120    avg_loss:0.353, val_acc:0.858]
Epoch [12/120    avg_loss:0.333, val_acc:0.832]
Epoch [13/120    avg_loss:0.355, val_acc:0.846]
Epoch [14/120    avg_loss:0.252, val_acc:0.908]
Epoch [15/120    avg_loss:0.225, val_acc:0.887]
Epoch [16/120    avg_loss:0.205, val_acc:0.891]
Epoch [17/120    avg_loss:0.205, val_acc:0.908]
Epoch [18/120    avg_loss:0.172, val_acc:0.936]
Epoch [19/120    avg_loss:0.142, val_acc:0.932]
Epoch [20/120    avg_loss:0.101, val_acc:0.923]
Epoch [21/120    avg_loss:0.125, val_acc:0.938]
Epoch [22/120    avg_loss:0.148, val_acc:0.927]
Epoch [23/120    avg_loss:0.115, val_acc:0.958]
Epoch [24/120    avg_loss:0.099, val_acc:0.954]
Epoch [25/120    avg_loss:0.111, val_acc:0.946]
Epoch [26/120    avg_loss:0.084, val_acc:0.962]
Epoch [27/120    avg_loss:0.077, val_acc:0.935]
Epoch [28/120    avg_loss:0.084, val_acc:0.953]
Epoch [29/120    avg_loss:0.084, val_acc:0.959]
Epoch [30/120    avg_loss:0.107, val_acc:0.951]
Epoch [31/120    avg_loss:0.079, val_acc:0.967]
Epoch [32/120    avg_loss:0.079, val_acc:0.965]
Epoch [33/120    avg_loss:0.090, val_acc:0.955]
Epoch [34/120    avg_loss:0.068, val_acc:0.958]
Epoch [35/120    avg_loss:0.072, val_acc:0.971]
Epoch [36/120    avg_loss:0.069, val_acc:0.959]
Epoch [37/120    avg_loss:0.047, val_acc:0.967]
Epoch [38/120    avg_loss:0.046, val_acc:0.949]
Epoch [39/120    avg_loss:0.054, val_acc:0.963]
Epoch [40/120    avg_loss:0.064, val_acc:0.939]
Epoch [41/120    avg_loss:0.075, val_acc:0.963]
Epoch [42/120    avg_loss:0.080, val_acc:0.958]
Epoch [43/120    avg_loss:0.030, val_acc:0.952]
Epoch [44/120    avg_loss:0.031, val_acc:0.968]
Epoch [45/120    avg_loss:0.043, val_acc:0.913]
Epoch [46/120    avg_loss:0.037, val_acc:0.964]
Epoch [47/120    avg_loss:0.033, val_acc:0.915]
Epoch [48/120    avg_loss:0.052, val_acc:0.948]
Epoch [49/120    avg_loss:0.026, val_acc:0.968]
Epoch [50/120    avg_loss:0.021, val_acc:0.975]
Epoch [51/120    avg_loss:0.016, val_acc:0.976]
Epoch [52/120    avg_loss:0.018, val_acc:0.974]
Epoch [53/120    avg_loss:0.015, val_acc:0.975]
Epoch [54/120    avg_loss:0.018, val_acc:0.975]
Epoch [55/120    avg_loss:0.018, val_acc:0.978]
Epoch [56/120    avg_loss:0.016, val_acc:0.979]
Epoch [57/120    avg_loss:0.014, val_acc:0.978]
Epoch [58/120    avg_loss:0.013, val_acc:0.976]
Epoch [59/120    avg_loss:0.016, val_acc:0.978]
Epoch [60/120    avg_loss:0.014, val_acc:0.978]
Epoch [61/120    avg_loss:0.015, val_acc:0.973]
Epoch [62/120    avg_loss:0.013, val_acc:0.975]
Epoch [63/120    avg_loss:0.016, val_acc:0.977]
Epoch [64/120    avg_loss:0.015, val_acc:0.977]
Epoch [65/120    avg_loss:0.014, val_acc:0.975]
Epoch [66/120    avg_loss:0.013, val_acc:0.975]
Epoch [67/120    avg_loss:0.016, val_acc:0.975]
Epoch [68/120    avg_loss:0.012, val_acc:0.977]
Epoch [69/120    avg_loss:0.012, val_acc:0.977]
Epoch [70/120    avg_loss:0.014, val_acc:0.977]
Epoch [71/120    avg_loss:0.009, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.013, val_acc:0.978]
Epoch [77/120    avg_loss:0.015, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.978]
Epoch [80/120    avg_loss:0.013, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.010, val_acc:0.978]
Epoch [83/120    avg_loss:0.014, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.978]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.014, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.978]
Epoch [89/120    avg_loss:0.015, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.978]
Epoch [92/120    avg_loss:0.011, val_acc:0.978]
Epoch [93/120    avg_loss:0.013, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.012, val_acc:0.978]
Epoch [97/120    avg_loss:0.013, val_acc:0.978]
Epoch [98/120    avg_loss:0.013, val_acc:0.978]
Epoch [99/120    avg_loss:0.015, val_acc:0.978]
Epoch [100/120    avg_loss:0.016, val_acc:0.978]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.978]
Epoch [103/120    avg_loss:0.010, val_acc:0.978]
Epoch [104/120    avg_loss:0.011, val_acc:0.978]
Epoch [105/120    avg_loss:0.013, val_acc:0.978]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.016, val_acc:0.978]
Epoch [108/120    avg_loss:0.018, val_acc:0.978]
Epoch [109/120    avg_loss:0.011, val_acc:0.978]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.012, val_acc:0.978]
Epoch [112/120    avg_loss:0.014, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.978]
Epoch [114/120    avg_loss:0.014, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.016, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.016, val_acc:0.978]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6284     0     8     1     0     1    11   126     1]
 [    0     2 18035     0    42     0     7     0     4     0]
 [    0     1     0  1963     0     0     0     0    65     7]
 [    0    23    19     0  2910     0     3     5    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    21     0     0     0  4851     0     6     0]
 [    0    11     0     0     0     0     0  1277     0     2]
 [    0     1     0     6    57     0     1     0  3502     4]
 [    0     0     0     0    10    17     0     0     0   892]]

Accuracy:
98.85763863784253

F1 scores:
[       nan 0.98541634 0.99737315 0.97832046 0.97129506 0.99352874
 0.9959963  0.98877274 0.96129564 0.97753425]

Kappa:
0.9848688583568302
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb56f46b940>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.906, val_acc:0.505]
Epoch [2/120    avg_loss:1.400, val_acc:0.630]
Epoch [3/120    avg_loss:1.117, val_acc:0.782]
Epoch [4/120    avg_loss:0.910, val_acc:0.678]
Epoch [5/120    avg_loss:0.721, val_acc:0.652]
Epoch [6/120    avg_loss:0.597, val_acc:0.594]
Epoch [7/120    avg_loss:0.508, val_acc:0.772]
Epoch [8/120    avg_loss:0.426, val_acc:0.708]
Epoch [9/120    avg_loss:0.354, val_acc:0.848]
Epoch [10/120    avg_loss:0.363, val_acc:0.802]
Epoch [11/120    avg_loss:0.288, val_acc:0.904]
Epoch [12/120    avg_loss:0.240, val_acc:0.796]
Epoch [13/120    avg_loss:0.300, val_acc:0.893]
Epoch [14/120    avg_loss:0.215, val_acc:0.853]
Epoch [15/120    avg_loss:0.179, val_acc:0.933]
Epoch [16/120    avg_loss:0.158, val_acc:0.922]
Epoch [17/120    avg_loss:0.177, val_acc:0.904]
Epoch [18/120    avg_loss:0.171, val_acc:0.902]
Epoch [19/120    avg_loss:0.157, val_acc:0.953]
Epoch [20/120    avg_loss:0.125, val_acc:0.893]
Epoch [21/120    avg_loss:0.150, val_acc:0.927]
Epoch [22/120    avg_loss:0.138, val_acc:0.953]
Epoch [23/120    avg_loss:0.098, val_acc:0.966]
Epoch [24/120    avg_loss:0.071, val_acc:0.962]
Epoch [25/120    avg_loss:0.079, val_acc:0.969]
Epoch [26/120    avg_loss:0.090, val_acc:0.957]
Epoch [27/120    avg_loss:0.061, val_acc:0.966]
Epoch [28/120    avg_loss:0.088, val_acc:0.965]
Epoch [29/120    avg_loss:0.062, val_acc:0.963]
Epoch [30/120    avg_loss:0.058, val_acc:0.973]
Epoch [31/120    avg_loss:0.060, val_acc:0.973]
Epoch [32/120    avg_loss:0.050, val_acc:0.959]
Epoch [33/120    avg_loss:0.066, val_acc:0.961]
Epoch [34/120    avg_loss:0.055, val_acc:0.976]
Epoch [35/120    avg_loss:0.040, val_acc:0.975]
Epoch [36/120    avg_loss:0.032, val_acc:0.982]
Epoch [37/120    avg_loss:0.042, val_acc:0.958]
Epoch [38/120    avg_loss:0.098, val_acc:0.927]
Epoch [39/120    avg_loss:0.118, val_acc:0.970]
Epoch [40/120    avg_loss:0.046, val_acc:0.975]
Epoch [41/120    avg_loss:0.045, val_acc:0.973]
Epoch [42/120    avg_loss:0.036, val_acc:0.945]
Epoch [43/120    avg_loss:0.040, val_acc:0.965]
Epoch [44/120    avg_loss:0.037, val_acc:0.975]
Epoch [45/120    avg_loss:0.034, val_acc:0.978]
Epoch [46/120    avg_loss:0.027, val_acc:0.980]
Epoch [47/120    avg_loss:0.017, val_acc:0.981]
Epoch [48/120    avg_loss:0.017, val_acc:0.986]
Epoch [49/120    avg_loss:0.018, val_acc:0.985]
Epoch [50/120    avg_loss:0.020, val_acc:0.979]
Epoch [51/120    avg_loss:0.025, val_acc:0.983]
Epoch [52/120    avg_loss:0.019, val_acc:0.991]
Epoch [53/120    avg_loss:0.017, val_acc:0.977]
Epoch [54/120    avg_loss:0.016, val_acc:0.984]
Epoch [55/120    avg_loss:0.047, val_acc:0.972]
Epoch [56/120    avg_loss:0.021, val_acc:0.980]
Epoch [57/120    avg_loss:0.024, val_acc:0.983]
Epoch [58/120    avg_loss:0.030, val_acc:0.979]
Epoch [59/120    avg_loss:0.026, val_acc:0.977]
Epoch [60/120    avg_loss:0.038, val_acc:0.983]
Epoch [61/120    avg_loss:0.028, val_acc:0.980]
Epoch [62/120    avg_loss:0.018, val_acc:0.975]
Epoch [63/120    avg_loss:0.019, val_acc:0.988]
Epoch [64/120    avg_loss:0.015, val_acc:0.987]
Epoch [65/120    avg_loss:0.015, val_acc:0.987]
Epoch [66/120    avg_loss:0.015, val_acc:0.988]
Epoch [67/120    avg_loss:0.008, val_acc:0.989]
Epoch [68/120    avg_loss:0.009, val_acc:0.988]
Epoch [69/120    avg_loss:0.009, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.987]
Epoch [71/120    avg_loss:0.009, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.988]
Epoch [73/120    avg_loss:0.008, val_acc:0.989]
Epoch [74/120    avg_loss:0.011, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.007, val_acc:0.989]
Epoch [77/120    avg_loss:0.007, val_acc:0.989]
Epoch [78/120    avg_loss:0.008, val_acc:0.989]
Epoch [79/120    avg_loss:0.009, val_acc:0.989]
Epoch [80/120    avg_loss:0.007, val_acc:0.989]
Epoch [81/120    avg_loss:0.006, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.007, val_acc:0.989]
Epoch [85/120    avg_loss:0.008, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.008, val_acc:0.989]
Epoch [88/120    avg_loss:0.007, val_acc:0.989]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.989]
Epoch [91/120    avg_loss:0.009, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.008, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.007, val_acc:0.989]
Epoch [107/120    avg_loss:0.010, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.989]
Epoch [119/120    avg_loss:0.012, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6338     0     6     0     0     0     0    87     1]
 [    0     0 17973     0    24     0    86     0     7     0]
 [    0     0     0  1984     0     0     0     0    46     6]
 [    0    22    12     0  2923     0    11     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     0     6     0  4856     0     4     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     9     0    17    56     0     0     0  3488     1]
 [    0     0     0     0     0    16     0     0     0   903]]

Accuracy:
98.95645048562409

F1 scores:
[       nan 0.99023514 0.99609278 0.98144942 0.97742852 0.99390708
 0.98789543 1.         0.96794783 0.98688525]

Kappa:
0.9861874890613607
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c6b28f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.933, val_acc:0.273]
Epoch [2/120    avg_loss:1.458, val_acc:0.298]
Epoch [3/120    avg_loss:1.156, val_acc:0.429]
Epoch [4/120    avg_loss:0.977, val_acc:0.502]
Epoch [5/120    avg_loss:0.788, val_acc:0.702]
Epoch [6/120    avg_loss:0.653, val_acc:0.649]
Epoch [7/120    avg_loss:0.540, val_acc:0.714]
Epoch [8/120    avg_loss:0.447, val_acc:0.771]
Epoch [9/120    avg_loss:0.395, val_acc:0.770]
Epoch [10/120    avg_loss:0.327, val_acc:0.789]
Epoch [11/120    avg_loss:0.298, val_acc:0.783]
Epoch [12/120    avg_loss:0.292, val_acc:0.889]
Epoch [13/120    avg_loss:0.206, val_acc:0.895]
Epoch [14/120    avg_loss:0.200, val_acc:0.943]
Epoch [15/120    avg_loss:0.169, val_acc:0.942]
Epoch [16/120    avg_loss:0.163, val_acc:0.927]
Epoch [17/120    avg_loss:0.162, val_acc:0.946]
Epoch [18/120    avg_loss:0.166, val_acc:0.899]
Epoch [19/120    avg_loss:0.200, val_acc:0.878]
Epoch [20/120    avg_loss:0.143, val_acc:0.930]
Epoch [21/120    avg_loss:0.161, val_acc:0.960]
Epoch [22/120    avg_loss:0.114, val_acc:0.947]
Epoch [23/120    avg_loss:0.120, val_acc:0.779]
Epoch [24/120    avg_loss:0.498, val_acc:0.863]
Epoch [25/120    avg_loss:0.258, val_acc:0.900]
Epoch [26/120    avg_loss:0.158, val_acc:0.956]
Epoch [27/120    avg_loss:0.120, val_acc:0.972]
Epoch [28/120    avg_loss:0.094, val_acc:0.965]
Epoch [29/120    avg_loss:0.072, val_acc:0.975]
Epoch [30/120    avg_loss:0.064, val_acc:0.972]
Epoch [31/120    avg_loss:0.054, val_acc:0.960]
Epoch [32/120    avg_loss:0.050, val_acc:0.956]
Epoch [33/120    avg_loss:0.059, val_acc:0.970]
Epoch [34/120    avg_loss:0.044, val_acc:0.981]
Epoch [35/120    avg_loss:0.045, val_acc:0.978]
Epoch [36/120    avg_loss:0.052, val_acc:0.964]
Epoch [37/120    avg_loss:0.052, val_acc:0.982]
Epoch [38/120    avg_loss:0.076, val_acc:0.981]
Epoch [39/120    avg_loss:0.054, val_acc:0.971]
Epoch [40/120    avg_loss:0.050, val_acc:0.965]
Epoch [41/120    avg_loss:0.034, val_acc:0.980]
Epoch [42/120    avg_loss:0.031, val_acc:0.988]
Epoch [43/120    avg_loss:0.019, val_acc:0.973]
Epoch [44/120    avg_loss:0.025, val_acc:0.983]
Epoch [45/120    avg_loss:0.042, val_acc:0.983]
Epoch [46/120    avg_loss:0.060, val_acc:0.983]
Epoch [47/120    avg_loss:0.080, val_acc:0.978]
Epoch [48/120    avg_loss:0.039, val_acc:0.987]
Epoch [49/120    avg_loss:0.021, val_acc:0.979]
Epoch [50/120    avg_loss:0.028, val_acc:0.984]
Epoch [51/120    avg_loss:0.018, val_acc:0.986]
Epoch [52/120    avg_loss:0.015, val_acc:0.990]
Epoch [53/120    avg_loss:0.011, val_acc:0.986]
Epoch [54/120    avg_loss:0.012, val_acc:0.988]
Epoch [55/120    avg_loss:0.015, val_acc:0.992]
Epoch [56/120    avg_loss:0.015, val_acc:0.989]
Epoch [57/120    avg_loss:0.012, val_acc:0.980]
Epoch [58/120    avg_loss:0.015, val_acc:0.975]
Epoch [59/120    avg_loss:0.019, val_acc:0.981]
Epoch [60/120    avg_loss:0.016, val_acc:0.980]
Epoch [61/120    avg_loss:0.027, val_acc:0.988]
Epoch [62/120    avg_loss:0.011, val_acc:0.992]
Epoch [63/120    avg_loss:0.052, val_acc:0.928]
Epoch [64/120    avg_loss:0.048, val_acc:0.973]
Epoch [65/120    avg_loss:0.029, val_acc:0.986]
Epoch [66/120    avg_loss:0.016, val_acc:0.990]
Epoch [67/120    avg_loss:0.007, val_acc:0.992]
Epoch [68/120    avg_loss:0.007, val_acc:0.993]
Epoch [69/120    avg_loss:0.008, val_acc:0.993]
Epoch [70/120    avg_loss:0.015, val_acc:0.973]
Epoch [71/120    avg_loss:0.025, val_acc:0.974]
Epoch [72/120    avg_loss:0.021, val_acc:0.978]
Epoch [73/120    avg_loss:0.028, val_acc:0.957]
Epoch [74/120    avg_loss:0.021, val_acc:0.976]
Epoch [75/120    avg_loss:0.015, val_acc:0.969]
Epoch [76/120    avg_loss:0.014, val_acc:0.988]
Epoch [77/120    avg_loss:0.046, val_acc:0.965]
Epoch [78/120    avg_loss:0.026, val_acc:0.991]
Epoch [79/120    avg_loss:0.013, val_acc:0.983]
Epoch [80/120    avg_loss:0.016, val_acc:0.991]
Epoch [81/120    avg_loss:0.025, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.010, val_acc:0.993]
Epoch [84/120    avg_loss:0.008, val_acc:0.993]
Epoch [85/120    avg_loss:0.010, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.008, val_acc:0.993]
Epoch [88/120    avg_loss:0.006, val_acc:0.993]
Epoch [89/120    avg_loss:0.006, val_acc:0.993]
Epoch [90/120    avg_loss:0.006, val_acc:0.993]
Epoch [91/120    avg_loss:0.005, val_acc:0.994]
Epoch [92/120    avg_loss:0.008, val_acc:0.993]
Epoch [93/120    avg_loss:0.005, val_acc:0.992]
Epoch [94/120    avg_loss:0.005, val_acc:0.993]
Epoch [95/120    avg_loss:0.007, val_acc:0.994]
Epoch [96/120    avg_loss:0.006, val_acc:0.993]
Epoch [97/120    avg_loss:0.006, val_acc:0.993]
Epoch [98/120    avg_loss:0.005, val_acc:0.993]
Epoch [99/120    avg_loss:0.006, val_acc:0.993]
Epoch [100/120    avg_loss:0.004, val_acc:0.993]
Epoch [101/120    avg_loss:0.005, val_acc:0.993]
Epoch [102/120    avg_loss:0.006, val_acc:0.993]
Epoch [103/120    avg_loss:0.007, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.993]
Epoch [105/120    avg_loss:0.005, val_acc:0.993]
Epoch [106/120    avg_loss:0.006, val_acc:0.993]
Epoch [107/120    avg_loss:0.005, val_acc:0.993]
Epoch [108/120    avg_loss:0.007, val_acc:0.993]
Epoch [109/120    avg_loss:0.005, val_acc:0.993]
Epoch [110/120    avg_loss:0.005, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.993]
Epoch [112/120    avg_loss:0.005, val_acc:0.993]
Epoch [113/120    avg_loss:0.005, val_acc:0.993]
Epoch [114/120    avg_loss:0.006, val_acc:0.993]
Epoch [115/120    avg_loss:0.006, val_acc:0.993]
Epoch [116/120    avg_loss:0.005, val_acc:0.993]
Epoch [117/120    avg_loss:0.006, val_acc:0.993]
Epoch [118/120    avg_loss:0.005, val_acc:0.993]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.008, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     0     1     0    11     0    29     1]
 [    0     0 18066     0    18     0     2     0     4     0]
 [    0     3     0  1987     0     0     0     0    43     3]
 [    0    27     3     0  2922     0     7     0    12     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     2     0     0  4873     0     3     0]
 [    0     7     0     0     0     0     0  1281     0     2]
 [    0     1     0    44    53     0     0     0  3465     8]
 [    0     1     0     0     6    38     0     0     0   874]]

Accuracy:
99.20468512761188

F1 scores:
[       nan 0.99370189 0.9992533  0.97665274 0.97856664 0.98564955
 0.99744141 0.99649942 0.97235864 0.96681416]

Kappa:
0.9894639658539448
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc57662a978>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.868, val_acc:0.534]
Epoch [2/120    avg_loss:1.410, val_acc:0.578]
Epoch [3/120    avg_loss:1.114, val_acc:0.643]
Epoch [4/120    avg_loss:0.879, val_acc:0.677]
Epoch [5/120    avg_loss:0.708, val_acc:0.720]
Epoch [6/120    avg_loss:0.548, val_acc:0.713]
Epoch [7/120    avg_loss:0.496, val_acc:0.738]
Epoch [8/120    avg_loss:0.483, val_acc:0.790]
Epoch [9/120    avg_loss:0.424, val_acc:0.828]
Epoch [10/120    avg_loss:0.382, val_acc:0.834]
Epoch [11/120    avg_loss:0.334, val_acc:0.819]
Epoch [12/120    avg_loss:0.291, val_acc:0.902]
Epoch [13/120    avg_loss:0.269, val_acc:0.816]
Epoch [14/120    avg_loss:0.246, val_acc:0.891]
Epoch [15/120    avg_loss:0.195, val_acc:0.937]
Epoch [16/120    avg_loss:0.235, val_acc:0.877]
Epoch [17/120    avg_loss:0.211, val_acc:0.930]
Epoch [18/120    avg_loss:0.164, val_acc:0.946]
Epoch [19/120    avg_loss:0.153, val_acc:0.941]
Epoch [20/120    avg_loss:0.168, val_acc:0.906]
Epoch [21/120    avg_loss:0.153, val_acc:0.942]
Epoch [22/120    avg_loss:0.100, val_acc:0.901]
Epoch [23/120    avg_loss:0.114, val_acc:0.965]
Epoch [24/120    avg_loss:0.091, val_acc:0.963]
Epoch [25/120    avg_loss:0.120, val_acc:0.945]
Epoch [26/120    avg_loss:0.099, val_acc:0.912]
Epoch [27/120    avg_loss:0.110, val_acc:0.922]
Epoch [28/120    avg_loss:0.105, val_acc:0.936]
Epoch [29/120    avg_loss:0.113, val_acc:0.947]
Epoch [30/120    avg_loss:0.071, val_acc:0.970]
Epoch [31/120    avg_loss:0.066, val_acc:0.943]
Epoch [32/120    avg_loss:0.069, val_acc:0.955]
Epoch [33/120    avg_loss:0.072, val_acc:0.968]
Epoch [34/120    avg_loss:0.050, val_acc:0.961]
Epoch [35/120    avg_loss:0.046, val_acc:0.980]
Epoch [36/120    avg_loss:0.043, val_acc:0.975]
Epoch [37/120    avg_loss:0.045, val_acc:0.976]
Epoch [38/120    avg_loss:0.035, val_acc:0.976]
Epoch [39/120    avg_loss:0.034, val_acc:0.979]
Epoch [40/120    avg_loss:0.029, val_acc:0.978]
Epoch [41/120    avg_loss:0.031, val_acc:0.976]
Epoch [42/120    avg_loss:0.032, val_acc:0.966]
Epoch [43/120    avg_loss:0.027, val_acc:0.977]
Epoch [44/120    avg_loss:0.088, val_acc:0.966]
Epoch [45/120    avg_loss:0.051, val_acc:0.959]
Epoch [46/120    avg_loss:0.031, val_acc:0.973]
Epoch [47/120    avg_loss:0.025, val_acc:0.978]
Epoch [48/120    avg_loss:0.022, val_acc:0.980]
Epoch [49/120    avg_loss:0.044, val_acc:0.977]
Epoch [50/120    avg_loss:0.068, val_acc:0.939]
Epoch [51/120    avg_loss:0.034, val_acc:0.980]
Epoch [52/120    avg_loss:0.029, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.980]
Epoch [54/120    avg_loss:0.025, val_acc:0.982]
Epoch [55/120    avg_loss:0.039, val_acc:0.973]
Epoch [56/120    avg_loss:0.020, val_acc:0.981]
Epoch [57/120    avg_loss:0.029, val_acc:0.978]
Epoch [58/120    avg_loss:0.055, val_acc:0.976]
Epoch [59/120    avg_loss:0.032, val_acc:0.977]
Epoch [60/120    avg_loss:0.027, val_acc:0.981]
Epoch [61/120    avg_loss:0.020, val_acc:0.966]
Epoch [62/120    avg_loss:0.020, val_acc:0.977]
Epoch [63/120    avg_loss:0.019, val_acc:0.984]
Epoch [64/120    avg_loss:0.068, val_acc:0.961]
Epoch [65/120    avg_loss:0.037, val_acc:0.978]
Epoch [66/120    avg_loss:0.015, val_acc:0.982]
Epoch [67/120    avg_loss:0.037, val_acc:0.974]
Epoch [68/120    avg_loss:0.033, val_acc:0.963]
Epoch [69/120    avg_loss:0.022, val_acc:0.983]
Epoch [70/120    avg_loss:0.027, val_acc:0.977]
Epoch [71/120    avg_loss:0.022, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.977]
Epoch [73/120    avg_loss:0.012, val_acc:0.973]
Epoch [74/120    avg_loss:0.010, val_acc:0.977]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.007, val_acc:0.979]
Epoch [78/120    avg_loss:0.007, val_acc:0.979]
Epoch [79/120    avg_loss:0.008, val_acc:0.981]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.008, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.983]
Epoch [86/120    avg_loss:0.005, val_acc:0.983]
Epoch [87/120    avg_loss:0.005, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.007, val_acc:0.982]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.982]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.005, val_acc:0.982]
Epoch [94/120    avg_loss:0.004, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.982]
Epoch [101/120    avg_loss:0.005, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.982]
Epoch [105/120    avg_loss:0.006, val_acc:0.982]
Epoch [106/120    avg_loss:0.006, val_acc:0.982]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.004, val_acc:0.982]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.005, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.982]
Epoch [118/120    avg_loss:0.007, val_acc:0.982]
Epoch [119/120    avg_loss:0.006, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6368     0     1     0     0     0    30    33     0]
 [    0     7 18049     0    17     0    16     0     1     0]
 [    0     0     0  1974     0     0     0     0    58     4]
 [    0    27     2     0  2923     0     9     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4877     0     0     0]
 [    0     4     0     0     0     0     2  1281     0     3]
 [    0     4     0    49    44     0     0     0  3464    10]
 [    0     0     0     0     2    23     0     0     0   894]]

Accuracy:
99.1372038657123

F1 scores:
[       nan 0.99174583 0.99878258 0.97241379 0.98120175 0.99126472
 0.9971376  0.98500577 0.97057999 0.97704918]

Kappa:
0.9885731416750192
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5656b4e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.992, val_acc:0.378]
Epoch [2/120    avg_loss:1.479, val_acc:0.618]
Epoch [3/120    avg_loss:1.149, val_acc:0.639]
Epoch [4/120    avg_loss:0.920, val_acc:0.612]
Epoch [5/120    avg_loss:0.695, val_acc:0.711]
Epoch [6/120    avg_loss:0.564, val_acc:0.691]
Epoch [7/120    avg_loss:0.467, val_acc:0.801]
Epoch [8/120    avg_loss:0.422, val_acc:0.832]
Epoch [9/120    avg_loss:0.372, val_acc:0.876]
Epoch [10/120    avg_loss:0.298, val_acc:0.925]
Epoch [11/120    avg_loss:0.251, val_acc:0.899]
Epoch [12/120    avg_loss:0.256, val_acc:0.889]
Epoch [13/120    avg_loss:0.231, val_acc:0.913]
Epoch [14/120    avg_loss:0.195, val_acc:0.935]
Epoch [15/120    avg_loss:0.174, val_acc:0.888]
Epoch [16/120    avg_loss:0.115, val_acc:0.918]
Epoch [17/120    avg_loss:0.118, val_acc:0.934]
Epoch [18/120    avg_loss:0.129, val_acc:0.864]
Epoch [19/120    avg_loss:0.126, val_acc:0.874]
Epoch [20/120    avg_loss:0.100, val_acc:0.968]
Epoch [21/120    avg_loss:0.082, val_acc:0.950]
Epoch [22/120    avg_loss:0.069, val_acc:0.973]
Epoch [23/120    avg_loss:0.075, val_acc:0.961]
Epoch [24/120    avg_loss:0.096, val_acc:0.969]
Epoch [25/120    avg_loss:0.066, val_acc:0.961]
Epoch [26/120    avg_loss:0.077, val_acc:0.949]
Epoch [27/120    avg_loss:0.054, val_acc:0.962]
Epoch [28/120    avg_loss:0.087, val_acc:0.793]
Epoch [29/120    avg_loss:0.088, val_acc:0.963]
Epoch [30/120    avg_loss:0.048, val_acc:0.983]
Epoch [31/120    avg_loss:0.076, val_acc:0.969]
Epoch [32/120    avg_loss:0.063, val_acc:0.940]
Epoch [33/120    avg_loss:0.059, val_acc:0.974]
Epoch [34/120    avg_loss:0.059, val_acc:0.973]
Epoch [35/120    avg_loss:0.048, val_acc:0.882]
Epoch [36/120    avg_loss:0.048, val_acc:0.955]
Epoch [37/120    avg_loss:0.028, val_acc:0.979]
Epoch [38/120    avg_loss:0.033, val_acc:0.962]
Epoch [39/120    avg_loss:0.034, val_acc:0.969]
Epoch [40/120    avg_loss:0.041, val_acc:0.957]
Epoch [41/120    avg_loss:0.036, val_acc:0.982]
Epoch [42/120    avg_loss:0.035, val_acc:0.946]
Epoch [43/120    avg_loss:0.026, val_acc:0.973]
Epoch [44/120    avg_loss:0.024, val_acc:0.978]
Epoch [45/120    avg_loss:0.015, val_acc:0.980]
Epoch [46/120    avg_loss:0.018, val_acc:0.982]
Epoch [47/120    avg_loss:0.019, val_acc:0.979]
Epoch [48/120    avg_loss:0.015, val_acc:0.980]
Epoch [49/120    avg_loss:0.015, val_acc:0.980]
Epoch [50/120    avg_loss:0.015, val_acc:0.980]
Epoch [51/120    avg_loss:0.013, val_acc:0.980]
Epoch [52/120    avg_loss:0.015, val_acc:0.981]
Epoch [53/120    avg_loss:0.012, val_acc:0.983]
Epoch [54/120    avg_loss:0.013, val_acc:0.980]
Epoch [55/120    avg_loss:0.012, val_acc:0.983]
Epoch [56/120    avg_loss:0.014, val_acc:0.983]
Epoch [57/120    avg_loss:0.014, val_acc:0.982]
Epoch [58/120    avg_loss:0.018, val_acc:0.982]
Epoch [59/120    avg_loss:0.013, val_acc:0.980]
Epoch [60/120    avg_loss:0.011, val_acc:0.981]
Epoch [61/120    avg_loss:0.011, val_acc:0.983]
Epoch [62/120    avg_loss:0.011, val_acc:0.984]
Epoch [63/120    avg_loss:0.012, val_acc:0.982]
Epoch [64/120    avg_loss:0.010, val_acc:0.986]
Epoch [65/120    avg_loss:0.011, val_acc:0.985]
Epoch [66/120    avg_loss:0.013, val_acc:0.986]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.016, val_acc:0.984]
Epoch [69/120    avg_loss:0.012, val_acc:0.981]
Epoch [70/120    avg_loss:0.014, val_acc:0.980]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.011, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.981]
Epoch [74/120    avg_loss:0.014, val_acc:0.986]
Epoch [75/120    avg_loss:0.010, val_acc:0.985]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.010, val_acc:0.986]
Epoch [92/120    avg_loss:0.018, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.009, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.013, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.986]
Epoch [110/120    avg_loss:0.013, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6348     0    12     1     0     9     0    60     2]
 [    0     0 18004     0    43     0    30     0    13     0]
 [    0     3     0  2011     0     0     0     0    19     3]
 [    0    26     5     1  2917     0    12     0    10     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4872     0     0     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0     4     0    29    54     0     0     0  3480     4]
 [    0     0     0     0     8    12     0     0     0   899]]

Accuracy:
99.11310341503386

F1 scores:
[       nan 0.99079132 0.99731339 0.98361458 0.97314429 0.99542334
 0.99418427 0.99961225 0.97301831 0.98358862]

Kappa:
0.9882591827040729
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f90960929e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.970, val_acc:0.260]
Epoch [2/120    avg_loss:1.453, val_acc:0.328]
Epoch [3/120    avg_loss:1.181, val_acc:0.418]
Epoch [4/120    avg_loss:0.967, val_acc:0.613]
Epoch [5/120    avg_loss:0.762, val_acc:0.619]
Epoch [6/120    avg_loss:0.604, val_acc:0.730]
Epoch [7/120    avg_loss:0.519, val_acc:0.794]
Epoch [8/120    avg_loss:0.455, val_acc:0.784]
Epoch [9/120    avg_loss:0.414, val_acc:0.846]
Epoch [10/120    avg_loss:0.377, val_acc:0.848]
Epoch [11/120    avg_loss:0.288, val_acc:0.895]
Epoch [12/120    avg_loss:0.269, val_acc:0.866]
Epoch [13/120    avg_loss:0.223, val_acc:0.923]
Epoch [14/120    avg_loss:0.199, val_acc:0.912]
Epoch [15/120    avg_loss:0.179, val_acc:0.929]
Epoch [16/120    avg_loss:0.173, val_acc:0.925]
Epoch [17/120    avg_loss:0.199, val_acc:0.936]
Epoch [18/120    avg_loss:0.173, val_acc:0.905]
Epoch [19/120    avg_loss:0.161, val_acc:0.912]
Epoch [20/120    avg_loss:0.136, val_acc:0.929]
Epoch [21/120    avg_loss:0.121, val_acc:0.952]
Epoch [22/120    avg_loss:0.113, val_acc:0.945]
Epoch [23/120    avg_loss:0.131, val_acc:0.956]
Epoch [24/120    avg_loss:0.089, val_acc:0.953]
Epoch [25/120    avg_loss:0.116, val_acc:0.947]
Epoch [26/120    avg_loss:0.072, val_acc:0.943]
Epoch [27/120    avg_loss:0.092, val_acc:0.949]
Epoch [28/120    avg_loss:0.081, val_acc:0.924]
Epoch [29/120    avg_loss:0.122, val_acc:0.873]
Epoch [30/120    avg_loss:0.108, val_acc:0.956]
Epoch [31/120    avg_loss:0.096, val_acc:0.970]
Epoch [32/120    avg_loss:0.076, val_acc:0.948]
Epoch [33/120    avg_loss:0.081, val_acc:0.937]
Epoch [34/120    avg_loss:0.086, val_acc:0.930]
Epoch [35/120    avg_loss:0.054, val_acc:0.975]
Epoch [36/120    avg_loss:0.052, val_acc:0.978]
Epoch [37/120    avg_loss:0.033, val_acc:0.983]
Epoch [38/120    avg_loss:0.052, val_acc:0.939]
Epoch [39/120    avg_loss:0.069, val_acc:0.953]
Epoch [40/120    avg_loss:0.071, val_acc:0.973]
Epoch [41/120    avg_loss:0.033, val_acc:0.978]
Epoch [42/120    avg_loss:0.049, val_acc:0.981]
Epoch [43/120    avg_loss:0.035, val_acc:0.986]
Epoch [44/120    avg_loss:0.030, val_acc:0.950]
Epoch [45/120    avg_loss:0.028, val_acc:0.970]
Epoch [46/120    avg_loss:0.027, val_acc:0.987]
Epoch [47/120    avg_loss:0.023, val_acc:0.983]
Epoch [48/120    avg_loss:0.028, val_acc:0.988]
Epoch [49/120    avg_loss:0.022, val_acc:0.984]
Epoch [50/120    avg_loss:0.017, val_acc:0.989]
Epoch [51/120    avg_loss:0.020, val_acc:0.989]
Epoch [52/120    avg_loss:0.016, val_acc:0.982]
Epoch [53/120    avg_loss:0.020, val_acc:0.986]
Epoch [54/120    avg_loss:0.021, val_acc:0.987]
Epoch [55/120    avg_loss:0.052, val_acc:0.962]
Epoch [56/120    avg_loss:0.046, val_acc:0.973]
Epoch [57/120    avg_loss:0.061, val_acc:0.977]
Epoch [58/120    avg_loss:0.029, val_acc:0.982]
Epoch [59/120    avg_loss:0.024, val_acc:0.967]
Epoch [60/120    avg_loss:0.029, val_acc:0.986]
Epoch [61/120    avg_loss:0.027, val_acc:0.981]
Epoch [62/120    avg_loss:0.026, val_acc:0.988]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.032, val_acc:0.989]
Epoch [65/120    avg_loss:0.011, val_acc:0.990]
Epoch [66/120    avg_loss:0.017, val_acc:0.988]
Epoch [67/120    avg_loss:0.017, val_acc:0.977]
Epoch [68/120    avg_loss:0.021, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.987]
Epoch [70/120    avg_loss:0.013, val_acc:0.978]
Epoch [71/120    avg_loss:0.025, val_acc:0.987]
Epoch [72/120    avg_loss:0.012, val_acc:0.980]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.985]
Epoch [75/120    avg_loss:0.031, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.987]
Epoch [77/120    avg_loss:0.016, val_acc:0.987]
Epoch [78/120    avg_loss:0.141, val_acc:0.970]
Epoch [79/120    avg_loss:0.044, val_acc:0.980]
Epoch [80/120    avg_loss:0.033, val_acc:0.981]
Epoch [81/120    avg_loss:0.030, val_acc:0.982]
Epoch [82/120    avg_loss:0.019, val_acc:0.983]
Epoch [83/120    avg_loss:0.019, val_acc:0.982]
Epoch [84/120    avg_loss:0.021, val_acc:0.982]
Epoch [85/120    avg_loss:0.018, val_acc:0.982]
Epoch [86/120    avg_loss:0.017, val_acc:0.984]
Epoch [87/120    avg_loss:0.016, val_acc:0.983]
Epoch [88/120    avg_loss:0.017, val_acc:0.983]
Epoch [89/120    avg_loss:0.012, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.014, val_acc:0.983]
Epoch [92/120    avg_loss:0.014, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.983]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.016, val_acc:0.984]
Epoch [96/120    avg_loss:0.012, val_acc:0.984]
Epoch [97/120    avg_loss:0.014, val_acc:0.984]
Epoch [98/120    avg_loss:0.017, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.984]
Epoch [102/120    avg_loss:0.012, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.984]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.018, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.012, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.013, val_acc:0.984]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.014, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.984]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6303     0     1     2     0     2    37    83     4]
 [    0     0 18003     0    50     0    36     0     0     1]
 [    0     4     0  1951     0     0     0     0    75     6]
 [    0    15     0     4  2937     0     6     0     9     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     3     0     0  4861     0     6     0]
 [    0    40     0     0     0     4     2  1238     1     5]
 [    0    17     0    44    63     0     1     0  3445     1]
 [    0     0     0     0    16    29     0     0     0   874]]

Accuracy:
98.61181404092257

F1 scores:
[       nan 0.98399813 0.99736849 0.96608071 0.97251656 0.98751419
 0.99346004 0.96530214 0.95827538 0.96521259]

Kappa:
0.9816231680140498
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f069c56d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.942, val_acc:0.163]
Epoch [2/120    avg_loss:1.504, val_acc:0.277]
Epoch [3/120    avg_loss:1.253, val_acc:0.463]
Epoch [4/120    avg_loss:1.088, val_acc:0.486]
Epoch [5/120    avg_loss:0.914, val_acc:0.616]
Epoch [6/120    avg_loss:0.744, val_acc:0.600]
Epoch [7/120    avg_loss:0.583, val_acc:0.716]
Epoch [8/120    avg_loss:0.546, val_acc:0.705]
Epoch [9/120    avg_loss:0.432, val_acc:0.772]
Epoch [10/120    avg_loss:0.351, val_acc:0.858]
Epoch [11/120    avg_loss:0.337, val_acc:0.841]
Epoch [12/120    avg_loss:0.303, val_acc:0.861]
Epoch [13/120    avg_loss:0.235, val_acc:0.882]
Epoch [14/120    avg_loss:0.215, val_acc:0.902]
Epoch [15/120    avg_loss:0.202, val_acc:0.928]
Epoch [16/120    avg_loss:0.165, val_acc:0.924]
Epoch [17/120    avg_loss:0.182, val_acc:0.895]
Epoch [18/120    avg_loss:0.188, val_acc:0.908]
Epoch [19/120    avg_loss:0.161, val_acc:0.953]
Epoch [20/120    avg_loss:0.105, val_acc:0.968]
Epoch [21/120    avg_loss:0.150, val_acc:0.943]
Epoch [22/120    avg_loss:0.131, val_acc:0.913]
Epoch [23/120    avg_loss:0.098, val_acc:0.961]
Epoch [24/120    avg_loss:0.134, val_acc:0.941]
Epoch [25/120    avg_loss:0.086, val_acc:0.958]
Epoch [26/120    avg_loss:0.093, val_acc:0.969]
Epoch [27/120    avg_loss:0.083, val_acc:0.946]
Epoch [28/120    avg_loss:0.068, val_acc:0.959]
Epoch [29/120    avg_loss:0.057, val_acc:0.969]
Epoch [30/120    avg_loss:0.080, val_acc:0.962]
Epoch [31/120    avg_loss:0.065, val_acc:0.969]
Epoch [32/120    avg_loss:0.069, val_acc:0.978]
Epoch [33/120    avg_loss:0.117, val_acc:0.939]
Epoch [34/120    avg_loss:0.091, val_acc:0.947]
Epoch [35/120    avg_loss:0.049, val_acc:0.978]
Epoch [36/120    avg_loss:0.057, val_acc:0.919]
Epoch [37/120    avg_loss:0.059, val_acc:0.972]
Epoch [38/120    avg_loss:0.038, val_acc:0.970]
Epoch [39/120    avg_loss:0.145, val_acc:0.918]
Epoch [40/120    avg_loss:0.117, val_acc:0.950]
Epoch [41/120    avg_loss:0.071, val_acc:0.970]
Epoch [42/120    avg_loss:0.047, val_acc:0.973]
Epoch [43/120    avg_loss:0.061, val_acc:0.953]
Epoch [44/120    avg_loss:0.048, val_acc:0.976]
Epoch [45/120    avg_loss:0.048, val_acc:0.970]
Epoch [46/120    avg_loss:0.040, val_acc:0.978]
Epoch [47/120    avg_loss:0.045, val_acc:0.969]
Epoch [48/120    avg_loss:0.026, val_acc:0.982]
Epoch [49/120    avg_loss:0.027, val_acc:0.982]
Epoch [50/120    avg_loss:0.025, val_acc:0.981]
Epoch [51/120    avg_loss:0.067, val_acc:0.973]
Epoch [52/120    avg_loss:0.035, val_acc:0.970]
Epoch [53/120    avg_loss:0.032, val_acc:0.968]
Epoch [54/120    avg_loss:0.026, val_acc:0.972]
Epoch [55/120    avg_loss:0.040, val_acc:0.973]
Epoch [56/120    avg_loss:0.070, val_acc:0.951]
Epoch [57/120    avg_loss:0.050, val_acc:0.973]
Epoch [58/120    avg_loss:0.031, val_acc:0.984]
Epoch [59/120    avg_loss:0.026, val_acc:0.966]
Epoch [60/120    avg_loss:0.024, val_acc:0.982]
Epoch [61/120    avg_loss:0.020, val_acc:0.984]
Epoch [62/120    avg_loss:0.018, val_acc:0.961]
Epoch [63/120    avg_loss:0.038, val_acc:0.970]
Epoch [64/120    avg_loss:0.018, val_acc:0.974]
Epoch [65/120    avg_loss:0.017, val_acc:0.974]
Epoch [66/120    avg_loss:0.021, val_acc:0.985]
Epoch [67/120    avg_loss:0.016, val_acc:0.983]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.970]
Epoch [70/120    avg_loss:0.015, val_acc:0.982]
Epoch [71/120    avg_loss:0.008, val_acc:0.982]
Epoch [72/120    avg_loss:0.011, val_acc:0.978]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.010, val_acc:0.983]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.010, val_acc:0.973]
Epoch [77/120    avg_loss:0.013, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.016, val_acc:0.973]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.014, val_acc:0.978]
Epoch [86/120    avg_loss:0.023, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.985]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.017, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.975]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.013, val_acc:0.972]
Epoch [97/120    avg_loss:0.106, val_acc:0.950]
Epoch [98/120    avg_loss:0.072, val_acc:0.972]
Epoch [99/120    avg_loss:0.030, val_acc:0.942]
Epoch [100/120    avg_loss:0.031, val_acc:0.962]
Epoch [101/120    avg_loss:0.028, val_acc:0.973]
Epoch [102/120    avg_loss:0.027, val_acc:0.978]
Epoch [103/120    avg_loss:0.019, val_acc:0.980]
Epoch [104/120    avg_loss:0.018, val_acc:0.978]
Epoch [105/120    avg_loss:0.018, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6278     0    20     0     0    17     4   112     1]
 [    0     2 17995     0    28     0    59     0     6     0]
 [    0     0     0  2002     3     0     0     0    27     4]
 [    0    29    16     0  2900     0     2     0    24     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4870     0     7     0]
 [    0     9     0     0     0     0     0  1271     0    10]
 [    0    10     0     9    43     0     0     0  3508     1]
 [    0     1     0     0    14    30     0     0     0   874]]

Accuracy:
98.81907791675705

F1 scores:
[       nan 0.98393543 0.99692529 0.98426745 0.97315436 0.98863636
 0.99124771 0.99103314 0.9670572  0.96574586]

Kappa:
0.9843678676335051
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75a5312828>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.968, val_acc:0.178]
Epoch [2/120    avg_loss:1.491, val_acc:0.347]
Epoch [3/120    avg_loss:1.209, val_acc:0.473]
Epoch [4/120    avg_loss:1.031, val_acc:0.761]
Epoch [5/120    avg_loss:0.830, val_acc:0.740]
Epoch [6/120    avg_loss:0.706, val_acc:0.803]
Epoch [7/120    avg_loss:0.550, val_acc:0.789]
Epoch [8/120    avg_loss:0.517, val_acc:0.804]
Epoch [9/120    avg_loss:0.433, val_acc:0.776]
Epoch [10/120    avg_loss:0.380, val_acc:0.802]
Epoch [11/120    avg_loss:0.404, val_acc:0.846]
Epoch [12/120    avg_loss:0.319, val_acc:0.867]
Epoch [13/120    avg_loss:0.254, val_acc:0.906]
Epoch [14/120    avg_loss:0.245, val_acc:0.912]
Epoch [15/120    avg_loss:0.201, val_acc:0.946]
Epoch [16/120    avg_loss:0.174, val_acc:0.874]
Epoch [17/120    avg_loss:0.159, val_acc:0.916]
Epoch [18/120    avg_loss:0.175, val_acc:0.944]
Epoch [19/120    avg_loss:0.142, val_acc:0.935]
Epoch [20/120    avg_loss:0.138, val_acc:0.927]
Epoch [21/120    avg_loss:0.109, val_acc:0.913]
Epoch [22/120    avg_loss:0.112, val_acc:0.950]
Epoch [23/120    avg_loss:0.102, val_acc:0.971]
Epoch [24/120    avg_loss:0.091, val_acc:0.962]
Epoch [25/120    avg_loss:0.087, val_acc:0.956]
Epoch [26/120    avg_loss:0.063, val_acc:0.957]
Epoch [27/120    avg_loss:0.064, val_acc:0.900]
Epoch [28/120    avg_loss:0.078, val_acc:0.943]
Epoch [29/120    avg_loss:0.101, val_acc:0.954]
Epoch [30/120    avg_loss:0.061, val_acc:0.963]
Epoch [31/120    avg_loss:0.053, val_acc:0.977]
Epoch [32/120    avg_loss:0.058, val_acc:0.970]
Epoch [33/120    avg_loss:0.051, val_acc:0.942]
Epoch [34/120    avg_loss:0.050, val_acc:0.974]
Epoch [35/120    avg_loss:0.037, val_acc:0.971]
Epoch [36/120    avg_loss:0.040, val_acc:0.972]
Epoch [37/120    avg_loss:0.045, val_acc:0.984]
Epoch [38/120    avg_loss:0.043, val_acc:0.982]
Epoch [39/120    avg_loss:0.036, val_acc:0.972]
Epoch [40/120    avg_loss:0.040, val_acc:0.936]
Epoch [41/120    avg_loss:0.045, val_acc:0.948]
Epoch [42/120    avg_loss:0.033, val_acc:0.985]
Epoch [43/120    avg_loss:0.021, val_acc:0.986]
Epoch [44/120    avg_loss:0.020, val_acc:0.983]
Epoch [45/120    avg_loss:0.013, val_acc:0.983]
Epoch [46/120    avg_loss:0.016, val_acc:0.965]
Epoch [47/120    avg_loss:0.019, val_acc:0.989]
Epoch [48/120    avg_loss:0.012, val_acc:0.984]
Epoch [49/120    avg_loss:0.014, val_acc:0.980]
Epoch [50/120    avg_loss:0.051, val_acc:0.939]
Epoch [51/120    avg_loss:0.035, val_acc:0.970]
Epoch [52/120    avg_loss:0.020, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.987]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.013, val_acc:0.982]
Epoch [56/120    avg_loss:0.020, val_acc:0.981]
Epoch [57/120    avg_loss:0.038, val_acc:0.986]
Epoch [58/120    avg_loss:0.029, val_acc:0.985]
Epoch [59/120    avg_loss:0.028, val_acc:0.986]
Epoch [60/120    avg_loss:0.019, val_acc:0.984]
Epoch [61/120    avg_loss:0.012, val_acc:0.986]
Epoch [62/120    avg_loss:0.009, val_acc:0.985]
Epoch [63/120    avg_loss:0.010, val_acc:0.984]
Epoch [64/120    avg_loss:0.009, val_acc:0.986]
Epoch [65/120    avg_loss:0.008, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.987]
Epoch [67/120    avg_loss:0.008, val_acc:0.988]
Epoch [68/120    avg_loss:0.007, val_acc:0.986]
Epoch [69/120    avg_loss:0.008, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.987]
Epoch [72/120    avg_loss:0.008, val_acc:0.987]
Epoch [73/120    avg_loss:0.006, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.008, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     2     0     0     0     2    49     5]
 [    0     1 18045     0    20     0    20     0     4     0]
 [    0     0     0  2033     0     0     0     0     3     0]
 [    0    19    19     0  2904     0     1    11    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    15    13     0     0  4837     0    11     2]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0    11     0     5    59     0     0     0  3473    23]
 [    0     0     0     0    24    27     0     0     0   868]]

Accuracy:
99.11792350516954

F1 scores:
[       nan 0.99306692 0.99781581 0.99437515 0.9713999  0.98976109
 0.99363188 0.99421073 0.9743302  0.95437053]

Kappa:
0.9883150656343939
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6334e53940>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.912, val_acc:0.163]
Epoch [2/120    avg_loss:1.518, val_acc:0.372]
Epoch [3/120    avg_loss:1.235, val_acc:0.699]
Epoch [4/120    avg_loss:1.081, val_acc:0.710]
Epoch [5/120    avg_loss:0.885, val_acc:0.793]
Epoch [6/120    avg_loss:0.747, val_acc:0.631]
Epoch [7/120    avg_loss:0.589, val_acc:0.783]
Epoch [8/120    avg_loss:0.501, val_acc:0.730]
Epoch [9/120    avg_loss:0.501, val_acc:0.795]
Epoch [10/120    avg_loss:0.420, val_acc:0.831]
Epoch [11/120    avg_loss:0.335, val_acc:0.834]
Epoch [12/120    avg_loss:0.308, val_acc:0.845]
Epoch [13/120    avg_loss:0.261, val_acc:0.886]
Epoch [14/120    avg_loss:0.259, val_acc:0.830]
Epoch [15/120    avg_loss:0.244, val_acc:0.906]
Epoch [16/120    avg_loss:0.195, val_acc:0.897]
Epoch [17/120    avg_loss:0.177, val_acc:0.922]
Epoch [18/120    avg_loss:0.191, val_acc:0.886]
Epoch [19/120    avg_loss:0.166, val_acc:0.944]
Epoch [20/120    avg_loss:0.154, val_acc:0.878]
Epoch [21/120    avg_loss:0.134, val_acc:0.932]
Epoch [22/120    avg_loss:0.093, val_acc:0.961]
Epoch [23/120    avg_loss:0.085, val_acc:0.935]
Epoch [24/120    avg_loss:0.119, val_acc:0.954]
Epoch [25/120    avg_loss:0.095, val_acc:0.941]
Epoch [26/120    avg_loss:0.074, val_acc:0.961]
Epoch [27/120    avg_loss:0.060, val_acc:0.953]
Epoch [28/120    avg_loss:0.075, val_acc:0.965]
Epoch [29/120    avg_loss:0.070, val_acc:0.936]
Epoch [30/120    avg_loss:0.081, val_acc:0.901]
Epoch [31/120    avg_loss:0.065, val_acc:0.972]
Epoch [32/120    avg_loss:0.055, val_acc:0.963]
Epoch [33/120    avg_loss:0.076, val_acc:0.971]
Epoch [34/120    avg_loss:0.056, val_acc:0.963]
Epoch [35/120    avg_loss:0.058, val_acc:0.931]
Epoch [36/120    avg_loss:0.066, val_acc:0.957]
Epoch [37/120    avg_loss:0.054, val_acc:0.963]
Epoch [38/120    avg_loss:0.050, val_acc:0.966]
Epoch [39/120    avg_loss:0.089, val_acc:0.938]
Epoch [40/120    avg_loss:0.045, val_acc:0.975]
Epoch [41/120    avg_loss:0.054, val_acc:0.963]
Epoch [42/120    avg_loss:0.032, val_acc:0.974]
Epoch [43/120    avg_loss:0.023, val_acc:0.974]
Epoch [44/120    avg_loss:0.025, val_acc:0.980]
Epoch [45/120    avg_loss:0.060, val_acc:0.978]
Epoch [46/120    avg_loss:0.033, val_acc:0.980]
Epoch [47/120    avg_loss:0.030, val_acc:0.973]
Epoch [48/120    avg_loss:0.036, val_acc:0.976]
Epoch [49/120    avg_loss:0.034, val_acc:0.974]
Epoch [50/120    avg_loss:0.035, val_acc:0.977]
Epoch [51/120    avg_loss:0.022, val_acc:0.980]
Epoch [52/120    avg_loss:0.014, val_acc:0.985]
Epoch [53/120    avg_loss:0.013, val_acc:0.983]
Epoch [54/120    avg_loss:0.022, val_acc:0.984]
Epoch [55/120    avg_loss:0.021, val_acc:0.974]
Epoch [56/120    avg_loss:0.015, val_acc:0.980]
Epoch [57/120    avg_loss:0.027, val_acc:0.971]
Epoch [58/120    avg_loss:0.012, val_acc:0.980]
Epoch [59/120    avg_loss:0.015, val_acc:0.976]
Epoch [60/120    avg_loss:0.019, val_acc:0.980]
Epoch [61/120    avg_loss:0.010, val_acc:0.979]
Epoch [62/120    avg_loss:0.019, val_acc:0.983]
Epoch [63/120    avg_loss:0.011, val_acc:0.984]
Epoch [64/120    avg_loss:0.009, val_acc:0.984]
Epoch [65/120    avg_loss:0.009, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.975]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.012, val_acc:0.979]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.011, val_acc:0.981]
Epoch [71/120    avg_loss:0.007, val_acc:0.984]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.979]
Epoch [75/120    avg_loss:0.006, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.005, val_acc:0.981]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.008, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.005, val_acc:0.987]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.989]
Epoch [88/120    avg_loss:0.004, val_acc:0.986]
Epoch [89/120    avg_loss:0.004, val_acc:0.984]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.003, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.049, val_acc:0.938]
Epoch [100/120    avg_loss:0.070, val_acc:0.971]
Epoch [101/120    avg_loss:0.021, val_acc:0.977]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.974]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.019, val_acc:0.952]
Epoch [106/120    avg_loss:0.022, val_acc:0.979]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6363     0     0     0     0     0     7    62     0]
 [    0     0 18055     0    16     0    11     0     8     0]
 [    0     5     0  1983     1     0     0     0    38     9]
 [    0    37    19     2  2890     0     3     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    26     1     0     0  4851     0     0     0]
 [    0     9     0     0     0     0     1  1266     0    14]
 [    0     7     0    16    33     0     0     0  3509     6]
 [    0     0     0     0    14    42     0     0     0   863]]

Accuracy:
99.01670161232015

F1 scores:
[       nan 0.99011904 0.99778944 0.98216939 0.97536281 0.9841629
 0.99568966 0.9879048  0.97350534 0.95306461]

Kappa:
0.9869689351061397
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac7ea7b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.904, val_acc:0.155]
Epoch [2/120    avg_loss:1.504, val_acc:0.328]
Epoch [3/120    avg_loss:1.258, val_acc:0.440]
Epoch [4/120    avg_loss:0.977, val_acc:0.520]
Epoch [5/120    avg_loss:0.792, val_acc:0.623]
Epoch [6/120    avg_loss:0.615, val_acc:0.696]
Epoch [7/120    avg_loss:0.468, val_acc:0.671]
Epoch [8/120    avg_loss:0.417, val_acc:0.799]
Epoch [9/120    avg_loss:0.391, val_acc:0.782]
Epoch [10/120    avg_loss:0.348, val_acc:0.753]
Epoch [11/120    avg_loss:0.337, val_acc:0.790]
Epoch [12/120    avg_loss:0.294, val_acc:0.794]
Epoch [13/120    avg_loss:0.260, val_acc:0.872]
Epoch [14/120    avg_loss:0.256, val_acc:0.816]
Epoch [15/120    avg_loss:0.245, val_acc:0.879]
Epoch [16/120    avg_loss:0.194, val_acc:0.884]
Epoch [17/120    avg_loss:0.161, val_acc:0.923]
Epoch [18/120    avg_loss:0.145, val_acc:0.901]
Epoch [19/120    avg_loss:0.122, val_acc:0.938]
Epoch [20/120    avg_loss:0.153, val_acc:0.923]
Epoch [21/120    avg_loss:0.143, val_acc:0.923]
Epoch [22/120    avg_loss:0.129, val_acc:0.923]
Epoch [23/120    avg_loss:0.158, val_acc:0.916]
Epoch [24/120    avg_loss:0.116, val_acc:0.923]
Epoch [25/120    avg_loss:0.164, val_acc:0.919]
Epoch [26/120    avg_loss:0.171, val_acc:0.904]
Epoch [27/120    avg_loss:0.128, val_acc:0.924]
Epoch [28/120    avg_loss:0.146, val_acc:0.945]
Epoch [29/120    avg_loss:0.112, val_acc:0.939]
Epoch [30/120    avg_loss:0.065, val_acc:0.959]
Epoch [31/120    avg_loss:0.089, val_acc:0.963]
Epoch [32/120    avg_loss:0.074, val_acc:0.964]
Epoch [33/120    avg_loss:0.074, val_acc:0.958]
Epoch [34/120    avg_loss:0.069, val_acc:0.948]
Epoch [35/120    avg_loss:0.071, val_acc:0.967]
Epoch [36/120    avg_loss:0.055, val_acc:0.970]
Epoch [37/120    avg_loss:0.052, val_acc:0.952]
Epoch [38/120    avg_loss:0.042, val_acc:0.976]
Epoch [39/120    avg_loss:0.045, val_acc:0.976]
Epoch [40/120    avg_loss:0.041, val_acc:0.956]
Epoch [41/120    avg_loss:0.030, val_acc:0.977]
Epoch [42/120    avg_loss:0.029, val_acc:0.969]
Epoch [43/120    avg_loss:0.027, val_acc:0.960]
Epoch [44/120    avg_loss:0.030, val_acc:0.972]
Epoch [45/120    avg_loss:0.020, val_acc:0.978]
Epoch [46/120    avg_loss:0.043, val_acc:0.975]
Epoch [47/120    avg_loss:0.038, val_acc:0.967]
Epoch [48/120    avg_loss:0.038, val_acc:0.970]
Epoch [49/120    avg_loss:0.043, val_acc:0.977]
Epoch [50/120    avg_loss:0.062, val_acc:0.933]
Epoch [51/120    avg_loss:0.032, val_acc:0.977]
Epoch [52/120    avg_loss:0.028, val_acc:0.959]
Epoch [53/120    avg_loss:0.040, val_acc:0.933]
Epoch [54/120    avg_loss:0.028, val_acc:0.976]
Epoch [55/120    avg_loss:0.025, val_acc:0.973]
Epoch [56/120    avg_loss:0.021, val_acc:0.985]
Epoch [57/120    avg_loss:0.015, val_acc:0.972]
Epoch [58/120    avg_loss:0.014, val_acc:0.983]
Epoch [59/120    avg_loss:0.035, val_acc:0.944]
Epoch [60/120    avg_loss:0.036, val_acc:0.963]
Epoch [61/120    avg_loss:0.019, val_acc:0.978]
Epoch [62/120    avg_loss:0.023, val_acc:0.982]
Epoch [63/120    avg_loss:0.012, val_acc:0.980]
Epoch [64/120    avg_loss:0.014, val_acc:0.987]
Epoch [65/120    avg_loss:0.019, val_acc:0.974]
Epoch [66/120    avg_loss:0.015, val_acc:0.974]
Epoch [67/120    avg_loss:0.010, val_acc:0.975]
Epoch [68/120    avg_loss:0.016, val_acc:0.956]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.015, val_acc:0.980]
Epoch [71/120    avg_loss:0.080, val_acc:0.946]
Epoch [72/120    avg_loss:0.044, val_acc:0.963]
Epoch [73/120    avg_loss:0.039, val_acc:0.967]
Epoch [74/120    avg_loss:0.026, val_acc:0.968]
Epoch [75/120    avg_loss:0.018, val_acc:0.982]
Epoch [76/120    avg_loss:0.052, val_acc:0.980]
Epoch [77/120    avg_loss:0.023, val_acc:0.974]
Epoch [78/120    avg_loss:0.020, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.985]
Epoch [85/120    avg_loss:0.014, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.982]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.015, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6356     0    10     1     0     0     2    59     4]
 [    0     0 18039     0    45     0     5     0     1     0]
 [    0     5     0  1968     2     0     0     0    61     0]
 [    0    32     6     0  2918     0     0     1    15     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    42    10     0     0  4796     0    30     0]
 [    0     0     0     0     0     0     0  1287     2     1]
 [    0     6     0    23    56     0     0     0  3476    10]
 [    0     0     0     0     4    20     0     0     0   895]]

Accuracy:
98.90824958426722

F1 scores:
[       nan 0.99072559 0.99726345 0.97257228 0.972991   0.99239544
 0.99101147 0.99767442 0.96354816 0.97867687]

Kappa:
0.9855364753280957
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5d30d0898>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.995, val_acc:0.529]
Epoch [2/120    avg_loss:1.479, val_acc:0.580]
Epoch [3/120    avg_loss:1.190, val_acc:0.680]
Epoch [4/120    avg_loss:1.018, val_acc:0.726]
Epoch [5/120    avg_loss:0.840, val_acc:0.747]
Epoch [6/120    avg_loss:0.730, val_acc:0.627]
Epoch [7/120    avg_loss:0.605, val_acc:0.670]
Epoch [8/120    avg_loss:0.515, val_acc:0.710]
Epoch [9/120    avg_loss:0.437, val_acc:0.764]
Epoch [10/120    avg_loss:0.396, val_acc:0.825]
Epoch [11/120    avg_loss:0.324, val_acc:0.879]
Epoch [12/120    avg_loss:0.276, val_acc:0.889]
Epoch [13/120    avg_loss:0.263, val_acc:0.889]
Epoch [14/120    avg_loss:0.225, val_acc:0.875]
Epoch [15/120    avg_loss:0.208, val_acc:0.910]
Epoch [16/120    avg_loss:0.175, val_acc:0.877]
Epoch [17/120    avg_loss:0.154, val_acc:0.929]
Epoch [18/120    avg_loss:0.183, val_acc:0.935]
Epoch [19/120    avg_loss:0.148, val_acc:0.960]
Epoch [20/120    avg_loss:0.194, val_acc:0.945]
Epoch [21/120    avg_loss:0.140, val_acc:0.906]
Epoch [22/120    avg_loss:0.157, val_acc:0.942]
Epoch [23/120    avg_loss:0.155, val_acc:0.942]
Epoch [24/120    avg_loss:0.107, val_acc:0.910]
Epoch [25/120    avg_loss:0.111, val_acc:0.954]
Epoch [26/120    avg_loss:0.067, val_acc:0.944]
Epoch [27/120    avg_loss:0.075, val_acc:0.971]
Epoch [28/120    avg_loss:0.095, val_acc:0.963]
Epoch [29/120    avg_loss:0.074, val_acc:0.957]
Epoch [30/120    avg_loss:0.056, val_acc:0.969]
Epoch [31/120    avg_loss:0.059, val_acc:0.970]
Epoch [32/120    avg_loss:0.060, val_acc:0.976]
Epoch [33/120    avg_loss:0.057, val_acc:0.964]
Epoch [34/120    avg_loss:0.058, val_acc:0.979]
Epoch [35/120    avg_loss:0.059, val_acc:0.965]
Epoch [36/120    avg_loss:0.061, val_acc:0.959]
Epoch [37/120    avg_loss:0.045, val_acc:0.963]
Epoch [38/120    avg_loss:0.033, val_acc:0.973]
Epoch [39/120    avg_loss:0.074, val_acc:0.955]
Epoch [40/120    avg_loss:0.041, val_acc:0.974]
Epoch [41/120    avg_loss:0.042, val_acc:0.957]
Epoch [42/120    avg_loss:0.046, val_acc:0.972]
Epoch [43/120    avg_loss:0.036, val_acc:0.963]
Epoch [44/120    avg_loss:0.035, val_acc:0.981]
Epoch [45/120    avg_loss:0.029, val_acc:0.980]
Epoch [46/120    avg_loss:0.039, val_acc:0.973]
Epoch [47/120    avg_loss:0.040, val_acc:0.976]
Epoch [48/120    avg_loss:0.034, val_acc:0.974]
Epoch [49/120    avg_loss:0.024, val_acc:0.983]
Epoch [50/120    avg_loss:0.049, val_acc:0.975]
Epoch [51/120    avg_loss:0.035, val_acc:0.978]
Epoch [52/120    avg_loss:0.033, val_acc:0.972]
Epoch [53/120    avg_loss:0.043, val_acc:0.979]
Epoch [54/120    avg_loss:0.019, val_acc:0.980]
Epoch [55/120    avg_loss:0.018, val_acc:0.980]
Epoch [56/120    avg_loss:0.027, val_acc:0.977]
Epoch [57/120    avg_loss:0.020, val_acc:0.983]
Epoch [58/120    avg_loss:0.050, val_acc:0.970]
Epoch [59/120    avg_loss:0.021, val_acc:0.983]
Epoch [60/120    avg_loss:0.020, val_acc:0.967]
Epoch [61/120    avg_loss:0.040, val_acc:0.973]
Epoch [62/120    avg_loss:0.033, val_acc:0.978]
Epoch [63/120    avg_loss:0.016, val_acc:0.982]
Epoch [64/120    avg_loss:0.016, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.985]
Epoch [66/120    avg_loss:0.014, val_acc:0.984]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.013, val_acc:0.986]
Epoch [69/120    avg_loss:0.008, val_acc:0.986]
Epoch [70/120    avg_loss:0.011, val_acc:0.988]
Epoch [71/120    avg_loss:0.019, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.986]
Epoch [73/120    avg_loss:0.011, val_acc:0.985]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.007, val_acc:0.987]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.983]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.004, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6404     0     0     0     0     0     0    26     2]
 [    0     4 18027     0    47     0     7     0     5     0]
 [    0     1     5  1947     2     0     0     0    79     2]
 [    0    31    11     0  2911     0     0     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     6     0     0  4844     0    14     0]
 [    0     2     0     0     0     0     0  1286     0     2]
 [    0    11     0    14    57     0     0     0  3483     6]
 [    0     1     0     0    14    23     0     0     0   881]]

Accuracy:
99.02393174752368

F1 scores:
[       nan 0.99394692 0.99742717 0.97277042 0.96984841 0.99126472
 0.9957858  0.9984472  0.96790329 0.97240618]

Kappa:
0.9870700382310389
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f956004a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.909, val_acc:0.497]
Epoch [2/120    avg_loss:1.481, val_acc:0.344]
Epoch [3/120    avg_loss:1.205, val_acc:0.439]
Epoch [4/120    avg_loss:1.014, val_acc:0.585]
Epoch [5/120    avg_loss:0.830, val_acc:0.706]
Epoch [6/120    avg_loss:0.709, val_acc:0.737]
Epoch [7/120    avg_loss:0.560, val_acc:0.721]
Epoch [8/120    avg_loss:0.479, val_acc:0.659]
Epoch [9/120    avg_loss:0.400, val_acc:0.747]
Epoch [10/120    avg_loss:0.373, val_acc:0.768]
Epoch [11/120    avg_loss:0.345, val_acc:0.819]
Epoch [12/120    avg_loss:0.280, val_acc:0.799]
Epoch [13/120    avg_loss:0.263, val_acc:0.912]
Epoch [14/120    avg_loss:0.238, val_acc:0.867]
Epoch [15/120    avg_loss:0.195, val_acc:0.893]
Epoch [16/120    avg_loss:0.200, val_acc:0.917]
Epoch [17/120    avg_loss:0.169, val_acc:0.931]
Epoch [18/120    avg_loss:0.135, val_acc:0.912]
Epoch [19/120    avg_loss:0.113, val_acc:0.949]
Epoch [20/120    avg_loss:0.097, val_acc:0.925]
Epoch [21/120    avg_loss:0.113, val_acc:0.950]
Epoch [22/120    avg_loss:0.107, val_acc:0.928]
Epoch [23/120    avg_loss:0.435, val_acc:0.913]
Epoch [24/120    avg_loss:0.168, val_acc:0.946]
Epoch [25/120    avg_loss:0.107, val_acc:0.924]
Epoch [26/120    avg_loss:0.121, val_acc:0.912]
Epoch [27/120    avg_loss:0.108, val_acc:0.947]
Epoch [28/120    avg_loss:0.070, val_acc:0.966]
Epoch [29/120    avg_loss:0.138, val_acc:0.917]
Epoch [30/120    avg_loss:0.140, val_acc:0.940]
Epoch [31/120    avg_loss:0.103, val_acc:0.954]
Epoch [32/120    avg_loss:0.100, val_acc:0.935]
Epoch [33/120    avg_loss:0.098, val_acc:0.950]
Epoch [34/120    avg_loss:0.096, val_acc:0.932]
Epoch [35/120    avg_loss:0.095, val_acc:0.960]
Epoch [36/120    avg_loss:0.076, val_acc:0.956]
Epoch [37/120    avg_loss:0.059, val_acc:0.976]
Epoch [38/120    avg_loss:0.049, val_acc:0.967]
Epoch [39/120    avg_loss:0.044, val_acc:0.969]
Epoch [40/120    avg_loss:0.039, val_acc:0.962]
Epoch [41/120    avg_loss:0.036, val_acc:0.970]
Epoch [42/120    avg_loss:0.033, val_acc:0.969]
Epoch [43/120    avg_loss:0.034, val_acc:0.978]
Epoch [44/120    avg_loss:0.050, val_acc:0.908]
Epoch [45/120    avg_loss:0.070, val_acc:0.946]
Epoch [46/120    avg_loss:0.040, val_acc:0.951]
Epoch [47/120    avg_loss:0.044, val_acc:0.967]
Epoch [48/120    avg_loss:0.069, val_acc:0.975]
Epoch [49/120    avg_loss:0.042, val_acc:0.972]
Epoch [50/120    avg_loss:0.027, val_acc:0.979]
Epoch [51/120    avg_loss:0.034, val_acc:0.942]
Epoch [52/120    avg_loss:0.039, val_acc:0.954]
Epoch [53/120    avg_loss:0.044, val_acc:0.974]
Epoch [54/120    avg_loss:0.038, val_acc:0.959]
Epoch [55/120    avg_loss:0.031, val_acc:0.980]
Epoch [56/120    avg_loss:0.024, val_acc:0.971]
Epoch [57/120    avg_loss:0.020, val_acc:0.978]
Epoch [58/120    avg_loss:0.024, val_acc:0.978]
Epoch [59/120    avg_loss:0.020, val_acc:0.972]
Epoch [60/120    avg_loss:0.022, val_acc:0.978]
Epoch [61/120    avg_loss:0.021, val_acc:0.976]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.014, val_acc:0.979]
Epoch [64/120    avg_loss:0.016, val_acc:0.980]
Epoch [65/120    avg_loss:0.016, val_acc:0.982]
Epoch [66/120    avg_loss:0.014, val_acc:0.980]
Epoch [67/120    avg_loss:0.018, val_acc:0.978]
Epoch [68/120    avg_loss:0.024, val_acc:0.982]
Epoch [69/120    avg_loss:0.018, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.975]
Epoch [71/120    avg_loss:0.017, val_acc:0.973]
Epoch [72/120    avg_loss:0.009, val_acc:0.973]
Epoch [73/120    avg_loss:0.009, val_acc:0.976]
Epoch [74/120    avg_loss:0.009, val_acc:0.955]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.014, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.016, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.012, val_acc:0.981]
Epoch [82/120    avg_loss:0.011, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.979]
Epoch [88/120    avg_loss:0.009, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.169, val_acc:0.863]
Epoch [96/120    avg_loss:0.125, val_acc:0.972]
Epoch [97/120    avg_loss:0.045, val_acc:0.976]
Epoch [98/120    avg_loss:0.039, val_acc:0.979]
Epoch [99/120    avg_loss:0.035, val_acc:0.980]
Epoch [100/120    avg_loss:0.027, val_acc:0.979]
Epoch [101/120    avg_loss:0.027, val_acc:0.979]
Epoch [102/120    avg_loss:0.026, val_acc:0.978]
Epoch [103/120    avg_loss:0.025, val_acc:0.981]
Epoch [104/120    avg_loss:0.025, val_acc:0.981]
Epoch [105/120    avg_loss:0.027, val_acc:0.979]
Epoch [106/120    avg_loss:0.025, val_acc:0.979]
Epoch [107/120    avg_loss:0.023, val_acc:0.978]
Epoch [108/120    avg_loss:0.021, val_acc:0.981]
Epoch [109/120    avg_loss:0.020, val_acc:0.980]
Epoch [110/120    avg_loss:0.019, val_acc:0.980]
Epoch [111/120    avg_loss:0.018, val_acc:0.981]
Epoch [112/120    avg_loss:0.019, val_acc:0.981]
Epoch [113/120    avg_loss:0.021, val_acc:0.981]
Epoch [114/120    avg_loss:0.019, val_acc:0.981]
Epoch [115/120    avg_loss:0.015, val_acc:0.982]
Epoch [116/120    avg_loss:0.021, val_acc:0.981]
Epoch [117/120    avg_loss:0.022, val_acc:0.982]
Epoch [118/120    avg_loss:0.017, val_acc:0.982]
Epoch [119/120    avg_loss:0.020, val_acc:0.980]
Epoch [120/120    avg_loss:0.024, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6199     0    37     6     0    14     4   172     0]
 [    0     0 18050     0    33     0     7     0     0     0]
 [    0     0     0  1966     0     0     0     0    64     6]
 [    0    24    13     0  2900     0    14     0    21     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4866     0    12     0]
 [    0     0     0     0     0     0    10  1276     0     4]
 [    0    10     0    43    65     0     0     0  3422    31]
 [    0     0     0     0    17    14     0     0     0   888]]

Accuracy:
98.50336201286964

F1 scores:
[       nan 0.97891828 0.99853401 0.96325331 0.96779576 0.99466463
 0.99417714 0.99299611 0.9424401  0.96103896]

Kappa:
0.9801838687959803
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5f0bed9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.952, val_acc:0.230]
Epoch [2/120    avg_loss:1.483, val_acc:0.316]
Epoch [3/120    avg_loss:1.165, val_acc:0.375]
Epoch [4/120    avg_loss:0.980, val_acc:0.651]
Epoch [5/120    avg_loss:0.785, val_acc:0.644]
Epoch [6/120    avg_loss:0.657, val_acc:0.764]
Epoch [7/120    avg_loss:0.572, val_acc:0.769]
Epoch [8/120    avg_loss:0.510, val_acc:0.779]
Epoch [9/120    avg_loss:0.422, val_acc:0.827]
Epoch [10/120    avg_loss:0.355, val_acc:0.858]
Epoch [11/120    avg_loss:0.322, val_acc:0.811]
Epoch [12/120    avg_loss:0.298, val_acc:0.843]
Epoch [13/120    avg_loss:0.236, val_acc:0.909]
Epoch [14/120    avg_loss:0.216, val_acc:0.897]
Epoch [15/120    avg_loss:0.175, val_acc:0.928]
Epoch [16/120    avg_loss:0.172, val_acc:0.933]
Epoch [17/120    avg_loss:0.193, val_acc:0.935]
Epoch [18/120    avg_loss:0.143, val_acc:0.937]
Epoch [19/120    avg_loss:0.142, val_acc:0.959]
Epoch [20/120    avg_loss:0.117, val_acc:0.948]
Epoch [21/120    avg_loss:0.126, val_acc:0.952]
Epoch [22/120    avg_loss:0.117, val_acc:0.935]
Epoch [23/120    avg_loss:0.123, val_acc:0.906]
Epoch [24/120    avg_loss:0.125, val_acc:0.929]
Epoch [25/120    avg_loss:0.079, val_acc:0.946]
Epoch [26/120    avg_loss:0.127, val_acc:0.955]
Epoch [27/120    avg_loss:0.086, val_acc:0.959]
Epoch [28/120    avg_loss:0.080, val_acc:0.964]
Epoch [29/120    avg_loss:0.102, val_acc:0.973]
Epoch [30/120    avg_loss:0.078, val_acc:0.968]
Epoch [31/120    avg_loss:0.065, val_acc:0.970]
Epoch [32/120    avg_loss:0.053, val_acc:0.966]
Epoch [33/120    avg_loss:0.059, val_acc:0.952]
Epoch [34/120    avg_loss:0.074, val_acc:0.965]
Epoch [35/120    avg_loss:0.047, val_acc:0.969]
Epoch [36/120    avg_loss:0.048, val_acc:0.963]
Epoch [37/120    avg_loss:0.059, val_acc:0.940]
Epoch [38/120    avg_loss:0.057, val_acc:0.966]
Epoch [39/120    avg_loss:0.063, val_acc:0.973]
Epoch [40/120    avg_loss:0.070, val_acc:0.970]
Epoch [41/120    avg_loss:0.039, val_acc:0.978]
Epoch [42/120    avg_loss:0.034, val_acc:0.984]
Epoch [43/120    avg_loss:0.029, val_acc:0.967]
Epoch [44/120    avg_loss:0.027, val_acc:0.981]
Epoch [45/120    avg_loss:0.021, val_acc:0.969]
Epoch [46/120    avg_loss:0.030, val_acc:0.930]
Epoch [47/120    avg_loss:0.076, val_acc:0.963]
Epoch [48/120    avg_loss:0.044, val_acc:0.969]
Epoch [49/120    avg_loss:0.042, val_acc:0.974]
Epoch [50/120    avg_loss:0.042, val_acc:0.969]
Epoch [51/120    avg_loss:0.034, val_acc:0.971]
Epoch [52/120    avg_loss:0.018, val_acc:0.970]
Epoch [53/120    avg_loss:0.016, val_acc:0.978]
Epoch [54/120    avg_loss:0.016, val_acc:0.967]
Epoch [55/120    avg_loss:0.016, val_acc:0.982]
Epoch [56/120    avg_loss:0.018, val_acc:0.985]
Epoch [57/120    avg_loss:0.012, val_acc:0.985]
Epoch [58/120    avg_loss:0.010, val_acc:0.984]
Epoch [59/120    avg_loss:0.010, val_acc:0.983]
Epoch [60/120    avg_loss:0.010, val_acc:0.982]
Epoch [61/120    avg_loss:0.011, val_acc:0.981]
Epoch [62/120    avg_loss:0.011, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.980]
Epoch [64/120    avg_loss:0.009, val_acc:0.981]
Epoch [65/120    avg_loss:0.009, val_acc:0.982]
Epoch [66/120    avg_loss:0.009, val_acc:0.980]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.010, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.981]
Epoch [70/120    avg_loss:0.008, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.981]
Epoch [78/120    avg_loss:0.009, val_acc:0.981]
Epoch [79/120    avg_loss:0.010, val_acc:0.981]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.982]
Epoch [82/120    avg_loss:0.007, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.012, val_acc:0.982]
Epoch [87/120    avg_loss:0.008, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.982]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.982]
Epoch [103/120    avg_loss:0.009, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.012, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6360     0     0     0     0     0    46    26     0]
 [    0     0 18007     0    79     0     4     0     0     0]
 [    0     2     0  1947     4     0     0     0    78     5]
 [    0    21     7     0  2923     0     9     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     4     0     0  4863     0     1     0]
 [    0     1     0     0     0     0     5  1270     0    14]
 [    0     7     0    14    72     0     0     0  3478     0]
 [    0     0     0     0    19    49     0     0     0   851]]

Accuracy:
98.82148796182489

F1 scores:
[       nan 0.99196756 0.99723099 0.97325669 0.96325589 0.98157202
 0.99661851 0.97467383 0.97069495 0.95136948]

Kappa:
0.9843969479807831
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f175c4c4940>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.981, val_acc:0.578]
Epoch [2/120    avg_loss:1.527, val_acc:0.388]
Epoch [3/120    avg_loss:1.272, val_acc:0.643]
Epoch [4/120    avg_loss:1.023, val_acc:0.609]
Epoch [5/120    avg_loss:0.797, val_acc:0.687]
Epoch [6/120    avg_loss:0.638, val_acc:0.732]
Epoch [7/120    avg_loss:0.522, val_acc:0.819]
Epoch [8/120    avg_loss:0.449, val_acc:0.858]
Epoch [9/120    avg_loss:0.431, val_acc:0.845]
Epoch [10/120    avg_loss:0.354, val_acc:0.861]
Epoch [11/120    avg_loss:0.300, val_acc:0.848]
Epoch [12/120    avg_loss:0.262, val_acc:0.916]
Epoch [13/120    avg_loss:0.326, val_acc:0.897]
Epoch [14/120    avg_loss:0.303, val_acc:0.867]
Epoch [15/120    avg_loss:0.264, val_acc:0.927]
Epoch [16/120    avg_loss:0.229, val_acc:0.878]
Epoch [17/120    avg_loss:0.193, val_acc:0.892]
Epoch [18/120    avg_loss:0.148, val_acc:0.879]
Epoch [19/120    avg_loss:0.145, val_acc:0.942]
Epoch [20/120    avg_loss:0.162, val_acc:0.922]
Epoch [21/120    avg_loss:0.153, val_acc:0.934]
Epoch [22/120    avg_loss:0.151, val_acc:0.963]
Epoch [23/120    avg_loss:0.112, val_acc:0.959]
Epoch [24/120    avg_loss:0.109, val_acc:0.967]
Epoch [25/120    avg_loss:0.090, val_acc:0.948]
Epoch [26/120    avg_loss:0.105, val_acc:0.965]
Epoch [27/120    avg_loss:0.101, val_acc:0.949]
Epoch [28/120    avg_loss:0.119, val_acc:0.906]
Epoch [29/120    avg_loss:0.147, val_acc:0.918]
Epoch [30/120    avg_loss:0.089, val_acc:0.966]
Epoch [31/120    avg_loss:0.067, val_acc:0.964]
Epoch [32/120    avg_loss:0.053, val_acc:0.943]
Epoch [33/120    avg_loss:0.050, val_acc:0.951]
Epoch [34/120    avg_loss:0.066, val_acc:0.956]
Epoch [35/120    avg_loss:0.074, val_acc:0.983]
Epoch [36/120    avg_loss:0.078, val_acc:0.963]
Epoch [37/120    avg_loss:0.101, val_acc:0.973]
Epoch [38/120    avg_loss:0.057, val_acc:0.968]
Epoch [39/120    avg_loss:0.063, val_acc:0.978]
Epoch [40/120    avg_loss:0.063, val_acc:0.967]
Epoch [41/120    avg_loss:0.058, val_acc:0.974]
Epoch [42/120    avg_loss:0.034, val_acc:0.988]
Epoch [43/120    avg_loss:0.028, val_acc:0.978]
Epoch [44/120    avg_loss:0.020, val_acc:0.978]
Epoch [45/120    avg_loss:0.018, val_acc:0.988]
Epoch [46/120    avg_loss:0.024, val_acc:0.964]
Epoch [47/120    avg_loss:0.021, val_acc:0.986]
Epoch [48/120    avg_loss:0.045, val_acc:0.979]
Epoch [49/120    avg_loss:0.032, val_acc:0.983]
Epoch [50/120    avg_loss:0.017, val_acc:0.985]
Epoch [51/120    avg_loss:0.029, val_acc:0.988]
Epoch [52/120    avg_loss:0.017, val_acc:0.988]
Epoch [53/120    avg_loss:0.023, val_acc:0.985]
Epoch [54/120    avg_loss:0.017, val_acc:0.986]
Epoch [55/120    avg_loss:0.017, val_acc:0.986]
Epoch [56/120    avg_loss:0.020, val_acc:0.983]
Epoch [57/120    avg_loss:0.012, val_acc:0.988]
Epoch [58/120    avg_loss:0.030, val_acc:0.979]
Epoch [59/120    avg_loss:0.037, val_acc:0.965]
Epoch [60/120    avg_loss:0.026, val_acc:0.982]
Epoch [61/120    avg_loss:0.016, val_acc:0.987]
Epoch [62/120    avg_loss:0.012, val_acc:0.988]
Epoch [63/120    avg_loss:0.012, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.970]
Epoch [65/120    avg_loss:0.016, val_acc:0.978]
Epoch [66/120    avg_loss:0.013, val_acc:0.988]
Epoch [67/120    avg_loss:0.010, val_acc:0.990]
Epoch [68/120    avg_loss:0.010, val_acc:0.988]
Epoch [69/120    avg_loss:0.026, val_acc:0.947]
Epoch [70/120    avg_loss:0.023, val_acc:0.988]
Epoch [71/120    avg_loss:0.027, val_acc:0.978]
Epoch [72/120    avg_loss:0.017, val_acc:0.988]
Epoch [73/120    avg_loss:0.011, val_acc:0.987]
Epoch [74/120    avg_loss:0.013, val_acc:0.987]
Epoch [75/120    avg_loss:0.007, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.014, val_acc:0.988]
Epoch [78/120    avg_loss:0.018, val_acc:0.981]
Epoch [79/120    avg_loss:0.020, val_acc:0.982]
Epoch [80/120    avg_loss:0.016, val_acc:0.989]
Epoch [81/120    avg_loss:0.013, val_acc:0.989]
Epoch [82/120    avg_loss:0.008, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.991]
Epoch [84/120    avg_loss:0.008, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.991]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.006, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.991]
Epoch [91/120    avg_loss:0.006, val_acc:0.991]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.007, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.991]
Epoch [96/120    avg_loss:0.006, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.006, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.008, val_acc:0.991]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6399     0     0     0     0     7    22     4     0]
 [    0     0 18054     0    33     0     3     0     0     0]
 [    0     9     0  2006     1     0     0     0    15     5]
 [    0    32    11     0  2904     0     6     0    19     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     8     0     0  4862     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    27     0    10    57     0     0     0  3468     9]
 [    0     0     0     0    12    27     0     0     1   879]]

Accuracy:
99.21432530788326

F1 scores:
[       nan 0.99216994 0.99847911 0.98817734 0.9713999  0.98976109
 0.99671997 0.99154497 0.97993784 0.97019868]

Kappa:
0.9895908710656341
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f522a9bb860>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.954, val_acc:0.214]
Epoch [2/120    avg_loss:1.491, val_acc:0.336]
Epoch [3/120    avg_loss:1.204, val_acc:0.417]
Epoch [4/120    avg_loss:0.976, val_acc:0.573]
Epoch [5/120    avg_loss:0.772, val_acc:0.604]
Epoch [6/120    avg_loss:0.661, val_acc:0.682]
Epoch [7/120    avg_loss:0.507, val_acc:0.723]
Epoch [8/120    avg_loss:0.445, val_acc:0.789]
Epoch [9/120    avg_loss:0.375, val_acc:0.828]
Epoch [10/120    avg_loss:0.363, val_acc:0.811]
Epoch [11/120    avg_loss:0.328, val_acc:0.757]
Epoch [12/120    avg_loss:0.356, val_acc:0.792]
Epoch [13/120    avg_loss:0.326, val_acc:0.792]
Epoch [14/120    avg_loss:0.306, val_acc:0.868]
Epoch [15/120    avg_loss:0.249, val_acc:0.909]
Epoch [16/120    avg_loss:0.213, val_acc:0.910]
Epoch [17/120    avg_loss:0.189, val_acc:0.932]
Epoch [18/120    avg_loss:0.169, val_acc:0.936]
Epoch [19/120    avg_loss:0.151, val_acc:0.954]
Epoch [20/120    avg_loss:0.162, val_acc:0.938]
Epoch [21/120    avg_loss:0.174, val_acc:0.933]
Epoch [22/120    avg_loss:0.230, val_acc:0.412]
Epoch [23/120    avg_loss:0.992, val_acc:0.776]
Epoch [24/120    avg_loss:0.450, val_acc:0.786]
Epoch [25/120    avg_loss:0.342, val_acc:0.855]
Epoch [26/120    avg_loss:0.371, val_acc:0.764]
Epoch [27/120    avg_loss:0.318, val_acc:0.850]
Epoch [28/120    avg_loss:0.269, val_acc:0.856]
Epoch [29/120    avg_loss:0.234, val_acc:0.904]
Epoch [30/120    avg_loss:0.231, val_acc:0.797]
Epoch [31/120    avg_loss:0.212, val_acc:0.824]
Epoch [32/120    avg_loss:0.189, val_acc:0.860]
Epoch [33/120    avg_loss:0.180, val_acc:0.917]
Epoch [34/120    avg_loss:0.161, val_acc:0.921]
Epoch [35/120    avg_loss:0.148, val_acc:0.922]
Epoch [36/120    avg_loss:0.158, val_acc:0.934]
Epoch [37/120    avg_loss:0.144, val_acc:0.919]
Epoch [38/120    avg_loss:0.138, val_acc:0.927]
Epoch [39/120    avg_loss:0.132, val_acc:0.936]
Epoch [40/120    avg_loss:0.138, val_acc:0.939]
Epoch [41/120    avg_loss:0.143, val_acc:0.943]
Epoch [42/120    avg_loss:0.131, val_acc:0.924]
Epoch [43/120    avg_loss:0.122, val_acc:0.933]
Epoch [44/120    avg_loss:0.124, val_acc:0.940]
Epoch [45/120    avg_loss:0.124, val_acc:0.939]
Epoch [46/120    avg_loss:0.113, val_acc:0.942]
Epoch [47/120    avg_loss:0.130, val_acc:0.940]
Epoch [48/120    avg_loss:0.116, val_acc:0.939]
Epoch [49/120    avg_loss:0.115, val_acc:0.940]
Epoch [50/120    avg_loss:0.122, val_acc:0.940]
Epoch [51/120    avg_loss:0.120, val_acc:0.938]
Epoch [52/120    avg_loss:0.119, val_acc:0.939]
Epoch [53/120    avg_loss:0.115, val_acc:0.938]
Epoch [54/120    avg_loss:0.123, val_acc:0.942]
Epoch [55/120    avg_loss:0.119, val_acc:0.944]
Epoch [56/120    avg_loss:0.116, val_acc:0.945]
Epoch [57/120    avg_loss:0.111, val_acc:0.941]
Epoch [58/120    avg_loss:0.113, val_acc:0.941]
Epoch [59/120    avg_loss:0.113, val_acc:0.941]
Epoch [60/120    avg_loss:0.119, val_acc:0.941]
Epoch [61/120    avg_loss:0.121, val_acc:0.941]
Epoch [62/120    avg_loss:0.120, val_acc:0.941]
Epoch [63/120    avg_loss:0.116, val_acc:0.940]
Epoch [64/120    avg_loss:0.112, val_acc:0.941]
Epoch [65/120    avg_loss:0.115, val_acc:0.941]
Epoch [66/120    avg_loss:0.116, val_acc:0.940]
Epoch [67/120    avg_loss:0.109, val_acc:0.940]
Epoch [68/120    avg_loss:0.119, val_acc:0.940]
Epoch [69/120    avg_loss:0.112, val_acc:0.940]
Epoch [70/120    avg_loss:0.125, val_acc:0.941]
Epoch [71/120    avg_loss:0.118, val_acc:0.941]
Epoch [72/120    avg_loss:0.124, val_acc:0.940]
Epoch [73/120    avg_loss:0.114, val_acc:0.940]
Epoch [74/120    avg_loss:0.114, val_acc:0.940]
Epoch [75/120    avg_loss:0.118, val_acc:0.940]
Epoch [76/120    avg_loss:0.116, val_acc:0.940]
Epoch [77/120    avg_loss:0.121, val_acc:0.940]
Epoch [78/120    avg_loss:0.125, val_acc:0.940]
Epoch [79/120    avg_loss:0.116, val_acc:0.940]
Epoch [80/120    avg_loss:0.118, val_acc:0.940]
Epoch [81/120    avg_loss:0.121, val_acc:0.940]
Epoch [82/120    avg_loss:0.120, val_acc:0.940]
Epoch [83/120    avg_loss:0.126, val_acc:0.941]
Epoch [84/120    avg_loss:0.110, val_acc:0.941]
Epoch [85/120    avg_loss:0.112, val_acc:0.941]
Epoch [86/120    avg_loss:0.117, val_acc:0.941]
Epoch [87/120    avg_loss:0.121, val_acc:0.941]
Epoch [88/120    avg_loss:0.113, val_acc:0.941]
Epoch [89/120    avg_loss:0.112, val_acc:0.941]
Epoch [90/120    avg_loss:0.116, val_acc:0.941]
Epoch [91/120    avg_loss:0.114, val_acc:0.941]
Epoch [92/120    avg_loss:0.118, val_acc:0.941]
Epoch [93/120    avg_loss:0.109, val_acc:0.941]
Epoch [94/120    avg_loss:0.121, val_acc:0.941]
Epoch [95/120    avg_loss:0.120, val_acc:0.941]
Epoch [96/120    avg_loss:0.111, val_acc:0.941]
Epoch [97/120    avg_loss:0.124, val_acc:0.941]
Epoch [98/120    avg_loss:0.113, val_acc:0.941]
Epoch [99/120    avg_loss:0.114, val_acc:0.941]
Epoch [100/120    avg_loss:0.117, val_acc:0.941]
Epoch [101/120    avg_loss:0.106, val_acc:0.941]
Epoch [102/120    avg_loss:0.117, val_acc:0.941]
Epoch [103/120    avg_loss:0.114, val_acc:0.941]
Epoch [104/120    avg_loss:0.118, val_acc:0.941]
Epoch [105/120    avg_loss:0.117, val_acc:0.941]
Epoch [106/120    avg_loss:0.117, val_acc:0.941]
Epoch [107/120    avg_loss:0.110, val_acc:0.941]
Epoch [108/120    avg_loss:0.128, val_acc:0.941]
Epoch [109/120    avg_loss:0.110, val_acc:0.941]
Epoch [110/120    avg_loss:0.112, val_acc:0.941]
Epoch [111/120    avg_loss:0.117, val_acc:0.941]
Epoch [112/120    avg_loss:0.114, val_acc:0.941]
Epoch [113/120    avg_loss:0.106, val_acc:0.941]
Epoch [114/120    avg_loss:0.132, val_acc:0.941]
Epoch [115/120    avg_loss:0.112, val_acc:0.941]
Epoch [116/120    avg_loss:0.117, val_acc:0.941]
Epoch [117/120    avg_loss:0.127, val_acc:0.941]
Epoch [118/120    avg_loss:0.113, val_acc:0.941]
Epoch [119/120    avg_loss:0.111, val_acc:0.941]
Epoch [120/120    avg_loss:0.112, val_acc:0.941]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6148     0    17    94     0     0     5   157    11]
 [    0     0 17512     0   362     0   216     0     0     0]
 [    0     1     0  1936     4     0     1     0    82    12]
 [    0    52    23     0  2874     0    12     0    10     1]
 [    0     0     0     0     0  1302     0     1     0     2]
 [    0     0    40     0     0     0  4777     0    61     0]
 [    0    19     0     0     0     0     3  1265     0     3]
 [    0    26     0    58    76     0     3     0  3407     1]
 [    0     1     0     0    14    30     0     0     1   873]]

Accuracy:
96.62834695008797

F1 scores:
[       nan 0.96979257 0.9820272  0.95675809 0.89868668 0.98748578
 0.96602629 0.98789535 0.93483331 0.9582876 ]

Kappa:
0.9555982862621977
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:50
Validation dataloader:50
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8efe45d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.951, val_acc:0.188]
Epoch [2/120    avg_loss:1.510, val_acc:0.391]
Epoch [3/120    avg_loss:1.205, val_acc:0.575]
Epoch [4/120    avg_loss:0.964, val_acc:0.660]
Epoch [5/120    avg_loss:0.742, val_acc:0.719]
Epoch [6/120    avg_loss:0.618, val_acc:0.766]
Epoch [7/120    avg_loss:0.483, val_acc:0.807]
Epoch [8/120    avg_loss:0.409, val_acc:0.871]
Epoch [9/120    avg_loss:0.339, val_acc:0.910]
Epoch [10/120    avg_loss:0.302, val_acc:0.885]
Epoch [11/120    avg_loss:0.288, val_acc:0.899]
Epoch [12/120    avg_loss:0.281, val_acc:0.885]
Epoch [13/120    avg_loss:0.228, val_acc:0.899]
Epoch [14/120    avg_loss:0.193, val_acc:0.925]
Epoch [15/120    avg_loss:0.163, val_acc:0.955]
Epoch [16/120    avg_loss:0.157, val_acc:0.931]
Epoch [17/120    avg_loss:0.151, val_acc:0.875]
Epoch [18/120    avg_loss:0.241, val_acc:0.936]
Epoch [19/120    avg_loss:0.140, val_acc:0.939]
Epoch [20/120    avg_loss:0.115, val_acc:0.952]
Epoch [21/120    avg_loss:0.145, val_acc:0.954]
Epoch [22/120    avg_loss:0.119, val_acc:0.950]
Epoch [23/120    avg_loss:0.098, val_acc:0.952]
Epoch [24/120    avg_loss:0.101, val_acc:0.962]
Epoch [25/120    avg_loss:0.088, val_acc:0.961]
Epoch [26/120    avg_loss:0.079, val_acc:0.970]
Epoch [27/120    avg_loss:0.070, val_acc:0.961]
Epoch [28/120    avg_loss:0.067, val_acc:0.967]
Epoch [29/120    avg_loss:0.063, val_acc:0.964]
Epoch [30/120    avg_loss:0.070, val_acc:0.976]
Epoch [31/120    avg_loss:0.062, val_acc:0.971]
Epoch [32/120    avg_loss:0.056, val_acc:0.979]
Epoch [33/120    avg_loss:0.053, val_acc:0.981]
Epoch [34/120    avg_loss:0.045, val_acc:0.977]
Epoch [35/120    avg_loss:0.041, val_acc:0.962]
Epoch [36/120    avg_loss:0.040, val_acc:0.983]
Epoch [37/120    avg_loss:0.030, val_acc:0.974]
Epoch [38/120    avg_loss:0.030, val_acc:0.979]
Epoch [39/120    avg_loss:0.030, val_acc:0.981]
Epoch [40/120    avg_loss:0.027, val_acc:0.987]
Epoch [41/120    avg_loss:0.022, val_acc:0.988]
Epoch [42/120    avg_loss:0.021, val_acc:0.980]
Epoch [43/120    avg_loss:0.029, val_acc:0.973]
Epoch [44/120    avg_loss:0.027, val_acc:0.986]
Epoch [45/120    avg_loss:0.013, val_acc:0.977]
Epoch [46/120    avg_loss:0.041, val_acc:0.987]
Epoch [47/120    avg_loss:0.036, val_acc:0.987]
Epoch [48/120    avg_loss:0.061, val_acc:0.965]
Epoch [49/120    avg_loss:0.037, val_acc:0.931]
Epoch [50/120    avg_loss:0.024, val_acc:0.981]
Epoch [51/120    avg_loss:0.029, val_acc:0.966]
Epoch [52/120    avg_loss:0.108, val_acc:0.934]
Epoch [53/120    avg_loss:0.091, val_acc:0.976]
Epoch [54/120    avg_loss:0.033, val_acc:0.978]
Epoch [55/120    avg_loss:0.025, val_acc:0.987]
Epoch [56/120    avg_loss:0.020, val_acc:0.986]
Epoch [57/120    avg_loss:0.019, val_acc:0.986]
Epoch [58/120    avg_loss:0.021, val_acc:0.986]
Epoch [59/120    avg_loss:0.017, val_acc:0.986]
Epoch [60/120    avg_loss:0.017, val_acc:0.987]
Epoch [61/120    avg_loss:0.016, val_acc:0.987]
Epoch [62/120    avg_loss:0.015, val_acc:0.987]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.021, val_acc:0.988]
Epoch [65/120    avg_loss:0.017, val_acc:0.987]
Epoch [66/120    avg_loss:0.016, val_acc:0.988]
Epoch [67/120    avg_loss:0.014, val_acc:0.988]
Epoch [68/120    avg_loss:0.014, val_acc:0.988]
Epoch [69/120    avg_loss:0.013, val_acc:0.988]
Epoch [70/120    avg_loss:0.015, val_acc:0.988]
Epoch [71/120    avg_loss:0.011, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.988]
Epoch [73/120    avg_loss:0.014, val_acc:0.987]
Epoch [74/120    avg_loss:0.013, val_acc:0.987]
Epoch [75/120    avg_loss:0.014, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.988]
Epoch [77/120    avg_loss:0.012, val_acc:0.988]
Epoch [78/120    avg_loss:0.011, val_acc:0.988]
Epoch [79/120    avg_loss:0.013, val_acc:0.988]
Epoch [80/120    avg_loss:0.011, val_acc:0.988]
Epoch [81/120    avg_loss:0.012, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.989]
Epoch [83/120    avg_loss:0.015, val_acc:0.988]
Epoch [84/120    avg_loss:0.012, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.989]
Epoch [86/120    avg_loss:0.013, val_acc:0.989]
Epoch [87/120    avg_loss:0.014, val_acc:0.989]
Epoch [88/120    avg_loss:0.011, val_acc:0.989]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.013, val_acc:0.989]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.011, val_acc:0.988]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.012, val_acc:0.989]
Epoch [95/120    avg_loss:0.010, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.013, val_acc:0.989]
Epoch [98/120    avg_loss:0.010, val_acc:0.989]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.989]
Epoch [101/120    avg_loss:0.009, val_acc:0.989]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.010, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.010, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.990]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.011, val_acc:0.989]
Epoch [110/120    avg_loss:0.015, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.989]
Epoch [114/120    avg_loss:0.011, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     0     0     0     0     0    62     0]
 [    0     5 18000     0    33     0    50     0     2     0]
 [    0     8     0  1950     0     0     0     0    72     6]
 [    0    18     1     0  2925     0     5     0    22     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     0     0     0  4861     0    12     0]
 [    0     1     0     0     0     0     0  1281     0     8]
 [    0     2     0     3    54     0     0     0  3488    24]
 [    0     0     0     0     1    23     0     0     0   895]]

Accuracy:
98.99260116164173

F1 scores:
[       nan 0.99252103 0.99734043 0.97768864 0.97744361 0.99126472
 0.99264856 0.99649942 0.96500207 0.96600108]

Kappa:
0.9866638641729281
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5539c8940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.023, val_acc:0.205]
Epoch [2/120    avg_loss:1.583, val_acc:0.316]
Epoch [3/120    avg_loss:1.324, val_acc:0.373]
Epoch [4/120    avg_loss:1.099, val_acc:0.475]
Epoch [5/120    avg_loss:0.902, val_acc:0.537]
Epoch [6/120    avg_loss:0.765, val_acc:0.688]
Epoch [7/120    avg_loss:0.645, val_acc:0.779]
Epoch [8/120    avg_loss:0.513, val_acc:0.811]
Epoch [9/120    avg_loss:0.454, val_acc:0.884]
Epoch [10/120    avg_loss:0.345, val_acc:0.861]
Epoch [11/120    avg_loss:0.354, val_acc:0.895]
Epoch [12/120    avg_loss:0.311, val_acc:0.919]
Epoch [13/120    avg_loss:0.258, val_acc:0.911]
Epoch [14/120    avg_loss:0.307, val_acc:0.895]
Epoch [15/120    avg_loss:0.255, val_acc:0.902]
Epoch [16/120    avg_loss:0.229, val_acc:0.944]
Epoch [17/120    avg_loss:0.187, val_acc:0.939]
Epoch [18/120    avg_loss:0.189, val_acc:0.949]
Epoch [19/120    avg_loss:0.153, val_acc:0.945]
Epoch [20/120    avg_loss:0.174, val_acc:0.935]
Epoch [21/120    avg_loss:0.153, val_acc:0.963]
Epoch [22/120    avg_loss:0.096, val_acc:0.962]
Epoch [23/120    avg_loss:0.127, val_acc:0.959]
Epoch [24/120    avg_loss:0.085, val_acc:0.959]
Epoch [25/120    avg_loss:0.093, val_acc:0.957]
Epoch [26/120    avg_loss:0.097, val_acc:0.963]
Epoch [27/120    avg_loss:0.076, val_acc:0.962]
Epoch [28/120    avg_loss:0.068, val_acc:0.969]
Epoch [29/120    avg_loss:0.068, val_acc:0.972]
Epoch [30/120    avg_loss:0.071, val_acc:0.964]
Epoch [31/120    avg_loss:0.058, val_acc:0.978]
Epoch [32/120    avg_loss:0.052, val_acc:0.973]
Epoch [33/120    avg_loss:0.070, val_acc:0.976]
Epoch [34/120    avg_loss:0.038, val_acc:0.980]
Epoch [35/120    avg_loss:0.035, val_acc:0.960]
Epoch [36/120    avg_loss:0.043, val_acc:0.969]
Epoch [37/120    avg_loss:0.039, val_acc:0.980]
Epoch [38/120    avg_loss:0.045, val_acc:0.959]
Epoch [39/120    avg_loss:0.067, val_acc:0.945]
Epoch [40/120    avg_loss:0.722, val_acc:0.548]
Epoch [41/120    avg_loss:0.842, val_acc:0.656]
Epoch [42/120    avg_loss:0.664, val_acc:0.724]
Epoch [43/120    avg_loss:0.590, val_acc:0.768]
Epoch [44/120    avg_loss:0.477, val_acc:0.819]
Epoch [45/120    avg_loss:0.423, val_acc:0.866]
Epoch [46/120    avg_loss:0.390, val_acc:0.851]
Epoch [47/120    avg_loss:0.319, val_acc:0.855]
Epoch [48/120    avg_loss:0.357, val_acc:0.855]
Epoch [49/120    avg_loss:0.302, val_acc:0.923]
Epoch [50/120    avg_loss:0.231, val_acc:0.929]
Epoch [51/120    avg_loss:0.202, val_acc:0.940]
Epoch [52/120    avg_loss:0.177, val_acc:0.946]
Epoch [53/120    avg_loss:0.159, val_acc:0.957]
Epoch [54/120    avg_loss:0.161, val_acc:0.951]
Epoch [55/120    avg_loss:0.150, val_acc:0.953]
Epoch [56/120    avg_loss:0.146, val_acc:0.960]
Epoch [57/120    avg_loss:0.145, val_acc:0.958]
Epoch [58/120    avg_loss:0.132, val_acc:0.957]
Epoch [59/120    avg_loss:0.140, val_acc:0.958]
Epoch [60/120    avg_loss:0.127, val_acc:0.952]
Epoch [61/120    avg_loss:0.129, val_acc:0.955]
Epoch [62/120    avg_loss:0.129, val_acc:0.967]
Epoch [63/120    avg_loss:0.137, val_acc:0.955]
Epoch [64/120    avg_loss:0.121, val_acc:0.962]
Epoch [65/120    avg_loss:0.131, val_acc:0.963]
Epoch [66/120    avg_loss:0.122, val_acc:0.964]
Epoch [67/120    avg_loss:0.129, val_acc:0.964]
Epoch [68/120    avg_loss:0.137, val_acc:0.964]
Epoch [69/120    avg_loss:0.121, val_acc:0.964]
Epoch [70/120    avg_loss:0.114, val_acc:0.964]
Epoch [71/120    avg_loss:0.120, val_acc:0.965]
Epoch [72/120    avg_loss:0.127, val_acc:0.965]
Epoch [73/120    avg_loss:0.135, val_acc:0.965]
Epoch [74/120    avg_loss:0.129, val_acc:0.964]
Epoch [75/120    avg_loss:0.129, val_acc:0.964]
Epoch [76/120    avg_loss:0.122, val_acc:0.965]
Epoch [77/120    avg_loss:0.116, val_acc:0.964]
Epoch [78/120    avg_loss:0.119, val_acc:0.964]
Epoch [79/120    avg_loss:0.118, val_acc:0.964]
Epoch [80/120    avg_loss:0.120, val_acc:0.964]
Epoch [81/120    avg_loss:0.124, val_acc:0.964]
Epoch [82/120    avg_loss:0.115, val_acc:0.964]
Epoch [83/120    avg_loss:0.129, val_acc:0.964]
Epoch [84/120    avg_loss:0.116, val_acc:0.964]
Epoch [85/120    avg_loss:0.114, val_acc:0.964]
Epoch [86/120    avg_loss:0.123, val_acc:0.964]
Epoch [87/120    avg_loss:0.114, val_acc:0.964]
Epoch [88/120    avg_loss:0.126, val_acc:0.964]
Epoch [89/120    avg_loss:0.112, val_acc:0.964]
Epoch [90/120    avg_loss:0.120, val_acc:0.964]
Epoch [91/120    avg_loss:0.119, val_acc:0.964]
Epoch [92/120    avg_loss:0.127, val_acc:0.964]
Epoch [93/120    avg_loss:0.127, val_acc:0.964]
Epoch [94/120    avg_loss:0.119, val_acc:0.964]
Epoch [95/120    avg_loss:0.122, val_acc:0.964]
Epoch [96/120    avg_loss:0.125, val_acc:0.964]
Epoch [97/120    avg_loss:0.124, val_acc:0.964]
Epoch [98/120    avg_loss:0.126, val_acc:0.964]
Epoch [99/120    avg_loss:0.124, val_acc:0.964]
Epoch [100/120    avg_loss:0.115, val_acc:0.964]
Epoch [101/120    avg_loss:0.111, val_acc:0.964]
Epoch [102/120    avg_loss:0.120, val_acc:0.964]
Epoch [103/120    avg_loss:0.122, val_acc:0.964]
Epoch [104/120    avg_loss:0.119, val_acc:0.964]
Epoch [105/120    avg_loss:0.119, val_acc:0.964]
Epoch [106/120    avg_loss:0.126, val_acc:0.964]
Epoch [107/120    avg_loss:0.121, val_acc:0.964]
Epoch [108/120    avg_loss:0.126, val_acc:0.964]
Epoch [109/120    avg_loss:0.129, val_acc:0.964]
Epoch [110/120    avg_loss:0.118, val_acc:0.964]
Epoch [111/120    avg_loss:0.126, val_acc:0.964]
Epoch [112/120    avg_loss:0.114, val_acc:0.964]
Epoch [113/120    avg_loss:0.117, val_acc:0.964]
Epoch [114/120    avg_loss:0.111, val_acc:0.964]
Epoch [115/120    avg_loss:0.124, val_acc:0.964]
Epoch [116/120    avg_loss:0.115, val_acc:0.964]
Epoch [117/120    avg_loss:0.117, val_acc:0.964]
Epoch [118/120    avg_loss:0.125, val_acc:0.964]
Epoch [119/120    avg_loss:0.114, val_acc:0.964]
Epoch [120/120    avg_loss:0.118, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5744     0    56   141     0     4    29   296   162]
 [    0     5 17762     0   310     0    10     0     3     0]
 [    0    20     0  1979     0     0     0     0    23    14]
 [    0    70    61     0  2801     0    10     0    25     5]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0    11     0     0     0  4863     0     4     0]
 [    0    19     0     0     0     0     2  1260     1     8]
 [    0    99     0    18    69     0     2     0  3370    13]
 [    0     1     0     8    33    54     0     0     0   823]]

Accuracy:
96.17284843226568

F1 scores:
[       nan 0.92719935 0.98886538 0.96607274 0.88555169 0.97896319
 0.99559832 0.97712292 0.92417387 0.84583762]

Kappa:
0.9495104015610092
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f813335b978>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.981, val_acc:0.217]
Epoch [2/120    avg_loss:1.540, val_acc:0.404]
Epoch [3/120    avg_loss:1.267, val_acc:0.574]
Epoch [4/120    avg_loss:1.043, val_acc:0.613]
Epoch [5/120    avg_loss:0.830, val_acc:0.653]
Epoch [6/120    avg_loss:0.669, val_acc:0.652]
Epoch [7/120    avg_loss:0.569, val_acc:0.721]
Epoch [8/120    avg_loss:0.487, val_acc:0.754]
Epoch [9/120    avg_loss:0.427, val_acc:0.776]
Epoch [10/120    avg_loss:0.394, val_acc:0.801]
Epoch [11/120    avg_loss:0.376, val_acc:0.821]
Epoch [12/120    avg_loss:0.314, val_acc:0.855]
Epoch [13/120    avg_loss:0.275, val_acc:0.794]
Epoch [14/120    avg_loss:0.272, val_acc:0.867]
Epoch [15/120    avg_loss:0.244, val_acc:0.836]
Epoch [16/120    avg_loss:0.205, val_acc:0.870]
Epoch [17/120    avg_loss:0.198, val_acc:0.901]
Epoch [18/120    avg_loss:0.227, val_acc:0.854]
Epoch [19/120    avg_loss:0.195, val_acc:0.931]
Epoch [20/120    avg_loss:0.152, val_acc:0.938]
Epoch [21/120    avg_loss:0.102, val_acc:0.957]
Epoch [22/120    avg_loss:0.121, val_acc:0.946]
Epoch [23/120    avg_loss:0.155, val_acc:0.946]
Epoch [24/120    avg_loss:0.117, val_acc:0.944]
Epoch [25/120    avg_loss:0.096, val_acc:0.953]
Epoch [26/120    avg_loss:0.070, val_acc:0.963]
Epoch [27/120    avg_loss:0.087, val_acc:0.967]
Epoch [28/120    avg_loss:0.085, val_acc:0.946]
Epoch [29/120    avg_loss:0.074, val_acc:0.943]
Epoch [30/120    avg_loss:0.062, val_acc:0.925]
Epoch [31/120    avg_loss:0.036, val_acc:0.962]
Epoch [32/120    avg_loss:0.063, val_acc:0.971]
Epoch [33/120    avg_loss:0.054, val_acc:0.969]
Epoch [34/120    avg_loss:0.056, val_acc:0.967]
Epoch [35/120    avg_loss:0.073, val_acc:0.972]
Epoch [36/120    avg_loss:0.048, val_acc:0.972]
Epoch [37/120    avg_loss:0.042, val_acc:0.978]
Epoch [38/120    avg_loss:0.041, val_acc:0.966]
Epoch [39/120    avg_loss:0.033, val_acc:0.970]
Epoch [40/120    avg_loss:0.050, val_acc:0.969]
Epoch [41/120    avg_loss:0.119, val_acc:0.963]
Epoch [42/120    avg_loss:0.051, val_acc:0.967]
Epoch [43/120    avg_loss:0.060, val_acc:0.974]
Epoch [44/120    avg_loss:0.051, val_acc:0.963]
Epoch [45/120    avg_loss:0.044, val_acc:0.976]
Epoch [46/120    avg_loss:0.029, val_acc:0.976]
Epoch [47/120    avg_loss:0.029, val_acc:0.977]
Epoch [48/120    avg_loss:0.019, val_acc:0.979]
Epoch [49/120    avg_loss:0.017, val_acc:0.980]
Epoch [50/120    avg_loss:0.019, val_acc:0.982]
Epoch [51/120    avg_loss:0.081, val_acc:0.957]
Epoch [52/120    avg_loss:0.054, val_acc:0.979]
Epoch [53/120    avg_loss:0.025, val_acc:0.977]
Epoch [54/120    avg_loss:0.047, val_acc:0.980]
Epoch [55/120    avg_loss:0.023, val_acc:0.982]
Epoch [56/120    avg_loss:0.014, val_acc:0.980]
Epoch [57/120    avg_loss:0.020, val_acc:0.980]
Epoch [58/120    avg_loss:0.015, val_acc:0.981]
Epoch [59/120    avg_loss:0.012, val_acc:0.985]
Epoch [60/120    avg_loss:0.011, val_acc:0.985]
Epoch [61/120    avg_loss:0.011, val_acc:0.984]
Epoch [62/120    avg_loss:0.015, val_acc:0.952]
Epoch [63/120    avg_loss:0.051, val_acc:0.958]
Epoch [64/120    avg_loss:0.035, val_acc:0.968]
Epoch [65/120    avg_loss:0.068, val_acc:0.971]
Epoch [66/120    avg_loss:0.045, val_acc:0.963]
Epoch [67/120    avg_loss:0.035, val_acc:0.977]
Epoch [68/120    avg_loss:0.030, val_acc:0.980]
Epoch [69/120    avg_loss:0.021, val_acc:0.986]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.015, val_acc:0.954]
Epoch [72/120    avg_loss:0.010, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.985]
Epoch [74/120    avg_loss:0.018, val_acc:0.981]
Epoch [75/120    avg_loss:0.016, val_acc:0.985]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.979]
Epoch [78/120    avg_loss:0.018, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.012, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.018, val_acc:0.986]
Epoch [88/120    avg_loss:0.030, val_acc:0.972]
Epoch [89/120    avg_loss:0.072, val_acc:0.976]
Epoch [90/120    avg_loss:0.023, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.982]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.017, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.004, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6374     0     0     2     0     0     0    26    30]
 [    0     0 18031     0    46     0    11     0     2     0]
 [    0     2     0  1999     2     0     0     0    27     6]
 [    0    25    14     0  2898     0     7     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    16     0     0  4862     0     0     0]
 [    0     1     0     0     0     0     1  1285     0     3]
 [    0     4     0     0    56     0     0     0  3496    15]
 [    0     0     0     1    14    47     0     0     0   857]]

Accuracy:
99.06972260381269

F1 scores:
[       nan 0.99298956 0.9979798  0.98667325 0.96761269 0.98231088
 0.99641357 0.99805825 0.9779021  0.93661202]

Kappa:
0.9876807960018165
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2baba81898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.944, val_acc:0.140]
Epoch [2/120    avg_loss:1.456, val_acc:0.391]
Epoch [3/120    avg_loss:1.163, val_acc:0.463]
Epoch [4/120    avg_loss:0.976, val_acc:0.548]
Epoch [5/120    avg_loss:0.850, val_acc:0.655]
Epoch [6/120    avg_loss:0.662, val_acc:0.721]
Epoch [7/120    avg_loss:0.558, val_acc:0.814]
Epoch [8/120    avg_loss:0.455, val_acc:0.859]
Epoch [9/120    avg_loss:0.585, val_acc:0.817]
Epoch [10/120    avg_loss:0.341, val_acc:0.821]
Epoch [11/120    avg_loss:0.311, val_acc:0.907]
Epoch [12/120    avg_loss:0.260, val_acc:0.824]
Epoch [13/120    avg_loss:0.235, val_acc:0.897]
Epoch [14/120    avg_loss:0.305, val_acc:0.828]
Epoch [15/120    avg_loss:0.264, val_acc:0.903]
Epoch [16/120    avg_loss:0.235, val_acc:0.852]
Epoch [17/120    avg_loss:0.187, val_acc:0.926]
Epoch [18/120    avg_loss:0.196, val_acc:0.912]
Epoch [19/120    avg_loss:0.188, val_acc:0.966]
Epoch [20/120    avg_loss:0.134, val_acc:0.884]
Epoch [21/120    avg_loss:1.269, val_acc:0.645]
Epoch [22/120    avg_loss:1.210, val_acc:0.763]
Epoch [23/120    avg_loss:1.024, val_acc:0.718]
Epoch [24/120    avg_loss:0.867, val_acc:0.766]
Epoch [25/120    avg_loss:0.733, val_acc:0.772]
Epoch [26/120    avg_loss:0.659, val_acc:0.806]
Epoch [27/120    avg_loss:0.599, val_acc:0.812]
Epoch [28/120    avg_loss:0.534, val_acc:0.819]
Epoch [29/120    avg_loss:0.519, val_acc:0.839]
Epoch [30/120    avg_loss:0.470, val_acc:0.826]
Epoch [31/120    avg_loss:0.428, val_acc:0.823]
Epoch [32/120    avg_loss:0.390, val_acc:0.859]
Epoch [33/120    avg_loss:0.326, val_acc:0.859]
Epoch [34/120    avg_loss:0.306, val_acc:0.874]
Epoch [35/120    avg_loss:0.324, val_acc:0.869]
Epoch [36/120    avg_loss:0.300, val_acc:0.871]
Epoch [37/120    avg_loss:0.309, val_acc:0.862]
Epoch [38/120    avg_loss:0.298, val_acc:0.872]
Epoch [39/120    avg_loss:0.294, val_acc:0.872]
Epoch [40/120    avg_loss:0.297, val_acc:0.875]
Epoch [41/120    avg_loss:0.304, val_acc:0.882]
Epoch [42/120    avg_loss:0.296, val_acc:0.876]
Epoch [43/120    avg_loss:0.291, val_acc:0.889]
Epoch [44/120    avg_loss:0.278, val_acc:0.873]
Epoch [45/120    avg_loss:0.296, val_acc:0.885]
Epoch [46/120    avg_loss:0.283, val_acc:0.885]
Epoch [47/120    avg_loss:0.274, val_acc:0.888]
Epoch [48/120    avg_loss:0.267, val_acc:0.885]
Epoch [49/120    avg_loss:0.252, val_acc:0.885]
Epoch [50/120    avg_loss:0.273, val_acc:0.885]
Epoch [51/120    avg_loss:0.267, val_acc:0.887]
Epoch [52/120    avg_loss:0.268, val_acc:0.888]
Epoch [53/120    avg_loss:0.263, val_acc:0.889]
Epoch [54/120    avg_loss:0.253, val_acc:0.889]
Epoch [55/120    avg_loss:0.281, val_acc:0.886]
Epoch [56/120    avg_loss:0.270, val_acc:0.888]
Epoch [57/120    avg_loss:0.279, val_acc:0.887]
Epoch [58/120    avg_loss:0.270, val_acc:0.889]
Epoch [59/120    avg_loss:0.284, val_acc:0.889]
Epoch [60/120    avg_loss:0.284, val_acc:0.889]
Epoch [61/120    avg_loss:0.271, val_acc:0.889]
Epoch [62/120    avg_loss:0.279, val_acc:0.889]
Epoch [63/120    avg_loss:0.256, val_acc:0.889]
Epoch [64/120    avg_loss:0.263, val_acc:0.889]
Epoch [65/120    avg_loss:0.271, val_acc:0.889]
Epoch [66/120    avg_loss:0.270, val_acc:0.889]
Epoch [67/120    avg_loss:0.267, val_acc:0.890]
Epoch [68/120    avg_loss:0.273, val_acc:0.890]
Epoch [69/120    avg_loss:0.268, val_acc:0.890]
Epoch [70/120    avg_loss:0.282, val_acc:0.890]
Epoch [71/120    avg_loss:0.262, val_acc:0.890]
Epoch [72/120    avg_loss:0.282, val_acc:0.890]
Epoch [73/120    avg_loss:0.284, val_acc:0.889]
Epoch [74/120    avg_loss:0.268, val_acc:0.889]
Epoch [75/120    avg_loss:0.259, val_acc:0.889]
Epoch [76/120    avg_loss:0.264, val_acc:0.889]
Epoch [77/120    avg_loss:0.270, val_acc:0.889]
Epoch [78/120    avg_loss:0.271, val_acc:0.889]
Epoch [79/120    avg_loss:0.259, val_acc:0.889]
Epoch [80/120    avg_loss:0.276, val_acc:0.889]
Epoch [81/120    avg_loss:0.273, val_acc:0.889]
Epoch [82/120    avg_loss:0.254, val_acc:0.889]
Epoch [83/120    avg_loss:0.280, val_acc:0.889]
Epoch [84/120    avg_loss:0.256, val_acc:0.889]
Epoch [85/120    avg_loss:0.298, val_acc:0.889]
Epoch [86/120    avg_loss:0.266, val_acc:0.889]
Epoch [87/120    avg_loss:0.286, val_acc:0.889]
Epoch [88/120    avg_loss:0.272, val_acc:0.889]
Epoch [89/120    avg_loss:0.263, val_acc:0.889]
Epoch [90/120    avg_loss:0.279, val_acc:0.889]
Epoch [91/120    avg_loss:0.286, val_acc:0.889]
Epoch [92/120    avg_loss:0.281, val_acc:0.889]
Epoch [93/120    avg_loss:0.296, val_acc:0.889]
Epoch [94/120    avg_loss:0.282, val_acc:0.889]
Epoch [95/120    avg_loss:0.270, val_acc:0.889]
Epoch [96/120    avg_loss:0.276, val_acc:0.889]
Epoch [97/120    avg_loss:0.268, val_acc:0.889]
Epoch [98/120    avg_loss:0.273, val_acc:0.889]
Epoch [99/120    avg_loss:0.268, val_acc:0.889]
Epoch [100/120    avg_loss:0.255, val_acc:0.889]
Epoch [101/120    avg_loss:0.258, val_acc:0.889]
Epoch [102/120    avg_loss:0.267, val_acc:0.889]
Epoch [103/120    avg_loss:0.266, val_acc:0.889]
Epoch [104/120    avg_loss:0.279, val_acc:0.889]
Epoch [105/120    avg_loss:0.273, val_acc:0.889]
Epoch [106/120    avg_loss:0.278, val_acc:0.889]
Epoch [107/120    avg_loss:0.271, val_acc:0.889]
Epoch [108/120    avg_loss:0.254, val_acc:0.889]
Epoch [109/120    avg_loss:0.258, val_acc:0.889]
Epoch [110/120    avg_loss:0.282, val_acc:0.889]
Epoch [111/120    avg_loss:0.290, val_acc:0.889]
Epoch [112/120    avg_loss:0.280, val_acc:0.889]
Epoch [113/120    avg_loss:0.281, val_acc:0.889]
Epoch [114/120    avg_loss:0.276, val_acc:0.889]
Epoch [115/120    avg_loss:0.292, val_acc:0.889]
Epoch [116/120    avg_loss:0.267, val_acc:0.889]
Epoch [117/120    avg_loss:0.286, val_acc:0.889]
Epoch [118/120    avg_loss:0.276, val_acc:0.889]
Epoch [119/120    avg_loss:0.276, val_acc:0.889]
Epoch [120/120    avg_loss:0.272, val_acc:0.889]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5437     0    26   257     0    34     5   551   122]
 [    0    54 15435     0    28     0  2573     0     0     0]
 [    0    33     0  1837     0     0     0     0   144    22]
 [    0   187   131     0  2494     0    14     0   124    22]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     3    90     0     5     0  4747     0    33     0]
 [    0    30     0     0     0     0     5  1213    16    26]
 [    0   223     0    35    65     0     0     0  3234    14]
 [    0    20     0     1    15    75     0     8     1   799]]

Accuracy:
87.96423493119322

F1 scores:
[       nan 0.87559385 0.91477508 0.93367217 0.854695   0.97130078
 0.77495715 0.96422893 0.84284597 0.82969886]

Kappa:
0.8445074485870345
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03bcc2a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.903, val_acc:0.174]
Epoch [2/120    avg_loss:1.542, val_acc:0.301]
Epoch [3/120    avg_loss:1.361, val_acc:0.388]
Epoch [4/120    avg_loss:1.223, val_acc:0.408]
Epoch [5/120    avg_loss:1.019, val_acc:0.423]
Epoch [6/120    avg_loss:0.856, val_acc:0.551]
Epoch [7/120    avg_loss:0.777, val_acc:0.758]
Epoch [8/120    avg_loss:0.633, val_acc:0.782]
Epoch [9/120    avg_loss:0.527, val_acc:0.759]
Epoch [10/120    avg_loss:0.455, val_acc:0.819]
Epoch [11/120    avg_loss:0.379, val_acc:0.838]
Epoch [12/120    avg_loss:0.384, val_acc:0.828]
Epoch [13/120    avg_loss:0.324, val_acc:0.877]
Epoch [14/120    avg_loss:0.306, val_acc:0.877]
Epoch [15/120    avg_loss:0.234, val_acc:0.914]
Epoch [16/120    avg_loss:0.218, val_acc:0.901]
Epoch [17/120    avg_loss:0.243, val_acc:0.891]
Epoch [18/120    avg_loss:0.224, val_acc:0.923]
Epoch [19/120    avg_loss:0.211, val_acc:0.903]
Epoch [20/120    avg_loss:0.202, val_acc:0.933]
Epoch [21/120    avg_loss:0.139, val_acc:0.910]
Epoch [22/120    avg_loss:0.141, val_acc:0.897]
Epoch [23/120    avg_loss:0.113, val_acc:0.942]
Epoch [24/120    avg_loss:0.112, val_acc:0.946]
Epoch [25/120    avg_loss:0.103, val_acc:0.872]
Epoch [26/120    avg_loss:0.099, val_acc:0.953]
Epoch [27/120    avg_loss:0.074, val_acc:0.958]
Epoch [28/120    avg_loss:0.066, val_acc:0.949]
Epoch [29/120    avg_loss:0.067, val_acc:0.961]
Epoch [30/120    avg_loss:0.056, val_acc:0.963]
Epoch [31/120    avg_loss:0.060, val_acc:0.971]
Epoch [32/120    avg_loss:0.057, val_acc:0.941]
Epoch [33/120    avg_loss:0.069, val_acc:0.964]
Epoch [34/120    avg_loss:0.048, val_acc:0.957]
Epoch [35/120    avg_loss:0.033, val_acc:0.968]
Epoch [36/120    avg_loss:0.036, val_acc:0.976]
Epoch [37/120    avg_loss:0.047, val_acc:0.966]
Epoch [38/120    avg_loss:0.135, val_acc:0.934]
Epoch [39/120    avg_loss:0.067, val_acc:0.959]
Epoch [40/120    avg_loss:0.091, val_acc:0.939]
Epoch [41/120    avg_loss:0.085, val_acc:0.968]
Epoch [42/120    avg_loss:0.063, val_acc:0.963]
Epoch [43/120    avg_loss:0.051, val_acc:0.952]
Epoch [44/120    avg_loss:0.045, val_acc:0.968]
Epoch [45/120    avg_loss:0.044, val_acc:0.969]
Epoch [46/120    avg_loss:0.042, val_acc:0.948]
Epoch [47/120    avg_loss:0.054, val_acc:0.968]
Epoch [48/120    avg_loss:0.032, val_acc:0.978]
Epoch [49/120    avg_loss:0.027, val_acc:0.976]
Epoch [50/120    avg_loss:0.032, val_acc:0.980]
Epoch [51/120    avg_loss:0.021, val_acc:0.979]
Epoch [52/120    avg_loss:0.016, val_acc:0.980]
Epoch [53/120    avg_loss:0.035, val_acc:0.970]
Epoch [54/120    avg_loss:0.027, val_acc:0.980]
Epoch [55/120    avg_loss:0.032, val_acc:0.968]
Epoch [56/120    avg_loss:0.016, val_acc:0.979]
Epoch [57/120    avg_loss:0.014, val_acc:0.979]
Epoch [58/120    avg_loss:0.017, val_acc:0.979]
Epoch [59/120    avg_loss:0.020, val_acc:0.966]
Epoch [60/120    avg_loss:0.015, val_acc:0.980]
Epoch [61/120    avg_loss:0.016, val_acc:0.980]
Epoch [62/120    avg_loss:0.013, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.980]
Epoch [64/120    avg_loss:0.011, val_acc:0.978]
Epoch [65/120    avg_loss:0.009, val_acc:0.984]
Epoch [66/120    avg_loss:0.014, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.980]
Epoch [68/120    avg_loss:0.010, val_acc:0.956]
Epoch [69/120    avg_loss:0.012, val_acc:0.981]
Epoch [70/120    avg_loss:0.008, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.010, val_acc:0.978]
Epoch [73/120    avg_loss:0.010, val_acc:0.982]
Epoch [74/120    avg_loss:0.007, val_acc:0.983]
Epoch [75/120    avg_loss:0.022, val_acc:0.982]
Epoch [76/120    avg_loss:0.030, val_acc:0.975]
Epoch [77/120    avg_loss:0.016, val_acc:0.984]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.010, val_acc:0.974]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.980]
Epoch [90/120    avg_loss:0.018, val_acc:0.983]
Epoch [91/120    avg_loss:0.035, val_acc:0.981]
Epoch [92/120    avg_loss:0.012, val_acc:0.980]
Epoch [93/120    avg_loss:0.006, val_acc:0.982]
Epoch [94/120    avg_loss:0.027, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.012, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.978]
Epoch [98/120    avg_loss:0.044, val_acc:0.975]
Epoch [99/120    avg_loss:0.030, val_acc:0.980]
Epoch [100/120    avg_loss:0.018, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.982]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.982]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.982]
Epoch [112/120    avg_loss:0.005, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     0     0    19     9    65     0]
 [    0     0 18077     0    13     0     0     0     0     0]
 [    0     0     0  2032     3     0     0     0     0     1]
 [    0    21    24     0  2900     0     7     0    20     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0     0     3     0     0  4872     0     3     0]
 [    0     0     0     0     0     0     1  1281     0     8]
 [    0     0     0     8    87     0     0     0  3466    10]
 [    0     0     0     0    15    43     0     0     0   861]]

Accuracy:
99.12756368544092

F1 scores:
[       nan 0.99108818 0.99897765 0.99632263 0.96828047 0.98302527
 0.99662473 0.99302326 0.97291228 0.95613548]

Kappa:
0.9884401537439252
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efdba9df940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.984, val_acc:0.112]
Epoch [2/120    avg_loss:1.600, val_acc:0.303]
Epoch [3/120    avg_loss:1.321, val_acc:0.398]
Epoch [4/120    avg_loss:1.115, val_acc:0.527]
Epoch [5/120    avg_loss:0.900, val_acc:0.569]
Epoch [6/120    avg_loss:0.762, val_acc:0.644]
Epoch [7/120    avg_loss:0.646, val_acc:0.739]
Epoch [8/120    avg_loss:0.540, val_acc:0.766]
Epoch [9/120    avg_loss:0.468, val_acc:0.793]
Epoch [10/120    avg_loss:0.403, val_acc:0.778]
Epoch [11/120    avg_loss:0.344, val_acc:0.821]
Epoch [12/120    avg_loss:0.304, val_acc:0.886]
Epoch [13/120    avg_loss:0.276, val_acc:0.855]
Epoch [14/120    avg_loss:0.467, val_acc:0.801]
Epoch [15/120    avg_loss:0.296, val_acc:0.873]
Epoch [16/120    avg_loss:0.316, val_acc:0.905]
Epoch [17/120    avg_loss:0.265, val_acc:0.920]
Epoch [18/120    avg_loss:0.226, val_acc:0.927]
Epoch [19/120    avg_loss:0.202, val_acc:0.907]
Epoch [20/120    avg_loss:0.186, val_acc:0.935]
Epoch [21/120    avg_loss:0.137, val_acc:0.950]
Epoch [22/120    avg_loss:0.144, val_acc:0.948]
Epoch [23/120    avg_loss:0.125, val_acc:0.918]
Epoch [24/120    avg_loss:0.125, val_acc:0.947]
Epoch [25/120    avg_loss:0.102, val_acc:0.952]
Epoch [26/120    avg_loss:0.077, val_acc:0.963]
Epoch [27/120    avg_loss:0.097, val_acc:0.936]
Epoch [28/120    avg_loss:0.065, val_acc:0.964]
Epoch [29/120    avg_loss:0.063, val_acc:0.966]
Epoch [30/120    avg_loss:0.079, val_acc:0.955]
Epoch [31/120    avg_loss:0.067, val_acc:0.954]
Epoch [32/120    avg_loss:0.059, val_acc:0.968]
Epoch [33/120    avg_loss:0.044, val_acc:0.973]
Epoch [34/120    avg_loss:0.040, val_acc:0.970]
Epoch [35/120    avg_loss:0.047, val_acc:0.944]
Epoch [36/120    avg_loss:1.052, val_acc:0.302]
Epoch [37/120    avg_loss:1.184, val_acc:0.492]
Epoch [38/120    avg_loss:1.032, val_acc:0.580]
Epoch [39/120    avg_loss:0.992, val_acc:0.523]
Epoch [40/120    avg_loss:0.946, val_acc:0.564]
Epoch [41/120    avg_loss:0.897, val_acc:0.618]
Epoch [42/120    avg_loss:0.892, val_acc:0.681]
Epoch [43/120    avg_loss:0.855, val_acc:0.693]
Epoch [44/120    avg_loss:0.827, val_acc:0.679]
Epoch [45/120    avg_loss:0.752, val_acc:0.661]
Epoch [46/120    avg_loss:0.745, val_acc:0.684]
Epoch [47/120    avg_loss:0.677, val_acc:0.727]
Epoch [48/120    avg_loss:0.658, val_acc:0.732]
Epoch [49/120    avg_loss:0.667, val_acc:0.738]
Epoch [50/120    avg_loss:0.654, val_acc:0.728]
Epoch [51/120    avg_loss:0.639, val_acc:0.733]
Epoch [52/120    avg_loss:0.627, val_acc:0.724]
Epoch [53/120    avg_loss:0.644, val_acc:0.734]
Epoch [54/120    avg_loss:0.650, val_acc:0.734]
Epoch [55/120    avg_loss:0.630, val_acc:0.728]
Epoch [56/120    avg_loss:0.646, val_acc:0.727]
Epoch [57/120    avg_loss:0.636, val_acc:0.732]
Epoch [58/120    avg_loss:0.606, val_acc:0.734]
Epoch [59/120    avg_loss:0.641, val_acc:0.740]
Epoch [60/120    avg_loss:0.615, val_acc:0.736]
Epoch [61/120    avg_loss:0.664, val_acc:0.744]
Epoch [62/120    avg_loss:0.604, val_acc:0.743]
Epoch [63/120    avg_loss:0.641, val_acc:0.740]
Epoch [64/120    avg_loss:0.613, val_acc:0.743]
Epoch [65/120    avg_loss:0.635, val_acc:0.747]
Epoch [66/120    avg_loss:0.640, val_acc:0.740]
Epoch [67/120    avg_loss:0.631, val_acc:0.739]
Epoch [68/120    avg_loss:0.636, val_acc:0.746]
Epoch [69/120    avg_loss:0.619, val_acc:0.744]
Epoch [70/120    avg_loss:0.625, val_acc:0.741]
Epoch [71/120    avg_loss:0.637, val_acc:0.746]
Epoch [72/120    avg_loss:0.619, val_acc:0.748]
Epoch [73/120    avg_loss:0.642, val_acc:0.747]
Epoch [74/120    avg_loss:0.616, val_acc:0.747]
Epoch [75/120    avg_loss:0.622, val_acc:0.747]
Epoch [76/120    avg_loss:0.625, val_acc:0.747]
Epoch [77/120    avg_loss:0.608, val_acc:0.747]
Epoch [78/120    avg_loss:0.604, val_acc:0.746]
Epoch [79/120    avg_loss:0.621, val_acc:0.745]
Epoch [80/120    avg_loss:0.629, val_acc:0.747]
Epoch [81/120    avg_loss:0.602, val_acc:0.746]
Epoch [82/120    avg_loss:0.620, val_acc:0.746]
Epoch [83/120    avg_loss:0.635, val_acc:0.746]
Epoch [84/120    avg_loss:0.644, val_acc:0.746]
Epoch [85/120    avg_loss:0.613, val_acc:0.746]
Epoch [86/120    avg_loss:0.615, val_acc:0.746]
Epoch [87/120    avg_loss:0.611, val_acc:0.746]
Epoch [88/120    avg_loss:0.648, val_acc:0.746]
Epoch [89/120    avg_loss:0.639, val_acc:0.746]
Epoch [90/120    avg_loss:0.612, val_acc:0.746]
Epoch [91/120    avg_loss:0.620, val_acc:0.746]
Epoch [92/120    avg_loss:0.626, val_acc:0.746]
Epoch [93/120    avg_loss:0.623, val_acc:0.746]
Epoch [94/120    avg_loss:0.601, val_acc:0.746]
Epoch [95/120    avg_loss:0.609, val_acc:0.746]
Epoch [96/120    avg_loss:0.593, val_acc:0.746]
Epoch [97/120    avg_loss:0.619, val_acc:0.746]
Epoch [98/120    avg_loss:0.614, val_acc:0.746]
Epoch [99/120    avg_loss:0.634, val_acc:0.746]
Epoch [100/120    avg_loss:0.630, val_acc:0.746]
Epoch [101/120    avg_loss:0.616, val_acc:0.746]
Epoch [102/120    avg_loss:0.619, val_acc:0.746]
Epoch [103/120    avg_loss:0.633, val_acc:0.746]
Epoch [104/120    avg_loss:0.654, val_acc:0.746]
Epoch [105/120    avg_loss:0.617, val_acc:0.746]
Epoch [106/120    avg_loss:0.624, val_acc:0.746]
Epoch [107/120    avg_loss:0.595, val_acc:0.746]
Epoch [108/120    avg_loss:0.625, val_acc:0.746]
Epoch [109/120    avg_loss:0.628, val_acc:0.746]
Epoch [110/120    avg_loss:0.599, val_acc:0.746]
Epoch [111/120    avg_loss:0.607, val_acc:0.746]
Epoch [112/120    avg_loss:0.618, val_acc:0.746]
Epoch [113/120    avg_loss:0.637, val_acc:0.746]
Epoch [114/120    avg_loss:0.609, val_acc:0.746]
Epoch [115/120    avg_loss:0.620, val_acc:0.746]
Epoch [116/120    avg_loss:0.619, val_acc:0.746]
Epoch [117/120    avg_loss:0.611, val_acc:0.746]
Epoch [118/120    avg_loss:0.613, val_acc:0.746]
Epoch [119/120    avg_loss:0.607, val_acc:0.746]
Epoch [120/120    avg_loss:0.627, val_acc:0.746]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  3840   459   178   564     0   843     8   416   124]
 [    0   258 12055   257   722     0  3740     0  1058     0]
 [    0     5     4  1720    10     0    19     0   197    81]
 [    0    52   105     0  2560     0   178     0    66    11]
 [    0     0     0     0     0  1301     0     2     0     2]
 [    0     0   800   160    68     0  3760     0    90     0]
 [    0    65     0     0    34     0     0  1168    10    13]
 [    0    60    63    12   221     0   152     0  3063     0]
 [    0    27     0    12    19    50    13     0     2   796]]

Accuracy:
72.93519388812571

F1 scores:
[       nan 0.71515039 0.7635546  0.78628571 0.71408647 0.97966867
 0.55363322 0.9465154  0.72300248 0.81808839]

Kappa:
0.6595235336278611
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f1cba05c0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.932, val_acc:0.358]
Epoch [2/120    avg_loss:1.512, val_acc:0.645]
Epoch [3/120    avg_loss:1.288, val_acc:0.702]
Epoch [4/120    avg_loss:1.075, val_acc:0.681]
Epoch [5/120    avg_loss:0.901, val_acc:0.734]
Epoch [6/120    avg_loss:0.752, val_acc:0.722]
Epoch [7/120    avg_loss:0.619, val_acc:0.696]
Epoch [8/120    avg_loss:0.509, val_acc:0.684]
Epoch [9/120    avg_loss:0.432, val_acc:0.781]
Epoch [10/120    avg_loss:0.389, val_acc:0.833]
Epoch [11/120    avg_loss:0.300, val_acc:0.910]
Epoch [12/120    avg_loss:0.296, val_acc:0.891]
Epoch [13/120    avg_loss:0.234, val_acc:0.923]
Epoch [14/120    avg_loss:0.214, val_acc:0.833]
Epoch [15/120    avg_loss:0.180, val_acc:0.860]
Epoch [16/120    avg_loss:0.163, val_acc:0.915]
Epoch [17/120    avg_loss:0.143, val_acc:0.944]
Epoch [18/120    avg_loss:0.123, val_acc:0.966]
Epoch [19/120    avg_loss:0.437, val_acc:0.491]
Epoch [20/120    avg_loss:1.099, val_acc:0.495]
Epoch [21/120    avg_loss:0.941, val_acc:0.602]
Epoch [22/120    avg_loss:0.842, val_acc:0.707]
Epoch [23/120    avg_loss:0.726, val_acc:0.745]
Epoch [24/120    avg_loss:0.688, val_acc:0.768]
Epoch [25/120    avg_loss:0.610, val_acc:0.792]
Epoch [26/120    avg_loss:0.620, val_acc:0.754]
Epoch [27/120    avg_loss:0.579, val_acc:0.832]
Epoch [28/120    avg_loss:0.476, val_acc:0.828]
Epoch [29/120    avg_loss:0.451, val_acc:0.804]
Epoch [30/120    avg_loss:0.424, val_acc:0.822]
Epoch [31/120    avg_loss:0.409, val_acc:0.864]
Epoch [32/120    avg_loss:0.354, val_acc:0.872]
Epoch [33/120    avg_loss:0.303, val_acc:0.863]
Epoch [34/120    avg_loss:0.296, val_acc:0.894]
Epoch [35/120    avg_loss:0.275, val_acc:0.889]
Epoch [36/120    avg_loss:0.287, val_acc:0.885]
Epoch [37/120    avg_loss:0.276, val_acc:0.897]
Epoch [38/120    avg_loss:0.291, val_acc:0.896]
Epoch [39/120    avg_loss:0.266, val_acc:0.898]
Epoch [40/120    avg_loss:0.259, val_acc:0.900]
Epoch [41/120    avg_loss:0.256, val_acc:0.906]
Epoch [42/120    avg_loss:0.270, val_acc:0.914]
Epoch [43/120    avg_loss:0.263, val_acc:0.900]
Epoch [44/120    avg_loss:0.260, val_acc:0.913]
Epoch [45/120    avg_loss:0.247, val_acc:0.914]
Epoch [46/120    avg_loss:0.233, val_acc:0.916]
Epoch [47/120    avg_loss:0.258, val_acc:0.914]
Epoch [48/120    avg_loss:0.243, val_acc:0.912]
Epoch [49/120    avg_loss:0.248, val_acc:0.913]
Epoch [50/120    avg_loss:0.264, val_acc:0.912]
Epoch [51/120    avg_loss:0.262, val_acc:0.917]
Epoch [52/120    avg_loss:0.251, val_acc:0.914]
Epoch [53/120    avg_loss:0.246, val_acc:0.916]
Epoch [54/120    avg_loss:0.243, val_acc:0.918]
Epoch [55/120    avg_loss:0.242, val_acc:0.916]
Epoch [56/120    avg_loss:0.246, val_acc:0.919]
Epoch [57/120    avg_loss:0.238, val_acc:0.919]
Epoch [58/120    avg_loss:0.237, val_acc:0.919]
Epoch [59/120    avg_loss:0.241, val_acc:0.919]
Epoch [60/120    avg_loss:0.250, val_acc:0.918]
Epoch [61/120    avg_loss:0.246, val_acc:0.918]
Epoch [62/120    avg_loss:0.253, val_acc:0.918]
Epoch [63/120    avg_loss:0.244, val_acc:0.918]
Epoch [64/120    avg_loss:0.244, val_acc:0.918]
Epoch [65/120    avg_loss:0.248, val_acc:0.917]
Epoch [66/120    avg_loss:0.228, val_acc:0.918]
Epoch [67/120    avg_loss:0.243, val_acc:0.917]
Epoch [68/120    avg_loss:0.222, val_acc:0.917]
Epoch [69/120    avg_loss:0.236, val_acc:0.918]
Epoch [70/120    avg_loss:0.234, val_acc:0.916]
Epoch [71/120    avg_loss:0.236, val_acc:0.916]
Epoch [72/120    avg_loss:0.242, val_acc:0.917]
Epoch [73/120    avg_loss:0.259, val_acc:0.918]
Epoch [74/120    avg_loss:0.234, val_acc:0.918]
Epoch [75/120    avg_loss:0.249, val_acc:0.918]
Epoch [76/120    avg_loss:0.225, val_acc:0.918]
Epoch [77/120    avg_loss:0.253, val_acc:0.918]
Epoch [78/120    avg_loss:0.246, val_acc:0.918]
Epoch [79/120    avg_loss:0.235, val_acc:0.918]
Epoch [80/120    avg_loss:0.243, val_acc:0.918]
Epoch [81/120    avg_loss:0.246, val_acc:0.918]
Epoch [82/120    avg_loss:0.234, val_acc:0.918]
Epoch [83/120    avg_loss:0.237, val_acc:0.918]
Epoch [84/120    avg_loss:0.247, val_acc:0.918]
Epoch [85/120    avg_loss:0.235, val_acc:0.918]
Epoch [86/120    avg_loss:0.246, val_acc:0.918]
Epoch [87/120    avg_loss:0.242, val_acc:0.918]
Epoch [88/120    avg_loss:0.232, val_acc:0.918]
Epoch [89/120    avg_loss:0.241, val_acc:0.918]
Epoch [90/120    avg_loss:0.259, val_acc:0.918]
Epoch [91/120    avg_loss:0.245, val_acc:0.918]
Epoch [92/120    avg_loss:0.245, val_acc:0.918]
Epoch [93/120    avg_loss:0.240, val_acc:0.918]
Epoch [94/120    avg_loss:0.255, val_acc:0.918]
Epoch [95/120    avg_loss:0.242, val_acc:0.918]
Epoch [96/120    avg_loss:0.229, val_acc:0.918]
Epoch [97/120    avg_loss:0.236, val_acc:0.918]
Epoch [98/120    avg_loss:0.245, val_acc:0.918]
Epoch [99/120    avg_loss:0.236, val_acc:0.918]
Epoch [100/120    avg_loss:0.235, val_acc:0.918]
Epoch [101/120    avg_loss:0.241, val_acc:0.918]
Epoch [102/120    avg_loss:0.240, val_acc:0.918]
Epoch [103/120    avg_loss:0.239, val_acc:0.918]
Epoch [104/120    avg_loss:0.244, val_acc:0.918]
Epoch [105/120    avg_loss:0.246, val_acc:0.918]
Epoch [106/120    avg_loss:0.240, val_acc:0.918]
Epoch [107/120    avg_loss:0.232, val_acc:0.918]
Epoch [108/120    avg_loss:0.231, val_acc:0.918]
Epoch [109/120    avg_loss:0.229, val_acc:0.918]
Epoch [110/120    avg_loss:0.259, val_acc:0.918]
Epoch [111/120    avg_loss:0.240, val_acc:0.918]
Epoch [112/120    avg_loss:0.258, val_acc:0.918]
Epoch [113/120    avg_loss:0.243, val_acc:0.918]
Epoch [114/120    avg_loss:0.245, val_acc:0.918]
Epoch [115/120    avg_loss:0.242, val_acc:0.918]
Epoch [116/120    avg_loss:0.248, val_acc:0.918]
Epoch [117/120    avg_loss:0.248, val_acc:0.918]
Epoch [118/120    avg_loss:0.234, val_acc:0.918]
Epoch [119/120    avg_loss:0.247, val_acc:0.918]
Epoch [120/120    avg_loss:0.242, val_acc:0.918]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  5336     0   173   347     0     1    11   460   104]
 [    0     0 17729     0   339     0    22     0     0     0]
 [    0    15     0  1866     2     0     0     0   105    48]
 [    0    63    77     0  2791     0    10     0    30     1]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     4   286     0    13     0  4562     0    13     0]
 [    0    19     0     0     0     0    13  1235     0    23]
 [    0   102     0     0   146     0     0     0  3313    10]
 [    0    15     0     5    18    56     3     0     3   819]]

Accuracy:
93.88330561781505

F1 scores:
[       nan 0.8903721  0.97999005 0.91470588 0.84218467 0.97861163
 0.96153441 0.97397476 0.88405604 0.85090909]

Kappa:
0.9191683953320461
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbbaed7f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.949, val_acc:0.307]
Epoch [2/120    avg_loss:1.537, val_acc:0.532]
Epoch [3/120    avg_loss:1.257, val_acc:0.689]
Epoch [4/120    avg_loss:1.038, val_acc:0.728]
Epoch [5/120    avg_loss:0.852, val_acc:0.689]
Epoch [6/120    avg_loss:0.750, val_acc:0.720]
Epoch [7/120    avg_loss:0.574, val_acc:0.749]
Epoch [8/120    avg_loss:0.440, val_acc:0.807]
Epoch [9/120    avg_loss:0.382, val_acc:0.838]
Epoch [10/120    avg_loss:0.344, val_acc:0.859]
Epoch [11/120    avg_loss:0.345, val_acc:0.854]
Epoch [12/120    avg_loss:0.249, val_acc:0.820]
Epoch [13/120    avg_loss:0.232, val_acc:0.920]
Epoch [14/120    avg_loss:0.203, val_acc:0.918]
Epoch [15/120    avg_loss:0.228, val_acc:0.932]
Epoch [16/120    avg_loss:0.172, val_acc:0.920]
Epoch [17/120    avg_loss:0.151, val_acc:0.962]
Epoch [18/120    avg_loss:0.121, val_acc:0.922]
Epoch [19/120    avg_loss:0.131, val_acc:0.895]
Epoch [20/120    avg_loss:0.131, val_acc:0.964]
Epoch [21/120    avg_loss:0.091, val_acc:0.951]
Epoch [22/120    avg_loss:0.105, val_acc:0.956]
Epoch [23/120    avg_loss:0.106, val_acc:0.954]
Epoch [24/120    avg_loss:0.096, val_acc:0.960]
Epoch [25/120    avg_loss:0.085, val_acc:0.952]
Epoch [26/120    avg_loss:0.094, val_acc:0.957]
Epoch [27/120    avg_loss:0.118, val_acc:0.946]
Epoch [28/120    avg_loss:0.111, val_acc:0.961]
Epoch [29/120    avg_loss:0.090, val_acc:0.967]
Epoch [30/120    avg_loss:0.062, val_acc:0.946]
Epoch [31/120    avg_loss:0.055, val_acc:0.977]
Epoch [32/120    avg_loss:0.034, val_acc:0.979]
Epoch [33/120    avg_loss:0.044, val_acc:0.981]
Epoch [34/120    avg_loss:0.046, val_acc:0.976]
Epoch [35/120    avg_loss:0.051, val_acc:0.975]
Epoch [36/120    avg_loss:0.035, val_acc:0.981]
Epoch [37/120    avg_loss:0.040, val_acc:0.983]
Epoch [38/120    avg_loss:0.055, val_acc:0.975]
Epoch [39/120    avg_loss:0.042, val_acc:0.970]
Epoch [40/120    avg_loss:0.040, val_acc:0.979]
Epoch [41/120    avg_loss:0.042, val_acc:0.937]
Epoch [42/120    avg_loss:0.023, val_acc:0.979]
Epoch [43/120    avg_loss:0.028, val_acc:0.982]
Epoch [44/120    avg_loss:0.023, val_acc:0.985]
Epoch [45/120    avg_loss:0.023, val_acc:0.980]
Epoch [46/120    avg_loss:0.024, val_acc:0.980]
Epoch [47/120    avg_loss:0.047, val_acc:0.980]
Epoch [48/120    avg_loss:0.021, val_acc:0.977]
Epoch [49/120    avg_loss:0.075, val_acc:0.890]
Epoch [50/120    avg_loss:0.185, val_acc:0.934]
Epoch [51/120    avg_loss:0.138, val_acc:0.964]
Epoch [52/120    avg_loss:0.076, val_acc:0.960]
Epoch [53/120    avg_loss:0.050, val_acc:0.969]
Epoch [54/120    avg_loss:0.046, val_acc:0.977]
Epoch [55/120    avg_loss:0.034, val_acc:0.977]
Epoch [56/120    avg_loss:0.031, val_acc:0.982]
Epoch [57/120    avg_loss:0.022, val_acc:0.980]
Epoch [58/120    avg_loss:0.019, val_acc:0.983]
Epoch [59/120    avg_loss:0.023, val_acc:0.985]
Epoch [60/120    avg_loss:0.018, val_acc:0.984]
Epoch [61/120    avg_loss:0.023, val_acc:0.984]
Epoch [62/120    avg_loss:0.017, val_acc:0.985]
Epoch [63/120    avg_loss:0.022, val_acc:0.986]
Epoch [64/120    avg_loss:0.012, val_acc:0.986]
Epoch [65/120    avg_loss:0.018, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.987]
Epoch [67/120    avg_loss:0.014, val_acc:0.986]
Epoch [68/120    avg_loss:0.015, val_acc:0.985]
Epoch [69/120    avg_loss:0.015, val_acc:0.984]
Epoch [70/120    avg_loss:0.024, val_acc:0.984]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.014, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.984]
Epoch [74/120    avg_loss:0.021, val_acc:0.984]
Epoch [75/120    avg_loss:0.015, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.019, val_acc:0.982]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.987]
Epoch [80/120    avg_loss:0.015, val_acc:0.985]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.016, val_acc:0.985]
Epoch [83/120    avg_loss:0.012, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.987]
Epoch [86/120    avg_loss:0.014, val_acc:0.985]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.014, val_acc:0.984]
Epoch [91/120    avg_loss:0.013, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.986]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.989]
Epoch [98/120    avg_loss:0.012, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.987]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.014, val_acc:0.984]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.987]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.986]
Epoch [109/120    avg_loss:0.011, val_acc:0.987]
Epoch [110/120    avg_loss:0.011, val_acc:0.987]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.987]
Epoch [116/120    avg_loss:0.011, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.988]
Epoch [120/120    avg_loss:0.015, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6372     0     0     4     0    17     3    36     0]
 [    0     0 18032     0    20     0    37     0     1     0]
 [    0     1     0  2022     1     0     0     0     9     3]
 [    0    32    10     0  2898     0     1     0    30     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    12     0     0  4861     0     5     0]
 [    0     1     0     0     0     0     2  1285     0     2]
 [    0    14     0    32    73     0     0     0  3435    17]
 [    0     0     0     0    17    29     0     0     0   873]]

Accuracy:
99.01188152218447

F1 scores:
[       nan 0.99159664 0.99811801 0.98586056 0.96842105 0.98901099
 0.9924459  0.99689682 0.96938056 0.96198347]

Kappa:
0.9869143839568381
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe175723940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.000, val_acc:0.264]
Epoch [2/120    avg_loss:1.537, val_acc:0.321]
Epoch [3/120    avg_loss:1.305, val_acc:0.448]
Epoch [4/120    avg_loss:1.114, val_acc:0.476]
Epoch [5/120    avg_loss:0.899, val_acc:0.707]
Epoch [6/120    avg_loss:0.728, val_acc:0.762]
Epoch [7/120    avg_loss:0.632, val_acc:0.784]
Epoch [8/120    avg_loss:0.507, val_acc:0.772]
Epoch [9/120    avg_loss:0.433, val_acc:0.802]
Epoch [10/120    avg_loss:0.379, val_acc:0.800]
Epoch [11/120    avg_loss:0.341, val_acc:0.861]
Epoch [12/120    avg_loss:0.332, val_acc:0.810]
Epoch [13/120    avg_loss:0.245, val_acc:0.936]
Epoch [14/120    avg_loss:0.203, val_acc:0.917]
Epoch [15/120    avg_loss:0.201, val_acc:0.918]
Epoch [16/120    avg_loss:0.198, val_acc:0.912]
Epoch [17/120    avg_loss:0.172, val_acc:0.875]
Epoch [18/120    avg_loss:0.194, val_acc:0.852]
Epoch [19/120    avg_loss:0.249, val_acc:0.895]
Epoch [20/120    avg_loss:0.132, val_acc:0.953]
Epoch [21/120    avg_loss:0.109, val_acc:0.957]
Epoch [22/120    avg_loss:0.102, val_acc:0.922]
Epoch [23/120    avg_loss:0.081, val_acc:0.968]
Epoch [24/120    avg_loss:0.096, val_acc:0.970]
Epoch [25/120    avg_loss:0.088, val_acc:0.949]
Epoch [26/120    avg_loss:0.093, val_acc:0.914]
Epoch [27/120    avg_loss:0.126, val_acc:0.958]
Epoch [28/120    avg_loss:0.087, val_acc:0.947]
Epoch [29/120    avg_loss:0.094, val_acc:0.972]
Epoch [30/120    avg_loss:0.053, val_acc:0.961]
Epoch [31/120    avg_loss:0.044, val_acc:0.979]
Epoch [32/120    avg_loss:0.047, val_acc:0.951]
Epoch [33/120    avg_loss:0.051, val_acc:0.973]
Epoch [34/120    avg_loss:0.036, val_acc:0.965]
Epoch [35/120    avg_loss:0.052, val_acc:0.962]
Epoch [36/120    avg_loss:0.085, val_acc:0.974]
Epoch [37/120    avg_loss:0.042, val_acc:0.983]
Epoch [38/120    avg_loss:0.044, val_acc:0.987]
Epoch [39/120    avg_loss:0.027, val_acc:0.974]
Epoch [40/120    avg_loss:0.025, val_acc:0.963]
Epoch [41/120    avg_loss:0.023, val_acc:0.980]
Epoch [42/120    avg_loss:0.027, val_acc:0.991]
Epoch [43/120    avg_loss:0.018, val_acc:0.987]
Epoch [44/120    avg_loss:0.019, val_acc:0.990]
Epoch [45/120    avg_loss:0.018, val_acc:0.987]
Epoch [46/120    avg_loss:0.033, val_acc:0.974]
Epoch [47/120    avg_loss:0.028, val_acc:0.978]
Epoch [48/120    avg_loss:0.037, val_acc:0.977]
Epoch [49/120    avg_loss:0.025, val_acc:0.981]
Epoch [50/120    avg_loss:0.019, val_acc:0.960]
Epoch [51/120    avg_loss:0.089, val_acc:0.974]
Epoch [52/120    avg_loss:0.043, val_acc:0.978]
Epoch [53/120    avg_loss:0.045, val_acc:0.979]
Epoch [54/120    avg_loss:0.091, val_acc:0.927]
Epoch [55/120    avg_loss:0.116, val_acc:0.924]
Epoch [56/120    avg_loss:0.093, val_acc:0.966]
Epoch [57/120    avg_loss:0.045, val_acc:0.969]
Epoch [58/120    avg_loss:0.041, val_acc:0.964]
Epoch [59/120    avg_loss:0.043, val_acc:0.971]
Epoch [60/120    avg_loss:0.036, val_acc:0.975]
Epoch [61/120    avg_loss:0.025, val_acc:0.976]
Epoch [62/120    avg_loss:0.031, val_acc:0.977]
Epoch [63/120    avg_loss:0.027, val_acc:0.977]
Epoch [64/120    avg_loss:0.023, val_acc:0.980]
Epoch [65/120    avg_loss:0.032, val_acc:0.980]
Epoch [66/120    avg_loss:0.025, val_acc:0.980]
Epoch [67/120    avg_loss:0.022, val_acc:0.982]
Epoch [68/120    avg_loss:0.024, val_acc:0.982]
Epoch [69/120    avg_loss:0.022, val_acc:0.982]
Epoch [70/120    avg_loss:0.019, val_acc:0.983]
Epoch [71/120    avg_loss:0.021, val_acc:0.982]
Epoch [72/120    avg_loss:0.020, val_acc:0.981]
Epoch [73/120    avg_loss:0.023, val_acc:0.982]
Epoch [74/120    avg_loss:0.018, val_acc:0.982]
Epoch [75/120    avg_loss:0.026, val_acc:0.982]
Epoch [76/120    avg_loss:0.024, val_acc:0.982]
Epoch [77/120    avg_loss:0.019, val_acc:0.982]
Epoch [78/120    avg_loss:0.018, val_acc:0.982]
Epoch [79/120    avg_loss:0.023, val_acc:0.982]
Epoch [80/120    avg_loss:0.030, val_acc:0.982]
Epoch [81/120    avg_loss:0.023, val_acc:0.981]
Epoch [82/120    avg_loss:0.018, val_acc:0.981]
Epoch [83/120    avg_loss:0.023, val_acc:0.981]
Epoch [84/120    avg_loss:0.019, val_acc:0.981]
Epoch [85/120    avg_loss:0.018, val_acc:0.981]
Epoch [86/120    avg_loss:0.023, val_acc:0.981]
Epoch [87/120    avg_loss:0.020, val_acc:0.981]
Epoch [88/120    avg_loss:0.020, val_acc:0.981]
Epoch [89/120    avg_loss:0.019, val_acc:0.981]
Epoch [90/120    avg_loss:0.021, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.981]
Epoch [92/120    avg_loss:0.022, val_acc:0.981]
Epoch [93/120    avg_loss:0.017, val_acc:0.981]
Epoch [94/120    avg_loss:0.023, val_acc:0.981]
Epoch [95/120    avg_loss:0.019, val_acc:0.981]
Epoch [96/120    avg_loss:0.020, val_acc:0.981]
Epoch [97/120    avg_loss:0.021, val_acc:0.981]
Epoch [98/120    avg_loss:0.020, val_acc:0.981]
Epoch [99/120    avg_loss:0.021, val_acc:0.981]
Epoch [100/120    avg_loss:0.021, val_acc:0.981]
Epoch [101/120    avg_loss:0.021, val_acc:0.981]
Epoch [102/120    avg_loss:0.019, val_acc:0.981]
Epoch [103/120    avg_loss:0.022, val_acc:0.981]
Epoch [104/120    avg_loss:0.018, val_acc:0.981]
Epoch [105/120    avg_loss:0.017, val_acc:0.981]
Epoch [106/120    avg_loss:0.019, val_acc:0.981]
Epoch [107/120    avg_loss:0.019, val_acc:0.981]
Epoch [108/120    avg_loss:0.023, val_acc:0.981]
Epoch [109/120    avg_loss:0.023, val_acc:0.981]
Epoch [110/120    avg_loss:0.019, val_acc:0.981]
Epoch [111/120    avg_loss:0.017, val_acc:0.981]
Epoch [112/120    avg_loss:0.017, val_acc:0.981]
Epoch [113/120    avg_loss:0.017, val_acc:0.981]
Epoch [114/120    avg_loss:0.022, val_acc:0.981]
Epoch [115/120    avg_loss:0.016, val_acc:0.981]
Epoch [116/120    avg_loss:0.021, val_acc:0.981]
Epoch [117/120    avg_loss:0.019, val_acc:0.981]
Epoch [118/120    avg_loss:0.020, val_acc:0.981]
Epoch [119/120    avg_loss:0.018, val_acc:0.981]
Epoch [120/120    avg_loss:0.019, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6286     0     0    14     0    17     0   115     0]
 [    0     7 17835     0    86     0   161     0     1     0]
 [    0    16     0  2009     0     0     0     0     5     6]
 [    0    35    10     0  2889     0     3     0    33     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     7     0     0  4859     0     1     0]
 [    0     2     0     0     0     0     0  1288     0     0]
 [    0    19     0     0    61     0     0     0  3486     5]
 [    0     1     0     0    12    36     0     0     0   870]]

Accuracy:
98.39490998481672

F1 scores:
[       nan 0.98234099 0.99232182 0.99160908 0.95757375 0.98639456
 0.97983464 0.9992242  0.96672213 0.96559378]

Kappa:
0.9787867597260396
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5fe793f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.919, val_acc:0.196]
Epoch [2/120    avg_loss:1.538, val_acc:0.257]
Epoch [3/120    avg_loss:1.328, val_acc:0.355]
Epoch [4/120    avg_loss:1.113, val_acc:0.454]
Epoch [5/120    avg_loss:0.944, val_acc:0.661]
Epoch [6/120    avg_loss:0.813, val_acc:0.651]
Epoch [7/120    avg_loss:0.695, val_acc:0.691]
Epoch [8/120    avg_loss:0.601, val_acc:0.707]
Epoch [9/120    avg_loss:0.518, val_acc:0.687]
Epoch [10/120    avg_loss:0.442, val_acc:0.773]
Epoch [11/120    avg_loss:0.375, val_acc:0.849]
Epoch [12/120    avg_loss:0.306, val_acc:0.873]
Epoch [13/120    avg_loss:0.300, val_acc:0.849]
Epoch [14/120    avg_loss:0.266, val_acc:0.838]
Epoch [15/120    avg_loss:0.259, val_acc:0.905]
Epoch [16/120    avg_loss:0.192, val_acc:0.888]
Epoch [17/120    avg_loss:0.169, val_acc:0.933]
Epoch [18/120    avg_loss:0.142, val_acc:0.940]
Epoch [19/120    avg_loss:0.127, val_acc:0.950]
Epoch [20/120    avg_loss:0.121, val_acc:0.891]
Epoch [21/120    avg_loss:0.138, val_acc:0.918]
Epoch [22/120    avg_loss:0.116, val_acc:0.971]
Epoch [23/120    avg_loss:0.096, val_acc:0.967]
Epoch [24/120    avg_loss:0.096, val_acc:0.933]
Epoch [25/120    avg_loss:0.083, val_acc:0.973]
Epoch [26/120    avg_loss:0.065, val_acc:0.970]
Epoch [27/120    avg_loss:0.100, val_acc:0.968]
Epoch [28/120    avg_loss:0.087, val_acc:0.955]
Epoch [29/120    avg_loss:0.079, val_acc:0.975]
Epoch [30/120    avg_loss:0.046, val_acc:0.968]
Epoch [31/120    avg_loss:0.064, val_acc:0.973]
Epoch [32/120    avg_loss:0.054, val_acc:0.974]
Epoch [33/120    avg_loss:0.042, val_acc:0.919]
Epoch [34/120    avg_loss:0.061, val_acc:0.954]
Epoch [35/120    avg_loss:0.105, val_acc:0.950]
Epoch [36/120    avg_loss:0.071, val_acc:0.966]
Epoch [37/120    avg_loss:0.041, val_acc:0.972]
Epoch [38/120    avg_loss:0.033, val_acc:0.969]
Epoch [39/120    avg_loss:0.036, val_acc:0.972]
Epoch [40/120    avg_loss:0.050, val_acc:0.975]
Epoch [41/120    avg_loss:0.023, val_acc:0.980]
Epoch [42/120    avg_loss:0.024, val_acc:0.978]
Epoch [43/120    avg_loss:0.026, val_acc:0.976]
Epoch [44/120    avg_loss:0.035, val_acc:0.971]
Epoch [45/120    avg_loss:0.046, val_acc:0.981]
Epoch [46/120    avg_loss:0.032, val_acc:0.978]
Epoch [47/120    avg_loss:0.021, val_acc:0.982]
Epoch [48/120    avg_loss:0.018, val_acc:0.980]
Epoch [49/120    avg_loss:0.013, val_acc:0.981]
Epoch [50/120    avg_loss:0.015, val_acc:0.980]
Epoch [51/120    avg_loss:0.017, val_acc:0.983]
Epoch [52/120    avg_loss:0.016, val_acc:0.982]
Epoch [53/120    avg_loss:0.044, val_acc:0.966]
Epoch [54/120    avg_loss:0.027, val_acc:0.975]
Epoch [55/120    avg_loss:0.022, val_acc:0.970]
Epoch [56/120    avg_loss:0.020, val_acc:0.972]
Epoch [57/120    avg_loss:0.035, val_acc:0.979]
Epoch [58/120    avg_loss:0.021, val_acc:0.969]
Epoch [59/120    avg_loss:0.021, val_acc:0.982]
Epoch [60/120    avg_loss:0.036, val_acc:0.980]
Epoch [61/120    avg_loss:0.020, val_acc:0.975]
Epoch [62/120    avg_loss:0.014, val_acc:0.982]
Epoch [63/120    avg_loss:0.013, val_acc:0.985]
Epoch [64/120    avg_loss:0.012, val_acc:0.980]
Epoch [65/120    avg_loss:0.013, val_acc:0.982]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.010, val_acc:0.983]
Epoch [69/120    avg_loss:0.009, val_acc:0.985]
Epoch [70/120    avg_loss:0.011, val_acc:0.974]
Epoch [71/120    avg_loss:0.017, val_acc:0.941]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.980]
Epoch [74/120    avg_loss:0.008, val_acc:0.978]
Epoch [75/120    avg_loss:0.020, val_acc:0.982]
Epoch [76/120    avg_loss:0.013, val_acc:0.972]
Epoch [77/120    avg_loss:0.010, val_acc:0.983]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.021, val_acc:0.977]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.018, val_acc:0.973]
Epoch [91/120    avg_loss:0.012, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.955]
Epoch [96/120    avg_loss:0.010, val_acc:0.969]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.045, val_acc:0.958]
Epoch [99/120    avg_loss:0.234, val_acc:0.926]
Epoch [100/120    avg_loss:0.286, val_acc:0.904]
Epoch [101/120    avg_loss:0.164, val_acc:0.953]
Epoch [102/120    avg_loss:0.106, val_acc:0.960]
Epoch [103/120    avg_loss:0.065, val_acc:0.952]
Epoch [104/120    avg_loss:0.044, val_acc:0.945]
Epoch [105/120    avg_loss:0.037, val_acc:0.978]
Epoch [106/120    avg_loss:0.025, val_acc:0.977]
Epoch [107/120    avg_loss:0.033, val_acc:0.976]
Epoch [108/120    avg_loss:0.019, val_acc:0.980]
Epoch [109/120    avg_loss:0.019, val_acc:0.980]
Epoch [110/120    avg_loss:0.015, val_acc:0.980]
Epoch [111/120    avg_loss:0.018, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.982]
Epoch [113/120    avg_loss:0.017, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.015, val_acc:0.983]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.017, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6310     0     0     5     0     1     1   101    14]
 [    0     0 17874     0    45     0   170     0     1     0]
 [    0     7     0  1994     0     0     0     0    32     3]
 [    0    49    21     0  2866     0    10     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     8     0     0  4861     0     2     1]
 [    0     0     0     0     0     3     1  1281     0     5]
 [    0     7     0     0    65     0     0     0  3483    16]
 [    0     0     0     0    16    37     0     0     0   866]]

Accuracy:
98.42624057069868

F1 scores:
[       nan 0.98555252 0.99324831 0.98761763 0.96029486 0.98490566
 0.97994154 0.99611198 0.96535477 0.9495614 ]

Kappa:
0.9791899052107722
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:49
Validation dataloader:49
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5916275940>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.972, val_acc:0.258]
Epoch [2/120    avg_loss:1.575, val_acc:0.590]
Epoch [3/120    avg_loss:1.312, val_acc:0.696]
Epoch [4/120    avg_loss:1.136, val_acc:0.643]
Epoch [5/120    avg_loss:0.930, val_acc:0.744]
Epoch [6/120    avg_loss:0.744, val_acc:0.741]
Epoch [7/120    avg_loss:0.598, val_acc:0.787]
Epoch [8/120    avg_loss:0.521, val_acc:0.815]
Epoch [9/120    avg_loss:0.461, val_acc:0.830]
Epoch [10/120    avg_loss:0.422, val_acc:0.843]
Epoch [11/120    avg_loss:0.384, val_acc:0.866]
Epoch [12/120    avg_loss:0.334, val_acc:0.871]
Epoch [13/120    avg_loss:0.293, val_acc:0.886]
Epoch [14/120    avg_loss:0.246, val_acc:0.901]
Epoch [15/120    avg_loss:0.268, val_acc:0.904]
Epoch [16/120    avg_loss:0.231, val_acc:0.893]
Epoch [17/120    avg_loss:0.177, val_acc:0.926]
Epoch [18/120    avg_loss:0.150, val_acc:0.946]
Epoch [19/120    avg_loss:0.161, val_acc:0.941]
Epoch [20/120    avg_loss:0.137, val_acc:0.948]
Epoch [21/120    avg_loss:0.132, val_acc:0.958]
Epoch [22/120    avg_loss:0.109, val_acc:0.955]
Epoch [23/120    avg_loss:0.120, val_acc:0.942]
Epoch [24/120    avg_loss:0.118, val_acc:0.974]
Epoch [25/120    avg_loss:0.076, val_acc:0.946]
Epoch [26/120    avg_loss:0.080, val_acc:0.968]
Epoch [27/120    avg_loss:0.071, val_acc:0.970]
Epoch [28/120    avg_loss:0.072, val_acc:0.974]
Epoch [29/120    avg_loss:0.098, val_acc:0.937]
Epoch [30/120    avg_loss:0.109, val_acc:0.969]
Epoch [31/120    avg_loss:0.070, val_acc:0.976]
Epoch [32/120    avg_loss:0.070, val_acc:0.979]
Epoch [33/120    avg_loss:0.038, val_acc:0.950]
Epoch [34/120    avg_loss:0.057, val_acc:0.977]
Epoch [35/120    avg_loss:0.060, val_acc:0.979]
Epoch [36/120    avg_loss:0.053, val_acc:0.980]
Epoch [37/120    avg_loss:0.037, val_acc:0.981]
Epoch [38/120    avg_loss:0.048, val_acc:0.939]
Epoch [39/120    avg_loss:0.056, val_acc:0.975]
Epoch [40/120    avg_loss:0.046, val_acc:0.980]
Epoch [41/120    avg_loss:0.040, val_acc:0.962]
Epoch [42/120    avg_loss:0.034, val_acc:0.975]
Epoch [43/120    avg_loss:0.038, val_acc:0.959]
Epoch [44/120    avg_loss:0.055, val_acc:0.975]
Epoch [45/120    avg_loss:0.048, val_acc:0.976]
Epoch [46/120    avg_loss:0.033, val_acc:0.988]
Epoch [47/120    avg_loss:0.020, val_acc:0.973]
Epoch [48/120    avg_loss:0.024, val_acc:0.986]
Epoch [49/120    avg_loss:0.025, val_acc:0.981]
Epoch [50/120    avg_loss:0.030, val_acc:0.980]
Epoch [51/120    avg_loss:0.016, val_acc:0.978]
Epoch [52/120    avg_loss:0.017, val_acc:0.985]
Epoch [53/120    avg_loss:0.022, val_acc:0.977]
Epoch [54/120    avg_loss:0.022, val_acc:0.986]
Epoch [55/120    avg_loss:0.023, val_acc:0.975]
Epoch [56/120    avg_loss:0.024, val_acc:0.986]
Epoch [57/120    avg_loss:0.862, val_acc:0.452]
Epoch [58/120    avg_loss:1.281, val_acc:0.585]
Epoch [59/120    avg_loss:0.946, val_acc:0.686]
Epoch [60/120    avg_loss:0.799, val_acc:0.694]
Epoch [61/120    avg_loss:0.729, val_acc:0.685]
Epoch [62/120    avg_loss:0.733, val_acc:0.689]
Epoch [63/120    avg_loss:0.686, val_acc:0.686]
Epoch [64/120    avg_loss:0.680, val_acc:0.740]
Epoch [65/120    avg_loss:0.658, val_acc:0.752]
Epoch [66/120    avg_loss:0.710, val_acc:0.770]
Epoch [67/120    avg_loss:0.634, val_acc:0.710]
Epoch [68/120    avg_loss:0.633, val_acc:0.761]
Epoch [69/120    avg_loss:0.596, val_acc:0.770]
Epoch [70/120    avg_loss:0.617, val_acc:0.782]
Epoch [71/120    avg_loss:0.559, val_acc:0.785]
Epoch [72/120    avg_loss:0.538, val_acc:0.770]
Epoch [73/120    avg_loss:0.546, val_acc:0.785]
Epoch [74/120    avg_loss:0.544, val_acc:0.788]
Epoch [75/120    avg_loss:0.557, val_acc:0.788]
Epoch [76/120    avg_loss:0.526, val_acc:0.788]
Epoch [77/120    avg_loss:0.527, val_acc:0.792]
Epoch [78/120    avg_loss:0.524, val_acc:0.781]
Epoch [79/120    avg_loss:0.523, val_acc:0.788]
Epoch [80/120    avg_loss:0.543, val_acc:0.796]
Epoch [81/120    avg_loss:0.548, val_acc:0.787]
Epoch [82/120    avg_loss:0.520, val_acc:0.788]
Epoch [83/120    avg_loss:0.528, val_acc:0.783]
Epoch [84/120    avg_loss:0.507, val_acc:0.797]
Epoch [85/120    avg_loss:0.521, val_acc:0.798]
Epoch [86/120    avg_loss:0.511, val_acc:0.795]
Epoch [87/120    avg_loss:0.510, val_acc:0.795]
Epoch [88/120    avg_loss:0.503, val_acc:0.796]
Epoch [89/120    avg_loss:0.528, val_acc:0.794]
Epoch [90/120    avg_loss:0.506, val_acc:0.796]
Epoch [91/120    avg_loss:0.517, val_acc:0.795]
Epoch [92/120    avg_loss:0.524, val_acc:0.795]
Epoch [93/120    avg_loss:0.506, val_acc:0.795]
Epoch [94/120    avg_loss:0.493, val_acc:0.796]
Epoch [95/120    avg_loss:0.509, val_acc:0.796]
Epoch [96/120    avg_loss:0.508, val_acc:0.795]
Epoch [97/120    avg_loss:0.514, val_acc:0.795]
Epoch [98/120    avg_loss:0.526, val_acc:0.795]
Epoch [99/120    avg_loss:0.523, val_acc:0.795]
Epoch [100/120    avg_loss:0.536, val_acc:0.794]
Epoch [101/120    avg_loss:0.516, val_acc:0.794]
Epoch [102/120    avg_loss:0.494, val_acc:0.795]
Epoch [103/120    avg_loss:0.520, val_acc:0.795]
Epoch [104/120    avg_loss:0.547, val_acc:0.795]
Epoch [105/120    avg_loss:0.513, val_acc:0.795]
Epoch [106/120    avg_loss:0.487, val_acc:0.795]
Epoch [107/120    avg_loss:0.528, val_acc:0.794]
Epoch [108/120    avg_loss:0.515, val_acc:0.795]
Epoch [109/120    avg_loss:0.530, val_acc:0.796]
Epoch [110/120    avg_loss:0.515, val_acc:0.796]
Epoch [111/120    avg_loss:0.510, val_acc:0.796]
Epoch [112/120    avg_loss:0.525, val_acc:0.796]
Epoch [113/120    avg_loss:0.525, val_acc:0.796]
Epoch [114/120    avg_loss:0.523, val_acc:0.796]
Epoch [115/120    avg_loss:0.529, val_acc:0.796]
Epoch [116/120    avg_loss:0.499, val_acc:0.796]
Epoch [117/120    avg_loss:0.513, val_acc:0.796]
Epoch [118/120    avg_loss:0.524, val_acc:0.796]
Epoch [119/120    avg_loss:0.522, val_acc:0.796]
Epoch [120/120    avg_loss:0.507, val_acc:0.796]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4484     5    10   647    10   140   223   616   297]
 [    0    80 16921     0   419     0   670     0     0     0]
 [    0    17    50  1799     0     0     0     0    98    72]
 [    0   112     0    11  2718     0    46     0    78     7]
 [    0     0     0     0     0  1288     0    17     0     0]
 [    0    31  1598    38    37     0  3092     0    82     0]
 [    0    95     1     0     0     1     3  1187     1     2]
 [    0   398     0   235    61     0    17     0  2849    11]
 [    0    16     0     3    14   160     0     0     2   724]]

Accuracy:
84.50100016870316

F1 scores:
[       nan 0.76879554 0.92300559 0.87076476 0.7914968  0.93198263
 0.69907303 0.87375782 0.78086885 0.71259843]

Kappa:
0.7945339335452161
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9354df38d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.989, val_acc:0.247]
Epoch [2/120    avg_loss:1.602, val_acc:0.447]
Epoch [3/120    avg_loss:1.325, val_acc:0.516]
Epoch [4/120    avg_loss:1.090, val_acc:0.580]
Epoch [5/120    avg_loss:0.906, val_acc:0.602]
Epoch [6/120    avg_loss:0.741, val_acc:0.686]
Epoch [7/120    avg_loss:0.619, val_acc:0.730]
Epoch [8/120    avg_loss:0.508, val_acc:0.797]
Epoch [9/120    avg_loss:0.492, val_acc:0.778]
Epoch [10/120    avg_loss:0.390, val_acc:0.849]
Epoch [11/120    avg_loss:0.355, val_acc:0.845]
Epoch [12/120    avg_loss:0.280, val_acc:0.885]
Epoch [13/120    avg_loss:0.279, val_acc:0.858]
Epoch [14/120    avg_loss:0.263, val_acc:0.890]
Epoch [15/120    avg_loss:0.220, val_acc:0.879]
Epoch [16/120    avg_loss:0.234, val_acc:0.916]
Epoch [17/120    avg_loss:0.164, val_acc:0.928]
Epoch [18/120    avg_loss:0.135, val_acc:0.935]
Epoch [19/120    avg_loss:0.151, val_acc:0.939]
Epoch [20/120    avg_loss:0.113, val_acc:0.942]
Epoch [21/120    avg_loss:0.128, val_acc:0.939]
Epoch [22/120    avg_loss:0.123, val_acc:0.941]
Epoch [23/120    avg_loss:0.091, val_acc:0.965]
Epoch [24/120    avg_loss:0.104, val_acc:0.919]
Epoch [25/120    avg_loss:0.103, val_acc:0.949]
Epoch [26/120    avg_loss:0.093, val_acc:0.947]
Epoch [27/120    avg_loss:0.080, val_acc:0.939]
Epoch [28/120    avg_loss:0.058, val_acc:0.954]
Epoch [29/120    avg_loss:0.068, val_acc:0.957]
Epoch [30/120    avg_loss:0.084, val_acc:0.922]
Epoch [31/120    avg_loss:0.130, val_acc:0.938]
Epoch [32/120    avg_loss:0.089, val_acc:0.944]
Epoch [33/120    avg_loss:0.061, val_acc:0.957]
Epoch [34/120    avg_loss:0.058, val_acc:0.954]
Epoch [35/120    avg_loss:0.065, val_acc:0.964]
Epoch [36/120    avg_loss:0.054, val_acc:0.964]
Epoch [37/120    avg_loss:0.045, val_acc:0.971]
Epoch [38/120    avg_loss:0.033, val_acc:0.971]
Epoch [39/120    avg_loss:0.033, val_acc:0.972]
Epoch [40/120    avg_loss:0.026, val_acc:0.973]
Epoch [41/120    avg_loss:0.024, val_acc:0.974]
Epoch [42/120    avg_loss:0.030, val_acc:0.971]
Epoch [43/120    avg_loss:0.028, val_acc:0.974]
Epoch [44/120    avg_loss:0.027, val_acc:0.972]
Epoch [45/120    avg_loss:0.025, val_acc:0.972]
Epoch [46/120    avg_loss:0.031, val_acc:0.975]
Epoch [47/120    avg_loss:0.027, val_acc:0.973]
Epoch [48/120    avg_loss:0.025, val_acc:0.970]
Epoch [49/120    avg_loss:0.027, val_acc:0.976]
Epoch [50/120    avg_loss:0.028, val_acc:0.974]
Epoch [51/120    avg_loss:0.028, val_acc:0.975]
Epoch [52/120    avg_loss:0.020, val_acc:0.974]
Epoch [53/120    avg_loss:0.025, val_acc:0.975]
Epoch [54/120    avg_loss:0.023, val_acc:0.972]
Epoch [55/120    avg_loss:0.026, val_acc:0.976]
Epoch [56/120    avg_loss:0.026, val_acc:0.974]
Epoch [57/120    avg_loss:0.023, val_acc:0.976]
Epoch [58/120    avg_loss:0.022, val_acc:0.975]
Epoch [59/120    avg_loss:0.021, val_acc:0.977]
Epoch [60/120    avg_loss:0.026, val_acc:0.973]
Epoch [61/120    avg_loss:0.025, val_acc:0.977]
Epoch [62/120    avg_loss:0.023, val_acc:0.976]
Epoch [63/120    avg_loss:0.027, val_acc:0.975]
Epoch [64/120    avg_loss:0.019, val_acc:0.979]
Epoch [65/120    avg_loss:0.022, val_acc:0.978]
Epoch [66/120    avg_loss:0.024, val_acc:0.976]
Epoch [67/120    avg_loss:0.024, val_acc:0.977]
Epoch [68/120    avg_loss:0.020, val_acc:0.977]
Epoch [69/120    avg_loss:0.021, val_acc:0.978]
Epoch [70/120    avg_loss:0.017, val_acc:0.978]
Epoch [71/120    avg_loss:0.021, val_acc:0.982]
Epoch [72/120    avg_loss:0.023, val_acc:0.979]
Epoch [73/120    avg_loss:0.027, val_acc:0.977]
Epoch [74/120    avg_loss:0.021, val_acc:0.977]
Epoch [75/120    avg_loss:0.022, val_acc:0.977]
Epoch [76/120    avg_loss:0.019, val_acc:0.977]
Epoch [77/120    avg_loss:0.019, val_acc:0.976]
Epoch [78/120    avg_loss:0.030, val_acc:0.974]
Epoch [79/120    avg_loss:0.020, val_acc:0.979]
Epoch [80/120    avg_loss:0.021, val_acc:0.980]
Epoch [81/120    avg_loss:0.018, val_acc:0.982]
Epoch [82/120    avg_loss:0.027, val_acc:0.979]
Epoch [83/120    avg_loss:0.016, val_acc:0.981]
Epoch [84/120    avg_loss:0.021, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.982]
Epoch [86/120    avg_loss:0.024, val_acc:0.980]
Epoch [87/120    avg_loss:0.021, val_acc:0.979]
Epoch [88/120    avg_loss:0.017, val_acc:0.978]
Epoch [89/120    avg_loss:0.019, val_acc:0.978]
Epoch [90/120    avg_loss:0.020, val_acc:0.978]
Epoch [91/120    avg_loss:0.019, val_acc:0.981]
Epoch [92/120    avg_loss:0.022, val_acc:0.982]
Epoch [93/120    avg_loss:0.016, val_acc:0.981]
Epoch [94/120    avg_loss:0.017, val_acc:0.980]
Epoch [95/120    avg_loss:0.018, val_acc:0.981]
Epoch [96/120    avg_loss:0.015, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.980]
Epoch [98/120    avg_loss:0.016, val_acc:0.981]
Epoch [99/120    avg_loss:0.024, val_acc:0.979]
Epoch [100/120    avg_loss:0.016, val_acc:0.980]
Epoch [101/120    avg_loss:0.028, val_acc:0.979]
Epoch [102/120    avg_loss:0.016, val_acc:0.980]
Epoch [103/120    avg_loss:0.019, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.980]
Epoch [106/120    avg_loss:0.020, val_acc:0.981]
Epoch [107/120    avg_loss:0.015, val_acc:0.980]
Epoch [108/120    avg_loss:0.016, val_acc:0.982]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.017, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.983]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.017, val_acc:0.982]
Epoch [114/120    avg_loss:0.019, val_acc:0.981]
Epoch [115/120    avg_loss:0.017, val_acc:0.981]
Epoch [116/120    avg_loss:0.015, val_acc:0.982]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.023, val_acc:0.981]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6353     0     1     0     0     0    13    65     0]
 [    0     0 18018     0    66     0     6     0     0     0]
 [    0     4     0  1970     7     0     0     0    46     9]
 [    0    33    18     0  2881     0     7     0    33     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3    10     0     0  4839     0     3    23]
 [    0     0     0     0     0     0     4  1285     0     1]
 [    0    26     0     0    47     0     0     0  3498     0]
 [    0     0     0     2    21    43     0     0     0   853]]

Accuracy:
98.8166678716892

F1 scores:
[       nan 0.9889477  0.99742589 0.98034337 0.96129463 0.98379193
 0.99424697 0.99304482 0.9695122  0.94515235]

Kappa:
0.9843296835538559
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f712fcae940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.932, val_acc:0.137]
Epoch [2/120    avg_loss:1.590, val_acc:0.366]
Epoch [3/120    avg_loss:1.342, val_acc:0.374]
Epoch [4/120    avg_loss:1.172, val_acc:0.433]
Epoch [5/120    avg_loss:0.926, val_acc:0.535]
Epoch [6/120    avg_loss:0.747, val_acc:0.516]
Epoch [7/120    avg_loss:0.700, val_acc:0.598]
Epoch [8/120    avg_loss:0.603, val_acc:0.797]
Epoch [9/120    avg_loss:0.521, val_acc:0.832]
Epoch [10/120    avg_loss:0.486, val_acc:0.794]
Epoch [11/120    avg_loss:0.468, val_acc:0.791]
Epoch [12/120    avg_loss:0.388, val_acc:0.876]
Epoch [13/120    avg_loss:0.365, val_acc:0.903]
Epoch [14/120    avg_loss:0.290, val_acc:0.896]
Epoch [15/120    avg_loss:0.301, val_acc:0.919]
Epoch [16/120    avg_loss:0.267, val_acc:0.860]
Epoch [17/120    avg_loss:0.259, val_acc:0.872]
Epoch [18/120    avg_loss:0.233, val_acc:0.938]
Epoch [19/120    avg_loss:0.196, val_acc:0.949]
Epoch [20/120    avg_loss:0.164, val_acc:0.937]
Epoch [21/120    avg_loss:0.175, val_acc:0.962]
Epoch [22/120    avg_loss:0.190, val_acc:0.937]
Epoch [23/120    avg_loss:0.191, val_acc:0.938]
Epoch [24/120    avg_loss:0.124, val_acc:0.951]
Epoch [25/120    avg_loss:0.161, val_acc:0.962]
Epoch [26/120    avg_loss:0.124, val_acc:0.956]
Epoch [27/120    avg_loss:0.119, val_acc:0.946]
Epoch [28/120    avg_loss:0.091, val_acc:0.954]
Epoch [29/120    avg_loss:0.092, val_acc:0.969]
Epoch [30/120    avg_loss:0.074, val_acc:0.970]
Epoch [31/120    avg_loss:0.064, val_acc:0.976]
Epoch [32/120    avg_loss:0.071, val_acc:0.977]
Epoch [33/120    avg_loss:0.079, val_acc:0.963]
Epoch [34/120    avg_loss:0.054, val_acc:0.985]
Epoch [35/120    avg_loss:0.055, val_acc:0.978]
Epoch [36/120    avg_loss:0.061, val_acc:0.977]
Epoch [37/120    avg_loss:0.041, val_acc:0.975]
Epoch [38/120    avg_loss:0.040, val_acc:0.981]
Epoch [39/120    avg_loss:0.056, val_acc:0.954]
Epoch [40/120    avg_loss:0.098, val_acc:0.961]
Epoch [41/120    avg_loss:0.048, val_acc:0.965]
Epoch [42/120    avg_loss:0.031, val_acc:0.984]
Epoch [43/120    avg_loss:0.040, val_acc:0.972]
Epoch [44/120    avg_loss:0.044, val_acc:0.974]
Epoch [45/120    avg_loss:0.049, val_acc:0.973]
Epoch [46/120    avg_loss:0.048, val_acc:0.977]
Epoch [47/120    avg_loss:0.033, val_acc:0.984]
Epoch [48/120    avg_loss:0.025, val_acc:0.988]
Epoch [49/120    avg_loss:0.018, val_acc:0.988]
Epoch [50/120    avg_loss:0.021, val_acc:0.989]
Epoch [51/120    avg_loss:0.019, val_acc:0.989]
Epoch [52/120    avg_loss:0.019, val_acc:0.989]
Epoch [53/120    avg_loss:0.019, val_acc:0.989]
Epoch [54/120    avg_loss:0.017, val_acc:0.990]
Epoch [55/120    avg_loss:0.022, val_acc:0.990]
Epoch [56/120    avg_loss:0.025, val_acc:0.988]
Epoch [57/120    avg_loss:0.019, val_acc:0.988]
Epoch [58/120    avg_loss:0.015, val_acc:0.987]
Epoch [59/120    avg_loss:0.018, val_acc:0.988]
Epoch [60/120    avg_loss:0.017, val_acc:0.987]
Epoch [61/120    avg_loss:0.016, val_acc:0.988]
Epoch [62/120    avg_loss:0.017, val_acc:0.987]
Epoch [63/120    avg_loss:0.017, val_acc:0.988]
Epoch [64/120    avg_loss:0.020, val_acc:0.988]
Epoch [65/120    avg_loss:0.018, val_acc:0.988]
Epoch [66/120    avg_loss:0.016, val_acc:0.988]
Epoch [67/120    avg_loss:0.016, val_acc:0.989]
Epoch [68/120    avg_loss:0.013, val_acc:0.988]
Epoch [69/120    avg_loss:0.014, val_acc:0.988]
Epoch [70/120    avg_loss:0.015, val_acc:0.989]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.988]
Epoch [73/120    avg_loss:0.014, val_acc:0.988]
Epoch [74/120    avg_loss:0.018, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.988]
Epoch [77/120    avg_loss:0.013, val_acc:0.988]
Epoch [78/120    avg_loss:0.015, val_acc:0.988]
Epoch [79/120    avg_loss:0.016, val_acc:0.989]
Epoch [80/120    avg_loss:0.018, val_acc:0.988]
Epoch [81/120    avg_loss:0.016, val_acc:0.988]
Epoch [82/120    avg_loss:0.015, val_acc:0.988]
Epoch [83/120    avg_loss:0.013, val_acc:0.988]
Epoch [84/120    avg_loss:0.017, val_acc:0.988]
Epoch [85/120    avg_loss:0.016, val_acc:0.988]
Epoch [86/120    avg_loss:0.015, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.988]
Epoch [88/120    avg_loss:0.018, val_acc:0.988]
Epoch [89/120    avg_loss:0.016, val_acc:0.988]
Epoch [90/120    avg_loss:0.015, val_acc:0.988]
Epoch [91/120    avg_loss:0.017, val_acc:0.988]
Epoch [92/120    avg_loss:0.015, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.988]
Epoch [94/120    avg_loss:0.015, val_acc:0.988]
Epoch [95/120    avg_loss:0.015, val_acc:0.988]
Epoch [96/120    avg_loss:0.016, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.016, val_acc:0.988]
Epoch [99/120    avg_loss:0.017, val_acc:0.988]
Epoch [100/120    avg_loss:0.016, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.988]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.017, val_acc:0.988]
Epoch [104/120    avg_loss:0.014, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.017, val_acc:0.988]
Epoch [107/120    avg_loss:0.015, val_acc:0.988]
Epoch [108/120    avg_loss:0.017, val_acc:0.988]
Epoch [109/120    avg_loss:0.016, val_acc:0.988]
Epoch [110/120    avg_loss:0.017, val_acc:0.988]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.988]
Epoch [113/120    avg_loss:0.017, val_acc:0.988]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.988]
Epoch [116/120    avg_loss:0.017, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.988]
Epoch [118/120    avg_loss:0.017, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6346     0     0     0     0     3     0    77     6]
 [    0     0 18030     0    55     0     5     0     0     0]
 [    0     4     0  2006     2     0     0     0    22     2]
 [    0    50    19     0  2867     0     7     0    28     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1    12     0     0  4863     0     0     2]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0     0    63     0     0     0  3486    19]
 [    0     1     0     0    14    54     0     1     0   849]]

Accuracy:
98.91306967440292

F1 scores:
[       nan 0.98878155 0.99778639 0.98963986 0.95998661 0.97972973
 0.99692497 0.99961255 0.97048998 0.94438265]

Kappa:
0.9856049303999719
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcbef1b0978>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.021, val_acc:0.178]
Epoch [2/120    avg_loss:1.584, val_acc:0.500]
Epoch [3/120    avg_loss:1.266, val_acc:0.490]
Epoch [4/120    avg_loss:1.073, val_acc:0.597]
Epoch [5/120    avg_loss:0.903, val_acc:0.685]
Epoch [6/120    avg_loss:0.731, val_acc:0.719]
Epoch [7/120    avg_loss:0.626, val_acc:0.746]
Epoch [8/120    avg_loss:0.595, val_acc:0.713]
Epoch [9/120    avg_loss:0.522, val_acc:0.812]
Epoch [10/120    avg_loss:0.446, val_acc:0.828]
Epoch [11/120    avg_loss:0.399, val_acc:0.796]
Epoch [12/120    avg_loss:0.375, val_acc:0.819]
Epoch [13/120    avg_loss:0.328, val_acc:0.883]
Epoch [14/120    avg_loss:0.286, val_acc:0.898]
Epoch [15/120    avg_loss:0.272, val_acc:0.894]
Epoch [16/120    avg_loss:0.257, val_acc:0.902]
Epoch [17/120    avg_loss:0.220, val_acc:0.898]
Epoch [18/120    avg_loss:0.213, val_acc:0.926]
Epoch [19/120    avg_loss:0.187, val_acc:0.865]
Epoch [20/120    avg_loss:0.215, val_acc:0.935]
Epoch [21/120    avg_loss:0.176, val_acc:0.949]
Epoch [22/120    avg_loss:0.138, val_acc:0.930]
Epoch [23/120    avg_loss:0.159, val_acc:0.943]
Epoch [24/120    avg_loss:0.132, val_acc:0.951]
Epoch [25/120    avg_loss:0.107, val_acc:0.939]
Epoch [26/120    avg_loss:0.126, val_acc:0.957]
Epoch [27/120    avg_loss:0.115, val_acc:0.930]
Epoch [28/120    avg_loss:0.174, val_acc:0.922]
Epoch [29/120    avg_loss:0.127, val_acc:0.959]
Epoch [30/120    avg_loss:0.097, val_acc:0.946]
Epoch [31/120    avg_loss:0.081, val_acc:0.961]
Epoch [32/120    avg_loss:0.077, val_acc:0.958]
Epoch [33/120    avg_loss:0.073, val_acc:0.973]
Epoch [34/120    avg_loss:0.063, val_acc:0.969]
Epoch [35/120    avg_loss:0.061, val_acc:0.963]
Epoch [36/120    avg_loss:0.080, val_acc:0.965]
Epoch [37/120    avg_loss:0.060, val_acc:0.979]
Epoch [38/120    avg_loss:0.086, val_acc:0.959]
Epoch [39/120    avg_loss:0.057, val_acc:0.952]
Epoch [40/120    avg_loss:0.061, val_acc:0.980]
Epoch [41/120    avg_loss:0.040, val_acc:0.973]
Epoch [42/120    avg_loss:0.049, val_acc:0.955]
Epoch [43/120    avg_loss:0.042, val_acc:0.974]
Epoch [44/120    avg_loss:0.039, val_acc:0.966]
Epoch [45/120    avg_loss:0.043, val_acc:0.980]
Epoch [46/120    avg_loss:0.028, val_acc:0.976]
Epoch [47/120    avg_loss:0.062, val_acc:0.957]
Epoch [48/120    avg_loss:0.059, val_acc:0.963]
Epoch [49/120    avg_loss:0.031, val_acc:0.984]
Epoch [50/120    avg_loss:0.030, val_acc:0.976]
Epoch [51/120    avg_loss:0.029, val_acc:0.969]
Epoch [52/120    avg_loss:0.041, val_acc:0.969]
Epoch [53/120    avg_loss:0.054, val_acc:0.964]
Epoch [54/120    avg_loss:0.028, val_acc:0.974]
Epoch [55/120    avg_loss:0.042, val_acc:0.976]
Epoch [56/120    avg_loss:0.023, val_acc:0.973]
Epoch [57/120    avg_loss:0.032, val_acc:0.974]
Epoch [58/120    avg_loss:0.018, val_acc:0.978]
Epoch [59/120    avg_loss:0.019, val_acc:0.971]
Epoch [60/120    avg_loss:0.019, val_acc:0.963]
Epoch [61/120    avg_loss:0.028, val_acc:0.982]
Epoch [62/120    avg_loss:0.017, val_acc:0.980]
Epoch [63/120    avg_loss:0.014, val_acc:0.982]
Epoch [64/120    avg_loss:0.012, val_acc:0.980]
Epoch [65/120    avg_loss:0.015, val_acc:0.980]
Epoch [66/120    avg_loss:0.011, val_acc:0.980]
Epoch [67/120    avg_loss:0.011, val_acc:0.980]
Epoch [68/120    avg_loss:0.011, val_acc:0.980]
Epoch [69/120    avg_loss:0.012, val_acc:0.982]
Epoch [70/120    avg_loss:0.010, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.982]
Epoch [72/120    avg_loss:0.010, val_acc:0.981]
Epoch [73/120    avg_loss:0.013, val_acc:0.981]
Epoch [74/120    avg_loss:0.013, val_acc:0.981]
Epoch [75/120    avg_loss:0.010, val_acc:0.982]
Epoch [76/120    avg_loss:0.010, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.011, val_acc:0.982]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.982]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.012, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.982]
Epoch [97/120    avg_loss:0.012, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.982]
Epoch [99/120    avg_loss:0.011, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.012, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.982]
Epoch [105/120    avg_loss:0.010, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6376     0     6     3     0     0     0    47     0]
 [    0     0 18030     0    41     0    19     0     0     0]
 [    0     0     0  2000     2     0     0     0    32     2]
 [    0    32    22     0  2883     0     8     0    27     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4846     0    19    13]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0    26     0     0    24     0     0     0  3497    24]
 [    0     0     0     4    16   106     0     0     0   793]]

Accuracy:
98.85763863784253

F1 scores:
[       nan 0.99113944 0.99773117 0.98863075 0.97054368 0.96097202
 0.99394934 0.99961225 0.97233421 0.90525114]

Kappa:
0.984869085294683
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68a014c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.026, val_acc:0.264]
Epoch [2/120    avg_loss:1.595, val_acc:0.534]
Epoch [3/120    avg_loss:1.336, val_acc:0.406]
Epoch [4/120    avg_loss:1.102, val_acc:0.539]
Epoch [5/120    avg_loss:0.881, val_acc:0.622]
Epoch [6/120    avg_loss:0.734, val_acc:0.688]
Epoch [7/120    avg_loss:0.633, val_acc:0.741]
Epoch [8/120    avg_loss:0.528, val_acc:0.760]
Epoch [9/120    avg_loss:0.474, val_acc:0.773]
Epoch [10/120    avg_loss:0.390, val_acc:0.801]
Epoch [11/120    avg_loss:0.400, val_acc:0.832]
Epoch [12/120    avg_loss:0.335, val_acc:0.888]
Epoch [13/120    avg_loss:0.274, val_acc:0.812]
Epoch [14/120    avg_loss:0.236, val_acc:0.921]
Epoch [15/120    avg_loss:0.226, val_acc:0.942]
Epoch [16/120    avg_loss:0.225, val_acc:0.935]
Epoch [17/120    avg_loss:0.207, val_acc:0.937]
Epoch [18/120    avg_loss:0.198, val_acc:0.955]
Epoch [19/120    avg_loss:0.145, val_acc:0.947]
Epoch [20/120    avg_loss:0.130, val_acc:0.951]
Epoch [21/120    avg_loss:0.100, val_acc:0.916]
Epoch [22/120    avg_loss:0.101, val_acc:0.950]
Epoch [23/120    avg_loss:0.089, val_acc:0.964]
Epoch [24/120    avg_loss:0.133, val_acc:0.939]
Epoch [25/120    avg_loss:0.131, val_acc:0.956]
Epoch [26/120    avg_loss:0.097, val_acc:0.941]
Epoch [27/120    avg_loss:0.126, val_acc:0.962]
Epoch [28/120    avg_loss:0.090, val_acc:0.957]
Epoch [29/120    avg_loss:0.065, val_acc:0.966]
Epoch [30/120    avg_loss:0.060, val_acc:0.966]
Epoch [31/120    avg_loss:0.053, val_acc:0.946]
Epoch [32/120    avg_loss:0.054, val_acc:0.959]
Epoch [33/120    avg_loss:0.062, val_acc:0.968]
Epoch [34/120    avg_loss:0.072, val_acc:0.970]
Epoch [35/120    avg_loss:0.066, val_acc:0.963]
Epoch [36/120    avg_loss:0.073, val_acc:0.970]
Epoch [37/120    avg_loss:0.054, val_acc:0.976]
Epoch [38/120    avg_loss:0.053, val_acc:0.973]
Epoch [39/120    avg_loss:0.034, val_acc:0.973]
Epoch [40/120    avg_loss:0.032, val_acc:0.978]
Epoch [41/120    avg_loss:0.023, val_acc:0.978]
Epoch [42/120    avg_loss:0.024, val_acc:0.976]
Epoch [43/120    avg_loss:0.022, val_acc:0.977]
Epoch [44/120    avg_loss:0.023, val_acc:0.978]
Epoch [45/120    avg_loss:0.037, val_acc:0.977]
Epoch [46/120    avg_loss:0.036, val_acc:0.978]
Epoch [47/120    avg_loss:0.031, val_acc:0.973]
Epoch [48/120    avg_loss:0.042, val_acc:0.968]
Epoch [49/120    avg_loss:0.029, val_acc:0.973]
Epoch [50/120    avg_loss:0.019, val_acc:0.981]
Epoch [51/120    avg_loss:0.017, val_acc:0.977]
Epoch [52/120    avg_loss:0.028, val_acc:0.980]
Epoch [53/120    avg_loss:0.048, val_acc:0.974]
Epoch [54/120    avg_loss:0.023, val_acc:0.974]
Epoch [55/120    avg_loss:0.029, val_acc:0.978]
Epoch [56/120    avg_loss:0.021, val_acc:0.976]
Epoch [57/120    avg_loss:0.019, val_acc:0.983]
Epoch [58/120    avg_loss:0.013, val_acc:0.984]
Epoch [59/120    avg_loss:0.013, val_acc:0.980]
Epoch [60/120    avg_loss:0.010, val_acc:0.977]
Epoch [61/120    avg_loss:0.020, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.984]
Epoch [63/120    avg_loss:0.015, val_acc:0.970]
Epoch [64/120    avg_loss:0.018, val_acc:0.977]
Epoch [65/120    avg_loss:0.029, val_acc:0.977]
Epoch [66/120    avg_loss:0.062, val_acc:0.969]
Epoch [67/120    avg_loss:0.048, val_acc:0.977]
Epoch [68/120    avg_loss:0.049, val_acc:0.971]
Epoch [69/120    avg_loss:0.031, val_acc:0.980]
Epoch [70/120    avg_loss:0.026, val_acc:0.976]
Epoch [71/120    avg_loss:0.019, val_acc:0.981]
Epoch [72/120    avg_loss:0.015, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.982]
Epoch [74/120    avg_loss:0.015, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.015, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.008, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6384     0     0     9     0     2     0    34     3]
 [    0     0 18062     0    21     0     5     0     2     0]
 [    0     0     0  2014     3     0     0     0    17     2]
 [    0    33    19     0  2879     0    13     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4857     0     0    21]
 [    0     0     0     0     0     0     2  1287     0     1]
 [    0     4     0     6    43     0     0     0  3498    20]
 [    0     0     0     3    15    39     0     0     0   862]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.99338676 0.99870062 0.99236265 0.969034   0.98527746
 0.99559291 0.99883586 0.97846154 0.94310722]

Kappa:
0.9889842298794289
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f382a7fa908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.010, val_acc:0.219]
Epoch [2/120    avg_loss:1.571, val_acc:0.294]
Epoch [3/120    avg_loss:1.311, val_acc:0.392]
Epoch [4/120    avg_loss:1.140, val_acc:0.442]
Epoch [5/120    avg_loss:0.991, val_acc:0.592]
Epoch [6/120    avg_loss:0.819, val_acc:0.668]
Epoch [7/120    avg_loss:0.666, val_acc:0.719]
Epoch [8/120    avg_loss:0.521, val_acc:0.806]
Epoch [9/120    avg_loss:0.460, val_acc:0.822]
Epoch [10/120    avg_loss:0.438, val_acc:0.819]
Epoch [11/120    avg_loss:0.347, val_acc:0.834]
Epoch [12/120    avg_loss:0.482, val_acc:0.785]
Epoch [13/120    avg_loss:0.371, val_acc:0.836]
Epoch [14/120    avg_loss:0.341, val_acc:0.818]
Epoch [15/120    avg_loss:0.288, val_acc:0.905]
Epoch [16/120    avg_loss:0.232, val_acc:0.922]
Epoch [17/120    avg_loss:0.219, val_acc:0.865]
Epoch [18/120    avg_loss:0.192, val_acc:0.916]
Epoch [19/120    avg_loss:0.166, val_acc:0.930]
Epoch [20/120    avg_loss:0.154, val_acc:0.922]
Epoch [21/120    avg_loss:0.137, val_acc:0.935]
Epoch [22/120    avg_loss:0.134, val_acc:0.931]
Epoch [23/120    avg_loss:0.163, val_acc:0.921]
Epoch [24/120    avg_loss:0.154, val_acc:0.899]
Epoch [25/120    avg_loss:0.143, val_acc:0.951]
Epoch [26/120    avg_loss:0.110, val_acc:0.948]
Epoch [27/120    avg_loss:0.102, val_acc:0.949]
Epoch [28/120    avg_loss:0.093, val_acc:0.945]
Epoch [29/120    avg_loss:0.108, val_acc:0.952]
Epoch [30/120    avg_loss:0.092, val_acc:0.936]
Epoch [31/120    avg_loss:0.096, val_acc:0.951]
Epoch [32/120    avg_loss:0.079, val_acc:0.957]
Epoch [33/120    avg_loss:0.061, val_acc:0.968]
Epoch [34/120    avg_loss:0.513, val_acc:0.290]
Epoch [35/120    avg_loss:1.256, val_acc:0.422]
Epoch [36/120    avg_loss:1.127, val_acc:0.463]
Epoch [37/120    avg_loss:1.029, val_acc:0.480]
Epoch [38/120    avg_loss:0.987, val_acc:0.403]
Epoch [39/120    avg_loss:0.928, val_acc:0.581]
Epoch [40/120    avg_loss:0.882, val_acc:0.644]
Epoch [41/120    avg_loss:0.851, val_acc:0.687]
Epoch [42/120    avg_loss:0.823, val_acc:0.680]
Epoch [43/120    avg_loss:0.776, val_acc:0.716]
Epoch [44/120    avg_loss:0.738, val_acc:0.686]
Epoch [45/120    avg_loss:0.726, val_acc:0.720]
Epoch [46/120    avg_loss:0.720, val_acc:0.688]
Epoch [47/120    avg_loss:0.667, val_acc:0.727]
Epoch [48/120    avg_loss:0.646, val_acc:0.732]
Epoch [49/120    avg_loss:0.646, val_acc:0.738]
Epoch [50/120    avg_loss:0.640, val_acc:0.742]
Epoch [51/120    avg_loss:0.627, val_acc:0.746]
Epoch [52/120    avg_loss:0.598, val_acc:0.742]
Epoch [53/120    avg_loss:0.620, val_acc:0.750]
Epoch [54/120    avg_loss:0.596, val_acc:0.743]
Epoch [55/120    avg_loss:0.639, val_acc:0.741]
Epoch [56/120    avg_loss:0.591, val_acc:0.750]
Epoch [57/120    avg_loss:0.606, val_acc:0.741]
Epoch [58/120    avg_loss:0.571, val_acc:0.740]
Epoch [59/120    avg_loss:0.575, val_acc:0.745]
Epoch [60/120    avg_loss:0.577, val_acc:0.747]
Epoch [61/120    avg_loss:0.590, val_acc:0.750]
Epoch [62/120    avg_loss:0.577, val_acc:0.747]
Epoch [63/120    avg_loss:0.587, val_acc:0.749]
Epoch [64/120    avg_loss:0.590, val_acc:0.749]
Epoch [65/120    avg_loss:0.579, val_acc:0.750]
Epoch [66/120    avg_loss:0.579, val_acc:0.747]
Epoch [67/120    avg_loss:0.589, val_acc:0.749]
Epoch [68/120    avg_loss:0.588, val_acc:0.747]
Epoch [69/120    avg_loss:0.586, val_acc:0.747]
Epoch [70/120    avg_loss:0.589, val_acc:0.747]
Epoch [71/120    avg_loss:0.579, val_acc:0.747]
Epoch [72/120    avg_loss:0.573, val_acc:0.747]
Epoch [73/120    avg_loss:0.575, val_acc:0.747]
Epoch [74/120    avg_loss:0.563, val_acc:0.747]
Epoch [75/120    avg_loss:0.594, val_acc:0.747]
Epoch [76/120    avg_loss:0.586, val_acc:0.747]
Epoch [77/120    avg_loss:0.595, val_acc:0.747]
Epoch [78/120    avg_loss:0.586, val_acc:0.747]
Epoch [79/120    avg_loss:0.576, val_acc:0.747]
Epoch [80/120    avg_loss:0.572, val_acc:0.747]
Epoch [81/120    avg_loss:0.568, val_acc:0.747]
Epoch [82/120    avg_loss:0.582, val_acc:0.747]
Epoch [83/120    avg_loss:0.566, val_acc:0.747]
Epoch [84/120    avg_loss:0.582, val_acc:0.747]
Epoch [85/120    avg_loss:0.579, val_acc:0.747]
Epoch [86/120    avg_loss:0.587, val_acc:0.747]
Epoch [87/120    avg_loss:0.571, val_acc:0.747]
Epoch [88/120    avg_loss:0.584, val_acc:0.747]
Epoch [89/120    avg_loss:0.588, val_acc:0.747]
Epoch [90/120    avg_loss:0.610, val_acc:0.747]
Epoch [91/120    avg_loss:0.566, val_acc:0.747]
Epoch [92/120    avg_loss:0.568, val_acc:0.747]
Epoch [93/120    avg_loss:0.550, val_acc:0.747]
Epoch [94/120    avg_loss:0.597, val_acc:0.747]
Epoch [95/120    avg_loss:0.581, val_acc:0.747]
Epoch [96/120    avg_loss:0.592, val_acc:0.747]
Epoch [97/120    avg_loss:0.581, val_acc:0.747]
Epoch [98/120    avg_loss:0.583, val_acc:0.747]
Epoch [99/120    avg_loss:0.567, val_acc:0.747]
Epoch [100/120    avg_loss:0.556, val_acc:0.747]
Epoch [101/120    avg_loss:0.611, val_acc:0.747]
Epoch [102/120    avg_loss:0.585, val_acc:0.747]
Epoch [103/120    avg_loss:0.579, val_acc:0.747]
Epoch [104/120    avg_loss:0.567, val_acc:0.747]
Epoch [105/120    avg_loss:0.579, val_acc:0.747]
Epoch [106/120    avg_loss:0.563, val_acc:0.747]
Epoch [107/120    avg_loss:0.610, val_acc:0.747]
Epoch [108/120    avg_loss:0.566, val_acc:0.747]
Epoch [109/120    avg_loss:0.583, val_acc:0.747]
Epoch [110/120    avg_loss:0.582, val_acc:0.747]
Epoch [111/120    avg_loss:0.592, val_acc:0.747]
Epoch [112/120    avg_loss:0.591, val_acc:0.747]
Epoch [113/120    avg_loss:0.606, val_acc:0.747]
Epoch [114/120    avg_loss:0.604, val_acc:0.747]
Epoch [115/120    avg_loss:0.610, val_acc:0.747]
Epoch [116/120    avg_loss:0.567, val_acc:0.747]
Epoch [117/120    avg_loss:0.591, val_acc:0.747]
Epoch [118/120    avg_loss:0.550, val_acc:0.747]
Epoch [119/120    avg_loss:0.596, val_acc:0.747]
Epoch [120/120    avg_loss:0.598, val_acc:0.747]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4053   290   156   406     0   390    44   844   249]
 [    0    38 13735     0   855     0  3462     0     0     0]
 [    0     9     0  1807     0     0    18     0   151    51]
 [    0   123   179     0  2478     0   125     0    47    20]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     4   786   147   145     0  3796     0     0     0]
 [    0    10     2     0    10     0     3  1237     0    28]
 [    0   226   110   155   100     0   153     0  2807    20]
 [    0    24     0    21    25   143    12     1     2   691]]

Accuracy:
76.90212806979491

F1 scores:
[       nan 0.74237568 0.82760906 0.83618695 0.70891146 0.94805667
 0.59141544 0.96189736 0.75639989 0.69868554]

Kappa:
0.7045379556820833
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9c7aef7908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.000, val_acc:0.247]
Epoch [2/120    avg_loss:1.579, val_acc:0.289]
Epoch [3/120    avg_loss:1.357, val_acc:0.407]
Epoch [4/120    avg_loss:1.177, val_acc:0.441]
Epoch [5/120    avg_loss:1.012, val_acc:0.497]
Epoch [6/120    avg_loss:0.845, val_acc:0.539]
Epoch [7/120    avg_loss:0.774, val_acc:0.588]
Epoch [8/120    avg_loss:0.626, val_acc:0.736]
Epoch [9/120    avg_loss:0.558, val_acc:0.786]
Epoch [10/120    avg_loss:0.494, val_acc:0.783]
Epoch [11/120    avg_loss:0.444, val_acc:0.811]
Epoch [12/120    avg_loss:0.373, val_acc:0.835]
Epoch [13/120    avg_loss:0.352, val_acc:0.891]
Epoch [14/120    avg_loss:0.315, val_acc:0.912]
Epoch [15/120    avg_loss:0.229, val_acc:0.897]
Epoch [16/120    avg_loss:0.207, val_acc:0.924]
Epoch [17/120    avg_loss:0.200, val_acc:0.910]
Epoch [18/120    avg_loss:0.202, val_acc:0.918]
Epoch [19/120    avg_loss:0.169, val_acc:0.946]
Epoch [20/120    avg_loss:0.147, val_acc:0.922]
Epoch [21/120    avg_loss:0.162, val_acc:0.896]
Epoch [22/120    avg_loss:0.159, val_acc:0.924]
Epoch [23/120    avg_loss:0.144, val_acc:0.897]
Epoch [24/120    avg_loss:0.127, val_acc:0.947]
Epoch [25/120    avg_loss:0.127, val_acc:0.924]
Epoch [26/120    avg_loss:0.091, val_acc:0.963]
Epoch [27/120    avg_loss:0.081, val_acc:0.950]
Epoch [28/120    avg_loss:0.097, val_acc:0.951]
Epoch [29/120    avg_loss:0.104, val_acc:0.944]
Epoch [30/120    avg_loss:0.104, val_acc:0.955]
Epoch [31/120    avg_loss:0.067, val_acc:0.964]
Epoch [32/120    avg_loss:0.071, val_acc:0.962]
Epoch [33/120    avg_loss:0.067, val_acc:0.964]
Epoch [34/120    avg_loss:0.039, val_acc:0.965]
Epoch [35/120    avg_loss:0.044, val_acc:0.960]
Epoch [36/120    avg_loss:0.070, val_acc:0.943]
Epoch [37/120    avg_loss:0.094, val_acc:0.956]
Epoch [38/120    avg_loss:0.055, val_acc:0.976]
Epoch [39/120    avg_loss:0.049, val_acc:0.958]
Epoch [40/120    avg_loss:0.049, val_acc:0.966]
Epoch [41/120    avg_loss:0.039, val_acc:0.977]
Epoch [42/120    avg_loss:0.049, val_acc:0.970]
Epoch [43/120    avg_loss:0.046, val_acc:0.970]
Epoch [44/120    avg_loss:0.067, val_acc:0.973]
Epoch [45/120    avg_loss:0.049, val_acc:0.917]
Epoch [46/120    avg_loss:0.052, val_acc:0.969]
Epoch [47/120    avg_loss:0.045, val_acc:0.977]
Epoch [48/120    avg_loss:0.035, val_acc:0.967]
Epoch [49/120    avg_loss:0.035, val_acc:0.975]
Epoch [50/120    avg_loss:0.038, val_acc:0.946]
Epoch [51/120    avg_loss:0.068, val_acc:0.969]
Epoch [52/120    avg_loss:0.047, val_acc:0.971]
Epoch [53/120    avg_loss:0.017, val_acc:0.981]
Epoch [54/120    avg_loss:0.018, val_acc:0.983]
Epoch [55/120    avg_loss:0.057, val_acc:0.951]
Epoch [56/120    avg_loss:0.050, val_acc:0.974]
Epoch [57/120    avg_loss:0.023, val_acc:0.981]
Epoch [58/120    avg_loss:0.018, val_acc:0.981]
Epoch [59/120    avg_loss:0.017, val_acc:0.984]
Epoch [60/120    avg_loss:0.018, val_acc:0.979]
Epoch [61/120    avg_loss:0.019, val_acc:0.984]
Epoch [62/120    avg_loss:0.016, val_acc:0.987]
Epoch [63/120    avg_loss:0.012, val_acc:0.987]
Epoch [64/120    avg_loss:0.020, val_acc:0.981]
Epoch [65/120    avg_loss:0.014, val_acc:0.984]
Epoch [66/120    avg_loss:0.017, val_acc:0.984]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.017, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.989]
Epoch [71/120    avg_loss:0.010, val_acc:0.985]
Epoch [72/120    avg_loss:0.013, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.985]
Epoch [76/120    avg_loss:0.014, val_acc:0.984]
Epoch [77/120    avg_loss:0.014, val_acc:0.964]
Epoch [78/120    avg_loss:0.014, val_acc:0.982]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.987]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0    13     0     0     2    18    12     0]
 [    0     7 18019     0    30     0    26     0     0     8]
 [    0     5     0  2016     0     0     0     0    15     0]
 [    0    55    20     0  2862     0     6     1    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     7     0     0  4840     0     2    24]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     6     0     0    28     0     0     0  3537     0]
 [    0     1     0     7    15    41     0     0     0   855]]

Accuracy:
99.07936278408407

F1 scores:
[       nan 0.99077019 0.99734322 0.98847757 0.96901981 0.98453414
 0.9926169  0.9926895  0.98729937 0.94684385]

Kappa:
0.9878069137654134
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4094b0c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.037, val_acc:0.147]
Epoch [2/120    avg_loss:1.626, val_acc:0.322]
Epoch [3/120    avg_loss:1.341, val_acc:0.444]
Epoch [4/120    avg_loss:1.101, val_acc:0.477]
Epoch [5/120    avg_loss:0.916, val_acc:0.508]
Epoch [6/120    avg_loss:0.835, val_acc:0.638]
Epoch [7/120    avg_loss:0.645, val_acc:0.699]
Epoch [8/120    avg_loss:0.549, val_acc:0.761]
Epoch [9/120    avg_loss:0.458, val_acc:0.798]
Epoch [10/120    avg_loss:0.402, val_acc:0.867]
Epoch [11/120    avg_loss:0.357, val_acc:0.839]
Epoch [12/120    avg_loss:0.310, val_acc:0.872]
Epoch [13/120    avg_loss:0.277, val_acc:0.878]
Epoch [14/120    avg_loss:0.271, val_acc:0.903]
Epoch [15/120    avg_loss:0.222, val_acc:0.932]
Epoch [16/120    avg_loss:0.205, val_acc:0.932]
Epoch [17/120    avg_loss:0.186, val_acc:0.949]
Epoch [18/120    avg_loss:0.180, val_acc:0.938]
Epoch [19/120    avg_loss:0.143, val_acc:0.946]
Epoch [20/120    avg_loss:0.164, val_acc:0.819]
Epoch [21/120    avg_loss:0.266, val_acc:0.885]
Epoch [22/120    avg_loss:0.184, val_acc:0.945]
Epoch [23/120    avg_loss:0.163, val_acc:0.945]
Epoch [24/120    avg_loss:0.166, val_acc:0.946]
Epoch [25/120    avg_loss:0.110, val_acc:0.957]
Epoch [26/120    avg_loss:0.097, val_acc:0.953]
Epoch [27/120    avg_loss:0.118, val_acc:0.928]
Epoch [28/120    avg_loss:0.167, val_acc:0.900]
Epoch [29/120    avg_loss:0.101, val_acc:0.934]
Epoch [30/120    avg_loss:0.097, val_acc:0.964]
Epoch [31/120    avg_loss:0.076, val_acc:0.969]
Epoch [32/120    avg_loss:0.067, val_acc:0.966]
Epoch [33/120    avg_loss:0.062, val_acc:0.970]
Epoch [34/120    avg_loss:0.074, val_acc:0.852]
Epoch [35/120    avg_loss:0.085, val_acc:0.964]
Epoch [36/120    avg_loss:0.070, val_acc:0.970]
Epoch [37/120    avg_loss:0.051, val_acc:0.966]
Epoch [38/120    avg_loss:0.046, val_acc:0.973]
Epoch [39/120    avg_loss:0.054, val_acc:0.964]
Epoch [40/120    avg_loss:0.048, val_acc:0.969]
Epoch [41/120    avg_loss:0.048, val_acc:0.959]
Epoch [42/120    avg_loss:0.041, val_acc:0.964]
Epoch [43/120    avg_loss:0.040, val_acc:0.967]
Epoch [44/120    avg_loss:0.051, val_acc:0.967]
Epoch [45/120    avg_loss:0.035, val_acc:0.974]
Epoch [46/120    avg_loss:0.054, val_acc:0.968]
Epoch [47/120    avg_loss:0.061, val_acc:0.971]
Epoch [48/120    avg_loss:0.044, val_acc:0.969]
Epoch [49/120    avg_loss:0.043, val_acc:0.965]
Epoch [50/120    avg_loss:0.034, val_acc:0.977]
Epoch [51/120    avg_loss:0.040, val_acc:0.985]
Epoch [52/120    avg_loss:0.028, val_acc:0.981]
Epoch [53/120    avg_loss:0.041, val_acc:0.982]
Epoch [54/120    avg_loss:0.042, val_acc:0.981]
Epoch [55/120    avg_loss:0.033, val_acc:0.986]
Epoch [56/120    avg_loss:0.028, val_acc:0.982]
Epoch [57/120    avg_loss:0.031, val_acc:0.977]
Epoch [58/120    avg_loss:0.023, val_acc:0.987]
Epoch [59/120    avg_loss:0.017, val_acc:0.991]
Epoch [60/120    avg_loss:0.020, val_acc:0.984]
Epoch [61/120    avg_loss:0.059, val_acc:0.971]
Epoch [62/120    avg_loss:0.062, val_acc:0.966]
Epoch [63/120    avg_loss:0.060, val_acc:0.975]
Epoch [64/120    avg_loss:0.045, val_acc:0.974]
Epoch [65/120    avg_loss:0.025, val_acc:0.985]
Epoch [66/120    avg_loss:0.023, val_acc:0.984]
Epoch [67/120    avg_loss:0.020, val_acc:0.985]
Epoch [68/120    avg_loss:0.026, val_acc:0.981]
Epoch [69/120    avg_loss:0.053, val_acc:0.970]
Epoch [70/120    avg_loss:0.042, val_acc:0.976]
Epoch [71/120    avg_loss:0.049, val_acc:0.973]
Epoch [72/120    avg_loss:0.023, val_acc:0.979]
Epoch [73/120    avg_loss:0.017, val_acc:0.984]
Epoch [74/120    avg_loss:0.014, val_acc:0.981]
Epoch [75/120    avg_loss:0.016, val_acc:0.984]
Epoch [76/120    avg_loss:0.017, val_acc:0.984]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.015, val_acc:0.986]
Epoch [80/120    avg_loss:0.018, val_acc:0.985]
Epoch [81/120    avg_loss:0.014, val_acc:0.987]
Epoch [82/120    avg_loss:0.014, val_acc:0.985]
Epoch [83/120    avg_loss:0.015, val_acc:0.984]
Epoch [84/120    avg_loss:0.018, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.985]
Epoch [90/120    avg_loss:0.014, val_acc:0.985]
Epoch [91/120    avg_loss:0.013, val_acc:0.986]
Epoch [92/120    avg_loss:0.012, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.015, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.015, val_acc:0.986]
Epoch [97/120    avg_loss:0.014, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.013, val_acc:0.986]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.013, val_acc:0.986]
Epoch [104/120    avg_loss:0.017, val_acc:0.986]
Epoch [105/120    avg_loss:0.014, val_acc:0.986]
Epoch [106/120    avg_loss:0.015, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.019, val_acc:0.986]
Epoch [109/120    avg_loss:0.013, val_acc:0.986]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.015, val_acc:0.986]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.014, val_acc:0.986]
Epoch [115/120    avg_loss:0.015, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6369     0     0     1     0     1     0    58     3]
 [    0     0 18041     0    34     0    15     0     0     0]
 [    0     0     0  2025     0     0     0     0     5     6]
 [    0    22    23     0  2880     0    10     0    37     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     7     0     0  4865     0     2     0]
 [    0     0     0     0     0     0     3  1281     0     6]
 [    0     5     0    19    82     0     0     0  3450    15]
 [    0     0     2     0    15    44     0     0     0   858]]

Accuracy:
98.99019111657388

F1 scores:
[       nan 0.9929841  0.99784292 0.9909469  0.96256684 0.98342125
 0.99570201 0.99649942 0.96869297 0.94964029]

Kappa:
0.9866237092436962
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb105613908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.988, val_acc:0.234]
Epoch [2/120    avg_loss:1.614, val_acc:0.330]
Epoch [3/120    avg_loss:1.339, val_acc:0.338]
Epoch [4/120    avg_loss:1.159, val_acc:0.605]
Epoch [5/120    avg_loss:0.995, val_acc:0.615]
Epoch [6/120    avg_loss:0.854, val_acc:0.667]
Epoch [7/120    avg_loss:0.719, val_acc:0.659]
Epoch [8/120    avg_loss:0.613, val_acc:0.715]
Epoch [9/120    avg_loss:0.524, val_acc:0.782]
Epoch [10/120    avg_loss:0.428, val_acc:0.854]
Epoch [11/120    avg_loss:0.368, val_acc:0.781]
Epoch [12/120    avg_loss:0.314, val_acc:0.796]
Epoch [13/120    avg_loss:0.318, val_acc:0.895]
Epoch [14/120    avg_loss:0.500, val_acc:0.653]
Epoch [15/120    avg_loss:0.476, val_acc:0.869]
Epoch [16/120    avg_loss:0.324, val_acc:0.927]
Epoch [17/120    avg_loss:0.267, val_acc:0.924]
Epoch [18/120    avg_loss:0.213, val_acc:0.888]
Epoch [19/120    avg_loss:0.228, val_acc:0.917]
Epoch [20/120    avg_loss:0.213, val_acc:0.946]
Epoch [21/120    avg_loss:0.180, val_acc:0.804]
Epoch [22/120    avg_loss:0.176, val_acc:0.954]
Epoch [23/120    avg_loss:0.130, val_acc:0.958]
Epoch [24/120    avg_loss:0.124, val_acc:0.965]
Epoch [25/120    avg_loss:0.147, val_acc:0.957]
Epoch [26/120    avg_loss:0.096, val_acc:0.968]
Epoch [27/120    avg_loss:0.083, val_acc:0.974]
Epoch [28/120    avg_loss:0.106, val_acc:0.957]
Epoch [29/120    avg_loss:0.067, val_acc:0.974]
Epoch [30/120    avg_loss:0.078, val_acc:0.969]
Epoch [31/120    avg_loss:0.079, val_acc:0.968]
Epoch [32/120    avg_loss:0.074, val_acc:0.966]
Epoch [33/120    avg_loss:0.063, val_acc:0.970]
Epoch [34/120    avg_loss:0.063, val_acc:0.970]
Epoch [35/120    avg_loss:0.057, val_acc:0.956]
Epoch [36/120    avg_loss:0.054, val_acc:0.982]
Epoch [37/120    avg_loss:0.037, val_acc:0.977]
Epoch [38/120    avg_loss:0.035, val_acc:0.984]
Epoch [39/120    avg_loss:0.031, val_acc:0.979]
Epoch [40/120    avg_loss:0.064, val_acc:0.951]
Epoch [41/120    avg_loss:0.061, val_acc:0.958]
Epoch [42/120    avg_loss:0.044, val_acc:0.977]
Epoch [43/120    avg_loss:0.021, val_acc:0.981]
Epoch [44/120    avg_loss:0.025, val_acc:0.982]
Epoch [45/120    avg_loss:0.022, val_acc:0.987]
Epoch [46/120    avg_loss:0.029, val_acc:0.985]
Epoch [47/120    avg_loss:0.028, val_acc:0.975]
Epoch [48/120    avg_loss:0.025, val_acc:0.965]
Epoch [49/120    avg_loss:0.023, val_acc:0.987]
Epoch [50/120    avg_loss:0.021, val_acc:0.985]
Epoch [51/120    avg_loss:0.013, val_acc:0.987]
Epoch [52/120    avg_loss:0.012, val_acc:0.985]
Epoch [53/120    avg_loss:0.014, val_acc:0.988]
Epoch [54/120    avg_loss:0.016, val_acc:0.990]
Epoch [55/120    avg_loss:0.015, val_acc:0.990]
Epoch [56/120    avg_loss:0.014, val_acc:0.979]
Epoch [57/120    avg_loss:0.061, val_acc:0.970]
Epoch [58/120    avg_loss:0.069, val_acc:0.976]
Epoch [59/120    avg_loss:0.025, val_acc:0.977]
Epoch [60/120    avg_loss:0.022, val_acc:0.984]
Epoch [61/120    avg_loss:0.035, val_acc:0.900]
Epoch [62/120    avg_loss:0.035, val_acc:0.981]
Epoch [63/120    avg_loss:0.026, val_acc:0.984]
Epoch [64/120    avg_loss:0.041, val_acc:0.982]
Epoch [65/120    avg_loss:0.017, val_acc:0.984]
Epoch [66/120    avg_loss:0.022, val_acc:0.980]
Epoch [67/120    avg_loss:0.020, val_acc:0.987]
Epoch [68/120    avg_loss:0.026, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.988]
Epoch [70/120    avg_loss:0.010, val_acc:0.988]
Epoch [71/120    avg_loss:0.010, val_acc:0.988]
Epoch [72/120    avg_loss:0.012, val_acc:0.989]
Epoch [73/120    avg_loss:0.012, val_acc:0.988]
Epoch [74/120    avg_loss:0.008, val_acc:0.989]
Epoch [75/120    avg_loss:0.010, val_acc:0.989]
Epoch [76/120    avg_loss:0.009, val_acc:0.989]
Epoch [77/120    avg_loss:0.010, val_acc:0.989]
Epoch [78/120    avg_loss:0.012, val_acc:0.989]
Epoch [79/120    avg_loss:0.009, val_acc:0.989]
Epoch [80/120    avg_loss:0.011, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.989]
Epoch [82/120    avg_loss:0.012, val_acc:0.989]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.008, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.009, val_acc:0.989]
Epoch [88/120    avg_loss:0.008, val_acc:0.989]
Epoch [89/120    avg_loss:0.007, val_acc:0.989]
Epoch [90/120    avg_loss:0.008, val_acc:0.989]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.008, val_acc:0.989]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.008, val_acc:0.989]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.010, val_acc:0.989]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.008, val_acc:0.989]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.008, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.989]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.009, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6350     0    42     0     0     0     0    31     9]
 [    0     6 18030     0    42     0    11     0     1     0]
 [    0     6     0  2010     0     0     0     0    19     1]
 [    0    37    20     0  2886     0     0     0    28     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    18     0     0  4850     0     8     2]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0     0    53     0     0     0  3493    22]
 [    0     0     0     0    15    78     0     0     0   826]]

Accuracy:
98.90824958426722

F1 scores:
[       nan 0.98955898 0.99778639 0.97905504 0.96715818 0.97098214
 0.99599548 1.         0.9769263  0.92808989]

Kappa:
0.9855423078882081
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f76b909e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.984, val_acc:0.167]
Epoch [2/120    avg_loss:1.583, val_acc:0.344]
Epoch [3/120    avg_loss:1.410, val_acc:0.382]
Epoch [4/120    avg_loss:1.167, val_acc:0.489]
Epoch [5/120    avg_loss:0.939, val_acc:0.589]
Epoch [6/120    avg_loss:0.810, val_acc:0.692]
Epoch [7/120    avg_loss:0.731, val_acc:0.710]
Epoch [8/120    avg_loss:0.581, val_acc:0.806]
Epoch [9/120    avg_loss:0.543, val_acc:0.807]
Epoch [10/120    avg_loss:0.476, val_acc:0.881]
Epoch [11/120    avg_loss:0.420, val_acc:0.890]
Epoch [12/120    avg_loss:0.335, val_acc:0.909]
Epoch [13/120    avg_loss:0.294, val_acc:0.853]
Epoch [14/120    avg_loss:0.267, val_acc:0.884]
Epoch [15/120    avg_loss:0.257, val_acc:0.889]
Epoch [16/120    avg_loss:0.218, val_acc:0.870]
Epoch [17/120    avg_loss:0.232, val_acc:0.916]
Epoch [18/120    avg_loss:0.199, val_acc:0.922]
Epoch [19/120    avg_loss:0.172, val_acc:0.909]
Epoch [20/120    avg_loss:0.219, val_acc:0.931]
Epoch [21/120    avg_loss:0.166, val_acc:0.948]
Epoch [22/120    avg_loss:0.138, val_acc:0.938]
Epoch [23/120    avg_loss:0.121, val_acc:0.962]
Epoch [24/120    avg_loss:0.134, val_acc:0.891]
Epoch [25/120    avg_loss:0.120, val_acc:0.959]
Epoch [26/120    avg_loss:0.101, val_acc:0.958]
Epoch [27/120    avg_loss:0.116, val_acc:0.939]
Epoch [28/120    avg_loss:0.110, val_acc:0.935]
Epoch [29/120    avg_loss:0.084, val_acc:0.960]
Epoch [30/120    avg_loss:0.101, val_acc:0.942]
Epoch [31/120    avg_loss:0.119, val_acc:0.959]
Epoch [32/120    avg_loss:0.071, val_acc:0.968]
Epoch [33/120    avg_loss:0.067, val_acc:0.952]
Epoch [34/120    avg_loss:0.074, val_acc:0.952]
Epoch [35/120    avg_loss:0.069, val_acc:0.957]
Epoch [36/120    avg_loss:0.052, val_acc:0.971]
Epoch [37/120    avg_loss:0.048, val_acc:0.970]
Epoch [38/120    avg_loss:0.036, val_acc:0.959]
Epoch [39/120    avg_loss:0.045, val_acc:0.979]
Epoch [40/120    avg_loss:0.034, val_acc:0.955]
Epoch [41/120    avg_loss:0.041, val_acc:0.978]
Epoch [42/120    avg_loss:0.061, val_acc:0.976]
Epoch [43/120    avg_loss:0.071, val_acc:0.962]
Epoch [44/120    avg_loss:0.060, val_acc:0.974]
Epoch [45/120    avg_loss:0.069, val_acc:0.952]
Epoch [46/120    avg_loss:0.055, val_acc:0.973]
Epoch [47/120    avg_loss:0.037, val_acc:0.969]
Epoch [48/120    avg_loss:0.035, val_acc:0.980]
Epoch [49/120    avg_loss:0.032, val_acc:0.977]
Epoch [50/120    avg_loss:0.035, val_acc:0.977]
Epoch [51/120    avg_loss:0.043, val_acc:0.970]
Epoch [52/120    avg_loss:0.080, val_acc:0.980]
Epoch [53/120    avg_loss:0.047, val_acc:0.983]
Epoch [54/120    avg_loss:0.035, val_acc:0.972]
Epoch [55/120    avg_loss:0.040, val_acc:0.973]
Epoch [56/120    avg_loss:0.084, val_acc:0.977]
Epoch [57/120    avg_loss:0.027, val_acc:0.980]
Epoch [58/120    avg_loss:0.029, val_acc:0.983]
Epoch [59/120    avg_loss:0.021, val_acc:0.970]
Epoch [60/120    avg_loss:0.025, val_acc:0.979]
Epoch [61/120    avg_loss:0.020, val_acc:0.983]
Epoch [62/120    avg_loss:0.022, val_acc:0.978]
Epoch [63/120    avg_loss:0.016, val_acc:0.985]
Epoch [64/120    avg_loss:0.018, val_acc:0.940]
Epoch [65/120    avg_loss:0.019, val_acc:0.987]
Epoch [66/120    avg_loss:0.024, val_acc:0.978]
Epoch [67/120    avg_loss:0.020, val_acc:0.984]
Epoch [68/120    avg_loss:0.019, val_acc:0.989]
Epoch [69/120    avg_loss:0.044, val_acc:0.963]
Epoch [70/120    avg_loss:0.020, val_acc:0.990]
Epoch [71/120    avg_loss:0.011, val_acc:0.990]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.018, val_acc:0.976]
Epoch [74/120    avg_loss:0.020, val_acc:0.984]
Epoch [75/120    avg_loss:0.024, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.990]
Epoch [77/120    avg_loss:0.027, val_acc:0.984]
Epoch [78/120    avg_loss:0.015, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.012, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.017, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.990]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.006, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.004, val_acc:0.990]
Epoch [97/120    avg_loss:0.007, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.004, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6403     0     0     3     0     2     0    18     6]
 [    0     4 18035     0    35     0    16     0     0     0]
 [    0     0     0  2021     0     0     0     0    14     1]
 [    0    36    19     0  2871     0    11     3    30     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4876     0     0     2]
 [    0     0     0     0     0     0    10  1278     0     2]
 [    0     2     0     0    49     0     0     0  3510    10]
 [    0     0     0     3    15    46     0     0     0   855]]

Accuracy:
99.1829947220013

F1 scores:
[       nan 0.99448629 0.99795263 0.9955665  0.96585366 0.98268072
 0.99581334 0.99416569 0.98278034 0.95158598]

Kappa:
0.9891774707351172
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:48
Validation dataloader:48
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa283d15860>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.012, val_acc:0.300]
Epoch [2/120    avg_loss:1.563, val_acc:0.579]
Epoch [3/120    avg_loss:1.333, val_acc:0.596]
Epoch [4/120    avg_loss:1.129, val_acc:0.694]
Epoch [5/120    avg_loss:1.027, val_acc:0.697]
Epoch [6/120    avg_loss:0.821, val_acc:0.619]
Epoch [7/120    avg_loss:0.723, val_acc:0.634]
Epoch [8/120    avg_loss:0.628, val_acc:0.683]
Epoch [9/120    avg_loss:0.509, val_acc:0.720]
Epoch [10/120    avg_loss:0.448, val_acc:0.831]
Epoch [11/120    avg_loss:0.382, val_acc:0.779]
Epoch [12/120    avg_loss:0.321, val_acc:0.898]
Epoch [13/120    avg_loss:0.261, val_acc:0.900]
Epoch [14/120    avg_loss:0.271, val_acc:0.844]
Epoch [15/120    avg_loss:0.254, val_acc:0.918]
Epoch [16/120    avg_loss:0.190, val_acc:0.929]
Epoch [17/120    avg_loss:0.154, val_acc:0.917]
Epoch [18/120    avg_loss:0.176, val_acc:0.836]
Epoch [19/120    avg_loss:0.156, val_acc:0.935]
Epoch [20/120    avg_loss:0.139, val_acc:0.910]
Epoch [21/120    avg_loss:0.170, val_acc:0.939]
Epoch [22/120    avg_loss:0.142, val_acc:0.953]
Epoch [23/120    avg_loss:0.138, val_acc:0.880]
Epoch [24/120    avg_loss:0.169, val_acc:0.927]
Epoch [25/120    avg_loss:0.141, val_acc:0.941]
Epoch [26/120    avg_loss:0.131, val_acc:0.925]
Epoch [27/120    avg_loss:0.164, val_acc:0.951]
Epoch [28/120    avg_loss:0.114, val_acc:0.936]
Epoch [29/120    avg_loss:0.091, val_acc:0.957]
Epoch [30/120    avg_loss:0.083, val_acc:0.923]
Epoch [31/120    avg_loss:0.087, val_acc:0.964]
Epoch [32/120    avg_loss:0.058, val_acc:0.975]
Epoch [33/120    avg_loss:0.074, val_acc:0.974]
Epoch [34/120    avg_loss:0.061, val_acc:0.973]
Epoch [35/120    avg_loss:0.061, val_acc:0.968]
Epoch [36/120    avg_loss:0.045, val_acc:0.977]
Epoch [37/120    avg_loss:0.032, val_acc:0.975]
Epoch [38/120    avg_loss:0.047, val_acc:0.944]
Epoch [39/120    avg_loss:0.052, val_acc:0.970]
Epoch [40/120    avg_loss:0.057, val_acc:0.972]
Epoch [41/120    avg_loss:0.036, val_acc:0.974]
Epoch [42/120    avg_loss:0.026, val_acc:0.982]
Epoch [43/120    avg_loss:0.025, val_acc:0.981]
Epoch [44/120    avg_loss:0.072, val_acc:0.966]
Epoch [45/120    avg_loss:0.045, val_acc:0.970]
Epoch [46/120    avg_loss:0.039, val_acc:0.974]
Epoch [47/120    avg_loss:0.031, val_acc:0.977]
Epoch [48/120    avg_loss:0.026, val_acc:0.984]
Epoch [49/120    avg_loss:0.031, val_acc:0.984]
Epoch [50/120    avg_loss:0.018, val_acc:0.980]
Epoch [51/120    avg_loss:0.021, val_acc:0.982]
Epoch [52/120    avg_loss:0.023, val_acc:0.972]
Epoch [53/120    avg_loss:0.021, val_acc:0.980]
Epoch [54/120    avg_loss:0.028, val_acc:0.977]
Epoch [55/120    avg_loss:0.026, val_acc:0.990]
Epoch [56/120    avg_loss:0.026, val_acc:0.972]
Epoch [57/120    avg_loss:0.024, val_acc:0.985]
Epoch [58/120    avg_loss:0.025, val_acc:0.970]
Epoch [59/120    avg_loss:0.036, val_acc:0.981]
Epoch [60/120    avg_loss:0.024, val_acc:0.984]
Epoch [61/120    avg_loss:0.016, val_acc:0.985]
Epoch [62/120    avg_loss:0.032, val_acc:0.961]
Epoch [63/120    avg_loss:0.040, val_acc:0.979]
Epoch [64/120    avg_loss:0.047, val_acc:0.980]
Epoch [65/120    avg_loss:0.022, val_acc:0.984]
Epoch [66/120    avg_loss:0.020, val_acc:0.980]
Epoch [67/120    avg_loss:0.018, val_acc:0.987]
Epoch [68/120    avg_loss:0.016, val_acc:0.989]
Epoch [69/120    avg_loss:0.016, val_acc:0.988]
Epoch [70/120    avg_loss:0.015, val_acc:0.989]
Epoch [71/120    avg_loss:0.010, val_acc:0.990]
Epoch [72/120    avg_loss:0.011, val_acc:0.988]
Epoch [73/120    avg_loss:0.010, val_acc:0.989]
Epoch [74/120    avg_loss:0.008, val_acc:0.988]
Epoch [75/120    avg_loss:0.011, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.010, val_acc:0.988]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.989]
Epoch [83/120    avg_loss:0.008, val_acc:0.989]
Epoch [84/120    avg_loss:0.011, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.009, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.009, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.009, val_acc:0.989]
Epoch [110/120    avg_loss:0.007, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.011, val_acc:0.989]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6385     0     2     1     0     0     0    44     0]
 [    0     3 18052     0    30     0     4     0     1     0]
 [    0     5     0  2023     0     0     0     0     8     0]
 [    0    45    20     0  2876     0     3     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     7     0     0  4859     0     0    12]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     2     0     7    73     0     0     0  3481     8]
 [    0     0     0     0    14    35     0     0     0   870]]

Accuracy:
99.1492540910515

F1 scores:
[       nan 0.99207582 0.99839611 0.99288344 0.96413007 0.98676749
 0.99733169 0.99961225 0.97602692 0.96132597]

Kappa:
0.9887294855729114
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d6cc52908>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.020, val_acc:0.094]
Epoch [2/120    avg_loss:1.562, val_acc:0.544]
Epoch [3/120    avg_loss:1.310, val_acc:0.445]
Epoch [4/120    avg_loss:1.109, val_acc:0.688]
Epoch [5/120    avg_loss:0.942, val_acc:0.662]
Epoch [6/120    avg_loss:0.807, val_acc:0.673]
Epoch [7/120    avg_loss:0.681, val_acc:0.675]
Epoch [8/120    avg_loss:0.620, val_acc:0.666]
Epoch [9/120    avg_loss:0.526, val_acc:0.704]
Epoch [10/120    avg_loss:0.440, val_acc:0.703]
Epoch [11/120    avg_loss:0.394, val_acc:0.711]
Epoch [12/120    avg_loss:0.360, val_acc:0.773]
Epoch [13/120    avg_loss:0.325, val_acc:0.780]
Epoch [14/120    avg_loss:0.283, val_acc:0.784]
Epoch [15/120    avg_loss:0.305, val_acc:0.792]
Epoch [16/120    avg_loss:0.262, val_acc:0.853]
Epoch [17/120    avg_loss:0.221, val_acc:0.889]
Epoch [18/120    avg_loss:0.191, val_acc:0.857]
Epoch [19/120    avg_loss:0.189, val_acc:0.902]
Epoch [20/120    avg_loss:0.150, val_acc:0.911]
Epoch [21/120    avg_loss:0.132, val_acc:0.912]
Epoch [22/120    avg_loss:0.109, val_acc:0.921]
Epoch [23/120    avg_loss:0.121, val_acc:0.952]
Epoch [24/120    avg_loss:0.099, val_acc:0.927]
Epoch [25/120    avg_loss:0.122, val_acc:0.938]
Epoch [26/120    avg_loss:0.134, val_acc:0.936]
Epoch [27/120    avg_loss:0.140, val_acc:0.947]
Epoch [28/120    avg_loss:0.117, val_acc:0.933]
Epoch [29/120    avg_loss:0.100, val_acc:0.949]
Epoch [30/120    avg_loss:0.077, val_acc:0.961]
Epoch [31/120    avg_loss:0.091, val_acc:0.949]
Epoch [32/120    avg_loss:0.081, val_acc:0.956]
Epoch [33/120    avg_loss:0.085, val_acc:0.941]
Epoch [34/120    avg_loss:0.063, val_acc:0.896]
Epoch [35/120    avg_loss:0.070, val_acc:0.967]
Epoch [36/120    avg_loss:0.056, val_acc:0.923]
Epoch [37/120    avg_loss:0.062, val_acc:0.964]
Epoch [38/120    avg_loss:0.042, val_acc:0.960]
Epoch [39/120    avg_loss:0.059, val_acc:0.973]
Epoch [40/120    avg_loss:0.045, val_acc:0.971]
Epoch [41/120    avg_loss:0.070, val_acc:0.946]
Epoch [42/120    avg_loss:0.057, val_acc:0.963]
Epoch [43/120    avg_loss:0.036, val_acc:0.975]
Epoch [44/120    avg_loss:0.031, val_acc:0.971]
Epoch [45/120    avg_loss:0.046, val_acc:0.971]
Epoch [46/120    avg_loss:0.054, val_acc:0.965]
Epoch [47/120    avg_loss:0.031, val_acc:0.978]
Epoch [48/120    avg_loss:0.038, val_acc:0.978]
Epoch [49/120    avg_loss:0.039, val_acc:0.969]
Epoch [50/120    avg_loss:0.027, val_acc:0.975]
Epoch [51/120    avg_loss:0.022, val_acc:0.976]
Epoch [52/120    avg_loss:0.021, val_acc:0.980]
Epoch [53/120    avg_loss:0.021, val_acc:0.981]
Epoch [54/120    avg_loss:0.021, val_acc:0.974]
Epoch [55/120    avg_loss:0.031, val_acc:0.974]
Epoch [56/120    avg_loss:0.022, val_acc:0.982]
Epoch [57/120    avg_loss:0.017, val_acc:0.975]
Epoch [58/120    avg_loss:0.032, val_acc:0.979]
Epoch [59/120    avg_loss:0.018, val_acc:0.978]
Epoch [60/120    avg_loss:0.023, val_acc:0.966]
Epoch [61/120    avg_loss:0.018, val_acc:0.980]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.009, val_acc:0.983]
Epoch [64/120    avg_loss:0.022, val_acc:0.972]
Epoch [65/120    avg_loss:0.021, val_acc:0.974]
Epoch [66/120    avg_loss:0.015, val_acc:0.975]
Epoch [67/120    avg_loss:0.018, val_acc:0.980]
Epoch [68/120    avg_loss:0.014, val_acc:0.976]
Epoch [69/120    avg_loss:0.015, val_acc:0.973]
Epoch [70/120    avg_loss:0.010, val_acc:0.987]
Epoch [71/120    avg_loss:0.010, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.011, val_acc:0.979]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.025, val_acc:0.980]
Epoch [77/120    avg_loss:0.024, val_acc:0.979]
Epoch [78/120    avg_loss:0.015, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.976]
Epoch [80/120    avg_loss:0.015, val_acc:0.982]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.009, val_acc:0.974]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.004, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.003, val_acc:0.985]
Epoch [98/120    avg_loss:0.005, val_acc:0.979]
Epoch [99/120    avg_loss:0.005, val_acc:0.979]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.970]
Epoch [102/120    avg_loss:0.027, val_acc:0.955]
Epoch [103/120    avg_loss:0.029, val_acc:0.962]
Epoch [104/120    avg_loss:0.029, val_acc:0.976]
Epoch [105/120    avg_loss:0.022, val_acc:0.965]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.011, val_acc:0.972]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6332     0     0     0     0     0    21    77     2]
 [    0     0 18066     0    18     0     3     0     3     0]
 [    0     2     0  1947     0     0     0     0    86     1]
 [    0    19     6     0  2931     0    11     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    26     0     0     0  4849     0     3     0]
 [    0    31     0     0     0     0     0  1256     3     0]
 [    0    18     3    52    35     0     0     0  3463     0]
 [    0     0     0     0     3     5     0     0     0   911]]

Accuracy:
98.95645048562409

F1 scores:
[       nan 0.98675393 0.99836976 0.96505576 0.9837221  0.99808795
 0.99558567 0.97857421 0.9608768  0.99237473]

Kappa:
0.9861710006058916
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe2e66e77f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.913, val_acc:0.123]
Epoch [2/120    avg_loss:1.525, val_acc:0.321]
Epoch [3/120    avg_loss:1.273, val_acc:0.358]
Epoch [4/120    avg_loss:1.060, val_acc:0.706]
Epoch [5/120    avg_loss:0.891, val_acc:0.717]
Epoch [6/120    avg_loss:0.789, val_acc:0.761]
Epoch [7/120    avg_loss:0.642, val_acc:0.663]
Epoch [8/120    avg_loss:0.580, val_acc:0.768]
Epoch [9/120    avg_loss:0.484, val_acc:0.780]
Epoch [10/120    avg_loss:0.391, val_acc:0.776]
Epoch [11/120    avg_loss:0.366, val_acc:0.811]
Epoch [12/120    avg_loss:0.314, val_acc:0.838]
Epoch [13/120    avg_loss:0.303, val_acc:0.907]
Epoch [14/120    avg_loss:0.247, val_acc:0.911]
Epoch [15/120    avg_loss:0.238, val_acc:0.936]
Epoch [16/120    avg_loss:0.185, val_acc:0.933]
Epoch [17/120    avg_loss:0.210, val_acc:0.927]
Epoch [18/120    avg_loss:0.147, val_acc:0.950]
Epoch [19/120    avg_loss:0.133, val_acc:0.942]
Epoch [20/120    avg_loss:0.139, val_acc:0.940]
Epoch [21/120    avg_loss:0.123, val_acc:0.939]
Epoch [22/120    avg_loss:0.120, val_acc:0.932]
Epoch [23/120    avg_loss:0.108, val_acc:0.968]
Epoch [24/120    avg_loss:0.104, val_acc:0.956]
Epoch [25/120    avg_loss:0.108, val_acc:0.937]
Epoch [26/120    avg_loss:0.096, val_acc:0.949]
Epoch [27/120    avg_loss:0.084, val_acc:0.946]
Epoch [28/120    avg_loss:0.086, val_acc:0.937]
Epoch [29/120    avg_loss:0.095, val_acc:0.964]
Epoch [30/120    avg_loss:0.062, val_acc:0.962]
Epoch [31/120    avg_loss:0.073, val_acc:0.949]
Epoch [32/120    avg_loss:0.076, val_acc:0.955]
Epoch [33/120    avg_loss:0.066, val_acc:0.961]
Epoch [34/120    avg_loss:0.055, val_acc:0.964]
Epoch [35/120    avg_loss:0.085, val_acc:0.951]
Epoch [36/120    avg_loss:0.062, val_acc:0.950]
Epoch [37/120    avg_loss:0.045, val_acc:0.976]
Epoch [38/120    avg_loss:0.030, val_acc:0.975]
Epoch [39/120    avg_loss:0.029, val_acc:0.975]
Epoch [40/120    avg_loss:0.033, val_acc:0.977]
Epoch [41/120    avg_loss:0.031, val_acc:0.977]
Epoch [42/120    avg_loss:0.028, val_acc:0.978]
Epoch [43/120    avg_loss:0.031, val_acc:0.979]
Epoch [44/120    avg_loss:0.025, val_acc:0.978]
Epoch [45/120    avg_loss:0.026, val_acc:0.980]
Epoch [46/120    avg_loss:0.027, val_acc:0.979]
Epoch [47/120    avg_loss:0.029, val_acc:0.980]
Epoch [48/120    avg_loss:0.024, val_acc:0.982]
Epoch [49/120    avg_loss:0.024, val_acc:0.979]
Epoch [50/120    avg_loss:0.023, val_acc:0.982]
Epoch [51/120    avg_loss:0.026, val_acc:0.982]
Epoch [52/120    avg_loss:0.024, val_acc:0.979]
Epoch [53/120    avg_loss:0.030, val_acc:0.980]
Epoch [54/120    avg_loss:0.030, val_acc:0.979]
Epoch [55/120    avg_loss:0.022, val_acc:0.980]
Epoch [56/120    avg_loss:0.023, val_acc:0.981]
Epoch [57/120    avg_loss:0.025, val_acc:0.979]
Epoch [58/120    avg_loss:0.022, val_acc:0.979]
Epoch [59/120    avg_loss:0.021, val_acc:0.978]
Epoch [60/120    avg_loss:0.023, val_acc:0.979]
Epoch [61/120    avg_loss:0.025, val_acc:0.980]
Epoch [62/120    avg_loss:0.022, val_acc:0.979]
Epoch [63/120    avg_loss:0.024, val_acc:0.983]
Epoch [64/120    avg_loss:0.025, val_acc:0.981]
Epoch [65/120    avg_loss:0.024, val_acc:0.980]
Epoch [66/120    avg_loss:0.023, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.982]
Epoch [68/120    avg_loss:0.027, val_acc:0.979]
Epoch [69/120    avg_loss:0.023, val_acc:0.979]
Epoch [70/120    avg_loss:0.015, val_acc:0.979]
Epoch [71/120    avg_loss:0.017, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.022, val_acc:0.979]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.019, val_acc:0.980]
Epoch [76/120    avg_loss:0.018, val_acc:0.981]
Epoch [77/120    avg_loss:0.021, val_acc:0.981]
Epoch [78/120    avg_loss:0.020, val_acc:0.980]
Epoch [79/120    avg_loss:0.020, val_acc:0.980]
Epoch [80/120    avg_loss:0.017, val_acc:0.980]
Epoch [81/120    avg_loss:0.018, val_acc:0.981]
Epoch [82/120    avg_loss:0.015, val_acc:0.980]
Epoch [83/120    avg_loss:0.018, val_acc:0.979]
Epoch [84/120    avg_loss:0.021, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.979]
Epoch [86/120    avg_loss:0.017, val_acc:0.979]
Epoch [87/120    avg_loss:0.016, val_acc:0.978]
Epoch [88/120    avg_loss:0.020, val_acc:0.979]
Epoch [89/120    avg_loss:0.018, val_acc:0.979]
Epoch [90/120    avg_loss:0.021, val_acc:0.979]
Epoch [91/120    avg_loss:0.018, val_acc:0.979]
Epoch [92/120    avg_loss:0.018, val_acc:0.979]
Epoch [93/120    avg_loss:0.022, val_acc:0.979]
Epoch [94/120    avg_loss:0.023, val_acc:0.979]
Epoch [95/120    avg_loss:0.016, val_acc:0.979]
Epoch [96/120    avg_loss:0.020, val_acc:0.979]
Epoch [97/120    avg_loss:0.016, val_acc:0.979]
Epoch [98/120    avg_loss:0.023, val_acc:0.979]
Epoch [99/120    avg_loss:0.019, val_acc:0.979]
Epoch [100/120    avg_loss:0.017, val_acc:0.979]
Epoch [101/120    avg_loss:0.021, val_acc:0.979]
Epoch [102/120    avg_loss:0.025, val_acc:0.979]
Epoch [103/120    avg_loss:0.016, val_acc:0.979]
Epoch [104/120    avg_loss:0.016, val_acc:0.979]
Epoch [105/120    avg_loss:0.017, val_acc:0.979]
Epoch [106/120    avg_loss:0.018, val_acc:0.979]
Epoch [107/120    avg_loss:0.021, val_acc:0.979]
Epoch [108/120    avg_loss:0.016, val_acc:0.979]
Epoch [109/120    avg_loss:0.024, val_acc:0.979]
Epoch [110/120    avg_loss:0.018, val_acc:0.979]
Epoch [111/120    avg_loss:0.021, val_acc:0.979]
Epoch [112/120    avg_loss:0.019, val_acc:0.979]
Epoch [113/120    avg_loss:0.018, val_acc:0.979]
Epoch [114/120    avg_loss:0.017, val_acc:0.979]
Epoch [115/120    avg_loss:0.017, val_acc:0.979]
Epoch [116/120    avg_loss:0.018, val_acc:0.979]
Epoch [117/120    avg_loss:0.018, val_acc:0.979]
Epoch [118/120    avg_loss:0.020, val_acc:0.979]
Epoch [119/120    avg_loss:0.016, val_acc:0.979]
Epoch [120/120    avg_loss:0.022, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6310     0     9     3     0    15     6    84     5]
 [    0     0 18043     0    10     0    31     0     6     0]
 [    0     1     0  1933     0     0     0     0   101     1]
 [    0    21    11     1  2930     0     6     0     2     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    55     0     2     0  4821     0     0     0]
 [    0     9     0     0     0     0     0  1280     1     0]
 [    0     6     0   107    58     0     0     0  3399     1]
 [    0     0     0     0     1     6     0     0     0   912]]

Accuracy:
98.65037476200806

F1 scores:
[       nan 0.98755771 0.99687837 0.94615761 0.98058902 0.99770642
 0.98882166 0.99378882 0.94891122 0.99184339]

Kappa:
0.9821169482218514
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb55640f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.958, val_acc:0.204]
Epoch [2/120    avg_loss:1.500, val_acc:0.274]
Epoch [3/120    avg_loss:1.214, val_acc:0.354]
Epoch [4/120    avg_loss:1.043, val_acc:0.447]
Epoch [5/120    avg_loss:0.880, val_acc:0.759]
Epoch [6/120    avg_loss:0.778, val_acc:0.724]
Epoch [7/120    avg_loss:0.648, val_acc:0.736]
Epoch [8/120    avg_loss:0.576, val_acc:0.768]
Epoch [9/120    avg_loss:0.488, val_acc:0.812]
Epoch [10/120    avg_loss:0.432, val_acc:0.777]
Epoch [11/120    avg_loss:0.386, val_acc:0.779]
Epoch [12/120    avg_loss:0.344, val_acc:0.827]
Epoch [13/120    avg_loss:0.332, val_acc:0.843]
Epoch [14/120    avg_loss:0.275, val_acc:0.819]
Epoch [15/120    avg_loss:0.273, val_acc:0.831]
Epoch [16/120    avg_loss:0.273, val_acc:0.882]
Epoch [17/120    avg_loss:0.214, val_acc:0.907]
Epoch [18/120    avg_loss:0.179, val_acc:0.935]
Epoch [19/120    avg_loss:0.138, val_acc:0.921]
Epoch [20/120    avg_loss:0.191, val_acc:0.888]
Epoch [21/120    avg_loss:0.163, val_acc:0.882]
Epoch [22/120    avg_loss:0.151, val_acc:0.926]
Epoch [23/120    avg_loss:0.136, val_acc:0.888]
Epoch [24/120    avg_loss:0.121, val_acc:0.948]
Epoch [25/120    avg_loss:0.107, val_acc:0.951]
Epoch [26/120    avg_loss:0.093, val_acc:0.955]
Epoch [27/120    avg_loss:0.107, val_acc:0.933]
Epoch [28/120    avg_loss:0.145, val_acc:0.955]
Epoch [29/120    avg_loss:0.086, val_acc:0.954]
Epoch [30/120    avg_loss:0.086, val_acc:0.965]
Epoch [31/120    avg_loss:0.070, val_acc:0.970]
Epoch [32/120    avg_loss:0.059, val_acc:0.967]
Epoch [33/120    avg_loss:0.050, val_acc:0.958]
Epoch [34/120    avg_loss:0.087, val_acc:0.947]
Epoch [35/120    avg_loss:0.077, val_acc:0.959]
Epoch [36/120    avg_loss:0.063, val_acc:0.966]
Epoch [37/120    avg_loss:0.063, val_acc:0.973]
Epoch [38/120    avg_loss:0.063, val_acc:0.960]
Epoch [39/120    avg_loss:0.044, val_acc:0.957]
Epoch [40/120    avg_loss:0.038, val_acc:0.972]
Epoch [41/120    avg_loss:0.054, val_acc:0.917]
Epoch [42/120    avg_loss:0.057, val_acc:0.951]
Epoch [43/120    avg_loss:0.057, val_acc:0.966]
Epoch [44/120    avg_loss:0.043, val_acc:0.968]
Epoch [45/120    avg_loss:0.069, val_acc:0.933]
Epoch [46/120    avg_loss:0.080, val_acc:0.955]
Epoch [47/120    avg_loss:0.045, val_acc:0.975]
Epoch [48/120    avg_loss:0.039, val_acc:0.961]
Epoch [49/120    avg_loss:0.040, val_acc:0.964]
Epoch [50/120    avg_loss:0.041, val_acc:0.959]
Epoch [51/120    avg_loss:0.045, val_acc:0.925]
Epoch [52/120    avg_loss:0.035, val_acc:0.969]
Epoch [53/120    avg_loss:0.037, val_acc:0.971]
Epoch [54/120    avg_loss:0.023, val_acc:0.979]
Epoch [55/120    avg_loss:0.022, val_acc:0.975]
Epoch [56/120    avg_loss:0.019, val_acc:0.973]
Epoch [57/120    avg_loss:0.018, val_acc:0.980]
Epoch [58/120    avg_loss:0.026, val_acc:0.968]
Epoch [59/120    avg_loss:0.017, val_acc:0.979]
Epoch [60/120    avg_loss:0.016, val_acc:0.980]
Epoch [61/120    avg_loss:0.014, val_acc:0.973]
Epoch [62/120    avg_loss:0.029, val_acc:0.975]
Epoch [63/120    avg_loss:0.034, val_acc:0.966]
Epoch [64/120    avg_loss:0.020, val_acc:0.979]
Epoch [65/120    avg_loss:0.014, val_acc:0.979]
Epoch [66/120    avg_loss:0.020, val_acc:0.968]
Epoch [67/120    avg_loss:0.022, val_acc:0.972]
Epoch [68/120    avg_loss:0.028, val_acc:0.955]
Epoch [69/120    avg_loss:0.038, val_acc:0.971]
Epoch [70/120    avg_loss:0.041, val_acc:0.970]
Epoch [71/120    avg_loss:0.038, val_acc:0.970]
Epoch [72/120    avg_loss:0.020, val_acc:0.975]
Epoch [73/120    avg_loss:0.019, val_acc:0.984]
Epoch [74/120    avg_loss:0.020, val_acc:0.980]
Epoch [75/120    avg_loss:0.019, val_acc:0.986]
Epoch [76/120    avg_loss:0.019, val_acc:0.987]
Epoch [77/120    avg_loss:0.020, val_acc:0.979]
Epoch [78/120    avg_loss:0.026, val_acc:0.984]
Epoch [79/120    avg_loss:0.019, val_acc:0.970]
Epoch [80/120    avg_loss:0.033, val_acc:0.984]
Epoch [81/120    avg_loss:0.032, val_acc:0.984]
Epoch [82/120    avg_loss:0.026, val_acc:0.978]
Epoch [83/120    avg_loss:0.013, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.012, val_acc:0.981]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.979]
Epoch [90/120    avg_loss:0.019, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0     0     1     0     0     1    89     2]
 [    0     0 18006     0    29     0    54     0     1     0]
 [    0     0     0  1916     0     0     0     0   119     1]
 [    0    18    11     0  2928     0     6     9     0     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    18     0     0     0  4860     0     0     0]
 [    0    23     1     0     0     0     1  1254     9     2]
 [    0     2     0    34    45     0     2     0  3486     2]
 [    0     0     0     0     5    10     0     0     0   904]]

Accuracy:
98.80702769141783

F1 scores:
[       nan 0.98938661 0.99684438 0.96136478 0.97926421 0.99618321
 0.99173554 0.98198904 0.95835052 0.98797814]

Kappa:
0.9842009074325159
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe401ec48d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.967, val_acc:0.222]
Epoch [2/120    avg_loss:1.489, val_acc:0.317]
Epoch [3/120    avg_loss:1.271, val_acc:0.352]
Epoch [4/120    avg_loss:1.056, val_acc:0.475]
Epoch [5/120    avg_loss:0.866, val_acc:0.667]
Epoch [6/120    avg_loss:0.714, val_acc:0.725]
Epoch [7/120    avg_loss:0.645, val_acc:0.745]
Epoch [8/120    avg_loss:0.534, val_acc:0.669]
Epoch [9/120    avg_loss:0.464, val_acc:0.780]
Epoch [10/120    avg_loss:0.407, val_acc:0.783]
Epoch [11/120    avg_loss:0.403, val_acc:0.756]
Epoch [12/120    avg_loss:0.364, val_acc:0.791]
Epoch [13/120    avg_loss:0.300, val_acc:0.752]
Epoch [14/120    avg_loss:0.296, val_acc:0.817]
Epoch [15/120    avg_loss:0.287, val_acc:0.795]
Epoch [16/120    avg_loss:0.306, val_acc:0.825]
Epoch [17/120    avg_loss:0.259, val_acc:0.866]
Epoch [18/120    avg_loss:0.224, val_acc:0.857]
Epoch [19/120    avg_loss:0.198, val_acc:0.909]
Epoch [20/120    avg_loss:0.213, val_acc:0.915]
Epoch [21/120    avg_loss:0.168, val_acc:0.928]
Epoch [22/120    avg_loss:0.151, val_acc:0.931]
Epoch [23/120    avg_loss:0.122, val_acc:0.938]
Epoch [24/120    avg_loss:0.116, val_acc:0.952]
Epoch [25/120    avg_loss:0.100, val_acc:0.945]
Epoch [26/120    avg_loss:0.099, val_acc:0.946]
Epoch [27/120    avg_loss:0.114, val_acc:0.948]
Epoch [28/120    avg_loss:0.097, val_acc:0.947]
Epoch [29/120    avg_loss:0.074, val_acc:0.953]
Epoch [30/120    avg_loss:0.060, val_acc:0.952]
Epoch [31/120    avg_loss:0.059, val_acc:0.934]
Epoch [32/120    avg_loss:0.074, val_acc:0.950]
Epoch [33/120    avg_loss:0.073, val_acc:0.942]
Epoch [34/120    avg_loss:0.063, val_acc:0.958]
Epoch [35/120    avg_loss:0.048, val_acc:0.966]
Epoch [36/120    avg_loss:0.031, val_acc:0.971]
Epoch [37/120    avg_loss:0.033, val_acc:0.967]
Epoch [38/120    avg_loss:0.040, val_acc:0.957]
Epoch [39/120    avg_loss:0.042, val_acc:0.974]
Epoch [40/120    avg_loss:0.044, val_acc:0.961]
Epoch [41/120    avg_loss:0.027, val_acc:0.968]
Epoch [42/120    avg_loss:0.051, val_acc:0.966]
Epoch [43/120    avg_loss:0.035, val_acc:0.952]
Epoch [44/120    avg_loss:0.039, val_acc:0.969]
Epoch [45/120    avg_loss:0.045, val_acc:0.973]
Epoch [46/120    avg_loss:0.034, val_acc:0.928]
Epoch [47/120    avg_loss:0.035, val_acc:0.974]
Epoch [48/120    avg_loss:0.030, val_acc:0.973]
Epoch [49/120    avg_loss:0.026, val_acc:0.972]
Epoch [50/120    avg_loss:0.031, val_acc:0.964]
Epoch [51/120    avg_loss:0.144, val_acc:0.924]
Epoch [52/120    avg_loss:0.119, val_acc:0.910]
Epoch [53/120    avg_loss:0.067, val_acc:0.941]
Epoch [54/120    avg_loss:0.063, val_acc:0.937]
Epoch [55/120    avg_loss:0.047, val_acc:0.948]
Epoch [56/120    avg_loss:0.062, val_acc:0.941]
Epoch [57/120    avg_loss:0.050, val_acc:0.962]
Epoch [58/120    avg_loss:0.042, val_acc:0.968]
Epoch [59/120    avg_loss:0.036, val_acc:0.970]
Epoch [60/120    avg_loss:0.035, val_acc:0.971]
Epoch [61/120    avg_loss:0.021, val_acc:0.973]
Epoch [62/120    avg_loss:0.018, val_acc:0.975]
Epoch [63/120    avg_loss:0.014, val_acc:0.973]
Epoch [64/120    avg_loss:0.017, val_acc:0.974]
Epoch [65/120    avg_loss:0.014, val_acc:0.975]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.020, val_acc:0.973]
Epoch [68/120    avg_loss:0.016, val_acc:0.974]
Epoch [69/120    avg_loss:0.013, val_acc:0.975]
Epoch [70/120    avg_loss:0.015, val_acc:0.976]
Epoch [71/120    avg_loss:0.014, val_acc:0.975]
Epoch [72/120    avg_loss:0.019, val_acc:0.973]
Epoch [73/120    avg_loss:0.014, val_acc:0.975]
Epoch [74/120    avg_loss:0.012, val_acc:0.975]
Epoch [75/120    avg_loss:0.015, val_acc:0.975]
Epoch [76/120    avg_loss:0.014, val_acc:0.976]
Epoch [77/120    avg_loss:0.017, val_acc:0.978]
Epoch [78/120    avg_loss:0.014, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.979]
Epoch [80/120    avg_loss:0.015, val_acc:0.978]
Epoch [81/120    avg_loss:0.018, val_acc:0.976]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.012, val_acc:0.978]
Epoch [84/120    avg_loss:0.011, val_acc:0.976]
Epoch [85/120    avg_loss:0.021, val_acc:0.977]
Epoch [86/120    avg_loss:0.014, val_acc:0.977]
Epoch [87/120    avg_loss:0.015, val_acc:0.976]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.009, val_acc:0.978]
Epoch [90/120    avg_loss:0.015, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.975]
Epoch [92/120    avg_loss:0.016, val_acc:0.978]
Epoch [93/120    avg_loss:0.014, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.012, val_acc:0.979]
Epoch [96/120    avg_loss:0.012, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.010, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.978]
Epoch [103/120    avg_loss:0.012, val_acc:0.978]
Epoch [104/120    avg_loss:0.014, val_acc:0.978]
Epoch [105/120    avg_loss:0.013, val_acc:0.978]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.009, val_acc:0.978]
Epoch [108/120    avg_loss:0.009, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.010, val_acc:0.978]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.009, val_acc:0.978]
Epoch [113/120    avg_loss:0.013, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.011, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.013, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.014, val_acc:0.978]
Epoch [120/120    avg_loss:0.014, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6317     0     1     0     0    11     9    89     5]
 [    0     2 18029     0    49     0    10     0     0     0]
 [    0     5     0  1900     0     0     0     0   131     0]
 [    0     9    10     0  2934     0    10     4     3     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    44     0     0     0  4827     0     7     0]
 [    0     3     0     0     0     0     0  1285     2     0]
 [    0    77     1    62    27     0     4     0  3399     1]
 [    0     0     0     0     8     7     0     0     0   904]]

Accuracy:
98.57084327476925

F1 scores:
[       nan 0.98357337 0.99679328 0.95023756 0.97963272 0.99732518
 0.99117043 0.99304482 0.94390447 0.98743856]

Kappa:
0.9810641717428743
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8ca4e5978>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.000, val_acc:0.127]
Epoch [2/120    avg_loss:1.528, val_acc:0.288]
Epoch [3/120    avg_loss:1.261, val_acc:0.679]
Epoch [4/120    avg_loss:1.041, val_acc:0.683]
Epoch [5/120    avg_loss:0.877, val_acc:0.720]
Epoch [6/120    avg_loss:0.776, val_acc:0.671]
Epoch [7/120    avg_loss:0.630, val_acc:0.646]
Epoch [8/120    avg_loss:0.553, val_acc:0.663]
Epoch [9/120    avg_loss:0.503, val_acc:0.702]
Epoch [10/120    avg_loss:0.442, val_acc:0.771]
Epoch [11/120    avg_loss:0.410, val_acc:0.753]
Epoch [12/120    avg_loss:0.363, val_acc:0.773]
Epoch [13/120    avg_loss:0.350, val_acc:0.801]
Epoch [14/120    avg_loss:0.303, val_acc:0.810]
Epoch [15/120    avg_loss:0.272, val_acc:0.846]
Epoch [16/120    avg_loss:0.267, val_acc:0.873]
Epoch [17/120    avg_loss:0.253, val_acc:0.853]
Epoch [18/120    avg_loss:0.254, val_acc:0.840]
Epoch [19/120    avg_loss:0.242, val_acc:0.924]
Epoch [20/120    avg_loss:0.192, val_acc:0.933]
Epoch [21/120    avg_loss:0.174, val_acc:0.918]
Epoch [22/120    avg_loss:0.146, val_acc:0.925]
Epoch [23/120    avg_loss:0.137, val_acc:0.929]
Epoch [24/120    avg_loss:0.147, val_acc:0.894]
Epoch [25/120    avg_loss:0.167, val_acc:0.926]
Epoch [26/120    avg_loss:0.134, val_acc:0.951]
Epoch [27/120    avg_loss:0.123, val_acc:0.928]
Epoch [28/120    avg_loss:0.104, val_acc:0.937]
Epoch [29/120    avg_loss:0.138, val_acc:0.926]
Epoch [30/120    avg_loss:0.110, val_acc:0.950]
Epoch [31/120    avg_loss:0.091, val_acc:0.946]
Epoch [32/120    avg_loss:0.097, val_acc:0.944]
Epoch [33/120    avg_loss:0.080, val_acc:0.946]
Epoch [34/120    avg_loss:0.105, val_acc:0.948]
Epoch [35/120    avg_loss:0.058, val_acc:0.960]
Epoch [36/120    avg_loss:0.086, val_acc:0.929]
Epoch [37/120    avg_loss:0.084, val_acc:0.940]
Epoch [38/120    avg_loss:0.064, val_acc:0.933]
Epoch [39/120    avg_loss:0.063, val_acc:0.959]
Epoch [40/120    avg_loss:0.059, val_acc:0.957]
Epoch [41/120    avg_loss:0.044, val_acc:0.966]
Epoch [42/120    avg_loss:0.041, val_acc:0.966]
Epoch [43/120    avg_loss:0.046, val_acc:0.961]
Epoch [44/120    avg_loss:0.036, val_acc:0.971]
Epoch [45/120    avg_loss:0.056, val_acc:0.952]
Epoch [46/120    avg_loss:0.057, val_acc:0.965]
Epoch [47/120    avg_loss:0.058, val_acc:0.944]
Epoch [48/120    avg_loss:0.067, val_acc:0.958]
Epoch [49/120    avg_loss:0.067, val_acc:0.933]
Epoch [50/120    avg_loss:0.085, val_acc:0.938]
Epoch [51/120    avg_loss:0.063, val_acc:0.972]
Epoch [52/120    avg_loss:0.033, val_acc:0.971]
Epoch [53/120    avg_loss:0.027, val_acc:0.970]
Epoch [54/120    avg_loss:0.028, val_acc:0.967]
Epoch [55/120    avg_loss:0.031, val_acc:0.971]
Epoch [56/120    avg_loss:0.112, val_acc:0.950]
Epoch [57/120    avg_loss:0.065, val_acc:0.960]
Epoch [58/120    avg_loss:0.053, val_acc:0.970]
Epoch [59/120    avg_loss:0.044, val_acc:0.971]
Epoch [60/120    avg_loss:0.027, val_acc:0.969]
Epoch [61/120    avg_loss:0.037, val_acc:0.973]
Epoch [62/120    avg_loss:0.037, val_acc:0.979]
Epoch [63/120    avg_loss:0.020, val_acc:0.980]
Epoch [64/120    avg_loss:0.018, val_acc:0.979]
Epoch [65/120    avg_loss:0.014, val_acc:0.980]
Epoch [66/120    avg_loss:0.018, val_acc:0.976]
Epoch [67/120    avg_loss:0.026, val_acc:0.970]
Epoch [68/120    avg_loss:0.020, val_acc:0.979]
Epoch [69/120    avg_loss:0.021, val_acc:0.975]
Epoch [70/120    avg_loss:0.021, val_acc:0.978]
Epoch [71/120    avg_loss:0.023, val_acc:0.972]
Epoch [72/120    avg_loss:0.027, val_acc:0.979]
Epoch [73/120    avg_loss:0.024, val_acc:0.978]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.019, val_acc:0.982]
Epoch [76/120    avg_loss:0.021, val_acc:0.979]
Epoch [77/120    avg_loss:0.009, val_acc:0.980]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.960]
Epoch [82/120    avg_loss:0.027, val_acc:0.979]
Epoch [83/120    avg_loss:0.081, val_acc:0.947]
Epoch [84/120    avg_loss:0.045, val_acc:0.946]
Epoch [85/120    avg_loss:0.058, val_acc:0.969]
Epoch [86/120    avg_loss:0.019, val_acc:0.978]
Epoch [87/120    avg_loss:0.031, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.972]
Epoch [89/120    avg_loss:0.021, val_acc:0.974]
Epoch [90/120    avg_loss:0.013, val_acc:0.977]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.978]
Epoch [93/120    avg_loss:0.032, val_acc:0.958]
Epoch [94/120    avg_loss:0.022, val_acc:0.979]
Epoch [95/120    avg_loss:0.019, val_acc:0.979]
Epoch [96/120    avg_loss:0.011, val_acc:0.978]
Epoch [97/120    avg_loss:0.021, val_acc:0.972]
Epoch [98/120    avg_loss:0.019, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.015, val_acc:0.970]
Epoch [105/120    avg_loss:0.017, val_acc:0.970]
Epoch [106/120    avg_loss:0.041, val_acc:0.983]
Epoch [107/120    avg_loss:0.031, val_acc:0.974]
Epoch [108/120    avg_loss:0.030, val_acc:0.967]
Epoch [109/120    avg_loss:0.017, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.017, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     0     0     2     0    46     2]
 [    0     0 17959     0     2     0   125     0     4     0]
 [    0     5     0  1932     0     0     0     0    99     0]
 [    0    28     7     0  2932     0     4     0     1     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     0     0  4861     0     3     0]
 [    0    22     0     0     0     0     0  1267     1     0]
 [    0    27     0    15    44     0     0     0  3485     0]
 [    0     0     0     0     1     8     0     0     0   910]]

Accuracy:
98.89137926879233

F1 scores:
[       nan 0.98976427 0.99578597 0.97012302 0.98538061 0.99694423
 0.98500507 0.99100508 0.9667129  0.99399235]

Kappa:
0.9853233354037381
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1241347898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.960, val_acc:0.170]
Epoch [2/120    avg_loss:1.513, val_acc:0.258]
Epoch [3/120    avg_loss:1.267, val_acc:0.372]
Epoch [4/120    avg_loss:1.032, val_acc:0.539]
Epoch [5/120    avg_loss:0.842, val_acc:0.569]
Epoch [6/120    avg_loss:0.698, val_acc:0.569]
Epoch [7/120    avg_loss:0.577, val_acc:0.601]
Epoch [8/120    avg_loss:0.500, val_acc:0.671]
Epoch [9/120    avg_loss:0.450, val_acc:0.712]
Epoch [10/120    avg_loss:0.401, val_acc:0.715]
Epoch [11/120    avg_loss:0.349, val_acc:0.741]
Epoch [12/120    avg_loss:0.328, val_acc:0.783]
Epoch [13/120    avg_loss:0.290, val_acc:0.841]
Epoch [14/120    avg_loss:0.264, val_acc:0.834]
Epoch [15/120    avg_loss:0.230, val_acc:0.919]
Epoch [16/120    avg_loss:0.209, val_acc:0.921]
Epoch [17/120    avg_loss:0.178, val_acc:0.905]
Epoch [18/120    avg_loss:0.198, val_acc:0.924]
Epoch [19/120    avg_loss:0.228, val_acc:0.883]
Epoch [20/120    avg_loss:0.222, val_acc:0.880]
Epoch [21/120    avg_loss:0.186, val_acc:0.952]
Epoch [22/120    avg_loss:0.157, val_acc:0.951]
Epoch [23/120    avg_loss:0.146, val_acc:0.958]
Epoch [24/120    avg_loss:0.126, val_acc:0.961]
Epoch [25/120    avg_loss:0.112, val_acc:0.966]
Epoch [26/120    avg_loss:0.130, val_acc:0.966]
Epoch [27/120    avg_loss:0.098, val_acc:0.834]
Epoch [28/120    avg_loss:0.113, val_acc:0.972]
Epoch [29/120    avg_loss:0.081, val_acc:0.971]
Epoch [30/120    avg_loss:0.095, val_acc:0.975]
Epoch [31/120    avg_loss:0.085, val_acc:0.911]
Epoch [32/120    avg_loss:0.079, val_acc:0.966]
Epoch [33/120    avg_loss:0.076, val_acc:0.971]
Epoch [34/120    avg_loss:0.061, val_acc:0.961]
Epoch [35/120    avg_loss:0.050, val_acc:0.975]
Epoch [36/120    avg_loss:0.047, val_acc:0.975]
Epoch [37/120    avg_loss:0.070, val_acc:0.971]
Epoch [38/120    avg_loss:0.059, val_acc:0.977]
Epoch [39/120    avg_loss:0.037, val_acc:0.979]
Epoch [40/120    avg_loss:0.042, val_acc:0.971]
Epoch [41/120    avg_loss:0.039, val_acc:0.975]
Epoch [42/120    avg_loss:0.052, val_acc:0.969]
Epoch [43/120    avg_loss:0.056, val_acc:0.965]
Epoch [44/120    avg_loss:0.051, val_acc:0.982]
Epoch [45/120    avg_loss:0.037, val_acc:0.979]
Epoch [46/120    avg_loss:0.032, val_acc:0.971]
Epoch [47/120    avg_loss:0.035, val_acc:0.979]
Epoch [48/120    avg_loss:0.030, val_acc:0.973]
Epoch [49/120    avg_loss:0.048, val_acc:0.969]
Epoch [50/120    avg_loss:0.053, val_acc:0.947]
Epoch [51/120    avg_loss:0.062, val_acc:0.964]
Epoch [52/120    avg_loss:0.043, val_acc:0.983]
Epoch [53/120    avg_loss:0.035, val_acc:0.975]
Epoch [54/120    avg_loss:0.020, val_acc:0.974]
Epoch [55/120    avg_loss:0.034, val_acc:0.968]
Epoch [56/120    avg_loss:0.024, val_acc:0.977]
Epoch [57/120    avg_loss:0.032, val_acc:0.982]
Epoch [58/120    avg_loss:0.023, val_acc:0.952]
Epoch [59/120    avg_loss:0.053, val_acc:0.977]
Epoch [60/120    avg_loss:0.037, val_acc:0.976]
Epoch [61/120    avg_loss:0.035, val_acc:0.972]
Epoch [62/120    avg_loss:0.049, val_acc:0.981]
Epoch [63/120    avg_loss:0.029, val_acc:0.934]
Epoch [64/120    avg_loss:0.019, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.981]
Epoch [66/120    avg_loss:0.014, val_acc:0.981]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.011, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.987]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.008, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.011, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.986]
Epoch [75/120    avg_loss:0.014, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.986]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.987]
Epoch [83/120    avg_loss:0.008, val_acc:0.988]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.989]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0     2     0     0    11     0    61     4]
 [    0     0 18068     0    11     0     9     0     2     0]
 [    0    10     0  1921     0     0     0     0   105     0]
 [    0    20    17     0  2927     0     4     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    38     0     0     0  4839     0     1     0]
 [    0    25     0     0     0     0     0  1262     3     0]
 [    0    53     8    61    31     0     0     0  3418     0]
 [    0     0     0     0     3    10     0     0     0   906]]

Accuracy:
98.81184778155351

F1 scores:
[       nan 0.98557469 0.9976533  0.95572139 0.98485868 0.99618321
 0.99353249 0.98902821 0.95408234 0.9907053 ]

Kappa:
0.9842464882255713
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8bee7bd8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.959, val_acc:0.215]
Epoch [2/120    avg_loss:1.491, val_acc:0.273]
Epoch [3/120    avg_loss:1.245, val_acc:0.362]
Epoch [4/120    avg_loss:1.050, val_acc:0.451]
Epoch [5/120    avg_loss:0.887, val_acc:0.610]
Epoch [6/120    avg_loss:0.731, val_acc:0.649]
Epoch [7/120    avg_loss:0.606, val_acc:0.677]
Epoch [8/120    avg_loss:0.522, val_acc:0.774]
Epoch [9/120    avg_loss:0.451, val_acc:0.793]
Epoch [10/120    avg_loss:0.411, val_acc:0.815]
Epoch [11/120    avg_loss:0.362, val_acc:0.803]
Epoch [12/120    avg_loss:0.344, val_acc:0.813]
Epoch [13/120    avg_loss:0.311, val_acc:0.823]
Epoch [14/120    avg_loss:0.307, val_acc:0.858]
Epoch [15/120    avg_loss:0.317, val_acc:0.858]
Epoch [16/120    avg_loss:0.266, val_acc:0.875]
Epoch [17/120    avg_loss:0.275, val_acc:0.897]
Epoch [18/120    avg_loss:0.264, val_acc:0.885]
Epoch [19/120    avg_loss:0.207, val_acc:0.899]
Epoch [20/120    avg_loss:0.173, val_acc:0.934]
Epoch [21/120    avg_loss:0.179, val_acc:0.940]
Epoch [22/120    avg_loss:0.156, val_acc:0.934]
Epoch [23/120    avg_loss:0.156, val_acc:0.917]
Epoch [24/120    avg_loss:0.136, val_acc:0.945]
Epoch [25/120    avg_loss:0.177, val_acc:0.957]
Epoch [26/120    avg_loss:0.142, val_acc:0.931]
Epoch [27/120    avg_loss:0.339, val_acc:0.907]
Epoch [28/120    avg_loss:0.222, val_acc:0.941]
Epoch [29/120    avg_loss:0.179, val_acc:0.929]
Epoch [30/120    avg_loss:0.149, val_acc:0.895]
Epoch [31/120    avg_loss:0.128, val_acc:0.951]
Epoch [32/120    avg_loss:0.110, val_acc:0.955]
Epoch [33/120    avg_loss:0.094, val_acc:0.920]
Epoch [34/120    avg_loss:0.084, val_acc:0.933]
Epoch [35/120    avg_loss:0.072, val_acc:0.924]
Epoch [36/120    avg_loss:0.073, val_acc:0.963]
Epoch [37/120    avg_loss:0.070, val_acc:0.967]
Epoch [38/120    avg_loss:0.096, val_acc:0.970]
Epoch [39/120    avg_loss:0.096, val_acc:0.950]
Epoch [40/120    avg_loss:0.089, val_acc:0.948]
Epoch [41/120    avg_loss:0.095, val_acc:0.918]
Epoch [42/120    avg_loss:0.072, val_acc:0.959]
Epoch [43/120    avg_loss:0.060, val_acc:0.975]
Epoch [44/120    avg_loss:0.052, val_acc:0.967]
Epoch [45/120    avg_loss:0.047, val_acc:0.976]
Epoch [46/120    avg_loss:0.033, val_acc:0.981]
Epoch [47/120    avg_loss:0.056, val_acc:0.966]
Epoch [48/120    avg_loss:0.047, val_acc:0.979]
Epoch [49/120    avg_loss:0.043, val_acc:0.953]
Epoch [50/120    avg_loss:0.040, val_acc:0.972]
Epoch [51/120    avg_loss:0.055, val_acc:0.970]
Epoch [52/120    avg_loss:0.029, val_acc:0.982]
Epoch [53/120    avg_loss:0.027, val_acc:0.980]
Epoch [54/120    avg_loss:0.031, val_acc:0.973]
Epoch [55/120    avg_loss:0.031, val_acc:0.981]
Epoch [56/120    avg_loss:0.028, val_acc:0.981]
Epoch [57/120    avg_loss:0.016, val_acc:0.965]
Epoch [58/120    avg_loss:0.020, val_acc:0.985]
Epoch [59/120    avg_loss:0.025, val_acc:0.981]
Epoch [60/120    avg_loss:0.043, val_acc:0.977]
Epoch [61/120    avg_loss:0.018, val_acc:0.986]
Epoch [62/120    avg_loss:0.016, val_acc:0.985]
Epoch [63/120    avg_loss:0.020, val_acc:0.977]
Epoch [64/120    avg_loss:0.019, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.982]
Epoch [66/120    avg_loss:0.018, val_acc:0.982]
Epoch [67/120    avg_loss:0.014, val_acc:0.985]
Epoch [68/120    avg_loss:0.010, val_acc:0.987]
Epoch [69/120    avg_loss:0.011, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.983]
Epoch [72/120    avg_loss:0.014, val_acc:0.963]
Epoch [73/120    avg_loss:0.018, val_acc:0.983]
Epoch [74/120    avg_loss:0.013, val_acc:0.987]
Epoch [75/120    avg_loss:0.011, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.985]
Epoch [77/120    avg_loss:0.014, val_acc:0.981]
Epoch [78/120    avg_loss:0.020, val_acc:0.972]
Epoch [79/120    avg_loss:0.018, val_acc:0.978]
Epoch [80/120    avg_loss:0.020, val_acc:0.982]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.982]
Epoch [83/120    avg_loss:0.012, val_acc:0.985]
Epoch [84/120    avg_loss:0.035, val_acc:0.969]
Epoch [85/120    avg_loss:0.023, val_acc:0.982]
Epoch [86/120    avg_loss:0.020, val_acc:0.982]
Epoch [87/120    avg_loss:0.019, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6318     0     0     0     0     0     3   101    10]
 [    0     0 18051     0     8     0    28     0     3     0]
 [    0     4     0  1907     0     0     0     0   125     0]
 [    0    21     9     2  2935     0     2     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    10     0     0     0  4849     0    19     0]
 [    0     8     0     0     0     0     0  1281     1     0]
 [    0    63     0    61    30     0     6     0  3411     0]
 [    0     1     0     0     0    14     0     0     0   904]]

Accuracy:
98.71785602390764

F1 scores:
[       nan 0.98357593 0.99839602 0.95207189 0.98738436 0.99466463
 0.99334221 0.995338   0.94304672 0.98636116]

Kappa:
0.9830138761161423
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75237298d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.899, val_acc:0.139]
Epoch [2/120    avg_loss:1.463, val_acc:0.312]
Epoch [3/120    avg_loss:1.248, val_acc:0.369]
Epoch [4/120    avg_loss:1.032, val_acc:0.605]
Epoch [5/120    avg_loss:0.894, val_acc:0.746]
Epoch [6/120    avg_loss:0.774, val_acc:0.750]
Epoch [7/120    avg_loss:0.708, val_acc:0.749]
Epoch [8/120    avg_loss:0.622, val_acc:0.709]
Epoch [9/120    avg_loss:0.551, val_acc:0.660]
Epoch [10/120    avg_loss:0.503, val_acc:0.693]
Epoch [11/120    avg_loss:0.433, val_acc:0.742]
Epoch [12/120    avg_loss:0.376, val_acc:0.752]
Epoch [13/120    avg_loss:0.349, val_acc:0.837]
Epoch [14/120    avg_loss:0.315, val_acc:0.813]
Epoch [15/120    avg_loss:0.280, val_acc:0.830]
Epoch [16/120    avg_loss:0.253, val_acc:0.785]
Epoch [17/120    avg_loss:0.231, val_acc:0.927]
Epoch [18/120    avg_loss:0.173, val_acc:0.941]
Epoch [19/120    avg_loss:0.148, val_acc:0.916]
Epoch [20/120    avg_loss:0.146, val_acc:0.953]
Epoch [21/120    avg_loss:0.163, val_acc:0.920]
Epoch [22/120    avg_loss:0.122, val_acc:0.899]
Epoch [23/120    avg_loss:0.175, val_acc:0.928]
Epoch [24/120    avg_loss:0.138, val_acc:0.954]
Epoch [25/120    avg_loss:0.097, val_acc:0.955]
Epoch [26/120    avg_loss:0.088, val_acc:0.931]
Epoch [27/120    avg_loss:0.085, val_acc:0.927]
Epoch [28/120    avg_loss:0.082, val_acc:0.955]
Epoch [29/120    avg_loss:0.064, val_acc:0.974]
Epoch [30/120    avg_loss:0.077, val_acc:0.964]
Epoch [31/120    avg_loss:0.076, val_acc:0.960]
Epoch [32/120    avg_loss:0.088, val_acc:0.966]
Epoch [33/120    avg_loss:0.090, val_acc:0.940]
Epoch [34/120    avg_loss:0.066, val_acc:0.963]
Epoch [35/120    avg_loss:0.072, val_acc:0.934]
Epoch [36/120    avg_loss:0.085, val_acc:0.962]
Epoch [37/120    avg_loss:0.073, val_acc:0.971]
Epoch [38/120    avg_loss:0.047, val_acc:0.960]
Epoch [39/120    avg_loss:0.064, val_acc:0.973]
Epoch [40/120    avg_loss:0.059, val_acc:0.973]
Epoch [41/120    avg_loss:0.040, val_acc:0.943]
Epoch [42/120    avg_loss:0.069, val_acc:0.935]
Epoch [43/120    avg_loss:0.048, val_acc:0.975]
Epoch [44/120    avg_loss:0.034, val_acc:0.976]
Epoch [45/120    avg_loss:0.033, val_acc:0.977]
Epoch [46/120    avg_loss:0.030, val_acc:0.980]
Epoch [47/120    avg_loss:0.028, val_acc:0.984]
Epoch [48/120    avg_loss:0.023, val_acc:0.984]
Epoch [49/120    avg_loss:0.023, val_acc:0.985]
Epoch [50/120    avg_loss:0.027, val_acc:0.983]
Epoch [51/120    avg_loss:0.025, val_acc:0.982]
Epoch [52/120    avg_loss:0.023, val_acc:0.983]
Epoch [53/120    avg_loss:0.023, val_acc:0.981]
Epoch [54/120    avg_loss:0.020, val_acc:0.985]
Epoch [55/120    avg_loss:0.021, val_acc:0.985]
Epoch [56/120    avg_loss:0.021, val_acc:0.985]
Epoch [57/120    avg_loss:0.024, val_acc:0.983]
Epoch [58/120    avg_loss:0.020, val_acc:0.984]
Epoch [59/120    avg_loss:0.020, val_acc:0.985]
Epoch [60/120    avg_loss:0.021, val_acc:0.982]
Epoch [61/120    avg_loss:0.017, val_acc:0.984]
Epoch [62/120    avg_loss:0.018, val_acc:0.986]
Epoch [63/120    avg_loss:0.022, val_acc:0.985]
Epoch [64/120    avg_loss:0.018, val_acc:0.986]
Epoch [65/120    avg_loss:0.016, val_acc:0.982]
Epoch [66/120    avg_loss:0.022, val_acc:0.986]
Epoch [67/120    avg_loss:0.018, val_acc:0.982]
Epoch [68/120    avg_loss:0.019, val_acc:0.985]
Epoch [69/120    avg_loss:0.019, val_acc:0.985]
Epoch [70/120    avg_loss:0.018, val_acc:0.985]
Epoch [71/120    avg_loss:0.018, val_acc:0.986]
Epoch [72/120    avg_loss:0.018, val_acc:0.986]
Epoch [73/120    avg_loss:0.017, val_acc:0.987]
Epoch [74/120    avg_loss:0.019, val_acc:0.987]
Epoch [75/120    avg_loss:0.020, val_acc:0.983]
Epoch [76/120    avg_loss:0.016, val_acc:0.987]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.017, val_acc:0.984]
Epoch [80/120    avg_loss:0.019, val_acc:0.984]
Epoch [81/120    avg_loss:0.019, val_acc:0.986]
Epoch [82/120    avg_loss:0.014, val_acc:0.984]
Epoch [83/120    avg_loss:0.016, val_acc:0.986]
Epoch [84/120    avg_loss:0.015, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.985]
Epoch [86/120    avg_loss:0.015, val_acc:0.986]
Epoch [87/120    avg_loss:0.019, val_acc:0.986]
Epoch [88/120    avg_loss:0.017, val_acc:0.987]
Epoch [89/120    avg_loss:0.017, val_acc:0.987]
Epoch [90/120    avg_loss:0.018, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.987]
Epoch [92/120    avg_loss:0.015, val_acc:0.988]
Epoch [93/120    avg_loss:0.015, val_acc:0.987]
Epoch [94/120    avg_loss:0.016, val_acc:0.984]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.015, val_acc:0.985]
Epoch [97/120    avg_loss:0.015, val_acc:0.986]
Epoch [98/120    avg_loss:0.015, val_acc:0.985]
Epoch [99/120    avg_loss:0.018, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.986]
Epoch [101/120    avg_loss:0.014, val_acc:0.987]
Epoch [102/120    avg_loss:0.015, val_acc:0.987]
Epoch [103/120    avg_loss:0.015, val_acc:0.987]
Epoch [104/120    avg_loss:0.018, val_acc:0.986]
Epoch [105/120    avg_loss:0.019, val_acc:0.986]
Epoch [106/120    avg_loss:0.013, val_acc:0.986]
Epoch [107/120    avg_loss:0.015, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.015, val_acc:0.986]
Epoch [110/120    avg_loss:0.016, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.986]
Epoch [112/120    avg_loss:0.014, val_acc:0.986]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.018, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.015, val_acc:0.986]
Epoch [117/120    avg_loss:0.013, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6395     0     0     2     0     1     0    32     2]
 [    0     0 17963     0     8     0   115     0     4     0]
 [    0     9     0  1941     0     0     0     0    86     0]
 [    0    10     8     3  2926     0    19     0     3     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    62     0     2     0  4808     0     6     0]
 [    0    11     0     0     0     0     1  1278     0     0]
 [    0    13     1    38    37     0    10     0  3472     0]
 [    0     1     0     0     2    13     0     0     0   903]]

Accuracy:
98.79015737594293

F1 scores:
[       nan 0.99370678 0.99451888 0.96615231 0.98369474 0.99504384
 0.97803092 0.9953271  0.96793978 0.98850575]

Kappa:
0.9839760902658357
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f596ea99898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.990, val_acc:0.209]
Epoch [2/120    avg_loss:1.514, val_acc:0.319]
Epoch [3/120    avg_loss:1.214, val_acc:0.449]
Epoch [4/120    avg_loss:1.008, val_acc:0.631]
Epoch [5/120    avg_loss:0.869, val_acc:0.747]
Epoch [6/120    avg_loss:0.766, val_acc:0.731]
Epoch [7/120    avg_loss:0.712, val_acc:0.753]
Epoch [8/120    avg_loss:0.625, val_acc:0.769]
Epoch [9/120    avg_loss:0.552, val_acc:0.771]
Epoch [10/120    avg_loss:0.468, val_acc:0.763]
Epoch [11/120    avg_loss:0.432, val_acc:0.799]
Epoch [12/120    avg_loss:0.374, val_acc:0.794]
Epoch [13/120    avg_loss:0.338, val_acc:0.815]
Epoch [14/120    avg_loss:0.310, val_acc:0.808]
Epoch [15/120    avg_loss:0.284, val_acc:0.840]
Epoch [16/120    avg_loss:0.274, val_acc:0.812]
Epoch [17/120    avg_loss:0.260, val_acc:0.867]
Epoch [18/120    avg_loss:0.228, val_acc:0.902]
Epoch [19/120    avg_loss:0.228, val_acc:0.928]
Epoch [20/120    avg_loss:0.160, val_acc:0.836]
Epoch [21/120    avg_loss:0.178, val_acc:0.912]
Epoch [22/120    avg_loss:0.165, val_acc:0.907]
Epoch [23/120    avg_loss:0.136, val_acc:0.908]
Epoch [24/120    avg_loss:0.138, val_acc:0.919]
Epoch [25/120    avg_loss:0.158, val_acc:0.961]
Epoch [26/120    avg_loss:0.096, val_acc:0.968]
Epoch [27/120    avg_loss:0.097, val_acc:0.911]
Epoch [28/120    avg_loss:0.067, val_acc:0.955]
Epoch [29/120    avg_loss:0.058, val_acc:0.953]
Epoch [30/120    avg_loss:0.078, val_acc:0.963]
Epoch [31/120    avg_loss:0.065, val_acc:0.962]
Epoch [32/120    avg_loss:0.055, val_acc:0.966]
Epoch [33/120    avg_loss:0.081, val_acc:0.975]
Epoch [34/120    avg_loss:0.074, val_acc:0.969]
Epoch [35/120    avg_loss:0.068, val_acc:0.927]
Epoch [36/120    avg_loss:0.070, val_acc:0.938]
Epoch [37/120    avg_loss:0.059, val_acc:0.971]
Epoch [38/120    avg_loss:0.057, val_acc:0.962]
Epoch [39/120    avg_loss:0.056, val_acc:0.958]
Epoch [40/120    avg_loss:0.065, val_acc:0.963]
Epoch [41/120    avg_loss:0.062, val_acc:0.952]
Epoch [42/120    avg_loss:0.070, val_acc:0.970]
Epoch [43/120    avg_loss:0.057, val_acc:0.977]
Epoch [44/120    avg_loss:0.054, val_acc:0.952]
Epoch [45/120    avg_loss:0.093, val_acc:0.952]
Epoch [46/120    avg_loss:0.040, val_acc:0.970]
Epoch [47/120    avg_loss:0.043, val_acc:0.966]
Epoch [48/120    avg_loss:0.042, val_acc:0.965]
Epoch [49/120    avg_loss:0.087, val_acc:0.962]
Epoch [50/120    avg_loss:0.065, val_acc:0.943]
Epoch [51/120    avg_loss:0.049, val_acc:0.968]
Epoch [52/120    avg_loss:0.082, val_acc:0.969]
Epoch [53/120    avg_loss:0.040, val_acc:0.969]
Epoch [54/120    avg_loss:0.039, val_acc:0.972]
Epoch [55/120    avg_loss:0.031, val_acc:0.975]
Epoch [56/120    avg_loss:0.036, val_acc:0.976]
Epoch [57/120    avg_loss:0.018, val_acc:0.978]
Epoch [58/120    avg_loss:0.023, val_acc:0.981]
Epoch [59/120    avg_loss:0.023, val_acc:0.981]
Epoch [60/120    avg_loss:0.016, val_acc:0.981]
Epoch [61/120    avg_loss:0.018, val_acc:0.981]
Epoch [62/120    avg_loss:0.016, val_acc:0.982]
Epoch [63/120    avg_loss:0.016, val_acc:0.981]
Epoch [64/120    avg_loss:0.013, val_acc:0.980]
Epoch [65/120    avg_loss:0.013, val_acc:0.980]
Epoch [66/120    avg_loss:0.015, val_acc:0.980]
Epoch [67/120    avg_loss:0.017, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.979]
Epoch [69/120    avg_loss:0.014, val_acc:0.979]
Epoch [70/120    avg_loss:0.012, val_acc:0.979]
Epoch [71/120    avg_loss:0.011, val_acc:0.979]
Epoch [72/120    avg_loss:0.014, val_acc:0.979]
Epoch [73/120    avg_loss:0.014, val_acc:0.978]
Epoch [74/120    avg_loss:0.014, val_acc:0.979]
Epoch [75/120    avg_loss:0.012, val_acc:0.977]
Epoch [76/120    avg_loss:0.014, val_acc:0.977]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.012, val_acc:0.977]
Epoch [79/120    avg_loss:0.011, val_acc:0.977]
Epoch [80/120    avg_loss:0.012, val_acc:0.977]
Epoch [81/120    avg_loss:0.012, val_acc:0.977]
Epoch [82/120    avg_loss:0.015, val_acc:0.977]
Epoch [83/120    avg_loss:0.013, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.977]
Epoch [85/120    avg_loss:0.013, val_acc:0.977]
Epoch [86/120    avg_loss:0.009, val_acc:0.977]
Epoch [87/120    avg_loss:0.011, val_acc:0.977]
Epoch [88/120    avg_loss:0.011, val_acc:0.977]
Epoch [89/120    avg_loss:0.014, val_acc:0.977]
Epoch [90/120    avg_loss:0.011, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.014, val_acc:0.977]
Epoch [93/120    avg_loss:0.013, val_acc:0.977]
Epoch [94/120    avg_loss:0.015, val_acc:0.977]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.012, val_acc:0.977]
Epoch [97/120    avg_loss:0.013, val_acc:0.977]
Epoch [98/120    avg_loss:0.011, val_acc:0.977]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.014, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.977]
Epoch [102/120    avg_loss:0.017, val_acc:0.977]
Epoch [103/120    avg_loss:0.013, val_acc:0.977]
Epoch [104/120    avg_loss:0.012, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.977]
Epoch [106/120    avg_loss:0.013, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.977]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.012, val_acc:0.977]
Epoch [110/120    avg_loss:0.011, val_acc:0.977]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.016, val_acc:0.977]
Epoch [113/120    avg_loss:0.011, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.018, val_acc:0.977]
Epoch [116/120    avg_loss:0.011, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.977]
Epoch [119/120    avg_loss:0.013, val_acc:0.977]
Epoch [120/120    avg_loss:0.014, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6321     0     0     2     0    15     1    80    13]
 [    0     0 18003     0    36     0    51     0     0     0]
 [    0     7     0  1933     0     0     0     0    94     2]
 [    0    19     8     0  2927     0     3     0    13     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     0     0     0  4870     0     0     0]
 [    0     1     0     0     0     0     0  1286     2     1]
 [    0    31     0    38    50     0     0     0  3451     1]
 [    0     6     0     0    12    12     0     0     0   889]]

Accuracy:
98.77569710553587

F1 scores:
[       nan 0.98634626 0.99714753 0.96481158 0.9758293  0.99542334
 0.99215646 0.99805976 0.9571488  0.97318008]

Kappa:
0.9837898470147703
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:38
Validation dataloader:38
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f573979d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.914, val_acc:0.522]
Epoch [2/120    avg_loss:1.491, val_acc:0.596]
Epoch [3/120    avg_loss:1.277, val_acc:0.558]
Epoch [4/120    avg_loss:1.084, val_acc:0.715]
Epoch [5/120    avg_loss:0.936, val_acc:0.676]
Epoch [6/120    avg_loss:0.794, val_acc:0.611]
Epoch [7/120    avg_loss:0.661, val_acc:0.663]
Epoch [8/120    avg_loss:0.568, val_acc:0.703]
Epoch [9/120    avg_loss:0.466, val_acc:0.730]
Epoch [10/120    avg_loss:0.432, val_acc:0.747]
Epoch [11/120    avg_loss:0.359, val_acc:0.798]
Epoch [12/120    avg_loss:0.362, val_acc:0.822]
Epoch [13/120    avg_loss:0.313, val_acc:0.839]
Epoch [14/120    avg_loss:0.268, val_acc:0.844]
Epoch [15/120    avg_loss:0.217, val_acc:0.902]
Epoch [16/120    avg_loss:0.205, val_acc:0.870]
Epoch [17/120    avg_loss:0.204, val_acc:0.898]
Epoch [18/120    avg_loss:0.165, val_acc:0.941]
Epoch [19/120    avg_loss:0.137, val_acc:0.951]
Epoch [20/120    avg_loss:0.114, val_acc:0.956]
Epoch [21/120    avg_loss:0.125, val_acc:0.949]
Epoch [22/120    avg_loss:0.120, val_acc:0.956]
Epoch [23/120    avg_loss:0.101, val_acc:0.969]
Epoch [24/120    avg_loss:0.082, val_acc:0.963]
Epoch [25/120    avg_loss:0.094, val_acc:0.951]
Epoch [26/120    avg_loss:0.087, val_acc:0.959]
Epoch [27/120    avg_loss:0.073, val_acc:0.918]
Epoch [28/120    avg_loss:0.077, val_acc:0.953]
Epoch [29/120    avg_loss:0.088, val_acc:0.971]
Epoch [30/120    avg_loss:0.065, val_acc:0.956]
Epoch [31/120    avg_loss:0.068, val_acc:0.965]
Epoch [32/120    avg_loss:0.060, val_acc:0.967]
Epoch [33/120    avg_loss:0.048, val_acc:0.971]
Epoch [34/120    avg_loss:0.043, val_acc:0.969]
Epoch [35/120    avg_loss:0.042, val_acc:0.971]
Epoch [36/120    avg_loss:0.047, val_acc:0.972]
Epoch [37/120    avg_loss:0.037, val_acc:0.975]
Epoch [38/120    avg_loss:0.025, val_acc:0.973]
Epoch [39/120    avg_loss:0.026, val_acc:0.981]
Epoch [40/120    avg_loss:0.029, val_acc:0.971]
Epoch [41/120    avg_loss:0.039, val_acc:0.974]
Epoch [42/120    avg_loss:0.027, val_acc:0.979]
Epoch [43/120    avg_loss:0.033, val_acc:0.977]
Epoch [44/120    avg_loss:0.028, val_acc:0.977]
Epoch [45/120    avg_loss:0.034, val_acc:0.976]
Epoch [46/120    avg_loss:0.021, val_acc:0.981]
Epoch [47/120    avg_loss:0.025, val_acc:0.979]
Epoch [48/120    avg_loss:0.024, val_acc:0.975]
Epoch [49/120    avg_loss:0.040, val_acc:0.979]
Epoch [50/120    avg_loss:0.025, val_acc:0.975]
Epoch [51/120    avg_loss:0.015, val_acc:0.981]
Epoch [52/120    avg_loss:0.020, val_acc:0.980]
Epoch [53/120    avg_loss:0.018, val_acc:0.980]
Epoch [54/120    avg_loss:0.019, val_acc:0.975]
Epoch [55/120    avg_loss:0.025, val_acc:0.971]
Epoch [56/120    avg_loss:0.022, val_acc:0.973]
Epoch [57/120    avg_loss:0.021, val_acc:0.979]
Epoch [58/120    avg_loss:0.020, val_acc:0.979]
Epoch [59/120    avg_loss:0.025, val_acc:0.969]
Epoch [60/120    avg_loss:0.029, val_acc:0.977]
Epoch [61/120    avg_loss:0.018, val_acc:0.976]
Epoch [62/120    avg_loss:0.012, val_acc:0.981]
Epoch [63/120    avg_loss:0.010, val_acc:0.979]
Epoch [64/120    avg_loss:0.025, val_acc:0.965]
Epoch [65/120    avg_loss:0.025, val_acc:0.962]
Epoch [66/120    avg_loss:0.055, val_acc:0.961]
Epoch [67/120    avg_loss:0.049, val_acc:0.968]
Epoch [68/120    avg_loss:0.036, val_acc:0.968]
Epoch [69/120    avg_loss:0.071, val_acc:0.942]
Epoch [70/120    avg_loss:0.049, val_acc:0.973]
Epoch [71/120    avg_loss:0.020, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.977]
Epoch [73/120    avg_loss:0.019, val_acc:0.974]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.016, val_acc:0.979]
Epoch [76/120    avg_loss:0.010, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.985]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6337     0     4     2     0     3    33    50     3]
 [    0     0 18071     0    14     0     2     0     3     0]
 [    0     1     0  1933     0     0     0     0   101     1]
 [    0     9    17     0  2926    10     3     0     4     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    55     0     0     0  4822     0     1     0]
 [    0    24     0     0     0     0     0  1264     2     0]
 [    0    32     0    68    48     0     0     0  3423     0]
 [    0     0     0     0     0    34     0     0     0   885]]

Accuracy:
98.72990624924687

F1 scores:
[       nan 0.98745617 0.99748848 0.95669389 0.98154982 0.98342125
 0.9934075  0.97719366 0.95681342 0.97736057]

Kappa:
0.9831625010425064
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6de755c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.970, val_acc:0.205]
Epoch [2/120    avg_loss:1.555, val_acc:0.311]
Epoch [3/120    avg_loss:1.347, val_acc:0.345]
Epoch [4/120    avg_loss:1.131, val_acc:0.666]
Epoch [5/120    avg_loss:1.004, val_acc:0.724]
Epoch [6/120    avg_loss:0.876, val_acc:0.772]
Epoch [7/120    avg_loss:0.739, val_acc:0.751]
Epoch [8/120    avg_loss:0.645, val_acc:0.770]
Epoch [9/120    avg_loss:0.522, val_acc:0.778]
Epoch [10/120    avg_loss:0.479, val_acc:0.786]
Epoch [11/120    avg_loss:0.431, val_acc:0.792]
Epoch [12/120    avg_loss:0.376, val_acc:0.787]
Epoch [13/120    avg_loss:0.340, val_acc:0.869]
Epoch [14/120    avg_loss:0.317, val_acc:0.821]
Epoch [15/120    avg_loss:0.256, val_acc:0.899]
Epoch [16/120    avg_loss:0.238, val_acc:0.914]
Epoch [17/120    avg_loss:0.231, val_acc:0.866]
Epoch [18/120    avg_loss:0.207, val_acc:0.914]
Epoch [19/120    avg_loss:0.221, val_acc:0.921]
Epoch [20/120    avg_loss:0.177, val_acc:0.931]
Epoch [21/120    avg_loss:0.146, val_acc:0.947]
Epoch [22/120    avg_loss:0.159, val_acc:0.910]
Epoch [23/120    avg_loss:0.179, val_acc:0.959]
Epoch [24/120    avg_loss:0.140, val_acc:0.938]
Epoch [25/120    avg_loss:0.127, val_acc:0.936]
Epoch [26/120    avg_loss:0.112, val_acc:0.970]
Epoch [27/120    avg_loss:0.098, val_acc:0.947]
Epoch [28/120    avg_loss:0.082, val_acc:0.976]
Epoch [29/120    avg_loss:0.073, val_acc:0.970]
Epoch [30/120    avg_loss:0.065, val_acc:0.975]
Epoch [31/120    avg_loss:0.059, val_acc:0.965]
Epoch [32/120    avg_loss:0.077, val_acc:0.959]
Epoch [33/120    avg_loss:0.061, val_acc:0.982]
Epoch [34/120    avg_loss:0.051, val_acc:0.980]
Epoch [35/120    avg_loss:0.045, val_acc:0.962]
Epoch [36/120    avg_loss:0.045, val_acc:0.975]
Epoch [37/120    avg_loss:0.056, val_acc:0.972]
Epoch [38/120    avg_loss:0.092, val_acc:0.961]
Epoch [39/120    avg_loss:0.107, val_acc:0.970]
Epoch [40/120    avg_loss:0.097, val_acc:0.950]
Epoch [41/120    avg_loss:0.057, val_acc:0.968]
Epoch [42/120    avg_loss:0.047, val_acc:0.977]
Epoch [43/120    avg_loss:0.037, val_acc:0.976]
Epoch [44/120    avg_loss:0.030, val_acc:0.979]
Epoch [45/120    avg_loss:0.027, val_acc:0.965]
Epoch [46/120    avg_loss:0.029, val_acc:0.968]
Epoch [47/120    avg_loss:0.038, val_acc:0.975]
Epoch [48/120    avg_loss:0.022, val_acc:0.981]
Epoch [49/120    avg_loss:0.019, val_acc:0.980]
Epoch [50/120    avg_loss:0.021, val_acc:0.979]
Epoch [51/120    avg_loss:0.018, val_acc:0.981]
Epoch [52/120    avg_loss:0.018, val_acc:0.981]
Epoch [53/120    avg_loss:0.016, val_acc:0.982]
Epoch [54/120    avg_loss:0.018, val_acc:0.981]
Epoch [55/120    avg_loss:0.017, val_acc:0.981]
Epoch [56/120    avg_loss:0.015, val_acc:0.980]
Epoch [57/120    avg_loss:0.015, val_acc:0.982]
Epoch [58/120    avg_loss:0.021, val_acc:0.981]
Epoch [59/120    avg_loss:0.018, val_acc:0.981]
Epoch [60/120    avg_loss:0.016, val_acc:0.981]
Epoch [61/120    avg_loss:0.015, val_acc:0.981]
Epoch [62/120    avg_loss:0.014, val_acc:0.981]
Epoch [63/120    avg_loss:0.015, val_acc:0.981]
Epoch [64/120    avg_loss:0.017, val_acc:0.981]
Epoch [65/120    avg_loss:0.017, val_acc:0.981]
Epoch [66/120    avg_loss:0.017, val_acc:0.981]
Epoch [67/120    avg_loss:0.015, val_acc:0.981]
Epoch [68/120    avg_loss:0.013, val_acc:0.981]
Epoch [69/120    avg_loss:0.015, val_acc:0.981]
Epoch [70/120    avg_loss:0.013, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.981]
Epoch [72/120    avg_loss:0.012, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.981]
Epoch [74/120    avg_loss:0.018, val_acc:0.981]
Epoch [75/120    avg_loss:0.014, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.981]
Epoch [77/120    avg_loss:0.015, val_acc:0.981]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.016, val_acc:0.981]
Epoch [81/120    avg_loss:0.015, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.015, val_acc:0.981]
Epoch [84/120    avg_loss:0.025, val_acc:0.981]
Epoch [85/120    avg_loss:0.015, val_acc:0.981]
Epoch [86/120    avg_loss:0.014, val_acc:0.981]
Epoch [87/120    avg_loss:0.017, val_acc:0.981]
Epoch [88/120    avg_loss:0.018, val_acc:0.981]
Epoch [89/120    avg_loss:0.014, val_acc:0.981]
Epoch [90/120    avg_loss:0.012, val_acc:0.981]
Epoch [91/120    avg_loss:0.015, val_acc:0.981]
Epoch [92/120    avg_loss:0.013, val_acc:0.981]
Epoch [93/120    avg_loss:0.016, val_acc:0.981]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.017, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.981]
Epoch [97/120    avg_loss:0.016, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.981]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.014, val_acc:0.981]
Epoch [101/120    avg_loss:0.014, val_acc:0.981]
Epoch [102/120    avg_loss:0.015, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.014, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.981]
Epoch [106/120    avg_loss:0.012, val_acc:0.981]
Epoch [107/120    avg_loss:0.015, val_acc:0.981]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.981]
Epoch [112/120    avg_loss:0.014, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.981]
Epoch [116/120    avg_loss:0.016, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.013, val_acc:0.981]
Epoch [120/120    avg_loss:0.015, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6281     0    13     0     0    13    26    84    15]
 [    0     0 18062     0    15     0    13     0     0     0]
 [    0     2     0  1964     0     0     0     0    66     4]
 [    0    25    10     0  2923     0     5     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     2     0     0  4871     0     0     0]
 [    0     3     0     0     0     0     1  1283     0     3]
 [    0    24     0    15    44     0     0     0  3486     2]
 [    0     0     0     0     2    28     0     0     0   889]]

Accuracy:
98.96609066589545

F1 scores:
[       nan 0.98394298 0.99881107 0.97468983 0.98153123 0.9893859
 0.99601268 0.98730281 0.96632017 0.96999454]

Kappa:
0.9863050289234792
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf2eda1908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.984, val_acc:0.492]
Epoch [2/120    avg_loss:1.573, val_acc:0.610]
Epoch [3/120    avg_loss:1.363, val_acc:0.651]
Epoch [4/120    avg_loss:1.165, val_acc:0.515]
Epoch [5/120    avg_loss:1.019, val_acc:0.605]
Epoch [6/120    avg_loss:0.888, val_acc:0.581]
Epoch [7/120    avg_loss:0.751, val_acc:0.650]
Epoch [8/120    avg_loss:0.643, val_acc:0.659]
Epoch [9/120    avg_loss:0.552, val_acc:0.699]
Epoch [10/120    avg_loss:0.469, val_acc:0.699]
Epoch [11/120    avg_loss:0.413, val_acc:0.747]
Epoch [12/120    avg_loss:0.376, val_acc:0.777]
Epoch [13/120    avg_loss:0.343, val_acc:0.795]
Epoch [14/120    avg_loss:0.327, val_acc:0.769]
Epoch [15/120    avg_loss:0.286, val_acc:0.801]
Epoch [16/120    avg_loss:0.261, val_acc:0.816]
Epoch [17/120    avg_loss:0.248, val_acc:0.857]
Epoch [18/120    avg_loss:0.194, val_acc:0.878]
Epoch [19/120    avg_loss:0.196, val_acc:0.885]
Epoch [20/120    avg_loss:0.158, val_acc:0.914]
Epoch [21/120    avg_loss:0.149, val_acc:0.810]
Epoch [22/120    avg_loss:0.147, val_acc:0.935]
Epoch [23/120    avg_loss:0.128, val_acc:0.879]
Epoch [24/120    avg_loss:0.129, val_acc:0.954]
Epoch [25/120    avg_loss:0.087, val_acc:0.951]
Epoch [26/120    avg_loss:0.089, val_acc:0.924]
Epoch [27/120    avg_loss:0.080, val_acc:0.948]
Epoch [28/120    avg_loss:0.077, val_acc:0.965]
Epoch [29/120    avg_loss:0.068, val_acc:0.969]
Epoch [30/120    avg_loss:0.061, val_acc:0.959]
Epoch [31/120    avg_loss:0.098, val_acc:0.906]
Epoch [32/120    avg_loss:0.091, val_acc:0.965]
Epoch [33/120    avg_loss:0.069, val_acc:0.961]
Epoch [34/120    avg_loss:0.065, val_acc:0.970]
Epoch [35/120    avg_loss:0.050, val_acc:0.976]
Epoch [36/120    avg_loss:0.044, val_acc:0.977]
Epoch [37/120    avg_loss:0.044, val_acc:0.982]
Epoch [38/120    avg_loss:0.041, val_acc:0.978]
Epoch [39/120    avg_loss:0.037, val_acc:0.976]
Epoch [40/120    avg_loss:0.037, val_acc:0.975]
Epoch [41/120    avg_loss:0.034, val_acc:0.975]
Epoch [42/120    avg_loss:0.034, val_acc:0.973]
Epoch [43/120    avg_loss:0.028, val_acc:0.974]
Epoch [44/120    avg_loss:0.029, val_acc:0.975]
Epoch [45/120    avg_loss:0.028, val_acc:0.970]
Epoch [46/120    avg_loss:0.025, val_acc:0.976]
Epoch [47/120    avg_loss:0.019, val_acc:0.975]
Epoch [48/120    avg_loss:0.036, val_acc:0.974]
Epoch [49/120    avg_loss:0.037, val_acc:0.978]
Epoch [50/120    avg_loss:0.029, val_acc:0.977]
Epoch [51/120    avg_loss:0.023, val_acc:0.981]
Epoch [52/120    avg_loss:0.017, val_acc:0.981]
Epoch [53/120    avg_loss:0.015, val_acc:0.986]
Epoch [54/120    avg_loss:0.013, val_acc:0.984]
Epoch [55/120    avg_loss:0.015, val_acc:0.985]
Epoch [56/120    avg_loss:0.014, val_acc:0.986]
Epoch [57/120    avg_loss:0.015, val_acc:0.984]
Epoch [58/120    avg_loss:0.012, val_acc:0.986]
Epoch [59/120    avg_loss:0.017, val_acc:0.986]
Epoch [60/120    avg_loss:0.011, val_acc:0.983]
Epoch [61/120    avg_loss:0.012, val_acc:0.984]
Epoch [62/120    avg_loss:0.015, val_acc:0.985]
Epoch [63/120    avg_loss:0.011, val_acc:0.986]
Epoch [64/120    avg_loss:0.014, val_acc:0.985]
Epoch [65/120    avg_loss:0.013, val_acc:0.984]
Epoch [66/120    avg_loss:0.013, val_acc:0.981]
Epoch [67/120    avg_loss:0.014, val_acc:0.982]
Epoch [68/120    avg_loss:0.012, val_acc:0.986]
Epoch [69/120    avg_loss:0.013, val_acc:0.983]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.013, val_acc:0.983]
Epoch [72/120    avg_loss:0.014, val_acc:0.983]
Epoch [73/120    avg_loss:0.012, val_acc:0.984]
Epoch [74/120    avg_loss:0.013, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.011, val_acc:0.982]
Epoch [80/120    avg_loss:0.013, val_acc:0.983]
Epoch [81/120    avg_loss:0.013, val_acc:0.982]
Epoch [82/120    avg_loss:0.009, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.982]
Epoch [85/120    avg_loss:0.011, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.983]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.983]
Epoch [99/120    avg_loss:0.010, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6332     0     0     0     0    14     6    80     0]
 [    0     3 18047     0    38     0     1     0     1     0]
 [    0     6     0  1962     0     0     0     0    67     1]
 [    0    36     2     0  2901     0    15     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    37     1     0     0  4837     0     3     0]
 [    0     4     0     0     0     0     0  1286     0     0]
 [    0    62     0    13    46     0     0     0  3450     0]
 [    0     0     0     0    16     7     0     0     0   896]]

Accuracy:
98.850408502639

F1 scores:
[       nan 0.98361165 0.9977333  0.9780658  0.97137117 0.99732518
 0.99271421 0.99612703 0.9596662  0.98678414]

Kappa:
0.9847663780090934
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88a2d159e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.938, val_acc:0.137]
Epoch [2/120    avg_loss:1.510, val_acc:0.428]
Epoch [3/120    avg_loss:1.318, val_acc:0.640]
Epoch [4/120    avg_loss:1.157, val_acc:0.641]
Epoch [5/120    avg_loss:1.022, val_acc:0.656]
Epoch [6/120    avg_loss:0.904, val_acc:0.719]
Epoch [7/120    avg_loss:0.804, val_acc:0.677]
Epoch [8/120    avg_loss:0.666, val_acc:0.732]
Epoch [9/120    avg_loss:0.603, val_acc:0.749]
Epoch [10/120    avg_loss:0.533, val_acc:0.754]
Epoch [11/120    avg_loss:0.445, val_acc:0.783]
Epoch [12/120    avg_loss:0.409, val_acc:0.784]
Epoch [13/120    avg_loss:0.362, val_acc:0.791]
Epoch [14/120    avg_loss:0.350, val_acc:0.844]
Epoch [15/120    avg_loss:0.291, val_acc:0.806]
Epoch [16/120    avg_loss:0.241, val_acc:0.825]
Epoch [17/120    avg_loss:0.242, val_acc:0.895]
Epoch [18/120    avg_loss:0.231, val_acc:0.897]
Epoch [19/120    avg_loss:0.219, val_acc:0.856]
Epoch [20/120    avg_loss:0.184, val_acc:0.934]
Epoch [21/120    avg_loss:0.164, val_acc:0.908]
Epoch [22/120    avg_loss:0.165, val_acc:0.946]
Epoch [23/120    avg_loss:0.157, val_acc:0.912]
Epoch [24/120    avg_loss:0.152, val_acc:0.920]
Epoch [25/120    avg_loss:0.149, val_acc:0.928]
Epoch [26/120    avg_loss:0.115, val_acc:0.957]
Epoch [27/120    avg_loss:0.087, val_acc:0.964]
Epoch [28/120    avg_loss:0.119, val_acc:0.948]
Epoch [29/120    avg_loss:0.102, val_acc:0.906]
Epoch [30/120    avg_loss:0.078, val_acc:0.890]
Epoch [31/120    avg_loss:0.123, val_acc:0.922]
Epoch [32/120    avg_loss:0.118, val_acc:0.931]
Epoch [33/120    avg_loss:0.086, val_acc:0.954]
Epoch [34/120    avg_loss:0.087, val_acc:0.939]
Epoch [35/120    avg_loss:0.057, val_acc:0.954]
Epoch [36/120    avg_loss:0.079, val_acc:0.938]
Epoch [37/120    avg_loss:0.051, val_acc:0.948]
Epoch [38/120    avg_loss:0.053, val_acc:0.948]
Epoch [39/120    avg_loss:0.042, val_acc:0.969]
Epoch [40/120    avg_loss:0.042, val_acc:0.965]
Epoch [41/120    avg_loss:0.062, val_acc:0.974]
Epoch [42/120    avg_loss:0.039, val_acc:0.975]
Epoch [43/120    avg_loss:0.069, val_acc:0.951]
Epoch [44/120    avg_loss:0.071, val_acc:0.950]
Epoch [45/120    avg_loss:0.072, val_acc:0.956]
Epoch [46/120    avg_loss:0.051, val_acc:0.965]
Epoch [47/120    avg_loss:0.039, val_acc:0.972]
Epoch [48/120    avg_loss:0.025, val_acc:0.975]
Epoch [49/120    avg_loss:0.033, val_acc:0.964]
Epoch [50/120    avg_loss:0.035, val_acc:0.974]
Epoch [51/120    avg_loss:0.028, val_acc:0.953]
Epoch [52/120    avg_loss:0.063, val_acc:0.970]
Epoch [53/120    avg_loss:0.058, val_acc:0.967]
Epoch [54/120    avg_loss:0.027, val_acc:0.973]
Epoch [55/120    avg_loss:0.037, val_acc:0.968]
Epoch [56/120    avg_loss:0.032, val_acc:0.975]
Epoch [57/120    avg_loss:0.023, val_acc:0.961]
Epoch [58/120    avg_loss:0.021, val_acc:0.980]
Epoch [59/120    avg_loss:0.019, val_acc:0.981]
Epoch [60/120    avg_loss:0.016, val_acc:0.979]
Epoch [61/120    avg_loss:0.028, val_acc:0.977]
Epoch [62/120    avg_loss:0.019, val_acc:0.980]
Epoch [63/120    avg_loss:0.019, val_acc:0.967]
Epoch [64/120    avg_loss:0.027, val_acc:0.971]
Epoch [65/120    avg_loss:0.019, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.976]
Epoch [67/120    avg_loss:0.021, val_acc:0.973]
Epoch [68/120    avg_loss:0.014, val_acc:0.983]
Epoch [69/120    avg_loss:0.019, val_acc:0.975]
Epoch [70/120    avg_loss:0.022, val_acc:0.977]
Epoch [71/120    avg_loss:0.013, val_acc:0.975]
Epoch [72/120    avg_loss:0.011, val_acc:0.979]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.963]
Epoch [75/120    avg_loss:0.012, val_acc:0.981]
Epoch [76/120    avg_loss:0.014, val_acc:0.976]
Epoch [77/120    avg_loss:0.009, val_acc:0.982]
Epoch [78/120    avg_loss:0.007, val_acc:0.982]
Epoch [79/120    avg_loss:0.012, val_acc:0.951]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.010, val_acc:0.978]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.981]
Epoch [84/120    avg_loss:0.007, val_acc:0.980]
Epoch [85/120    avg_loss:0.006, val_acc:0.981]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.006, val_acc:0.981]
Epoch [90/120    avg_loss:0.007, val_acc:0.981]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.005, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.005, val_acc:0.981]
Epoch [96/120    avg_loss:0.007, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.981]
Epoch [99/120    avg_loss:0.005, val_acc:0.981]
Epoch [100/120    avg_loss:0.005, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.005, val_acc:0.981]
Epoch [106/120    avg_loss:0.005, val_acc:0.981]
Epoch [107/120    avg_loss:0.005, val_acc:0.981]
Epoch [108/120    avg_loss:0.005, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.006, val_acc:0.981]
Epoch [111/120    avg_loss:0.005, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.005, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.005, val_acc:0.981]
Epoch [117/120    avg_loss:0.004, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6343     0     1     0     0     6     6    75     1]
 [    0     0 18061     0     9     0     9     0    11     0]
 [    0     2     0  1987     0     0     0     0    44     3]
 [    0    34     8     0  2902     0    12     0     6    10]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    49     0     0     0  4829     0     0     0]
 [    0     3     0     0     0     0     3  1272     0    12]
 [    0    27     0    36    57     0     0     0  3450     1]
 [    0     0     0     0     0    16     0     0     0   903]]

Accuracy:
98.93717012508134

F1 scores:
[       nan 0.98792929 0.99762483 0.97881773 0.97710438 0.99390708
 0.99188662 0.99065421 0.9640911  0.97674419]

Kappa:
0.9859136243154234
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f811ee46908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.944, val_acc:0.169]
Epoch [2/120    avg_loss:1.511, val_acc:0.256]
Epoch [3/120    avg_loss:1.262, val_acc:0.357]
Epoch [4/120    avg_loss:1.136, val_acc:0.695]
Epoch [5/120    avg_loss:0.988, val_acc:0.606]
Epoch [6/120    avg_loss:0.880, val_acc:0.483]
Epoch [7/120    avg_loss:0.714, val_acc:0.604]
Epoch [8/120    avg_loss:0.593, val_acc:0.642]
Epoch [9/120    avg_loss:0.500, val_acc:0.633]
Epoch [10/120    avg_loss:0.456, val_acc:0.733]
Epoch [11/120    avg_loss:0.386, val_acc:0.810]
Epoch [12/120    avg_loss:0.366, val_acc:0.781]
Epoch [13/120    avg_loss:0.346, val_acc:0.810]
Epoch [14/120    avg_loss:0.304, val_acc:0.791]
Epoch [15/120    avg_loss:0.299, val_acc:0.839]
Epoch [16/120    avg_loss:0.256, val_acc:0.825]
Epoch [17/120    avg_loss:0.213, val_acc:0.891]
Epoch [18/120    avg_loss:0.208, val_acc:0.896]
Epoch [19/120    avg_loss:0.217, val_acc:0.923]
Epoch [20/120    avg_loss:0.185, val_acc:0.935]
Epoch [21/120    avg_loss:0.164, val_acc:0.936]
Epoch [22/120    avg_loss:0.138, val_acc:0.927]
Epoch [23/120    avg_loss:0.138, val_acc:0.948]
Epoch [24/120    avg_loss:0.106, val_acc:0.956]
Epoch [25/120    avg_loss:0.136, val_acc:0.920]
Epoch [26/120    avg_loss:0.219, val_acc:0.893]
Epoch [27/120    avg_loss:0.174, val_acc:0.933]
Epoch [28/120    avg_loss:0.154, val_acc:0.942]
Epoch [29/120    avg_loss:0.099, val_acc:0.962]
Epoch [30/120    avg_loss:0.076, val_acc:0.947]
Epoch [31/120    avg_loss:0.063, val_acc:0.962]
Epoch [32/120    avg_loss:0.074, val_acc:0.960]
Epoch [33/120    avg_loss:0.099, val_acc:0.941]
Epoch [34/120    avg_loss:0.085, val_acc:0.954]
Epoch [35/120    avg_loss:0.062, val_acc:0.964]
Epoch [36/120    avg_loss:0.054, val_acc:0.960]
Epoch [37/120    avg_loss:0.052, val_acc:0.974]
Epoch [38/120    avg_loss:0.035, val_acc:0.970]
Epoch [39/120    avg_loss:0.038, val_acc:0.957]
Epoch [40/120    avg_loss:0.052, val_acc:0.968]
Epoch [41/120    avg_loss:0.041, val_acc:0.976]
Epoch [42/120    avg_loss:0.043, val_acc:0.956]
Epoch [43/120    avg_loss:0.044, val_acc:0.964]
Epoch [44/120    avg_loss:0.027, val_acc:0.970]
Epoch [45/120    avg_loss:0.026, val_acc:0.970]
Epoch [46/120    avg_loss:0.027, val_acc:0.967]
Epoch [47/120    avg_loss:0.019, val_acc:0.972]
Epoch [48/120    avg_loss:0.017, val_acc:0.968]
Epoch [49/120    avg_loss:0.032, val_acc:0.973]
Epoch [50/120    avg_loss:0.041, val_acc:0.961]
Epoch [51/120    avg_loss:0.083, val_acc:0.969]
Epoch [52/120    avg_loss:0.034, val_acc:0.975]
Epoch [53/120    avg_loss:0.027, val_acc:0.972]
Epoch [54/120    avg_loss:0.020, val_acc:0.973]
Epoch [55/120    avg_loss:0.014, val_acc:0.974]
Epoch [56/120    avg_loss:0.013, val_acc:0.976]
Epoch [57/120    avg_loss:0.017, val_acc:0.974]
Epoch [58/120    avg_loss:0.014, val_acc:0.975]
Epoch [59/120    avg_loss:0.014, val_acc:0.975]
Epoch [60/120    avg_loss:0.014, val_acc:0.977]
Epoch [61/120    avg_loss:0.013, val_acc:0.976]
Epoch [62/120    avg_loss:0.011, val_acc:0.975]
Epoch [63/120    avg_loss:0.011, val_acc:0.975]
Epoch [64/120    avg_loss:0.011, val_acc:0.975]
Epoch [65/120    avg_loss:0.013, val_acc:0.975]
Epoch [66/120    avg_loss:0.012, val_acc:0.976]
Epoch [67/120    avg_loss:0.012, val_acc:0.975]
Epoch [68/120    avg_loss:0.013, val_acc:0.975]
Epoch [69/120    avg_loss:0.013, val_acc:0.976]
Epoch [70/120    avg_loss:0.010, val_acc:0.975]
Epoch [71/120    avg_loss:0.010, val_acc:0.976]
Epoch [72/120    avg_loss:0.011, val_acc:0.976]
Epoch [73/120    avg_loss:0.011, val_acc:0.976]
Epoch [74/120    avg_loss:0.010, val_acc:0.976]
Epoch [75/120    avg_loss:0.011, val_acc:0.976]
Epoch [76/120    avg_loss:0.012, val_acc:0.977]
Epoch [77/120    avg_loss:0.013, val_acc:0.976]
Epoch [78/120    avg_loss:0.013, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.011, val_acc:0.977]
Epoch [81/120    avg_loss:0.011, val_acc:0.976]
Epoch [82/120    avg_loss:0.012, val_acc:0.976]
Epoch [83/120    avg_loss:0.012, val_acc:0.977]
Epoch [84/120    avg_loss:0.012, val_acc:0.977]
Epoch [85/120    avg_loss:0.010, val_acc:0.978]
Epoch [86/120    avg_loss:0.011, val_acc:0.976]
Epoch [87/120    avg_loss:0.011, val_acc:0.976]
Epoch [88/120    avg_loss:0.011, val_acc:0.976]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.011, val_acc:0.976]
Epoch [91/120    avg_loss:0.010, val_acc:0.976]
Epoch [92/120    avg_loss:0.012, val_acc:0.976]
Epoch [93/120    avg_loss:0.011, val_acc:0.977]
Epoch [94/120    avg_loss:0.011, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.977]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.012, val_acc:0.977]
Epoch [98/120    avg_loss:0.010, val_acc:0.977]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.011, val_acc:0.977]
Epoch [101/120    avg_loss:0.010, val_acc:0.977]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.977]
Epoch [104/120    avg_loss:0.010, val_acc:0.977]
Epoch [105/120    avg_loss:0.009, val_acc:0.977]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.977]
Epoch [108/120    avg_loss:0.011, val_acc:0.977]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.011, val_acc:0.977]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.011, val_acc:0.977]
Epoch [113/120    avg_loss:0.011, val_acc:0.977]
Epoch [114/120    avg_loss:0.013, val_acc:0.977]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.008, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.011, val_acc:0.977]
Epoch [119/120    avg_loss:0.011, val_acc:0.977]
Epoch [120/120    avg_loss:0.009, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6298     0    10     0     0    30    25    68     1]
 [    0     0 18054     0    18     0     2     0    16     0]
 [    0     0     0  1981     0     0     0     0    50     5]
 [    0    43     8     0  2911     0     1     0     8     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    34     4     0     0  4830     0    10     0]
 [    0     8     0     0     0     0     1  1276     0     5]
 [    0     2     0    42    58     0     0     0  3467     2]
 [    0     0     0     0     8    20     0     0     0   891]]

Accuracy:
98.84317836743547

F1 scores:
[       nan 0.9853712  0.99784447 0.97274736 0.97569968 0.99239544
 0.99158284 0.9849479  0.96439499 0.97697368]

Kappa:
0.9846739095902102
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f631a5ca8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.002, val_acc:0.169]
Epoch [2/120    avg_loss:1.518, val_acc:0.253]
Epoch [3/120    avg_loss:1.249, val_acc:0.385]
Epoch [4/120    avg_loss:1.065, val_acc:0.467]
Epoch [5/120    avg_loss:0.902, val_acc:0.490]
Epoch [6/120    avg_loss:0.719, val_acc:0.808]
Epoch [7/120    avg_loss:0.626, val_acc:0.842]
Epoch [8/120    avg_loss:0.536, val_acc:0.840]
Epoch [9/120    avg_loss:0.473, val_acc:0.806]
Epoch [10/120    avg_loss:0.434, val_acc:0.799]
Epoch [11/120    avg_loss:0.388, val_acc:0.835]
Epoch [12/120    avg_loss:0.329, val_acc:0.817]
Epoch [13/120    avg_loss:0.306, val_acc:0.831]
Epoch [14/120    avg_loss:0.274, val_acc:0.839]
Epoch [15/120    avg_loss:0.247, val_acc:0.812]
Epoch [16/120    avg_loss:0.266, val_acc:0.823]
Epoch [17/120    avg_loss:0.223, val_acc:0.840]
Epoch [18/120    avg_loss:0.226, val_acc:0.791]
Epoch [19/120    avg_loss:0.201, val_acc:0.925]
Epoch [20/120    avg_loss:0.144, val_acc:0.874]
Epoch [21/120    avg_loss:0.115, val_acc:0.937]
Epoch [22/120    avg_loss:0.115, val_acc:0.934]
Epoch [23/120    avg_loss:0.110, val_acc:0.939]
Epoch [24/120    avg_loss:0.127, val_acc:0.942]
Epoch [25/120    avg_loss:0.096, val_acc:0.955]
Epoch [26/120    avg_loss:0.083, val_acc:0.906]
Epoch [27/120    avg_loss:0.097, val_acc:0.919]
Epoch [28/120    avg_loss:0.075, val_acc:0.942]
Epoch [29/120    avg_loss:0.076, val_acc:0.947]
Epoch [30/120    avg_loss:0.080, val_acc:0.928]
Epoch [31/120    avg_loss:0.058, val_acc:0.956]
Epoch [32/120    avg_loss:0.071, val_acc:0.947]
Epoch [33/120    avg_loss:0.069, val_acc:0.955]
Epoch [34/120    avg_loss:0.050, val_acc:0.945]
Epoch [35/120    avg_loss:0.083, val_acc:0.956]
Epoch [36/120    avg_loss:0.090, val_acc:0.958]
Epoch [37/120    avg_loss:0.070, val_acc:0.951]
Epoch [38/120    avg_loss:0.054, val_acc:0.956]
Epoch [39/120    avg_loss:0.054, val_acc:0.944]
Epoch [40/120    avg_loss:0.038, val_acc:0.959]
Epoch [41/120    avg_loss:0.055, val_acc:0.963]
Epoch [42/120    avg_loss:0.053, val_acc:0.961]
Epoch [43/120    avg_loss:0.037, val_acc:0.961]
Epoch [44/120    avg_loss:0.028, val_acc:0.975]
Epoch [45/120    avg_loss:0.031, val_acc:0.951]
Epoch [46/120    avg_loss:0.045, val_acc:0.962]
Epoch [47/120    avg_loss:0.038, val_acc:0.970]
Epoch [48/120    avg_loss:0.020, val_acc:0.975]
Epoch [49/120    avg_loss:0.026, val_acc:0.964]
Epoch [50/120    avg_loss:0.022, val_acc:0.975]
Epoch [51/120    avg_loss:0.041, val_acc:0.970]
Epoch [52/120    avg_loss:0.022, val_acc:0.971]
Epoch [53/120    avg_loss:0.022, val_acc:0.975]
Epoch [54/120    avg_loss:0.028, val_acc:0.951]
Epoch [55/120    avg_loss:0.020, val_acc:0.974]
Epoch [56/120    avg_loss:0.014, val_acc:0.975]
Epoch [57/120    avg_loss:0.021, val_acc:0.964]
Epoch [58/120    avg_loss:0.013, val_acc:0.977]
Epoch [59/120    avg_loss:0.013, val_acc:0.978]
Epoch [60/120    avg_loss:0.018, val_acc:0.975]
Epoch [61/120    avg_loss:0.021, val_acc:0.971]
Epoch [62/120    avg_loss:0.037, val_acc:0.962]
Epoch [63/120    avg_loss:0.047, val_acc:0.973]
Epoch [64/120    avg_loss:0.041, val_acc:0.961]
Epoch [65/120    avg_loss:0.032, val_acc:0.975]
Epoch [66/120    avg_loss:0.018, val_acc:0.969]
Epoch [67/120    avg_loss:0.019, val_acc:0.975]
Epoch [68/120    avg_loss:0.020, val_acc:0.974]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.019, val_acc:0.972]
Epoch [71/120    avg_loss:0.019, val_acc:0.975]
Epoch [72/120    avg_loss:0.015, val_acc:0.982]
Epoch [73/120    avg_loss:0.035, val_acc:0.959]
Epoch [74/120    avg_loss:0.038, val_acc:0.970]
Epoch [75/120    avg_loss:0.019, val_acc:0.979]
Epoch [76/120    avg_loss:0.024, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.979]
Epoch [78/120    avg_loss:0.014, val_acc:0.976]
Epoch [79/120    avg_loss:0.012, val_acc:0.973]
Epoch [80/120    avg_loss:0.026, val_acc:0.981]
Epoch [81/120    avg_loss:0.016, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.981]
Epoch [86/120    avg_loss:0.007, val_acc:0.979]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.978]
Epoch [90/120    avg_loss:0.024, val_acc:0.965]
Epoch [91/120    avg_loss:0.044, val_acc:0.968]
Epoch [92/120    avg_loss:0.022, val_acc:0.960]
Epoch [93/120    avg_loss:0.018, val_acc:0.980]
Epoch [94/120    avg_loss:0.015, val_acc:0.976]
Epoch [95/120    avg_loss:0.020, val_acc:0.979]
Epoch [96/120    avg_loss:0.045, val_acc:0.971]
Epoch [97/120    avg_loss:0.024, val_acc:0.977]
Epoch [98/120    avg_loss:0.020, val_acc:0.971]
Epoch [99/120    avg_loss:0.016, val_acc:0.976]
Epoch [100/120    avg_loss:0.014, val_acc:0.976]
Epoch [101/120    avg_loss:0.008, val_acc:0.978]
Epoch [102/120    avg_loss:0.006, val_acc:0.976]
Epoch [103/120    avg_loss:0.008, val_acc:0.977]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.008, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.005, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.005, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.979]
Epoch [114/120    avg_loss:0.006, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.006, val_acc:0.979]
Epoch [118/120    avg_loss:0.005, val_acc:0.980]
Epoch [119/120    avg_loss:0.005, val_acc:0.980]
Epoch [120/120    avg_loss:0.005, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6338     0     0     0     0     0    18    76     0]
 [    0     1 17937     0    82     0    57     0    12     1]
 [    0     1     0  1927     0     0     0     0   108     0]
 [    0    27     0     7  2934     0     1     0     3     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4866     0     6     0]
 [    0     1     0     0     0     0     1  1283     0     5]
 [    0    40     0    80    53     0     0     0  3396     2]
 [    0     0     0     0    14    30     0     0     0   875]]

Accuracy:
98.47685151712338

F1 scores:
[       nan 0.98722741 0.99558738 0.95160494 0.96911643 0.98863636
 0.99275732 0.99035122 0.94701617 0.97114317]

Kappa:
0.9798510422106284
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a131c6898>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.933, val_acc:0.137]
Epoch [2/120    avg_loss:1.555, val_acc:0.277]
Epoch [3/120    avg_loss:1.340, val_acc:0.410]
Epoch [4/120    avg_loss:1.129, val_acc:0.510]
Epoch [5/120    avg_loss:0.996, val_acc:0.497]
Epoch [6/120    avg_loss:0.861, val_acc:0.665]
Epoch [7/120    avg_loss:0.704, val_acc:0.671]
Epoch [8/120    avg_loss:0.608, val_acc:0.705]
Epoch [9/120    avg_loss:0.529, val_acc:0.730]
Epoch [10/120    avg_loss:0.423, val_acc:0.830]
Epoch [11/120    avg_loss:0.337, val_acc:0.790]
Epoch [12/120    avg_loss:0.307, val_acc:0.837]
Epoch [13/120    avg_loss:0.252, val_acc:0.908]
Epoch [14/120    avg_loss:0.226, val_acc:0.904]
Epoch [15/120    avg_loss:0.217, val_acc:0.896]
Epoch [16/120    avg_loss:0.188, val_acc:0.948]
Epoch [17/120    avg_loss:0.140, val_acc:0.946]
Epoch [18/120    avg_loss:0.139, val_acc:0.933]
Epoch [19/120    avg_loss:0.157, val_acc:0.878]
Epoch [20/120    avg_loss:0.133, val_acc:0.874]
Epoch [21/120    avg_loss:0.131, val_acc:0.959]
Epoch [22/120    avg_loss:0.108, val_acc:0.919]
Epoch [23/120    avg_loss:0.096, val_acc:0.960]
Epoch [24/120    avg_loss:0.098, val_acc:0.966]
Epoch [25/120    avg_loss:0.064, val_acc:0.981]
Epoch [26/120    avg_loss:0.052, val_acc:0.971]
Epoch [27/120    avg_loss:0.069, val_acc:0.971]
Epoch [28/120    avg_loss:0.076, val_acc:0.970]
Epoch [29/120    avg_loss:0.047, val_acc:0.970]
Epoch [30/120    avg_loss:0.062, val_acc:0.970]
Epoch [31/120    avg_loss:0.087, val_acc:0.965]
Epoch [32/120    avg_loss:0.090, val_acc:0.961]
Epoch [33/120    avg_loss:0.058, val_acc:0.979]
Epoch [34/120    avg_loss:0.052, val_acc:0.970]
Epoch [35/120    avg_loss:0.035, val_acc:0.965]
Epoch [36/120    avg_loss:0.031, val_acc:0.980]
Epoch [37/120    avg_loss:0.024, val_acc:0.982]
Epoch [38/120    avg_loss:0.024, val_acc:0.981]
Epoch [39/120    avg_loss:0.022, val_acc:0.984]
Epoch [40/120    avg_loss:0.024, val_acc:0.978]
Epoch [41/120    avg_loss:0.030, val_acc:0.986]
Epoch [42/120    avg_loss:0.039, val_acc:0.984]
Epoch [43/120    avg_loss:0.033, val_acc:0.938]
Epoch [44/120    avg_loss:0.044, val_acc:0.971]
Epoch [45/120    avg_loss:0.050, val_acc:0.963]
Epoch [46/120    avg_loss:0.040, val_acc:0.985]
Epoch [47/120    avg_loss:0.034, val_acc:0.978]
Epoch [48/120    avg_loss:0.030, val_acc:0.974]
Epoch [49/120    avg_loss:0.029, val_acc:0.989]
Epoch [50/120    avg_loss:0.036, val_acc:0.985]
Epoch [51/120    avg_loss:0.037, val_acc:0.979]
Epoch [52/120    avg_loss:0.045, val_acc:0.962]
Epoch [53/120    avg_loss:0.038, val_acc:0.964]
Epoch [54/120    avg_loss:0.024, val_acc:0.982]
Epoch [55/120    avg_loss:0.021, val_acc:0.948]
Epoch [56/120    avg_loss:0.030, val_acc:0.986]
Epoch [57/120    avg_loss:0.013, val_acc:0.981]
Epoch [58/120    avg_loss:0.018, val_acc:0.982]
Epoch [59/120    avg_loss:0.022, val_acc:0.988]
Epoch [60/120    avg_loss:0.012, val_acc:0.987]
Epoch [61/120    avg_loss:0.014, val_acc:0.989]
Epoch [62/120    avg_loss:0.014, val_acc:0.988]
Epoch [63/120    avg_loss:0.009, val_acc:0.986]
Epoch [64/120    avg_loss:0.010, val_acc:0.982]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.988]
Epoch [67/120    avg_loss:0.008, val_acc:0.986]
Epoch [68/120    avg_loss:0.006, val_acc:0.991]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.974]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.016, val_acc:0.981]
Epoch [73/120    avg_loss:0.017, val_acc:0.991]
Epoch [74/120    avg_loss:0.009, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.986]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.005, val_acc:0.989]
Epoch [78/120    avg_loss:0.009, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.008, val_acc:0.990]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.992]
Epoch [85/120    avg_loss:0.005, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.003, val_acc:0.990]
Epoch [89/120    avg_loss:0.004, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.991]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.988]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.991]
Epoch [96/120    avg_loss:0.004, val_acc:0.991]
Epoch [97/120    avg_loss:0.004, val_acc:0.991]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.003, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.989]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.003, val_acc:0.991]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.003, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6310     0     8     1     0    27     0    85     1]
 [    0     0 18073     0     8     0     0     0     9     0]
 [    0     1     0  2021     0     0     0     0     9     5]
 [    0    27     8     1  2917     0     2     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    48     0     0     0  4830     0     0     0]
 [    0     8     0     0     0     0     0  1275     3     4]
 [    0     5     0    72    55     0     0     0  3423    16]
 [    0     0     0     0     2    15     0     0     0   902]]

Accuracy:
98.94681030535271

F1 scores:
[       nan 0.98724869 0.99798448 0.97680039 0.97968094 0.99428571
 0.99209202 0.99415205 0.96192216 0.976719  ]

Kappa:
0.9860426869449844
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e6b9ab8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.956, val_acc:0.133]
Epoch [2/120    avg_loss:1.556, val_acc:0.537]
Epoch [3/120    avg_loss:1.338, val_acc:0.646]
Epoch [4/120    avg_loss:1.171, val_acc:0.693]
Epoch [5/120    avg_loss:1.036, val_acc:0.741]
Epoch [6/120    avg_loss:0.917, val_acc:0.776]
Epoch [7/120    avg_loss:0.742, val_acc:0.659]
Epoch [8/120    avg_loss:0.614, val_acc:0.683]
Epoch [9/120    avg_loss:0.503, val_acc:0.711]
Epoch [10/120    avg_loss:0.410, val_acc:0.840]
Epoch [11/120    avg_loss:0.378, val_acc:0.830]
Epoch [12/120    avg_loss:0.307, val_acc:0.817]
Epoch [13/120    avg_loss:0.247, val_acc:0.906]
Epoch [14/120    avg_loss:0.261, val_acc:0.892]
Epoch [15/120    avg_loss:0.206, val_acc:0.942]
Epoch [16/120    avg_loss:0.187, val_acc:0.907]
Epoch [17/120    avg_loss:0.160, val_acc:0.946]
Epoch [18/120    avg_loss:0.165, val_acc:0.949]
Epoch [19/120    avg_loss:0.162, val_acc:0.938]
Epoch [20/120    avg_loss:0.134, val_acc:0.959]
Epoch [21/120    avg_loss:0.115, val_acc:0.943]
Epoch [22/120    avg_loss:0.100, val_acc:0.956]
Epoch [23/120    avg_loss:0.099, val_acc:0.945]
Epoch [24/120    avg_loss:0.106, val_acc:0.965]
Epoch [25/120    avg_loss:0.073, val_acc:0.927]
Epoch [26/120    avg_loss:0.092, val_acc:0.970]
Epoch [27/120    avg_loss:0.074, val_acc:0.921]
Epoch [28/120    avg_loss:0.086, val_acc:0.949]
Epoch [29/120    avg_loss:0.083, val_acc:0.975]
Epoch [30/120    avg_loss:0.048, val_acc:0.975]
Epoch [31/120    avg_loss:0.051, val_acc:0.977]
Epoch [32/120    avg_loss:0.035, val_acc:0.985]
Epoch [33/120    avg_loss:0.052, val_acc:0.954]
Epoch [34/120    avg_loss:0.077, val_acc:0.972]
Epoch [35/120    avg_loss:0.049, val_acc:0.959]
Epoch [36/120    avg_loss:0.035, val_acc:0.970]
Epoch [37/120    avg_loss:0.039, val_acc:0.983]
Epoch [38/120    avg_loss:0.039, val_acc:0.979]
Epoch [39/120    avg_loss:0.037, val_acc:0.969]
Epoch [40/120    avg_loss:0.045, val_acc:0.960]
Epoch [41/120    avg_loss:0.036, val_acc:0.963]
Epoch [42/120    avg_loss:0.044, val_acc:0.971]
Epoch [43/120    avg_loss:0.056, val_acc:0.981]
Epoch [44/120    avg_loss:0.026, val_acc:0.986]
Epoch [45/120    avg_loss:0.020, val_acc:0.987]
Epoch [46/120    avg_loss:0.018, val_acc:0.987]
Epoch [47/120    avg_loss:0.016, val_acc:0.976]
Epoch [48/120    avg_loss:0.024, val_acc:0.988]
Epoch [49/120    avg_loss:0.016, val_acc:0.984]
Epoch [50/120    avg_loss:0.014, val_acc:0.986]
Epoch [51/120    avg_loss:0.015, val_acc:0.982]
Epoch [52/120    avg_loss:0.020, val_acc:0.989]
Epoch [53/120    avg_loss:0.016, val_acc:0.986]
Epoch [54/120    avg_loss:0.012, val_acc:0.992]
Epoch [55/120    avg_loss:0.019, val_acc:0.982]
Epoch [56/120    avg_loss:0.022, val_acc:0.988]
Epoch [57/120    avg_loss:0.019, val_acc:0.984]
Epoch [58/120    avg_loss:0.018, val_acc:0.983]
Epoch [59/120    avg_loss:0.014, val_acc:0.985]
Epoch [60/120    avg_loss:0.018, val_acc:0.990]
Epoch [61/120    avg_loss:0.015, val_acc:0.989]
Epoch [62/120    avg_loss:0.010, val_acc:0.992]
Epoch [63/120    avg_loss:0.011, val_acc:0.988]
Epoch [64/120    avg_loss:0.011, val_acc:0.979]
Epoch [65/120    avg_loss:0.017, val_acc:0.990]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.011, val_acc:0.989]
Epoch [68/120    avg_loss:0.007, val_acc:0.992]
Epoch [69/120    avg_loss:0.009, val_acc:0.992]
Epoch [70/120    avg_loss:0.022, val_acc:0.988]
Epoch [71/120    avg_loss:0.020, val_acc:0.984]
Epoch [72/120    avg_loss:0.012, val_acc:0.983]
Epoch [73/120    avg_loss:0.014, val_acc:0.979]
Epoch [74/120    avg_loss:0.030, val_acc:0.974]
Epoch [75/120    avg_loss:0.012, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.990]
Epoch [77/120    avg_loss:0.008, val_acc:0.990]
Epoch [78/120    avg_loss:0.009, val_acc:0.992]
Epoch [79/120    avg_loss:0.007, val_acc:0.992]
Epoch [80/120    avg_loss:0.007, val_acc:0.992]
Epoch [81/120    avg_loss:0.009, val_acc:0.993]
Epoch [82/120    avg_loss:0.006, val_acc:0.993]
Epoch [83/120    avg_loss:0.005, val_acc:0.992]
Epoch [84/120    avg_loss:0.006, val_acc:0.992]
Epoch [85/120    avg_loss:0.007, val_acc:0.992]
Epoch [86/120    avg_loss:0.006, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.992]
Epoch [88/120    avg_loss:0.007, val_acc:0.993]
Epoch [89/120    avg_loss:0.008, val_acc:0.992]
Epoch [90/120    avg_loss:0.007, val_acc:0.992]
Epoch [91/120    avg_loss:0.005, val_acc:0.992]
Epoch [92/120    avg_loss:0.004, val_acc:0.993]
Epoch [93/120    avg_loss:0.007, val_acc:0.994]
Epoch [94/120    avg_loss:0.006, val_acc:0.993]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.993]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.005, val_acc:0.994]
Epoch [99/120    avg_loss:0.008, val_acc:0.993]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.993]
Epoch [102/120    avg_loss:0.008, val_acc:0.993]
Epoch [103/120    avg_loss:0.007, val_acc:0.993]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.992]
Epoch [106/120    avg_loss:0.008, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.994]
Epoch [108/120    avg_loss:0.005, val_acc:0.994]
Epoch [109/120    avg_loss:0.006, val_acc:0.993]
Epoch [110/120    avg_loss:0.006, val_acc:0.993]
Epoch [111/120    avg_loss:0.005, val_acc:0.994]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.006, val_acc:0.993]
Epoch [116/120    avg_loss:0.004, val_acc:0.993]
Epoch [117/120    avg_loss:0.004, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.004, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6402     0     0     1     0     0     7    21     1]
 [    0     0 18077     0     9     0     1     0     3     0]
 [    0     0     0  1984     0     0     0     0    50     2]
 [    0    19     1     1  2928     0     6     0    16     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     1     0     0  4866     0     2     0]
 [    0     8     0     0     0     0     0  1278     0     4]
 [    0     0     0    26    47     0     0     0  3481    17]
 [    0     0     0     0     0    15     0     0     0   904]]

Accuracy:
99.35410792181814

F1 scores:
[       nan 0.995568   0.99936424 0.98023715 0.98304516 0.99428571
 0.99805148 0.99262136 0.97452408 0.97835498]

Kappa:
0.9914420583545357
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd2f995a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.001, val_acc:0.526]
Epoch [2/120    avg_loss:1.618, val_acc:0.560]
Epoch [3/120    avg_loss:1.327, val_acc:0.583]
Epoch [4/120    avg_loss:1.098, val_acc:0.653]
Epoch [5/120    avg_loss:0.931, val_acc:0.665]
Epoch [6/120    avg_loss:0.787, val_acc:0.638]
Epoch [7/120    avg_loss:0.681, val_acc:0.671]
Epoch [8/120    avg_loss:0.600, val_acc:0.693]
Epoch [9/120    avg_loss:0.505, val_acc:0.739]
Epoch [10/120    avg_loss:0.426, val_acc:0.774]
Epoch [11/120    avg_loss:0.433, val_acc:0.774]
Epoch [12/120    avg_loss:0.384, val_acc:0.761]
Epoch [13/120    avg_loss:0.306, val_acc:0.864]
Epoch [14/120    avg_loss:0.259, val_acc:0.907]
Epoch [15/120    avg_loss:0.230, val_acc:0.904]
Epoch [16/120    avg_loss:0.217, val_acc:0.903]
Epoch [17/120    avg_loss:0.212, val_acc:0.929]
Epoch [18/120    avg_loss:0.163, val_acc:0.900]
Epoch [19/120    avg_loss:0.164, val_acc:0.824]
Epoch [20/120    avg_loss:0.148, val_acc:0.945]
Epoch [21/120    avg_loss:0.142, val_acc:0.919]
Epoch [22/120    avg_loss:0.125, val_acc:0.948]
Epoch [23/120    avg_loss:0.114, val_acc:0.939]
Epoch [24/120    avg_loss:0.105, val_acc:0.949]
Epoch [25/120    avg_loss:0.128, val_acc:0.925]
Epoch [26/120    avg_loss:0.130, val_acc:0.937]
Epoch [27/120    avg_loss:0.097, val_acc:0.952]
Epoch [28/120    avg_loss:0.131, val_acc:0.939]
Epoch [29/120    avg_loss:0.092, val_acc:0.928]
Epoch [30/120    avg_loss:0.107, val_acc:0.966]
Epoch [31/120    avg_loss:0.073, val_acc:0.958]
Epoch [32/120    avg_loss:0.060, val_acc:0.965]
Epoch [33/120    avg_loss:0.084, val_acc:0.972]
Epoch [34/120    avg_loss:0.057, val_acc:0.966]
Epoch [35/120    avg_loss:0.039, val_acc:0.976]
Epoch [36/120    avg_loss:0.057, val_acc:0.974]
Epoch [37/120    avg_loss:0.038, val_acc:0.973]
Epoch [38/120    avg_loss:0.043, val_acc:0.965]
Epoch [39/120    avg_loss:0.083, val_acc:0.970]
Epoch [40/120    avg_loss:0.051, val_acc:0.972]
Epoch [41/120    avg_loss:0.070, val_acc:0.946]
Epoch [42/120    avg_loss:0.060, val_acc:0.973]
Epoch [43/120    avg_loss:0.047, val_acc:0.980]
Epoch [44/120    avg_loss:0.041, val_acc:0.980]
Epoch [45/120    avg_loss:0.036, val_acc:0.976]
Epoch [46/120    avg_loss:0.037, val_acc:0.977]
Epoch [47/120    avg_loss:0.047, val_acc:0.975]
Epoch [48/120    avg_loss:0.025, val_acc:0.980]
Epoch [49/120    avg_loss:0.025, val_acc:0.976]
Epoch [50/120    avg_loss:0.025, val_acc:0.980]
Epoch [51/120    avg_loss:0.028, val_acc:0.962]
Epoch [52/120    avg_loss:0.021, val_acc:0.976]
Epoch [53/120    avg_loss:0.012, val_acc:0.979]
Epoch [54/120    avg_loss:0.016, val_acc:0.976]
Epoch [55/120    avg_loss:0.013, val_acc:0.976]
Epoch [56/120    avg_loss:0.015, val_acc:0.982]
Epoch [57/120    avg_loss:0.014, val_acc:0.978]
Epoch [58/120    avg_loss:0.017, val_acc:0.971]
Epoch [59/120    avg_loss:0.023, val_acc:0.981]
Epoch [60/120    avg_loss:0.015, val_acc:0.978]
Epoch [61/120    avg_loss:0.027, val_acc:0.976]
Epoch [62/120    avg_loss:0.025, val_acc:0.983]
Epoch [63/120    avg_loss:0.034, val_acc:0.970]
Epoch [64/120    avg_loss:0.037, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.982]
Epoch [66/120    avg_loss:0.022, val_acc:0.983]
Epoch [67/120    avg_loss:0.018, val_acc:0.979]
Epoch [68/120    avg_loss:0.079, val_acc:0.954]
Epoch [69/120    avg_loss:0.027, val_acc:0.973]
Epoch [70/120    avg_loss:0.023, val_acc:0.975]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.012, val_acc:0.988]
Epoch [73/120    avg_loss:0.020, val_acc:0.975]
Epoch [74/120    avg_loss:0.024, val_acc:0.978]
Epoch [75/120    avg_loss:0.048, val_acc:0.966]
Epoch [76/120    avg_loss:0.027, val_acc:0.975]
Epoch [77/120    avg_loss:0.020, val_acc:0.979]
Epoch [78/120    avg_loss:0.028, val_acc:0.974]
Epoch [79/120    avg_loss:0.025, val_acc:0.981]
Epoch [80/120    avg_loss:0.014, val_acc:0.981]
Epoch [81/120    avg_loss:0.025, val_acc:0.902]
Epoch [82/120    avg_loss:0.045, val_acc:0.980]
Epoch [83/120    avg_loss:0.014, val_acc:0.981]
Epoch [84/120    avg_loss:0.018, val_acc:0.985]
Epoch [85/120    avg_loss:0.011, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.017, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.988]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6371     0     0     0     0     0     0    59     2]
 [    0     3 18059     0    17     0     9     0     2     0]
 [    0     2     0  1908     0     0     0     0   121     5]
 [    0    19     5     0  2929     0     8     0    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4868     0     3     0]
 [    0    12     0     0     0     0     0  1278     0     0]
 [    0    50     0    21    48     0     0     0  3452     0]
 [    0     0     0     0     3     3     0     0     0   913]]

Accuracy:
99.01188152218447

F1 scores:
[       nan 0.98859493 0.99881087 0.96242119 0.98140392 0.99885189
 0.99723446 0.9953271  0.95636515 0.99293094]

Kappa:
0.9869071154034426
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd90f567908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.035, val_acc:0.358]
Epoch [2/120    avg_loss:1.590, val_acc:0.269]
Epoch [3/120    avg_loss:1.379, val_acc:0.378]
Epoch [4/120    avg_loss:1.177, val_acc:0.383]
Epoch [5/120    avg_loss:1.003, val_acc:0.453]
Epoch [6/120    avg_loss:0.894, val_acc:0.571]
Epoch [7/120    avg_loss:0.761, val_acc:0.588]
Epoch [8/120    avg_loss:0.666, val_acc:0.590]
Epoch [9/120    avg_loss:0.571, val_acc:0.648]
Epoch [10/120    avg_loss:0.485, val_acc:0.676]
Epoch [11/120    avg_loss:0.435, val_acc:0.712]
Epoch [12/120    avg_loss:0.366, val_acc:0.771]
Epoch [13/120    avg_loss:0.350, val_acc:0.782]
Epoch [14/120    avg_loss:0.320, val_acc:0.831]
Epoch [15/120    avg_loss:0.274, val_acc:0.770]
Epoch [16/120    avg_loss:0.260, val_acc:0.875]
Epoch [17/120    avg_loss:0.201, val_acc:0.868]
Epoch [18/120    avg_loss:0.206, val_acc:0.872]
Epoch [19/120    avg_loss:0.180, val_acc:0.934]
Epoch [20/120    avg_loss:0.162, val_acc:0.958]
Epoch [21/120    avg_loss:0.127, val_acc:0.883]
Epoch [22/120    avg_loss:0.131, val_acc:0.944]
Epoch [23/120    avg_loss:0.148, val_acc:0.933]
Epoch [24/120    avg_loss:0.113, val_acc:0.963]
Epoch [25/120    avg_loss:0.104, val_acc:0.959]
Epoch [26/120    avg_loss:0.095, val_acc:0.926]
Epoch [27/120    avg_loss:0.099, val_acc:0.958]
Epoch [28/120    avg_loss:0.086, val_acc:0.970]
Epoch [29/120    avg_loss:0.087, val_acc:0.951]
Epoch [30/120    avg_loss:0.096, val_acc:0.940]
Epoch [31/120    avg_loss:0.095, val_acc:0.965]
Epoch [32/120    avg_loss:0.077, val_acc:0.959]
Epoch [33/120    avg_loss:0.108, val_acc:0.951]
Epoch [34/120    avg_loss:0.060, val_acc:0.899]
Epoch [35/120    avg_loss:0.076, val_acc:0.971]
Epoch [36/120    avg_loss:0.041, val_acc:0.976]
Epoch [37/120    avg_loss:0.054, val_acc:0.969]
Epoch [38/120    avg_loss:0.050, val_acc:0.974]
Epoch [39/120    avg_loss:0.039, val_acc:0.926]
Epoch [40/120    avg_loss:0.046, val_acc:0.983]
Epoch [41/120    avg_loss:0.030, val_acc:0.986]
Epoch [42/120    avg_loss:0.044, val_acc:0.976]
Epoch [43/120    avg_loss:0.037, val_acc:0.976]
Epoch [44/120    avg_loss:0.033, val_acc:0.975]
Epoch [45/120    avg_loss:0.031, val_acc:0.984]
Epoch [46/120    avg_loss:0.024, val_acc:0.976]
Epoch [47/120    avg_loss:0.018, val_acc:0.972]
Epoch [48/120    avg_loss:0.017, val_acc:0.982]
Epoch [49/120    avg_loss:0.022, val_acc:0.957]
Epoch [50/120    avg_loss:0.026, val_acc:0.970]
Epoch [51/120    avg_loss:0.025, val_acc:0.983]
Epoch [52/120    avg_loss:0.013, val_acc:0.983]
Epoch [53/120    avg_loss:0.016, val_acc:0.983]
Epoch [54/120    avg_loss:0.012, val_acc:0.984]
Epoch [55/120    avg_loss:0.011, val_acc:0.986]
Epoch [56/120    avg_loss:0.011, val_acc:0.987]
Epoch [57/120    avg_loss:0.010, val_acc:0.987]
Epoch [58/120    avg_loss:0.013, val_acc:0.987]
Epoch [59/120    avg_loss:0.009, val_acc:0.987]
Epoch [60/120    avg_loss:0.009, val_acc:0.987]
Epoch [61/120    avg_loss:0.010, val_acc:0.986]
Epoch [62/120    avg_loss:0.013, val_acc:0.986]
Epoch [63/120    avg_loss:0.009, val_acc:0.987]
Epoch [64/120    avg_loss:0.010, val_acc:0.986]
Epoch [65/120    avg_loss:0.009, val_acc:0.986]
Epoch [66/120    avg_loss:0.008, val_acc:0.987]
Epoch [67/120    avg_loss:0.009, val_acc:0.987]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.009, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.986]
Epoch [72/120    avg_loss:0.010, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.009, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.011, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6362     0     0     0     0     0    31    39     0]
 [    0     0 17941     0    53     0    78     0    16     2]
 [    0     8     0  1959     0     0     0     0    69     0]
 [    0    22    12     4  2915     0     4     0    14     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     2     0     0  4875     0     0     0]
 [    0     3     0     0     0     0     1  1285     0     1]
 [    0    17     0    17    36     0     0     0  3497     4]
 [    0     0     0     0    13    19     0     0     0   887]]

Accuracy:
98.87450895331743

F1 scores:
[       nan 0.99065712 0.99550549 0.975112   0.97345133 0.99277292
 0.99125661 0.98618573 0.97058007 0.97794928]

Kappa:
0.9851084547029695
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f445cc8c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.029, val_acc:0.212]
Epoch [2/120    avg_loss:1.553, val_acc:0.305]
Epoch [3/120    avg_loss:1.284, val_acc:0.626]
Epoch [4/120    avg_loss:1.103, val_acc:0.692]
Epoch [5/120    avg_loss:0.959, val_acc:0.584]
Epoch [6/120    avg_loss:0.791, val_acc:0.663]
Epoch [7/120    avg_loss:0.697, val_acc:0.682]
Epoch [8/120    avg_loss:0.593, val_acc:0.709]
Epoch [9/120    avg_loss:0.486, val_acc:0.745]
Epoch [10/120    avg_loss:0.451, val_acc:0.747]
Epoch [11/120    avg_loss:0.388, val_acc:0.821]
Epoch [12/120    avg_loss:0.384, val_acc:0.798]
Epoch [13/120    avg_loss:0.353, val_acc:0.854]
Epoch [14/120    avg_loss:0.297, val_acc:0.841]
Epoch [15/120    avg_loss:0.281, val_acc:0.757]
Epoch [16/120    avg_loss:0.282, val_acc:0.879]
Epoch [17/120    avg_loss:0.224, val_acc:0.919]
Epoch [18/120    avg_loss:0.186, val_acc:0.901]
Epoch [19/120    avg_loss:0.252, val_acc:0.911]
Epoch [20/120    avg_loss:0.202, val_acc:0.900]
Epoch [21/120    avg_loss:0.181, val_acc:0.940]
Epoch [22/120    avg_loss:0.150, val_acc:0.924]
Epoch [23/120    avg_loss:0.119, val_acc:0.900]
Epoch [24/120    avg_loss:0.134, val_acc:0.886]
Epoch [25/120    avg_loss:0.139, val_acc:0.919]
Epoch [26/120    avg_loss:0.139, val_acc:0.936]
Epoch [27/120    avg_loss:0.167, val_acc:0.912]
Epoch [28/120    avg_loss:0.115, val_acc:0.905]
Epoch [29/120    avg_loss:0.124, val_acc:0.901]
Epoch [30/120    avg_loss:0.109, val_acc:0.945]
Epoch [31/120    avg_loss:0.115, val_acc:0.905]
Epoch [32/120    avg_loss:0.089, val_acc:0.960]
Epoch [33/120    avg_loss:0.067, val_acc:0.959]
Epoch [34/120    avg_loss:0.060, val_acc:0.952]
Epoch [35/120    avg_loss:0.063, val_acc:0.959]
Epoch [36/120    avg_loss:0.052, val_acc:0.953]
Epoch [37/120    avg_loss:0.058, val_acc:0.954]
Epoch [38/120    avg_loss:0.044, val_acc:0.963]
Epoch [39/120    avg_loss:0.049, val_acc:0.970]
Epoch [40/120    avg_loss:0.034, val_acc:0.957]
Epoch [41/120    avg_loss:0.047, val_acc:0.969]
Epoch [42/120    avg_loss:0.042, val_acc:0.973]
Epoch [43/120    avg_loss:0.041, val_acc:0.957]
Epoch [44/120    avg_loss:0.025, val_acc:0.958]
Epoch [45/120    avg_loss:0.039, val_acc:0.947]
Epoch [46/120    avg_loss:0.038, val_acc:0.958]
Epoch [47/120    avg_loss:0.036, val_acc:0.961]
Epoch [48/120    avg_loss:0.024, val_acc:0.976]
Epoch [49/120    avg_loss:0.028, val_acc:0.970]
Epoch [50/120    avg_loss:0.067, val_acc:0.969]
Epoch [51/120    avg_loss:0.042, val_acc:0.965]
Epoch [52/120    avg_loss:0.046, val_acc:0.949]
Epoch [53/120    avg_loss:0.044, val_acc:0.967]
Epoch [54/120    avg_loss:0.027, val_acc:0.974]
Epoch [55/120    avg_loss:0.019, val_acc:0.971]
Epoch [56/120    avg_loss:0.031, val_acc:0.965]
Epoch [57/120    avg_loss:0.044, val_acc:0.963]
Epoch [58/120    avg_loss:0.025, val_acc:0.975]
Epoch [59/120    avg_loss:0.022, val_acc:0.971]
Epoch [60/120    avg_loss:0.053, val_acc:0.969]
Epoch [61/120    avg_loss:0.027, val_acc:0.979]
Epoch [62/120    avg_loss:0.020, val_acc:0.972]
Epoch [63/120    avg_loss:0.023, val_acc:0.976]
Epoch [64/120    avg_loss:0.021, val_acc:0.971]
Epoch [65/120    avg_loss:0.017, val_acc:0.981]
Epoch [66/120    avg_loss:0.018, val_acc:0.965]
Epoch [67/120    avg_loss:0.037, val_acc:0.975]
Epoch [68/120    avg_loss:0.015, val_acc:0.973]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.015, val_acc:0.978]
Epoch [71/120    avg_loss:0.016, val_acc:0.972]
Epoch [72/120    avg_loss:0.014, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.982]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.976]
Epoch [77/120    avg_loss:0.009, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.983]
Epoch [81/120    avg_loss:0.007, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.980]
Epoch [85/120    avg_loss:0.008, val_acc:0.982]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.981]
Epoch [98/120    avg_loss:0.004, val_acc:0.981]
Epoch [99/120    avg_loss:0.004, val_acc:0.982]
Epoch [100/120    avg_loss:0.005, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.981]
Epoch [102/120    avg_loss:0.005, val_acc:0.980]
Epoch [103/120    avg_loss:0.004, val_acc:0.983]
Epoch [104/120    avg_loss:0.004, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.982]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.982]
Epoch [111/120    avg_loss:0.005, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.003, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6381     0     0     0     0     0    24    20     7]
 [    0     0 18047     0    31     0     3     0     9     0]
 [    0     0     1  1952     0     0     0     0    80     3]
 [    0    14     8     0  2914     0    24     0    11     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    26     0     0     0  4852     0     0     0]
 [    0     5     0     0     0     0     0  1277     1     7]
 [    0    11     0    66    46     0     0     0  3442     6]
 [    0     0     0     0     5    12     0     0     0   902]]

Accuracy:
98.9853710264382

F1 scores:
[       nan 0.99369306 0.99784364 0.96299951 0.97654155 0.99542334
 0.994568   0.9857198  0.96495655 0.97777778]

Kappa:
0.9865579587460793
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f57da9d9908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.987, val_acc:0.084]
Epoch [2/120    avg_loss:1.589, val_acc:0.225]
Epoch [3/120    avg_loss:1.380, val_acc:0.443]
Epoch [4/120    avg_loss:1.188, val_acc:0.667]
Epoch [5/120    avg_loss:1.001, val_acc:0.726]
Epoch [6/120    avg_loss:0.896, val_acc:0.721]
Epoch [7/120    avg_loss:0.771, val_acc:0.736]
Epoch [8/120    avg_loss:0.641, val_acc:0.768]
Epoch [9/120    avg_loss:0.584, val_acc:0.749]
Epoch [10/120    avg_loss:0.482, val_acc:0.711]
Epoch [11/120    avg_loss:0.444, val_acc:0.789]
Epoch [12/120    avg_loss:0.389, val_acc:0.797]
Epoch [13/120    avg_loss:0.342, val_acc:0.796]
Epoch [14/120    avg_loss:0.319, val_acc:0.783]
Epoch [15/120    avg_loss:0.331, val_acc:0.834]
Epoch [16/120    avg_loss:0.274, val_acc:0.808]
Epoch [17/120    avg_loss:0.295, val_acc:0.813]
Epoch [18/120    avg_loss:0.252, val_acc:0.845]
Epoch [19/120    avg_loss:0.222, val_acc:0.809]
Epoch [20/120    avg_loss:0.203, val_acc:0.857]
Epoch [21/120    avg_loss:0.236, val_acc:0.925]
Epoch [22/120    avg_loss:0.189, val_acc:0.857]
Epoch [23/120    avg_loss:0.156, val_acc:0.884]
Epoch [24/120    avg_loss:0.145, val_acc:0.874]
Epoch [25/120    avg_loss:0.147, val_acc:0.898]
Epoch [26/120    avg_loss:0.151, val_acc:0.954]
Epoch [27/120    avg_loss:0.132, val_acc:0.957]
Epoch [28/120    avg_loss:0.098, val_acc:0.956]
Epoch [29/120    avg_loss:0.093, val_acc:0.969]
Epoch [30/120    avg_loss:0.084, val_acc:0.965]
Epoch [31/120    avg_loss:0.104, val_acc:0.946]
Epoch [32/120    avg_loss:0.065, val_acc:0.970]
Epoch [33/120    avg_loss:0.074, val_acc:0.975]
Epoch [34/120    avg_loss:0.067, val_acc:0.966]
Epoch [35/120    avg_loss:0.101, val_acc:0.959]
Epoch [36/120    avg_loss:0.083, val_acc:0.976]
Epoch [37/120    avg_loss:0.052, val_acc:0.976]
Epoch [38/120    avg_loss:0.064, val_acc:0.970]
Epoch [39/120    avg_loss:0.045, val_acc:0.980]
Epoch [40/120    avg_loss:0.033, val_acc:0.968]
Epoch [41/120    avg_loss:0.037, val_acc:0.978]
Epoch [42/120    avg_loss:0.075, val_acc:0.962]
Epoch [43/120    avg_loss:0.051, val_acc:0.973]
Epoch [44/120    avg_loss:0.034, val_acc:0.981]
Epoch [45/120    avg_loss:0.029, val_acc:0.986]
Epoch [46/120    avg_loss:0.028, val_acc:0.985]
Epoch [47/120    avg_loss:0.024, val_acc:0.983]
Epoch [48/120    avg_loss:0.022, val_acc:0.986]
Epoch [49/120    avg_loss:0.024, val_acc:0.981]
Epoch [50/120    avg_loss:0.028, val_acc:0.982]
Epoch [51/120    avg_loss:0.016, val_acc:0.983]
Epoch [52/120    avg_loss:0.019, val_acc:0.957]
Epoch [53/120    avg_loss:0.019, val_acc:0.988]
Epoch [54/120    avg_loss:0.013, val_acc:0.990]
Epoch [55/120    avg_loss:0.019, val_acc:0.987]
Epoch [56/120    avg_loss:0.021, val_acc:0.984]
Epoch [57/120    avg_loss:0.015, val_acc:0.987]
Epoch [58/120    avg_loss:0.016, val_acc:0.986]
Epoch [59/120    avg_loss:0.024, val_acc:0.960]
Epoch [60/120    avg_loss:0.052, val_acc:0.976]
Epoch [61/120    avg_loss:0.025, val_acc:0.981]
Epoch [62/120    avg_loss:0.017, val_acc:0.978]
Epoch [63/120    avg_loss:0.030, val_acc:0.981]
Epoch [64/120    avg_loss:0.026, val_acc:0.985]
Epoch [65/120    avg_loss:0.046, val_acc:0.981]
Epoch [66/120    avg_loss:0.037, val_acc:0.978]
Epoch [67/120    avg_loss:0.041, val_acc:0.984]
Epoch [68/120    avg_loss:0.021, val_acc:0.986]
Epoch [69/120    avg_loss:0.015, val_acc:0.987]
Epoch [70/120    avg_loss:0.019, val_acc:0.988]
Epoch [71/120    avg_loss:0.013, val_acc:0.987]
Epoch [72/120    avg_loss:0.012, val_acc:0.988]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.010, val_acc:0.989]
Epoch [75/120    avg_loss:0.013, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.989]
Epoch [77/120    avg_loss:0.012, val_acc:0.990]
Epoch [78/120    avg_loss:0.010, val_acc:0.990]
Epoch [79/120    avg_loss:0.011, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.989]
Epoch [81/120    avg_loss:0.010, val_acc:0.991]
Epoch [82/120    avg_loss:0.010, val_acc:0.989]
Epoch [83/120    avg_loss:0.010, val_acc:0.990]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.009, val_acc:0.990]
Epoch [87/120    avg_loss:0.009, val_acc:0.990]
Epoch [88/120    avg_loss:0.011, val_acc:0.991]
Epoch [89/120    avg_loss:0.009, val_acc:0.991]
Epoch [90/120    avg_loss:0.010, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.991]
Epoch [92/120    avg_loss:0.010, val_acc:0.991]
Epoch [93/120    avg_loss:0.011, val_acc:0.991]
Epoch [94/120    avg_loss:0.010, val_acc:0.991]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.007, val_acc:0.990]
Epoch [97/120    avg_loss:0.010, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.010, val_acc:0.991]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.010, val_acc:0.990]
Epoch [103/120    avg_loss:0.009, val_acc:0.991]
Epoch [104/120    avg_loss:0.009, val_acc:0.991]
Epoch [105/120    avg_loss:0.008, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.991]
Epoch [107/120    avg_loss:0.007, val_acc:0.991]
Epoch [108/120    avg_loss:0.010, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.991]
Epoch [113/120    avg_loss:0.010, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6360     0     0     0     0     0     0    68     4]
 [    0     3 18038     0    42     0     6     0     1     0]
 [    0     8     1  2001     0     0     0     0    24     2]
 [    0    44     8     0  2905     0     2     0    12     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     9     8     4     0  4856     0     1     0]
 [    0     9     0     0     0     0     0  1281     0     0]
 [    0    19     0     0    61     0     0     0  3481    10]
 [    0     0     0     0    14    20     0     0     0   885]]

Accuracy:
99.0817728291519

F1 scores:
[       nan 0.98796117 0.99806341 0.98936959 0.96865622 0.99239544
 0.99692055 0.99649942 0.97261805 0.97199341]

Kappa:
0.9878371578966891
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a5989f8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.993, val_acc:0.465]
Epoch [2/120    avg_loss:1.595, val_acc:0.274]
Epoch [3/120    avg_loss:1.330, val_acc:0.323]
Epoch [4/120    avg_loss:1.165, val_acc:0.453]
Epoch [5/120    avg_loss:1.028, val_acc:0.572]
Epoch [6/120    avg_loss:0.890, val_acc:0.609]
Epoch [7/120    avg_loss:0.762, val_acc:0.658]
Epoch [8/120    avg_loss:0.637, val_acc:0.714]
Epoch [9/120    avg_loss:0.609, val_acc:0.717]
Epoch [10/120    avg_loss:0.525, val_acc:0.747]
Epoch [11/120    avg_loss:0.431, val_acc:0.829]
Epoch [12/120    avg_loss:0.378, val_acc:0.778]
Epoch [13/120    avg_loss:0.336, val_acc:0.823]
Epoch [14/120    avg_loss:0.299, val_acc:0.915]
Epoch [15/120    avg_loss:0.278, val_acc:0.898]
Epoch [16/120    avg_loss:0.253, val_acc:0.917]
Epoch [17/120    avg_loss:0.231, val_acc:0.911]
Epoch [18/120    avg_loss:0.233, val_acc:0.906]
Epoch [19/120    avg_loss:0.195, val_acc:0.927]
Epoch [20/120    avg_loss:0.139, val_acc:0.934]
Epoch [21/120    avg_loss:0.140, val_acc:0.923]
Epoch [22/120    avg_loss:0.141, val_acc:0.944]
Epoch [23/120    avg_loss:0.133, val_acc:0.938]
Epoch [24/120    avg_loss:0.115, val_acc:0.953]
Epoch [25/120    avg_loss:0.112, val_acc:0.943]
Epoch [26/120    avg_loss:0.077, val_acc:0.970]
Epoch [27/120    avg_loss:0.079, val_acc:0.959]
Epoch [28/120    avg_loss:0.081, val_acc:0.976]
Epoch [29/120    avg_loss:0.060, val_acc:0.968]
Epoch [30/120    avg_loss:0.068, val_acc:0.961]
Epoch [31/120    avg_loss:0.051, val_acc:0.965]
Epoch [32/120    avg_loss:0.055, val_acc:0.962]
Epoch [33/120    avg_loss:0.056, val_acc:0.969]
Epoch [34/120    avg_loss:0.087, val_acc:0.973]
Epoch [35/120    avg_loss:0.049, val_acc:0.970]
Epoch [36/120    avg_loss:0.053, val_acc:0.950]
Epoch [37/120    avg_loss:0.051, val_acc:0.976]
Epoch [38/120    avg_loss:0.035, val_acc:0.975]
Epoch [39/120    avg_loss:0.042, val_acc:0.954]
Epoch [40/120    avg_loss:0.043, val_acc:0.972]
Epoch [41/120    avg_loss:0.066, val_acc:0.963]
Epoch [42/120    avg_loss:0.120, val_acc:0.941]
Epoch [43/120    avg_loss:0.079, val_acc:0.967]
Epoch [44/120    avg_loss:0.048, val_acc:0.960]
Epoch [45/120    avg_loss:0.062, val_acc:0.977]
Epoch [46/120    avg_loss:0.046, val_acc:0.933]
Epoch [47/120    avg_loss:0.037, val_acc:0.960]
Epoch [48/120    avg_loss:0.055, val_acc:0.968]
Epoch [49/120    avg_loss:0.062, val_acc:0.969]
Epoch [50/120    avg_loss:0.039, val_acc:0.960]
Epoch [51/120    avg_loss:0.032, val_acc:0.976]
Epoch [52/120    avg_loss:0.029, val_acc:0.976]
Epoch [53/120    avg_loss:0.044, val_acc:0.939]
Epoch [54/120    avg_loss:0.037, val_acc:0.976]
Epoch [55/120    avg_loss:0.021, val_acc:0.976]
Epoch [56/120    avg_loss:0.027, val_acc:0.976]
Epoch [57/120    avg_loss:0.020, val_acc:0.979]
Epoch [58/120    avg_loss:0.021, val_acc:0.979]
Epoch [59/120    avg_loss:0.018, val_acc:0.973]
Epoch [60/120    avg_loss:0.016, val_acc:0.975]
Epoch [61/120    avg_loss:0.020, val_acc:0.981]
Epoch [62/120    avg_loss:0.024, val_acc:0.976]
Epoch [63/120    avg_loss:0.077, val_acc:0.955]
Epoch [64/120    avg_loss:0.053, val_acc:0.976]
Epoch [65/120    avg_loss:0.056, val_acc:0.948]
Epoch [66/120    avg_loss:0.032, val_acc:0.981]
Epoch [67/120    avg_loss:0.029, val_acc:0.967]
Epoch [68/120    avg_loss:0.031, val_acc:0.980]
Epoch [69/120    avg_loss:0.024, val_acc:0.974]
Epoch [70/120    avg_loss:0.028, val_acc:0.960]
Epoch [71/120    avg_loss:0.030, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.984]
Epoch [73/120    avg_loss:0.015, val_acc:0.981]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.012, val_acc:0.982]
Epoch [78/120    avg_loss:0.015, val_acc:0.977]
Epoch [79/120    avg_loss:0.027, val_acc:0.954]
Epoch [80/120    avg_loss:0.031, val_acc:0.979]
Epoch [81/120    avg_loss:0.019, val_acc:0.970]
Epoch [82/120    avg_loss:0.019, val_acc:0.970]
Epoch [83/120    avg_loss:0.020, val_acc:0.981]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.015, val_acc:0.976]
Epoch [87/120    avg_loss:0.012, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.977]
Epoch [93/120    avg_loss:0.018, val_acc:0.947]
Epoch [94/120    avg_loss:0.015, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.021, val_acc:0.969]
Epoch [100/120    avg_loss:0.014, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.976]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.018, val_acc:0.970]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     0     0     0    26     0    49     0]
 [    0     1 18023     0    49     0    12     0     5     0]
 [    0     3     0  2016     0     0     0     0    15     2]
 [    0    42    13     0  2903     0     2     0    12     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     2     0     0  4870     0     1     0]
 [    0     2     0     0     0     0     0  1285     0     3]
 [    0    10     0    13    57     0     0     0  3473    18]
 [    0     0     0     0    15    29     0     0     0   875]]

Accuracy:
99.06972260381269

F1 scores:
[       nan 0.98964739 0.99764745 0.99139415 0.96831221 0.98901099
 0.99509604 0.99805825 0.97474039 0.96312603]

Kappa:
0.9876802822080999
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3025f608d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.976, val_acc:0.132]
Epoch [2/120    avg_loss:1.543, val_acc:0.238]
Epoch [3/120    avg_loss:1.369, val_acc:0.373]
Epoch [4/120    avg_loss:1.178, val_acc:0.413]
Epoch [5/120    avg_loss:0.979, val_acc:0.602]
Epoch [6/120    avg_loss:0.822, val_acc:0.622]
Epoch [7/120    avg_loss:0.718, val_acc:0.561]
Epoch [8/120    avg_loss:0.621, val_acc:0.647]
Epoch [9/120    avg_loss:0.516, val_acc:0.746]
Epoch [10/120    avg_loss:0.430, val_acc:0.761]
Epoch [11/120    avg_loss:0.387, val_acc:0.807]
Epoch [12/120    avg_loss:0.332, val_acc:0.818]
Epoch [13/120    avg_loss:0.305, val_acc:0.880]
Epoch [14/120    avg_loss:0.314, val_acc:0.880]
Epoch [15/120    avg_loss:0.248, val_acc:0.887]
Epoch [16/120    avg_loss:0.213, val_acc:0.924]
Epoch [17/120    avg_loss:0.204, val_acc:0.956]
Epoch [18/120    avg_loss:0.169, val_acc:0.943]
Epoch [19/120    avg_loss:0.142, val_acc:0.958]
Epoch [20/120    avg_loss:0.151, val_acc:0.849]
Epoch [21/120    avg_loss:0.174, val_acc:0.946]
Epoch [22/120    avg_loss:0.113, val_acc:0.949]
Epoch [23/120    avg_loss:0.118, val_acc:0.966]
Epoch [24/120    avg_loss:0.106, val_acc:0.969]
Epoch [25/120    avg_loss:0.113, val_acc:0.955]
Epoch [26/120    avg_loss:0.097, val_acc:0.960]
Epoch [27/120    avg_loss:0.101, val_acc:0.941]
Epoch [28/120    avg_loss:0.137, val_acc:0.944]
Epoch [29/120    avg_loss:0.083, val_acc:0.978]
Epoch [30/120    avg_loss:0.103, val_acc:0.965]
Epoch [31/120    avg_loss:0.061, val_acc:0.964]
Epoch [32/120    avg_loss:0.049, val_acc:0.972]
Epoch [33/120    avg_loss:0.054, val_acc:0.939]
Epoch [34/120    avg_loss:0.049, val_acc:0.981]
Epoch [35/120    avg_loss:0.072, val_acc:0.976]
Epoch [36/120    avg_loss:0.056, val_acc:0.976]
Epoch [37/120    avg_loss:0.055, val_acc:0.977]
Epoch [38/120    avg_loss:0.051, val_acc:0.974]
Epoch [39/120    avg_loss:0.047, val_acc:0.980]
Epoch [40/120    avg_loss:0.046, val_acc:0.976]
Epoch [41/120    avg_loss:0.032, val_acc:0.981]
Epoch [42/120    avg_loss:0.042, val_acc:0.980]
Epoch [43/120    avg_loss:0.038, val_acc:0.982]
Epoch [44/120    avg_loss:0.029, val_acc:0.978]
Epoch [45/120    avg_loss:0.034, val_acc:0.986]
Epoch [46/120    avg_loss:0.026, val_acc:0.983]
Epoch [47/120    avg_loss:0.053, val_acc:0.970]
Epoch [48/120    avg_loss:0.031, val_acc:0.969]
Epoch [49/120    avg_loss:0.030, val_acc:0.979]
Epoch [50/120    avg_loss:0.036, val_acc:0.984]
Epoch [51/120    avg_loss:0.023, val_acc:0.981]
Epoch [52/120    avg_loss:0.027, val_acc:0.972]
Epoch [53/120    avg_loss:0.029, val_acc:0.984]
Epoch [54/120    avg_loss:0.026, val_acc:0.971]
Epoch [55/120    avg_loss:0.034, val_acc:0.980]
Epoch [56/120    avg_loss:0.018, val_acc:0.987]
Epoch [57/120    avg_loss:0.055, val_acc:0.966]
Epoch [58/120    avg_loss:0.047, val_acc:0.973]
Epoch [59/120    avg_loss:0.027, val_acc:0.961]
Epoch [60/120    avg_loss:0.051, val_acc:0.973]
Epoch [61/120    avg_loss:0.019, val_acc:0.982]
Epoch [62/120    avg_loss:0.017, val_acc:0.986]
Epoch [63/120    avg_loss:0.014, val_acc:0.982]
Epoch [64/120    avg_loss:0.017, val_acc:0.987]
Epoch [65/120    avg_loss:0.010, val_acc:0.990]
Epoch [66/120    avg_loss:0.012, val_acc:0.980]
Epoch [67/120    avg_loss:0.017, val_acc:0.990]
Epoch [68/120    avg_loss:0.011, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.979]
Epoch [70/120    avg_loss:0.010, val_acc:0.989]
Epoch [71/120    avg_loss:0.021, val_acc:0.984]
Epoch [72/120    avg_loss:0.019, val_acc:0.988]
Epoch [73/120    avg_loss:0.017, val_acc:0.990]
Epoch [74/120    avg_loss:0.015, val_acc:0.992]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.014, val_acc:0.992]
Epoch [78/120    avg_loss:0.014, val_acc:0.987]
Epoch [79/120    avg_loss:0.016, val_acc:0.983]
Epoch [80/120    avg_loss:0.030, val_acc:0.963]
Epoch [81/120    avg_loss:0.171, val_acc:0.920]
Epoch [82/120    avg_loss:0.130, val_acc:0.961]
Epoch [83/120    avg_loss:0.059, val_acc:0.980]
Epoch [84/120    avg_loss:0.060, val_acc:0.947]
Epoch [85/120    avg_loss:0.047, val_acc:0.978]
Epoch [86/120    avg_loss:0.042, val_acc:0.985]
Epoch [87/120    avg_loss:0.027, val_acc:0.967]
Epoch [88/120    avg_loss:0.022, val_acc:0.989]
Epoch [89/120    avg_loss:0.021, val_acc:0.989]
Epoch [90/120    avg_loss:0.037, val_acc:0.974]
Epoch [91/120    avg_loss:0.025, val_acc:0.988]
Epoch [92/120    avg_loss:0.017, val_acc:0.990]
Epoch [93/120    avg_loss:0.017, val_acc:0.991]
Epoch [94/120    avg_loss:0.013, val_acc:0.992]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.015, val_acc:0.992]
Epoch [97/120    avg_loss:0.016, val_acc:0.989]
Epoch [98/120    avg_loss:0.016, val_acc:0.990]
Epoch [99/120    avg_loss:0.011, val_acc:0.990]
Epoch [100/120    avg_loss:0.014, val_acc:0.989]
Epoch [101/120    avg_loss:0.013, val_acc:0.989]
Epoch [102/120    avg_loss:0.011, val_acc:0.990]
Epoch [103/120    avg_loss:0.013, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.012, val_acc:0.990]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.990]
Epoch [109/120    avg_loss:0.012, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.010, val_acc:0.990]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.990]
Epoch [114/120    avg_loss:0.012, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.011, val_acc:0.990]
Epoch [117/120    avg_loss:0.010, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.991]
Epoch [119/120    avg_loss:0.010, val_acc:0.990]
Epoch [120/120    avg_loss:0.012, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6346     0     0     0     0    15    39    32     0]
 [    0     2 18047     0    12     0    27     0     2     0]
 [    0     5     0  1980     0     0     0     0    47     4]
 [    0    44    11     2  2904     0     1     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     6     0     0     0  4863     0     9     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     9     0    21    37     0     0     0  3489    15]
 [    0     0     0     0    14    25     0     0     0   880]]

Accuracy:
99.06249246860916

F1 scores:
[       nan 0.98862751 0.99834043 0.9804407  0.97794241 0.99051233
 0.99407195 0.98510882 0.97458101 0.96809681]

Kappa:
0.9875817999641732
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f233e2aa908>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.946, val_acc:0.182]
Epoch [2/120    avg_loss:1.560, val_acc:0.270]
Epoch [3/120    avg_loss:1.318, val_acc:0.320]
Epoch [4/120    avg_loss:1.155, val_acc:0.399]
Epoch [5/120    avg_loss:1.037, val_acc:0.511]
Epoch [6/120    avg_loss:0.910, val_acc:0.731]
Epoch [7/120    avg_loss:0.776, val_acc:0.621]
Epoch [8/120    avg_loss:0.668, val_acc:0.741]
Epoch [9/120    avg_loss:0.556, val_acc:0.691]
Epoch [10/120    avg_loss:0.508, val_acc:0.801]
Epoch [11/120    avg_loss:0.436, val_acc:0.792]
Epoch [12/120    avg_loss:0.386, val_acc:0.839]
Epoch [13/120    avg_loss:0.343, val_acc:0.871]
Epoch [14/120    avg_loss:0.309, val_acc:0.825]
Epoch [15/120    avg_loss:0.284, val_acc:0.880]
Epoch [16/120    avg_loss:0.242, val_acc:0.879]
Epoch [17/120    avg_loss:0.223, val_acc:0.903]
Epoch [18/120    avg_loss:0.223, val_acc:0.916]
Epoch [19/120    avg_loss:0.237, val_acc:0.861]
Epoch [20/120    avg_loss:0.211, val_acc:0.915]
Epoch [21/120    avg_loss:0.181, val_acc:0.917]
Epoch [22/120    avg_loss:0.133, val_acc:0.924]
Epoch [23/120    avg_loss:0.216, val_acc:0.952]
Epoch [24/120    avg_loss:0.174, val_acc:0.927]
Epoch [25/120    avg_loss:0.169, val_acc:0.958]
Epoch [26/120    avg_loss:0.177, val_acc:0.933]
Epoch [27/120    avg_loss:0.139, val_acc:0.965]
Epoch [28/120    avg_loss:0.104, val_acc:0.941]
Epoch [29/120    avg_loss:0.091, val_acc:0.975]
Epoch [30/120    avg_loss:0.087, val_acc:0.969]
Epoch [31/120    avg_loss:0.095, val_acc:0.960]
Epoch [32/120    avg_loss:0.094, val_acc:0.971]
Epoch [33/120    avg_loss:0.055, val_acc:0.967]
Epoch [34/120    avg_loss:0.066, val_acc:0.980]
Epoch [35/120    avg_loss:0.085, val_acc:0.970]
Epoch [36/120    avg_loss:0.063, val_acc:0.969]
Epoch [37/120    avg_loss:0.045, val_acc:0.959]
Epoch [38/120    avg_loss:0.046, val_acc:0.985]
Epoch [39/120    avg_loss:0.038, val_acc:0.976]
Epoch [40/120    avg_loss:0.045, val_acc:0.971]
Epoch [41/120    avg_loss:0.040, val_acc:0.986]
Epoch [42/120    avg_loss:0.040, val_acc:0.971]
Epoch [43/120    avg_loss:0.044, val_acc:0.981]
Epoch [44/120    avg_loss:0.038, val_acc:0.978]
Epoch [45/120    avg_loss:0.029, val_acc:0.986]
Epoch [46/120    avg_loss:0.026, val_acc:0.981]
Epoch [47/120    avg_loss:0.020, val_acc:0.992]
Epoch [48/120    avg_loss:0.024, val_acc:0.986]
Epoch [49/120    avg_loss:0.049, val_acc:0.986]
Epoch [50/120    avg_loss:0.050, val_acc:0.980]
Epoch [51/120    avg_loss:0.028, val_acc:0.979]
Epoch [52/120    avg_loss:0.024, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.987]
Epoch [54/120    avg_loss:0.024, val_acc:0.986]
Epoch [55/120    avg_loss:0.041, val_acc:0.955]
Epoch [56/120    avg_loss:0.041, val_acc:0.978]
Epoch [57/120    avg_loss:0.025, val_acc:0.989]
Epoch [58/120    avg_loss:0.028, val_acc:0.981]
Epoch [59/120    avg_loss:0.023, val_acc:0.993]
Epoch [60/120    avg_loss:0.025, val_acc:0.982]
Epoch [61/120    avg_loss:0.020, val_acc:0.987]
Epoch [62/120    avg_loss:0.012, val_acc:0.989]
Epoch [63/120    avg_loss:0.015, val_acc:0.988]
Epoch [64/120    avg_loss:0.014, val_acc:0.986]
Epoch [65/120    avg_loss:0.019, val_acc:0.992]
Epoch [66/120    avg_loss:0.010, val_acc:0.987]
Epoch [67/120    avg_loss:0.012, val_acc:0.993]
Epoch [68/120    avg_loss:0.041, val_acc:0.968]
Epoch [69/120    avg_loss:0.042, val_acc:0.971]
Epoch [70/120    avg_loss:0.018, val_acc:0.989]
Epoch [71/120    avg_loss:0.017, val_acc:0.987]
Epoch [72/120    avg_loss:0.023, val_acc:0.957]
Epoch [73/120    avg_loss:0.019, val_acc:0.973]
Epoch [74/120    avg_loss:0.020, val_acc:0.989]
Epoch [75/120    avg_loss:0.016, val_acc:0.992]
Epoch [76/120    avg_loss:0.011, val_acc:0.992]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.994]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.012, val_acc:0.988]
Epoch [81/120    avg_loss:0.021, val_acc:0.986]
Epoch [82/120    avg_loss:0.032, val_acc:0.984]
Epoch [83/120    avg_loss:0.027, val_acc:0.986]
Epoch [84/120    avg_loss:0.019, val_acc:0.992]
Epoch [85/120    avg_loss:0.015, val_acc:0.991]
Epoch [86/120    avg_loss:0.007, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.993]
Epoch [88/120    avg_loss:0.023, val_acc:0.982]
Epoch [89/120    avg_loss:0.065, val_acc:0.979]
Epoch [90/120    avg_loss:0.054, val_acc:0.985]
Epoch [91/120    avg_loss:0.017, val_acc:0.982]
Epoch [92/120    avg_loss:0.015, val_acc:0.994]
Epoch [93/120    avg_loss:0.011, val_acc:0.994]
Epoch [94/120    avg_loss:0.011, val_acc:0.994]
Epoch [95/120    avg_loss:0.008, val_acc:0.993]
Epoch [96/120    avg_loss:0.010, val_acc:0.994]
Epoch [97/120    avg_loss:0.010, val_acc:0.994]
Epoch [98/120    avg_loss:0.009, val_acc:0.992]
Epoch [99/120    avg_loss:0.009, val_acc:0.994]
Epoch [100/120    avg_loss:0.008, val_acc:0.994]
Epoch [101/120    avg_loss:0.011, val_acc:0.994]
Epoch [102/120    avg_loss:0.011, val_acc:0.995]
Epoch [103/120    avg_loss:0.008, val_acc:0.994]
Epoch [104/120    avg_loss:0.008, val_acc:0.994]
Epoch [105/120    avg_loss:0.009, val_acc:0.993]
Epoch [106/120    avg_loss:0.009, val_acc:0.993]
Epoch [107/120    avg_loss:0.007, val_acc:0.993]
Epoch [108/120    avg_loss:0.010, val_acc:0.994]
Epoch [109/120    avg_loss:0.009, val_acc:0.994]
Epoch [110/120    avg_loss:0.008, val_acc:0.994]
Epoch [111/120    avg_loss:0.010, val_acc:0.993]
Epoch [112/120    avg_loss:0.007, val_acc:0.993]
Epoch [113/120    avg_loss:0.008, val_acc:0.994]
Epoch [114/120    avg_loss:0.009, val_acc:0.995]
Epoch [115/120    avg_loss:0.007, val_acc:0.995]
Epoch [116/120    avg_loss:0.008, val_acc:0.995]
Epoch [117/120    avg_loss:0.008, val_acc:0.993]
Epoch [118/120    avg_loss:0.007, val_acc:0.995]
Epoch [119/120    avg_loss:0.007, val_acc:0.994]
Epoch [120/120    avg_loss:0.007, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6351     0     0     0     0     0     4    77     0]
 [    0     0 18004     0    23     0    63     0     0     0]
 [    0     1     0  2019     0     0     0     0    12     4]
 [    0    28     9     0  2920     0     2     0    12     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     9     0     0  4866     0     3     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     1     0    19    51     0     0     0  3482    18]
 [    0     0     0     0    13    29     0     0     0   877]]

Accuracy:
99.0817728291519

F1 scores:
[       nan 0.99133692 0.99736864 0.98897869 0.97675197 0.98901099
 0.99194781 0.99767622 0.97303339 0.96426608]

Kappa:
0.9878446268178395
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6e78408d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.925, val_acc:0.217]
Epoch [2/120    avg_loss:1.517, val_acc:0.394]
Epoch [3/120    avg_loss:1.322, val_acc:0.340]
Epoch [4/120    avg_loss:1.152, val_acc:0.515]
Epoch [5/120    avg_loss:1.008, val_acc:0.476]
Epoch [6/120    avg_loss:0.882, val_acc:0.711]
Epoch [7/120    avg_loss:0.755, val_acc:0.649]
Epoch [8/120    avg_loss:0.671, val_acc:0.719]
Epoch [9/120    avg_loss:0.621, val_acc:0.774]
Epoch [10/120    avg_loss:0.530, val_acc:0.747]
Epoch [11/120    avg_loss:0.424, val_acc:0.774]
Epoch [12/120    avg_loss:0.389, val_acc:0.781]
Epoch [13/120    avg_loss:0.359, val_acc:0.820]
Epoch [14/120    avg_loss:0.300, val_acc:0.851]
Epoch [15/120    avg_loss:0.300, val_acc:0.827]
Epoch [16/120    avg_loss:0.283, val_acc:0.821]
Epoch [17/120    avg_loss:0.264, val_acc:0.836]
Epoch [18/120    avg_loss:0.224, val_acc:0.826]
Epoch [19/120    avg_loss:0.225, val_acc:0.884]
Epoch [20/120    avg_loss:0.216, val_acc:0.856]
Epoch [21/120    avg_loss:0.194, val_acc:0.888]
Epoch [22/120    avg_loss:0.213, val_acc:0.934]
Epoch [23/120    avg_loss:0.173, val_acc:0.921]
Epoch [24/120    avg_loss:0.199, val_acc:0.873]
Epoch [25/120    avg_loss:0.369, val_acc:0.768]
Epoch [26/120    avg_loss:0.310, val_acc:0.889]
Epoch [27/120    avg_loss:0.175, val_acc:0.934]
Epoch [28/120    avg_loss:0.142, val_acc:0.931]
Epoch [29/120    avg_loss:0.134, val_acc:0.918]
Epoch [30/120    avg_loss:0.122, val_acc:0.943]
Epoch [31/120    avg_loss:0.123, val_acc:0.958]
Epoch [32/120    avg_loss:0.114, val_acc:0.952]
Epoch [33/120    avg_loss:0.120, val_acc:0.960]
Epoch [34/120    avg_loss:0.126, val_acc:0.962]
Epoch [35/120    avg_loss:0.096, val_acc:0.969]
Epoch [36/120    avg_loss:0.088, val_acc:0.969]
Epoch [37/120    avg_loss:0.071, val_acc:0.960]
Epoch [38/120    avg_loss:0.068, val_acc:0.954]
Epoch [39/120    avg_loss:0.073, val_acc:0.958]
Epoch [40/120    avg_loss:0.082, val_acc:0.953]
Epoch [41/120    avg_loss:0.058, val_acc:0.975]
Epoch [42/120    avg_loss:0.065, val_acc:0.971]
Epoch [43/120    avg_loss:0.041, val_acc:0.980]
Epoch [44/120    avg_loss:0.048, val_acc:0.968]
Epoch [45/120    avg_loss:0.037, val_acc:0.981]
Epoch [46/120    avg_loss:0.028, val_acc:0.982]
Epoch [47/120    avg_loss:0.035, val_acc:0.977]
Epoch [48/120    avg_loss:0.031, val_acc:0.976]
Epoch [49/120    avg_loss:0.033, val_acc:0.975]
Epoch [50/120    avg_loss:0.041, val_acc:0.979]
Epoch [51/120    avg_loss:0.030, val_acc:0.977]
Epoch [52/120    avg_loss:0.025, val_acc:0.983]
Epoch [53/120    avg_loss:0.023, val_acc:0.981]
Epoch [54/120    avg_loss:0.039, val_acc:0.975]
Epoch [55/120    avg_loss:0.042, val_acc:0.976]
Epoch [56/120    avg_loss:0.035, val_acc:0.980]
Epoch [57/120    avg_loss:0.047, val_acc:0.928]
Epoch [58/120    avg_loss:0.051, val_acc:0.976]
Epoch [59/120    avg_loss:0.063, val_acc:0.974]
Epoch [60/120    avg_loss:0.033, val_acc:0.980]
Epoch [61/120    avg_loss:0.038, val_acc:0.961]
Epoch [62/120    avg_loss:0.040, val_acc:0.972]
Epoch [63/120    avg_loss:0.030, val_acc:0.949]
Epoch [64/120    avg_loss:0.027, val_acc:0.980]
Epoch [65/120    avg_loss:0.023, val_acc:0.966]
Epoch [66/120    avg_loss:0.019, val_acc:0.982]
Epoch [67/120    avg_loss:0.013, val_acc:0.984]
Epoch [68/120    avg_loss:0.016, val_acc:0.984]
Epoch [69/120    avg_loss:0.016, val_acc:0.983]
Epoch [70/120    avg_loss:0.014, val_acc:0.983]
Epoch [71/120    avg_loss:0.015, val_acc:0.982]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.010, val_acc:0.982]
Epoch [74/120    avg_loss:0.014, val_acc:0.984]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.014, val_acc:0.984]
Epoch [77/120    avg_loss:0.012, val_acc:0.984]
Epoch [78/120    avg_loss:0.012, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.013, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.987]
Epoch [90/120    avg_loss:0.013, val_acc:0.987]
Epoch [91/120    avg_loss:0.015, val_acc:0.986]
Epoch [92/120    avg_loss:0.015, val_acc:0.986]
Epoch [93/120    avg_loss:0.013, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.986]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.010, val_acc:0.987]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.014, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.987]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.987]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6373     0     0     2     0    11    29    17     0]
 [    0     0 18078     0    10     0     1     0     1     0]
 [    0     6     0  1994     0     0     0     0    29     7]
 [    0    33    12     0  2886     0     1     0    36     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     0     0     0  4843     0    15     0]
 [    0     1     0     0     0     0     0  1278     0    11]
 [    0    10     0    23    65     0     0     0  3466     7]
 [    0     0     0     0     7    34     0     0     0   878]]

Accuracy:
99.05526233340564

F1 scores:
[       nan 0.99152081 0.99878453 0.9839625  0.9713901  0.9871407
 0.99506883 0.98421255 0.9715487  0.96166484]

Kappa:
0.9874798991751866
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd160185940>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.013, val_acc:0.117]
Epoch [2/120    avg_loss:1.627, val_acc:0.215]
Epoch [3/120    avg_loss:1.429, val_acc:0.364]
Epoch [4/120    avg_loss:1.217, val_acc:0.551]
Epoch [5/120    avg_loss:1.049, val_acc:0.677]
Epoch [6/120    avg_loss:0.865, val_acc:0.711]
Epoch [7/120    avg_loss:0.752, val_acc:0.711]
Epoch [8/120    avg_loss:0.616, val_acc:0.755]
Epoch [9/120    avg_loss:0.567, val_acc:0.802]
Epoch [10/120    avg_loss:0.464, val_acc:0.820]
Epoch [11/120    avg_loss:0.423, val_acc:0.814]
Epoch [12/120    avg_loss:0.346, val_acc:0.818]
Epoch [13/120    avg_loss:0.330, val_acc:0.861]
Epoch [14/120    avg_loss:0.309, val_acc:0.855]
Epoch [15/120    avg_loss:0.274, val_acc:0.861]
Epoch [16/120    avg_loss:0.256, val_acc:0.899]
Epoch [17/120    avg_loss:0.258, val_acc:0.882]
Epoch [18/120    avg_loss:0.220, val_acc:0.913]
Epoch [19/120    avg_loss:0.173, val_acc:0.914]
Epoch [20/120    avg_loss:0.181, val_acc:0.943]
Epoch [21/120    avg_loss:0.164, val_acc:0.883]
Epoch [22/120    avg_loss:0.147, val_acc:0.943]
Epoch [23/120    avg_loss:0.125, val_acc:0.920]
Epoch [24/120    avg_loss:0.178, val_acc:0.927]
Epoch [25/120    avg_loss:0.109, val_acc:0.959]
Epoch [26/120    avg_loss:0.086, val_acc:0.938]
Epoch [27/120    avg_loss:0.081, val_acc:0.944]
Epoch [28/120    avg_loss:0.103, val_acc:0.939]
Epoch [29/120    avg_loss:0.074, val_acc:0.953]
Epoch [30/120    avg_loss:0.101, val_acc:0.952]
Epoch [31/120    avg_loss:0.094, val_acc:0.945]
Epoch [32/120    avg_loss:0.096, val_acc:0.943]
Epoch [33/120    avg_loss:0.063, val_acc:0.954]
Epoch [34/120    avg_loss:0.065, val_acc:0.969]
Epoch [35/120    avg_loss:0.067, val_acc:0.952]
Epoch [36/120    avg_loss:0.081, val_acc:0.961]
Epoch [37/120    avg_loss:0.057, val_acc:0.958]
Epoch [38/120    avg_loss:0.051, val_acc:0.965]
Epoch [39/120    avg_loss:0.058, val_acc:0.959]
Epoch [40/120    avg_loss:0.045, val_acc:0.970]
Epoch [41/120    avg_loss:0.053, val_acc:0.959]
Epoch [42/120    avg_loss:0.050, val_acc:0.970]
Epoch [43/120    avg_loss:0.049, val_acc:0.968]
Epoch [44/120    avg_loss:0.058, val_acc:0.959]
Epoch [45/120    avg_loss:0.040, val_acc:0.970]
Epoch [46/120    avg_loss:0.031, val_acc:0.972]
Epoch [47/120    avg_loss:0.028, val_acc:0.976]
Epoch [48/120    avg_loss:0.023, val_acc:0.979]
Epoch [49/120    avg_loss:0.025, val_acc:0.974]
Epoch [50/120    avg_loss:0.024, val_acc:0.966]
Epoch [51/120    avg_loss:0.030, val_acc:0.970]
Epoch [52/120    avg_loss:0.025, val_acc:0.982]
Epoch [53/120    avg_loss:0.022, val_acc:0.978]
Epoch [54/120    avg_loss:0.032, val_acc:0.953]
Epoch [55/120    avg_loss:0.026, val_acc:0.976]
Epoch [56/120    avg_loss:0.024, val_acc:0.979]
Epoch [57/120    avg_loss:0.017, val_acc:0.983]
Epoch [58/120    avg_loss:0.020, val_acc:0.981]
Epoch [59/120    avg_loss:0.021, val_acc:0.975]
Epoch [60/120    avg_loss:0.017, val_acc:0.975]
Epoch [61/120    avg_loss:0.017, val_acc:0.980]
Epoch [62/120    avg_loss:0.024, val_acc:0.953]
Epoch [63/120    avg_loss:0.026, val_acc:0.975]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.013, val_acc:0.982]
Epoch [66/120    avg_loss:0.015, val_acc:0.982]
Epoch [67/120    avg_loss:0.024, val_acc:0.975]
Epoch [68/120    avg_loss:0.011, val_acc:0.976]
Epoch [69/120    avg_loss:0.037, val_acc:0.962]
Epoch [70/120    avg_loss:0.049, val_acc:0.975]
Epoch [71/120    avg_loss:0.018, val_acc:0.981]
Epoch [72/120    avg_loss:0.020, val_acc:0.981]
Epoch [73/120    avg_loss:0.012, val_acc:0.986]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.013, val_acc:0.986]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.986]
Epoch [80/120    avg_loss:0.010, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.010, val_acc:0.988]
Epoch [84/120    avg_loss:0.012, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.989]
Epoch [88/120    avg_loss:0.013, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.013, val_acc:0.987]
Epoch [96/120    avg_loss:0.009, val_acc:0.989]
Epoch [97/120    avg_loss:0.009, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.010, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6390     0     0     1     0     2     0    35     4]
 [    0     0 18042     0    44     0     1     0     3     0]
 [    0     9     0  1972     0     0     0     0    52     3]
 [    0    23    19     2  2904     0     7     0    17     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4878     0     0     0]
 [    0     0     0     0     0     0     1  1287     0     2]
 [    0    15     0     8    55     0     0     0  3492     1]
 [    0     0     0     0    14    39     0     0     0   866]]

Accuracy:
99.13961391078013

F1 scores:
[       nan 0.99308416 0.99814666 0.98158288 0.96961603 0.98527746
 0.99887376 0.99883586 0.97405858 0.96490251]

Kappa:
0.9886020360186379
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4976d5940>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.095, val_acc:0.082]
Epoch [2/120    avg_loss:1.637, val_acc:0.211]
Epoch [3/120    avg_loss:1.427, val_acc:0.387]
Epoch [4/120    avg_loss:1.226, val_acc:0.410]
Epoch [5/120    avg_loss:1.080, val_acc:0.520]
Epoch [6/120    avg_loss:0.946, val_acc:0.534]
Epoch [7/120    avg_loss:0.798, val_acc:0.621]
Epoch [8/120    avg_loss:0.634, val_acc:0.730]
Epoch [9/120    avg_loss:0.541, val_acc:0.729]
Epoch [10/120    avg_loss:0.454, val_acc:0.815]
Epoch [11/120    avg_loss:0.420, val_acc:0.858]
Epoch [12/120    avg_loss:0.414, val_acc:0.751]
Epoch [13/120    avg_loss:0.404, val_acc:0.816]
Epoch [14/120    avg_loss:0.301, val_acc:0.885]
Epoch [15/120    avg_loss:0.277, val_acc:0.920]
Epoch [16/120    avg_loss:0.258, val_acc:0.912]
Epoch [17/120    avg_loss:0.245, val_acc:0.914]
Epoch [18/120    avg_loss:0.212, val_acc:0.946]
Epoch [19/120    avg_loss:0.179, val_acc:0.933]
Epoch [20/120    avg_loss:0.221, val_acc:0.912]
Epoch [21/120    avg_loss:0.187, val_acc:0.943]
Epoch [22/120    avg_loss:0.168, val_acc:0.934]
Epoch [23/120    avg_loss:0.165, val_acc:0.905]
Epoch [24/120    avg_loss:0.167, val_acc:0.916]
Epoch [25/120    avg_loss:0.211, val_acc:0.939]
Epoch [26/120    avg_loss:0.141, val_acc:0.909]
Epoch [27/120    avg_loss:0.128, val_acc:0.957]
Epoch [28/120    avg_loss:0.137, val_acc:0.946]
Epoch [29/120    avg_loss:0.152, val_acc:0.942]
Epoch [30/120    avg_loss:0.132, val_acc:0.942]
Epoch [31/120    avg_loss:0.121, val_acc:0.941]
Epoch [32/120    avg_loss:0.101, val_acc:0.942]
Epoch [33/120    avg_loss:0.099, val_acc:0.953]
Epoch [34/120    avg_loss:0.079, val_acc:0.960]
Epoch [35/120    avg_loss:0.090, val_acc:0.943]
Epoch [36/120    avg_loss:0.094, val_acc:0.951]
Epoch [37/120    avg_loss:0.081, val_acc:0.960]
Epoch [38/120    avg_loss:0.057, val_acc:0.962]
Epoch [39/120    avg_loss:0.049, val_acc:0.962]
Epoch [40/120    avg_loss:0.053, val_acc:0.943]
Epoch [41/120    avg_loss:0.066, val_acc:0.944]
Epoch [42/120    avg_loss:0.077, val_acc:0.965]
Epoch [43/120    avg_loss:0.051, val_acc:0.965]
Epoch [44/120    avg_loss:0.040, val_acc:0.967]
Epoch [45/120    avg_loss:0.050, val_acc:0.951]
Epoch [46/120    avg_loss:0.075, val_acc:0.957]
Epoch [47/120    avg_loss:0.058, val_acc:0.961]
Epoch [48/120    avg_loss:0.051, val_acc:0.957]
Epoch [49/120    avg_loss:0.044, val_acc:0.970]
Epoch [50/120    avg_loss:0.033, val_acc:0.966]
Epoch [51/120    avg_loss:0.034, val_acc:0.963]
Epoch [52/120    avg_loss:0.022, val_acc:0.970]
Epoch [53/120    avg_loss:0.029, val_acc:0.972]
Epoch [54/120    avg_loss:0.037, val_acc:0.952]
Epoch [55/120    avg_loss:0.045, val_acc:0.973]
Epoch [56/120    avg_loss:0.032, val_acc:0.956]
Epoch [57/120    avg_loss:0.025, val_acc:0.966]
Epoch [58/120    avg_loss:0.022, val_acc:0.975]
Epoch [59/120    avg_loss:0.031, val_acc:0.973]
Epoch [60/120    avg_loss:0.043, val_acc:0.966]
Epoch [61/120    avg_loss:0.025, val_acc:0.977]
Epoch [62/120    avg_loss:0.028, val_acc:0.973]
Epoch [63/120    avg_loss:0.021, val_acc:0.978]
Epoch [64/120    avg_loss:0.020, val_acc:0.979]
Epoch [65/120    avg_loss:0.029, val_acc:0.975]
Epoch [66/120    avg_loss:0.021, val_acc:0.963]
Epoch [67/120    avg_loss:0.018, val_acc:0.975]
Epoch [68/120    avg_loss:0.015, val_acc:0.970]
Epoch [69/120    avg_loss:0.013, val_acc:0.977]
Epoch [70/120    avg_loss:0.013, val_acc:0.978]
Epoch [71/120    avg_loss:0.021, val_acc:0.970]
Epoch [72/120    avg_loss:0.013, val_acc:0.975]
Epoch [73/120    avg_loss:0.014, val_acc:0.976]
Epoch [74/120    avg_loss:0.021, val_acc:0.979]
Epoch [75/120    avg_loss:0.032, val_acc:0.970]
Epoch [76/120    avg_loss:0.030, val_acc:0.975]
Epoch [77/120    avg_loss:0.037, val_acc:0.941]
Epoch [78/120    avg_loss:0.034, val_acc:0.969]
Epoch [79/120    avg_loss:0.029, val_acc:0.972]
Epoch [80/120    avg_loss:0.016, val_acc:0.975]
Epoch [81/120    avg_loss:0.012, val_acc:0.977]
Epoch [82/120    avg_loss:0.022, val_acc:0.979]
Epoch [83/120    avg_loss:0.014, val_acc:0.975]
Epoch [84/120    avg_loss:0.012, val_acc:0.975]
Epoch [85/120    avg_loss:0.015, val_acc:0.976]
Epoch [86/120    avg_loss:0.015, val_acc:0.975]
Epoch [87/120    avg_loss:0.021, val_acc:0.979]
Epoch [88/120    avg_loss:0.018, val_acc:0.979]
Epoch [89/120    avg_loss:0.057, val_acc:0.962]
Epoch [90/120    avg_loss:0.034, val_acc:0.979]
Epoch [91/120    avg_loss:0.022, val_acc:0.979]
Epoch [92/120    avg_loss:0.025, val_acc:0.972]
Epoch [93/120    avg_loss:0.026, val_acc:0.977]
Epoch [94/120    avg_loss:0.011, val_acc:0.973]
Epoch [95/120    avg_loss:0.012, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.970]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.014, val_acc:0.978]
Epoch [102/120    avg_loss:0.009, val_acc:0.975]
Epoch [103/120    avg_loss:0.012, val_acc:0.978]
Epoch [104/120    avg_loss:0.017, val_acc:0.966]
Epoch [105/120    avg_loss:0.026, val_acc:0.975]
Epoch [106/120    avg_loss:0.012, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.025, val_acc:0.978]
Epoch [109/120    avg_loss:0.007, val_acc:0.979]
Epoch [110/120    avg_loss:0.006, val_acc:0.974]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.979]
Epoch [114/120    avg_loss:0.011, val_acc:0.975]
Epoch [115/120    avg_loss:0.011, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.975]
Epoch [117/120    avg_loss:0.015, val_acc:0.950]
Epoch [118/120    avg_loss:0.018, val_acc:0.977]
Epoch [119/120    avg_loss:0.015, val_acc:0.957]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6309     0     1     0     0    23     4    95     0]
 [    0     3 18005     0    38     0    43     0     0     1]
 [    0     1     0  1998     0     0     0     0    28     9]
 [    0    26    10     0  2889     0     7     0    33     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     8     0     0  4869     0     1     0]
 [    0     7     0     0     0     6     3  1264     1     9]
 [    0    13     0     3    49     0     0     0  3472    34]
 [    0     0     0     0     4    17     0     0     0   898]]

Accuracy:
98.83353818716411

F1 scores:
[       nan 0.98647487 0.99736879 0.98764212 0.97076613 0.99126472
 0.99134684 0.98827209 0.96431051 0.95684603]

Kappa:
0.9845588933781475
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f781c5f8898>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.990, val_acc:0.374]
Epoch [2/120    avg_loss:1.577, val_acc:0.650]
Epoch [3/120    avg_loss:1.357, val_acc:0.682]
Epoch [4/120    avg_loss:1.159, val_acc:0.642]
Epoch [5/120    avg_loss:0.988, val_acc:0.530]
Epoch [6/120    avg_loss:0.844, val_acc:0.533]
Epoch [7/120    avg_loss:0.722, val_acc:0.611]
Epoch [8/120    avg_loss:0.655, val_acc:0.620]
Epoch [9/120    avg_loss:0.585, val_acc:0.639]
Epoch [10/120    avg_loss:0.479, val_acc:0.698]
Epoch [11/120    avg_loss:0.418, val_acc:0.734]
Epoch [12/120    avg_loss:0.375, val_acc:0.739]
Epoch [13/120    avg_loss:0.340, val_acc:0.771]
Epoch [14/120    avg_loss:0.330, val_acc:0.825]
Epoch [15/120    avg_loss:0.271, val_acc:0.878]
Epoch [16/120    avg_loss:0.271, val_acc:0.783]
Epoch [17/120    avg_loss:0.262, val_acc:0.888]
Epoch [18/120    avg_loss:0.212, val_acc:0.895]
Epoch [19/120    avg_loss:0.204, val_acc:0.894]
Epoch [20/120    avg_loss:0.156, val_acc:0.920]
Epoch [21/120    avg_loss:0.224, val_acc:0.909]
Epoch [22/120    avg_loss:0.161, val_acc:0.938]
Epoch [23/120    avg_loss:0.128, val_acc:0.949]
Epoch [24/120    avg_loss:0.126, val_acc:0.935]
Epoch [25/120    avg_loss:0.093, val_acc:0.965]
Epoch [26/120    avg_loss:0.087, val_acc:0.959]
Epoch [27/120    avg_loss:0.075, val_acc:0.948]
Epoch [28/120    avg_loss:0.076, val_acc:0.948]
Epoch [29/120    avg_loss:0.084, val_acc:0.964]
Epoch [30/120    avg_loss:0.069, val_acc:0.969]
Epoch [31/120    avg_loss:0.046, val_acc:0.969]
Epoch [32/120    avg_loss:0.050, val_acc:0.974]
Epoch [33/120    avg_loss:0.042, val_acc:0.976]
Epoch [34/120    avg_loss:0.034, val_acc:0.976]
Epoch [35/120    avg_loss:0.053, val_acc:0.970]
Epoch [36/120    avg_loss:0.050, val_acc:0.976]
Epoch [37/120    avg_loss:0.050, val_acc:0.956]
Epoch [38/120    avg_loss:0.056, val_acc:0.975]
Epoch [39/120    avg_loss:0.039, val_acc:0.976]
Epoch [40/120    avg_loss:0.045, val_acc:0.967]
Epoch [41/120    avg_loss:0.035, val_acc:0.961]
Epoch [42/120    avg_loss:0.069, val_acc:0.962]
Epoch [43/120    avg_loss:0.052, val_acc:0.968]
Epoch [44/120    avg_loss:0.033, val_acc:0.969]
Epoch [45/120    avg_loss:0.047, val_acc:0.967]
Epoch [46/120    avg_loss:0.039, val_acc:0.980]
Epoch [47/120    avg_loss:0.031, val_acc:0.978]
Epoch [48/120    avg_loss:0.022, val_acc:0.978]
Epoch [49/120    avg_loss:0.020, val_acc:0.981]
Epoch [50/120    avg_loss:0.016, val_acc:0.978]
Epoch [51/120    avg_loss:0.011, val_acc:0.977]
Epoch [52/120    avg_loss:0.014, val_acc:0.981]
Epoch [53/120    avg_loss:0.018, val_acc:0.970]
Epoch [54/120    avg_loss:0.019, val_acc:0.978]
Epoch [55/120    avg_loss:0.014, val_acc:0.972]
Epoch [56/120    avg_loss:0.015, val_acc:0.978]
Epoch [57/120    avg_loss:0.018, val_acc:0.960]
Epoch [58/120    avg_loss:0.015, val_acc:0.982]
Epoch [59/120    avg_loss:0.024, val_acc:0.977]
Epoch [60/120    avg_loss:0.034, val_acc:0.972]
Epoch [61/120    avg_loss:0.023, val_acc:0.981]
Epoch [62/120    avg_loss:0.015, val_acc:0.981]
Epoch [63/120    avg_loss:0.015, val_acc:0.979]
Epoch [64/120    avg_loss:0.014, val_acc:0.979]
Epoch [65/120    avg_loss:0.020, val_acc:0.982]
Epoch [66/120    avg_loss:0.009, val_acc:0.983]
Epoch [67/120    avg_loss:0.014, val_acc:0.973]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.979]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.007, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.012, val_acc:0.986]
Epoch [79/120    avg_loss:0.018, val_acc:0.984]
Epoch [80/120    avg_loss:0.011, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6338     0     2     3     0     6    26    46    11]
 [    0     0 18054     0    27     0     9     0     0     0]
 [    0     2     0  2017     2     0     0     0     9     6]
 [    0    40    11     0  2895     0     5     0    20     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     8     5     0     0  4865     0     0     0]
 [    0     1     0     0     0     0     4  1283     0     2]
 [    0     5     0    15    53     0     0     0  3483    15]
 [    0     0     0     0    15    28     0     0     0   876]]

Accuracy:
99.09141300942328

F1 scores:
[       nan 0.98892183 0.99847911 0.98993865 0.97033685 0.9893859
 0.99621173 0.98730281 0.97713564 0.95737705]

Kappa:
0.9879650084824748
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb32e3b38d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.000, val_acc:0.347]
Epoch [2/120    avg_loss:1.567, val_acc:0.353]
Epoch [3/120    avg_loss:1.350, val_acc:0.360]
Epoch [4/120    avg_loss:1.180, val_acc:0.456]
Epoch [5/120    avg_loss:1.029, val_acc:0.486]
Epoch [6/120    avg_loss:0.886, val_acc:0.627]
Epoch [7/120    avg_loss:0.780, val_acc:0.651]
Epoch [8/120    avg_loss:0.644, val_acc:0.715]
Epoch [9/120    avg_loss:0.522, val_acc:0.736]
Epoch [10/120    avg_loss:0.447, val_acc:0.786]
Epoch [11/120    avg_loss:0.388, val_acc:0.769]
Epoch [12/120    avg_loss:0.351, val_acc:0.830]
Epoch [13/120    avg_loss:0.331, val_acc:0.813]
Epoch [14/120    avg_loss:0.291, val_acc:0.856]
Epoch [15/120    avg_loss:0.288, val_acc:0.870]
Epoch [16/120    avg_loss:0.247, val_acc:0.928]
Epoch [17/120    avg_loss:0.219, val_acc:0.910]
Epoch [18/120    avg_loss:0.207, val_acc:0.928]
Epoch [19/120    avg_loss:0.177, val_acc:0.953]
Epoch [20/120    avg_loss:0.202, val_acc:0.886]
Epoch [21/120    avg_loss:0.186, val_acc:0.903]
Epoch [22/120    avg_loss:0.149, val_acc:0.897]
Epoch [23/120    avg_loss:0.127, val_acc:0.962]
Epoch [24/120    avg_loss:0.102, val_acc:0.961]
Epoch [25/120    avg_loss:0.112, val_acc:0.940]
Epoch [26/120    avg_loss:0.091, val_acc:0.948]
Epoch [27/120    avg_loss:0.103, val_acc:0.959]
Epoch [28/120    avg_loss:0.127, val_acc:0.943]
Epoch [29/120    avg_loss:0.095, val_acc:0.971]
Epoch [30/120    avg_loss:0.072, val_acc:0.962]
Epoch [31/120    avg_loss:0.078, val_acc:0.950]
Epoch [32/120    avg_loss:0.097, val_acc:0.957]
Epoch [33/120    avg_loss:0.084, val_acc:0.969]
Epoch [34/120    avg_loss:0.060, val_acc:0.967]
Epoch [35/120    avg_loss:0.062, val_acc:0.963]
Epoch [36/120    avg_loss:0.077, val_acc:0.965]
Epoch [37/120    avg_loss:0.063, val_acc:0.981]
Epoch [38/120    avg_loss:0.054, val_acc:0.977]
Epoch [39/120    avg_loss:0.058, val_acc:0.979]
Epoch [40/120    avg_loss:0.069, val_acc:0.956]
Epoch [41/120    avg_loss:0.059, val_acc:0.980]
Epoch [42/120    avg_loss:0.060, val_acc:0.967]
Epoch [43/120    avg_loss:0.093, val_acc:0.969]
Epoch [44/120    avg_loss:0.065, val_acc:0.965]
Epoch [45/120    avg_loss:0.047, val_acc:0.979]
Epoch [46/120    avg_loss:0.042, val_acc:0.976]
Epoch [47/120    avg_loss:0.032, val_acc:0.976]
Epoch [48/120    avg_loss:0.029, val_acc:0.983]
Epoch [49/120    avg_loss:0.024, val_acc:0.983]
Epoch [50/120    avg_loss:0.022, val_acc:0.978]
Epoch [51/120    avg_loss:0.027, val_acc:0.986]
Epoch [52/120    avg_loss:0.021, val_acc:0.985]
Epoch [53/120    avg_loss:0.020, val_acc:0.987]
Epoch [54/120    avg_loss:0.016, val_acc:0.982]
Epoch [55/120    avg_loss:0.019, val_acc:0.989]
Epoch [56/120    avg_loss:0.017, val_acc:0.984]
Epoch [57/120    avg_loss:0.024, val_acc:0.981]
Epoch [58/120    avg_loss:0.038, val_acc:0.981]
Epoch [59/120    avg_loss:0.049, val_acc:0.985]
Epoch [60/120    avg_loss:0.034, val_acc:0.985]
Epoch [61/120    avg_loss:0.030, val_acc:0.983]
Epoch [62/120    avg_loss:0.023, val_acc:0.986]
Epoch [63/120    avg_loss:0.020, val_acc:0.991]
Epoch [64/120    avg_loss:0.015, val_acc:0.992]
Epoch [65/120    avg_loss:0.017, val_acc:0.987]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.010, val_acc:0.991]
Epoch [68/120    avg_loss:0.009, val_acc:0.989]
Epoch [69/120    avg_loss:0.011, val_acc:0.990]
Epoch [70/120    avg_loss:0.011, val_acc:0.990]
Epoch [71/120    avg_loss:0.035, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.988]
Epoch [73/120    avg_loss:0.022, val_acc:0.989]
Epoch [74/120    avg_loss:0.018, val_acc:0.990]
Epoch [75/120    avg_loss:0.011, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.992]
Epoch [79/120    avg_loss:0.008, val_acc:0.992]
Epoch [80/120    avg_loss:0.009, val_acc:0.992]
Epoch [81/120    avg_loss:0.007, val_acc:0.992]
Epoch [82/120    avg_loss:0.008, val_acc:0.992]
Epoch [83/120    avg_loss:0.009, val_acc:0.992]
Epoch [84/120    avg_loss:0.007, val_acc:0.992]
Epoch [85/120    avg_loss:0.007, val_acc:0.992]
Epoch [86/120    avg_loss:0.009, val_acc:0.992]
Epoch [87/120    avg_loss:0.008, val_acc:0.992]
Epoch [88/120    avg_loss:0.006, val_acc:0.992]
Epoch [89/120    avg_loss:0.006, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.992]
Epoch [91/120    avg_loss:0.007, val_acc:0.992]
Epoch [92/120    avg_loss:0.007, val_acc:0.992]
Epoch [93/120    avg_loss:0.006, val_acc:0.992]
Epoch [94/120    avg_loss:0.007, val_acc:0.992]
Epoch [95/120    avg_loss:0.008, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.992]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.007, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.008, val_acc:0.992]
Epoch [104/120    avg_loss:0.007, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.005, val_acc:0.992]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.007, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.005, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.006, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.992]
Epoch [119/120    avg_loss:0.005, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6407     0     0     1     0     0     0    24     0]
 [    0     0 18040     0    38     0     9     0     3     0]
 [    0    12     0  1966     0     0     0     0    53     5]
 [    0    36    18     0  2890     0     8     0    20     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     1     0     0  4859     0     4     0]
 [    0     0     0     0     0     0     0  1288     0     2]
 [    0     4     0    11    52     0     0     0  3503     1]
 [    0     0     0     0    14    31     0     0     0   874]]

Accuracy:
99.12997373050877

F1 scores:
[       nan 0.99402684 0.99773243 0.9795715  0.96866097 0.98826202
 0.99630921 0.9992242  0.97603789 0.9705719 ]

Kappa:
0.988472327436976
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:13
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81ab8608d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 22031==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.043, val_acc:0.110]
Epoch [2/120    avg_loss:1.600, val_acc:0.602]
Epoch [3/120    avg_loss:1.394, val_acc:0.689]
Epoch [4/120    avg_loss:1.194, val_acc:0.726]
Epoch [5/120    avg_loss:1.020, val_acc:0.704]
Epoch [6/120    avg_loss:0.879, val_acc:0.754]
Epoch [7/120    avg_loss:0.762, val_acc:0.677]
Epoch [8/120    avg_loss:0.710, val_acc:0.778]
Epoch [9/120    avg_loss:0.621, val_acc:0.699]
Epoch [10/120    avg_loss:0.508, val_acc:0.786]
Epoch [11/120    avg_loss:0.455, val_acc:0.751]
Epoch [12/120    avg_loss:0.415, val_acc:0.801]
Epoch [13/120    avg_loss:0.327, val_acc:0.807]
Epoch [14/120    avg_loss:0.328, val_acc:0.861]
Epoch [15/120    avg_loss:0.308, val_acc:0.816]
Epoch [16/120    avg_loss:0.248, val_acc:0.865]
Epoch [17/120    avg_loss:0.227, val_acc:0.901]
Epoch [18/120    avg_loss:0.179, val_acc:0.883]
Epoch [19/120    avg_loss:0.183, val_acc:0.884]
Epoch [20/120    avg_loss:0.199, val_acc:0.921]
Epoch [21/120    avg_loss:0.183, val_acc:0.916]
Epoch [22/120    avg_loss:0.182, val_acc:0.943]
Epoch [23/120    avg_loss:0.127, val_acc:0.937]
Epoch [24/120    avg_loss:0.156, val_acc:0.943]
Epoch [25/120    avg_loss:0.153, val_acc:0.918]
Epoch [26/120    avg_loss:0.134, val_acc:0.939]
Epoch [27/120    avg_loss:0.109, val_acc:0.920]
Epoch [28/120    avg_loss:0.083, val_acc:0.967]
Epoch [29/120    avg_loss:0.088, val_acc:0.956]
Epoch [30/120    avg_loss:0.083, val_acc:0.965]
Epoch [31/120    avg_loss:0.086, val_acc:0.940]
Epoch [32/120    avg_loss:0.103, val_acc:0.955]
Epoch [33/120    avg_loss:0.060, val_acc:0.969]
Epoch [34/120    avg_loss:0.054, val_acc:0.976]
Epoch [35/120    avg_loss:0.047, val_acc:0.963]
Epoch [36/120    avg_loss:0.044, val_acc:0.965]
Epoch [37/120    avg_loss:0.050, val_acc:0.962]
Epoch [38/120    avg_loss:0.128, val_acc:0.943]
Epoch [39/120    avg_loss:0.088, val_acc:0.958]
Epoch [40/120    avg_loss:0.057, val_acc:0.972]
Epoch [41/120    avg_loss:0.063, val_acc:0.969]
Epoch [42/120    avg_loss:0.058, val_acc:0.950]
Epoch [43/120    avg_loss:0.054, val_acc:0.956]
Epoch [44/120    avg_loss:0.056, val_acc:0.984]
Epoch [45/120    avg_loss:0.049, val_acc:0.909]
Epoch [46/120    avg_loss:0.045, val_acc:0.971]
Epoch [47/120    avg_loss:0.052, val_acc:0.926]
Epoch [48/120    avg_loss:0.043, val_acc:0.981]
Epoch [49/120    avg_loss:0.048, val_acc:0.958]
Epoch [50/120    avg_loss:0.030, val_acc:0.981]
Epoch [51/120    avg_loss:0.023, val_acc:0.985]
Epoch [52/120    avg_loss:0.018, val_acc:0.983]
Epoch [53/120    avg_loss:0.027, val_acc:0.982]
Epoch [54/120    avg_loss:0.031, val_acc:0.982]
Epoch [55/120    avg_loss:0.024, val_acc:0.981]
Epoch [56/120    avg_loss:0.038, val_acc:0.972]
Epoch [57/120    avg_loss:0.038, val_acc:0.978]
Epoch [58/120    avg_loss:0.028, val_acc:0.963]
Epoch [59/120    avg_loss:0.018, val_acc:0.981]
Epoch [60/120    avg_loss:0.017, val_acc:0.986]
Epoch [61/120    avg_loss:0.018, val_acc:0.985]
Epoch [62/120    avg_loss:0.018, val_acc:0.989]
Epoch [63/120    avg_loss:0.043, val_acc:0.965]
Epoch [64/120    avg_loss:0.044, val_acc:0.988]
Epoch [65/120    avg_loss:0.026, val_acc:0.974]
Epoch [66/120    avg_loss:0.025, val_acc:0.986]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.011, val_acc:0.986]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.015, val_acc:0.986]
Epoch [72/120    avg_loss:0.013, val_acc:0.989]
Epoch [73/120    avg_loss:0.020, val_acc:0.970]
Epoch [74/120    avg_loss:0.016, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.006, val_acc:0.990]
Epoch [79/120    avg_loss:0.007, val_acc:0.982]
Epoch [80/120    avg_loss:0.007, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.983]
Epoch [82/120    avg_loss:0.021, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.981]
Epoch [84/120    avg_loss:0.039, val_acc:0.982]
Epoch [85/120    avg_loss:0.023, val_acc:0.971]
Epoch [86/120    avg_loss:0.029, val_acc:0.983]
Epoch [87/120    avg_loss:0.026, val_acc:0.986]
Epoch [88/120    avg_loss:0.014, val_acc:0.990]
Epoch [89/120    avg_loss:0.016, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.991]
Epoch [92/120    avg_loss:0.007, val_acc:0.993]
Epoch [93/120    avg_loss:0.010, val_acc:0.991]
Epoch [94/120    avg_loss:0.033, val_acc:0.987]
Epoch [95/120    avg_loss:0.015, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.013, val_acc:0.986]
Epoch [99/120    avg_loss:0.060, val_acc:0.932]
Epoch [100/120    avg_loss:0.086, val_acc:0.965]
Epoch [101/120    avg_loss:0.040, val_acc:0.981]
Epoch [102/120    avg_loss:0.026, val_acc:0.970]
Epoch [103/120    avg_loss:0.038, val_acc:0.929]
Epoch [104/120    avg_loss:0.026, val_acc:0.985]
Epoch [105/120    avg_loss:0.016, val_acc:0.987]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.987]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.010, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6349     0     0     0     0     0     3    80     0]
 [    0     7 17880     0   171     0    22     0    10     0]
 [    0     2     0  2018     1     0     0     3    11     1]
 [    0    36    13     0  2908     0     3     0    11     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     6     0     0  4853     0     5     0]
 [    0     2     0     0     0     0     0  1280     0     8]
 [    0    12     0     1    45     0     0     0  3496    17]
 [    0     0     0     0    15    17     0     0     0   887]]

Accuracy:
98.75400669992528

F1 scores:
[       nan 0.98894081 0.99341612 0.99384388 0.95157068 0.99352874
 0.99487495 0.99378882 0.97327394 0.96781233]

Kappa:
0.9835256617272765
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e8861b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.011, val_acc:0.149]
Epoch [2/120    avg_loss:1.675, val_acc:0.303]
Epoch [3/120    avg_loss:1.466, val_acc:0.336]
Epoch [4/120    avg_loss:1.303, val_acc:0.383]
Epoch [5/120    avg_loss:1.117, val_acc:0.487]
Epoch [6/120    avg_loss:0.980, val_acc:0.569]
Epoch [7/120    avg_loss:0.832, val_acc:0.621]
Epoch [8/120    avg_loss:0.706, val_acc:0.690]
Epoch [9/120    avg_loss:0.618, val_acc:0.731]
Epoch [10/120    avg_loss:0.543, val_acc:0.757]
Epoch [11/120    avg_loss:0.500, val_acc:0.793]
Epoch [12/120    avg_loss:0.426, val_acc:0.766]
Epoch [13/120    avg_loss:0.377, val_acc:0.785]
Epoch [14/120    avg_loss:0.333, val_acc:0.796]
Epoch [15/120    avg_loss:0.351, val_acc:0.847]
Epoch [16/120    avg_loss:0.317, val_acc:0.856]
Epoch [17/120    avg_loss:0.271, val_acc:0.856]
Epoch [18/120    avg_loss:0.253, val_acc:0.876]
Epoch [19/120    avg_loss:0.208, val_acc:0.909]
Epoch [20/120    avg_loss:0.179, val_acc:0.931]
Epoch [21/120    avg_loss:0.193, val_acc:0.947]
Epoch [22/120    avg_loss:0.161, val_acc:0.910]
Epoch [23/120    avg_loss:0.154, val_acc:0.917]
Epoch [24/120    avg_loss:0.180, val_acc:0.953]
Epoch [25/120    avg_loss:0.169, val_acc:0.942]
Epoch [26/120    avg_loss:0.147, val_acc:0.956]
Epoch [27/120    avg_loss:0.107, val_acc:0.954]
Epoch [28/120    avg_loss:0.087, val_acc:0.969]
Epoch [29/120    avg_loss:0.071, val_acc:0.932]
Epoch [30/120    avg_loss:0.073, val_acc:0.963]
Epoch [31/120    avg_loss:0.074, val_acc:0.966]
Epoch [32/120    avg_loss:0.071, val_acc:0.956]
Epoch [33/120    avg_loss:0.094, val_acc:0.959]
Epoch [34/120    avg_loss:0.087, val_acc:0.973]
Epoch [35/120    avg_loss:0.065, val_acc:0.959]
Epoch [36/120    avg_loss:0.063, val_acc:0.971]
Epoch [37/120    avg_loss:0.108, val_acc:0.961]
Epoch [38/120    avg_loss:0.056, val_acc:0.959]
Epoch [39/120    avg_loss:0.056, val_acc:0.966]
Epoch [40/120    avg_loss:0.096, val_acc:0.970]
Epoch [41/120    avg_loss:0.075, val_acc:0.972]
Epoch [42/120    avg_loss:0.041, val_acc:0.965]
Epoch [43/120    avg_loss:0.071, val_acc:0.972]
Epoch [44/120    avg_loss:0.052, val_acc:0.981]
Epoch [45/120    avg_loss:0.036, val_acc:0.976]
Epoch [46/120    avg_loss:0.034, val_acc:0.977]
Epoch [47/120    avg_loss:0.037, val_acc:0.978]
Epoch [48/120    avg_loss:0.039, val_acc:0.979]
Epoch [49/120    avg_loss:0.051, val_acc:0.981]
Epoch [50/120    avg_loss:0.037, val_acc:0.979]
Epoch [51/120    avg_loss:0.037, val_acc:0.957]
Epoch [52/120    avg_loss:0.055, val_acc:0.972]
Epoch [53/120    avg_loss:0.039, val_acc:0.982]
Epoch [54/120    avg_loss:0.031, val_acc:0.976]
Epoch [55/120    avg_loss:0.026, val_acc:0.977]
Epoch [56/120    avg_loss:0.018, val_acc:0.984]
Epoch [57/120    avg_loss:0.019, val_acc:0.983]
Epoch [58/120    avg_loss:0.023, val_acc:0.980]
Epoch [59/120    avg_loss:0.054, val_acc:0.976]
Epoch [60/120    avg_loss:0.034, val_acc:0.976]
Epoch [61/120    avg_loss:0.020, val_acc:0.982]
Epoch [62/120    avg_loss:0.034, val_acc:0.981]
Epoch [63/120    avg_loss:0.030, val_acc:0.977]
Epoch [64/120    avg_loss:0.019, val_acc:0.981]
Epoch [65/120    avg_loss:0.018, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.984]
Epoch [67/120    avg_loss:0.017, val_acc:0.984]
Epoch [68/120    avg_loss:0.019, val_acc:0.984]
Epoch [69/120    avg_loss:0.028, val_acc:0.976]
Epoch [70/120    avg_loss:0.048, val_acc:0.979]
Epoch [71/120    avg_loss:0.022, val_acc:0.983]
Epoch [72/120    avg_loss:0.021, val_acc:0.981]
Epoch [73/120    avg_loss:0.017, val_acc:0.986]
Epoch [74/120    avg_loss:0.013, val_acc:0.984]
Epoch [75/120    avg_loss:0.014, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.988]
Epoch [81/120    avg_loss:0.024, val_acc:0.971]
Epoch [82/120    avg_loss:0.020, val_acc:0.979]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.987]
Epoch [85/120    avg_loss:0.028, val_acc:0.970]
Epoch [86/120    avg_loss:0.041, val_acc:0.979]
Epoch [87/120    avg_loss:0.019, val_acc:0.986]
Epoch [88/120    avg_loss:0.025, val_acc:0.983]
Epoch [89/120    avg_loss:0.025, val_acc:0.973]
Epoch [90/120    avg_loss:0.026, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.009, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.009, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6353     0     1     0     0     2     3    73     0]
 [    0     2 17978     0    75     0    21     0    13     1]
 [    0     2     0  2003     0     0     0     0    26     5]
 [    0    25    18     3  2892     0     8     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2    11     0     0  4863     0     2     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0    15    51     0     0     0  3497     6]
 [    0     0     0     0    14    34     0     0     0   871]]

Accuracy:
98.93717012508134

F1 scores:
[       nan 0.99141698 0.99634227 0.98451708 0.96335776 0.9871407
 0.99529267 0.99883856 0.97031077 0.96670366]

Kappa:
0.9859327980479685
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e6dbdb8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.949, val_acc:0.136]
Epoch [2/120    avg_loss:1.555, val_acc:0.198]
Epoch [3/120    avg_loss:1.370, val_acc:0.333]
Epoch [4/120    avg_loss:1.195, val_acc:0.592]
Epoch [5/120    avg_loss:1.076, val_acc:0.736]
Epoch [6/120    avg_loss:0.946, val_acc:0.745]
Epoch [7/120    avg_loss:0.793, val_acc:0.747]
Epoch [8/120    avg_loss:0.697, val_acc:0.736]
Epoch [9/120    avg_loss:0.634, val_acc:0.791]
Epoch [10/120    avg_loss:0.546, val_acc:0.811]
Epoch [11/120    avg_loss:0.454, val_acc:0.840]
Epoch [12/120    avg_loss:0.383, val_acc:0.867]
Epoch [13/120    avg_loss:0.298, val_acc:0.886]
Epoch [14/120    avg_loss:0.252, val_acc:0.917]
Epoch [15/120    avg_loss:0.223, val_acc:0.935]
Epoch [16/120    avg_loss:0.250, val_acc:0.923]
Epoch [17/120    avg_loss:0.179, val_acc:0.922]
Epoch [18/120    avg_loss:0.178, val_acc:0.899]
Epoch [19/120    avg_loss:0.175, val_acc:0.888]
Epoch [20/120    avg_loss:0.188, val_acc:0.937]
Epoch [21/120    avg_loss:0.144, val_acc:0.941]
Epoch [22/120    avg_loss:0.109, val_acc:0.959]
Epoch [23/120    avg_loss:0.123, val_acc:0.927]
Epoch [24/120    avg_loss:0.114, val_acc:0.944]
Epoch [25/120    avg_loss:0.123, val_acc:0.954]
Epoch [26/120    avg_loss:0.119, val_acc:0.919]
Epoch [27/120    avg_loss:0.112, val_acc:0.960]
Epoch [28/120    avg_loss:0.094, val_acc:0.966]
Epoch [29/120    avg_loss:0.082, val_acc:0.948]
Epoch [30/120    avg_loss:0.077, val_acc:0.959]
Epoch [31/120    avg_loss:0.059, val_acc:0.970]
Epoch [32/120    avg_loss:0.055, val_acc:0.957]
Epoch [33/120    avg_loss:0.050, val_acc:0.970]
Epoch [34/120    avg_loss:0.059, val_acc:0.971]
Epoch [35/120    avg_loss:0.062, val_acc:0.964]
Epoch [36/120    avg_loss:0.068, val_acc:0.966]
Epoch [37/120    avg_loss:0.047, val_acc:0.970]
Epoch [38/120    avg_loss:0.044, val_acc:0.968]
Epoch [39/120    avg_loss:0.033, val_acc:0.975]
Epoch [40/120    avg_loss:0.030, val_acc:0.970]
Epoch [41/120    avg_loss:0.025, val_acc:0.976]
Epoch [42/120    avg_loss:0.024, val_acc:0.975]
Epoch [43/120    avg_loss:0.025, val_acc:0.973]
Epoch [44/120    avg_loss:0.033, val_acc:0.970]
Epoch [45/120    avg_loss:0.026, val_acc:0.978]
Epoch [46/120    avg_loss:0.016, val_acc:0.979]
Epoch [47/120    avg_loss:0.018, val_acc:0.981]
Epoch [48/120    avg_loss:0.018, val_acc:0.982]
Epoch [49/120    avg_loss:0.019, val_acc:0.973]
Epoch [50/120    avg_loss:0.034, val_acc:0.976]
Epoch [51/120    avg_loss:0.025, val_acc:0.954]
Epoch [52/120    avg_loss:0.028, val_acc:0.971]
Epoch [53/120    avg_loss:0.031, val_acc:0.971]
Epoch [54/120    avg_loss:0.029, val_acc:0.975]
Epoch [55/120    avg_loss:0.032, val_acc:0.966]
Epoch [56/120    avg_loss:0.025, val_acc:0.981]
Epoch [57/120    avg_loss:0.015, val_acc:0.985]
Epoch [58/120    avg_loss:0.018, val_acc:0.982]
Epoch [59/120    avg_loss:0.028, val_acc:0.983]
Epoch [60/120    avg_loss:0.077, val_acc:0.953]
Epoch [61/120    avg_loss:0.043, val_acc:0.972]
Epoch [62/120    avg_loss:0.026, val_acc:0.979]
Epoch [63/120    avg_loss:0.019, val_acc:0.982]
Epoch [64/120    avg_loss:0.032, val_acc:0.975]
Epoch [65/120    avg_loss:0.022, val_acc:0.981]
Epoch [66/120    avg_loss:0.018, val_acc:0.985]
Epoch [67/120    avg_loss:0.022, val_acc:0.980]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.011, val_acc:0.985]
Epoch [70/120    avg_loss:0.008, val_acc:0.982]
Epoch [71/120    avg_loss:0.009, val_acc:0.980]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.010, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.017, val_acc:0.943]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.009, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.012, val_acc:0.972]
Epoch [87/120    avg_loss:0.015, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.007, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.004, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6420     0     0     0     0     0     0    12     0]
 [    0     7 18076     0     7     0     0     0     0     0]
 [    0     5     1  2019     4     0     0     0     6     1]
 [    0    50    18     0  2889     0     5     0    10     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     6     0     0  4872     0     0     0]
 [    0     0     0     0     0     0     1  1288     0     1]
 [    0     5     0     0    49     0     0     0  3506    11]
 [    0     0     0     0    14    50     0     0     0   855]]

Accuracy:
99.36615814715735

F1 scores:
[       nan 0.99388498 0.99908802 0.99433637 0.97354676 0.98120301
 0.99876999 0.9992242  0.98691063 0.95691102]

Kappa:
0.9915994104158374
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e5da7a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.021, val_acc:0.157]
Epoch [2/120    avg_loss:1.666, val_acc:0.259]
Epoch [3/120    avg_loss:1.436, val_acc:0.355]
Epoch [4/120    avg_loss:1.240, val_acc:0.381]
Epoch [5/120    avg_loss:1.138, val_acc:0.422]
Epoch [6/120    avg_loss:1.020, val_acc:0.546]
Epoch [7/120    avg_loss:0.903, val_acc:0.602]
Epoch [8/120    avg_loss:0.804, val_acc:0.705]
Epoch [9/120    avg_loss:0.704, val_acc:0.721]
Epoch [10/120    avg_loss:0.572, val_acc:0.732]
Epoch [11/120    avg_loss:0.523, val_acc:0.773]
Epoch [12/120    avg_loss:0.452, val_acc:0.771]
Epoch [13/120    avg_loss:0.377, val_acc:0.828]
Epoch [14/120    avg_loss:0.419, val_acc:0.801]
Epoch [15/120    avg_loss:0.344, val_acc:0.845]
Epoch [16/120    avg_loss:0.290, val_acc:0.878]
Epoch [17/120    avg_loss:0.272, val_acc:0.828]
Epoch [18/120    avg_loss:0.232, val_acc:0.928]
Epoch [19/120    avg_loss:0.197, val_acc:0.943]
Epoch [20/120    avg_loss:0.184, val_acc:0.897]
Epoch [21/120    avg_loss:0.172, val_acc:0.900]
Epoch [22/120    avg_loss:0.194, val_acc:0.919]
Epoch [23/120    avg_loss:0.157, val_acc:0.939]
Epoch [24/120    avg_loss:0.147, val_acc:0.957]
Epoch [25/120    avg_loss:0.134, val_acc:0.961]
Epoch [26/120    avg_loss:0.120, val_acc:0.938]
Epoch [27/120    avg_loss:0.113, val_acc:0.961]
Epoch [28/120    avg_loss:0.093, val_acc:0.961]
Epoch [29/120    avg_loss:0.102, val_acc:0.968]
Epoch [30/120    avg_loss:0.094, val_acc:0.975]
Epoch [31/120    avg_loss:0.077, val_acc:0.969]
Epoch [32/120    avg_loss:0.066, val_acc:0.964]
Epoch [33/120    avg_loss:0.070, val_acc:0.956]
Epoch [34/120    avg_loss:0.076, val_acc:0.967]
Epoch [35/120    avg_loss:0.059, val_acc:0.973]
Epoch [36/120    avg_loss:0.040, val_acc:0.973]
Epoch [37/120    avg_loss:0.034, val_acc:0.974]
Epoch [38/120    avg_loss:0.041, val_acc:0.981]
Epoch [39/120    avg_loss:0.044, val_acc:0.964]
Epoch [40/120    avg_loss:0.058, val_acc:0.969]
Epoch [41/120    avg_loss:0.031, val_acc:0.982]
Epoch [42/120    avg_loss:0.051, val_acc:0.979]
Epoch [43/120    avg_loss:0.040, val_acc:0.978]
Epoch [44/120    avg_loss:0.024, val_acc:0.983]
Epoch [45/120    avg_loss:0.025, val_acc:0.982]
Epoch [46/120    avg_loss:0.024, val_acc:0.980]
Epoch [47/120    avg_loss:0.027, val_acc:0.980]
Epoch [48/120    avg_loss:0.020, val_acc:0.987]
Epoch [49/120    avg_loss:0.034, val_acc:0.980]
Epoch [50/120    avg_loss:0.034, val_acc:0.981]
Epoch [51/120    avg_loss:0.035, val_acc:0.985]
Epoch [52/120    avg_loss:0.031, val_acc:0.985]
Epoch [53/120    avg_loss:0.037, val_acc:0.962]
Epoch [54/120    avg_loss:0.039, val_acc:0.981]
Epoch [55/120    avg_loss:0.032, val_acc:0.983]
Epoch [56/120    avg_loss:0.016, val_acc:0.984]
Epoch [57/120    avg_loss:0.021, val_acc:0.984]
Epoch [58/120    avg_loss:0.022, val_acc:0.978]
Epoch [59/120    avg_loss:0.020, val_acc:0.987]
Epoch [60/120    avg_loss:0.013, val_acc:0.984]
Epoch [61/120    avg_loss:0.014, val_acc:0.989]
Epoch [62/120    avg_loss:0.009, val_acc:0.987]
Epoch [63/120    avg_loss:0.011, val_acc:0.988]
Epoch [64/120    avg_loss:0.012, val_acc:0.987]
Epoch [65/120    avg_loss:0.018, val_acc:0.984]
Epoch [66/120    avg_loss:0.031, val_acc:0.983]
Epoch [67/120    avg_loss:0.018, val_acc:0.987]
Epoch [68/120    avg_loss:0.011, val_acc:0.989]
Epoch [69/120    avg_loss:0.010, val_acc:0.988]
Epoch [70/120    avg_loss:0.008, val_acc:0.989]
Epoch [71/120    avg_loss:0.012, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.982]
Epoch [74/120    avg_loss:0.009, val_acc:0.990]
Epoch [75/120    avg_loss:0.007, val_acc:0.991]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.021, val_acc:0.989]
Epoch [78/120    avg_loss:0.010, val_acc:0.987]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.013, val_acc:0.984]
Epoch [81/120    avg_loss:0.018, val_acc:0.983]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.022, val_acc:0.966]
Epoch [88/120    avg_loss:0.035, val_acc:0.984]
Epoch [89/120    avg_loss:0.016, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.010, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.989]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6292     0    14     1     0     0    27    85    13]
 [    0     1 18048     0    27     0    11     0     0     3]
 [    0     1     0  2008     0     0     0     0    26     1]
 [    0    26    22     0  2898     0     3     0    23     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    11     0     0     0  4867     0     0     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    16     0     0    65     0     0     0  3489     1]
 [    0     0     0     0    14    32     0     0     0   873]]

Accuracy:
98.9805509363025

F1 scores:
[       nan 0.98558897 0.99792652 0.98965007 0.96971725 0.98788796
 0.99743826 0.98964327 0.96997498 0.96464088]

Kappa:
0.9864964598899704
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff081e25b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.054, val_acc:0.095]
Epoch [2/120    avg_loss:1.643, val_acc:0.236]
Epoch [3/120    avg_loss:1.446, val_acc:0.311]
Epoch [4/120    avg_loss:1.236, val_acc:0.356]
Epoch [5/120    avg_loss:1.083, val_acc:0.404]
Epoch [6/120    avg_loss:0.969, val_acc:0.471]
Epoch [7/120    avg_loss:0.855, val_acc:0.622]
Epoch [8/120    avg_loss:0.732, val_acc:0.628]
Epoch [9/120    avg_loss:0.615, val_acc:0.693]
Epoch [10/120    avg_loss:0.515, val_acc:0.741]
Epoch [11/120    avg_loss:0.507, val_acc:0.740]
Epoch [12/120    avg_loss:0.407, val_acc:0.766]
Epoch [13/120    avg_loss:0.346, val_acc:0.764]
Epoch [14/120    avg_loss:0.291, val_acc:0.810]
Epoch [15/120    avg_loss:0.281, val_acc:0.868]
Epoch [16/120    avg_loss:0.265, val_acc:0.917]
Epoch [17/120    avg_loss:0.213, val_acc:0.940]
Epoch [18/120    avg_loss:0.182, val_acc:0.937]
Epoch [19/120    avg_loss:0.172, val_acc:0.938]
Epoch [20/120    avg_loss:0.182, val_acc:0.938]
Epoch [21/120    avg_loss:0.166, val_acc:0.839]
Epoch [22/120    avg_loss:0.130, val_acc:0.931]
Epoch [23/120    avg_loss:0.104, val_acc:0.968]
Epoch [24/120    avg_loss:0.124, val_acc:0.943]
Epoch [25/120    avg_loss:0.113, val_acc:0.939]
Epoch [26/120    avg_loss:0.107, val_acc:0.934]
Epoch [27/120    avg_loss:0.102, val_acc:0.968]
Epoch [28/120    avg_loss:0.098, val_acc:0.955]
Epoch [29/120    avg_loss:0.089, val_acc:0.967]
Epoch [30/120    avg_loss:0.075, val_acc:0.970]
Epoch [31/120    avg_loss:0.077, val_acc:0.968]
Epoch [32/120    avg_loss:0.056, val_acc:0.976]
Epoch [33/120    avg_loss:0.052, val_acc:0.956]
Epoch [34/120    avg_loss:0.058, val_acc:0.973]
Epoch [35/120    avg_loss:0.057, val_acc:0.973]
Epoch [36/120    avg_loss:0.046, val_acc:0.981]
Epoch [37/120    avg_loss:0.044, val_acc:0.965]
Epoch [38/120    avg_loss:0.045, val_acc:0.955]
Epoch [39/120    avg_loss:0.036, val_acc:0.973]
Epoch [40/120    avg_loss:0.034, val_acc:0.981]
Epoch [41/120    avg_loss:0.031, val_acc:0.981]
Epoch [42/120    avg_loss:0.070, val_acc:0.970]
Epoch [43/120    avg_loss:0.039, val_acc:0.959]
Epoch [44/120    avg_loss:0.037, val_acc:0.977]
Epoch [45/120    avg_loss:0.032, val_acc:0.977]
Epoch [46/120    avg_loss:0.051, val_acc:0.972]
Epoch [47/120    avg_loss:0.037, val_acc:0.981]
Epoch [48/120    avg_loss:0.039, val_acc:0.979]
Epoch [49/120    avg_loss:0.029, val_acc:0.981]
Epoch [50/120    avg_loss:0.031, val_acc:0.975]
Epoch [51/120    avg_loss:0.030, val_acc:0.980]
Epoch [52/120    avg_loss:0.021, val_acc:0.983]
Epoch [53/120    avg_loss:0.023, val_acc:0.985]
Epoch [54/120    avg_loss:0.016, val_acc:0.986]
Epoch [55/120    avg_loss:0.019, val_acc:0.988]
Epoch [56/120    avg_loss:0.019, val_acc:0.983]
Epoch [57/120    avg_loss:0.025, val_acc:0.973]
Epoch [58/120    avg_loss:0.025, val_acc:0.981]
Epoch [59/120    avg_loss:0.025, val_acc:0.980]
Epoch [60/120    avg_loss:0.016, val_acc:0.988]
Epoch [61/120    avg_loss:0.019, val_acc:0.983]
Epoch [62/120    avg_loss:0.017, val_acc:0.978]
Epoch [63/120    avg_loss:0.015, val_acc:0.986]
Epoch [64/120    avg_loss:0.020, val_acc:0.968]
Epoch [65/120    avg_loss:0.015, val_acc:0.980]
Epoch [66/120    avg_loss:0.010, val_acc:0.986]
Epoch [67/120    avg_loss:0.019, val_acc:0.977]
Epoch [68/120    avg_loss:0.022, val_acc:0.982]
Epoch [69/120    avg_loss:0.018, val_acc:0.992]
Epoch [70/120    avg_loss:0.010, val_acc:0.988]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.010, val_acc:0.989]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.008, val_acc:0.990]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.018, val_acc:0.985]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.010, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.005, val_acc:0.987]
Epoch [83/120    avg_loss:0.005, val_acc:0.988]
Epoch [84/120    avg_loss:0.006, val_acc:0.990]
Epoch [85/120    avg_loss:0.006, val_acc:0.989]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.989]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6396     0     0     0     0     0     0    36     0]
 [    0     0 18068     0    18     0     4     0     0     0]
 [    0     2     0  2010     0     0     0     0    20     4]
 [    0    39    18     0  2879     0     4     0    32     0]
 [    0     0     0     0     0  1303     0     0     0     2]
 [    0     0    10     8     0     0  4857     0     2     1]
 [    0     5     0     0     0     0     0  1284     0     1]
 [    0     1     0     0    64     0     0     0  3504     2]
 [    0     0     0     0    14    24     0     0     0   881]]

Accuracy:
99.2504759839009

F1 scores:
[       nan 0.9935534  0.99861825 0.99161322 0.96821927 0.99012158
 0.9970235  0.997669   0.97808793 0.97348066]

Kappa:
0.9900670887090236
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb034b69b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.995, val_acc:0.087]
Epoch [2/120    avg_loss:1.629, val_acc:0.222]
Epoch [3/120    avg_loss:1.399, val_acc:0.354]
Epoch [4/120    avg_loss:1.211, val_acc:0.530]
Epoch [5/120    avg_loss:1.068, val_acc:0.493]
Epoch [6/120    avg_loss:0.909, val_acc:0.649]
Epoch [7/120    avg_loss:0.783, val_acc:0.681]
Epoch [8/120    avg_loss:0.667, val_acc:0.794]
Epoch [9/120    avg_loss:0.593, val_acc:0.837]
Epoch [10/120    avg_loss:0.498, val_acc:0.802]
Epoch [11/120    avg_loss:0.434, val_acc:0.868]
Epoch [12/120    avg_loss:0.394, val_acc:0.847]
Epoch [13/120    avg_loss:0.324, val_acc:0.841]
Epoch [14/120    avg_loss:0.316, val_acc:0.870]
Epoch [15/120    avg_loss:0.254, val_acc:0.883]
Epoch [16/120    avg_loss:0.247, val_acc:0.912]
Epoch [17/120    avg_loss:0.204, val_acc:0.902]
Epoch [18/120    avg_loss:0.190, val_acc:0.922]
Epoch [19/120    avg_loss:0.147, val_acc:0.936]
Epoch [20/120    avg_loss:0.146, val_acc:0.893]
Epoch [21/120    avg_loss:0.190, val_acc:0.933]
Epoch [22/120    avg_loss:0.186, val_acc:0.947]
Epoch [23/120    avg_loss:0.142, val_acc:0.947]
Epoch [24/120    avg_loss:0.106, val_acc:0.951]
Epoch [25/120    avg_loss:0.100, val_acc:0.929]
Epoch [26/120    avg_loss:0.105, val_acc:0.944]
Epoch [27/120    avg_loss:0.124, val_acc:0.914]
Epoch [28/120    avg_loss:0.150, val_acc:0.920]
Epoch [29/120    avg_loss:0.114, val_acc:0.955]
Epoch [30/120    avg_loss:0.112, val_acc:0.959]
Epoch [31/120    avg_loss:0.086, val_acc:0.952]
Epoch [32/120    avg_loss:0.080, val_acc:0.951]
Epoch [33/120    avg_loss:0.058, val_acc:0.961]
Epoch [34/120    avg_loss:0.059, val_acc:0.966]
Epoch [35/120    avg_loss:0.070, val_acc:0.959]
Epoch [36/120    avg_loss:0.067, val_acc:0.924]
Epoch [37/120    avg_loss:0.107, val_acc:0.970]
Epoch [38/120    avg_loss:0.072, val_acc:0.952]
Epoch [39/120    avg_loss:0.119, val_acc:0.950]
Epoch [40/120    avg_loss:0.074, val_acc:0.964]
Epoch [41/120    avg_loss:0.057, val_acc:0.965]
Epoch [42/120    avg_loss:0.034, val_acc:0.968]
Epoch [43/120    avg_loss:0.044, val_acc:0.953]
Epoch [44/120    avg_loss:0.048, val_acc:0.951]
Epoch [45/120    avg_loss:0.058, val_acc:0.959]
Epoch [46/120    avg_loss:0.036, val_acc:0.970]
Epoch [47/120    avg_loss:0.025, val_acc:0.968]
Epoch [48/120    avg_loss:0.048, val_acc:0.952]
Epoch [49/120    avg_loss:0.039, val_acc:0.967]
Epoch [50/120    avg_loss:0.039, val_acc:0.969]
Epoch [51/120    avg_loss:0.056, val_acc:0.952]
Epoch [52/120    avg_loss:0.067, val_acc:0.965]
Epoch [53/120    avg_loss:0.062, val_acc:0.956]
Epoch [54/120    avg_loss:0.051, val_acc:0.939]
Epoch [55/120    avg_loss:0.038, val_acc:0.968]
Epoch [56/120    avg_loss:0.024, val_acc:0.980]
Epoch [57/120    avg_loss:0.036, val_acc:0.968]
Epoch [58/120    avg_loss:0.026, val_acc:0.975]
Epoch [59/120    avg_loss:0.015, val_acc:0.976]
Epoch [60/120    avg_loss:0.021, val_acc:0.977]
Epoch [61/120    avg_loss:0.030, val_acc:0.969]
Epoch [62/120    avg_loss:0.028, val_acc:0.977]
Epoch [63/120    avg_loss:0.017, val_acc:0.981]
Epoch [64/120    avg_loss:0.012, val_acc:0.977]
Epoch [65/120    avg_loss:0.016, val_acc:0.977]
Epoch [66/120    avg_loss:0.018, val_acc:0.981]
Epoch [67/120    avg_loss:0.013, val_acc:0.974]
Epoch [68/120    avg_loss:0.021, val_acc:0.979]
Epoch [69/120    avg_loss:0.019, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.981]
Epoch [72/120    avg_loss:0.011, val_acc:0.980]
Epoch [73/120    avg_loss:0.013, val_acc:0.979]
Epoch [74/120    avg_loss:0.011, val_acc:0.980]
Epoch [75/120    avg_loss:0.017, val_acc:0.973]
Epoch [76/120    avg_loss:0.011, val_acc:0.979]
Epoch [77/120    avg_loss:0.017, val_acc:0.978]
Epoch [78/120    avg_loss:0.038, val_acc:0.974]
Epoch [79/120    avg_loss:0.021, val_acc:0.976]
Epoch [80/120    avg_loss:0.013, val_acc:0.968]
Epoch [81/120    avg_loss:0.027, val_acc:0.971]
Epoch [82/120    avg_loss:0.020, val_acc:0.977]
Epoch [83/120    avg_loss:0.012, val_acc:0.978]
Epoch [84/120    avg_loss:0.012, val_acc:0.978]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.011, val_acc:0.981]
Epoch [87/120    avg_loss:0.008, val_acc:0.981]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.982]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.982]
Epoch [98/120    avg_loss:0.006, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.982]
Epoch [101/120    avg_loss:0.007, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.982]
Epoch [112/120    avg_loss:0.005, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     0     0     0    14     0    57     6]
 [    0     0 18039     0    44     0     1     0     6     0]
 [    0     0     0  2022     0     0     0     0    14     0]
 [    0    29    13     0  2902     0     6     0    22     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     4     0     0     0  4874     0     0     0]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     8     0     4    60     0     0     0  3491     8]
 [    0     0     0     0    14    38     0     0     0   867]]

Accuracy:
99.15648422625503

F1 scores:
[       nan 0.99111042 0.99811874 0.99556869 0.96862483 0.98564955
 0.99723785 0.9992242  0.97500349 0.96333333]

Kappa:
0.9888278714593419
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f05ac5898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.973, val_acc:0.192]
Epoch [2/120    avg_loss:1.642, val_acc:0.247]
Epoch [3/120    avg_loss:1.477, val_acc:0.315]
Epoch [4/120    avg_loss:1.261, val_acc:0.341]
Epoch [5/120    avg_loss:1.126, val_acc:0.447]
Epoch [6/120    avg_loss:0.963, val_acc:0.551]
Epoch [7/120    avg_loss:0.837, val_acc:0.587]
Epoch [8/120    avg_loss:0.715, val_acc:0.668]
Epoch [9/120    avg_loss:0.618, val_acc:0.673]
Epoch [10/120    avg_loss:0.523, val_acc:0.761]
Epoch [11/120    avg_loss:0.442, val_acc:0.764]
Epoch [12/120    avg_loss:0.419, val_acc:0.791]
Epoch [13/120    avg_loss:0.364, val_acc:0.800]
Epoch [14/120    avg_loss:0.308, val_acc:0.821]
Epoch [15/120    avg_loss:0.268, val_acc:0.835]
Epoch [16/120    avg_loss:0.249, val_acc:0.854]
Epoch [17/120    avg_loss:0.250, val_acc:0.858]
Epoch [18/120    avg_loss:0.230, val_acc:0.942]
Epoch [19/120    avg_loss:0.169, val_acc:0.948]
Epoch [20/120    avg_loss:0.151, val_acc:0.938]
Epoch [21/120    avg_loss:0.152, val_acc:0.894]
Epoch [22/120    avg_loss:0.152, val_acc:0.926]
Epoch [23/120    avg_loss:0.130, val_acc:0.958]
Epoch [24/120    avg_loss:0.124, val_acc:0.937]
Epoch [25/120    avg_loss:0.114, val_acc:0.938]
Epoch [26/120    avg_loss:0.167, val_acc:0.921]
Epoch [27/120    avg_loss:0.139, val_acc:0.922]
Epoch [28/120    avg_loss:0.099, val_acc:0.969]
Epoch [29/120    avg_loss:0.092, val_acc:0.973]
Epoch [30/120    avg_loss:0.069, val_acc:0.979]
Epoch [31/120    avg_loss:0.070, val_acc:0.945]
Epoch [32/120    avg_loss:0.071, val_acc:0.979]
Epoch [33/120    avg_loss:0.077, val_acc:0.970]
Epoch [34/120    avg_loss:0.080, val_acc:0.966]
Epoch [35/120    avg_loss:0.151, val_acc:0.818]
Epoch [36/120    avg_loss:0.271, val_acc:0.919]
Epoch [37/120    avg_loss:0.147, val_acc:0.956]
Epoch [38/120    avg_loss:0.076, val_acc:0.969]
Epoch [39/120    avg_loss:0.069, val_acc:0.974]
Epoch [40/120    avg_loss:0.045, val_acc:0.956]
Epoch [41/120    avg_loss:0.045, val_acc:0.976]
Epoch [42/120    avg_loss:0.034, val_acc:0.978]
Epoch [43/120    avg_loss:0.056, val_acc:0.951]
Epoch [44/120    avg_loss:0.052, val_acc:0.965]
Epoch [45/120    avg_loss:0.038, val_acc:0.980]
Epoch [46/120    avg_loss:0.038, val_acc:0.978]
Epoch [47/120    avg_loss:0.029, val_acc:0.980]
Epoch [48/120    avg_loss:0.026, val_acc:0.975]
Epoch [49/120    avg_loss:0.030, val_acc:0.981]
Epoch [50/120    avg_loss:0.024, val_acc:0.979]
Epoch [51/120    avg_loss:0.031, val_acc:0.964]
Epoch [52/120    avg_loss:0.027, val_acc:0.973]
Epoch [53/120    avg_loss:0.033, val_acc:0.982]
Epoch [54/120    avg_loss:0.026, val_acc:0.963]
Epoch [55/120    avg_loss:0.027, val_acc:0.981]
Epoch [56/120    avg_loss:0.030, val_acc:0.971]
Epoch [57/120    avg_loss:0.032, val_acc:0.964]
Epoch [58/120    avg_loss:0.511, val_acc:0.605]
Epoch [59/120    avg_loss:1.336, val_acc:0.657]
Epoch [60/120    avg_loss:1.178, val_acc:0.396]
Epoch [61/120    avg_loss:1.104, val_acc:0.688]
Epoch [62/120    avg_loss:1.052, val_acc:0.719]
Epoch [63/120    avg_loss:1.003, val_acc:0.722]
Epoch [64/120    avg_loss:0.928, val_acc:0.743]
Epoch [65/120    avg_loss:0.900, val_acc:0.753]
Epoch [66/120    avg_loss:0.874, val_acc:0.756]
Epoch [67/120    avg_loss:0.835, val_acc:0.758]
Epoch [68/120    avg_loss:0.792, val_acc:0.758]
Epoch [69/120    avg_loss:0.857, val_acc:0.753]
Epoch [70/120    avg_loss:0.816, val_acc:0.764]
Epoch [71/120    avg_loss:0.808, val_acc:0.736]
Epoch [72/120    avg_loss:0.812, val_acc:0.765]
Epoch [73/120    avg_loss:0.805, val_acc:0.763]
Epoch [74/120    avg_loss:0.797, val_acc:0.763]
Epoch [75/120    avg_loss:0.800, val_acc:0.762]
Epoch [76/120    avg_loss:0.785, val_acc:0.758]
Epoch [77/120    avg_loss:0.796, val_acc:0.748]
Epoch [78/120    avg_loss:0.789, val_acc:0.756]
Epoch [79/120    avg_loss:0.773, val_acc:0.755]
Epoch [80/120    avg_loss:0.789, val_acc:0.756]
Epoch [81/120    avg_loss:0.791, val_acc:0.753]
Epoch [82/120    avg_loss:0.781, val_acc:0.758]
Epoch [83/120    avg_loss:0.786, val_acc:0.755]
Epoch [84/120    avg_loss:0.767, val_acc:0.756]
Epoch [85/120    avg_loss:0.777, val_acc:0.753]
Epoch [86/120    avg_loss:0.793, val_acc:0.756]
Epoch [87/120    avg_loss:0.788, val_acc:0.757]
Epoch [88/120    avg_loss:0.829, val_acc:0.753]
Epoch [89/120    avg_loss:0.781, val_acc:0.755]
Epoch [90/120    avg_loss:0.782, val_acc:0.758]
Epoch [91/120    avg_loss:0.774, val_acc:0.758]
Epoch [92/120    avg_loss:0.793, val_acc:0.758]
Epoch [93/120    avg_loss:0.761, val_acc:0.758]
Epoch [94/120    avg_loss:0.795, val_acc:0.758]
Epoch [95/120    avg_loss:0.772, val_acc:0.758]
Epoch [96/120    avg_loss:0.763, val_acc:0.758]
Epoch [97/120    avg_loss:0.787, val_acc:0.758]
Epoch [98/120    avg_loss:0.785, val_acc:0.758]
Epoch [99/120    avg_loss:0.771, val_acc:0.758]
Epoch [100/120    avg_loss:0.762, val_acc:0.758]
Epoch [101/120    avg_loss:0.776, val_acc:0.758]
Epoch [102/120    avg_loss:0.793, val_acc:0.758]
Epoch [103/120    avg_loss:0.814, val_acc:0.758]
Epoch [104/120    avg_loss:0.797, val_acc:0.758]
Epoch [105/120    avg_loss:0.780, val_acc:0.758]
Epoch [106/120    avg_loss:0.773, val_acc:0.758]
Epoch [107/120    avg_loss:0.764, val_acc:0.758]
Epoch [108/120    avg_loss:0.774, val_acc:0.758]
Epoch [109/120    avg_loss:0.769, val_acc:0.758]
Epoch [110/120    avg_loss:0.803, val_acc:0.758]
Epoch [111/120    avg_loss:0.808, val_acc:0.758]
Epoch [112/120    avg_loss:0.813, val_acc:0.758]
Epoch [113/120    avg_loss:0.787, val_acc:0.758]
Epoch [114/120    avg_loss:0.776, val_acc:0.758]
Epoch [115/120    avg_loss:0.785, val_acc:0.758]
Epoch [116/120    avg_loss:0.796, val_acc:0.758]
Epoch [117/120    avg_loss:0.780, val_acc:0.758]
Epoch [118/120    avg_loss:0.797, val_acc:0.758]
Epoch [119/120    avg_loss:0.790, val_acc:0.758]
Epoch [120/120    avg_loss:0.771, val_acc:0.758]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  4701    30   156   649    29    73     0   430   364]
 [    0     0 15959     0   114     0  2017     0     0     0]
 [    0    78    78  1403    64     0    26     0   308    79]
 [    0    87   586    45  1879     0   274    25    62    14]
 [    0     0     0     3     0  1302     0     0     0     0]
 [    0     3  1194    24   372     0  3133     0   152     0]
 [    0    86     0     0     0     0     8  1162     4    30]
 [    0   177     3   180    82     0    55     0  3047    27]
 [    0    21     0    11     9   170     0     5     0   703]]

Accuracy:
80.22799026341792

F1 scores:
[       nan 0.81156668 0.88809126 0.72731985 0.61195245 0.9280114
 0.59881498 0.93634166 0.80459467 0.6582397 ]

Kappa:
0.7394165869860829
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f132b89b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.028, val_acc:0.193]
Epoch [2/120    avg_loss:1.593, val_acc:0.326]
Epoch [3/120    avg_loss:1.339, val_acc:0.372]
Epoch [4/120    avg_loss:1.149, val_acc:0.440]
Epoch [5/120    avg_loss:1.035, val_acc:0.460]
Epoch [6/120    avg_loss:0.923, val_acc:0.648]
Epoch [7/120    avg_loss:0.826, val_acc:0.564]
Epoch [8/120    avg_loss:0.690, val_acc:0.648]
Epoch [9/120    avg_loss:0.576, val_acc:0.755]
Epoch [10/120    avg_loss:0.513, val_acc:0.746]
Epoch [11/120    avg_loss:0.454, val_acc:0.754]
Epoch [12/120    avg_loss:0.403, val_acc:0.773]
Epoch [13/120    avg_loss:0.411, val_acc:0.806]
Epoch [14/120    avg_loss:0.412, val_acc:0.796]
Epoch [15/120    avg_loss:0.354, val_acc:0.819]
Epoch [16/120    avg_loss:0.299, val_acc:0.852]
Epoch [17/120    avg_loss:0.281, val_acc:0.819]
Epoch [18/120    avg_loss:0.266, val_acc:0.825]
Epoch [19/120    avg_loss:0.213, val_acc:0.883]
Epoch [20/120    avg_loss:0.231, val_acc:0.909]
Epoch [21/120    avg_loss:0.353, val_acc:0.849]
Epoch [22/120    avg_loss:0.302, val_acc:0.883]
Epoch [23/120    avg_loss:0.208, val_acc:0.917]
Epoch [24/120    avg_loss:0.170, val_acc:0.919]
Epoch [25/120    avg_loss:0.153, val_acc:0.943]
Epoch [26/120    avg_loss:0.152, val_acc:0.908]
Epoch [27/120    avg_loss:0.165, val_acc:0.902]
Epoch [28/120    avg_loss:0.141, val_acc:0.938]
Epoch [29/120    avg_loss:0.096, val_acc:0.934]
Epoch [30/120    avg_loss:0.111, val_acc:0.944]
Epoch [31/120    avg_loss:0.103, val_acc:0.942]
Epoch [32/120    avg_loss:0.086, val_acc:0.961]
Epoch [33/120    avg_loss:0.079, val_acc:0.970]
Epoch [34/120    avg_loss:0.067, val_acc:0.980]
Epoch [35/120    avg_loss:0.059, val_acc:0.968]
Epoch [36/120    avg_loss:0.049, val_acc:0.973]
Epoch [37/120    avg_loss:0.083, val_acc:0.968]
Epoch [38/120    avg_loss:0.089, val_acc:0.961]
Epoch [39/120    avg_loss:0.079, val_acc:0.973]
Epoch [40/120    avg_loss:0.063, val_acc:0.984]
Epoch [41/120    avg_loss:0.052, val_acc:0.954]
Epoch [42/120    avg_loss:0.066, val_acc:0.971]
Epoch [43/120    avg_loss:0.058, val_acc:0.980]
Epoch [44/120    avg_loss:0.038, val_acc:0.978]
Epoch [45/120    avg_loss:0.048, val_acc:0.968]
Epoch [46/120    avg_loss:0.030, val_acc:0.985]
Epoch [47/120    avg_loss:0.025, val_acc:0.982]
Epoch [48/120    avg_loss:0.023, val_acc:0.987]
Epoch [49/120    avg_loss:0.026, val_acc:0.981]
Epoch [50/120    avg_loss:0.024, val_acc:0.979]
Epoch [51/120    avg_loss:0.028, val_acc:0.982]
Epoch [52/120    avg_loss:0.040, val_acc:0.969]
Epoch [53/120    avg_loss:0.029, val_acc:0.988]
Epoch [54/120    avg_loss:0.040, val_acc:0.963]
Epoch [55/120    avg_loss:0.031, val_acc:0.981]
Epoch [56/120    avg_loss:0.020, val_acc:0.984]
Epoch [57/120    avg_loss:0.024, val_acc:0.979]
Epoch [58/120    avg_loss:0.023, val_acc:0.988]
Epoch [59/120    avg_loss:0.016, val_acc:0.986]
Epoch [60/120    avg_loss:0.015, val_acc:0.974]
Epoch [61/120    avg_loss:0.019, val_acc:0.978]
Epoch [62/120    avg_loss:0.017, val_acc:0.979]
Epoch [63/120    avg_loss:0.017, val_acc:0.984]
Epoch [64/120    avg_loss:0.015, val_acc:0.981]
Epoch [65/120    avg_loss:0.016, val_acc:0.987]
Epoch [66/120    avg_loss:0.041, val_acc:0.983]
Epoch [67/120    avg_loss:0.065, val_acc:0.984]
Epoch [68/120    avg_loss:0.041, val_acc:0.970]
Epoch [69/120    avg_loss:0.064, val_acc:0.959]
Epoch [70/120    avg_loss:0.041, val_acc:0.981]
Epoch [71/120    avg_loss:0.029, val_acc:0.981]
Epoch [72/120    avg_loss:0.028, val_acc:0.985]
Epoch [73/120    avg_loss:0.014, val_acc:0.985]
Epoch [74/120    avg_loss:0.019, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.986]
Epoch [77/120    avg_loss:0.013, val_acc:0.989]
Epoch [78/120    avg_loss:0.013, val_acc:0.988]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.989]
Epoch [81/120    avg_loss:0.012, val_acc:0.988]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.014, val_acc:0.988]
Epoch [84/120    avg_loss:0.012, val_acc:0.989]
Epoch [85/120    avg_loss:0.009, val_acc:0.989]
Epoch [86/120    avg_loss:0.012, val_acc:0.989]
Epoch [87/120    avg_loss:0.013, val_acc:0.990]
Epoch [88/120    avg_loss:0.014, val_acc:0.989]
Epoch [89/120    avg_loss:0.010, val_acc:0.989]
Epoch [90/120    avg_loss:0.012, val_acc:0.989]
Epoch [91/120    avg_loss:0.011, val_acc:0.989]
Epoch [92/120    avg_loss:0.011, val_acc:0.989]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.989]
Epoch [96/120    avg_loss:0.010, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.990]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.011, val_acc:0.989]
Epoch [100/120    avg_loss:0.010, val_acc:0.989]
Epoch [101/120    avg_loss:0.010, val_acc:0.989]
Epoch [102/120    avg_loss:0.011, val_acc:0.990]
Epoch [103/120    avg_loss:0.012, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.989]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.009, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.990]
Epoch [109/120    avg_loss:0.009, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.010, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.989]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6431     0     0     0     0     0     0     1     0]
 [    0     0 18017     0    63     0    10     0     0     0]
 [    0     0     0  2026     5     0     0     0     4     1]
 [    0    36    18     0  2883     0    10     0    25     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4874     0     0     4]
 [    0     0     0     0     0     0     2  1288     0     0]
 [    0     4     0    24    50     0     0     0  3466    27]
 [    0     0     0     0    14    29     0     0     0   876]]

Accuracy:
99.21191526281541

F1 scores:
[       nan 0.99682244 0.99748097 0.9916789  0.96308669 0.98901099
 0.99733988 0.9992242  0.98089713 0.9589491 ]

Kappa:
0.9895633666099205
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd932ab6908>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.024, val_acc:0.234]
Epoch [2/120    avg_loss:1.625, val_acc:0.249]
Epoch [3/120    avg_loss:1.347, val_acc:0.303]
Epoch [4/120    avg_loss:1.167, val_acc:0.366]
Epoch [5/120    avg_loss:0.992, val_acc:0.396]
Epoch [6/120    avg_loss:0.878, val_acc:0.599]
Epoch [7/120    avg_loss:0.790, val_acc:0.613]
Epoch [8/120    avg_loss:0.686, val_acc:0.687]
Epoch [9/120    avg_loss:0.606, val_acc:0.688]
Epoch [10/120    avg_loss:0.509, val_acc:0.758]
Epoch [11/120    avg_loss:0.464, val_acc:0.769]
Epoch [12/120    avg_loss:0.434, val_acc:0.776]
Epoch [13/120    avg_loss:0.385, val_acc:0.837]
Epoch [14/120    avg_loss:0.324, val_acc:0.845]
Epoch [15/120    avg_loss:0.305, val_acc:0.874]
Epoch [16/120    avg_loss:0.232, val_acc:0.932]
Epoch [17/120    avg_loss:0.228, val_acc:0.929]
Epoch [18/120    avg_loss:0.216, val_acc:0.864]
Epoch [19/120    avg_loss:0.184, val_acc:0.931]
Epoch [20/120    avg_loss:0.175, val_acc:0.917]
Epoch [21/120    avg_loss:0.172, val_acc:0.954]
Epoch [22/120    avg_loss:0.157, val_acc:0.971]
Epoch [23/120    avg_loss:0.122, val_acc:0.953]
Epoch [24/120    avg_loss:0.110, val_acc:0.922]
Epoch [25/120    avg_loss:0.095, val_acc:0.927]
Epoch [26/120    avg_loss:0.108, val_acc:0.956]
Epoch [27/120    avg_loss:0.089, val_acc:0.972]
Epoch [28/120    avg_loss:0.082, val_acc:0.960]
Epoch [29/120    avg_loss:0.064, val_acc:0.967]
Epoch [30/120    avg_loss:0.076, val_acc:0.963]
Epoch [31/120    avg_loss:0.074, val_acc:0.974]
Epoch [32/120    avg_loss:0.052, val_acc:0.973]
Epoch [33/120    avg_loss:0.076, val_acc:0.957]
Epoch [34/120    avg_loss:0.045, val_acc:0.968]
Epoch [35/120    avg_loss:0.058, val_acc:0.978]
Epoch [36/120    avg_loss:0.048, val_acc:0.977]
Epoch [37/120    avg_loss:0.045, val_acc:0.971]
Epoch [38/120    avg_loss:0.052, val_acc:0.976]
Epoch [39/120    avg_loss:0.051, val_acc:0.960]
Epoch [40/120    avg_loss:0.034, val_acc:0.969]
Epoch [41/120    avg_loss:0.037, val_acc:0.975]
Epoch [42/120    avg_loss:0.036, val_acc:0.948]
Epoch [43/120    avg_loss:0.027, val_acc:0.979]
Epoch [44/120    avg_loss:0.040, val_acc:0.976]
Epoch [45/120    avg_loss:0.036, val_acc:0.976]
Epoch [46/120    avg_loss:0.033, val_acc:0.981]
Epoch [47/120    avg_loss:0.018, val_acc:0.982]
Epoch [48/120    avg_loss:0.021, val_acc:0.978]
Epoch [49/120    avg_loss:0.021, val_acc:0.977]
Epoch [50/120    avg_loss:0.021, val_acc:0.979]
Epoch [51/120    avg_loss:0.028, val_acc:0.983]
Epoch [52/120    avg_loss:0.021, val_acc:0.978]
Epoch [53/120    avg_loss:0.033, val_acc:0.973]
Epoch [54/120    avg_loss:0.023, val_acc:0.978]
Epoch [55/120    avg_loss:0.024, val_acc:0.978]
Epoch [56/120    avg_loss:0.017, val_acc:0.981]
Epoch [57/120    avg_loss:0.031, val_acc:0.965]
Epoch [58/120    avg_loss:0.024, val_acc:0.972]
Epoch [59/120    avg_loss:0.013, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.981]
Epoch [61/120    avg_loss:0.015, val_acc:0.983]
Epoch [62/120    avg_loss:0.015, val_acc:0.982]
Epoch [63/120    avg_loss:0.025, val_acc:0.970]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.021, val_acc:0.982]
Epoch [66/120    avg_loss:0.017, val_acc:0.977]
Epoch [67/120    avg_loss:0.012, val_acc:0.984]
Epoch [68/120    avg_loss:0.011, val_acc:0.986]
Epoch [69/120    avg_loss:0.009, val_acc:0.982]
Epoch [70/120    avg_loss:0.014, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.011, val_acc:0.980]
Epoch [73/120    avg_loss:0.012, val_acc:0.982]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.983]
Epoch [78/120    avg_loss:0.007, val_acc:0.981]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.008, val_acc:0.981]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.984]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.004, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.003, val_acc:0.985]
Epoch [98/120    avg_loss:0.005, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.004, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     3     0     0    18     2     0]
 [    0     0 18058     0    28     0     4     0     0     0]
 [    0     0     0  2022     2     0     0     0    12     0]
 [    0    42    19     0  2882     0     1     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    12     0     0     0  4864     0     0     2]
 [    0     0     0     0     0     0     0  1289     0     1]
 [    0     4     0     0    65     0     0     0  3496     6]
 [    0     0     0     0    19    42     0     0     0   858]]

Accuracy:
99.25288602896875

F1 scores:
[       nan 0.99464577 0.99825866 0.99655002 0.96533244 0.9841629
 0.99805068 0.99268387 0.98354199 0.96080627]

Kappa:
0.9900999168190375
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f33495fd898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.005, val_acc:0.107]
Epoch [2/120    avg_loss:1.623, val_acc:0.299]
Epoch [3/120    avg_loss:1.421, val_acc:0.317]
Epoch [4/120    avg_loss:1.253, val_acc:0.410]
Epoch [5/120    avg_loss:1.112, val_acc:0.437]
Epoch [6/120    avg_loss:0.952, val_acc:0.577]
Epoch [7/120    avg_loss:0.832, val_acc:0.657]
Epoch [8/120    avg_loss:0.695, val_acc:0.704]
Epoch [9/120    avg_loss:0.570, val_acc:0.742]
Epoch [10/120    avg_loss:0.491, val_acc:0.819]
Epoch [11/120    avg_loss:0.415, val_acc:0.838]
Epoch [12/120    avg_loss:0.373, val_acc:0.897]
Epoch [13/120    avg_loss:0.297, val_acc:0.932]
Epoch [14/120    avg_loss:0.268, val_acc:0.932]
Epoch [15/120    avg_loss:0.216, val_acc:0.945]
Epoch [16/120    avg_loss:0.182, val_acc:0.946]
Epoch [17/120    avg_loss:0.187, val_acc:0.955]
Epoch [18/120    avg_loss:0.152, val_acc:0.954]
Epoch [19/120    avg_loss:0.129, val_acc:0.954]
Epoch [20/120    avg_loss:0.117, val_acc:0.965]
Epoch [21/120    avg_loss:0.114, val_acc:0.951]
Epoch [22/120    avg_loss:0.106, val_acc:0.964]
Epoch [23/120    avg_loss:0.099, val_acc:0.965]
Epoch [24/120    avg_loss:0.091, val_acc:0.970]
Epoch [25/120    avg_loss:0.066, val_acc:0.978]
Epoch [26/120    avg_loss:0.089, val_acc:0.967]
Epoch [27/120    avg_loss:0.082, val_acc:0.960]
Epoch [28/120    avg_loss:0.076, val_acc:0.971]
Epoch [29/120    avg_loss:0.060, val_acc:0.976]
Epoch [30/120    avg_loss:0.062, val_acc:0.982]
Epoch [31/120    avg_loss:0.072, val_acc:0.959]
Epoch [32/120    avg_loss:0.055, val_acc:0.984]
Epoch [33/120    avg_loss:0.067, val_acc:0.981]
Epoch [34/120    avg_loss:0.066, val_acc:0.961]
Epoch [35/120    avg_loss:0.071, val_acc:0.971]
Epoch [36/120    avg_loss:0.046, val_acc:0.971]
Epoch [37/120    avg_loss:0.034, val_acc:0.981]
Epoch [38/120    avg_loss:0.033, val_acc:0.979]
Epoch [39/120    avg_loss:0.044, val_acc:0.978]
Epoch [40/120    avg_loss:0.035, val_acc:0.986]
Epoch [41/120    avg_loss:0.029, val_acc:0.985]
Epoch [42/120    avg_loss:0.032, val_acc:0.991]
Epoch [43/120    avg_loss:0.034, val_acc:0.977]
Epoch [44/120    avg_loss:0.035, val_acc:0.972]
Epoch [45/120    avg_loss:0.032, val_acc:0.986]
Epoch [46/120    avg_loss:0.031, val_acc:0.991]
Epoch [47/120    avg_loss:0.026, val_acc:0.988]
Epoch [48/120    avg_loss:0.025, val_acc:0.983]
Epoch [49/120    avg_loss:0.022, val_acc:0.981]
Epoch [50/120    avg_loss:0.033, val_acc:0.981]
Epoch [51/120    avg_loss:0.015, val_acc:0.991]
Epoch [52/120    avg_loss:0.020, val_acc:0.987]
Epoch [53/120    avg_loss:0.024, val_acc:0.986]
Epoch [54/120    avg_loss:0.015, val_acc:0.990]
Epoch [55/120    avg_loss:0.018, val_acc:0.986]
Epoch [56/120    avg_loss:0.019, val_acc:0.986]
Epoch [57/120    avg_loss:0.011, val_acc:0.991]
Epoch [58/120    avg_loss:0.016, val_acc:0.985]
Epoch [59/120    avg_loss:0.022, val_acc:0.984]
Epoch [60/120    avg_loss:0.032, val_acc:0.981]
Epoch [61/120    avg_loss:0.045, val_acc:0.978]
Epoch [62/120    avg_loss:0.017, val_acc:0.987]
Epoch [63/120    avg_loss:0.015, val_acc:0.992]
Epoch [64/120    avg_loss:0.015, val_acc:0.986]
Epoch [65/120    avg_loss:0.014, val_acc:0.991]
Epoch [66/120    avg_loss:0.016, val_acc:0.981]
Epoch [67/120    avg_loss:0.031, val_acc:0.977]
Epoch [68/120    avg_loss:0.018, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.992]
Epoch [70/120    avg_loss:0.008, val_acc:0.994]
Epoch [71/120    avg_loss:0.007, val_acc:0.991]
Epoch [72/120    avg_loss:0.008, val_acc:0.992]
Epoch [73/120    avg_loss:0.021, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.990]
Epoch [75/120    avg_loss:0.012, val_acc:0.991]
Epoch [76/120    avg_loss:0.011, val_acc:0.994]
Epoch [77/120    avg_loss:0.007, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.991]
Epoch [79/120    avg_loss:0.008, val_acc:0.994]
Epoch [80/120    avg_loss:0.008, val_acc:0.989]
Epoch [81/120    avg_loss:0.015, val_acc:0.980]
Epoch [82/120    avg_loss:0.020, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.993]
Epoch [84/120    avg_loss:0.005, val_acc:0.994]
Epoch [85/120    avg_loss:0.008, val_acc:0.992]
Epoch [86/120    avg_loss:0.013, val_acc:0.991]
Epoch [87/120    avg_loss:0.008, val_acc:0.993]
Epoch [88/120    avg_loss:0.005, val_acc:0.993]
Epoch [89/120    avg_loss:0.005, val_acc:0.992]
Epoch [90/120    avg_loss:0.005, val_acc:0.994]
Epoch [91/120    avg_loss:0.006, val_acc:0.994]
Epoch [92/120    avg_loss:0.010, val_acc:0.992]
Epoch [93/120    avg_loss:0.007, val_acc:0.991]
Epoch [94/120    avg_loss:0.006, val_acc:0.992]
Epoch [95/120    avg_loss:0.005, val_acc:0.993]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.992]
Epoch [98/120    avg_loss:0.012, val_acc:0.996]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.005, val_acc:0.992]
Epoch [101/120    avg_loss:0.005, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.020, val_acc:0.979]
Epoch [104/120    avg_loss:0.018, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.995]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.994]
Epoch [108/120    avg_loss:0.004, val_acc:0.995]
Epoch [109/120    avg_loss:0.004, val_acc:0.993]
Epoch [110/120    avg_loss:0.005, val_acc:0.993]
Epoch [111/120    avg_loss:0.007, val_acc:0.962]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.004, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.993]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.003, val_acc:0.994]
Epoch [117/120    avg_loss:0.003, val_acc:0.994]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.004, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6339     0    28     0     0     0     0    65     0]
 [    0     1 18059     0    29     0     1     0     0     0]
 [    0     6     0  2022     0     0     0     0     7     1]
 [    0    32    16     0  2890     0     8     0    26     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     2    10     0     0  4866     0     0     0]
 [    0     0     0     0     0     0     0  1287     0     3]
 [    0     1     0     0    59     0     0     0  3505     6]
 [    0     0     0     0    14    77     0     0     0   828]]

Accuracy:
99.05526233340564

F1 scores:
[       nan 0.9896183  0.99864517 0.98730469 0.96914822 0.97134351
 0.99784682 0.99883586 0.97713967 0.94251565]

Kappa:
0.9874849874341943
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:15
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20c8e05898>
supervision:full
center_pixel:True
Network :
Number of parameter: 24831==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.093, val_acc:0.107]
Epoch [2/120    avg_loss:1.661, val_acc:0.225]
Epoch [3/120    avg_loss:1.424, val_acc:0.351]
Epoch [4/120    avg_loss:1.231, val_acc:0.383]
Epoch [5/120    avg_loss:1.090, val_acc:0.476]
Epoch [6/120    avg_loss:0.903, val_acc:0.660]
Epoch [7/120    avg_loss:0.774, val_acc:0.681]
Epoch [8/120    avg_loss:0.629, val_acc:0.672]
Epoch [9/120    avg_loss:0.523, val_acc:0.761]
Epoch [10/120    avg_loss:0.435, val_acc:0.755]
Epoch [11/120    avg_loss:0.423, val_acc:0.793]
Epoch [12/120    avg_loss:0.383, val_acc:0.780]
Epoch [13/120    avg_loss:0.346, val_acc:0.780]
Epoch [14/120    avg_loss:0.345, val_acc:0.778]
Epoch [15/120    avg_loss:0.286, val_acc:0.830]
Epoch [16/120    avg_loss:0.244, val_acc:0.888]
Epoch [17/120    avg_loss:0.245, val_acc:0.823]
Epoch [18/120    avg_loss:0.233, val_acc:0.885]
Epoch [19/120    avg_loss:0.215, val_acc:0.905]
Epoch [20/120    avg_loss:0.217, val_acc:0.868]
Epoch [21/120    avg_loss:0.187, val_acc:0.918]
Epoch [22/120    avg_loss:0.160, val_acc:0.921]
Epoch [23/120    avg_loss:0.132, val_acc:0.912]
Epoch [24/120    avg_loss:0.130, val_acc:0.953]
Epoch [25/120    avg_loss:0.100, val_acc:0.937]
Epoch [26/120    avg_loss:0.098, val_acc:0.951]
Epoch [27/120    avg_loss:0.076, val_acc:0.949]
Epoch [28/120    avg_loss:0.089, val_acc:0.951]
Epoch [29/120    avg_loss:0.080, val_acc:0.928]
Epoch [30/120    avg_loss:0.080, val_acc:0.947]
Epoch [31/120    avg_loss:0.090, val_acc:0.942]
Epoch [32/120    avg_loss:0.066, val_acc:0.956]
Epoch [33/120    avg_loss:0.057, val_acc:0.927]
Epoch [34/120    avg_loss:0.113, val_acc:0.917]
Epoch [35/120    avg_loss:0.088, val_acc:0.967]
Epoch [36/120    avg_loss:0.072, val_acc:0.929]
Epoch [37/120    avg_loss:0.057, val_acc:0.956]
Epoch [38/120    avg_loss:0.068, val_acc:0.957]
Epoch [39/120    avg_loss:0.049, val_acc:0.971]
Epoch [40/120    avg_loss:0.040, val_acc:0.971]
Epoch [41/120    avg_loss:0.036, val_acc:0.964]
Epoch [42/120    avg_loss:0.034, val_acc:0.956]
Epoch [43/120    avg_loss:0.045, val_acc:0.939]
Epoch [44/120    avg_loss:0.038, val_acc:0.966]
Epoch [45/120    avg_loss:0.025, val_acc:0.975]
Epoch [46/120    avg_loss:0.029, val_acc:0.977]
Epoch [47/120    avg_loss:0.034, val_acc:0.957]
Epoch [48/120    avg_loss:0.062, val_acc:0.964]
Epoch [49/120    avg_loss:0.051, val_acc:0.961]
Epoch [50/120    avg_loss:0.044, val_acc:0.965]
Epoch [51/120    avg_loss:0.048, val_acc:0.969]
Epoch [52/120    avg_loss:0.035, val_acc:0.966]
Epoch [53/120    avg_loss:0.023, val_acc:0.983]
Epoch [54/120    avg_loss:0.033, val_acc:0.959]
Epoch [55/120    avg_loss:0.022, val_acc:0.978]
Epoch [56/120    avg_loss:0.024, val_acc:0.974]
Epoch [57/120    avg_loss:0.024, val_acc:0.973]
Epoch [58/120    avg_loss:0.020, val_acc:0.964]
Epoch [59/120    avg_loss:0.022, val_acc:0.977]
Epoch [60/120    avg_loss:0.012, val_acc:0.979]
Epoch [61/120    avg_loss:0.015, val_acc:0.972]
Epoch [62/120    avg_loss:0.020, val_acc:0.974]
Epoch [63/120    avg_loss:0.023, val_acc:0.977]
Epoch [64/120    avg_loss:0.017, val_acc:0.975]
Epoch [65/120    avg_loss:0.012, val_acc:0.982]
Epoch [66/120    avg_loss:0.063, val_acc:0.943]
Epoch [67/120    avg_loss:0.064, val_acc:0.963]
Epoch [68/120    avg_loss:0.040, val_acc:0.971]
Epoch [69/120    avg_loss:0.032, val_acc:0.970]
Epoch [70/120    avg_loss:0.021, val_acc:0.975]
Epoch [71/120    avg_loss:0.025, val_acc:0.976]
Epoch [72/120    avg_loss:0.017, val_acc:0.976]
Epoch [73/120    avg_loss:0.016, val_acc:0.977]
Epoch [74/120    avg_loss:0.018, val_acc:0.980]
Epoch [75/120    avg_loss:0.017, val_acc:0.979]
Epoch [76/120    avg_loss:0.012, val_acc:0.980]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.014, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.012, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.014, val_acc:0.980]
Epoch [84/120    avg_loss:0.015, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.014, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.013, val_acc:0.984]
Epoch [94/120    avg_loss:0.016, val_acc:0.984]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     0     0     0     0     4     2     0]
 [    0     2 18046     0    31     0     9     0     2     0]
 [    0     4     0  2028     0     0     0     0     3     1]
 [    0    27    20     0  2886     0    11     0    28     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5     4     0     0  4854     0    11     4]
 [    0     1     0     0     0     0     0  1286     0     3]
 [    0     1     0     1    55     0     0     0  3512     2]
 [    0     0     0     0    15    48     0     0     1   855]]

Accuracy:
99.28903670498639

F1 scores:
[       nan 0.99681998 0.99809187 0.99680511 0.9686189  0.98194131
 0.99548811 0.99689922 0.98513324 0.95852018]

Kappa:
0.9905805252536325
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9bc0c6a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.977, val_acc:0.104]
Epoch [2/120    avg_loss:1.635, val_acc:0.472]
Epoch [3/120    avg_loss:1.421, val_acc:0.598]
Epoch [4/120    avg_loss:1.262, val_acc:0.621]
Epoch [5/120    avg_loss:1.128, val_acc:0.635]
Epoch [6/120    avg_loss:0.989, val_acc:0.682]
Epoch [7/120    avg_loss:0.844, val_acc:0.646]
Epoch [8/120    avg_loss:0.716, val_acc:0.693]
Epoch [9/120    avg_loss:0.603, val_acc:0.749]
Epoch [10/120    avg_loss:0.550, val_acc:0.781]
Epoch [11/120    avg_loss:0.547, val_acc:0.744]
Epoch [12/120    avg_loss:0.430, val_acc:0.759]
Epoch [13/120    avg_loss:0.401, val_acc:0.805]
Epoch [14/120    avg_loss:0.335, val_acc:0.850]
Epoch [15/120    avg_loss:0.313, val_acc:0.808]
Epoch [16/120    avg_loss:0.312, val_acc:0.861]
Epoch [17/120    avg_loss:0.240, val_acc:0.902]
Epoch [18/120    avg_loss:0.203, val_acc:0.908]
Epoch [19/120    avg_loss:0.215, val_acc:0.868]
Epoch [20/120    avg_loss:0.207, val_acc:0.903]
Epoch [21/120    avg_loss:0.166, val_acc:0.883]
Epoch [22/120    avg_loss:0.157, val_acc:0.864]
Epoch [23/120    avg_loss:0.165, val_acc:0.934]
Epoch [24/120    avg_loss:0.130, val_acc:0.912]
Epoch [25/120    avg_loss:0.170, val_acc:0.930]
Epoch [26/120    avg_loss:0.104, val_acc:0.953]
Epoch [27/120    avg_loss:0.124, val_acc:0.935]
Epoch [28/120    avg_loss:0.122, val_acc:0.947]
Epoch [29/120    avg_loss:0.158, val_acc:0.932]
Epoch [30/120    avg_loss:0.107, val_acc:0.959]
Epoch [31/120    avg_loss:0.085, val_acc:0.943]
Epoch [32/120    avg_loss:0.107, val_acc:0.963]
Epoch [33/120    avg_loss:0.101, val_acc:0.926]
Epoch [34/120    avg_loss:0.119, val_acc:0.944]
Epoch [35/120    avg_loss:0.099, val_acc:0.957]
Epoch [36/120    avg_loss:0.086, val_acc:0.963]
Epoch [37/120    avg_loss:0.064, val_acc:0.964]
Epoch [38/120    avg_loss:0.044, val_acc:0.967]
Epoch [39/120    avg_loss:0.052, val_acc:0.956]
Epoch [40/120    avg_loss:0.052, val_acc:0.968]
Epoch [41/120    avg_loss:0.043, val_acc:0.968]
Epoch [42/120    avg_loss:0.041, val_acc:0.964]
Epoch [43/120    avg_loss:0.049, val_acc:0.943]
Epoch [44/120    avg_loss:0.105, val_acc:0.935]
Epoch [45/120    avg_loss:0.082, val_acc:0.950]
Epoch [46/120    avg_loss:0.076, val_acc:0.962]
Epoch [47/120    avg_loss:0.044, val_acc:0.964]
Epoch [48/120    avg_loss:0.043, val_acc:0.965]
Epoch [49/120    avg_loss:0.032, val_acc:0.944]
Epoch [50/120    avg_loss:0.041, val_acc:0.973]
Epoch [51/120    avg_loss:0.031, val_acc:0.970]
Epoch [52/120    avg_loss:0.034, val_acc:0.976]
Epoch [53/120    avg_loss:0.028, val_acc:0.971]
Epoch [54/120    avg_loss:0.029, val_acc:0.967]
Epoch [55/120    avg_loss:0.031, val_acc:0.968]
Epoch [56/120    avg_loss:0.035, val_acc:0.973]
Epoch [57/120    avg_loss:0.024, val_acc:0.976]
Epoch [58/120    avg_loss:0.038, val_acc:0.965]
Epoch [59/120    avg_loss:0.025, val_acc:0.978]
Epoch [60/120    avg_loss:0.041, val_acc:0.977]
Epoch [61/120    avg_loss:0.032, val_acc:0.976]
Epoch [62/120    avg_loss:0.022, val_acc:0.977]
Epoch [63/120    avg_loss:0.022, val_acc:0.977]
Epoch [64/120    avg_loss:0.019, val_acc:0.980]
Epoch [65/120    avg_loss:0.016, val_acc:0.970]
Epoch [66/120    avg_loss:0.032, val_acc:0.980]
Epoch [67/120    avg_loss:0.019, val_acc:0.979]
Epoch [68/120    avg_loss:0.023, val_acc:0.976]
Epoch [69/120    avg_loss:0.017, val_acc:0.977]
Epoch [70/120    avg_loss:0.021, val_acc:0.958]
Epoch [71/120    avg_loss:0.017, val_acc:0.978]
Epoch [72/120    avg_loss:0.011, val_acc:0.981]
Epoch [73/120    avg_loss:0.015, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.013, val_acc:0.983]
Epoch [76/120    avg_loss:0.014, val_acc:0.978]
Epoch [77/120    avg_loss:0.017, val_acc:0.977]
Epoch [78/120    avg_loss:0.012, val_acc:0.979]
Epoch [79/120    avg_loss:0.015, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.982]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.979]
Epoch [85/120    avg_loss:0.007, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.018, val_acc:0.982]
Epoch [90/120    avg_loss:0.021, val_acc:0.973]
Epoch [91/120    avg_loss:0.034, val_acc:0.952]
Epoch [92/120    avg_loss:0.128, val_acc:0.964]
Epoch [93/120    avg_loss:0.050, val_acc:0.971]
Epoch [94/120    avg_loss:0.044, val_acc:0.971]
Epoch [95/120    avg_loss:0.028, val_acc:0.970]
Epoch [96/120    avg_loss:0.019, val_acc:0.979]
Epoch [97/120    avg_loss:0.014, val_acc:0.974]
Epoch [98/120    avg_loss:0.016, val_acc:0.972]
Epoch [99/120    avg_loss:0.015, val_acc:0.977]
Epoch [100/120    avg_loss:0.009, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.977]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.010, val_acc:0.977]
Epoch [104/120    avg_loss:0.011, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.977]
Epoch [106/120    avg_loss:0.008, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.977]
Epoch [109/120    avg_loss:0.008, val_acc:0.977]
Epoch [110/120    avg_loss:0.011, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.009, val_acc:0.977]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.010, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.023, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6387     0    15     3     0    11     0    15     1]
 [    0     0 18046     0    22     0    18     0     4     0]
 [    0     0     0  2013     2     0     0     0    17     4]
 [    0    30    20     0  2882     0    10     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     5    18     0     0  4839     0     0    16]
 [    0     0     0     0     0     0     5  1285     0     0]
 [    0     4     0     0    50     0     0     0  3490    27]
 [    0     0     0     1    14    50     0     0     0   854]]

Accuracy:
99.05526233340564

F1 scores:
[       nan 0.99385358 0.99809187 0.98603968 0.96955425 0.98120301
 0.99149677 0.99805825 0.97937421 0.93794618]

Kappa:
0.9874852680687174
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9db36ab898>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.089, val_acc:0.136]
Epoch [2/120    avg_loss:1.730, val_acc:0.254]
Epoch [3/120    avg_loss:1.481, val_acc:0.304]
Epoch [4/120    avg_loss:1.282, val_acc:0.355]
Epoch [5/120    avg_loss:1.152, val_acc:0.396]
Epoch [6/120    avg_loss:1.021, val_acc:0.665]
Epoch [7/120    avg_loss:0.920, val_acc:0.791]
Epoch [8/120    avg_loss:0.793, val_acc:0.809]
Epoch [9/120    avg_loss:0.667, val_acc:0.818]
Epoch [10/120    avg_loss:0.562, val_acc:0.830]
Epoch [11/120    avg_loss:0.508, val_acc:0.822]
Epoch [12/120    avg_loss:0.482, val_acc:0.847]
Epoch [13/120    avg_loss:0.401, val_acc:0.883]
Epoch [14/120    avg_loss:0.363, val_acc:0.913]
Epoch [15/120    avg_loss:0.286, val_acc:0.896]
Epoch [16/120    avg_loss:0.257, val_acc:0.899]
Epoch [17/120    avg_loss:0.232, val_acc:0.936]
Epoch [18/120    avg_loss:0.241, val_acc:0.938]
Epoch [19/120    avg_loss:0.184, val_acc:0.947]
Epoch [20/120    avg_loss:0.180, val_acc:0.954]
Epoch [21/120    avg_loss:0.183, val_acc:0.954]
Epoch [22/120    avg_loss:0.177, val_acc:0.948]
Epoch [23/120    avg_loss:0.139, val_acc:0.929]
Epoch [24/120    avg_loss:0.121, val_acc:0.949]
Epoch [25/120    avg_loss:0.107, val_acc:0.926]
Epoch [26/120    avg_loss:0.129, val_acc:0.951]
Epoch [27/120    avg_loss:0.105, val_acc:0.961]
Epoch [28/120    avg_loss:0.089, val_acc:0.960]
Epoch [29/120    avg_loss:0.089, val_acc:0.952]
Epoch [30/120    avg_loss:0.085, val_acc:0.978]
Epoch [31/120    avg_loss:0.060, val_acc:0.971]
Epoch [32/120    avg_loss:0.069, val_acc:0.965]
Epoch [33/120    avg_loss:0.073, val_acc:0.963]
Epoch [34/120    avg_loss:0.067, val_acc:0.977]
Epoch [35/120    avg_loss:0.064, val_acc:0.964]
Epoch [36/120    avg_loss:0.047, val_acc:0.962]
Epoch [37/120    avg_loss:0.047, val_acc:0.979]
Epoch [38/120    avg_loss:0.050, val_acc:0.977]
Epoch [39/120    avg_loss:0.035, val_acc:0.977]
Epoch [40/120    avg_loss:0.340, val_acc:0.847]
Epoch [41/120    avg_loss:0.250, val_acc:0.929]
Epoch [42/120    avg_loss:0.152, val_acc:0.950]
Epoch [43/120    avg_loss:0.105, val_acc:0.949]
Epoch [44/120    avg_loss:0.109, val_acc:0.969]
Epoch [45/120    avg_loss:0.077, val_acc:0.968]
Epoch [46/120    avg_loss:0.076, val_acc:0.947]
Epoch [47/120    avg_loss:0.086, val_acc:0.954]
Epoch [48/120    avg_loss:0.068, val_acc:0.963]
Epoch [49/120    avg_loss:0.086, val_acc:0.943]
Epoch [50/120    avg_loss:0.059, val_acc:0.966]
Epoch [51/120    avg_loss:0.043, val_acc:0.973]
Epoch [52/120    avg_loss:0.034, val_acc:0.973]
Epoch [53/120    avg_loss:0.029, val_acc:0.975]
Epoch [54/120    avg_loss:0.037, val_acc:0.974]
Epoch [55/120    avg_loss:0.033, val_acc:0.974]
Epoch [56/120    avg_loss:0.030, val_acc:0.974]
Epoch [57/120    avg_loss:0.036, val_acc:0.977]
Epoch [58/120    avg_loss:0.032, val_acc:0.975]
Epoch [59/120    avg_loss:0.028, val_acc:0.975]
Epoch [60/120    avg_loss:0.027, val_acc:0.976]
Epoch [61/120    avg_loss:0.026, val_acc:0.975]
Epoch [62/120    avg_loss:0.032, val_acc:0.976]
Epoch [63/120    avg_loss:0.026, val_acc:0.976]
Epoch [64/120    avg_loss:0.032, val_acc:0.976]
Epoch [65/120    avg_loss:0.026, val_acc:0.975]
Epoch [66/120    avg_loss:0.042, val_acc:0.976]
Epoch [67/120    avg_loss:0.026, val_acc:0.976]
Epoch [68/120    avg_loss:0.030, val_acc:0.975]
Epoch [69/120    avg_loss:0.033, val_acc:0.975]
Epoch [70/120    avg_loss:0.029, val_acc:0.975]
Epoch [71/120    avg_loss:0.024, val_acc:0.975]
Epoch [72/120    avg_loss:0.023, val_acc:0.975]
Epoch [73/120    avg_loss:0.030, val_acc:0.975]
Epoch [74/120    avg_loss:0.024, val_acc:0.975]
Epoch [75/120    avg_loss:0.027, val_acc:0.975]
Epoch [76/120    avg_loss:0.032, val_acc:0.975]
Epoch [77/120    avg_loss:0.027, val_acc:0.975]
Epoch [78/120    avg_loss:0.025, val_acc:0.975]
Epoch [79/120    avg_loss:0.021, val_acc:0.975]
Epoch [80/120    avg_loss:0.028, val_acc:0.975]
Epoch [81/120    avg_loss:0.027, val_acc:0.975]
Epoch [82/120    avg_loss:0.028, val_acc:0.975]
Epoch [83/120    avg_loss:0.029, val_acc:0.975]
Epoch [84/120    avg_loss:0.031, val_acc:0.975]
Epoch [85/120    avg_loss:0.024, val_acc:0.975]
Epoch [86/120    avg_loss:0.025, val_acc:0.975]
Epoch [87/120    avg_loss:0.029, val_acc:0.975]
Epoch [88/120    avg_loss:0.025, val_acc:0.975]
Epoch [89/120    avg_loss:0.030, val_acc:0.975]
Epoch [90/120    avg_loss:0.029, val_acc:0.975]
Epoch [91/120    avg_loss:0.031, val_acc:0.975]
Epoch [92/120    avg_loss:0.030, val_acc:0.975]
Epoch [93/120    avg_loss:0.030, val_acc:0.975]
Epoch [94/120    avg_loss:0.030, val_acc:0.975]
Epoch [95/120    avg_loss:0.026, val_acc:0.975]
Epoch [96/120    avg_loss:0.026, val_acc:0.975]
Epoch [97/120    avg_loss:0.032, val_acc:0.975]
Epoch [98/120    avg_loss:0.028, val_acc:0.975]
Epoch [99/120    avg_loss:0.028, val_acc:0.975]
Epoch [100/120    avg_loss:0.023, val_acc:0.975]
Epoch [101/120    avg_loss:0.030, val_acc:0.975]
Epoch [102/120    avg_loss:0.028, val_acc:0.975]
Epoch [103/120    avg_loss:0.029, val_acc:0.975]
Epoch [104/120    avg_loss:0.028, val_acc:0.975]
Epoch [105/120    avg_loss:0.030, val_acc:0.975]
Epoch [106/120    avg_loss:0.025, val_acc:0.975]
Epoch [107/120    avg_loss:0.025, val_acc:0.975]
Epoch [108/120    avg_loss:0.032, val_acc:0.975]
Epoch [109/120    avg_loss:0.028, val_acc:0.975]
Epoch [110/120    avg_loss:0.029, val_acc:0.975]
Epoch [111/120    avg_loss:0.024, val_acc:0.975]
Epoch [112/120    avg_loss:0.029, val_acc:0.975]
Epoch [113/120    avg_loss:0.031, val_acc:0.975]
Epoch [114/120    avg_loss:0.027, val_acc:0.975]
Epoch [115/120    avg_loss:0.026, val_acc:0.975]
Epoch [116/120    avg_loss:0.028, val_acc:0.975]
Epoch [117/120    avg_loss:0.025, val_acc:0.975]
Epoch [118/120    avg_loss:0.026, val_acc:0.975]
Epoch [119/120    avg_loss:0.032, val_acc:0.975]
Epoch [120/120    avg_loss:0.029, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6380     0    13    20     0     2     1    16     0]
 [    0     0 18017     0    39     0    34     0     0     0]
 [    0    13     0  1972     0     0     0     0    50     1]
 [    0    66    18     0  2851     0     0     0    36     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3    14     0     0  4856     0     5     0]
 [    0     0     0     0     0     0     0  1286     0     4]
 [    0     8     0     2    89     0     0     0  3470     2]
 [    0     0     0     9    18    67     0     2     0   823]]

Accuracy:
98.71544597883981

F1 scores:
[       nan 0.98922397 0.99739814 0.97478992 0.95207881 0.97497198
 0.99406346 0.99728577 0.97090095 0.94057143]

Kappa:
0.982985951325731
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fade872a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.984, val_acc:0.233]
Epoch [2/120    avg_loss:1.610, val_acc:0.383]
Epoch [3/120    avg_loss:1.428, val_acc:0.524]
Epoch [4/120    avg_loss:1.257, val_acc:0.444]
Epoch [5/120    avg_loss:1.108, val_acc:0.530]
Epoch [6/120    avg_loss:0.973, val_acc:0.625]
Epoch [7/120    avg_loss:0.863, val_acc:0.609]
Epoch [8/120    avg_loss:0.743, val_acc:0.766]
Epoch [9/120    avg_loss:0.636, val_acc:0.751]
Epoch [10/120    avg_loss:0.537, val_acc:0.809]
Epoch [11/120    avg_loss:0.476, val_acc:0.844]
Epoch [12/120    avg_loss:0.363, val_acc:0.836]
Epoch [13/120    avg_loss:0.323, val_acc:0.873]
Epoch [14/120    avg_loss:0.329, val_acc:0.914]
Epoch [15/120    avg_loss:0.269, val_acc:0.900]
Epoch [16/120    avg_loss:0.305, val_acc:0.837]
Epoch [17/120    avg_loss:0.227, val_acc:0.880]
Epoch [18/120    avg_loss:0.228, val_acc:0.894]
Epoch [19/120    avg_loss:0.202, val_acc:0.898]
Epoch [20/120    avg_loss:0.202, val_acc:0.905]
Epoch [21/120    avg_loss:0.160, val_acc:0.928]
Epoch [22/120    avg_loss:0.151, val_acc:0.889]
Epoch [23/120    avg_loss:0.153, val_acc:0.946]
Epoch [24/120    avg_loss:0.122, val_acc:0.956]
Epoch [25/120    avg_loss:0.092, val_acc:0.967]
Epoch [26/120    avg_loss:0.102, val_acc:0.944]
Epoch [27/120    avg_loss:0.099, val_acc:0.933]
Epoch [28/120    avg_loss:0.099, val_acc:0.953]
Epoch [29/120    avg_loss:0.125, val_acc:0.960]
Epoch [30/120    avg_loss:0.120, val_acc:0.957]
Epoch [31/120    avg_loss:0.097, val_acc:0.967]
Epoch [32/120    avg_loss:0.080, val_acc:0.970]
Epoch [33/120    avg_loss:0.074, val_acc:0.965]
Epoch [34/120    avg_loss:0.085, val_acc:0.961]
Epoch [35/120    avg_loss:0.078, val_acc:0.974]
Epoch [36/120    avg_loss:0.064, val_acc:0.969]
Epoch [37/120    avg_loss:0.063, val_acc:0.935]
Epoch [38/120    avg_loss:0.081, val_acc:0.971]
Epoch [39/120    avg_loss:0.066, val_acc:0.970]
Epoch [40/120    avg_loss:0.067, val_acc:0.968]
Epoch [41/120    avg_loss:0.064, val_acc:0.972]
Epoch [42/120    avg_loss:0.060, val_acc:0.980]
Epoch [43/120    avg_loss:0.055, val_acc:0.970]
Epoch [44/120    avg_loss:0.040, val_acc:0.980]
Epoch [45/120    avg_loss:0.033, val_acc:0.964]
Epoch [46/120    avg_loss:0.031, val_acc:0.977]
Epoch [47/120    avg_loss:0.029, val_acc:0.977]
Epoch [48/120    avg_loss:0.035, val_acc:0.984]
Epoch [49/120    avg_loss:0.030, val_acc:0.954]
Epoch [50/120    avg_loss:0.032, val_acc:0.985]
Epoch [51/120    avg_loss:0.030, val_acc:0.979]
Epoch [52/120    avg_loss:0.033, val_acc:0.981]
Epoch [53/120    avg_loss:0.031, val_acc:0.984]
Epoch [54/120    avg_loss:0.023, val_acc:0.985]
Epoch [55/120    avg_loss:0.022, val_acc:0.987]
Epoch [56/120    avg_loss:0.014, val_acc:0.990]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.022, val_acc:0.985]
Epoch [59/120    avg_loss:0.019, val_acc:0.970]
Epoch [60/120    avg_loss:0.026, val_acc:0.979]
Epoch [61/120    avg_loss:0.033, val_acc:0.979]
Epoch [62/120    avg_loss:0.025, val_acc:0.987]
Epoch [63/120    avg_loss:0.019, val_acc:0.989]
Epoch [64/120    avg_loss:0.018, val_acc:0.984]
Epoch [65/120    avg_loss:0.022, val_acc:0.986]
Epoch [66/120    avg_loss:0.038, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.983]
Epoch [68/120    avg_loss:0.016, val_acc:0.986]
Epoch [69/120    avg_loss:0.020, val_acc:0.987]
Epoch [70/120    avg_loss:0.016, val_acc:0.990]
Epoch [71/120    avg_loss:0.011, val_acc:0.990]
Epoch [72/120    avg_loss:0.012, val_acc:0.990]
Epoch [73/120    avg_loss:0.011, val_acc:0.990]
Epoch [74/120    avg_loss:0.011, val_acc:0.990]
Epoch [75/120    avg_loss:0.011, val_acc:0.990]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.010, val_acc:0.990]
Epoch [78/120    avg_loss:0.010, val_acc:0.989]
Epoch [79/120    avg_loss:0.011, val_acc:0.990]
Epoch [80/120    avg_loss:0.013, val_acc:0.989]
Epoch [81/120    avg_loss:0.010, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.990]
Epoch [83/120    avg_loss:0.009, val_acc:0.990]
Epoch [84/120    avg_loss:0.009, val_acc:0.990]
Epoch [85/120    avg_loss:0.010, val_acc:0.990]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.008, val_acc:0.990]
Epoch [88/120    avg_loss:0.008, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.990]
Epoch [92/120    avg_loss:0.011, val_acc:0.992]
Epoch [93/120    avg_loss:0.009, val_acc:0.990]
Epoch [94/120    avg_loss:0.009, val_acc:0.991]
Epoch [95/120    avg_loss:0.008, val_acc:0.991]
Epoch [96/120    avg_loss:0.009, val_acc:0.991]
Epoch [97/120    avg_loss:0.007, val_acc:0.991]
Epoch [98/120    avg_loss:0.010, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.991]
Epoch [100/120    avg_loss:0.008, val_acc:0.991]
Epoch [101/120    avg_loss:0.008, val_acc:0.991]
Epoch [102/120    avg_loss:0.009, val_acc:0.991]
Epoch [103/120    avg_loss:0.008, val_acc:0.991]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.992]
Epoch [106/120    avg_loss:0.009, val_acc:0.991]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.007, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.006, val_acc:0.992]
Epoch [115/120    avg_loss:0.010, val_acc:0.991]
Epoch [116/120    avg_loss:0.008, val_acc:0.991]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6409     0     0     1     0    16     1     5     0]
 [    0     0 18052     0    29     0     9     0     0     0]
 [    0     8     0  2018     0     0     0     0     8     2]
 [    0    21    19     0  2895     0     8     0    28     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     0     0     0  4861     0     3    13]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     3     0    11    72     0     0     0  3462    23]
 [    0     0     0     0    19    48     0     0     0   852]]

Accuracy:
99.15889427132288

F1 scores:
[       nan 0.99572749 0.99839611 0.99286593 0.96693387 0.98194131
 0.99488334 0.99961255 0.97838067 0.94143646]

Kappa:
0.9888571315438237
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e52bca908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.048, val_acc:0.472]
Epoch [2/120    avg_loss:1.708, val_acc:0.576]
Epoch [3/120    avg_loss:1.460, val_acc:0.569]
Epoch [4/120    avg_loss:1.227, val_acc:0.668]
Epoch [5/120    avg_loss:1.122, val_acc:0.719]
Epoch [6/120    avg_loss:0.970, val_acc:0.724]
Epoch [7/120    avg_loss:0.862, val_acc:0.768]
Epoch [8/120    avg_loss:0.744, val_acc:0.772]
Epoch [9/120    avg_loss:0.608, val_acc:0.798]
Epoch [10/120    avg_loss:0.527, val_acc:0.797]
Epoch [11/120    avg_loss:0.428, val_acc:0.829]
Epoch [12/120    avg_loss:0.387, val_acc:0.883]
Epoch [13/120    avg_loss:0.351, val_acc:0.860]
Epoch [14/120    avg_loss:0.312, val_acc:0.917]
Epoch [15/120    avg_loss:0.255, val_acc:0.918]
Epoch [16/120    avg_loss:0.235, val_acc:0.939]
Epoch [17/120    avg_loss:0.215, val_acc:0.954]
Epoch [18/120    avg_loss:0.224, val_acc:0.934]
Epoch [19/120    avg_loss:0.224, val_acc:0.950]
Epoch [20/120    avg_loss:0.180, val_acc:0.930]
Epoch [21/120    avg_loss:0.161, val_acc:0.931]
Epoch [22/120    avg_loss:0.181, val_acc:0.924]
Epoch [23/120    avg_loss:0.169, val_acc:0.934]
Epoch [24/120    avg_loss:0.145, val_acc:0.955]
Epoch [25/120    avg_loss:0.117, val_acc:0.957]
Epoch [26/120    avg_loss:0.116, val_acc:0.926]
Epoch [27/120    avg_loss:0.109, val_acc:0.957]
Epoch [28/120    avg_loss:0.102, val_acc:0.952]
Epoch [29/120    avg_loss:0.094, val_acc:0.949]
Epoch [30/120    avg_loss:0.103, val_acc:0.965]
Epoch [31/120    avg_loss:0.090, val_acc:0.949]
Epoch [32/120    avg_loss:0.084, val_acc:0.970]
Epoch [33/120    avg_loss:0.098, val_acc:0.962]
Epoch [34/120    avg_loss:0.097, val_acc:0.961]
Epoch [35/120    avg_loss:0.090, val_acc:0.975]
Epoch [36/120    avg_loss:0.099, val_acc:0.944]
Epoch [37/120    avg_loss:0.071, val_acc:0.983]
Epoch [38/120    avg_loss:0.076, val_acc:0.972]
Epoch [39/120    avg_loss:0.063, val_acc:0.975]
Epoch [40/120    avg_loss:0.050, val_acc:0.986]
Epoch [41/120    avg_loss:0.058, val_acc:0.975]
Epoch [42/120    avg_loss:0.057, val_acc:0.967]
Epoch [43/120    avg_loss:0.039, val_acc:0.980]
Epoch [44/120    avg_loss:0.029, val_acc:0.987]
Epoch [45/120    avg_loss:0.028, val_acc:0.987]
Epoch [46/120    avg_loss:0.029, val_acc:0.986]
Epoch [47/120    avg_loss:0.028, val_acc:0.976]
Epoch [48/120    avg_loss:0.031, val_acc:0.986]
Epoch [49/120    avg_loss:0.025, val_acc:0.989]
Epoch [50/120    avg_loss:0.024, val_acc:0.992]
Epoch [51/120    avg_loss:0.027, val_acc:0.977]
Epoch [52/120    avg_loss:0.020, val_acc:0.988]
Epoch [53/120    avg_loss:0.029, val_acc:0.987]
Epoch [54/120    avg_loss:0.023, val_acc:0.987]
Epoch [55/120    avg_loss:0.025, val_acc:0.989]
Epoch [56/120    avg_loss:0.035, val_acc:0.985]
Epoch [57/120    avg_loss:0.116, val_acc:0.967]
Epoch [58/120    avg_loss:0.046, val_acc:0.984]
Epoch [59/120    avg_loss:0.028, val_acc:0.984]
Epoch [60/120    avg_loss:0.045, val_acc:0.967]
Epoch [61/120    avg_loss:0.033, val_acc:0.984]
Epoch [62/120    avg_loss:0.027, val_acc:0.978]
Epoch [63/120    avg_loss:0.029, val_acc:0.977]
Epoch [64/120    avg_loss:0.015, val_acc:0.984]
Epoch [65/120    avg_loss:0.017, val_acc:0.986]
Epoch [66/120    avg_loss:0.015, val_acc:0.989]
Epoch [67/120    avg_loss:0.014, val_acc:0.990]
Epoch [68/120    avg_loss:0.013, val_acc:0.991]
Epoch [69/120    avg_loss:0.017, val_acc:0.990]
Epoch [70/120    avg_loss:0.017, val_acc:0.990]
Epoch [71/120    avg_loss:0.013, val_acc:0.991]
Epoch [72/120    avg_loss:0.013, val_acc:0.991]
Epoch [73/120    avg_loss:0.013, val_acc:0.990]
Epoch [74/120    avg_loss:0.013, val_acc:0.990]
Epoch [75/120    avg_loss:0.013, val_acc:0.990]
Epoch [76/120    avg_loss:0.013, val_acc:0.991]
Epoch [77/120    avg_loss:0.011, val_acc:0.991]
Epoch [78/120    avg_loss:0.014, val_acc:0.991]
Epoch [79/120    avg_loss:0.014, val_acc:0.991]
Epoch [80/120    avg_loss:0.011, val_acc:0.991]
Epoch [81/120    avg_loss:0.013, val_acc:0.991]
Epoch [82/120    avg_loss:0.016, val_acc:0.990]
Epoch [83/120    avg_loss:0.011, val_acc:0.991]
Epoch [84/120    avg_loss:0.010, val_acc:0.990]
Epoch [85/120    avg_loss:0.014, val_acc:0.990]
Epoch [86/120    avg_loss:0.012, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.991]
Epoch [88/120    avg_loss:0.010, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.991]
Epoch [90/120    avg_loss:0.013, val_acc:0.991]
Epoch [91/120    avg_loss:0.011, val_acc:0.991]
Epoch [92/120    avg_loss:0.014, val_acc:0.991]
Epoch [93/120    avg_loss:0.014, val_acc:0.991]
Epoch [94/120    avg_loss:0.014, val_acc:0.991]
Epoch [95/120    avg_loss:0.013, val_acc:0.991]
Epoch [96/120    avg_loss:0.012, val_acc:0.991]
Epoch [97/120    avg_loss:0.013, val_acc:0.991]
Epoch [98/120    avg_loss:0.011, val_acc:0.991]
Epoch [99/120    avg_loss:0.014, val_acc:0.991]
Epoch [100/120    avg_loss:0.011, val_acc:0.991]
Epoch [101/120    avg_loss:0.011, val_acc:0.991]
Epoch [102/120    avg_loss:0.013, val_acc:0.991]
Epoch [103/120    avg_loss:0.014, val_acc:0.991]
Epoch [104/120    avg_loss:0.014, val_acc:0.991]
Epoch [105/120    avg_loss:0.013, val_acc:0.991]
Epoch [106/120    avg_loss:0.012, val_acc:0.991]
Epoch [107/120    avg_loss:0.012, val_acc:0.991]
Epoch [108/120    avg_loss:0.011, val_acc:0.991]
Epoch [109/120    avg_loss:0.010, val_acc:0.991]
Epoch [110/120    avg_loss:0.014, val_acc:0.991]
Epoch [111/120    avg_loss:0.011, val_acc:0.991]
Epoch [112/120    avg_loss:0.016, val_acc:0.991]
Epoch [113/120    avg_loss:0.014, val_acc:0.991]
Epoch [114/120    avg_loss:0.013, val_acc:0.991]
Epoch [115/120    avg_loss:0.011, val_acc:0.991]
Epoch [116/120    avg_loss:0.011, val_acc:0.991]
Epoch [117/120    avg_loss:0.015, val_acc:0.991]
Epoch [118/120    avg_loss:0.014, val_acc:0.991]
Epoch [119/120    avg_loss:0.013, val_acc:0.991]
Epoch [120/120    avg_loss:0.011, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6327     0     0     0     0     8     2    95     0]
 [    0     0 18056     0    29     0     5     0     0     0]
 [    0     0     0  2031     0     0     0     0     4     1]
 [    0    33    22     0  2881     0     1     0    33     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     1     3     0     0  4854     0     0    20]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0     2     0     5    64     0     0     0  3479    21]
 [    0     0     0     0    15    54     0     0     0   850]]

Accuracy:
98.98778107150603

F1 scores:
[       nan 0.98905737 0.99842406 0.99680982 0.96661634 0.97972973
 0.99610096 0.99922541 0.96881092 0.93767237]

Kappa:
0.9865921155569749
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4929f0e940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.057, val_acc:0.125]
Epoch [2/120    avg_loss:1.705, val_acc:0.264]
Epoch [3/120    avg_loss:1.471, val_acc:0.316]
Epoch [4/120    avg_loss:1.288, val_acc:0.344]
Epoch [5/120    avg_loss:1.177, val_acc:0.414]
Epoch [6/120    avg_loss:1.058, val_acc:0.432]
Epoch [7/120    avg_loss:0.965, val_acc:0.486]
Epoch [8/120    avg_loss:0.800, val_acc:0.559]
Epoch [9/120    avg_loss:0.721, val_acc:0.629]
Epoch [10/120    avg_loss:0.620, val_acc:0.661]
Epoch [11/120    avg_loss:0.588, val_acc:0.651]
Epoch [12/120    avg_loss:0.534, val_acc:0.681]
Epoch [13/120    avg_loss:0.484, val_acc:0.673]
Epoch [14/120    avg_loss:0.487, val_acc:0.693]
Epoch [15/120    avg_loss:0.438, val_acc:0.721]
Epoch [16/120    avg_loss:0.383, val_acc:0.767]
Epoch [17/120    avg_loss:0.337, val_acc:0.776]
Epoch [18/120    avg_loss:0.309, val_acc:0.849]
Epoch [19/120    avg_loss:0.287, val_acc:0.865]
Epoch [20/120    avg_loss:0.271, val_acc:0.882]
Epoch [21/120    avg_loss:0.257, val_acc:0.894]
Epoch [22/120    avg_loss:0.218, val_acc:0.930]
Epoch [23/120    avg_loss:0.184, val_acc:0.914]
Epoch [24/120    avg_loss:0.169, val_acc:0.935]
Epoch [25/120    avg_loss:0.141, val_acc:0.924]
Epoch [26/120    avg_loss:0.138, val_acc:0.925]
Epoch [27/120    avg_loss:0.134, val_acc:0.944]
Epoch [28/120    avg_loss:0.143, val_acc:0.945]
Epoch [29/120    avg_loss:0.114, val_acc:0.936]
Epoch [30/120    avg_loss:0.107, val_acc:0.955]
Epoch [31/120    avg_loss:0.086, val_acc:0.963]
Epoch [32/120    avg_loss:0.094, val_acc:0.951]
Epoch [33/120    avg_loss:0.102, val_acc:0.957]
Epoch [34/120    avg_loss:0.100, val_acc:0.946]
Epoch [35/120    avg_loss:0.083, val_acc:0.958]
Epoch [36/120    avg_loss:0.079, val_acc:0.957]
Epoch [37/120    avg_loss:0.072, val_acc:0.963]
Epoch [38/120    avg_loss:0.086, val_acc:0.936]
Epoch [39/120    avg_loss:0.070, val_acc:0.958]
Epoch [40/120    avg_loss:0.193, val_acc:0.935]
Epoch [41/120    avg_loss:0.105, val_acc:0.951]
Epoch [42/120    avg_loss:0.075, val_acc:0.966]
Epoch [43/120    avg_loss:0.063, val_acc:0.951]
Epoch [44/120    avg_loss:0.062, val_acc:0.971]
Epoch [45/120    avg_loss:0.048, val_acc:0.966]
Epoch [46/120    avg_loss:0.048, val_acc:0.966]
Epoch [47/120    avg_loss:0.050, val_acc:0.972]
Epoch [48/120    avg_loss:0.039, val_acc:0.975]
Epoch [49/120    avg_loss:0.039, val_acc:0.973]
Epoch [50/120    avg_loss:0.030, val_acc:0.978]
Epoch [51/120    avg_loss:0.026, val_acc:0.974]
Epoch [52/120    avg_loss:0.022, val_acc:0.965]
Epoch [53/120    avg_loss:0.030, val_acc:0.977]
Epoch [54/120    avg_loss:0.026, val_acc:0.977]
Epoch [55/120    avg_loss:0.027, val_acc:0.977]
Epoch [56/120    avg_loss:0.026, val_acc:0.973]
Epoch [57/120    avg_loss:0.024, val_acc:0.977]
Epoch [58/120    avg_loss:0.017, val_acc:0.977]
Epoch [59/120    avg_loss:0.026, val_acc:0.971]
Epoch [60/120    avg_loss:0.026, val_acc:0.975]
Epoch [61/120    avg_loss:0.024, val_acc:0.977]
Epoch [62/120    avg_loss:0.020, val_acc:0.977]
Epoch [63/120    avg_loss:0.026, val_acc:0.975]
Epoch [64/120    avg_loss:0.019, val_acc:0.978]
Epoch [65/120    avg_loss:0.015, val_acc:0.977]
Epoch [66/120    avg_loss:0.015, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.977]
Epoch [69/120    avg_loss:0.013, val_acc:0.979]
Epoch [70/120    avg_loss:0.012, val_acc:0.979]
Epoch [71/120    avg_loss:0.013, val_acc:0.979]
Epoch [72/120    avg_loss:0.013, val_acc:0.980]
Epoch [73/120    avg_loss:0.012, val_acc:0.979]
Epoch [74/120    avg_loss:0.012, val_acc:0.979]
Epoch [75/120    avg_loss:0.013, val_acc:0.979]
Epoch [76/120    avg_loss:0.016, val_acc:0.981]
Epoch [77/120    avg_loss:0.016, val_acc:0.980]
Epoch [78/120    avg_loss:0.010, val_acc:0.979]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.981]
Epoch [83/120    avg_loss:0.014, val_acc:0.980]
Epoch [84/120    avg_loss:0.013, val_acc:0.979]
Epoch [85/120    avg_loss:0.011, val_acc:0.982]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.014, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     6     0     0     0    26     0]
 [    0     3 18052     0    29     0     5     0     1     0]
 [    0     1     0  2014     3     0     0     0    14     4]
 [    0    43    18     0  2872     0     9     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     4     0     0  4856     0     0    18]
 [    0     0     0     0     0     0     3  1284     0     3]
 [    0     5     0     2    54     0     0     0  3481    29]
 [    0     1     0     0    16    61     0     0     0   841]]

Accuracy:
99.06490251367701

F1 scores:
[       nan 0.99340318 0.99845133 0.99309665 0.96505376 0.97716211
 0.99600041 0.997669   0.97739716 0.92723264]

Kappa:
0.9876120435842278
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f00ae9fe940>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.099, val_acc:0.200]
Epoch [2/120    avg_loss:1.711, val_acc:0.286]
Epoch [3/120    avg_loss:1.468, val_acc:0.368]
Epoch [4/120    avg_loss:1.301, val_acc:0.454]
Epoch [5/120    avg_loss:1.109, val_acc:0.562]
Epoch [6/120    avg_loss:0.920, val_acc:0.671]
Epoch [7/120    avg_loss:0.782, val_acc:0.702]
Epoch [8/120    avg_loss:0.652, val_acc:0.766]
Epoch [9/120    avg_loss:0.560, val_acc:0.759]
Epoch [10/120    avg_loss:0.468, val_acc:0.770]
Epoch [11/120    avg_loss:0.429, val_acc:0.788]
Epoch [12/120    avg_loss:0.380, val_acc:0.778]
Epoch [13/120    avg_loss:0.358, val_acc:0.813]
Epoch [14/120    avg_loss:0.335, val_acc:0.804]
Epoch [15/120    avg_loss:0.296, val_acc:0.848]
Epoch [16/120    avg_loss:0.236, val_acc:0.826]
Epoch [17/120    avg_loss:0.276, val_acc:0.793]
Epoch [18/120    avg_loss:0.240, val_acc:0.922]
Epoch [19/120    avg_loss:0.203, val_acc:0.915]
Epoch [20/120    avg_loss:0.169, val_acc:0.922]
Epoch [21/120    avg_loss:0.161, val_acc:0.940]
Epoch [22/120    avg_loss:0.190, val_acc:0.931]
Epoch [23/120    avg_loss:0.185, val_acc:0.931]
Epoch [24/120    avg_loss:0.155, val_acc:0.938]
Epoch [25/120    avg_loss:0.132, val_acc:0.910]
Epoch [26/120    avg_loss:0.137, val_acc:0.959]
Epoch [27/120    avg_loss:0.097, val_acc:0.959]
Epoch [28/120    avg_loss:0.102, val_acc:0.886]
Epoch [29/120    avg_loss:0.133, val_acc:0.912]
Epoch [30/120    avg_loss:0.124, val_acc:0.931]
Epoch [31/120    avg_loss:0.104, val_acc:0.957]
Epoch [32/120    avg_loss:0.075, val_acc:0.958]
Epoch [33/120    avg_loss:0.081, val_acc:0.967]
Epoch [34/120    avg_loss:0.057, val_acc:0.964]
Epoch [35/120    avg_loss:0.053, val_acc:0.973]
Epoch [36/120    avg_loss:0.061, val_acc:0.977]
Epoch [37/120    avg_loss:0.062, val_acc:0.978]
Epoch [38/120    avg_loss:0.107, val_acc:0.963]
Epoch [39/120    avg_loss:0.079, val_acc:0.957]
Epoch [40/120    avg_loss:0.059, val_acc:0.966]
Epoch [41/120    avg_loss:0.067, val_acc:0.970]
Epoch [42/120    avg_loss:0.032, val_acc:0.977]
Epoch [43/120    avg_loss:0.035, val_acc:0.975]
Epoch [44/120    avg_loss:0.039, val_acc:0.985]
Epoch [45/120    avg_loss:0.049, val_acc:0.979]
Epoch [46/120    avg_loss:0.036, val_acc:0.976]
Epoch [47/120    avg_loss:0.030, val_acc:0.977]
Epoch [48/120    avg_loss:0.038, val_acc:0.970]
Epoch [49/120    avg_loss:0.043, val_acc:0.979]
Epoch [50/120    avg_loss:0.027, val_acc:0.984]
Epoch [51/120    avg_loss:0.019, val_acc:0.985]
Epoch [52/120    avg_loss:0.036, val_acc:0.970]
Epoch [53/120    avg_loss:0.024, val_acc:0.979]
Epoch [54/120    avg_loss:0.021, val_acc:0.977]
Epoch [55/120    avg_loss:0.026, val_acc:0.988]
Epoch [56/120    avg_loss:0.015, val_acc:0.986]
Epoch [57/120    avg_loss:0.018, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.986]
Epoch [59/120    avg_loss:0.030, val_acc:0.972]
Epoch [60/120    avg_loss:0.041, val_acc:0.982]
Epoch [61/120    avg_loss:0.029, val_acc:0.981]
Epoch [62/120    avg_loss:0.021, val_acc:0.984]
Epoch [63/120    avg_loss:0.019, val_acc:0.983]
Epoch [64/120    avg_loss:0.018, val_acc:0.981]
Epoch [65/120    avg_loss:0.022, val_acc:0.984]
Epoch [66/120    avg_loss:0.034, val_acc:0.968]
Epoch [67/120    avg_loss:0.035, val_acc:0.977]
Epoch [68/120    avg_loss:0.017, val_acc:0.981]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.011, val_acc:0.986]
Epoch [73/120    avg_loss:0.011, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.987]
Epoch [75/120    avg_loss:0.014, val_acc:0.985]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.010, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.008, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.008, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.988]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.989]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.012, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.008, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.010, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6354     0    10     0     0    17     1    50     0]
 [    0     1 18017     0    52     0    15     0     5     0]
 [    0     2     0  2007     1     0     0     0    25     1]
 [    0    34     9     0  2904     0     7     0    18     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0    12     0     0  4858     0     8     0]
 [    0     1     0     0     0     0     0  1289     0     0]
 [    0    27     0    17    39     0     0     0  3478    10]
 [    0     0     0     8    15    51     0     0     0   845]]

Accuracy:
98.94922035042056

F1 scores:
[       nan 0.98887246 0.99772954 0.98141809 0.97075046 0.98083427
 0.99396419 0.99922481 0.97218728 0.95211268]

Kappa:
0.9860866120113079
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f523a2c2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:1.963, val_acc:0.129]
Epoch [2/120    avg_loss:1.646, val_acc:0.293]
Epoch [3/120    avg_loss:1.429, val_acc:0.380]
Epoch [4/120    avg_loss:1.223, val_acc:0.441]
Epoch [5/120    avg_loss:1.083, val_acc:0.494]
Epoch [6/120    avg_loss:0.914, val_acc:0.572]
Epoch [7/120    avg_loss:0.806, val_acc:0.771]
Epoch [8/120    avg_loss:0.688, val_acc:0.766]
Epoch [9/120    avg_loss:0.570, val_acc:0.802]
Epoch [10/120    avg_loss:0.527, val_acc:0.820]
Epoch [11/120    avg_loss:0.418, val_acc:0.851]
Epoch [12/120    avg_loss:0.371, val_acc:0.864]
Epoch [13/120    avg_loss:0.318, val_acc:0.899]
Epoch [14/120    avg_loss:0.312, val_acc:0.904]
Epoch [15/120    avg_loss:0.268, val_acc:0.891]
Epoch [16/120    avg_loss:0.228, val_acc:0.918]
Epoch [17/120    avg_loss:0.201, val_acc:0.918]
Epoch [18/120    avg_loss:0.223, val_acc:0.904]
Epoch [19/120    avg_loss:0.243, val_acc:0.914]
Epoch [20/120    avg_loss:0.201, val_acc:0.941]
Epoch [21/120    avg_loss:0.170, val_acc:0.939]
Epoch [22/120    avg_loss:0.168, val_acc:0.922]
Epoch [23/120    avg_loss:0.153, val_acc:0.941]
Epoch [24/120    avg_loss:0.142, val_acc:0.919]
Epoch [25/120    avg_loss:0.133, val_acc:0.939]
Epoch [26/120    avg_loss:0.128, val_acc:0.924]
Epoch [27/120    avg_loss:0.112, val_acc:0.931]
Epoch [28/120    avg_loss:0.102, val_acc:0.966]
Epoch [29/120    avg_loss:0.117, val_acc:0.932]
Epoch [30/120    avg_loss:0.103, val_acc:0.930]
Epoch [31/120    avg_loss:0.111, val_acc:0.952]
Epoch [32/120    avg_loss:0.100, val_acc:0.953]
Epoch [33/120    avg_loss:0.083, val_acc:0.952]
Epoch [34/120    avg_loss:0.071, val_acc:0.973]
Epoch [35/120    avg_loss:0.057, val_acc:0.972]
Epoch [36/120    avg_loss:0.068, val_acc:0.960]
Epoch [37/120    avg_loss:0.050, val_acc:0.964]
Epoch [38/120    avg_loss:0.123, val_acc:0.932]
Epoch [39/120    avg_loss:0.111, val_acc:0.938]
Epoch [40/120    avg_loss:0.092, val_acc:0.968]
Epoch [41/120    avg_loss:0.078, val_acc:0.950]
Epoch [42/120    avg_loss:0.094, val_acc:0.938]
Epoch [43/120    avg_loss:0.082, val_acc:0.939]
Epoch [44/120    avg_loss:0.091, val_acc:0.961]
Epoch [45/120    avg_loss:0.061, val_acc:0.969]
Epoch [46/120    avg_loss:0.069, val_acc:0.974]
Epoch [47/120    avg_loss:0.050, val_acc:0.970]
Epoch [48/120    avg_loss:0.045, val_acc:0.975]
Epoch [49/120    avg_loss:0.032, val_acc:0.975]
Epoch [50/120    avg_loss:0.056, val_acc:0.969]
Epoch [51/120    avg_loss:0.050, val_acc:0.972]
Epoch [52/120    avg_loss:0.046, val_acc:0.969]
Epoch [53/120    avg_loss:0.035, val_acc:0.970]
Epoch [54/120    avg_loss:0.047, val_acc:0.979]
Epoch [55/120    avg_loss:0.046, val_acc:0.974]
Epoch [56/120    avg_loss:0.037, val_acc:0.982]
Epoch [57/120    avg_loss:0.026, val_acc:0.978]
Epoch [58/120    avg_loss:0.021, val_acc:0.984]
Epoch [59/120    avg_loss:0.023, val_acc:0.984]
Epoch [60/120    avg_loss:0.016, val_acc:0.986]
Epoch [61/120    avg_loss:0.020, val_acc:0.975]
Epoch [62/120    avg_loss:0.038, val_acc:0.980]
Epoch [63/120    avg_loss:0.020, val_acc:0.982]
Epoch [64/120    avg_loss:0.017, val_acc:0.986]
Epoch [65/120    avg_loss:0.023, val_acc:0.984]
Epoch [66/120    avg_loss:0.024, val_acc:0.982]
Epoch [67/120    avg_loss:0.020, val_acc:0.973]
Epoch [68/120    avg_loss:0.024, val_acc:0.985]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.984]
Epoch [71/120    avg_loss:0.015, val_acc:0.985]
Epoch [72/120    avg_loss:0.023, val_acc:0.984]
Epoch [73/120    avg_loss:0.022, val_acc:0.983]
Epoch [74/120    avg_loss:0.018, val_acc:0.982]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.021, val_acc:0.983]
Epoch [78/120    avg_loss:0.015, val_acc:0.983]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.989]
Epoch [81/120    avg_loss:0.008, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.989]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.009, val_acc:0.987]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.007, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6426     0     1     0     0     0     0     5     0]
 [    0     0 18070     0    18     0     2     0     0     0]
 [    0     0     0  2027     0     0     0     0     7     2]
 [    0    30    21     1  2881     0     9     0    29     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4876     0     0     1]
 [    0     0     0     0     0     0     0  1283     0     7]
 [    0    12     0     4    59     0     0     0  3469    27]
 [    0     0     0    11    16    81     0     0     0   811]]

Accuracy:
99.16853445159424

F1 scores:
[       nan 0.99627907 0.99886681 0.99338397 0.96905483 0.96989967
 0.99866871 0.99727944 0.97980511 0.91742081]

Kappa:
0.9889814570823552
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a10036908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.060, val_acc:0.176]
Epoch [2/120    avg_loss:1.716, val_acc:0.525]
Epoch [3/120    avg_loss:1.521, val_acc:0.646]
Epoch [4/120    avg_loss:1.349, val_acc:0.662]
Epoch [5/120    avg_loss:1.169, val_acc:0.720]
Epoch [6/120    avg_loss:1.024, val_acc:0.719]
Epoch [7/120    avg_loss:0.908, val_acc:0.556]
Epoch [8/120    avg_loss:0.794, val_acc:0.684]
Epoch [9/120    avg_loss:0.675, val_acc:0.673]
Epoch [10/120    avg_loss:0.566, val_acc:0.709]
Epoch [11/120    avg_loss:0.494, val_acc:0.753]
Epoch [12/120    avg_loss:0.442, val_acc:0.747]
Epoch [13/120    avg_loss:0.406, val_acc:0.732]
Epoch [14/120    avg_loss:0.376, val_acc:0.769]
Epoch [15/120    avg_loss:0.356, val_acc:0.768]
Epoch [16/120    avg_loss:0.321, val_acc:0.804]
Epoch [17/120    avg_loss:0.305, val_acc:0.794]
Epoch [18/120    avg_loss:0.260, val_acc:0.852]
Epoch [19/120    avg_loss:0.224, val_acc:0.903]
Epoch [20/120    avg_loss:0.326, val_acc:0.741]
Epoch [21/120    avg_loss:0.437, val_acc:0.819]
Epoch [22/120    avg_loss:0.280, val_acc:0.882]
Epoch [23/120    avg_loss:0.208, val_acc:0.925]
Epoch [24/120    avg_loss:0.552, val_acc:0.740]
Epoch [25/120    avg_loss:0.525, val_acc:0.761]
Epoch [26/120    avg_loss:0.396, val_acc:0.838]
Epoch [27/120    avg_loss:0.309, val_acc:0.885]
Epoch [28/120    avg_loss:0.255, val_acc:0.897]
Epoch [29/120    avg_loss:0.210, val_acc:0.940]
Epoch [30/120    avg_loss:0.169, val_acc:0.944]
Epoch [31/120    avg_loss:0.153, val_acc:0.926]
Epoch [32/120    avg_loss:0.133, val_acc:0.955]
Epoch [33/120    avg_loss:0.127, val_acc:0.946]
Epoch [34/120    avg_loss:0.115, val_acc:0.951]
Epoch [35/120    avg_loss:0.098, val_acc:0.963]
Epoch [36/120    avg_loss:0.100, val_acc:0.966]
Epoch [37/120    avg_loss:0.063, val_acc:0.972]
Epoch [38/120    avg_loss:0.070, val_acc:0.924]
Epoch [39/120    avg_loss:0.062, val_acc:0.965]
Epoch [40/120    avg_loss:0.067, val_acc:0.960]
Epoch [41/120    avg_loss:0.079, val_acc:0.957]
Epoch [42/120    avg_loss:0.070, val_acc:0.961]
Epoch [43/120    avg_loss:0.055, val_acc:0.976]
Epoch [44/120    avg_loss:0.052, val_acc:0.963]
Epoch [45/120    avg_loss:0.068, val_acc:0.964]
Epoch [46/120    avg_loss:0.047, val_acc:0.968]
Epoch [47/120    avg_loss:0.046, val_acc:0.975]
Epoch [48/120    avg_loss:0.048, val_acc:0.968]
Epoch [49/120    avg_loss:0.038, val_acc:0.981]
Epoch [50/120    avg_loss:0.045, val_acc:0.977]
Epoch [51/120    avg_loss:0.037, val_acc:0.983]
Epoch [52/120    avg_loss:0.026, val_acc:0.971]
Epoch [53/120    avg_loss:0.037, val_acc:0.969]
Epoch [54/120    avg_loss:0.034, val_acc:0.974]
Epoch [55/120    avg_loss:0.023, val_acc:0.979]
Epoch [56/120    avg_loss:0.018, val_acc:0.978]
Epoch [57/120    avg_loss:0.018, val_acc:0.970]
Epoch [58/120    avg_loss:0.015, val_acc:0.985]
Epoch [59/120    avg_loss:0.017, val_acc:0.983]
Epoch [60/120    avg_loss:0.022, val_acc:0.973]
Epoch [61/120    avg_loss:0.034, val_acc:0.975]
Epoch [62/120    avg_loss:0.025, val_acc:0.981]
Epoch [63/120    avg_loss:0.025, val_acc:0.984]
Epoch [64/120    avg_loss:0.013, val_acc:0.980]
Epoch [65/120    avg_loss:0.013, val_acc:0.979]
Epoch [66/120    avg_loss:0.013, val_acc:0.984]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.014, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.984]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.984]
Epoch [77/120    avg_loss:0.011, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.009, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6364     0     0     3     0    35    13    17     0]
 [    0     2 18039     0    16     0    19     0    14     0]
 [    0     6     0  2002     2     0     0     0    19     7]
 [    0    50    20     1  2866     0     5     0    30     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4870     0     0     8]
 [    0     0     0     0     0     0     1  1285     0     4]
 [    0     4     0     0    54     0     0     0  3490    23]
 [    0     0     0     0    27    64     0     0     0   828]]

Accuracy:
98.92993998987781

F1 scores:
[       nan 0.98988956 0.99803591 0.99133449 0.96498316 0.97606582
 0.99306688 0.99304482 0.97745414 0.92565679]

Kappa:
0.9858251993406216
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4442f8e908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.066, val_acc:0.109]
Epoch [2/120    avg_loss:1.734, val_acc:0.372]
Epoch [3/120    avg_loss:1.509, val_acc:0.303]
Epoch [4/120    avg_loss:1.330, val_acc:0.380]
Epoch [5/120    avg_loss:1.174, val_acc:0.436]
Epoch [6/120    avg_loss:1.014, val_acc:0.516]
Epoch [7/120    avg_loss:0.855, val_acc:0.569]
Epoch [8/120    avg_loss:0.748, val_acc:0.609]
Epoch [9/120    avg_loss:0.661, val_acc:0.613]
Epoch [10/120    avg_loss:0.573, val_acc:0.770]
Epoch [11/120    avg_loss:0.448, val_acc:0.766]
Epoch [12/120    avg_loss:0.416, val_acc:0.853]
Epoch [13/120    avg_loss:0.354, val_acc:0.808]
Epoch [14/120    avg_loss:0.319, val_acc:0.898]
Epoch [15/120    avg_loss:0.250, val_acc:0.913]
Epoch [16/120    avg_loss:0.230, val_acc:0.931]
Epoch [17/120    avg_loss:0.224, val_acc:0.938]
Epoch [18/120    avg_loss:0.196, val_acc:0.940]
Epoch [19/120    avg_loss:0.187, val_acc:0.954]
Epoch [20/120    avg_loss:0.142, val_acc:0.949]
Epoch [21/120    avg_loss:0.131, val_acc:0.955]
Epoch [22/120    avg_loss:0.170, val_acc:0.894]
Epoch [23/120    avg_loss:0.134, val_acc:0.954]
Epoch [24/120    avg_loss:0.109, val_acc:0.952]
Epoch [25/120    avg_loss:0.103, val_acc:0.964]
Epoch [26/120    avg_loss:0.107, val_acc:0.964]
Epoch [27/120    avg_loss:0.087, val_acc:0.960]
Epoch [28/120    avg_loss:0.080, val_acc:0.967]
Epoch [29/120    avg_loss:0.099, val_acc:0.956]
Epoch [30/120    avg_loss:0.081, val_acc:0.950]
Epoch [31/120    avg_loss:0.089, val_acc:0.937]
Epoch [32/120    avg_loss:0.164, val_acc:0.964]
Epoch [33/120    avg_loss:0.100, val_acc:0.967]
Epoch [34/120    avg_loss:0.113, val_acc:0.959]
Epoch [35/120    avg_loss:0.076, val_acc:0.964]
Epoch [36/120    avg_loss:0.058, val_acc:0.967]
Epoch [37/120    avg_loss:0.052, val_acc:0.974]
Epoch [38/120    avg_loss:0.063, val_acc:0.971]
Epoch [39/120    avg_loss:0.058, val_acc:0.975]
Epoch [40/120    avg_loss:0.136, val_acc:0.939]
Epoch [41/120    avg_loss:0.303, val_acc:0.689]
Epoch [42/120    avg_loss:0.345, val_acc:0.865]
Epoch [43/120    avg_loss:0.138, val_acc:0.939]
Epoch [44/120    avg_loss:0.084, val_acc:0.958]
Epoch [45/120    avg_loss:0.070, val_acc:0.974]
Epoch [46/120    avg_loss:0.070, val_acc:0.963]
Epoch [47/120    avg_loss:0.107, val_acc:0.962]
Epoch [48/120    avg_loss:0.061, val_acc:0.972]
Epoch [49/120    avg_loss:0.062, val_acc:0.954]
Epoch [50/120    avg_loss:0.096, val_acc:0.963]
Epoch [51/120    avg_loss:0.082, val_acc:0.956]
Epoch [52/120    avg_loss:0.054, val_acc:0.973]
Epoch [53/120    avg_loss:0.035, val_acc:0.979]
Epoch [54/120    avg_loss:0.031, val_acc:0.979]
Epoch [55/120    avg_loss:0.028, val_acc:0.980]
Epoch [56/120    avg_loss:0.027, val_acc:0.978]
Epoch [57/120    avg_loss:0.027, val_acc:0.981]
Epoch [58/120    avg_loss:0.033, val_acc:0.980]
Epoch [59/120    avg_loss:0.027, val_acc:0.978]
Epoch [60/120    avg_loss:0.029, val_acc:0.978]
Epoch [61/120    avg_loss:0.028, val_acc:0.979]
Epoch [62/120    avg_loss:0.029, val_acc:0.979]
Epoch [63/120    avg_loss:0.025, val_acc:0.984]
Epoch [64/120    avg_loss:0.025, val_acc:0.984]
Epoch [65/120    avg_loss:0.023, val_acc:0.982]
Epoch [66/120    avg_loss:0.027, val_acc:0.983]
Epoch [67/120    avg_loss:0.026, val_acc:0.979]
Epoch [68/120    avg_loss:0.020, val_acc:0.981]
Epoch [69/120    avg_loss:0.025, val_acc:0.982]
Epoch [70/120    avg_loss:0.024, val_acc:0.984]
Epoch [71/120    avg_loss:0.023, val_acc:0.983]
Epoch [72/120    avg_loss:0.023, val_acc:0.979]
Epoch [73/120    avg_loss:0.027, val_acc:0.981]
Epoch [74/120    avg_loss:0.027, val_acc:0.979]
Epoch [75/120    avg_loss:0.023, val_acc:0.980]
Epoch [76/120    avg_loss:0.020, val_acc:0.981]
Epoch [77/120    avg_loss:0.020, val_acc:0.980]
Epoch [78/120    avg_loss:0.022, val_acc:0.982]
Epoch [79/120    avg_loss:0.022, val_acc:0.981]
Epoch [80/120    avg_loss:0.021, val_acc:0.979]
Epoch [81/120    avg_loss:0.019, val_acc:0.981]
Epoch [82/120    avg_loss:0.021, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.981]
Epoch [85/120    avg_loss:0.020, val_acc:0.981]
Epoch [86/120    avg_loss:0.024, val_acc:0.981]
Epoch [87/120    avg_loss:0.019, val_acc:0.981]
Epoch [88/120    avg_loss:0.021, val_acc:0.981]
Epoch [89/120    avg_loss:0.020, val_acc:0.981]
Epoch [90/120    avg_loss:0.018, val_acc:0.981]
Epoch [91/120    avg_loss:0.021, val_acc:0.981]
Epoch [92/120    avg_loss:0.021, val_acc:0.981]
Epoch [93/120    avg_loss:0.018, val_acc:0.981]
Epoch [94/120    avg_loss:0.023, val_acc:0.981]
Epoch [95/120    avg_loss:0.019, val_acc:0.981]
Epoch [96/120    avg_loss:0.019, val_acc:0.981]
Epoch [97/120    avg_loss:0.019, val_acc:0.981]
Epoch [98/120    avg_loss:0.023, val_acc:0.981]
Epoch [99/120    avg_loss:0.020, val_acc:0.981]
Epoch [100/120    avg_loss:0.023, val_acc:0.981]
Epoch [101/120    avg_loss:0.018, val_acc:0.981]
Epoch [102/120    avg_loss:0.021, val_acc:0.981]
Epoch [103/120    avg_loss:0.022, val_acc:0.981]
Epoch [104/120    avg_loss:0.022, val_acc:0.981]
Epoch [105/120    avg_loss:0.020, val_acc:0.981]
Epoch [106/120    avg_loss:0.019, val_acc:0.981]
Epoch [107/120    avg_loss:0.022, val_acc:0.981]
Epoch [108/120    avg_loss:0.021, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.981]
Epoch [110/120    avg_loss:0.018, val_acc:0.981]
Epoch [111/120    avg_loss:0.019, val_acc:0.981]
Epoch [112/120    avg_loss:0.022, val_acc:0.981]
Epoch [113/120    avg_loss:0.020, val_acc:0.981]
Epoch [114/120    avg_loss:0.021, val_acc:0.981]
Epoch [115/120    avg_loss:0.022, val_acc:0.981]
Epoch [116/120    avg_loss:0.023, val_acc:0.981]
Epoch [117/120    avg_loss:0.018, val_acc:0.981]
Epoch [118/120    avg_loss:0.019, val_acc:0.981]
Epoch [119/120    avg_loss:0.022, val_acc:0.981]
Epoch [120/120    avg_loss:0.021, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6287     0     0    19     0    25     0    97     4]
 [    0     0 18017     0    34     0    20     0    19     0]
 [    0     6     0  2020     2     0     0     0     4     4]
 [    0    42    19     0  2867     0    12     0    32     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     0     0     0  4846     0     0    32]
 [    0     0     0     0     0     0     3  1287     0     0]
 [    0     2     0    40    56     0     0     0  3460    13]
 [    0     0     0     0    19    53     0     0     0   847]]

Accuracy:
98.65760489721158

F1 scores:
[       nan 0.98472864 0.99745336 0.98632812 0.96062992 0.98009763
 0.99059689 0.99883586 0.96338577 0.93128092]

Kappa:
0.9822279636268696
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:36
Validation dataloader:36
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:17
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f912c6be908>
supervision:full
center_pixel:True
Network :
Number of parameter: 28031==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.059, val_acc:0.101]
Epoch [2/120    avg_loss:1.690, val_acc:0.282]
Epoch [3/120    avg_loss:1.459, val_acc:0.331]
Epoch [4/120    avg_loss:1.334, val_acc:0.379]
Epoch [5/120    avg_loss:1.142, val_acc:0.390]
Epoch [6/120    avg_loss:0.989, val_acc:0.458]
Epoch [7/120    avg_loss:0.878, val_acc:0.497]
Epoch [8/120    avg_loss:0.769, val_acc:0.689]
Epoch [9/120    avg_loss:0.663, val_acc:0.711]
Epoch [10/120    avg_loss:0.618, val_acc:0.783]
Epoch [11/120    avg_loss:0.523, val_acc:0.827]
Epoch [12/120    avg_loss:0.511, val_acc:0.799]
Epoch [13/120    avg_loss:0.416, val_acc:0.841]
Epoch [14/120    avg_loss:0.412, val_acc:0.871]
Epoch [15/120    avg_loss:0.357, val_acc:0.856]
Epoch [16/120    avg_loss:0.323, val_acc:0.879]
Epoch [17/120    avg_loss:0.272, val_acc:0.911]
Epoch [18/120    avg_loss:0.255, val_acc:0.923]
Epoch [19/120    avg_loss:0.228, val_acc:0.916]
Epoch [20/120    avg_loss:0.234, val_acc:0.924]
Epoch [21/120    avg_loss:0.179, val_acc:0.936]
Epoch [22/120    avg_loss:0.191, val_acc:0.922]
Epoch [23/120    avg_loss:0.163, val_acc:0.939]
Epoch [24/120    avg_loss:0.137, val_acc:0.937]
Epoch [25/120    avg_loss:0.119, val_acc:0.931]
Epoch [26/120    avg_loss:0.145, val_acc:0.951]
Epoch [27/120    avg_loss:0.149, val_acc:0.951]
Epoch [28/120    avg_loss:0.122, val_acc:0.946]
Epoch [29/120    avg_loss:0.167, val_acc:0.943]
Epoch [30/120    avg_loss:0.114, val_acc:0.959]
Epoch [31/120    avg_loss:0.114, val_acc:0.940]
Epoch [32/120    avg_loss:0.135, val_acc:0.951]
Epoch [33/120    avg_loss:0.148, val_acc:0.941]
Epoch [34/120    avg_loss:0.094, val_acc:0.959]
Epoch [35/120    avg_loss:0.097, val_acc:0.938]
Epoch [36/120    avg_loss:0.090, val_acc:0.958]
Epoch [37/120    avg_loss:0.060, val_acc:0.956]
Epoch [38/120    avg_loss:0.056, val_acc:0.963]
Epoch [39/120    avg_loss:0.050, val_acc:0.964]
Epoch [40/120    avg_loss:0.058, val_acc:0.971]
Epoch [41/120    avg_loss:0.051, val_acc:0.966]
Epoch [42/120    avg_loss:0.045, val_acc:0.962]
Epoch [43/120    avg_loss:0.049, val_acc:0.974]
Epoch [44/120    avg_loss:0.042, val_acc:0.979]
Epoch [45/120    avg_loss:0.041, val_acc:0.972]
Epoch [46/120    avg_loss:0.038, val_acc:0.964]
Epoch [47/120    avg_loss:0.044, val_acc:0.951]
Epoch [48/120    avg_loss:0.057, val_acc:0.972]
Epoch [49/120    avg_loss:0.081, val_acc:0.932]
Epoch [50/120    avg_loss:0.068, val_acc:0.970]
Epoch [51/120    avg_loss:0.072, val_acc:0.974]
Epoch [52/120    avg_loss:0.084, val_acc:0.963]
Epoch [53/120    avg_loss:0.082, val_acc:0.960]
Epoch [54/120    avg_loss:0.101, val_acc:0.964]
Epoch [55/120    avg_loss:0.054, val_acc:0.973]
Epoch [56/120    avg_loss:0.037, val_acc:0.978]
Epoch [57/120    avg_loss:0.035, val_acc:0.979]
Epoch [58/120    avg_loss:0.050, val_acc:0.874]
Epoch [59/120    avg_loss:0.057, val_acc:0.958]
Epoch [60/120    avg_loss:0.048, val_acc:0.965]
Epoch [61/120    avg_loss:0.027, val_acc:0.984]
Epoch [62/120    avg_loss:0.019, val_acc:0.981]
Epoch [63/120    avg_loss:0.018, val_acc:0.982]
Epoch [64/120    avg_loss:0.018, val_acc:0.986]
Epoch [65/120    avg_loss:0.015, val_acc:0.984]
Epoch [66/120    avg_loss:0.017, val_acc:0.979]
Epoch [67/120    avg_loss:0.017, val_acc:0.981]
Epoch [68/120    avg_loss:0.017, val_acc:0.982]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.020, val_acc:0.983]
Epoch [71/120    avg_loss:0.020, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.984]
Epoch [73/120    avg_loss:0.013, val_acc:0.988]
Epoch [74/120    avg_loss:0.014, val_acc:0.988]
Epoch [75/120    avg_loss:0.011, val_acc:0.990]
Epoch [76/120    avg_loss:0.016, val_acc:0.990]
Epoch [77/120    avg_loss:0.024, val_acc:0.979]
Epoch [78/120    avg_loss:0.025, val_acc:0.983]
Epoch [79/120    avg_loss:0.016, val_acc:0.987]
Epoch [80/120    avg_loss:0.015, val_acc:0.976]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.013, val_acc:0.971]
Epoch [86/120    avg_loss:0.013, val_acc:0.987]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.990]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.010, val_acc:0.954]
Epoch [92/120    avg_loss:0.011, val_acc:0.988]
Epoch [93/120    avg_loss:0.015, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.989]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.004, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6408     0     0    11     0     0     0    13     0]
 [    0     1 18077     0     8     0     4     0     0     0]
 [    0     1     0  2029     3     0     0     0     3     0]
 [    0    46    22     0  2868     0     7     0    29     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     4     3    13     0     0  4857     0     0     1]
 [    0     0     0     0     0     0     2  1285     0     3]
 [    0    15     0     5    34     0     0     0  3492    25]
 [    0     0     0     6    14    45     0     1     0   853]]

Accuracy:
99.23119562335816

F1 scores:
[       nan 0.99294956 0.99895004 0.99241868 0.97055838 0.98305085
 0.99651211 0.99767081 0.98255487 0.94725153]

Kappa:
0.9898106320768674
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbcd441b940>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.930, val_acc:0.199]
Epoch [2/120    avg_loss:1.573, val_acc:0.307]
Epoch [3/120    avg_loss:1.361, val_acc:0.406]
Epoch [4/120    avg_loss:1.155, val_acc:0.458]
Epoch [5/120    avg_loss:0.980, val_acc:0.618]
Epoch [6/120    avg_loss:0.865, val_acc:0.669]
Epoch [7/120    avg_loss:0.757, val_acc:0.693]
Epoch [8/120    avg_loss:0.680, val_acc:0.738]
Epoch [9/120    avg_loss:0.566, val_acc:0.727]
Epoch [10/120    avg_loss:0.488, val_acc:0.790]
Epoch [11/120    avg_loss:0.442, val_acc:0.779]
Epoch [12/120    avg_loss:0.417, val_acc:0.773]
Epoch [13/120    avg_loss:0.375, val_acc:0.797]
Epoch [14/120    avg_loss:0.361, val_acc:0.773]
Epoch [15/120    avg_loss:0.360, val_acc:0.789]
Epoch [16/120    avg_loss:0.332, val_acc:0.792]
Epoch [17/120    avg_loss:0.300, val_acc:0.802]
Epoch [18/120    avg_loss:0.278, val_acc:0.797]
Epoch [19/120    avg_loss:0.265, val_acc:0.794]
Epoch [20/120    avg_loss:0.232, val_acc:0.784]
Epoch [21/120    avg_loss:0.259, val_acc:0.833]
Epoch [22/120    avg_loss:0.196, val_acc:0.823]
Epoch [23/120    avg_loss:0.245, val_acc:0.823]
Epoch [24/120    avg_loss:0.193, val_acc:0.877]
Epoch [25/120    avg_loss:0.180, val_acc:0.838]
Epoch [26/120    avg_loss:0.188, val_acc:0.895]
Epoch [27/120    avg_loss:0.147, val_acc:0.889]
Epoch [28/120    avg_loss:0.147, val_acc:0.901]
Epoch [29/120    avg_loss:0.181, val_acc:0.914]
Epoch [30/120    avg_loss:0.120, val_acc:0.934]
Epoch [31/120    avg_loss:0.102, val_acc:0.938]
Epoch [32/120    avg_loss:0.107, val_acc:0.838]
Epoch [33/120    avg_loss:0.117, val_acc:0.897]
Epoch [34/120    avg_loss:0.088, val_acc:0.944]
Epoch [35/120    avg_loss:0.086, val_acc:0.957]
Epoch [36/120    avg_loss:0.084, val_acc:0.953]
Epoch [37/120    avg_loss:0.082, val_acc:0.952]
Epoch [38/120    avg_loss:0.073, val_acc:0.957]
Epoch [39/120    avg_loss:0.100, val_acc:0.938]
Epoch [40/120    avg_loss:0.072, val_acc:0.944]
Epoch [41/120    avg_loss:0.069, val_acc:0.951]
Epoch [42/120    avg_loss:0.085, val_acc:0.946]
Epoch [43/120    avg_loss:0.096, val_acc:0.935]
Epoch [44/120    avg_loss:0.125, val_acc:0.922]
Epoch [45/120    avg_loss:0.084, val_acc:0.949]
Epoch [46/120    avg_loss:0.061, val_acc:0.954]
Epoch [47/120    avg_loss:0.072, val_acc:0.959]
Epoch [48/120    avg_loss:0.073, val_acc:0.939]
Epoch [49/120    avg_loss:0.065, val_acc:0.957]
Epoch [50/120    avg_loss:0.051, val_acc:0.958]
Epoch [51/120    avg_loss:0.072, val_acc:0.963]
Epoch [52/120    avg_loss:0.040, val_acc:0.963]
Epoch [53/120    avg_loss:0.038, val_acc:0.957]
Epoch [54/120    avg_loss:0.038, val_acc:0.968]
Epoch [55/120    avg_loss:0.048, val_acc:0.957]
Epoch [56/120    avg_loss:0.041, val_acc:0.953]
Epoch [57/120    avg_loss:0.041, val_acc:0.945]
Epoch [58/120    avg_loss:0.053, val_acc:0.945]
Epoch [59/120    avg_loss:0.043, val_acc:0.962]
Epoch [60/120    avg_loss:0.037, val_acc:0.968]
Epoch [61/120    avg_loss:0.035, val_acc:0.936]
Epoch [62/120    avg_loss:0.037, val_acc:0.967]
Epoch [63/120    avg_loss:0.025, val_acc:0.970]
Epoch [64/120    avg_loss:0.023, val_acc:0.978]
Epoch [65/120    avg_loss:0.024, val_acc:0.968]
Epoch [66/120    avg_loss:0.032, val_acc:0.951]
Epoch [67/120    avg_loss:0.038, val_acc:0.964]
Epoch [68/120    avg_loss:0.046, val_acc:0.969]
Epoch [69/120    avg_loss:0.034, val_acc:0.958]
Epoch [70/120    avg_loss:0.036, val_acc:0.970]
Epoch [71/120    avg_loss:0.036, val_acc:0.971]
Epoch [72/120    avg_loss:0.023, val_acc:0.980]
Epoch [73/120    avg_loss:0.026, val_acc:0.964]
Epoch [74/120    avg_loss:0.066, val_acc:0.951]
Epoch [75/120    avg_loss:0.048, val_acc:0.969]
Epoch [76/120    avg_loss:0.025, val_acc:0.967]
Epoch [77/120    avg_loss:0.042, val_acc:0.952]
Epoch [78/120    avg_loss:0.032, val_acc:0.963]
Epoch [79/120    avg_loss:0.041, val_acc:0.955]
Epoch [80/120    avg_loss:0.049, val_acc:0.968]
Epoch [81/120    avg_loss:0.030, val_acc:0.971]
Epoch [82/120    avg_loss:0.016, val_acc:0.975]
Epoch [83/120    avg_loss:0.040, val_acc:0.956]
Epoch [84/120    avg_loss:0.043, val_acc:0.957]
Epoch [85/120    avg_loss:0.019, val_acc:0.971]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.017, val_acc:0.972]
Epoch [88/120    avg_loss:0.015, val_acc:0.974]
Epoch [89/120    avg_loss:0.015, val_acc:0.976]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.013, val_acc:0.978]
Epoch [92/120    avg_loss:0.012, val_acc:0.978]
Epoch [93/120    avg_loss:0.012, val_acc:0.977]
Epoch [94/120    avg_loss:0.013, val_acc:0.977]
Epoch [95/120    avg_loss:0.015, val_acc:0.973]
Epoch [96/120    avg_loss:0.014, val_acc:0.975]
Epoch [97/120    avg_loss:0.012, val_acc:0.977]
Epoch [98/120    avg_loss:0.009, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.014, val_acc:0.979]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.009, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.979]
Epoch [108/120    avg_loss:0.010, val_acc:0.979]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.978]
Epoch [111/120    avg_loss:0.012, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.010, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.979]
Epoch [116/120    avg_loss:0.011, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6382     0     0     4     0     0     0    40     6]
 [    0     0 17988     0    57     0    38     0     7     0]
 [    0     0     0  1964     0     0     0     0    71     1]
 [    0    11    20     0  2910     0    23     5     1     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    56     0     0     0  4822     0     0     0]
 [    0    11     0     0     0     0     0  1278     1     0]
 [    0    14     5    64    61     0     1     0  3425     1]
 [    0     0     0     0     3     9     0     0     0   907]]

Accuracy:
98.76605692526451

F1 scores:
[       nan 0.99330739 0.99493902 0.96653543 0.96886965 0.99656357
 0.98791231 0.99339293 0.96261945 0.98801743]

Kappa:
0.9836540591397317
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ebb24f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.012, val_acc:0.123]
Epoch [2/120    avg_loss:1.588, val_acc:0.312]
Epoch [3/120    avg_loss:1.338, val_acc:0.347]
Epoch [4/120    avg_loss:1.192, val_acc:0.428]
Epoch [5/120    avg_loss:1.046, val_acc:0.456]
Epoch [6/120    avg_loss:0.938, val_acc:0.479]
Epoch [7/120    avg_loss:0.809, val_acc:0.542]
Epoch [8/120    avg_loss:0.689, val_acc:0.623]
Epoch [9/120    avg_loss:0.607, val_acc:0.615]
Epoch [10/120    avg_loss:0.555, val_acc:0.728]
Epoch [11/120    avg_loss:0.468, val_acc:0.747]
Epoch [12/120    avg_loss:0.444, val_acc:0.733]
Epoch [13/120    avg_loss:0.391, val_acc:0.767]
Epoch [14/120    avg_loss:0.373, val_acc:0.793]
Epoch [15/120    avg_loss:0.325, val_acc:0.818]
Epoch [16/120    avg_loss:0.285, val_acc:0.835]
Epoch [17/120    avg_loss:0.279, val_acc:0.859]
Epoch [18/120    avg_loss:0.253, val_acc:0.883]
Epoch [19/120    avg_loss:0.198, val_acc:0.881]
Epoch [20/120    avg_loss:0.172, val_acc:0.928]
Epoch [21/120    avg_loss:0.180, val_acc:0.840]
Epoch [22/120    avg_loss:0.210, val_acc:0.912]
Epoch [23/120    avg_loss:0.162, val_acc:0.915]
Epoch [24/120    avg_loss:0.194, val_acc:0.932]
Epoch [25/120    avg_loss:0.145, val_acc:0.946]
Epoch [26/120    avg_loss:0.150, val_acc:0.907]
Epoch [27/120    avg_loss:0.145, val_acc:0.891]
Epoch [28/120    avg_loss:0.147, val_acc:0.927]
Epoch [29/120    avg_loss:0.107, val_acc:0.950]
Epoch [30/120    avg_loss:0.084, val_acc:0.956]
Epoch [31/120    avg_loss:0.083, val_acc:0.955]
Epoch [32/120    avg_loss:0.080, val_acc:0.947]
Epoch [33/120    avg_loss:0.087, val_acc:0.957]
Epoch [34/120    avg_loss:0.075, val_acc:0.960]
Epoch [35/120    avg_loss:0.062, val_acc:0.959]
Epoch [36/120    avg_loss:0.074, val_acc:0.948]
Epoch [37/120    avg_loss:0.067, val_acc:0.965]
Epoch [38/120    avg_loss:0.066, val_acc:0.961]
Epoch [39/120    avg_loss:0.055, val_acc:0.968]
Epoch [40/120    avg_loss:0.043, val_acc:0.965]
Epoch [41/120    avg_loss:0.060, val_acc:0.962]
Epoch [42/120    avg_loss:0.061, val_acc:0.959]
Epoch [43/120    avg_loss:0.041, val_acc:0.968]
Epoch [44/120    avg_loss:0.037, val_acc:0.964]
Epoch [45/120    avg_loss:0.031, val_acc:0.965]
Epoch [46/120    avg_loss:0.037, val_acc:0.968]
Epoch [47/120    avg_loss:0.028, val_acc:0.963]
Epoch [48/120    avg_loss:0.033, val_acc:0.971]
Epoch [49/120    avg_loss:0.032, val_acc:0.970]
Epoch [50/120    avg_loss:0.035, val_acc:0.976]
Epoch [51/120    avg_loss:0.029, val_acc:0.964]
Epoch [52/120    avg_loss:0.038, val_acc:0.969]
Epoch [53/120    avg_loss:0.030, val_acc:0.970]
Epoch [54/120    avg_loss:0.037, val_acc:0.965]
Epoch [55/120    avg_loss:0.041, val_acc:0.962]
Epoch [56/120    avg_loss:0.023, val_acc:0.977]
Epoch [57/120    avg_loss:0.023, val_acc:0.969]
Epoch [58/120    avg_loss:0.021, val_acc:0.974]
Epoch [59/120    avg_loss:0.066, val_acc:0.964]
Epoch [60/120    avg_loss:0.033, val_acc:0.977]
Epoch [61/120    avg_loss:0.040, val_acc:0.971]
Epoch [62/120    avg_loss:0.038, val_acc:0.969]
Epoch [63/120    avg_loss:0.032, val_acc:0.968]
Epoch [64/120    avg_loss:0.048, val_acc:0.960]
Epoch [65/120    avg_loss:0.048, val_acc:0.959]
Epoch [66/120    avg_loss:0.047, val_acc:0.963]
Epoch [67/120    avg_loss:0.041, val_acc:0.973]
Epoch [68/120    avg_loss:0.022, val_acc:0.976]
Epoch [69/120    avg_loss:0.017, val_acc:0.976]
Epoch [70/120    avg_loss:0.021, val_acc:0.974]
Epoch [71/120    avg_loss:0.024, val_acc:0.973]
Epoch [72/120    avg_loss:0.016, val_acc:0.979]
Epoch [73/120    avg_loss:0.019, val_acc:0.975]
Epoch [74/120    avg_loss:0.023, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.973]
Epoch [76/120    avg_loss:0.014, val_acc:0.981]
Epoch [77/120    avg_loss:0.013, val_acc:0.978]
Epoch [78/120    avg_loss:0.011, val_acc:0.977]
Epoch [79/120    avg_loss:0.009, val_acc:0.979]
Epoch [80/120    avg_loss:0.012, val_acc:0.973]
Epoch [81/120    avg_loss:0.018, val_acc:0.967]
Epoch [82/120    avg_loss:0.026, val_acc:0.972]
Epoch [83/120    avg_loss:0.015, val_acc:0.981]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.977]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.977]
Epoch [90/120    avg_loss:0.014, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.983]
Epoch [94/120    avg_loss:0.036, val_acc:0.951]
Epoch [95/120    avg_loss:0.027, val_acc:0.978]
Epoch [96/120    avg_loss:0.023, val_acc:0.980]
Epoch [97/120    avg_loss:0.015, val_acc:0.979]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.072, val_acc:0.953]
Epoch [102/120    avg_loss:0.064, val_acc:0.966]
Epoch [103/120    avg_loss:0.030, val_acc:0.973]
Epoch [104/120    avg_loss:0.025, val_acc:0.965]
Epoch [105/120    avg_loss:0.032, val_acc:0.976]
Epoch [106/120    avg_loss:0.026, val_acc:0.983]
Epoch [107/120    avg_loss:0.016, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6304     0     3     3     0     6     3   107     6]
 [    0     0 18029     0    18     0    40     0     3     0]
 [    0     2     0  1919     0     0     0     0   115     0]
 [    0     9    17     1  2921     0     4     0    17     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     0     0     0  4862     0     9     0]
 [    0    27     0     0     0     7     0  1236    20     0]
 [    0    52     1    63    38     0     4     0  3413     0]
 [    0     0     0     0     0    12     0     0     0   907]]

Accuracy:
98.56120309449787

F1 scores:
[       nan 0.98300327 0.99762063 0.95425162 0.98151882 0.99277292
 0.99285277 0.97746145 0.94086837 0.98855586]

Kappa:
0.9809413579091057
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2606e0c898>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.956, val_acc:0.548]
Epoch [2/120    avg_loss:1.541, val_acc:0.571]
Epoch [3/120    avg_loss:1.343, val_acc:0.662]
Epoch [4/120    avg_loss:1.184, val_acc:0.699]
Epoch [5/120    avg_loss:1.023, val_acc:0.732]
Epoch [6/120    avg_loss:0.909, val_acc:0.748]
Epoch [7/120    avg_loss:0.833, val_acc:0.772]
Epoch [8/120    avg_loss:0.766, val_acc:0.797]
Epoch [9/120    avg_loss:0.674, val_acc:0.816]
Epoch [10/120    avg_loss:0.633, val_acc:0.770]
Epoch [11/120    avg_loss:0.586, val_acc:0.789]
Epoch [12/120    avg_loss:0.565, val_acc:0.776]
Epoch [13/120    avg_loss:0.511, val_acc:0.709]
Epoch [14/120    avg_loss:0.432, val_acc:0.811]
Epoch [15/120    avg_loss:0.384, val_acc:0.795]
Epoch [16/120    avg_loss:0.374, val_acc:0.774]
Epoch [17/120    avg_loss:0.344, val_acc:0.823]
Epoch [18/120    avg_loss:0.328, val_acc:0.825]
Epoch [19/120    avg_loss:0.285, val_acc:0.816]
Epoch [20/120    avg_loss:0.285, val_acc:0.806]
Epoch [21/120    avg_loss:0.262, val_acc:0.822]
Epoch [22/120    avg_loss:0.217, val_acc:0.826]
Epoch [23/120    avg_loss:0.213, val_acc:0.833]
Epoch [24/120    avg_loss:0.194, val_acc:0.847]
Epoch [25/120    avg_loss:0.193, val_acc:0.854]
Epoch [26/120    avg_loss:0.178, val_acc:0.902]
Epoch [27/120    avg_loss:0.188, val_acc:0.875]
Epoch [28/120    avg_loss:0.174, val_acc:0.927]
Epoch [29/120    avg_loss:0.154, val_acc:0.917]
Epoch [30/120    avg_loss:0.147, val_acc:0.923]
Epoch [31/120    avg_loss:0.127, val_acc:0.947]
Epoch [32/120    avg_loss:0.110, val_acc:0.943]
Epoch [33/120    avg_loss:0.111, val_acc:0.953]
Epoch [34/120    avg_loss:0.083, val_acc:0.958]
Epoch [35/120    avg_loss:0.085, val_acc:0.956]
Epoch [36/120    avg_loss:0.069, val_acc:0.962]
Epoch [37/120    avg_loss:0.071, val_acc:0.954]
Epoch [38/120    avg_loss:0.070, val_acc:0.942]
Epoch [39/120    avg_loss:0.050, val_acc:0.962]
Epoch [40/120    avg_loss:0.052, val_acc:0.977]
Epoch [41/120    avg_loss:0.050, val_acc:0.968]
Epoch [42/120    avg_loss:0.059, val_acc:0.960]
Epoch [43/120    avg_loss:0.042, val_acc:0.977]
Epoch [44/120    avg_loss:0.032, val_acc:0.980]
Epoch [45/120    avg_loss:0.034, val_acc:0.966]
Epoch [46/120    avg_loss:0.044, val_acc:0.979]
Epoch [47/120    avg_loss:0.031, val_acc:0.980]
Epoch [48/120    avg_loss:0.037, val_acc:0.946]
Epoch [49/120    avg_loss:0.043, val_acc:0.977]
Epoch [50/120    avg_loss:0.041, val_acc:0.973]
Epoch [51/120    avg_loss:0.039, val_acc:0.973]
Epoch [52/120    avg_loss:0.035, val_acc:0.952]
Epoch [53/120    avg_loss:0.047, val_acc:0.978]
Epoch [54/120    avg_loss:0.034, val_acc:0.986]
Epoch [55/120    avg_loss:0.051, val_acc:0.962]
Epoch [56/120    avg_loss:0.031, val_acc:0.975]
Epoch [57/120    avg_loss:0.039, val_acc:0.967]
Epoch [58/120    avg_loss:0.036, val_acc:0.970]
Epoch [59/120    avg_loss:0.041, val_acc:0.966]
Epoch [60/120    avg_loss:0.029, val_acc:0.978]
Epoch [61/120    avg_loss:0.020, val_acc:0.977]
Epoch [62/120    avg_loss:0.017, val_acc:0.973]
Epoch [63/120    avg_loss:0.026, val_acc:0.976]
Epoch [64/120    avg_loss:0.017, val_acc:0.975]
Epoch [65/120    avg_loss:0.021, val_acc:0.987]
Epoch [66/120    avg_loss:0.016, val_acc:0.978]
Epoch [67/120    avg_loss:0.015, val_acc:0.979]
Epoch [68/120    avg_loss:0.017, val_acc:0.981]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.017, val_acc:0.977]
Epoch [71/120    avg_loss:0.028, val_acc:0.984]
Epoch [72/120    avg_loss:0.017, val_acc:0.977]
Epoch [73/120    avg_loss:0.020, val_acc:0.978]
Epoch [74/120    avg_loss:0.026, val_acc:0.970]
Epoch [75/120    avg_loss:0.016, val_acc:0.973]
Epoch [76/120    avg_loss:0.015, val_acc:0.980]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.029, val_acc:0.970]
Epoch [79/120    avg_loss:0.020, val_acc:0.981]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.011, val_acc:0.983]
Epoch [83/120    avg_loss:0.012, val_acc:0.985]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6309     0    14     1     0     0    29    70     9]
 [    0     0 17925     0    68     0    94     0     3     0]
 [    0     1     0  1951     0     0     0     0    83     1]
 [    0    21     8     0  2921     1     9     0     7     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    24     0     0     0  4828     0    26     0]
 [    0    20     0     0     0     0     1  1269     0     0]
 [    0    46     3    51    46     0     0     0  3425     0]
 [    0     1     0     0     0     5     0     0     0   913]]

Accuracy:
98.44070084110574

F1 scores:
[       nan 0.98347623 0.99445215 0.96298124 0.97237017 0.99770642
 0.98430173 0.98068006 0.95337509 0.98863021]

Kappa:
0.9793697428578837
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9863b9d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.066, val_acc:0.219]
Epoch [2/120    avg_loss:1.635, val_acc:0.224]
Epoch [3/120    avg_loss:1.372, val_acc:0.316]
Epoch [4/120    avg_loss:1.163, val_acc:0.317]
Epoch [5/120    avg_loss:1.022, val_acc:0.393]
Epoch [6/120    avg_loss:0.888, val_acc:0.562]
Epoch [7/120    avg_loss:0.779, val_acc:0.709]
Epoch [8/120    avg_loss:0.677, val_acc:0.669]
Epoch [9/120    avg_loss:0.597, val_acc:0.717]
Epoch [10/120    avg_loss:0.556, val_acc:0.759]
Epoch [11/120    avg_loss:0.492, val_acc:0.774]
Epoch [12/120    avg_loss:0.434, val_acc:0.793]
Epoch [13/120    avg_loss:0.403, val_acc:0.763]
Epoch [14/120    avg_loss:0.357, val_acc:0.764]
Epoch [15/120    avg_loss:0.336, val_acc:0.757]
Epoch [16/120    avg_loss:0.294, val_acc:0.792]
Epoch [17/120    avg_loss:0.288, val_acc:0.787]
Epoch [18/120    avg_loss:0.284, val_acc:0.761]
Epoch [19/120    avg_loss:0.275, val_acc:0.774]
Epoch [20/120    avg_loss:0.237, val_acc:0.781]
Epoch [21/120    avg_loss:0.225, val_acc:0.792]
Epoch [22/120    avg_loss:0.251, val_acc:0.817]
Epoch [23/120    avg_loss:0.217, val_acc:0.828]
Epoch [24/120    avg_loss:0.222, val_acc:0.833]
Epoch [25/120    avg_loss:0.204, val_acc:0.824]
Epoch [26/120    avg_loss:0.190, val_acc:0.870]
Epoch [27/120    avg_loss:0.163, val_acc:0.882]
Epoch [28/120    avg_loss:0.148, val_acc:0.868]
Epoch [29/120    avg_loss:0.159, val_acc:0.864]
Epoch [30/120    avg_loss:0.140, val_acc:0.927]
Epoch [31/120    avg_loss:0.128, val_acc:0.945]
Epoch [32/120    avg_loss:0.138, val_acc:0.860]
Epoch [33/120    avg_loss:0.118, val_acc:0.943]
Epoch [34/120    avg_loss:0.130, val_acc:0.956]
Epoch [35/120    avg_loss:0.094, val_acc:0.923]
Epoch [36/120    avg_loss:0.127, val_acc:0.946]
Epoch [37/120    avg_loss:0.125, val_acc:0.961]
Epoch [38/120    avg_loss:0.099, val_acc:0.810]
Epoch [39/120    avg_loss:0.132, val_acc:0.847]
Epoch [40/120    avg_loss:0.095, val_acc:0.955]
Epoch [41/120    avg_loss:0.074, val_acc:0.962]
Epoch [42/120    avg_loss:0.055, val_acc:0.968]
Epoch [43/120    avg_loss:0.068, val_acc:0.934]
Epoch [44/120    avg_loss:0.064, val_acc:0.948]
Epoch [45/120    avg_loss:0.053, val_acc:0.968]
Epoch [46/120    avg_loss:0.055, val_acc:0.975]
Epoch [47/120    avg_loss:0.041, val_acc:0.974]
Epoch [48/120    avg_loss:0.042, val_acc:0.943]
Epoch [49/120    avg_loss:0.039, val_acc:0.971]
Epoch [50/120    avg_loss:0.038, val_acc:0.967]
Epoch [51/120    avg_loss:0.040, val_acc:0.968]
Epoch [52/120    avg_loss:0.066, val_acc:0.973]
Epoch [53/120    avg_loss:0.029, val_acc:0.975]
Epoch [54/120    avg_loss:0.035, val_acc:0.972]
Epoch [55/120    avg_loss:0.038, val_acc:0.976]
Epoch [56/120    avg_loss:0.036, val_acc:0.973]
Epoch [57/120    avg_loss:0.045, val_acc:0.968]
Epoch [58/120    avg_loss:0.029, val_acc:0.975]
Epoch [59/120    avg_loss:0.031, val_acc:0.972]
Epoch [60/120    avg_loss:0.029, val_acc:0.978]
Epoch [61/120    avg_loss:0.036, val_acc:0.968]
Epoch [62/120    avg_loss:0.035, val_acc:0.958]
Epoch [63/120    avg_loss:0.036, val_acc:0.969]
Epoch [64/120    avg_loss:0.049, val_acc:0.972]
Epoch [65/120    avg_loss:0.033, val_acc:0.972]
Epoch [66/120    avg_loss:0.023, val_acc:0.973]
Epoch [67/120    avg_loss:0.020, val_acc:0.978]
Epoch [68/120    avg_loss:0.029, val_acc:0.973]
Epoch [69/120    avg_loss:0.044, val_acc:0.956]
Epoch [70/120    avg_loss:0.034, val_acc:0.969]
Epoch [71/120    avg_loss:0.045, val_acc:0.946]
Epoch [72/120    avg_loss:0.025, val_acc:0.976]
Epoch [73/120    avg_loss:0.017, val_acc:0.983]
Epoch [74/120    avg_loss:0.024, val_acc:0.978]
Epoch [75/120    avg_loss:0.021, val_acc:0.979]
Epoch [76/120    avg_loss:0.022, val_acc:0.972]
Epoch [77/120    avg_loss:0.023, val_acc:0.977]
Epoch [78/120    avg_loss:0.032, val_acc:0.965]
Epoch [79/120    avg_loss:0.024, val_acc:0.975]
Epoch [80/120    avg_loss:0.065, val_acc:0.959]
Epoch [81/120    avg_loss:0.033, val_acc:0.969]
Epoch [82/120    avg_loss:0.026, val_acc:0.965]
Epoch [83/120    avg_loss:0.062, val_acc:0.962]
Epoch [84/120    avg_loss:0.029, val_acc:0.976]
Epoch [85/120    avg_loss:0.020, val_acc:0.969]
Epoch [86/120    avg_loss:0.015, val_acc:0.975]
Epoch [87/120    avg_loss:0.014, val_acc:0.978]
Epoch [88/120    avg_loss:0.011, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.980]
Epoch [90/120    avg_loss:0.012, val_acc:0.980]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.018, val_acc:0.980]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.980]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.011, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.011, val_acc:0.981]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.016, val_acc:0.981]
Epoch [110/120    avg_loss:0.010, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.013, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.010, val_acc:0.981]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6320     0     0     2     0     2     6    89    13]
 [    0     0 17913     0    55     0   111     0    11     0]
 [    0     1     0  1940     0     0     0     0    95     0]
 [    0    14     5     0  2923     0    25     0     2     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    20     0     1     0  4855     0     2     0]
 [    0    15     0     0     0    10     0  1263     2     0]
 [    0    25     0    35    34     0     0     0  3477     0]
 [    0     0     0     0    11    18     0     0     0   890]]

Accuracy:
98.53710264381944

F1 scores:
[       nan 0.98696026 0.99439325 0.96733982 0.97465822 0.9893859
 0.9836896  0.98710434 0.95930473 0.97534247]

Kappa:
0.9806475919382283
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe25efa88d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.036, val_acc:0.091]
Epoch [2/120    avg_loss:1.585, val_acc:0.295]
Epoch [3/120    avg_loss:1.332, val_acc:0.417]
Epoch [4/120    avg_loss:1.182, val_acc:0.413]
Epoch [5/120    avg_loss:0.999, val_acc:0.709]
Epoch [6/120    avg_loss:0.831, val_acc:0.726]
Epoch [7/120    avg_loss:0.720, val_acc:0.713]
Epoch [8/120    avg_loss:0.590, val_acc:0.708]
Epoch [9/120    avg_loss:0.522, val_acc:0.689]
Epoch [10/120    avg_loss:0.467, val_acc:0.761]
Epoch [11/120    avg_loss:0.412, val_acc:0.791]
Epoch [12/120    avg_loss:0.378, val_acc:0.841]
Epoch [13/120    avg_loss:0.340, val_acc:0.829]
Epoch [14/120    avg_loss:0.342, val_acc:0.843]
Epoch [15/120    avg_loss:0.288, val_acc:0.874]
Epoch [16/120    avg_loss:0.265, val_acc:0.870]
Epoch [17/120    avg_loss:0.233, val_acc:0.858]
Epoch [18/120    avg_loss:0.233, val_acc:0.890]
Epoch [19/120    avg_loss:0.230, val_acc:0.889]
Epoch [20/120    avg_loss:0.200, val_acc:0.880]
Epoch [21/120    avg_loss:0.175, val_acc:0.906]
Epoch [22/120    avg_loss:0.164, val_acc:0.866]
Epoch [23/120    avg_loss:0.165, val_acc:0.940]
Epoch [24/120    avg_loss:0.137, val_acc:0.923]
Epoch [25/120    avg_loss:0.122, val_acc:0.952]
Epoch [26/120    avg_loss:0.148, val_acc:0.934]
Epoch [27/120    avg_loss:0.111, val_acc:0.949]
Epoch [28/120    avg_loss:0.100, val_acc:0.928]
Epoch [29/120    avg_loss:0.097, val_acc:0.942]
Epoch [30/120    avg_loss:0.091, val_acc:0.928]
Epoch [31/120    avg_loss:0.140, val_acc:0.946]
Epoch [32/120    avg_loss:0.084, val_acc:0.950]
Epoch [33/120    avg_loss:0.070, val_acc:0.917]
Epoch [34/120    avg_loss:0.065, val_acc:0.951]
Epoch [35/120    avg_loss:0.068, val_acc:0.938]
Epoch [36/120    avg_loss:0.068, val_acc:0.960]
Epoch [37/120    avg_loss:0.078, val_acc:0.948]
Epoch [38/120    avg_loss:0.066, val_acc:0.921]
Epoch [39/120    avg_loss:0.071, val_acc:0.954]
Epoch [40/120    avg_loss:0.052, val_acc:0.956]
Epoch [41/120    avg_loss:0.056, val_acc:0.968]
Epoch [42/120    avg_loss:0.041, val_acc:0.971]
Epoch [43/120    avg_loss:0.036, val_acc:0.958]
Epoch [44/120    avg_loss:0.044, val_acc:0.955]
Epoch [45/120    avg_loss:0.049, val_acc:0.973]
Epoch [46/120    avg_loss:0.029, val_acc:0.968]
Epoch [47/120    avg_loss:0.032, val_acc:0.973]
Epoch [48/120    avg_loss:0.042, val_acc:0.970]
Epoch [49/120    avg_loss:0.039, val_acc:0.968]
Epoch [50/120    avg_loss:0.053, val_acc:0.966]
Epoch [51/120    avg_loss:0.035, val_acc:0.961]
Epoch [52/120    avg_loss:0.029, val_acc:0.935]
Epoch [53/120    avg_loss:0.037, val_acc:0.968]
Epoch [54/120    avg_loss:0.027, val_acc:0.962]
Epoch [55/120    avg_loss:0.031, val_acc:0.966]
Epoch [56/120    avg_loss:0.028, val_acc:0.964]
Epoch [57/120    avg_loss:0.018, val_acc:0.968]
Epoch [58/120    avg_loss:0.018, val_acc:0.976]
Epoch [59/120    avg_loss:0.015, val_acc:0.973]
Epoch [60/120    avg_loss:0.018, val_acc:0.971]
Epoch [61/120    avg_loss:0.011, val_acc:0.972]
Epoch [62/120    avg_loss:0.017, val_acc:0.975]
Epoch [63/120    avg_loss:0.023, val_acc:0.972]
Epoch [64/120    avg_loss:0.021, val_acc:0.977]
Epoch [65/120    avg_loss:0.021, val_acc:0.969]
Epoch [66/120    avg_loss:0.019, val_acc:0.983]
Epoch [67/120    avg_loss:0.024, val_acc:0.945]
Epoch [68/120    avg_loss:0.030, val_acc:0.973]
Epoch [69/120    avg_loss:0.025, val_acc:0.957]
Epoch [70/120    avg_loss:0.016, val_acc:0.974]
Epoch [71/120    avg_loss:0.016, val_acc:0.971]
Epoch [72/120    avg_loss:0.016, val_acc:0.969]
Epoch [73/120    avg_loss:0.029, val_acc:0.963]
Epoch [74/120    avg_loss:0.031, val_acc:0.973]
Epoch [75/120    avg_loss:0.026, val_acc:0.972]
Epoch [76/120    avg_loss:0.015, val_acc:0.960]
Epoch [77/120    avg_loss:0.014, val_acc:0.978]
Epoch [78/120    avg_loss:0.014, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.008, val_acc:0.982]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.009, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6306     0    15     0     0     0     0    97    14]
 [    0     2 18036     0    13     0    35     0     4     0]
 [    0     3     0  1948     0     0     0     0    85     0]
 [    0    21     6     3  2919     0    15     0     4     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    67     0     0     0  4785     0    26     0]
 [    0    21     0     0     0     0     0  1268     0     1]
 [    0     2     0    53    33     0     2     0  3481     0]
 [    0     0     0     0     0     7     0     0     0   912]]

Accuracy:
98.71544597883981

F1 scores:
[       nan 0.98631423 0.99649162 0.96078915 0.98332491 0.99732518
 0.98507463 0.99139953 0.95789763 0.98594595]

Kappa:
0.9829784279113726
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99178fd7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.041, val_acc:0.182]
Epoch [2/120    avg_loss:1.603, val_acc:0.236]
Epoch [3/120    avg_loss:1.341, val_acc:0.296]
Epoch [4/120    avg_loss:1.186, val_acc:0.415]
Epoch [5/120    avg_loss:1.038, val_acc:0.674]
Epoch [6/120    avg_loss:0.886, val_acc:0.618]
Epoch [7/120    avg_loss:0.779, val_acc:0.773]
Epoch [8/120    avg_loss:0.669, val_acc:0.768]
Epoch [9/120    avg_loss:0.552, val_acc:0.735]
Epoch [10/120    avg_loss:0.495, val_acc:0.714]
Epoch [11/120    avg_loss:0.462, val_acc:0.674]
Epoch [12/120    avg_loss:0.438, val_acc:0.783]
Epoch [13/120    avg_loss:0.373, val_acc:0.728]
Epoch [14/120    avg_loss:0.359, val_acc:0.791]
Epoch [15/120    avg_loss:0.322, val_acc:0.809]
Epoch [16/120    avg_loss:0.369, val_acc:0.821]
Epoch [17/120    avg_loss:0.328, val_acc:0.848]
Epoch [18/120    avg_loss:0.273, val_acc:0.899]
Epoch [19/120    avg_loss:0.217, val_acc:0.909]
Epoch [20/120    avg_loss:0.215, val_acc:0.907]
Epoch [21/120    avg_loss:0.225, val_acc:0.925]
Epoch [22/120    avg_loss:0.177, val_acc:0.927]
Epoch [23/120    avg_loss:0.147, val_acc:0.930]
Epoch [24/120    avg_loss:0.133, val_acc:0.912]
Epoch [25/120    avg_loss:0.140, val_acc:0.888]
Epoch [26/120    avg_loss:0.124, val_acc:0.949]
Epoch [27/120    avg_loss:0.102, val_acc:0.924]
Epoch [28/120    avg_loss:0.109, val_acc:0.953]
Epoch [29/120    avg_loss:0.111, val_acc:0.941]
Epoch [30/120    avg_loss:0.084, val_acc:0.951]
Epoch [31/120    avg_loss:0.090, val_acc:0.927]
Epoch [32/120    avg_loss:0.091, val_acc:0.948]
Epoch [33/120    avg_loss:0.064, val_acc:0.961]
Epoch [34/120    avg_loss:0.058, val_acc:0.958]
Epoch [35/120    avg_loss:0.073, val_acc:0.940]
Epoch [36/120    avg_loss:0.068, val_acc:0.953]
Epoch [37/120    avg_loss:0.066, val_acc:0.954]
Epoch [38/120    avg_loss:0.051, val_acc:0.954]
Epoch [39/120    avg_loss:0.046, val_acc:0.968]
Epoch [40/120    avg_loss:0.060, val_acc:0.964]
Epoch [41/120    avg_loss:0.045, val_acc:0.956]
Epoch [42/120    avg_loss:0.036, val_acc:0.958]
Epoch [43/120    avg_loss:0.054, val_acc:0.959]
Epoch [44/120    avg_loss:0.058, val_acc:0.968]
Epoch [45/120    avg_loss:0.054, val_acc:0.962]
Epoch [46/120    avg_loss:0.073, val_acc:0.956]
Epoch [47/120    avg_loss:0.038, val_acc:0.965]
Epoch [48/120    avg_loss:0.032, val_acc:0.955]
Epoch [49/120    avg_loss:0.042, val_acc:0.968]
Epoch [50/120    avg_loss:0.036, val_acc:0.968]
Epoch [51/120    avg_loss:0.035, val_acc:0.968]
Epoch [52/120    avg_loss:0.031, val_acc:0.944]
Epoch [53/120    avg_loss:0.032, val_acc:0.966]
Epoch [54/120    avg_loss:0.025, val_acc:0.968]
Epoch [55/120    avg_loss:0.032, val_acc:0.968]
Epoch [56/120    avg_loss:0.037, val_acc:0.971]
Epoch [57/120    avg_loss:0.043, val_acc:0.907]
Epoch [58/120    avg_loss:0.039, val_acc:0.934]
Epoch [59/120    avg_loss:0.026, val_acc:0.956]
Epoch [60/120    avg_loss:0.024, val_acc:0.956]
Epoch [61/120    avg_loss:0.024, val_acc:0.958]
Epoch [62/120    avg_loss:0.018, val_acc:0.962]
Epoch [63/120    avg_loss:0.019, val_acc:0.973]
Epoch [64/120    avg_loss:0.023, val_acc:0.951]
Epoch [65/120    avg_loss:0.022, val_acc:0.963]
Epoch [66/120    avg_loss:0.025, val_acc:0.958]
Epoch [67/120    avg_loss:0.028, val_acc:0.939]
Epoch [68/120    avg_loss:0.022, val_acc:0.971]
Epoch [69/120    avg_loss:0.020, val_acc:0.968]
Epoch [70/120    avg_loss:0.020, val_acc:0.975]
Epoch [71/120    avg_loss:0.014, val_acc:0.975]
Epoch [72/120    avg_loss:0.016, val_acc:0.973]
Epoch [73/120    avg_loss:0.017, val_acc:0.969]
Epoch [74/120    avg_loss:0.017, val_acc:0.947]
Epoch [75/120    avg_loss:0.019, val_acc:0.970]
Epoch [76/120    avg_loss:0.038, val_acc:0.953]
Epoch [77/120    avg_loss:0.021, val_acc:0.971]
Epoch [78/120    avg_loss:0.018, val_acc:0.941]
Epoch [79/120    avg_loss:0.046, val_acc:0.974]
Epoch [80/120    avg_loss:0.021, val_acc:0.970]
Epoch [81/120    avg_loss:0.021, val_acc:0.958]
Epoch [82/120    avg_loss:0.032, val_acc:0.973]
Epoch [83/120    avg_loss:0.020, val_acc:0.974]
Epoch [84/120    avg_loss:0.014, val_acc:0.977]
Epoch [85/120    avg_loss:0.023, val_acc:0.970]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.008, val_acc:0.974]
Epoch [89/120    avg_loss:0.019, val_acc:0.967]
Epoch [90/120    avg_loss:0.009, val_acc:0.976]
Epoch [91/120    avg_loss:0.011, val_acc:0.975]
Epoch [92/120    avg_loss:0.013, val_acc:0.978]
Epoch [93/120    avg_loss:0.023, val_acc:0.961]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.009, val_acc:0.973]
Epoch [96/120    avg_loss:0.012, val_acc:0.974]
Epoch [97/120    avg_loss:0.010, val_acc:0.979]
Epoch [98/120    avg_loss:0.009, val_acc:0.975]
Epoch [99/120    avg_loss:0.007, val_acc:0.979]
Epoch [100/120    avg_loss:0.006, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.007, val_acc:0.978]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.008, val_acc:0.958]
Epoch [106/120    avg_loss:0.014, val_acc:0.978]
Epoch [107/120    avg_loss:0.021, val_acc:0.978]
Epoch [108/120    avg_loss:0.007, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.977]
Epoch [110/120    avg_loss:0.007, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.979]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.004, val_acc:0.981]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6347     0     4     1     0     0    19    50    11]
 [    0     0 18037     0    40     0    11     0     2     0]
 [    0     3     0  1909     0     0     0     0   124     0]
 [    0    15     8     0  2923     0    13     4     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    52     0     9     0  4781     0    36     0]
 [    0    19     0     0     0     0     0  1269     0     2]
 [    0    43     1    60    16     0    25     4  3422     0]
 [    0     0     0     0     0     5     0     0     0   914]]

Accuracy:
98.58771359024414

F1 scores:
[       nan 0.98716852 0.99684978 0.9523572  0.98070793 0.99808795
 0.98496086 0.98143852 0.94910553 0.98864251]

Kappa:
0.9812853229321974
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f169c7828>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.963, val_acc:0.239]
Epoch [2/120    avg_loss:1.564, val_acc:0.261]
Epoch [3/120    avg_loss:1.309, val_acc:0.430]
Epoch [4/120    avg_loss:1.150, val_acc:0.600]
Epoch [5/120    avg_loss:0.979, val_acc:0.681]
Epoch [6/120    avg_loss:0.845, val_acc:0.689]
Epoch [7/120    avg_loss:0.738, val_acc:0.671]
Epoch [8/120    avg_loss:0.631, val_acc:0.720]
Epoch [9/120    avg_loss:0.561, val_acc:0.677]
Epoch [10/120    avg_loss:0.500, val_acc:0.725]
Epoch [11/120    avg_loss:0.468, val_acc:0.682]
Epoch [12/120    avg_loss:0.435, val_acc:0.740]
Epoch [13/120    avg_loss:0.370, val_acc:0.714]
Epoch [14/120    avg_loss:0.348, val_acc:0.817]
Epoch [15/120    avg_loss:0.312, val_acc:0.760]
Epoch [16/120    avg_loss:0.314, val_acc:0.818]
Epoch [17/120    avg_loss:0.287, val_acc:0.805]
Epoch [18/120    avg_loss:0.268, val_acc:0.800]
Epoch [19/120    avg_loss:0.239, val_acc:0.815]
Epoch [20/120    avg_loss:0.219, val_acc:0.841]
Epoch [21/120    avg_loss:0.197, val_acc:0.883]
Epoch [22/120    avg_loss:0.194, val_acc:0.933]
Epoch [23/120    avg_loss:0.166, val_acc:0.921]
Epoch [24/120    avg_loss:0.167, val_acc:0.921]
Epoch [25/120    avg_loss:0.138, val_acc:0.928]
Epoch [26/120    avg_loss:0.129, val_acc:0.922]
Epoch [27/120    avg_loss:0.125, val_acc:0.926]
Epoch [28/120    avg_loss:0.099, val_acc:0.945]
Epoch [29/120    avg_loss:0.096, val_acc:0.951]
Epoch [30/120    avg_loss:0.104, val_acc:0.949]
Epoch [31/120    avg_loss:0.072, val_acc:0.939]
Epoch [32/120    avg_loss:0.068, val_acc:0.953]
Epoch [33/120    avg_loss:0.076, val_acc:0.943]
Epoch [34/120    avg_loss:0.094, val_acc:0.937]
Epoch [35/120    avg_loss:0.069, val_acc:0.960]
Epoch [36/120    avg_loss:0.054, val_acc:0.942]
Epoch [37/120    avg_loss:0.092, val_acc:0.960]
Epoch [38/120    avg_loss:0.075, val_acc:0.959]
Epoch [39/120    avg_loss:0.059, val_acc:0.960]
Epoch [40/120    avg_loss:0.054, val_acc:0.925]
Epoch [41/120    avg_loss:0.073, val_acc:0.958]
Epoch [42/120    avg_loss:0.044, val_acc:0.970]
Epoch [43/120    avg_loss:0.049, val_acc:0.960]
Epoch [44/120    avg_loss:0.042, val_acc:0.958]
Epoch [45/120    avg_loss:0.039, val_acc:0.977]
Epoch [46/120    avg_loss:0.047, val_acc:0.970]
Epoch [47/120    avg_loss:0.043, val_acc:0.964]
Epoch [48/120    avg_loss:0.032, val_acc:0.968]
Epoch [49/120    avg_loss:0.032, val_acc:0.968]
Epoch [50/120    avg_loss:0.050, val_acc:0.934]
Epoch [51/120    avg_loss:0.043, val_acc:0.964]
Epoch [52/120    avg_loss:0.035, val_acc:0.968]
Epoch [53/120    avg_loss:0.058, val_acc:0.953]
Epoch [54/120    avg_loss:0.044, val_acc:0.951]
Epoch [55/120    avg_loss:0.030, val_acc:0.972]
Epoch [56/120    avg_loss:0.025, val_acc:0.977]
Epoch [57/120    avg_loss:0.024, val_acc:0.978]
Epoch [58/120    avg_loss:0.016, val_acc:0.979]
Epoch [59/120    avg_loss:0.019, val_acc:0.971]
Epoch [60/120    avg_loss:0.013, val_acc:0.980]
Epoch [61/120    avg_loss:0.014, val_acc:0.978]
Epoch [62/120    avg_loss:0.021, val_acc:0.978]
Epoch [63/120    avg_loss:0.035, val_acc:0.973]
Epoch [64/120    avg_loss:0.029, val_acc:0.965]
Epoch [65/120    avg_loss:0.037, val_acc:0.975]
Epoch [66/120    avg_loss:0.022, val_acc:0.976]
Epoch [67/120    avg_loss:0.019, val_acc:0.956]
Epoch [68/120    avg_loss:0.051, val_acc:0.972]
Epoch [69/120    avg_loss:0.021, val_acc:0.968]
Epoch [70/120    avg_loss:0.032, val_acc:0.951]
Epoch [71/120    avg_loss:0.048, val_acc:0.978]
Epoch [72/120    avg_loss:0.032, val_acc:0.966]
Epoch [73/120    avg_loss:0.027, val_acc:0.978]
Epoch [74/120    avg_loss:0.017, val_acc:0.978]
Epoch [75/120    avg_loss:0.018, val_acc:0.979]
Epoch [76/120    avg_loss:0.012, val_acc:0.979]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.016, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.012, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6332     0     0     0     0     4     5    91     0]
 [    0     2 18043     0     9     0    33     0     3     0]
 [    0     8     0  1957     0     0     0     0    71     0]
 [    0    22     7     0  2926     0     8     0     6     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     3     0     0     0  4872     0     3     0]
 [    0    19     0     0     0     0     0  1268     0     3]
 [    0    34     1    17    38     0     0     0  3481     0]
 [    0     0     0     0    16     5     0     0     0   898]]

Accuracy:
99.00947147711662

F1 scores:
[       nan 0.98560199 0.99839531 0.97605985 0.98171448 0.99808795
 0.99479326 0.98946547 0.96346526 0.98518925]

Kappa:
0.9868785586589167
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca7f8758d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.043, val_acc:0.087]
Epoch [2/120    avg_loss:1.629, val_acc:0.163]
Epoch [3/120    avg_loss:1.389, val_acc:0.247]
Epoch [4/120    avg_loss:1.232, val_acc:0.370]
Epoch [5/120    avg_loss:1.115, val_acc:0.477]
Epoch [6/120    avg_loss:0.972, val_acc:0.483]
Epoch [7/120    avg_loss:0.847, val_acc:0.560]
Epoch [8/120    avg_loss:0.744, val_acc:0.603]
Epoch [9/120    avg_loss:0.625, val_acc:0.642]
Epoch [10/120    avg_loss:0.556, val_acc:0.713]
Epoch [11/120    avg_loss:0.463, val_acc:0.734]
Epoch [12/120    avg_loss:0.405, val_acc:0.747]
Epoch [13/120    avg_loss:0.383, val_acc:0.797]
Epoch [14/120    avg_loss:0.339, val_acc:0.794]
Epoch [15/120    avg_loss:0.272, val_acc:0.797]
Epoch [16/120    avg_loss:0.283, val_acc:0.868]
Epoch [17/120    avg_loss:0.263, val_acc:0.819]
Epoch [18/120    avg_loss:0.255, val_acc:0.878]
Epoch [19/120    avg_loss:0.231, val_acc:0.815]
Epoch [20/120    avg_loss:0.190, val_acc:0.948]
Epoch [21/120    avg_loss:0.166, val_acc:0.932]
Epoch [22/120    avg_loss:0.145, val_acc:0.916]
Epoch [23/120    avg_loss:0.134, val_acc:0.945]
Epoch [24/120    avg_loss:0.137, val_acc:0.890]
Epoch [25/120    avg_loss:0.143, val_acc:0.943]
Epoch [26/120    avg_loss:0.110, val_acc:0.942]
Epoch [27/120    avg_loss:0.143, val_acc:0.946]
Epoch [28/120    avg_loss:0.090, val_acc:0.956]
Epoch [29/120    avg_loss:0.071, val_acc:0.944]
Epoch [30/120    avg_loss:0.111, val_acc:0.946]
Epoch [31/120    avg_loss:0.110, val_acc:0.961]
Epoch [32/120    avg_loss:0.084, val_acc:0.955]
Epoch [33/120    avg_loss:0.080, val_acc:0.953]
Epoch [34/120    avg_loss:0.069, val_acc:0.973]
Epoch [35/120    avg_loss:0.068, val_acc:0.932]
Epoch [36/120    avg_loss:0.082, val_acc:0.956]
Epoch [37/120    avg_loss:0.067, val_acc:0.970]
Epoch [38/120    avg_loss:0.064, val_acc:0.953]
Epoch [39/120    avg_loss:0.059, val_acc:0.972]
Epoch [40/120    avg_loss:0.048, val_acc:0.950]
Epoch [41/120    avg_loss:0.069, val_acc:0.974]
Epoch [42/120    avg_loss:0.051, val_acc:0.973]
Epoch [43/120    avg_loss:0.053, val_acc:0.973]
Epoch [44/120    avg_loss:0.040, val_acc:0.949]
Epoch [45/120    avg_loss:0.036, val_acc:0.967]
Epoch [46/120    avg_loss:0.047, val_acc:0.964]
Epoch [47/120    avg_loss:0.050, val_acc:0.960]
Epoch [48/120    avg_loss:0.040, val_acc:0.964]
Epoch [49/120    avg_loss:0.048, val_acc:0.973]
Epoch [50/120    avg_loss:0.029, val_acc:0.970]
Epoch [51/120    avg_loss:0.037, val_acc:0.970]
Epoch [52/120    avg_loss:0.031, val_acc:0.963]
Epoch [53/120    avg_loss:0.025, val_acc:0.962]
Epoch [54/120    avg_loss:0.038, val_acc:0.961]
Epoch [55/120    avg_loss:0.021, val_acc:0.975]
Epoch [56/120    avg_loss:0.020, val_acc:0.980]
Epoch [57/120    avg_loss:0.018, val_acc:0.982]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.016, val_acc:0.979]
Epoch [60/120    avg_loss:0.016, val_acc:0.979]
Epoch [61/120    avg_loss:0.019, val_acc:0.978]
Epoch [62/120    avg_loss:0.015, val_acc:0.979]
Epoch [63/120    avg_loss:0.015, val_acc:0.978]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.016, val_acc:0.978]
Epoch [66/120    avg_loss:0.015, val_acc:0.978]
Epoch [67/120    avg_loss:0.017, val_acc:0.977]
Epoch [68/120    avg_loss:0.014, val_acc:0.979]
Epoch [69/120    avg_loss:0.016, val_acc:0.979]
Epoch [70/120    avg_loss:0.014, val_acc:0.981]
Epoch [71/120    avg_loss:0.015, val_acc:0.980]
Epoch [72/120    avg_loss:0.017, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.980]
Epoch [74/120    avg_loss:0.015, val_acc:0.980]
Epoch [75/120    avg_loss:0.016, val_acc:0.980]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.015, val_acc:0.980]
Epoch [78/120    avg_loss:0.013, val_acc:0.979]
Epoch [79/120    avg_loss:0.015, val_acc:0.979]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.014, val_acc:0.979]
Epoch [82/120    avg_loss:0.016, val_acc:0.979]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.017, val_acc:0.979]
Epoch [85/120    avg_loss:0.019, val_acc:0.979]
Epoch [86/120    avg_loss:0.018, val_acc:0.979]
Epoch [87/120    avg_loss:0.014, val_acc:0.979]
Epoch [88/120    avg_loss:0.014, val_acc:0.979]
Epoch [89/120    avg_loss:0.016, val_acc:0.979]
Epoch [90/120    avg_loss:0.013, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.979]
Epoch [92/120    avg_loss:0.014, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.979]
Epoch [94/120    avg_loss:0.018, val_acc:0.979]
Epoch [95/120    avg_loss:0.014, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.015, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.979]
Epoch [99/120    avg_loss:0.015, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.979]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.015, val_acc:0.979]
Epoch [103/120    avg_loss:0.019, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.012, val_acc:0.979]
Epoch [106/120    avg_loss:0.015, val_acc:0.979]
Epoch [107/120    avg_loss:0.016, val_acc:0.979]
Epoch [108/120    avg_loss:0.015, val_acc:0.979]
Epoch [109/120    avg_loss:0.013, val_acc:0.979]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.013, val_acc:0.979]
Epoch [112/120    avg_loss:0.015, val_acc:0.979]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.015, val_acc:0.979]
Epoch [115/120    avg_loss:0.018, val_acc:0.979]
Epoch [116/120    avg_loss:0.014, val_acc:0.979]
Epoch [117/120    avg_loss:0.013, val_acc:0.979]
Epoch [118/120    avg_loss:0.016, val_acc:0.979]
Epoch [119/120    avg_loss:0.018, val_acc:0.979]
Epoch [120/120    avg_loss:0.016, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6400     0     0     3     0     0     0    23     6]
 [    0     3 18007     0    62     0    15     0     3     0]
 [    0     8     0  1902     0     0     0     0   123     3]
 [    0    10    10     1  2919     0    23     0     2     7]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    50     0     0     0  4828     0     0     0]
 [    0    36     0     0     0     0     0  1250     4     0]
 [    0    31     0    75    33     0    12     1  3419     0]
 [    0     0     0     0     2    16     0     0     0   901]]

Accuracy:
98.64555467187236

F1 scores:
[       nan 0.99071207 0.99604503 0.94768311 0.97446169 0.99390708
 0.9897499  0.98386462 0.95703289 0.98148148]

Kappa:
0.9820535386840242
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6e76bb7940>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.034, val_acc:0.510]
Epoch [2/120    avg_loss:1.657, val_acc:0.534]
Epoch [3/120    avg_loss:1.399, val_acc:0.590]
Epoch [4/120    avg_loss:1.183, val_acc:0.564]
Epoch [5/120    avg_loss:1.040, val_acc:0.584]
Epoch [6/120    avg_loss:0.899, val_acc:0.743]
Epoch [7/120    avg_loss:0.776, val_acc:0.687]
Epoch [8/120    avg_loss:0.663, val_acc:0.660]
Epoch [9/120    avg_loss:0.552, val_acc:0.727]
Epoch [10/120    avg_loss:0.488, val_acc:0.748]
Epoch [11/120    avg_loss:0.427, val_acc:0.736]
Epoch [12/120    avg_loss:0.361, val_acc:0.758]
Epoch [13/120    avg_loss:0.350, val_acc:0.789]
Epoch [14/120    avg_loss:0.318, val_acc:0.750]
Epoch [15/120    avg_loss:0.298, val_acc:0.783]
Epoch [16/120    avg_loss:0.258, val_acc:0.841]
Epoch [17/120    avg_loss:0.252, val_acc:0.897]
Epoch [18/120    avg_loss:0.209, val_acc:0.925]
Epoch [19/120    avg_loss:0.218, val_acc:0.863]
Epoch [20/120    avg_loss:0.225, val_acc:0.817]
Epoch [21/120    avg_loss:0.176, val_acc:0.943]
Epoch [22/120    avg_loss:0.158, val_acc:0.924]
Epoch [23/120    avg_loss:0.134, val_acc:0.927]
Epoch [24/120    avg_loss:0.136, val_acc:0.917]
Epoch [25/120    avg_loss:0.124, val_acc:0.938]
Epoch [26/120    avg_loss:0.112, val_acc:0.951]
Epoch [27/120    avg_loss:0.125, val_acc:0.958]
Epoch [28/120    avg_loss:0.138, val_acc:0.927]
Epoch [29/120    avg_loss:0.126, val_acc:0.964]
Epoch [30/120    avg_loss:0.097, val_acc:0.953]
Epoch [31/120    avg_loss:0.102, val_acc:0.938]
Epoch [32/120    avg_loss:0.085, val_acc:0.964]
Epoch [33/120    avg_loss:0.071, val_acc:0.963]
Epoch [34/120    avg_loss:0.062, val_acc:0.962]
Epoch [35/120    avg_loss:0.054, val_acc:0.969]
Epoch [36/120    avg_loss:0.051, val_acc:0.943]
Epoch [37/120    avg_loss:0.067, val_acc:0.968]
Epoch [38/120    avg_loss:0.054, val_acc:0.974]
Epoch [39/120    avg_loss:0.042, val_acc:0.975]
Epoch [40/120    avg_loss:0.039, val_acc:0.975]
Epoch [41/120    avg_loss:0.033, val_acc:0.977]
Epoch [42/120    avg_loss:0.041, val_acc:0.978]
Epoch [43/120    avg_loss:0.056, val_acc:0.978]
Epoch [44/120    avg_loss:0.031, val_acc:0.974]
Epoch [45/120    avg_loss:0.026, val_acc:0.976]
Epoch [46/120    avg_loss:0.032, val_acc:0.972]
Epoch [47/120    avg_loss:0.025, val_acc:0.974]
Epoch [48/120    avg_loss:0.031, val_acc:0.973]
Epoch [49/120    avg_loss:0.053, val_acc:0.959]
Epoch [50/120    avg_loss:0.053, val_acc:0.978]
Epoch [51/120    avg_loss:0.036, val_acc:0.977]
Epoch [52/120    avg_loss:0.028, val_acc:0.981]
Epoch [53/120    avg_loss:0.042, val_acc:0.975]
Epoch [54/120    avg_loss:0.028, val_acc:0.976]
Epoch [55/120    avg_loss:0.030, val_acc:0.980]
Epoch [56/120    avg_loss:0.030, val_acc:0.975]
Epoch [57/120    avg_loss:0.031, val_acc:0.943]
Epoch [58/120    avg_loss:0.034, val_acc:0.976]
Epoch [59/120    avg_loss:0.046, val_acc:0.974]
Epoch [60/120    avg_loss:0.058, val_acc:0.971]
Epoch [61/120    avg_loss:0.030, val_acc:0.975]
Epoch [62/120    avg_loss:0.038, val_acc:0.977]
Epoch [63/120    avg_loss:0.028, val_acc:0.980]
Epoch [64/120    avg_loss:0.026, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.983]
Epoch [66/120    avg_loss:0.015, val_acc:0.985]
Epoch [67/120    avg_loss:0.016, val_acc:0.974]
Epoch [68/120    avg_loss:0.034, val_acc:0.958]
Epoch [69/120    avg_loss:0.032, val_acc:0.984]
Epoch [70/120    avg_loss:0.031, val_acc:0.976]
Epoch [71/120    avg_loss:0.032, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.956]
Epoch [74/120    avg_loss:0.016, val_acc:0.985]
Epoch [75/120    avg_loss:0.014, val_acc:0.981]
Epoch [76/120    avg_loss:0.014, val_acc:0.979]
Epoch [77/120    avg_loss:0.011, val_acc:0.984]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.985]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.973]
Epoch [86/120    avg_loss:0.016, val_acc:0.982]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.020, val_acc:0.983]
Epoch [89/120    avg_loss:0.011, val_acc:0.981]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.016, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.972]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.010, val_acc:0.972]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.033, val_acc:0.976]
Epoch [105/120    avg_loss:0.034, val_acc:0.978]
Epoch [106/120    avg_loss:0.022, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.988]
Epoch [108/120    avg_loss:0.015, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6370     0     0     0     0     0     2    56     4]
 [    0     0 18061     0     8     0    18     0     3     0]
 [    0     7     0  1898     0     0     0     0   131     0]
 [    0     9     9     0  2941     0     7     0     1     5]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    28     0     0     0  4839     0    11     0]
 [    0    10     0     0     0     0     0  1280     0     0]
 [    0    20     0    26    61     0     1     0  3463     0]
 [    0     4     0     0     0     5     0     0     0   910]]

Accuracy:
98.97332080109898

F1 scores:
[       nan 0.9912854  0.99817619 0.95858586 0.98328318 0.99808795
 0.99332854 0.99533437 0.95715865 0.99020675]

Kappa:
0.9863935671369617
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:9
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ceeb67780>
supervision:full
center_pixel:True
Network :
Number of parameter: 17631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.994, val_acc:0.116]
Epoch [2/120    avg_loss:1.578, val_acc:0.272]
Epoch [3/120    avg_loss:1.362, val_acc:0.378]
Epoch [4/120    avg_loss:1.166, val_acc:0.460]
Epoch [5/120    avg_loss:1.036, val_acc:0.406]
Epoch [6/120    avg_loss:0.919, val_acc:0.698]
Epoch [7/120    avg_loss:0.799, val_acc:0.558]
Epoch [8/120    avg_loss:0.681, val_acc:0.554]
Epoch [9/120    avg_loss:0.617, val_acc:0.618]
Epoch [10/120    avg_loss:0.533, val_acc:0.600]
Epoch [11/120    avg_loss:0.475, val_acc:0.688]
Epoch [12/120    avg_loss:0.441, val_acc:0.718]
Epoch [13/120    avg_loss:0.389, val_acc:0.776]
Epoch [14/120    avg_loss:0.402, val_acc:0.748]
Epoch [15/120    avg_loss:0.351, val_acc:0.815]
Epoch [16/120    avg_loss:0.338, val_acc:0.820]
Epoch [17/120    avg_loss:0.291, val_acc:0.853]
Epoch [18/120    avg_loss:0.252, val_acc:0.807]
Epoch [19/120    avg_loss:0.272, val_acc:0.839]
Epoch [20/120    avg_loss:0.261, val_acc:0.838]
Epoch [21/120    avg_loss:0.228, val_acc:0.892]
Epoch [22/120    avg_loss:0.214, val_acc:0.908]
Epoch [23/120    avg_loss:0.178, val_acc:0.932]
Epoch [24/120    avg_loss:0.164, val_acc:0.936]
Epoch [25/120    avg_loss:0.163, val_acc:0.912]
Epoch [26/120    avg_loss:0.174, val_acc:0.940]
Epoch [27/120    avg_loss:0.139, val_acc:0.935]
Epoch [28/120    avg_loss:0.117, val_acc:0.953]
Epoch [29/120    avg_loss:0.108, val_acc:0.936]
Epoch [30/120    avg_loss:0.130, val_acc:0.911]
Epoch [31/120    avg_loss:0.110, val_acc:0.940]
Epoch [32/120    avg_loss:0.121, val_acc:0.931]
Epoch [33/120    avg_loss:0.113, val_acc:0.935]
Epoch [34/120    avg_loss:0.152, val_acc:0.938]
Epoch [35/120    avg_loss:0.126, val_acc:0.928]
Epoch [36/120    avg_loss:0.113, val_acc:0.940]
Epoch [37/120    avg_loss:0.097, val_acc:0.954]
Epoch [38/120    avg_loss:0.111, val_acc:0.938]
Epoch [39/120    avg_loss:0.099, val_acc:0.941]
Epoch [40/120    avg_loss:0.100, val_acc:0.943]
Epoch [41/120    avg_loss:0.055, val_acc:0.949]
Epoch [42/120    avg_loss:0.066, val_acc:0.956]
Epoch [43/120    avg_loss:0.068, val_acc:0.958]
Epoch [44/120    avg_loss:0.065, val_acc:0.952]
Epoch [45/120    avg_loss:0.081, val_acc:0.943]
Epoch [46/120    avg_loss:0.058, val_acc:0.956]
Epoch [47/120    avg_loss:0.053, val_acc:0.963]
Epoch [48/120    avg_loss:0.045, val_acc:0.959]
Epoch [49/120    avg_loss:0.051, val_acc:0.959]
Epoch [50/120    avg_loss:0.045, val_acc:0.953]
Epoch [51/120    avg_loss:0.043, val_acc:0.960]
Epoch [52/120    avg_loss:0.044, val_acc:0.963]
Epoch [53/120    avg_loss:0.031, val_acc:0.945]
Epoch [54/120    avg_loss:0.030, val_acc:0.959]
Epoch [55/120    avg_loss:0.025, val_acc:0.969]
Epoch [56/120    avg_loss:0.029, val_acc:0.968]
Epoch [57/120    avg_loss:0.048, val_acc:0.951]
Epoch [58/120    avg_loss:0.061, val_acc:0.960]
Epoch [59/120    avg_loss:0.044, val_acc:0.963]
Epoch [60/120    avg_loss:0.030, val_acc:0.963]
Epoch [61/120    avg_loss:0.037, val_acc:0.964]
Epoch [62/120    avg_loss:0.033, val_acc:0.967]
Epoch [63/120    avg_loss:0.029, val_acc:0.961]
Epoch [64/120    avg_loss:0.025, val_acc:0.963]
Epoch [65/120    avg_loss:0.017, val_acc:0.964]
Epoch [66/120    avg_loss:0.020, val_acc:0.962]
Epoch [67/120    avg_loss:0.018, val_acc:0.963]
Epoch [68/120    avg_loss:0.017, val_acc:0.968]
Epoch [69/120    avg_loss:0.020, val_acc:0.966]
Epoch [70/120    avg_loss:0.016, val_acc:0.965]
Epoch [71/120    avg_loss:0.015, val_acc:0.968]
Epoch [72/120    avg_loss:0.015, val_acc:0.968]
Epoch [73/120    avg_loss:0.014, val_acc:0.969]
Epoch [74/120    avg_loss:0.014, val_acc:0.968]
Epoch [75/120    avg_loss:0.015, val_acc:0.966]
Epoch [76/120    avg_loss:0.011, val_acc:0.968]
Epoch [77/120    avg_loss:0.011, val_acc:0.967]
Epoch [78/120    avg_loss:0.012, val_acc:0.967]
Epoch [79/120    avg_loss:0.013, val_acc:0.967]
Epoch [80/120    avg_loss:0.011, val_acc:0.967]
Epoch [81/120    avg_loss:0.012, val_acc:0.968]
Epoch [82/120    avg_loss:0.012, val_acc:0.968]
Epoch [83/120    avg_loss:0.014, val_acc:0.969]
Epoch [84/120    avg_loss:0.011, val_acc:0.968]
Epoch [85/120    avg_loss:0.009, val_acc:0.969]
Epoch [86/120    avg_loss:0.010, val_acc:0.967]
Epoch [87/120    avg_loss:0.010, val_acc:0.969]
Epoch [88/120    avg_loss:0.010, val_acc:0.967]
Epoch [89/120    avg_loss:0.012, val_acc:0.967]
Epoch [90/120    avg_loss:0.011, val_acc:0.968]
Epoch [91/120    avg_loss:0.010, val_acc:0.968]
Epoch [92/120    avg_loss:0.010, val_acc:0.968]
Epoch [93/120    avg_loss:0.013, val_acc:0.966]
Epoch [94/120    avg_loss:0.009, val_acc:0.968]
Epoch [95/120    avg_loss:0.010, val_acc:0.969]
Epoch [96/120    avg_loss:0.011, val_acc:0.968]
Epoch [97/120    avg_loss:0.011, val_acc:0.968]
Epoch [98/120    avg_loss:0.009, val_acc:0.968]
Epoch [99/120    avg_loss:0.010, val_acc:0.968]
Epoch [100/120    avg_loss:0.013, val_acc:0.968]
Epoch [101/120    avg_loss:0.014, val_acc:0.969]
Epoch [102/120    avg_loss:0.010, val_acc:0.970]
Epoch [103/120    avg_loss:0.009, val_acc:0.969]
Epoch [104/120    avg_loss:0.014, val_acc:0.968]
Epoch [105/120    avg_loss:0.010, val_acc:0.968]
Epoch [106/120    avg_loss:0.012, val_acc:0.968]
Epoch [107/120    avg_loss:0.010, val_acc:0.968]
Epoch [108/120    avg_loss:0.010, val_acc:0.968]
Epoch [109/120    avg_loss:0.009, val_acc:0.969]
Epoch [110/120    avg_loss:0.012, val_acc:0.968]
Epoch [111/120    avg_loss:0.011, val_acc:0.968]
Epoch [112/120    avg_loss:0.009, val_acc:0.969]
Epoch [113/120    avg_loss:0.010, val_acc:0.968]
Epoch [114/120    avg_loss:0.009, val_acc:0.968]
Epoch [115/120    avg_loss:0.010, val_acc:0.969]
Epoch [116/120    avg_loss:0.011, val_acc:0.969]
Epoch [117/120    avg_loss:0.010, val_acc:0.969]
Epoch [118/120    avg_loss:0.010, val_acc:0.969]
Epoch [119/120    avg_loss:0.010, val_acc:0.968]
Epoch [120/120    avg_loss:0.009, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6289     0     4     2     0    20     4    81    32]
 [    0     0 18049     0    31     0     4     0     6     0]
 [    0    19     0  1888     0     0     0     0   129     0]
 [    0    33     9     1  2911     0     7     0    10     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    51     0     0     0  4795     0    32     0]
 [    0    34     0     0     0     0     0  1251     5     0]
 [    0    58    10    69    25     0     2     1  3406     0]
 [    0     3     0     0    18    21     0     0     0   877]]

Accuracy:
98.25994746101752

F1 scores:
[       nan 0.97746348 0.99693446 0.94447224 0.97700957 0.99201824
 0.98804863 0.98271799 0.94088398 0.95899399]

Kappa:
0.9769339223238678
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96325458d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.066, val_acc:0.121]
Epoch [2/120    avg_loss:1.657, val_acc:0.180]
Epoch [3/120    avg_loss:1.452, val_acc:0.386]
Epoch [4/120    avg_loss:1.279, val_acc:0.394]
Epoch [5/120    avg_loss:1.118, val_acc:0.459]
Epoch [6/120    avg_loss:0.989, val_acc:0.482]
Epoch [7/120    avg_loss:0.872, val_acc:0.492]
Epoch [8/120    avg_loss:0.771, val_acc:0.540]
Epoch [9/120    avg_loss:0.669, val_acc:0.541]
Epoch [10/120    avg_loss:0.602, val_acc:0.734]
Epoch [11/120    avg_loss:0.528, val_acc:0.690]
Epoch [12/120    avg_loss:0.479, val_acc:0.735]
Epoch [13/120    avg_loss:0.465, val_acc:0.761]
Epoch [14/120    avg_loss:0.429, val_acc:0.795]
Epoch [15/120    avg_loss:0.363, val_acc:0.830]
Epoch [16/120    avg_loss:0.371, val_acc:0.793]
Epoch [17/120    avg_loss:0.313, val_acc:0.807]
Epoch [18/120    avg_loss:0.291, val_acc:0.803]
Epoch [19/120    avg_loss:0.275, val_acc:0.830]
Epoch [20/120    avg_loss:0.292, val_acc:0.803]
Epoch [21/120    avg_loss:0.253, val_acc:0.889]
Epoch [22/120    avg_loss:0.233, val_acc:0.857]
Epoch [23/120    avg_loss:0.220, val_acc:0.912]
Epoch [24/120    avg_loss:0.227, val_acc:0.869]
Epoch [25/120    avg_loss:0.196, val_acc:0.863]
Epoch [26/120    avg_loss:0.183, val_acc:0.914]
Epoch [27/120    avg_loss:0.159, val_acc:0.922]
Epoch [28/120    avg_loss:0.156, val_acc:0.803]
Epoch [29/120    avg_loss:0.167, val_acc:0.882]
Epoch [30/120    avg_loss:0.145, val_acc:0.895]
Epoch [31/120    avg_loss:0.117, val_acc:0.947]
Epoch [32/120    avg_loss:0.132, val_acc:0.955]
Epoch [33/120    avg_loss:0.090, val_acc:0.940]
Epoch [34/120    avg_loss:0.110, val_acc:0.947]
Epoch [35/120    avg_loss:0.102, val_acc:0.950]
Epoch [36/120    avg_loss:0.084, val_acc:0.969]
Epoch [37/120    avg_loss:0.073, val_acc:0.966]
Epoch [38/120    avg_loss:0.078, val_acc:0.959]
Epoch [39/120    avg_loss:0.073, val_acc:0.966]
Epoch [40/120    avg_loss:0.053, val_acc:0.961]
Epoch [41/120    avg_loss:0.076, val_acc:0.945]
Epoch [42/120    avg_loss:0.079, val_acc:0.964]
Epoch [43/120    avg_loss:0.056, val_acc:0.972]
Epoch [44/120    avg_loss:0.070, val_acc:0.959]
Epoch [45/120    avg_loss:0.063, val_acc:0.970]
Epoch [46/120    avg_loss:0.074, val_acc:0.888]
Epoch [47/120    avg_loss:0.074, val_acc:0.975]
Epoch [48/120    avg_loss:0.052, val_acc:0.977]
Epoch [49/120    avg_loss:0.043, val_acc:0.976]
Epoch [50/120    avg_loss:0.043, val_acc:0.958]
Epoch [51/120    avg_loss:0.056, val_acc:0.963]
Epoch [52/120    avg_loss:0.054, val_acc:0.971]
Epoch [53/120    avg_loss:0.052, val_acc:0.906]
Epoch [54/120    avg_loss:0.042, val_acc:0.981]
Epoch [55/120    avg_loss:0.027, val_acc:0.980]
Epoch [56/120    avg_loss:0.037, val_acc:0.974]
Epoch [57/120    avg_loss:0.053, val_acc:0.959]
Epoch [58/120    avg_loss:0.051, val_acc:0.969]
Epoch [59/120    avg_loss:0.024, val_acc:0.982]
Epoch [60/120    avg_loss:0.043, val_acc:0.972]
Epoch [61/120    avg_loss:0.049, val_acc:0.972]
Epoch [62/120    avg_loss:0.043, val_acc:0.978]
Epoch [63/120    avg_loss:0.032, val_acc:0.979]
Epoch [64/120    avg_loss:0.032, val_acc:0.981]
Epoch [65/120    avg_loss:0.031, val_acc:0.976]
Epoch [66/120    avg_loss:0.022, val_acc:0.979]
Epoch [67/120    avg_loss:0.020, val_acc:0.981]
Epoch [68/120    avg_loss:0.019, val_acc:0.984]
Epoch [69/120    avg_loss:0.015, val_acc:0.984]
Epoch [70/120    avg_loss:0.022, val_acc:0.982]
Epoch [71/120    avg_loss:0.027, val_acc:0.981]
Epoch [72/120    avg_loss:0.022, val_acc:0.981]
Epoch [73/120    avg_loss:0.049, val_acc:0.965]
Epoch [74/120    avg_loss:0.047, val_acc:0.938]
Epoch [75/120    avg_loss:0.028, val_acc:0.978]
Epoch [76/120    avg_loss:0.025, val_acc:0.984]
Epoch [77/120    avg_loss:0.022, val_acc:0.982]
Epoch [78/120    avg_loss:0.017, val_acc:0.979]
Epoch [79/120    avg_loss:0.015, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.985]
Epoch [81/120    avg_loss:0.017, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.980]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.987]
Epoch [88/120    avg_loss:0.014, val_acc:0.975]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.011, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.985]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6346     0     0     0     0     0     0    86     0]
 [    0     6 18044     0    32     0     0     0     8     0]
 [    0     4     0  2010     0     0     0     0    18     4]
 [    0    23     4     4  2937     0     0     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    23     2     0     0  4853     0     0     0]
 [    0     6     0     1     0     0     4  1279     0     0]
 [    0    36     0    37    50     0     0     0  3448     0]
 [    0     0     0     0    10    24     0     0     0   885]]

Accuracy:
99.06972260381269

F1 scores:
[       nan 0.98747374 0.99798125 0.98288509 0.97883686 0.99088838
 0.99702106 0.99571818 0.96663863 0.97844113]

Kappa:
0.9876765720908032
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e0ed25940>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.031, val_acc:0.087]
Epoch [2/120    avg_loss:1.676, val_acc:0.251]
Epoch [3/120    avg_loss:1.433, val_acc:0.316]
Epoch [4/120    avg_loss:1.260, val_acc:0.390]
Epoch [5/120    avg_loss:1.118, val_acc:0.448]
Epoch [6/120    avg_loss:0.967, val_acc:0.481]
Epoch [7/120    avg_loss:0.853, val_acc:0.594]
Epoch [8/120    avg_loss:0.741, val_acc:0.623]
Epoch [9/120    avg_loss:0.626, val_acc:0.583]
Epoch [10/120    avg_loss:0.524, val_acc:0.702]
Epoch [11/120    avg_loss:0.448, val_acc:0.741]
Epoch [12/120    avg_loss:0.413, val_acc:0.774]
Epoch [13/120    avg_loss:0.368, val_acc:0.762]
Epoch [14/120    avg_loss:0.368, val_acc:0.788]
Epoch [15/120    avg_loss:0.309, val_acc:0.793]
Epoch [16/120    avg_loss:0.327, val_acc:0.801]
Epoch [17/120    avg_loss:0.275, val_acc:0.802]
Epoch [18/120    avg_loss:0.236, val_acc:0.808]
Epoch [19/120    avg_loss:0.223, val_acc:0.849]
Epoch [20/120    avg_loss:0.230, val_acc:0.833]
Epoch [21/120    avg_loss:0.203, val_acc:0.811]
Epoch [22/120    avg_loss:0.193, val_acc:0.862]
Epoch [23/120    avg_loss:0.178, val_acc:0.910]
Epoch [24/120    avg_loss:0.153, val_acc:0.917]
Epoch [25/120    avg_loss:0.143, val_acc:0.929]
Epoch [26/120    avg_loss:0.150, val_acc:0.904]
Epoch [27/120    avg_loss:0.142, val_acc:0.948]
Epoch [28/120    avg_loss:0.154, val_acc:0.951]
Epoch [29/120    avg_loss:0.119, val_acc:0.953]
Epoch [30/120    avg_loss:0.108, val_acc:0.938]
Epoch [31/120    avg_loss:0.106, val_acc:0.954]
Epoch [32/120    avg_loss:0.079, val_acc:0.972]
Epoch [33/120    avg_loss:0.075, val_acc:0.977]
Epoch [34/120    avg_loss:0.066, val_acc:0.923]
Epoch [35/120    avg_loss:0.067, val_acc:0.965]
Epoch [36/120    avg_loss:0.066, val_acc:0.956]
Epoch [37/120    avg_loss:0.045, val_acc:0.980]
Epoch [38/120    avg_loss:0.052, val_acc:0.965]
Epoch [39/120    avg_loss:0.058, val_acc:0.928]
Epoch [40/120    avg_loss:0.048, val_acc:0.985]
Epoch [41/120    avg_loss:0.039, val_acc:0.983]
Epoch [42/120    avg_loss:0.042, val_acc:0.980]
Epoch [43/120    avg_loss:0.036, val_acc:0.983]
Epoch [44/120    avg_loss:0.034, val_acc:0.981]
Epoch [45/120    avg_loss:0.027, val_acc:0.985]
Epoch [46/120    avg_loss:0.029, val_acc:0.971]
Epoch [47/120    avg_loss:0.025, val_acc:0.989]
Epoch [48/120    avg_loss:0.020, val_acc:0.989]
Epoch [49/120    avg_loss:0.019, val_acc:0.989]
Epoch [50/120    avg_loss:0.019, val_acc:0.994]
Epoch [51/120    avg_loss:0.024, val_acc:0.989]
Epoch [52/120    avg_loss:0.020, val_acc:0.988]
Epoch [53/120    avg_loss:0.017, val_acc:0.978]
Epoch [54/120    avg_loss:0.026, val_acc:0.980]
Epoch [55/120    avg_loss:0.022, val_acc:0.990]
Epoch [56/120    avg_loss:0.029, val_acc:0.987]
Epoch [57/120    avg_loss:0.017, val_acc:0.987]
Epoch [58/120    avg_loss:0.022, val_acc:0.983]
Epoch [59/120    avg_loss:0.017, val_acc:0.984]
Epoch [60/120    avg_loss:0.012, val_acc:0.987]
Epoch [61/120    avg_loss:0.013, val_acc:0.993]
Epoch [62/120    avg_loss:0.009, val_acc:0.989]
Epoch [63/120    avg_loss:0.021, val_acc:0.988]
Epoch [64/120    avg_loss:0.015, val_acc:0.991]
Epoch [65/120    avg_loss:0.010, val_acc:0.992]
Epoch [66/120    avg_loss:0.010, val_acc:0.992]
Epoch [67/120    avg_loss:0.011, val_acc:0.992]
Epoch [68/120    avg_loss:0.009, val_acc:0.993]
Epoch [69/120    avg_loss:0.011, val_acc:0.993]
Epoch [70/120    avg_loss:0.008, val_acc:0.993]
Epoch [71/120    avg_loss:0.009, val_acc:0.992]
Epoch [72/120    avg_loss:0.009, val_acc:0.993]
Epoch [73/120    avg_loss:0.009, val_acc:0.993]
Epoch [74/120    avg_loss:0.008, val_acc:0.993]
Epoch [75/120    avg_loss:0.009, val_acc:0.993]
Epoch [76/120    avg_loss:0.010, val_acc:0.993]
Epoch [77/120    avg_loss:0.010, val_acc:0.993]
Epoch [78/120    avg_loss:0.010, val_acc:0.993]
Epoch [79/120    avg_loss:0.011, val_acc:0.993]
Epoch [80/120    avg_loss:0.010, val_acc:0.993]
Epoch [81/120    avg_loss:0.009, val_acc:0.993]
Epoch [82/120    avg_loss:0.009, val_acc:0.993]
Epoch [83/120    avg_loss:0.008, val_acc:0.993]
Epoch [84/120    avg_loss:0.008, val_acc:0.993]
Epoch [85/120    avg_loss:0.008, val_acc:0.993]
Epoch [86/120    avg_loss:0.010, val_acc:0.993]
Epoch [87/120    avg_loss:0.009, val_acc:0.993]
Epoch [88/120    avg_loss:0.007, val_acc:0.993]
Epoch [89/120    avg_loss:0.008, val_acc:0.993]
Epoch [90/120    avg_loss:0.009, val_acc:0.993]
Epoch [91/120    avg_loss:0.010, val_acc:0.993]
Epoch [92/120    avg_loss:0.009, val_acc:0.993]
Epoch [93/120    avg_loss:0.008, val_acc:0.993]
Epoch [94/120    avg_loss:0.008, val_acc:0.993]
Epoch [95/120    avg_loss:0.007, val_acc:0.993]
Epoch [96/120    avg_loss:0.009, val_acc:0.993]
Epoch [97/120    avg_loss:0.009, val_acc:0.993]
Epoch [98/120    avg_loss:0.007, val_acc:0.993]
Epoch [99/120    avg_loss:0.009, val_acc:0.993]
Epoch [100/120    avg_loss:0.009, val_acc:0.993]
Epoch [101/120    avg_loss:0.009, val_acc:0.993]
Epoch [102/120    avg_loss:0.008, val_acc:0.993]
Epoch [103/120    avg_loss:0.009, val_acc:0.993]
Epoch [104/120    avg_loss:0.008, val_acc:0.993]
Epoch [105/120    avg_loss:0.008, val_acc:0.993]
Epoch [106/120    avg_loss:0.011, val_acc:0.993]
Epoch [107/120    avg_loss:0.012, val_acc:0.993]
Epoch [108/120    avg_loss:0.008, val_acc:0.993]
Epoch [109/120    avg_loss:0.010, val_acc:0.993]
Epoch [110/120    avg_loss:0.009, val_acc:0.993]
Epoch [111/120    avg_loss:0.009, val_acc:0.993]
Epoch [112/120    avg_loss:0.007, val_acc:0.993]
Epoch [113/120    avg_loss:0.009, val_acc:0.993]
Epoch [114/120    avg_loss:0.008, val_acc:0.993]
Epoch [115/120    avg_loss:0.009, val_acc:0.993]
Epoch [116/120    avg_loss:0.009, val_acc:0.993]
Epoch [117/120    avg_loss:0.008, val_acc:0.993]
Epoch [118/120    avg_loss:0.008, val_acc:0.993]
Epoch [119/120    avg_loss:0.008, val_acc:0.993]
Epoch [120/120    avg_loss:0.008, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6321     0     0     0     0     1     4   100     6]
 [    0     1 18040     0    12     0    27     0    10     0]
 [    0     5     0  2000     0     0     0     0    27     4]
 [    0    29     2     2  2916     0     8     2    10     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     0     1     0     0  4874     0     3     0]
 [    0     0     0     0     0     0     0  1290     0     0]
 [    0    37     0    27    48     0     0     0  3454     5]
 [    0     2     0     0     1    10     0     0     0   906]]

Accuracy:
99.06731255874485

F1 scores:
[       nan 0.9855773  0.99856083 0.98376783 0.98033283 0.99618321
 0.99591336 0.99767981 0.96278746 0.9831796 ]

Kappa:
0.987648973885039
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc5c60b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.983, val_acc:0.171]
Epoch [2/120    avg_loss:1.645, val_acc:0.263]
Epoch [3/120    avg_loss:1.435, val_acc:0.410]
Epoch [4/120    avg_loss:1.239, val_acc:0.684]
Epoch [5/120    avg_loss:1.082, val_acc:0.687]
Epoch [6/120    avg_loss:0.941, val_acc:0.734]
Epoch [7/120    avg_loss:0.822, val_acc:0.741]
Epoch [8/120    avg_loss:0.696, val_acc:0.653]
Epoch [9/120    avg_loss:0.583, val_acc:0.735]
Epoch [10/120    avg_loss:0.500, val_acc:0.732]
Epoch [11/120    avg_loss:0.452, val_acc:0.753]
Epoch [12/120    avg_loss:0.399, val_acc:0.786]
Epoch [13/120    avg_loss:0.362, val_acc:0.761]
Epoch [14/120    avg_loss:0.345, val_acc:0.809]
Epoch [15/120    avg_loss:0.297, val_acc:0.816]
Epoch [16/120    avg_loss:0.297, val_acc:0.842]
Epoch [17/120    avg_loss:0.275, val_acc:0.837]
Epoch [18/120    avg_loss:0.231, val_acc:0.897]
Epoch [19/120    avg_loss:0.205, val_acc:0.905]
Epoch [20/120    avg_loss:0.186, val_acc:0.891]
Epoch [21/120    avg_loss:0.211, val_acc:0.937]
Epoch [22/120    avg_loss:0.161, val_acc:0.943]
Epoch [23/120    avg_loss:0.174, val_acc:0.964]
Epoch [24/120    avg_loss:0.127, val_acc:0.952]
Epoch [25/120    avg_loss:0.149, val_acc:0.947]
Epoch [26/120    avg_loss:0.166, val_acc:0.947]
Epoch [27/120    avg_loss:0.160, val_acc:0.957]
Epoch [28/120    avg_loss:0.131, val_acc:0.941]
Epoch [29/120    avg_loss:0.115, val_acc:0.956]
Epoch [30/120    avg_loss:0.131, val_acc:0.953]
Epoch [31/120    avg_loss:0.092, val_acc:0.973]
Epoch [32/120    avg_loss:0.084, val_acc:0.970]
Epoch [33/120    avg_loss:0.103, val_acc:0.950]
Epoch [34/120    avg_loss:0.070, val_acc:0.969]
Epoch [35/120    avg_loss:0.066, val_acc:0.953]
Epoch [36/120    avg_loss:0.054, val_acc:0.983]
Epoch [37/120    avg_loss:0.059, val_acc:0.970]
Epoch [38/120    avg_loss:0.053, val_acc:0.975]
Epoch [39/120    avg_loss:0.043, val_acc:0.986]
Epoch [40/120    avg_loss:0.033, val_acc:0.979]
Epoch [41/120    avg_loss:0.035, val_acc:0.974]
Epoch [42/120    avg_loss:0.053, val_acc:0.962]
Epoch [43/120    avg_loss:0.091, val_acc:0.947]
Epoch [44/120    avg_loss:0.073, val_acc:0.958]
Epoch [45/120    avg_loss:0.080, val_acc:0.979]
Epoch [46/120    avg_loss:0.062, val_acc:0.959]
Epoch [47/120    avg_loss:0.069, val_acc:0.959]
Epoch [48/120    avg_loss:0.046, val_acc:0.979]
Epoch [49/120    avg_loss:0.047, val_acc:0.972]
Epoch [50/120    avg_loss:0.052, val_acc:0.976]
Epoch [51/120    avg_loss:0.045, val_acc:0.985]
Epoch [52/120    avg_loss:0.044, val_acc:0.971]
Epoch [53/120    avg_loss:0.033, val_acc:0.983]
Epoch [54/120    avg_loss:0.026, val_acc:0.984]
Epoch [55/120    avg_loss:0.020, val_acc:0.984]
Epoch [56/120    avg_loss:0.019, val_acc:0.985]
Epoch [57/120    avg_loss:0.023, val_acc:0.985]
Epoch [58/120    avg_loss:0.019, val_acc:0.987]
Epoch [59/120    avg_loss:0.020, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.987]
Epoch [61/120    avg_loss:0.018, val_acc:0.988]
Epoch [62/120    avg_loss:0.021, val_acc:0.987]
Epoch [63/120    avg_loss:0.019, val_acc:0.988]
Epoch [64/120    avg_loss:0.020, val_acc:0.991]
Epoch [65/120    avg_loss:0.019, val_acc:0.990]
Epoch [66/120    avg_loss:0.014, val_acc:0.990]
Epoch [67/120    avg_loss:0.017, val_acc:0.989]
Epoch [68/120    avg_loss:0.016, val_acc:0.989]
Epoch [69/120    avg_loss:0.021, val_acc:0.987]
Epoch [70/120    avg_loss:0.017, val_acc:0.988]
Epoch [71/120    avg_loss:0.018, val_acc:0.988]
Epoch [72/120    avg_loss:0.015, val_acc:0.987]
Epoch [73/120    avg_loss:0.016, val_acc:0.986]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.014, val_acc:0.986]
Epoch [76/120    avg_loss:0.014, val_acc:0.987]
Epoch [77/120    avg_loss:0.015, val_acc:0.988]
Epoch [78/120    avg_loss:0.015, val_acc:0.987]
Epoch [79/120    avg_loss:0.013, val_acc:0.987]
Epoch [80/120    avg_loss:0.014, val_acc:0.987]
Epoch [81/120    avg_loss:0.015, val_acc:0.987]
Epoch [82/120    avg_loss:0.015, val_acc:0.987]
Epoch [83/120    avg_loss:0.016, val_acc:0.987]
Epoch [84/120    avg_loss:0.014, val_acc:0.987]
Epoch [85/120    avg_loss:0.017, val_acc:0.987]
Epoch [86/120    avg_loss:0.013, val_acc:0.988]
Epoch [87/120    avg_loss:0.014, val_acc:0.988]
Epoch [88/120    avg_loss:0.015, val_acc:0.987]
Epoch [89/120    avg_loss:0.014, val_acc:0.987]
Epoch [90/120    avg_loss:0.015, val_acc:0.987]
Epoch [91/120    avg_loss:0.018, val_acc:0.987]
Epoch [92/120    avg_loss:0.015, val_acc:0.987]
Epoch [93/120    avg_loss:0.018, val_acc:0.987]
Epoch [94/120    avg_loss:0.017, val_acc:0.987]
Epoch [95/120    avg_loss:0.015, val_acc:0.987]
Epoch [96/120    avg_loss:0.014, val_acc:0.987]
Epoch [97/120    avg_loss:0.019, val_acc:0.987]
Epoch [98/120    avg_loss:0.016, val_acc:0.987]
Epoch [99/120    avg_loss:0.016, val_acc:0.987]
Epoch [100/120    avg_loss:0.017, val_acc:0.987]
Epoch [101/120    avg_loss:0.016, val_acc:0.987]
Epoch [102/120    avg_loss:0.018, val_acc:0.987]
Epoch [103/120    avg_loss:0.014, val_acc:0.987]
Epoch [104/120    avg_loss:0.014, val_acc:0.987]
Epoch [105/120    avg_loss:0.016, val_acc:0.987]
Epoch [106/120    avg_loss:0.015, val_acc:0.987]
Epoch [107/120    avg_loss:0.017, val_acc:0.987]
Epoch [108/120    avg_loss:0.013, val_acc:0.987]
Epoch [109/120    avg_loss:0.013, val_acc:0.987]
Epoch [110/120    avg_loss:0.013, val_acc:0.987]
Epoch [111/120    avg_loss:0.015, val_acc:0.987]
Epoch [112/120    avg_loss:0.015, val_acc:0.987]
Epoch [113/120    avg_loss:0.014, val_acc:0.987]
Epoch [114/120    avg_loss:0.013, val_acc:0.987]
Epoch [115/120    avg_loss:0.015, val_acc:0.987]
Epoch [116/120    avg_loss:0.014, val_acc:0.987]
Epoch [117/120    avg_loss:0.015, val_acc:0.987]
Epoch [118/120    avg_loss:0.015, val_acc:0.987]
Epoch [119/120    avg_loss:0.014, val_acc:0.987]
Epoch [120/120    avg_loss:0.012, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6357     0     0     0     0     0     2    58    15]
 [    0     0 17986     0    57     0    32     0    15     0]
 [    0     5     0  1995     0     0     0     0    33     3]
 [    0    31    12     0  2900     0    13     5    11     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    30     3     0     0  4845     0     0     0]
 [    0    12     0     0     0     0     1  1275     1     1]
 [    0     6     0     5    32     0     0     0  3512    16]
 [    0     0     0     0    15    25     0     0     0   879]]

Accuracy:
98.94199021521703

F1 scores:
[       nan 0.98995562 0.99595769 0.98786828 0.97054886 0.99051233
 0.99191319 0.99144635 0.97542008 0.95908347]

Kappa:
0.9859908331498604
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d75558978>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.052, val_acc:0.223]
Epoch [2/120    avg_loss:1.611, val_acc:0.247]
Epoch [3/120    avg_loss:1.369, val_acc:0.346]
Epoch [4/120    avg_loss:1.209, val_acc:0.431]
Epoch [5/120    avg_loss:1.021, val_acc:0.427]
Epoch [6/120    avg_loss:0.877, val_acc:0.477]
Epoch [7/120    avg_loss:0.765, val_acc:0.520]
Epoch [8/120    avg_loss:0.672, val_acc:0.535]
Epoch [9/120    avg_loss:0.618, val_acc:0.547]
Epoch [10/120    avg_loss:0.530, val_acc:0.598]
Epoch [11/120    avg_loss:0.440, val_acc:0.657]
Epoch [12/120    avg_loss:0.387, val_acc:0.688]
Epoch [13/120    avg_loss:0.373, val_acc:0.728]
Epoch [14/120    avg_loss:0.336, val_acc:0.792]
Epoch [15/120    avg_loss:0.324, val_acc:0.826]
Epoch [16/120    avg_loss:0.280, val_acc:0.861]
Epoch [17/120    avg_loss:0.260, val_acc:0.854]
Epoch [18/120    avg_loss:0.248, val_acc:0.872]
Epoch [19/120    avg_loss:0.224, val_acc:0.872]
Epoch [20/120    avg_loss:0.223, val_acc:0.866]
Epoch [21/120    avg_loss:0.206, val_acc:0.914]
Epoch [22/120    avg_loss:0.194, val_acc:0.922]
Epoch [23/120    avg_loss:0.154, val_acc:0.934]
Epoch [24/120    avg_loss:0.159, val_acc:0.941]
Epoch [25/120    avg_loss:0.170, val_acc:0.902]
Epoch [26/120    avg_loss:0.151, val_acc:0.907]
Epoch [27/120    avg_loss:0.151, val_acc:0.944]
Epoch [28/120    avg_loss:0.130, val_acc:0.929]
Epoch [29/120    avg_loss:0.123, val_acc:0.948]
Epoch [30/120    avg_loss:0.101, val_acc:0.953]
Epoch [31/120    avg_loss:0.133, val_acc:0.925]
Epoch [32/120    avg_loss:0.093, val_acc:0.968]
Epoch [33/120    avg_loss:0.069, val_acc:0.971]
Epoch [34/120    avg_loss:0.066, val_acc:0.958]
Epoch [35/120    avg_loss:0.090, val_acc:0.949]
Epoch [36/120    avg_loss:0.095, val_acc:0.931]
Epoch [37/120    avg_loss:0.070, val_acc:0.964]
Epoch [38/120    avg_loss:0.067, val_acc:0.947]
Epoch [39/120    avg_loss:0.057, val_acc:0.972]
Epoch [40/120    avg_loss:0.083, val_acc:0.960]
Epoch [41/120    avg_loss:0.066, val_acc:0.943]
Epoch [42/120    avg_loss:0.060, val_acc:0.959]
Epoch [43/120    avg_loss:0.065, val_acc:0.969]
Epoch [44/120    avg_loss:0.055, val_acc:0.978]
Epoch [45/120    avg_loss:0.043, val_acc:0.968]
Epoch [46/120    avg_loss:0.030, val_acc:0.983]
Epoch [47/120    avg_loss:0.059, val_acc:0.949]
Epoch [48/120    avg_loss:0.063, val_acc:0.968]
Epoch [49/120    avg_loss:0.036, val_acc:0.973]
Epoch [50/120    avg_loss:0.036, val_acc:0.974]
Epoch [51/120    avg_loss:0.038, val_acc:0.983]
Epoch [52/120    avg_loss:0.023, val_acc:0.981]
Epoch [53/120    avg_loss:0.030, val_acc:0.975]
Epoch [54/120    avg_loss:0.025, val_acc:0.982]
Epoch [55/120    avg_loss:0.023, val_acc:0.978]
Epoch [56/120    avg_loss:0.037, val_acc:0.913]
Epoch [57/120    avg_loss:0.089, val_acc:0.966]
Epoch [58/120    avg_loss:0.059, val_acc:0.977]
Epoch [59/120    avg_loss:0.039, val_acc:0.980]
Epoch [60/120    avg_loss:0.028, val_acc:0.972]
Epoch [61/120    avg_loss:0.033, val_acc:0.981]
Epoch [62/120    avg_loss:0.035, val_acc:0.978]
Epoch [63/120    avg_loss:0.026, val_acc:0.979]
Epoch [64/120    avg_loss:0.024, val_acc:0.982]
Epoch [65/120    avg_loss:0.018, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.021, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.985]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.013, val_acc:0.984]
Epoch [75/120    avg_loss:0.011, val_acc:0.984]
Epoch [76/120    avg_loss:0.014, val_acc:0.985]
Epoch [77/120    avg_loss:0.011, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.985]
Epoch [80/120    avg_loss:0.014, val_acc:0.985]
Epoch [81/120    avg_loss:0.013, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.012, val_acc:0.987]
Epoch [86/120    avg_loss:0.014, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.985]
Epoch [90/120    avg_loss:0.013, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.009, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.987]
Epoch [94/120    avg_loss:0.010, val_acc:0.986]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.012, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.011, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.987]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.987]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6375     0     0     2     0    15    25    12     3]
 [    0     0 18063     0    25     0     2     0     0     0]
 [    0     8     0  1963     0     0     0     0    65     0]
 [    0    16     8     0  2937     0     7     0     3     1]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    55     0     0     0  4823     0     0     0]
 [    0    14     0     0     0     2     1  1267     0     6]
 [    0    27     0    24    62     0     7     1  3449     1]
 [    0     0     0     0     7    18     0     0     0   894]]

Accuracy:
98.99501120670956

F1 scores:
[       nan 0.99052206 0.99751491 0.97588864 0.97818485 0.99239544
 0.99106134 0.98102981 0.9715493  0.98026316]

Kappa:
0.9866777633532223
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3f5a338d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.018, val_acc:0.518]
Epoch [2/120    avg_loss:1.655, val_acc:0.550]
Epoch [3/120    avg_loss:1.420, val_acc:0.630]
Epoch [4/120    avg_loss:1.228, val_acc:0.680]
Epoch [5/120    avg_loss:1.051, val_acc:0.682]
Epoch [6/120    avg_loss:0.912, val_acc:0.693]
Epoch [7/120    avg_loss:0.803, val_acc:0.705]
Epoch [8/120    avg_loss:0.702, val_acc:0.698]
Epoch [9/120    avg_loss:0.610, val_acc:0.727]
Epoch [10/120    avg_loss:0.515, val_acc:0.757]
Epoch [11/120    avg_loss:0.482, val_acc:0.771]
Epoch [12/120    avg_loss:0.406, val_acc:0.815]
Epoch [13/120    avg_loss:0.386, val_acc:0.786]
Epoch [14/120    avg_loss:0.321, val_acc:0.830]
Epoch [15/120    avg_loss:0.296, val_acc:0.841]
Epoch [16/120    avg_loss:0.272, val_acc:0.869]
Epoch [17/120    avg_loss:0.249, val_acc:0.816]
Epoch [18/120    avg_loss:0.242, val_acc:0.863]
Epoch [19/120    avg_loss:0.218, val_acc:0.895]
Epoch [20/120    avg_loss:0.210, val_acc:0.910]
Epoch [21/120    avg_loss:0.221, val_acc:0.891]
Epoch [22/120    avg_loss:0.180, val_acc:0.936]
Epoch [23/120    avg_loss:0.161, val_acc:0.949]
Epoch [24/120    avg_loss:0.146, val_acc:0.947]
Epoch [25/120    avg_loss:0.140, val_acc:0.947]
Epoch [26/120    avg_loss:0.133, val_acc:0.943]
Epoch [27/120    avg_loss:0.097, val_acc:0.953]
Epoch [28/120    avg_loss:0.113, val_acc:0.943]
Epoch [29/120    avg_loss:0.111, val_acc:0.957]
Epoch [30/120    avg_loss:0.088, val_acc:0.958]
Epoch [31/120    avg_loss:0.079, val_acc:0.953]
Epoch [32/120    avg_loss:0.081, val_acc:0.939]
Epoch [33/120    avg_loss:0.064, val_acc:0.946]
Epoch [34/120    avg_loss:0.073, val_acc:0.940]
Epoch [35/120    avg_loss:0.090, val_acc:0.966]
Epoch [36/120    avg_loss:0.061, val_acc:0.973]
Epoch [37/120    avg_loss:0.056, val_acc:0.956]
Epoch [38/120    avg_loss:0.078, val_acc:0.936]
Epoch [39/120    avg_loss:0.067, val_acc:0.966]
Epoch [40/120    avg_loss:0.066, val_acc:0.973]
Epoch [41/120    avg_loss:0.055, val_acc:0.968]
Epoch [42/120    avg_loss:0.055, val_acc:0.967]
Epoch [43/120    avg_loss:0.059, val_acc:0.968]
Epoch [44/120    avg_loss:0.058, val_acc:0.977]
Epoch [45/120    avg_loss:0.042, val_acc:0.982]
Epoch [46/120    avg_loss:0.028, val_acc:0.981]
Epoch [47/120    avg_loss:0.023, val_acc:0.979]
Epoch [48/120    avg_loss:0.020, val_acc:0.971]
Epoch [49/120    avg_loss:0.036, val_acc:0.970]
Epoch [50/120    avg_loss:0.029, val_acc:0.978]
Epoch [51/120    avg_loss:0.023, val_acc:0.980]
Epoch [52/120    avg_loss:0.033, val_acc:0.978]
Epoch [53/120    avg_loss:0.030, val_acc:0.979]
Epoch [54/120    avg_loss:0.024, val_acc:0.978]
Epoch [55/120    avg_loss:0.026, val_acc:0.973]
Epoch [56/120    avg_loss:0.021, val_acc:0.978]
Epoch [57/120    avg_loss:0.020, val_acc:0.968]
Epoch [58/120    avg_loss:0.027, val_acc:0.964]
Epoch [59/120    avg_loss:0.020, val_acc:0.980]
Epoch [60/120    avg_loss:0.014, val_acc:0.982]
Epoch [61/120    avg_loss:0.011, val_acc:0.983]
Epoch [62/120    avg_loss:0.013, val_acc:0.982]
Epoch [63/120    avg_loss:0.014, val_acc:0.981]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.012, val_acc:0.982]
Epoch [66/120    avg_loss:0.012, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.983]
Epoch [68/120    avg_loss:0.012, val_acc:0.983]
Epoch [69/120    avg_loss:0.016, val_acc:0.983]
Epoch [70/120    avg_loss:0.015, val_acc:0.983]
Epoch [71/120    avg_loss:0.010, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.012, val_acc:0.985]
Epoch [76/120    avg_loss:0.011, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.983]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.016, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.983]
Epoch [81/120    avg_loss:0.013, val_acc:0.983]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.013, val_acc:0.986]
Epoch [85/120    avg_loss:0.014, val_acc:0.985]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.013, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.015, val_acc:0.981]
Epoch [96/120    avg_loss:0.016, val_acc:0.983]
Epoch [97/120    avg_loss:0.017, val_acc:0.986]
Epoch [98/120    avg_loss:0.013, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.983]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.014, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.016, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.012, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6310     0     0     2     0     0    10   110     0]
 [    0     0 18008     0    37     0    42     0     3     0]
 [    0     0     1  1988     0     0     0     0    43     4]
 [    0    16     3     0  2945     0     4     0     4     0]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    14     0     2     0  4839     0    23     0]
 [    0     3     0     0     0     0     2  1285     0     0]
 [    0    46     0    24    68     0     4     0  3427     2]
 [    0     0     0     0    14    31     0     0     0   874]]

Accuracy:
98.76605692526451

F1 scores:
[       nan 0.98539861 0.99723114 0.98221344 0.97516556 0.98826202
 0.99068482 0.99419729 0.95446317 0.97165092]

Kappa:
0.9836630152757129
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92efc18908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.038, val_acc:0.492]
Epoch [2/120    avg_loss:1.617, val_acc:0.584]
Epoch [3/120    avg_loss:1.399, val_acc:0.340]
Epoch [4/120    avg_loss:1.211, val_acc:0.407]
Epoch [5/120    avg_loss:1.059, val_acc:0.423]
Epoch [6/120    avg_loss:0.952, val_acc:0.457]
Epoch [7/120    avg_loss:0.810, val_acc:0.477]
Epoch [8/120    avg_loss:0.689, val_acc:0.528]
Epoch [9/120    avg_loss:0.604, val_acc:0.723]
Epoch [10/120    avg_loss:0.555, val_acc:0.785]
Epoch [11/120    avg_loss:0.493, val_acc:0.763]
Epoch [12/120    avg_loss:0.427, val_acc:0.838]
Epoch [13/120    avg_loss:0.398, val_acc:0.835]
Epoch [14/120    avg_loss:0.381, val_acc:0.740]
Epoch [15/120    avg_loss:0.416, val_acc:0.821]
Epoch [16/120    avg_loss:0.355, val_acc:0.837]
Epoch [17/120    avg_loss:0.340, val_acc:0.799]
Epoch [18/120    avg_loss:0.301, val_acc:0.828]
Epoch [19/120    avg_loss:0.255, val_acc:0.857]
Epoch [20/120    avg_loss:0.256, val_acc:0.858]
Epoch [21/120    avg_loss:0.230, val_acc:0.887]
Epoch [22/120    avg_loss:0.252, val_acc:0.872]
Epoch [23/120    avg_loss:0.207, val_acc:0.906]
Epoch [24/120    avg_loss:0.218, val_acc:0.864]
Epoch [25/120    avg_loss:0.201, val_acc:0.885]
Epoch [26/120    avg_loss:0.164, val_acc:0.869]
Epoch [27/120    avg_loss:0.160, val_acc:0.902]
Epoch [28/120    avg_loss:0.154, val_acc:0.894]
Epoch [29/120    avg_loss:0.133, val_acc:0.939]
Epoch [30/120    avg_loss:0.118, val_acc:0.948]
Epoch [31/120    avg_loss:0.139, val_acc:0.934]
Epoch [32/120    avg_loss:0.111, val_acc:0.938]
Epoch [33/120    avg_loss:0.110, val_acc:0.953]
Epoch [34/120    avg_loss:0.105, val_acc:0.940]
Epoch [35/120    avg_loss:0.085, val_acc:0.927]
Epoch [36/120    avg_loss:0.075, val_acc:0.942]
Epoch [37/120    avg_loss:0.086, val_acc:0.928]
Epoch [38/120    avg_loss:0.064, val_acc:0.943]
Epoch [39/120    avg_loss:0.061, val_acc:0.958]
Epoch [40/120    avg_loss:0.060, val_acc:0.960]
Epoch [41/120    avg_loss:0.053, val_acc:0.968]
Epoch [42/120    avg_loss:0.044, val_acc:0.953]
Epoch [43/120    avg_loss:0.062, val_acc:0.944]
Epoch [44/120    avg_loss:0.060, val_acc:0.961]
Epoch [45/120    avg_loss:0.038, val_acc:0.960]
Epoch [46/120    avg_loss:0.042, val_acc:0.946]
Epoch [47/120    avg_loss:0.053, val_acc:0.966]
Epoch [48/120    avg_loss:0.076, val_acc:0.917]
Epoch [49/120    avg_loss:0.064, val_acc:0.964]
Epoch [50/120    avg_loss:0.044, val_acc:0.971]
Epoch [51/120    avg_loss:0.032, val_acc:0.933]
Epoch [52/120    avg_loss:0.039, val_acc:0.964]
Epoch [53/120    avg_loss:0.040, val_acc:0.968]
Epoch [54/120    avg_loss:0.029, val_acc:0.964]
Epoch [55/120    avg_loss:0.042, val_acc:0.969]
Epoch [56/120    avg_loss:0.029, val_acc:0.967]
Epoch [57/120    avg_loss:0.028, val_acc:0.977]
Epoch [58/120    avg_loss:0.057, val_acc:0.949]
Epoch [59/120    avg_loss:0.050, val_acc:0.966]
Epoch [60/120    avg_loss:0.039, val_acc:0.965]
Epoch [61/120    avg_loss:0.036, val_acc:0.952]
Epoch [62/120    avg_loss:0.046, val_acc:0.960]
Epoch [63/120    avg_loss:0.031, val_acc:0.973]
Epoch [64/120    avg_loss:0.031, val_acc:0.974]
Epoch [65/120    avg_loss:0.042, val_acc:0.953]
Epoch [66/120    avg_loss:0.024, val_acc:0.977]
Epoch [67/120    avg_loss:0.028, val_acc:0.951]
Epoch [68/120    avg_loss:0.030, val_acc:0.963]
Epoch [69/120    avg_loss:0.018, val_acc:0.975]
Epoch [70/120    avg_loss:0.015, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.946]
Epoch [73/120    avg_loss:0.030, val_acc:0.967]
Epoch [74/120    avg_loss:0.022, val_acc:0.970]
Epoch [75/120    avg_loss:0.017, val_acc:0.973]
Epoch [76/120    avg_loss:0.015, val_acc:0.970]
Epoch [77/120    avg_loss:0.013, val_acc:0.982]
Epoch [78/120    avg_loss:0.014, val_acc:0.976]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.013, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.979]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.019, val_acc:0.978]
Epoch [86/120    avg_loss:0.019, val_acc:0.983]
Epoch [87/120    avg_loss:0.019, val_acc:0.975]
Epoch [88/120    avg_loss:0.025, val_acc:0.972]
Epoch [89/120    avg_loss:0.026, val_acc:0.974]
Epoch [90/120    avg_loss:0.103, val_acc:0.927]
Epoch [91/120    avg_loss:0.080, val_acc:0.966]
Epoch [92/120    avg_loss:0.031, val_acc:0.964]
Epoch [93/120    avg_loss:0.026, val_acc:0.975]
Epoch [94/120    avg_loss:0.025, val_acc:0.960]
Epoch [95/120    avg_loss:0.034, val_acc:0.971]
Epoch [96/120    avg_loss:0.022, val_acc:0.979]
Epoch [97/120    avg_loss:0.028, val_acc:0.982]
Epoch [98/120    avg_loss:0.040, val_acc:0.931]
Epoch [99/120    avg_loss:0.081, val_acc:0.963]
Epoch [100/120    avg_loss:0.041, val_acc:0.968]
Epoch [101/120    avg_loss:0.025, val_acc:0.969]
Epoch [102/120    avg_loss:0.026, val_acc:0.972]
Epoch [103/120    avg_loss:0.025, val_acc:0.973]
Epoch [104/120    avg_loss:0.026, val_acc:0.976]
Epoch [105/120    avg_loss:0.018, val_acc:0.976]
Epoch [106/120    avg_loss:0.020, val_acc:0.973]
Epoch [107/120    avg_loss:0.019, val_acc:0.975]
Epoch [108/120    avg_loss:0.015, val_acc:0.975]
Epoch [109/120    avg_loss:0.016, val_acc:0.976]
Epoch [110/120    avg_loss:0.014, val_acc:0.976]
Epoch [111/120    avg_loss:0.017, val_acc:0.974]
Epoch [112/120    avg_loss:0.014, val_acc:0.974]
Epoch [113/120    avg_loss:0.015, val_acc:0.974]
Epoch [114/120    avg_loss:0.016, val_acc:0.975]
Epoch [115/120    avg_loss:0.015, val_acc:0.975]
Epoch [116/120    avg_loss:0.013, val_acc:0.974]
Epoch [117/120    avg_loss:0.015, val_acc:0.974]
Epoch [118/120    avg_loss:0.017, val_acc:0.974]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.014, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6332     0     2     0     0     0    18    80     0]
 [    0     0 17990     0    19     0    75     0     5     1]
 [    0     5     0  1978     0     0     0     0    51     2]
 [    0    33    10     9  2882     0    10     0    22     6]
 [    0     0     0     0     0  1304     0     0     0     1]
 [    0     0    36     0     0     0  4836     0     6     0]
 [    0     2     0     0     0     0     2  1284     1     1]
 [    0     4     0    16    52     0     1     0  3491     7]
 [    0     0     0     1     0    26     0     0     0   892]]

Accuracy:
98.78533728580724

F1 scores:
[       nan 0.98875703 0.99595859 0.9787234  0.972827   0.98975332
 0.9867374  0.99074074 0.96609935 0.97539639]

Kappa:
0.9839159634456892
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1cc2221940>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:2.007, val_acc:0.118]
Epoch [2/120    avg_loss:1.668, val_acc:0.191]
Epoch [3/120    avg_loss:1.464, val_acc:0.272]
Epoch [4/120    avg_loss:1.242, val_acc:0.472]
Epoch [5/120    avg_loss:1.111, val_acc:0.527]
Epoch [6/120    avg_loss:0.961, val_acc:0.525]
Epoch [7/120    avg_loss:0.849, val_acc:0.547]
Epoch [8/120    avg_loss:0.710, val_acc:0.609]
Epoch [9/120    avg_loss:0.602, val_acc:0.652]
Epoch [10/120    avg_loss:0.531, val_acc:0.711]
Epoch [11/120    avg_loss:0.488, val_acc:0.709]
Epoch [12/120    avg_loss:0.424, val_acc:0.792]
Epoch [13/120    avg_loss:0.368, val_acc:0.798]
Epoch [14/120    avg_loss:0.355, val_acc:0.798]
Epoch [15/120    avg_loss:0.335, val_acc:0.808]
Epoch [16/120    avg_loss:0.297, val_acc:0.786]
Epoch [17/120    avg_loss:0.327, val_acc:0.815]
Epoch [18/120    avg_loss:0.280, val_acc:0.778]
Epoch [19/120    avg_loss:0.247, val_acc:0.850]
Epoch [20/120    avg_loss:0.242, val_acc:0.873]
Epoch [21/120    avg_loss:0.205, val_acc:0.890]
Epoch [22/120    avg_loss:0.231, val_acc:0.885]
Epoch [23/120    avg_loss:0.190, val_acc:0.903]
Epoch [24/120    avg_loss:0.174, val_acc:0.867]
Epoch [25/120    avg_loss:0.163, val_acc:0.869]
Epoch [26/120    avg_loss:0.152, val_acc:0.941]
Epoch [27/120    avg_loss:0.115, val_acc:0.942]
Epoch [28/120    avg_loss:0.110, val_acc:0.939]
Epoch [29/120    avg_loss:0.097, val_acc:0.903]
Epoch [30/120    avg_loss:0.094, val_acc:0.952]
Epoch [31/120    avg_loss:0.093, val_acc:0.947]
Epoch [32/120    avg_loss:0.085, val_acc:0.953]
Epoch [33/120    avg_loss:0.064, val_acc:0.949]
Epoch [34/120    avg_loss:0.065, val_acc:0.950]
Epoch [35/120    avg_loss:0.065, val_acc:0.937]
Epoch [36/120    avg_loss:0.061, val_acc:0.953]
Epoch [37/120    avg_loss:0.083, val_acc:0.961]
Epoch [38/120    avg_loss:0.074, val_acc:0.948]
Epoch [39/120    avg_loss:0.067, val_acc:0.957]
Epoch [40/120    avg_loss:0.072, val_acc:0.942]
Epoch [41/120    avg_loss:0.053, val_acc:0.974]
Epoch [42/120    avg_loss:0.038, val_acc:0.970]
Epoch [43/120    avg_loss:0.029, val_acc:0.976]
Epoch [44/120    avg_loss:0.035, val_acc:0.964]
Epoch [45/120    avg_loss:0.033, val_acc:0.970]
Epoch [46/120    avg_loss:0.032, val_acc:0.971]
Epoch [47/120    avg_loss:0.032, val_acc:0.974]
Epoch [48/120    avg_loss:0.029, val_acc:0.964]
Epoch [49/120    avg_loss:0.032, val_acc:0.967]
Epoch [50/120    avg_loss:0.029, val_acc:0.972]
Epoch [51/120    avg_loss:0.021, val_acc:0.974]
Epoch [52/120    avg_loss:0.038, val_acc:0.958]
Epoch [53/120    avg_loss:0.033, val_acc:0.971]
Epoch [54/120    avg_loss:0.033, val_acc:0.959]
Epoch [55/120    avg_loss:0.050, val_acc:0.950]
Epoch [56/120    avg_loss:0.031, val_acc:0.973]
Epoch [57/120    avg_loss:0.019, val_acc:0.978]
Epoch [58/120    avg_loss:0.018, val_acc:0.977]
Epoch [59/120    avg_loss:0.015, val_acc:0.978]
Epoch [60/120    avg_loss:0.017, val_acc:0.980]
Epoch [61/120    avg_loss:0.019, val_acc:0.976]
Epoch [62/120    avg_loss:0.016, val_acc:0.979]
Epoch [63/120    avg_loss:0.020, val_acc:0.978]
Epoch [64/120    avg_loss:0.018, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.979]
Epoch [66/120    avg_loss:0.017, val_acc:0.980]
Epoch [67/120    avg_loss:0.016, val_acc:0.982]
Epoch [68/120    avg_loss:0.016, val_acc:0.978]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.011, val_acc:0.979]
Epoch [71/120    avg_loss:0.012, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.978]
Epoch [73/120    avg_loss:0.014, val_acc:0.979]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.013, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.979]
Epoch [77/120    avg_loss:0.014, val_acc:0.979]
Epoch [78/120    avg_loss:0.011, val_acc:0.978]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.017, val_acc:0.979]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.014, val_acc:0.980]
Epoch [84/120    avg_loss:0.011, val_acc:0.980]
Epoch [85/120    avg_loss:0.013, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.012, val_acc:0.981]
Epoch [88/120    avg_loss:0.012, val_acc:0.981]
Epoch [89/120    avg_loss:0.012, val_acc:0.981]
Epoch [90/120    avg_loss:0.015, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.981]
Epoch [92/120    avg_loss:0.015, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.980]
Epoch [94/120    avg_loss:0.012, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.980]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.011, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.980]
Epoch [103/120    avg_loss:0.015, val_acc:0.980]
Epoch [104/120    avg_loss:0.012, val_acc:0.980]
Epoch [105/120    avg_loss:0.014, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.980]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.012, val_acc:0.980]
Epoch [112/120    avg_loss:0.015, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.980]
Epoch [114/120    avg_loss:0.012, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6290     0     0     3     0     0    12   118     9]
 [    0     5 18013     0    34     0    38     0     0     0]
 [    0    10     0  1934     0     0     0     0    90     2]
 [    0    20    12     4  2890     0    22    16     6     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    28     0     0     0  4832     0    18     0]
 [    0     1     0     0     0     0     0  1288     0     1]
 [    0    41     0    49    47     0     0     0  3434     0]
 [    0     0     0     0    12    14     0     0     0   893]]

Accuracy:
98.52023232834455

F1 scores:
[       nan 0.98288929 0.99676286 0.96147154 0.9701242  0.99466463
 0.98915046 0.9884881  0.94901202 0.97809419]

Kappa:
0.98040270095247
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa201cd2908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.986, val_acc:0.194]
Epoch [2/120    avg_loss:1.650, val_acc:0.372]
Epoch [3/120    avg_loss:1.377, val_acc:0.405]
Epoch [4/120    avg_loss:1.194, val_acc:0.426]
Epoch [5/120    avg_loss:1.004, val_acc:0.488]
Epoch [6/120    avg_loss:0.866, val_acc:0.540]
Epoch [7/120    avg_loss:0.734, val_acc:0.631]
Epoch [8/120    avg_loss:0.633, val_acc:0.647]
Epoch [9/120    avg_loss:0.562, val_acc:0.660]
Epoch [10/120    avg_loss:0.475, val_acc:0.675]
Epoch [11/120    avg_loss:0.414, val_acc:0.784]
Epoch [12/120    avg_loss:0.377, val_acc:0.767]
Epoch [13/120    avg_loss:0.371, val_acc:0.718]
Epoch [14/120    avg_loss:0.317, val_acc:0.734]
Epoch [15/120    avg_loss:0.303, val_acc:0.779]
Epoch [16/120    avg_loss:0.278, val_acc:0.840]
Epoch [17/120    avg_loss:0.290, val_acc:0.777]
Epoch [18/120    avg_loss:0.259, val_acc:0.836]
Epoch [19/120    avg_loss:0.228, val_acc:0.910]
Epoch [20/120    avg_loss:0.224, val_acc:0.922]
Epoch [21/120    avg_loss:0.193, val_acc:0.923]
Epoch [22/120    avg_loss:0.180, val_acc:0.917]
Epoch [23/120    avg_loss:0.151, val_acc:0.948]
Epoch [24/120    avg_loss:0.134, val_acc:0.948]
Epoch [25/120    avg_loss:0.126, val_acc:0.943]
Epoch [26/120    avg_loss:0.117, val_acc:0.950]
Epoch [27/120    avg_loss:0.135, val_acc:0.949]
Epoch [28/120    avg_loss:0.111, val_acc:0.947]
Epoch [29/120    avg_loss:0.102, val_acc:0.910]
Epoch [30/120    avg_loss:0.085, val_acc:0.877]
Epoch [31/120    avg_loss:0.066, val_acc:0.963]
Epoch [32/120    avg_loss:0.087, val_acc:0.957]
Epoch [33/120    avg_loss:0.055, val_acc:0.968]
Epoch [34/120    avg_loss:0.041, val_acc:0.963]
Epoch [35/120    avg_loss:0.066, val_acc:0.950]
Epoch [36/120    avg_loss:0.055, val_acc:0.977]
Epoch [37/120    avg_loss:0.045, val_acc:0.923]
Epoch [38/120    avg_loss:0.089, val_acc:0.930]
Epoch [39/120    avg_loss:0.065, val_acc:0.977]
Epoch [40/120    avg_loss:0.050, val_acc:0.972]
Epoch [41/120    avg_loss:0.057, val_acc:0.966]
Epoch [42/120    avg_loss:0.040, val_acc:0.963]
Epoch [43/120    avg_loss:0.045, val_acc:0.971]
Epoch [44/120    avg_loss:0.028, val_acc:0.973]
Epoch [45/120    avg_loss:0.028, val_acc:0.977]
Epoch [46/120    avg_loss:0.028, val_acc:0.978]
Epoch [47/120    avg_loss:0.033, val_acc:0.971]
Epoch [48/120    avg_loss:0.024, val_acc:0.979]
Epoch [49/120    avg_loss:0.033, val_acc:0.935]
Epoch [50/120    avg_loss:0.040, val_acc:0.973]
Epoch [51/120    avg_loss:0.028, val_acc:0.979]
Epoch [52/120    avg_loss:0.017, val_acc:0.982]
Epoch [53/120    avg_loss:0.015, val_acc:0.983]
Epoch [54/120    avg_loss:0.015, val_acc:0.981]
Epoch [55/120    avg_loss:0.022, val_acc:0.983]
Epoch [56/120    avg_loss:0.019, val_acc:0.978]
Epoch [57/120    avg_loss:0.019, val_acc:0.983]
Epoch [58/120    avg_loss:0.014, val_acc:0.983]
Epoch [59/120    avg_loss:0.018, val_acc:0.969]
Epoch [60/120    avg_loss:0.020, val_acc:0.979]
Epoch [61/120    avg_loss:0.019, val_acc:0.981]
Epoch [62/120    avg_loss:0.011, val_acc:0.983]
Epoch [63/120    avg_loss:0.015, val_acc:0.980]
Epoch [64/120    avg_loss:0.015, val_acc:0.984]
Epoch [65/120    avg_loss:0.025, val_acc:0.978]
Epoch [66/120    avg_loss:0.023, val_acc:0.983]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.014, val_acc:0.948]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.014, val_acc:0.979]
Epoch [71/120    avg_loss:0.019, val_acc:0.973]
Epoch [72/120    avg_loss:0.018, val_acc:0.979]
Epoch [73/120    avg_loss:0.012, val_acc:0.984]
Epoch [74/120    avg_loss:0.014, val_acc:0.981]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.023, val_acc:0.958]
Epoch [78/120    avg_loss:0.014, val_acc:0.979]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.021, val_acc:0.979]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.021, val_acc:0.963]
Epoch [85/120    avg_loss:0.100, val_acc:0.958]
Epoch [86/120    avg_loss:0.053, val_acc:0.968]
Epoch [87/120    avg_loss:0.091, val_acc:0.966]
Epoch [88/120    avg_loss:0.057, val_acc:0.922]
Epoch [89/120    avg_loss:0.035, val_acc:0.974]
Epoch [90/120    avg_loss:0.026, val_acc:0.978]
Epoch [91/120    avg_loss:0.035, val_acc:0.953]
Epoch [92/120    avg_loss:0.034, val_acc:0.981]
Epoch [93/120    avg_loss:0.020, val_acc:0.975]
Epoch [94/120    avg_loss:0.023, val_acc:0.971]
Epoch [95/120    avg_loss:0.018, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.982]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.010, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6312     0     0     1     0    39     5    59    16]
 [    0     0 18056     0    23     0     0     0    11     0]
 [    0     0     0  1972     0     0     0     0    56     8]
 [    0    12     6     0  2933     0     2     0    17     2]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0     7     3     0     0  4853     0    15     0]
 [    0     2     0     0     0     0     0  1285     2     1]
 [    0    50     0    32    59     0     0     0  3428     2]
 [    0     0     0     0    10     7     0     0     0   902]]

Accuracy:
98.92270985467428

F1 scores:
[       nan 0.98563398 0.99870019 0.97551323 0.97799266 0.99732518
 0.99324601 0.99612403 0.95767565 0.97513514]

Kappa:
0.9857305232843959
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f096fc0b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.910, val_acc:0.198]
Epoch [2/120    avg_loss:1.473, val_acc:0.242]
Epoch [3/120    avg_loss:1.266, val_acc:0.355]
Epoch [4/120    avg_loss:1.139, val_acc:0.403]
Epoch [5/120    avg_loss:0.964, val_acc:0.582]
Epoch [6/120    avg_loss:0.840, val_acc:0.638]
Epoch [7/120    avg_loss:0.729, val_acc:0.673]
Epoch [8/120    avg_loss:0.620, val_acc:0.674]
Epoch [9/120    avg_loss:0.551, val_acc:0.694]
Epoch [10/120    avg_loss:0.456, val_acc:0.747]
Epoch [11/120    avg_loss:0.432, val_acc:0.785]
Epoch [12/120    avg_loss:0.364, val_acc:0.873]
Epoch [13/120    avg_loss:0.333, val_acc:0.893]
Epoch [14/120    avg_loss:0.268, val_acc:0.828]
Epoch [15/120    avg_loss:0.277, val_acc:0.865]
Epoch [16/120    avg_loss:0.222, val_acc:0.893]
Epoch [17/120    avg_loss:0.213, val_acc:0.902]
Epoch [18/120    avg_loss:0.174, val_acc:0.900]
Epoch [19/120    avg_loss:0.159, val_acc:0.961]
Epoch [20/120    avg_loss:0.145, val_acc:0.944]
Epoch [21/120    avg_loss:0.169, val_acc:0.948]
Epoch [22/120    avg_loss:0.114, val_acc:0.953]
Epoch [23/120    avg_loss:0.161, val_acc:0.887]
Epoch [24/120    avg_loss:0.132, val_acc:0.948]
Epoch [25/120    avg_loss:0.112, val_acc:0.949]
Epoch [26/120    avg_loss:0.094, val_acc:0.953]
Epoch [27/120    avg_loss:0.086, val_acc:0.967]
Epoch [28/120    avg_loss:0.075, val_acc:0.958]
Epoch [29/120    avg_loss:0.071, val_acc:0.967]
Epoch [30/120    avg_loss:0.063, val_acc:0.966]
Epoch [31/120    avg_loss:0.057, val_acc:0.958]
Epoch [32/120    avg_loss:0.067, val_acc:0.961]
Epoch [33/120    avg_loss:0.054, val_acc:0.971]
Epoch [34/120    avg_loss:0.089, val_acc:0.959]
Epoch [35/120    avg_loss:0.077, val_acc:0.936]
Epoch [36/120    avg_loss:0.099, val_acc:0.954]
Epoch [37/120    avg_loss:0.049, val_acc:0.972]
Epoch [38/120    avg_loss:0.041, val_acc:0.977]
Epoch [39/120    avg_loss:0.035, val_acc:0.970]
Epoch [40/120    avg_loss:0.035, val_acc:0.978]
Epoch [41/120    avg_loss:0.030, val_acc:0.975]
Epoch [42/120    avg_loss:0.065, val_acc:0.969]
Epoch [43/120    avg_loss:0.046, val_acc:0.974]
Epoch [44/120    avg_loss:0.041, val_acc:0.974]
Epoch [45/120    avg_loss:0.034, val_acc:0.978]
Epoch [46/120    avg_loss:0.028, val_acc:0.963]
Epoch [47/120    avg_loss:0.028, val_acc:0.981]
Epoch [48/120    avg_loss:0.041, val_acc:0.964]
Epoch [49/120    avg_loss:0.066, val_acc:0.978]
Epoch [50/120    avg_loss:0.027, val_acc:0.978]
Epoch [51/120    avg_loss:0.031, val_acc:0.980]
Epoch [52/120    avg_loss:0.025, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.979]
Epoch [54/120    avg_loss:0.012, val_acc:0.985]
Epoch [55/120    avg_loss:0.013, val_acc:0.985]
Epoch [56/120    avg_loss:0.016, val_acc:0.983]
Epoch [57/120    avg_loss:0.013, val_acc:0.988]
Epoch [58/120    avg_loss:0.010, val_acc:0.986]
Epoch [59/120    avg_loss:0.010, val_acc:0.981]
Epoch [60/120    avg_loss:0.012, val_acc:0.988]
Epoch [61/120    avg_loss:0.013, val_acc:0.989]
Epoch [62/120    avg_loss:0.015, val_acc:0.966]
Epoch [63/120    avg_loss:0.019, val_acc:0.975]
Epoch [64/120    avg_loss:0.015, val_acc:0.987]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.010, val_acc:0.988]
Epoch [67/120    avg_loss:0.009, val_acc:0.988]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.017, val_acc:0.983]
Epoch [70/120    avg_loss:0.020, val_acc:0.970]
Epoch [71/120    avg_loss:0.046, val_acc:0.969]
Epoch [72/120    avg_loss:0.036, val_acc:0.989]
Epoch [73/120    avg_loss:0.013, val_acc:0.989]
Epoch [74/120    avg_loss:0.009, val_acc:0.986]
Epoch [75/120    avg_loss:0.007, val_acc:0.989]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.987]
Epoch [79/120    avg_loss:0.022, val_acc:0.977]
Epoch [80/120    avg_loss:0.013, val_acc:0.988]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.011, val_acc:0.989]
Epoch [83/120    avg_loss:0.006, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.991]
Epoch [85/120    avg_loss:0.007, val_acc:0.977]
Epoch [86/120    avg_loss:0.027, val_acc:0.982]
Epoch [87/120    avg_loss:0.027, val_acc:0.973]
Epoch [88/120    avg_loss:0.010, val_acc:0.987]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.991]
Epoch [91/120    avg_loss:0.006, val_acc:0.989]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.989]
Epoch [101/120    avg_loss:0.005, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.992]
Epoch [108/120    avg_loss:0.004, val_acc:0.992]
Epoch [109/120    avg_loss:0.003, val_acc:0.992]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.991]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.003, val_acc:0.991]
Epoch [114/120    avg_loss:0.003, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6340     0     0     0     0     0     0    92     0]
 [    0     0 18078     0     9     0     0     0     3     0]
 [    0     1     0  2009     0     0     0     0    25     1]
 [    0    26     3     0  2922     0     2     0    15     4]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    38     0     1     0  4839     0     0     0]
 [    0     2     0     0     0     0     0  1288     0     0]
 [    0    11     0    19    41     0     0     0  3499     1]
 [    0     0     0     0     4    16     0     0     0   899]]

Accuracy:
99.24324584869737

F1 scores:
[       nan 0.98969716 0.99853628 0.9886811  0.98234997 0.99390708
 0.99578146 0.9992242  0.97126995 0.98574561]

Kappa:
0.9899707083500595
creating ./logs/logs-2022-01-18PaviaU.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/train_gt.npy)
1283 samples selected for training(over 42776)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/PaviaU/0.03/test_gt.npy)
41493 samples selected for training(over 42776)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1283 samples selected for validation(over 42776)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:PaviaU
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.03
sample_nums:20
load_data:0.03
epoch:120
save_epoch:5
patch_size:11
patch_bands:25
reserve_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:10
n_bands:100
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.4623, 0.1646, 1.4603, 1.0000, 2.3000, 0.6093, 2.3000, 0.8288,
        3.2857], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0683c5c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 19631==>0.02M
----------Training process----------
Epoch [1/120    avg_loss:1.974, val_acc:0.159]
Epoch [2/120    avg_loss:1.659, val_acc:0.215]
Epoch [3/120    avg_loss:1.427, val_acc:0.324]
Epoch [4/120    avg_loss:1.207, val_acc:0.369]
Epoch [5/120    avg_loss:1.075, val_acc:0.475]
Epoch [6/120    avg_loss:0.926, val_acc:0.509]
Epoch [7/120    avg_loss:0.813, val_acc:0.618]
Epoch [8/120    avg_loss:0.720, val_acc:0.652]
Epoch [9/120    avg_loss:0.629, val_acc:0.647]
Epoch [10/120    avg_loss:0.565, val_acc:0.691]
Epoch [11/120    avg_loss:0.505, val_acc:0.712]
Epoch [12/120    avg_loss:0.428, val_acc:0.745]
Epoch [13/120    avg_loss:0.399, val_acc:0.773]
Epoch [14/120    avg_loss:0.366, val_acc:0.788]
Epoch [15/120    avg_loss:0.328, val_acc:0.796]
Epoch [16/120    avg_loss:0.297, val_acc:0.847]
Epoch [17/120    avg_loss:0.256, val_acc:0.874]
Epoch [18/120    avg_loss:0.220, val_acc:0.937]
Epoch [19/120    avg_loss:0.185, val_acc:0.920]
Epoch [20/120    avg_loss:0.175, val_acc:0.919]
Epoch [21/120    avg_loss:0.171, val_acc:0.935]
Epoch [22/120    avg_loss:0.152, val_acc:0.919]
Epoch [23/120    avg_loss:0.166, val_acc:0.924]
Epoch [24/120    avg_loss:0.156, val_acc:0.934]
Epoch [25/120    avg_loss:0.174, val_acc:0.953]
Epoch [26/120    avg_loss:0.114, val_acc:0.959]
Epoch [27/120    avg_loss:0.109, val_acc:0.948]
Epoch [28/120    avg_loss:0.100, val_acc:0.967]
Epoch [29/120    avg_loss:0.082, val_acc:0.962]
Epoch [30/120    avg_loss:0.086, val_acc:0.962]
Epoch [31/120    avg_loss:0.062, val_acc:0.970]
Epoch [32/120    avg_loss:0.059, val_acc:0.964]
Epoch [33/120    avg_loss:0.047, val_acc:0.976]
Epoch [34/120    avg_loss:0.067, val_acc:0.963]
Epoch [35/120    avg_loss:0.073, val_acc:0.971]
Epoch [36/120    avg_loss:0.064, val_acc:0.931]
Epoch [37/120    avg_loss:0.053, val_acc:0.961]
Epoch [38/120    avg_loss:0.058, val_acc:0.952]
Epoch [39/120    avg_loss:0.058, val_acc:0.961]
Epoch [40/120    avg_loss:0.030, val_acc:0.979]
Epoch [41/120    avg_loss:0.031, val_acc:0.982]
Epoch [42/120    avg_loss:0.029, val_acc:0.977]
Epoch [43/120    avg_loss:0.029, val_acc:0.979]
Epoch [44/120    avg_loss:0.035, val_acc:0.981]
Epoch [45/120    avg_loss:0.033, val_acc:0.983]
Epoch [46/120    avg_loss:0.029, val_acc:0.981]
Epoch [47/120    avg_loss:0.024, val_acc:0.983]
Epoch [48/120    avg_loss:0.018, val_acc:0.982]
Epoch [49/120    avg_loss:0.019, val_acc:0.983]
Epoch [50/120    avg_loss:0.018, val_acc:0.981]
Epoch [51/120    avg_loss:0.025, val_acc:0.980]
Epoch [52/120    avg_loss:0.019, val_acc:0.978]
Epoch [53/120    avg_loss:0.025, val_acc:0.959]
Epoch [54/120    avg_loss:0.101, val_acc:0.972]
Epoch [55/120    avg_loss:0.036, val_acc:0.969]
Epoch [56/120    avg_loss:0.029, val_acc:0.964]
Epoch [57/120    avg_loss:0.018, val_acc:0.977]
Epoch [58/120    avg_loss:0.019, val_acc:0.978]
Epoch [59/120    avg_loss:0.012, val_acc:0.979]
Epoch [60/120    avg_loss:0.017, val_acc:0.983]
Epoch [61/120    avg_loss:0.020, val_acc:0.979]
Epoch [62/120    avg_loss:0.017, val_acc:0.981]
Epoch [63/120    avg_loss:0.019, val_acc:0.981]
Epoch [64/120    avg_loss:0.022, val_acc:0.980]
Epoch [65/120    avg_loss:0.020, val_acc:0.979]
Epoch [66/120    avg_loss:0.031, val_acc:0.972]
Epoch [67/120    avg_loss:0.034, val_acc:0.971]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.981]
Epoch [70/120    avg_loss:0.014, val_acc:0.984]
Epoch [71/120    avg_loss:0.032, val_acc:0.974]
Epoch [72/120    avg_loss:0.022, val_acc:0.979]
Epoch [73/120    avg_loss:0.017, val_acc:0.978]
Epoch [74/120    avg_loss:0.012, val_acc:0.980]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.010, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.981]
Epoch [78/120    avg_loss:0.028, val_acc:0.976]
Epoch [79/120    avg_loss:0.019, val_acc:0.983]
Epoch [80/120    avg_loss:0.013, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.010, val_acc:0.977]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.013, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.984]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.004, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[    0     0     0     0     0     0     0     0     0     0]
 [    0  6355     0     0     0     0     0     0    70     7]
 [    0     0 18069     0    13     0     0     0     8     0]
 [    0     8     0  1967     0     0     0     0    59     2]
 [    0    31    17     0  2901     0     9     2     9     3]
 [    0     0     0     0     0  1305     0     0     0     0]
 [    0     0    19     0     0     0  4855     0     4     0]
 [    0    10     0     0     0     1     0  1278     0     1]
 [    0    36     0    15    44     0     1     0  3472     3]
 [    0     0     0     0    14    17     0     0     0   888]]

Accuracy:
99.02875183765937

F1 scores:
[       nan 0.98741454 0.9984252  0.97909408 0.97611036 0.99315068
 0.99661295 0.99455253 0.96538301 0.97421832]

Kappa:
0.9871271218273873
